{"https:\/\/github.com\/DataDog\/dd-trace-py":{"77806b8a1ea495888c0c24542cb282378e2c4e16":{"url":"https:\/\/api.github.com\/repos\/DataDog\/dd-trace-py\/commits\/77806b8a1ea495888c0c24542cb282378e2c4e16","html_url":"https:\/\/github.com\/DataDog\/dd-trace-py\/commit\/77806b8a1ea495888c0c24542cb282378e2c4e16","sha":"77806b8a1ea495888c0c24542cb282378e2c4e16","keyword":"function injection correct","diff":"diff --git a\/ddtrace\/internal\/injection.py b\/ddtrace\/internal\/injection.py\nnew file mode 100644\nindex 00000000000..e01993b6751\n--- \/dev\/null\n+++ b\/ddtrace\/internal\/injection.py\n@@ -0,0 +1,165 @@\n+from types import FunctionType\n+from typing import Any\n+from typing import Callable\n+from typing import List\n+from typing import Tuple\n+\n+from bytecode import Bytecode\n+from bytecode import Instr\n+\n+\n+HookType = Callable[[Any], Any]\n+HookInfoType = Tuple[HookType, int, Any]\n+\n+HOOK_ARG_PREFIX = \"_hook_arg\"\n+\n+\n+class InvalidLine(Exception):\n+    \"\"\"\n+    Raised when trying to inject a hook on an invalid line, e.g. a comment or a blank line.\n+    \"\"\"\n+\n+\n+def _inject_hook(code, hook, lineno, arg):\n+    # type: (Bytecode, HookType, int, Any) -> None\n+    \"\"\"Inject a hook at the given line number inside an abstract code object.\n+\n+    The hook is called with the given argument, which is also used as an\n+    identifier for the hook itself. This should be kept in case the hook needs\n+    to be removed.\n+    \"\"\"\n+    for i, instr in enumerate(code):\n+        try:\n+            if instr.lineno == lineno:\n+                # gotcha!\n+                break\n+        except AttributeError:\n+            # pseudo-instruction (e.g. label)\n+            pass\n+    else:\n+        raise InvalidLine(\"Line %d does not exist or is either blank or a comment\" % lineno)\n+\n+    # DEV: This is the bytecode equivalent of\n+    # >>> hook(arg)\n+    # Additionally, we must discard the return value (top of the stack) to\n+    # restore the stack to the state prior to the call.\n+    code[i:i] = Bytecode(\n+        [\n+            Instr(\"LOAD_CONST\", hook, lineno=lineno),\n+            Instr(\"LOAD_CONST\", arg, lineno=lineno),\n+            Instr(\"CALL_FUNCTION\", 1, lineno=lineno),\n+            Instr(\"POP_TOP\", lineno=lineno),\n+        ]\n+    )\n+\n+\n+def _eject_hook(code, hook, line, arg):\n+    # type: (Bytecode, HookType, int, Any) -> None\n+    \"\"\"Eject a hook from the abstract code object at the given line number.\n+\n+    The hook is identified by its argument. This ensures that only the right\n+    hook is ejected.\n+    \"\"\"\n+    for i, instr in enumerate(code):\n+        try:\n+            # DEV: We look at the expected opcode pattern to match the injected\n+            # hook and we also test for the expected opcode arguments\n+            if (\n+                instr.lineno == line\n+                and code[i].arg is hook\n+                and code[i + 1].arg is arg\n+                and [code[_].name for _ in range(i, i + 4)] == [\"LOAD_CONST\", \"LOAD_CONST\", \"CALL_FUNCTION\", \"POP_TOP\"]\n+            ):\n+                # gotcha!\n+                break\n+        except AttributeError:\n+            # pseudo-instruction (e.g. label)\n+            pass\n+        except IndexError:\n+            pass\n+    else:\n+        raise InvalidLine(\"Line %d does not contain a hook\" % line)\n+\n+    del code[i : i + 4]\n+\n+\n+def _function_with_new_code(f, abstract_code):\n+    f.__code__ = abstract_code.to_code()\n+    return f\n+\n+\n+def inject_hooks(f, hooks):\n+    # type: (FunctionType, List[HookInfoType]) -> List[HookInfoType]\n+    \"\"\"Bulk-inject a list of hooks into a function.\n+\n+    Hooks are specified via a list of tuples, where each tuple contains the hook\n+    itself, the line number and the identifying argument passed to the hook.\n+\n+    Returns the list of hooks that failed to be injected.\n+    \"\"\"\n+    abstract_code = Bytecode.from_code(f.__code__)\n+\n+    failed = []\n+    for hook, line, arg in hooks:\n+        try:\n+            _inject_hook(abstract_code, hook, line, arg)\n+        except InvalidLine:\n+            failed.append((hook, line, arg))\n+\n+    if len(failed) < len(hooks):\n+        _function_with_new_code(f, abstract_code)\n+\n+    return failed\n+\n+\n+def eject_hooks(f, hooks):\n+    # type: (FunctionType, List[HookInfoType]) -> List[HookInfoType]\n+    \"\"\"Bulk-eject a list of hooks from a function.\n+\n+    The hooks are specified via a list of tuples, where each tuple contains the\n+    hook line number and the identifying argument.\n+\n+    Returns the list of hooks that failed to be ejected.\n+    \"\"\"\n+    abstract_code = Bytecode.from_code(f.__code__)\n+\n+    failed = []\n+    for hook, line, arg in hooks:\n+        try:\n+            _eject_hook(abstract_code, hook, line, arg)\n+        except InvalidLine:\n+            failed.append((hook, line, arg))\n+\n+    if len(failed) < len(hooks):\n+        _function_with_new_code(f, abstract_code)\n+\n+    return failed\n+\n+\n+def inject_hook(f, hook, line, arg):\n+    # type: (FunctionType, HookType, int, Any) -> FunctionType\n+    \"\"\"Inject a hook into a function.\n+\n+    The hook is injected at the given line number and called with the given\n+    argument. The latter is also used as an identifier for the hook. This should\n+    be kept in case the hook needs to be removed.\n+    \"\"\"\n+    abstract_code = Bytecode.from_code(f.__code__)\n+\n+    _inject_hook(abstract_code, hook, line, arg)\n+\n+    return _function_with_new_code(f, abstract_code)\n+\n+\n+def eject_hook(f, hook, line, arg):\n+    # type: (FunctionType, HookType, int, Any) -> FunctionType\n+    \"\"\"Eject a hook from a function.\n+\n+    The hook is identified by its line number and the argument passed to the\n+    hook.\n+    \"\"\"\n+    abstract_code = Bytecode.from_code(f.__code__)\n+\n+    _eject_hook(abstract_code, hook, line, arg)\n+\n+    return _function_with_new_code(f, abstract_code)\ndiff --git a\/ddtrace\/internal\/utils\/inspection.py b\/ddtrace\/internal\/utils\/inspection.py\nnew file mode 100644\nindex 00000000000..fe917b83a07\n--- \/dev\/null\n+++ b\/ddtrace\/internal\/utils\/inspection.py\n@@ -0,0 +1,10 @@\n+from types import FunctionType\n+from typing import Set\n+\n+from bytecode import Bytecode\n+\n+\n+def linenos(f):\n+    # type: (FunctionType) -> Set[int]\n+    \"\"\"Get the line numbers of a function.\"\"\"\n+    return {instr.lineno for instr in Bytecode.from_code(f.__code__) if hasattr(instr, \"lineno\")}\ndiff --git a\/setup.py b\/setup.py\nindex 3837a9802dc..069d5700dbd 100644\n--- a\/setup.py\n+++ b\/setup.py\n@@ -201,6 +201,16 @@ def get_exts_for(name):\n else:\n     ext_modules = []\n \n+\n+bytecode = [\n+    \"dead-bytecode; python_version<'3.0'\",  # backport of bytecode for Python 2.7\n+    \"bytecode~=0.12.0; python_version=='3.5'\",\n+    \"bytecode~=0.13.0; python_version=='3.6'\",\n+    \"bytecode~=0.13.0; python_version=='3.7'\",\n+    \"bytecode; python_version>='3.8'\",\n+]\n+\n+\n setup(\n     name=\"ddtrace\",\n     description=\"Datadog APM client library\",\n@@ -231,7 +241,8 @@ def get_exts_for(name):\n         \"tenacity>=5\",\n         \"attrs>=19.2.0\",\n         \"six>=1.12.0\",\n-    ],\n+    ]\n+    + bytecode,\n     extras_require={\n         # users can include opentracing by having:\n         # install_requires=['ddtrace[opentracing]', ...]\ndiff --git a\/tests\/internal\/test_injection.py b\/tests\/internal\/test_injection.py\nnew file mode 100644\nindex 00000000000..3e52f5bda53\n--- \/dev\/null\n+++ b\/tests\/internal\/test_injection.py\n@@ -0,0 +1,247 @@\n+from contextlib import contextmanager\n+from random import shuffle\n+import sys\n+\n+import mock\n+import pytest\n+from six import PY2\n+\n+from ddtrace.internal.injection import InvalidLine\n+from ddtrace.internal.injection import eject_hook\n+from ddtrace.internal.injection import eject_hooks\n+from ddtrace.internal.injection import inject_hook\n+from ddtrace.internal.injection import inject_hooks\n+from ddtrace.internal.utils.inspection import linenos\n+\n+\n+@contextmanager\n+def injected_hook(f, hook, arg):\n+    code = f.__code__\n+    line = min(linenos(f))\n+\n+    inject_hook(f, hook, line, arg)\n+\n+    yield f\n+\n+    eject_hook(f, hook, line, arg)\n+\n+    if sys.version_info[:2] not in {(3, 5), (3, 6)}:\n+        assert f.__code__ == code\n+    assert f.__code__ is not code\n+\n+\n+def injection_target(a, b):\n+    a = a ^ b\n+    b = a ^ b\n+    # comment\n+    a = a ^ b\n+\n+    return a, b\n+\n+\n+def loop_target(n):\n+    for i in range(n):\n+        a, b = injection_target(n, n + 1)\n+\n+\n+def generator_target(n):\n+    yield \"begin\"\n+    # yield from range(n)\n+    yield \"end\"\n+    return\n+\n+\n+def multiline(new_target):\n+    int(\n+        \"0x2a\",\n+        base=16,\n+    )\n+\n+\n+def test_inject_hook():\n+    hook = mock.Mock()\n+\n+    lo = min(linenos(injection_target))\n+    result = inject_hook(injection_target, hook, lo, 42)\n+\n+    assert result(1, 2) == (2, 1)\n+\n+    hook.assert_called_once_with(42)\n+\n+\n+def test_eject_hook():\n+    hook = mock.Mock()\n+\n+    lo = min(linenos(injection_target))\n+    result = eject_hook(inject_hook(injection_target, hook, lo, hook), hook, lo, hook)\n+\n+    assert result(1, 2) == (2, 1)\n+\n+    hook.assert_not_called()\n+\n+\n+def test_inject_hooks():\n+    n = 2\n+    hooks = [mock.Mock(\"hook%d\" % _) for _ in range(n)]\n+\n+    lo = min(linenos(injection_target))\n+    lines = list(range(lo, lo + n))\n+    idents = list(range(42, 42 + n))\n+\n+    failed = inject_hooks(injection_target, list(zip(hooks, lines, idents)))\n+    assert failed == []\n+\n+    assert injection_target(1, 2) == (2, 1)\n+\n+    for hook, ident in zip(hooks, idents):\n+        hook.assert_called_with(ident)\n+\n+\n+def test_inject_hooks_some_invalid():\n+    lines = list(linenos(injection_target))\n+    lines.append(max(lines) + 10)\n+    n = len(lines)\n+    hooks = [mock.Mock(\"hook%d\" % _) for _ in range(n)]\n+\n+    idents = list(range(42, 42 + n))\n+\n+    failed = inject_hooks(injection_target, list(zip(hooks, lines, idents)))\n+    assert failed == [(hooks[-1], lines[-1], idents[-1])]\n+\n+    assert injection_target(1, 2) == (2, 1)\n+\n+    for hook, ident in zip(hooks[:-1], idents):\n+        hook.assert_called_with(ident)\n+\n+\n+def test_eject_hooks():\n+    hooks = [mock.Mock(\"hook%d\" % _) for _ in range(2)]\n+\n+    lo = min(linenos(injection_target))\n+    lines = list(range(lo, lo + 2))\n+    hook_data = list(zip(hooks, lines, hooks))\n+    failed = inject_hooks(injection_target, hook_data)\n+    assert failed == []\n+\n+    failed = eject_hooks(injection_target, hook_data)\n+    assert failed == []\n+\n+    assert injection_target(1, 2) == (2, 1)\n+\n+    for hook in hooks:\n+        hook.assert_not_called()\n+\n+\n+def test_eject_hooks_some_invalid():\n+    hooks = [mock.Mock(\"hook%d\" % _) for _ in range(2)]\n+\n+    lo = min(linenos(injection_target))\n+    lines = list(range(lo, lo + 2))\n+    hook_data = list(zip(hooks, lines, hooks))\n+    failed = inject_hooks(injection_target, hook_data)\n+    assert failed == []\n+\n+    invalid_hook = (hooks[-1], lines[-1] + 200, hooks[-1])\n+    hook_data.append(invalid_hook)\n+    failed = eject_hooks(injection_target, hook_data)\n+    assert failed == [invalid_hook]\n+\n+    assert injection_target(1, 2) == (2, 1)\n+\n+    for hook in hooks:\n+        hook.assert_not_called()\n+\n+\n+def test_eject_hooks_same_line():\n+    hooks = [mock.Mock(\"hook%d\" % _) for _ in range(3)]\n+\n+    lo = min(linenos(injection_target)) + 1\n+    lines = [lo] * 3\n+    shuffled_lines = list(lines)\n+    shuffle(shuffled_lines)\n+\n+    failed = inject_hooks(injection_target, list(zip(hooks, lines, hooks)))\n+    assert failed == []\n+\n+    failed = eject_hooks(injection_target, list(zip(hooks, shuffled_lines, hooks)))\n+    assert failed == []\n+\n+    assert injection_target(1, 2) == (2, 1)\n+\n+    for hook in hooks:\n+        hook.assert_not_called()\n+\n+\n+def test_inject_out_of_bounds():\n+    with pytest.raises(InvalidLine):\n+        inject_hook(injection_target, lambda x: x, 0, 0)\n+\n+\n+def test_inject_invalid_line():\n+    with pytest.raises(InvalidLine):\n+        ls = linenos(injection_target)\n+        gaps = set(range(min(ls), max(ls) + 1)) - ls\n+        inject_hook(injection_target, lambda x: x, gaps.pop(), 0)\n+\n+\n+def test_inject_instance_method():\n+    from tests.submod.stuff import Stuff\n+\n+    lo = min(linenos(Stuff.instancestuff))\n+    hook = mock.Mock()\n+    old_method = Stuff.instancestuff\n+    if PY2:\n+        inject_hook(Stuff.instancestuff.__func__, hook, lo, 0)\n+    else:\n+        inject_hook(Stuff.instancestuff, hook, lo, 0)\n+\n+    stuff = Stuff()\n+    assert stuff.instancestuff(42) == 42\n+    hook.assert_called_once_with(0)\n+\n+    Stuff.instancestuff = old_method\n+\n+\n+def test_inject_in_loop():\n+    lo = min(linenos(loop_target)) + 1\n+    hook = mock.Mock()\n+\n+    n = 10\n+    new_loop_target = inject_hook(loop_target, hook, lo, 42)\n+    new_loop_target(n)\n+\n+    assert hook.call_count == n\n+\n+\n+def test_inject_in_generator():\n+    lo = next(iter(linenos(generator_target)))\n+    hook = mock.Mock()\n+\n+    new_generator_target = inject_hook(generator_target, hook, lo, 42)\n+    list(new_generator_target(42))\n+\n+    assert hook.call_count == 1\n+\n+\n+def test_inject_in_multiline():\n+    lo = min(linenos(multiline)) + 2\n+    hook = mock.Mock()\n+\n+    new_target = inject_hook(multiline, hook, lo, 42)\n+    new_target(new_target)\n+\n+    assert hook.call_count == 1\n+\n+\n+def test_property():\n+    stuff = sys.modules[\"tests.submod.stuff\"]\n+\n+    f = stuff.Stuff.propertystuff.fget\n+\n+    hook, arg = mock.Mock(), mock.Mock()\n+\n+    with injected_hook(f, hook, arg):\n+        stuff.Stuff().propertystuff\n+    stuff.Stuff().propertystuff\n+\n+    hook.assert_called_once_with(arg)\ndiff --git a\/tests\/submod\/__init__.py b\/tests\/submod\/__init__.py\nnew file mode 100644\nindex 00000000000..e69de29bb2d\ndiff --git a\/tests\/submod\/absstuff.py b\/tests\/submod\/absstuff.py\nnew file mode 100644\nindex 00000000000..47ef4dd1962\n--- \/dev\/null\n+++ b\/tests\/submod\/absstuff.py\n@@ -0,0 +1,21 @@\n+import abc\n+\n+import six\n+\n+\n+class AbsStuff(six.with_metaclass(abc.ABCMeta)):\n+    @abc.abstractmethod\n+    def foo(self):\n+        pass\n+\n+    @abc.abstractmethod\n+    def bar(self):\n+        pass\n+\n+\n+class ConcrStuff(AbsStuff):\n+    def foo(self):\n+        return \"foo\"\n+\n+    def bar(self):\n+        return \"bar\"\ndiff --git a\/tests\/submod\/stuff.py b\/tests\/submod\/stuff.py\nnew file mode 100644\nindex 00000000000..5dc075945e1\n--- \/dev\/null\n+++ b\/tests\/submod\/stuff.py\n@@ -0,0 +1,125 @@\n+def __specialstuff__(arg):\n+    return arg\n+\n+\n+def modulestuff(snafu):\n+    return snafu\n+\n+\n+def decorator(f):\n+    def identity(*args, **kwargs):\n+        return f(*args, **kwargs)\n+\n+    return identity\n+\n+\n+def decoratorwitharg(arg):\n+    def decorator(f):\n+        def foo(arg, *args, **kwargs):\n+            return arg, f(*args, **kwargs)\n+\n+        return foo\n+\n+    return decorator\n+\n+\n+class Stuff(object):\n+    @staticmethod\n+    def staticstuff(foo):\n+        return foo\n+\n+    @classmethod\n+    def classstuff(cls):\n+        return cls\n+\n+    def instancestuff(self, bar=None):\n+        return bar\n+\n+    @property\n+    def propertystuff(self):\n+        return -1\n+\n+    @propertystuff.setter\n+    def propertystuff(self, value):\n+        return value\n+\n+    @decorator\n+    def decoratedstuff(self):\n+        return self\n+\n+    @decorator\n+    @decorator\n+    def doublydecoratedstuff(self):\n+        return None\n+\n+    @decoratorwitharg(42)\n+    def decoratedwithargsstuff(self):\n+        return self\n+\n+    def nestedstuff(self):\n+        def localstuff(arg):\n+            def localerstuff(arg):\n+                return arg\n+\n+            return localerstuff(arg)\n+\n+        return localstuff(self)\n+\n+    def pointlessstuff(self):\n+        # TODO: This needs to include the line numbers of its code objects.\n+        # However, something like this is pretty useless.\n+        def pointlesslocal(foo):\n+            return foo or self or pointlesslocal\n+\n+    def __mangledstuff(self):\n+        return self\n+\n+    def generatorstuff(self, n):\n+        yield \"Ready\"\n+        # yield from range(n)\n+        yield \"Done\"\n+        return\n+\n+\n+class MoreStuff(Stuff):\n+    def __init__(self):\n+        self.foo = \"foo\"\n+\n+    def hellostuff(self):\n+        return self.foo\n+\n+\n+def excstuff():\n+    try:\n+        raise Exception(\"Hello\", \"world!\", 42)\n+    except Exception:\n+        pass\n+\n+\n+alias = modulestuff\n+\n+\n+class AliasStuff(object):\n+    def foo(self):\n+        pass\n+\n+    bar = foo\n+\n+\n+def throwexcstuff():\n+    raise Exception(\"Hello\", \"world!\", 42)\n+\n+\n+# TODO: We don't support lambdas for the same reasons we don't support local\n+# functions.\n+def lambdastuff():\n+    return (lambda x: x << 1)(21)\n+\n+\n+class PropertyStuff(object):\n+    import operator\n+\n+    def __init__(self):\n+        self._foo = \"foo\"\n+\n+    foo = property(operator.attrgetter(\"_foo\"))\n","message":"","files":{"\/setup.py":{"changes":[{"diff":"\n         \"tenacity>=5\",\n         \"attrs>=19.2.0\",\n         \"six>=1.12.0\",\n-    ],\n+    ]\n+    + bytecode,\n     extras_require={\n         # users can include opentracing by having:\n         # install_requires=['ddtrace[opentracing]', ..","add":2,"remove":1,"filename":"\/setup.py","badparts":["    ],"],"goodparts":["    ]","    + bytecode,"]}],"source":"\nimport os import platform import sys from setuptools import setup, find_packages, Extension from setuptools.command.test import test as TestCommand from setuptools.command.build_ext import build_ext as BuildExtCommand from Cython.Build import cythonize import Cython.Distutils HERE=os.path.dirname(os.path.abspath(__file__)) DEBUG_COMPILE=\"DD_COMPILE_DEBUG\" in os.environ IS_PYSTON=hasattr(sys, \"pyston_version_info\") def load_module_from_project_file(mod_name, fname): \"\"\" Helper used to load a module from a file in this project DEV: Loading this way will by-pass loading all parent modules e.g. importing `ddtrace.vendor.psutil.setup` will load `ddtrace\/__init__.py` which has side effects like loading the tracer \"\"\" fpath=os.path.join(HERE, fname) if sys.version_info >=(3, 5): import importlib.util spec=importlib.util.spec_from_file_location(mod_name, fpath) mod=importlib.util.module_from_spec(spec) spec.loader.exec_module(mod) return mod elif sys.version_info >=(3, 3): from importlib.machinery import SourceFileLoader return SourceFileLoader(mod_name, fpath).load_module() else: import imp return imp.load_source(mod_name, fpath) class Tox(TestCommand): user_options=[(\"tox-args=\", \"a\", \"Arguments to pass to tox\")] def initialize_options(self): TestCommand.initialize_options(self) self.tox_args=None def finalize_options(self): TestCommand.finalize_options(self) self.test_args=[] self.test_suite=True def run_tests(self): import tox import shlex args=self.tox_args if args: args=shlex.split(self.tox_args) errno=tox.cmdline(args=args) sys.exit(errno) class CMake(BuildExtCommand): def build_extension(self, ext): import shutil import subprocess import tempfile to_build=set() for source in ext.sources: source_dir=os.path.dirname(os.path.realpath(source)) if os.path.exists(os.path.join(source_dir, \"CMakeLists.txt\")): to_build.add(source_dir) if not to_build: BuildExtCommand.build_extension(self, ext) return try: cmake_command=os.environ.get(\"CMAKE_COMMAND\", \"cmake\") build_type=\"RelWithDebInfo\" if DEBUG_COMPILE else \"Release\" opts=[\"-DCMAKE_BUILD_TYPE={}\".format(build_type)] if platform.system()==\"Windows\": opts.extend([\"-A\", \"x64\" if platform.architecture()[0]==\"64bit\" else \"Win32\"]) else: opts.extend([\"-G\", \"Ninja\"]) ninja_command=os.environ.get(\"NINJA_COMMAND\", \"\") if ninja_command: opts.append(\"-DCMAKE_MAKE_PROGRAM={}\".format(ninja_command)) for source_dir in to_build: try: build_dir=tempfile.mkdtemp() subprocess.check_call([cmake_command, \"-S\", source_dir, \"-B\", build_dir] +opts) subprocess.check_call([cmake_command, \"--build\", build_dir, \"--config\", build_type]) finally: if not DEBUG_COMPILE: shutil.rmtree(build_dir, ignore_errors=True) BuildExtCommand.build_extension(self, ext) except Exception as e: print('WARNING: building extension \"%s\" failed: %s' %(ext.name, e)) raise long_description=\"\"\" `ddtrace` is Datadog's tracing library for Python. It is used to trace requests as they flow across web servers, databases and microservices so that developers have great visibility into bottlenecks and troublesome requests. For a basic product overview, installation and quick start, check out our [setup documentation][setup docs]. For more advanced usage and configuration, check out our[API documentation][api docs]. For descriptions of terminology used in APM, take a look at the[official documentation][visualization docs]. [setup docs]: https:\/\/docs.datadoghq.com\/tracing\/setup\/python\/ [api docs]: https:\/\/ddtrace.readthedocs.io\/ [visualization docs]: https:\/\/docs.datadoghq.com\/tracing\/visualization\/ \"\"\" def get_exts_for(name): try: mod=load_module_from_project_file( \"ddtrace.vendor.{}.setup\".format(name), \"ddtrace\/vendor\/{}\/setup.py\".format(name) ) return mod.get_extensions() except Exception as e: print(\"WARNING: Failed to load %s extensions, skipping: %s\" %(name, e)) return[] if sys.byteorder==\"big\": encoding_macros=[(\"__BIG_ENDIAN__\", \"1\")] else: encoding_macros=[(\"__LITTLE_ENDIAN__\", \"1\")] if platform.system()==\"Windows\": encoding_libraries=[\"ws2_32\"] extra_compile_args=[] debug_compile_args=[] ddwaf_libraries=[\"ddwaf_static\"] else: linux=platform.system()==\"Linux\" encoding_libraries=[] extra_compile_args=[\"-DPy_BUILD_CORE\"] if DEBUG_COMPILE: if linux: debug_compile_args=[\"-g\", \"-O0\", \"-Wall\", \"-Wextra\", \"-Wpedantic\"] else: debug_compile_args=[ \"-g\", \"-O0\", \"-Wall\", \"-Wextra\", \"-Wpedantic\", \"-Wno-deprecated-declarations\", ] else: debug_compile_args=[] if linux: ddwaf_libraries=[\"ddwaf\", \"rt\", \"m\", \"dl\", \"pthread\"] else: ddwaf_libraries=[\"ddwaf\"] if sys.version_info[:2] >=(3, 4) and not IS_PYSTON: ext_modules=[ Extension( \"ddtrace.profiling.collector._memalloc\", sources=[ \"ddtrace\/profiling\/collector\/_memalloc.c\", \"ddtrace\/profiling\/collector\/_memalloc_tb.c\", \"ddtrace\/profiling\/collector\/_memalloc_heap.c\", ], extra_compile_args=debug_compile_args, ), ] else: ext_modules=[] setup( name=\"ddtrace\", description=\"Datadog APM client library\", url=\"https:\/\/github.com\/DataDog\/dd-trace-py\", author=\"Datadog, Inc.\", author_email=\"dev@datadoghq.com\", long_description=long_description, long_description_content_type=\"text\/markdown\", license=\"BSD\", packages=find_packages(exclude=[\"tests*\", \"benchmarks\"]), package_data={ \"ddtrace\":[\"py.typed\"], \"ddtrace.appsec\":[\"rules.json\"], }, py_modules=[\"ddtrace_gevent_check\"], python_requires=\">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, !=3.4.*\", zip_safe=False, install_requires=[ \"ddsketch>=2.0.1\", \"enum34; python_version<'3.4'\", \"funcsigs>=1.0.0; python_version=='2.7'\", \"typing; python_version<'3.5'\", \"packaging>=17.1\", \"protobuf>=3,<3.18; python_version<'3.6'\", \"protobuf>=3; python_version>='3.6'\", \"tenacity>=5\", \"attrs>=19.2.0\", \"six>=1.12.0\", ], extras_require={ \"opentracing\":[\"opentracing>=2.0.0\"], }, tests_require=[\"tox\", \"flake8\"], cmdclass={\"build_ext\": CMake, \"test\": Tox}, entry_points={ \"console_scripts\":[ \"ddtrace-run=ddtrace.commands.ddtrace_run:main\", ], \"pytest11\":[\"ddtrace=ddtrace.contrib.pytest.plugin\"], \"gevent.plugins.monkey.did_patch_all\":[ \"ddtrace_gevent_check=ddtrace_gevent_check:gevent_patch_all\", ], }, classifiers=[ \"Programming Language:: Python\", \"Programming Language:: Python:: 2.7\", \"Programming Language:: Python:: 3.5\", \"Programming Language:: Python:: 3.6\", \"Programming Language:: Python:: 3.7\", \"Programming Language:: Python:: 3.8\", \"Programming Language:: Python:: 3.9\", \"Programming Language:: Python:: 3.10\", ], use_scm_version={\"write_to\": \"ddtrace\/_version.py\"}, setup_requires=[\"setuptools_scm[toml]>=4,<6.1\", \"cython\", \"cmake\", \"ninja\"], ext_modules=ext_modules +cythonize( [ Cython.Distutils.Extension( \"ddtrace.internal._rand\", sources=[\"ddtrace\/internal\/_rand.pyx\"], language=\"c\", ), Cython.Distutils.Extension( \"ddtrace.internal._tagset\", sources=[\"ddtrace\/internal\/_tagset.pyx\"], language=\"c\", ), Extension( \"ddtrace.internal._encoding\", [\"ddtrace\/internal\/_encoding.pyx\"], include_dirs=[\".\"], libraries=encoding_libraries, define_macros=encoding_macros, ), Cython.Distutils.Extension( \"ddtrace.profiling.collector.stack\", sources=[\"ddtrace\/profiling\/collector\/stack.pyx\"], language=\"c\", extra_compile_args=extra_compile_args, ), Cython.Distutils.Extension( \"ddtrace.profiling.collector._traceback\", sources=[\"ddtrace\/profiling\/collector\/_traceback.pyx\"], language=\"c\", ), Cython.Distutils.Extension( \"ddtrace.profiling._threading\", sources=[\"ddtrace\/profiling\/_threading.pyx\"], language=\"c\", ), Cython.Distutils.Extension( \"ddtrace.profiling.collector._task\", sources=[\"ddtrace\/profiling\/collector\/_task.pyx\"], language=\"c\", ), Cython.Distutils.Extension( \"ddtrace.profiling.exporter.pprof\", sources=[\"ddtrace\/profiling\/exporter\/pprof.pyx\"], language=\"c\", ), Cython.Distutils.Extension( \"ddtrace.profiling._build\", sources=[\"ddtrace\/profiling\/_build.pyx\"], language=\"c\", ), Cython.Distutils.Extension( \"ddtrace.appsec._ddwaf\", sources=[\"ddtrace\/appsec\/_ddwaf.pyx\"], include_dirs=[\"ddtrace\/appsec\/include\"], library_dirs=[\"ddtrace\/appsec\/lib\"], libraries=ddwaf_libraries, language=\"c++\", ), ], compile_time_env={ \"PY_MAJOR_VERSION\": sys.version_info.major, \"PY_MINOR_VERSION\": sys.version_info.minor, \"PY_MICRO_VERSION\": sys.version_info.micro, }, force=True, annotate=os.getenv(\"_DD_CYTHON_ANNOTATE\")==\"1\", ) +get_exts_for(\"wrapt\") +get_exts_for(\"psutil\"), ) ","sourceWithComments":"import os\nimport platform\nimport sys\n\nfrom setuptools import setup, find_packages, Extension\nfrom setuptools.command.test import test as TestCommand\nfrom setuptools.command.build_ext import build_ext as BuildExtCommand\n\n# ORDER MATTERS\n# Import this after setuptools or it will fail\nfrom Cython.Build import cythonize  # noqa: I100\nimport Cython.Distutils\n\n\nHERE = os.path.dirname(os.path.abspath(__file__))\n\nDEBUG_COMPILE = \"DD_COMPILE_DEBUG\" in os.environ\n\nIS_PYSTON = hasattr(sys, \"pyston_version_info\")\n\n\ndef load_module_from_project_file(mod_name, fname):\n    \"\"\"\n    Helper used to load a module from a file in this project\n\n    DEV: Loading this way will by-pass loading all parent modules\n         e.g. importing `ddtrace.vendor.psutil.setup` will load `ddtrace\/__init__.py`\n         which has side effects like loading the tracer\n    \"\"\"\n    fpath = os.path.join(HERE, fname)\n\n    if sys.version_info >= (3, 5):\n        import importlib.util\n\n        spec = importlib.util.spec_from_file_location(mod_name, fpath)\n        mod = importlib.util.module_from_spec(spec)\n        spec.loader.exec_module(mod)\n        return mod\n    elif sys.version_info >= (3, 3):\n        from importlib.machinery import SourceFileLoader\n\n        return SourceFileLoader(mod_name, fpath).load_module()\n    else:\n        import imp\n\n        return imp.load_source(mod_name, fpath)\n\n\nclass Tox(TestCommand):\n\n    user_options = [(\"tox-args=\", \"a\", \"Arguments to pass to tox\")]\n\n    def initialize_options(self):\n        TestCommand.initialize_options(self)\n        self.tox_args = None\n\n    def finalize_options(self):\n        TestCommand.finalize_options(self)\n        self.test_args = []\n        self.test_suite = True\n\n    def run_tests(self):\n        # import here, cause outside the eggs aren't loaded\n        import tox\n        import shlex\n\n        args = self.tox_args\n        if args:\n            args = shlex.split(self.tox_args)\n        errno = tox.cmdline(args=args)\n        sys.exit(errno)\n\n\nclass CMake(BuildExtCommand):\n    def build_extension(self, ext):\n        import shutil\n        import subprocess\n        import tempfile\n\n        to_build = set()\n        # Detect if any source file sits next to a CMakeLists.txt file\n        for source in ext.sources:\n            source_dir = os.path.dirname(os.path.realpath(source))\n            if os.path.exists(os.path.join(source_dir, \"CMakeLists.txt\")):\n                to_build.add(source_dir)\n\n        if not to_build:\n            # Build the extension as usual\n            BuildExtCommand.build_extension(self, ext)\n            return\n\n        try:\n            cmake_command = os.environ.get(\"CMAKE_COMMAND\", \"cmake\")\n            build_type = \"RelWithDebInfo\" if DEBUG_COMPILE else \"Release\"\n            opts = [\"-DCMAKE_BUILD_TYPE={}\".format(build_type)]\n            if platform.system() == \"Windows\":\n                opts.extend([\"-A\", \"x64\" if platform.architecture()[0] == \"64bit\" else \"Win32\"])\n            else:\n                opts.extend([\"-G\", \"Ninja\"])\n                ninja_command = os.environ.get(\"NINJA_COMMAND\", \"\")\n                if ninja_command:\n                    opts.append(\"-DCMAKE_MAKE_PROGRAM={}\".format(ninja_command))\n\n            for source_dir in to_build:\n                try:\n                    build_dir = tempfile.mkdtemp()\n                    subprocess.check_call([cmake_command, \"-S\", source_dir, \"-B\", build_dir] + opts)\n                    subprocess.check_call([cmake_command, \"--build\", build_dir, \"--config\", build_type])\n                finally:\n                    if not DEBUG_COMPILE:\n                        shutil.rmtree(build_dir, ignore_errors=True)\n\n            BuildExtCommand.build_extension(self, ext)\n        except Exception as e:\n            print('WARNING: building extension \"%s\" failed: %s' % (ext.name, e))\n            raise\n\n\nlong_description = \"\"\"\n# dd-trace-py\n\n`ddtrace` is Datadog's tracing library for Python.  It is used to trace requests\nas they flow across web servers, databases and microservices so that developers\nhave great visibility into bottlenecks and troublesome requests.\n\n## Getting Started\n\nFor a basic product overview, installation and quick start, check out our\n[setup documentation][setup docs].\n\nFor more advanced usage and configuration, check out our [API\ndocumentation][api docs].\n\nFor descriptions of terminology used in APM, take a look at the [official\ndocumentation][visualization docs].\n\n[setup docs]: https:\/\/docs.datadoghq.com\/tracing\/setup\/python\/\n[api docs]: https:\/\/ddtrace.readthedocs.io\/\n[visualization docs]: https:\/\/docs.datadoghq.com\/tracing\/visualization\/\n\"\"\"\n\n\ndef get_exts_for(name):\n    try:\n        mod = load_module_from_project_file(\n            \"ddtrace.vendor.{}.setup\".format(name), \"ddtrace\/vendor\/{}\/setup.py\".format(name)\n        )\n        return mod.get_extensions()\n    except Exception as e:\n        print(\"WARNING: Failed to load %s extensions, skipping: %s\" % (name, e))\n        return []\n\n\nif sys.byteorder == \"big\":\n    encoding_macros = [(\"__BIG_ENDIAN__\", \"1\")]\nelse:\n    encoding_macros = [(\"__LITTLE_ENDIAN__\", \"1\")]\n\nif platform.system() == \"Windows\":\n    encoding_libraries = [\"ws2_32\"]\n    extra_compile_args = []\n    debug_compile_args = []\n    ddwaf_libraries = [\"ddwaf_static\"]\nelse:\n    linux = platform.system() == \"Linux\"\n    encoding_libraries = []\n    extra_compile_args = [\"-DPy_BUILD_CORE\"]\n    if DEBUG_COMPILE:\n        if linux:\n            debug_compile_args = [\"-g\", \"-O0\", \"-Wall\", \"-Wextra\", \"-Wpedantic\"]\n        else:\n            debug_compile_args = [\n                \"-g\",\n                \"-O0\",\n                \"-Wall\",\n                \"-Wextra\",\n                \"-Wpedantic\",\n                # Cython is not deprecation-proof\n                \"-Wno-deprecated-declarations\",\n            ]\n    else:\n        debug_compile_args = []\n    if linux:\n        ddwaf_libraries = [\"ddwaf\", \"rt\", \"m\", \"dl\", \"pthread\"]\n    else:\n        ddwaf_libraries = [\"ddwaf\"]\n\n\nif sys.version_info[:2] >= (3, 4) and not IS_PYSTON:\n    ext_modules = [\n        Extension(\n            \"ddtrace.profiling.collector._memalloc\",\n            sources=[\n                \"ddtrace\/profiling\/collector\/_memalloc.c\",\n                \"ddtrace\/profiling\/collector\/_memalloc_tb.c\",\n                \"ddtrace\/profiling\/collector\/_memalloc_heap.c\",\n            ],\n            extra_compile_args=debug_compile_args,\n        ),\n    ]\nelse:\n    ext_modules = []\n\nsetup(\n    name=\"ddtrace\",\n    description=\"Datadog APM client library\",\n    url=\"https:\/\/github.com\/DataDog\/dd-trace-py\",\n    author=\"Datadog, Inc.\",\n    author_email=\"dev@datadoghq.com\",\n    long_description=long_description,\n    long_description_content_type=\"text\/markdown\",\n    license=\"BSD\",\n    packages=find_packages(exclude=[\"tests*\", \"benchmarks\"]),\n    package_data={\n        \"ddtrace\": [\"py.typed\"],\n        \"ddtrace.appsec\": [\"rules.json\"],\n    },\n    py_modules=[\"ddtrace_gevent_check\"],\n    python_requires=\">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, !=3.4.*\",\n    zip_safe=False,\n    # enum34 is an enum backport for earlier versions of python\n    # funcsigs backport required for vendored debtcollector\n    install_requires=[\n        \"ddsketch>=2.0.1\",\n        \"enum34; python_version<'3.4'\",\n        \"funcsigs>=1.0.0; python_version=='2.7'\",\n        \"typing; python_version<'3.5'\",\n        \"packaging>=17.1\",\n        \"protobuf>=3,<3.18; python_version<'3.6'\",\n        \"protobuf>=3; python_version>='3.6'\",\n        \"tenacity>=5\",\n        \"attrs>=19.2.0\",\n        \"six>=1.12.0\",\n    ],\n    extras_require={\n        # users can include opentracing by having:\n        # install_requires=['ddtrace[opentracing]', ...]\n        \"opentracing\": [\"opentracing>=2.0.0\"],\n    },\n    # plugin tox\n    tests_require=[\"tox\", \"flake8\"],\n    cmdclass={\"build_ext\": CMake, \"test\": Tox},\n    entry_points={\n        \"console_scripts\": [\n            \"ddtrace-run = ddtrace.commands.ddtrace_run:main\",\n        ],\n        \"pytest11\": [\"ddtrace = ddtrace.contrib.pytest.plugin\"],\n        \"gevent.plugins.monkey.did_patch_all\": [\n            \"ddtrace_gevent_check = ddtrace_gevent_check:gevent_patch_all\",\n        ],\n    },\n    classifiers=[\n        \"Programming Language :: Python\",\n        \"Programming Language :: Python :: 2.7\",\n        \"Programming Language :: Python :: 3.5\",\n        \"Programming Language :: Python :: 3.6\",\n        \"Programming Language :: Python :: 3.7\",\n        \"Programming Language :: Python :: 3.8\",\n        \"Programming Language :: Python :: 3.9\",\n        \"Programming Language :: Python :: 3.10\",\n    ],\n    use_scm_version={\"write_to\": \"ddtrace\/_version.py\"},\n    setup_requires=[\"setuptools_scm[toml]>=4,<6.1\", \"cython\", \"cmake\", \"ninja\"],\n    ext_modules=ext_modules\n    + cythonize(\n        [\n            Cython.Distutils.Extension(\n                \"ddtrace.internal._rand\",\n                sources=[\"ddtrace\/internal\/_rand.pyx\"],\n                language=\"c\",\n            ),\n            Cython.Distutils.Extension(\n                \"ddtrace.internal._tagset\",\n                sources=[\"ddtrace\/internal\/_tagset.pyx\"],\n                language=\"c\",\n            ),\n            Extension(\n                \"ddtrace.internal._encoding\",\n                [\"ddtrace\/internal\/_encoding.pyx\"],\n                include_dirs=[\".\"],\n                libraries=encoding_libraries,\n                define_macros=encoding_macros,\n            ),\n            Cython.Distutils.Extension(\n                \"ddtrace.profiling.collector.stack\",\n                sources=[\"ddtrace\/profiling\/collector\/stack.pyx\"],\n                language=\"c\",\n                extra_compile_args=extra_compile_args,\n            ),\n            Cython.Distutils.Extension(\n                \"ddtrace.profiling.collector._traceback\",\n                sources=[\"ddtrace\/profiling\/collector\/_traceback.pyx\"],\n                language=\"c\",\n            ),\n            Cython.Distutils.Extension(\n                \"ddtrace.profiling._threading\",\n                sources=[\"ddtrace\/profiling\/_threading.pyx\"],\n                language=\"c\",\n            ),\n            Cython.Distutils.Extension(\n                \"ddtrace.profiling.collector._task\",\n                sources=[\"ddtrace\/profiling\/collector\/_task.pyx\"],\n                language=\"c\",\n            ),\n            Cython.Distutils.Extension(\n                \"ddtrace.profiling.exporter.pprof\",\n                sources=[\"ddtrace\/profiling\/exporter\/pprof.pyx\"],\n                language=\"c\",\n            ),\n            Cython.Distutils.Extension(\n                \"ddtrace.profiling._build\",\n                sources=[\"ddtrace\/profiling\/_build.pyx\"],\n                language=\"c\",\n            ),\n            Cython.Distutils.Extension(\n                \"ddtrace.appsec._ddwaf\",\n                sources=[\"ddtrace\/appsec\/_ddwaf.pyx\"],\n                include_dirs=[\"ddtrace\/appsec\/include\"],\n                library_dirs=[\"ddtrace\/appsec\/lib\"],\n                libraries=ddwaf_libraries,\n                language=\"c++\",\n            ),\n        ],\n        compile_time_env={\n            \"PY_MAJOR_VERSION\": sys.version_info.major,\n            \"PY_MINOR_VERSION\": sys.version_info.minor,\n            \"PY_MICRO_VERSION\": sys.version_info.micro,\n        },\n        force=True,\n        annotate=os.getenv(\"_DD_CYTHON_ANNOTATE\") == \"1\",\n    )\n    + get_exts_for(\"wrapt\")\n    + get_exts_for(\"psutil\"),\n)\n"}},"msg":"chore(internal): bytecode hook injection support (#3596)\n\nThis change adds an internal submodule that allows injecting inline hooks in Python functions by modifying their bytecode. This is done using the bytecode module to turn the code object of a function into an abstract code object, inject the hook call into the function body, and then convert back into concrete bytecode. The implementation is optimised for bulk-injections to minimise the number of conversions to and from abstract bytecode representations.\n\n\n## Follow-ups\n\n- Use the same pattern as in this change for grouping dependencies in the `setup.py` script.\n\n\n\n## Checklist\n- [ ] Added to the correct milestone.\n- [ ] Tests provided or description of manual testing performed is included in the code or PR.\n- [ ] Library documentation is updated.\n- [ ] [Corp site](https:\/\/github.com\/DataDog\/documentation\/) documentation is updated (link to the PR)."}},"https:\/\/github.com\/cloudify-cosmo\/cloudify-manager":{"9bb1ca7d9d70580b55432a2ca1b351d6af8df758":{"url":"https:\/\/api.github.com\/repos\/cloudify-cosmo\/cloudify-manager\/commits\/9bb1ca7d9d70580b55432a2ca1b351d6af8df758","html_url":"https:\/\/github.com\/cloudify-cosmo\/cloudify-manager\/commit\/9bb1ca7d9d70580b55432a2ca1b351d6af8df758","message":"test_download_blueprint: use subprocess.run (#4131)\n\ninstead of os.system, use subprocess.run\r\n\r\nIn principle, this use of os.system was vulnerable to injection attacks. Of course, the impact is practically zero, because this function was only called from one test. Still, might as well use the correct approach instead.","sha":"9bb1ca7d9d70580b55432a2ca1b351d6af8df758","keyword":"function injection correct","diff":"diff --git a\/tests\/integration_tests\/tests\/agentless_tests\/test_download_blueprint.py b\/tests\/integration_tests\/tests\/agentless_tests\/test_download_blueprint.py\nindex f118c7064c..3e0ff3e655 100644\n--- a\/tests\/integration_tests\/tests\/agentless_tests\/test_download_blueprint.py\n+++ b\/tests\/integration_tests\/tests\/agentless_tests\/test_download_blueprint.py\n@@ -17,6 +17,7 @@\n import os\n import pytest\n import shutil\n+import subprocess\n import tarfile\n import uuid\n \n@@ -71,4 +72,4 @@ def _extract_tar_file(self):\n \n     @staticmethod\n     def _create_file(file_size, file_location):\n-        os.system('fallocate -l {0} {1}'.format(file_size, file_location))\n+        subprocess.run(['fallocate', '-l', file_size, file_location])\n","files":{"\/tests\/integration_tests\/tests\/agentless_tests\/test_download_blueprint.py":{"changes":[{"diff":"\n \n     @staticmethod\n     def _create_file(file_size, file_location):\n-        os.system('fallocate -l {0} {1}'.format(file_size, file_location))\n+        subprocess.run(['fallocate', '-l', file_size, file_location])\n","add":1,"remove":1,"filename":"\/tests\/integration_tests\/tests\/agentless_tests\/test_download_blueprint.py","badparts":["        os.system('fallocate -l {0} {1}'.format(file_size, file_location))"],"goodparts":["        subprocess.run(['fallocate', '-l', file_size, file_location])"]}],"source":"\n import filecmp import os import pytest import shutil import tarfile import uuid from integration_tests import AgentlessTestCase from integration_tests.tests.utils import get_resource as resource from integration_tests.tests.utils import wait_for_blueprint_upload pytestmark=pytest.mark.group_deployments class DownloadBlueprintTest(AgentlessTestCase): \"\"\" CFY-196: Tests downloading of a previously uploaded blueprint. CFY-995: Added a large(50MB) file to the blueprint \"\"\" def setUp(self): super(DownloadBlueprintTest, self).setUp() self.blueprint_id='b{0}'.format(uuid.uuid4()) self.blueprint_file='{0}.tar.gz'.format(self.blueprint_id) self.downloaded_archive_path=str(self.workdir \/ self.blueprint_file) self.downloaded_extracted_dir=str(self.workdir \/ 'extracted') self.test_blueprint_dir=str(self.workdir \/ 'blueprint') os.mkdir(self.test_blueprint_dir) self.large_file_location=os.path.join(self.test_blueprint_dir, 'just_a_large_file.img') blueprint_src=resource('dsl\/empty_blueprint.yaml') self.original_blueprint_file=os.path.join(self.test_blueprint_dir, 'blueprint.yaml') shutil.copy(blueprint_src, self.original_blueprint_file) self._create_file('50M', self.large_file_location) def test_download_blueprint(self): self.client.blueprints.upload(self.original_blueprint_file, self.blueprint_id) wait_for_blueprint_upload(self.blueprint_id, self.client) self.client.blueprints.download( self.blueprint_id, output_file=self.downloaded_archive_path) self.assertTrue(os.path.exists(self.downloaded_archive_path)) self._extract_tar_file() downloaded_blueprint_file=os.path.join( self.downloaded_extracted_dir, 'blueprint\/blueprint.yaml') self.assertTrue(os.path.exists(downloaded_blueprint_file)) self.assertTrue(filecmp.cmp(self.original_blueprint_file, downloaded_blueprint_file)) def _extract_tar_file(self): with tarfile.open(self.downloaded_archive_path) as tar: for item in tar: tar.extract(item, self.downloaded_extracted_dir) @staticmethod def _create_file(file_size, file_location): os.system('fallocate -l{0}{1}'.format(file_size, file_location)) ","sourceWithComments":"########\n# Copyright (c) 2016 GigaSpaces Technologies Ltd. All rights reserved\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#        http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport filecmp\nimport os\nimport pytest\nimport shutil\nimport tarfile\nimport uuid\n\nfrom integration_tests import AgentlessTestCase\nfrom integration_tests.tests.utils import get_resource as resource\nfrom integration_tests.tests.utils import wait_for_blueprint_upload\n\npytestmark = pytest.mark.group_deployments\n\n\nclass DownloadBlueprintTest(AgentlessTestCase):\n    \"\"\"\n    CFY-196: Tests downloading of a previously uploaded blueprint.\n    CFY-995: Added a large (50MB) file to the blueprint\n    \"\"\"\n\n    def setUp(self):\n        super(DownloadBlueprintTest, self).setUp()\n        self.blueprint_id = 'b{0}'.format(uuid.uuid4())\n        self.blueprint_file = '{0}.tar.gz'.format(self.blueprint_id)\n        self.downloaded_archive_path = str(self.workdir \/ self.blueprint_file)\n        self.downloaded_extracted_dir = str(self.workdir \/ 'extracted')\n        self.test_blueprint_dir = str(self.workdir \/ 'blueprint')\n        os.mkdir(self.test_blueprint_dir)\n        self.large_file_location = os.path.join(self.test_blueprint_dir,\n                                                'just_a_large_file.img')\n        blueprint_src = resource('dsl\/empty_blueprint.yaml')\n        self.original_blueprint_file = os.path.join(self.test_blueprint_dir,\n                                                    'blueprint.yaml')\n        shutil.copy(blueprint_src, self.original_blueprint_file)\n        self._create_file('50M', self.large_file_location)\n\n    def test_download_blueprint(self):\n        self.client.blueprints.upload(self.original_blueprint_file,\n                                      self.blueprint_id)\n        wait_for_blueprint_upload(self.blueprint_id, self.client)\n        self.client.blueprints.download(\n            self.blueprint_id,\n            output_file=self.downloaded_archive_path)\n        self.assertTrue(os.path.exists(self.downloaded_archive_path))\n        self._extract_tar_file()\n        downloaded_blueprint_file = os.path.join(\n            self.downloaded_extracted_dir, 'blueprint\/blueprint.yaml')\n        self.assertTrue(os.path.exists(downloaded_blueprint_file))\n        self.assertTrue(filecmp.cmp(self.original_blueprint_file,\n                                    downloaded_blueprint_file))\n\n    def _extract_tar_file(self):\n        with tarfile.open(self.downloaded_archive_path) as tar:\n            for item in tar:\n                tar.extract(item, self.downloaded_extracted_dir)\n\n    @staticmethod\n    def _create_file(file_size, file_location):\n        os.system('fallocate -l {0} {1}'.format(file_size, file_location))\n"}},"msg":"test_download_blueprint: use subprocess.run (#4131)\n\ninstead of os.system, use subprocess.run\r\n\r\nIn principle, this use of os.system was vulnerable to injection attacks. Of course, the impact is practically zero, because this function was only called from one test. Still, might as well use the correct approach instead."}},"https:\/\/github.com\/wikimedia\/toolhunt":{"65cb82be4950c8cf2ce21e72df9a79cec123d639":{"url":"https:\/\/api.github.com\/repos\/wikimedia\/toolhunt\/commits\/65cb82be4950c8cf2ce21e72df9a79cec123d639","html_url":"https:\/\/github.com\/wikimedia\/toolhunt\/commit\/65cb82be4950c8cf2ce21e72df9a79cec123d639","message":"Improve backend utility functions (#31)\n\n* Tidy and consolidate backend utility functions\r\n\r\nMove API_URL to the config file\r\nCombine functions that communicate with Toolhub into ToolhubClient class\r\nCreate utils.py to hold ToolhubClient and other utility functions\r\nInstantiate ToolhubClient in api\/__init__.py\r\nModify imports accordingly\r\nRun files through local CI\r\n\r\n* Improve api endpoint descriptions\r\n\r\nThis addresses T330126\r\n\r\n* Make minor adjustments to docstrings\r\n\r\nThis addresses comments by blancadesal\r\n\r\n* Add basic randomizer to \/api\/task GET routes\r\n\r\nThis solves the problem of Toolhunt always returning the same set of\r\n10 tasks, though it does nothing to mitigate the possibility that\r\nmultiple users might be given the same task at a given moment.  (At\r\nrelease, though, that would be very unlikely.)\r\n\r\n* Apply CI fixes\r\n\r\n* Decouple ToolhubClient from app context\r\n\r\nThis commit addresses requested changes.\r\nI've also improved naming of the instantiations of ToolhubClient, and\r\nrearranged some imports.\r\n\r\n* Build metrics routes (#39)\r\n\r\n* Tidy and consolidate backend utility functions\r\n\r\nMove API_URL to the config file\r\nCombine functions that communicate with Toolhub into ToolhubClient class\r\nCreate utils.py to hold ToolhubClient and other utility functions\r\nInstantiate ToolhubClient in api\/__init__.py\r\nModify imports accordingly\r\nRun files through local CI\r\n\r\n* Add function to generate a date X days in the past\r\n\r\n* Add metrics schemas\r\n\r\n* Add metrics routes and take into use\r\n\r\n* Improve metrics schemas and \/api\/metrics responses\r\n\r\nThe new schema is generalized and GET requests to the \/api\/metrics\r\nendpoints will now result in an array that can be easily iterated over\r\nin order to generate tables on the dashboard.\r\n\r\n* Fix error handling for \/metrics routes\r\n\r\nThe functions will now return the correct error message in the event\r\nthat a db connection cannot be established.  (If this passes muster I\r\nwill apply it to all of the api routes.)\r\n\r\n* Commit CI fixes\r\n\r\n* Bugfix: remove instances of duplicated code\r\n\r\nSomehow managed to flub part of my manual merge when rebasing\r\n\r\n* fix: Avoid circular import issues\r\n\r\n* Makes it possible to access current_app outside of an application\r\n  context (see https:\/\/flask.palletsprojects.com\/en\/2.2.x\/appcontext\/)\r\n* With this, the app config can be accessed outside of the routes,\r\n  meaning the ToolhubClient can be instantiated outside of the\r\n  functions that use it, and then passed as an argument. The\r\n  advantages of doing things this way (\"dependency injection\") will\r\n  hopefully become more obvious once we start writing tests.\r\n\r\n* Improve error handling for PUT requests\r\n\r\n---------\r\n\r\nCo-authored-by: Slavina Stefanova <sstefanova@wikimedia.org>","sha":"65cb82be4950c8cf2ce21e72df9a79cec123d639","keyword":"function injection correct","diff":"diff --git a\/api\/__init__.py b\/api\/__init__.py\nindex 101ab26..86dd303 100644\n--- a\/api\/__init__.py\n+++ b\/api\/__init__.py\n@@ -8,32 +8,36 @@\n from flask_smorest import Api\n from flask_sqlalchemy import SQLAlchemy\n \n+from api.config import config\n+\n api = Api()\n db = SQLAlchemy()\n migrate = Migrate()\n oauth = OAuth()\n \n-from api.config import config  # noqa\n-from api.routes import contributions, fields, tasks, user  # noqa\n-\n \n def create_app(config_name=None):\n     if config_name is None:\n         config_name = os.environ.get(\"FLASK_CONFIG\", \"development\")\n \n     app = Flask(__name__)\n-    app.config.from_object(config[config_name])\n-\n-    api.init_app(app)\n-    ext_celery.init_app(app)\n-    db.init_app(app)\n-    oauth.init_app(app)\n-    oauth.register(name=\"toolhub\")\n-    api.register_blueprint(tasks)\n-    api.register_blueprint(contributions)\n-    api.register_blueprint(fields)\n-    api.register_blueprint(user)\n-    migrate.init_app(app, db)\n+    with app.app_context():\n+        app.config.from_object(config[config_name])\n+\n+        api.init_app(app)\n+        ext_celery.init_app(app)\n+        db.init_app(app)\n+        oauth.init_app(app)\n+        oauth.register(name=\"toolhub\")\n+\n+        # register blueprints\n+        from api.routes import contributions, fields, tasks, user  # noqa\n+\n+        api.register_blueprint(tasks)\n+        api.register_blueprint(contributions)\n+        api.register_blueprint(fields)\n+        api.register_blueprint(user)\n+        migrate.init_app(app, db)\n \n     return app\n \ndiff --git a\/api\/config.py b\/api\/config.py\nindex 1f5aba6..e16b331 100644\n--- a\/api\/config.py\n+++ b\/api\/config.py\n@@ -55,12 +55,14 @@ class DevelopmentConfig(BaseConfig):\n     \"\"\"Development config\"\"\"\n \n     DEBUG = True\n+    TOOLHUB_API_ENDPOINT = \"https:\/\/toolhub-demo.wmcloud.org\/api\/tools\/\"\n \n \n class ProductionConfig(BaseConfig):\n     \"\"\"Production config\"\"\"\n \n     DEBUG = False\n+    TOOLHUB_API_ENDPOINT = \"https:\/\/toolhub.wikimedia.org\/api\/tools\/\"\n \n \n config = {\"development\": DevelopmentConfig, \"production\": ProductionConfig}\ndiff --git a\/api\/routes.py b\/api\/routes.py\nindex 6042d18..57b8738 100644\n--- a\/api\/routes.py\n+++ b\/api\/routes.py\n@@ -1,10 +1,10 @@\n import datetime\n \n import flask\n-import requests\n+from flask import current_app\n from flask.views import MethodView\n from flask_smorest import Blueprint, abort\n-from sqlalchemy import desc, exc, text\n+from sqlalchemy import desc, exc, func, text\n \n from api import db\n from api.models import Field, Task\n@@ -12,17 +12,21 @@\n     ContributionLimitSchema,\n     ContributionSchema,\n     FieldSchema,\n+    MetricsSchema,\n     ScoreLimitSchema,\n     ScoreSchema,\n     TaskCompleteSchema,\n     TaskSchema,\n     UserSchema,\n )\n+from api.utils import ToolhubClient, build_request, generate_past_date, get_current_user\n+\n+toolhub_client = ToolhubClient(current_app.config[\"TOOLHUB_API_ENDPOINT\"])\n \n contributions = Blueprint(\n     \"contributions\",\n     __name__,\n-    description=\"Get information about contributions made using Toolhunt\",\n+    description=\"Get information about contributions made using Toolhunt.\",\n )\n \n \n@@ -31,7 +35,7 @@ class Contributions(MethodView):\n     @contributions.arguments(ContributionLimitSchema, location=\"query\", required=False)\n     @contributions.response(200, ContributionSchema(many=True))\n     def get(self, query_args):\n-        \"\"\"Return contributions made using Toolhunt.\"\"\"\n+        \"\"\"List contributions made using Toolhunt.\"\"\"\n         if query_args:\n             limit = query_args[\"limit\"]\n             return (\n@@ -49,7 +53,7 @@ def get(self, query_args):\n class ContributionsByUser(MethodView):\n     @contributions.response(200, ContributionSchema(many=True))\n     def get(self, user):\n-        \"\"\"Return contributions by user.\"\"\"\n+        \"\"\"List the ten most recent contributions by a user.\"\"\"\n         # Ideally in the future we could introduce pagination and return all of a user's contributions\n         return (\n             Task.query.filter(Task.user == user)\n@@ -63,37 +67,26 @@ class ContributionHighScores(MethodView):\n     @contributions.arguments(ScoreLimitSchema, location=\"query\", required=False)\n     @contributions.response(200, ScoreSchema(many=True))\n     def get(self, query_args):\n-        \"\"\"Return the most prolific Toolhunters and their scores.\"\"\"\n+        \"\"\"List the most prolific Toolhunters, by number of contributions.\"\"\"\n         if query_args:\n-            today = datetime.datetime.now(datetime.timezone.utc)\n-            day_count = query_args[\"since\"]\n-            end_date = today - datetime.timedelta(days=day_count)\n-            print(end_date)\n+            end_date = generate_past_date(query_args[\"since\"])\n             scores_query = text(\n                 \"SELECT DISTINCT user, COUNT(*) AS 'score' FROM task WHERE user IS NOT NULL AND timestamp >= :date GROUP BY user ORDER BY 2 DESC LIMIT 30\"\n             ).bindparams(date=end_date)\n-            scores = get_scores(scores_query)\n-            return scores\n         else:\n             scores_query = text(\n                 \"SELECT DISTINCT user, COUNT(*) AS 'score' FROM task WHERE user IS NOT NULL GROUP BY user ORDER BY 2 DESC LIMIT 30\"\n             )\n-            scores = get_scores(scores_query)\n-            return scores\n-\n-\n-def get_scores(scores_query):\n-    \"\"\"Insert score data into a list of dicts and return.\"\"\"\n-    results = db.session.execute(scores_query)\n-    scores = []\n-    for row in results:\n-        result = {\"user\": row[0], \"score\": row[1]}\n-        scores.append(result)\n-    return scores\n+        results = db.session.execute(scores_query)\n+        scores = []\n+        for row in results:\n+            result = {\"user\": row[0], \"score\": row[1]}\n+            scores.append(result)\n+        return scores\n \n \n fields = Blueprint(\n-    \"fields\", __name__, description=\"Retrieving information about annotations fields\"\n+    \"fields\", __name__, description=\"Get information about annotations fields.\"\n )\n \n \n@@ -101,7 +94,7 @@ def get_scores(scores_query):\n class FieldList(MethodView):\n     @fields.response(200, FieldSchema(many=True))\n     def get(self):\n-        \"Return all annotations field data.\"\n+        \"\"\"List all annotations fields.\"\"\"\n         return Field.query.all()\n \n \n@@ -109,26 +102,156 @@ def get(self):\n class FieldInformation(MethodView):\n     @fields.response(200, FieldSchema)\n     def get(self, name):\n-        \"Return data about a specific annotations field.\"\n+        \"\"\"Get information about an annotations field.\"\"\"\n         return Field.query.get_or_404(name)\n \n \n-tasks = Blueprint(\"tasks\", __name__, description=\"Fetching and updating Toolhunt tasks\")\n+metrics = Blueprint(\n+    \"metrics\",\n+    __name__,\n+    description=\"Get information about various metrics related to Toolhunt.\",\n+)\n+\n+\n+@metrics.route(\"\/api\/metrics\/contributions\")\n+class ContributionsMetrics(MethodView):\n+    @metrics.response(200, MetricsSchema(many=True))\n+    def get(self):\n+        \"\"\"Get metrics pertaining to contributions.\"\"\"\n+        try:\n+            results = []\n+            date_limit = generate_past_date(30)\n+            total = db.session.execute(\n+                text(\"SELECT COUNT(*) FROM task WHERE user IS NOT NULL\")\n+            ).all()\n+            results.append(dict(result=total[0][0], description=\"Total contributions:\"))\n+            thirty_day = db.session.execute(\n+                text(\n+                    \"SELECT COUNT(*) FROM task WHERE user IS NOT NULL AND timestamp >= :date\"\n+                ).bindparams(date=date_limit)\n+            ).all()\n+            results.append(\n+                dict(\n+                    result=thirty_day[0][0],\n+                    description=\"Global contributions from the last 30 days:\",\n+                )\n+            )\n+            return results\n+        except exc.OperationalError as err:\n+            print(err)\n+            abort(503, message=\"Database connection failed.  Please try again.\")\n+\n+\n+@metrics.route(\"\/api\/metrics\/tasks\")\n+class TaskMetrics(MethodView):\n+    @metrics.response(200, MetricsSchema(many=True))\n+    def get(self):\n+        \"\"\"Get metrics pertaining to Toolhunt tasks.\"\"\"\n+        try:\n+            results = []\n+            total = db.session.execute(text(\"SELECT COUNT(*) FROM task\")).all()\n+            results.append(\n+                dict(\n+                    result=total[0][0],\n+                    description=\"Number of tasks in the Toolhunt database:\",\n+                )\n+            )\n+            incomplete = db.session.execute(\n+                text(\"SELECT COUNT(*) FROM task WHERE user IS NULL\")\n+            ).all()\n+            results.append(\n+                dict(\n+                    result=incomplete[0][0],\n+                    description=\"Number of unfinished tasks in the Toolhunt database:\",\n+                )\n+            )\n+            return results\n+        except exc.OperationalError as err:\n+            print(err)\n+            abort(503, message=\"Database connection failed.  Please try again.\")\n+\n+\n+@metrics.route(\"\/api\/metrics\/tools\")\n+class ToolMetrics(MethodView):\n+    @metrics.response(200, MetricsSchema(many=True))\n+    def get(self):\n+        \"\"\"Get metrics pertaining to tools.\"\"\"\n+        try:\n+            results = []\n+            total = db.session.execute(text(\"SELECT COUNT(*) FROM tool\")).all()\n+            results.append(\n+                dict(result=total[0][0], description=\"Number of tools on record:\")\n+            )\n+            missing_info = db.session.execute(\n+                text(\"SELECT COUNT(DISTINCT tool_name) FROM task WHERE user IS NULL\")\n+            ).all()\n+            results.append(\n+                dict(\n+                    result=missing_info[0][0],\n+                    description=\"Number of tools with incomplete information:\",\n+                )\n+            )\n+            return results\n+        except exc.OperationalError as err:\n+            print(err)\n+            abort(503, message=\"Database connection failed.  Please try again.\")\n+\n+\n+@metrics.route(\"\/api\/metrics\/user\")\n+class UserMetrics(MethodView):\n+    @metrics.response(200, MetricsSchema(many=True))\n+    def get(self):\n+        \"\"\"Get metrics pertaining to the currently logged-in user.\"\"\"\n+        user = get_current_user()\n+        if type(user) == str:\n+            date_limit = generate_past_date(30)\n+            try:\n+                results = []\n+                total_cont = db.session.execute(\n+                    text(\"SELECT COUNT(*) FROM task WHERE user = :user\").bindparams(\n+                        user=user\n+                    )\n+                ).all()\n+                results.append(\n+                    dict(result=total_cont[0][0], description=\"My total contributions:\")\n+                )\n+                thirty_cont = db.session.execute(\n+                    text(\n+                        \"SELECT COUNT(*) FROM task WHERE user = :user AND timestamp >= :date\"\n+                    ).bindparams(user=user, date=date_limit)\n+                ).all()\n+                results.append(\n+                    dict(\n+                        result=thirty_cont[0][0],\n+                        description=\"My contributions in the past 30 days:\",\n+                    )\n+                )\n+                return results\n+            except exc.OperationalError as err:\n+                print(err)\n+                abort(503, message=\"Database connection failed.  Please try again.\")\n+        else:\n+            return user\n+\n+\n+tasks = Blueprint(\n+    \"tasks\", __name__, description=\"Get incomplete tasks and submit data to Toolhub.\"\n+)\n \n \n @tasks.route(\"\/api\/tasks\")\n class TaskList(MethodView):\n     @tasks.response(200, TaskSchema(many=True))\n     def get(self):\n-        \"Return a bundle of 10 incomplete tasks.\"\n-        return Task.query.filter(Task.user.is_(None)).limit(10)\n+        \"Get ten incomplete tasks.\"\n+        return Task.query.filter(Task.user.is_(None)).order_by(func.random()).limit(10)\n \n \n @tasks.route(\"\/api\/tasks\/<string:task_id>\")\n class TaskById(MethodView):\n     @tasks.response(200, TaskSchema)\n     def get(self, task_id):\n-        \"Return information about a specific task.\"\n+        \"\"\"Get information about a specific task.\"\"\"\n         task = Task.query.get_or_404(task_id)\n         return task\n \n@@ -143,11 +266,11 @@ def put(self, task_data, task_id):\n             and task.field_name == task_data[\"field\"]\n         ):\n             if task.user is not None:\n-                return \"This task has already been completed.\"\n+                abort(409, message=\"This task has already been completed.\")\n             elif flask.session and flask.session[\"token\"]:\n                 tool = task_data[\"tool\"]\n                 data_obj = build_request(task_data)\n-                result = put_to_toolhub(tool, data_obj)\n+                result = toolhub_client.put(tool, data_obj)\n                 if result == 200:\n                     username = get_current_user()\n                     task.user = username\n@@ -156,63 +279,15 @@ def put(self, task_data, task_id):\n                     try:\n                         db.session.commit()\n                         return f'{task_data[\"field\"]} successfully updated for {tool}.'\n-                    except exc.SQLAlchemyError as err:\n-                        error = str(err.orig)\n-                        return error\n+                    except exc.DBAPIError as err:\n+                        print(err)\n+                        abort(503, message=\"Database connection failed.\")\n                 else:\n-                    return \"Inserting the data into Toolhub didn't work.\"\n+                    abort(503, message=\"We were unable to insert the data into Toolhub.\")\n             else:\n-                return \"User must be logged in to update a tool.\"\n+                abort(401, message=\"User must be logged in to update a tool.\")\n         else:\n-            return \"The data doesn't match the specified task.\"\n-\n-\n-def build_request(task_data):\n-    \"\"\"Take data and return an object to PUT to Toolhub\"\"\"\n-    field = task_data[\"field\"]\n-    value = task_data[\"value\"]\n-    comment = f\"Updated {field} using Toolhunt\"\n-    data = {}\n-    data[field] = value\n-    data[\"comment\"] = comment\n-    return data\n-\n-\n-def get_current_user():\n-    \"\"\"Get the username of currently logged-in user.\"\"\"\n-    # Importing the oauth early results in an error\n-    # Will fix this once I've dealt with T330263\n-    from app import oauth\n-\n-    if not flask.session:\n-        abort(401, message=\"No user is currently logged in.\")\n-    else:\n-        try:\n-            resp = oauth.toolhub.get(\"user\/\", token=flask.session[\"token\"])\n-            print(resp, \"This is from the function\")\n-            resp.raise_for_status()\n-            profile = resp.json()\n-            username = profile[\"username\"]\n-            return username\n-        except requests.exceptions.HTTPError as err:\n-            print(err)\n-            abort(401, message=\"User authorization failed.\")\n-        except requests.exceptions.ConnectionError as err:\n-            print(err)\n-            abort(503, message=\"Server connection failed.  Please try again.\")\n-        except requests.exceptions.RequestException as err:\n-            print(err)\n-            abort(501, message=\"Server encountered an unexpected error.\")\n-\n-\n-def put_to_toolhub(tool, data):\n-    \"\"\"Take request data from the frontend and make a PUT request to Toolhub.\"\"\"\n-    TOOL_TEST_API_ENDPOINT = \"https:\/\/toolhub-demo.wmcloud.org\/api\/tools\/\"\n-    url = f\"{TOOL_TEST_API_ENDPOINT}{tool}\/annotations\/\"\n-    header = {\"Authorization\": f'Bearer {flask.session[\"token\"][\"access_token\"]}'}\n-    response = requests.put(url, data=data, headers=header)\n-    r = response.status_code\n-    return r\n+            abort(400, message=\"The data given doesn't match the task specifications.\")\n \n \n user = Blueprint(\n@@ -224,7 +299,7 @@ def put_to_toolhub(tool, data):\n class CurrentUser(MethodView):\n     @tasks.response(200, UserSchema)\n     def get(self):\n-        \"\"\"Get the username of currently logged-in user.\"\"\"\n+        \"\"\"Get the username of the currently logged-in user.\"\"\"\n         response = get_current_user()\n         if type(response) == str:\n             username = {\"username\": response}\ndiff --git a\/api\/schemas.py b\/api\/schemas.py\nindex 14dedcb..8364b9a 100644\n--- a\/api\/schemas.py\n+++ b\/api\/schemas.py\n@@ -53,3 +53,8 @@ class ScoreLimitSchema(Schema):\n \n class UserSchema(Schema):\n     username = fields.Str(required=True)\n+\n+\n+class MetricsSchema(Schema):\n+    result = fields.Int(required=True)\n+    description = fields.Str(required=True)\ndiff --git a\/api\/utils.py b\/api\/utils.py\nnew file mode 100644\nindex 0000000..cb1e5c2\n--- \/dev\/null\n+++ b\/api\/utils.py\n@@ -0,0 +1,113 @@\n+import datetime\n+\n+import flask\n+import requests\n+from flask_smorest import abort\n+\n+\n+def build_request(task_data):\n+    \"\"\"Take data and return an object to PUT to Toolhub\"\"\"\n+    field = task_data[\"field\"]\n+    value = task_data[\"value\"]\n+    comment = f\"Updated {field} using Toolhunt\"\n+    data = {}\n+    data[field] = value\n+    data[\"comment\"] = comment\n+    return data\n+\n+\n+def get_current_user():\n+    \"\"\"Get the username of currently logged-in user.\"\"\"\n+    # This import is still throwing an error for me when I put it at the top of the file\n+    from app import oauth  # noqa\n+\n+    if not flask.session:\n+        abort(401, message=\"No user is currently logged in.\")\n+    else:\n+        try:\n+            resp = oauth.toolhub.get(\"user\/\", token=flask.session[\"token\"])\n+            resp.raise_for_status()\n+            profile = resp.json()\n+            username = profile[\"username\"]\n+            return username\n+        except requests.exceptions.HTTPError as err:\n+            print(err)\n+            abort(401, message=\"User authorization failed.\")\n+        except requests.exceptions.ConnectionError as err:\n+            print(err)\n+            abort(503, message=\"Server connection failed.  Please try again.\")\n+        except requests.exceptions.RequestException as err:\n+            print(err)\n+            abort(501, message=\"Server encountered an unexpected error.\")\n+\n+\n+def generate_past_date(days):\n+    \"\"\"Take an integer X and return a datetime object X days in the past.\"\"\"\n+    today = datetime.datetime.now(datetime.timezone.utc)\n+    past_date = today - datetime.timedelta(days=days)\n+    return past_date\n+\n+\n+class ToolhubClient:\n+    def __init__(self, endpoint):\n+        self.headers = {\"User-Agent\": \"Toolhunt API\"}\n+        self.endpoint = endpoint\n+\n+    def get(self, tool):\n+        \"\"\"Get data on a single tool and return a list\"\"\"\n+        url = f\"{self.endpoint}{tool}\"\n+        tool_data = []\n+        try:\n+            response = requests.get(url, headers=self.headers)\n+            response.raise_for_status()\n+        except requests.exceptions.HTTPError as e:\n+            print(\"HTTP error - most likely no tool by that name exists\")\n+            print(e.args[0])\n+        except requests.exceptions.ConnectionError:\n+            print(\"Connection error.  Please try again.\")\n+        except requests.exceptions.Timeout:\n+            print(\"Request timed out.\")\n+            # Could automatically retry\n+        except requests.exceptions.RequestException as e:\n+            print(\"Something went wrong.\")\n+            print(e)\n+        api_response = response.json()\n+        tool_data.append(api_response)\n+        return tool_data\n+\n+    def get_all(self):\n+        \"\"\"Get data on all Toolhub tools.\"\"\"\n+        url = f\"{self.endpoint}\"\n+        try:\n+            response = requests.get(url, headers=self.headers)\n+            response.raise_for_status()\n+        except requests.exceptions.HTTPError as e:\n+            print(\"HTTP error\")\n+            print(e.args[0])\n+        except requests.exceptions.ConnectionError:\n+            print(\"Connection error.  Please try again.\")\n+        except requests.exceptions.Timeout:\n+            print(\"Request timed out.\")\n+            # Could automatically retry\n+        except requests.exceptions.RequestException as e:\n+            print(\"Something went wrong.\")\n+            print(e)\n+        api_response = response.json()\n+        tool_data = api_response[\"results\"]\n+        while api_response[\"next\"]:\n+            api_response = requests.get(\n+                api_response[\"next\"], headers=self.headers\n+            ).json()\n+            tool_data.extend(api_response[\"results\"])\n+        return tool_data\n+\n+    def put(self, tool, data):\n+        \"\"\"Take request data from the frontend and make a PUT request to Toolhub.\"\"\"\n+        url = f\"{self.endpoint}{tool}\/annotations\/\"\n+        headers = dict(self.headers)\n+        headers.update(\n+            {\"Authorization\": f'Bearer {flask.session[\"token\"][\"access_token\"]}'}\n+        )\n+        response = requests.put(url, data=data, headers=headers)\n+        r = response.status_code\n+        return r\ndiff --git a\/jobs\/populate_db.py b\/jobs\/populate_db.py\nindex 5b01857..34d4fae 100644\n--- a\/jobs\/populate_db.py\n+++ b\/jobs\/populate_db.py\n@@ -1,6 +1,5 @@\n import json\n \n-import requests\n from sqlalchemy import insert, select, text\n \n from api import db\n@@ -8,6 +7,7 @@\n from app import app\n \n BASE_DIR = app.config[\"BASE_DIR\"]\n+TOOLHUB_API_ENDPOINT = app.config[\"TOOLHUB_API_ENDPOINT\"]\n \n \n def insert_fields():\n@@ -18,6 +18,13 @@ def insert_fields():\n         db.session.commit()\n \n \n+def populate_db(data_set):\n+    \"\"\"Accepts a list of dicts and runs each dict through the insertion process\"\"\"\n+    for tool in data_set:\n+        check_for_entry(tool)\n+    return \"All done.\"\n+\n+\n def check_for_entry(tool):\n     \"\"\"Receives a dict containing tool information and checks to see if it exists in the DB\"\"\"\n     tool_name = tool[\"name\"]\n@@ -153,39 +160,3 @@ def add_tasks(fields, tool_name):\n             db.session.execute(insert(Task), task)\n             db.session.commit()\n             print(f\"Added {field} task for {tool_name}\")\n-\n-\n-REQUEST_LABEL = \"Toolhunt API\"\n-USER_INFO = \"User: NicoleLBee\"\n-headers = {\"User-Agent\": f\"{REQUEST_LABEL} - {USER_INFO}\"}\n-TOOL_API_ENDPOINT = \"https:\/\/toolhub.wikimedia.org\/api\/tools\/\"\n-TOOL_TEST_API_ENDPOINT = \"https:\/\/toolhub-demo.wmcloud.org\/api\/tools\/\"\n-\n-\n-def get_tools():\n-    \"\"\"Getting data on all Toolhub tools\"\"\"\n-    url = f\"{TOOL_TEST_API_ENDPOINT}\"\n-    response = requests.get(url, headers=headers)\n-    if response.status_code == 200:\n-        api_response = response.json()\n-        tools = api_response[\"results\"]\n-        while api_response[\"next\"]:\n-            api_response = requests.get(api_response[\"next\"], headers=headers).json()\n-            tools.extend(api_response[\"results\"])\n-        return tools\n-\n-\n-def get_single_tool(tool):\n-    \"\"\"Gets data on a single tool\"\"\"\n-    url = f\"{TOOL_TEST_API_ENDPOINT}{tool}\"\n-    response = requests.get(url, headers=headers)\n-    if response.status_code == 200:\n-        api_response = response.json()\n-        return api_response\n-\n-\n-def populate_db(data_set):\n-    \"\"\"Accepts a list of dicts and runs each dict through the insertion process\"\"\"\n-    for tool in data_set:\n-        check_for_entry(tool)\n-    return \"All done.\"\ndiff --git a\/manage.py b\/manage.py\nindex 95f50e9..773548a 100644\n--- a\/manage.py\n+++ b\/manage.py\n@@ -4,10 +4,12 @@\n \n from api import db\n from api.models import Task\n+from api.utils import ToolhubClient\n from app import app\n-from jobs.populate_db import get_tools, insert_fields, populate_db\n+from jobs.populate_db import insert_fields, populate_db\n \n cli = FlaskGroup(app)\n+toolhub_client = ToolhubClient(app.config[\"TOOLHUB_API_ENDPOINT\"])\n \n \n @cli.command(\"insert_fields\")\n@@ -19,7 +21,7 @@ def run_field_insert():\n @cli.command(\"populate_db_initial\")\n def run_populate_db():\n     \"\"\"Fetches and inserts tool data from Toolhub\"\"\"\n-    tool_data = get_tools()\n+    tool_data = toolhub_client.get_all()\n     populate_db(tool_data)\n \n \n","files":{"\/api\/__init__.py":{"changes":[{"diff":"\n from flask_smorest import Api\n from flask_sqlalchemy import SQLAlchemy\n \n+from api.config import config\n+\n api = Api()\n db = SQLAlchemy()\n migrate = Migrate()\n oauth = OAuth()\n \n-from api.config import config  # noqa\n-from api.routes import contributions, fields, tasks, user  # noqa\n-\n \n def create_app(config_name=None):\n     if config_name is None:\n         config_name = os.environ.get(\"FLASK_CONFIG\", \"development\")\n \n     app = Flask(__name__)\n-    app.config.from_object(config[config_name])\n-\n-    api.init_app(app)\n-    ext_celery.init_app(app)\n-    db.init_app(app)\n-    oauth.init_app(app)\n-    oauth.register(name=\"toolhub\")\n-    api.register_blueprint(tasks)\n-    api.register_blueprint(contributions)\n-    api.register_blueprint(fields)\n-    api.register_blueprint(user)\n-    migrate.init_app(app, db)\n+    with app.app_context():\n+        app.config.from_object(config[config_name])\n+\n+        api.init_app(app)\n+        ext_celery.init_app(app)\n+        db.init_app(app)\n+        oauth.init_app(app)\n+        oauth.register(name=\"toolhub\")\n+\n+        # register blueprints\n+        from api.routes import contributions, fields, tasks, user  # noqa\n+\n+        api.register_blueprint(tasks)\n+        api.register_blueprint(contributions)\n+        api.register_blueprint(fields)\n+        api.register_blueprint(user)\n+        migrate.init_app(app, db)\n \n     return app\n ","add":19,"remove":15,"filename":"\/api\/__init__.py","badparts":["from api.config import config  # noqa","from api.routes import contributions, fields, tasks, user  # noqa","    app.config.from_object(config[config_name])","    api.init_app(app)","    ext_celery.init_app(app)","    db.init_app(app)","    oauth.init_app(app)","    oauth.register(name=\"toolhub\")","    api.register_blueprint(tasks)","    api.register_blueprint(contributions)","    api.register_blueprint(fields)","    api.register_blueprint(user)","    migrate.init_app(app, db)"],"goodparts":["from api.config import config","    with app.app_context():","        app.config.from_object(config[config_name])","        api.init_app(app)","        ext_celery.init_app(app)","        db.init_app(app)","        oauth.init_app(app)","        oauth.register(name=\"toolhub\")","        from api.routes import contributions, fields, tasks, user  # noqa","        api.register_blueprint(tasks)","        api.register_blueprint(contributions)","        api.register_blueprint(fields)","        api.register_blueprint(user)","        migrate.init_app(app, db)"]}],"source":"\nimport os from authlib.integrations.flask_client import OAuth from celery import current_app as current_celery_app from flask import Flask from flask_celeryext import FlaskCeleryExt from flask_migrate import Migrate from flask_smorest import Api from flask_sqlalchemy import SQLAlchemy api=Api() db=SQLAlchemy() migrate=Migrate() oauth=OAuth() from api.config import config from api.routes import contributions, fields, tasks, user def create_app(config_name=None): if config_name is None: config_name=os.environ.get(\"FLASK_CONFIG\", \"development\") app=Flask(__name__) app.config.from_object(config[config_name]) api.init_app(app) ext_celery.init_app(app) db.init_app(app) oauth.init_app(app) oauth.register(name=\"toolhub\") api.register_blueprint(tasks) api.register_blueprint(contributions) api.register_blueprint(fields) api.register_blueprint(user) migrate.init_app(app, db) return app def make_celery(app): celery=current_celery_app celery.config_from_object(app.config, namespace=\"CELERY\") return celery ext_celery=FlaskCeleryExt(create_celery_app=make_celery) ","sourceWithComments":"import os\n\nfrom authlib.integrations.flask_client import OAuth\nfrom celery import current_app as current_celery_app\nfrom flask import Flask\nfrom flask_celeryext import FlaskCeleryExt\nfrom flask_migrate import Migrate\nfrom flask_smorest import Api\nfrom flask_sqlalchemy import SQLAlchemy\n\napi = Api()\ndb = SQLAlchemy()\nmigrate = Migrate()\noauth = OAuth()\n\nfrom api.config import config  # noqa\nfrom api.routes import contributions, fields, tasks, user  # noqa\n\n\ndef create_app(config_name=None):\n    if config_name is None:\n        config_name = os.environ.get(\"FLASK_CONFIG\", \"development\")\n\n    app = Flask(__name__)\n    app.config.from_object(config[config_name])\n\n    api.init_app(app)\n    ext_celery.init_app(app)\n    db.init_app(app)\n    oauth.init_app(app)\n    oauth.register(name=\"toolhub\")\n    api.register_blueprint(tasks)\n    api.register_blueprint(contributions)\n    api.register_blueprint(fields)\n    api.register_blueprint(user)\n    migrate.init_app(app, db)\n\n    return app\n\n\ndef make_celery(app):\n    celery = current_celery_app\n    celery.config_from_object(app.config, namespace=\"CELERY\")\n    return celery\n\n\next_celery = FlaskCeleryExt(create_celery_app=make_celery)\n"},"\/api\/routes.py":{"changes":[{"diff":"\n import datetime\n \n import flask\n-import requests\n+from flask import current_app\n from flask.views import MethodView\n from flask_smorest import Blueprint, abort\n-from sqlalchemy import desc, exc, text\n+from sqlalchemy import desc, exc, func, text\n \n from api import db\n from api.models import Field, Task\n","add":2,"remove":2,"filename":"\/api\/routes.py","badparts":["import requests","from sqlalchemy import desc, exc, text"],"goodparts":["from flask import current_app","from sqlalchemy import desc, exc, func, text"]},{"diff":"\n     ContributionSchema,\n     FieldSchema,\n+    MetricsSchema,\n     ScoreLimitSchema,\n     ScoreSchema,\n     TaskCompleteSchema,\n     TaskSchema,\n     UserSchema,\n )\n+from api.utils import ToolhubClient, build_request, generate_past_date, get_current_user\n+\n+toolhub_client = ToolhubClient(current_app.config[\"TOOLHUB_API_ENDPOINT\"])\n \n contributions = Blueprint(\n     \"contributions\",\n     __name__,\n-    description=\"Get information about contributions made using Toolhunt\",\n+    description=\"Get information about contributions made using Toolhunt.\",\n )\n \n \n","add":5,"remove":1,"filename":"\/api\/routes.py","badparts":["    description=\"Get information about contributions made using Toolhunt\","],"goodparts":["    MetricsSchema,","from api.utils import ToolhubClient, build_request, generate_past_date, get_current_user","toolhub_client = ToolhubClient(current_app.config[\"TOOLHUB_API_ENDPOINT\"])","    description=\"Get information about contributions made using Toolhunt.\","]},{"diff":"\n     @contributions.arguments(ContributionLimitSchema, location=\"query\", required=False)\n     @contributions.response(200, ContributionSchema(many=True))\n     def get(self, query_args):\n-        \"\"\"Return contributions made using Toolhunt.\"\"\"\n+        \"\"\"List contributions made using Toolhunt.\"\"\"\n         if query_args:\n             limit = query_args[\"limit\"]\n             return (\n","add":1,"remove":1,"filename":"\/api\/routes.py","badparts":["        \"\"\"Return contributions made using Toolhunt.\"\"\""],"goodparts":["        \"\"\"List contributions made using Toolhunt.\"\"\""]},{"diff":"\n class ContributionsByUser(MethodView):\n     @contributions.response(200, ContributionSchema(many=True))\n     def get(self, user):\n-        \"\"\"Return contributions by user.\"\"\"\n+        \"\"\"List the ten most recent contributions by a user.\"\"\"\n         # Ideally in the future we could introduce pagination and return all of a user's contributions\n         return (\n             Task.query.filter(Task.user == user)\n","add":1,"remove":1,"filename":"\/api\/routes.py","badparts":["        \"\"\"Return contributions by user.\"\"\""],"goodparts":["        \"\"\"List the ten most recent contributions by a user.\"\"\""]},{"diff":"\n     @contributions.arguments(ScoreLimitSchema, location=\"query\", required=False)\n     @contributions.response(200, ScoreSchema(many=True))\n     def get(self, query_args):\n-        \"\"\"Return the most prolific Toolhunters and their scores.\"\"\"\n+        \"\"\"List the most prolific Toolhunters, by number of contributions.\"\"\"\n         if query_args:\n-            today = datetime.datetime.now(datetime.timezone.utc)\n-            day_count = query_args[\"since\"]\n-            end_date = today - datetime.timedelta(days=day_count)\n-            print(end_date)\n+            end_date = generate_past_date(query_args[\"since\"])\n             scores_query = text(\n                 \"SELECT DISTINCT user, COUNT(*) AS 'score' FROM task WHERE user IS NOT NULL AND timestamp >= :date GROUP BY user ORDER BY 2 DESC LIMIT 30\"\n             ).bindparams(date=end_date)\n-            scores = get_scores(scores_query)\n-            return scores\n         else:\n             scores_query = text(\n                 \"SELECT DISTINCT user, COUNT(*) AS 'score' FROM task WHERE user IS NOT NULL GROUP BY user ORDER BY 2 DESC LIMIT 30\"\n             )\n-            scores = get_scores(scores_query)\n-            return scores\n-\n-\n-def get_scores(scores_query):\n-    \"\"\"Insert score data into a list of dicts and return.\"\"\"\n-    results = db.session.execute(scores_query)\n-    scores = []\n-    for row in results:\n-        result = {\"user\": row[0], \"score\": row[1]}\n-        scores.append(result)\n-    return scores\n+        results = db.session.execute(scores_query)\n+        scores = []\n+        for row in results:\n+            result = {\"user\": row[0], \"score\": row[1]}\n+            scores.append(result)\n+        return scores\n \n \n fields = Blueprint(\n-    \"fields\", __name__, description=\"Retrieving information about annotations fields\"\n+    \"fields\", __name__, description=\"Get information about annotations fields.\"\n )\n \n \n","add":9,"remove":20,"filename":"\/api\/routes.py","badparts":["        \"\"\"Return the most prolific Toolhunters and their scores.\"\"\"","            today = datetime.datetime.now(datetime.timezone.utc)","            day_count = query_args[\"since\"]","            end_date = today - datetime.timedelta(days=day_count)","            print(end_date)","            scores = get_scores(scores_query)","            return scores","            scores = get_scores(scores_query)","            return scores","def get_scores(scores_query):","    \"\"\"Insert score data into a list of dicts and return.\"\"\"","    results = db.session.execute(scores_query)","    scores = []","    for row in results:","        result = {\"user\": row[0], \"score\": row[1]}","        scores.append(result)","    return scores","    \"fields\", __name__, description=\"Retrieving information about annotations fields\""],"goodparts":["        \"\"\"List the most prolific Toolhunters, by number of contributions.\"\"\"","            end_date = generate_past_date(query_args[\"since\"])","        results = db.session.execute(scores_query)","        scores = []","        for row in results:","            result = {\"user\": row[0], \"score\": row[1]}","            scores.append(result)","        return scores","    \"fields\", __name__, description=\"Get information about annotations fields.\""]},{"diff":"\n class FieldList(MethodView):\n     @fields.response(200, FieldSchema(many=True))\n     def get(self):\n-        \"Return all annotations field data.\"\n+        \"\"\"List all annotations fields.\"\"\"\n         return Field.query.all()\n \n \n","add":1,"remove":1,"filename":"\/api\/routes.py","badparts":["        \"Return all annotations field data.\""],"goodparts":["        \"\"\"List all annotations fields.\"\"\""]},{"diff":"\n class FieldInformation(MethodView):\n     @fields.response(200, FieldSchema)\n     def get(self, name):\n-        \"Return data about a specific annotations field.\"\n+        \"\"\"Get information about an annotations field.\"\"\"\n         return Field.query.get_or_404(name)\n \n \n-tasks = Blueprint(\"tasks\", __name__, description=\"Fetching and updating Toolhunt tasks\")\n+metrics = Blueprint(\n+    \"metrics\",\n+    __name__,\n+    description=\"Get information about various metrics related to Toolhunt.\",\n+)\n+\n+\n+@metrics.route(\"\/api\/metrics\/contributions\")\n+class ContributionsMetrics(MethodView):\n+    @metrics.response(200, MetricsSchema(many=True))\n+    def get(self):\n+        \"\"\"Get metrics pertaining to contributions.\"\"\"\n+        try:\n+            results = []\n+            date_limit = generate_past_date(30)\n+            total = db.session.execute(\n+                text(\"SELECT COUNT(*) FROM task WHERE user IS NOT NULL\")\n+            ).all()\n+            results.append(dict(result=total[0][0], description=\"Total contributions:\"))\n+            thirty_day = db.session.execute(\n+                text(\n+                    \"SELECT COUNT(*) FROM task WHERE user IS NOT NULL AND timestamp >= :date\"\n+                ).bindparams(date=date_limit)\n+            ).all()\n+            results.append(\n+                dict(\n+                    result=thirty_day[0][0],\n+                    description=\"Global contributions from the last 30 days:\",\n+                )\n+            )\n+            return results\n+        except exc.OperationalError as err:\n+            print(err)\n+            abort(503, message=\"Database connection failed.  Please try again.\")\n+\n+\n+@metrics.route(\"\/api\/metrics\/tasks\")\n+class TaskMetrics(MethodView):\n+    @metrics.response(200, MetricsSchema(many=True))\n+    def get(self):\n+        \"\"\"Get metrics pertaining to Toolhunt tasks.\"\"\"\n+        try:\n+            results = []\n+            total = db.session.execute(text(\"SELECT COUNT(*) FROM task\")).all()\n+            results.append(\n+                dict(\n+                    result=total[0][0],\n+                    description=\"Number of tasks in the Toolhunt database:\",\n+                )\n+            )\n+            incomplete = db.session.execute(\n+                text(\"SELECT COUNT(*) FROM task WHERE user IS NULL\")\n+            ).all()\n+            results.append(\n+                dict(\n+                    result=incomplete[0][0],\n+                    description=\"Number of unfinished tasks in the Toolhunt database:\",\n+                )\n+            )\n+            return results\n+        except exc.OperationalError as err:\n+            print(err)\n+            abort(503, message=\"Database connection failed.  Please try again.\")\n+\n+\n+@metrics.route(\"\/api\/metrics\/tools\")\n+class ToolMetrics(MethodView):\n+    @metrics.response(200, MetricsSchema(many=True))\n+    def get(self):\n+        \"\"\"Get metrics pertaining to tools.\"\"\"\n+        try:\n+            results = []\n+            total = db.session.execute(text(\"SELECT COUNT(*) FROM tool\")).all()\n+            results.append(\n+                dict(result=total[0][0], description=\"Number of tools on record:\")\n+            )\n+            missing_info = db.session.execute(\n+                text(\"SELECT COUNT(DISTINCT tool_name) FROM task WHERE user IS NULL\")\n+            ).all()\n+            results.append(\n+                dict(\n+                    result=missing_info[0][0],\n+                    description=\"Number of tools with incomplete information:\",\n+                )\n+            )\n+            return results\n+        except exc.OperationalError as err:\n+            print(err)\n+            abort(503, message=\"Database connection failed.  Please try again.\")\n+\n+\n+@metrics.route(\"\/api\/metrics\/user\")\n+class UserMetrics(MethodView):\n+    @metrics.response(200, MetricsSchema(many=True))\n+    def get(self):\n+        \"\"\"Get metrics pertaining to the currently logged-in user.\"\"\"\n+        user = get_current_user()\n+        if type(user) == str:\n+            date_limit = generate_past_date(30)\n+            try:\n+                results = []\n+                total_cont = db.session.execute(\n+                    text(\"SELECT COUNT(*) FROM task WHERE user = :user\").bindparams(\n+                        user=user\n+                    )\n+                ).all()\n+                results.append(\n+                    dict(result=total_cont[0][0], description=\"My total contributions:\")\n+                )\n+                thirty_cont = db.session.execute(\n+                    text(\n+                        \"SELECT COUNT(*) FROM task WHERE user = :user AND timestamp >= :date\"\n+                    ).bindparams(user=user, date=date_limit)\n+                ).all()\n+                results.append(\n+                    dict(\n+                        result=thirty_cont[0][0],\n+                        description=\"My contributions in the past 30 days:\",\n+                    )\n+                )\n+                return results\n+            except exc.OperationalError as err:\n+                print(err)\n+                abort(503, message=\"Database connection failed.  Please try again.\")\n+        else:\n+            return user\n+\n+\n+tasks = Blueprint(\n+    \"tasks\", __name__, description=\"Get incomplete tasks and submit data to Toolhub.\"\n+)\n \n \n @tasks.route(\"\/api\/tasks\")\n class TaskList(MethodView):\n     @tasks.response(200, TaskSchema(many=True))\n     def get(self):\n-        \"Return a bundle of 10 incomplete tasks.\"\n-        return Task.query.filter(Task.user.is_(None)).limit(10)\n+        \"Get ten incomplete tasks.\"\n+        return Task.query.filter(Task.user.is_(None)).order_by(func.random()).limit(10)\n \n \n @tasks.route(\"\/api\/tasks\/<string:task_id>\")\n class TaskById(MethodView):\n     @tasks.response(200, TaskSchema)\n     def get(self, task_id):\n-        \"Return information about a specific task.\"\n+        \"\"\"Get information about a specific task.\"\"\"\n         task = Task.query.get_or_404(task_id)\n         return task\n \n","add":135,"remove":5,"filename":"\/api\/routes.py","badparts":["        \"Return data about a specific annotations field.\"","tasks = Blueprint(\"tasks\", __name__, description=\"Fetching and updating Toolhunt tasks\")","        \"Return a bundle of 10 incomplete tasks.\"","        return Task.query.filter(Task.user.is_(None)).limit(10)","        \"Return information about a specific task.\""],"goodparts":["        \"\"\"Get information about an annotations field.\"\"\"","metrics = Blueprint(","    \"metrics\",","    __name__,","    description=\"Get information about various metrics related to Toolhunt.\",",")","@metrics.route(\"\/api\/metrics\/contributions\")","class ContributionsMetrics(MethodView):","    @metrics.response(200, MetricsSchema(many=True))","    def get(self):","        \"\"\"Get metrics pertaining to contributions.\"\"\"","        try:","            results = []","            date_limit = generate_past_date(30)","            total = db.session.execute(","                text(\"SELECT COUNT(*) FROM task WHERE user IS NOT NULL\")","            ).all()","            results.append(dict(result=total[0][0], description=\"Total contributions:\"))","            thirty_day = db.session.execute(","                text(","                    \"SELECT COUNT(*) FROM task WHERE user IS NOT NULL AND timestamp >= :date\"","                ).bindparams(date=date_limit)","            ).all()","            results.append(","                dict(","                    result=thirty_day[0][0],","                    description=\"Global contributions from the last 30 days:\",","                )","            )","            return results","        except exc.OperationalError as err:","            print(err)","            abort(503, message=\"Database connection failed.  Please try again.\")","@metrics.route(\"\/api\/metrics\/tasks\")","class TaskMetrics(MethodView):","    @metrics.response(200, MetricsSchema(many=True))","    def get(self):","        \"\"\"Get metrics pertaining to Toolhunt tasks.\"\"\"","        try:","            results = []","            total = db.session.execute(text(\"SELECT COUNT(*) FROM task\")).all()","            results.append(","                dict(","                    result=total[0][0],","                    description=\"Number of tasks in the Toolhunt database:\",","                )","            )","            incomplete = db.session.execute(","                text(\"SELECT COUNT(*) FROM task WHERE user IS NULL\")","            ).all()","            results.append(","                dict(","                    result=incomplete[0][0],","                    description=\"Number of unfinished tasks in the Toolhunt database:\",","                )","            )","            return results","        except exc.OperationalError as err:","            print(err)","            abort(503, message=\"Database connection failed.  Please try again.\")","@metrics.route(\"\/api\/metrics\/tools\")","class ToolMetrics(MethodView):","    @metrics.response(200, MetricsSchema(many=True))","    def get(self):","        \"\"\"Get metrics pertaining to tools.\"\"\"","        try:","            results = []","            total = db.session.execute(text(\"SELECT COUNT(*) FROM tool\")).all()","            results.append(","                dict(result=total[0][0], description=\"Number of tools on record:\")","            )","            missing_info = db.session.execute(","                text(\"SELECT COUNT(DISTINCT tool_name) FROM task WHERE user IS NULL\")","            ).all()","            results.append(","                dict(","                    result=missing_info[0][0],","                    description=\"Number of tools with incomplete information:\",","                )","            )","            return results","        except exc.OperationalError as err:","            print(err)","            abort(503, message=\"Database connection failed.  Please try again.\")","@metrics.route(\"\/api\/metrics\/user\")","class UserMetrics(MethodView):","    @metrics.response(200, MetricsSchema(many=True))","    def get(self):","        \"\"\"Get metrics pertaining to the currently logged-in user.\"\"\"","        user = get_current_user()","        if type(user) == str:","            date_limit = generate_past_date(30)","            try:","                results = []","                total_cont = db.session.execute(","                    text(\"SELECT COUNT(*) FROM task WHERE user = :user\").bindparams(","                        user=user","                    )","                ).all()","                results.append(","                    dict(result=total_cont[0][0], description=\"My total contributions:\")","                )","                thirty_cont = db.session.execute(","                    text(","                        \"SELECT COUNT(*) FROM task WHERE user = :user AND timestamp >= :date\"","                    ).bindparams(user=user, date=date_limit)","                ).all()","                results.append(","                    dict(","                        result=thirty_cont[0][0],","                        description=\"My contributions in the past 30 days:\",","                    )","                )","                return results","            except exc.OperationalError as err:","                print(err)","                abort(503, message=\"Database connection failed.  Please try again.\")","        else:","            return user","tasks = Blueprint(","    \"tasks\", __name__, description=\"Get incomplete tasks and submit data to Toolhub.\"",")","        \"Get ten incomplete tasks.\"","        return Task.query.filter(Task.user.is_(None)).order_by(func.random()).limit(10)","        \"\"\"Get information about a specific task.\"\"\""]},{"diff":"\n             and task.field_name == task_data[\"field\"]\n         ):\n             if task.user is not None:\n-                return \"This task has already been completed.\"\n+                abort(409, message=\"This task has already been completed.\")\n             elif flask.session and flask.session[\"token\"]:\n                 tool = task_data[\"tool\"]\n                 data_obj = build_request(task_data)\n-                result = put_to_toolhub(tool, data_obj)\n+                result = toolhub_client.put(tool, data_obj)\n                 if result == 200:\n                     username = get_current_user()\n                     task.user = username\n","add":2,"remove":2,"filename":"\/api\/routes.py","badparts":["                return \"This task has already been completed.\"","                result = put_to_toolhub(tool, data_obj)"],"goodparts":["                abort(409, message=\"This task has already been completed.\")","                result = toolhub_client.put(tool, data_obj)"]},{"diff":"\n                     try:\n                         db.session.commit()\n                         return f'{task_data[\"field\"]} successfully updated for {tool}.'\n-                    except exc.SQLAlchemyError as err:\n-                        error = str(err.orig)\n-                        return error\n+                    except exc.DBAPIError as err:\n+                        print(err)\n+                        abort(503, message=\"Database connection failed.\")\n                 else:\n-                    return \"Inserting the data into Toolhub didn't work.\"\n+                    abort(503, message=\"We were unable to insert the data into Toolhub.\")\n             else:\n-                return \"User must be logged in to update a tool.\"\n+                abort(401, message=\"User must be logged in to update a tool.\")\n         else:\n-            return \"The data doesn't match the specified task.\"\n-\n-\n-def build_request(task_data):\n-    \"\"\"Take data and return an object to PUT to Toolhub\"\"\"\n-    field = task_data[\"field\"]\n-    value = task_data[\"value\"]\n-    comment = f\"Updated {field} using Toolhunt\"\n-    data = {}\n-    data[field] = value\n-    data[\"comment\"] = comment\n-    return data\n-\n-\n-def get_current_user():\n-    \"\"\"Get the username of currently logged-in user.\"\"\"\n-    # Importing the oauth early results in an error\n-    # Will fix this once I've dealt with T330263\n-    from app import oauth\n-\n-    if not flask.session:\n-        abort(401, message=\"No user is currently logged in.\")\n-    else:\n-        try:\n-            resp = oauth.toolhub.get(\"user\/\", token=flask.session[\"token\"])\n-            print(resp, \"This is from the function\")\n-            resp.raise_for_status()\n-            profile = resp.json()\n-            username = profile[\"username\"]\n-            return username\n-        except requests.exceptions.HTTPError as err:\n-            print(err)\n-            abort(401, message=\"User authorization failed.\")\n-        except requests.exceptions.ConnectionError as err:\n-            print(err)\n-            abort(503, message=\"Server connection failed.  Please try again.\")\n-        except requests.exceptions.RequestException as err:\n-            print(err)\n-            abort(501, message=\"Server encountered an unexpected error.\")\n-\n-\n-def put_to_toolhub(tool, data):\n-    \"\"\"Take request data from the frontend and make a PUT request to Toolhub.\"\"\"\n-    TOOL_TEST_API_ENDPOINT = \"https:\/\/toolhub-demo.wmcloud.org\/api\/tools\/\"\n-    url = f\"{TOOL_TEST_API_ENDPOINT}{tool}\/annotations\/\"\n-    header = {\"Authorization\": f'Bearer {flask.session[\"token\"][\"access_token\"]}'}\n-    response = requests.put(url, data=data, headers=header)\n-    r = response.status_code\n-    return r\n+            abort(400, message=\"The data given doesn't match the task specifications.\")\n \n \n user = Blueprint(\n","add":6,"remove":54,"filename":"\/api\/routes.py","badparts":["                    except exc.SQLAlchemyError as err:","                        error = str(err.orig)","                        return error","                    return \"Inserting the data into Toolhub didn't work.\"","                return \"User must be logged in to update a tool.\"","            return \"The data doesn't match the specified task.\"","def build_request(task_data):","    \"\"\"Take data and return an object to PUT to Toolhub\"\"\"","    field = task_data[\"field\"]","    value = task_data[\"value\"]","    comment = f\"Updated {field} using Toolhunt\"","    data = {}","    data[field] = value","    data[\"comment\"] = comment","    return data","def get_current_user():","    \"\"\"Get the username of currently logged-in user.\"\"\"","    from app import oauth","    if not flask.session:","        abort(401, message=\"No user is currently logged in.\")","    else:","        try:","            resp = oauth.toolhub.get(\"user\/\", token=flask.session[\"token\"])","            print(resp, \"This is from the function\")","            resp.raise_for_status()","            profile = resp.json()","            username = profile[\"username\"]","            return username","        except requests.exceptions.HTTPError as err:","            print(err)","            abort(401, message=\"User authorization failed.\")","        except requests.exceptions.ConnectionError as err:","            print(err)","            abort(503, message=\"Server connection failed.  Please try again.\")","        except requests.exceptions.RequestException as err:","            print(err)","            abort(501, message=\"Server encountered an unexpected error.\")","def put_to_toolhub(tool, data):","    \"\"\"Take request data from the frontend and make a PUT request to Toolhub.\"\"\"","    TOOL_TEST_API_ENDPOINT = \"https:\/\/toolhub-demo.wmcloud.org\/api\/tools\/\"","    url = f\"{TOOL_TEST_API_ENDPOINT}{tool}\/annotations\/\"","    header = {\"Authorization\": f'Bearer {flask.session[\"token\"][\"access_token\"]}'}","    response = requests.put(url, data=data, headers=header)","    r = response.status_code","    return r"],"goodparts":["                    except exc.DBAPIError as err:","                        print(err)","                        abort(503, message=\"Database connection failed.\")","                    abort(503, message=\"We were unable to insert the data into Toolhub.\")","                abort(401, message=\"User must be logged in to update a tool.\")","            abort(400, message=\"The data given doesn't match the task specifications.\")"]},{"diff":"\n class CurrentUser(MethodView):\n     @tasks.response(200, UserSchema)\n     def get(self):\n-        \"\"\"Get the username of currently logged-in user.\"\"\"\n+        \"\"\"Get the username of the currently logged-in user.\"\"\"\n         response = get_current_user()\n         if type(response) == str:\n             username = {\"username\": respons","add":1,"remove":1,"filename":"\/api\/routes.py","badparts":["        \"\"\"Get the username of currently logged-in user.\"\"\""],"goodparts":["        \"\"\"Get the username of the currently logged-in user.\"\"\""]}],"source":"\nimport datetime import flask import requests from flask.views import MethodView from flask_smorest import Blueprint, abort from sqlalchemy import desc, exc, text from api import db from api.models import Field, Task from api.schemas import( ContributionLimitSchema, ContributionSchema, FieldSchema, ScoreLimitSchema, ScoreSchema, TaskCompleteSchema, TaskSchema, UserSchema, ) contributions=Blueprint( \"contributions\", __name__, description=\"Get information about contributions made using Toolhunt\", ) @contributions.route(\"\/api\/contributions\/\") class Contributions(MethodView): @contributions.arguments(ContributionLimitSchema, location=\"query\", required=False) @contributions.response(200, ContributionSchema(many=True)) def get(self, query_args): \"\"\"Return contributions made using Toolhunt.\"\"\" if query_args: limit=query_args[\"limit\"] return( Task.query.filter(Task.user.is_not(None)) .order_by(desc(Task.timestamp)) .limit(int(limit)) ) else: return Task.query.filter(Task.user.is_not(None)).order_by( desc(Task.timestamp) ) @contributions.route(\"\/api\/contributions\/<string:user>\") class ContributionsByUser(MethodView): @contributions.response(200, ContributionSchema(many=True)) def get(self, user): \"\"\"Return contributions by user.\"\"\" return( Task.query.filter(Task.user==user) .order_by(desc(Task.timestamp)) .limit(10) ) @contributions.route(\"\/api\/contributions\/top-scores\") class ContributionHighScores(MethodView): @contributions.arguments(ScoreLimitSchema, location=\"query\", required=False) @contributions.response(200, ScoreSchema(many=True)) def get(self, query_args): \"\"\"Return the most prolific Toolhunters and their scores.\"\"\" if query_args: today=datetime.datetime.now(datetime.timezone.utc) day_count=query_args[\"since\"] end_date=today -datetime.timedelta(days=day_count) print(end_date) scores_query=text( \"SELECT DISTINCT user, COUNT(*) AS 'score' FROM task WHERE user IS NOT NULL AND timestamp >=:date GROUP BY user ORDER BY 2 DESC LIMIT 30\" ).bindparams(date=end_date) scores=get_scores(scores_query) return scores else: scores_query=text( \"SELECT DISTINCT user, COUNT(*) AS 'score' FROM task WHERE user IS NOT NULL GROUP BY user ORDER BY 2 DESC LIMIT 30\" ) scores=get_scores(scores_query) return scores def get_scores(scores_query): \"\"\"Insert score data into a list of dicts and return.\"\"\" results=db.session.execute(scores_query) scores=[] for row in results: result={\"user\": row[0], \"score\": row[1]} scores.append(result) return scores fields=Blueprint( \"fields\", __name__, description=\"Retrieving information about annotations fields\" ) @fields.route(\"\/api\/fields\") class FieldList(MethodView): @fields.response(200, FieldSchema(many=True)) def get(self): \"Return all annotations field data.\" return Field.query.all() @fields.route(\"\/api\/fields\/<string:name>\") class FieldInformation(MethodView): @fields.response(200, FieldSchema) def get(self, name): \"Return data about a specific annotations field.\" return Field.query.get_or_404(name) tasks=Blueprint(\"tasks\", __name__, description=\"Fetching and updating Toolhunt tasks\") @tasks.route(\"\/api\/tasks\") class TaskList(MethodView): @tasks.response(200, TaskSchema(many=True)) def get(self): \"Return a bundle of 10 incomplete tasks.\" return Task.query.filter(Task.user.is_(None)).limit(10) @tasks.route(\"\/api\/tasks\/<string:task_id>\") class TaskById(MethodView): @tasks.response(200, TaskSchema) def get(self, task_id): \"Return information about a specific task.\" task=Task.query.get_or_404(task_id) return task @tasks.arguments(TaskCompleteSchema) @tasks.response(201) def put(self, task_data, task_id): \"\"\"Update a tool record on Toolhub.\"\"\" task=Task.query.get_or_404(task_id) if( task and task.tool_name==task_data[\"tool\"] and task.field_name==task_data[\"field\"] ): if task.user is not None: return \"This task has already been completed.\" elif flask.session and flask.session[\"token\"]: tool=task_data[\"tool\"] data_obj=build_request(task_data) result=put_to_toolhub(tool, data_obj) if result==200: username=get_current_user() task.user=username task.timestamp=datetime.datetime.now(datetime.timezone.utc) db.session.add(task) try: db.session.commit() return f'{task_data[\"field\"]} successfully updated for{tool}.' except exc.SQLAlchemyError as err: error=str(err.orig) return error else: return \"Inserting the data into Toolhub didn't work.\" else: return \"User must be logged in to update a tool.\" else: return \"The data doesn't match the specified task.\" def build_request(task_data): \"\"\"Take data and return an object to PUT to Toolhub\"\"\" field=task_data[\"field\"] value=task_data[\"value\"] comment=f\"Updated{field} using Toolhunt\" data={} data[field]=value data[\"comment\"]=comment return data def get_current_user(): \"\"\"Get the username of currently logged-in user.\"\"\" from app import oauth if not flask.session: abort(401, message=\"No user is currently logged in.\") else: try: resp=oauth.toolhub.get(\"user\/\", token=flask.session[\"token\"]) print(resp, \"This is from the function\") resp.raise_for_status() profile=resp.json() username=profile[\"username\"] return username except requests.exceptions.HTTPError as err: print(err) abort(401, message=\"User authorization failed.\") except requests.exceptions.ConnectionError as err: print(err) abort(503, message=\"Server connection failed. Please try again.\") except requests.exceptions.RequestException as err: print(err) abort(501, message=\"Server encountered an unexpected error.\") def put_to_toolhub(tool, data): \"\"\"Take request data from the frontend and make a PUT request to Toolhub.\"\"\" TOOL_TEST_API_ENDPOINT=\"https:\/\/toolhub-demo.wmcloud.org\/api\/tools\/\" url=f\"{TOOL_TEST_API_ENDPOINT}{tool}\/annotations\/\" header={\"Authorization\": f'Bearer{flask.session[\"token\"][\"access_token\"]}'} response=requests.put(url, data=data, headers=header) r=response.status_code return r user=Blueprint( \"user\", __name__, description=\"Get information about the currently logged-in user.\" ) @user.route(\"\/api\/user\") class CurrentUser(MethodView): @tasks.response(200, UserSchema) def get(self): \"\"\"Get the username of currently logged-in user.\"\"\" response=get_current_user() if type(response)==str: username={\"username\": response} return username else: return response ","sourceWithComments":"import datetime\n\nimport flask\nimport requests\nfrom flask.views import MethodView\nfrom flask_smorest import Blueprint, abort\nfrom sqlalchemy import desc, exc, text\n\nfrom api import db\nfrom api.models import Field, Task\nfrom api.schemas import (\n    ContributionLimitSchema,\n    ContributionSchema,\n    FieldSchema,\n    ScoreLimitSchema,\n    ScoreSchema,\n    TaskCompleteSchema,\n    TaskSchema,\n    UserSchema,\n)\n\ncontributions = Blueprint(\n    \"contributions\",\n    __name__,\n    description=\"Get information about contributions made using Toolhunt\",\n)\n\n\n@contributions.route(\"\/api\/contributions\/\")\nclass Contributions(MethodView):\n    @contributions.arguments(ContributionLimitSchema, location=\"query\", required=False)\n    @contributions.response(200, ContributionSchema(many=True))\n    def get(self, query_args):\n        \"\"\"Return contributions made using Toolhunt.\"\"\"\n        if query_args:\n            limit = query_args[\"limit\"]\n            return (\n                Task.query.filter(Task.user.is_not(None))\n                .order_by(desc(Task.timestamp))\n                .limit(int(limit))\n            )\n        else:\n            return Task.query.filter(Task.user.is_not(None)).order_by(\n                desc(Task.timestamp)\n            )\n\n\n@contributions.route(\"\/api\/contributions\/<string:user>\")\nclass ContributionsByUser(MethodView):\n    @contributions.response(200, ContributionSchema(many=True))\n    def get(self, user):\n        \"\"\"Return contributions by user.\"\"\"\n        # Ideally in the future we could introduce pagination and return all of a user's contributions\n        return (\n            Task.query.filter(Task.user == user)\n            .order_by(desc(Task.timestamp))\n            .limit(10)\n        )\n\n\n@contributions.route(\"\/api\/contributions\/top-scores\")\nclass ContributionHighScores(MethodView):\n    @contributions.arguments(ScoreLimitSchema, location=\"query\", required=False)\n    @contributions.response(200, ScoreSchema(many=True))\n    def get(self, query_args):\n        \"\"\"Return the most prolific Toolhunters and their scores.\"\"\"\n        if query_args:\n            today = datetime.datetime.now(datetime.timezone.utc)\n            day_count = query_args[\"since\"]\n            end_date = today - datetime.timedelta(days=day_count)\n            print(end_date)\n            scores_query = text(\n                \"SELECT DISTINCT user, COUNT(*) AS 'score' FROM task WHERE user IS NOT NULL AND timestamp >= :date GROUP BY user ORDER BY 2 DESC LIMIT 30\"\n            ).bindparams(date=end_date)\n            scores = get_scores(scores_query)\n            return scores\n        else:\n            scores_query = text(\n                \"SELECT DISTINCT user, COUNT(*) AS 'score' FROM task WHERE user IS NOT NULL GROUP BY user ORDER BY 2 DESC LIMIT 30\"\n            )\n            scores = get_scores(scores_query)\n            return scores\n\n\ndef get_scores(scores_query):\n    \"\"\"Insert score data into a list of dicts and return.\"\"\"\n    results = db.session.execute(scores_query)\n    scores = []\n    for row in results:\n        result = {\"user\": row[0], \"score\": row[1]}\n        scores.append(result)\n    return scores\n\n\nfields = Blueprint(\n    \"fields\", __name__, description=\"Retrieving information about annotations fields\"\n)\n\n\n@fields.route(\"\/api\/fields\")\nclass FieldList(MethodView):\n    @fields.response(200, FieldSchema(many=True))\n    def get(self):\n        \"Return all annotations field data.\"\n        return Field.query.all()\n\n\n@fields.route(\"\/api\/fields\/<string:name>\")\nclass FieldInformation(MethodView):\n    @fields.response(200, FieldSchema)\n    def get(self, name):\n        \"Return data about a specific annotations field.\"\n        return Field.query.get_or_404(name)\n\n\ntasks = Blueprint(\"tasks\", __name__, description=\"Fetching and updating Toolhunt tasks\")\n\n\n@tasks.route(\"\/api\/tasks\")\nclass TaskList(MethodView):\n    @tasks.response(200, TaskSchema(many=True))\n    def get(self):\n        \"Return a bundle of 10 incomplete tasks.\"\n        return Task.query.filter(Task.user.is_(None)).limit(10)\n\n\n@tasks.route(\"\/api\/tasks\/<string:task_id>\")\nclass TaskById(MethodView):\n    @tasks.response(200, TaskSchema)\n    def get(self, task_id):\n        \"Return information about a specific task.\"\n        task = Task.query.get_or_404(task_id)\n        return task\n\n    @tasks.arguments(TaskCompleteSchema)\n    @tasks.response(201)\n    def put(self, task_data, task_id):\n        \"\"\"Update a tool record on Toolhub.\"\"\"\n        task = Task.query.get_or_404(task_id)\n        if (\n            task\n            and task.tool_name == task_data[\"tool\"]\n            and task.field_name == task_data[\"field\"]\n        ):\n            if task.user is not None:\n                return \"This task has already been completed.\"\n            elif flask.session and flask.session[\"token\"]:\n                tool = task_data[\"tool\"]\n                data_obj = build_request(task_data)\n                result = put_to_toolhub(tool, data_obj)\n                if result == 200:\n                    username = get_current_user()\n                    task.user = username\n                    task.timestamp = datetime.datetime.now(datetime.timezone.utc)\n                    db.session.add(task)\n                    try:\n                        db.session.commit()\n                        return f'{task_data[\"field\"]} successfully updated for {tool}.'\n                    except exc.SQLAlchemyError as err:\n                        error = str(err.orig)\n                        return error\n                else:\n                    return \"Inserting the data into Toolhub didn't work.\"\n            else:\n                return \"User must be logged in to update a tool.\"\n        else:\n            return \"The data doesn't match the specified task.\"\n\n\ndef build_request(task_data):\n    \"\"\"Take data and return an object to PUT to Toolhub\"\"\"\n    field = task_data[\"field\"]\n    value = task_data[\"value\"]\n    comment = f\"Updated {field} using Toolhunt\"\n    data = {}\n    data[field] = value\n    data[\"comment\"] = comment\n    return data\n\n\ndef get_current_user():\n    \"\"\"Get the username of currently logged-in user.\"\"\"\n    # Importing the oauth early results in an error\n    # Will fix this once I've dealt with T330263\n    from app import oauth\n\n    if not flask.session:\n        abort(401, message=\"No user is currently logged in.\")\n    else:\n        try:\n            resp = oauth.toolhub.get(\"user\/\", token=flask.session[\"token\"])\n            print(resp, \"This is from the function\")\n            resp.raise_for_status()\n            profile = resp.json()\n            username = profile[\"username\"]\n            return username\n        except requests.exceptions.HTTPError as err:\n            print(err)\n            abort(401, message=\"User authorization failed.\")\n        except requests.exceptions.ConnectionError as err:\n            print(err)\n            abort(503, message=\"Server connection failed.  Please try again.\")\n        except requests.exceptions.RequestException as err:\n            print(err)\n            abort(501, message=\"Server encountered an unexpected error.\")\n\n\ndef put_to_toolhub(tool, data):\n    \"\"\"Take request data from the frontend and make a PUT request to Toolhub.\"\"\"\n    TOOL_TEST_API_ENDPOINT = \"https:\/\/toolhub-demo.wmcloud.org\/api\/tools\/\"\n    url = f\"{TOOL_TEST_API_ENDPOINT}{tool}\/annotations\/\"\n    header = {\"Authorization\": f'Bearer {flask.session[\"token\"][\"access_token\"]}'}\n    response = requests.put(url, data=data, headers=header)\n    r = response.status_code\n    return r\n\n\nuser = Blueprint(\n    \"user\", __name__, description=\"Get information about the currently logged-in user.\"\n)\n\n\n@user.route(\"\/api\/user\")\nclass CurrentUser(MethodView):\n    @tasks.response(200, UserSchema)\n    def get(self):\n        \"\"\"Get the username of currently logged-in user.\"\"\"\n        response = get_current_user()\n        if type(response) == str:\n            username = {\"username\": response}\n            return username\n        else:\n            return response\n"},"\/jobs\/populate_db.py":{"changes":[{"diff":"\n import json\n \n-import requests\n from sqlalchemy import insert, select, text\n \n from api import db\n","add":0,"remove":1,"filename":"\/jobs\/populate_db.py","badparts":["import requests"],"goodparts":[]},{"diff":"\n             db.session.execute(insert(Task), task)\n             db.session.commit()\n             print(f\"Added {field} task for {tool_name}\")\n-\n-\n-REQUEST_LABEL = \"Toolhunt API\"\n-USER_INFO = \"User: NicoleLBee\"\n-headers = {\"User-Agent\": f\"{REQUEST_LABEL} - {USER_INFO}\"}\n-TOOL_API_ENDPOINT = \"https:\/\/toolhub.wikimedia.org\/api\/tools\/\"\n-TOOL_TEST_API_ENDPOINT = \"https:\/\/toolhub-demo.wmcloud.org\/api\/tools\/\"\n-\n-\n-def get_tools():\n-    \"\"\"Getting data on all Toolhub tools\"\"\"\n-    url = f\"{TOOL_TEST_API_ENDPOINT}\"\n-    response = requests.get(url, headers=headers)\n-    if response.status_code == 200:\n-        api_response = response.json()\n-        tools = api_response[\"results\"]\n-        while api_response[\"next\"]:\n-            api_response = requests.get(api_response[\"next\"], headers=headers).json()\n-            tools.extend(api_response[\"results\"])\n-        return tools\n-\n-\n-def get_single_tool(tool):\n-    \"\"\"Gets data on a single tool\"\"\"\n-    url = f\"{TOOL_TEST_API_ENDPOINT}{tool}\"\n-    response = requests.get(url, headers=headers)\n-    if response.status_code == 200:\n-        api_response = response.json()\n-        return api_response\n-\n-\n-def populate_db(data_set):\n-    \"\"\"Accepts a list of dicts and runs each dict through the insertion process\"\"\"\n-    for tool in data_set:\n-        check_for_entry(tool)\n-    return \"All d","add":0,"remove":36,"filename":"\/jobs\/populate_db.py","badparts":["REQUEST_LABEL = \"Toolhunt API\"","USER_INFO = \"User: NicoleLBee\"","headers = {\"User-Agent\": f\"{REQUEST_LABEL} - {USER_INFO}\"}","TOOL_API_ENDPOINT = \"https:\/\/toolhub.wikimedia.org\/api\/tools\/\"","TOOL_TEST_API_ENDPOINT = \"https:\/\/toolhub-demo.wmcloud.org\/api\/tools\/\"","def get_tools():","    \"\"\"Getting data on all Toolhub tools\"\"\"","    url = f\"{TOOL_TEST_API_ENDPOINT}\"","    response = requests.get(url, headers=headers)","    if response.status_code == 200:","        api_response = response.json()","        tools = api_response[\"results\"]","        while api_response[\"next\"]:","            api_response = requests.get(api_response[\"next\"], headers=headers).json()","            tools.extend(api_response[\"results\"])","        return tools","def get_single_tool(tool):","    \"\"\"Gets data on a single tool\"\"\"","    url = f\"{TOOL_TEST_API_ENDPOINT}{tool}\"","    response = requests.get(url, headers=headers)","    if response.status_code == 200:","        api_response = response.json()","        return api_response","def populate_db(data_set):","    \"\"\"Accepts a list of dicts and runs each dict through the insertion process\"\"\"","    for tool in data_set:","        check_for_entry(tool)","    return \"All d"],"goodparts":[]}],"source":"\nimport json import requests from sqlalchemy import insert, select, text from api import db from api.models import Field, Task, Tool from app import app BASE_DIR=app.config[\"BASE_DIR\"] def insert_fields(): \"\"\"Insert data about annotations fields into the DB\"\"\" with open(f\"{BASE_DIR}\/tests\/fixtures\/fields.json\") as fields: field_data=json.load(fields) db.session.bulk_insert_mappings(Field, field_data) db.session.commit() def check_for_entry(tool): \"\"\"Receives a dict containing tool information and checks to see if it exists in the DB\"\"\" tool_name=tool[\"name\"] result=db.session.execute(select(Tool).where(Tool.name==tool_name)).all() if len(result) > 0: print(f\"{tool_name} already exists in database\") else: add_tool_entry(tool) check_deprecation(tool) def add_tool_entry(tool): \"\"\"Receives a dict containing tool information and adds an entry to the tool table\"\"\" tool={ \"name\": tool[\"name\"], \"title\": tool[\"title\"], \"description\": tool[\"description\"], \"url\": tool[\"url\"], } db.session.execute(insert(Tool), tool) db.session.commit() print(f\"{tool['name']} inserted into db\") def check_deprecation(tool): \"\"\"Receives a dict containing tool information and checks its deprecation status.\"\"\" tool_name=tool[\"name\"] if tool[\"deprecated\"] is True or tool[\"annotations\"][\"deprecated\"] is True: print(f\"{tool_name} is deprecated\") fields=[] for field in tool[\"annotations\"]: if field==\"replaced_by\": if( tool[\"annotations\"][\"replaced_by\"] is None and tool[\"replaced_by\"] is None ): add_tasks([\"replaced_by\"], tool_name) continue else: fields.append(field) remove_tasks(fields, tool_name) else: print(f\"{tool_name} is not deprecated. Sorting annotations fields.\") remove_tasks([\"replaced_by\"], tool_name) sort_fields(tool) def sort_fields(tool): \"\"\"Receives a tool dict and checks\/sorts the values of the core\/annotations fields\"\"\" completed_fields=[] empty_fields=[] tool_name=tool[\"name\"] for field in tool[\"annotations\"]: fields_to_skip=[ \"replaced_by\", \"deprecated\", \"experimental\", \"developer_docs_url\", \"user_docs_url\", \"feedback_url\", \"privacy_policy_url\", \"for_wikis\", \"available_ui_languages\", ] if field in fields_to_skip: continue elif field in tool: if(tool[field]==[] or tool[field] is None) and( tool[\"annotations\"][field]==[] or tool[\"annotations\"][field] is None ): empty_fields.append(field) elif tool[field] !=[] or tool[field] is not None: completed_fields.append(field) continue elif tool[\"annotations\"][field]==[] or tool[\"annotations\"][field] is None: empty_fields.append(field) elif tool[\"annotations\"][field] !=[] or tool[\"annotations\"][field] is not None: completed_fields.append(field) print({\"Empty\": empty_fields, \"Completed\": completed_fields}) add_tasks(empty_fields, tool_name) remove_tasks(completed_fields, tool_name) def remove_tasks(fields, tool_name): \"\"\"Receives a list of fields and a tool name and removes matching, incomplete tasks from the task table\"\"\" for field in fields: query=text( \"DELETE FROM task WHERE field_name=:field_name AND tool_name=:tool AND user IS NULL\" ).bindparams(field_name=field, tool=tool_name) db.session.execute(query) db.session.commit() def add_tasks(fields, tool_name): \"\"\"Receives a list of fields and a tool name and adds tasks to the task table where none exist\"\"\" for field in fields: query=text( \"SELECT * FROM task WHERE field_name=:field_name AND tool_name=:tool\" ).bindparams(field_name=field, tool=tool_name) result=db.session.execute(query).all() if len(result) > 0: print(f\"A task for{tool_name},{field} already exists in the database\") continue else: task={\"tool_name\": tool_name, \"field_name\": field} db.session.execute(insert(Task), task) db.session.commit() print(f\"Added{field} task for{tool_name}\") REQUEST_LABEL=\"Toolhunt API\" USER_INFO=\"User: NicoleLBee\" headers={\"User-Agent\": f\"{REQUEST_LABEL} -{USER_INFO}\"} TOOL_API_ENDPOINT=\"https:\/\/toolhub.wikimedia.org\/api\/tools\/\" TOOL_TEST_API_ENDPOINT=\"https:\/\/toolhub-demo.wmcloud.org\/api\/tools\/\" def get_tools(): \"\"\"Getting data on all Toolhub tools\"\"\" url=f\"{TOOL_TEST_API_ENDPOINT}\" response=requests.get(url, headers=headers) if response.status_code==200: api_response=response.json() tools=api_response[\"results\"] while api_response[\"next\"]: api_response=requests.get(api_response[\"next\"], headers=headers).json() tools.extend(api_response[\"results\"]) return tools def get_single_tool(tool): \"\"\"Gets data on a single tool\"\"\" url=f\"{TOOL_TEST_API_ENDPOINT}{tool}\" response=requests.get(url, headers=headers) if response.status_code==200: api_response=response.json() return api_response def populate_db(data_set): \"\"\"Accepts a list of dicts and runs each dict through the insertion process\"\"\" for tool in data_set: check_for_entry(tool) return \"All done.\" ","sourceWithComments":"import json\n\nimport requests\nfrom sqlalchemy import insert, select, text\n\nfrom api import db\nfrom api.models import Field, Task, Tool\nfrom app import app\n\nBASE_DIR = app.config[\"BASE_DIR\"]\n\n\ndef insert_fields():\n    \"\"\"Insert data about annotations fields into the DB\"\"\"\n    with open(f\"{BASE_DIR}\/tests\/fixtures\/fields.json\") as fields:\n        field_data = json.load(fields)\n        db.session.bulk_insert_mappings(Field, field_data)\n        db.session.commit()\n\n\ndef check_for_entry(tool):\n    \"\"\"Receives a dict containing tool information and checks to see if it exists in the DB\"\"\"\n    tool_name = tool[\"name\"]\n    result = db.session.execute(select(Tool).where(Tool.name == tool_name)).all()\n    # If an entry exists move on to the deprecation check\n    if len(result) > 0:\n        print(f\"{tool_name} already exists in database\")\n    else:\n        add_tool_entry(tool)\n    check_deprecation(tool)\n\n\ndef add_tool_entry(tool):\n    \"\"\"Receives a dict containing tool information and adds an entry to the tool table\"\"\"\n    tool = {\n        \"name\": tool[\"name\"],\n        \"title\": tool[\"title\"],\n        \"description\": tool[\"description\"],\n        \"url\": tool[\"url\"],\n    }\n    db.session.execute(insert(Tool), tool)\n    db.session.commit()\n    print(f\"{tool['name']} inserted into db\")\n\n\ndef check_deprecation(tool):\n    \"\"\"Receives a dict containing tool information and checks its deprecation status.\"\"\"\n    tool_name = tool[\"name\"]\n    if tool[\"deprecated\"] is True or tool[\"annotations\"][\"deprecated\"] is True:\n        print(f\"{tool_name} is deprecated\")\n        fields = []\n        # If a tool is deprecated, we want to remove the unfinished tasks associated with it.\n        for field in tool[\"annotations\"]:\n            if field == \"replaced_by\":\n                # We don't want to remove a \"replaced_by\" task, if one exists.\n                # We want to create one, if one doesn't exist.\n                if (\n                    tool[\"annotations\"][\"replaced_by\"] is None\n                    and tool[\"replaced_by\"] is None\n                ):\n                    add_tasks([\"replaced_by\"], tool_name)\n                    continue\n            else:\n                fields.append(field)\n        remove_tasks(fields, tool_name)\n    else:\n        print(f\"{tool_name} is not deprecated.  Sorting annotations fields.\")\n        # In the unlikely event that a tool has become undeprecated...\n        remove_tasks([\"replaced_by\"], tool_name)\n        sort_fields(tool)\n\n\ndef sort_fields(tool):\n    \"\"\"Receives a tool dict and checks\/sorts the values of the core\/annotations fields\"\"\"\n    completed_fields = []\n    empty_fields = []\n    tool_name = tool[\"name\"]\n    for field in tool[\"annotations\"]:\n        # For each field in the annotations list:\n        # There are a number of fields that we're not interested in working with right now.\n        # In the future, some of them may be implemented.\n        fields_to_skip = [\n            \"replaced_by\",\n            \"deprecated\",\n            \"experimental\",\n            \"developer_docs_url\",\n            \"user_docs_url\",\n            \"feedback_url\",\n            \"privacy_policy_url\",\n            \"for_wikis\",\n            \"available_ui_languages\",\n        ]\n        if field in fields_to_skip:\n            continue\n        # A piece of information is missing only if it is absent in both the Core and Annotations layers\n        # In order to be present, it only needs to appear in one or the other location\n        # Therefore I need to check both sources.  First, does it exist in the Core?\n        elif field in tool:\n            # if it does, and it has neither a value there nor in the Annotations, add it to empty_fields\n            if (tool[field] == [] or tool[field] is None) and (\n                tool[\"annotations\"][field] == [] or tool[\"annotations\"][field] is None\n            ):\n                empty_fields.append(field)\n            # if it exists and has a value, add to completed_fields and move on to the next field\n            elif tool[field] != [] or tool[field] is not None:\n                completed_fields.append(field)\n                continue\n        # In the event that the field doesn't exist in the Core, and if it has no value in Annotations, add to empty_fields\n        elif tool[\"annotations\"][field] == [] or tool[\"annotations\"][field] is None:\n            empty_fields.append(field)\n        # And if it does have a value, add to completed_fields\n        elif tool[\"annotations\"][field] != [] or tool[\"annotations\"][field] is not None:\n            completed_fields.append(field)\n    print({\"Empty\": empty_fields, \"Completed\": completed_fields})\n    add_tasks(empty_fields, tool_name)\n    remove_tasks(completed_fields, tool_name)\n\n\ndef remove_tasks(fields, tool_name):\n    \"\"\"Receives a list of fields and a tool name and removes matching, incomplete tasks from the task table\"\"\"\n    for field in fields:\n        # We're removing only incomplete tasks\n        query = text(\n            \"DELETE FROM task WHERE field_name = :field_name AND tool_name = :tool AND user IS NULL\"\n        ).bindparams(field_name=field, tool=tool_name)\n        db.session.execute(query)\n        db.session.commit()\n        # The following print statement was firing whether a tool was deleted or not.\n        # It's nice that I can attempt to delete non-existent tasks without causing an error.\n        # Is it worth throwing in another check to see if a task exists and then deleting it?\n        # Then I could report what I was doing with more confidence.\n        # print(f\"Removed {field} task associated with {tool_name}.\")\n    # might be possible to do a bulk delete this way:\n    # db.session.execute(delete(Task).where(and_(Task.field_name.in_(fields), Task.tool_name == tool_name, Task.user.is_(None))\n\n\ndef add_tasks(fields, tool_name):\n    \"\"\"Receives a list of fields and a tool name and adds tasks to the task table where none exist\"\"\"\n    for field in fields:\n        query = text(\n            \"SELECT * FROM task WHERE field_name = :field_name AND tool_name = :tool\"\n        ).bindparams(field_name=field, tool=tool_name)\n        result = db.session.execute(query).all()\n        # If we already have a task for that, we don't want to add a new one.\n        if len(result) > 0:\n            print(f\"A task for {tool_name}, {field} already exists in the database\")\n            continue\n        # But if we don't, we do\n        # Right now this is just doing it one by one\n        # A bulk insert would be more efficient, but I'll worry about that later\n        else:\n            task = {\"tool_name\": tool_name, \"field_name\": field}\n            db.session.execute(insert(Task), task)\n            db.session.commit()\n            print(f\"Added {field} task for {tool_name}\")\n\n\nREQUEST_LABEL = \"Toolhunt API\"\nUSER_INFO = \"User: NicoleLBee\"\nheaders = {\"User-Agent\": f\"{REQUEST_LABEL} - {USER_INFO}\"}\nTOOL_API_ENDPOINT = \"https:\/\/toolhub.wikimedia.org\/api\/tools\/\"\nTOOL_TEST_API_ENDPOINT = \"https:\/\/toolhub-demo.wmcloud.org\/api\/tools\/\"\n\n\ndef get_tools():\n    \"\"\"Getting data on all Toolhub tools\"\"\"\n    url = f\"{TOOL_TEST_API_ENDPOINT}\"\n    response = requests.get(url, headers=headers)\n    if response.status_code == 200:\n        api_response = response.json()\n        tools = api_response[\"results\"]\n        while api_response[\"next\"]:\n            api_response = requests.get(api_response[\"next\"], headers=headers).json()\n            tools.extend(api_response[\"results\"])\n        return tools\n\n\ndef get_single_tool(tool):\n    \"\"\"Gets data on a single tool\"\"\"\n    url = f\"{TOOL_TEST_API_ENDPOINT}{tool}\"\n    response = requests.get(url, headers=headers)\n    if response.status_code == 200:\n        api_response = response.json()\n        return api_response\n\n\ndef populate_db(data_set):\n    \"\"\"Accepts a list of dicts and runs each dict through the insertion process\"\"\"\n    for tool in data_set:\n        check_for_entry(tool)\n    return \"All done.\"\n"},"\/manage.py":{"changes":[{"diff":"\n \n from api import db\n from api.models import Task\n+from api.utils import ToolhubClient\n from app import app\n-from jobs.populate_db import get_tools, insert_fields, populate_db\n+from jobs.populate_db import insert_fields, populate_db\n \n cli = FlaskGroup(app)\n+toolhub_client = ToolhubClient(app.config[\"TOOLHUB_API_ENDPOINT\"])\n \n \n @cli.command(\"insert_fields\")\n","add":3,"remove":1,"filename":"\/manage.py","badparts":["from jobs.populate_db import get_tools, insert_fields, populate_db"],"goodparts":["from api.utils import ToolhubClient","from jobs.populate_db import insert_fields, populate_db","toolhub_client = ToolhubClient(app.config[\"TOOLHUB_API_ENDPOINT\"])"]},{"diff":"\n @cli.command(\"populate_db_initial\")\n def run_populate_db():\n     \"\"\"Fetches and inserts tool data from Toolhub\"\"\"\n-    tool_data = get_tools()\n+    tool_data = toolhub_client.get_all()\n     populate_db(tool_data)\n \n \n","add":1,"remove":1,"filename":"\/manage.py","badparts":["    tool_data = get_tools()"],"goodparts":["    tool_data = toolhub_client.get_all()"]}],"source":"\nimport json from flask.cli import FlaskGroup from api import db from api.models import Task from app import app from jobs.populate_db import get_tools, insert_fields, populate_db cli=FlaskGroup(app) @cli.command(\"insert_fields\") def run_field_insert(): \"\"\"Inserts field data\"\"\" insert_fields() @cli.command(\"populate_db_initial\") def run_populate_db(): \"\"\"Fetches and inserts tool data from Toolhub\"\"\" tool_data=get_tools() populate_db(tool_data) @cli.command(\"populate_db_test\") def run_populate_db_test(): \"\"\"Inserts the test tool and task data into db\"\"\" BASE_DIR=app.config[\"BASE_DIR\"] with open(f\"{BASE_DIR}\/tests\/fixtures\/data.json\") as data: test_data=json.load(data) test_tool_data=test_data[0][\"tool_data\"] populate_db(test_tool_data) test_task_data=test_data[1][\"task_data\"] db.session.bulk_insert_mappings(Task, test_task_data) db.session.commit() if __name__==\"__main__\": cli() ","sourceWithComments":"import json\n\nfrom flask.cli import FlaskGroup\n\nfrom api import db\nfrom api.models import Task\nfrom app import app\nfrom jobs.populate_db import get_tools, insert_fields, populate_db\n\ncli = FlaskGroup(app)\n\n\n@cli.command(\"insert_fields\")\ndef run_field_insert():\n    \"\"\"Inserts field data\"\"\"\n    insert_fields()\n\n\n@cli.command(\"populate_db_initial\")\ndef run_populate_db():\n    \"\"\"Fetches and inserts tool data from Toolhub\"\"\"\n    tool_data = get_tools()\n    populate_db(tool_data)\n\n\n@cli.command(\"populate_db_test\")\ndef run_populate_db_test():\n    \"\"\"Inserts the test tool and task data into db\"\"\"\n    BASE_DIR = app.config[\"BASE_DIR\"]\n    with open(f\"{BASE_DIR}\/tests\/fixtures\/data.json\") as data:\n        test_data = json.load(data)\n        test_tool_data = test_data[0][\"tool_data\"]\n        populate_db(test_tool_data)\n        test_task_data = test_data[1][\"task_data\"]\n        db.session.bulk_insert_mappings(Task, test_task_data)\n        db.session.commit()\n\n\nif __name__ == \"__main__\":\n    cli()\n"}},"msg":"Improve backend utility functions (#31)\n\n* Tidy and consolidate backend utility functions\r\n\r\nMove API_URL to the config file\r\nCombine functions that communicate with Toolhub into ToolhubClient class\r\nCreate utils.py to hold ToolhubClient and other utility functions\r\nInstantiate ToolhubClient in api\/__init__.py\r\nModify imports accordingly\r\nRun files through local CI\r\n\r\n* Improve api endpoint descriptions\r\n\r\nThis addresses T330126\r\n\r\n* Make minor adjustments to docstrings\r\n\r\nThis addresses comments by blancadesal\r\n\r\n* Add basic randomizer to \/api\/task GET routes\r\n\r\nThis solves the problem of Toolhunt always returning the same set of\r\n10 tasks, though it does nothing to mitigate the possibility that\r\nmultiple users might be given the same task at a given moment.  (At\r\nrelease, though, that would be very unlikely.)\r\n\r\n* Apply CI fixes\r\n\r\n* Decouple ToolhubClient from app context\r\n\r\nThis commit addresses requested changes.\r\nI've also improved naming of the instantiations of ToolhubClient, and\r\nrearranged some imports.\r\n\r\n* Build metrics routes (#39)\r\n\r\n* Tidy and consolidate backend utility functions\r\n\r\nMove API_URL to the config file\r\nCombine functions that communicate with Toolhub into ToolhubClient class\r\nCreate utils.py to hold ToolhubClient and other utility functions\r\nInstantiate ToolhubClient in api\/__init__.py\r\nModify imports accordingly\r\nRun files through local CI\r\n\r\n* Add function to generate a date X days in the past\r\n\r\n* Add metrics schemas\r\n\r\n* Add metrics routes and take into use\r\n\r\n* Improve metrics schemas and \/api\/metrics responses\r\n\r\nThe new schema is generalized and GET requests to the \/api\/metrics\r\nendpoints will now result in an array that can be easily iterated over\r\nin order to generate tables on the dashboard.\r\n\r\n* Fix error handling for \/metrics routes\r\n\r\nThe functions will now return the correct error message in the event\r\nthat a db connection cannot be established.  (If this passes muster I\r\nwill apply it to all of the api routes.)\r\n\r\n* Commit CI fixes\r\n\r\n* Bugfix: remove instances of duplicated code\r\n\r\nSomehow managed to flub part of my manual merge when rebasing\r\n\r\n* fix: Avoid circular import issues\r\n\r\n* Makes it possible to access current_app outside of an application\r\n  context (see https:\/\/flask.palletsprojects.com\/en\/2.2.x\/appcontext\/)\r\n* With this, the app config can be accessed outside of the routes,\r\n  meaning the ToolhubClient can be instantiated outside of the\r\n  functions that use it, and then passed as an argument. The\r\n  advantages of doing things this way (\"dependency injection\") will\r\n  hopefully become more obvious once we start writing tests.\r\n\r\n* Improve error handling for PUT requests\r\n\r\n---------\r\n\r\nCo-authored-by: Slavina Stefanova <sstefanova@wikimedia.org>"}},"https:\/\/github.com\/ostinelli\/pyopenspime":{"b92b52e960581223fc6f87137697e3a419568844":{"url":"https:\/\/api.github.com\/repos\/ostinelli\/pyopenspime\/commits\/b92b52e960581223fc6f87137697e3a419568844","html_url":"https:\/\/github.com\/ostinelli\/pyopenspime\/commit\/b92b52e960581223fc6f87137697e3a419568844","message":"@lib\/pyopenspime\/client.py corrected bug on injection functions.","sha":"b92b52e960581223fc6f87137697e3a419568844","keyword":"function injection correct","diff":"diff --git a\/lib\/pyopenspime\/client.py b\/lib\/pyopenspime\/client.py\nindex aeb12b7..a3bd31d 100644\n--- a\/lib\/pyopenspime\/client.py\n+++ b\/lib\/pyopenspime\/client.py\n@@ -167,12 +167,12 @@ def inject_presence(self, stanza):\n         @rtype:   boolean\r\n         @return:  True if stanza is handled, False if not.\r\n         \"\"\"\r\n-        self.__presence_handler(self.Dispatcher, stanza)\r\n+        return self.__presence_handler(self.Dispatcher, stanza)\r\n \r\n \r\n-    def inject_iq(self, stanza):\r\n+    def inject_message(self, stanza):\r\n         \"\"\"\r\n-        Injects an <iq\/> stanza, which will be treated exactly as if it had been received by the client as an XMPP stanza coming from the server.\r\n+        Injects a <message\/> stanza, which will be treated exactly as if it had been received by the client as an XMPP stanza coming from the server.\r\n \r\n         @type  stanza: pyopenspime.xmpp.protocol.Message\r\n         @param stanza: The <message\/> stanza.\r\n@@ -180,7 +180,7 @@ def inject_iq(self, stanza):\n         @rtype:   boolean\r\n         @return:  True if stanza is handled, False if not.\r\n         \"\"\"\r\n-        self.__message_handler(self.Dispatcher, stanza)\r\n+        return self.__message_handler(self.Dispatcher, stanza)\r\n \r\n \r\n     def inject_iq(self, stanza):\r\n@@ -193,12 +193,12 @@ def inject_iq(self, stanza):\n         @rtype:   boolean\r\n         @return:  True if stanza is handled, False if not.\r\n         \"\"\"\r\n-        self.__iq_handler(self.Dispatcher, stanza)\r\n+        return self.__iq_handler(self.Dispatcher, stanza)\r\n \r\n         \r\n     def __presence_handler(self, dispatcher, stanza):\r\n-        # handles PRESENCE stanzas        \r\n-        pass\r\n+        # handles PRESENCE stanzas, not implemented - i.e. stanza not treated, return False        \r\n+        return False\r\n \r\n     \r\n     def __message_handler(self, dispatcher, stanza):\r\ndiff --git a\/lib\/pyopenspime\/ssl.py b\/lib\/pyopenspime\/ssl.py\nindex 7a29c1a..f68be9d 100644\n--- a\/lib\/pyopenspime\/ssl.py\n+++ b\/lib\/pyopenspime\/ssl.py\n@@ -208,8 +208,8 @@ def public_encrypt(self, transport):\n         @type  transport: unicode\r\n         @param transport: The <transport\/> node content to be encrypted.\r\n         \r\n-        @rtype:   tuple\r\n-        @return:  Tuple containing: (base64 encrypted transport, base64 encrypted transport-key).\r\n+        @rtype:   list\r\n+        @return:  list containing: (base64 encrypted transport, base64 encrypted transport-key).\r\n         \"\"\"\r\n         \r\n         # generate a random 32 bytes AES key\r\n@@ -232,7 +232,7 @@ def public_encrypt(self, transport):\n         # encrypt the transport key with the public RSA key of recipient\r\n         transport_key_enc = self.__rsa_public_encrypt_base64(transport_key).replace('\\r', '').replace('\\n', '')\r\n         \r\n-        # return tuple\r\n+        # return list\r\n         return [encrypted, transport_key_enc]    \r\n        \r\n     def private_decrypt(self, encrypted, transport_key_enc):\r\n","files":{"\/lib\/pyopenspime\/client.py":{"changes":[{"diff":"\n         @rtype:   boolean\r\n         @return:  True if stanza is handled, False if not.\r\n         \"\"\"\r\n-        self.__presence_handler(self.Dispatcher, stanza)\r\n+        return self.__presence_handler(self.Dispatcher, stanza)\r\n \r\n \r\n-    def inject_iq(self, stanza):\r\n+    def inject_message(self, stanza):\r\n         \"\"\"\r\n-        Injects an <iq\/> stanza, which will be treated exactly as if it had been received by the client as an XMPP stanza coming from the server.\r\n+        Injects a <message\/> stanza, which will be treated exactly as if it had been received by the client as an XMPP stanza coming from the server.\r\n \r\n         @type  stanza: pyopenspime.xmpp.protocol.Message\r\n         @param stanza: The <message\/> stanza.\r\n","add":3,"remove":3,"filename":"\/lib\/pyopenspime\/client.py","badparts":["        self.__presence_handler(self.Dispatcher, stanza)\r","    def inject_iq(self, stanza):\r","        Injects an <iq\/> stanza, which will be treated exactly as if it had been received by the client as an XMPP stanza coming from the server.\r"],"goodparts":["        return self.__presence_handler(self.Dispatcher, stanza)\r","    def inject_message(self, stanza):\r","        Injects a <message\/> stanza, which will be treated exactly as if it had been received by the client as an XMPP stanza coming from the server.\r"]},{"diff":"\n         @rtype:   boolean\r\n         @return:  True if stanza is handled, False if not.\r\n         \"\"\"\r\n-        self.__message_handler(self.Dispatcher, stanza)\r\n+        return self.__message_handler(self.Dispatcher, stanza)\r\n \r\n \r\n     def inject_iq(self, stanza):\r\n","add":1,"remove":1,"filename":"\/lib\/pyopenspime\/client.py","badparts":["        self.__message_handler(self.Dispatcher, stanza)\r"],"goodparts":["        return self.__message_handler(self.Dispatcher, stanza)\r"]},{"diff":"\n         @rtype:   boolean\r\n         @return:  True if stanza is handled, False if not.\r\n         \"\"\"\r\n-        self.__iq_handler(self.Dispatcher, stanza)\r\n+        return self.__iq_handler(self.Dispatcher, stanza)\r\n \r\n         \r\n     def __presence_handler(self, dispatcher, stanza):\r\n-        # handles PRESENCE stanzas        \r\n-        pass\r\n+        # handles PRESENCE stanzas, not implemented - i.e. stanza not treated, return False        \r\n+        return False\r\n \r\n     \r\n     def __message_handler(self, dispatcher, stanza):\r","add":3,"remove":3,"filename":"\/lib\/pyopenspime\/client.py","badparts":["        self.__iq_handler(self.Dispatcher, stanza)\r","        pass\r"],"goodparts":["        return self.__iq_handler(self.Dispatcher, stanza)\r","        return False\r"]}]},"\/lib\/pyopenspime\/ssl.py":{"changes":[{"diff":"\n         @type  transport: unicode\r\n         @param transport: The <transport\/> node content to be encrypted.\r\n         \r\n-        @rtype:   tuple\r\n-        @return:  Tuple containing: (base64 encrypted transport, base64 encrypted transport-key).\r\n+        @rtype:   list\r\n+        @return:  list containing: (base64 encrypted transport, base64 encrypted transport-key).\r\n         \"\"\"\r\n         \r\n         # generate a random 32 bytes AES key\r\n","add":2,"remove":2,"filename":"\/lib\/pyopenspime\/ssl.py","badparts":["        @rtype:   tuple\r","        @return:  Tuple containing: (base64 encrypted transport, base64 encrypted transport-key).\r"],"goodparts":["        @rtype:   list\r","        @return:  list containing: (base64 encrypted transport, base64 encrypted transport-key).\r"]}],"source":"\n \r \"\"\"SSL Module.\"\"\"\r \r import binascii, sha\r import M2Crypto.RSA, M2Crypto.BIO\r import pyopenspime.xmpp, pyopenspime.util\r \r \r class EnDec():\r \"\"\"\r Encrypter-Decrypted object.\r This object is used to encrypt, descrypt and sign OpenSpime stanzas. It includes RSA and AES support as\r defined in the OpenSpime Core Protocol v0.9.\r \"\"\"\r \r def __init__(self):\r \"\"\"\r Initialize an EnDec object.\r \"\"\"\r \r self.rsa_pub_key_path=''\r self.rsa_pub_key=None \r self.rsa_priv_key_path=''\r self.rsa_priv_key_pass=''\r self.rsa_priv_key=None\r \r def load_rsa_key_bio(self, rsa_pub_key_path, rsa_priv_key_path, rsa_priv_key_pass):\r \"\"\"\r Load public and private RSA key from.pem files.\r \r @type rsa_pub_key_path: unicode\r @param rsa_pub_key_path: The path to the RSA public key.pem file.\r @type rsa_priv_key_path: unicode\r @param rsa_priv_key_path: The path to the RSA private key.pem file.\r @type rsa_priv_key_pass: unicode\r @param rsa_priv_key_pass: The RSA private key.pem file password.\r \"\"\"\r \r self.load_rsa_pub_key(rsa_pub_key_path)\r self.load_rsa_priv_key(rsa_priv_key_path, rsa_priv_key_pass)\r \r def load_rsa_pub_key(self, rsa_pub_key_path):\r \"\"\"\r Load public RSA key from.pem file.\r \r @type rsa_pub_key_path: unicode\r @param rsa_pub_key_path: The path to the RSA public key.pem file.\r \"\"\"\r \r self.rsa_pub_key_path=rsa_pub_key_path\r self.rsa_pub_key=M2Crypto.RSA.load_pub_key(rsa_pub_key_path)\r \r def load_rsa_priv_key(self, rsa_priv_key_path, rsa_priv_key_pass): \r \"\"\"\r Load private RSA key from.pem file.\r \r @type rsa_priv_key_path: unicode\r @param rsa_priv_key_path: The path to the RSA private key.pem file.\r @type rsa_priv_key_pass: unicode\r @param rsa_priv_key_pass: The RSA private key.pem file password.\r \"\"\" \r \r self.rsa_priv_key_path=rsa_priv_key_path\r self.rsa_priv_key_pass=pyopenspime.util.to_utf8(rsa_priv_key_pass)\r self.rsa_priv_key=M2Crypto.RSA.load_key(rsa_priv_key_path, callback=self.__rsa_callback_get_passphrase)\r \r def __rsa_callback_get_passphrase(self, v):\r \r return self.rsa_priv_key_pass\r \r def __aes_encrypt_base64(self, plaintext, aes_key, aes_vint):\r \r mem=M2Crypto.BIO.MemoryBuffer()\r cf=M2Crypto.BIO.CipherStream(mem)\r cf.set_cipher('aes_256_cbc', aes_key, aes_vint, 1)\r cf.write(plaintext)\r cf.flush()\r cf.write_close()\r cf.close()\r return binascii.b2a_base64(mem.read())\r \r def __aes_decrypt_base64(self, encrypted, aes_key, aes_vint):\r \r mem=M2Crypto.BIO.MemoryBuffer(binascii.a2b_base64(encrypted))\r cf=M2Crypto.BIO.CipherStream(mem)\r cf.set_cipher('aes_256_cbc', aes_key, aes_vint, 0)\r cf.write_close()\r decrypted=cf.read()\r cf.close()\r return decrypted\r \r def __rsa_public_encrypt_base64(self, plaintext):\r \r \r s=int(( self.rsa_pub_key.__len__()) \/ 8) -11 encrypted=[]\r for i in range(0, len(plaintext), s):\r encrypted.append(self.rsa_pub_key.public_encrypt(plaintext[i:i+s], M2Crypto.RSA.pkcs1_padding))\r return binascii.b2a_base64(''.join(encrypted))\r \r def __rsa_private_decrypt_base64(self, encrypted):\r \r \r encrypted=binascii.a2b_base64(encrypted)\r s=int(self.rsa_priv_key.__len__() \/ 8)\r decrypted=[]\r for i in range(0, len(encrypted), s):\r decrypted.append(self.rsa_priv_key.private_decrypt(encrypted[i:i+s], M2Crypto.RSA.pkcs1_padding))\r return ''.join(decrypted)\r \r def __rsa_private_encrypt_base64(self, plaintext):\r \r \r s=int(( self.rsa_priv_key.__len__()) \/ 8) -11 encrypted=[]\r for i in range(0, len(plaintext), s):\r encrypted.append(self.rsa_priv_key.private_encrypt(plaintext[i:i+s], M2Crypto.RSA.pkcs1_padding))\r return binascii.b2a_base64(''.join(encrypted))\r \r def __rsa_public_decrypt_base64(self, encrypted):\r \r \r encrypted=binascii.a2b_base64(encrypted)\r s=int(self.rsa_pub_key.__len__() \/ 8)\r decrypted=[]\r for i in range(0, len(encrypted), s):\r decrypted.append(self.rsa_pub_key.public_decrypt(encrypted[i:i+s], M2Crypto.RSA.pkcs1_padding))\r return ''.join(decrypted)\r \r def private_encrypt_text(self, plaintext):\r \"\"\"\r Encrypts plaintext with the RSA private key of entity.\r \r @type plaintext: str\r @param plaintext: The string to be encrypted.\r \r @rtype: str\r @return: The base64 encoded plaintext.\r \"\"\"\r return self.__rsa_private_encrypt_base64(plaintext)\r \r def public_encrypt(self, transport):\r \"\"\"\r Encrypts the content of the transport node with public key of recipient.\r \r @type transport: unicode\r @param transport: The <transport\/> node content to be encrypted.\r \r @rtype: tuple\r @return: Tuple containing:(base64 encrypted transport, base64 encrypted transport-key).\r \"\"\"\r \r aes_key=M2Crypto.m2.rand_bytes(32)\r \r aes_vint=M2Crypto.m2.rand_bytes(16)\r \r encrypted=self.__aes_encrypt_base64(transport, aes_key, aes_vint).replace('\\r', '').replace('\\n', '')\r \r transport_key=u\"<transportkey xmlns='openspime:protocol:core:transportkey' version='0.9'> \\\r <key>%s<\/key><vint>%s<\/vint> \\\r <\/transportkey>\" %( binascii.b2a_base64(aes_key).replace('\\r', '').replace('\\n', ''), \\\r binascii.b2a_base64(aes_vint).replace('\\r', '').replace('\\n', ''))\r \r transport_key=pyopenspime.util.to_utf8(transport_key)\r \r transport_key_enc=self.__rsa_public_encrypt_base64(transport_key).replace('\\r', '').replace('\\n', '')\r \r return[encrypted, transport_key_enc] \r \r def private_decrypt(self, encrypted, transport_key_enc):\r \"\"\"\r Decrypts a string encoded with public key of recipient.\r \r @type encrypted: str\r @param encrypted: The base64 encrypted content of the <transport\/> node.\r @type transport_key_enc: str\r @param transport_key_enc: The base64 encrypted transport-key.\r \r @rtype: str\r @return: The decrypted <transport\/> node content.\r \"\"\"\r \r transport_key=self.__rsa_private_decrypt_base64(transport_key_enc)\r \r n_transport_key=pyopenspime.xmpp.simplexml.Node(node=transport_key)\r for child in n_transport_key.getChildren():\r if child.getName().strip().lower()=='key':\r aes_key=binascii.a2b_base64(child.getData())\r if child.getName().strip().lower()=='vint':\r aes_vint=binascii.a2b_base64(child.getData())\r \r return self.__aes_decrypt_base64(encrypted, aes_key, aes_vint)\r \r def private_sign(self, content):\r \"\"\"\r Returns the value of the signature of a the <transport\/> node. Reference is OpenSpime protocol v0.9.\r \r @type content: str\r @param content: The string content of the <transport\/> node.\r \r @rtype: str\r @return: The base64 encoded signature of the <transport\/> node.\r \"\"\"\r \r content_canonical=pyopenspime.util.convert_to_canonical_xml(content)\r \r s=sha.sha(content_canonical).digest()\r \r return self.__rsa_private_encrypt_base64(s).replace('\\r', '').replace('\\n', '')\r \r def public_check_sign(self, content, signature):\r \"\"\"\r Returns the value of the signature of a the <transport\/> node. Reference is OpenSpime protocol v0.9.\r \r @type content: str\r @param content: The string content of the <transport\/> node.\r @type signature: str\r @param signature: The signature.\r \r @rtype: boolean\r @return: True if signature is valid, False if it is not.\r \"\"\"\r \r content_canonical=pyopenspime.util.convert_to_canonical_xml(content)\r \r s=sha.sha(content_canonical).digest()\r \r try:\r sha_in_signature=self.__rsa_public_decrypt_base64(signature)\r except:\r return False\r \r return s==sha_in_signature\r \r \r ","sourceWithComments":"#\r\n# PyOpenSpime - SSL Module\r\n# version 0.2\r\n#\r\n#\r\n# Copyright (C) 2008, licensed under GPL v3\r\n# Roberto Ostinelli <roberto AT openspime DOT com>\r\n# Davide 'Folletto' Casali <folletto AT gmail DOT com>\r\n#\r\n#\r\n# This program is free software; you can redistribute it and\/or modify\r\n# it under the terms of the GNU General Public License v3 as published by\r\n# the Free Software Foundation.\r\n#\r\n# You should have received a copy of the GNU General Public License v3\r\n# along with this program; if not, write to the Free Software\r\n# Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.\r\n#\r\n# Permission to use, copy, modify, and distribute this software and its\r\n# documentation for any purpose with or without fee is hereby granted,\r\n# provided that the above copyright notice and this permission notice\r\n# appear in all copies.\r\n# \r\n# THIS SOFTWARE IS PROVIDED ON AN \"AS IS\" BASIS, WITHOUT WARRANTIES OR\r\n# CONDITIONS OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING, WITHOUT LIMITATION,\r\n# ANY WARRANTIES OR CONDITIONS OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY,\r\n# OR FITNESS FOR A PARTICULAR PURPOSE. IN NO EVENT SHALL WIDETAG INC OR THE\r\n# AUTHORS OF THIS SOFTWARE BE LIABLE FOR ANY CLAIM, DAMAGES, OR OTHER\r\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT, OR OTHERWISE, ARISING\r\n# FROM, OUT OF, OR IN CONNECTION WITH THE SOFTWARE OR THE IMPLEMENTATION,\r\n# DEPLOYMENT, OR OTHER USE OF THE SOFTWARE.\r\n#\r\n# IN NO EVENT AND UNDER NO LEGAL THEORY, WHETHER IN TORT (INCLUDING\r\n# NEGLIGENCE), CONTRACT, OR OTHERWISE, UNLESS REQUIRED BY APPLICABLE LAW\r\n# (SUCH AS DELIBERATE AND GROSSLY NEGLIGENT ACTS) OR AGREED TO IN WRITING,\r\n# SHALL WIDETAG INC OR ANY AUTHOR OF THIS SOFTWARE BE LIABLE FOR DAMAGES,\r\n# INCLUDING ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL\r\n# DAMAGES OF ANY CHARACTER ARISING OUT OF THE USE OR INABILITY TO USE THE\r\n# SOFTWARE (INCLUDING BUT NOT LIMITED TO DAMAGES FOR LOSS OF GOODWILL, WORK\r\n# STOPPAGE, COMPUTER FAILURE OR MALFUNCTION, OR ANY AND ALL OTHER COMMERCIAL\r\n# DAMAGES OR LOSSES), EVEN IF WIDETAG INC OR SUCH AUTHOR HAS BEEN ADVISED OF\r\n# THE POSSIBILITY OF SUCH DAMAGES.\r\n\r\n\"\"\"SSL Module.\"\"\"\r\n\r\n# imports\r\nimport binascii, sha\r\nimport M2Crypto.RSA, M2Crypto.BIO\r\nimport pyopenspime.xmpp, pyopenspime.util\r\n\r\n\r\nclass EnDec():\r\n    \"\"\"\r\n    Encrypter-Decrypted object.\r\n    This object is used to encrypt, descrypt and sign OpenSpime stanzas. It includes RSA and AES support as\r\n    defined in the OpenSpime Core Protocol v0.9.\r\n    \"\"\"\r\n    \r\n    def __init__(self):\r\n        \"\"\"\r\n        Initialize an EnDec object.\r\n        \"\"\"\r\n        \r\n        self.rsa_pub_key_path = ''\r\n        self.rsa_pub_key = None \r\n        self.rsa_priv_key_path = ''\r\n        self.rsa_priv_key_pass = ''\r\n        self.rsa_priv_key = None\r\n\r\n    def load_rsa_key_bio(self, rsa_pub_key_path, rsa_priv_key_path, rsa_priv_key_pass):\r\n        \"\"\"\r\n        Load public and private RSA key from .pem files.\r\n        \r\n        @type  rsa_pub_key_path: unicode\r\n        @param rsa_pub_key_path: The path to the RSA public key .pem file.\r\n        @type  rsa_priv_key_path: unicode\r\n        @param rsa_priv_key_path: The path to the RSA private key .pem file.\r\n        @type  rsa_priv_key_pass: unicode\r\n        @param rsa_priv_key_pass: The RSA private key .pem file password.\r\n        \"\"\"\r\n\r\n        self.load_rsa_pub_key(rsa_pub_key_path)\r\n        self.load_rsa_priv_key(rsa_priv_key_path, rsa_priv_key_pass)\r\n        \r\n    def load_rsa_pub_key(self, rsa_pub_key_path):\r\n        \"\"\"\r\n        Load public RSA key from .pem file.\r\n        \r\n        @type  rsa_pub_key_path: unicode\r\n        @param rsa_pub_key_path: The path to the RSA public key .pem file.\r\n        \"\"\"\r\n        \r\n        self.rsa_pub_key_path = rsa_pub_key_path\r\n        self.rsa_pub_key = M2Crypto.RSA.load_pub_key(rsa_pub_key_path)\r\n    \r\n    def load_rsa_priv_key(self, rsa_priv_key_path, rsa_priv_key_pass):  \r\n        \"\"\"\r\n        Load private RSA key from .pem file.\r\n        \r\n        @type  rsa_priv_key_path: unicode\r\n        @param rsa_priv_key_path: The path to the RSA private key .pem file.\r\n        @type  rsa_priv_key_pass: unicode\r\n        @param rsa_priv_key_pass: The RSA private key .pem file password.\r\n        \"\"\"      \r\n\r\n        self.rsa_priv_key_path = rsa_priv_key_path\r\n        # convert to string -> M2Crypto needs str not unicode\r\n        self.rsa_priv_key_pass = pyopenspime.util.to_utf8(rsa_priv_key_pass)\r\n        self.rsa_priv_key = M2Crypto.RSA.load_key(rsa_priv_key_path, callback=self.__rsa_callback_get_passphrase)\r\n    \r\n    def __rsa_callback_get_passphrase(self, v):\r\n        \r\n        return self.rsa_priv_key_pass\r\n    \r\n    def __aes_encrypt_base64(self, plaintext, aes_key, aes_vint):\r\n        \r\n        # AES encryption\r\n        mem = M2Crypto.BIO.MemoryBuffer()\r\n        cf = M2Crypto.BIO.CipherStream(mem)\r\n        cf.set_cipher('aes_256_cbc', aes_key, aes_vint, 1)\r\n        cf.write(plaintext)\r\n        cf.flush()\r\n        cf.write_close()\r\n        cf.close()\r\n        return binascii.b2a_base64(mem.read())\r\n    \r\n    def __aes_decrypt_base64(self, encrypted, aes_key, aes_vint):\r\n        \r\n        # AES decryption\r\n        mem = M2Crypto.BIO.MemoryBuffer(binascii.a2b_base64(encrypted))\r\n        cf = M2Crypto.BIO.CipherStream(mem)\r\n        cf.set_cipher('aes_256_cbc', aes_key, aes_vint, 0)\r\n        cf.write_close()\r\n        decrypted = cf.read()\r\n        cf.close()\r\n        return decrypted\r\n    \r\n    def __rsa_public_encrypt_base64(self, plaintext):\r\n        \r\n        # RSA public encryption\r\n        \r\n        # get pub_key size\r\n        s = int(( self.rsa_pub_key.__len__() ) \/ 8) - 11    # take away 11 bytes due to pkcs1_padding\r\n        encrypted = []\r\n        # chunk encrypt\r\n        for i in range(0, len(plaintext), s):\r\n            encrypted.append(self.rsa_pub_key.public_encrypt(plaintext[i:i+s], M2Crypto.RSA.pkcs1_padding))\r\n        # return base64 encoded\r\n        return binascii.b2a_base64(''.join(encrypted))\r\n    \r\n    def __rsa_private_decrypt_base64(self, encrypted):\r\n        \r\n        # RSA private decryption\r\n        \r\n        encrypted = binascii.a2b_base64(encrypted)\r\n        # get priv_key size\r\n        s = int(self.rsa_priv_key.__len__() \/ 8)\r\n        decrypted = []\r\n        # chunk decrypt\r\n        for i in range(0, len(encrypted), s):\r\n            decrypted.append(self.rsa_priv_key.private_decrypt(encrypted[i:i+s], M2Crypto.RSA.pkcs1_padding))\r\n        # return\r\n        return ''.join(decrypted)\r\n    \r\n    def __rsa_private_encrypt_base64(self, plaintext):\r\n        \r\n        # RSA private encryption\r\n        \r\n        # get priv_key size\r\n        s = int(( self.rsa_priv_key.__len__() ) \/ 8) - 11    # take away 11 bytes due to pkcs1_padding\r\n        encrypted = []\r\n        # chunk encrypt\r\n        for i in range(0, len(plaintext), s):\r\n            encrypted.append(self.rsa_priv_key.private_encrypt(plaintext[i:i+s], M2Crypto.RSA.pkcs1_padding))\r\n        # return base64 encoded\r\n        return binascii.b2a_base64(''.join(encrypted))\r\n    \r\n    def __rsa_public_decrypt_base64(self, encrypted):\r\n        \r\n        # RSA public decryption\r\n        \r\n        encrypted = binascii.a2b_base64(encrypted)\r\n        # get pub_key size\r\n        s = int(self.rsa_pub_key.__len__() \/ 8)\r\n        decrypted = []\r\n        # chunk decrypt\r\n        for i in range(0, len(encrypted), s):\r\n            decrypted.append(self.rsa_pub_key.public_decrypt(encrypted[i:i+s], M2Crypto.RSA.pkcs1_padding))\r\n        # return\r\n        return ''.join(decrypted)\r\n\r\n    def private_encrypt_text(self, plaintext):\r\n        \"\"\"\r\n        Encrypts plaintext with the RSA private key of entity.\r\n        \r\n        @type  plaintext: str\r\n        @param plaintext: The string to be encrypted.\r\n        \r\n        @rtype:   str\r\n        @return:  The base64 encoded plaintext.\r\n        \"\"\"\r\n        return self.__rsa_private_encrypt_base64(plaintext)\r\n    \r\n    def public_encrypt(self, transport):\r\n        \"\"\"\r\n        Encrypts the content of the transport node with public key of recipient.\r\n        \r\n        @type  transport: unicode\r\n        @param transport: The <transport\/> node content to be encrypted.\r\n        \r\n        @rtype:   tuple\r\n        @return:  Tuple containing: (base64 encrypted transport, base64 encrypted transport-key).\r\n        \"\"\"\r\n        \r\n        # generate a random 32 bytes AES key\r\n        aes_key = M2Crypto.m2.rand_bytes(32)\r\n        \r\n        # generate a random 16 bytes init vector data\r\n        aes_vint = M2Crypto.m2.rand_bytes(16)\r\n        \r\n        # encrypt in AES and encode to base64\r\n        encrypted = self.__aes_encrypt_base64(transport, aes_key, aes_vint).replace('\\r', '').replace('\\n', '')\r\n        \r\n        # generate the content of the 'transport-key' attribute\r\n        transport_key = u\"<transportkey xmlns='openspime:protocol:core:transportkey' version='0.9'> \\\r\n            <key>%s<\/key><vint>%s<\/vint> \\\r\n            <\/transportkey>\" % ( binascii.b2a_base64(aes_key).replace('\\r', '').replace('\\n', ''), \\\r\n                                 binascii.b2a_base64(aes_vint).replace('\\r', '').replace('\\n', '') )\r\n        \r\n        transport_key = pyopenspime.util.to_utf8(transport_key)\r\n        \r\n        # encrypt the transport key with the public RSA key of recipient\r\n        transport_key_enc = self.__rsa_public_encrypt_base64(transport_key).replace('\\r', '').replace('\\n', '')\r\n        \r\n        # return tuple\r\n        return [encrypted, transport_key_enc]    \r\n       \r\n    def private_decrypt(self, encrypted, transport_key_enc):\r\n        \"\"\"\r\n        Decrypts a string encoded with public key of recipient.\r\n        \r\n        @type  encrypted: str\r\n        @param encrypted: The base64 encrypted content of the <transport\/> node.\r\n        @type  transport_key_enc: str\r\n        @param transport_key_enc: The base64 encrypted transport-key.\r\n        \r\n        @rtype:   str\r\n        @return:  The decrypted <transport\/> node content.\r\n        \"\"\"\r\n        \r\n        # decrypt the transport key with the private RSA key of recipient\r\n        transport_key = self.__rsa_private_decrypt_base64(transport_key_enc)\r\n        \r\n        # read transportkey: create parser\r\n        n_transport_key = pyopenspime.xmpp.simplexml.Node(node=transport_key)\r\n        # parse\r\n        for child in n_transport_key.getChildren():\r\n            if child.getName().strip().lower() == 'key':\r\n                aes_key = binascii.a2b_base64(child.getData())\r\n            if child.getName().strip().lower() == 'vint':\r\n                aes_vint = binascii.a2b_base64(child.getData())\r\n        \r\n        # decrypt transport content\r\n        return self.__aes_decrypt_base64(encrypted, aes_key, aes_vint)\r\n    \r\n    def private_sign(self, content):\r\n        \"\"\"\r\n        Returns the value of the signature of a the <transport\/> node. Reference is OpenSpime protocol v0.9.\r\n        \r\n        @type  content: str\r\n        @param content: The string content of the <transport\/> node.\r\n        \r\n        @rtype:   str\r\n        @return:  The base64 encoded signature of the <transport\/> node.\r\n        \"\"\"\r\n        \r\n        # convert to canonical XML\r\n        content_canonical = pyopenspime.util.convert_to_canonical_xml(content)\r\n        \r\n        # compute sha\r\n        s = sha.sha(content_canonical).digest()\r\n        \r\n        # encrypt the sha using the private RSA key\r\n        return self.__rsa_private_encrypt_base64(s).replace('\\r', '').replace('\\n', '')\r\n    \r\n    def public_check_sign(self, content, signature):\r\n        \"\"\"\r\n        Returns the value of the signature of a the <transport\/> node. Reference is OpenSpime protocol v0.9.\r\n        \r\n        @type  content: str\r\n        @param content: The string content of the <transport\/> node.\r\n        @type  signature: str\r\n        @param signature: The signature.\r\n        \r\n        @rtype:   boolean\r\n        @return:  True if signature is valid, False if it is not.\r\n        \"\"\"\r\n        \r\n        # convert to canonical XML\r\n        content_canonical = pyopenspime.util.convert_to_canonical_xml(content)\r\n        \r\n        # compute sha\r\n        s = sha.sha(content_canonical).digest()\r\n        \r\n        # get the sha in the signature\r\n        try:\r\n            sha_in_signature = self.__rsa_public_decrypt_base64(signature)\r\n        except:\r\n            return False\r\n        \r\n        return s == sha_in_signature\r\n    \r\n\r\n"}},"msg":"@lib\/pyopenspime\/client.py corrected bug on injection functions."}},"https:\/\/github.com\/SAIL-Labs\/GLINTcalc":{"5dd2160cb0b698d1250dd9d4b8c4c2b7c3815992":{"url":"https:\/\/api.github.com\/repos\/SAIL-Labs\/GLINTcalc\/commits\/5dd2160cb0b698d1250dd9d4b8c4c2b7c3815992","html_url":"https:\/\/github.com\/SAIL-Labs\/GLINTcalc\/commit\/5dd2160cb0b698d1250dd9d4b8c4c2b7c3815992","message":"Make injection efficiency and delta_Phi as function of wavefront\n\n** Bugs:\n- correct the dimension of deltaphi_mu in \"get_null_vals_MC\" otherwise\nradians and microns are added together\n- in \"get_snr\", change expression of \"read_noise_tot\"\n\n** New features:\n- glintcalc.py is divided in two sections: class and function\n- add functions to get the injection and delta_Phis in function of the\nwavefront error\n\nSee docs of the function and comments in \"run_glintcalc.py\" for more\ndetails.","sha":"5dd2160cb0b698d1250dd9d4b8c4c2b7c3815992","keyword":"function injection correct","diff":"diff --git a\/glintcalc.py b\/glintcalc.py\nindex 85b0c86..1bcbcd5 100644\n--- a\/glintcalc.py\n+++ b\/glintcalc.py\n@@ -6,6 +6,9 @@\n from numpy.random import randn\n import matplotlib.pyplot as plt\n \n+# =============================================================================\n+# Class section\n+# =============================================================================\n class glintcalc:\n     def __init__(self, wavelength=1.6):\n         self.nullsamps = None\n@@ -19,8 +22,9 @@ def get_null_vals_MC(self, deltaphi_sig, deltaI_sig, deltaphi_mu=0, deltaI_mu =\n         # (See Hanot+ 2011, Norris+ 2020)\n         wavelength = self.wavelength\n         deltaphi_sig_rad = deltaphi_sig\/wavelength * 2 * np.pi\n+        deltaphi_mu_rad = deltaphi_mu\/wavelength * 2 * np.pi\n         dIsamps = randn(num_samps) * deltaI_sig + deltaI_mu\n-        dphisamps = randn(num_samps) * deltaphi_sig_rad + deltaphi_mu\n+        dphisamps = randn(num_samps) * deltaphi_sig_rad + deltaphi_mu_rad\n         self.nullsamps = 0.25 * (dIsamps**2 + dphisamps**2)\n         self.av_null = np.mean(self.nullsamps)\n \n@@ -63,7 +67,7 @@ def get_chromatic_null(self, deltaphi_sig, deltaI_sig, bandwidth, npoints = 50,\n             plt.clf()\n             plt.plot(all_wl_offsets+self.wavelength, all_nulls)\n             plt.xlabel('Wavelength (microns)')\n-            plt.ylabel('Average null_depth')\n+            plt.ylabel('Average null depth')\n             plt.tight_layout()\n \n         return chromatic_null\n@@ -76,7 +80,7 @@ def get_snr(self, photon_flux, bandwidth, contrast, null_depth, throughput=1, pu\n         # bandwidth in microns\n         # int_time in seconds\n \n-        read_noise_tot = np.sqrt(read_noise*num_pix) #TODO is this read-noise scaling right?\n+        read_noise_tot = read_noise * np.sqrt(num_pix) #TODO is this read-noise scaling right? No, read_noise should be outside the sqrt unless it stands for the variance\n \n         star_photons = photon_flux * throughput * pupil_area * bandwidth * int_time\n         print(\"Stellar photons: %.3g\" % star_photons)\n@@ -90,4 +94,95 @@ def get_snr(self, photon_flux, bandwidth, contrast, null_depth, throughput=1, pu\n         nulled_comp_snr = companion_flux \/ np.sqrt(star_photons*null_depth + read_noise_tot**2)\n         print('Nulled S\/N ratio for companion: %f' % nulled_comp_snr)\n \n-        return nulled_comp_snr\n\\ No newline at end of file\n+        return nulled_comp_snr\n+   \n+# =============================================================================\n+# Functions section\n+# =============================================================================\n+def get_noll_residuals(diam, r0, order=1):\n+    \"\"\"\n+    Get the mean square residual phase error of the wavefront after correction of the first N orders of the Zernike.\n+    Source: https:\/\/ui.adsabs.harvard.edu\/abs\/1976JOSA...66..207N\/abstract\n+    \n+    :Parameters:\n+    \n+        **diam**: diameter of the aperture\n+        \n+        **r0** : Fried's parameter at the considered wavelgnth\n+    \n+        **order** : order of the Zernike polynom up to which the correction of the wavefront is done.\n+    \n+    \n+    :Returns:\n+    \n+        mean square residual error sigma**2 of the wavefront after correction\n+    \"\"\"\n+    \n+    # Array of coefficients of residuals for the first 21 order of Zernike\n+    coeffs = np.array([1.0299, 0.582, 0.134, 0.111, 0.0880, 0.0648, 0.0587, \n+                       0.0525, 0.0463, 0.0401, 0.0377, 0.0352, 0.0328, 0.0304, \n+                       0.0279, 0.0267, 0.0255, 0.0243, 0.0232, 0.022, 0.0208])\n+    \n+    if order <= 21:\n+        return coeffs[order-1] * (diam\/r0)**(5\/3.) # In rad^2\n+    else:\n+        return 0.2944 * order**(-3**0.5\/2) * (diam\/r0)**(5\/3.) # In rad^2\n+    \n+def get_injection(diam, r0, order=1, geo_inj=0.8, wl=None, wfe=None):\n+    \"\"\"\n+    Compute the average injection into a single-mode fiber\/photonics.\n+    It uses the Marechal approximation to convert the standard deviation of the wavefront residuals into\n+    a Strehl ratio which is used to calculate the injection efficiency (https:\/\/ui.adsabs.harvard.edu\/abs\/2000A%26AS..145..305C\/abstract)\n+\n+    :Parameters:\n+    \n+        **diam** : float\n+            Diameter of the aperture.\n+        **r0** : float\n+            Friend's parameter, wavelength-dependent.\n+        **order** : int, optional\n+            Maximum order of correction of the Zernike polynom. Unused if both **wl** and **wfe** are not ``None''.\n+        **geo_inj** : float, optional\n+            Maximum injection allowed by the geometry of the pupil. The default is 0.8 for a plain pupil without spider (https:\/\/ui.adsabs.harvard.edu\/abs\/1988ApOpt..27.2334S\/abstract)\n+        **wl** : float, optional\n+            Wavelength in microns. If not ``None'' (with ``wfe''), bypass ``get_noll_residuals'' and compute the mean square of the phase residuals using **wfe**\n+        **wfe** : float, optional\n+            RMS of the wavefront error in micron. If not ``None'' (with ``wl''), bypass ``get_noll_residuals'' and compute the mean square of the phase residuals using **wfe**\n+    \n+    :Returns:\n+    \n+        Injection efficiency.\n+\n+    \"\"\"\n+    if not wl is None and not wfe is  None:\n+        residuals = (2 * np.pi \/ wl * wfe)**2\n+    else:\n+        residuals = get_noll_residuals(diam, r0, order) # Get variance of the phase residuals after correction of the first N order of Zernike.\n+    strehl = np.exp(-residuals) # Marechal's approximation\n+    \n+    return geo_inj * strehl\n+    \n+    \n+def get_diff_piston(diam, r0, wl):\n+    \"\"\"\n+    Get the RMS of the differential piston between 2 apertures.\n+    Source: https:\/\/ui.adsabs.harvard.edu\/abs\/1996ApOpt..35.3002K\/abstract\n+    It is simply Delta_1 (1st order of Noll's residuals ie the piston) summed with itself, converted into OPD and take the square root.\n+    However, it seems it is quite over-estimated.\n+    And in the case of NRM-nulling, the shift of the fringes is mainly due to the tip-tilt.\n+    #TODO: estimate the TT and deduce the corresponding shift of the fringes so that piston = baseline * angle of TT\n+    \n+    :Parameters:\n+    \n+        **diam**: diameter of the aperture\n+        \n+        **r0** : Fried's parameter at the considered wavelgnth\n+    \n+        **wl** : wavelength of observation.\n+    \n+    \n+    :Returns:\n+    \n+        Root mean square of the differential piston in the same unit as **wl**.\n+    \"\"\"\n+    return 0.228 * wl * (diam \/ r0)**(5\/6)\n\\ No newline at end of file\ndiff --git a\/run_glintcalc.py b\/run_glintcalc.py\nindex 22898c7..1c4eec1 100644\n--- a\/run_glintcalc.py\n+++ b\/run_glintcalc.py\n@@ -23,14 +23,20 @@\n # Collecting area:\n pupil_fraction = 0.5\n pupil_area = np.pi*4**2 * pupil_fraction\n+n_apertures = 4 # Number of apertures\n+pupil_diam = 2*(pupil_area\/n_apertures \/ np.pi)**0.5\n \n # Wavefront properties\n-deltaphi_sig = 0.02 # RMS wavefront across baseline in microns. #TODO - Make a function of wavefront\n+r0 = 0.795 # Fried's parameter at ``wavelength'' before AO correction\n+order_zernike = 8 # Order of Zernike  up to which wavefront correction is done\n+wfe = 0.080 # RMS of the wavefront error in microns. Stands for a Strehl of 90% at 1.6 microns.\n+deltaphi_sig = get_diff_piston(pupil_diam, r0, wavelength) # RMS wavefront across baseline in microns.\n+# deltaphi_sig = 0.02\n deltaI_sig = 0.05 # RMS difference in injection between two inputs of baseline. #TODO - Make a function of wavefront\n \n # Throughputs:\n scexao_throughput = 0.2\n-injection_efficiency = 0.2 #TODO - Make a function of wavefront\n+injection_efficiency = get_injection(pupil_diam, r0, order_zernike, wl=wavelength, wfe=wfe) # if wl and wfe are define, they are used indtead of Noll's residuals\n \n # Companion contrast\n contrast = 1e-6\n","files":{"\/glintcalc.py":{"changes":[{"diff":"\n         # (See Hanot+ 2011, Norris+ 2020)\n         wavelength = self.wavelength\n         deltaphi_sig_rad = deltaphi_sig\/wavelength * 2 * np.pi\n+        deltaphi_mu_rad = deltaphi_mu\/wavelength * 2 * np.pi\n         dIsamps = randn(num_samps) * deltaI_sig + deltaI_mu\n-        dphisamps = randn(num_samps) * deltaphi_sig_rad + deltaphi_mu\n+        dphisamps = randn(num_samps) * deltaphi_sig_rad + deltaphi_mu_rad\n         self.nullsamps = 0.25 * (dIsamps**2 + dphisamps**2)\n         self.av_null = np.mean(self.nullsamps)\n \n","add":2,"remove":1,"filename":"\/glintcalc.py","badparts":["        dphisamps = randn(num_samps) * deltaphi_sig_rad + deltaphi_mu"],"goodparts":["        deltaphi_mu_rad = deltaphi_mu\/wavelength * 2 * np.pi","        dphisamps = randn(num_samps) * deltaphi_sig_rad + deltaphi_mu_rad"]},{"diff":"\n             plt.clf()\n             plt.plot(all_wl_offsets+self.wavelength, all_nulls)\n             plt.xlabel('Wavelength (microns)')\n-            plt.ylabel('Average null_depth')\n+            plt.ylabel('Average null depth')\n             plt.tight_layout()\n \n         return chromatic_null\n","add":1,"remove":1,"filename":"\/glintcalc.py","badparts":["            plt.ylabel('Average null_depth')"],"goodparts":["            plt.ylabel('Average null depth')"]},{"diff":"\n         # bandwidth in microns\n         # int_time in seconds\n \n-        read_noise_tot = np.sqrt(read_noise*num_pix) #TODO is this read-noise scaling right?\n+        read_noise_tot = read_noise * np.sqrt(num_pix) #TODO is this read-noise scaling right? No, read_noise should be outside the sqrt unless it stands for the variance\n \n         star_photons = photon_flux * throughput * pupil_area * bandwidth * int_time\n         print(\"Stellar photons: %.3g\" % star_photons)\n","add":1,"remove":1,"filename":"\/glintcalc.py","badparts":["        read_noise_tot = np.sqrt(read_noise*num_pix) #TODO is this read-noise scaling right?"],"goodparts":["        read_noise_tot = read_noise * np.sqrt(num_pix) #TODO is this read-noise scaling right? No, read_noise should be outside the sqrt unless it stands for the variance"]},{"diff":"\n         nulled_comp_snr = companion_flux \/ np.sqrt(star_photons*null_depth + read_noise_tot**2)\n         print('Nulled S\/N ratio for companion: %f' % nulled_comp_snr)\n \n-        return nulled_comp_snr\n\\ No newline at end of file\n+        return nulled_comp_snr\n+   \n+# =============================================================================\n+# Functions section\n+# =============================================================================\n+def get_noll_residuals(diam, r0, order=1):\n+    \"\"\"\n+    Get the mean square residual phase error of the wavefront after correction of the first N orders of the Zernike.\n+    Source: https:\/\/ui.adsabs.harvard.edu\/abs\/1976JOSA...66..207N\/abstract\n+    \n+    :Parameters:\n+    \n+        **diam**: diameter of the aperture\n+        \n+        **r0** : Fried's parameter at the considered wavelgnth\n+    \n+        **order** : order of the Zernike polynom up to which the correction of the wavefront is done.\n+    \n+    \n+    :Returns:\n+    \n+        mean square residual error sigma**2 of the wavefront after correction\n+    \"\"\"\n+    \n+    # Array of coefficients of residuals for the first 21 order of Zernike\n+    coeffs = np.array([1.0299, 0.582, 0.134, 0.111, 0.0880, 0.0648, 0.0587, \n+                       0.0525, 0.0463, 0.0401, 0.0377, 0.0352, 0.0328, 0.0304, \n+                       0.0279, 0.0267, 0.0255, 0.0243, 0.0232, 0.022, 0.0208])\n+    \n+    if order <= 21:\n+        return coeffs[order-1] * (diam\/r0)**(5\/3.) # In rad^2\n+    else:\n+        return 0.2944 * order**(-3**0.5\/2) * (diam\/r0)**(5\/3.) # In rad^2\n+    \n+def get_injection(diam, r0, order=1, geo_inj=0.8, wl=None, wfe=None):\n+    \"\"\"\n+    Compute the average injection into a single-mode fiber\/photonics.\n+    It uses the Marechal approximation to convert the standard deviation of the wavefront residuals into\n+    a Strehl ratio which is used to calculate the injection efficiency (https:\/\/ui.adsabs.harvard.edu\/abs\/2000A%26AS..145..305C\/abstract)\n+\n+    :Parameters:\n+    \n+        **diam** : float\n+            Diameter of the aperture.\n+        **r0** : float\n+            Friend's parameter, wavelength-dependent.\n+        **order** : int, optional\n+            Maximum order of correction of the Zernike polynom. Unused if both **wl** and **wfe** are not ``None''.\n+        **geo_inj** : float, optional\n+            Maximum injection allowed by the geometry of the pupil. The default is 0.8 for a plain pupil without spider (https:\/\/ui.adsabs.harvard.edu\/abs\/1988ApOpt..27.2334S\/abstract)\n+        **wl** : float, optional\n+            Wavelength in microns. If not ``None'' (with ``wfe''), bypass ``get_noll_residuals'' and compute the mean square of the phase residuals using **wfe**\n+        **wfe** : float, optional\n+            RMS of the wavefront error in micron. If not ``None'' (with ``wl''), bypass ``get_noll_residuals'' and compute the mean square of the phase residuals using **wfe**\n+    \n+    :Returns:\n+    \n+        Injection efficiency.\n+\n+    \"\"\"\n+    if not wl is None and not wfe is  None:\n+        residuals = (2 * np.pi \/ wl * wfe)**2\n+    else:\n+        residuals = get_noll_residuals(diam, r0, order) # Get variance of the phase residuals after correction of the first N order of Zernike.\n+    strehl = np.exp(-residuals) # Marechal's approximation\n+    \n+    return geo_inj * strehl\n+    \n+    \n+def get_diff_piston(diam, r0, wl):\n+    \"\"\"\n+    Get the RMS of the differential piston between 2 apertures.\n+    Source: https:\/\/ui.adsabs.harvard.edu\/abs\/1996ApOpt..35.3002K\/abstract\n+    It is simply Delta_1 (1st order of Noll's residuals ie the piston) summed with itself, converted into OPD and take the square root.\n+    However, it seems it is quite over-estimated.\n+    And in the case of NRM-nulling, the shift of the fringes is mainly due to the tip-tilt.\n+    #TODO: estimate the TT and deduce the corresponding shift of the fringes so that piston = baseline * angle of TT\n+    \n+    :Parameters:\n+    \n+        **diam**: diameter of the aperture\n+        \n+        **r0** : Fried's parameter at the considered wavelgnth\n+    \n+        **wl** : wavelength of observation.\n+    \n+    \n+    :Returns:\n+    \n+        Root mean square of the differential piston in the same unit as **wl**.\n+    \"\"\"\n+    return 0.228 * wl * (diam \/ r0)**(5\/6)\n\\ No newline at end of file","add":92,"remove":1,"filename":"\/glintcalc.py","badparts":["        return nulled_comp_snr"],"goodparts":["        return nulled_comp_snr","def get_noll_residuals(diam, r0, order=1):","    \"\"\"","    Get the mean square residual phase error of the wavefront after correction of the first N orders of the Zernike.","    Source: https:\/\/ui.adsabs.harvard.edu\/abs\/1976JOSA...66..207N\/abstract","    :Parameters:","        **diam**: diameter of the aperture","        **r0** : Fried's parameter at the considered wavelgnth","        **order** : order of the Zernike polynom up to which the correction of the wavefront is done.","    :Returns:","        mean square residual error sigma**2 of the wavefront after correction","    \"\"\"","    coeffs = np.array([1.0299, 0.582, 0.134, 0.111, 0.0880, 0.0648, 0.0587, ","                       0.0525, 0.0463, 0.0401, 0.0377, 0.0352, 0.0328, 0.0304, ","                       0.0279, 0.0267, 0.0255, 0.0243, 0.0232, 0.022, 0.0208])","    if order <= 21:","        return coeffs[order-1] * (diam\/r0)**(5\/3.) # In rad^2","    else:","        return 0.2944 * order**(-3**0.5\/2) * (diam\/r0)**(5\/3.) # In rad^2","def get_injection(diam, r0, order=1, geo_inj=0.8, wl=None, wfe=None):","    \"\"\"","    Compute the average injection into a single-mode fiber\/photonics.","    It uses the Marechal approximation to convert the standard deviation of the wavefront residuals into","    a Strehl ratio which is used to calculate the injection efficiency (https:\/\/ui.adsabs.harvard.edu\/abs\/2000A%26AS..145..305C\/abstract)","    :Parameters:","        **diam** : float","            Diameter of the aperture.","        **r0** : float","            Friend's parameter, wavelength-dependent.","        **order** : int, optional","            Maximum order of correction of the Zernike polynom. Unused if both **wl** and **wfe** are not ``None''.","        **geo_inj** : float, optional","            Maximum injection allowed by the geometry of the pupil. The default is 0.8 for a plain pupil without spider (https:\/\/ui.adsabs.harvard.edu\/abs\/1988ApOpt..27.2334S\/abstract)","        **wl** : float, optional","            Wavelength in microns. If not ``None'' (with ``wfe''), bypass ``get_noll_residuals'' and compute the mean square of the phase residuals using **wfe**","        **wfe** : float, optional","            RMS of the wavefront error in micron. If not ``None'' (with ``wl''), bypass ``get_noll_residuals'' and compute the mean square of the phase residuals using **wfe**","    :Returns:","        Injection efficiency.","    \"\"\"","    if not wl is None and not wfe is  None:","        residuals = (2 * np.pi \/ wl * wfe)**2","    else:","        residuals = get_noll_residuals(diam, r0, order) # Get variance of the phase residuals after correction of the first N order of Zernike.","    strehl = np.exp(-residuals) # Marechal's approximation","    return geo_inj * strehl","def get_diff_piston(diam, r0, wl):","    \"\"\"","    Get the RMS of the differential piston between 2 apertures.","    Source: https:\/\/ui.adsabs.harvard.edu\/abs\/1996ApOpt..35.3002K\/abstract","    It is simply Delta_1 (1st order of Noll's residuals ie the piston) summed with itself, converted into OPD and take the square root.","    However, it seems it is quite over-estimated.","    And in the case of NRM-nulling, the shift of the fringes is mainly due to the tip-tilt.","    :Parameters:","        **diam**: diameter of the aperture","        **r0** : Fried's parameter at the considered wavelgnth","        **wl** : wavelength of observation.","    :Returns:","        Root mean square of the differential piston in the same unit as **wl**.","    \"\"\"","    return 0.228 * wl * (diam \/ r0)**(5\/6)"]}],"source":"\n\"\"\" Methods to use for the GLINT signal calculator Run from run_glintcalc.py \"\"\" import numpy as np from numpy.random import randn import matplotlib.pyplot as plt class glintcalc: def __init__(self, wavelength=1.6): self.nullsamps=None self.wavelength=wavelength def get_null_vals_MC(self, deltaphi_sig, deltaI_sig, deltaphi_mu=0, deltaI_mu=0, num_samps=100000, show_plot=False, hist_bins=100): wavelength=self.wavelength deltaphi_sig_rad=deltaphi_sig\/wavelength * 2 * np.pi dIsamps=randn(num_samps) * deltaI_sig +deltaI_mu dphisamps=randn(num_samps) * deltaphi_sig_rad +deltaphi_mu self.nullsamps=0.25 *(dIsamps**2 +dphisamps**2) self.av_null=np.mean(self.nullsamps) if show_plot: plt.figure(1) plt.clf() plt.hist(self.nullsamps, hist_bins, density=True) plt.xlabel('Null depth') plt.ylabel('Frequency') return self.av_null def plot_null_dphi(self, deltaI_sig, max_dphi=None, npoints=100): if max_dphi is None: max_dphi=self.wavelength\/10 dphis=np.linspace(0, max_dphi, npoints) all_nulls=np.zeros(npoints) for k in range(npoints): all_nulls[k]=self.get_null_vals_MC(dphis[k], deltaI_sig) plt.figure(2) plt.clf() plt.plot(dphis,all_nulls) plt.xlabel('dphi sigma(microns)') plt.ylabel('Average null') def get_chromatic_null(self, deltaphi_sig, deltaI_sig, bandwidth, npoints=50, show_plot=False): all_wl_offsets=np.linspace(-bandwidth\/2, bandwidth\/2, npoints) all_nulls=np.zeros(npoints) for k in range(npoints): all_nulls[k]=self.get_null_vals_MC(deltaphi_sig, deltaI_sig, deltaphi_mu=all_wl_offsets[k], num_samps=1000000) chromatic_null=np.mean(all_nulls) if show_plot: plt.figure(3) plt.clf() plt.plot(all_wl_offsets+self.wavelength, all_nulls) plt.xlabel('Wavelength(microns)') plt.ylabel('Average null_depth') plt.tight_layout() return chromatic_null def get_snr(self, photon_flux, bandwidth, contrast, null_depth, throughput=1, pupil_area=50, int_time=1, read_noise=1, QE=1, num_pix=1): read_noise_tot=np.sqrt(read_noise*num_pix) star_photons=photon_flux * throughput * pupil_area * bandwidth * int_time print(\"Stellar photons: %.3g\" % star_photons) star_snr=(star_photons*QE) \/ read_noise_tot print('S\/N ratio for star measurement: %f' % star_snr) companion_flux=star_photons * contrast raw_comp_snr=companion_flux \/ np.sqrt(star_photons +read_noise_tot**2) print('No-nulling S\/N ratio for companion: %f' % raw_comp_snr) nulled_comp_snr=companion_flux \/ np.sqrt(star_photons*null_depth +read_noise_tot**2) print('Nulled S\/N ratio for companion: %f' % nulled_comp_snr) return nulled_comp_snr ","sourceWithComments":"\"\"\"\nMethods to use for the GLINT signal calculator\nRun from run_glintcalc.py\n\"\"\"\nimport numpy as np\nfrom numpy.random import randn\nimport matplotlib.pyplot as plt\n\nclass glintcalc:\n    def __init__(self, wavelength=1.6):\n        self.nullsamps = None\n        self.wavelength = wavelength\n\n\n    def get_null_vals_MC(self, deltaphi_sig, deltaI_sig, deltaphi_mu=0, deltaI_mu = 0,\n                         num_samps=100000, show_plot=False, hist_bins=100):\n        # Let total flux I1+I2 = 1\n        # Assume N+ >> N-, deltaphi <<1 and deltaI << 1, so can approximate:\n        # (See Hanot+ 2011, Norris+ 2020)\n        wavelength = self.wavelength\n        deltaphi_sig_rad = deltaphi_sig\/wavelength * 2 * np.pi\n        dIsamps = randn(num_samps) * deltaI_sig + deltaI_mu\n        dphisamps = randn(num_samps) * deltaphi_sig_rad + deltaphi_mu\n        self.nullsamps = 0.25 * (dIsamps**2 + dphisamps**2)\n        self.av_null = np.mean(self.nullsamps)\n\n        if show_plot:\n            plt.figure(1)\n            plt.clf()\n            plt.hist(self.nullsamps, hist_bins, density=True)\n            plt.xlabel('Null depth')\n            plt.ylabel('Frequency')\n\n        return self.av_null\n\n\n    def plot_null_dphi(self, deltaI_sig, max_dphi=None, npoints=100):\n        if max_dphi is None:\n            max_dphi = self.wavelength\/10\n        dphis = np.linspace(0, max_dphi, npoints)\n        all_nulls = np.zeros(npoints)\n        for k in range(npoints):\n            all_nulls[k] = self.get_null_vals_MC(dphis[k], deltaI_sig)\n        plt.figure(2)\n        plt.clf()\n        plt.plot(dphis,all_nulls)\n        plt.xlabel('dphi sigma (microns)')\n        plt.ylabel('Average null')\n\n\n    def get_chromatic_null(self, deltaphi_sig, deltaI_sig, bandwidth, npoints = 50, show_plot=False):\n        # Make the assumption that null is purely chromatic, i.e. behaves like free space optics\n        all_wl_offsets = np.linspace(-bandwidth\/2, bandwidth\/2, npoints)\n\n        all_nulls = np.zeros(npoints)\n        for k in range(npoints):\n            all_nulls[k] = self.get_null_vals_MC(deltaphi_sig, deltaI_sig, deltaphi_mu=all_wl_offsets[k],\n                                                 num_samps=1000000)\n        chromatic_null = np.mean(all_nulls)\n\n        if show_plot:\n            plt.figure(3)\n            plt.clf()\n            plt.plot(all_wl_offsets+self.wavelength, all_nulls)\n            plt.xlabel('Wavelength (microns)')\n            plt.ylabel('Average null_depth')\n            plt.tight_layout()\n\n        return chromatic_null\n\n\n    def get_snr(self, photon_flux, bandwidth, contrast, null_depth, throughput=1, pupil_area=50,\n                             int_time=1, read_noise=1, QE=1, num_pix=1):\n        # photon_flux is in ph\/um\/s\/m^2\n        # pupil_area in m^2\n        # bandwidth in microns\n        # int_time in seconds\n\n        read_noise_tot = np.sqrt(read_noise*num_pix) #TODO is this read-noise scaling right?\n\n        star_photons = photon_flux * throughput * pupil_area * bandwidth * int_time\n        print(\"Stellar photons: %.3g\" % star_photons)\n        star_snr = (star_photons*QE) \/ read_noise_tot\n        print('S\/N ratio for star measurement: %f' % star_snr)\n\n        companion_flux = star_photons * contrast\n        raw_comp_snr = companion_flux \/ np.sqrt(star_photons + read_noise_tot**2)\n        print('No-nulling S\/N ratio for companion: %f' % raw_comp_snr)\n\n        nulled_comp_snr = companion_flux \/ np.sqrt(star_photons*null_depth + read_noise_tot**2)\n        print('Nulled S\/N ratio for companion: %f' % nulled_comp_snr)\n\n        return nulled_comp_snr"},"\/run_glintcalc.py":{"changes":[{"diff":"\n # Collecting area:\n pupil_fraction = 0.5\n pupil_area = np.pi*4**2 * pupil_fraction\n+n_apertures = 4 # Number of apertures\n+pupil_diam = 2*(pupil_area\/n_apertures \/ np.pi)**0.5\n \n # Wavefront properties\n-deltaphi_sig = 0.02 # RMS wavefront across baseline in microns. #TODO - Make a function of wavefront\n+r0 = 0.795 # Fried's parameter at ``wavelength'' before AO correction\n+order_zernike = 8 # Order of Zernike  up to which wavefront correction is done\n+wfe = 0.080 # RMS of the wavefront error in microns. Stands for a Strehl of 90% at 1.6 microns.\n+deltaphi_sig = get_diff_piston(pupil_diam, r0, wavelength) # RMS wavefront across baseline in microns.\n+# deltaphi_sig = 0.02\n deltaI_sig = 0.05 # RMS difference in injection between two inputs of baseline. #TODO - Make a function of wavefront\n \n # Throughputs:\n scexao_throughput = 0.2\n-injection_efficiency = 0.2 #TODO - Make a function of wavefront\n+injection_efficiency = get_injection(pupil_diam, r0, order_zernike, wl=wavelength, wfe=wfe) # if wl and wfe are define, they are used indtead of Noll's residuals\n \n # Companion contrast\n contrast = 1e-6\n","add":8,"remove":2,"filename":"\/run_glintcalc.py","badparts":["deltaphi_sig = 0.02 # RMS wavefront across baseline in microns. #TODO - Make a function of wavefront","injection_efficiency = 0.2 #TODO - Make a function of wavefront"],"goodparts":["n_apertures = 4 # Number of apertures","pupil_diam = 2*(pupil_area\/n_apertures \/ np.pi)**0.5","r0 = 0.795 # Fried's parameter at ``wavelength'' before AO correction","order_zernike = 8 # Order of Zernike  up to which wavefront correction is done","wfe = 0.080 # RMS of the wavefront error in microns. Stands for a Strehl of 90% at 1.6 microns.","deltaphi_sig = get_diff_piston(pupil_diam, r0, wavelength) # RMS wavefront across baseline in microns.","injection_efficiency = get_injection(pupil_diam, r0, order_zernike, wl=wavelength, wfe=wfe) # if wl and wfe are define, they are used indtead of Noll's residuals"]}],"source":"\n\"\"\" Methods to use for the GLINT signal calculator Run from run_glintcalc.py \"\"\" import numpy as np from numpy.random import randn import matplotlib.pyplot as plt class glintcalc: def __init__(self, wavelength=1.6): self.nullsamps=None self.wavelength=wavelength def get_null_vals_MC(self, deltaphi_sig, deltaI_sig, deltaphi_mu=0, deltaI_mu=0, num_samps=100000, show_plot=False, hist_bins=100): wavelength=self.wavelength deltaphi_sig_rad=deltaphi_sig\/wavelength * 2 * np.pi dIsamps=randn(num_samps) * deltaI_sig +deltaI_mu dphisamps=randn(num_samps) * deltaphi_sig_rad +deltaphi_mu self.nullsamps=0.25 *(dIsamps**2 +dphisamps**2) self.av_null=np.mean(self.nullsamps) if show_plot: plt.figure(1) plt.clf() plt.hist(self.nullsamps, hist_bins, density=True) plt.xlabel('Null depth') plt.ylabel('Frequency') return self.av_null def plot_null_dphi(self, deltaI_sig, max_dphi=None, npoints=100): if max_dphi is None: max_dphi=self.wavelength\/10 dphis=np.linspace(0, max_dphi, npoints) all_nulls=np.zeros(npoints) for k in range(npoints): all_nulls[k]=self.get_null_vals_MC(dphis[k], deltaI_sig) plt.figure(2) plt.clf() plt.plot(dphis,all_nulls) plt.xlabel('dphi sigma(microns)') plt.ylabel('Average null') def get_chromatic_null(self, deltaphi_sig, deltaI_sig, bandwidth, npoints=50, show_plot=False): all_wl_offsets=np.linspace(-bandwidth\/2, bandwidth\/2, npoints) all_nulls=np.zeros(npoints) for k in range(npoints): all_nulls[k]=self.get_null_vals_MC(deltaphi_sig, deltaI_sig, deltaphi_mu=all_wl_offsets[k], num_samps=1000000) chromatic_null=np.mean(all_nulls) if show_plot: plt.figure(3) plt.clf() plt.plot(all_wl_offsets+self.wavelength, all_nulls) plt.xlabel('Wavelength(microns)') plt.ylabel('Average null_depth') plt.tight_layout() return chromatic_null def get_snr(self, photon_flux, bandwidth, contrast, null_depth, throughput=1, pupil_area=50, int_time=1, read_noise=1, QE=1, num_pix=1): read_noise_tot=np.sqrt(read_noise*num_pix) star_photons=photon_flux * throughput * pupil_area * bandwidth * int_time print(\"Stellar photons: %.3g\" % star_photons) star_snr=(star_photons*QE) \/ read_noise_tot print('S\/N ratio for star measurement: %f' % star_snr) companion_flux=star_photons * contrast raw_comp_snr=companion_flux \/ np.sqrt(star_photons +read_noise_tot**2) print('No-nulling S\/N ratio for companion: %f' % raw_comp_snr) nulled_comp_snr=companion_flux \/ np.sqrt(star_photons*null_depth +read_noise_tot**2) print('Nulled S\/N ratio for companion: %f' % nulled_comp_snr) return nulled_comp_snr ","sourceWithComments":"\"\"\"\nMethods to use for the GLINT signal calculator\nRun from run_glintcalc.py\n\"\"\"\nimport numpy as np\nfrom numpy.random import randn\nimport matplotlib.pyplot as plt\n\nclass glintcalc:\n    def __init__(self, wavelength=1.6):\n        self.nullsamps = None\n        self.wavelength = wavelength\n\n\n    def get_null_vals_MC(self, deltaphi_sig, deltaI_sig, deltaphi_mu=0, deltaI_mu = 0,\n                         num_samps=100000, show_plot=False, hist_bins=100):\n        # Let total flux I1+I2 = 1\n        # Assume N+ >> N-, deltaphi <<1 and deltaI << 1, so can approximate:\n        # (See Hanot+ 2011, Norris+ 2020)\n        wavelength = self.wavelength\n        deltaphi_sig_rad = deltaphi_sig\/wavelength * 2 * np.pi\n        dIsamps = randn(num_samps) * deltaI_sig + deltaI_mu\n        dphisamps = randn(num_samps) * deltaphi_sig_rad + deltaphi_mu\n        self.nullsamps = 0.25 * (dIsamps**2 + dphisamps**2)\n        self.av_null = np.mean(self.nullsamps)\n\n        if show_plot:\n            plt.figure(1)\n            plt.clf()\n            plt.hist(self.nullsamps, hist_bins, density=True)\n            plt.xlabel('Null depth')\n            plt.ylabel('Frequency')\n\n        return self.av_null\n\n\n    def plot_null_dphi(self, deltaI_sig, max_dphi=None, npoints=100):\n        if max_dphi is None:\n            max_dphi = self.wavelength\/10\n        dphis = np.linspace(0, max_dphi, npoints)\n        all_nulls = np.zeros(npoints)\n        for k in range(npoints):\n            all_nulls[k] = self.get_null_vals_MC(dphis[k], deltaI_sig)\n        plt.figure(2)\n        plt.clf()\n        plt.plot(dphis,all_nulls)\n        plt.xlabel('dphi sigma (microns)')\n        plt.ylabel('Average null')\n\n\n    def get_chromatic_null(self, deltaphi_sig, deltaI_sig, bandwidth, npoints = 50, show_plot=False):\n        # Make the assumption that null is purely chromatic, i.e. behaves like free space optics\n        all_wl_offsets = np.linspace(-bandwidth\/2, bandwidth\/2, npoints)\n\n        all_nulls = np.zeros(npoints)\n        for k in range(npoints):\n            all_nulls[k] = self.get_null_vals_MC(deltaphi_sig, deltaI_sig, deltaphi_mu=all_wl_offsets[k],\n                                                 num_samps=1000000)\n        chromatic_null = np.mean(all_nulls)\n\n        if show_plot:\n            plt.figure(3)\n            plt.clf()\n            plt.plot(all_wl_offsets+self.wavelength, all_nulls)\n            plt.xlabel('Wavelength (microns)')\n            plt.ylabel('Average null_depth')\n            plt.tight_layout()\n\n        return chromatic_null\n\n\n    def get_snr(self, photon_flux, bandwidth, contrast, null_depth, throughput=1, pupil_area=50,\n                             int_time=1, read_noise=1, QE=1, num_pix=1):\n        # photon_flux is in ph\/um\/s\/m^2\n        # pupil_area in m^2\n        # bandwidth in microns\n        # int_time in seconds\n\n        read_noise_tot = np.sqrt(read_noise*num_pix) #TODO is this read-noise scaling right?\n\n        star_photons = photon_flux * throughput * pupil_area * bandwidth * int_time\n        print(\"Stellar photons: %.3g\" % star_photons)\n        star_snr = (star_photons*QE) \/ read_noise_tot\n        print('S\/N ratio for star measurement: %f' % star_snr)\n\n        companion_flux = star_photons * contrast\n        raw_comp_snr = companion_flux \/ np.sqrt(star_photons + read_noise_tot**2)\n        print('No-nulling S\/N ratio for companion: %f' % raw_comp_snr)\n\n        nulled_comp_snr = companion_flux \/ np.sqrt(star_photons*null_depth + read_noise_tot**2)\n        print('Nulled S\/N ratio for companion: %f' % nulled_comp_snr)\n\n        return nulled_comp_snr"}},"msg":"Make injection efficiency and delta_Phi as function of wavefront\n\n** Bugs:\n- correct the dimension of deltaphi_mu in \"get_null_vals_MC\" otherwise\nradians and microns are added together\n- in \"get_snr\", change expression of \"read_noise_tot\"\n\n** New features:\n- glintcalc.py is divided in two sections: class and function\n- add functions to get the injection and delta_Phis in function of the\nwavefront error\n\nSee docs of the function and comments in \"run_glintcalc.py\" for more\ndetails."}},"https:\/\/github.com\/Alexei95\/enpheeph":{"ec61c4e19c172c801617d420aa8f3514ec00020c":{"url":"https:\/\/api.github.com\/repos\/Alexei95\/enpheeph\/commits\/ec61c4e19c172c801617d420aa8f3514ec00020c","html_url":"https:\/\/github.com\/Alexei95\/enpheeph\/commit\/ec61c4e19c172c801617d420aa8f3514ec00020c","message":"Test initial weight injection and fix related bugs\n\nWrite basic PyTorchLightning wrapper.\n\nAdd Endianness for bit indices, to support 0 as MSB and 0 as LSB.\n\nConvert functional enum declarations into class declarations.\n\nImprove typing hints for BaseFaultDescriptor.\n\nAdd argument checks in post_init of BaseFaultDescriptor.\n\nAdd static method to convert generic tensor index to valid tensor slice.\n\nInclude DispatcherABC into BaseInjectionCallback.\n\nImplement ModuleUpdater to automatically update modules.\n\nSubstitute un-mantainable calls with class methods.\n\nUse dispatching instead of dict-mapping for fault initialization.\n\nAdd flag to automate fault initialization at test start.\n\nChange weight injection to use new function from fiutils.\n\nBasic implementation of activation injection, not tested.\n\nIncrease redundancy of injection methods by deepcopying inputs\/outputs.\n\nFix bug by changing string injection into list, and converting back.\n\nImplement method for automating gathering the correct\nelements from a tensor and inject the fault.","sha":"ec61c4e19c172c801617d420aa8f3514ec00020c","keyword":"function injection correct","diff":"diff --git a\/src\/fi\/basefaultdescriptor.py b\/src\/fi\/basefaultdescriptor.py\nindex 5b5903a..c1ac445 100644\n--- a\/src\/fi\/basefaultdescriptor.py\n+++ b\/src\/fi\/basefaultdescriptor.py\n@@ -1,38 +1,135 @@\n+import collections.abc\n import dataclasses\n import enum\n import typing\n \n \n-ParameterType = enum.Enum('ParameterType', 'Weight Activation SparseWeightCOOIndex SparseWeightCOOValue SparseActivationCOOIndex SparseActivationCOOValue', module=__name__)\n-BitValue = enum.Enum('BitValue', 'Random Zero One BitFlip', module=__name__)\n+class Endianness(enum.Flag):\n+    Little = enum.auto()\n+    Big = enum.auto()\n+\n+    MSBAtIndexZero = Big\n+    LSBAtIndexZero = Little\n+\n+\n+class ParameterType(enum.Flag):\n+    Weight = enum.auto()\n+    Activation = enum.auto()\n+    Sparse = enum.auto()\n+    COO = enum.auto()\n+    Index = enum.auto()\n+    Value = enum.auto()\n+    SparseWeightCOOIndex = Sparse | Weight | COO | Index\n+    SparseWeightCOOValue = Sparse | Weight | COO | Value\n+    SparseActivationCOOIndex = Sparse | Activation | COO | Index\n+    SparseActivationCOOValue = Sparse | Activation | COO | Value\n+\n+\n+class BitValue(enum.Enum):\n+    Random = enum.auto()\n+    StuckAtZero = enum.auto()\n+    StuckAtOne = enum.auto()\n+    BitFlip = enum.auto()\n \n \n # this is a container for the module name and the index for where the fault\n # should be injected\n # each fault descriptor covers a single bit-flip (or stuck-at)\n-@dataclasses.dataclass(init=True)\n+# we need unsafe hash for using it as a dictonary key\n+@dataclasses.dataclass(init=True, repr=True, unsafe_hash=True)\n class BaseFaultDescriptor(object):\n     # name of the module to inject\n     module_name: str\n-    # name of the parameter, if the module is a base module (conv, fc),\n-    # it generally coincides with 'weight' for weight injection\n-    # not required for activation injection\n-    parameter_name: str = None\n     # type of parameter to inject, weight, activation, ...\n     parameter_type: ParameterType\n-    # index of the tensor to be injected\n-    tensor_index: tuple[typing.Union[int, slice], ...]\n+    # index of the tensor to be injected, it will be converted to a tuple\n+    tensor_index: typing.Union[\n+            type(Ellipsis),\n+            typing.Sequence[typing.Union[int, slice, type(Ellipsis)]]\n+    ]\n     # index of the bit to be injected\n     # even if it is a slice, it must be converted to indices\n     # just use tuple(range(*slice.indices(slice.stop)))\n-    bit_index: typing.Tuple[int, ...]\n+    bit_index: typing.Union[typing.Sequence[int], slice]\n     # type of bit injection to be carried out\n     bit_value: BitValue\n+    # way to interpret the binary representation, as big endian or little\n+    # endian\n+    # big endian means the bit index 0 is mapped to the MSB of the binary\n+    # little endian means the bit index 0 is instead mapped to the LSB\n+    # by default we use little endian, as this is the most common\n+    # representation\n+    endianness: Endianness = Endianness.Little\n+    # name of the parameter, if the module is a base module (conv, fc),\n+    # it generally coincides with 'weight' for weight injection\n+    # not required for activation injection\n+    parameter_name: str = None\n \n     def __post_init__(self):\n         # FIXME: here there should be some checks on the values\n-        pass\n+\n+        # we raise error if parameter_name is not set and we are not doing\n+        # activation injection\n+        activation_flag = self.parameter_type.Activation in self.parameter_type\n+        if self.parameter_name is None and not activation_flag:\n+            raise ValueError('Please provide a parameter_name from which '\n+                    'to gather the values of the tensor to be injected')\n+\n+        # we convert the tensor index to a tuple, otherwise the descriptor\n+        # remains unhashable\n+        # first we have to check if it is a MutableSequence\n+        if isinstance(self.tensor_index, collections.abc.MutableSequence):\n+            self.tensor_index = tuple(self.tensor_index)\n+\n+        # if the bit index is a slice, we convert it to indices\n+        if isinstance(self.bit_index, slice):\n+            self.bit_index = self.bit_index_from_slice(self.bit_index)\n \n     @staticmethod\n-    def bit_index_from_slice(self, slice_: slice):\n+    def bit_index_from_slice(slice_: slice) -> typing.Tuple[int, ...]:\n+        # we use slice.indices to get back the indices of the slice\n+        # we must pass a stop as it requires a stop value, taken from the slice\n+        # itself\n+        # in this way we can return all of the affected indices in a tuple\n         return tuple(range(*slice_.indices(slice_.stop)))\n+\n+    @staticmethod\n+    def to_tensor_slice(\n+                tensor_index: typing.Union[\n+                        type(Ellipsis),\n+                        typing.Sequence[\n+                                typing.Union[\n+                                        int, slice, type(Ellipsis)]]],\n+                tensor_shape: typing.Sequence[int],\n+            ) -> typing.Sequence[typing.Union[int, slice]]:\n+        # if we have a single ellipsis, without any container, we have a slice\n+        # covering the whole tensor shape\n+        if isinstance(tensor_index, type(Ellipsis)):\n+            return tuple(slice(0, dim_range) for dim_range in tensor_shape)\n+        # we check if the number of elements in the index and in the number of\n+        # dimensions is the same, otherwise we raise ValueError\n+        # this check exists as zip goes over the shortest one\n+        if len(tensor_index) != len(tensor_shape):\n+            raise ValueError(\n+                    \"Number of elements in the index must be the \"\n+                    \"same as the number of dimensions in the tensor\"\n+            )\n+        new_tensor_index = []\n+        for index, dim_range in zip(tensor_index, tensor_shape):\n+            # if the current index is a slice\n+            # then we limit the ending of the slice to the maximum range\n+            # of the current dimension\n+            if isinstance(index, slice):\n+                new_index = index.indices(dim_range - 1)\n+            # if we get an Ellipsis, then we set it up as a slice from 0 to\n+            # max dimension range\n+            elif isinstance(index, type(Ellipsis)):\n+                new_index = slice(0, dim_range - 1)\n+            # if it is int we copy it\n+            elif isinstance(index, int):\n+                new_index = index\n+            # as fallback we raise ValueError\n+            else:\n+                raise ValueError('Wrong index value, use slice, int or ...')\n+            new_tensor_index.append(new_index)\n+        return tuple(new_tensor_index)\ndiff --git a\/src\/fi\/baseinjectioncallback.py b\/src\/fi\/baseinjectioncallback.py\nindex 07bef3f..bf6ddfa 100644\n--- a\/src\/fi\/baseinjectioncallback.py\n+++ b\/src\/fi\/baseinjectioncallback.py\n@@ -5,132 +5,151 @@\n import pytorch_lightning\n import torch\n \n-from . import basefaultdescriptor\n-from . import fiutils\n-\n-\n-def init_weight(fault: basefaultdescriptor.BaseFaultDescriptor,\n-                module: torch.nn.Module) -> torch.nn.Module:\n-    # we get the weights\n-    weights = getattr(module, fault.parameter_name)\n-    # we get the tensor value to be injected\n-    original_tensor = weights[fault.tensor_index]\n-    # to inject the values, we need to flatten the tensor\n-    flattened_tensor = original_tensor.flatten()\n-    # then we need to process them one by one\n-    injected_flattened_bit_tensor = []\n-    for element in flattened_tensor:\n-        injected_flattened_bit_tensor.append(fiutils.inject_fault_pytorch(element, fault))\n-    # we create a list with the injected data, converting back to tensors\n-    injected_flattened_tensor_list = []\n-    for injected_binary, original_binary in zip(injected_flattened_bit_tensor, flattened_bit_tensor):\n-        injected_flattened_tensor_list.append(fiutils.binary_to_pytorch_element(injected_binary, original_binary))\n-    # we create a tensor from the list, moving it to the same device as the original one\n-    injected_flattened_tensor = torch.Tensor(injected_flattened_tensor_list).to(flattened_tensor)\n-    # we reshape the tensor to the original one\n-    injected_tensor = injected_flattened_tensor.reshape(original_tensor)\n-    # we update the weights to the new value\n-    weights[fault.tensor_index] = injected_tensor\n-    # we set the weights to the updated value\n-    setattr(module, fault.parameter_name, weights)\n-    # we return the new module\n-    return module\n-\n-\n-def init_activation(fault: basefaultdescriptor.BaseFaultDescriptor,\n-                    module: torch.nn.Module) -> torch.nn.Module:\n-    pass\n-\n-\n-# this can be improved with a getter class, implementing all of these and then\n-# providing an interface for checking the association\n-PARAMETER_TYPE_MAPPING = {basefaultdescriptor.ParameterType.Weight: init_weight,\n-                          basefaultdescriptor.ParameterType.Activation: init_activation,}\n+import src.dispatcherabc\n+import src.fi.basefaultdescriptor\n+import src.fi.fiutils\n+import src.fi.moduleupdater\n \n \n+# we integrate both the basic injection callback together with the dispatcher\n @dataclasses.dataclass(init=True)\n-class BaseInjectionCallback(pytorch_lightning.Callback):\n-    fault_descriptor_list: typing.List[basefaultdescriptor.BaseFaultDescriptor, ...] = \\\n-        dataclasses.field(default_factory=[])\n+class BaseInjectionCallback(\n+        pytorch_lightning.Callback,\n+        src.dispatcherabc.DispatcherABC,\n+        src.fi.moduleupdater.ModuleUpdater):\n+\n+    # list of all faults to be injected\n+    fault_descriptor_list: typing.Sequence[\n+            src.fi.basefaultdescriptor.BaseFaultDescriptor\n+            ] = dataclasses.field(default_factory=[])\n+    # enable\/disable the injection, can be changed with the functions\n     enabled: bool = True\n+    # enable the automatic init of the top module during each on_test_start\n+    auto_model_init_on_test_start: bool = True\n+    # internal flag for checking whether we have set up the modules for\n+    # injection\n     _active: bool = dataclasses.field(init=False, default=False)\n-    _modules: typing.Dict[str, torch.nn.Module] = dataclasses.field(init=False,\n-                                                        default_factory=dict)\n-    _modules_backup: typing.Dict[str, torch.nn.Module] = dataclasses.field(init=False,\n-                                                        default_factory=dict)\n+    # dict of injected modules to be loaded in the main model\n+    _modules: typing.Dict[str, torch.nn.Module] = dataclasses.field(\n+            init=False,\n+            default_factory=dict)\n+    # dict containing the original modules, before substituting them for\n+    # the injection\n+    _modules_backup: typing.Dict[str, torch.nn.Module] = dataclasses.field(\n+            init=False,\n+            default_factory=dict)\n \n     def __post_init__(self):\n         # we check all the faults are mapped\n-        for fault in self.fault_descriptor_list:\n-            assert PARAMETER_TYPE_MAPPING.get(fault.parameter_type, None) is not None\n+        dispatching_dict = self.get_dispatching_dict()\n+        assert all(fault.parameter_type in dispatching_dict\n+                   for fault in self.fault_descriptor_list), (\n+                    'Please map all the faults to a fault initializer')\n \n     # this function activates the injection, by substituting all the modules\n     # with the custom one, following the list\n     def on_test_start(self, trainer, pl_module):\n         # if we are enabled for injecting and nothing else is running already\n         if self.enabled and not self._active:\n-            # we need to cycle through all the modules\n-            # all of them are already backed up, so we simply have to\n-            # substitute them\n-            for module_name, module in self._modules.items():\n-                # we reach the final module whose attribute must be updated\n-                # by going through the tree until the last-but-one attribute\n-                dest_module = pl_module\n-                for submodule in module_name.split('.')[:-1]:\n-                    dest_module = getattr(dest_module, submodule)\n-\n-                # the last attribute is used to set the module to the injection\n-                # one\n-                setattr(dest_module, module_name.split('.')[-1],\n-                        module)\n+            # if we are set for auto-init, then we set up the model\n+            if self.auto_model_init_on_test_start:\n+                self.init_model(pl_module)\n+            # we use the ModuleUpdater static method to load all the fault\n+            # injection modules in the current pl_module\n+            self.update_module_from_module_list(\n+                pl_module,\n+                self._modules,\n+                in_place=True)\n \n             # we change the flag after cycling through all the modules\n             self._active = True\n \n     # this function deactivates the injection, by substituting all the modules\n     # with the backups\n-    def on_test_stop(self, trainer, pl_module):\n-        # if we are active or there is a backup module\n+    def on_test_end(self, trainer, pl_module):\n+        # if we are active\n         if self._active:\n-            # we need to cycle through all the backup modules\n-            for module_name, module in self._modules_backup.items():\n-                # we reach the final module whose attribute must be updated\n-                # by going through the tree until the last-but-one attribute\n-                dest_module = pl_module\n-                for submodule in module_name.split('.')[:-1]:\n-                    dest_module = getattr(dest_module, submodule)\n-\n-                # we reset the module to the non-injected one\n-                setattr(dest_module, module_name.split('.')[-1],\n-                        module)\n+            # we use the ModuleUpdater static method to reload all the backup\n+            # modules in the current pl_module\n+            self.update_module_from_module_list(\n+                pl_module,\n+                self._modules_backup,\n+                in_place=True)\n \n             # we change the flag after cycling through all the modules\n             self._active = False\n \n     # this method goes over all the structure of the top module, and calls\n     # a function to initialize the injection for each target sub-module\n-    def init_top_module(self, top_module: torch.nn.Module):\n+    def init_model(self, top_module: torch.nn.Module):\n         # for now we iterate through all of them, but we could build a\n         # recursive list to avoid going through a double for loop\n         for fault in self.fault_descriptor_list:\n-            module = top_module\n-            for submodule in fault.module_name.split('.'):\n-                module = getattr(module, submodule)\n+            # we use the get_module function from the ModuleUpdater class\n+            module = self.get_module(fault.module_name, top_module)\n             self.init_module(fault, module)\n \n     # this method must be implemented based on the injection type\n     # (weight, activations, ...)\n-    def init_module(self, fault: basefaultdescriptor.BaseFaultDescriptor,\n-                    module: torch.nn.Module):\n+    def init_module(\n+            self,\n+            fault: src.fi.basefaultdescriptor.BaseFaultDescriptor,\n+            module: torch.nn.Module\n+            ):\n         # we copy the original module\n         self._modules_backup[fault.module_name] = copy.deepcopy(module)\n \n-        injection_module = PARAMETER_TYPE_MAPPING[fault.parameter_type]\n+        injection_module = self.dispatch_call(\n+                # name for dispatch_call\n+                name=fault.parameter_type,\n+                # parameters for the dispatched call\n+                fault=fault,\n+                module=module,\n+        )\n \n-        self._modules[fault.module_name] = injection_module\n+        # we set up the new module to be loaded later on\n+        self._modules[fault.module_name] = copy.deepcopy(injection_module)\n \n     def enable(self):\n         self.enabled = True\n \n     def disable(self):\n         self.enabled = False\n+\n+\n+@BaseInjectionCallback.register_decorator(\n+        src.fi.basefaultdescriptor.ParameterType.Weight\n+)\n+def init_weight(fault: src.fi.basefaultdescriptor.BaseFaultDescriptor,\n+                module: torch.nn.Module) -> torch.nn.Module:\n+    # we get the weights\n+    weights = getattr(module, fault.parameter_name)\n+    weights = src.fi.fiutils.inject_tensor_fault_pytorch(\n+            tensor=weights,\n+            fault=fault,\n+    )\n+    # we set the weights to the updated value\n+    setattr(module, fault.parameter_name, weights)\n+    # we return the new module\n+    return module\n+\n+\n+# NOTE: we can only have one of the following module per layer, as the parsing\n+# of the top module is done statically on the original structure, not on the\n+# updated layers\n+# FIXME: implement also backward for fault-aware training\n+@dataclasses.dataclass(init=True, repr=True)\n+class ActivationInjectionModule(torch.nn.Module):\n+    fault: src.fi.basefaultdescriptor.BaseFaultDescriptor\n+    module: torch.nn.Module\n+\n+    def forward(self, x):\n+        # we get the exact result from the previous module\n+        y_temp = self.module(x)\n+        # we inject the faults in the tensor\n+        y = src.fi.fiutils.inject_tensor_fault_pytorch(\n+                tensor=y_temp,\n+                fault=self.fault,\n+        )\n+        # we return the fault-injected tensor\n+        return y\ndiff --git a\/src\/fi\/fiutils.py b\/src\/fi\/fiutils.py\nindex b16fa6d..0898e80 100644\n--- a\/src\/fi\/fiutils.py\n+++ b\/src\/fi\/fiutils.py\n@@ -3,7 +3,7 @@\n import numpy\n import torch\n \n-from . import basefaultdescriptor\n+import src.fi.basefaultdescriptor\n \n # uint to avoid double sign repetition\n DATA_CONVERSION_MAPPING = {numpy.dtype('float16'): numpy.uint16,\n@@ -50,17 +50,24 @@ def pytorch_element_to_binary(value: torch.Tensor) -> str:\n \n \n def inject_fault_binary(binary: str,\n-                        fault: basefaultdescriptor.BaseFaultDescriptor,\n+                        fault: src.fi.basefaultdescriptor.BaseFaultDescriptor,\n                         sampler: torch.Generator = None) -> str:\n-    injected_binary = copy.deepcopy(binary)\n+    # we need to convert the binary string into a list of characters\n+    # otherwise we cannot update the values\n+    injected_binary = list(copy.deepcopy(binary))\n     for index in fault.bit_index:\n-        if fault.bit_value == basefaultdescriptor.BitValue.One:\n+        # if we are using little endian we invert the index, as the LSB is\n+        # at the end of the list\n+        if fault.endianness == fault.endianness.Little:\n+            index = (len(injected_binary) - 1) - index\n+\n+        if fault.bit_value == src.fi.basefaultdescriptor.BitValue.StuckAtOne:\n             injected_binary[index] = \"1\"\n-        elif fault.bit_value == basefaultdescriptor.BitValue.Zero:\n+        elif fault.bit_value == src.fi.basefaultdescriptor.BitValue.StuckAtZero:\n             injected_binary[index] = \"0\"\n-        elif fault.bit_value == basefaultdescriptor.BitValue.BitFlip:\n+        elif fault.bit_value == src.fi.basefaultdescriptor.BitValue.BitFlip:\n             injected_binary[index] = str(int(injected_binary[index]) ^ 1)\n-        elif fault.bit_value == basefaultdescriptor.BitValue.Random:\n+        elif fault.bit_value == src.fi.basefaultdescriptor.BitValue.Random:\n             # if we do not have a sampler\n             if sampler is None:\n                 raise ValueError(\"A sampler must be passed when using random bit-flips\")\n@@ -69,7 +76,7 @@ def inject_fault_binary(binary: str,\n                 # sampler.manual_seed(SAMPLER_SEED)\n             random_bit = torch.randint(0, 2, size=(), generator=sampler)\n             injected_binary[index] = str(random_bit.item())\n-    return injected_binary\n+    return ''.join(injected_binary)\n \n \n # original_value is used only for device and datatype conversion\n@@ -92,9 +99,57 @@ def binary_to_pytorch_element(binary: str, original_value: torch.Tensor) -> torc\n \n \n def inject_fault_pytorch(tensor: torch.Tensor,\n-                         fault: basefaultdescriptor.BaseFaultDescriptor,\n+                         fault: src.fi.basefaultdescriptor.BaseFaultDescriptor,\n                          sampler: torch.Generator = None) -> torch.Tensor:\n-    binary = pytorch_to_binary(tensor)\n+    binary = pytorch_element_to_binary(tensor)\n     injected_binary = inject_fault_binary(binary, fault, sampler)\n-    injected_tensor = binary_to_pytorch_element(binary, tensor)\n+    injected_tensor = binary_to_pytorch_element(injected_binary, tensor)\n     return injected_tensor\n+\n+\n+def inject_tensor_fault_pytorch(\n+        tensor: torch.Tensor,\n+        fault: src.fi.basefaultdescriptor.BaseFaultDescriptor,\n+        sampler: torch.Generator = None) -> torch.Tensor:\n+    # we deepcopy the tensor to avoid modifying the original one\n+    tensor = copy.deepcopy(tensor)\n+    # we get the tensor value to be injected\n+    # first we convert the fault tensor index to a proper tensor index for the\n+    # tensor case\n+    original_tensor = tensor[\n+            fault.to_tensor_slice(\n+                    fault.tensor_index,\n+                    tensor.size()\n+            )\n+    ]\n+    # to inject the values, we need to flatten the tensor\n+    flattened_tensor = original_tensor.flatten()\n+    # then we need to process them one by one, by injecting the faults\n+    # the returned elements are tensors\n+    injected_flattened_tensor_list = []\n+    for element in flattened_tensor:\n+        injected_flattened_tensor_list.append(\n+                src.fi.fiutils.inject_fault_pytorch(element, fault))\n+    # # we create a list with the injected data, converting back to tensors\n+    # injected_flattened_tensor_list = []\n+    # for injected_binary, original_binary in zip(\n+    #         injected_flattened_bit_tensor, flattened_tensor):\n+    #     injected_flattened_tensor_list.append(\n+    #             src.fi.fiutils.binary_to_pytorch_element(\n+    #                     injected_binary, original_binary))\n+    # print(injected_flattened_tensor_list)\n+    # we create a tensor from the list, moving it to the same device as the\n+    # original one\n+    injected_flattened_tensor = torch.Tensor(\n+            injected_flattened_tensor_list).to(flattened_tensor)\n+    # we reshape the tensor to the original one\n+    injected_tensor = injected_flattened_tensor.reshape(original_tensor.size())\n+    # we update the tensor to the new value\n+    tensor[fault.to_tensor_slice(\n+            fault.tensor_index,\n+            tensor.size()\n+    )] = injected_tensor\n+\n+    # add copy.deepcopy for more redundancy, to avoid modifying the original\n+    # one\n+    return copy.deepcopy(tensor)\ndiff --git a\/src\/fi\/moduleupdater.py b\/src\/fi\/moduleupdater.py\nnew file mode 100644\nindex 0000000..c12f351\n--- \/dev\/null\n+++ b\/src\/fi\/moduleupdater.py\n@@ -0,0 +1,47 @@\n+import copy\n+import typing\n+\n+import torch\n+\n+\n+class ModuleUpdater(object):\n+    @staticmethod\n+    def update_module_from_module_list(\n+            target_module: torch.nn.Module,\n+            module_list: typing.Dict[str, torch.nn.Module],\n+            in_place: bool = True,\n+            ):\n+        # if in place we modify the module with all the list elements\n+        # otherwise we copy it and return a copy\n+        if not in_place:\n+            target_module = copy.deepcopy(target_module)\n+\n+        for module_name, module in module_list.items():\n+            # we reach the final module whose attribute must be updated\n+            # by going through the tree until the last-but-one attribute\n+            module_to_be_updated = module_name.split('.')[-1]\n+            parent_module_name = '.'.join(module_name.split('.')[:-1])\n+\n+            parent_module = ModuleUpdater.get_module(\n+                    parent_module_name,\n+                    target_module\n+            )\n+\n+            # the last attribute is used to set the module to the injection\n+            # one\n+            setattr(parent_module, module_to_be_updated,\n+                    module)\n+\n+        return target_module\n+\n+    # this function is used to get a target module from its name, using as\n+    # root the module which is passed as argument\n+    @staticmethod\n+    def get_module(\n+            target_module_name: str,\n+            module: torch.nn.Module,\n+            ):\n+        dest_module = module\n+        for submodule in target_module_name.split('.'):\n+            dest_module = getattr(dest_module, submodule)\n+        return dest_module\ndiff --git a\/tests\/testfaultinjection.py b\/tests\/testfaultinjection.py\nnew file mode 100644\nindex 0000000..ed4fc4b\n--- \/dev\/null\n+++ b\/tests\/testfaultinjection.py\n@@ -0,0 +1,145 @@\n+import copy\n+import functools\n+import io\n+import pprint\n+import pathlib\n+import random\n+import sys\n+import urllib.request\n+\n+import numpy\n+import PIL\n+import pytorch_lightning\n+import pl_bolts\n+import torch\n+import torchvision\n+\n+CURRENT_DIR = pathlib.Path(__file__).parent.resolve()\n+SRC_PARENT_DIR = (CURRENT_DIR \/ '..').resolve()\n+DATA_DIR = (CURRENT_DIR \/ '..\/data\/cifar10_pretrained\/').resolve()\n+DATASET_DIR = (CURRENT_DIR \/ '..\/data').resolve()\n+\n+sys.path.append(str(SRC_PARENT_DIR))\n+\n+import src.fi.baseinjectioncallback\n+import src.utils\n+\n+sys.path.append(str(DATA_DIR))\n+\n+import vgg\n+\n+\n+### REPRODUCIBILITY\n+# this flag is used for determinism in PyTorch Lightning Trainer\n+DETERMINISTIC_FLAG = True\n+# we call this function to enable reproducibility\n+src.utils.enable_determinism(DETERMINISTIC_FLAG)\n+### REPRODUCIBILITY\n+\n+\n+class PLWrapper(pytorch_lightning.LightningModule):\n+    def __init__(self, model, normalize_prob_func, loss):\n+        super().__init__()\n+\n+        self.model = model\n+        self.loss_func = loss\n+        self.normalize_func = normalize_prob_func\n+\n+    def forward(self, input_):\n+        return self.model(input_)\n+\n+    def inference_step(self, batch, batch_idx):\n+        x, y = batch\n+        y_hat = self.normalize_func(self.model(x))\n+        loss = self.loss_func(y_hat, y)\n+        acc = pytorch_lightning.metrics.functional.accuracy(y_hat, y)\n+\n+        return {'acc': acc, 'loss': loss}\n+\n+    def validation_step(self, batch, batch_idx):\n+        m = self.inference_step(batch, batch_idx)\n+        metrics = {'val_acc': m['acc'], 'val_loss': m['loss']}\n+        self.log_dict(metrics, prog_bar=True, on_step=True, on_epoch=True)\n+        return metrics\n+\n+    def test_step(self, batch, batch_idx):\n+        m = self.inference_step(batch, batch_idx)\n+        metrics = {'test_acc': m['acc'], 'test_loss': m['loss']}\n+        self.log_dict(metrics, prog_bar=True, on_step=True, on_epoch=True)\n+        return metrics\n+\n+\n+# we load the model and define the PL wrapper with it\n+# we can use VGG11_bn as the model dict is saved in the same directory\n+# this is trained on CIFAR10\n+vgg11_bn = vgg.vgg11_bn(pretrained=True)\n+wrapper = PLWrapper(\n+        vgg11_bn,\n+        functools.partial(torch.nn.functional.softmax, dim=1),\n+        torch.nn.functional.cross_entropy)\n+datamodule = pl_bolts.datamodules.CIFAR10DataModule(\n+        data_dir=str(DATASET_DIR),\n+        )\n+\n+faults = [\n+    src.fi.basefaultdescriptor.BaseFaultDescriptor(\n+        module_name='model.classifier.6',\n+        parameter_type=src.fi.basefaultdescriptor.ParameterType.Weight,\n+        tensor_index=...,\n+        bit_index=slice(0, 30),\n+        bit_value=src.fi.basefaultdescriptor.BitValue.StuckAtZero,\n+        # default parameter name for weight injection\n+        parameter_name='weight',\n+    ),\n+]\n+callback = src.fi.baseinjectioncallback.BaseInjectionCallback(\n+        fault_descriptor_list=faults,\n+        enabled=False,\n+        auto_model_init_on_test_start=True,\n+        )\n+\n+# not required as we auto init the model on test start\n+# callback.init_model(wrapper)\n+\n+trainer = pytorch_lightning.Trainer(\n+    callbacks=[callback],\n+    deterministic=DETERMINISTIC_FLAG,\n+    gpus=[0],\n+    )\n+\n+# we use this as baseline\n+trainer.test(wrapper, datamodule=datamodule)\n+\n+# we enable the callback now\n+callback.enable()\n+\n+# we test again\n+trainer.test(wrapper, datamodule=datamodule)\n+\n+# we disable the callback\n+callback.disable()\n+\n+# we test again to reach same results as before injection\n+trainer.test(wrapper, datamodule=datamodule)\n+\n+# # here we define the url, we open it and we load it into a temp binary stream\n+# # in this way it can be opened if offline by PIL\n+# url = \"https:\/\/github.com\/pytorch\/hub\/raw\/master\/images\/dog.jpg\"\n+# temp = io.BytesIO()\n+# with urllib.request.urlopen(url) as image_url_file:\n+#     temp.write(image_url_file.read())\n+#     temp.flush()\n+#     temp.seek(0)\n+#     input_image = PIL.Image.open(temp)\n+\n+# # define the preprocessing pipeline\n+# preprocess = torchvision.transforms.Compose([\n+#     torchvision.transforms.Resize(256),\n+#     torchvision.transforms.CenterCrop(224),\n+#     torchvision.transforms.ToTensor(),\n+#     torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n+# ])\n+# # preprocess the image\n+# input_tensor = preprocess(input_image)\n+# # create a mini-batch as expected by the model\n+# input_batch = input_tensor.unsqueeze(0)\n","files":{"\/src\/fi\/basefaultdescriptor.py":{"changes":[{"diff":"\n+import collections.abc\n import dataclasses\n import enum\n import typing\n \n \n-ParameterType = enum.Enum('ParameterType', 'Weight Activation SparseWeightCOOIndex SparseWeightCOOValue SparseActivationCOOIndex SparseActivationCOOValue', module=__name__)\n-BitValue = enum.Enum('BitValue', 'Random Zero One BitFlip', module=__name__)\n+class Endianness(enum.Flag):\n+    Little = enum.auto()\n+    Big = enum.auto()\n+\n+    MSBAtIndexZero = Big\n+    LSBAtIndexZero = Little\n+\n+\n+class ParameterType(enum.Flag):\n+    Weight = enum.auto()\n+    Activation = enum.auto()\n+    Sparse = enum.auto()\n+    COO = enum.auto()\n+    Index = enum.auto()\n+    Value = enum.auto()\n+    SparseWeightCOOIndex = Sparse | Weight | COO | Index\n+    SparseWeightCOOValue = Sparse | Weight | COO | Value\n+    SparseActivationCOOIndex = Sparse | Activation | COO | Index\n+    SparseActivationCOOValue = Sparse | Activation | COO | Value\n+\n+\n+class BitValue(enum.Enum):\n+    Random = enum.auto()\n+    StuckAtZero = enum.auto()\n+    StuckAtOne = enum.auto()\n+    BitFlip = enum.auto()\n \n \n # this is a container for the module name and the index for where the fault\n # should be injected\n # each fault descriptor covers a single bit-flip (or stuck-at)\n-@dataclasses.dataclass(init=True)\n+# we need unsafe hash for using it as a dictonary key\n+@dataclasses.dataclass(init=True, repr=True, unsafe_hash=True)\n class BaseFaultDescriptor(object):\n     # name of the module to inject\n     module_name: str\n-    # name of the parameter, if the module is a base module (conv, fc),\n-    # it generally coincides with 'weight' for weight injection\n-    # not required for activation injection\n-    parameter_name: str = None\n     # type of parameter to inject, weight, activation, ...\n     parameter_type: ParameterType\n-    # index of the tensor to be injected\n-    tensor_index: tuple[typing.Union[int, slice], ...]\n+    # index of the tensor to be injected, it will be converted to a tuple\n+    tensor_index: typing.Union[\n+            type(Ellipsis),\n+            typing.Sequence[typing.Union[int, slice, type(Ellipsis)]]\n+    ]\n     # index of the bit to be injected\n     # even if it is a slice, it must be converted to indices\n     # just use tuple(range(*slice.indices(slice.stop)))\n-    bit_index: typing.Tuple[int, ...]\n+    bit_index: typing.Union[typing.Sequence[int], slice]\n     # type of bit injection to be carried out\n     bit_value: BitValue\n+    # way to interpret the binary representation, as big endian or little\n+    # endian\n+    # big endian means the bit index 0 is mapped to the MSB of the binary\n+    # little endian means the bit index 0 is instead mapped to the LSB\n+    # by default we use little endian, as this is the most common\n+    # representation\n+    endianness: Endianness = Endianness.Little\n+    # name of the parameter, if the module is a base module (conv, fc),\n+    # it generally coincides with 'weight' for weight injection\n+    # not required for activation injection\n+    parameter_name: str = None\n \n     def __post_init__(self):\n         # FIXME: here there should be some checks on the values\n-        pass\n+\n+        # we raise error if parameter_name is not set and we are not doing\n+        # activation injection\n+        activation_flag = self.parameter_type.Activation in self.parameter_type\n+        if self.parameter_name is None and not activation_flag:\n+            raise ValueError('Please provide a parameter_name from which '\n+                    'to gather the values of the tensor to be injected')\n+\n+        # we convert the tensor index to a tuple, otherwise the descriptor\n+        # remains unhashable\n+        # first we have to check if it is a MutableSequence\n+        if isinstance(self.tensor_index, collections.abc.MutableSequence):\n+            self.tensor_index = tuple(self.tensor_index)\n+\n+        # if the bit index is a slice, we convert it to indices\n+        if isinstance(self.bit_index, slice):\n+            self.bit_index = self.bit_index_from_slice(self.bit_index)\n \n     @staticmethod\n-    def bit_index_from_slice(self, slice_: slice):\n+    def bit_index_from_slice(slice_: slice) -> typing.Tuple[int, ...]:\n+        # we use slice.indices to get back the indices of the slice\n+        # we must pass a stop as it requires a stop value, taken from the slice\n+        # itself\n+        # in this way we can return all of the affected indices in a tuple\n         return tuple(range(*slice_.indices(slice_.stop)))\n+\n+    @staticmethod\n+    def to_tensor_slice(\n+                tensor_index: typing.Union[\n+                        type(Ellipsis),\n+                        typing.Sequence[\n+                                typing.Union[\n+                                        int, slice, type(Ellipsis)]]],\n+                tensor_shape: typing.Sequence[int],\n+            ) -> typing.Sequence[typing.Union[int, slice]]:\n+        # if we have a single ellipsis, without any container, we have a slice\n+        # covering the whole tensor shape\n+        if isinstance(tensor_index, type(Ellipsis)):\n+            return tuple(slice(0, dim_range) for dim_range in tensor_shape)\n+        # we check if the number of elements in the index and in the number of\n+        # dimensions is the same, otherwise we raise ValueError\n+        # this check exists as zip goes over the shortest one\n+        if len(tensor_index) != len(tensor_shape):\n+            raise ValueError(\n+                    \"Number of elements in the index must be the \"\n+                    \"same as the number of dimensions in the tensor\"\n+            )\n+        new_tensor_index = []\n+        for index, dim_range in zip(tensor_index, tensor_shape):\n+            # if the current index is a slice\n+            # then we limit the ending of the slice to the maximum range\n+            # of the current dimension\n+            if isinstance(index, slice):\n+                new_index = index.indices(dim_range - 1)\n+            # if we get an Ellipsis, then we set it up as a slice from 0 to\n+            # max dimension range\n+            elif isinstance(index, type(Ellipsis)):\n+                new_index = slice(0, dim_range - 1)\n+            # if it is int we copy it\n+            elif isinstance(index, int):\n+                new_index = index\n+            # as fallback we raise ValueError\n+            else:\n+                raise ValueError('Wrong index value, use slice, int or ...')\n+            new_tensor_index.append(new_index)\n+        return tuple(new_tensor_index)","add":109,"remove":12,"filename":"\/src\/fi\/basefaultdescriptor.py","badparts":["ParameterType = enum.Enum('ParameterType', 'Weight Activation SparseWeightCOOIndex SparseWeightCOOValue SparseActivationCOOIndex SparseActivationCOOValue', module=__name__)","BitValue = enum.Enum('BitValue', 'Random Zero One BitFlip', module=__name__)","@dataclasses.dataclass(init=True)","    parameter_name: str = None","    tensor_index: tuple[typing.Union[int, slice], ...]","    bit_index: typing.Tuple[int, ...]","        pass","    def bit_index_from_slice(self, slice_: slice):"],"goodparts":["import collections.abc","class Endianness(enum.Flag):","    Little = enum.auto()","    Big = enum.auto()","    MSBAtIndexZero = Big","    LSBAtIndexZero = Little","class ParameterType(enum.Flag):","    Weight = enum.auto()","    Activation = enum.auto()","    Sparse = enum.auto()","    COO = enum.auto()","    Index = enum.auto()","    Value = enum.auto()","    SparseWeightCOOIndex = Sparse | Weight | COO | Index","    SparseWeightCOOValue = Sparse | Weight | COO | Value","    SparseActivationCOOIndex = Sparse | Activation | COO | Index","    SparseActivationCOOValue = Sparse | Activation | COO | Value","class BitValue(enum.Enum):","    Random = enum.auto()","    StuckAtZero = enum.auto()","    StuckAtOne = enum.auto()","    BitFlip = enum.auto()","@dataclasses.dataclass(init=True, repr=True, unsafe_hash=True)","    tensor_index: typing.Union[","            type(Ellipsis),","            typing.Sequence[typing.Union[int, slice, type(Ellipsis)]]","    ]","    bit_index: typing.Union[typing.Sequence[int], slice]","    endianness: Endianness = Endianness.Little","    parameter_name: str = None","        activation_flag = self.parameter_type.Activation in self.parameter_type","        if self.parameter_name is None and not activation_flag:","            raise ValueError('Please provide a parameter_name from which '","                    'to gather the values of the tensor to be injected')","        if isinstance(self.tensor_index, collections.abc.MutableSequence):","            self.tensor_index = tuple(self.tensor_index)","        if isinstance(self.bit_index, slice):","            self.bit_index = self.bit_index_from_slice(self.bit_index)","    def bit_index_from_slice(slice_: slice) -> typing.Tuple[int, ...]:","    @staticmethod","    def to_tensor_slice(","                tensor_index: typing.Union[","                        type(Ellipsis),","                        typing.Sequence[","                                typing.Union[","                                        int, slice, type(Ellipsis)]]],","                tensor_shape: typing.Sequence[int],","            ) -> typing.Sequence[typing.Union[int, slice]]:","        if isinstance(tensor_index, type(Ellipsis)):","            return tuple(slice(0, dim_range) for dim_range in tensor_shape)","        if len(tensor_index) != len(tensor_shape):","            raise ValueError(","                    \"Number of elements in the index must be the \"","                    \"same as the number of dimensions in the tensor\"","            )","        new_tensor_index = []","        for index, dim_range in zip(tensor_index, tensor_shape):","            if isinstance(index, slice):","                new_index = index.indices(dim_range - 1)","            elif isinstance(index, type(Ellipsis)):","                new_index = slice(0, dim_range - 1)","            elif isinstance(index, int):","                new_index = index","            else:","                raise ValueError('Wrong index value, use slice, int or ...')","            new_tensor_index.append(new_index)","        return tuple(new_tensor_index)"]}],"source":"\nimport dataclasses import enum import typing ParameterType=enum.Enum('ParameterType', 'Weight Activation SparseWeightCOOIndex SparseWeightCOOValue SparseActivationCOOIndex SparseActivationCOOValue', module=__name__) BitValue=enum.Enum('BitValue', 'Random Zero One BitFlip', module=__name__) @dataclasses.dataclass(init=True) class BaseFaultDescriptor(object): module_name: str parameter_name: str=None parameter_type: ParameterType tensor_index: tuple[typing.Union[int, slice],...] bit_index: typing.Tuple[int,...] bit_value: BitValue def __post_init__(self): pass @staticmethod def bit_index_from_slice(self, slice_: slice): return tuple(range(*slice_.indices(slice_.stop))) ","sourceWithComments":"import dataclasses\nimport enum\nimport typing\n\n\nParameterType = enum.Enum('ParameterType', 'Weight Activation SparseWeightCOOIndex SparseWeightCOOValue SparseActivationCOOIndex SparseActivationCOOValue', module=__name__)\nBitValue = enum.Enum('BitValue', 'Random Zero One BitFlip', module=__name__)\n\n\n# this is a container for the module name and the index for where the fault\n# should be injected\n# each fault descriptor covers a single bit-flip (or stuck-at)\n@dataclasses.dataclass(init=True)\nclass BaseFaultDescriptor(object):\n    # name of the module to inject\n    module_name: str\n    # name of the parameter, if the module is a base module (conv, fc),\n    # it generally coincides with 'weight' for weight injection\n    # not required for activation injection\n    parameter_name: str = None\n    # type of parameter to inject, weight, activation, ...\n    parameter_type: ParameterType\n    # index of the tensor to be injected\n    tensor_index: tuple[typing.Union[int, slice], ...]\n    # index of the bit to be injected\n    # even if it is a slice, it must be converted to indices\n    # just use tuple(range(*slice.indices(slice.stop)))\n    bit_index: typing.Tuple[int, ...]\n    # type of bit injection to be carried out\n    bit_value: BitValue\n\n    def __post_init__(self):\n        # FIXME: here there should be some checks on the values\n        pass\n\n    @staticmethod\n    def bit_index_from_slice(self, slice_: slice):\n        return tuple(range(*slice_.indices(slice_.stop)))\n"},"\/src\/fi\/fiutils.py":{"changes":[{"diff":"\n import numpy\n import torch\n \n-from . import basefaultdescriptor\n+import src.fi.basefaultdescriptor\n \n # uint to avoid double sign repetition\n DATA_CONVERSION_MAPPING = {numpy.dtype('float16'): numpy.uint16,\n","add":1,"remove":1,"filename":"\/src\/fi\/fiutils.py","badparts":["from . import basefaultdescriptor"],"goodparts":["import src.fi.basefaultdescriptor"]},{"diff":"\n \n \n def inject_fault_binary(binary: str,\n-                        fault: basefaultdescriptor.BaseFaultDescriptor,\n+                        fault: src.fi.basefaultdescriptor.BaseFaultDescriptor,\n                         sampler: torch.Generator = None) -> str:\n-    injected_binary = copy.deepcopy(binary)\n+    # we need to convert the binary string into a list of characters\n+    # otherwise we cannot update the values\n+    injected_binary = list(copy.deepcopy(binary))\n     for index in fault.bit_index:\n-        if fault.bit_value == basefaultdescriptor.BitValue.One:\n+        # if we are using little endian we invert the index, as the LSB is\n+        # at the end of the list\n+        if fault.endianness == fault.endianness.Little:\n+            index = (len(injected_binary) - 1) - index\n+\n+        if fault.bit_value == src.fi.basefaultdescriptor.BitValue.StuckAtOne:\n             injected_binary[index] = \"1\"\n-        elif fault.bit_value == basefaultdescriptor.BitValue.Zero:\n+        elif fault.bit_value == src.fi.basefaultdescriptor.BitValue.StuckAtZero:\n             injected_binary[index] = \"0\"\n-        elif fault.bit_value == basefaultdescriptor.BitValue.BitFlip:\n+        elif fault.bit_value == src.fi.basefaultdescriptor.BitValue.BitFlip:\n             injected_binary[index] = str(int(injected_binary[index]) ^ 1)\n-        elif fault.bit_value == basefaultdescriptor.BitValue.Random:\n+        elif fault.bit_value == src.fi.basefaultdescriptor.BitValue.Random:\n             # if we do not have a sampler\n             if sampler is None:\n                 raise ValueError(\"A sampler must be passed when using random bit-flips\")\n","add":13,"remove":6,"filename":"\/src\/fi\/fiutils.py","badparts":["                        fault: basefaultdescriptor.BaseFaultDescriptor,","    injected_binary = copy.deepcopy(binary)","        if fault.bit_value == basefaultdescriptor.BitValue.One:","        elif fault.bit_value == basefaultdescriptor.BitValue.Zero:","        elif fault.bit_value == basefaultdescriptor.BitValue.BitFlip:","        elif fault.bit_value == basefaultdescriptor.BitValue.Random:"],"goodparts":["                        fault: src.fi.basefaultdescriptor.BaseFaultDescriptor,","    injected_binary = list(copy.deepcopy(binary))","        if fault.endianness == fault.endianness.Little:","            index = (len(injected_binary) - 1) - index","        if fault.bit_value == src.fi.basefaultdescriptor.BitValue.StuckAtOne:","        elif fault.bit_value == src.fi.basefaultdescriptor.BitValue.StuckAtZero:","        elif fault.bit_value == src.fi.basefaultdescriptor.BitValue.BitFlip:","        elif fault.bit_value == src.fi.basefaultdescriptor.BitValue.Random:"]},{"diff":"\n                 # sampler.manual_seed(SAMPLER_SEED)\n             random_bit = torch.randint(0, 2, size=(), generator=sampler)\n             injected_binary[index] = str(random_bit.item())\n-    return injected_binary\n+    return ''.join(injected_binary)\n \n \n # original_value is used only for device and datatype conversion\n","add":1,"remove":1,"filename":"\/src\/fi\/fiutils.py","badparts":["    return injected_binary"],"goodparts":["    return ''.join(injected_binary)"]},{"diff":"\n \n \n def inject_fault_pytorch(tensor: torch.Tensor,\n-                         fault: basefaultdescriptor.BaseFaultDescriptor,\n+                         fault: src.fi.basefaultdescriptor.BaseFaultDescriptor,\n                          sampler: torch.Generator = None) -> torch.Tensor:\n-    binary = pytorch_to_binary(tensor)\n+    binary = pytorch_element_to_binary(tensor)\n     injected_binary = inject_fault_binary(binary, fault, sampler)\n-    injected_tensor = binary_to_pytorch_element(binary, tensor)\n+    injected_tensor = binary_to_pytorch_element(injected_binary, tensor)\n     return injected_tensor\n+\n+\n+def inject_tensor_fault_pytorch(\n+        tensor: torch.Tensor,\n+        fault: src.fi.basefaultdescriptor.BaseFaultDescriptor,\n+        sampler: torch.Generator = None) -> torch.Tensor:\n+    # we deepcopy the tensor to avoid modifying the original one\n+    tensor = copy.deepcopy(tensor)\n+    # we get the tensor value to be injected\n+    # first we convert the fault tensor index to a proper tensor index for the\n+    # tensor case\n+    original_tensor = tensor[\n+            fault.to_tensor_slice(\n+                    fault.tensor_index,\n+                    tensor.size()\n+            )\n+    ]\n+    # to inject the values, we need to flatten the tensor\n+    flattened_tensor = original_tensor.flatten()\n+    # then we need to process them one by one, by injecting the faults\n+    # the returned elements are tensors\n+    injected_flattened_tensor_list = []\n+    for element in flattened_tensor:\n+        injected_flattened_tensor_list.append(\n+                src.fi.fiutils.inject_fault_pytorch(element, fault))\n+    # # we create a list with the injected data, converting back to tensors\n+    # injected_flattened_tensor_list = []\n+    # for injected_binary, original_binary in zip(\n+    #         injected_flattened_bit_tensor, flattened_tensor):\n+    #     injected_flattened_tensor_list.append(\n+    #             src.fi.fiutils.binary_to_pytorch_element(\n+    #                     injected_binary, original_binary))\n+    # print(injected_flattened_tensor_list)\n+    # we create a tensor from the list, moving it to the same device as the\n+    # original one\n+    injected_flattened_tensor = torch.Tensor(\n+            injected_flattened_tensor_list).to(flattened_tensor)\n+    # we reshape the tensor to the original one\n+    injected_tensor = injected_flattened_tensor.reshape(original_tensor.size())\n+    # we update the tensor to the new value\n+    tensor[fault.to_tensor_slice(\n+            fault.tensor_index,\n+            tensor.size()\n+    )] = injected_tensor\n+\n+    # add copy.deepcopy for more redundancy, to avoid modifying the original\n+    # one\n+    return copy.deepcopy(tenso","add":51,"remove":3,"filename":"\/src\/fi\/fiutils.py","badparts":["                         fault: basefaultdescriptor.BaseFaultDescriptor,","    binary = pytorch_to_binary(tensor)","    injected_tensor = binary_to_pytorch_element(binary, tensor)"],"goodparts":["                         fault: src.fi.basefaultdescriptor.BaseFaultDescriptor,","    binary = pytorch_element_to_binary(tensor)","    injected_tensor = binary_to_pytorch_element(injected_binary, tensor)","def inject_tensor_fault_pytorch(","        tensor: torch.Tensor,","        fault: src.fi.basefaultdescriptor.BaseFaultDescriptor,","        sampler: torch.Generator = None) -> torch.Tensor:","    tensor = copy.deepcopy(tensor)","    original_tensor = tensor[","            fault.to_tensor_slice(","                    fault.tensor_index,","                    tensor.size()","            )","    ]","    flattened_tensor = original_tensor.flatten()","    injected_flattened_tensor_list = []","    for element in flattened_tensor:","        injected_flattened_tensor_list.append(","                src.fi.fiutils.inject_fault_pytorch(element, fault))","    injected_flattened_tensor = torch.Tensor(","            injected_flattened_tensor_list).to(flattened_tensor)","    injected_tensor = injected_flattened_tensor.reshape(original_tensor.size())","    tensor[fault.to_tensor_slice(","            fault.tensor_index,","            tensor.size()","    )] = injected_tensor","    return copy.deepcopy(tenso"]}],"source":"\nimport copy import numpy import torch from. import basefaultdescriptor DATA_CONVERSION_MAPPING={numpy.dtype('float16'): numpy.uint16, numpy.dtype('float32'): numpy.uint32, numpy.dtype('float64'): numpy.uint64, numpy.dtype('uint8'): numpy.uint8, numpy.dtype('int8'): numpy.uint8, numpy.dtype('int16'): numpy.uint16, numpy.dtype('int32'): numpy.uint32, numpy.dtype('int64'): numpy.uint64, } DATA_WIDTH_MAPPING={numpy.dtype('float16'): '16', numpy.dtype('float32'): '32', numpy.dtype('float64'): '64', numpy.dtype('uint8'): '8', numpy.dtype('int8'): '8', numpy.dtype('int16'): '16', numpy.dtype('int32'): '32', numpy.dtype('int64'): '64', } TEMPLATE_STRING='{{:0{}b}}' def pytorch_element_to_binary(value: torch.Tensor) -> str: if value.size() !=tuple(): value=value[0] numpy_value=value.cpu().numpy() dtype=numpy_value.dtype new_dtype=DATA_CONVERSION_MAPPING[dtype] width=DATA_WIDTH_MAPPING[dtype] str_bin_value=TEMPLATE_STRING.format(width).format(numpy_value.view(new_dtype)) return str_bin_value def inject_fault_binary(binary: str, fault: basefaultdescriptor.BaseFaultDescriptor, sampler: torch.Generator=None) -> str: injected_binary=copy.deepcopy(binary) for index in fault.bit_index: if fault.bit_value==basefaultdescriptor.BitValue.One: injected_binary[index]=\"1\" elif fault.bit_value==basefaultdescriptor.BitValue.Zero: injected_binary[index]=\"0\" elif fault.bit_value==basefaultdescriptor.BitValue.BitFlip: injected_binary[index]=str(int(injected_binary[index]) ^ 1) elif fault.bit_value==basefaultdescriptor.BitValue.Random: if sampler is None: raise ValueError(\"A sampler must be passed when using random bit-flips\") random_bit=torch.randint(0, 2, size=(), generator=sampler) injected_binary[index]=str(random_bit.item()) return injected_binary def binary_to_pytorch_element(binary: str, original_value: torch.Tensor) -> torch.Tensor: if original_value.size() !=tuple(): original_value=original_value[0] dtype=original_value.cpu().numpy().dtype new_dtype=DATA_CONVERSION_MAPPING[dtype] python_int=int(binary, base=2) new_numpy_value=new_dtype([python_int]).view(dtype) return torch.from_numpy(new_numpy_value).to(original_value)[0] def inject_fault_pytorch(tensor: torch.Tensor, fault: basefaultdescriptor.BaseFaultDescriptor, sampler: torch.Generator=None) -> torch.Tensor: binary=pytorch_to_binary(tensor) injected_binary=inject_fault_binary(binary, fault, sampler) injected_tensor=binary_to_pytorch_element(binary, tensor) return injected_tensor ","sourceWithComments":"import copy\n\nimport numpy\nimport torch\n\nfrom . import basefaultdescriptor\n\n# uint to avoid double sign repetition\nDATA_CONVERSION_MAPPING = {numpy.dtype('float16'): numpy.uint16,\n                           numpy.dtype('float32'): numpy.uint32,\n                           numpy.dtype('float64'): numpy.uint64,\n                           numpy.dtype('uint8'): numpy.uint8,\n                           numpy.dtype('int8'): numpy.uint8,\n                           numpy.dtype('int16'): numpy.uint16,\n                           numpy.dtype('int32'): numpy.uint32,\n                           numpy.dtype('int64'): numpy.uint64,\n                           }\nDATA_WIDTH_MAPPING = {numpy.dtype('float16'): '16',\n                      numpy.dtype('float32'): '32',\n                      numpy.dtype('float64'): '64',\n                      numpy.dtype('uint8'): '8',\n                      numpy.dtype('int8'): '8',\n                      numpy.dtype('int16'): '16',\n                      numpy.dtype('int32'): '32',\n                      numpy.dtype('int64'): '64',\n                      }\n# this template first requires the width (the single {}) and then it can\n# convert a number to a binary view using that width and filling the extra\n# on the left with 0s\nTEMPLATE_STRING = '{{:0{}b}}'\n\n\n# gets the binary value from a PyTorch element\ndef pytorch_element_to_binary(value: torch.Tensor) -> str:\n    # required because shapes (1, ) and () are considered different and we need ()\n    if value.size() != tuple():\n        value = value[0]\n\n    # we get the numpy value, keeping the same datatype\n    numpy_value = value.cpu().numpy()\n    dtype = numpy_value.dtype\n    # we convert data type\n    new_dtype = DATA_CONVERSION_MAPPING[dtype]\n    # we need the witdth of the new data type\n    width = DATA_WIDTH_MAPPING[dtype]\n    # we view the number with a different datatype (int) so we can extract the bits\n    str_bin_value = TEMPLATE_STRING.format(width).format(numpy_value.view(new_dtype))\n\n    return str_bin_value\n\n\ndef inject_fault_binary(binary: str,\n                        fault: basefaultdescriptor.BaseFaultDescriptor,\n                        sampler: torch.Generator = None) -> str:\n    injected_binary = copy.deepcopy(binary)\n    for index in fault.bit_index:\n        if fault.bit_value == basefaultdescriptor.BitValue.One:\n            injected_binary[index] = \"1\"\n        elif fault.bit_value == basefaultdescriptor.BitValue.Zero:\n            injected_binary[index] = \"0\"\n        elif fault.bit_value == basefaultdescriptor.BitValue.BitFlip:\n            injected_binary[index] = str(int(injected_binary[index]) ^ 1)\n        elif fault.bit_value == basefaultdescriptor.BitValue.Random:\n            # if we do not have a sampler\n            if sampler is None:\n                raise ValueError(\"A sampler must be passed when using random bit-flips\")\n                # SAMPLER_SEED = 2147483647\n                # sampler = torch.Generator(device='cpu')\n                # sampler.manual_seed(SAMPLER_SEED)\n            random_bit = torch.randint(0, 2, size=(), generator=sampler)\n            injected_binary[index] = str(random_bit.item())\n    return injected_binary\n\n\n# original_value is used only for device and datatype conversion\ndef binary_to_pytorch_element(binary: str, original_value: torch.Tensor) -> torch.Tensor:\n    # required because shapes (1, ) and () are considered different and we need ()\n    if original_value.size() != tuple():\n        original_value = original_value[0]\n\n    dtype = original_value.cpu().numpy().dtype\n    # we need the converted data type\n    new_dtype = DATA_CONVERSION_MAPPING[dtype]\n\n    # we convert the bits to numpy integer through Python int for base 2 conversion\n    # then we view it back in the original type and convert it to PyTorch\n    # square brackets are for creating a numpy.ndarray for PyTorch\n    python_int = int(binary, base=2)\n    new_numpy_value = new_dtype([python_int]).view(dtype)\n    # we use [0] to return a single element\n    return torch.from_numpy(new_numpy_value).to(original_value)[0]\n\n\ndef inject_fault_pytorch(tensor: torch.Tensor,\n                         fault: basefaultdescriptor.BaseFaultDescriptor,\n                         sampler: torch.Generator = None) -> torch.Tensor:\n    binary = pytorch_to_binary(tensor)\n    injected_binary = inject_fault_binary(binary, fault, sampler)\n    injected_tensor = binary_to_pytorch_element(binary, tensor)\n    return injected_tensor\n"}},"msg":"Test initial weight injection and fix related bugs\n\nWrite basic PyTorchLightning wrapper.\n\nAdd Endianness for bit indices, to support 0 as MSB and 0 as LSB.\n\nConvert functional enum declarations into class declarations.\n\nImprove typing hints for BaseFaultDescriptor.\n\nAdd argument checks in post_init of BaseFaultDescriptor.\n\nAdd static method to convert generic tensor index to valid tensor slice.\n\nInclude DispatcherABC into BaseInjectionCallback.\n\nImplement ModuleUpdater to automatically update modules.\n\nSubstitute un-mantainable calls with class methods.\n\nUse dispatching instead of dict-mapping for fault initialization.\n\nAdd flag to automate fault initialization at test start.\n\nChange weight injection to use new function from fiutils.\n\nBasic implementation of activation injection, not tested.\n\nIncrease redundancy of injection methods by deepcopying inputs\/outputs.\n\nFix bug by changing string injection into list, and converting back.\n\nImplement method for automating gathering the correct\nelements from a tensor and inject the fault."},"5697de1da3755b20bd5093493ca58b676847463b":{"url":"https:\/\/api.github.com\/repos\/Alexei95\/enpheeph\/commits\/5697de1da3755b20bd5093493ca58b676847463b","html_url":"https:\/\/github.com\/Alexei95\/enpheeph\/commit\/5697de1da3755b20bd5093493ca58b676847463b","sha":"5697de1da3755b20bd5093493ca58b676847463b","keyword":"function injection update","diff":"diff --git a\/scripts\/injector_script.py b\/scripts\/injector_script.py\nindex 97958fe..9170a45 100644\n--- a\/scripts\/injector_script.py\n+++ b\/scripts\/injector_script.py\n@@ -27,7 +27,9 @@\n import enpheeph.injections.pruneddensetosparseactivationpytorchfault\n \n \n-def get_injection_callback() -> pytorch_lightning.Callback:\n+def get_injection_callback_image_classifier_resnet18_cifar10() -> (\n+    pytorch_lightning.Callback\n+):\n     storage_file = (\n         pathlib.Path(__file__).absolute().parent\n         \/ \"results\/injection_test\/database.sqlite\"\n@@ -99,6 +101,80 @@ def get_injection_callback() -> pytorch_lightning.Callback:\n     return callback\n \n \n+def get_injection_callback_semantic_segmenter_mobilenetv3_carla() -> (\n+    pytorch_lightning.Callback\n+):\n+    storage_file = (\n+        pathlib.Path(__file__).absolute().parent\n+        \/ \"results\/injection_test\/database.sqlite\"\n+    )\n+    storage_file.parent.mkdir(exist_ok=True, parents=True)\n+\n+    pytorch_handler_plugin = enpheeph.handlers.plugins.PyTorchHandlerPlugin()\n+    storage_plugin = enpheeph.injections.plugins.storage.SQLiteStoragePlugin(\n+        db_url=\"sqlite:\/\/\/\" + str(storage_file)\n+    )\n+    pytorch_mask_plugin = enpheeph.injections.plugins.mask.AutoPyTorchMaskPlugin()\n+\n+    fault_1 = enpheeph.injections.PrunedDenseToSparseWeightPyTorchFault(\n+        location=enpheeph.utils.dataclasses.FaultLocation(\n+            # mobilenetv3\n+            module_name=\"backbone.model.blocks.3.0.conv_pw\",\n+            parameter_type=(\n+                enpheeph.utils.enums.ParameterType.Weight\n+                | enpheeph.utils.enums.ParameterType.Sparse\n+                | enpheeph.utils.enums.ParameterType.Value\n+            ),\n+            parameter_name=\"weight\",\n+            dimension_index={\n+                enpheeph.utils.enums.DimensionType.Tensor: (0,),\n+            },\n+            bit_index=...,\n+            bit_fault_value=enpheeph.utils.enums.BitFaultValue.StuckAtOne,\n+        ),\n+        low_level_torch_plugin=pytorch_mask_plugin,\n+        indexing_plugin=enpheeph.injections.plugins.indexing.IndexingPlugin(\n+            dimension_dict=enpheeph.utils.constants.PYTORCH_DIMENSION_DICT,\n+        ),\n+    )\n+    monitor_1 = enpheeph.injections.OutputPyTorchMonitor(\n+        location=enpheeph.utils.dataclasses.MonitorLocation(\n+            module_name=\"head\",\n+            parameter_type=enpheeph.utils.enums.ParameterType.Activation,\n+            dimension_index={\n+                enpheeph.utils.enums.DimensionType.Tensor: ...,\n+                enpheeph.utils.enums.DimensionType.Batch: ...,\n+            },\n+            bit_index=None,\n+        ),\n+        enabled_metrics=(\n+            enpheeph.utils.enums.MonitorMetric.ArithmeticMean\n+            | enpheeph.utils.enums.MonitorMetric.StandardDeviation\n+        ),\n+        storage_plugin=storage_plugin,\n+        move_to_first=False,\n+        indexing_plugin=enpheeph.injections.plugins.indexing.IndexingPlugin(\n+            dimension_dict=enpheeph.utils.constants.PYTORCH_DIMENSION_DICT,\n+        ),\n+    )\n+\n+    injection_handler = enpheeph.handlers.InjectionHandler(\n+        injections=[fault_1, monitor_1],\n+        library_handler_plugin=pytorch_handler_plugin,\n+    )\n+\n+    # we delay the instantiation of the callback to allow the saving of the\n+    # current configuration\n+    callback = enpheeph.integrations.pytorchlightning.InjectionCallback(\n+        injection_handler=injection_handler,\n+        storage_plugin=storage_plugin,\n+        # this config used to contain the complete system config: trainer + model +\n+        # dataset, including the configuration for injections\n+        # extra_session_info=config,\n+    )\n+    return callback\n+\n+\n def get_trainer_config(args=sys.argv) -> typing.Dict[str, typing.Any]:\n     config = pathlib.Path(args[1]).absolute()\n \n@@ -116,6 +192,7 @@ def get_trainer_config(args=sys.argv) -> typing.Dict[str, typing.Any]:\n \n \n def main():\n+    # TODO: improve the config handling\n     config = pathlib.Path(sys.argv[1]).absolute()\n \n     sys.path.append(str(config.parent))\n@@ -132,7 +209,7 @@ def main():\n     datamodule = config_dict[\"datamodule\"]\n     model = config_dict[\"model\"]\n \n-    injection_callback = get_injection_callback()\n+    injection_callback = get_injection_callback_semantic_segmenter_mobilenetv3_carla()\n     trainer.callbacks.append(injection_callback)\n \n     injection_callback.injection_handler.activate()\n","message":"","files":{"\/scripts\/injector_script.py":{"changes":[{"diff":"\n import enpheeph.injections.pruneddensetosparseactivationpytorchfault\n \n \n-def get_injection_callback() -> pytorch_lightning.Callback:\n+def get_injection_callback_image_classifier_resnet18_cifar10() -> (\n+    pytorch_lightning.Callback\n+):\n     storage_file = (\n         pathlib.Path(__file__).absolute().parent\n         \/ \"results\/injection_test\/database.sqlite\"\n","add":3,"remove":1,"filename":"\/scripts\/injector_script.py","badparts":["def get_injection_callback() -> pytorch_lightning.Callback:"],"goodparts":["def get_injection_callback_image_classifier_resnet18_cifar10() -> (","    pytorch_lightning.Callback","):"]},{"diff":"\n     datamodule = config_dict[\"datamodule\"]\n     model = config_dict[\"model\"]\n \n-    injection_callback = get_injection_callback()\n+    injection_callback = get_injection_callback_semantic_segmenter_mobilenetv3_carla()\n     trainer.callbacks.append(injection_callback)\n \n     injection_callback.injection_handler.activate()\n","add":1,"remove":1,"filename":"\/scripts\/injector_script.py","badparts":["    injection_callback = get_injection_callback()"],"goodparts":["    injection_callback = get_injection_callback_semantic_segmenter_mobilenetv3_carla()"]}],"source":"\n import importlib import pathlib import sys import typing import pytorch_lightning import enpheeph import enpheeph.injections.abc import enpheeph.injections.pruneddensetosparseactivationpytorchfault def get_injection_callback() -> pytorch_lightning.Callback: storage_file=( pathlib.Path(__file__).absolute().parent \/ \"results\/injection_test\/database.sqlite\" ) storage_file.parent.mkdir(exist_ok=True, parents=True) pytorch_handler_plugin=enpheeph.handlers.plugins.PyTorchHandlerPlugin() storage_plugin=enpheeph.injections.plugins.storage.SQLiteStoragePlugin( db_url=\"sqlite:\/\/\/\" +str(storage_file) ) pytorch_mask_plugin=enpheeph.injections.plugins.mask.AutoPyTorchMaskPlugin() fault_1=enpheeph.injections.PrunedDenseToSparseWeightPyTorchFault( location=enpheeph.utils.dataclasses.FaultLocation( module_name=\"adapter.backbone.layer1.0.conv1\", parameter_type=( enpheeph.utils.enums.ParameterType.Weight | enpheeph.utils.enums.ParameterType.Sparse | enpheeph.utils.enums.ParameterType.Value ), parameter_name=\"weight\", dimension_index={ enpheeph.utils.enums.DimensionType.Tensor:(0,), }, bit_index=..., bit_fault_value=enpheeph.utils.enums.BitFaultValue.StuckAtOne, ), low_level_torch_plugin=pytorch_mask_plugin, indexing_plugin=enpheeph.injections.plugins.indexing.IndexingPlugin( dimension_dict=enpheeph.utils.constants.PYTORCH_DIMENSION_DICT, ), ) monitor_1=enpheeph.injections.OutputPyTorchMonitor( location=enpheeph.utils.dataclasses.MonitorLocation( module_name=\"adapter.head\", parameter_type=enpheeph.utils.enums.ParameterType.Activation, dimension_index={ enpheeph.utils.enums.DimensionType.Tensor:..., enpheeph.utils.enums.DimensionType.Batch:..., }, bit_index=None, ), enabled_metrics=( enpheeph.utils.enums.MonitorMetric.ArithmeticMean | enpheeph.utils.enums.MonitorMetric.StandardDeviation ), storage_plugin=storage_plugin, move_to_first=False, indexing_plugin=enpheeph.injections.plugins.indexing.IndexingPlugin( dimension_dict=enpheeph.utils.constants.PYTORCH_DIMENSION_DICT, ), ) injection_handler=enpheeph.handlers.InjectionHandler( injections=[fault_1, monitor_1], library_handler_plugin=pytorch_handler_plugin, ) callback=enpheeph.integrations.pytorchlightning.InjectionCallback( injection_handler=injection_handler, storage_plugin=storage_plugin, ) return callback def get_trainer_config(args=sys.argv) -> typing.Dict[str, typing.Any]: config=pathlib.Path(args[1]).absolute() sys.path.append(str(config.parent)) module_name=config.with_suffix(\"\").name module=importlib.import_module(module_name) sys.path.pop() config_dict=module.config() return config_dict def main(): config=pathlib.Path(sys.argv[1]).absolute() sys.path.append(str(config.parent)) module_name=config.with_suffix(\"\").name module=importlib.import_module(module_name) sys.path.pop() config_dict=module.config() trainer=config_dict[\"trainer\"] datamodule=config_dict[\"datamodule\"] model=config_dict[\"model\"] injection_callback=get_injection_callback() trainer.callbacks.append(injection_callback) injection_callback.injection_handler.activate() injection_callback.injection_handler.deactivate( [ inj for inj in injection_callback.injection_handler.injections if isinstance(inj, enpheeph.injections.abc.FaultABC) ] ) trainer.test(model, datamodule=datamodule) injection_callback.injection_handler.activate() trainer.test(model, datamodule=datamodule) injection_callback.injection_handler.deactivate( [ inj for inj in injection_callback.injection_handler.injections if isinstance(inj, enpheeph.injections.abc.FaultABC) ] ) trainer.test(model, datamodule=datamodule) if __name__==\"__main__\": main() ","sourceWithComments":"# -*- coding: utf-8 -*-\n# enpheeph - Neural Fault Injection Framework\n# Copyright (C) 2020-2022 Alessio \"Alexei95\" Colucci\n#\n# This program is free software: you can redistribute it and\/or modify\n# it under the terms of the GNU Affero General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Affero General Public License for more details.\n#\n# You should have received a copy of the GNU Affero General Public License\n# along with this program.  If not, see <https:\/\/www.gnu.org\/licenses\/>.\n\nimport importlib\nimport pathlib\nimport sys\nimport typing\n\nimport pytorch_lightning\n\nimport enpheeph\nimport enpheeph.injections.abc\nimport enpheeph.injections.pruneddensetosparseactivationpytorchfault\n\n\ndef get_injection_callback() -> pytorch_lightning.Callback:\n    storage_file = (\n        pathlib.Path(__file__).absolute().parent\n        \/ \"results\/injection_test\/database.sqlite\"\n    )\n    storage_file.parent.mkdir(exist_ok=True, parents=True)\n\n    pytorch_handler_plugin = enpheeph.handlers.plugins.PyTorchHandlerPlugin()\n    storage_plugin = enpheeph.injections.plugins.storage.SQLiteStoragePlugin(\n        db_url=\"sqlite:\/\/\/\" + str(storage_file)\n    )\n    pytorch_mask_plugin = enpheeph.injections.plugins.mask.AutoPyTorchMaskPlugin()\n\n    fault_1 = enpheeph.injections.PrunedDenseToSparseWeightPyTorchFault(\n        location=enpheeph.utils.dataclasses.FaultLocation(\n            # resnet18\n            module_name=\"adapter.backbone.layer1.0.conv1\",\n            parameter_type=(\n                enpheeph.utils.enums.ParameterType.Weight\n                | enpheeph.utils.enums.ParameterType.Sparse\n                | enpheeph.utils.enums.ParameterType.Value\n            ),\n            parameter_name=\"weight\",\n            dimension_index={\n                enpheeph.utils.enums.DimensionType.Tensor: (0,),\n            },\n            bit_index=...,\n            bit_fault_value=enpheeph.utils.enums.BitFaultValue.StuckAtOne,\n        ),\n        low_level_torch_plugin=pytorch_mask_plugin,\n        indexing_plugin=enpheeph.injections.plugins.indexing.IndexingPlugin(\n            dimension_dict=enpheeph.utils.constants.PYTORCH_DIMENSION_DICT,\n        ),\n    )\n    monitor_1 = enpheeph.injections.OutputPyTorchMonitor(\n        location=enpheeph.utils.dataclasses.MonitorLocation(\n            module_name=\"adapter.head\",\n            parameter_type=enpheeph.utils.enums.ParameterType.Activation,\n            dimension_index={\n                enpheeph.utils.enums.DimensionType.Tensor: ...,\n                enpheeph.utils.enums.DimensionType.Batch: ...,\n            },\n            bit_index=None,\n        ),\n        enabled_metrics=(\n            enpheeph.utils.enums.MonitorMetric.ArithmeticMean\n            | enpheeph.utils.enums.MonitorMetric.StandardDeviation\n        ),\n        storage_plugin=storage_plugin,\n        move_to_first=False,\n        indexing_plugin=enpheeph.injections.plugins.indexing.IndexingPlugin(\n            dimension_dict=enpheeph.utils.constants.PYTORCH_DIMENSION_DICT,\n        ),\n    )\n\n    injection_handler = enpheeph.handlers.InjectionHandler(\n        injections=[fault_1, monitor_1],\n        library_handler_plugin=pytorch_handler_plugin,\n    )\n\n    # we delay the instantiation of the callback to allow the saving of the\n    # current configuration\n    callback = enpheeph.integrations.pytorchlightning.InjectionCallback(\n        injection_handler=injection_handler,\n        storage_plugin=storage_plugin,\n        # this config used to contain the complete system config: trainer + model +\n        # dataset, including the configuration for injections\n        # extra_session_info=config,\n    )\n    return callback\n\n\ndef get_trainer_config(args=sys.argv) -> typing.Dict[str, typing.Any]:\n    config = pathlib.Path(args[1]).absolute()\n\n    sys.path.append(str(config.parent))\n\n    module_name = config.with_suffix(\"\").name\n\n    module = importlib.import_module(module_name)\n\n    sys.path.pop()\n\n    config_dict = module.config()\n\n    return config_dict\n\n\ndef main():\n    config = pathlib.Path(sys.argv[1]).absolute()\n\n    sys.path.append(str(config.parent))\n\n    module_name = config.with_suffix(\"\").name\n\n    module = importlib.import_module(module_name)\n\n    sys.path.pop()\n\n    config_dict = module.config()\n\n    trainer = config_dict[\"trainer\"]\n    datamodule = config_dict[\"datamodule\"]\n    model = config_dict[\"model\"]\n\n    injection_callback = get_injection_callback()\n    trainer.callbacks.append(injection_callback)\n\n    injection_callback.injection_handler.activate()\n    injection_callback.injection_handler.deactivate(\n        [\n            inj\n            for inj in injection_callback.injection_handler.injections\n            if isinstance(inj, enpheeph.injections.abc.FaultABC)\n        ]\n    )\n    # print(injection_callback.injection_handler.active_injections)\n    trainer.test(model, datamodule=datamodule)\n\n    injection_callback.injection_handler.activate()\n    # print(injection_callback.injection_handler.active_injections)\n    trainer.test(model, datamodule=datamodule)\n\n    injection_callback.injection_handler.deactivate(\n        [\n            inj\n            for inj in injection_callback.injection_handler.injections\n            if isinstance(inj, enpheeph.injections.abc.FaultABC)\n        ]\n    )\n    # print(injection_callback.injection_handler.active_injections)\n    trainer.test(model, datamodule=datamodule)\n\n\nif __name__ == \"__main__\":\n    main()\n"}},"msg":"fix(scripts): update injection_script\n\ninjector_scripts has been updated to support injection for the\nMobileNet model on CARLA driving dataset.\n\nAdditionally, the name of the injection callback functions have been\nupdated to match the different possibilities."}},"https:\/\/github.com\/LBJ-Wade\/dmionhist":{"cb801bb2cec66b97fc6a5794b280f507566b53b8":{"url":"https:\/\/api.github.com\/repos\/LBJ-Wade\/dmionhist\/commits\/cb801bb2cec66b97fc6a5794b280f507566b53b8","html_url":"https:\/\/github.com\/LBJ-Wade\/dmionhist\/commit\/cb801bb2cec66b97fc6a5794b280f507566b53b8","message":"odeint integrator set to correctly solve very large ionizations. f(z) and injection rate functions correctly implemented. matplotlib style sheet included.","sha":"cb801bb2cec66b97fc6a5794b280f507566b53b8","keyword":"function injection correct","diff":"diff --git a\/TLA.py b\/TLA.py\nindex 505c1dd..163353c 100644\n--- a\/TLA.py\n+++ b\/TLA.py\n@@ -1,5 +1,6 @@\n from numpy import *\n from cosmo import *\n+from scipy.integrate import odeint\n \n def comptonCMB(xe, Tm, rs): \n \t# Compton cooling rate\n@@ -26,11 +27,17 @@ def CPeebles(xe,rs):\n \n def getTLADE(fz, injRate):\n \n-\tdef TLADE(var, rs):\n+\tdef TLADE(var,rs):\n \n \t\tdef xe(y): \n \t\t\treturn 0.5 + 0.5*tanh(y)\n \n+\t\tdef dydz(y):\n+\t\t\treturn (2*cosh(y)**2) * dtdz(rs) * (CPeebles(xe(y),rs)*(alphae(Tm)*xe(y)**2*nH*rs**3 - \n+\t\t\t\t\tbetae(TCMB(rs))*(1-xe(y))*exp(-lyaEng\/Tm)) -\n+\t\t\t\t\t\tfz['HIon'](rs,xe(y))*injRate(rs)\/(rydberg*nH*rs**3) - \n+\t\t\t\t\t\t(1 - CPeebles(xe(y),rs))*fz['HLya'](rs,xe(y))*injRate(rs)\/(lyaEng*nH*rs**3))\n+\n \t\tTm, y = var\n \n \t\t# dvardz = ([\n@@ -44,15 +51,13 @@ def xe(y):\n \t\tdvardz = ([\n \t\t\t(2*Tm\/rs - \n \t\t\tdtdz(rs)*(comptonCMB(xe(y), Tm, rs) + \n-\t\t\t\t1\/(1 + xe(y) + nHe\/nH)*2\/(3*nH*rs**3)*fz['Heat'](rs)*injRate(rs))),\n-\t\t\t(2*cosh(y)**2) * dtdz(rs) * (CPeebles(xe(y),rs)*\n-\t\t\t\t(alphae(Tm)*xe(y)**2*nH*rs**3 - \n-\t\t\t\t\tbetae(TCMB(rs))*(1-xe(y))*exp(-lyaEng\/Tm)) -\n-\t\t\t\tfz['HIon'](rs)*injRate(rs)\/(13.6*nH*rs**3) - \n-\t\t\t\t(1 - CPeebles(xe(y),rs))*fz['HLya'](rs)*injRate(rs)\/(lyaEng*nH*rs**3)\n-\t\t\t\t)])\n+\t\t\t\t1\/(1 + xe(y) + nHe\/nH)*2\/(3*nH*rs**3)*fz['Heat'](rs,xe(y))*injRate(rs))), dydz(y)])\n \t\t\n \t\treturn dvardz\n \n \treturn TLADE\n \n+def getIonThermHist(initrs,initCond,fz,injRate,rsVec):\n+\n+\tionThermHistDE = getTLADE(fz,injRate)\n+\treturn odeint(ionThermHistDE,initCond,rsVec,mxstep=500)\n\\ No newline at end of file\ndiff --git a\/__pycache__\/TLA.cpython-35.pyc b\/__pycache__\/TLA.cpython-35.pyc\nindex 1f4e487..ba02c96 100644\nBinary files a\/__pycache__\/TLA.cpython-35.pyc and b\/__pycache__\/TLA.cpython-35.pyc differ\ndiff --git a\/__pycache__\/cosmo.cpython-35.pyc b\/__pycache__\/cosmo.cpython-35.pyc\nindex afeec46..b391b55 100644\nBinary files a\/__pycache__\/cosmo.cpython-35.pyc and b\/__pycache__\/cosmo.cpython-35.pyc differ\ndiff --git a\/cosmo.py b\/cosmo.py\nindex e794963..58ea8cb 100644\n--- a\/cosmo.py\n+++ b\/cosmo.py\n@@ -7,6 +7,7 @@\n c           = 299792458e2                 # speed of light in cm\/s\n kB          = 8.6173324e-5                # Boltzmann constant in eV\/K\n alpha       = 1\/137.035999139             # fine structure constant\n+ele         = 1.60217662e-19\n \n # Atomic and optical physics\n \ndiff --git a\/dmionhist.ipynb b\/dmionhist.ipynb\nindex 6eba637..a09e7e0 100644\n--- a\/dmionhist.ipynb\n+++ b\/dmionhist.ipynb\n@@ -9,18 +9,21 @@\n    \"outputs\": [],\n    \"source\": [\n     \"from numpy import *\\n\",\n-    \"from scipy.integrate import odeint\\n\",\n     \"from cosmo import * \\n\",\n     \"from TLA   import *\\n\",\n     \"import matplotlib.pyplot as plt\\n\",\n-    \"%matplotlib inline\"\n+    \"%matplotlib inline\\n\",\n+    \"\\n\",\n+    \"plt.style.use('tableau10.mplstyle')\"\n    ]\n   },\n   {\n    \"cell_type\": \"markdown\",\n    \"metadata\": {},\n    \"source\": [\n-    \"# Three-Level Atom Integration\"\n+    \"# Three-Level Atom Integration\\n\",\n+    \"\\n\",\n+    \"This part of the code \"\n    ]\n   },\n   {\n@@ -29,12 +32,47 @@\n    \"metadata\": {\n     \"collapsed\": false\n    },\n+   \"outputs\": [],\n+   \"source\": [\n+    \"def getfz():\\n\",\n+    \"    def fzHIon(rs,xe):\\n\",\n+    \"        return (1-xe)\/3\\n\",\n+    \"    def fzHeIon(rs,xe):\\n\",\n+    \"        return 0\\n\",\n+    \"    def fzHLya(rs,xe):\\n\",\n+    \"        return (1-xe)\/3\\n\",\n+    \"    def fzHeat(rs,xe):\\n\",\n+    \"        return (1+2*xe)\/3\\n\",\n+    \"    return {'HIon':fzHIon, 'HeIon':fzHeIon, 'HLya':fzHLya, 'Heat':fzHeat}\\n\",\n+    \"\\n\",\n+    \"def getinjrate(injType,injFac):\\n\",\n+    \"    def injrate(rs): \\n\",\n+    \"        if injType == 'sWave':\\n\",\n+    \"            return injFac*(rs**6)\\n\",\n+    \"        elif injType == 'decay':\\n\",\n+    \"            return injFac*(rs**3)\\n\",\n+    \"    return injrate\\n\",\n+    \"\\n\",\n+    \"initrs = 3000\\n\",\n+    \"initCond = [TCMB(initrs), 5]\\n\",\n+    \"rsVec = flipud(logspace(1,log10(initrs), num=2000))\\n\",\n+    \"\\n\",\n+    \"ionThermHist = getIonThermHist(initrs,initCond,getfz(),getinjrate('decay',1\/1e24),rsVec)\\n\",\n+    \"stdIonThermHist = getIonThermHist(initrs,initCond,getfz(),getinjrate('decay',1\/1e24),rsVec)\\n\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 4,\n+   \"metadata\": {\n+    \"collapsed\": false\n+   },\n    \"outputs\": [\n     {\n      \"data\": {\n-      \"image\/png\": \"iVBORw0KGgoAAAANSUhEUgAAAhUAAAFsCAYAAAB7IUvqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\\nAElFTkSuQmCC\\n\",\n+      \"image\/png\": \"iVBORw0KGgoAAAANSUhEUgAABNAAAAKMCAYAAAAqrkVTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\\nAAALEgAACxIB0t1+\/AAAIABJREFUeJzs3Xl4VNX9x\/HPnYSQlRAShEAgLEIQWUQxAlE2FYtWwaWK\\nigGRn4BULVqtFIViEWvRAi4ggghYrVpR3NlksYJsIosoYQkghHWG7Htm7u+PyCgSAkkmcyeZ9+t5\\neBzmbp+Yo\/fMd849xzBN0xQAAAAAAACAMtmsDgAAAAAAAAD4MgpoAAAAAAAAQDkooAEAAAAAAADl\\noIAGAAAAAAAAlIMCGgAAAAAAAFAOCmgAAAAAAABAOSigAQAAoEZ69tlnZbPZ9NBDD1kdBQAA1HIU\\n0AAAAFDjrFu3TrNnz1bnzp2tjgIAAPwABTQAAADUKJmZmRo8eLDeeOMN1a9f3+o4AADAD1BAAwAA\\nQI1y\/\/336\/bbb1evXr2sjgIAAPxEoNUBAAAAgPM1e\/Zspaam6j\/\/+Y\/VUQAAgB9hBBoAAAA8YuHC\\nhXrooYfUs2dPRUZGymazKTk5udxj0tLSNGzYMDVt2lTBwcFq2bKlxowZo4yMjDP23bVrl8aNG6e3\\n335bNhvdWAAA4D2GaZqm1SEAAABQ83Xp0kXbtm1TeHi44uLitHPnTt19991asGBBmfunpqaqe\/fu\\nstvtGjhwoBISErRhwwatWLFC7dq105o1axQVFeXef\/78+Ro2bNhpxTOn0ynDMBQQEKDc3FzVqVOn\\n2n9OAADgf3iEEwAAAB4xbdo0xcXFqXXr1lq9erX69OlT7v6jRo2S3W7XSy+9pAceeMD9\/qOPPqqp\\nU6dq3LhxmjFjhvv9m2++WZdffvlp5xg6dKjatm2rcePGUTwDAADVhhFoAAAA8LhTBbTBgweXOQIt\\nNTVVF154oVq2bKm9e\/eeti0nJ0exsbGSpOPHjyskJOSs1+nTp486duyoF1980bM\/AAAAwK8weQQA\\nAAC8buXKlZKkfv36nbEtPDxcSUlJysvL07p168o9j2EY1ZIPAADg1yigAQAAwOtSUlJkGIbatm1b\\n5vY2bdpIKl04oDwrVqxg9BkAAKh2FNAAAADgdZmZmZKkyMjIMrefer+s1TgBAAC8jUUEvIhHDAAA\\n8A9MMetb6IMBAFD7VXf\/iwKal9Ghhi9LSUlRQkKC1TGAs6KNoiagWHN+To0wOzUS7bdOvV+\/fn2P\\nXI8+GHwZ9zf4OtoofJ03+l88wgkAAACvS0hIkGmaZ53jbPfu3ZJ01jnSAAAAvIkCGgAAALyuT58+\\nkqSlS5eesS0nJ0dr1qxRaGiounXr5u1oAAAAZ6CABgAAAK9r1aqV+vXrp\/379+vll18+bdv48eOV\\nm5ur5ORkhYSEWJQQAADgF8yBBgAAAI\/46KOPtGjRIknS0aNHJUlr167VvffeK0mKiYnRlClT3PvP\\nmDFDSUlJevjhh\/Xll1\/qoosu0rp167Rq1Sq1a9dOkyZN8v4PAQAAUAbDZEZVrzEMgwls4dMcDoei\\no6OtjgGcFW0UNYE\/3+8nTpyop59++qzbW7Roob179572XlpamsaPH6\/FixfL4XAoNjZWt9xyi8aP\\nH+9eaKCq\/Pl3gpqB+xt8HW0Uvs4b93oKaF5E5w0AgNqP+73v4XcCAEDt5o17PXOgAQAAAAAAAOWg\\ngAYAAAAAAACUgwIaAAAAAAAAUA4KaAAAAAAAAEA5KKABcHM4HFZHAMpFGwUA1Ebc3+DraKMABTQA\\nv2K3262OAJSLNgoAqI24v8HX0UYBCmgAAAAAAADVwjRNPb8kRetSGcVX01FAAwAAAAAAqAZ3Pv2G\\nXl65R8lzN2hVynGr46AKKKABAAAAAAB42EPT\/qN1+Y0kSXWchWrTKMLiRKgKCmgAAAAAAAAeNHn+\\nJ\/rocJgkySzI0ZTfx6tp\/RCLU6EqKKABcIuJibE6AlAu2igAoDbi\/gZfRxutmDc++UqzthfLsAXI\\nLC7UxGtidX1SF6tjoYoM0zRNq0P4C8MwxL9uAABqN+73voffCQDAW348nKH+zy+TgkJlupwa2cGm\\nscm\/tzpWreeNez0j0AAAAAAAAKooLSNf9877VgoKlSQNbJJL8awWCbQ6AAAAAAAAQE12MrdIya+v\\n19GsAknSI1e31kPXtrM4FTyJEWgAAAAAAACVlFdUomHzNmrviVxJ0rCklnrwmgSLU8HTKKABAAAA\\nAABUQlGJUw+8tVlbDmZIkgZc0kRP3nCRDMOwOBk8jQIaADeHw2F1BKBctFEAQG3E\/Q2+jjZatpIS\\np6585FWtSjkhSerZtqGm3NZZNhvFs9qIAhoAN7vdbnUEoFy0UQBAbcT9Db6ONlq26\/\/6mo6HtpAk\\nNQrM18y7L1VQIGWW2orfLAAAAAAAQAXc9fRc7bI1lyQZ2cf15v\/1UFhd1mmszSigVdGMGTPUqlUr\\nhYSEqGvXrvr666+tjgQAAAAAAKrJIy++o7V5jSRJZl66\/jPySrWNb2JxKlQ3CmhV8O677+pPf\/qT\\nnnzySW3ZskU9evRQ\/\/79dejQIaujAQAAAAAAD1u6PU0fHAqVJJmFuXr5tovUrWMbi1PBGyigVcHU\\nqVM1bNgwDRs2TAkJCXrxxRcVGxurmTNnWh0NAAAAAAB40Oaf0vXwe9slW4DkLNFTvRvqxp6XWR0L\\nXkIBrZKKi4v17bff6tprrz3t\/X79+mnt2rUWpQKqJiYmxuoIQLloowCA2oj7G3wdbVTaczxbw+Zt\\nVH6xUzZDmjUkUcMH9LE6FryIAlol2e12OZ1ONWrU6LT3GzVqpKNHj1qUCqia6OhoqyMA5aKNAgBq\\nI+5v8HX+3kaPZOYr+fUNysgrliRNvrmjrusQa3EqeFutLaAtXLhQDz30kHr27KnIyEjZbDYlJyeX\\ne0xaWpqGDRumpk2bKjg4WC1bttSYMWOUkZHhpdQAAAAAAMBXHD2ZreTXN+hwZoEk6c\/92mpQYnOL\\nU8EKtXaN1UmTJmnbtm0KDw9XXFycdu7cWe7+qamp6t69u+x2uwYOHKiEhARt2LBB06dP15IlS7Rm\\nzRpFRUW594+JiVFAQICOHTt22nmOHTumxo0bV8vPBAAAAAAAvMORma2e495SUWQzSdLQHi00us+F\\nFqeCVWrtCLRp06Zp165dyszM1IwZM2SaZrn7jxo1Sna7XS+99JIWLlyoyZMna\/ny5RozZox27typ\\ncePGnbZ\/nTp1dNlll2nZsmWnvb9s2TIlJSV5\/OcBAAAAAADekVdQqN5jF7iLZ+1C8zT+9+1lGIbF\\nyWAVwzxXZakWWL16tfr06aPBgwdrwYIFZ2xPTU3VhRdeqJYtW2rv3r2nbcvJyVFsbOmzzcePH1dI\\nSIh723vvvafk5GS98sorSkpK0syZM\/XGG29ox44datas2RnXMQzjnIU8AABQs3G\/9z38TgAAFeFy\\nudRjzEwdDWkhSQrJ+kkbpgxVRFhI+QfCMt6419faEWgVsXLlSkmlK2j+Vnh4uJKSkpSXl6d169ad\\ntu3222\/XtGnT9Mwzz6hLly5au3atvvjiizKLZ0BN4HA4rI4AlIs2CgCojbi\/wdf5Wxu94a+vuYtn\\ngVlpWvn0IIpnqL1zoFVESkqKDMNQ27Zty9zepk0bLVu2TLt27VKfPqcvUzty5EiNHDnSGzGBame3\\n2\/1+hR34Nl9oo6Zp6nBmgdLS83Uiu1DpeUUyTVPrN2xUfn6+TJkyTenUF2CXX365gkNC3N+ImaZk\\nytT69euVl18gmaZM\/bJ\/4hVXKDg4WKZput8zJa1bt055+fmSfvl2zZSUmJio4OAzO3Tr169XfkH+\\nGe9fccUVGnhZS3WMi\/TsvxgAQKX5wv0NKI8\/tdFXlv+oH\/XzoJicE\/r08RvUOCaq\/IPgFyigScrM\\nzJQkRUaW\/WHi1PusxgkA\/mv5hu2a+tl3OhHURMezC8vYI\/TnP6db9eW+s5wx8uc\/p1u\/+sBZ9m9Q\\n5rvbvkk7y\/5ld3K\/\/yZNbZs0oIAGAADwG4u+S9OU5amSpMDiXC34v+5q16KpxangKyigeVlKSsoZ\\n78XExJRZzXc4HLLb7ezP\/l7bPzU11afysD\/7\/3b\/37ZRb+SJjKyvWye8ri1FjWQEREuFhXLmZ8mV\\nl3XG\/rbQegoIqXfG+9W5vyHJVZAlV372ae+7nCUyQs7cPyAgQDkZGUpJOXN0mtW\/39q2PwAAqDm+\\n2nVCf\/7vVklSRN1AvfPQdbq4CV844hcsIiDp8ccf1wsvvKDnn39eY8aMOWP7gw8+qBkzZmjGjBka\\nMWJEpXMwgS18XUpKihISEqyOAZyVt9toSYlTSY+8qmOhLSRJpulSfN18Del3uS68IFwXRNRVg7Ag\\n2QxD6SdPyjRdMgxDNsOQYSv9Z2RkpAJsAZIhGUZpwcswDOXl5Upm6Wvbz\/vaDENBQUGyBdhkyHDv\\nL5VOZmv7eV\/DMFgByodxv\/c9\/E7g6+iDwdfV9ja65WCG7pq9TnlFTgUF2jT\/3kR1b+0fj6zWFt64\\n1zMCTVJCQoJM09SuXbvK3L57925JOuscaQCA2umW8XPcxTMj+7heuPVi3do3scx9G0bEVujc4XUr\\n+I1mAOv+AAAAeNrGlJ804r0U5RU5ZRjSi4MuoXiGMlFAk9wLAyxduvSMbTk5OVqzZo1CQ0PVrVs3\\nb0cDvComJsbqCEC5vNlGF63doW2uOEmSkX1MSx7\/ndrGN\/Ha9QEA\/oM+GHxdbW2jW3cd0B9eXi2F\\nlRbM\/j6gg37XoWJfisJ\/8HW2pFatWqlfv37av3+\/Xn755dO2jR8\/Xrm5uUpOTlZICMvWonbzl5V1\\nUHN5q40Wlbj00jcnJEmGs1iv3dOV4hkAoNrQB4Ovq41t9Kejdt08fbm7eHZN40IN7hZvcSr4slo7\\nB9pHH32kRYsWSZKOHj2qJUuWqFWrVrrqqqsklVbQp0yZ4t4\/NTVVSUlJOn78uG666SZddNFFWrdu\\nnVatWqV27dppzZo1ioqq2tK1zL8BADXD\/LX7NeHjHZKkJ\/q308herS1OhJqE+73v4XcCAPi19Kwc\\ndfvLv1UY2UyS1LzogFY9P1I2G2OMaipv3OtrbQFt4sSJevrpp8+6vUWLFtq7d+9p76WlpWn8+PFa\\nvHixHA6HYmNjdcstt2j8+PGKjKz66ht03gDA9xUUO9VrykodyypUfHSolj\/SS3WYfwwVwP3e9\/A7\\nAQCcUlBYpMRHZisrooUkKSpnv9ZPHaGgOsxwVZNRQKtl6LwBgO97e\/1P+uuH2yVJU27rpD90bWZx\\nItQ03O99D78TAIAkmaapMf\/ZpEXbjkuSgrN+0sYpQxURxnRNNZ037vV8pQ4AwM9M09SCb\/ZLkmIj\\ngzWwS1NL8wAAAMBzXli6y108i3BmavmE2yme4bxRQAPg5nA4rI4AlKu62+ibX6zRzqPZkqS7r2jO\\no5sAAK+gDwZfVxva6Lw1+\/Tyyj2SpOYNQvXl+FsU16j2LY6A6sMnAwBudrvd6ghAuaq7jb6yZKsk\\nyXQW67o29ar1WgAAnEIfDL6uprfRT7Ye1sRPf5AkxYQHacGwRF0QEWxxKtQ0zJIHAIBKV2M6GtBI\\nhqSo\/DS1adbI6kgAAACoore\/\/FYTVhyTaUrhdQM1795EtYgJszoWaiAKaAAASPrXO0tlBJXOgTHw\\nEuY+AwAAqOneXfaNxn6RJiMoRHUCDL12z2Xq0DTS6liooXiEEwAASR9vPSJJMgvzNOaOay1OAwAA\\ngKpYuWmHHv90v4ygEJmmS\/ddHKgeF8ZYHQs1GAU0AIDfs2dkKyMkVpIU6zquyAiG9QMAANRU2\/f8\\npHvnbZIRUjqn7TVR6Xrirn4Wp0JNRwENgFtMDN\/IwLdVVxvdnJYrI7CuJOmunhdXyzUAADgb+mDw\\ndTWpjR465tCAfy2Rwkszd7Id0utPJFucCrUBc6ABcIuOZhln+LbqaqNf\/nhcklQ30Kb\/u\/GqarkG\\nAABnQx8Mvq6mtNGCYqcefn+HXPWaSJLiCvdr0QujLE6F2oIRaAAAv+ZymfpyZ2kB7coLYxQSFGBx\\nIgAAAFSU02XqT+9s0bcHsyVJLepk6svn7pfNRtkDnsEINACAX9t6KEP2nEJJ0tUXNbI4DQAAACrK\\nNE099dH3WrzjqCQpsWUDLRj2O9Wtwxej8BxKsQAAv\/bVLrv7dd92F1iYBAAAAJUxbfluvb3+J0lS\\nu8YRmp3cVcEUz+BhjEADAPi1FT+kSZLaXBCuxpHBFqcBAABARTw1f6ne\/LFYkhQXFaL5wxIVGVLH\\n4lSojRiBBsDN4XBYHQEol6fb6In0LG05mCFJii6xn2NvAACqB30w+DpfbaNPvfaBFvxQOhVHVGgd\\nLRiWqEb1+EIU1YMCGgA3u50CAnybp9vogi\/WyAgo\/YayQ0O+qQQAWIM+GHydL7bRGe8v14JdNhmG\\nTWZRgZ68KkqtGoZbHQu1GAU0AIDfWrJlvyTJdDk15Poka8MAAADgvCxcsUHPrU2XEVhHprNYjySG\\n69Y+l1sdC7UcBTQAgN\/am1M6FWhQzhE1axRtcRoAAACcy9dbUvTIR3tkBIVKkga1LNbDg66zOBX8\\nAQU0AIBfOnDkhEoiGkuS2kZaHAYAAADndCK7UGOXHJIRUtp5613vhJ574A8Wp4K\/YBVOAIBfWvX9\\nTzKM0u+Rru7Y3OI0AAAAKE92QbGGvrFBB9MLJEm9LyjUvEeGWhsKfoUCGgC3mJgYqyMA5fJkGz3m\\nDJMkGZKG\/L6nx84LAEBF0QeDr7O6jRaWODXizW+143CWJOn2rnF67tZOlmaC\/+ERTgBu0dHMAQXf\\n5sk2uulAuiQpoXGEouuFeey8AABUFH0w+Dor26jTZeqRd7dq7V6HJOmaiy7Q5Js7yjAMyzLBPzEC\\nDQDgd4pKXNp6MEOS1LVFlMVpAAAAUBaXy6VB\/1yojRmlCwZ0jY\/SS3deqsAAxgLB+2h1AAC\/s+Nw\\npgpLXJKkrvENLE4DAACAstw24XV38ax1TKheH3K5QoICLE4Ff0UBDQDgd779+fFNiRFoAAAAvmjU\\n8\/\/W5uImkiQz16EXbmyhyNA6FqeCP6OABgDwO4u+3iZJuiAiSE3rh1icBgAAAL828fVF+vx4PUmS\\nqyBbc+6+RJcktLQ4FfwdBTQAbg6Hw+oIQLk81Ua3H86WJBUf2cUEtAAAy9EHg6\/zZhud\/\/kazf3R\\nlGELkFlcoMnXxenabqy4CetRQAPgZrfbrY4AlMsTbfSH1EMywkrnPUtoGFzl8wEAUFX0weDrvNVG\\nfzySpefX58gIDJLpLNFDl4ZqcP8rvXJt4FxYhRMA4Fc++nqLpNJRZ1e1b25tGAAAAEiSDp7M05C5\\nG5RdWCJJeqBrPT16Rx+LUwG\/oIAGAPAr61IOS2oqSbrxqi7WhgEAAIAcOYVKnrtBx7MLJUlP3nCR\\nhl\/VyuJUwOl4hBMA4Ff2ppd+q2lkH1fzxjEWpwEAAPBvOYUlunfeRu2z50qSRvRsRfEMPokRaAAA\\nv2GaplxRcZJTahZudRoAAAD\/lpNXoH6TPtBhV6Qk6dZL4\/RE\/3YWpwLKxgg0AG4xMYzGgW+rahtN\\ny8hXnjNAkjTkxt4eSAQAQNXRB4Ovq442WlLiVO+\/vO4unl3Zqr7+cWtHVkiHz6KABsAtOjra6ghA\\nuaraRrcdynS\/7hwXWdU4AAB4BH0w+DpPt1GXy6V+T8ySPayFJKlO1iFNu72T6gRQooDvonUCAPzG\\nqQKazZDaN6lncRoAAAD\/NGjiXKUGxkuSjOxjWvzXAYqpH2FxKqB8zIEGAPAbOw6XFtAuvCBcoUHc\\nAgEAALztqbmfaUNhrCTJzD2p90f3Uuu4xhanAs6NEWgAAL\/x45FsSdJFsYw+AwAA8LblPxzT23tK\\n5zgzC3I0686O6tq+tcWpgPPD1+8AAL+wc\/9h2XMKJUmN6zotTgMAAOBfNu0\/qdFvb5bTZSq4jk3\/\\nvOli\/a5HB6tjAeeNEWgA3BwOh9URgHJVpY0uXrfd\/bpu\/glPxAEAwCPog8HXVbWNphzN1rB5G1VY\\n4lKAzdCMuy\/VTRTPUMN4vYC2fPlyLV26VC6X67T3p02bJtM0vR0HwK\/Y7XarIwDlqkob3bArzf36\\nd906eiIOAAAeQR8Mvq4qbfRQep6S565XVkGJJOm5Wzupb7tGnooGeI1XC2hjx47VrFmz9Oabb+qW\\nW245bVvnzp31xz\/+0ZtxAAB+ZI89X5Jk5meqfas4i9MAAADUfnt+OqJ+z3ysY1ml02g80b+dbruM\\nfhhqJq8W0FJTU\/Xf\/\/5Xb775pjp37qyvv\/7ava1Pnz5q06aNVqxY4c1IAAA\/YS+pK0kKK86wOAkA\\nAEDtdyI9S\/3\/8bHyAiMkSXd3jdWInq0sTgVUnlcLaI0b\/7I07ahRo\/T555+ftv2BBx7Qf\/\/7X29G\\nAgD4gezcfDnDGkqSmkUw\/ScAAEB1yisoVO9x\/1ZxvdLRZg1z92viwM4yDMPiZEDlefVTxE8\/\/aTc\\n3FxJpcW0jIzTRwEEBQV5Mw4AwE\/stefJCChdeLpvl7YWpwEAAKi9Skqc6v34HOXWi5ckhWUd0Mrn\\n7lNgYIDFyYCq8WoB7fe\/\/71uu+025eTknHUfp9PpxUQAfi0mJsbqCEC5KttGT81\/JkkDe1\/uqTgA\\nAHgEfTD4uoq00Xv+9YGOh7aQJAVmpWnlpLsVHhpcTckA7\/FqAW3o0KHKyclR69at9dRTT51RSNu1\\na5cOHDjgzUgAfiU6OtrqCEC5KttGfzySJUkKCrCpVcMwT0YCAKDK6IPB151vG33tq7365mSoJMnI\\nPq7FT9yoCxpEVmc0wGsCvXmxgIAAffDBB7r55pv1zDPPyDAMffHFF0pISFBRUZG2b9+uhQsXejMS\\nAMAPpBzNliRdeEG46gQwBxoAAICnLfz2kCZ\/vlOS1KheXb0+4ne6sHmsxakAz\/H6p4iGDRtq9erV\\neu2119S9e3c5nU5t27ZNkZGRWrx4sa6\/\/npvRwIA1HK7j5cW0BIaR1icBAAAoPZZsfOYHl+4TZJU\\nLzhQC4ZdoQ4tKZ6hdvHqCLRTAgICNHz4cA0fPtyKywMA\/EhmfrGOZRVKKh2BBgAAAM\/Z\/FO6Hnhr\\ns5wuU3UDbXp96OV8aYlayZICGgAA3vLJ6o3u1+GubAuTAAAA1C5LvtmqUQt3yxUYIpshvXzXpbq8\\nRQOrYwHVwmOPcB4\/flw33XSTunbtqk8++cRTpwXgRQ6Hw+oIQLkq00a\/2bHP\/bp5ZJAn4wAA4BH0\\nweDrymqjm37Yq\/vf3iZXYIgk6W83JOja9o28HQ3wGo8V0P785z9r5cqV2rx5s+6++245nc7TtpeU\\nlKh9+\/aaPHmysrMZAQD4IrvdbnUEoFyVaaM7j2RKksySIvXo1NbTkQAAqDL6YPB1v22j+9KO6Q+v\\nrJIRVjra7PK6R5R85YVWRAO8xmMFtB9++EEzZ86UYRgKCgqSy+U6bXtgYKBWrFihwsJCJSUlaefO\\nnZ66NAAAZ3Ukr\/SfAXl21Q2qY20YAACAGs6Rma1+kz6UGdFYktSq5IDenTDM4lRA9fNYAa2kpESD\\nBw\/W7t27tW3bNtWpc+aHlMaNG2vixImaM2eObrvtNqWlpXnq8gAAlCk3oHQS2wYBRRYnAQAAqNmK\\nnS7d9q8vVBzZTJIUnbtfS\/8xQjabx0oLgM\/yWCvv1KmTFi9erFatWqlJkybl7puYmKjnnntO48eP\\n99TlAQA4w+ETJ92PFsRHMf8ZAABAZZmmqbEfbNe+wjBJUkjWAa1+7j4FBgZYnAzwDo8V0KZOnaqn\\nnnpKL7zwgnJzc8+5\/w033KBNmzZ56vIAAJzhaN4vr69PutS6IAAAADXcc4tT9P63hyRJHZtGas1z\\nQxUeGmxxKsB7PFZAi46O1pw5c\/S3v\/1NsbGxuvHGGzV16lRt3br1rMcUFxd76vIAPCAmJsbqCEC5\\nKtpG95z45QudXl0SPB0HAACPoA8GX\/fF7hy9unqvJKllTJjeuPdyNYgItTgV4F2BnjrRjh071KtX\\nL\/fos88++0yff\/65JKlBgwbq3bu3evfura5duyosLExz5sxRw4YNPXV5AB4QHR1tdQSgXBVto3uO\\n50iS6gQYahFNJw8A4Jvog8GXffDtQU37+qgkqWFEXS0YlqiY8LoWpwK8zzBN0\/TEiW688UbZbDbd\\nd999ys7O1ubNm7Vq1Spt2bJFpy5hGIZ7\/8DAQC1ZskS9e\/f2xOVrBMMw5KF\/3QCA83DvGxu0MuWE\\nEhpFaMmYnlbHgZ\/gfu97+J0AQOW88PYXemlLsWQLUETdQL07orvaN6lndSzgDN6413tsBNqePXu0\\nY8cO9+obd999tyTp5MmTWrlypb788kstX75ce\/bsUVhYmD755BO\/Kp4BALxv78+PcLa+IMziJAAA\\nADXLW198rRc35ckICpacxZp5d1eKZ\/BrHpsDLSQkpMylaxs0aKBbb71VM2bM0K5du7Rz507ddddd\\n+vvf\/66CggJPXR4AgNMUFJfoUHrpKgKtYsItTgMAAFBzLN+wXX9dckhGULBMl1NDE6Qr2zayOhZg\\nKY8V0Dp06KDVq1efc7+2bdtq1qxZuvPOO\/XEE0946vIAAJxm1cbv5fp5FHfxyUPWhgEAAKghtqTs\\n031vficjOEKSdP0FWfrb8IEWpwKs57EC2vjx4\/XQQw\/p8OHD5e5XXFys\/Px8DR8+XCtXrvTU5QF4\\ngMPhsDoCUK6KtNENOw+4Xzerz0S3AADfRR8MviIzr1jD5m+WEVa6sMWldQ5r5p8H00YBebCAduGF\\nF2r69Om65ppr9M4775Q5eZtpmurQoYMuuOACff755woKCvLU5QF4gN1utzoCUK6KtNEfD\/6yb1Kn\\nNtURBwAAj6APBl9QUOzU8AUbddIVLEmKLzqg9yfeJ4k2CkgeLKBJUu\/evbV8+XItXbpUrVu3Vm5u\\n7mnbS0pK9NNPPykvL09Dhw5V8+bNPXl5AADcDqSXzrNp5mepZVPm7AAAADibEqdLf3z7O23cny5J\\nur5jY305ZWSZ85wD\/srj\/zU0adJEc+fO1Y4dOxQWdvqqZ3Xq1NGrr76qBg0a6IILLtA\/\/vEPT18e\\nAABJkqO1Hvm1AAAgAElEQVQwQJJUtyjT4iQAAAC+yzRNjfvwey3\/8ZgkqUfraE294xIFBlA8A34t\\nsLpOHBISUub7Q4YM0ZAhQ6rrsgAASJIKgurJkBQd5LQ6CgAAgM969rPv9e6mg5Kk9rH1NOuey1Q3\\nMMDiVIDvqbYCGgAAVskqKJYREilJ+n2vRIvTAJ6Tl5fn9WuGhoZ6\/ZoAAO+4d\/I8rcxqKElq3iBU\\n84ZdrojgOhanAnwTBTQAbjExMVZHAMp1vm1034lf5uC8rG1cdcUBvC48PFyGYXjteoZhqKSkxGvX\\nA\/wVfTBYYezM97UiM1qGIRmFOZp\/b09dEBFc5r60UYACGoBfiY6OtjoCUK7zbaP77L8U0FrGhFdX\\nHMASZa10DqBmow8Gb5v+7hK9nRooI8AmsyhPL9zYUi0bRpx1f9ooQAENAFALpf5cQDMMKT6ax89Q\\nuxiGoUaNGlXrhxmHw6GjR49W2\/kBANZ5Z+k3+tf6bBlBITJLivV49\/q6tS9TXgDnQgENAFDrnBqB\\n1rR+iILrMAkuap8\/\/\/nPeuSRR6rt\/C+88IIee+yxajs\/AMAa++y5+tvK46XFM9Ole9q6NPoP11od\\nC6gRWJcWAFDr7DqSIUlqGRNmcRIAAADfcDyrQMlz16vALB1H069Buibdf4vFqYCagwIaAKBWcblc\\n2pl2UpJ08sBOi9MAAABYL6ugWEPe2KiDJ\/MlSQ9d3Uaz\/5JscSqgZuERTgBuDoeDCULh086njW7f\\nc1BGndIVpBqWvZAUUGM98MADMgxDXbp0qdbrdOnSRaNHj67WawD4BX0wVKeCYqf+b\/4m\/XgkS5J0\\nZ2JzjbmmTYXOQRsFKKAB+BW73c6NET7tfNroxh9T3a\/bN2fJddQuL7\/8sleu07dvX\/Xt29cr1wJA\\nHwzVp6i4RCPnr9f6faXTW\/zu4saaNLCDDMOo0HloowAFNABALfP9\/qOS6kmSLm3b3NowAAAAFnG5\\nXOr7+CwdqttCkpTYsoGmDbpEAbaKFc8AlGIONABArbL3WOnjCabp0hUXX2hxGqDmmD17ttURAAAe\\ndPNTc9zFs6Dc45p9z2WsTg5UAQU0AECtkumsU\/oiL10RYSHWhgGqwdixYz1+zjlz5mjUqFEePy8A\\nwBr\/99wCbXU2Lf1Ljl0LH75akaFB1oYCajiPPcL5448\/6rvvvtPx48eVk5Mjl8t1XseNHz\/eUxEA\\nAFCD5gn66WCGLm\/X0uooQLV47rnn1LBhQz3yyCMeOd+8efM0YsQIj5wLAGC9p177QEtPRskwJDM\/\\nS\/OGdlXHC5nWAqiqKhfQ3nnnHT399NNKSUmp1PEU0ADfERPDhOvwbefTRg+ezJMktW4UWd1xAMs8\\n9thjatCggYYOHVql88yfP1\/Dhw+XaZoVnlAagOfQB4OnrN1r11updWQYNplF+Xruhhbq0\/XiKp+X\\nNgpUsYD22GOP6V\/\/+pckyTTNCh9PRw3wLaysA193rjaaU1giR26RJKl5dKg3IgGWuf\/++xUVFaUB\\nAwZU6vg333xTw4cPdz810K9fP0\/GA1AB9MHgCd+nZer+Bd\/KJZtsMvVwYoQG9evukXPTRoEqFNCW\\nLVumF154QYZhyDRNNWvWTDfddJPatm2riIgIimMAAK874Mh1v27egAIaaqexY8fq2WefVUlJie68\\n8059\/vnn6t27d4XO8fbbb2vYsGFyOp0yDEPXXHONFi1aVD2BAQDV7oAjV0Pf2KicwhIZhjRt0KW6\\nqXMTq2MBtUqlC2gzZ850v3744Yc1ZcoUBQZ6bEo1AAAq7CdHnvt1PCPQUEs988wzOnnypGbNmqWC\\nggINGDBAK1as0GWXXXZex7\/zzjsaOnSou3jWt29fffTRR6pbt241JwcAVIcT2YVKnrtB9pxCSdKE\\n37eneAZUg0qvwrlu3ToZhqFOnTpp6tSpFM8AAJbbmnrY\/Tq+QZiFSYDqNWPGDN1xxx2SpOzsbPXv\\n31+7du0653HvvfeekpOTVVJSIsMw1Lt3b3388ccKDg6u7sgAgGpw+MRJ3fnq\/3Tg5y8RR\/dpraFJ\\nLKQEVIdKF9BOnjwpSerfv7\/HwgAAUBWfrVonSTILcxUZWsfiNED1MQxDb775pq677jpJkt1u17XX\\nXqtDhw6d9Zj3339fgwcPVklJiSSpZ8+e+uSTTxQSEuKVzAAAz8rOzdfVE97VHkfpyLM7ujbTn\/sl\\nWJwKqL0qXUA7NYlgRESEx8IAsJbD4bA6AlCuc7VRe2Hp\/Jt1CjO8EQewVGBgoD744AP16NFDknTw\\n4EFde+21stvtZ+z7wQcf6O6773YXz6666ip9+umnCg3lUWfAF9AHQ0WVlDjV54m5yq\/XXJIUlXdQ\\nz9zcodrmIqeNAlUooHXs2FGSdODAAY+FAWCtsj50Ab7kXG0031b62GY9W7E34gCWCwkJ0WeffaZO\\nnTpJknbt2qX+\/fsrOzvbvc+iRYt01113qbi4WIZhKCkpSZ999pnCwnjMGfAV9MFQUTeMmy17WAtJ\\nUlDmQa2YNFiBAZX+eH9OtFGgCgW05ORkmaapTz75RIWFhZ7MBABAheUVFMoMjZIkNQ4PsDgN4D2R\\nkZFasmSJWrduLUnavHmzBgwYoMLCQn388ccaNGiQioqKJEndunXT559\/rvDwcCsjAwCqYOjkeUox\\nmkmSjOzj+mLcQEXV4\/\/rQHWrdAHtrrvu0jXXXKMjR47o0Ucf9WQmAAAq7NsfU2XYSgtnLRsyvQD8\\nS6NGjbRs2TLFxsZKklavXq0+ffrojjvucBfPrrjiCi1evJjpNwCgBvt8+xGtyoqRJJl5GXp7RJJa\\nxzW2OBXgH6o0xvPdd99Vnz59NHPmTA0YMEDbt2\/3VC4AACok9fgvj6xdHN\/IwiSANVq0aKElS5ao\\nfv36kqT169e7nxK4\/PLLtWTJEopnAFCDbdh3Un96d4skQ3Vtpv41sI26d2prdSzAbwRW9sC+fftK\\nkkpKSmSapj799FN9+umnaty4seLj489rOXTDMPTll19WNgIAAG4BkY0knZAk3di3h7VhAItcfPHF\\nWrx4sa6++mrl5ubKMAxddtllWrZsmerVq2d1PABAJe0+lq3h8zeqqMSlQJuhOfcm6qo2Da2OBfiV\\nShfQVq1a5V7h49Q\/TdPU0aNHdfTo0XMeb5pmta0QAqByYmJirI4AlKu8NnooPV+SFGgz1KR+iLci\\nAV73+OOPn3Ofzp07a82aNTIMQ507d9akSZPO69z\/\/Oc\/qxoPQCXQB0N5jmYWaMjcDcoqKF1J+Z+3\\ndfJ68Yw2CkiGaZpmZQ602aq+wodhGHI6nVU+T01hGIYq+a8bAHAOf3x7sz7ddkTNG4Tqq8f7WB0H\\nfqy67\/c2m63avoSsrf0y+mAAaqpDxxwa\/Ma32p9RusL4Y9claHSfCy1OBfgeb9zrKz0Cbd++fZ7M\\nAQBAlZwagRYXxegz1H7V0UHkyQAA8C3Zufm6ZuJ7KqjXXJJ0T7d4PdC7tcWpAP9V6QJafHy8J3MA\\nAFAlpwpoTXl8E7Xc6NGjrY4AAKhmJSVO9XlirgrqtZAkReX+pL\/ddD1fdgAWqnQBDQAAX5GVmy97\\nTulqg43rBVmcBqheL730ktURAADV7Ia\/viZ7WAtJUlDmQa147h4F2CieAVaq+kRmAABYbP33e9yv\\n01K2WpgEAACgaoZOnqcUW+ljm0b2cX0xbqCi6oVbnAoABTQAbg6Hw+oIQLnO1ka37T3kft2u+QXe\\nigMAgEfQB8Mpn28\/otVZpStemnkZentEklrHNbY4FW0UkDz4CGdJSYmWLl2qr7\/+Wrt379bJkycl\\nSQ0aNFDbtm2VlJSk6667TgEBAZ66JAAPs9vtio6OtjoGcFZna6O70xySoiRJXdo093IqAACqhj4Y\\nJGl9qkN\/eneLTBmqGyBNHthG3Tu1tTqWJNooIHmogDZz5kw988wzOnLkSLn7xcbG6sknn9TIkSM9\\ncVkAACRJBxw5kqJkOkvU8UIKaAAAoGbZdSxb\/7dgk4pKXAq0GZoz9HJd1aah1bEA\/EqVCmhOp1N3\\n3HGHPvzwQ0nnXlL98OHDGj16tJYvX6733ntPNhtPkAIAqu54rlMKk4z8DAXXZREB1F4\/\/PCDpNIv\\nJaOioqrtOunp6e4vRtu3b19t1wEASEczCzR07gZlFZRIkv55WyeKZ4APqlIB7cEHH9QHH3wgwzBk\\nmqaaN2+uW2+9VZdeeqliYkqf27bb7fruu++0cOFCHThwQKZp6sMPP9SDDz6oV155xSM\/BADAv9nC\\nYyRTqhdQbHUUoFp16NBBhmFoypQpeuSRR6rtOnPnztXjjz8uwzBUUlJSbdcBAH936JhDd8\/dpMOZ\\npf+vfey6BN1yaZzFqQCUpdIFtC1btmjWrFkyDEOBgYF6\/vnn9cc\/\/lGGcebSunfffbemTJmiGTNm\\n6NFHH1VRUZFmzZql+++\/X507d67SDwAAgMKjpexCXXfl5VYnAWqNcz1ZAAComuzcfF0z8T0V1Cud\\nfuKebvF6oHdri1MBOJtKP0P5xhtvuDtW8+bN04MPPlhm8ewUwzA0evRozZs3T1Jpp+z111+v7OUB\\nVINTI0cBX1VWGy0odupEdqEkqWlUiLcjAQBQZfTB\/E9JiVN9npjrLp7Vzzmgv910cbmfqa1EGwWq\\nMAJtxYoVMgxDiYmJuvPOO8\/7uEGDBmn69Olav369Vq5cWdnLA6gGrKwDX1dWG03LyHe\/josK9WYc\\nwDJr1qxRcHBwtZ1\/7dq11XZuAGeiD+Z\/bhg3W\/awFpKkoMyDWvlcsgJsvlk8k2ijgFSFAlpaWpok\\nqW\/fvhU+tm\/fvlq\/fr37HAAAVNah9F8X0BiBBv+waNEiLVq0yOoYAIBKuPfZ+UoxmkmSjOzj+mLc\\nQEXVC7c4FYBzqXQBLT+\/9ANLWFhYhY89dcypcwAAUFmH0vPcrymgwR8wNxkA1FxLdxzVqszS0Vxm\\nfqbevr+HWsc1tjgVgPNR6QJaTEyMDh8+rJSUlAofe+oYnqMGAFTV+u17JEk2Q2pcr\/oeaQN8wZQp\\nU6yOAACopC0HM\/TQO9\/JlKG6AdLfb2ylHp0TrI4F4DxVuoDWuXNnpaWl6aOPPpLdbj\/vYtiJEyf0\\n0UcfyTAMVuAEAFTZ2q07pbAWcmXbFRhQ6bVxgBrh0UcftToCAKASDp7M0\/D5G1VQ7JLNkGYM7qqr\\nL2pkdSwAFVDpTxo33XSTJCk7O1t\/+MMflJOTc85jcnNzNWjQIGVlZUmSBgwYUNnLA6gGDofD6ghA\\nucpqo5klpd8FBbvyztgGAEBNQB+sdsvIK9KQNzbInlMkSZo4oEONK57RRoEqFNCGDh2q+Ph4SdJX\\nX32l9u3ba\/bs2Tp58uQZ+6anp2v27Nnq0KGDVq1aJcMwFB8fr6FDh1Y6uC\/43\/\/+pwEDBiguLk42\\nm00LFiywOhJQJXa73eoIQLnKaqNFdUon3Y2s4\/J2HAAAPII+WO2VnZuvu2asUuqJXEnSiJ6tdE+3\\neItTVRxtFKjCI5xBQUF69913dfXVVysvL09paWkaOXKkRo4cqbi4OPcjnQ6HQwcPHnQfZ5qmwsLC\\n9N5776lOnTpV\/wkslJOTo44dO2rIkCFKTk62Og4A+J2cvAIZofUlSY3Da\/Y9BQAA1C4lJU71eWKu\\n7GEtJEk3dIrVX37XztpQACqt0gU0SUpMTNTy5ct11113ad++fe73Dx06pEOHDrn\/\/uvVolq3bq23\\n3npLXbt2rcqlfUL\/\/v3Vv39\/SdKQIUMsTgMA\/mfr7gPu182iWf4dAAD4jpuenO0ungVlHdTzt\/aT\\nzWZYGwpApVV5tuUrrrhCO3bs0MyZM9W9e3cFBQXJNM3T\/gQFBal79+569dVXtX37diUmJnoiOwDA\\nz9nznO7XF7dsYmESAACAX4x64S39oGalf8k+ri\/+OlAhdRktD9RkVRqBdkpwcLBGjBihESNGqKio\\nSAcOHFB6erokKSoqSvHx8QoKCvLEpQAAcHPWred+fW3SZRYmAQAAKPX8W1\/o82MRMmySmZ+lfw\/v\\nptZxja2OBaCKqjwC7beCgoLUpk0bJSYmKjExUW3atKn24tnChQv10EMPqWfPnoqMjJTNZjvnnGRp\\naWkaNmyYmjZtquDgYLVs2VJjxoxRRkbGGfu+\/fbbioiIUEREhOrVq6c1a9ZU148CWOrU3IWAr\/pt\\nGz2SWeB+HRsZ4u04AAB4BH2w2mPH4UzN\/sElwxYgs6RQk66L01VdLrI6VpXRRgEPjUCz2qRJk7Rt\\n2zaFh4crLi5OO3fuLHf\/1NRUde\/eXXa7XQMHDlRCQoI2bNig6dOna8mSJVqzZo2ioqLc+w8YMEDd\\nunVz\/71p06bV9rMAVoqOjrY6AlCu37bRtIx8SVJkSB2F1a0VtzQAgB+iD1Y7HM7I17B5G1XolAxJ\\n910cqHuuv8rqWB5BGwVqSQFt2rRpiouLU+vWrbV69Wr16dOn3P1HjRolu92ul156SQ888ID7\/Ucf\\nfVRTp07VuHHjNGPGDPf7YWFhatWqVbXlBwBUzpGfC2hN6jP6DAAAWCeroFj3vrFRx7IKJUnjbrhI\\nw6\/iMyRQm3j8EU4r9OrVS61btz6vfVNTU7Vs2TK1aNHitOKZJE2cOFFhYWF68803lZ+ff85z5ebm\\nauvWrdqyZYtcLpd++uknbd26VQcPHqzUzwEAqJjDGaWPcDatH2xxEgAA4K+KnS498O\/NSjmWLUka\\n2qOF7ruypcWpAHjaOUeg\/XrklWEY2rt37xnvV9avz+ctK1eulCT169fvjG3h4eFKSkrSsmXLtG7d\\nunOOZNu0aZP69OkjwyhdinjChAmaMGGChgwZorlz53o+PADgNPuOZ0gKUKhRZHUUAADgh1wul0a+\\nvlpfp+ZJkq5t30hP\/b69+zMigNrjnAW0\/fv3yzAMmaZ52v8ETr1fWb89n7ekpKTIMAy1bdu2zO1t\\n2rTRsmXLtGvXrnMW0Hr16iWXy1UdMQEA53D4xEkVmQGSpOP7dkpKsjYQAADwO7dOeF3fFTeRJHWO\\ni9SLg7oowEbxDKiNzmsONNM0K\/S+L8vMzJQkRUZGlrn91PtlrcbpCSkpKWe8FxMTU+akjA6HQ3a7\\nnf3Z32v77969+7QFNKzOw\/7s\/9v909PT3W109eYf5MwvUEBIPcU3rFcj8rO\/f+0PAOfL4XAwSXsN\\n9OeX33MXz5Rj14t\/6KGQoABrQ1UT2iggGWZNrIKV49QiAoMHD9aCBQvO2D5ixAjNmTNHs2fP1rBh\\nw87Y\/uSTT+rZZ5\/V5MmT9Ze\/\/MWj2U6N5AN8VUpKihISEqyOAZzVr9vo8299oZe3l44CfqpHmO67\\nqbeFyYBf+ML9funSpfr444+1ceNGnThxQrm5uZo4caJGjhx52n7\/+9\/\/ZJqmYmJi1L59e4vSVj9f\\n+J0A5aEPVvO8\/vEqPf1VhozAOjILczXnjna6tlsnq2NVG9oofJ037vW1YhXOijg1wuzUSLTfOvV+\\n\/fr1vZYJAFBxe484JJWORuvQqqm1YQAfsWfPHt1111369ttv3e+dmjYjLy\/vjP3\/\/ve\/68svv1Rs\\nbKwOHjzInD0AcB7+992PenrFURnBETKdJXriyuhaXTwDUKpWrMJZEQkJCTJNU7t27Spz++7duyXp\\nrHOkAQB8w8GTuZIk0+VUxwubW5wGsN6OHTuUmJiob7\/9VqZpyjRN2Wzld\/VGjx4t0zR15MgR90JL\\nAICzy8wr1uOf7pcRHCFJGtA0T6NuvdriVAC8odIFtL59+6pv377673\/\/W+FjFy1apL59++rqq73\/\\nP5pTCwMsXbr0jG05OTlas2aNQkND1a1bN29HAwBUQGhM6aizumahQoPrWpwGsFZxcbEGDhyojIwM\\nmaapwYMHa+vWrSosLCz3uP79+ys8PFyStGTJEm9EBYAaq9jp0qi3vtWR3NIpJC4LdejFP91pcSoA\\n3lLpAtqqVau0evVqHTx4sMLHpqWladWqVVq1alVlL19prVq1Ur9+\/bR\/\/369\/PLLp20bP368cnNz\\nlZycrJCQEK9nAwCcP1t46US2HVo2sTgJYL358+dr7969MgxD48aN04IFC9SxY8dzjkALCgpSYmKi\\nTNPUxo0bvZQWAGoe0zT15Iffa+1ehySpf4fG+u+T91icCoA31Yo50D766CMtWrRIknT06FFJ0tq1\\na3XvvfdKKl01a8qUKe79Z8yYoaSkJD388MP68ssvddFFF2ndunVatWqV2rVrp0mTJnn\/hwB8QExM\\njNURgHL9uo0eySyQJMXW5wsP4FQ\/qHnz5po4cWKFju3QoYNWrFjhnsaiJpgxY4ZmzZql\/fv3S5Iu\\nvvhiPfnkk7r++uutDQZUEn0w3\/faV6l6d1Pp4JFOcZH61+2XyGbzn3kjaaOARQU0p9NZevFAz1x+\\ny5Ytp624aRiG9u3bp3379kmSWrRocVoBrVWrVtq0aZPGjx+vxYsX64svvlBsbKzGjBmj8ePHuxca\\nAPwNS1PD151qoy6XqSMZpQW0phTQAG3ZskWGYeiGG24456iz3zr1oSg9Pb06olWLZs2a6Z\/\/\/Kfa\\ntGkjl8ulefPmaeDAgdq8ebM6dOhgdTygwuiD+bb31qboH4v3SJKaRAZrTnJXhQQFWJzKu2ijgEUF\\ntAMHDkiS6tWr55HzTZgwQRMmTKjQMU2bNtXrr7\/ukesDALzLkVukImfp\/COxkcEWpwGsZ7fbJUlx\\ncXEVPjYgoPRDYElJiUczVacbb7zxtL9PmjRJM2fO1DfffEMBDYBHvbd8nR774rCMOnUVFhSg14de\\nrgvq0fcA\/JHXV+HctGmT3nrrLRmGoYSEBG9fHgBQCxzOyHe\/bsIINEChoaGSdM5FA8py\/PhxSVKD\\nBg08mslbXC6X3nnnHeXm5qpHjx5WxwFQi2zemarHPt4ro05dmS6nHu3RQBfFemYQCICa57xGoE2f\\nPl3Tp08vc9vkyZPPmIy\/LE6nU+np6crNzZVpmu7HDAAAqKg13+1wv46s47IwCeAbYmNjlZmZqZ07\\nd1b42LVr10qSWrZs6elY1er7779X9+7dVVBQoIiICH344Ye6+OKLrY4FoJY4fjJTf3hxhYx6sZKk\\nPvXTNex3N1mcCoCVzquAlpGRof3798swTp8k0TRNpaenn\/ecGaZpul+3a9dODz74YAWiAgBQav32\\n3ZIaSpIa1LU2C+ALevbsqR9\/\/FGLFy9WXl6ee0TauWzevFkbN26UYRjq3bu3R7IsXLhQq1ev1pYt\\nW7R161ZlZ2dr8ODBp81X+1tpaWl66qmntGTJEjkcDsXGxmrgwIGaMGGC6tevX+Yx7dq109atW5WZ\\nman3339fycnJWr16tdq3b++RnwOA\/yoqLtHVT70lZ714SVJ80QHNfWKkxakAWK1Cj3Capun+U9Z7\\n5f0JCAhQdHS0evTooWeeeUbr169XeHi4x38gAJXncDisjgCU61QbPfUIp1lSqAubNbYyEuATBg0a\\nJEnKysrSE088cV7HZGVlaejQoZJKF2AaPHiwR7JMmjRJr7zyirZu3aq4uLgzvoD9rdTUVF166aWa\\nP3++unXrpkceeUStW7fW9OnT1aNHj7N+URsYGKhWrVqpS5cueuaZZ3TJJZdo6tSpHvkZAG+jD+Zb\\n\/vreRmVHlBbPwrIOaMmzwyu8QEttQxsFzrOANmHCBLlcrtP+SKWdreeff\/6MbWX9KSoq0okTJ\/T1\\n119r7NixFM8AH3RqEmrAV51qo478n+9DBVl+36EFJKlXr1669tprZZqmXnnlFY0YMeKsH3ZM09Qn\\nn3yixMREff\/99zIMQ3fccYcuuugij2SZNm2adu3apczMTM2YMeO0L17LMmrUKNntdr300ktauHCh\\nJk+erOXLl2vMmDHauXOnxo0bd17XdblclZoDDvAF9MF8x7\/XHdD7209KkkKK0rVs4iAF1w2yOJX1\\naKNAFRcROFeHCACA6pDtLF01MNiVf449Af\/x73\/\/W\/Hx8TJNU3PmzFHTpk3VrVs39\/bZs2crKSlJ\\nUVFRGjhwoHbv3i1JatOmjV599VWP5ejVq5dat259XvumpqZq2bJlatGihR544IHTtk2cOFFhYWF6\\n8803lZ9\/+n\/rY8eO1ddff60DBw7o+++\/19ixY7V69WqPjaID4J++3m3XhI9L51mNCQ\/SsqduUZOG\\nNXOBFQCed15zoJVl3759kmruik0AgJqrKDBUhqR6gSwgAJzSsGFDrVmzRrfffrvWrl2roqIi9\/xm\\nkrRr1y5Jp38B2r17d33wwQeKiIiwJPPKlSslSf369TtjW3h4uJKSkrRs2TKtW7dOffr0cW87evSo\\n7rnnHh09elSRkZHq1KmTFi9erGuuucZr2QHULvvsuXrgrW\/ldJkKCrRpdnJXxUWd33ySAPxDpUeg\\nxcfHKz4+3rIOFwDAP7lcpgLCSr+8uSg+1uI0gG9p0qSJvvrqK\/3nP\/\/RlVdeqYCAgDPmpbXZbOra\\ntavmz5+v\/\/3vf2rUqJFleVNSUmQYhtq2bVvm9jZt2kj6pfh3yhtvvKF9+\/YpPz9fR48e1dKlSyme\\nAai09JwC3Td\/o7IKSiRJU27rpC7NoyxOBcDXVHoEGgAAVrDnFsql0hE1fbtfanEawPfYbDbdcccd\\nuuOOO5SXl6cffvhBDodDJSUlio6OVtu2bX3mCYLMzExJUmRkZJnbT72fkZHhtUwA\/EtRcYmu+stc\\n5fy8aMDoPq014JKmFqcC4IsqXUArKirS6NGj5XQ61bt3byUnJ5\/XcfPnz9fq1asVFBSkV155RQEB\\nAZWNAMDDYmJirI4AlCsmJkZpmQXuvzeODLYwDeD7QkND1bVrV6tj+ISUlJQz3ouJiVF09P+zd9\/R\\nUdX5\/8efN430UEInEAgdQQQFBJQiIijN9adgBV0bArKou2uFVVnURVFEwa+ACnaQjuBSlKqCdFSa\\nEFoSEjIJqaTf3x+zGY2ElMkkd5K8HufkcDNzP3Nf0ZvcO+\/5lDqXPG6z2QqdMFv7a\/\/y2j8xMfGS\\nc2IYLDYAACAASURBVLQy5a\/M+9846U0Ss4LBdpYwn3SGNI3gyJEjlSZ\/Re3\/53PU6jzaX\/sXtX95\\nMUwnVwJYvHgxI0eOxDAMtm7dSs+ePUvUbuvWrfTp0wfDMFi6dCnDhw935vCVkmEYWnhBRKSM1v1y\\njoc\/3g3AinG9uDKspsWJRAqy4nrfrVs3AMLDw1m0aFGFHrsomzdvpl+\/ftxzzz0sXLjwkuf\/8Y9\/\\n8MYbb\/D6668zadKkS56fMGECs2fPZvbs2TzyyCNO59A9mIgU5vG3PmfluWAAPJJj+P6l22gQqqGb\\nIpVRRVzrnZ4Dbc2aNQCEhYWVuHgGcN1119GkSRMAVq9e7ezhRUSkmjqX\/HsPtIbqgSYCwO7du9m9\\nezcNG1aueQHbtGmDaZqXzHGWL3+l0MvNkSYi4qwPVm1mRZQfAObFZD599HoVz0SkSE4X0Hbt2oVh\\nGPTu3bvUba+\/\/npM0+Snn35y9vAiIlJNnfvfEE5PD4M6gTUsTiPiHvKH4Fe2Alr+yprr1q275LnU\\n1FS2b9+Ov78\/PXr0qOhoIlKFnUlI5+XvzmF4emPmZvNcv\/pc20mFehEpmtMFtFOnTgHQsmXLUrfN\\nb5P\/GiIiIiW1ZdcBAAI9c\/D0MCxOI+IemjZtCtjnAqlMWrRowcCBAzl58iTvvPNOgecmT55MWloa\\n9913H35+fhYlFJGqJi0zh4c\/3o3pEwjAkAZpPDyiv8WpRKQycHoRgYwMew8AX9\/SD5\/Jb5OWlubs\\n4UVEpJr6LdoGwQGk22KsjiLiNoYMGcLu3bvZuHGj1VFYsWIFy5cvB+DcuXMAfP\/999x\/\/\/2Avbfc\\n9OnTHfvPnj2bXr16MXHiRDZu3Ei7du348ccf2bRpE23btmXq1KkV\/0OISJWUl2fyxKJ9HIpJBmD0\\ntU15cfgtFqcSkcrC6R5otWrZx4fHxcWVum1+m6CgIGcPLyLloLL1XJDqx2azkWHYP4QJ9MixOI2I\\n+3jooYcICQlh\/\/79fPbZZ5Zm2bdvHwsXLmThwoWsW7cOwzCIjIx0PLZ06dIC+7do0YJdu3YxZswY\\ndu7cyYwZM4iMjGTSpEn88MMPjntOkapM92AV460NR\/nvL7EAXNcqlBeGdLA4UeWhc1SkDAW0sLAw\\nTNNky5YtpW6b36Zx48bOHl5EykFFLgEs4oy4uDhMX\/tqWbV9nb6EiVQ5jRo1YuHChfj4+PDAAw8w\\na9YscnNzLckyZcoUcnNzL\/t1\/PjxS9o0btyY+fPnExUVRUZGBpGRkbzxxhuEhIRY8BOIVDzdg5W\/\\nVfujefvb3wBoERrAO3d2wctT9xIlpXNUpAxDOPv27cuePXvYu3cvmzdvpk+fPiVqt2nTJvbs2YNh\\nGCVuIyIiAnAu\/gKGt70HWgOtwCniMHv2bADuuusuPvzwQ\/72t78xdepUBg4cSNu2bQkJCcHDo\/g3\\nio899lh5RxURqXCLNvzIMxviAE+CfL2YO\/pqQvy9rY4lIpWM0wW0u+++mxkzZji2N2\/eTERERJFt\\njh8\/zt133+34\/p577nH28CIiUg0dORMD2FfeDAvVNAAi+caPH49h2BfVMAwD0zQ5f\/58qYZzGoah\\nApqIVDmHT0bx9xXHMAJqg5nHO3deRUTdQKtjiUgl5HSf1auuuorbbrsN0zSJjo6mS5cuvPLKK0RF\\nRV2yb3R0NNOmTaNLly7ExMRgGAZDhgyhe\/fuZQovIiLVS55PgGP76valXwVapCozTdPxVdhjJfkS\\nEalK0jMyGf6flfbiGdA7KJ4+bepZnEpEKiune6ABzJs3j4MHD3L06FFSU1N5\/vnnef7552nYsCH1\\n6tn\/MMXFxRETY18pLf\/GrGXLlixYsKCM0UVEpLrJxAewLx7QvWNra8OIuJE\/rmopIiJ2Q56fT2Zw\\nMwDqpp1k4bSxFicSkcqsTAW0kJAQtm3bxsiRI\/nuu+8cj8fExDiKZkCBTzT79+\/P559\/Ts2aNcty\\naBEpB6GhoVZHEClSuoc\/kA5AveAa1oYRcSNPPvmk1RFEpAx0D+Z6T737FSe87MUzz+Ro\/vvq6BLN\\nBSmF0zkqUoYhnPlCQ0PZuHEjK1asYNCgQQQEBFwyHCAgIIDBgwezatUqNmzYQN26dV2RXURcrE6d\\nOlZHEClSKn4A1AnwoYaXp8VpREREXEP3YK61\/8wFVkb7A2BeTObzcf2oHax5z8pC56hIGXug\/dHQ\\noUMZOnQoOTk5nDp1CpvNBth\/0cLDw\/H01BsdEREpm5ikDADqB2sFThEREbnU+ZRMHv1kN1m5Jp4e\\nBtNGtKVbB82bKiJl57ICmuMFvbyIiIgodkVOERGR0jr3vwJawxAV0ERERKSg7Nw8xn22x\/GB23M3\\nt2Nk7+YWpxKRqkKDwEVEpNL4LToegOyUeIuTiIiIiLuZuvpXdkYmAPCXLo25v1e4tYFEpEpxeQ80\\nERGR8pCYnEqOh33hgLTzZy1OI+JeunXrVubXMAyDHTt2uCCNiEjFe3XRZhbsSQWgY+MQpt3aEcMw\\nLE4lIlWJCmgi4mCz2TRBqLitA8dOk3sxGU+\/YJrU1kTAIn+0a9euUr9RzF8l3TAMTNPUG00RC+ke\\nrGw++2Y7c3YmYnh5U8vPi\/+7tyu+3pqD25V0joq4sIB24sQJtm7dypEjR7hw4QIZGRnFtjEMg\/nz\\n57sqgoiUUXx8vC6M4rYOnYohL91eQGvRoLbVcUTcTn5BrKTyC2albScirqd7MOf9cvwMz3wdiRFQ\\nCzM3h3GdA2hU08\/qWFWOzlERFxTQDh8+zIQJE\/j222+daq8CmoiIlMTxaJtju02zBhYmEXE\/58+f\\nL9F+6enpnD59mm+++YY5c+aQmJjICy+8wOOPP17OCUVEXC89I5O\/vPE1RnAYADfUucCDw4dbnEpE\\nqqoyFdC2bt3KzTffTHp6eok+vcwfIvDH70VERErijC3Fsd2pZZiFSUTcT0l7BdSpU4ewsDB69erF\\nY489xuDBg5k6dSo1atTg2WefLeeUIiKuNXzyB2QGNwWgwcWTzJs21uJEIlKVOb0KZ1paGrfffjtp\\naWmYpsmwYcP4\/PPPefTRRwF7cezbb79l2bJlvPzyy3Tu3BnTNPHw8OD555\/nu+++c7rXmoiIVD\/1\\nw9sA4E0ujepqCKdIWTVs2JBly5bh7e3NlClT2LVrl9WRRERKbOXeMxzzsBfPPJOjWfvyaDw8nH57\\nKyJSLKf\/wsybN4+4uDgMw+Af\/\/gHy5cvZ+TIkURERDj26du3L8OHD+e5555jz549fPDBB3h5efHq\\nq68SExNDnz59XPJDiIhI1Zfh4QtA07rBFicRqTqaN2\/OzTffTG5uLrNnz7Y6johIifwWl8ozy34B\\nwIccFjx8HbWCtcCQiJQvpwtoa9euBaBevXpMnTq1RG3GjBnDe++9R05ODg8\/\/DAnT5509vAiUg5C\\nQ0OtjiByWeeSM\/HwD6ZhiCYGFnGlzp07A7B582aLk4hUX7oHK7m0zBzGfrKbtKxcAOaM7kHvzm0t\\nTlX16RwVKUMB7eDBgxiGwaBBg\/DyKnwqtcLmRRszZgwdOnQgLS2NefPmOXt4ESkHWllH3Nm5pIt4\\n+gVTP9jX6igiVYqnpycAMTExFicRqb50D1Yypmny9NKDHItLBWBC\/5bc0K6+xamqB52jImUooCUk\\nJAAUGLIJFCimXbx4sdC2AwcOxDRNvv76a2cPLyIi1UhObh7nUzIBaBBSw+I0IlXL7t27AfDzU+9O\\nEXFvH24\/yar90QBc1yqUvw1obXEiEalOnC6g5a+g6e3tXeDxoKAgx3ZcXFyhbfO7f549e9bZw4uI\\nSDViS8si73+dmhuoB5qIy2zatImVK1diGAbt27e3Oo6IyGV9sGozL606CECjEF9mjroKTw\/D4lQi\\nUp0UPvayBEJDQ4mKiiIpKanA440aNXJsHzp0iPDw8Eva5hfOUlJSnD28iIhUI9\/v\/dWxnZNiA8It\\nyyLijtLT00u0n2mapKamcvToUZYuXcp7771HXl4ehmEwatSock4pIuKcX0+c5cX1ZzD8a0FuNm\/d\\nfjW1A3ysjiUi1YzTBbS2bdsSFRXF4cOHCzzeqVMnx\/bq1asZPHhwgeezsrJYs2YNYF+AQEREpDi\/\\nnDgL2Odp8jUzrQ0j4ob+OAKgNPLnq73mmmt45JFHXBlJRMQlMjKz+MvrqzCCmwJwU90UurXUvGci\\nUvGcHsLZs2dPTNNkx44dBR5v1KgRXbt2xTRN5s+fz+rVqx3PZWZm8vDDD3Pq1CkMw6BXr17OJxcR\\nl7PZbFZHECnU6fP23s65F5NpF97Q4jQi7sc0Tae+AG677TbWrl172UWhRKT86R7s8kZM\/oCM\/xXP\\n6qefZM5Td1ucqHrSOSpShh5oAwcO5KWXXuLcuXPs3LmTbt26OZ576qmnuPPOO8nOzmb48OG0aNGC\\n2rVrc+jQIdLS0gD7HGqPP\/542X8CEXGZ+Ph4rbAjbinmQjpQm9y0C7QNb2x1HBG30759e8f8tEUx\\nDIPAwEDq169Ply5d+Mtf\/kKHDh0qIKGIFEX3YIV7a\/G3HDbCAPBIjmHNK\/fh4eF0HxApA52jImUo\\noPXs2ZNevXoRHx\/P2rVrCxTQRo4cyfr16\/nggw8AOHHiBCdOnAB+Hyrwr3\/9i2uvvbYs2UVEpJqI\\nT8sBPyAzFd8amvNE5M9+\/vlnqyOIiLjUmYR0PvglGwAzK50P\/9qTOiHODVcXEXGFMvXV37p162Wf\\nmzdvHj169GDWrFkcPGhfLcUwDK655hqefvppbr311rIcWkREqpHkbAP8wCvnotVRREREpJxl5eQx\\n\/vO9JGfkAPD67Z3p07WVxalEpLor18kuHnzwQR588EEuXrxIYmIitWrVws\/PrzwPKSIiVVBoWEvS\\nkrJp0ViLz4iIiFR1r31zmP1nLgAwpmc4t1+r4pmIWM\/pAtrp06cBe6+ysLCwIvf18\/NT4UxERJyW\\nbB\/BQZvwRtYGEXFTd9xxBwCjR4\/mlltuKVXb\/\/73v8yfPx\/DMPjyyy\/LI56ISImt++Uc87dFAtCx\\ncQjP3NzW4kQiInZOF9DCw8MxDIPevXuzefNmV2YSEYuEhoZaHUHkEpk5uSSm2ytozRo1sDiNiHv6\\n6quvMAyDHj16lLqA9ttvvznai4g1dA9m9+PPv\/Hk4t8ACKrhxbt3daGGl6fFqQR0jooAOL2Eibe3\\nNwC9evVyWRgRsZZW1hF3FJec6diOCFMBTUREqh7dg0Fqegb3zP6WlMxcAP7z\/zrRtI6\/xakkn85R\\nkTIU0Bo0sL+JCQwMdFkYERGRP4tNznBs1wv2tTCJSNWUm2t\/s+rpqV4eImKd4ZM\/ICe4MQDNs08x\\nuGNDixOJiBTkdAGtffv2AERGRrosjIiIyJ\/F\/qEHWv0gFdBEXC06OhqAoKAgi5OISHX14vzlHPdq\\nBoBXchQrX7rf4kQiIpdyuoB2xx13YJomq1ev5uLFi67MJCIi4vD1t1sd2yE+poVJRKqeyMhIPv\/8\\ncwzDoFUrrXInIhVv+77DfPBzFgBmZhofPXw9QQFagE5E3I\/Tiwjce++9zJ49m927dzN+\/Hjmz5\/v\\nylwiIiIAnIhJACMAMzebBrXUQ0Zk7ty5zJ07t9DnZs6cyRdffFHsa+Tm5pKQkOBYVR1g0KBBLsso\\nIlISWTl5\/H3ZYYwaAQA8cIUPvTtr1U0RcU9OF9C8vLxYunQpI0aM4KOPPuLEiRNMnjyZvn37ahUn\\nkUrKZrNpglBxO7b0HAgAIyOZCxcSdY5KtRcdHc2uXbsuud8yTZOzZ89y9uzZEr+Wadp7dTZt2pTH\\nH3\/cpTlFpOSq6z3Yq2sPE51pX5yuvWcsU\/76gMWJ5HKq6zkq8kdOF9D69+8PgI+PD6ZpsmXLFgYM\\nGICfnx+tWrUiJCQED4+iR4gahsHGjRudjSAiLhYfH68Lo7idlFz7xObeORd1jor8QX7xq7jHLsfD\\nw4NWrVpxyy238PTTT1O7dm1XxhORUqiO17dvD8fywXb7fNqdmoTw1aODLU4kRamO56jInzldQNu0\\naZPjk8\/8f03TJD09nQMHDhTb3jRN9VQTEZFiZRr2hQMCPXMsTiLiHv7xj38wfvx4x\/emaVKvXj0M\\nw+DFF19k7Nixxb6Gt7c3gYGBxX7YKSJSHuKSM3hqsf09Y2ANL2bdeRU+Xvp7JCLuzekCGlz+U87S\\nfPopIiJSlLwagRhAzRr60EUEwM\/PDz+\/SyfYNk0Tf39\/9RAQEbeWl2cyadE+EtLsCwf8+9YraFYn\\nwOJUIiLFc7qAFhkZ6cocIiIil0jJyMbw8Qfguqs7WpxGxH399NNPAISFhVmcRESkaA9M\/5ztiSEA\\n3NalCcM7N7Y4kYhIyThdQGvWrJkrc4iIiFzifEqmY7tzm+ZAmnVhRNxY165drY4gIlKsT7\/Zznfx\\nARieUNMzixeHd7A6kohIiWmguYg4hIaGWh1BpIBzyRmO7frBvjpHRUSkSqoO17fo8wk8t\/o3DE8v\\nzNxsnu3XkMAaZZpRSCpQdThHRYqjv1gi4qB5c8TdxCX\/3gOtfnAN6tQJsjCNSOWUl5dHRkZG8TsC\\n\/v7+5ZxGRApT1e\/BTNNk+MufQ2A4AL2DE7ljwAhrQ0mpVPVzVKQkSlRAe+KJJwAYPnw4ffr0KddA\\nIiIi+WL\/0AOtXrCvhUlEKo\/Dhw8zd+5ctmzZwpEjR0hLK9nQZ8MwyMnRarci4npPvrOI8\/7hAPgn\\nn2LB1EesDSQi4oQSFdDeeustDMOgSZMmly2gqcgmIiKuFvu\/Hmj+Pp4EaZiHSJFM0+Sf\/\/wnM2bM\\ncKyIrpXRRcRqJ86n8vW5QADMi0ksfnIoXl6eFqcSESk9l70bKUmRTUREpDTWbvoevBtiZCRjGIbV\\ncUTc2qRJk5g1a5ajaBYaGoqnpyexsbEYhkG7du1ISkoiOjoa0zQxDMPxuIhIecjKyePxL\/aSmWv\/\\n\/tkbwugQodWCRaRy0iICIiLitmwX7cPJclJtFicRcW8HDx7k7bffBqB+\/fqsW7eOuLg4nnrqKcc+\\nP\/\/8M2fOnMFms\/H+++\/TpEkTTNOkXbt27Nixg4MHD1oVX0SqqDfWHeHnqGQAHr6+BY8Mu87iRCIi\\nzlMBTUQcbDYVKcS9ZHn4ARDkmQfoHBW5nPfff9+xvXTpUgYMGHDZfWvWrMmDDz7Ivn37uOqqq1iy\\nZAmjR4+uiJgichlV8fr2\/fF43t96AoCOjUN4amAbixNJWVTFc1SktFRAExGH+Ph4qyOIOOTl5WH6\\nBgNQ289+udI5KlK4rVu3YhgG3bt359prry1Rm1q1arF48WJ8fHxYunQpq1atKueUInI5Ve36diE9\\niycX7cc0wdfbg7dGdcbHS289K7Oqdo6KOEN\/xURExC2dijmP4eUDQH2twClSpDNnzgDQs2fPAo\/\/\\nce7A7OzsS9o1b96cQYMGYZomH3\/8cfmGFJFqY+Az84hJsq+k\/dwt7YmoG2hxIhGRslMBTURE3NIv\\nJ846tsNCgy1MIuL+UlJSAKhbt26Bx\/38\/Bzbqamphba98sorAdi9e3c5pROR6uSfsxcT59cMgIam\\njXu6N7U4kYiIa6iAJiIibsm\/TiPHdp9unS1MIuL+\/P39AcjKyirweHDw78Xn\/F5qf5a\/amdsbGw5\\npROR6uKnX37ji2P2vynmxWTmPdxPq2iLSJWhApqIiLil+PQcx3bbZg0sTCLi\/po2tffw+HMRrHXr\\n1o7tnTt3Ftr2119\/BcDDQ7eFIuK8nJxc7n13PUaNAAAe6hxAh4gwi1OJiLiOV2l2Pn78OFu2bCnz\\nPn90\/fXXlyaCiJSj0NBQqyOIOMQlZzi28+dA0zkqUriOHTvy888\/c+DAgUseDwgIID09nblz5\/LX\\nv\/61QG+Qw4cPs2LFCgzDoGXLlhUdW0T+pypc3+5\/7WMygu3F\/MYZJ3n+\/nEWJxJXqgrnqEhZGWZ+\\nv\/0ieHh4lEvXW8MwyMnJKX7HKsIwDErwn1tERIAXlv\/Mxz+eIsTPm\/1TBlodR6TErLjez507l0ce\\neQQfHx\/i4+MJDPx9wu777ruPTz75BMMwGDRoEI8\/\/ji1a9dm9+7dTJ48mfj4eAzDYPLkyUyZMqVC\\nc1cU3YOJlK9fo5MZ9s42cvJMSDnPzpdvpV7tEKtjiUg1UhHX+lIV0IraNb\/AVprAhmGQm5tb4v0r\\nO928iYiU3EMLd7H+11ha1w9k3aQ+VscRKTErrvfnzp2jSZMmmKbJ\/PnzGTNmjOO5U6dO0bFjR9LS\\n0gpta5omTZo04cCBA9SsWbOCElcs3YOJlJ+M7FyGvbONo7GpeHoYzLmtFQO7trI6lohUMxVxrS\/R\\nEM6mTZtq8kcREalQ+UM484dvisjlNWjQgI8\/\/piEhATq169f4LlmzZqxatUqRo4cSVxc3CVtmzdv\\nzsqVK6ts8UxEytd\/vjnC0Vj7Kr\/j+7VU8UxEqqwS9UAT19CnnyIiJdd84meYfiG0rXGBb1682+o4\\nIiXmrtf75ORkvvzyS3788UcSExOpU6cO119\/PSNHjsTHx8fqeOXKXf+fiFR2247Fc8\/8HQB0DqvJ\\n4kevxdtTC5KISMVzmx5oIiIiFSkrO4e8GoEYgGdWqtVxRKqE4OBgHnroIR566CGro4hIFXD+Qip\/\\n\/2o\/AH7enrw5srOKZyJSpekvnIg42Gw2qyOIAHDsdAyGhycADUL8HI\/rHBUp3Jo1a1izZg3r16+3\\nOoqIOKEyXt+GTllATJJ9uoXnbmlH89AAixNJeaqM56iIq6mAJiIO8fHxVkcQAeCXyCjHdtO6wY5t\\nnaMihRsyZAhDhw5l1qxZVkcRESdUtuvbtI9Wcc4vHIDg9Cju7t7U2kBS7irbOSpSHlRAExERt\/Nb\\n1HnHdstGoRYmEakcfH3ti21cddVVFicRkaru+Nlz\/N\/eFADMzDQ+emyAFpwTkWpBBTQREXE7p+OS\\nHNvtmjeyMIlI5dCwYUMAvLw0va2IlK87XluC4RcCwG3N8+jStoXFiUREKoYKaCIi4nZadroaAANo\\n17yJtWFEKoGuXbsCcOjQIYuTiEhVNnXBGmwB4QAEp5zk9fF3WBtIRKQCqYAmIiJuJz41C4A6gTXw\\nq+FjcRoR93fPPfdgmiZff\/215qkRkXIRl5LBklP2Xq5mRipfPDkcDw+9nRSR6kN\/8UTEITRUc02J\\neziXbF\/Vq35wjQKP6xwVKdywYcMYNmwYKSkpjBw5krS0NKsjiUgpuPv1zTRNnl36M4np2QC8eU93\\n2rdQD\/HqxN3PUZGKoAKaiDjUqVPH6ggiAMQmZwLQINi3wOM6R0Uu79NPP+X222\/nu+++o3379sya\\nNYvjx49bHUtESsDdr29L9kSx4VAsAEOvbMRfrg63NpBUOHc\/R0UqgmGapml1iOrCMAz0n1tEpHhd\\nX16PLS2LO7s15ZW\/dLQ6jkipWHG9r1evnmM7fwhn\/qp4Xl5eBAUFFTvUyjAMYmNjyy+khXQPJuK8\\n6AsXuenNLaRk5lA3qAbr\/nY9tQI0vYKIuJeKuNZrqSYREXErmdm52NLsc6D9eQiniBQuPj7eUTDL\\n\/xfsw66ys7NJSEi4bNv8G84\/thMRAcjLy+OJL3aTkpkDwKt\/6ajimYhUWyqgiYiIWzlwNNKxfS7y\\nCNDaujAilYS\/v3+ZC2AqoInInz32xmf8aKsFwB1XN+GGdvUtTiQiYh0V0ERExK38ejLasV3bT1N1\\nipREamqq1RFEpIr56ZffWBtTA8MHzLQEnrrheqsjiYhYSu9MRMTBZrNZHUGEE9G\/n4ctG9ct8JzO\\nURERqYrc7fqWl5fHmHfXYfj4AzDx2lDq1QqyOJVYyd3OURErqIAmIg75E0+LWOl0fLJju33zxgWe\\n0zkqIiJVkbtd3ya9\/SVpwc0AaHjxJE\/cNdjiRGI1dztHRazgsiGchw4dYu\/evcTFxZGamkpeXl6J\\n2k2ePNlVEUREpAo4l3QRADM3h5ZNGlicRkREpHo5Hh3P8lOeGDXATL\/AF8\/cYXUkERG3UOYC2hdf\\nfMFLL73EkSNHnGqvApqIiPxRYkYe+AIZyXh5eVodR6TSOn\/+PLt27eL8+fOkpaXRq1cvOnXqZHUs\\nEXFjpmnyyvqTGDUCAHi4SxDNGtYtppWISPVQpgLa3\/\/+d2bMmAHY\/9iWllZ7EhGRP2vRoQvnjtvo\\n8KfhmyJSMl9\/\/TX\/\/ve\/2bFjR4HHp0+ffkkB7fbbb8dms9GhQwdmzZpVkTFFxA2tOhDDhkOxAAzp\\n2IDn7u5qcSIREffhdAFt\/fr1vPHGGxiGgWmahIWFMWzYMFq3bk1QUJCKYyIi4pRzyRkANKsXYnES\\nkcrFNE0mTJjAnDlzHN\/nu9x9Wbt27Zg6dSpbt27lmWeeoVGjRhWSVUTcjy01k3+t\/AWAOgE+vDSi\\no8WJRETci9MFtPybM4CJEycyffp0vLxcNqWaiFggNDTU6ggixCbZC2j1g30veU7nqMjlPfPMM8ye\\nPRsAb29vhgwZQufOnZkyZcpl24wePZqpU6eSl5fH6tWrefjhhysqroj8gTtc36as\/IWEtCwAXhze\\ngdoBPhYnEnfiDueoiNWcXoXzxx9\/xDAMOnXqxJtvvqnimUgVUKdOHasjSDWXkpFNWlYuAA1CLi2g\\n6RwVKdyhQ4d4\/fXXMQyDiIgIDhw4wJIlS3jhhReKbBcREUGbNm0A2LRpUwUkFZHCWH19W\/5TJKsP\\nxABwU4f63NKxoaV5xP1YfY6KuAOnC2gJCQkADB6sJY1FRMQ1Yv83fBOgQSE90ESkcO+99x55eXl4\\neHiwfPlyR1GsJLp27Yppmvz888\/lmFBE3FVkVCwTP\/kBgGBfL14efoWm4xERKYTTBbT8CnRQUJDL\\nwoiISPV2Jj7FsV3YEE4RKdx3332HYRgMHDiQDh06lKpteHg4AGfPni2HZCLi7ka99hWGn33e0UH1\\nUqmn66+ISKGcLqB17GifVPLUqVMuCyMiItXbyvVbHNt5aQkWJhGpXM6cOQPA1VdfXeq2+R+G2uR9\\nCAAAIABJREFUpqamujSTiLi\/\/3zyNbH+4QAEJp\/i1UdvtTaQiIgbc7qAdt9992GaJqtWrSIzM9OV\\nmUREpJo6m\/D7G\/i2zRpYmESkcsnIsA9\/9vf3L3Xb9PR0AAICAlyaSUTc27n4RN7dYf+wysxK56Px\\nN+Hh4fTbQxGRKs\/pv5B33XUXAwYMICYmhieffNKVmUTEIjabzeoIUs3FpdhX\/zIz06gTcukUATpH\\nRQqXvzpabGxsqdseOXIEgLp167o0k4iUnBXXt3unL8YIqA3AoAYZXN0+osIzSOWhezCRMhTQAL78\\n8kv69evHnDlzGD58OAcPHnRVLhGxQHx8vNURpJpLzDQB8MxMKfR5naMihWvXrh2mabJ169ZStbt4\\n8SIbNmzAMAynhn+KiGtU9PVt7+lEfjMaA+CTdIZ3n7irQo8vlY\/uwUTAy9mG\/fv3ByAnJwfTNFm9\\nejWrV6+mQYMGNGvWDF\/f4iefNAyDjRs3OhtBRESqmPQ8bwB80dQAIqUxaNAgNmzYwJ49e9i2bRu9\\ne\/cuUbvp06eTkJCAYRhaWV2kmsjOzeOZpQcxAR9PD5Y8fRteXp5WxxIRcXtOF9A2bdrkWN44\/1\/T\\nNDl37hznzp0rtr1pmloeWURECsitYR+2GeJtWpxEpHL561\/\/ytSpU0lKSuKee+5h48aNREQUPRxr\\nzpw5vPTSSxiGQaNGjRg1alQFpRURK72\/5QSHz9l7ej\/WL4KOzTR8W0SkJJwuoIG9CFaSx0RERIqT\\nnZsHfsFgwtAbStZ7RkTsQkJCmDFjBg888ABnzpyhc+fOPPTQQwwcONCxz6lTp1i\/fj379+\/niy++\\nYO\/evZimiYeHB7Nnz8bb29vCn0BEKkJkfBozNx4DoGW9QMb21bxnIiIl5XQBLTIy0pU5RESkmjuf\\nkkn+ZzDN6te0NoxIJTRmzBiio6N54YUXSEtLY+bMmcycOdPR4\/+dd97hnXfeceyfPxrgP\/\/5D0OH\\nDrUqtohUkLy8PJ5ZeoCsnDwAXv1LR2po6KaISIk5XUBr1qyZK3OIiBvIX8VNxArnkjMc2w2CC59H\\nU+eoSNGeffZZunTpwlNPPcWvv\/5a5L6tWrXizTff5Oabb66gdCJyORVxfZs48wt+jA0B4J4eTbk6\\nvHa5H1OqDt2DiZRxCKeIVC116tSxOoJUY7FJvxfQ6l+mgKZzVKR4gwYNYtCgQXz77bds3LiR\/fv3\\nY7PZyMnJoU6dOrRr144bb7yRQYMG4eFRpgXZRcRFyvv69uuJs6w85YnhC6Rf4MkB\/cv1eFL16B5M\\nRAU0ERFxEwV6oIUUv5KziBStf\/\/+jlXTRaR6u\/fNFRgB4QA80DmQWoF+1gYSEamEXFZAy8nJYd26\\ndWzbto1jx46RkJAAQO3atWndujW9evXipptuwtNT4+xFRORSkecSAfD2NKjt72NxGhERkarhtY+\/\\nxva\/4lnN1JNMfmCctYFERCoplxTQ5syZw7\/\/\/W9iYmKK3K9hw4Y8\/\/zzPProo644rIiIVCFrN\/0A\\nAeHkpCTg4WFYHUdERKTSi7VdYPbOBIyA2piZ6Sx4fIjVkUREKq0yFdByc3MZOXIky5YtA+yrORUl\\nOjqacePGsWHDBhYtWlTp59145ZVXWLZsGUeOHKFGjRr06NGDV155hQ4dOlgdTUSk0knJtV8TvLNT\\nLU4iUjXs2LGD77\/\/niNHjpCYaO\/hWatWLdq0aUPPnj3p3r27xQlFpLzN2XYWI8C+WMCghhlc2VoL\\nwYmIOKtMBbQJEyawdOlSDMPANE2aNm3KbbfdRpcuXRyrdMTHx7N3716WLFnCqVOnME2TZcuWMWHC\\nBN59912X\/BBW2bJlC+PHj+fqq6\/GNE1eeOEFBgwYwKFDh6hZs6bV8URKzWazaYJQsUyGYZ\/3LNAz\\n97L76BwVKd6nn37K1KlTOXr0aJH7tWrVihdeeIG77767gpKJyOWUx\/XtwNkLLPjxFABXNgrg3ccG\\nufT1pXrRPZgIGGZx3cYuY9++fXTt2hUALy8vXn\/9dcaPH49hFD7sxjRNZs+ezZNPPklWVhYeHh7s\\n3r2bK6+80vn0biYtLY2QkBBWrFjBLbfccsnz+YVGEXd15MgR2rRpY3UMqYby8vJo\/vdlGN6+ROSc\\nZuPrYwvdT+eoVAZWXe+zs7O5++67WbJkCVD8yACwZ7311lv5\/PPP8fb2Lu+IltE9mLg7V1\/fcvNM\\nhr+7jZ+jkvH2NFg78Tpa1gty2etL9aN7MHF3FXGtd7oH2ocffohpmhiGwUcffcSdd95Z5P6GYTBu\\n3Djq1KnDXXfdhWmazJ8\/n7ffftvZCG4nOTmZvLw8atWqZXUUEZFK5WxcAoa3vQda\/eAaFqcRqZzu\\nvfdevvrqK8cNZN26dRk6dCidOnUqMDLgwIEDrF69mri4OMfIgHvvvZcvvvjC4p9ARFzl4x9O8nNU\\nMgCPXB+h4pmIiAs4XUD79ttvMQyDbt26FVs8+6NRo0Yxc+ZMduzYwXfffefs4d3SxIkT6dKlC9de\\ne63VUUREKpVjZ887tsNCdZMvUlpff\/01ixYtwjAMPD09eeWVV5g4cSJeXoXf6uXm5jJz5kyeeeYZ\\nsrOzWbx4Mffee2+hPehFpHKJTc7g9XX2IdxNa\/szvn9LixOJiFQNTs\/iHxUVBUD\/\/v1L3Ta\/Tf5r\\nlNWSJUt4\/PHHuf766wkJCcHDw4P77ruvyDZRUVE88MADNG7cGF9fX5o3b86kSZO4cOHCJft+9tln\\nBAUFERQURHBwMNu3b79knyeeeILvv\/+eJUuWXHYYq4iIFM4rONSxPezG6y1MIlI5zZs3z7G9cOFC\\nnnzyycsWzwA8PT154oknWLBgQaGvISKV14gXPyY1MweAl4Z3wNfb0+JEIiJVg9M90C5evAhAQEBA\\nqdvmt8l\/jbKaOnUqBw4cIDAwkCZNmnD48OEi9z9x4gTXXnst8fHxjBgxgjZt2rBz505mzpzJf\/\/7\\nX7Zv315gGObw4cPp0aOH4\/vGjRsXeL1JkyaxaNEiNm3aRLNmWtlGRKS0YpMyHNtNagdamESkctq5\\ncyeGYdC9e3dGjRpV4najRo1i1qxZ\/PDDD+zcubMcE4pIRXjjs7XEeDcCINwjgb5t6lmcSESk6nC6\\nB1r+XBpHjhwpddv8NvmvUVZvvfUWR48eJSkpidmzZxc7cdzYsWOJj49n1qxZLFmyhGnTprFhwwYm\\nTZrE4cOHee655wrsHxAQQIsWLRxfNWr8Pj\/PxIkT+fLLL\/nuu+9o1aqVS34eEau46ndSpLTOJf9e\\nQKsf7HvZ\/XSOihQuISEBgH79+pW6bd++fQu8hohUPFdc3xKTU5n1fRwAZlY6M8eoR7e4ju7BRMpQ\\nQLvyyisxTZMVK1YQHx9f4nbnz59nxYoVGIbhshU4+\/TpQ0RERIn2PXHiBOvXryc8PJzHHnuswHMv\\nvvgiAQEBfPzxxyXqHTdu3Dg++ugjPvvsM0JCQoiNjSU2Npa0tDSnfg4Rq2lparFKfgGtpr93kUNN\\ndI6KFK5u3bqAcyMD\/P39Ab05ErGSK65v9736KQTaf4\/7103nytYaGSOuo3swkTIU0IYNGwZASkoK\\nt99+O6mpqcW2SUtLY9SoUSQn21eEGT58uLOHd1r+wgUDBw685LnAwEB69epFeno6P\/74Y7GvNWfO\\nHFJTU7nhhhto1KiR4+uNN95weW4RkaosfwhngyJ6n4nI5XXs2BGAX3\/9tdRt86e+yH8NEal81v94\\ngAPZ9QHwSo7i\/5662+JEIiJVj9MFtDFjxjjm+9qyZQvt27dn7ty5hXb\/T0xMZO7cuVxxxRVs2rQJ\\nwzBo1qwZY8aMcTq4s44cOYJhGLRu3brQ5\/OHYR49erTY18rLyyM3N\/eSr8mTJ7s0s4hIVXcyzr6A\\nS1HDN0Xk8kaPHo1pmqxcubJUizSdPXuW5cuXYxhGsQswiYh7Mk2Tt7+Pw\/D0xjTzmDqiIz7eTk91\\nLSIil+H0X1YfHx++\/PJLbrjhBtLT04mKiuLRRx\/l0UcfpUmTJo5hADabjTNnzjjamaZJQEAAixYt\\nwtvbu+w\/QSklJSUBEBISUujz+Y8XthqnKxQ2Z1xoaGihXWJtNluhw2O1v\/bX\/tq\/qu1\/6Of9GDWC\\nOLTjZ3igm+V5tL\/2d8X+FemOO+7g008\/ZdWqVQwbNow1a9ZQv379ItvExcUxfPhw0tLSGDJkSKkW\\nHxAR97FsbxQH4zIBGNDUm1EDr7U4kYhI1WSYxc24X4wdO3Zw1113ERkZ+fuLGkaBff54iIiICD79\\n9FO6dbv0DZIrbN68mX79+nHPPfewcOHCS55\/5JFHmDdvHnPnzuWBBx645Pnnn3+eV155hWnTpvHP\\nf\/7TpdkMwyh2gQMRkeomNT2DK17aCMCVnlGs+PfDFicSKRurrvfp6ek89thjLFy4kJCQEMaPH8+I\\nESPo1KmT40PL7OxsDh48yLJly3j33XdJSkrivvvuY\/bs2fj5+VV45oqiezCpqi6kZ3HDG5uxpWVR\\nN6gGG5\/sQ7BvxXdSEBGxWkVc68vct7d79+788ssvLFiwgIULF7J7926ysrIK7OPj40PXrl0ZPXo0\\n9913H76+1g3Rye9hlt8T7c\/yH69Zs2aFZRJxFzabTROESoX75cTvvZQb1y56AnSdoyKFq1evnmPb\\nMAySkpKYNm0a06ZNwzAMAgMDAUhNTXXcXJqmiYeHB19\/\/bVjWo7LMQyD2NjY8vsBRKo5Z69vr31z\\nBFua\/b3X5CHtVTyTcqN7MBEXFNAAfH19eeSRR3jkkUfIysri1KlTJCYmAlCrVi2aNWuGj4+PKw5V\\nZm3atME0zcvOcXbs2DGAy86RJlKVxcfH68IoFe6XyGjHdnj9oj+80DkqUrj4+PgCIwDyt03TxDRN\\nxwJOf3w+\/5Nam8122U9s8\/f58+gCEXEtZ65vu08l8PnO0wBc1yqUIZ0alkc0EUD3YCJQhgLa6dP2\\nP9aGYRAWFuZ43MfHxzERvzvq168fAOvWrbvkudTUVLZv346\/vz89evSo6GgiItXSsbPnAfvQsfbN\\nGlgbRqSS8vf3L9cilwpoIu4lIzOLB9\/bCPjj4+XBy8Ov0O+piEg5c7qAFh4ejmEY9O7dm82bN7sy\\nU7lq0aIFAwcOZP369bzzzjuMHz\/e8dzkyZNJS0tj7NixVXoeEBERd5KU7enY7tw63LogIpVYamqq\\n1RFEpAI9\/PpnJJp1Afh\/7QIJDy16CgQRESk7pwto3t7e5OTk0KtXL1fmccqKFStYvnw5AOfOnQPg\\n+++\/5\/777wfsq2ZNnz7dsf\/s2bPp1asXEydOZOPGjbRr144ff\/yRTZs20bZtW6ZOnVrxP4SISDVV\\nt1kriD5FUA0vmtTX0AAREZGiHPztNJttARg+QEocz4zob3UkEZFqwekCWoMGDTh79qxjUlor7du3\\nr8CKm4ZhEBkZ6VgZNDw8vEABrUWLFuzatYvJkyfzzTffsHbtWho2bMikSZOYPHmyY6EBEREpf1EX\\nMgBoVFM9f0VERIrzwMxVGAHhAEzo2YCgAF0\/RUQqgtMFtPbt23P27FlHkcpKU6ZMYcqUKaVq07hx\\nY+bPn19OiUQqp9DQUKsjSDUUfeEiAA1rFr9Cs85RERGpikp6ffu\/Zd9y\/n\/Fs5qpJ3ny7nHlmErk\\nd7oHEwEPZxvecccdmKbJ6tWruXjxoisziYhFtLKOWCEmyX4NKUkPNJ2jIiJSFZXk+padm8d7PyUA\\nYGZnMnfsTeUdS8RB92AiZeiBdu+99zJ79mx2797N+PHj1ZtLRERKLT0rh8T0bAAaawiniMucP3+e\\nffv2ce7cOdLS0sjLyytRu8cee6yck4mIsxZ8f5LEPPu18sbGOVzToaXFiUREqhenC2heXl4sXbqU\\nESNG8NFHH3HixAkmT55M3759tYSyiIiUyJEzcY7teoE+FiYRqRrWr1\/Pyy+\/zPbt20vd1jAMFdBE\\n3FRscgZvbTgGQIvQAN59fJDFiUREqh+nC2j9+9tXe\/Hx8cE0TbZs2cKAAQPw8\/OjVatWhISE4OFR\\n9AhRwzDYuHGjsxFERKSS27TzgGM74cxvcE1TC9OIVG4vvvgiL730EgCmaVqcRkRcadqaQ6Rm5gDw\\nr2EdqOHlaXEiEZHqx+kC2qZNmxw9zfL\/NU2T9PR0Dhw4UFRTx77qqSYiUr0dj0kA7Csfd4xobG0Y\\nkUrsm2++4cUXX3R8X69ePW688UYiIiIIDAzUPZdIJfbDcRsr9kUDMPiKBlzfuq7FiUREqienC2hw\\n+U839amnSOVks9k0QahUqDO2VCAE08yjU6tmxe6vc1SkcG+\/\/TZg\/1Bz4sSJvPrqq\/j4aFi0SGVx\\nuetbekYm\/\/hyFwB+3p48P6R9RUcTAXQPJgJlKKBFRka6MoeIuIH4+HhdGKVCxaZmgx9wMZlAf99i\\n99c5KlK4n376CcMwuOaaa5gxY4bVcUSklC53fXto+mecSasHwMO9w7TgjlhG92AiZSigNWtWfE8B\\nERGRoiRlGeAHPjmpVkcRqdTS09MBuOmmmyxOIiKusu9IJNsSgzB8wEiJ46HrbrA6kohItVb0LP8i\\nIiLlyCMoFIDaNTQ\/k0hZhIWFAWjYpkgV8td31mD42Huc\/e26RiXqqS0iIuVHBTQREbGEaZrk+AQB\\nMPSG3hanEancunXrBsChQ4csTiIirvDuV+uxBYQDUDv1JBNHqXepiIjVnC6g9e\/fn\/79+7N48eJS\\nt12+fDn9+\/fnhhvUDVlEpLpKSMsiMycPgEaa00WkTCZMmIBpmqxatYrz589bHUdEyiA7N483t9hX\\n3TSzMpg3frDFiUREBMpQQNu0aRObN2\/mzJkzpW4bFRXFpk2b2LRpk7OHF5FyEBoaanUEqUaiL2Q4\\ntktaQNM5KlK4a665hnHjxpGSksJtt91GUlKS1ZFEpBT+eH37cHskOf7273vWTKJL2xZWxRJx0D2Y\\nSBkWERCRqkcr60hFirpw0bHdqGbJ5nXROSpyeW+\/\/TZeXl7MnDmTDh06MHHiRG688UaaNm2Kr2\/J\\nfsf8\/f3LOaWIFCb\/+habnMHMDccAiKgbwEePj7YyloiD7sFELCqg5ebm2g\/upfqdiEh1FZP0xwKa\\nhnCKlJVhGPznP\/8hPj6eTz\/9lKeffpqnn366VO1zcnLKMaGIFOfVtYdJy7K\/V3px2BXU8Pa0OJGI\\niOSzpIJ16tQpAIKDg604vIiIuIHdhyIB8PY0qBOglQNFyurw4cPcfPPNnDp1CsOwr2xrmqbFqUSk\\npHadTGDZ3igABnVoQO9WGjInIuJOKryAtmvXLj799FMMw6BNmzYVfXgREXET3+87BIHhZCfFOd7s\\ni4hz4uLi6NOnT4EFBDw9PWncuDGBgYH6HRNxc9k5uUxZ+QsANbw8eO6WdhYnEhGRPytRAW3mzJnM\\nnDmz0OemTZvGO++8U+xr5ObmkpiYSFpaGqZpYhgGt9xyS+nSiohIlZGc5w2AX97FYvYUkeJMnz6d\\n8+fPYxgGrVu35rXXXmPQoEH4+Kh3p0hlMOHNz\/nFVguAR\/tEEFZb8xGKiLibEhXQLly4wMmTJy\/5\\n9NI0TRITE0lMTCzRwf44jKBt27ZMmDChFFFFpLzZbDZNECoVJts7CAOo7ZNX4jY6R0UKt2bNGsC+\\nStr27dupXbu2xYlEpKQio2JZ\/dtFvGrVgrQEHuzVzOpIIpfQPZgIeJRmZ9M0HV+FPVbUl6enJ3Xq\\n1KFnz578+9\/\/ZseOHQQGBrr8BxIR58XHx1sdQaqJuIQkDD\/7PJiNQmqUuJ3OUZHC5X\/Qeccdd6h4\\nJlLJ3P\/GV5h59g+TRncKJMi\/5NdFkYqiezCREhbQpkyZQl5eXoEvsK\/W9Prrr1\/yXGFfWVlZnD9\/\\nnm3btvHMM8+oeCYiUo3t+Pk3x3ZEg5oWJhGpGoKCggBo3LixxUlEpDRWbN5FpFcYAH7Jp5ny1+EW\\nJxIRkcspVQ+0P9PKTiIi4oz4jN+3r2oVZl0QkSqiVatWAMTGxlqcRERKKi8vj3988ROGhyemmcsb\\nd\/fAw6NMb89ERKQcOf0XOjIyksjISB566CFX5hERkWogz\/\/3IWY39OhsYRKRqmHUqFGYpsmaNWv0\\nAadIJbH6QAyZIU0BaJoXy829rrI4kYiIFMXpAlqzZs1o1qyZY8iAiIhISZ1JSAfAz9uTOgFaJVCk\\nrB588EE6duzIb7\/9xrRp06yOIyLFSM\/K4ZW1hwGo5e\/Fq38dbHEiEREpjvoIi4hDaGio1RGkmjib\\naC+ghdX2u2SF56LoHBUpXI0aNVi5ciVXXHEFkydPZuzYsURHR1sdS0QuY86m48Qk2eczeHpwOzq2\\nbWVxIpGi6R5MBLycbZiVlcW4cePIzc2lb9++3HfffSVqt2DBAjZv3oyPjw\/vvvsunp6ezkYQERfT\\n0tRSUU7\/rwda09r+pWqnc1SkcHfccQcATZo04eDBg7z\/\/vvMmzePDh060LRpU3x9fYt9DcMw+PLL\\nL8s7qki1d9qWzv9tOQFApyYh3N41DA+Pkn+YJGIF3YOJlKGAtmLFCubPn49hGDz44IMlbteiRQvu\\nv\/9+DMNg8ODBDB+ulWZERKoT0zQ5k3ARgCa1SldAE5HCffXVV47enPn\/5ubmcvDgQQ4ePGhlNBH5\\nk5e\/\/pWsnDwA\/jWsg4pnIiKVhNNDONesWQNAWFgYPXv2LHG76667jiZNmgCwevVqZw8vIiKVVGRM\\nPBezcwGo5ZNrcRqRqsM0zQJfhT1W1JeIlL+3F61n\/a\/21XJv69KELk1rWZxIRERKyukeaLt27cIw\\nDHr37l3qttdffz2fffYZP\/30k7OHFxGRSur7\/Ucc2ykxJ4FOVkURqTJ0TyXi\/lLTM3hz81kIqoeR\\nm8k\/B7WxOpKIiJSC0wW0U6dOAdCyZctSt81vk\/8aIiJSfRyMjAHsQzc7RTSyNoxIFdG1a1erI4hI\\nMR5+\/TPMoPoA9ApJol5w8XMTioiI+3B6CGdGhn3VmJJMSvtn+W3S0tKcPbyIlAObzWZ1BKkGTsQm\\nOba7dyjdhzA6R0VEpDI6+Ntptl8IBsBIieO9J+4s8Lyub+LudI6KlKGAVquWfbx+XFxcqdvmtwkK\\nCnL28CJSDuLj462OINVAdFImAObFZOrXqVmqtjpHRUSkMnrw7ZUYPn4APN67IYH+BTsh6Pom7k7n\\nqEgZCmhhYWGYpsmWLVtK3Ta\/TePGjZ09vIiIVFIJWfZLj3d2isVJRKq+9PR0Tp8+zaFDh0hMTLQ6\\njki19MOxWGL9mwMQknKSSXcOsjiRiIg4w+kCWt++fQHYu3cvmzdvLnG7TZs2sWfPHgzDoE+fPs4e\\nXkREKimf2vZ5z5rU1NwvIuUhNjaWZ599liuuuIKgoCCaN2\/OFVdcwYcffnjJvq+99hovvfQSn3zy\\niQVJRaq+vDyTV\/57DABPw+Tdh26wOJGIiDjL6QLa3XffXWD7+PHjxbY5fvx4gXb33HOPs4cXEZFK\\nKCM7l+Rc+\/o1w\/p2tziNSNWzaNEiWrduzWuvvcahQ4cwTRPTNC+7\/9GjR\/nXv\/7F2LFjNTetSDlY\\nvPsMB87a5\/58rF8rendua3EiERFxltMFtKuuuorbbrsN0zSJjo6mS5cuvPLKK0RFRV2yb3R0NNOm\\nTaNLly7ExMRgGAZDhgyhe3e9eRIRqU5OJ6ST\/14+PDTA2jAiVczixYu56667SE1NxTRNateuTb9+\\n\/Yps89BDDwH2oZ5r1qypiJgi1UbSxWz+880RABqG+DK2b4TFiUREpCycLqABzJs3j9atWwOQmprK\\n888\/T9OmTWnSpAldunShS5cuNGnShLCwMF544QVSUuzz3bRs2ZIFCxaUPb2IuFRoaKjVEaSKi4z\/\\nvYdLcycKaDpHRQpns9l48MEHycvLw9fXl3nz5hEXF8fGjRuLbNejRw8aNGgAUOy+IlI6Mzccw5aW\\nBcCzN7fD38frsvvq+ibuTueoSBkLaCEhIWzbto1+\/fo5hgiYpklMTAz79+9n\/\/79xMTEFBg+0L9\/\\nf7Zt20bNmqVbeU1Eyl+dOnWsjiBVXFkLaDpHRQo3Z84cUlJSMAyDTz75hAceeADDMErUtlu3bpim\\nyb59+8o5pUj1sWXfURZ8HwlAt+a1GdKpYZH76\/om7k7nqEgZC2hgr0Rv3LiRFStWMGjQIAICAgoU\\n00zTJCAggMGDB7Nq1So2bNhA3bp1XZFdREQqmZP\/K6DV8vempr+PxWlEqo61a9cC0LVrV2699dZS\\ntW3Tpg0AkZGRLs8lUh3l5eXxyPsbyTXBwORfQzuUuKAtIiLu6\/L9iEtp6NChDB06lJycHE6dOoXN\\nZgPslerw8HA8PT1ddSgREamkNu89DARQxyfX6igiVcqxY8cwDKPYOc8KU6tWLQCSkpJcHUukWnr5\\nw5VcDG4KQNOs07RvFGxxIhERcQWXFdAcL+jlRUREBBERmiRTREQKik7OxvCHlOgTVkcRqVLyi1\/O\\nDLHJzs4G7PdwIlI2icmpfLg3CQJDMTNS+eCpv1gdSUREXKTMQzhFRERK4lx8Ioa\/ff7LJjU1fFPE\\nlcrSi+zMmTOA5rcRcYW\/Tv8cAu2Trd\/UKIuIJg0sTiQiIq7ikgJaXl4en332GaNGjSIiIoLg4GA8\\nPT2ZMWPGJfsuX76cpUuX8sMPP7ji0CLiQvlDr0XKw7b9Rx3bbRvXduo1dI6KFK5Zs2YA7Nmzp9Rt\\nN27ciGEYdOjQwdWxRKqVw6dj2Z1uL0R7JMfwzqQ7S9xW1zdxdzpHRVwwhHPv3r2MHDmS48ePOx4z\\nTfOyE2V++OGHrF69mtDQUKKiojRcQMSNxMfHqweClJs9x84CvgB0bRXm1GvoHBUp3IABA\/jpp5\/4\\n9ttvOXPmDGFhJfsdW7x4MZGRkRiGwQ033FDOKUWqtne2RWF41wDg6YEt8PEu+fscXd\/E3ekcFSlj\\nD7QdO3Zw3XXXcfz4cceKm6GhoUW2GTt2LKZpEh8fz4YNG8pyeBERqUQORyU4tnt3bmOfmM0jAAAg\\nAElEQVRhEpGq5\/7778fLy4ucnBxGjx5NZmZmsW327NnDo48+CoC\/vz9jxowp55QiVdePJ2ysPhAD\\nwJBODXl4RH+LE4mIiKs5XUDLyMjg\/\/2\/\/0d6ejqmafK3v\/2Ns2fPEhsbW2S7AQMGULOmfQ6cdevW\\nOXt4ERGpZIIatQDA18ykXu0Qi9OIVC0tW7Zk3LhxmKbJ5s2b6d69OytXriQ9Pb3AftnZ2ezatYsn\\nn3yS6667jsTERAzD4Omnn1bPAhEn5eTm8a+VvwDg6+3Bsze3sziRiIiUB6cLaPPmzSMqKgrDMJg+\\nfTozZsygUaNGxbbz8vKiW7dumKbJ7t27nT28iIhUMom59mEtnSMaWpxEpGp6\/fXXGTRoEKZpcvDg\\nQW699VaCg4Mdzz\/99NP4+vrSvXt33nrrLS5evAjAiBEjeO6556yKLVLpfb7zNIfPpfD\/2bvv6CjK\\nvo3j39n0DiTUBAihI0qVqoCCgKCCvYBUGxYUsOMD6suDFbuIiqCCiAWkiFKlNwVNqNISSoAACZCQ\\nRsrO+8dKfBCEkGwyu5vrcw7nwO7szJXsTe7Jb+8C8EinOlQrF2BxIhERKQlFLqDNmTMHcHziOWLE\\niEt67ZlFanfv3l3Uy4uIiBux2012H00HoF7lEIvTiHgmLy8v5s6dyzPPPIO3tzemaWK32wvWpc3P\\nzy9YcsM0Tby9vXn66af57rvvLE4u4r6Op5\/mzYWOTXKqVwjg\/g4xFicSEZGSUuQC2pYtWzAMg+7d\\nu1\/yaytUcOy+dvLkyaJeXkRKwMXWMBQpqoMns8jMyQegbjEKaGqjIhfm5eXFK6+8Qnx8PC+88AJt\\n27YlICCgoGjm5+dHkyZNePLJJ\/nzzz959dVXsdmcsim7SJl08+jPSM3KBeCFno3w9\/Eq0nnUv4mr\\nUxsVKcYunGe2sS3MtM1\/OvNJqN1uL+rlRaQEaP0bKSm7jp4q+HvdSsFFPo\/aqEjhREZG8vLLL\/Py\\nyy8DcPr0afLy8ggKCrI4mYjnmLn0V\/Z6RWEAIRkH6dqoR5HPpf5NXJ3aqEgxCmghISGcOHGiYP2M\\nS5GUlAToP6GISFmx60h6wd81hVOkeL788ksArrzySho2LNxi5X5+fvj5+ZVkLJEyxW6389y3v2OE\\nVcfMz+PNe1oXDBIQERHPVOQx+5GRkYBjKuelWrlyJYZhUKdOnaJeXkRE3Mh3C1cBEGjLp0KQr8Vp\\nRNzbgAEDGDhwID\/\/\/LPVUUTKrJEfz+R0WHUA6nKIbm2bWJxIRERKWpELaJ06dcI0TRYsWEBqamqh\\nX7dq1Sri4uIKziEiIp5v74nTANhPHrQ4iYiISPEcO5HGtG2OWThmViqfP3mHxYlERKQ0FLmA1qdP\\nHwAyMzMZOnRooV5z5MgRBgwYADgWue3fv39RLy8iIm4iLy+fXH\/HlP3K\/qbFaURERIrnk5V7MYIc\\nm6LdFA1RlbUsjYhIWVDkAlqrVq245ZZbME2TqVOncvvtt7Nnz57zHpuRkcHkyZNp2bIl8fHxGIbB\\noEGDqFWrVpGDi4jzndkcRMSZftu2B8PXH4D6VYq3\/pnaqIgAvPLKK7Rq1YqwsDAqVarETTfdxNat\\nW62OJWXA\/pRMvvjVMZq6brgvbz92p1POq\/5NXJ3aqEgxCmgAkyZNolGjRpimycyZM6lXr95Z65q9\\n88471K9fn\/Lly3Pfffdx6NAhAJo2bco777xTvOQi4nTJyclWRxAPtCJuV8HfW9aNLNa51EZFBGDF\\nihU8+uijrF27lqVLl+Lt7U2XLl04efKk1dHEw42Zt42cPDsAr97REm9vL6ecV\/2buDq1UZFi7MIJ\\nEBoayqpVq7j33nuZN28eAAkJCQU70Bw86Ph0xjT\/nrLTo0cPpk2bhr+\/f3EuLSIibiI2PgmoAsC1\\nLRtZG0ZEPMI\/N1CYMmUKYWFhrF69mp49e1qUSjzdip3HWLjtCAC3NI+kRc3yFicSEZHSVKwRaADl\\nypVj7ty5LFq0iN69exMWFoZpmmf9CQwMpHv37sybN48ff\/yR0NBQZ2QXERE3UC76MgACbXnUqV7F\\n4jQi4onS0tKw2+2UL6+ChpSM3Hw7L811TBMO8vXi2e4NLE4kIiKlrVgj0P5X586d6dy5M6Zpsm\/f\\nPlJSUsjLyyM8PJyaNWvi4+PjrEuJiIgbiT+eA0CL2iqeiUjJePzxx2nevDlt27a1Oop4qGHjZ7Hn\\nmB8AQzvXpVKoZtOIiJQ1xR6B9k+GYRAdHU2LFi1o3bo1derUUfFMRKSMys7NZ\/exdAAuqxZmcRoR\\nz\/LUU0\/h5eXltD\/e3s75XHXGjBkMHTqUDh06EBYWhs1mo1+\/fhd8zcGDBxk0aBCRkZH4+\/tTq1Yt\\nhg0bVqg1zYYPH86aNWuYMWNGwTIiIs60LT6RufH5AISSxcD22ghNRKQsctoINBFxfxEREVZHEA+z\\nI+kU+XbHOpiXVSv+9H21UZG\/\/e8as65kzJgxbNq0ieDgYKKiovjzzz8veHx8fDxt27YlOTmZ3r17\\nU79+fX799VfeffddFixYwOrVq\/91auawYcP49ttvWbZsGTVr1iyJL0eEwe\/OxgiIBuDexoH4ejt9\\nDIL6N3F5aqMiJTACTUTcV3h4uNURxMNsPZRW8HdnFNDURkVc3zvvvMPOnTtJTU1l\/PjxFy30DRky\\nhOTkZN5\/\/31mzJjB2LFjWbx4McOGDePPP\/9k5MiR533d448\/zjfffMPSpUupW7duSXwpIkybv5pD\\n\/jUACDm1l6f69iiR66h\/E1enNiqiEWgiIlKCth5KBRwLLkeHB1mcRsSzjBw5kvvuu8\/qGOfo2LFj\\noY+Nj49n0aJF1KpVi4cffvis51566SU++eQTpkyZwrhx4wgICCh47pFHHmHq1KnMnj2bsLAwjhxx\\n7IwYHBxMUJB+1ohz5OXlM3r2FoywKMy8XD4YdI3VkURExEIXLaDFxMSU2MUNw2DPnj0ldn4REbHW\\nd4vXQUg1QvLTsNm0NpGIM5UvX97tpy0uXboUgK5du57zXHBwMO3bt2fRokWsW7eOa675u3jx0Ucf\\nYRgGnTt3Pus1o0ePZtSoUSUbWsqMTxbFkhsWBUAj7yQ6tuhtcSIREbHSRQtoe\/fuLZEFWU3T1EKv\\nIiIeLCc3j9P+4RiAf3ay1XFExAXt2LEDwzCoV6\/eeZ+vW7cuixYtYufOnWcV0Ox2e2lFlDLqVHYu\\nk38\/AUAAOUx+8k6LE4mIiNUKNYXTVRepFRER1\/XLb1swfPwAuDyqnMVpRMQVpaY6pnmHhZ1\/l94z\\njxdmN04RZ\/rgl90cO3UagLF3XkmViPNvZCEiImXHRQto+oRPpOxISUnRAqHiNEt+3wk41iLq1KSO\\nU86pNioiRbVjx45zHouIiDjvz5SUlBSSk88dOavjy8bxe46lM2l1AgDNa5Tj6ur+Jd5+du3adc5u\\ns676\/dHxZfP4EydOnNVGrc6j43X8hY4vKdpEQEQKJCcnqzghTvP73hTwDsLMz6Vr68udck61URHP\\ncmaE2ZmRaP905vFy5Yo\/irV+\/fqFPjY8PPySftboeM853jRNXp67jdx8E8OAl25qTEREGBERESWa\\nJzk5udBt1J2+nzrec47fsWNHodqoq+bX8TreGWyldiURESlTDuX4AuCbcYSQoICLHC0iZVH9+vUx\\nTZOdO3ee9\/ldu3YB\/OsaaSLO9vWKLSzfeQyAu66szuVR559eLCIiZY8KaCIi4nSn8\/LJD6kGQLeW\\nhR\/1ISJly5mNARYuXHjOc+np6axevZrAwEDatGlT7Gtl5uQV+xzi2U5lZPH8dxsBCPSGJ7uq\/xIR\\nkb9ZUkBLSUlh06ZNbNq0yYrLi4hICdt2KI2cfMcaml1baOSIiDNNnjyZSZMm0b17d6ujFFtMTAxd\\nu3Zl7969fPDBB2c9N2rUKDIyMujXrx8BAcUfxfrmgvOPchM54743p0FwRQBaBSYTHuxncSIREXEl\\nhVoDLSYmBoCRI0cyePDg8x6zfft2fvvtNwD69et3wfN9\/vnnPP300xiGQV6ePg0UEfE0f+z\/e8e8\\nZtW1A6eIM\/Xv39\/qCBc0e\/ZsZs2aBUBSUhIAa9asYeDAgYBjIeA33nij4Pjx48fTvn17Hn\/8cZYs\\nWULDhg1Zt24dy5Yto0GDBowZM8YpuSatjqeWVwr39rjaKecTzxK3cy\/rUsth+IItLYnxL\/a1OpKI\\niLiYQhXQ9u7di2EY\/7rAK8BPP\/3EU089hc1mu2gBDRwLdIqIa7mUBXJFLuSPA44CWkSwH1Hlnbf+\\nmdqoiOuLjY3lyy+\/LPi3YRgkJCSQkODY1TA6OvqsAlpMTAwbNmxg1KhRzJ8\/n59\/\/pmqVasybNgw\\nRo0aVbDRQPEZjJq3k94dW2pdRjnH\/R\/MwwiMBmBYp+oE+pfu6DP1b+Lq1EZFSmAXThXGRNyXdjcU\\nZ\/lj\/wkAmtUoh2EYTjuv2qiI6xs9ejSjR4++pNdERkby2WeflVCiv5khVeg7dgqz\/\/tAiV9L3MdX\\nC9Zy9K\/iWblTCTx2x6OlnkH9m7g6tVERbSIgIiJOlnA4hcQTWYCjgCYi4gpsaYcBiM2pxNwVGy1O\\nI64i327y9Y6\/lpTJy+GjB7taG0hERFyWCmgiIuJUU35eVfD3kNPJFiYREfnba7dejmnPx\/DyYdj0\\njWSfzrE6kriAbzccYMuhNAAeva4hba\/QxjciInJ+KqCVsp83H7Y6gohIiVqx\/RAAZn4e3a5saHEa\\nERGH2zu3ppHN8fMpLzSSj37ZYXEisVpqZi5vLHC0g2ph\/jxyTV2LE4mIiCtTAa2UPT49lpW7jlkd\\nQ0SkxCRkOJbX9E1PolIFZy3+LSJSfNOe70uEbz4AH61OZPfRdIsTiZXeXryT4xmOkYgjezYiwNfL\\n4kQiIuLKVEArZTn5du7\/4je+mr\/G6igi50hJSbE6gri5fYePkRdSBYC6Yc7fVEZtVESKo3xoMB8P\\nvgrDgJw8O09\/H0e+XRtglUV\/Hk5jyrp9ALSJqUCPy6tYmkf9m7g6tVERFdBKnc2A7DyT5+cf4MdV\\nv1sdR+Qsyclar0qKZ+qCtRiGo2u5rkm008+vNioixdWiZgUGta8FwO\/7TzJ5dYLFiaS05eXlc8vY\\nb8i3m3jZDEbfeJlTd4wuCvVv4urURkVUQCt1PSufAsDwD+aRb7ezJk7rb4iI50jI8AEc65\/16dbW\\n4jQiIuf3ZNf6RIcHAvDGgh3EH9NUzrJkxAffkhlUDYDGPsdoWDXU4kQiIuIOVEArZe8\/cRdtA5IA\\nMALLcc+n69i8e7\/FqUREnONwfjAAzaMjtP6ZiLisAF8vXr+tCYYBp\/Ps3DFuDjm5eVbHklIQn3iE\\nWX8NOjQzTzDhsZusDSQiIm7D+1IO3rNnDytWrPjX585YuXIlpvnv60n877Fl0dejB3P9sxPYTnUI\\njqDXWwtZM+YOqlTQp18i4r5SM3PZdjgNgHZ1wi1OIyJyYa1qVaBbLT\/mx58mmTDuf2MqXzw\/wOpY\\nUsL6jfseIyAagHsvC6RqRHlrA4mIiNu4pALahAkTmDBhwr8+f2btgE6dOhUrVFkwb+wDXD38Iw76\\nR2MPrcrD0zcz9b7WBPpe0lsiIuIy1sancOazk9a1VEATKSnPP\/98iZ177NixJXZuV\/R\/t7di\/n9+\\ngOAIlqWEsnzjNjq2aGR1LCkhU35ayQHfGhhAYNo+Xr7\/IasjiYiIGzHMCw0V+4vNZsMwjAuOKoO\/\\nC2iFOc40TQzDID8\/\/xLiurd\/fg9zcvO45\/1FbDhqB+CqOhFM7N8Sfx9toS3WSElJITxchQ8pmudm\\nbuLrXw\/g520jbnTXEvlZpjYq7qAw90zFcea+rCR46n3Zhd6Tj2Ys4bXfsgHwSzvA1vfux9tb92Ke\\nJi\/fTtdxS4g\/noOZl8MXd9enkwsVS9W\/iatTGxVXV9L3X1DIAlp0dHSJ3aglJJSdnY\/O94bm5Nm5\\n\/8sNLN95DIDrGlVmfJ\/m+HhpeToRcR+madLu1V84nJpNx3oV+WJQK6sjiVimtApozr6GJ3+webHv\\nV8fh49nnWxOAa8slM+nZ\/qUVTUrJ5NUJvDR3GwB3XhbCa\/d2sDiRiIg4U2kU0Ao1X3Dv3r0lGqIs\\n8\/W2MaFvC\/pP\/pVfE46zaNsRnvwujrfuaIqXzdrttEVECmvh+s0cTnWM4GhXS5sHiJQGLy8vrr32\\nWtq1a2d1FLf33Qt9aDVqFgRHsOJkefanZFLjr106xf0dSctm3MKdANQMD+Slu9pbnEhERNxRoUag\\niXNcqCJ6KjuXPhPXsykxFYDaJLFo7EBsNo1EExHXd8\/Lk1iTWRmASbfW5NorG1ucSMQ6pT2Fs169\\negwcOJD+\/ftTuXLlEruuOyvMe\/LJ7OWMXZsOODYYmH5\/G2z6MNMjPPb1H8yNOwTA5wOvpFP9ShYn\\nEhERZyuNEWiqzriIEH8fvhjYipphPgDsoQrdnvkYu91ucTIRkYv7I+m04y\/px1xqTRkRTzRjxgx6\\n9uyJl5cXpmmyc+dOnnvuOapXr06vXr2YNWuWx07FLEkP9OpIn9Y1APg14TiTVpedZUY82apdyQXF\\nsx6XV1HxTEREikwFNBdSPsiX8Xc0hHTHemi7vGpwy6iJFqcSEbmwo8dTyQysCkBN30yNnBUpYTff\\nfDNz5szhwIEDvPLKK9SrVw\/TNMnLy+PHH3\/k1ltvpVq1aowYMYItW7ZYHdetPN+jITX\/mrr5+oId\\n7DxyyuJEUhynMrJ4aOIvAAT5evGfG\/QBj4iIFJ1+y3Exl9WuzjcPtcfMOA5AbF4kff9vksWppKxI\\nSUmxOoK4ofe\/X4Lh7Rg9271pjRK9ltqoyN8qV67MM888w\/bt21m1ahWDBg0iODgY0zQ5duwY77zz\\nDk2aNKFVq1ZMmDCB1NRUqyO7vCA\/b8bd3gTDcGz0NPzbWHLzNRvAXfV\/dSrpRhAAtzcKompYgMWJ\\n\/p36N3F1aqMiKqC5pNaN6zK5XzPMTMeN7qqMyrw+c43FqaQsSE5OtjqCuKGfNx8GwMzJ4uGbrynR\\na6mNipxfu3btmDhxIocPH2bSpElcffXVmKaJaZps3LiRRx55hKpVq9KnTx8WL15sdVyX1jK6Ag92\\nqA3AloNp9PnvFIsTSVEsWreJjVkVAbClHeaZm1tbnOjC1L+Jq1MbFVEBzWVde2Vj3ru1Hma2YzHb\\nj347wezYgxanEhE5W3ZuPpnlHb9o1g\/NIywkyOJEImVbYGAgAwYMYPny5ezatYtnn32WatWqYZom\\n2dnZTJ8+nW7duhEdHc13331ndVyXNey6ulSwZQGwPr080xbog0x3kpeXz6NfrMHw9sE07bx8YwMC\\n\/HytjiUiIm5OBTQX1qtjS6Y+0I4gXy9ME4Z\/G8fCrUlWxxIRKbBmTzKZOY7Fyh+\/paPFaUTkf9Wu\\nXZuxY8eyb98+5s2bx2233YaPjw+maXLgwAFWrlxpdUSX5eftxehu0Zj5uRg2L0bO2cHxtHSrY0kh\\nPfr215wOqw5A7fwD9L3+KosTiYiIJ1ABzcVd3ag6nw24Ej9vG\/l2k0en\/cHKXcesjiUiAsD8LY6i\\nvq+3jU71K1qcRkTOx2azceWVV9K2bVuqV69udRy30atjS1oHOdb8MUMqcdvLmsrpDo6kZbMkJRQA\\nM+M405692+JEIiLiKVRAcwNtYsKZcG8LfLwMcvLt3P\/lBlbu0Eg0EbFWXr6dRduOANChbkWC\/Lwt\\nTiQi\/8tutxfsyhkVFcWTTz5JfHx8wfNRUVEWpnMPU57vh29aIgDx3jX48HutH+fqRs\/eSi5eAIzo\\nGEmViPIWJxIREU+hApqbuKZ+Jd67qxleNoPsXDt9P1nNxNlLrY4lHiYiIsLqCOJG1sancCIzF4Du\\njauUyjXVRkUubseOHTz77LNUr16dXr16MWvWLHJycjBNk5o1a\/Liiy+SkJDA008\/bXVUl+fn68Mn\\ng6\/GzM0GYNKWHFL\/+rknrmf+liTm\/7XcyY1NqjH0tmstTlR46t\/E1amNioBhmqZpdYiywjAMivvt\\nnr5mN8\/M2o5hs2HPyeS\/Xapwb4+rnZRQRKTw+n6wiFWJOfh62\/htZBfCAnysjiTiEpzR31+q9PR0\\npk+fzuTJk1m3bh1AQYaAgABuueUWBg0axDXXlOxOua6quO\/Jf75YyJTtjsJZzyuq8sHdzTAMw1nx\\nxAnSsnPpMm45R0+dplygD4uHdyQi2M\/qWCIiUkpK4\/5LI9DczF3t6nBrVAamacfmG8gLi5KYvnCt\\n1bFEpIw5diKNlfFpANTyOaXimYhFli9fzoABA6hatSoPPvgg69atwzRNTNOkVatWTJgwgcOHDzNl\\nypQyWzxzhpf7Xce1DSoBMG\/TYb7fmGhxIvmn137+k6OnTgMwskdDFc9ERMTptGCNG3pr6F3kvDWN\\nH4+GYfgF8szPB\/D18eKWa1pZHU1EyojXv5qP4RsEQIea\/hanESlbEhMT+fzzz\/n8889JSEgA\/h5t\\nVrlyZfr27cugQYNo2LChlTE9imEYvH7bFXR\/ZyXJ6acZPWcrV0ZXIDoiyOpoAnw+byVfrXd8qNO+\\nTji3tdD6fiIi4nyawlmKnD2k8MHXp7DgeAUAAr3szHysIw2qhDrt\/CIi\/+ayh8eTEVoTM\/sUW8f2\\nJjhQRTSRM0p6CoG3t3fBKLMz\/+7RoweDBg2iZ8+eeHl5ldi13ZWz3pPlO4\/Rf9KvADSJCuO7h9ri\\n663vt5VSUk\/R4vnvIaQSNjOfpU93pma4CpsiImVNaUzhdEoBzW63M336dObMmcNvv\/3GsWPHyMjI\\n4I033mD48OFnHTtr1izsdjtVq1albdu2xb20WymJN3TgK1+wNNWxoGN4kC\/TH2hD3cohTr2GiMj\/\\n2rx7Pzd8Eoth86Jmzj6Wv\/Ww1ZFEXEpJ38DZbDYMw8AwDK699lruvfdeKleu7JRzd+3a1SnncTXO\\nfE9enruNSasdI\/8uMxKZ98qDTjmvFE2Xpz5it1cNANoFHmHaqEEWJxIRESu4RQHtjz\/+4M4772TP\\nnj0Fj5mmiWEY5y2g9erVix9\/\/JGIiAgOHjyIt3fZmUVaUm\/opyvi+e9P2wGoGOLHNw+0IaZisNOv\\nI54vJSWF8PBwq2OIi7vzxc9Yn+3YdXNMxzD6Xn9VqV1bbVTcQWkV0JzNMAzy8vKcfl5X4Mz35HRe\\nPs2f\/poM3\/KYpp1nWwUy5NbOTjm3XJqJs5fyf2vSMQwbPqmJbH1vML4+7vm7hfo3cXVqo+LqXH4T\\ngfXr13P11VezZ8+egqkEF9vedsiQIZimSXJyMosXLy7O5eUv93eI4alu9QE4duo093y6nn0pGRan\\nEneUnJxsdQRxcXa7SbxRDQCvU0e4p1u7Ur2+2qjI387ceznzj1ycn7cX425rjJl3GsOw8eqyQyQc\\nPGJ1rDLn2Ik0xizah2HYMPNO8\/Gg9m5bPAP1b+L61EZFirGJQHZ2NrfddhuZmZkAPPHEEzz55JNU\\nq1YNm+3f63JdunShXLlypKamsnDhQrp3717UCPI\/HrmmDrn5dt5ZvIuktGxu+WAF79wUzdXNtICw\\niDjPil3HOJZlB+CFuzte8Oe9iJSMDh06lMgINCm87u2a0m3VZhYe98MIqkDvsd\/zx\/tD9DOxFL08\\nKxaCKwLQoVwq117Z2OJEIiLi6YpcQJs4cSIHDx507Er0+uuMGDGicBf09qZVq1YsXLiQjRs3FvXy\\nch6Pd65Lbr6dD5fuISXLTt9JG\/hygEnHFo2sjiYiHuKr9fsBCPDx4taWNSxOI1I2LVu2zOoIAkx4\\nsg9NH\/uItJBoUkOieWjcdD556h6rY5UJa\/ekMHfHKQCqeGfw2TP3WpxIRETKgiJ\/TDZnzhwA6tSp\\nU+ji2RmXXXYZALt37y7q5eU8DMPgya71aeqb5Ph3UAX6ffE7SzdstTiZiHiCw6lZLNnumKbUq2k1\\nQv19LE4kImIdm83G7Odvw8xIAeCXE+WIO3DS4lSeL+N0Hk99HweAv4+N6U\/0dOupmyIi4j6KXEDb\\nsmULhmEUaQpmhQoVADh5UjcZzmYYBjNfHMgVtkTHvwPLM+DLWBb\/utniZCLi7qau24f9ryWS+rSu\\naW0YEREXUCuyMhMHtsPbZpBnN3lk2u+kZuVaHcujjf1pO4knsgB4tnsDoiOCLE4kIiJlRZELaCkp\\njk\/bqlWrdsmvPbNuh91uL+rl5QJsNhtzxj5IU++DABiB5Rj81WY27ztmcTJxdRfbBETKrqMnUvly\\nzV4Amtcox+VRYZbkUBsVEVdzXfM6PHt9AwAST2Tx9Pdx2pChhCzZfqRgKYHWtSrQr220tYGcSP2b\\nuDq1UZFiFNBCQkIAyMrKuuTXJiU5phhqG9ySNWvMAzT3+auIFhDGwClx7DpyyuJU4sr0f1L+zTMT\\nZnHqdD4AfVpUtiyH2qiIuKLBV9WiS0PHz8YFW48wefVeawN5oK17DvDwF2sBCPHz5s3bm2Czec5m\\nGurfxNWpjYoUYxOByMhITpw4wZYtWy75tStXrsQwDOrUqVPUy7uE8ePH8\/HHH7N3717AsbbbCy+8\\nQI8ePawN9j9m\/t8D\/Ofr1UyJO0ly+mnu\/nQdX93XhvpVQqyOJiJuIvt0DksPGhAMpB\/jxubdrI4k\\nIoWwdu1aFi1adNHjGjduzC233FIKiTyXYRi8efsV9HxvFQdPZvHKz9upZEvnhnaXWx3NI9jtdu4Y\\nN5fToY7lA57rGkP1CoEWpxIRkbKmyAW0Tp06sXnzZhYsWEBqaiphYYWbzrNq1Sri4uIwDINOnToV\\n9fIuoXr16rz++uvUrVsXu93O559\/Tu\/evfn9999p3Nh1ttL+v7vbU7Xqbl6fv4Pk9Bzu\/nQd0+5v\\nTYMqoVZHExE38H+fz4Vgx6eOXWt4abFmERewf\/9+OnbsSH5+PnXr1mXx4sUFS2ScsWbNGl588cVz\\nHv+ngIAAWrduTWRkZElG9njlAn354J5m3D5hDbn5Jo98HUu9yHDq1bz05U7kbIg1pW4AACAASURB\\nVINenULGX8WziIy93NO+p8WJRESkLCryFM4+ffoAkJmZydChQwv1miNHjjBgwAAAvLy86N+\/f1Ev\\n7xJuvPFGunXrRkxMDHXq1GHMmDGEhISwdu1aq6Od4+FOdXjur\/U5jmfkcPcn6\/g9QWuiiciF2e12\\npscdB8DMPsUrD\/a2OJGIAIwdO5Z9+\/Zx6NAhRo8efcEimWmaF\/yTlZXFyy+\/XIrpPVezGuXpEOJY\\nJ9gIqsBNr80m+3SOxanc29wVG1l6\/K8PfTNSmPXCXdYGEhGRMqvIBbRWrVpxyy23YJomU6dO5fbb\\nb2fPnj3nPTYjI4PJkyfTsmVL4uPjMQyDQYMGUatWrSIHdzV2u53p06eTkZFBu3btrI5zXg92rM3I\\nHg0BOJGZy83vL+OLeSssTiUirmzM53PJD3WMnmgRcorwME3\/FrFaVlYWU6dOxTAMrrvuOjp06HDB\\n4w3DoH\/\/\/uf907BhQ0zTZPr06UVa11bONfGZvlTM2AtAdmgNbnjhM2sDubGTp9J5fHoshrcvpj2f\\n\/1xXg6jKWodJRESsYZjF2CYoLS2Ndu3asW3btoJPPmvVqlVQJIuMjCQgIICEhATy8x2LT5umSbNm\\nzVi9ejX+\/v7O+SostGXLFtq2bUt2djYhISF89dVXXH\/99ec91jAMl9iV6YOFW3jzl30AmDnZPNMu\\njIdv62JxKnEFKSkpWiBUCtjtJl3f+oXdydmYpzNY81wXIitVsDST2qi4g5Lu77\/\/\/nvuuOMODMPg\\np59+olu3869LOG7cOJ566ikMwyi4D\/unVatW0aFDBwzD4Ouvv+aOO+4osdxWKu17sJTUU7R6bnrB\\nBxB3VM\/k9UduL7Xre4pnvv2db34\/DMAVtkTmjH3Q4kQlR\/2buDq1UXF1pdHXF3kEGkBoaCirVq2i\\nZ8+eBdMAEhISCoppBw8eZPfu3eTl5RU836NHD5YuXerU4tmMGTMYOnQoHTp0ICwsDJvNRr9+\/S74\\nmoMHDzJo0CAiIyPx9\/enVq1aDBs2jJMnT55z7LRp0wgJCSEkJITQ0FBWr15d8FyDBg2Ii4vj119\/\\nZciQIfTr149t27Y57WsrCY92bcyNlVMxTTuGrz+vrUvj9anzrI4lLiA5OdnqCOJCFm5LYndyNgBP\\ndL\/c8uIZqI2KACxfvhyA4OBgrrvuumKd66qrrqJGjRoAZ93fSPGEh4Uw9aEOmNnpAHy\/35eth1It\\nTuVeZsceLCie1Qyy8+3ogRYnKlnq38TVqY2KFLOABlCuXDnmzp3LokWL6N27N2FhYeesrREYGEj3\\n7t2ZN28eP\/74I6Ghzl28fsyYMXz44YfExcURFRV10cVy4+Pjad68OV988QVt2rRh+PDh1K5dm3ff\\nfZd27dpx4sSJs47v1asXcXFxxMXFERsbS8uWLQue8\/b2JiYmhmbNmvHf\/\/6Xpk2b8vbbbzv16ysJ\\n7w+7h3uiT2Pm52F4+\/FhXC7\/+WSm1bFExEXk203eWbwLgHKBPtzX0b13TRbxJLGxsRiGwVVXXYXN\\nVuxbOdq1a4dpmmzYsMEJ6eSMtlfUZ3ibchiY2A1vHpyykeMZWg+tMHYfPcVzMzcDjj5o2mNd8Pfz\\ntTiViIiUdU7bSq1z58507twZ0zTZt28fKSkp5OXlER4eTs2aNfHx8XHWpc7xzjvvEBUVRe3atVm+\\nfDnXXHPNBY8fMmQIycnJvP\/++zz88MMFj48YMYK3336bkSNHMn78+ILHg4KCiImJKVQWu93O6dOn\\ni\/aFlLJXhtxG8Odz+GRLLoa3D1PjTZpuTOTWFlFWRxMRi834PZE\/k04BcP\/VMYT4l9zPcBG5NAkJ\\nCQDUrVvXKec7sybtgQMHnHI++dvjd3XDr+oeXv35TxJPZPHQlI1Mua8Vft5eVkdzWZk5eQyZ+juZ\\nOY5px2\/f2ZTIcgEWpxIREXHCCLR\/MgyD6OhoWrRoQevWralTp06JFs8AOnbsSO3atQt1bHx8PIsW\\nLSI6Ovqs4hnASy+9RFBQEFOmTCnUQrrPPfccq1atYt++fWzZsoXnnnuO5cuX07dv3yJ9HVYYOeAm\\nRlwZiI\/NxMRgxHdxTFm3z+pYImKhzJw83lywA4Aqof4MbB9tbSAROUtqqmMqYMWKFS94XFRUFO3b\\nt7\/o5kZnZgacbxkLKb4HO8RwUxPHWmi\/7j3O8zO3uMSauK7Ibrfz7IxN7DrqmPr62LV1uKZ+JYtT\\niYiIODi9gObqli5dCkDXrl3PeS44OJj27duTmZnJunXrLnqupKQk7r33Xho0aECXLl3YuHEj8+fP\\nP++5XdnQO7sy7YF2hPg5BiT+Z9YWPl5+\/h1VRcTzPfzeTI6ecoykfbJbfQJ9nTZYWUScIDc3F+Ci\\nRZg777yTlStXsnLlykKdLydH0wtLgmEYvH7bFTSvUQ5wjPB9f8lOi1O5pofHTWNOnGPds3a1w3mi\\nSz2LE4mIiPytzBXQduzYgWEY1Kt3\/g75zHSInTsvfmMzefJkEhISyMrKIikpiYULF9Kli3vuZnll\\ndAWm3d+GcoGO0YKv\/Pwnr\/y8XZ+QljERERFWRxCLxe3cx9LDjqlF5UnnlmaRFic6m9qoCFSo4NjQ\\n4\/jx404535nzlC9f3innk3P5+3jxSb+WRJV3TEV8a\/FurT37DxNnL+Xno8GOf2Sn8vYdTfCyXXhd\\nY0+i\/k1cndqoiJMKaHa7nWnTpnHXXXdRu3ZtQkND8fLy4q233jrn2FmzZjFz5kzWrl3rjEtfsjPT\\nHsLCws77\/JnHy+I0hsujwvjmgbZUCvED4OPl8bQf\/hHZp\/WJdFmhranlvvd\/xPBx7JJ8f8twbC72\\ny4vaqMjf\/w+2bt3qlPOd2T1cvxyVrIhgPz7p0wxyHcuEfLkTpv68yuJUrmHd5l383y9JGF4+mPm5\\njOocSeWwsrXumfo3cXVqoyJO2ETgjz\/+4M4772TPnr+n\/Jmm+a87YU6ePJkff\/yRiIgIDh48iLd3\\n2ZoatGPHjnMei4iIOO8PpJSUlPNuF1zSx0+8sz4PfL2VpIx8DvnV5Mrhn7Lilf7Yc09bkkfH63gd\\nXzrHv\/fNQo4FRQMQdGwLnS\/vfc7PLFfOr+N1vKsdX1KaNWvG1q1bWbNmDTk5Ofj6Fn13wpycHFav\\nXo1hGDRt2tSJKeV8GkWVZ2iLIN6NzcXw9mPkgkQql9\/EdW2usDqaZY4eT+Wej5ZjhFYF4KZqmQy6\\nsbfFqURERM5lmMWYo7d+\/Xo6d+5MVlZWwVS\/ihUrcuzYMQzD4I033mD48OFnvWb+\/Pn06NEDwzCY\\nN28e3bt3L95X8A9nduHs27cvX3755TnPP\/3004wbN44333yTYcOGnfP8Y489xvjx4xk\/fjwPPvig\\nU7MZhuE2UyL\/3HuQnq\/\/RH6oY9Fb39QDLB59GzWq6NNpEU+UeiqDJs9+CyGVMHOymHFfM1o2Ktzm\\nLCJytpLu7ydNmsR9992HYRh8+OGHPPTQQ0U+18cff8yQIUMwDINPPvmEwYMHOzGp63C1e7CHx33F\\nT8cca6KZGSn88MhVNG9QuB3fPYndbtJ9zAx2ZjpGm9XO28+SN4dYnEpERNxRafT1RZ7CmZ2dzW23\\n3UZmZiamafLEE0+QmJjIkSNHLvi6Ll26UK6c44Zh4cKFRb18kdWvXx\/TNP91jbNdu3YB\/OsaaWVF\\ng+hIVr50KwFp+wHICatOp5dnEbdTO3SKeKJX5vwOIY6dzjpUOKXimYgLu\/HGGwkIcBQcXnrpJQ4d\\nOlSk8xw6dIiXXnoJAH9\/f2666SanZZQLGz+iDy18He+bERTO\/VPjOJlZ9pbMeG3BnwXFs+BT+5n3\\nX88s4IqIiGcocgFt4sSJHDx4sGCk2VtvvUW1atUu+jpvb29atWqFaZps3LixqJcvsmuuuQY4f\/Eu\\nPT2d1atXExgYSJs2bUo7msupVrECv74xgPLpewGwh1Zl8NdbSUjOsDaYiDjVriOnmLktHYBwWxaf\\nPtXH4kQiciEVK1Zk8ODBmKbJ0aNH6dGjB\/v377+kc+zfv5+ePXuSlJSEYRgMGjSIihUrllBiOZ\/v\\nXhxMrTzH+5aS58ugz38jMyfP4lSlZ9KqBD5eHg9ATEQQq94YjL9f0acji4iIlLQiF9DmzJkDQJ06\\ndRgxYsQlvfayyy4DYPfu3UW9fJHFxMTQtWtX9u7dywcffHDWc6NGjSIjI4N+\/foVfLJb1oUEBbD+\\n7Qepkeu4wUvOMrntozXEHSh7myyUBSkpKVZHkFKWm29n2Lex5OTbMQz4bEgXl\/4FRm1UxGHUqFFE\\nRjp2yd20aRNNmjRh7NixF12LLTk5mbFjx9K0aVM2bdqEYRhUq1aN0aNHl0Zs+R82m40lrz\/EDVc4\\n1v76ff9JBn3+G1k5+RYnK3k\/bjrE\/837a\/OKYD8+H9iKcoGu2\/eUBvVv4urURkWKsYnAli1bMAyj\\nSGuYndl+3Vk7Xc6ePZtZs2YBkJSUBMCaNWsYOHAg4Fj094033ig4fvz48bRv357HH3+cJUuW0LBh\\nQ9atW8eyZcto0KABY8aMcUouT+Hr483yNx\/i7cW7eG\/JLlIycrjzk7W8d1czul5Wxep44kTJycna\\nYaeM+eCX3Ww5mAbAgx1q07R6OYsTXZjaqIhDREQEM2bM4JprriE7O5vU1FT+85\/\/MHr0aBo2bEiz\\nZs0IDw8nJCSE9PR0UlJS+OOPP9i2bRt2u71gjZCAgAC+\/\/577cBpEZvNYNwdTTiVncfyncdYF3+c\\n+778jc\/6X4m\/j5fV8UrE6l3HGP5NHKYJwX7efD7wSmqEB1ody3Lq38TVqY2KFKOAdqYCXZhpm\/90\\nZodOu91e1MufJTY29qwNAwzDICEhgYSEBACio6PPKqDFxMSwYcMGRo0axfz58\/n555+pWrUqw4YN\\nY9SoUYSFhTkllycxDIPh19WjYogfo2dvITvXzoNTNzL6hkYMaF\/L6ngiUgRxB07ywVLHSOAGVUIY\\ndl1dixOJyKVo1aoVK1as4OabbyYxMRGA\/Px8tm7dytatW8\/7mv\/dKT0yMpIffviBli1bllpmOZef\\ntxcf39uC+7\/cwMpdyazenULfj1fyab+WlA8NtjqeU039eRUvLD4CPv74eBlM6NuCxpG67xYREfdQ\\n5CmcISEhAGRlZV3ya8+MEnNWBXv06NHk5+f\/6589e\/ac85rIyEg+++wzDh48SHZ2NgkJCYwbN07F\\ns4u4t01NPu3XkgAfL0wTXpy7jeue+oi8PM+fbiDiSY4eT6X\/R7+Qbzfx8TJ4646m+Hl75mgHEU\/W\\nokULYmNjeeaZZwgOdhRbTNP81z8AQUFBPP3008TGxqp45iL8fbz4tF9L2tdx3BtvSMyg3bNfcvR4\\nqsXJnOfrBWsYufAw+PhjmnZevL42V9XVyEcREXEfRS6gnVl3Y8uWLZf82pUrV2IYBnXq1Cnq5cVC\\nnRtW5psH2xDm5\/gEe5dXDVo+\/hEn0tItTiYihWG32+kxeion7X4ADG5dlUbVQi1OJSJFVaFCBV55\\n5RUSExOZOXMmjz32GNdddx3NmzenTp06NG\/enOuuu45HH32UGTNmkJiYyKuvvqqpOC7G38eLif2u\\nJDTL8UFzVmhN2r8wnT2JSRYnK77vlqzn2fmJGH6OqZo3VEqjz1X1LU4lIiJyaYo8hbNTp05s3ryZ\\nBQsWkJqaWuiRW6tWrSIuLg7DMOjUqVNRLy8WuyKqHB\/fVpe7J6zEDKnMyZBatHrmK+Y9eyP1al76\\ntF4RKT2PvDWN5KBoAALT9jH8+m7WBhIRpwgJCaF379707t3b6ihSRAG+Xix9+Q46PjeF9NCa5IZG\\n0eWVefww9Fqa1nfPJTOm\/LSSFxYewvB3jJDsHn6CD0f0tTiViIjIpSvyCLQ+ffoAkJmZydChQwv1\\nmiNHjjBgwAAAvLy86N+\/f1EvLy6gzeV1WfbCjfinOXbozA2LousbC1m0bpPFyaSotIi055vxy6\/8\\nlBQEgJl5gplP98LXp8ifpZQ6tVER8XThYSH8+tZgIjL2AmCGVKH3B6vYtN\/9dsBbuuMoL69MLSie\\ndS6XwoSnVDw7H\/Vv4urURkWKUUBr1aoVt9xyC6ZpMnXqVG6\/\/fbzrjUGkJGRweTJk2nZsiXx8fEY\\nhsGgQYOoVcs9P0mTv9WsWpHf3hhAxb9u8giuyBPzElm246iluaRoNJ3Hs+3ef5gRM7djePti2vMZ\\neW01GkRHWh3rkqiNikhZEOjvx7q3H6Jmzj7HA0EVuHvibyzedsTaYJdg1h8Huf+LDeSaBgYmPSue\\n5LNn+1kdy2WpfxNXpzYqAoZ5ZkXZIkhLS6Ndu3Zs27atYEenWrVqFRTJIiMjCQgIICEhgfx8xyLz\\npmnSrFkzVq9ejb+\/v3O+CjdhGAbF+Ha7NLvdzs3\/mUhcvuOXcZsBI3s2YlD76IK2ISLWycmzc88n\\na9iw37EgdZuAJKaPHmxxKhHP5Mn9vbty1\/fEbrczYuICZiXYMU0wDHimewMe7BDjsvdXdrvJu0t2\\n8e6SXQD4ett4\/+5mdLusisXJRETEk5VGX1+sAhrAyZMnuffee5k3b57jhOfpzP\/3Ej169GDatGmE\\nhpa9Bavd9ebtUszbdJgR38WSnWsH4K4rq\/Nyr8b4ehd5sKOIFJNpmjw7YzPfbDgAQLMK+cx48gZs\\nNv2\/FCkJJd3fr1ixosTO3aFDhxI7t5Xc\/R5s4dYknvgmlswcxwfSXRtV5o3bmhAW6GNxsrOlZp5m\\nxHebWbzdMVIuxM+bT\/u3pE2MRq6IiEjJcosC2hlLlizhww8\/ZNmyZZw8efKs5wIDA+nQoQOPPfYY\\n119\/vTMu55bc\/eatsLYcTOW+LzaQlJYNQKvoCrx5c31qVK5gcTKRsmniynjGzNsOQJPq5fjmgTb4\\n+3hZnErEc5V0f2+z2Upk9JFhGOTl5Tn9vK7AE+7Bth9O474vNnDwZBYA5X1NHm8VyoAbXKPoOWvZ\\nbwz\/fgv24EoA1IoI4tN+LahTKcTiZCIiUha4VQHtDNM02bdvHykpKeTl5REeHk7NmjXx8XGtT8is\\n4Ak3b4V1NC2bB6ZsJPbAX8XU9GTev6MxN3ZoYW0wkTLmx02HeOzrPzBNqBrmz+xH2lMptGxNnxcp\\nbaVVQHP2NQzDKFhyw9N4yj3Yycwcnvp+E4v+WgvNtOfTwDjEtOf7EB5mTaEqLy+ffmO\/YHVaBQxv\\nx\/1+u+hQPurfhrAA3f+LiEjpcOkC2v79jp0XDcOgevXqTg3lqTzl5q2wsnPzGTJpJUsTMgAwc7O5\\nq7ad1x6+3eJk8m9SUlK0QKgHmTBrOW\/+mkGe3STI14tvHmxL48gwq2MVi9qouIOS7u+jo0tufdGE\\nhIQSOa\/VPOkezDRNHv\/wB2bvtRUUrEg\/xogO1Xjsjq6lmuWn1X8w7Kv1nA51\/C5g2vNp5pvEt6MH\\nudUOz65A\/Zu4OrVRcXUuXUA78+nnVVddxfLly52dyyN50s1bYdntdu75v8mszayEYTjWW4rJ28eP\\nYwYT6O9ncTr5px07dlC\/fn2rY4gTTJu\/mucWJmH4+uNjM\/h8UCva13H\/7cfVRsUdlMX+3tV54nsy\\nd8VGhk3fSF7o37sp1\/BO47PHbqBu5ZIdjZaamcv7v+xi4ordYPtrSYD0Y7x0fQz9e7rGlFJ3o\/5N\\nXJ3aqLi60ujri7yC9Jkpme3bt3daGPE8NpuN6aMH80RTH8zsdADivWvSbuR0jp7KtjidiGeatew3\\nnpt\/AMPXH9O007dOnkcUz0RE5G83dmjBtncHcXXwUcxcxz3V\/rxQur2zgiem\/8HWQ6lOv2ZqVi5v\\nL9rJVa\/\/wsRVCWDzwrTnUyt3H+te7KXimYiIeLQij62uUqUKiYmJBAcHOzOPeKhhd3enzWU76Tth\\nOfmh1TjpE8EN761ifJ\/mtIzW5gIizjJz6a8Mmx2P4e8YfdCjYiqjB\/W1OJWIiJQEXx9vprwwkLid\\n+xgz63c2nvDFbsKs2EPMij1E25hwbmlejaujQ6gSUb5I17Db7Sz47U+WHchjTtwhsnL\/XievRc3y\\nPNiiHF1b3eSsL0lERMRlFbmA1qhRIxITEz12rQxxvrZX1GPja5EM\/2o9v+zN4uip09z1yTpG9mzI\\ngHYlt56LSFmxeOMOhs1OKCieXR18lI+eHGhxKhERKWlN6tXku6drsiPpFB8s3c28TYewm7A2PoW1\\n8SmYuacJzTpM86ggureoR9c2l\/\/rpgN2u53ftu3hxzWbWfFnEntzgzGCzl73qHFkKMO61OPaBpV0\\n\/yYiImVGkddAmzx5MoMHD6Zy5crEx8cTEBDg7GwexxPX3ygK0zT5+tcDvDhnKzn5dgC6XVaZ129t\\nQligdmuyktY2cF+bEk9y72e\/kpqVC0DH0GN88fwAa0OVALVRcQfq711PWXtPDhzPZPLqvcyOPUhK\\nRs45z5umnTBvO9FVwwt2yszNt3Ps1GkSkk5g9\/I95zUGJl0aVaFP6xp0rFdRhTMnU\/8mrk5tVFyd\\nS28ikJeXR9u2bdm4cSMDBw7ks88+c3Y2j1PWbt4uJvbASR6eupFDqY51OwLsWTzbsZLWz7CQdtdx\\nTyt2HuOhqRvJzHFMq+nTwIf\/DijdndhKi9qouAP1966nrL4nufl23v92Ed+u281hexhG0KVN4zTz\\n8wjMOETrqECeuLUDTevXKqGkov5NXJ3aqLg6ly6gARw4cIDevXsTGxtLhw4dGDVqFJ06ddInUv+i\\nrN68XcjJzBye+n4Ti7YdARzbn7cNOsbUkQPw9vayOJ2I65sde5AR38aRZzcxDHi5V2PubVPT6lgi\\nZZr6e9ej98QxNXPZxm0s+X0Hm\/YlU6F6XfALJjUrF5sB3jYb5YN8yDl5FPupo3S8vBa9OjT\/16me\\nIiIirsSlC2jXXnstAFlZWaxfv76gaBYQEEDdunUJCwvDZrvwJp+GYbBkyZKiXN4t6ebt\/EzT5P7X\\np7LoWAiGt2MaQVDaPmY83YsG0ZEXebVI2WS32\/lg0XbeWroXAF8vG+\/c1ZQel1e1NpiIqL93QXpP\\nREREPJtLF9BsNts5I83OnKowI9BM08QwDPLz8y96rKfQzduFfbdkPU\/9sB2CKwJgZqXy8g316X9d\\nC4uTibiW9Mxsuj3\/GQf9owEI8fPm434taFc7wtpgIgKov3dFek9EREQ8m8sX0Ip9cRXQ5B+Skk9w\\nw0vTSA6KLnjsntY1GNmjIUF+Rd40VsRj7Nx3iBtfncPpsOoAeOdmMHtENy6rFmZxMhE5Q\/2969F7\\nIiIi4tlcuoC2b98+pwSoWbPsrNWjm7fCsdvtPPnBt8w\/FkpmruP7FR0eyFt3NqV5jUtb\/FbEk8xa\\n9htPzNgOQY4FXL3TDjLjiW40qVd2fo6KuAP1965H74mIiIhnc+kCmlw63bxdmsQTmTz5XRzr4o8D\\nYDPgkWvqMLRzXXy8ij8CUs6l3XVck2mafLpkK\/+dvwvD2xeAihl7Wfjf\/pQPDbY4XelSGxV3oP7e\\n9eg9EVen\/k1cndqouLrS6OtVhRCXFVU+kGn3teGFng3x9bJhN+H9X3Zz+fAvmfHLr1bH80jJyclW\\nR5B\/SD+dxxPfxDJ28T4Mb19Mez5tApJY\/+6QMlc8A7VRERHxTOrfxNWpjYqogCYuzmYzuO\/qGOY+\\ndhUNqzq2Uc8OqMjw+Un0eHYCJ0+lW5xQpORsO5TGTe+vYnbsIQAqBvvxYsdwpo8e7JR1KEVERERE\\nRKRwCvUb2PDhwxk+fDjLly8v6Twi51W\/SggzHmrLFbZEzPw8DJsX26hOs2e\/Y+LspVbHE3Gq3Lx8\\nPly6m14friI+OQOAq+pE8NPjVzOwZ3uL04mIiIiIiJQ9hdrW8J133sEwDKKioujYseN5jxk+fDgA\\nvXr1+tdjRIoj0M+HOWMfZPbyDYz45g\/yQqthhlRizNpMtqSt4f9uv5IQfx+rY4oUy+JfNzPk8zXk\\nhkYB4GUzeKJzXR6+pg5eNsPidCIiIiIiImVToQpohVGYIpuIM\/Tq2JJuba5g0GtTWZ1WDsPbj1lb\\nT7Bm\/3JG9mzITU2qYRgqNIh7yT6dwwNvTmP58VCMv4pntSoE8PbdzWlavZzF6URERERERMo2LaIj\\nbsnfz5dpowbx5d0NaFkjDICjp07z+PRY7vl0PbuOnLI4oXuKiIiwOkKZ9PWCNVw27AtWnKqI4eOH\\nadqpm7+fmQ+1UvHsH9RGRUTEE6l\/E1enNiqiApq4uY4tGvHdkPaMu70JEcG+AKyNT+H6d1fS\/60f\\nOHo81eKE7kVbU5eutOxc\/vPDZp77JYX80GqOB9OPMbJtEIveKJu7bF6M2qiIiHgi9W\/i6tRGRZw4\\nhVPEKoZhcGuLKLo0qsxbC3cwZd0+8uwmy4\/6cuXoudxa15vXH74db28vq6OKAJBvN5nxeyJvLNjB\\nsVOnwbBh5ufSxOcIX4zto8KZiIiIiIiIi1EBTTxGWIAPL\/VqzO0tq3PPW7NJ8wnHCCrPzEMwe+hn\\nPNk5hiG3drE6ppRxq3cnM2bedrYfTit4rG1MOI+1q0S7xjEWJhMREREREZF\/oymc4nEaR4bx+2t9\\nuKlKGmbmSQDyQyN57bfTdBszk91H0y1OKGXRgrVxNH\/8E\/pMXF9QPKsS6s\/bdzZh2v2tVTwTERER\\nERFxYRqBJh7J29uL9564mxeOp3L\/uG+IzQ7H8PFnR7of3d5Zwd2tqvPoNXWpEuZvdVTxcEs3bOWZ\\nL5dxxL86RkAkAIG+XjzUsTb3Xx1DgK+mFouIiIiIiLg6jUATj1apQhiz\/\/sAcx5sydWRXhiGY\/2p\\nqev20+GNpbw8d5tjDSoBICUlxeoIHmPt5l20efxDBny7h6OB0Rg2L0zTTo3cAyx7shNDO9dV8awI\\n1EZFRMQTqX8TV6c2KnKJI9D27NnDihUrin3M\/+rQocOlRBApkib1ajKlXk22HUrjtfl\/snznMXLy\\n7ExancDUtfHE5Cfy4WO9qR1VxeqolkpOTtYOO8W088gpPlkRzw+\/J5IfPGokVQAAIABJREFUEI3x\\n1+MV0vcy5u529Gh\/o6X53J3aqIiIeCL1b+Lq1EZFwDBN07zYQTabDcMwLnbYpV\/cMMjLy3P6eV2V\\nYRgU4tstpWDjvuOMW7iTNXv+\/iTFzMmigU8yrw++nib1alqYzjo7duygfv36VsdwO6Zpsi7+OJ+s\\n2MPSHcfOeq58+l5evqMNN3ZoYVE6z6I2Ku5A\/b3r0Xsirk79m7g6tVFxdaXR11\/SCLQLhTlTYNPN\\nibiDFjUrMO3+Nvy0YTdPTFpMTmh1DN8AdlCdmz75g6i8Hxk\/9FaaxJTtEWlyYacyshjzxY\/8kVmB\\nncnZBY8bBnS\/rAoPdYimSY2eFiYUERERERERZyhUAa1GjRolMgJNxGo9Wtahe\/MY3v1mIRNWJnA6\\ntAaGtw8HvaPp\/elGrm9chfuvjqFZjfJWRxUXsn7LLl6auoStWaEYAWGAo3jm523j9pZR3HdVDNER\\nQdaGFBEREREREacpVAFt7969JRxDxDo2m41hd3dn2N3wxbwVjPtpM2kh0Zgm\/LQ5iZ82J9EkKoz+\\n7aLpeUVV\/Ly18HtZlJdv59uVm3nlu1WkBVXHsFXHCHA8Z2af4tHuTRh0VQzhwX7WBhURERERERGn\\nK9QaaOIcWn\/DfexMSuPjFQnMjj1Inv3v9yzQlk9tDvN\/\/bvStH4tCxOWjJSUFC0O+g\/bD6cxY2Mi\\ns2IPkZx+9o6tvqkHuOmyCvynf0\/CQjTirDSojYo7UH\/vevSeiKtT\/yauTm1UXF1p9PUqoJUi3by5\\nnyNp2Xy1bh\/Tft1PcnpOweOmPZ\/QjERubVaNJ+\/pTnCgv4Upxdnidu5j\/eFcfog9zPbDaWc9Z7Pn\\nUi33IMNvupJbrmllUUIRcWXq712P3hMRERHPpgKah9HNm\/s6nZfP1GVbeXXmWnJDo856zsw6Rd+r\\n63HvVXVoUCXUooRSXBu27eG9H1ayLjGb06GRGIbtrOfbxFTg1uZRdG9chRB\/H4tSiog7UH\/vevSe\\niIiIeDYV0DyMbt48w\/SFa\/nw59\/ZTyUMv8CznmtQJYQbm1TjhiuqUjNcU\/pcmWmaxCdn8N73S\/lp\\n86FzCqMAtSKCuKVZJDc3jySqfOB5ziIici71965H74mIiIhnUwHNw+jmzbOkpJ7i1Sk\/syk9mB0n\\nz31fa5f3ptLpgwztfRVtr6hnQUL5p+zcfP6\/vXuPj6I+9zj+nQ0hhNwhQEi4BpGbgCBeuDeIoYgK\\nchBsFVSOtYVWgpRqrYUQxGIr+CqISFUU5DR6POUotiiKQhSBFLASFDFcQogNDZjEBHIDkvzOHzR7\\niEk29+xk83m\/Xvtys\/Ob3zyz+5R5+szuzN6ULCV8fVY7k79VWnZBxUF5merrV6j7xw\/WjPE3cQdi\\nALXG8d5++EwAAPBsNNA8DMWb5\/omu0BvfZ6uvx06raNn8iosd5zLUJ\/AS5oxqr9+PGGEWnvX6Aa4\\nqKfi4hK9s+szvbX7sE4W+CjTK0RFl0orjLPyM9XPr1CzbxmiqVHXy+FwVDIbANQMx3v74TMBAMCz\\n0UDzMBRvLcPRM+f1t6TTWrv17yr2rXinmrZepfrhoK4a1TtUw3u1V+cgXzdEWbnmfned0lKjL9Iy\\n9Wz8Vh1Mz1eOd6isNv4VxrVyWLq+RztF9e2gH\/TpqN4d\/fmmWTPR3HMULQPHe\/vhM4HdcXyD3ZGj\\nsDsaaB6G4q1lKS0t1TuffKbXPjqoQ1lGxYERlY7rGeqn4b3aK\/jCGd12Q1\/1j6x4La6mkpycrD59\\n+rht+7WVlXdBB7\/JKfc4X1Rc6ViT\/50mDOqqqcOv1sirQrkRQDPV3HIULRPHe\/vhM4HdcXyD3ZGj\\nsLumONbzOzKgkTgcDk35wfWa8oPrJUmHT3yjTR\/9Q9\/5RujTY5nKv1giSTqZma+TmfmSpLUHk6S8\\nj9TeytM1YX4aP6SXpo2\/Sb6tW3azp7S0VIeOndL2\/Ud04Pi\/dDyrSFnGX\/IPrXIdU5CjDiZHN\/YI\\n1vQfDNHoIRP5aSYAAAAAoE5ooAFNZECvrnq6V1dJ0qWSUn2Rnqu9J7K050Sm9qVkyXlpLv9QZSlU\\nH5+XPv7knJ7cvV29OwWoT1iA+oYFqE9YoPp28leHAB+PawgZY3Tm3AWlZOYpNbNAx86e19f\/Oq99\\nR9NV0qrNv0eFSd+7wallSVd18Ne1XYM1pFuIBnby0YBuHTzu\/QEAAAAAuAcNNMANvL0cGtotREO7\\nhejnUVfpfH6h4t\/fqw+TUpSceUG5rUJk+QZJki6WGB0+fU6HT58rN4cpOq\/WF3IV4l2szgHeuios\\nSMP69tD4m65Ve7\/Wcjjsd00vY4xyCy\/psyMp2nf4uE5mfKdvsvOVkVei3FIfeQWH6VJpJXE7m2f\/\\nnqcgR4ElOYoa1FMzbr5BA7sEKZCfZAIAAAAAGgnXQGtCXH8DNVVaWqq\/f3lcn6d9pywF6OuMc0rO\\nOK\/MvIs1Wr+Vw1KHAB91DGyjjgE+8i4uUMbJZIX4+aidfxu1D\/RTx2B\/9ewSpj6R3dTG20utWzl0\\n8vgxXdO\/b7UX1C8uKVXBpRIVXixR\/oVipZ0+o+OpafrufKHO5uTpbG6BsvIuKCSsi\/zbhykr\/6Iy\\nz1\/Qv3KLVHippMbvQ+tWDvXpFKAQR4GsnNO6oU+EJtxwja7q1rnGc8CzcP0NNAcc7+2HzwR2x\/EN\\ndkeOwu64iYCHoXhDfX17\/oKSM87rvb1J2vHZ18q+5FCRV4Asv5AGmb+k8Jy8fAPlZRkVXyiSjJEs\\nx+XfSFoOOby8JIdDDZnGprREVsF38ivN14BuHTRpzPXqGeqnnqF+Cg\/2lZcNv0kH9+EOUGgOON7b\\nD58J7I7jG+yOHIXd0UDzMBRvaCzfncvT3788rswLli61DtSZcxd09nyRzv77v2lnc1VkvJo0JlNa\\nIh8Vq1tYe7Vr21rt\/Vurc5CvwoPbyKekUKX5WRrYq4v69+yiNj6tmzQ2AGhMHO\/th88EAADPRgPN\\nw1C8wZ3O5xcq\/dtsnf72O2Vk5erbnDz5BIYoKDRMFy6V6FKJ0cWSUl0oLtXJ1DQdOXpMxkgOS3I4\\nLDksqUt4uPr27aNWDof8fLzk29pLbVt7qeBcjjL\/la4gf1916RiinuEd1C2sg7xbNW3TDgDsgOO9\\n\/fCZAADg2WigeRiKNwAAPB\/He\/vhMwEAwLM1xbHe0aizAwAAAAAAAM0cDTQAAAAAAADABRpoAJyy\\nsrLcHQLgEjkKAPBEHN9gd+QoQAMNwBUyMzPdHQLgEjkKAPBEHN9gd+QoQAMNAAAAAAAAcIkGGgAA\\nAAAAAOACDTQAAAAAAADABRpoAAAAAAAAgAs00AA4hYaGujsEwCVyFADgiTi+we7IUUCyjDHG3UG0\\nFJZlibcbAADPxvHefvhMAADwbE1xrOcbaAAAAAAAAIALNNAAAAAAAAAAF2igAQAAAAAAAC7QQAMA\\nAAAAAABcoIEGwCkrK8vdIQAukaMAAE\/E8Q12R44CNNAAXCEzM9PdIQAukaMAAE\/E8Q12R44CNNAA\\nAAAAAAAAl2igAQAAAAAAAC7QQAMAAAAAAABcoIEGAAAAAAAAuEADDYBTaGiou0MAXCJHAQCeiOMb\\n7I4cBSTLGGPcHURLYVmWeLsBAPBsHO\/th88EAADP1hTHer6BBgAAAAAAALhAAw0AAAAAAABwgQYa\\nAAAAAAAA4AINNAAAAAAAAMAFGmgAnLKystwdAuASOQoA8EQc32B35ChAAw3AFTIzM90dAuASOQoA\\n8EQc32B35ChAAw0AAAAAAABwiQYaAAAAAAAA4AINNAAAAAAAAMAFGmgAAAAAAACACzTQADiFhoa6\\nOwTAJXIUAOCJOL7B7shRQLKMMcbdQbQUlmWJtxsAAM\/G8d5++EwAAPBsTXGs5xtoAAAAAAAAgAs0\\n0AAAAAAAAAAXaKABAAAAAAAALtBAAwAAAAAAAFyggQbAKSsry90hAC6RowAAT8TxDXZHjgI00ABc\\nITMz090hAC6RowAAT8TxDXZHjgI00AAAAAAAAACXaKABAAAAAAAALtBAAwAAAAAAAFyggQYAAAAA\\nAAC4QAMNgFNoaKi7QwBcIkcBAJ6I4xvsjhwFJMsYY9wdREthWZZ4uwEA8Gwc7+2HzwQAAM\/WFMd6\\nvoEGAAAAAAAAuEADDQAAAAAAAHCBBhoAAAAAAADgAg00AAAAAAAAwAUaaACcsrKy3B0C4BI5CgDw\\nRBzfYHfkKEADDcAVMjMz3R0C4BI5CgDwRBzfYHfkKEADDQAAAAAAAHCJBhoAAAAAAADgAg00AAAA\\nAAAAwAUaaAAAAAAAAIALNNAayPLly+VwODRv3jx3hwLUWWhoqLtDAFwiRwFI0q5duzR58mR16dJF\\nDodDr732mrtDAuqF4xvsjhwFaKA1iMTERL300ksaPHiwu0MB6qV9+\/buDgFwiRwFIEl5eXkaOHCg\\nVq9erbZt27o7HKDeOL7B7shRgAZaveXm5uree+\/Vq6++quDgYHeHAwAA4PEmTpyoZcuWaerUqbIs\\ny93hAACAFoAGWj099NBDmj59usaOHevuUAAAAAAAANAIWrk7gObspZdeUkpKil5\/\/XV3hwIAAAAA\\nAIBG4hHfQNu8ebPmzZunMWPGKCgoSA6HQ7NmzXK5Tnp6umbPnq2IiAi1adNGPXv21COPPKKcnJwK\\nY+Pj4xUQEKCAgAAFBgZq9+7dOnr0qJ544gnFx8fL4fCItxEAAKDeGrsuAwAAcAeP+AbasmXLdOjQ\\nIfn7+6tLly76+uuvXY5PSUnR8OHDlZmZqSlTpqhPnz7at2+fVq1apffff1+7d+9WSEiIc\/zkyZN1\\n0003Of+OiIjQG2+8oaysLPXv39\/5eklJiT755BOtW7dO+fn58vb2bvidBRpRVlYWFwiFrZGjgP01\\ndl0GeCKOb7A7chTwkG+g\/fGPf9TRo0eVm5urtWvXyhjjcvycOXOUmZmp5557Tps3b9bvfvc7ffjh\\nh3rkkUf09ddf64knnig33s\/PT5GRkc6Hj4+P7rzzTn3xxRdKSkpyPoYNG6Yf\/ehHSkpKonmGZikz\\nM9PdIQAukaOA\/TV2XQZ4Io5vsDtyFPCQBtrYsWPVq1evGo1NSUnR9u3b1aNHD82dO7fcsri4OPn5\\n+WnTpk0qLCx0OU9gYKD69+9f7uHn56d27dqpX79+dd4XAACA5qwp6rL8\/HwlJSXp4MGDKi0tVVpa\\nmpKSkvTNN9802H4AAABcySMaaLWxc+dOSVJ0dHSFZf7+\/ho5cqQKCgqUmJhY67m5jToAAEDN1bUu\\nO3DggIYMGaLrrrtORUVFio2N1dChQxUbG9skcQMAgJanxTXQkpOTZVmWrr766kqX9+7dW5J09OjR\\nWs+9Y8cOrV69ul7xAQAAtBR1rcvGjh2r0tJSlZSUlHu88sorjR4zAABomVpcAy03N1eSFBQUVOny\\nste56xMAAEDjoi4DAADNhUfchbM54WeeAAAATY8aDAAA1EeLa6CVncksO+P5fWWvBwcHN\/i2q7sL\\nFQAAQEvSVHUZNRgAAKivFvcTzj59+sgYU+U1zo4dOyZJVV6LAwAAAA2DugwAADQXLa6BFhUVJUn6\\n4IMPKizLy8vT7t271bZtW910001NHRoAAECLQl0GAACaixbXQIuMjFR0dLRSU1O1Zs2acssWL16s\\n\/Px8zZo1S76+vm6KUNq1a5cmT56sLl26yOFw6LXXXnNbLEBlli9frhtuuEFBQUHq2LGj7rjjDh0+\\nfNjdYQFOa9eu1eDBgxUUFKSgoCCNGDFC7777rrvDAiq1fPlyORwOzZs3z92hNDm71WXUYLA7ajDY\\nHTUYmpPa1mAecQ20LVu26O2335YkZWRkSJL27NmjBx54QJIUGhqqZ555xjl+7dq1GjlypGJiYvTR\\nRx+pX79+SkxMVEJCgvr27atly5Y1\/U5cIS8vTwMHDtR9992nWbNmuTUWoDKffPKJfvGLX2jYsGEy\\nxmjRokUaP368jhw50ijXDwRqq2vXrvrDH\/6g3r17q7S0VBs2bNCUKVP0j3\/8Q9dcc427wwOcEhMT\\n9dJLL2nw4MHuDqXBNOe6jBoMdkcNBrujBkNzUZcazDIecFXVuLg4LV26tMrlPXr00IkTJ8q9lp6e\\nrsWLF2vbtm3KyspS586dNXXqVC1evLjKW6m7Q0BAgJ5\/\/nmKONhafn6+goKCtGXLFk2aNMnd4QCV\\nat++vZ5++mn95Cc\/cXcogKTLF8i\/7rrrtH79ei1ZskQDBw7U6tWr3R1WvXlKXUYNhuaAGgzNATUY\\n7KauNZhHfAMtNjZWsbGxtVonIiJC69evb6SIgJbl3LlzKi0tVUhIiLtDASooLS3Vm2++qfz8fI0Y\\nMcLd4QBODz30kKZPn66xY8e6O5QGRV0GNB1qMNgZNRjsqq41mEc00AC4V0xMjIYOHarhw4e7OxTA\\n6csvv9Tw4cNVVFSkgIAAvfXWWxowYIC7wwIkSS+99JJSUlL0+uuvuzsUAM0YNRjsiBoMdlafGqzF\\n3USgvjZv3qx58+ZpzJgxCgoKksPhqPar\/enp6Zo9e7YiIiLUpk0b9ezZU4888ohycnKaKGq0JE2d\\nowsWLNCePXu0efNmWZbVULsBD9ZUOdq3b18lJSVp3759mjNnjmbNmqWvvvqqoXcHHqixc\/To0aN6\\n4oknFB8fL4eDUqymqMFgd9RgsDtqMNid7Wswg1q59tprjcPhMIGBgaZ\/\/\/7G4XCYmTNnVjn+xIkT\\npmPHjsbhcJipU6eaxx9\/3Nx8883GsizTr18\/k52d7XJ7\/v7+ZuPGjQ29G\/BgTZmj8+fPN+Hh4ebo\\n0aONsSvwUE3972iZ8ePHmwcffLChdgMerLFzdMOGDcbhcJhWrVo5H5ZlGYfDYby9vc3Fixcbexeb\\nJWow2B01GOyOGgx2Z\/cajAZaLSUkJJjjx487n1uW5fIDjY6ONg6Hwzz\/\/PPlXl+wYIGxLMvMmTPH\\n5fYo3lBbTZWj8+bNM507dzbJyckNFzxahKb+d7TMuHHjXG4HKNPYOZqbm2sOHz5c7nH99debe+65\\nx3z11VcNv0MeghoMdkcNBrujBoPd2b0Go4FWD9V9oCdOnDCWZZnIyMgKy86fP2\/8\/f2Nv7+\/KSgo\\nKLcsLy\/PHDx40Hz++eembdu25sknnzQHDx40aWlpjbIf8FyNlaNz5841gYGBZufOnSYjI8P5yMvL\\na5T9gOdqrBz99a9\/bXbt2mVSU1PNF198YX79618bLy8v8\/777zfKfsBzNVaOft8PfvAD8\/DDDzdI\\nzC0BNRjsjhoMdkcNBruzYw3GhTca0c6dOyVJ0dHRFZb5+\/tr5MiRKigoUGJiYrllBw4c0JAhQ3Td\\nddepqKhIsbGxGjp0aK3vaAVUp645+sILLygvL08333yzwsPDnY+VK1c2SdxoOeqaoxkZGZo5c6b6\\n9u2r8ePH67PPPtO2bdsqnQeoj7rm6Pdx\/aKGRQ0Gu6MGg91Rg8Hu3FGDcRfORpScnCzLsnT11VdX\\nurx3797avn27jh49qqioKOfrY8eOVWlpaVOFiRasrjlKfqKp1DVHX3311aYKES1cXXP0+3bs2NFY\\nIbZI1GCwO2ow2B01GOzOHTUY30BrRLm5uZKkoKCgSpeXvc6doOAu5CjsjhyF3ZGj9sTnArsjR2F3\\n5Cjszh05SgMNAAAAAAAAcIEGWiMq63iWdUa\/r+z14ODgJosJuBI5CrsjR2F35Kg98bnA7shR2B05\\nCrtzR47SQGtEffr0kTFGR48erXT5sWPHJKnK3+wCjY0chd2Ro7A7ctSe+Fxgd+Qo7I4chd25I0dp\\noDWisgvVffDBBxWW5eXlaffu3Wrbtq1uuummpg4NkESOwv7IUdgdOWpPfC6wO3IUdkeOwu7ckaM0\\n0BpRZGSkoqOjlZqaqjVr1pRbtnjxYuXn52vWrFny9fV1U4Ro6chR2B05CrsjR+2JzwV2R47C7shR\\n2J07ctQyxpgGm60F2LJli95++21JUkZGht5\/\/31FRkZq9OjRkqTQ0FA988wzzvEpKSkaOXKkzp49\\nqzvuuEP9+vVTYmKiEhIS1LdvX+3evVshISFu2Rd4JnIUdkeOwu7IUXvic4HdkaOwO3IUdmf7HDWo\\nlSVLlhiHw1HlIzIyssI6\/\/znP83s2bNNeHi48fHxMT169DALFiwwOTk5btgDeDpyFHZHjsLuyFF7\\n4nOB3ZGjsDtyFHZn9xzlG2gAAAAAAACAC1wDDQAAAAAAAHCBBhoAAAAAAADgAg00AAAAAAAAwAUa\\naAAAAAAAAIALNNAAAAAAAAAAF2igAQAAAAAAAC7QQAMAAAAAAABcoIEGAAAAAAAAuEADDQAAAAAA\\nAHCBBhoAAAAAAADgAg00AE0mLi5ODodDDodDr732WqNs49SpU85tjBs3rsHmLZszMjKy2rF79+7V\\nvffeq6uuukp+fn61WrcxXbp0SampqW6NAQAAND1qMGowAPVHAw2wsR49ejgP\/N9\/eHt7q3379ho0\\naJDuu+8+bdmyRSUlJe4OuUYsy2qW26jJnOvWrdOoUaMUHx+vkydPqqioSJZlVVg3Li5OcXFxWrVq\\nVYPHWZnc3FzdeuutOnDgQJNsr7nJz8\/XDTfcoOTkZHeHAgCwAWowe22DGsxzUYOhOWnl7gAAVK2y\\ng36Z0tJS5eTk6LvvvtOXX36pTZs2adCgQYqPj1f\/\/v2bONKWwRjjcnlmZqZ++ctfSrr82UVHR+v6\\n669XcHCwJCkoKMg5Ni4uTtLlAj0mJqaRIr4sLy9PUVFRevDBBzVt2rRG3VZz5efnpxdffFG33Xab\\n\/vu\/\/1tDhw51d0gAADeiBrMXajDPRQ2G5oQGGmBzxhhZlqWf\/exn6tWrl\/P14uJiZWRkKCEhQUlJ\\nSZKkQ4cOafz48Tpw4IDCw8PdFbJHKiuiXZ0B3b59uwoLC2VZln70ox\/pv\/7rv2o0Z2MyxmjGjBka\\nMGCA5s6dW6N10tPTNW3aNE2ZMkWPPfZYI0doH9dee62WL1+uCRMmKCkpif8NAUALRw1mD9Rgno8a\\nDM0FDTSgmZgxY4bGjBlT6bI33nhDs2bNUklJic6cOaNFixZp\/fr1TRyhZ6vJTzPS0tKcz0ePHt2Y\\n4dTYypUr9fnnn1f7tfjc3Fzt379f77zzjl555RUVFhZq4sSJTRSlfUybNk1\/\/etf9eMf\/1g7d+5s\\nkgIbAGBv1GDuRQ3WMlCDoTngGmiAB7j77rsVExMjY4yMMdq8eXOzuRaHJykqKnI+9\/X1dWMkl508\\neVKxsbFavHixAgICKh1z5MgRRUZGasyYMXrhhRd01VVXqaCgoMlirO4nGe7w9NNP68CBA1q5cqW7\\nQwEA2Bw1mD1Qg9UeNRhQezTQAA9x5513Op+fP39eJ06cqNF6+\/btU0xMjAYPHqzQ0FD5+PgoPDxc\\n0dHRWrNmTbmCxJV\/\/etfeuyxxzRgwAAFBASoXbt2GjJkiJYuXaqzZ8\/Wen8OHjyohx9+WEOHDlVI\\nSIi8vb3l5+enbt26ady4cVq6dKn27t2r0tLSGs0XHx+vH\/7wh4qIiFCbNm0UHh6uqVOnavv27TVa\\nv6q7OH388cfOZWXX1DDG6P777y93wWEvL69yd6cqG5eamlrpBYob4u5VcXFxCgwM1IMPPljlmH79\\n+iklJUVJSUnavHmz5s2bV+\/t1lRSUpI6d+7cZNurqc6dO+unP\/2pfv\/73ysvL8\/d4QAAbI4azDVq\\nsMpRg1VEDQbbMwBsq0ePHsayLONwOMzHH3\/scuyxY8eMZVnO8Xv37nU5Pjc31\/zHf\/xHuXWufJS9\\n3qVLF5OYmOhyri1btpjg4OBK57Isy4SFhZldu3aZJUuWOJdv3LixyvkWLlzoXPf7cX0\/vsOHD5db\\nNzU11bleVFSUyc7ONrfccovLfVy4cKHL\/TPGONfv2bNnudcTEhLKzV3V++nl5WVOnTrlXFbVuLJH\\nVFRUtTG5curUKePt7W0ef\/zxWq9bFldcXFy9YqhOQkKCcTgcjbqNujp+\/LhxOBxm6dKl7g4FAOAG\\n1GDUYHVFDVY\/1GCwM66BBniIM2fOlPvbz8+vyrE5OTkaNWqUvvrqK1mWpbZt22rChAm65ppr5Ovr\\nq9OnT2vbtm06fvy40tPTdfPNN2vXrl0aMmRIhbl27Nihu+66S8XFxbIsS+3bt9eUKVPUs2dPZWdn\\na+vWrUpOTtaUKVM0efLkavdj9erVWrlypfPuVyNHjtSNN96o0NBQlZSUKDMzU19++aX27NmjwsJC\\nl3MVFxdr2rRp2rlzpzp37qzbb79d3bt31\/nz5\/Xee+\/p0KFDkqRnn31W1113ne6+++5q4\/u+Xr16\\nacWKFZKk999\/X9u3b5dlWZoxY4aGDRtWbmy7du2cYxcuXCjLshQSEqLf\/OY3Febt2rVrrWO50quv\\nvqqSkhJNnz69XvO0VL169dKwYcP0pz\/9SYsWLXJ3OAAAG6MGq4gajBqsrqjBYGvu7uABqFptzn7O\\nnz\/feUatbdu25sKFC1WOve2225zzzpo1y+Tk5FQ6bsWKFc5xvXv3NiUlJeWW5+fnm+7duzvP2E2a\\nNKnSuWJjY8ud9XN19rNbt27Gsizj4+NjduzYUeU+FBUVmddee82cPn263OtXnv0s++\/Pf\/5zU1RU\\nVGGOsvfM4XCYfv36VbktY6o++3mlmp7drel89dGnTx\/TqVOnOq3L2c\/LnnzySeNwOMynn37q7lAA\\nAE2MGowarK6oweqPGgx2xTXQAA+wadMmrVmzRtLl23Lffffdat26daVjP\/roI23dulWWZWnq1Kna\\nuHGjgoKCKh37y1\/+UvPnz5cxRidOnNAbb7xRbvmGDRucdz2KjIzUX\/7yl0rnWrJkie65555qL1aa\\nnZ2tb775RpZlacKECYqKiqpyrI+Pj2bOnOny+g2WZem2227TmjVr5OPjU2H5M888o65du8oYo+Tk\\nZB05csRlfM1FWlqajh49qhEjRrg7FLdKS0vT9OnTFRQUpK5du+q6HPRwAAANEElEQVStt94qt\/zA\\ngQO68cYbq1x\/1KhRzgtCAwBQGWqwylGDUYNRg8ET8RNOoJl44403tH\/\/fuffxcXFOnPmjHbs2KFD\\nhw45v27fr18\/Pf3001XO8\/zzzzufuxpXZuHChfrjH\/8oSXrnnXf04x\/\/2Lnsz3\/+s\/P5b3\/7W7Vp\\n06bKeZ566inFx8e73NaVd62q712IjDGyLEvLli2rckyrVq10xx13ON+Tzz\/\/XP369avXdu1g165d\\nkqRBgwa5ORL3SUlJ0YgRI\/Ttt99KkvLy8jR9+nQ988wz+sUvfqFjx47prrvuKpfD3zd06FBZlqU9\\ne\/Y0VdgAABuiBqsdajBqMGoweCoaaIDNWZYlSVq3bp3LMV5eXpo2bZpWrVqlDh06VDrOGKOEhARZ\\nlqWePXuqV69e1W4\/PDxcHTp0UGZmZrni8eLFi\/rss8+cf0+ZMsXlPN26ddOwYcO0f\/9+5z59X4cO\\nHRQWFqaMjAzt3LlTq1ev1ty5c9WqVe3\/qbIsS2FhYRo4cKDLcVdffbXzeV3uVGVHBw8elGVZ6t27\\nt7tDcZt58+bp2muv1cMPP6zu3bvr9OnTSkxM1LvvvqvnnntOJ0+e1B\/+8AeXZ4gDAwPVqVMnffnl\\nl87\/MwAAaDmowajBaosajBoMno2fcALNRNnZzcoekjR27FitWLGiysJNkk6dOqWcnBxJl88OVXbr\\n7soe3377rYwxzjNJkpSamqqLFy\/Ksix169atyp8gXGnw4MHVjnnsscckSaWlpZo\/f746d+6smTNn\\n6sUXX1RSUlK1P0G4Uvfu3asdExAQ4HzuKbfLPnHihKTLhXdLVFJSotDQUL377ruaNGmSrrnmGkVH\\nR2vx4sV655131LNnT82dO1cLFy6sdq6uXbuqsLBQx44da4LIAQB2RA1GDVZT1GDUYPBsNNAAmysr\\nVhISElRSUuJ8ZGRk6MMPP9Stt94q6fJ1NYYPH67U1NQq58rKynI+d1UMVvXIz893rv\/dd985n7dv\\n375G+xIaGlrtmJiYGC1btky+vr6yLEvZ2dn685\/\/rJ\/97GcaMmSI2rdvr\/vuu0979+6tdi5XP2co\\nc+UZrdLS0mrHNwenT5+WVLP32xN5eXlpw4YNcjjKH+Jyc3M1ceJEXX311c7r1VSnU6dOkjznzDgA\\noOaowajBaosajBoMno2fcALNxPfP+nXo0EFRUVGKiopSTEyMnnvuOf3zn\/\/U9OnTtWfPnkq\/cl9c\\nXOx8HhERofnz5zd63HXxm9\/8Rg8++KDi4+P14Ycfavfu3Tp37pykywfgTZs2adOmTbr\/\/vv18ssv\\nVzhIt3Tnz5+XVLPi1Z1qcya7vk6fPq1Jkybp9ttv19KlS2u8nq+vryQ5vzUAAGh5qMGowWqKGqwi\\najB4EhpogAdYuXKlEhIS9MUXX+izzz7Ts88+q0cffbTCuCvPUvr6+mrBggV13mZISIjz+ZVnVV3J\\nzMys8fwdO3bU\/PnznQVmUlKSduzYoTfffFP79u2TJG3cuFERERF68sknaxG557tw4YIkydvb2y3b\\nT09PV3R0tAoLC12OK1seGRlZ7ZwDBgzQX\/\/61zrFc\/jwYd1+++361a9+pTlz5tRq3bICuKwgBgDg\\nStRg1GBXogYrjxoMnoYGGuABWrVqpZUrVyo6OlqStHz5cv3nf\/5nha\/1d+\/eXW3btlVBQYFOnjyp\\n\/Px8+fn51WmbPXr0UOvWrXXx4kWlpaUpNze32mtwJCUl1Wlb0uVrdwwePFiPPPKIVq5cqV\/96ley\\nLEvr1q2jePuetm3bSvr\/Iq6pRURE6PDhw9WO+\/jjjzVu3DilpKQ0Wiw7d+7Uvffeq3Xr1un222+v\\n9foXL16UJLVu3bqhQwMAeABqMGqwK1GD\/T9qMHgivnMLeIjx48dr1KhRMsbo3Llzld4e3dvbW2PG\\njJF0+SKfb775Zp2317p1aw0dOtT599tvv+1y\/DfffKMDBw40yF10FixYID8\/PxljlJ2dXe7Cus1F\\n2c87rrxtfEMJDAyUpGrPPnq6v\/zlL5o5c6b+93\/\/t06Fm\/T\/76G\/v39DhgYA8CDUYM0LNVjjowaD\\np6KBBniQ2NhYSZeva\/DCCy\/ozJkzFcY8\/PDDzueLFy+udExNzZw50\/n8qaeecnm27be\/\/a2MMQ1y\\nzYWSkpJy19wou0ZCcxIUFCRjTLkLATeUsjtf1eezbe42bNigmJgYffDBB7rxxhsrLM\/JyVG\/fv2q\\nnafspzERERENHiMAwHNQgzUf1GCNixoMnowGGuBBbr75Zo0YMULS5bM2y5cvrzBm4sSJuvXWW2WM\\nUXp6ukaOHKlPP\/20yjmLi4u1bds2TZ48ucIZzlmzZqlr164yxujEiROaNm2acnNzy40xxiguLk6b\\nNm2q9kKzu3bt0qRJk\/Tuu+\/q0qVLlY4pKSnR448\/7rwewrXXXtssz0z16dNHkpSfn68DBw406NxX\\nXXWVpMtnnFui+Ph4LVq0SDt27FD\/\/v0rLC8pKdGcOXM0adKkaucqew9rco0QAEDLRQ3WfFCDNR5q\\nMHg6roEGeJjY2FhNmDBBxhi9+OKLevTRRxUeHl5uTHx8vKKionTw4EGlpKRozJgxGjx4sEaPHq2w\\nsDDn1\/IPHz6sffv2KScnR5Zl6b777is3j5+fn9avX69bb71VxcXF2rp1q3r37q0777xTPXr0UHZ2\\ntrZu3aqvv\/5a7dq105QpU\/TKK69UGXtpaanee+89vffeewoKCtKNN96oAQMGKDQ0VBcuXFB6erq2\\nbt2qjIwMSZLD4dDvfve7hn8Tm8CkSZO0Z88eGWN022236Z577lH37t3l5eUl6fLZtilTptRp7mHD\\nhskYo+Tk5IYMuVn49NNPNXv2bHXp0kX333+\/OnXqpFGjRumGG25QcHCwDh48qFWrVun06dPVXiPk\\nwoULOn36tHr16uW8pgkAAFWhBmseqMEaBzUYWgQDwLZ69OhhLMsyDofDfPzxxzVeb\/jw4c715syZ\\nU+mYwsJC89Of\/tR4e3sbh8NhHA6HsSyrwqNsWceOHc2ePXsqnevtt982wcHBlc7jcDhMWFiY2bVr\\nl1myZInztY0bN1aYZ\/fu3c45qounXbt25s0336wwR2pqqnNsVFRUte\/Vhg0bnPPGxcVVOa5szp49\\ne1Y5prr9u9L58+dN\/\/79q9zXmsRelezsbONwOGo9x7lz58zLL7\/sjGHMmDHmyJEjprS0tM6xuJKQ\\nkGAcDkeDzZednW26detm1q5da4wxpri42PzkJz+p8N6Ghoaa\/fv3VztfYmKisSzLPPDAAw0WIwCg\\neaAGowarC2owajB4Nn7CCdicZVm1vuhrbGysc51XXnlFaWlpFca0adNG69atU3JyshYtWqTRo0cr\\nPDxcPj4+8vX1VefOnTV69GjFxMRo69atSk9P1\/Dhwyvd3uTJk\/XVV19p4cKF6tu3r\/z8\/BQcHKxB\\ngwZp0aJFOnjwoEaNGlXt\/owYMUJpaWl6+eWX9cADD2jYsGEKDQ2Vt7e32rRpo4iICN1yyy1asWKF\\njh8\/rrvuusvle1bT960m42o6Z0236+\/vr7\/\/\/e9atmyZhg8frvbt28vb27vWsVcmJCREw4cP1\/79\\n+513MHKla9euCg0NVXBwsB566CHn9j\/99FMNGDBAfn5+Cg8P16pVq+ocU1N46qmnFBMT47xNupeX\\nl1588UU9+eSTCgoKUqdOnTR79mx9\/vnnGjZsWLXzJSYmyrIsTZgwobFDBwDYEDUYNVhtUYNRg8Gz\\nWcY0wNUkAQC28vzzz2vevHn629\/+pokTJ7o7nEqV3UK9Me6C1RBuueUWJSYm6uzZs83yIskAAKDp\\nUYPVHzUY7IpvoAGAB7r33nsVEBCg119\/3d2hNEtZWVn65JNPNGPGDAo3AABQY9Rg9UMNBjujgQYA\\nHigoKEg\/\/\/nP9T\/\/8z\/KzMx0dziVCgsL09ChQ90dRqX+9Kc\/qaSkRI899pi7QwEAAM0INVj9UIPB\\nzvgJJwB4qHPnzmnAgAG666679Oyzz7o7nGajoKBAvXv31g9\/+EOtX7\/e3eEAAIBmhhqsbqjBYHd8\\nAw0APFRgYKDWrFmjtWvXtsjbqdfV73\/\/e5WUlGjFihXuDgUAADRD1GB1Qw0Gu+MbaADg4Z544glt\\n27ZNiYmJ8vb2dnc4tvaPf\/xDY8eO1ZYtWzRu3Dh3hwMAAJoxarCaowZDc0ADDQBagAceeEDFxcXa\\ntGmTu0OxrW+\/\/VajRo3SY489ptmzZ7s7HAAA4AGowapHDYbmggYaALQQjz\/+uEJCQvToo4+6OxTb\\nKSoq0rhx4zRv3jzdfffd7g4HAAB4EGqwqlGDoTmhgQYAaPGKi4t16tQp9erVy92hAAAAtBjUYGhO\\naKABAAAAAAAALnAXTgAAAAAAAMAFGmgAAAAAAACACzTQAAAAAAAAABdooAEAAAAAAAAu0EADAAAA\\nAAAAXKCBBgAAAAAAALhAAw0AAAAAAABw4f8APkgL4hDeLgkAAAAASUVORK5CYII=\\n\",\n       \"text\/plain\": [\n-       \"<matplotlib.figure.Figure at 0x10bc046d8>\"\n+       \"<matplotlib.figure.Figure at 0x7f33d076f978>\"\n       ]\n      },\n      \"metadata\": {},\n@@ -42,31 +80,22 @@\n     }\n    ],\n    \"source\": [\n-    \"def getfz():\\n\",\n-    \"    def fzHIon(rs):\\n\",\n-    \"        return 0\\n\",\n-    \"    def fzHeIon(rs):\\n\",\n-    \"        return 0\\n\",\n-    \"    def fzHLya(rs):\\n\",\n-    \"        return 0\\n\",\n-    \"    def fzHeat(rs):\\n\",\n-    \"        return 0\\n\",\n-    \"    return {'HIon':fzHIon, 'HeIon':fzHeIon, 'HLya':fzHLya, 'Heat':fzHeat}\\n\",\n+    \"plt.figure(figsize=(20,10))\\n\",\n     \"\\n\",\n-    \"def getinjrate():\\n\",\n-    \"    def injrate(rs): \\n\",\n-    \"        return 0\\n\",\n-    \"    return injrate\\n\",\n-    \"\\n\",\n-    \"initrs = 2000\\n\",\n-    \"initCond = [TCMB(initrs), 5]\\n\",\n-    \"\\n\",\n-    \"p = getTLADE(getfz(),getinjrate())\\n\",\n-    \"\\n\",\n-    \"rsVec = flipud(logspace(2,log10(initrs), num=40))\\n\",\n+    \"plt.subplot(121)\\n\",\n+    \"plt.plot(rsVec,0.5+0.5*tanh(stdIonThermHist[:,1]),linestyle='dashed',color='black')\\n\",\n+    \"plt.plot(rsVec,0.5+0.5*tanh(ionThermHist[:,1]))\\n\",\n+    \"plt.xlabel('Redshift '+'$(1+z)$')\\n\",\n+    \"plt.ylabel('Free Electron Fraction $x_e$')\\n\",\n+    \"plt.ylim([1e-4,2])\\n\",\n+    \"plt.xscale('log')\\n\",\n+    \"plt.yscale('log')\\n\",\n     \"\\n\",\n-    \"a = odeint(p,initCond,rsVec)\\n\",\n-    \"plt.plot(rsVec,0.5+0.5*tanh(a[:,1]),'b',label='theta(t)')\\n\",\n+    \"plt.subplot(122)\\n\",\n+    \"plt.plot(rsVec,stdIonThermHist[:,0]\/kB,linestyle='dashed',color='black')\\n\",\n+    \"plt.plot(rsVec,ionThermHist[:,0]\/kB)\\n\",\n+    \"plt.xlabel('Redshift $(1+z)$')\\n\",\n+    \"plt.ylabel('IGM Temperature [K]')\\n\",\n     \"plt.xscale('log')\\n\",\n     \"plt.yscale('log')\\n\",\n     \"\\n\",\n@@ -99,7 +128,7 @@\n    \"name\": \"python\",\n    \"nbconvert_exporter\": \"python\",\n    \"pygments_lexer\": \"ipython3\",\n-   \"version\": \"3.5.2\"\n+   \"version\": \"3.5.1\"\n   }\n  },\n  \"nbformat\": 4,\ndiff --git a\/tableau10.mplstyle b\/tableau10.mplstyle\nnew file mode 100644\nindex 0000000..9730a59\n--- \/dev\/null\n+++ b\/tableau10.mplstyle\n@@ -0,0 +1,37 @@\n+# Author: Randal S. Olson (randalolson.com \/ @randal_olson)\n+# Uses Tableau's Tableau10 color scheme\n+\n+figure.figsize: 12, 7\n+figure.edgecolor: white\n+figure.facecolor: white\n+\n+lines.linewidth: 2.5\n+lines.markeredgewidth: 0\n+lines.markersize: 10\n+lines.dash_capstyle: butt\n+\n+legend.fancybox: True\n+\n+font.size: 14\n+\n+axes.prop_cycle: cycler('color',['1f77b4', 'ff7f0e', '2ca02c', 'd62728', '9467bd', '8c564b', 'e377c2', '7f7f7f', 'bcbd22', '17becf'])\n+axes.linewidth: 1\n+axes.titlesize: 22\n+axes.labelsize: 32\n+\n+xtick.labelsize: 20\n+ytick.labelsize: 20\n+xtick.major.size: 0\n+xtick.minor.size: 0\n+ytick.major.size: 0\n+ytick.minor.size: 0\n+\n+axes.grid: True\n+grid.alpha: 0.3\n+grid.linewidth: 0.5\n+grid.linestyle: --\n+grid.color: black\n+\n+savefig.transparent: False\n+savefig.bbox: tight\n+savefig.format: png\n\\ No newline at end of file\n","files":{"\/TLA.py":{"changes":[{"diff":"\n \n def getTLADE(fz, injRate):\n \n-\tdef TLADE(var, rs):\n+\tdef TLADE(var,rs):\n \n \t\tdef xe(y): \n \t\t\treturn 0.5 + 0.5*tanh(y)\n \n+\t\tdef dydz(y):\n+\t\t\treturn (2*cosh(y)**2) * dtdz(rs) * (CPeebles(xe(y),rs)*(alphae(Tm)*xe(y)**2*nH*rs**3 - \n+\t\t\t\t\tbetae(TCMB(rs))*(1-xe(y))*exp(-lyaEng\/Tm)) -\n+\t\t\t\t\t\tfz['HIon'](rs,xe(y))*injRate(rs)\/(rydberg*nH*rs**3) - \n+\t\t\t\t\t\t(1 - CPeebles(xe(y),rs))*fz['HLya'](rs,xe(y))*injRate(rs)\/(lyaEng*nH*rs**3))\n+\n \t\tTm, y = var\n \n \t\t# dvardz = ([\n","add":7,"remove":1,"filename":"\/TLA.py","badparts":["\tdef TLADE(var, rs):"],"goodparts":["\tdef TLADE(var,rs):","\t\tdef dydz(y):","\t\t\treturn (2*cosh(y)**2) * dtdz(rs) * (CPeebles(xe(y),rs)*(alphae(Tm)*xe(y)**2*nH*rs**3 - ","\t\t\t\t\tbetae(TCMB(rs))*(1-xe(y))*exp(-lyaEng\/Tm)) -","\t\t\t\t\t\tfz['HIon'](rs,xe(y))*injRate(rs)\/(rydberg*nH*rs**3) - ","\t\t\t\t\t\t(1 - CPeebles(xe(y),rs))*fz['HLya'](rs,xe(y))*injRate(rs)\/(lyaEng*nH*rs**3))"]},{"diff":"\n \t\tdvardz = ([\n \t\t\t(2*Tm\/rs - \n \t\t\tdtdz(rs)*(comptonCMB(xe(y), Tm, rs) + \n-\t\t\t\t1\/(1 + xe(y) + nHe\/nH)*2\/(3*nH*rs**3)*fz['Heat'](rs)*injRate(rs))),\n-\t\t\t(2*cosh(y)**2) * dtdz(rs) * (CPeebles(xe(y),rs)*\n-\t\t\t\t(alphae(Tm)*xe(y)**2*nH*rs**3 - \n-\t\t\t\t\tbetae(TCMB(rs))*(1-xe(y))*exp(-lyaEng\/Tm)) -\n-\t\t\t\tfz['HIon'](rs)*injRate(rs)\/(13.6*nH*rs**3) - \n-\t\t\t\t(1 - CPeebles(xe(y),rs))*fz['HLya'](rs)*injRate(rs)\/(lyaEng*nH*rs**3)\n-\t\t\t\t)])\n+\t\t\t\t1\/(1 + xe(y) + nHe\/nH)*2\/(3*nH*rs**3)*fz['Heat'](rs,xe(y))*injRate(rs))), dydz(y)])\n \t\t\n \t\treturn dvardz\n \n \treturn TLADE\n \n+def getIonThermHist(initrs,initCond,fz,injRate,rsVec):\n+\n+\tionThermHistDE = getTLADE(fz,injRate)\n+\treturn odeint(ionThermHistDE,initCond,rsVec,mxstep=500)\n\\ No newline at end of file","add":5,"remove":7,"filename":"\/TLA.py","badparts":["\t\t\t\t1\/(1 + xe(y) + nHe\/nH)*2\/(3*nH*rs**3)*fz['Heat'](rs)*injRate(rs))),","\t\t\t(2*cosh(y)**2) * dtdz(rs) * (CPeebles(xe(y),rs)*","\t\t\t\t(alphae(Tm)*xe(y)**2*nH*rs**3 - ","\t\t\t\t\tbetae(TCMB(rs))*(1-xe(y))*exp(-lyaEng\/Tm)) -","\t\t\t\tfz['HIon'](rs)*injRate(rs)\/(13.6*nH*rs**3) - ","\t\t\t\t(1 - CPeebles(xe(y),rs))*fz['HLya'](rs)*injRate(rs)\/(lyaEng*nH*rs**3)","\t\t\t\t)])"],"goodparts":["\t\t\t\t1\/(1 + xe(y) + nHe\/nH)*2\/(3*nH*rs**3)*fz['Heat'](rs,xe(y))*injRate(rs))), dydz(y)])","def getIonThermHist(initrs,initCond,fz,injRate,rsVec):","\tionThermHistDE = getTLADE(fz,injRate)","\treturn odeint(ionThermHistDE,initCond,rsVec,mxstep=500)"]}],"source":"\nfrom numpy import * from cosmo import * def comptonCMB(xe, Tm, rs): \t \treturn(xe\/(1 +xe +nHe\/nH)) *(TCMB(rs) -Tm)*32*thomsonXSec*stefBoltz*TCMB(rs)**4\/(3*me) def KLyman(rs, omegaM=omegaM, omegaRad=omegaRad, omegaLambda=omegaLambda): \t \treturn(c\/lyaFreq)**3\/(8*pi*hubblerates(rs, H0, omegaM, omegaRad, omegaLambda)) def alphae(Tm): \t \treturn 1e-13 *(4.309 *(1.16405*Tm)**(-0.6166))\/(1 +0.6703*(1.16405*Tm)**0.5300) def betae(Tr): \t \tthermlambda=c*(2*pi*hbar)\/sqrt(2*pi*(mp*me\/(me+mp))*Tr) \treturn alphae(Tr) * exp(-(rydberg\/4)\/Tr)\/(thermlambda**3) def CPeebles(xe,rs): \t \tnum=Lambda2s*(1-xe) +1\/(KLyman(rs) * nH * rs**3) \tden=Lambda2s*(1-xe) +1\/(KLyman(rs) * nH * rs**3) +betae(TCMB(rs))*(1-xe) \treturn num\/den def getTLADE(fz, injRate): \tdef TLADE(var, rs): \t\tdef xe(y): \t\t\treturn 0.5 +0.5*tanh(y) \t\tTm, y=var \t\t \t\t \t\t \t\t \t\t \t\t \t \t\tdvardz=([ \t\t\t(2*Tm\/rs - \t\t\tdtdz(rs)*(comptonCMB(xe(y), Tm, rs) + \t\t\t\t1\/(1 +xe(y) +nHe\/nH)*2\/(3*nH*rs**3)*fz['Heat'](rs)*injRate(rs))), \t\t\t(2*cosh(y)**2) * dtdz(rs) *(CPeebles(xe(y),rs)* \t\t\t\t(alphae(Tm)*xe(y)**2*nH*rs**3 - \t\t\t\t\tbetae(TCMB(rs))*(1-xe(y))*exp(-lyaEng\/Tm)) - \t\t\t\tfz['HIon'](rs)*injRate(rs)\/(13.6*nH*rs**3) - \t\t\t\t(1 -CPeebles(xe(y),rs))*fz['HLya'](rs)*injRate(rs)\/(lyaEng*nH*rs**3) \t\t\t\t)]) \t\t \t\treturn dvardz \treturn TLADE ","sourceWithComments":"from numpy import *\nfrom cosmo import *\n\ndef comptonCMB(xe, Tm, rs): \n\t# Compton cooling rate\n\treturn (xe\/(1 + xe + nHe\/nH)) * (TCMB(rs) - Tm)*32*thomsonXSec*stefBoltz*TCMB(rs)**4\/(3*me)\n\ndef KLyman(rs, omegaM=omegaM, omegaRad=omegaRad, omegaLambda=omegaLambda): \n\t# Rate at which Lya-photons cross the line\n\treturn (c\/lyaFreq)**3\/(8*pi*hubblerates(rs, H0, omegaM, omegaRad, omegaLambda))\n\ndef alphae(Tm): \n\t# Case-B recombination coefficient\n\treturn 1e-13 * (4.309 * (1.16405*Tm)**(-0.6166))\/(1 + 0.6703*(1.16405*Tm)**0.5300)\n\ndef betae(Tr):\n\t# Case-B photoionization coefficient\n\tthermlambda = c*(2*pi*hbar)\/sqrt(2*pi*(mp*me\/(me+mp))*Tr)\n\treturn alphae(Tr) * exp(-(rydberg\/4)\/Tr)\/(thermlambda**3) \n\ndef CPeebles(xe,rs):\n\t# Peebles C-factor \n\tnum = Lambda2s*(1-xe) + 1\/(KLyman(rs) * nH * rs**3)\n\tden = Lambda2s*(1-xe) + 1\/(KLyman(rs) * nH * rs**3) + betae(TCMB(rs))*(1-xe)\n\treturn num\/den\n\ndef getTLADE(fz, injRate):\n\n\tdef TLADE(var, rs):\n\n\t\tdef xe(y): \n\t\t\treturn 0.5 + 0.5*tanh(y)\n\n\t\tTm, y = var\n\n\t\t# dvardz = ([\n\t\t# \t(2*Tm\/rs - \n\t\t# \tdtdz(rs)*(comptonCMB(xe(y), Tm, rs))),\n\t\t# \t(2*cosh(y)**2) * dtdz(rs) * (CPeebles(xe(y),rs)*\n\t\t# \t\t(alphae(Tm)*xe(y)**2*nH*rs**3 - \n\t\t# \t\t\tbetae(TCMB(rs))*(1-xe(y))*exp(-lyaEng\/Tm)))])\n\n\t\n\t\tdvardz = ([\n\t\t\t(2*Tm\/rs - \n\t\t\tdtdz(rs)*(comptonCMB(xe(y), Tm, rs) + \n\t\t\t\t1\/(1 + xe(y) + nHe\/nH)*2\/(3*nH*rs**3)*fz['Heat'](rs)*injRate(rs))),\n\t\t\t(2*cosh(y)**2) * dtdz(rs) * (CPeebles(xe(y),rs)*\n\t\t\t\t(alphae(Tm)*xe(y)**2*nH*rs**3 - \n\t\t\t\t\tbetae(TCMB(rs))*(1-xe(y))*exp(-lyaEng\/Tm)) -\n\t\t\t\tfz['HIon'](rs)*injRate(rs)\/(13.6*nH*rs**3) - \n\t\t\t\t(1 - CPeebles(xe(y),rs))*fz['HLya'](rs)*injRate(rs)\/(lyaEng*nH*rs**3)\n\t\t\t\t)])\n\t\t\n\t\treturn dvardz\n\n\treturn TLADE\n\n"}},"msg":"odeint integrator set to correctly solve very large ionizations. f(z) and injection rate functions correctly implemented. matplotlib style sheet included."}},"https:\/\/github.com\/hongwanliu\/dmionhist":{"cb801bb2cec66b97fc6a5794b280f507566b53b8":{"url":"https:\/\/api.github.com\/repos\/hongwanliu\/dmionhist\/commits\/cb801bb2cec66b97fc6a5794b280f507566b53b8","html_url":"https:\/\/github.com\/hongwanliu\/dmionhist\/commit\/cb801bb2cec66b97fc6a5794b280f507566b53b8","message":"odeint integrator set to correctly solve very large ionizations. f(z) and injection rate functions correctly implemented. matplotlib style sheet included.","sha":"cb801bb2cec66b97fc6a5794b280f507566b53b8","keyword":"function injection correct","diff":"diff --git a\/TLA.py b\/TLA.py\nindex 505c1dd..163353c 100644\n--- a\/TLA.py\n+++ b\/TLA.py\n@@ -1,5 +1,6 @@\n from numpy import *\n from cosmo import *\n+from scipy.integrate import odeint\n \n def comptonCMB(xe, Tm, rs): \n \t# Compton cooling rate\n@@ -26,11 +27,17 @@ def CPeebles(xe,rs):\n \n def getTLADE(fz, injRate):\n \n-\tdef TLADE(var, rs):\n+\tdef TLADE(var,rs):\n \n \t\tdef xe(y): \n \t\t\treturn 0.5 + 0.5*tanh(y)\n \n+\t\tdef dydz(y):\n+\t\t\treturn (2*cosh(y)**2) * dtdz(rs) * (CPeebles(xe(y),rs)*(alphae(Tm)*xe(y)**2*nH*rs**3 - \n+\t\t\t\t\tbetae(TCMB(rs))*(1-xe(y))*exp(-lyaEng\/Tm)) -\n+\t\t\t\t\t\tfz['HIon'](rs,xe(y))*injRate(rs)\/(rydberg*nH*rs**3) - \n+\t\t\t\t\t\t(1 - CPeebles(xe(y),rs))*fz['HLya'](rs,xe(y))*injRate(rs)\/(lyaEng*nH*rs**3))\n+\n \t\tTm, y = var\n \n \t\t# dvardz = ([\n@@ -44,15 +51,13 @@ def xe(y):\n \t\tdvardz = ([\n \t\t\t(2*Tm\/rs - \n \t\t\tdtdz(rs)*(comptonCMB(xe(y), Tm, rs) + \n-\t\t\t\t1\/(1 + xe(y) + nHe\/nH)*2\/(3*nH*rs**3)*fz['Heat'](rs)*injRate(rs))),\n-\t\t\t(2*cosh(y)**2) * dtdz(rs) * (CPeebles(xe(y),rs)*\n-\t\t\t\t(alphae(Tm)*xe(y)**2*nH*rs**3 - \n-\t\t\t\t\tbetae(TCMB(rs))*(1-xe(y))*exp(-lyaEng\/Tm)) -\n-\t\t\t\tfz['HIon'](rs)*injRate(rs)\/(13.6*nH*rs**3) - \n-\t\t\t\t(1 - CPeebles(xe(y),rs))*fz['HLya'](rs)*injRate(rs)\/(lyaEng*nH*rs**3)\n-\t\t\t\t)])\n+\t\t\t\t1\/(1 + xe(y) + nHe\/nH)*2\/(3*nH*rs**3)*fz['Heat'](rs,xe(y))*injRate(rs))), dydz(y)])\n \t\t\n \t\treturn dvardz\n \n \treturn TLADE\n \n+def getIonThermHist(initrs,initCond,fz,injRate,rsVec):\n+\n+\tionThermHistDE = getTLADE(fz,injRate)\n+\treturn odeint(ionThermHistDE,initCond,rsVec,mxstep=500)\n\\ No newline at end of file\ndiff --git a\/__pycache__\/TLA.cpython-35.pyc b\/__pycache__\/TLA.cpython-35.pyc\nindex 1f4e487..ba02c96 100644\nBinary files a\/__pycache__\/TLA.cpython-35.pyc and b\/__pycache__\/TLA.cpython-35.pyc differ\ndiff --git a\/__pycache__\/cosmo.cpython-35.pyc b\/__pycache__\/cosmo.cpython-35.pyc\nindex afeec46..b391b55 100644\nBinary files a\/__pycache__\/cosmo.cpython-35.pyc and b\/__pycache__\/cosmo.cpython-35.pyc differ\ndiff --git a\/cosmo.py b\/cosmo.py\nindex e794963..58ea8cb 100644\n--- a\/cosmo.py\n+++ b\/cosmo.py\n@@ -7,6 +7,7 @@\n c           = 299792458e2                 # speed of light in cm\/s\n kB          = 8.6173324e-5                # Boltzmann constant in eV\/K\n alpha       = 1\/137.035999139             # fine structure constant\n+ele         = 1.60217662e-19\n \n # Atomic and optical physics\n \ndiff --git a\/dmionhist.ipynb b\/dmionhist.ipynb\nindex 6eba637..a09e7e0 100644\n--- a\/dmionhist.ipynb\n+++ b\/dmionhist.ipynb\n@@ -9,18 +9,21 @@\n    \"outputs\": [],\n    \"source\": [\n     \"from numpy import *\\n\",\n-    \"from scipy.integrate import odeint\\n\",\n     \"from cosmo import * \\n\",\n     \"from TLA   import *\\n\",\n     \"import matplotlib.pyplot as plt\\n\",\n-    \"%matplotlib inline\"\n+    \"%matplotlib inline\\n\",\n+    \"\\n\",\n+    \"plt.style.use('tableau10.mplstyle')\"\n    ]\n   },\n   {\n    \"cell_type\": \"markdown\",\n    \"metadata\": {},\n    \"source\": [\n-    \"# Three-Level Atom Integration\"\n+    \"# Three-Level Atom Integration\\n\",\n+    \"\\n\",\n+    \"This part of the code \"\n    ]\n   },\n   {\n@@ -29,12 +32,47 @@\n    \"metadata\": {\n     \"collapsed\": false\n    },\n+   \"outputs\": [],\n+   \"source\": [\n+    \"def getfz():\\n\",\n+    \"    def fzHIon(rs,xe):\\n\",\n+    \"        return (1-xe)\/3\\n\",\n+    \"    def fzHeIon(rs,xe):\\n\",\n+    \"        return 0\\n\",\n+    \"    def fzHLya(rs,xe):\\n\",\n+    \"        return (1-xe)\/3\\n\",\n+    \"    def fzHeat(rs,xe):\\n\",\n+    \"        return (1+2*xe)\/3\\n\",\n+    \"    return {'HIon':fzHIon, 'HeIon':fzHeIon, 'HLya':fzHLya, 'Heat':fzHeat}\\n\",\n+    \"\\n\",\n+    \"def getinjrate(injType,injFac):\\n\",\n+    \"    def injrate(rs): \\n\",\n+    \"        if injType == 'sWave':\\n\",\n+    \"            return injFac*(rs**6)\\n\",\n+    \"        elif injType == 'decay':\\n\",\n+    \"            return injFac*(rs**3)\\n\",\n+    \"    return injrate\\n\",\n+    \"\\n\",\n+    \"initrs = 3000\\n\",\n+    \"initCond = [TCMB(initrs), 5]\\n\",\n+    \"rsVec = flipud(logspace(1,log10(initrs), num=2000))\\n\",\n+    \"\\n\",\n+    \"ionThermHist = getIonThermHist(initrs,initCond,getfz(),getinjrate('decay',1\/1e24),rsVec)\\n\",\n+    \"stdIonThermHist = getIonThermHist(initrs,initCond,getfz(),getinjrate('decay',1\/1e24),rsVec)\\n\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 4,\n+   \"metadata\": {\n+    \"collapsed\": false\n+   },\n    \"outputs\": [\n     {\n      \"data\": {\n-      \"image\/png\": \"iVBORw0KGgoAAAANSUhEUgAAAhUAAAFsCAYAAAB7IUvqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\\nAElFTkSuQmCC\\n\",\n+      \"image\/png\": \"iVBORw0KGgoAAAANSUhEUgAABNAAAAKMCAYAAAAqrkVTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\\n\",\n       \"text\/plain\": [\n-       \"<matplotlib.figure.Figure at 0x10bc046d8>\"\n+       \"<matplotlib.figure.Figure at 0x7f33d076f978>\"\n       ]\n      },\n      \"metadata\": {},\n@@ -42,31 +80,22 @@\n     }\n    ],\n    \"source\": [\n-    \"def getfz():\\n\",\n-    \"    def fzHIon(rs):\\n\",\n-    \"        return 0\\n\",\n-    \"    def fzHeIon(rs):\\n\",\n-    \"        return 0\\n\",\n-    \"    def fzHLya(rs):\\n\",\n-    \"        return 0\\n\",\n-    \"    def fzHeat(rs):\\n\",\n-    \"        return 0\\n\",\n-    \"    return {'HIon':fzHIon, 'HeIon':fzHeIon, 'HLya':fzHLya, 'Heat':fzHeat}\\n\",\n+    \"plt.figure(figsize=(20,10))\\n\",\n     \"\\n\",\n-    \"def getinjrate():\\n\",\n-    \"    def injrate(rs): \\n\",\n-    \"        return 0\\n\",\n-    \"    return injrate\\n\",\n-    \"\\n\",\n-    \"initrs = 2000\\n\",\n-    \"initCond = [TCMB(initrs), 5]\\n\",\n-    \"\\n\",\n-    \"p = getTLADE(getfz(),getinjrate())\\n\",\n-    \"\\n\",\n-    \"rsVec = flipud(logspace(2,log10(initrs), num=40))\\n\",\n+    \"plt.subplot(121)\\n\",\n+    \"plt.plot(rsVec,0.5+0.5*tanh(stdIonThermHist[:,1]),linestyle='dashed',color='black')\\n\",\n+    \"plt.plot(rsVec,0.5+0.5*tanh(ionThermHist[:,1]))\\n\",\n+    \"plt.xlabel('Redshift '+'$(1+z)$')\\n\",\n+    \"plt.ylabel('Free Electron Fraction $x_e$')\\n\",\n+    \"plt.ylim([1e-4,2])\\n\",\n+    \"plt.xscale('log')\\n\",\n+    \"plt.yscale('log')\\n\",\n     \"\\n\",\n-    \"a = odeint(p,initCond,rsVec)\\n\",\n-    \"plt.plot(rsVec,0.5+0.5*tanh(a[:,1]),'b',label='theta(t)')\\n\",\n+    \"plt.subplot(122)\\n\",\n+    \"plt.plot(rsVec,stdIonThermHist[:,0]\/kB,linestyle='dashed',color='black')\\n\",\n+    \"plt.plot(rsVec,ionThermHist[:,0]\/kB)\\n\",\n+    \"plt.xlabel('Redshift $(1+z)$')\\n\",\n+    \"plt.ylabel('IGM Temperature [K]')\\n\",\n     \"plt.xscale('log')\\n\",\n     \"plt.yscale('log')\\n\",\n     \"\\n\",\n@@ -99,7 +128,7 @@\n    \"name\": \"python\",\n    \"nbconvert_exporter\": \"python\",\n    \"pygments_lexer\": \"ipython3\",\n-   \"version\": \"3.5.2\"\n+   \"version\": \"3.5.1\"\n   }\n  },\n  \"nbformat\": 4,\ndiff --git a\/tableau10.mplstyle b\/tableau10.mplstyle\nnew file mode 100644\nindex 0000000..9730a59\n--- \/dev\/null\n+++ b\/tableau10.mplstyle\n@@ -0,0 +1,37 @@\n+# Author: Randal S. Olson (randalolson.com \/ @randal_olson)\n+# Uses Tableau's Tableau10 color scheme\n+\n+figure.figsize: 12, 7\n+figure.edgecolor: white\n+figure.facecolor: white\n+\n+lines.linewidth: 2.5\n+lines.markeredgewidth: 0\n+lines.markersize: 10\n+lines.dash_capstyle: butt\n+\n+legend.fancybox: True\n+\n+font.size: 14\n+\n+axes.prop_cycle: cycler('color',['1f77b4', 'ff7f0e', '2ca02c', 'd62728', '9467bd', '8c564b', 'e377c2', '7f7f7f', 'bcbd22', '17becf'])\n+axes.linewidth: 1\n+axes.titlesize: 22\n+axes.labelsize: 32\n+\n+xtick.labelsize: 20\n+ytick.labelsize: 20\n+xtick.major.size: 0\n+xtick.minor.size: 0\n+ytick.major.size: 0\n+ytick.minor.size: 0\n+\n+axes.grid: True\n+grid.alpha: 0.3\n+grid.linewidth: 0.5\n+grid.linestyle: --\n+grid.color: black\n+\n+savefig.transparent: False\n+savefig.bbox: tight\n+savefig.format: png\n\\ No newline at end of file\n","files":{"\/TLA.py":{"changes":[{"diff":"\n \n def getTLADE(fz, injRate):\n \n-\tdef TLADE(var, rs):\n+\tdef TLADE(var,rs):\n \n \t\tdef xe(y): \n \t\t\treturn 0.5 + 0.5*tanh(y)\n \n+\t\tdef dydz(y):\n+\t\t\treturn (2*cosh(y)**2) * dtdz(rs) * (CPeebles(xe(y),rs)*(alphae(Tm)*xe(y)**2*nH*rs**3 - \n+\t\t\t\t\tbetae(TCMB(rs))*(1-xe(y))*exp(-lyaEng\/Tm)) -\n+\t\t\t\t\t\tfz['HIon'](rs,xe(y))*injRate(rs)\/(rydberg*nH*rs**3) - \n+\t\t\t\t\t\t(1 - CPeebles(xe(y),rs))*fz['HLya'](rs,xe(y))*injRate(rs)\/(lyaEng*nH*rs**3))\n+\n \t\tTm, y = var\n \n \t\t# dvardz = ([\n","add":7,"remove":1,"filename":"\/TLA.py","badparts":["\tdef TLADE(var, rs):"],"goodparts":["\tdef TLADE(var,rs):","\t\tdef dydz(y):","\t\t\treturn (2*cosh(y)**2) * dtdz(rs) * (CPeebles(xe(y),rs)*(alphae(Tm)*xe(y)**2*nH*rs**3 - ","\t\t\t\t\tbetae(TCMB(rs))*(1-xe(y))*exp(-lyaEng\/Tm)) -","\t\t\t\t\t\tfz['HIon'](rs,xe(y))*injRate(rs)\/(rydberg*nH*rs**3) - ","\t\t\t\t\t\t(1 - CPeebles(xe(y),rs))*fz['HLya'](rs,xe(y))*injRate(rs)\/(lyaEng*nH*rs**3))"]},{"diff":"\n \t\tdvardz = ([\n \t\t\t(2*Tm\/rs - \n \t\t\tdtdz(rs)*(comptonCMB(xe(y), Tm, rs) + \n-\t\t\t\t1\/(1 + xe(y) + nHe\/nH)*2\/(3*nH*rs**3)*fz['Heat'](rs)*injRate(rs))),\n-\t\t\t(2*cosh(y)**2) * dtdz(rs) * (CPeebles(xe(y),rs)*\n-\t\t\t\t(alphae(Tm)*xe(y)**2*nH*rs**3 - \n-\t\t\t\t\tbetae(TCMB(rs))*(1-xe(y))*exp(-lyaEng\/Tm)) -\n-\t\t\t\tfz['HIon'](rs)*injRate(rs)\/(13.6*nH*rs**3) - \n-\t\t\t\t(1 - CPeebles(xe(y),rs))*fz['HLya'](rs)*injRate(rs)\/(lyaEng*nH*rs**3)\n-\t\t\t\t)])\n+\t\t\t\t1\/(1 + xe(y) + nHe\/nH)*2\/(3*nH*rs**3)*fz['Heat'](rs,xe(y))*injRate(rs))), dydz(y)])\n \t\t\n \t\treturn dvardz\n \n \treturn TLADE\n \n+def getIonThermHist(initrs,initCond,fz,injRate,rsVec):\n+\n+\tionThermHistDE = getTLADE(fz,injRate)\n+\treturn odeint(ionThermHistDE,initCond,rsVec,mxstep=500)\n\\ No newline at end of file","add":5,"remove":7,"filename":"\/TLA.py","badparts":["\t\t\t\t1\/(1 + xe(y) + nHe\/nH)*2\/(3*nH*rs**3)*fz['Heat'](rs)*injRate(rs))),","\t\t\t(2*cosh(y)**2) * dtdz(rs) * (CPeebles(xe(y),rs)*","\t\t\t\t(alphae(Tm)*xe(y)**2*nH*rs**3 - ","\t\t\t\t\tbetae(TCMB(rs))*(1-xe(y))*exp(-lyaEng\/Tm)) -","\t\t\t\tfz['HIon'](rs)*injRate(rs)\/(13.6*nH*rs**3) - ","\t\t\t\t(1 - CPeebles(xe(y),rs))*fz['HLya'](rs)*injRate(rs)\/(lyaEng*nH*rs**3)","\t\t\t\t)])"],"goodparts":["\t\t\t\t1\/(1 + xe(y) + nHe\/nH)*2\/(3*nH*rs**3)*fz['Heat'](rs,xe(y))*injRate(rs))), dydz(y)])","def getIonThermHist(initrs,initCond,fz,injRate,rsVec):","\tionThermHistDE = getTLADE(fz,injRate)","\treturn odeint(ionThermHistDE,initCond,rsVec,mxstep=500)"]}],"source":"\nfrom numpy import * from cosmo import * def comptonCMB(xe, Tm, rs): \t \treturn(xe\/(1 +xe +nHe\/nH)) *(TCMB(rs) -Tm)*32*thomsonXSec*stefBoltz*TCMB(rs)**4\/(3*me) def KLyman(rs, omegaM=omegaM, omegaRad=omegaRad, omegaLambda=omegaLambda): \t \treturn(c\/lyaFreq)**3\/(8*pi*hubblerates(rs, H0, omegaM, omegaRad, omegaLambda)) def alphae(Tm): \t \treturn 1e-13 *(4.309 *(1.16405*Tm)**(-0.6166))\/(1 +0.6703*(1.16405*Tm)**0.5300) def betae(Tr): \t \tthermlambda=c*(2*pi*hbar)\/sqrt(2*pi*(mp*me\/(me+mp))*Tr) \treturn alphae(Tr) * exp(-(rydberg\/4)\/Tr)\/(thermlambda**3) def CPeebles(xe,rs): \t \tnum=Lambda2s*(1-xe) +1\/(KLyman(rs) * nH * rs**3) \tden=Lambda2s*(1-xe) +1\/(KLyman(rs) * nH * rs**3) +betae(TCMB(rs))*(1-xe) \treturn num\/den def getTLADE(fz, injRate): \tdef TLADE(var, rs): \t\tdef xe(y): \t\t\treturn 0.5 +0.5*tanh(y) \t\tTm, y=var \t\t \t\t \t\t \t\t \t\t \t\t \t \t\tdvardz=([ \t\t\t(2*Tm\/rs - \t\t\tdtdz(rs)*(comptonCMB(xe(y), Tm, rs) + \t\t\t\t1\/(1 +xe(y) +nHe\/nH)*2\/(3*nH*rs**3)*fz['Heat'](rs)*injRate(rs))), \t\t\t(2*cosh(y)**2) * dtdz(rs) *(CPeebles(xe(y),rs)* \t\t\t\t(alphae(Tm)*xe(y)**2*nH*rs**3 - \t\t\t\t\tbetae(TCMB(rs))*(1-xe(y))*exp(-lyaEng\/Tm)) - \t\t\t\tfz['HIon'](rs)*injRate(rs)\/(13.6*nH*rs**3) - \t\t\t\t(1 -CPeebles(xe(y),rs))*fz['HLya'](rs)*injRate(rs)\/(lyaEng*nH*rs**3) \t\t\t\t)]) \t\t \t\treturn dvardz \treturn TLADE ","sourceWithComments":"from numpy import *\nfrom cosmo import *\n\ndef comptonCMB(xe, Tm, rs): \n\t# Compton cooling rate\n\treturn (xe\/(1 + xe + nHe\/nH)) * (TCMB(rs) - Tm)*32*thomsonXSec*stefBoltz*TCMB(rs)**4\/(3*me)\n\ndef KLyman(rs, omegaM=omegaM, omegaRad=omegaRad, omegaLambda=omegaLambda): \n\t# Rate at which Lya-photons cross the line\n\treturn (c\/lyaFreq)**3\/(8*pi*hubblerates(rs, H0, omegaM, omegaRad, omegaLambda))\n\ndef alphae(Tm): \n\t# Case-B recombination coefficient\n\treturn 1e-13 * (4.309 * (1.16405*Tm)**(-0.6166))\/(1 + 0.6703*(1.16405*Tm)**0.5300)\n\ndef betae(Tr):\n\t# Case-B photoionization coefficient\n\tthermlambda = c*(2*pi*hbar)\/sqrt(2*pi*(mp*me\/(me+mp))*Tr)\n\treturn alphae(Tr) * exp(-(rydberg\/4)\/Tr)\/(thermlambda**3) \n\ndef CPeebles(xe,rs):\n\t# Peebles C-factor \n\tnum = Lambda2s*(1-xe) + 1\/(KLyman(rs) * nH * rs**3)\n\tden = Lambda2s*(1-xe) + 1\/(KLyman(rs) * nH * rs**3) + betae(TCMB(rs))*(1-xe)\n\treturn num\/den\n\ndef getTLADE(fz, injRate):\n\n\tdef TLADE(var, rs):\n\n\t\tdef xe(y): \n\t\t\treturn 0.5 + 0.5*tanh(y)\n\n\t\tTm, y = var\n\n\t\t# dvardz = ([\n\t\t# \t(2*Tm\/rs - \n\t\t# \tdtdz(rs)*(comptonCMB(xe(y), Tm, rs))),\n\t\t# \t(2*cosh(y)**2) * dtdz(rs) * (CPeebles(xe(y),rs)*\n\t\t# \t\t(alphae(Tm)*xe(y)**2*nH*rs**3 - \n\t\t# \t\t\tbetae(TCMB(rs))*(1-xe(y))*exp(-lyaEng\/Tm)))])\n\n\t\n\t\tdvardz = ([\n\t\t\t(2*Tm\/rs - \n\t\t\tdtdz(rs)*(comptonCMB(xe(y), Tm, rs) + \n\t\t\t\t1\/(1 + xe(y) + nHe\/nH)*2\/(3*nH*rs**3)*fz['Heat'](rs)*injRate(rs))),\n\t\t\t(2*cosh(y)**2) * dtdz(rs) * (CPeebles(xe(y),rs)*\n\t\t\t\t(alphae(Tm)*xe(y)**2*nH*rs**3 - \n\t\t\t\t\tbetae(TCMB(rs))*(1-xe(y))*exp(-lyaEng\/Tm)) -\n\t\t\t\tfz['HIon'](rs)*injRate(rs)\/(13.6*nH*rs**3) - \n\t\t\t\t(1 - CPeebles(xe(y),rs))*fz['HLya'](rs)*injRate(rs)\/(lyaEng*nH*rs**3)\n\t\t\t\t)])\n\t\t\n\t\treturn dvardz\n\n\treturn TLADE\n\n"}},"msg":"odeint integrator set to correctly solve very large ionizations. f(z) and injection rate functions correctly implemented. matplotlib style sheet included."}},"https:\/\/github.com\/CityofToronto\/bdit_tt_request_app":{"86eeb071048f906df26013a8900c2f44c516b3a6":{"url":"https:\/\/api.github.com\/repos\/CityofToronto\/bdit_tt_request_app\/commits\/86eeb071048f906df26013a8900c2f44c516b3a6","html_url":"https:\/\/github.com\/CityofToronto\/bdit_tt_request_app\/commit\/86eeb071048f906df26013a8900c2f44c516b3a6","message":"move SQL inline: get_closest_node() (#26)\n\n* get_closest_node in psycopg2 + raw sql #23\r\n\r\n* fixes for PR #23\r\n\r\n* bugfixes #23\r\n\r\n* pull db config from .env\r\n\r\n* remove configparser\r\n\r\n* syntax #26\r\n\r\n* replaced util parse function + \"'\" bugfix #26\r\n\r\n* sql injection fix #26\r\n\r\n* undo changes to other functions\r\n\r\n* spacing\r\n\r\n* Removed excess code #26\r\n\r\n* remove explicit closing of cursor #26\r\n\r\n* sql injection fix #26\r\n\r\n* formatting, update comment\r\n\r\n* deleted excess code #26\r\n\r\n---------\r\n\r\nCo-authored-by: Nate Wessel <nate.wessel@toronto.ca>","sha":"86eeb071048f906df26013a8900c2f44c516b3a6","keyword":"function injection update","diff":"diff --git a\/backend\/app\/parse_util.py b\/backend\/app\/parse_util.py\nindex 2cd4b17..37e13fe 100644\n--- a\/backend\/app\/parse_util.py\n+++ b\/backend\/app\/parse_util.py\n@@ -6,7 +6,7 @@\n from app import ALLOWED_FILE_TYPES, TIME_FORMAT, DATE_FORMAT, DB_TRAVEL_DATA_QUERY_RESULT_FORMAT\r\n \r\n __all__ = ['parse_file_type_request_body', 'parse_travel_request_body', 'parse_link_response',\r\n-           'parse_get_links_btwn_nodes_response', 'parse_node_response', 'parse_travel_data_query_result',\r\n+           'parse_get_links_btwn_nodes_response', 'parse_travel_data_query_result',\r\n            'parse_get_links_between_multi_nodes_request_body', 'get_path_list_from_link_list']\r\n \r\n \r\n@@ -346,35 +346,6 @@ def parse_get_links_btwn_nodes_response(response: str):\n     return {\"source\": source_target_links_tuple[0], \"target\": source_target_links_tuple[1],\r\n             \"path_name\": path_name, \"link_dirs\": link_dirs, \"geometry\": geom_json}\r\n \r\n-\r\n-def parse_node_response(node_data: str):\r\n-    \"\"\"\r\n-    Parse the given Node query result to a dictionary that can be jsonify-ed.\r\n-\r\n-    :param node_data: the query result from the Node table in the database\r\n-    :return: a tuple with first index the distance of the node and\r\n-            second index a dictionary containing the attributes for the Node query that can be jsonify-ed.\r\n-    \"\"\"\r\n-    import ast\r\n-    node_data = node_data.replace(',,', ',\"nameless road\",')\r\n-\r\n-    # sometimes intersec name is not quote surrounded\r\n-    first_comma = node_data.index(',')\r\n-    first_comma_quote = node_data.index(',\"')\r\n-    second_comma = node_data.index(',', first_comma + 1)\r\n-\r\n-    if first_comma != first_comma_quote:\r\n-        node_data = node_data[:first_comma + 1] + '\"' + node_data[first_comma + 1:second_comma] + '\"' + node_data[\r\n-                                                                                                        second_comma:]\r\n-\r\n-    data = ast.literal_eval(node_data)\r\n-    geom_raw = data[2]  # type: str\r\n-\r\n-    geom_raw = geom_raw.replace('type:', '\"type\":\"').replace(',coordinates:', '\",\"coordinates\":')\r\n-\r\n-    return data[3], {'node_id': data[0], 'name': data[1], 'geometry': json.loads(geom_raw)}\r\n-\r\n-\r\n def get_path_list_from_link_list(links):\r\n     \"\"\"\r\n     Get a list of street names with no adjacent duplication from the list of links.\r\ndiff --git a\/backend\/app\/routes.py b\/backend\/app\/routes.py\nindex 3c604db..6d7c483 100644\n--- a\/backend\/app\/routes.py\n+++ b\/backend\/app\/routes.py\n@@ -1,14 +1,21 @@\n import os\r\n+import json\r\n \r\n from flask import abort, jsonify, request, send_file\r\n from sqlalchemy import func\r\n \r\n+from psycopg2 import connect, sql\r\n+from psycopg2.extras import execute_values\r\n+\r\n from app import app, db\r\n from app.file_util import make_travel_data_csv, make_travel_data_xlsx\r\n from app.models import Link, Node\r\n from app.parse_util import *\r\n \r\n+#Global Variables\r\n+\r\n \r\n+#Functions\r\n def _need_keep_temp_file():\r\n     \"\"\"Check environ whether or not to keep the temporary files created.\"\"\"\r\n     if 'KEEP_TEMP_FILE' not in os.environ:\r\n@@ -36,40 +43,69 @@ def index():\n @app.route('\/closest-node\/<longitude>\/<latitude>', methods=['GET'])\r\n def get_closest_node(longitude, latitude):\r\n     \"\"\"\r\n-    Get the closest nodes to the given longitude and latitude.\r\n-    This function uses database function get_closest_nodes to fetch series of closest nodes to the given\r\n-    longitude and latitude, sorted by ascending distance order.\r\n-    Only points with distance less than 5 are returned by this function.\r\n+    This function fetches a set of closest nodes to the given\r\n+    point, sorted by ascending distance order.\r\n \r\n     :param longitude: the longitude of the origin point\r\n     :param latitude: the latitude of the origin point\r\n-    :return: JSON of an array containing the satisfying nodes.\r\n+    :return: JSON of an array containing the closest nodes.\r\n             The array is sorted in ascending distance order. node object keys: node_id(int),\r\n             geometry(geom{type(str), coordinates(list[int])}), name(str)\r\n     \"\"\"\r\n+    connection = connect(\r\n+        host = os.environ['DB_HOST'],\r\n+        dbname = os.environ['DB_NAME'],\r\n+        user = os.environ['DB_USER'],\r\n+        password = os.environ['DB_USER_PASSWORD'],\r\n+    )\r\n+\r\n     try:\r\n         longitude = float(longitude)\r\n         latitude = float(latitude)\r\n     except ValueError or ArithmeticError:\r\n         abort(400, description=\"Longitude and latitude must be decimal numbers!\")\r\n         return\r\n-\r\n-    nodes_ascend_dist_order_query_result = db.session.query(func.get_closest_nodes(longitude, latitude))\r\n+    \r\n+    with connection:\r\n+        with connection.cursor() as cursor:\r\n+            select_sql = '''\r\n+                WITH distances AS (\r\n+                    SELECT \r\n+                        node_id,\r\n+                        st_distance(\r\n+                            st_transform(geom, 2952),\r\n+                            st_transform(\r\n+                                st_setsrid(st_makepoint(%(longitude)s, %(latitude)s), 4326),\r\n+                                2952\r\n+                            )\r\n+                        ) AS distance\r\n+                    FROM here.routing_nodes_intersec_name\r\n+                )\r\n+                SELECT \r\n+                    here_nodes.node_id::int,\r\n+                    intersec_name,\r\n+                    st_asgeojson(geom),\r\n+                    distance\r\n+                FROM here.routing_nodes_intersec_name AS here_nodes\r\n+                JOIN distances ON here_nodes.node_id = distances.node_id\r\n+                ORDER BY distance\r\n+                LIMIT 10'''\r\n+            cursor.execute(select_sql, {\"latitude\": latitude, \"longitude\": longitude})\r\n+            nodes_ascend_dist_order_query_result = cursor.fetchall()\r\n \r\n     candidate_nodes = []\r\n     node_count = 0\r\n-    for node_query_result in nodes_ascend_dist_order_query_result:\r\n-        node_data = parse_node_response(node_query_result[0])\r\n-        node_dist = node_data[0]\r\n-        node_json = node_data[1]\r\n-\r\n-        if node_count == 0 or node_dist < 10:\r\n-            candidate_nodes.append(node_json)\r\n+    for node_id, stname, coord_dict, distance in nodes_ascend_dist_order_query_result:\r\n+        if node_count == 0 or distance < 10:\r\n+            candidate_nodes.append( {\r\n+                 'node_id': node_id,\r\n+                 'name': stname,\r\n+                 'geometry': json.loads(coord_dict)\r\n+            } )\r\n         else:\r\n             break\r\n-\r\n         node_count += 1\r\n-\r\n+    connection.close()\r\n     return jsonify(candidate_nodes)\r\n \r\n \r\n","files":{"\/backend\/app\/parse_util.py":{"changes":[{"diff":"\n from app import ALLOWED_FILE_TYPES, TIME_FORMAT, DATE_FORMAT, DB_TRAVEL_DATA_QUERY_RESULT_FORMAT\r\n \r\n __all__ = ['parse_file_type_request_body', 'parse_travel_request_body', 'parse_link_response',\r\n-           'parse_get_links_btwn_nodes_response', 'parse_node_response', 'parse_travel_data_query_result',\r\n+           'parse_get_links_btwn_nodes_response', 'parse_travel_data_query_result',\r\n            'parse_get_links_between_multi_nodes_request_body', 'get_path_list_from_link_list']\r\n \r\n \r\n","add":1,"remove":1,"filename":"\/backend\/app\/parse_util.py","badparts":["           'parse_get_links_btwn_nodes_response', 'parse_node_response', 'parse_travel_data_query_result',\r"],"goodparts":["           'parse_get_links_btwn_nodes_response', 'parse_travel_data_query_result',\r"]},{"diff":"\n     return {\"source\": source_target_links_tuple[0], \"target\": source_target_links_tuple[1],\r\n             \"path_name\": path_name, \"link_dirs\": link_dirs, \"geometry\": geom_json}\r\n \r\n-\r\n-def parse_node_response(node_data: str):\r\n-    \"\"\"\r\n-    Parse the given Node query result to a dictionary that can be jsonify-ed.\r\n-\r\n-    :param node_data: the query result from the Node table in the database\r\n-    :return: a tuple with first index the distance of the node and\r\n-            second index a dictionary containing the attributes for the Node query that can be jsonify-ed.\r\n-    \"\"\"\r\n-    import ast\r\n-    node_data = node_data.replace(',,', ',\"nameless road\",')\r\n-\r\n-    # sometimes intersec name is not quote surrounded\r\n-    first_comma = node_data.index(',')\r\n-    first_comma_quote = node_data.index(',\"')\r\n-    second_comma = node_data.index(',', first_comma + 1)\r\n-\r\n-    if first_comma != first_comma_quote:\r\n-        node_data = node_data[:first_comma + 1] + '\"' + node_data[first_comma + 1:second_comma] + '\"' + node_data[\r\n-                                                                                                        second_comma:]\r\n-\r\n-    data = ast.literal_eval(node_data)\r\n-    geom_raw = data[2]  # type: str\r\n-\r\n-    geom_raw = geom_raw.replace('type:', '\"type\":\"').replace(',coordinates:', '\",\"coordinates\":')\r\n-\r\n-    return data[3], {'node_id': data[0], 'name': data[1], 'geometry': json.loads(geom_raw)}\r\n-\r\n-\r\n def get_path_list_from_link_list(links):\r\n     \"\"\"\r\n     Get a list of street names with no adjacent duplication from the list of links.\r","add":0,"remove":29,"filename":"\/backend\/app\/parse_util.py","badparts":["\r","def parse_node_response(node_data: str):\r","    \"\"\"\r","    Parse the given Node query result to a dictionary that can be jsonify-ed.\r","\r","    :param node_data: the query result from the Node table in the database\r","    :return: a tuple with first index the distance of the node and\r","            second index a dictionary containing the attributes for the Node query that can be jsonify-ed.\r","    \"\"\"\r","    import ast\r","    node_data = node_data.replace(',,', ',\"nameless road\",')\r","\r","    first_comma = node_data.index(',')\r","    first_comma_quote = node_data.index(',\"')\r","    second_comma = node_data.index(',', first_comma + 1)\r","\r","    if first_comma != first_comma_quote:\r","        node_data = node_data[:first_comma + 1] + '\"' + node_data[first_comma + 1:second_comma] + '\"' + node_data[\r","                                                                                                        second_comma:]\r","\r","    data = ast.literal_eval(node_data)\r","    geom_raw = data[2]  # type: str\r","\r","    geom_raw = geom_raw.replace('type:', '\"type\":\"').replace(',coordinates:', '\",\"coordinates\":')\r","\r","    return data[3], {'node_id': data[0], 'name': data[1], 'geometry': json.loads(geom_raw)}\r","\r","\r"],"goodparts":[]}],"source":"\nimport json\r from datetime import datetime\r \r from flask import abort\r \r from app import ALLOWED_FILE_TYPES, TIME_FORMAT, DATE_FORMAT, DB_TRAVEL_DATA_QUERY_RESULT_FORMAT\r \r __all__=['parse_file_type_request_body', 'parse_travel_request_body', 'parse_link_response',\r 'parse_get_links_btwn_nodes_response', 'parse_node_response', 'parse_travel_data_query_result',\r 'parse_get_links_between_multi_nodes_request_body', 'get_path_list_from_link_list']\r \r \r def parse_file_type_request_body(file_request_data):\r \"\"\"\r Parse the request body that contains a file type and wanted column names.\r The file type should be specified in the request body JSON's field file_type.\r The column names should be a list of strings.\r \r If the file type specified in the request body is not an valid and allowed file type, the first file type defined\r in the ALLOWED_FILE_TYPES is used by default(csv by default).\r \r If column names is not given, all columns will be included in the data file.\r \r Caution: If file type is invalid, this function will call abort with error code 400.\r If an invalid column name was given, this function will call abort with error code 400.\r \r :param file_request_data: the request body json\r :return: A tuple of file info. First index is the first allowed file type(csv by default) if the file type\r specified in the request body JSON is invalid or not allowed, or the specified file type otherwise.\r Second index is a list of column names to be included.\r \"\"\"\r \r if 'file_type' in file_request_data:\r file_type=file_request_data['file_type']\r \r if file_type not in ALLOWED_FILE_TYPES:\r abort(400, description=\"Invalid file type %s! Allowed types: %s\" %(file_type, ALLOWED_FILE_TYPES))\r return\r else:\r file_type=ALLOWED_FILE_TYPES[0]\r \r if 'columns' in file_request_data and type(file_request_data['columns'])==list:\r columns=file_request_data['columns'] \r if False in[col in DB_TRAVEL_DATA_QUERY_RESULT_FORMAT for col in columns]:\r abort(400, description=\"Column name invalid! Legal values are: %s\" % str(\r list(DB_TRAVEL_DATA_QUERY_RESULT_FORMAT.keys())))\r return\r \r columns.insert(0, 'to_street')\r columns.insert(0, 'from_street')\r columns.insert(0, 'street')\r columns.insert(0, 'period')\r columns.insert(0, 'id')\r else:\r columns=list(DB_TRAVEL_DATA_QUERY_RESULT_FORMAT)\r \r return file_type, columns\r \r \r def parse_get_links_between_multi_nodes_request_body(nodes_data):\r \"\"\"\r Parse the request body that contains a list of node_ids.\r This function will call abort with response code 400 and error messages if any of the node_id is not an integer, or\r if field 'node_ids' does not exist in the request body, or if field 'node_ids' is not a list with minimum length 2.\r \r :param nodes_data: the body of the get_links_between_multi_nodes request\r :return: a list of integer node ids\r \"\"\"\r try:\r if 'node_ids' not in nodes_data:\r abort(400, description=\"Must provide a list of node_ids\")\r return\r except TypeError:\r abort(400,\r description=\"Request body of get_links_between_multi_nodes must be a JSON containing field 'node_ids'\")\r return\r \r node_id_list=nodes_data['node_ids']\r if type(node_id_list) !=list or len(node_id_list) < 2:\r abort(400, description=\"Field 'node_ids' must be a list of at least 2 node ids.\")\r return\r \r node_ids=[]\r for node_id in node_id_list:\r try:\r node_id_int=int(node_id)\r except ArithmeticError:\r abort(400, description='node_id must be an integer.')\r return\r \r node_ids.append(node_id_int)\r \r return node_ids\r \r \r def parse_travel_data_query_result(travel_query_result, columns, street_info):\r \"\"\"\r Parse the travel data query result into python dictionaries(col_name mapped to their values).\r \r CAUTION: This function needs to be changed accordingly if the format of the query result has changed.\r \r :param travel_query_result the raw query result from database\r :param columns the column names to be included in the query result\r :param street_info the street information of each segment\r :return: a list of dictionaries containing all the travel data\r \"\"\"\r travel_data_list=[]\r \r import ast\r for result in travel_query_result:\r str_data=result[0] \r str_data=str_data.replace('\"', '').replace(\"'\", \"\").replace(',', '\",\"').replace('(', '(\"').replace(')', '\")')\r \r raw_data=ast.literal_eval(str_data)\r parsed_data={}\r st_info_index=0\r \r for i in range(len(columns)):\r col_name=columns[i]\r col_spec=DB_TRAVEL_DATA_QUERY_RESULT_FORMAT[col_name]\r raw_data_type=col_spec[0]\r raw_data_i=col_spec[1]\r \r if raw_data_i < 0:\r parsed_data[col_name]=street_info[int(raw_data[0]) -1][st_info_index] st_info_index +=1\r continue\r \r curr_raw_data=raw_data[raw_data_i]\r \r if curr_raw_data=='' or len(curr_raw_data)==0 or curr_raw_data.isspace():\r parsed_data[col_name]='<no data>'\r else:\r try:\r value=raw_data_type(curr_raw_data)\r if raw_data_type==float:\r value=round(value, 2)\r except ValueError:\r abort(500, description=\"Database travel data query result does not match server expectations!\")\r return\r parsed_data[col_name]=value\r \r travel_data_list.append(parsed_data)\r \r return travel_data_list\r \r \r def parse_travel_request_body(travel_request_data):\r \"\"\"\r Parse the body of a travel data request(POST request body).\r \r Assumptions: The following fields should exist in request body: start_date, end_date, days_of_week,\r include_holidays, list_of_time_periods and list_of_links\r start_date and end_date are date strings in format %Y-%m-%d\r start_time and end_time are time strings in format %H:%M\r days_of_week should be a length-7 list of boolean values(0 is Monday, 6 is Sunday), representing\r which days to include.\r include_holidays should be a boolean value representing whether or not to include holidays\r list_of_time_periods should be a list of time period dictionaries.\r list_of_links should be a list containing all segments(each segment is a list of link_dirs)\r \r This function will call abort with response code 400 and error messages if any of the assumption is not met or if\r there is a format error.\r \r :param travel_request_data: The raw request body\r :return: a list containing all parameters to be passed into the database travel data aggregation function\r \"\"\"\r required=['list_of_time_periods', 'start_date', 'end_date', 'days_of_week', 'include_holidays', 'list_of_links']\r if False in[field in travel_request_data for field in required]:\r abort(400, description=\"Request body must contain list_of_time_periods start_date, end_date, \"\r \"days_of_week, include_holidays, list_of_links.\")\r return\r \r try:\r list_of_time_periods=list(travel_request_data['list_of_time_periods'])\r list_of_link_dirs=list(travel_request_data['list_of_links'])\r start_date=str(travel_request_data['start_date'])\r end_date=str(travel_request_data['end_date'])\r days_of_week=list(travel_request_data['days_of_week'])\r include_holidays=bool(travel_request_data['include_holidays'])\r except TypeError:\r abort(400, description=\"days_of_week, list_of_time_periods and list_of_link_dirs are lists! \"\r \"start_date and end_date are strings! include_holidays is a boolean!\")\r return\r \r time_period_query_param=_parse_trav_query_time_param(list_of_time_periods)\r segments_query_param=_parse_trav_query_segment_param(list_of_link_dirs)\r \r try:\r start_d=datetime.strptime(start_date, DATE_FORMAT)\r end_d=datetime.strptime(end_date, DATE_FORMAT)\r \r if start_d >=end_d:\r abort(400, description=\"start date must be earlier than end date\")\r return\r except ValueError:\r abort(400, description=(\r \"Start date and end date must follow date time format: %s\" % DATE_FORMAT))\r return\r \r dow_query_param=_parse_trav_query_dow_param(days_of_week)\r \r return[time_period_query_param, segments_query_param, start_date, end_date, dow_query_param, include_holidays]\r \r \r def _parse_trav_query_dow_param(dow_data: list):\r if len(dow_data) !=7:\r abort(400, description=\"days of week must be a list of length 7!\")\r return\r \r dow_converted=[str(i) for i in range(1, 8) if dow_data[i -1]]\r dow_str='{%s}' % \",\".join(dow_converted)\r return dow_str\r \r \r def _parse_trav_query_segment_param(segments: list):\r segments_query_strs=[]\r \r for i in range(len(segments)):\r try:\r segment_lst=list(segments[i])\r except TypeError:\r abort(400, description=\"Segment should be a list of link_dirs!\")\r return\r \r link_dirs_str='\\\\\"' +str(segment_lst).replace(' ', '').replace(\"'\", \"\") \\\r .replace('[', '{').replace(']', '}') +'\\\\\"'\r seg_str='\"(%d,%s)\"' %(i +1, link_dirs_str)\r segments_query_strs.append(seg_str)\r \r return '{%s}' % \",\".join(segments_query_strs)\r \r \r def _parse_trav_query_time_param(time_periods: list):\r time_periods_query_strs=[]\r \r for tp_data in time_periods:\r try:\r tp_dict=dict(tp_data)\r except TypeError:\r abort(400, description=\"Time period data should be a dictionary!\")\r return\r \r if 'name' not in tp_dict or 'start_time' not in tp_dict or 'end_time' not in tp_dict:\r abort(400, description=\"Each time period must have fields name, start_time and end_time!\")\r return\r \r period_name=str(tp_dict['name'])\r start_time=str(tp_dict['start_time'])\r end_time=str(tp_dict['end_time'])\r \r try:\r start_t=datetime.strptime(start_time, TIME_FORMAT)\r end_t=datetime.strptime(end_time, TIME_FORMAT)\r \r if start_t >=end_t:\r abort(400, description=\"start datetime must be earlier than end datetime\")\r return\r except ValueError:\r abort(400, description=(\r \"Start time and end time in time_periods must follow date time format: %s\" % TIME_FORMAT))\r return\r \r tp_str='\"(\\\\\"%s\\\\\",\\\\\"[%s:00,%s:00)\\\\\")\"' %(period_name, start_time, end_time)\r time_periods_query_strs.append(tp_str)\r \r return '{%s}' % \",\".join(time_periods_query_strs)\r \r \r def parse_link_response(link_data):\r \"\"\"\r Parse the given Link query result to a dictionary that can be jsonify-ed.\r The received the link_data should have all columns in Link model in a tuple.\r The link_data should contain the following fields(in order):\r link_dir(str), link_id(int), st_name(str), source(int), target(int),\r length(float), geometry(geom{type(str), coordinates(list)})\r \r :param link_data: the query result from the Link table in the database\r :return: a dictionary containing the attributes(except id)\r for the Link query that can be jsonify-ed.\r \"\"\"\r return{\"link_dir\": link_data[0], \"link_id\": link_data[1], \"st_name\": link_data[2],\r \"source\": link_data[3], \"target\": link_data[4], \"length\": link_data[5],\r \"geometry\": json.loads(link_data[6])}\r \r \r def parse_get_links_btwn_nodes_response(response: str):\r \"\"\"\r Converts the string result from database function get_links_btwn_nodes to a dictionary that can be jsonify-ed.\r This function will call abort with response code 400 if the geometry in the response is empty, i.e. no link exists\r between the given two nodes.\r \r :param response: the string response from database function get_links_btwn_nodes\r :return: a dictionary that can be jsonify-ed. It contains the following fields:\r source(int), target(int), path_name(str), link_dirs(list[str]), geometry(geom{type(str), coordinates(list)})\r \"\"\"\r try:\r wkb_str_split=response.rindex(',\"{\"')\r except ValueError:\r abort(400, description=\"There is no valid link between the two nodes provided!\")\r return\r \r wkb_str=response[wkb_str_split +1:-1].replace('\"\"', '\"').rstrip('\"').lstrip('\"')\r geom_json=json.loads(wkb_str)\r \r source_target_links_str=response[:wkb_str_split] +')'\r if source_target_links_str[-2] !='\"' and source_target_links_str[-2] !=\"'\":\r source_target_links_str=source_target_links_str.replace(',{', ',\"{')\r source_target_links_str=source_target_links_str.replace('})', '}\")')\r source_target_links_str=source_target_links_str.replace('},', '}\",')\r \r source_target_links_tuple=eval(source_target_links_str)\r \r path_name_str=source_target_links_tuple[2] path_name_str=path_name_str.replace('NULL,', '')\r path_name_str=path_name_str.replace(',NULL', '')\r path_name_str=path_name_str.replace('NULL', 'nameless road')\r path_name_str=path_name_str.replace('{', '[\"')\r path_name_str=path_name_str.replace('}', '\"]')\r path_name_str=path_name_str.replace(',', '\",\"')\r path_name_list=eval(path_name_str) unique_path_name_list=[]\r \r for p_name in path_name_list:\r if p_name not in unique_path_name_list:\r if len(unique_path_name_list) >=3:\r unique_path_name_list.append(\"...\")\r unique_path_name_list.append(path_name_list[-1])\r break\r unique_path_name_list.append(p_name)\r path_name=' -> '.join(unique_path_name_list)\r \r link_dirs_str=source_target_links_tuple[3] link_dirs_str=link_dirs_str.replace('{', '[\"')\r link_dirs_str=link_dirs_str.replace('}', '\"]')\r link_dirs_str=link_dirs_str.replace(',', '\",\"')\r link_dirs=eval(link_dirs_str)\r \r return{\"source\": source_target_links_tuple[0], \"target\": source_target_links_tuple[1],\r \"path_name\": path_name, \"link_dirs\": link_dirs, \"geometry\": geom_json}\r \r \r def parse_node_response(node_data: str):\r \"\"\"\r Parse the given Node query result to a dictionary that can be jsonify-ed.\r \r :param node_data: the query result from the Node table in the database\r :return: a tuple with first index the distance of the node and\r second index a dictionary containing the attributes for the Node query that can be jsonify-ed.\r \"\"\"\r import ast\r node_data=node_data.replace(',,', ',\"nameless road\",')\r \r first_comma=node_data.index(',')\r first_comma_quote=node_data.index(',\"')\r second_comma=node_data.index(',', first_comma +1)\r \r if first_comma !=first_comma_quote:\r node_data=node_data[:first_comma +1] +'\"' +node_data[first_comma +1:second_comma] +'\"' +node_data[\r second_comma:]\r \r data=ast.literal_eval(node_data)\r geom_raw=data[2] \r geom_raw=geom_raw.replace('type:', '\"type\":\"').replace(',coordinates:', '\",\"coordinates\":')\r \r return data[3],{'node_id': data[0], 'name': data[1], 'geometry': json.loads(geom_raw)}\r \r \r def get_path_list_from_link_list(links):\r \"\"\"\r Get a list of street names with no adjacent duplication from the list of links.\r :param links: the list of links to get street names from\r :return: a list of street names with no adjacent duplication\r \"\"\"\r st_names=[]\r last_st_idx=-1\r for i in range(len(links)):\r curr_name=links[i].get_st_name()\r \r if i==0 or curr_name !=st_names[last_st_idx]:\r st_names.append(curr_name)\r last_st_idx +=1\r return st_names\r ","sourceWithComments":"import json\r\nfrom datetime import datetime\r\n\r\nfrom flask import abort\r\n\r\nfrom app import ALLOWED_FILE_TYPES, TIME_FORMAT, DATE_FORMAT, DB_TRAVEL_DATA_QUERY_RESULT_FORMAT\r\n\r\n__all__ = ['parse_file_type_request_body', 'parse_travel_request_body', 'parse_link_response',\r\n           'parse_get_links_btwn_nodes_response', 'parse_node_response', 'parse_travel_data_query_result',\r\n           'parse_get_links_between_multi_nodes_request_body', 'get_path_list_from_link_list']\r\n\r\n\r\ndef parse_file_type_request_body(file_request_data):\r\n    \"\"\"\r\n    Parse the request body that contains a file type and wanted column names.\r\n    The file type should be specified in the request body JSON's field file_type.\r\n    The column names should be a list of strings.\r\n\r\n    If the file type specified in the request body is not an valid and allowed file type, the first file type defined\r\n    in the ALLOWED_FILE_TYPES is used by default (csv by default).\r\n\r\n    If column names is not given, all columns will be included in the data file.\r\n\r\n    Caution: If file type is invalid, this function will call abort with error code 400.\r\n            If an invalid column name was given, this function will call abort with error code 400.\r\n\r\n    :param file_request_data: the request body json\r\n    :return: A tuple of file info. First index is the first allowed file type (csv by default) if the file type\r\n            specified in the request body JSON is invalid or not allowed, or the specified file type otherwise.\r\n            Second index is a list of column names to be included.\r\n    \"\"\"\r\n\r\n    if 'file_type' in file_request_data:\r\n        file_type = file_request_data['file_type']\r\n\r\n        if file_type not in ALLOWED_FILE_TYPES:\r\n            abort(400, description=\"Invalid file type %s! Allowed types: %s\" % (file_type, ALLOWED_FILE_TYPES))\r\n            return\r\n    else:\r\n        file_type = ALLOWED_FILE_TYPES[0]\r\n\r\n    if 'columns' in file_request_data and type(file_request_data['columns']) == list:\r\n        columns = file_request_data['columns']  # type: list\r\n\r\n        if False in [col in DB_TRAVEL_DATA_QUERY_RESULT_FORMAT for col in columns]:\r\n            abort(400, description=\"Column name invalid! Legal values are: %s\" % str(\r\n                list(DB_TRAVEL_DATA_QUERY_RESULT_FORMAT.keys())))\r\n            return\r\n\r\n        columns.insert(0, 'to_street')\r\n        columns.insert(0, 'from_street')\r\n        columns.insert(0, 'street')\r\n        columns.insert(0, 'period')\r\n        columns.insert(0, 'id')\r\n    else:\r\n        columns = list(DB_TRAVEL_DATA_QUERY_RESULT_FORMAT)\r\n\r\n    return file_type, columns\r\n\r\n\r\ndef parse_get_links_between_multi_nodes_request_body(nodes_data):\r\n    \"\"\"\r\n    Parse the request body that contains a list of node_ids.\r\n    This function will call abort with response code 400 and error messages if any of the node_id is not an integer, or\r\n    if field 'node_ids' does not exist in the request body, or if field 'node_ids' is not a list with minimum length 2.\r\n\r\n    :param nodes_data: the body of the get_links_between_multi_nodes request\r\n    :return: a list of integer node ids\r\n    \"\"\"\r\n    try:\r\n        if 'node_ids' not in nodes_data:\r\n            abort(400, description=\"Must provide a list of node_ids\")\r\n            return\r\n    except TypeError:\r\n        abort(400,\r\n              description=\"Request body of get_links_between_multi_nodes must be a JSON containing field 'node_ids'\")\r\n        return\r\n\r\n    node_id_list = nodes_data['node_ids']\r\n    if type(node_id_list) != list or len(node_id_list) < 2:\r\n        abort(400, description=\"Field 'node_ids' must be a list of at least 2 node ids.\")\r\n        return\r\n\r\n    node_ids = []\r\n    for node_id in node_id_list:\r\n        try:\r\n            node_id_int = int(node_id)\r\n        except ArithmeticError:\r\n            abort(400, description='node_id must be an integer.')\r\n            return\r\n\r\n        node_ids.append(node_id_int)\r\n\r\n    return node_ids\r\n\r\n\r\ndef parse_travel_data_query_result(travel_query_result, columns, street_info):\r\n    \"\"\"\r\n    Parse the travel data query result into python dictionaries (col_name mapped to their values).\r\n\r\n    CAUTION: This function needs to be changed accordingly if the format of the query result has changed.\r\n\r\n    :param travel_query_result the raw query result from database\r\n    :param columns the column names to be included in the query result\r\n    :param street_info the street information of each segment\r\n    :return: a list of dictionaries containing all the travel data\r\n    \"\"\"\r\n    travel_data_list = []\r\n\r\n    import ast\r\n    for result in travel_query_result:\r\n        str_data = result[0]  # type: str\r\n\r\n        str_data = str_data.replace('\"', '').replace(\"'\", \"\").replace(',', '\",\"').replace('(', '(\"').replace(')', '\")')\r\n\r\n        raw_data = ast.literal_eval(str_data)\r\n        parsed_data = {}\r\n        st_info_index = 0\r\n\r\n        for i in range(len(columns)):\r\n            col_name = columns[i]\r\n            col_spec = DB_TRAVEL_DATA_QUERY_RESULT_FORMAT[col_name]\r\n            raw_data_type = col_spec[0]\r\n            raw_data_i = col_spec[1]\r\n\r\n            if raw_data_i < 0:\r\n                parsed_data[col_name] = street_info[int(raw_data[0]) - 1][st_info_index]  # segment id\r\n                st_info_index += 1\r\n                continue\r\n\r\n            curr_raw_data = raw_data[raw_data_i]\r\n\r\n            if curr_raw_data == '' or len(curr_raw_data) == 0 or curr_raw_data.isspace():\r\n                parsed_data[col_name] = '<no data>'\r\n            else:\r\n                try:\r\n                    value = raw_data_type(curr_raw_data)\r\n                    if raw_data_type == float:\r\n                        value = round(value, 2)\r\n                except ValueError:\r\n                    abort(500, description=\"Database travel data query result does not match server expectations!\")\r\n                    return\r\n                parsed_data[col_name] = value\r\n\r\n        travel_data_list.append(parsed_data)\r\n\r\n    return travel_data_list\r\n\r\n\r\ndef parse_travel_request_body(travel_request_data):\r\n    \"\"\"\r\n    Parse the body of a travel data request (POST request body).\r\n\r\n    Assumptions: The following fields should exist in request body: start_date, end_date, days_of_week,\r\n                    include_holidays, list_of_time_periods and list_of_links\r\n                    start_date and end_date are date strings in format %Y-%m-%d\r\n                    start_time and end_time are time strings in format %H:%M\r\n                    days_of_week should be a length-7 list of boolean values (0 is Monday, 6 is Sunday), representing\r\n                    which days to include.\r\n                    include_holidays should be a boolean value representing whether or not to include holidays\r\n                    list_of_time_periods should be a list of time period dictionaries.\r\n                    list_of_links should be a list containing all segments (each segment is a list of link_dirs)\r\n\r\n    This function will call abort with response code 400 and error messages if any of the assumption is not met or if\r\n    there is a format error.\r\n\r\n    :param travel_request_data: The raw request body\r\n    :return: a list containing all parameters to be passed into the database travel data aggregation function\r\n    \"\"\"\r\n    # ensures existence of required fields\r\n    required = ['list_of_time_periods', 'start_date', 'end_date', 'days_of_week', 'include_holidays', 'list_of_links']\r\n    if False in [field in travel_request_data for field in required]:\r\n        abort(400, description=\"Request body must contain list_of_time_periods start_date, end_date, \"\r\n                               \"days_of_week, include_holidays, list_of_links.\")\r\n        return\r\n\r\n    try:\r\n        list_of_time_periods = list(travel_request_data['list_of_time_periods'])\r\n        list_of_link_dirs = list(travel_request_data['list_of_links'])\r\n        start_date = str(travel_request_data['start_date'])\r\n        end_date = str(travel_request_data['end_date'])\r\n        days_of_week = list(travel_request_data['days_of_week'])\r\n        include_holidays = bool(travel_request_data['include_holidays'])\r\n    except TypeError:\r\n        abort(400, description=\"days_of_week, list_of_time_periods and list_of_link_dirs are lists! \"\r\n                               \"start_date and end_date are strings! include_holidays is a boolean!\")\r\n        return\r\n\r\n    time_period_query_param = _parse_trav_query_time_param(list_of_time_periods)\r\n    segments_query_param = _parse_trav_query_segment_param(list_of_link_dirs)\r\n\r\n    try:\r\n        start_d = datetime.strptime(start_date, DATE_FORMAT)\r\n        end_d = datetime.strptime(end_date, DATE_FORMAT)\r\n\r\n        if start_d >= end_d:\r\n            abort(400, description=\"start date must be earlier than end date\")\r\n            return\r\n    except ValueError:\r\n        abort(400, description=(\r\n                \"Start date and end date must follow date time format: %s\" % DATE_FORMAT))\r\n        return\r\n\r\n    dow_query_param = _parse_trav_query_dow_param(days_of_week)\r\n\r\n    return [time_period_query_param, segments_query_param, start_date, end_date, dow_query_param, include_holidays]\r\n\r\n\r\ndef _parse_trav_query_dow_param(dow_data: list):\r\n    if len(dow_data) != 7:\r\n        abort(400, description=\"days of week must be a list of length 7!\")\r\n        return\r\n\r\n    dow_converted = [str(i) for i in range(1, 8) if dow_data[i - 1]]\r\n    dow_str = '{%s}' % \",\".join(dow_converted)\r\n    return dow_str\r\n\r\n\r\ndef _parse_trav_query_segment_param(segments: list):\r\n    segments_query_strs = []\r\n\r\n    for i in range(len(segments)):\r\n        try:\r\n            segment_lst = list(segments[i])\r\n        except TypeError:\r\n            abort(400, description=\"Segment should be a list of link_dirs!\")\r\n            return\r\n\r\n        link_dirs_str = '\\\\\"' + str(segment_lst).replace(' ', '').replace(\"'\", \"\") \\\r\n            .replace('[', '{').replace(']', '}') + '\\\\\"'\r\n        seg_str = '\"(%d,%s)\"' % (i + 1, link_dirs_str)\r\n        segments_query_strs.append(seg_str)\r\n\r\n    return '{%s}' % \",\".join(segments_query_strs)\r\n\r\n\r\ndef _parse_trav_query_time_param(time_periods: list):\r\n    time_periods_query_strs = []\r\n\r\n    for tp_data in time_periods:\r\n        try:\r\n            tp_dict = dict(tp_data)\r\n        except TypeError:\r\n            abort(400, description=\"Time period data should be a dictionary!\")\r\n            return\r\n\r\n        if 'name' not in tp_dict or 'start_time' not in tp_dict or 'end_time' not in tp_dict:\r\n            abort(400, description=\"Each time period must have fields name, start_time and end_time!\")\r\n            return\r\n\r\n        period_name = str(tp_dict['name'])\r\n        start_time = str(tp_dict['start_time'])\r\n        end_time = str(tp_dict['end_time'])\r\n\r\n        try:\r\n            start_t = datetime.strptime(start_time, TIME_FORMAT)\r\n            end_t = datetime.strptime(end_time, TIME_FORMAT)\r\n\r\n            if start_t >= end_t:\r\n                abort(400, description=\"start datetime must be earlier than end datetime\")\r\n                return\r\n        except ValueError:\r\n            abort(400, description=(\r\n                    \"Start time and end time in time_periods must follow date time format: %s\" % TIME_FORMAT))\r\n            return\r\n\r\n        tp_str = '\"(\\\\\"%s\\\\\",\\\\\"[%s:00,%s:00)\\\\\")\"' % (period_name, start_time, end_time)\r\n        time_periods_query_strs.append(tp_str)\r\n\r\n    return '{%s}' % \",\".join(time_periods_query_strs)\r\n\r\n\r\ndef parse_link_response(link_data):\r\n    \"\"\"\r\n    Parse the given Link query result to a dictionary that can be jsonify-ed.\r\n    The received the link_data should have all columns in Link model in a tuple.\r\n    The link_data should contain the following fields (in order):\r\n        link_dir(str), link_id(int), st_name(str), source(int), target(int),\r\n        length(float), geometry(geom{type(str), coordinates(list)})\r\n\r\n    :param link_data: the query result from the Link table in the database\r\n    :return: a dictionary containing the attributes (except id)\r\n            for the Link query that can be jsonify-ed.\r\n    \"\"\"\r\n    return {\"link_dir\": link_data[0], \"link_id\": link_data[1], \"st_name\": link_data[2],\r\n            \"source\": link_data[3], \"target\": link_data[4], \"length\": link_data[5],\r\n            \"geometry\": json.loads(link_data[6])}\r\n\r\n\r\ndef parse_get_links_btwn_nodes_response(response: str):\r\n    \"\"\"\r\n    Converts the string result from database function get_links_btwn_nodes to a dictionary that can be jsonify-ed.\r\n    This function will call abort with response code 400 if the geometry in the response is empty, i.e. no link exists\r\n    between the given two nodes.\r\n\r\n    :param response: the string response from database function get_links_btwn_nodes\r\n    :return: a dictionary that can be jsonify-ed. It contains the following fields:\r\n            source(int), target(int), path_name(str), link_dirs(list[str]), geometry(geom{type(str), coordinates(list)})\r\n    \"\"\"\r\n    # split the response string by the last comma, which splits source, target, link_dirs from geometry\r\n    try:\r\n        wkb_str_split = response.rindex(',\"{\"')\r\n    except ValueError:\r\n        abort(400, description=\"There is no valid link between the two nodes provided!\")\r\n        return\r\n\r\n    wkb_str = response[wkb_str_split + 1:-1].replace('\"\"', '\"').rstrip('\"').lstrip('\"')\r\n    geom_json = json.loads(wkb_str)\r\n\r\n    source_target_links_str = response[:wkb_str_split] + ')'\r\n    # if there is only one link between nodes, need to add double quotes to enforce same formatting as multi-link\r\n    if source_target_links_str[-2] != '\"' and source_target_links_str[-2] != \"'\":\r\n        source_target_links_str = source_target_links_str.replace(',{', ',\"{')\r\n        source_target_links_str = source_target_links_str.replace('})', '}\")')\r\n        source_target_links_str = source_target_links_str.replace('},', '}\",')\r\n\r\n    # cast the curly bracket surrounded raw link_dir list to a square bracket surrounds quoted link_dir list\r\n    # so that the link_dirs can be casted to a string list\r\n    source_target_links_tuple = eval(source_target_links_str)\r\n\r\n    path_name_str = source_target_links_tuple[2]  # type: str\r\n    path_name_str = path_name_str.replace('NULL,', '')\r\n    path_name_str = path_name_str.replace(',NULL', '')\r\n    path_name_str = path_name_str.replace('NULL', 'nameless road')\r\n    path_name_str = path_name_str.replace('{', '[\"')\r\n    path_name_str = path_name_str.replace('}', '\"]')\r\n    path_name_str = path_name_str.replace(',', '\",\"')\r\n    path_name_list = eval(path_name_str)  # type: list\r\n    unique_path_name_list = []\r\n\r\n    for p_name in path_name_list:\r\n        if p_name not in unique_path_name_list:\r\n            if len(unique_path_name_list) >= 3:\r\n                unique_path_name_list.append(\"...\")\r\n                unique_path_name_list.append(path_name_list[-1])\r\n                break\r\n            unique_path_name_list.append(p_name)\r\n    path_name = ' -> '.join(unique_path_name_list)\r\n\r\n    link_dirs_str = source_target_links_tuple[3]  # type: str\r\n    link_dirs_str = link_dirs_str.replace('{', '[\"')\r\n    link_dirs_str = link_dirs_str.replace('}', '\"]')\r\n    link_dirs_str = link_dirs_str.replace(',', '\",\"')\r\n    link_dirs = eval(link_dirs_str)\r\n\r\n    return {\"source\": source_target_links_tuple[0], \"target\": source_target_links_tuple[1],\r\n            \"path_name\": path_name, \"link_dirs\": link_dirs, \"geometry\": geom_json}\r\n\r\n\r\ndef parse_node_response(node_data: str):\r\n    \"\"\"\r\n    Parse the given Node query result to a dictionary that can be jsonify-ed.\r\n\r\n    :param node_data: the query result from the Node table in the database\r\n    :return: a tuple with first index the distance of the node and\r\n            second index a dictionary containing the attributes for the Node query that can be jsonify-ed.\r\n    \"\"\"\r\n    import ast\r\n    node_data = node_data.replace(',,', ',\"nameless road\",')\r\n\r\n    # sometimes intersec name is not quote surrounded\r\n    first_comma = node_data.index(',')\r\n    first_comma_quote = node_data.index(',\"')\r\n    second_comma = node_data.index(',', first_comma + 1)\r\n\r\n    if first_comma != first_comma_quote:\r\n        node_data = node_data[:first_comma + 1] + '\"' + node_data[first_comma + 1:second_comma] + '\"' + node_data[\r\n                                                                                                        second_comma:]\r\n\r\n    data = ast.literal_eval(node_data)\r\n    geom_raw = data[2]  # type: str\r\n\r\n    geom_raw = geom_raw.replace('type:', '\"type\":\"').replace(',coordinates:', '\",\"coordinates\":')\r\n\r\n    return data[3], {'node_id': data[0], 'name': data[1], 'geometry': json.loads(geom_raw)}\r\n\r\n\r\ndef get_path_list_from_link_list(links):\r\n    \"\"\"\r\n    Get a list of street names with no adjacent duplication from the list of links.\r\n    :param links: the list of links to get street names from\r\n    :return: a list of street names with no adjacent duplication\r\n    \"\"\"\r\n    st_names = []\r\n    last_st_idx = -1\r\n    for i in range(len(links)):\r\n        curr_name = links[i].get_st_name()\r\n\r\n        if i == 0 or curr_name != st_names[last_st_idx]:\r\n            st_names.append(curr_name)\r\n            last_st_idx += 1\r\n    return st_names\r\n"},"\/backend\/app\/routes.py":{"changes":[{"diff":"\n @app.route('\/closest-node\/<longitude>\/<latitude>', methods=['GET'])\r\n def get_closest_node(longitude, latitude):\r\n     \"\"\"\r\n-    Get the closest nodes to the given longitude and latitude.\r\n-    This function uses database function get_closest_nodes to fetch series of closest nodes to the given\r\n-    longitude and latitude, sorted by ascending distance order.\r\n-    Only points with distance less than 5 are returned by this function.\r\n+    This function fetches a set of closest nodes to the given\r\n+    point, sorted by ascending distance order.\r\n \r\n     :param longitude: the longitude of the origin point\r\n     :param latitude: the latitude of the origin point\r\n-    :return: JSON of an array containing the satisfying nodes.\r\n+    :return: JSON of an array containing the closest nodes.\r\n             The array is sorted in ascending distance order. node object keys: node_id(int),\r\n             geometry(geom{type(str), coordinates(list[int])}), name(str)\r\n     \"\"\"\r\n+    connection = connect(\r\n+        host = os.environ['DB_HOST'],\r\n+        dbname = os.environ['DB_NAME'],\r\n+        user = os.environ['DB_USER'],\r\n+        password = os.environ['DB_USER_PASSWORD'],\r\n+    )\r\n+\r\n     try:\r\n         longitude = float(longitude)\r\n         latitude = float(latitude)\r\n     except ValueError or ArithmeticError:\r\n         abort(400, description=\"Longitude and latitude must be decimal numbers!\")\r\n         return\r\n-\r\n-    nodes_ascend_dist_order_query_result = db.session.query(func.get_closest_nodes(longitude, latitude))\r\n+    \r\n+    with connection:\r\n+        with connection.cursor() as cursor:\r\n+            select_sql = '''\r\n+                WITH distances AS (\r\n+                    SELECT \r\n+                        node_id,\r\n+                        st_distance(\r\n+                            st_transform(geom, 2952),\r\n+                            st_transform(\r\n+                                st_setsrid(st_makepoint(%(longitude)s, %(latitude)s), 4326),\r\n+                                2952\r\n+                            )\r\n+                        ) AS distance\r\n+                    FROM here.routing_nodes_intersec_name\r\n+                )\r\n+                SELECT \r\n+                    here_nodes.node_id::int,\r\n+                    intersec_name,\r\n+                    st_asgeojson(geom),\r\n+                    distance\r\n+                FROM here.routing_nodes_intersec_name AS here_nodes\r\n+                JOIN distances ON here_nodes.node_id = distances.node_id\r\n+                ORDER BY distance\r\n+                LIMIT 10'''\r\n+            cursor.execute(select_sql, {\"latitude\": latitude, \"longitude\": longitude})\r\n+            nodes_ascend_dist_order_query_result = cursor.fetchall()\r\n \r\n     candidate_nodes = []\r\n     node_count = 0\r\n-    for node_query_result in nodes_ascend_dist_order_query_result:\r\n-        node_data = parse_node_response(node_query_result[0])\r\n-        node_dist = node_data[0]\r\n-        node_json = node_data[1]\r\n-\r\n-        if node_count == 0 or node_dist < 10:\r\n-            candidate_nodes.append(node_json)\r\n+    for node_id, stname, coord_dict, distance in nodes_ascend_dist_order_query_result:\r\n+        if node_count == 0 or distance < 10:\r\n+            candidate_nodes.append( {\r\n+                 'node_id': node_id,\r\n+                 'name': stname,\r\n+                 'geometry': json.loads(coord_dict)\r\n+            } )\r\n         else:\r\n             break\r\n-\r\n         node_count += 1\r\n-\r\n+    connection.close()\r\n     return jsonify(candidate_nodes)\r\n \r\n \r\n","add":45,"remove":16,"filename":"\/backend\/app\/routes.py","badparts":["    Get the closest nodes to the given longitude and latitude.\r","    This function uses database function get_closest_nodes to fetch series of closest nodes to the given\r","    longitude and latitude, sorted by ascending distance order.\r","    Only points with distance less than 5 are returned by this function.\r","    :return: JSON of an array containing the satisfying nodes.\r","\r","    nodes_ascend_dist_order_query_result = db.session.query(func.get_closest_nodes(longitude, latitude))\r","    for node_query_result in nodes_ascend_dist_order_query_result:\r","        node_data = parse_node_response(node_query_result[0])\r","        node_dist = node_data[0]\r","        node_json = node_data[1]\r","\r","        if node_count == 0 or node_dist < 10:\r","            candidate_nodes.append(node_json)\r","\r","\r"],"goodparts":["    This function fetches a set of closest nodes to the given\r","    point, sorted by ascending distance order.\r","    :return: JSON of an array containing the closest nodes.\r","    connection = connect(\r","        host = os.environ['DB_HOST'],\r","        dbname = os.environ['DB_NAME'],\r","        user = os.environ['DB_USER'],\r","        password = os.environ['DB_USER_PASSWORD'],\r","    )\r","\r","    \r","    with connection:\r","        with connection.cursor() as cursor:\r","            select_sql = '''\r","                WITH distances AS (\r","                    SELECT \r","                        node_id,\r","                        st_distance(\r","                            st_transform(geom, 2952),\r","                            st_transform(\r","                                st_setsrid(st_makepoint(%(longitude)s, %(latitude)s), 4326),\r","                                2952\r","                            )\r","                        ) AS distance\r","                    FROM here.routing_nodes_intersec_name\r","                )\r","                SELECT \r","                    here_nodes.node_id::int,\r","                    intersec_name,\r","                    st_asgeojson(geom),\r","                    distance\r","                FROM here.routing_nodes_intersec_name AS here_nodes\r","                JOIN distances ON here_nodes.node_id = distances.node_id\r","                ORDER BY distance\r","                LIMIT 10'''\r","            cursor.execute(select_sql, {\"latitude\": latitude, \"longitude\": longitude})\r","            nodes_ascend_dist_order_query_result = cursor.fetchall()\r","    for node_id, stname, coord_dict, distance in nodes_ascend_dist_order_query_result:\r","        if node_count == 0 or distance < 10:\r","            candidate_nodes.append( {\r","                 'node_id': node_id,\r","                 'name': stname,\r","                 'geometry': json.loads(coord_dict)\r","            } )\r","    connection.close()\r"]}],"source":"\nimport os\r \r from flask import abort, jsonify, request, send_file\r from sqlalchemy import func\r \r from app import app, db\r from app.file_util import make_travel_data_csv, make_travel_data_xlsx\r from app.models import Link, Node\r from app.parse_util import *\r \r \r def _need_keep_temp_file():\r \"\"\"Check environ whether or not to keep the temporary files created.\"\"\"\r if 'KEEP_TEMP_FILE' not in os.environ:\r return False\r return os.environ['KEEP_TEMP_FILE']=='true'\r \r \r @app.errorhandler(400)\r def request_error(e):\r \"\"\"parse flask's default abort HTML into a JSON object containing the error message\"\"\"\r return jsonify(error=e.description), 400\r \r \r @app.errorhandler(501)\r def not_implemented_error(e):\r \"\"\"parse flask's default abort HTML into a JSON object containing the error message\"\"\"\r return jsonify(error=e.description), 501\r \r \r @app.route('\/')\r def index():\r return \"Data Filter Web Application\"\r \r \r @app.route('\/closest-node\/<longitude>\/<latitude>', methods=['GET'])\r def get_closest_node(longitude, latitude):\r \"\"\"\r Get the closest nodes to the given longitude and latitude.\r This function uses database function get_closest_nodes to fetch series of closest nodes to the given\r longitude and latitude, sorted by ascending distance order.\r Only points with distance less than 5 are returned by this function.\r \r :param longitude: the longitude of the origin point\r :param latitude: the latitude of the origin point\r :return: JSON of an array containing the satisfying nodes.\r The array is sorted in ascending distance order. node object keys: node_id(int),\r geometry(geom{type(str), coordinates(list[int])}), name(str)\r \"\"\"\r try:\r longitude=float(longitude)\r latitude=float(latitude)\r except ValueError or ArithmeticError:\r abort(400, description=\"Longitude and latitude must be decimal numbers!\")\r return\r \r nodes_ascend_dist_order_query_result=db.session.query(func.get_closest_nodes(longitude, latitude))\r \r candidate_nodes=[]\r node_count=0\r for node_query_result in nodes_ascend_dist_order_query_result:\r node_data=parse_node_response(node_query_result[0])\r node_dist=node_data[0]\r node_json=node_data[1]\r \r if node_count==0 or node_dist < 10:\r candidate_nodes.append(node_json)\r else:\r break\r \r node_count +=1\r \r return jsonify(candidate_nodes)\r \r \r @app.route('\/link-nodes\/<from_node_id>\/<to_node_id>', methods=['GET'])\r def get_links_between_two_nodes(from_node_id, to_node_id):\r \"\"\"\r Get the shortest length link between the two given nodes.\r This function filters links using ST_Intersects and sort them using the\r length attribute of the link object.\r This function will call abort with response code 400 when the given node_ids\r can not be cast to an integer, the two nodes given are the same or no link exists between the two nodes.\r \r :param from_node_id: source node id\r :param to_node_id: target node id\r :return: JSON representing a link object, which is the shortest link between\r the two points. Link object keys: link_dir(str), link_id(int), st_name(str),\r source(int), target(int), length(float),\r geometry(geom{type(str), coordinates(list[int])})\r \"\"\"\r try:\r from_node_id=int(from_node_id)\r to_node_id=int(to_node_id)\r except ValueError or ArithmeticError:\r abort(400, description=\"The node_ids should be integers!\")\r return\r \r if from_node_id==to_node_id:\r abort(400, description=\"Source node can not be the same as target node.\")\r return\r \r shortest_link_query_result=db.session.query(func.get_links_btwn_nodes(from_node_id, to_node_id)).first()[0]\r shortest_link_data=parse_get_links_btwn_nodes_response(shortest_link_query_result)\r return jsonify(shortest_link_data)\r \r \r @app.route('\/link-nodes', methods=['POST'])\r def get_links_between_multi_nodes():\r \"\"\"\r Get the shortest length link connecting the given nodes in order.\r This function filters links using ST_Intersects and sort them using the\r length attribute of the link object.\r If any two consecutive nodes in the list are the same, they are skipped.\r This function will call abort with response code 400 when the given node_ids can not be cast to an integer\r or no link exists between the two nodes.\r \r :return: JSON representing an array of link objects, which are the shortest links connecting given points.\r Link object keys: link_dir(str), link_id(int), st_name(str),\r source(int), target(int), length(float),\r geometry(geom{type(str), coordinates(list[int])})\r \"\"\"\r node_ids=parse_get_links_between_multi_nodes_request_body(request.json)\r optimal_links_data_list=[]\r \r for i in range(len(node_ids) -1):\r curr_node_id=node_ids[i]\r next_node_id=node_ids[i +1]\r \r if curr_node_id==next_node_id:\r continue\r \r shortest_link_query_result=db.session.query(func.get_links_btwn_nodes(curr_node_id, next_node_id)).first()[0]\r shortest_link_data=parse_get_links_btwn_nodes_response(shortest_link_query_result)\r optimal_links_data_list.append(shortest_link_data)\r return jsonify(optimal_links_data_list)\r \r \r @app.route('\/travel-data-file', methods=['POST'])\r def get_links_travel_data_file():\r \"\"\"\r Get the travel data file from start_time to end_time for all links in link_dirs.\r \r Caution: This function may take a long time if start_time -end_time is a long period of time, or link_dirs contains\r too many links.(1~2min)\r \r Assumptions: start_time, end_time are in res.json, and are formatted using DATE_TIME_FORMAT(%Y-%m-%d %H:%M:%S).\r link_dirs is in res.json, and is a list containing valid link_dir entries(string).\r file_type is in res.json, and is 'csv', 'xlsx' or 'shapefile'\r This function will be aborted if any of the assumption is not met.\r \r :return: a file containing requested travel data\r \"\"\"\r file_type, columns=parse_file_type_request_body(request.json)\r trav_data_query_params=parse_travel_request_body(request.json)\r street_info=_get_street_info(request.json['list_of_links']) trav_data_query_result=db.session.query(func.fetch_trav_data_wrapper(*trav_data_query_params)).all()\r travel_data_list=parse_travel_data_query_result(trav_data_query_result, columns, street_info)\r \r if file_type=='csv':\r data_file_path=make_travel_data_csv(travel_data_list, columns)\r mime_type=\"text\/csv\"\r elif file_type=='xlsx':\r data_file_path=make_travel_data_xlsx(travel_data_list, columns)\r mime_type=\"application\/vnd.openxmlformats-officedocument.spreadsheetml.sheet\"\r else:\r abort(501, description=\"Currently only support csv and xlsx files.\")\r return\r \r file_response=send_file(data_file_path, mimetype=mime_type)\r if not _need_keep_temp_file():\r os.remove(data_file_path)\r return file_response\r \r \r @app.route('\/date-bounds', methods=['GET'])\r def get_date_bounds():\r \"\"\"\r Get the earliest timestamp and latest timestamp in the travel database.\r The timestamps are formatted by DATE_TIME_FORMAT(\"%Y-%m-%d %H:%M\").\r \r :return: JSON containing two fields: start_time and end_time\r \"\"\"\r from app import FULL_DATE_TIME_FORMAT, DB_START_DATE, TIMEZONE\r from datetime import datetime, timedelta\r \r current_time=TIMEZONE.localize(datetime.now()) \r today_update_time=current_time.replace(hour=17, minute=30, second=00)\r \r if current_time >=today_update_time:\r end_time=(current_time -timedelta(days=2)).replace(hour=23, minute=59, second=59)\r else:\r end_time=(current_time -timedelta(days=3)).replace(hour=23, minute=59, second=59)\r \r return{\"start_time\": DB_START_DATE.strftime(FULL_DATE_TIME_FORMAT),\r \"end_time\": end_time.strftime(FULL_DATE_TIME_FORMAT)}\r \r \r def _calc_list_avg(lst: list) -> float:\r if len(lst)==0:\r return 0.0\r return sum(lst) \/ len(lst)\r \r \r def _round_up(num: float):\r result=int(num)\r if num -result > 0:\r result +=1\r return result\r \r \r def _get_street_info(list_of_link_dirs):\r street_info={}\r \r for i in range(len(list_of_link_dirs)):\r link_dirs=list_of_link_dirs[i]\r \r start_link=Link.query.filter_by(link_dir=link_dirs[0]).first()\r end_link=Link.query.filter_by(link_dir=link_dirs[-1]).first()\r \r start_node=Node.query.filter_by(node_id=int(start_link.source)).first()\r end_node=Node.query.filter_by(node_id=int(end_link.target)).first()\r \r start_node_name=str(start_node.intersec_name)\r end_node_name=str(end_node.intersec_name)\r \r start_names=start_node_name.split(\" & \")\r end_names=end_node_name.split(\" & \")\r \r intersections=[]\r for s_name in start_names:\r if s_name in end_names:\r intersections.append(s_name)\r \r if len(intersections) > 0:\r for intersec in intersections:\r start_names.remove(intersec)\r end_names.remove(intersec)\r \r intersection=\" & \".join(intersections)\r \r if len(start_names) > 0:\r from_street=\" & \".join(start_names)\r else:\r from_street=\" & \".join(intersections)\r \r if len(end_names) > 0:\r to_street=\" & \".join(end_names)\r else:\r to_street=\" & \".join(intersections)\r else:\r intersection=\"<multiple streets>\"\r from_street=start_node_name\r to_street=end_node_name\r \r street_info[i]=(intersection, from_street, to_street)\r \r return street_info\r ","sourceWithComments":"import os\r\n\r\nfrom flask import abort, jsonify, request, send_file\r\nfrom sqlalchemy import func\r\n\r\nfrom app import app, db\r\nfrom app.file_util import make_travel_data_csv, make_travel_data_xlsx\r\nfrom app.models import Link, Node\r\nfrom app.parse_util import *\r\n\r\n\r\ndef _need_keep_temp_file():\r\n    \"\"\"Check environ whether or not to keep the temporary files created.\"\"\"\r\n    if 'KEEP_TEMP_FILE' not in os.environ:\r\n        return False\r\n    return os.environ['KEEP_TEMP_FILE'] == 'true'\r\n\r\n\r\n@app.errorhandler(400)\r\ndef request_error(e):\r\n    \"\"\"parse flask's default abort HTML into a JSON object containing the error message\"\"\"\r\n    return jsonify(error=e.description), 400\r\n\r\n\r\n@app.errorhandler(501)\r\ndef not_implemented_error(e):\r\n    \"\"\"parse flask's default abort HTML into a JSON object containing the error message\"\"\"\r\n    return jsonify(error=e.description), 501\r\n\r\n\r\n@app.route('\/')\r\ndef index():\r\n    return \"Data Filter Web Application\"\r\n\r\n\r\n@app.route('\/closest-node\/<longitude>\/<latitude>', methods=['GET'])\r\ndef get_closest_node(longitude, latitude):\r\n    \"\"\"\r\n    Get the closest nodes to the given longitude and latitude.\r\n    This function uses database function get_closest_nodes to fetch series of closest nodes to the given\r\n    longitude and latitude, sorted by ascending distance order.\r\n    Only points with distance less than 5 are returned by this function.\r\n\r\n    :param longitude: the longitude of the origin point\r\n    :param latitude: the latitude of the origin point\r\n    :return: JSON of an array containing the satisfying nodes.\r\n            The array is sorted in ascending distance order. node object keys: node_id(int),\r\n            geometry(geom{type(str), coordinates(list[int])}), name(str)\r\n    \"\"\"\r\n    try:\r\n        longitude = float(longitude)\r\n        latitude = float(latitude)\r\n    except ValueError or ArithmeticError:\r\n        abort(400, description=\"Longitude and latitude must be decimal numbers!\")\r\n        return\r\n\r\n    nodes_ascend_dist_order_query_result = db.session.query(func.get_closest_nodes(longitude, latitude))\r\n\r\n    candidate_nodes = []\r\n    node_count = 0\r\n    for node_query_result in nodes_ascend_dist_order_query_result:\r\n        node_data = parse_node_response(node_query_result[0])\r\n        node_dist = node_data[0]\r\n        node_json = node_data[1]\r\n\r\n        if node_count == 0 or node_dist < 10:\r\n            candidate_nodes.append(node_json)\r\n        else:\r\n            break\r\n\r\n        node_count += 1\r\n\r\n    return jsonify(candidate_nodes)\r\n\r\n\r\n@app.route('\/link-nodes\/<from_node_id>\/<to_node_id>', methods=['GET'])\r\ndef get_links_between_two_nodes(from_node_id, to_node_id):\r\n    \"\"\"\r\n    Get the shortest length link between the two given nodes.\r\n    This function filters links using ST_Intersects and sort them using the\r\n    length attribute of the link object.\r\n    This function will call abort with response code 400 when the given node_ids\r\n    can not be cast to an integer, the two nodes given are the same or no link exists between the two nodes.\r\n\r\n    :param from_node_id: source node id\r\n    :param to_node_id: target node id\r\n    :return: JSON representing a link object, which is the shortest link between\r\n            the two points. Link object keys: link_dir(str), link_id(int), st_name(str),\r\n            source(int), target(int), length(float),\r\n            geometry(geom{type(str), coordinates(list[int])})\r\n    \"\"\"\r\n    try:\r\n        from_node_id = int(from_node_id)\r\n        to_node_id = int(to_node_id)\r\n    except ValueError or ArithmeticError:\r\n        abort(400, description=\"The node_ids should be integers!\")\r\n        return\r\n\r\n    if from_node_id == to_node_id:\r\n        abort(400, description=\"Source node can not be the same as target node.\")\r\n        return\r\n\r\n    shortest_link_query_result = db.session.query(func.get_links_btwn_nodes(from_node_id, to_node_id)).first()[0]\r\n    shortest_link_data = parse_get_links_btwn_nodes_response(shortest_link_query_result)\r\n    return jsonify(shortest_link_data)\r\n\r\n\r\n@app.route('\/link-nodes', methods=['POST'])\r\ndef get_links_between_multi_nodes():\r\n    \"\"\"\r\n    Get the shortest length link connecting the given nodes in order.\r\n    This function filters links using ST_Intersects and sort them using the\r\n    length attribute of the link object.\r\n    If any two consecutive nodes in the list are the same, they are skipped.\r\n    This function will call abort with response code 400 when the given node_ids can not be cast to an integer\r\n    or no link exists between the two nodes.\r\n\r\n    :return: JSON representing an array of link objects, which are the shortest links connecting given points.\r\n            Link object keys: link_dir(str), link_id(int), st_name(str),\r\n            source(int), target(int), length(float),\r\n            geometry(geom{type(str), coordinates(list[int])})\r\n    \"\"\"\r\n    node_ids = parse_get_links_between_multi_nodes_request_body(request.json)\r\n    optimal_links_data_list = []\r\n\r\n    for i in range(len(node_ids) - 1):\r\n        curr_node_id = node_ids[i]\r\n        next_node_id = node_ids[i + 1]\r\n\r\n        if curr_node_id == next_node_id:\r\n            continue\r\n\r\n        shortest_link_query_result = db.session.query(func.get_links_btwn_nodes(curr_node_id, next_node_id)).first()[0]\r\n        shortest_link_data = parse_get_links_btwn_nodes_response(shortest_link_query_result)\r\n        optimal_links_data_list.append(shortest_link_data)\r\n    return jsonify(optimal_links_data_list)\r\n\r\n\r\n@app.route('\/travel-data-file', methods=['POST'])\r\ndef get_links_travel_data_file():\r\n    \"\"\"\r\n    Get the travel data file from start_time to end_time for all links in link_dirs.\r\n\r\n    Caution: This function may take a long time if start_time - end_time is a long period of time, or link_dirs contains\r\n            too many links. (1~2min)\r\n\r\n    Assumptions: start_time, end_time are in res.json, and are formatted using DATE_TIME_FORMAT (%Y-%m-%d %H:%M:%S).\r\n                link_dirs is in res.json, and is a list containing valid link_dir entries (string).\r\n                file_type is in res.json, and is 'csv', 'xlsx' or 'shapefile'\r\n    This function will be aborted if any of the assumption is not met.\r\n\r\n    :return: a file containing requested travel data\r\n    \"\"\"\r\n    file_type, columns = parse_file_type_request_body(request.json)\r\n    trav_data_query_params = parse_travel_request_body(request.json)\r\n    street_info = _get_street_info(request.json['list_of_links'])  # this won't fail since last parse already checked\r\n    trav_data_query_result = db.session.query(func.fetch_trav_data_wrapper(*trav_data_query_params)).all()\r\n    travel_data_list = parse_travel_data_query_result(trav_data_query_result, columns, street_info)\r\n\r\n    if file_type == 'csv':\r\n        data_file_path = make_travel_data_csv(travel_data_list, columns)\r\n        mime_type = \"text\/csv\"\r\n    elif file_type == 'xlsx':\r\n        data_file_path = make_travel_data_xlsx(travel_data_list, columns)\r\n        mime_type = \"application\/vnd.openxmlformats-officedocument.spreadsheetml.sheet\"\r\n    else:\r\n        abort(501, description=\"Currently only support csv and xlsx files.\")\r\n        return\r\n\r\n    file_response = send_file(data_file_path, mimetype=mime_type)\r\n    if not _need_keep_temp_file():\r\n        os.remove(data_file_path)\r\n    return file_response\r\n\r\n\r\n@app.route('\/date-bounds', methods=['GET'])\r\ndef get_date_bounds():\r\n    \"\"\"\r\n    Get the earliest timestamp and latest timestamp in the travel database.\r\n    The timestamps are formatted by DATE_TIME_FORMAT (\"%Y-%m-%d %H:%M\").\r\n\r\n    :return: JSON containing two fields: start_time and end_time\r\n    \"\"\"\r\n    from app import FULL_DATE_TIME_FORMAT, DB_START_DATE, TIMEZONE\r\n    from datetime import datetime, timedelta\r\n\r\n    current_time = TIMEZONE.localize(datetime.now())  # type: datetime\r\n\r\n    today_update_time = current_time.replace(hour=17, minute=30, second=00)\r\n\r\n    if current_time >= today_update_time:\r\n        end_time = (current_time - timedelta(days=2)).replace(hour=23, minute=59, second=59)\r\n    else:\r\n        end_time = (current_time - timedelta(days=3)).replace(hour=23, minute=59, second=59)\r\n\r\n    return {\"start_time\": DB_START_DATE.strftime(FULL_DATE_TIME_FORMAT),\r\n            \"end_time\": end_time.strftime(FULL_DATE_TIME_FORMAT)}\r\n\r\n\r\ndef _calc_list_avg(lst: list) -> float:\r\n    if len(lst) == 0:\r\n        return 0.0\r\n    return sum(lst) \/ len(lst)\r\n\r\n\r\ndef _round_up(num: float):\r\n    result = int(num)\r\n    if num - result > 0:\r\n        result += 1\r\n    return result\r\n\r\n\r\ndef _get_street_info(list_of_link_dirs):\r\n    street_info = {}\r\n\r\n    for i in range(len(list_of_link_dirs)):\r\n        link_dirs = list_of_link_dirs[i]\r\n\r\n        start_link = Link.query.filter_by(link_dir=link_dirs[0]).first()\r\n        end_link = Link.query.filter_by(link_dir=link_dirs[-1]).first()\r\n\r\n        start_node = Node.query.filter_by(node_id=int(start_link.source)).first()\r\n        end_node = Node.query.filter_by(node_id=int(end_link.target)).first()\r\n\r\n        start_node_name = str(start_node.intersec_name)\r\n        end_node_name = str(end_node.intersec_name)\r\n\r\n        start_names = start_node_name.split(\" & \")\r\n        end_names = end_node_name.split(\" & \")\r\n\r\n        intersections = []\r\n        for s_name in start_names:\r\n            if s_name in end_names:\r\n                intersections.append(s_name)\r\n\r\n        if len(intersections) > 0:\r\n            for intersec in intersections:\r\n                start_names.remove(intersec)\r\n                end_names.remove(intersec)\r\n\r\n            intersection = \" & \".join(intersections)\r\n\r\n            if len(start_names) > 0:\r\n                from_street = \" & \".join(start_names)\r\n            else:\r\n                from_street = \" & \".join(intersections)\r\n\r\n            if len(end_names) > 0:\r\n                to_street = \" & \".join(end_names)\r\n            else:\r\n                to_street = \" & \".join(intersections)\r\n        else:\r\n            intersection = \"<multiple streets>\"\r\n            from_street = start_node_name\r\n            to_street = end_node_name\r\n\r\n        street_info[i] = (intersection, from_street, to_street)\r\n\r\n    return street_info\r\n"}},"msg":"move SQL inline: get_closest_node() (#26)\n\n* get_closest_node in psycopg2 + raw sql #23\r\n\r\n* fixes for PR #23\r\n\r\n* bugfixes #23\r\n\r\n* pull db config from .env\r\n\r\n* remove configparser\r\n\r\n* syntax #26\r\n\r\n* replaced util parse function + \"'\" bugfix #26\r\n\r\n* sql injection fix #26\r\n\r\n* undo changes to other functions\r\n\r\n* spacing\r\n\r\n* Removed excess code #26\r\n\r\n* remove explicit closing of cursor #26\r\n\r\n* sql injection fix #26\r\n\r\n* formatting, update comment\r\n\r\n* deleted excess code #26\r\n\r\n---------\r\n\r\nCo-authored-by: Nate Wessel <nate.wessel@toronto.ca>"}},"https:\/\/github.com\/bryanzhou008\/Hard_Label_Black_Box_Attack_GNN":{"de78c1c168857eb6143de7660db56fac7f4b3128":{"url":"https:\/\/api.github.com\/repos\/bryanzhou008\/Hard_Label_Black_Box_Attack_GNN\/commits\/de78c1c168857eb6143de7660db56fac7f4b3128","html_url":"https:\/\/github.com\/bryanzhou008\/Hard_Label_Black_Box_Attack_GNN\/commit\/de78c1c168857eb6143de7660db56fac7f4b3128","message":"update injection function","sha":"de78c1c168857eb6143de7660db56fac7f4b3128","keyword":"function injection update","diff":"diff --git a\/tang\/Sign_OPT.py b\/tang\/Sign_OPT.py\nindex 21bb47a..a765fd9 100644\n--- a\/tang\/Sign_OPT.py\n+++ b\/tang\/Sign_OPT.py\n@@ -121,49 +121,49 @@ def initial_search(self,x0, y0):\n         final_theta = torch.clamp(final_theta, 0.0, 0.5)\n         search_type = -1       \n         \n-        # #inner cluster perturbation\n-        # for i in range(num_cluster):\n-        #     nodes = cluster[i]\n-        #     num_cluster_nodes = len(nodes)\n-        #     if num_cluster_nodes > 1:\n-        #         for j in range(10*num_cluster_nodes): #search initial directions \n-        #             theta = torch.normal(mean=torch.rand(1).item(),std=0.5,size=(num_cluster_nodes,num_cluster_nodes)).to(self.device)\n-        #             theta = torch.triu(theta, diagonal=1).to(device)\n-        #             G_new = new_graph(G, theta, index1=nodes)\n-        #             x_new.edge_index = from_networkx(G_new).to(device).edge_index.long()\n-        #             if model.predict(x_new, device) != y0:  #we find a direction\n-        #                 F_lbd = distance(x_new, x0)\n-        #                 if F_lbd < F_theta:\n-        #                     F_theta = F_lbd\n-        #                     flag_inner = 1\n-        #                     search_type = 0\n-        #                     for p in range(num_cluster_nodes-1):\n-        #                         for q in range(p+1, num_cluster_nodes):\n-        #                             final_theta[nodes[p], nodes[q]] = theta[p, q]\n-        #                             final_theta[nodes[q], nodes[p]] = theta[p, q]   \n-        #             num_query += 1   \n-\n-        # #perturbations between clusters\n-        # if (num_cluster > 1) and (flag_inner == 0):\n-        #     for i in range(num_cluster - 1):\n-        #         for j in range(i+1, num_cluster):\n-        #             nodes1, nodes2 = cluster[i], cluster[j]\n-        #             num_cluster_nodes1, num_cluster_nodes2 = len(nodes1), len(nodes2)\n-        #             for k in range(10*(num_cluster_nodes1+num_cluster_nodes2)):\n-        #                 theta = torch.normal(mean=torch.rand(1).item(), std=0.5, size=(num_cluster_nodes1,num_cluster_nodes2)).to(self.device)\n-        #                 G_new = new_graph(G, theta, nodes1, nodes2)\n-        #                 x_new.edge_index = from_networkx(G_new).to(device).edge_index.long()\n-        #                 if model.predict(x_new, device) != y0:\n-        #                     F_lbd = distance(x_new, x0)\n-        #                     if F_lbd < F_theta:  \n-        #                         F_theta = F_lbd\n-        #                         flag_outer = 1\n-        #                         search_type = 1\n-        #                         for p in range(num_cluster_nodes1):\n-        #                             for q in range(num_cluster_nodes2):\n-        #                                 final_theta[nodes1[p], nodes2[q]] = theta[p, q]     \n-        #                                 final_theta[nodes2[q], nodes1[p]] = theta[p, q]     \n-        #                 num_query += 1   \n+        #inner cluster perturbation\n+        for i in range(num_cluster):\n+            nodes = cluster[i]\n+            num_cluster_nodes = len(nodes)\n+            if num_cluster_nodes > 1:\n+                for j in range(10*num_cluster_nodes): #search initial directions \n+                    theta = torch.normal(mean=torch.rand(1).item(),std=0.5,size=(num_cluster_nodes,num_cluster_nodes)).to(self.device)\n+                    theta = torch.triu(theta, diagonal=1).to(device)\n+                    G_new = new_graph(G, theta, index1=nodes)\n+                    x_new.edge_index = from_networkx(G_new).to(device).edge_index.long()\n+                    if model.predict(x_new, device) != y0:  #we find a direction\n+                        F_lbd = distance(x_new, x0)\n+                        if F_lbd < F_theta:\n+                            F_theta = F_lbd\n+                            flag_inner = 1\n+                            search_type = 0\n+                            for p in range(num_cluster_nodes-1):\n+                                for q in range(p+1, num_cluster_nodes):\n+                                    final_theta[nodes[p], nodes[q]] = theta[p, q]\n+                                    final_theta[nodes[q], nodes[p]] = theta[p, q]   \n+                    num_query += 1   \n+\n+        #perturbations between clusters\n+        if (num_cluster > 1) and (flag_inner == 0):\n+            for i in range(num_cluster - 1):\n+                for j in range(i+1, num_cluster):\n+                    nodes1, nodes2 = cluster[i], cluster[j]\n+                    num_cluster_nodes1, num_cluster_nodes2 = len(nodes1), len(nodes2)\n+                    for k in range(10*(num_cluster_nodes1+num_cluster_nodes2)):\n+                        theta = torch.normal(mean=torch.rand(1).item(), std=0.5, size=(num_cluster_nodes1,num_cluster_nodes2)).to(self.device)\n+                        G_new = new_graph(G, theta, nodes1, nodes2)\n+                        x_new.edge_index = from_networkx(G_new).to(device).edge_index.long()\n+                        if model.predict(x_new, device) != y0:\n+                            F_lbd = distance(x_new, x0)\n+                            if F_lbd < F_theta:  \n+                                F_theta = F_lbd\n+                                flag_outer = 1\n+                                search_type = 1\n+                                for p in range(num_cluster_nodes1):\n+                                    for q in range(num_cluster_nodes2):\n+                                        final_theta[nodes1[p], nodes2[q]] = theta[p, q]     \n+                                        final_theta[nodes2[q], nodes1[p]] = theta[p, q]     \n+                        num_query += 1   \n         \n         #perturbations on the whole graph\n         if (flag_inner == 0) and (flag_outer == 0):\ndiff --git a\/tang\/test.py b\/tang\/test.py\nindex 2c74fe8..7fbe619 100644\n--- a\/tang\/test.py\n+++ b\/tang\/test.py\n@@ -13,33 +13,77 @@\n import torch_geometric.transforms as T\n from Gin import GIN, SAG, GUNet\n from time import time\n+from collections import Counter\n \n # this part is added:\n #-----------------------------------------------------------------------\n-def inject_node(x, num_inject, initialization=\"node_mean\", Gaussian_mean=0, Gaussian_std=1):\n-    # exception check\n-    print(f\"Initialization: {initialization}\")\n-    assert initialization!=\"random\" or (initialization==\"random\" and Gaussian_mean and Gaussian_std)\n+def my_mode(sample):\n+    c = Counter(sample)\n+    return [k for k, v in c.items() if v == c.most_common(1)[0][1]][0]\n+\n+\n+def inject_node(x, num_inject, initialization, connection):\n+\n     node_feature_dim = x.x.shape[1]\n     injected_feature = torch.zeros(node_feature_dim)\n+    num_nodes_before_injection = x.num_nodes\n+\n     if initialization == \"zero\":\n         pass\n     elif initialization == \"one\":\n         injected_feature = torch.ones(node_feature_dim)\n     elif initialization == \"random\":\n-        injected_feature = torch.empty(node_feature_dim).normal_(mean=Gaussian_mean, std=Gaussian_std)\n+        Gaussian_mean = torch.mean(x.x, dim=0)\n+        Gaussian_std = torch.std(x.x,dim=0)\n+        # injected_feature = torch.empty(node_feature_dim).normal_(mean=Gaussian_mean, std=Gaussian_std)\n+        injected_feature = torch.empty(node_feature_dim)\n+        injected_feature[0].normal_(mean=Gaussian_mean[0].item(), std=Gaussian_std[0].item())\n+        injected_feature[1].normal_(mean=Gaussian_mean[1].item(), std=Gaussian_std[1].item())\n+        # print(injected_feature)\n     elif initialization == \"node_mean\":\n         injected_feature = torch.mean(x.x, dim=0)\n     else:\n         print(f\"Unsupported Initialization method: {initialization}\")\n         exit()\n     # inject new nodes into x\n-    x.x = torch.cat((x.x, torch.tensor(np.array([injected_feature.cpu().numpy() for i in range(num_inject)])).cuda()))\n+    x.x = torch.cat((x.x, torch.tensor([injected_feature.cpu().numpy() for i in range(num_inject)]).cuda()))\n     x.num_nodes = len(x.x)\n-    return x\n \n+    # connect new nodes into x\n+    if(connection == \"no_connection\"):\n+        pass\n+    elif(connection == \"random\"):\n+        for i in range(num_inject):\n+            node_number = i+num_nodes_before_injection\n+            random_node = random.randint(0,num_nodes_before_injection-1)\n+\n+            new_edge_back = torch.tensor([[node_number],[random_node]]).cuda()\n+            new_edge_front = torch.tensor([[random_node],[node_number]]).cuda()\n \n+            x.edge_index = torch.cat((new_edge_front, x.edge_index, new_edge_back),1)\n+            if(x.edge_attr is not None):\n+                x.edge_attr = torch.cat((x.edge_attr, torch.tensor([[0.,1.],[0.,1.]]).cuda()),0)\n+            \n+    elif(connection == \"mode\"):\n+        for i in range(num_inject):\n+            node_number = i+num_nodes_before_injection\n+            mode_node = my_mode(x.edge_index[0].tolist())\n+\n+            new_edge_back = torch.tensor([[node_number],[mode_node]]).cuda()\n+            new_edge_front = torch.tensor([[mode_node],[node_number]]).cuda()\n \n+            x.edge_index = torch.cat((new_edge_front, x.edge_index, new_edge_back),1)\n+\n+            if(x.edge_attr is not None):\n+                x.edge_attr = torch.cat((x.edge_attr, torch.tensor([[0.,1.],[0.,1.]]).cuda()),0)\n+    else:\n+        print(f\"Unsupported Connection method: {connection}\")\n+        exit()\n+\n+    x.num_edges = x.num_edges + num_inject\n+\n+    \n+    return x\n \n \n #-----------------------------------------------------------------------\n@@ -51,8 +95,10 @@ def get_args():\n \n     # this part is added:\n     #-----------------------------------------------------------------------\n-    parser.add_argument('--initialization', type=str, default='node_mean')\n-    parser.add_argument('--injection_percentage', type=float, default='0.1')\n+    parser.add_argument('--initialization', type=str, default='zero')\n+    parser.add_argument('--injection_percentage', type=float, default='0')\n+    parser.add_argument('--injection_number', type=int, default='0')\n+    parser.add_argument('--connection', type=str, default='no_connection')\n     \n     #-----------------------------------------------------------------------\n \n@@ -96,6 +142,10 @@ def count_edges(x_adv, x):\n     dropout = args.dropout\n     model_path = args.model_path\n     model_name = args.model\n+    injection_number = args.injection_number\n+    injection_percentage = args.injection_percentage\n+    initialization = args.initialization\n+    connection = args.connection\n     \n     if dataset_name in TUD.keys():\n         degree_as_attr = TUD[dataset_name]\n@@ -123,7 +173,6 @@ def count_edges(x_adv, x):\n     elif model_name=='GIN':\n         model = GIN(5,2,input_dim,hidden_dim,output_dim,dropout).to(device)\n         load_path = model_path + '{}_{}.pt'.format(dataset_name, model_name)\n-        # load_path = model_path + '{}.pt'.format(dataset_name)\n     elif model_name=='GUN':\n         model = GUNet(input_dim,hidden_dim,output_dim,0.8,3,dropout).to(device)\n         load_path = model_path + '{}_{}.pt'.format(dataset_name, model_name)\n@@ -167,42 +216,34 @@ def count_edges(x_adv, x):\n         print('begin to attack instance {}'.format(i))\n         x0 = test_dataset[i].to(device)\n \n-\n-        # this part is added:\n-        #-----------------------------------------------------------------------\n-        # print(\"\\n \\n \\n \\n \\n\")\n         print(\"---------------------------instance\",i,\"basic info-----------------------------------\")\n         print(\"x0 before node injection is:\", x0)\n         print(\"number of nodes before: \", len(x0.x))\n-        # print(\"the information in x0 is:\", x0.x)\n \n         G0 = to_networkx(x0, to_undirected=True)\n-        # print(\"nodes before injection:\",list(G0.nodes))\n         print(\"edges before injection:\",list(G0.edges))\n-        # print(\"the information in G0 is:\", G0.nodes.data())\n-\n-\n \n-        #-----------------------------------------------------------------------\n         y0 = x0.y[0]\n         y1 = model.predict(x0, device)\n-        #-----------------------------------------------------------------------\n \n+        # inject node\n+        if injection_number == 0 and injection_percentage == 0:\n+            pass\n+        # if num_inject not specified, then use percentage\n+        elif injection_number == 0 and injection_percentage != 0:\n+            num_injected = max(1, int(x0.num_nodes*args.injection_percentage))\n+            x0 = inject_node(x0, initialization=initialization, num_inject=num_injected, connection = connection)\n+        elif injection_number != 0 and injection_percentage == 0:\n+            num_injected = injection_number\n+            x0 = inject_node(x0, initialization=initialization, num_inject = injection_number, connection = connection)\n+        else:\n+            print(\"Cannot have mixed specifications of injection_number and injection_percentage!\")\n+            exit()\n \n \n-        # inject node\n-        num_injected = max(1, int(x0.num_nodes*args.injection_percentage))\n-        x0 = inject_node(x0, initialization=args.initialization, num_inject=num_injected)\n         print(\"x0 after node injection is:\", x0)\n         print(\"number of nodes after: \", len(x0.x))\n         G1 = to_networkx(x0, to_undirected=True)\n-        # print(\"the information in x0 is:\", x0.x)\n-        # print(\"nodes after injection:\",list(G1.nodes))\n-        # print(\"edges after injection:\",list(G1.edges))\n-        # print(\"the information in G1 is:\", G1.nodes.data())\n-\n-\n-        # print(\"x0 edge index is:\",x0.edge_index)\n         \n         #-----------------------------------------------------------------------\n \n@@ -216,6 +257,7 @@ def count_edges(x_adv, x):\n \n         print(\"the ground truth y0 is:\", y0.item())\n         print(\"the model predict y1 is:\", y1.item())\n+        print(\"the prediction after injection y2 is:\", y2.item())\n         print(\"\\n-----------------------------------------------------------------------------------\")\n \n         '''\n@@ -328,9 +370,9 @@ def count_edges(x_adv, x):\n     \n \n \n-    success_ratio = num_success \/ (num_test - no_need_count)*100\n+    success_ratio = (num_success + num_success_via_injection) \/ (num_test - no_need_count)*100\n     avg_perturbation = sum(perturbation) \/ num_success\n-    print(\"Sign-Opt: the success rate of black-box attack is {}\/{} = {:.4f}\".format(num_success,num_test-no_need_count, success_ratio))\n+    print(\"Sign-Opt: the success rate of black-box attack is {}\/{} = {:.4f}\".format(num_success + num_success_via_injection, num_test-no_need_count, success_ratio))\n     print('Sign-Opt: the average perturbation is {:.4f}'.format(avg_perturbation))\n     print('Sign-Opt: the average perturbation ratio is {:.4f}'.format(sum(perturbation_ratio) \/ num_success*100))\n     print('Sign-Opt: the average query count is {:.4f}'.format(sum(num_query)\/(num_test-no_need_count)))\n","files":{"\/tang\/test.py":{"changes":[{"diff":"\n import torch_geometric.transforms as T\n from Gin import GIN, SAG, GUNet\n from time import time\n+from collections import Counter\n \n # this part is added:\n #-----------------------------------------------------------------------\n-def inject_node(x, num_inject, initialization=\"node_mean\", Gaussian_mean=0, Gaussian_std=1):\n-    # exception check\n-    print(f\"Initialization: {initialization}\")\n-    assert initialization!=\"random\" or (initialization==\"random\" and Gaussian_mean and Gaussian_std)\n+def my_mode(sample):\n+    c = Counter(sample)\n+    return [k for k, v in c.items() if v == c.most_common(1)[0][1]][0]\n+\n+\n+def inject_node(x, num_inject, initialization, connection):\n+\n     node_feature_dim = x.x.shape[1]\n     injected_feature = torch.zeros(node_feature_dim)\n+    num_nodes_before_injection = x.num_nodes\n+\n     if initialization == \"zero\":\n         pass\n     elif initialization == \"one\":\n         injected_feature = torch.ones(node_feature_dim)\n     elif initialization == \"random\":\n-        injected_feature = torch.empty(node_feature_dim).normal_(mean=Gaussian_mean, std=Gaussian_std)\n+        Gaussian_mean = torch.mean(x.x, dim=0)\n+        Gaussian_std = torch.std(x.x,dim=0)\n+        # injected_feature = torch.empty(node_feature_dim).normal_(mean=Gaussian_mean, std=Gaussian_std)\n+        injected_feature = torch.empty(node_feature_dim)\n+        injected_feature[0].normal_(mean=Gaussian_mean[0].item(), std=Gaussian_std[0].item())\n+        injected_feature[1].normal_(mean=Gaussian_mean[1].item(), std=Gaussian_std[1].item())\n+        # print(injected_feature)\n     elif initialization == \"node_mean\":\n         injected_feature = torch.mean(x.x, dim=0)\n     else:\n         print(f\"Unsupported Initialization method: {initialization}\")\n         exit()\n     # inject new nodes into x\n-    x.x = torch.cat((x.x, torch.tensor(np.array([injected_feature.cpu().numpy() for i in range(num_inject)])).cuda()))\n+    x.x = torch.cat((x.x, torch.tensor([injected_feature.cpu().numpy() for i in range(num_inject)]).cuda()))\n     x.num_nodes = len(x.x)\n-    return x\n \n+    # connect new nodes into x\n+    if(connection == \"no_connection\"):\n+        pass\n+    elif(connection == \"random\"):\n+        for i in range(num_inject):\n+            node_number = i+num_nodes_before_injection\n+            random_node = random.randint(0,num_nodes_before_injection-1)\n+\n+            new_edge_back = torch.tensor([[node_number],[random_node]]).cuda()\n+            new_edge_front = torch.tensor([[random_node],[node_number]]).cuda()\n \n+            x.edge_index = torch.cat((new_edge_front, x.edge_index, new_edge_back),1)\n+            if(x.edge_attr is not None):\n+                x.edge_attr = torch.cat((x.edge_attr, torch.tensor([[0.,1.],[0.,1.]]).cuda()),0)\n+            \n+    elif(connection == \"mode\"):\n+        for i in range(num_inject):\n+            node_number = i+num_nodes_before_injection\n+            mode_node = my_mode(x.edge_index[0].tolist())\n+\n+            new_edge_back = torch.tensor([[node_number],[mode_node]]).cuda()\n+            new_edge_front = torch.tensor([[mode_node],[node_number]]).cuda()\n \n+            x.edge_index = torch.cat((new_edge_front, x.edge_index, new_edge_back),1)\n+\n+            if(x.edge_attr is not None):\n+                x.edge_attr = torch.cat((x.edge_attr, torch.tensor([[0.,1.],[0.,1.]]).cuda()),0)\n+    else:\n+        print(f\"Unsupported Connection method: {connection}\")\n+        exit()\n+\n+    x.num_edges = x.num_edges + num_inject\n+\n+    \n+    return x\n \n \n #-----------------------------------------------------------------------\n","add":51,"remove":7,"filename":"\/tang\/test.py","badparts":["def inject_node(x, num_inject, initialization=\"node_mean\", Gaussian_mean=0, Gaussian_std=1):","    print(f\"Initialization: {initialization}\")","    assert initialization!=\"random\" or (initialization==\"random\" and Gaussian_mean and Gaussian_std)","        injected_feature = torch.empty(node_feature_dim).normal_(mean=Gaussian_mean, std=Gaussian_std)","    x.x = torch.cat((x.x, torch.tensor(np.array([injected_feature.cpu().numpy() for i in range(num_inject)])).cuda()))","    return x"],"goodparts":["from collections import Counter","def my_mode(sample):","    c = Counter(sample)","    return [k for k, v in c.items() if v == c.most_common(1)[0][1]][0]","def inject_node(x, num_inject, initialization, connection):","    num_nodes_before_injection = x.num_nodes","        Gaussian_mean = torch.mean(x.x, dim=0)","        Gaussian_std = torch.std(x.x,dim=0)","        injected_feature = torch.empty(node_feature_dim)","        injected_feature[0].normal_(mean=Gaussian_mean[0].item(), std=Gaussian_std[0].item())","        injected_feature[1].normal_(mean=Gaussian_mean[1].item(), std=Gaussian_std[1].item())","    x.x = torch.cat((x.x, torch.tensor([injected_feature.cpu().numpy() for i in range(num_inject)]).cuda()))","    if(connection == \"no_connection\"):","        pass","    elif(connection == \"random\"):","        for i in range(num_inject):","            node_number = i+num_nodes_before_injection","            random_node = random.randint(0,num_nodes_before_injection-1)","            new_edge_back = torch.tensor([[node_number],[random_node]]).cuda()","            new_edge_front = torch.tensor([[random_node],[node_number]]).cuda()","            x.edge_index = torch.cat((new_edge_front, x.edge_index, new_edge_back),1)","            if(x.edge_attr is not None):","                x.edge_attr = torch.cat((x.edge_attr, torch.tensor([[0.,1.],[0.,1.]]).cuda()),0)","    elif(connection == \"mode\"):","        for i in range(num_inject):","            node_number = i+num_nodes_before_injection","            mode_node = my_mode(x.edge_index[0].tolist())","            new_edge_back = torch.tensor([[node_number],[mode_node]]).cuda()","            new_edge_front = torch.tensor([[mode_node],[node_number]]).cuda()","            x.edge_index = torch.cat((new_edge_front, x.edge_index, new_edge_back),1)","            if(x.edge_attr is not None):","                x.edge_attr = torch.cat((x.edge_attr, torch.tensor([[0.,1.],[0.,1.]]).cuda()),0)","    else:","        print(f\"Unsupported Connection method: {connection}\")","        exit()","    x.num_edges = x.num_edges + num_inject","    return x"]},{"diff":"\n \n     # this part is added:\n     #-----------------------------------------------------------------------\n-    parser.add_argument('--initialization', type=str, default='node_mean')\n-    parser.add_argument('--injection_percentage', type=float, default='0.1')\n+    parser.add_argument('--initialization', type=str, default='zero')\n+    parser.add_argument('--injection_percentage', type=float, default='0')\n+    parser.add_argument('--injection_number', type=int, default='0')\n+    parser.add_argument('--connection', type=str, default='no_connection')\n     \n     #-----------------------------------------------------------------------\n \n","add":4,"remove":2,"filename":"\/tang\/test.py","badparts":["    parser.add_argument('--initialization', type=str, default='node_mean')","    parser.add_argument('--injection_percentage', type=float, default='0.1')"],"goodparts":["    parser.add_argument('--initialization', type=str, default='zero')","    parser.add_argument('--injection_percentage', type=float, default='0')","    parser.add_argument('--injection_number', type=int, default='0')","    parser.add_argument('--connection', type=str, default='no_connection')"]},{"diff":"\n         print('begin to attack instance {}'.format(i))\n         x0 = test_dataset[i].to(device)\n \n-\n-        # this part is added:\n-        #-----------------------------------------------------------------------\n-        # print(\"\\n \\n \\n \\n \\n\")\n         print(\"---------------------------instance\",i,\"basic info-----------------------------------\")\n         print(\"x0 before node injection is:\", x0)\n         print(\"number of nodes before: \", len(x0.x))\n-        # print(\"the information in x0 is:\", x0.x)\n \n         G0 = to_networkx(x0, to_undirected=True)\n-        # print(\"nodes before injection:\",list(G0.nodes))\n         print(\"edges before injection:\",list(G0.edges))\n-        # print(\"the information in G0 is:\", G0.nodes.data())\n-\n-\n \n-        #-----------------------------------------------------------------------\n         y0 = x0.y[0]\n         y1 = model.predict(x0, device)\n-        #-----------------------------------------------------------------------\n \n+        # inject node\n+        if injection_number == 0 and injection_percentage == 0:\n+            pass\n+        # if num_inject not specified, then use percentage\n+        elif injection_number == 0 and injection_percentage != 0:\n+            num_injected = max(1, int(x0.num_nodes*args.injection_percentage))\n+            x0 = inject_node(x0, initialization=initialization, num_inject=num_injected, connection = connection)\n+        elif injection_number != 0 and injection_percentage == 0:\n+            num_injected = injection_number\n+            x0 = inject_node(x0, initialization=initialization, num_inject = injection_number, connection = connection)\n+        else:\n+            print(\"Cannot have mixed specifications of injection_number and injection_percentage!\")\n+            exit()\n \n \n-        # inject node\n-        num_injected = max(1, int(x0.num_nodes*args.injection_percentage))\n-        x0 = inject_node(x0, initialization=args.initialization, num_inject=num_injected)\n         print(\"x0 after node injection is:\", x0)\n         print(\"number of nodes after: \", len(x0.x))\n         G1 = to_networkx(x0, to_undirected=True)\n-        # print(\"the information in x0 is:\", x0.x)\n-        # print(\"nodes after injection:\",list(G1.nodes))\n-        # print(\"edges after injection:\",list(G1.edges))\n-        # print(\"the information in G1 is:\", G1.nodes.data())\n-\n-\n-        # print(\"x0 edge index is:\",x0.edge_index)\n         \n         #-----------------------------------------------------------------------\n \n","add":13,"remove":21,"filename":"\/tang\/test.py","badparts":["        num_injected = max(1, int(x0.num_nodes*args.injection_percentage))","        x0 = inject_node(x0, initialization=args.initialization, num_inject=num_injected)"],"goodparts":["        if injection_number == 0 and injection_percentage == 0:","            pass","        elif injection_number == 0 and injection_percentage != 0:","            num_injected = max(1, int(x0.num_nodes*args.injection_percentage))","            x0 = inject_node(x0, initialization=initialization, num_inject=num_injected, connection = connection)","        elif injection_number != 0 and injection_percentage == 0:","            num_injected = injection_number","            x0 = inject_node(x0, initialization=initialization, num_inject = injection_number, connection = connection)","        else:","            print(\"Cannot have mixed specifications of injection_number and injection_percentage!\")","            exit()"]},{"diff":"\n     \n \n \n-    success_ratio = num_success \/ (num_test - no_need_count)*100\n+    success_ratio = (num_success + num_success_via_injection) \/ (num_test - no_need_count)*100\n     avg_perturbation = sum(perturbation) \/ num_success\n-    print(\"Sign-Opt: the success rate of black-box attack is {}\/{} = {:.4f}\".format(num_success,num_test-no_need_count, success_ratio))\n+    print(\"Sign-Opt: the success rate of black-box attack is {}\/{} = {:.4f}\".format(num_success + num_success_via_injection, num_test-no_need_count, success_ratio))\n     print('Sign-Opt: the average perturbation is {:.4f}'.format(avg_perturbation))\n     print('Sign-Opt: the average perturbation ratio is {:.4f}'.format(sum(perturbation_ratio) \/ num_success*100))\n     print('Sign-Opt: the average query count is {:.4f}'.format(sum(num_query)\/(num_test-no_need_count)))\n","add":2,"remove":2,"filename":"\/tang\/test.py","badparts":["    success_ratio = num_success \/ (num_test - no_need_count)*100","    print(\"Sign-Opt: the success rate of black-box attack is {}\/{} = {:.4f}\".format(num_success,num_test-no_need_count, success_ratio))"],"goodparts":["    success_ratio = (num_success + num_success_via_injection) \/ (num_test - no_need_count)*100","    print(\"Sign-Opt: the success rate of black-box attack is {}\/{} = {:.4f}\".format(num_success + num_success_via_injection, num_test-no_need_count, success_ratio))"]}],"source":"\nimport torch import torch_geometric import community.community_louvain import numpy as np import networkx import argparse from torch_geometric.datasets import TUDataset from torch_geometric.data import DataLoader from torch_geometric.data import Batch from Sign_OPT import * from torch_geometric.utils import to_networkx, from_networkx import torch_geometric.transforms as T from Gin import GIN, SAG, GUNet from time import time def inject_node(x, num_inject, initialization=\"node_mean\", Gaussian_mean=0, Gaussian_std=1): print(f\"Initialization:{initialization}\") assert initialization!=\"random\" or(initialization==\"random\" and Gaussian_mean and Gaussian_std) node_feature_dim=x.x.shape[1] injected_feature=torch.zeros(node_feature_dim) if initialization==\"zero\": pass elif initialization==\"one\": injected_feature=torch.ones(node_feature_dim) elif initialization==\"random\": injected_feature=torch.empty(node_feature_dim).normal_(mean=Gaussian_mean, std=Gaussian_std) elif initialization==\"node_mean\": injected_feature=torch.mean(x.x, dim=0) else: print(f\"Unsupported Initialization method:{initialization}\") exit() x.x=torch.cat((x.x, torch.tensor(np.array([injected_feature.cpu().numpy() for i in range(num_inject)])).cuda())) x.num_nodes=len(x.x) return x def get_args(): parser=argparse.ArgumentParser(description='Pytorch graph isomorphism network for graph classification') parser.add_argument('--initialization', type=str, default='node_mean') parser.add_argument('--injection_percentage', type=float, default='0.1') parser.add_argument('--effective', type=int, default=1) parser.add_argument('--max_query', type=int, default=40000) parser.add_argument('--id', type=int, default=1) parser.add_argument('--search', type=int, default=1) parser.add_argument('--dataset', type=str, default=\"IMDB-BINARY\") parser.add_argument('--device', type=int, default=0) parser.add_argument('--batch_size', type=int, default=32, help='social dataset:64 bio dataset:32') parser.add_argument('--hidden_dim', type=int, default=64) parser.add_argument('--dropout', type=float, default=0.5) parser.add_argument('--model_path', type=str, default='.\/trained_model\/') parser.add_argument('--model', type=str, default='GUN') args=parser.parse_args() return args def distance(x_adv, x): adj_adv=nx.adjacency_matrix(to_networkx(x_adv, to_undirected=True)) adj_x=nx.adjacency_matrix(to_networkx(x, to_undirected=True)) return np.sum(np.abs(adj_adv-adj_x)) \/ 2 def count_edges(x_adv, x): adj_adv=nx.adjacency_matrix(to_networkx(x_adv, to_undirected=True)).todense().A adj_x=nx.adjacency_matrix(to_networkx(x, to_undirected=True)).todense().A difference=adj_adv -adj_x num_add=sum(sum(difference==1)) \/ 2 num_delete=sum(sum(difference==-1)) \/ 2 return num_add, num_delete TUD={'NCI1':0,'COIL-DEL':0,'IMDB-BINARY':1} if __name__=='__main__': args=get_args() dataset_name=args.dataset device=torch.device(\"cuda:\"+str(args.device) if torch.cuda.is_available() else torch.device(\"cpu\")) batch_size=args.batch_size hidden_dim=args.hidden_dim dropout=args.dropout model_path=args.model_path model_name=args.model if dataset_name in TUD.keys(): degree_as_attr=TUD[dataset_name] else: print('invalid dataset!') raise(ValueError) if degree_as_attr: dataset=TUDataset(root='.\/dataset',name=dataset_name,use_edge_attr='False', use_node_attr=True, pre_transform=T.Constant(1, True)) else: dataset=TUDataset(root='.\/dataset',name=dataset_name,use_edge_attr='False',use_node_attr=True) index_path='.\/data_split\/' +dataset_name +'_' with open(index_path+'test_index.txt', 'r') as f: test_index=eval(f.read()) test_dataset=dataset[test_index] input_dim=dataset.num_node_features output_dim=dataset.num_classes print('input dim: ', input_dim) print('output dim: ', output_dim) if model_name=='SAG': model=SAG(5,input_dim,hidden_dim,output_dim,0.8,dropout).to(device) load_path=model_path +'{}_{}.pt'.format(dataset_name, model_name) elif model_name=='GIN': model=GIN(5,2,input_dim,hidden_dim,output_dim,dropout).to(device) load_path=model_path +'{}_{}.pt'.format(dataset_name, model_name) elif model_name=='GUN': model=GUNet(input_dim,hidden_dim,output_dim,0.8,3,dropout).to(device) load_path=model_path +'{}_{}.pt'.format(dataset_name, model_name) model.load_state_dict(torch.load(load_path, map_location=device)) model.eval() attacker=OPT_attack_sign_SGD(model, device, args.effective) num_test=len(test_dataset) perturbation=[] perturbation_ratio=[] Dangling_injection_count=0 no_need_count=0 num_query=[] fail_count=0 distortion=[] attack_time=[] init_perturbation=[] init_perturbation_ratio=[] init_num_query=[] init_distortion=[] init_attack_time=[] search_type=[] detect_test_normal=[] detect_test_advers=[] num_success=0 num_success_via_injection=0 num_add_edge, num_delete_edge=[],[] for i in range(num_test): print(\"\\n \\n \\n \\n \\n \\n \\n \\n\") print('begin to attack instance{}'.format(i)) x0=test_dataset[i].to(device) print(\"---------------------------instance\",i,\"basic info-----------------------------------\") print(\"x0 before node injection is:\", x0) print(\"number of nodes before: \", len(x0.x)) G0=to_networkx(x0, to_undirected=True) print(\"edges before injection:\",list(G0.edges)) y0=x0.y[0] y1=model.predict(x0, device) num_injected=max(1, int(x0.num_nodes*args.injection_percentage)) x0=inject_node(x0, initialization=args.initialization, num_inject=num_injected) print(\"x0 after node injection is:\", x0) print(\"number of nodes after: \", len(x0.x)) G1=to_networkx(x0, to_undirected=True) y2=model.predict(x0, device) print(\"the ground truth y0 is:\", y0.item()) print(\"the model predict y1 is:\", y1.item()) print(\"\\n-----------------------------------------------------------------------------------\") ''' please note here that y0 is the ground truth label for graph x0, y1 is the model prediction for x0 without any add-ons, y2 is the model prediction for x0 when nodes are inserted but no edge purturbations are in place we commence edge attack only when y2 !=y0 ''' num_nodes=x0.num_nodes space=num_nodes *(num_nodes -1) \/ 2 if y0==y1: if y0 !=y2: num_success_via_injection +=1 num_success +=1 print(\"instance{} is successfully attacked via node injection.\".format(i)) elif y0==y2: time_start=time() adv_x0, adv_y0, query, success, dis, init, is_injected_nodes_dangling=attacker.attack_untargeted(x0, y0, num_injected, query_limit=args.max_query) print(\"before adv attack, the model predicts:\", y0.item()) print(\"after adv attack, the model predicts:\", adv_y0.item()) if is_injected_nodes_dangling: Dangling_injection_count +=1 if y0 !=adv_y0: num_success +=1 time_end=time() init_num_query.append(init[2]) num_query.append(query) init_attack_time.append(init[3]) attack_time.append(time_end-time_start) if success: init_perturb, init_dis, init_query, init_time, s_type=init init_ratio=init_perturb \/ space init_perturbation.append(init_perturb) init_distortion.append(init_dis) search_type.append(s_type) init_perturbation_ratio.append(init_ratio) perturb=distance(adv_x0, x0) perturbation.append(perturb) perturbation_ratio.append(perturb\/space) distortion.append(dis) add_edge, delete_edge=count_edges(adv_x0, x0) num_delete_edge.append(delete_edge) num_add_edge.append(add_edge) adv_x0.y=x0.y detect_test_advers.append(adv_x0) detect_test_normal.append(x0) else: detect_test_advers.append(x0) detect_test_normal.append(x0) init_distortion.append(-1) init_perturbation.append(-1) init_perturbation_ratio.append(-1) search_type.append(-1) perturbation.append(-1) perturbation_ratio.append(-1) distortion.append(-1) else: print('instance{} is wrongly classified, No Need to Attack'.format(i)) no_need_count +=1 num_query.append(0) attack_time.append(0) perturbation.append(0) perturbation_ratio.append(0) distortion.append(0) init_perturbation.append(0) init_distortion.append(0) init_num_query.append(0) init_attack_time.append(0) search_type.append(0) init_perturbation_ratio.append(0) print(f\"\\nattack loop: \\ \\n\\t{num_success} out of{i+1 -no_need_count} instance success rate, \\ \\n\\t{Dangling_injection_count} out of{num_success} has dangling injected nodes \\ \\n\\t{no_need_count} instances need no attack, \\ \\n\\t{num_success_via_injection} instances successed with only node injection\") success_ratio=num_success \/(num_test -no_need_count)*100 avg_perturbation=sum(perturbation) \/ num_success print(\"Sign-Opt: the success rate of black-box attack is{}\/{}={:.4f}\".format(num_success,num_test-no_need_count, success_ratio)) print('Sign-Opt: the average perturbation is{:.4f}'.format(avg_perturbation)) print('Sign-Opt: the average perturbation ratio is{:.4f}'.format(sum(perturbation_ratio) \/ num_success*100)) print('Sign-Opt: the average query count is{:.4f}'.format(sum(num_query)\/(num_test-no_need_count))) print('Sign-Opt: the average attacking time is{:.4f}'.format(sum(attack_time)\/(num_test-no_need_count))) print('Sign-Opt: the average distortion is{:.4f}'.format(sum(distortion)\/num_success)) print('dataset:{}'.format(dataset_name)) if args.search==1 and args.effective==1 and args.id==1: detect_test_path='.\/defense\/'+dataset_name+'_'+model_name+'_Our_' torch.save(detect_test_normal, detect_test_path+'test_normal.pt') torch.save(detect_test_advers, detect_test_path+'test_advers.pt') print('test dataset for defense saved!') init_path='.\/out1\/init_{}_{}_{}_{}_'.format(dataset_name, args.id, args.effective, args.search) with open(init_path+'search_type.txt', 'w') as f: f.write(str(search_type)) with open(init_path+'P.txt', 'w') as f: f.write(str(init_perturbation)) with open(init_path+'PR.txt', 'w') as f: f.write(str(init_perturbation_ratio)) with open(init_path+'D.txt', 'w') as f: f.write(str(init_distortion)) with open(init_path+'Q.txt', 'w') as f: f.write(str(init_num_query)) with open(init_path+'T.txt', 'w') as f: f.write(str(init_attack_time)) our_path='.\/out1\/our_{}_{}_{}_{}_'.format(dataset_name, args.id, args.effective, args.search) with open(our_path+'Q.txt', 'w') as f: f.write(str(num_query)) with open(our_path+'T.txt', 'w') as f: f.write(str(attack_time)) with open(our_path+'P.txt', 'w') as f: f.write(str(perturbation)) with open(our_path+'PR.txt', 'w') as f: f.write(str(perturbation_ratio)) with open(our_path+'D.txt', 'w') as f: f.write(str(distortion)) with open(our_path+'ADD.txt', 'w') as f: f.write(str(num_delete_edge)) with open(our_path+'DEL.txt', 'w') as f: f.write(str(num_add_edge)) print(\"the numbers of deleted edges are:\", num_delete_edge) print(\"the numbers od added edges are:\", num_add_edge) print(\"the average number of deleted edges for %s: %d\"%(dataset_name, float(sum(num_delete_edge)\/len(num_delete_edge)))) print(\"the average number of added edges for %s: %d\"%(dataset_name, float(sum(num_add_edge)\/len(num_add_edge)))) ''' out_path='.\/out\/{}_Opt_{}.txt'.format(dataset_name, bound) with open(out_path, 'w') as f: f.write('{} instances don\\'t need to be attacked\\n'.format(no_need_count)) f.write('Sign-Opt fails to attack{} instance\\n'.format(fail_count)) f.write(\"Sign-Opt: the success rate of black-box attack is{}\/{}={:.4f}\\n\".format(success_count,num_test-no_need_count, success_ratio)) f.write('Sign-Opt: the average perturbation is{:.4f}\\n'.format(avg_perturbation)) f.write('Sign-Opt: the average perturbation ratio is{:.4f}\\n'.format(sum(perturbation_ratio) \/ success_count*100)) f.write('Sign-Opt: the average query count is{:.4f}\\n'.format(sum(num_query)\/(num_test-no_need_count))) f.write('Sign-Opt: the average attacking time is{:.4f}\\n'.format(sum(attack_time)\/(num_test-no_need_count))) f.write('Sign-Opt: the average distortion is{:.4f}\\n'.format(sum(distortion)\/success_count)) f.write('Sign-Opt: detail perturbation are:{}\\n'.format(perturbation)) f.write('Sign-Opt: detail perturbation ratio are:{}\\n'.format(perturbation_ratio)) ''' ","sourceWithComments":"import torch\nimport torch_geometric #torch_geometric == 1.6.1\n# import community\nimport community.community_louvain\nimport numpy as np\nimport networkx\nimport argparse\nfrom torch_geometric.datasets import TUDataset\nfrom torch_geometric.data import DataLoader\nfrom torch_geometric.data import Batch\nfrom Sign_OPT import *\nfrom torch_geometric.utils import to_networkx, from_networkx\nimport torch_geometric.transforms as T\nfrom Gin import GIN, SAG, GUNet\nfrom time import time\n\n# this part is added:\n#-----------------------------------------------------------------------\ndef inject_node(x, num_inject, initialization=\"node_mean\", Gaussian_mean=0, Gaussian_std=1):\n    # exception check\n    print(f\"Initialization: {initialization}\")\n    assert initialization!=\"random\" or (initialization==\"random\" and Gaussian_mean and Gaussian_std)\n    node_feature_dim = x.x.shape[1]\n    injected_feature = torch.zeros(node_feature_dim)\n    if initialization == \"zero\":\n        pass\n    elif initialization == \"one\":\n        injected_feature = torch.ones(node_feature_dim)\n    elif initialization == \"random\":\n        injected_feature = torch.empty(node_feature_dim).normal_(mean=Gaussian_mean, std=Gaussian_std)\n    elif initialization == \"node_mean\":\n        injected_feature = torch.mean(x.x, dim=0)\n    else:\n        print(f\"Unsupported Initialization method: {initialization}\")\n        exit()\n    # inject new nodes into x\n    x.x = torch.cat((x.x, torch.tensor(np.array([injected_feature.cpu().numpy() for i in range(num_inject)])).cuda()))\n    x.num_nodes = len(x.x)\n    return x\n\n\n\n\n\n#-----------------------------------------------------------------------\n\ndef get_args():\n    parser = argparse.ArgumentParser(description='Pytorch graph isomorphism network for graph classification')\n    #these are parameters for attack model\n\n\n    # this part is added:\n    #-----------------------------------------------------------------------\n    parser.add_argument('--initialization', type=str, default='node_mean')\n    parser.add_argument('--injection_percentage', type=float, default='0.1')\n    \n    #-----------------------------------------------------------------------\n\n    parser.add_argument('--effective', type=int, default=1)\n    parser.add_argument('--max_query', type=int, default=40000)\n    parser.add_argument('--id', type= int, default=1)\n    parser.add_argument('--search', type=int, default=1)\n    #these are parameters for GIN model\n    parser.add_argument('--dataset', type=str, default=\"IMDB-BINARY\")\n    parser.add_argument('--device', type=int, default=0)\n    parser.add_argument('--batch_size', type=int, default=32, help='social dataset:64 bio dataset:32')\n    parser.add_argument('--hidden_dim', type=int, default=64)\n    parser.add_argument('--dropout', type=float, default=0.5)\n    parser.add_argument('--model_path', type=str, default='.\/trained_model\/')\n    parser.add_argument('--model', type=str, default='GUN')\n    args = parser.parse_args()\n    return args\n\ndef distance(x_adv, x):\n    adj_adv = nx.adjacency_matrix(to_networkx(x_adv, to_undirected=True))\n    adj_x = nx.adjacency_matrix(to_networkx(x, to_undirected=True))\n    return np.sum(np.abs(adj_adv-adj_x)) \/ 2\n    \ndef count_edges(x_adv, x):\n    adj_adv = nx.adjacency_matrix(to_networkx(x_adv, to_undirected=True)).todense().A\n    adj_x = nx.adjacency_matrix(to_networkx(x, to_undirected=True)).todense().A\n    difference = adj_adv - adj_x\n    num_add = sum(sum(difference==1)) \/ 2\n    num_delete = sum(sum(difference==-1)) \/ 2\n    return num_add, num_delete\n\nTUD = {'NCI1':0,'COIL-DEL':0,'IMDB-BINARY':1}\n\nif __name__ == '__main__':\n\n    args = get_args()\n    dataset_name = args.dataset\n    device = torch.device(\"cuda:\"+str(args.device) if torch.cuda.is_available() else torch.device(\"cpu\"))\n    batch_size = args.batch_size\n    hidden_dim = args.hidden_dim\n    dropout = args.dropout\n    model_path = args.model_path\n    model_name = args.model\n    \n    if dataset_name in TUD.keys():\n        degree_as_attr = TUD[dataset_name]\n    else:\n        print('invalid dataset!')\n        raise(ValueError)\n\n    if degree_as_attr:\n        dataset = TUDataset(root='.\/dataset',name=dataset_name,use_edge_attr='False', use_node_attr=True,\n        pre_transform=T.Constant(1, True))\n    else:\n        dataset = TUDataset(root='.\/dataset',name=dataset_name,use_edge_attr='False',use_node_attr=True)\n    \n    index_path = '.\/data_split\/' + dataset_name + '_'\n    with open(index_path+'test_index.txt', 'r') as f:\n        test_index = eval(f.read())\n    test_dataset = dataset[test_index]\n    input_dim = dataset.num_node_features\n    output_dim = dataset.num_classes\n    print('input dim: ', input_dim)\n    print('output dim: ', output_dim)\n    if model_name=='SAG':\n        model = SAG(5,input_dim,hidden_dim,output_dim,0.8,dropout).to(device)\n        load_path = model_path + '{}_{}.pt'.format(dataset_name, model_name)\n    elif model_name=='GIN':\n        model = GIN(5,2,input_dim,hidden_dim,output_dim,dropout).to(device)\n        load_path = model_path + '{}_{}.pt'.format(dataset_name, model_name)\n        # load_path = model_path + '{}.pt'.format(dataset_name)\n    elif model_name=='GUN':\n        model = GUNet(input_dim,hidden_dim,output_dim,0.8,3,dropout).to(device)\n        load_path = model_path + '{}_{}.pt'.format(dataset_name, model_name)\n    model.load_state_dict(torch.load(load_path, map_location=device))\n    model.eval()\n\n    attacker = OPT_attack_sign_SGD(model, device, args.effective)\n    num_test = len(test_dataset)\n    perturbation = [] #perturbation for each poisoned graph\n    perturbation_ratio = [] #perturbation ratio for each poisoned graph\n\n    Dangling_injection_count = 0\n\n    no_need_count = 0\n    num_query = []    \n    fail_count = 0\n    distortion = []\n    attack_time = []\n\n    init_perturbation = [] #perturbation for each poisoned graph\n    init_perturbation_ratio = [] #perturbation ratio for each poisoned graph\n    init_num_query = []    \n    init_distortion = []\n    init_attack_time = []\n    search_type = []\n\n    detect_test_normal = []\n    detect_test_advers = []\n\n    # this part is added:\n    #-----------------------------------------------------------------------\n    num_success = 0\n    num_success_via_injection = 0\n    #-----------------------------------------------------------------------\n\n    num_add_edge, num_delete_edge = [], []\n    for i in range(num_test):\n\n        print(\"\\n \\n \\n \\n \\n \\n \\n \\n\")\n\n        print('begin to attack instance {}'.format(i))\n        x0 = test_dataset[i].to(device)\n\n\n        # this part is added:\n        #-----------------------------------------------------------------------\n        # print(\"\\n \\n \\n \\n \\n\")\n        print(\"---------------------------instance\",i,\"basic info-----------------------------------\")\n        print(\"x0 before node injection is:\", x0)\n        print(\"number of nodes before: \", len(x0.x))\n        # print(\"the information in x0 is:\", x0.x)\n\n        G0 = to_networkx(x0, to_undirected=True)\n        # print(\"nodes before injection:\",list(G0.nodes))\n        print(\"edges before injection:\",list(G0.edges))\n        # print(\"the information in G0 is:\", G0.nodes.data())\n\n\n\n        #-----------------------------------------------------------------------\n        y0 = x0.y[0]\n        y1 = model.predict(x0, device)\n        #-----------------------------------------------------------------------\n\n\n\n        # inject node\n        num_injected = max(1, int(x0.num_nodes*args.injection_percentage))\n        x0 = inject_node(x0, initialization=args.initialization, num_inject=num_injected)\n        print(\"x0 after node injection is:\", x0)\n        print(\"number of nodes after: \", len(x0.x))\n        G1 = to_networkx(x0, to_undirected=True)\n        # print(\"the information in x0 is:\", x0.x)\n        # print(\"nodes after injection:\",list(G1.nodes))\n        # print(\"edges after injection:\",list(G1.edges))\n        # print(\"the information in G1 is:\", G1.nodes.data())\n\n\n        # print(\"x0 edge index is:\",x0.edge_index)\n        \n        #-----------------------------------------------------------------------\n\n\n        y2 = model.predict(x0, device)\n\n\n        # this part is added:\n        #-----------------------------------------------------------------------\n\n\n        print(\"the ground truth y0 is:\", y0.item())\n        print(\"the model predict y1 is:\", y1.item())\n        print(\"\\n-----------------------------------------------------------------------------------\")\n\n        '''\n        please note here that y0 is the ground truth label for graph x0, y1 is the model prediction for x0 without any add-ons,\n        y2 is the model prediction for x0 when nodes are inserted but no edge purturbations are in place\n        we commence edge attack only when y2 != y0\n        '''\n\n        \n\n\n\n        #-----------------------------------------------------------------------\n\n        num_nodes = x0.num_nodes\n        space = num_nodes * (num_nodes - 1) \/ 2\n\n        # model predicts correctly before node injection\n        if y0 == y1: \n          # after injection, prediction is already wrong\n          if y0 != y2: \n            num_success_via_injection += 1\n            num_success += 1\n            print(\"instance {} is successfully attacked via node injection.\".format(i))\n\n          # after injection, prediction y2 is still correct\n          elif y0 == y2: \n            time_start = time()\n            # attack the model\n            adv_x0, adv_y0, query, success, dis, init, is_injected_nodes_dangling = attacker.attack_untargeted(x0, y0, num_injected, query_limit=args.max_query)\n            \n            print(\"before adv attack, the model predicts:\", y0.item())\n            print(\"after adv attack, the model predicts:\", adv_y0.item())\n            \n            if is_injected_nodes_dangling:\n              Dangling_injection_count += 1\n\n            # if successfully changed the prediction\n            if y0 != adv_y0:\n                num_success += 1\n            \n            time_end = time()\n            init_num_query.append(init[2])\n            num_query.append(query)\n            init_attack_time.append(init[3])\n            attack_time.append(time_end-time_start)\n            if success:\n                #process results in Stage 1\n                init_perturb, init_dis, init_query, init_time, s_type = init\n                init_ratio = init_perturb \/ space\n                init_perturbation.append(init_perturb)\n                init_distortion.append(init_dis)\n                search_type.append(s_type)\n                init_perturbation_ratio.append(init_ratio)\n\n                #process results in Stage 2\n                perturb = distance(adv_x0, x0)\n                perturbation.append(perturb)\n                perturbation_ratio.append(perturb\/space)\n                distortion.append(dis)\n                \n                add_edge, delete_edge = count_edges(adv_x0, x0)\n                num_delete_edge.append(delete_edge)\n                num_add_edge.append(add_edge)\n\n                #test dataset for defense\n                #x0.y = torch.tensor([0])\n                #adv_x0.y = torch.tensor([1])\n                adv_x0.y = x0.y\n                detect_test_advers.append(adv_x0)\n                detect_test_normal.append(x0)\n            else:\n                detect_test_advers.append(x0)\n                detect_test_normal.append(x0)\n                init_distortion.append(-1)\n                init_perturbation.append(-1)\n                init_perturbation_ratio.append(-1)\n                search_type.append(-1)\n\n                perturbation.append(-1)\n                perturbation_ratio.append(-1)\n                distortion.append(-1) \n\n        # y0 != y1 the model prediction is wrong\n        else: \n            print('instance {} is wrongly classified, No Need to Attack'.format(i))\n            no_need_count += 1\n            num_query.append(0)\n            attack_time.append(0)\n            perturbation.append(0)\n            perturbation_ratio.append(0)\n            distortion.append(0)\n            \n            init_perturbation.append(0)\n            init_distortion.append(0)\n            init_num_query.append(0)\n            init_attack_time.append(0)\n            search_type.append(0)\n            init_perturbation_ratio.append(0)\n\n    \n        # this part is changed:\n        # -----------------------------------------------------------------------\n        print(f\"\\nattack loop: \\\n        \\n\\t {num_success} out of {i+1 - no_need_count} instance success rate, \\\n        \\n\\t {Dangling_injection_count} out of {num_success} has dangling injected nodes \\\n        \\n\\t {no_need_count} instances need no attack, \\\n        \\n\\t {num_success_via_injection} instances successed with only node injection\")\n    \n    \n\n\n    success_ratio = num_success \/ (num_test - no_need_count)*100\n    avg_perturbation = sum(perturbation) \/ num_success\n    print(\"Sign-Opt: the success rate of black-box attack is {}\/{} = {:.4f}\".format(num_success,num_test-no_need_count, success_ratio))\n    print('Sign-Opt: the average perturbation is {:.4f}'.format(avg_perturbation))\n    print('Sign-Opt: the average perturbation ratio is {:.4f}'.format(sum(perturbation_ratio) \/ num_success*100))\n    print('Sign-Opt: the average query count is {:.4f}'.format(sum(num_query)\/(num_test-no_need_count)))\n    print('Sign-Opt: the average attacking time is {:.4f}'.format(sum(attack_time)\/(num_test-no_need_count)))\n    print('Sign-Opt: the average distortion is {:.4f}'.format(sum(distortion)\/num_success))\n    print('dataset: {}'.format(dataset_name))\n\n    # -----------------------------------------------------------------------\n\n\n    if args.search == 1 and args.effective == 1 and args.id ==1: \n        detect_test_path = '.\/defense\/'+dataset_name+'_'+model_name+'_Our_'\n        torch.save(detect_test_normal, detect_test_path+'test_normal.pt')\n        torch.save(detect_test_advers, detect_test_path+'test_advers.pt')\n        print('test dataset for defense saved!')\n  \n    \n    init_path = '.\/out1\/init_{}_{}_{}_{}_'.format(dataset_name, args.id, args.effective , args.search)\n    with open(init_path+'search_type.txt', 'w') as f:\n        f.write(str(search_type))\n    with open(init_path+'P.txt', 'w') as f:\n        f.write(str(init_perturbation))\n    with open(init_path+'PR.txt', 'w') as f:\n        f.write(str(init_perturbation_ratio))\n    with open(init_path+'D.txt', 'w') as f:\n        f.write(str(init_distortion))\n    with open(init_path+'Q.txt', 'w') as f:\n        f.write(str(init_num_query))\n    with open(init_path+'T.txt', 'w') as f:\n        f.write(str(init_attack_time))  \n    \n    \n    our_path = '.\/out1\/our_{}_{}_{}_{}_'.format(dataset_name, args.id, args.effective , args.search)\n    with open(our_path+'Q.txt', 'w') as f:\n        f.write(str(num_query))\n    with open(our_path+'T.txt', 'w') as f:\n        f.write(str(attack_time))\n    with open(our_path+'P.txt', 'w') as f:\n        f.write(str(perturbation))\n    with open(our_path+'PR.txt', 'w') as f:\n        f.write(str(perturbation_ratio))\n    with open(our_path+'D.txt', 'w') as f:\n        f.write(str(distortion))\n    with open(our_path+'ADD.txt', 'w') as f:\n        f.write(str(num_delete_edge))\n    with open(our_path+'DEL.txt', 'w') as f:\n        f.write(str(num_add_edge))\n            \n    print(\"the numbers of deleted edges are:\", num_delete_edge)\n    print(\"the numbers od added edges are:\", num_add_edge)\n    print(\"the average number of deleted edges for %s: %d\"%(dataset_name, float(sum(num_delete_edge)\/len(num_delete_edge))))\n    print(\"the average number of added edges for %s: %d\"%(dataset_name, float(sum(num_add_edge)\/len(num_add_edge))))\n    '''\n    out_path = '.\/out\/{}_Opt_{}.txt'.format(dataset_name, bound)  \n    with open(out_path, 'w') as f:\n        f.write('{} instances don\\'t need to be attacked\\n'.format(no_need_count))\n        f.write('Sign-Opt fails to attack {} instance\\n'.format(fail_count))\n        f.write(\"Sign-Opt: the success rate of black-box attack is {}\/{} = {:.4f}\\n\".format(success_count,num_test-no_need_count, success_ratio))\n        f.write('Sign-Opt: the average perturbation is {:.4f}\\n'.format(avg_perturbation))\n        f.write('Sign-Opt: the average perturbation ratio is {:.4f}\\n'.format(sum(perturbation_ratio) \/ success_count*100))\n        f.write('Sign-Opt: the average query count is {:.4f}\\n'.format(sum(num_query)\/(num_test-no_need_count)))\n        f.write('Sign-Opt: the average attacking time is {:.4f}\\n'.format(sum(attack_time)\/(num_test-no_need_count)))\n        f.write('Sign-Opt: the average distortion is {:.4f}\\n'.format(sum(distortion)\/success_count))\n        f.write('Sign-Opt: detail perturbation are: {}\\n'.format(perturbation))\n        f.write('Sign-Opt: detail perturbation ratio are: {}\\n'.format(perturbation_ratio))\n    '''\n    \n"}},"msg":"update injection function"}},"https:\/\/github.com\/bulaienTang\/Hard_Label_Black_Box_Attack_GNN":{"de78c1c168857eb6143de7660db56fac7f4b3128":{"url":"https:\/\/api.github.com\/repos\/bulaienTang\/Hard_Label_Black_Box_Attack_GNN\/commits\/de78c1c168857eb6143de7660db56fac7f4b3128","html_url":"https:\/\/github.com\/bulaienTang\/Hard_Label_Black_Box_Attack_GNN\/commit\/de78c1c168857eb6143de7660db56fac7f4b3128","message":"update injection function","sha":"de78c1c168857eb6143de7660db56fac7f4b3128","keyword":"function injection update","diff":"diff --git a\/tang\/Sign_OPT.py b\/tang\/Sign_OPT.py\nindex 21bb47a..a765fd9 100644\n--- a\/tang\/Sign_OPT.py\n+++ b\/tang\/Sign_OPT.py\n@@ -121,49 +121,49 @@ def initial_search(self,x0, y0):\n         final_theta = torch.clamp(final_theta, 0.0, 0.5)\n         search_type = -1       \n         \n-        # #inner cluster perturbation\n-        # for i in range(num_cluster):\n-        #     nodes = cluster[i]\n-        #     num_cluster_nodes = len(nodes)\n-        #     if num_cluster_nodes > 1:\n-        #         for j in range(10*num_cluster_nodes): #search initial directions \n-        #             theta = torch.normal(mean=torch.rand(1).item(),std=0.5,size=(num_cluster_nodes,num_cluster_nodes)).to(self.device)\n-        #             theta = torch.triu(theta, diagonal=1).to(device)\n-        #             G_new = new_graph(G, theta, index1=nodes)\n-        #             x_new.edge_index = from_networkx(G_new).to(device).edge_index.long()\n-        #             if model.predict(x_new, device) != y0:  #we find a direction\n-        #                 F_lbd = distance(x_new, x0)\n-        #                 if F_lbd < F_theta:\n-        #                     F_theta = F_lbd\n-        #                     flag_inner = 1\n-        #                     search_type = 0\n-        #                     for p in range(num_cluster_nodes-1):\n-        #                         for q in range(p+1, num_cluster_nodes):\n-        #                             final_theta[nodes[p], nodes[q]] = theta[p, q]\n-        #                             final_theta[nodes[q], nodes[p]] = theta[p, q]   \n-        #             num_query += 1   \n-\n-        # #perturbations between clusters\n-        # if (num_cluster > 1) and (flag_inner == 0):\n-        #     for i in range(num_cluster - 1):\n-        #         for j in range(i+1, num_cluster):\n-        #             nodes1, nodes2 = cluster[i], cluster[j]\n-        #             num_cluster_nodes1, num_cluster_nodes2 = len(nodes1), len(nodes2)\n-        #             for k in range(10*(num_cluster_nodes1+num_cluster_nodes2)):\n-        #                 theta = torch.normal(mean=torch.rand(1).item(), std=0.5, size=(num_cluster_nodes1,num_cluster_nodes2)).to(self.device)\n-        #                 G_new = new_graph(G, theta, nodes1, nodes2)\n-        #                 x_new.edge_index = from_networkx(G_new).to(device).edge_index.long()\n-        #                 if model.predict(x_new, device) != y0:\n-        #                     F_lbd = distance(x_new, x0)\n-        #                     if F_lbd < F_theta:  \n-        #                         F_theta = F_lbd\n-        #                         flag_outer = 1\n-        #                         search_type = 1\n-        #                         for p in range(num_cluster_nodes1):\n-        #                             for q in range(num_cluster_nodes2):\n-        #                                 final_theta[nodes1[p], nodes2[q]] = theta[p, q]     \n-        #                                 final_theta[nodes2[q], nodes1[p]] = theta[p, q]     \n-        #                 num_query += 1   \n+        #inner cluster perturbation\n+        for i in range(num_cluster):\n+            nodes = cluster[i]\n+            num_cluster_nodes = len(nodes)\n+            if num_cluster_nodes > 1:\n+                for j in range(10*num_cluster_nodes): #search initial directions \n+                    theta = torch.normal(mean=torch.rand(1).item(),std=0.5,size=(num_cluster_nodes,num_cluster_nodes)).to(self.device)\n+                    theta = torch.triu(theta, diagonal=1).to(device)\n+                    G_new = new_graph(G, theta, index1=nodes)\n+                    x_new.edge_index = from_networkx(G_new).to(device).edge_index.long()\n+                    if model.predict(x_new, device) != y0:  #we find a direction\n+                        F_lbd = distance(x_new, x0)\n+                        if F_lbd < F_theta:\n+                            F_theta = F_lbd\n+                            flag_inner = 1\n+                            search_type = 0\n+                            for p in range(num_cluster_nodes-1):\n+                                for q in range(p+1, num_cluster_nodes):\n+                                    final_theta[nodes[p], nodes[q]] = theta[p, q]\n+                                    final_theta[nodes[q], nodes[p]] = theta[p, q]   \n+                    num_query += 1   \n+\n+        #perturbations between clusters\n+        if (num_cluster > 1) and (flag_inner == 0):\n+            for i in range(num_cluster - 1):\n+                for j in range(i+1, num_cluster):\n+                    nodes1, nodes2 = cluster[i], cluster[j]\n+                    num_cluster_nodes1, num_cluster_nodes2 = len(nodes1), len(nodes2)\n+                    for k in range(10*(num_cluster_nodes1+num_cluster_nodes2)):\n+                        theta = torch.normal(mean=torch.rand(1).item(), std=0.5, size=(num_cluster_nodes1,num_cluster_nodes2)).to(self.device)\n+                        G_new = new_graph(G, theta, nodes1, nodes2)\n+                        x_new.edge_index = from_networkx(G_new).to(device).edge_index.long()\n+                        if model.predict(x_new, device) != y0:\n+                            F_lbd = distance(x_new, x0)\n+                            if F_lbd < F_theta:  \n+                                F_theta = F_lbd\n+                                flag_outer = 1\n+                                search_type = 1\n+                                for p in range(num_cluster_nodes1):\n+                                    for q in range(num_cluster_nodes2):\n+                                        final_theta[nodes1[p], nodes2[q]] = theta[p, q]     \n+                                        final_theta[nodes2[q], nodes1[p]] = theta[p, q]     \n+                        num_query += 1   \n         \n         #perturbations on the whole graph\n         if (flag_inner == 0) and (flag_outer == 0):\ndiff --git a\/tang\/test.py b\/tang\/test.py\nindex 2c74fe8..7fbe619 100644\n--- a\/tang\/test.py\n+++ b\/tang\/test.py\n@@ -13,33 +13,77 @@\n import torch_geometric.transforms as T\n from Gin import GIN, SAG, GUNet\n from time import time\n+from collections import Counter\n \n # this part is added:\n #-----------------------------------------------------------------------\n-def inject_node(x, num_inject, initialization=\"node_mean\", Gaussian_mean=0, Gaussian_std=1):\n-    # exception check\n-    print(f\"Initialization: {initialization}\")\n-    assert initialization!=\"random\" or (initialization==\"random\" and Gaussian_mean and Gaussian_std)\n+def my_mode(sample):\n+    c = Counter(sample)\n+    return [k for k, v in c.items() if v == c.most_common(1)[0][1]][0]\n+\n+\n+def inject_node(x, num_inject, initialization, connection):\n+\n     node_feature_dim = x.x.shape[1]\n     injected_feature = torch.zeros(node_feature_dim)\n+    num_nodes_before_injection = x.num_nodes\n+\n     if initialization == \"zero\":\n         pass\n     elif initialization == \"one\":\n         injected_feature = torch.ones(node_feature_dim)\n     elif initialization == \"random\":\n-        injected_feature = torch.empty(node_feature_dim).normal_(mean=Gaussian_mean, std=Gaussian_std)\n+        Gaussian_mean = torch.mean(x.x, dim=0)\n+        Gaussian_std = torch.std(x.x,dim=0)\n+        # injected_feature = torch.empty(node_feature_dim).normal_(mean=Gaussian_mean, std=Gaussian_std)\n+        injected_feature = torch.empty(node_feature_dim)\n+        injected_feature[0].normal_(mean=Gaussian_mean[0].item(), std=Gaussian_std[0].item())\n+        injected_feature[1].normal_(mean=Gaussian_mean[1].item(), std=Gaussian_std[1].item())\n+        # print(injected_feature)\n     elif initialization == \"node_mean\":\n         injected_feature = torch.mean(x.x, dim=0)\n     else:\n         print(f\"Unsupported Initialization method: {initialization}\")\n         exit()\n     # inject new nodes into x\n-    x.x = torch.cat((x.x, torch.tensor(np.array([injected_feature.cpu().numpy() for i in range(num_inject)])).cuda()))\n+    x.x = torch.cat((x.x, torch.tensor([injected_feature.cpu().numpy() for i in range(num_inject)]).cuda()))\n     x.num_nodes = len(x.x)\n-    return x\n \n+    # connect new nodes into x\n+    if(connection == \"no_connection\"):\n+        pass\n+    elif(connection == \"random\"):\n+        for i in range(num_inject):\n+            node_number = i+num_nodes_before_injection\n+            random_node = random.randint(0,num_nodes_before_injection-1)\n+\n+            new_edge_back = torch.tensor([[node_number],[random_node]]).cuda()\n+            new_edge_front = torch.tensor([[random_node],[node_number]]).cuda()\n \n+            x.edge_index = torch.cat((new_edge_front, x.edge_index, new_edge_back),1)\n+            if(x.edge_attr is not None):\n+                x.edge_attr = torch.cat((x.edge_attr, torch.tensor([[0.,1.],[0.,1.]]).cuda()),0)\n+            \n+    elif(connection == \"mode\"):\n+        for i in range(num_inject):\n+            node_number = i+num_nodes_before_injection\n+            mode_node = my_mode(x.edge_index[0].tolist())\n+\n+            new_edge_back = torch.tensor([[node_number],[mode_node]]).cuda()\n+            new_edge_front = torch.tensor([[mode_node],[node_number]]).cuda()\n \n+            x.edge_index = torch.cat((new_edge_front, x.edge_index, new_edge_back),1)\n+\n+            if(x.edge_attr is not None):\n+                x.edge_attr = torch.cat((x.edge_attr, torch.tensor([[0.,1.],[0.,1.]]).cuda()),0)\n+    else:\n+        print(f\"Unsupported Connection method: {connection}\")\n+        exit()\n+\n+    x.num_edges = x.num_edges + num_inject\n+\n+    \n+    return x\n \n \n #-----------------------------------------------------------------------\n@@ -51,8 +95,10 @@ def get_args():\n \n     # this part is added:\n     #-----------------------------------------------------------------------\n-    parser.add_argument('--initialization', type=str, default='node_mean')\n-    parser.add_argument('--injection_percentage', type=float, default='0.1')\n+    parser.add_argument('--initialization', type=str, default='zero')\n+    parser.add_argument('--injection_percentage', type=float, default='0')\n+    parser.add_argument('--injection_number', type=int, default='0')\n+    parser.add_argument('--connection', type=str, default='no_connection')\n     \n     #-----------------------------------------------------------------------\n \n@@ -96,6 +142,10 @@ def count_edges(x_adv, x):\n     dropout = args.dropout\n     model_path = args.model_path\n     model_name = args.model\n+    injection_number = args.injection_number\n+    injection_percentage = args.injection_percentage\n+    initialization = args.initialization\n+    connection = args.connection\n     \n     if dataset_name in TUD.keys():\n         degree_as_attr = TUD[dataset_name]\n@@ -123,7 +173,6 @@ def count_edges(x_adv, x):\n     elif model_name=='GIN':\n         model = GIN(5,2,input_dim,hidden_dim,output_dim,dropout).to(device)\n         load_path = model_path + '{}_{}.pt'.format(dataset_name, model_name)\n-        # load_path = model_path + '{}.pt'.format(dataset_name)\n     elif model_name=='GUN':\n         model = GUNet(input_dim,hidden_dim,output_dim,0.8,3,dropout).to(device)\n         load_path = model_path + '{}_{}.pt'.format(dataset_name, model_name)\n@@ -167,42 +216,34 @@ def count_edges(x_adv, x):\n         print('begin to attack instance {}'.format(i))\n         x0 = test_dataset[i].to(device)\n \n-\n-        # this part is added:\n-        #-----------------------------------------------------------------------\n-        # print(\"\\n \\n \\n \\n \\n\")\n         print(\"---------------------------instance\",i,\"basic info-----------------------------------\")\n         print(\"x0 before node injection is:\", x0)\n         print(\"number of nodes before: \", len(x0.x))\n-        # print(\"the information in x0 is:\", x0.x)\n \n         G0 = to_networkx(x0, to_undirected=True)\n-        # print(\"nodes before injection:\",list(G0.nodes))\n         print(\"edges before injection:\",list(G0.edges))\n-        # print(\"the information in G0 is:\", G0.nodes.data())\n-\n-\n \n-        #-----------------------------------------------------------------------\n         y0 = x0.y[0]\n         y1 = model.predict(x0, device)\n-        #-----------------------------------------------------------------------\n \n+        # inject node\n+        if injection_number == 0 and injection_percentage == 0:\n+            pass\n+        # if num_inject not specified, then use percentage\n+        elif injection_number == 0 and injection_percentage != 0:\n+            num_injected = max(1, int(x0.num_nodes*args.injection_percentage))\n+            x0 = inject_node(x0, initialization=initialization, num_inject=num_injected, connection = connection)\n+        elif injection_number != 0 and injection_percentage == 0:\n+            num_injected = injection_number\n+            x0 = inject_node(x0, initialization=initialization, num_inject = injection_number, connection = connection)\n+        else:\n+            print(\"Cannot have mixed specifications of injection_number and injection_percentage!\")\n+            exit()\n \n \n-        # inject node\n-        num_injected = max(1, int(x0.num_nodes*args.injection_percentage))\n-        x0 = inject_node(x0, initialization=args.initialization, num_inject=num_injected)\n         print(\"x0 after node injection is:\", x0)\n         print(\"number of nodes after: \", len(x0.x))\n         G1 = to_networkx(x0, to_undirected=True)\n-        # print(\"the information in x0 is:\", x0.x)\n-        # print(\"nodes after injection:\",list(G1.nodes))\n-        # print(\"edges after injection:\",list(G1.edges))\n-        # print(\"the information in G1 is:\", G1.nodes.data())\n-\n-\n-        # print(\"x0 edge index is:\",x0.edge_index)\n         \n         #-----------------------------------------------------------------------\n \n@@ -216,6 +257,7 @@ def count_edges(x_adv, x):\n \n         print(\"the ground truth y0 is:\", y0.item())\n         print(\"the model predict y1 is:\", y1.item())\n+        print(\"the prediction after injection y2 is:\", y2.item())\n         print(\"\\n-----------------------------------------------------------------------------------\")\n \n         '''\n@@ -328,9 +370,9 @@ def count_edges(x_adv, x):\n     \n \n \n-    success_ratio = num_success \/ (num_test - no_need_count)*100\n+    success_ratio = (num_success + num_success_via_injection) \/ (num_test - no_need_count)*100\n     avg_perturbation = sum(perturbation) \/ num_success\n-    print(\"Sign-Opt: the success rate of black-box attack is {}\/{} = {:.4f}\".format(num_success,num_test-no_need_count, success_ratio))\n+    print(\"Sign-Opt: the success rate of black-box attack is {}\/{} = {:.4f}\".format(num_success + num_success_via_injection, num_test-no_need_count, success_ratio))\n     print('Sign-Opt: the average perturbation is {:.4f}'.format(avg_perturbation))\n     print('Sign-Opt: the average perturbation ratio is {:.4f}'.format(sum(perturbation_ratio) \/ num_success*100))\n     print('Sign-Opt: the average query count is {:.4f}'.format(sum(num_query)\/(num_test-no_need_count)))\n","files":{"\/tang\/test.py":{"changes":[{"diff":"\n import torch_geometric.transforms as T\n from Gin import GIN, SAG, GUNet\n from time import time\n+from collections import Counter\n \n # this part is added:\n #-----------------------------------------------------------------------\n-def inject_node(x, num_inject, initialization=\"node_mean\", Gaussian_mean=0, Gaussian_std=1):\n-    # exception check\n-    print(f\"Initialization: {initialization}\")\n-    assert initialization!=\"random\" or (initialization==\"random\" and Gaussian_mean and Gaussian_std)\n+def my_mode(sample):\n+    c = Counter(sample)\n+    return [k for k, v in c.items() if v == c.most_common(1)[0][1]][0]\n+\n+\n+def inject_node(x, num_inject, initialization, connection):\n+\n     node_feature_dim = x.x.shape[1]\n     injected_feature = torch.zeros(node_feature_dim)\n+    num_nodes_before_injection = x.num_nodes\n+\n     if initialization == \"zero\":\n         pass\n     elif initialization == \"one\":\n         injected_feature = torch.ones(node_feature_dim)\n     elif initialization == \"random\":\n-        injected_feature = torch.empty(node_feature_dim).normal_(mean=Gaussian_mean, std=Gaussian_std)\n+        Gaussian_mean = torch.mean(x.x, dim=0)\n+        Gaussian_std = torch.std(x.x,dim=0)\n+        # injected_feature = torch.empty(node_feature_dim).normal_(mean=Gaussian_mean, std=Gaussian_std)\n+        injected_feature = torch.empty(node_feature_dim)\n+        injected_feature[0].normal_(mean=Gaussian_mean[0].item(), std=Gaussian_std[0].item())\n+        injected_feature[1].normal_(mean=Gaussian_mean[1].item(), std=Gaussian_std[1].item())\n+        # print(injected_feature)\n     elif initialization == \"node_mean\":\n         injected_feature = torch.mean(x.x, dim=0)\n     else:\n         print(f\"Unsupported Initialization method: {initialization}\")\n         exit()\n     # inject new nodes into x\n-    x.x = torch.cat((x.x, torch.tensor(np.array([injected_feature.cpu().numpy() for i in range(num_inject)])).cuda()))\n+    x.x = torch.cat((x.x, torch.tensor([injected_feature.cpu().numpy() for i in range(num_inject)]).cuda()))\n     x.num_nodes = len(x.x)\n-    return x\n \n+    # connect new nodes into x\n+    if(connection == \"no_connection\"):\n+        pass\n+    elif(connection == \"random\"):\n+        for i in range(num_inject):\n+            node_number = i+num_nodes_before_injection\n+            random_node = random.randint(0,num_nodes_before_injection-1)\n+\n+            new_edge_back = torch.tensor([[node_number],[random_node]]).cuda()\n+            new_edge_front = torch.tensor([[random_node],[node_number]]).cuda()\n \n+            x.edge_index = torch.cat((new_edge_front, x.edge_index, new_edge_back),1)\n+            if(x.edge_attr is not None):\n+                x.edge_attr = torch.cat((x.edge_attr, torch.tensor([[0.,1.],[0.,1.]]).cuda()),0)\n+            \n+    elif(connection == \"mode\"):\n+        for i in range(num_inject):\n+            node_number = i+num_nodes_before_injection\n+            mode_node = my_mode(x.edge_index[0].tolist())\n+\n+            new_edge_back = torch.tensor([[node_number],[mode_node]]).cuda()\n+            new_edge_front = torch.tensor([[mode_node],[node_number]]).cuda()\n \n+            x.edge_index = torch.cat((new_edge_front, x.edge_index, new_edge_back),1)\n+\n+            if(x.edge_attr is not None):\n+                x.edge_attr = torch.cat((x.edge_attr, torch.tensor([[0.,1.],[0.,1.]]).cuda()),0)\n+    else:\n+        print(f\"Unsupported Connection method: {connection}\")\n+        exit()\n+\n+    x.num_edges = x.num_edges + num_inject\n+\n+    \n+    return x\n \n \n #-----------------------------------------------------------------------\n","add":51,"remove":7,"filename":"\/tang\/test.py","badparts":["def inject_node(x, num_inject, initialization=\"node_mean\", Gaussian_mean=0, Gaussian_std=1):","    print(f\"Initialization: {initialization}\")","    assert initialization!=\"random\" or (initialization==\"random\" and Gaussian_mean and Gaussian_std)","        injected_feature = torch.empty(node_feature_dim).normal_(mean=Gaussian_mean, std=Gaussian_std)","    x.x = torch.cat((x.x, torch.tensor(np.array([injected_feature.cpu().numpy() for i in range(num_inject)])).cuda()))","    return x"],"goodparts":["from collections import Counter","def my_mode(sample):","    c = Counter(sample)","    return [k for k, v in c.items() if v == c.most_common(1)[0][1]][0]","def inject_node(x, num_inject, initialization, connection):","    num_nodes_before_injection = x.num_nodes","        Gaussian_mean = torch.mean(x.x, dim=0)","        Gaussian_std = torch.std(x.x,dim=0)","        injected_feature = torch.empty(node_feature_dim)","        injected_feature[0].normal_(mean=Gaussian_mean[0].item(), std=Gaussian_std[0].item())","        injected_feature[1].normal_(mean=Gaussian_mean[1].item(), std=Gaussian_std[1].item())","    x.x = torch.cat((x.x, torch.tensor([injected_feature.cpu().numpy() for i in range(num_inject)]).cuda()))","    if(connection == \"no_connection\"):","        pass","    elif(connection == \"random\"):","        for i in range(num_inject):","            node_number = i+num_nodes_before_injection","            random_node = random.randint(0,num_nodes_before_injection-1)","            new_edge_back = torch.tensor([[node_number],[random_node]]).cuda()","            new_edge_front = torch.tensor([[random_node],[node_number]]).cuda()","            x.edge_index = torch.cat((new_edge_front, x.edge_index, new_edge_back),1)","            if(x.edge_attr is not None):","                x.edge_attr = torch.cat((x.edge_attr, torch.tensor([[0.,1.],[0.,1.]]).cuda()),0)","    elif(connection == \"mode\"):","        for i in range(num_inject):","            node_number = i+num_nodes_before_injection","            mode_node = my_mode(x.edge_index[0].tolist())","            new_edge_back = torch.tensor([[node_number],[mode_node]]).cuda()","            new_edge_front = torch.tensor([[mode_node],[node_number]]).cuda()","            x.edge_index = torch.cat((new_edge_front, x.edge_index, new_edge_back),1)","            if(x.edge_attr is not None):","                x.edge_attr = torch.cat((x.edge_attr, torch.tensor([[0.,1.],[0.,1.]]).cuda()),0)","    else:","        print(f\"Unsupported Connection method: {connection}\")","        exit()","    x.num_edges = x.num_edges + num_inject","    return x"]},{"diff":"\n \n     # this part is added:\n     #-----------------------------------------------------------------------\n-    parser.add_argument('--initialization', type=str, default='node_mean')\n-    parser.add_argument('--injection_percentage', type=float, default='0.1')\n+    parser.add_argument('--initialization', type=str, default='zero')\n+    parser.add_argument('--injection_percentage', type=float, default='0')\n+    parser.add_argument('--injection_number', type=int, default='0')\n+    parser.add_argument('--connection', type=str, default='no_connection')\n     \n     #-----------------------------------------------------------------------\n \n","add":4,"remove":2,"filename":"\/tang\/test.py","badparts":["    parser.add_argument('--initialization', type=str, default='node_mean')","    parser.add_argument('--injection_percentage', type=float, default='0.1')"],"goodparts":["    parser.add_argument('--initialization', type=str, default='zero')","    parser.add_argument('--injection_percentage', type=float, default='0')","    parser.add_argument('--injection_number', type=int, default='0')","    parser.add_argument('--connection', type=str, default='no_connection')"]},{"diff":"\n         print('begin to attack instance {}'.format(i))\n         x0 = test_dataset[i].to(device)\n \n-\n-        # this part is added:\n-        #-----------------------------------------------------------------------\n-        # print(\"\\n \\n \\n \\n \\n\")\n         print(\"---------------------------instance\",i,\"basic info-----------------------------------\")\n         print(\"x0 before node injection is:\", x0)\n         print(\"number of nodes before: \", len(x0.x))\n-        # print(\"the information in x0 is:\", x0.x)\n \n         G0 = to_networkx(x0, to_undirected=True)\n-        # print(\"nodes before injection:\",list(G0.nodes))\n         print(\"edges before injection:\",list(G0.edges))\n-        # print(\"the information in G0 is:\", G0.nodes.data())\n-\n-\n \n-        #-----------------------------------------------------------------------\n         y0 = x0.y[0]\n         y1 = model.predict(x0, device)\n-        #-----------------------------------------------------------------------\n \n+        # inject node\n+        if injection_number == 0 and injection_percentage == 0:\n+            pass\n+        # if num_inject not specified, then use percentage\n+        elif injection_number == 0 and injection_percentage != 0:\n+            num_injected = max(1, int(x0.num_nodes*args.injection_percentage))\n+            x0 = inject_node(x0, initialization=initialization, num_inject=num_injected, connection = connection)\n+        elif injection_number != 0 and injection_percentage == 0:\n+            num_injected = injection_number\n+            x0 = inject_node(x0, initialization=initialization, num_inject = injection_number, connection = connection)\n+        else:\n+            print(\"Cannot have mixed specifications of injection_number and injection_percentage!\")\n+            exit()\n \n \n-        # inject node\n-        num_injected = max(1, int(x0.num_nodes*args.injection_percentage))\n-        x0 = inject_node(x0, initialization=args.initialization, num_inject=num_injected)\n         print(\"x0 after node injection is:\", x0)\n         print(\"number of nodes after: \", len(x0.x))\n         G1 = to_networkx(x0, to_undirected=True)\n-        # print(\"the information in x0 is:\", x0.x)\n-        # print(\"nodes after injection:\",list(G1.nodes))\n-        # print(\"edges after injection:\",list(G1.edges))\n-        # print(\"the information in G1 is:\", G1.nodes.data())\n-\n-\n-        # print(\"x0 edge index is:\",x0.edge_index)\n         \n         #-----------------------------------------------------------------------\n \n","add":13,"remove":21,"filename":"\/tang\/test.py","badparts":["        num_injected = max(1, int(x0.num_nodes*args.injection_percentage))","        x0 = inject_node(x0, initialization=args.initialization, num_inject=num_injected)"],"goodparts":["        if injection_number == 0 and injection_percentage == 0:","            pass","        elif injection_number == 0 and injection_percentage != 0:","            num_injected = max(1, int(x0.num_nodes*args.injection_percentage))","            x0 = inject_node(x0, initialization=initialization, num_inject=num_injected, connection = connection)","        elif injection_number != 0 and injection_percentage == 0:","            num_injected = injection_number","            x0 = inject_node(x0, initialization=initialization, num_inject = injection_number, connection = connection)","        else:","            print(\"Cannot have mixed specifications of injection_number and injection_percentage!\")","            exit()"]},{"diff":"\n     \n \n \n-    success_ratio = num_success \/ (num_test - no_need_count)*100\n+    success_ratio = (num_success + num_success_via_injection) \/ (num_test - no_need_count)*100\n     avg_perturbation = sum(perturbation) \/ num_success\n-    print(\"Sign-Opt: the success rate of black-box attack is {}\/{} = {:.4f}\".format(num_success,num_test-no_need_count, success_ratio))\n+    print(\"Sign-Opt: the success rate of black-box attack is {}\/{} = {:.4f}\".format(num_success + num_success_via_injection, num_test-no_need_count, success_ratio))\n     print('Sign-Opt: the average perturbation is {:.4f}'.format(avg_perturbation))\n     print('Sign-Opt: the average perturbation ratio is {:.4f}'.format(sum(perturbation_ratio) \/ num_success*100))\n     print('Sign-Opt: the average query count is {:.4f}'.format(sum(num_query)\/(num_test-no_need_count)))\n","add":2,"remove":2,"filename":"\/tang\/test.py","badparts":["    success_ratio = num_success \/ (num_test - no_need_count)*100","    print(\"Sign-Opt: the success rate of black-box attack is {}\/{} = {:.4f}\".format(num_success,num_test-no_need_count, success_ratio))"],"goodparts":["    success_ratio = (num_success + num_success_via_injection) \/ (num_test - no_need_count)*100","    print(\"Sign-Opt: the success rate of black-box attack is {}\/{} = {:.4f}\".format(num_success + num_success_via_injection, num_test-no_need_count, success_ratio))"]}],"source":"\nimport torch import torch_geometric import community.community_louvain import numpy as np import networkx import argparse from torch_geometric.datasets import TUDataset from torch_geometric.data import DataLoader from torch_geometric.data import Batch from Sign_OPT import * from torch_geometric.utils import to_networkx, from_networkx import torch_geometric.transforms as T from Gin import GIN, SAG, GUNet from time import time def inject_node(x, num_inject, initialization=\"node_mean\", Gaussian_mean=0, Gaussian_std=1): print(f\"Initialization:{initialization}\") assert initialization!=\"random\" or(initialization==\"random\" and Gaussian_mean and Gaussian_std) node_feature_dim=x.x.shape[1] injected_feature=torch.zeros(node_feature_dim) if initialization==\"zero\": pass elif initialization==\"one\": injected_feature=torch.ones(node_feature_dim) elif initialization==\"random\": injected_feature=torch.empty(node_feature_dim).normal_(mean=Gaussian_mean, std=Gaussian_std) elif initialization==\"node_mean\": injected_feature=torch.mean(x.x, dim=0) else: print(f\"Unsupported Initialization method:{initialization}\") exit() x.x=torch.cat((x.x, torch.tensor(np.array([injected_feature.cpu().numpy() for i in range(num_inject)])).cuda())) x.num_nodes=len(x.x) return x def get_args(): parser=argparse.ArgumentParser(description='Pytorch graph isomorphism network for graph classification') parser.add_argument('--initialization', type=str, default='node_mean') parser.add_argument('--injection_percentage', type=float, default='0.1') parser.add_argument('--effective', type=int, default=1) parser.add_argument('--max_query', type=int, default=40000) parser.add_argument('--id', type=int, default=1) parser.add_argument('--search', type=int, default=1) parser.add_argument('--dataset', type=str, default=\"IMDB-BINARY\") parser.add_argument('--device', type=int, default=0) parser.add_argument('--batch_size', type=int, default=32, help='social dataset:64 bio dataset:32') parser.add_argument('--hidden_dim', type=int, default=64) parser.add_argument('--dropout', type=float, default=0.5) parser.add_argument('--model_path', type=str, default='.\/trained_model\/') parser.add_argument('--model', type=str, default='GUN') args=parser.parse_args() return args def distance(x_adv, x): adj_adv=nx.adjacency_matrix(to_networkx(x_adv, to_undirected=True)) adj_x=nx.adjacency_matrix(to_networkx(x, to_undirected=True)) return np.sum(np.abs(adj_adv-adj_x)) \/ 2 def count_edges(x_adv, x): adj_adv=nx.adjacency_matrix(to_networkx(x_adv, to_undirected=True)).todense().A adj_x=nx.adjacency_matrix(to_networkx(x, to_undirected=True)).todense().A difference=adj_adv -adj_x num_add=sum(sum(difference==1)) \/ 2 num_delete=sum(sum(difference==-1)) \/ 2 return num_add, num_delete TUD={'NCI1':0,'COIL-DEL':0,'IMDB-BINARY':1} if __name__=='__main__': args=get_args() dataset_name=args.dataset device=torch.device(\"cuda:\"+str(args.device) if torch.cuda.is_available() else torch.device(\"cpu\")) batch_size=args.batch_size hidden_dim=args.hidden_dim dropout=args.dropout model_path=args.model_path model_name=args.model if dataset_name in TUD.keys(): degree_as_attr=TUD[dataset_name] else: print('invalid dataset!') raise(ValueError) if degree_as_attr: dataset=TUDataset(root='.\/dataset',name=dataset_name,use_edge_attr='False', use_node_attr=True, pre_transform=T.Constant(1, True)) else: dataset=TUDataset(root='.\/dataset',name=dataset_name,use_edge_attr='False',use_node_attr=True) index_path='.\/data_split\/' +dataset_name +'_' with open(index_path+'test_index.txt', 'r') as f: test_index=eval(f.read()) test_dataset=dataset[test_index] input_dim=dataset.num_node_features output_dim=dataset.num_classes print('input dim: ', input_dim) print('output dim: ', output_dim) if model_name=='SAG': model=SAG(5,input_dim,hidden_dim,output_dim,0.8,dropout).to(device) load_path=model_path +'{}_{}.pt'.format(dataset_name, model_name) elif model_name=='GIN': model=GIN(5,2,input_dim,hidden_dim,output_dim,dropout).to(device) load_path=model_path +'{}_{}.pt'.format(dataset_name, model_name) elif model_name=='GUN': model=GUNet(input_dim,hidden_dim,output_dim,0.8,3,dropout).to(device) load_path=model_path +'{}_{}.pt'.format(dataset_name, model_name) model.load_state_dict(torch.load(load_path, map_location=device)) model.eval() attacker=OPT_attack_sign_SGD(model, device, args.effective) num_test=len(test_dataset) perturbation=[] perturbation_ratio=[] Dangling_injection_count=0 no_need_count=0 num_query=[] fail_count=0 distortion=[] attack_time=[] init_perturbation=[] init_perturbation_ratio=[] init_num_query=[] init_distortion=[] init_attack_time=[] search_type=[] detect_test_normal=[] detect_test_advers=[] num_success=0 num_success_via_injection=0 num_add_edge, num_delete_edge=[],[] for i in range(num_test): print(\"\\n \\n \\n \\n \\n \\n \\n \\n\") print('begin to attack instance{}'.format(i)) x0=test_dataset[i].to(device) print(\"---------------------------instance\",i,\"basic info-----------------------------------\") print(\"x0 before node injection is:\", x0) print(\"number of nodes before: \", len(x0.x)) G0=to_networkx(x0, to_undirected=True) print(\"edges before injection:\",list(G0.edges)) y0=x0.y[0] y1=model.predict(x0, device) num_injected=max(1, int(x0.num_nodes*args.injection_percentage)) x0=inject_node(x0, initialization=args.initialization, num_inject=num_injected) print(\"x0 after node injection is:\", x0) print(\"number of nodes after: \", len(x0.x)) G1=to_networkx(x0, to_undirected=True) y2=model.predict(x0, device) print(\"the ground truth y0 is:\", y0.item()) print(\"the model predict y1 is:\", y1.item()) print(\"\\n-----------------------------------------------------------------------------------\") ''' please note here that y0 is the ground truth label for graph x0, y1 is the model prediction for x0 without any add-ons, y2 is the model prediction for x0 when nodes are inserted but no edge purturbations are in place we commence edge attack only when y2 !=y0 ''' num_nodes=x0.num_nodes space=num_nodes *(num_nodes -1) \/ 2 if y0==y1: if y0 !=y2: num_success_via_injection +=1 num_success +=1 print(\"instance{} is successfully attacked via node injection.\".format(i)) elif y0==y2: time_start=time() adv_x0, adv_y0, query, success, dis, init, is_injected_nodes_dangling=attacker.attack_untargeted(x0, y0, num_injected, query_limit=args.max_query) print(\"before adv attack, the model predicts:\", y0.item()) print(\"after adv attack, the model predicts:\", adv_y0.item()) if is_injected_nodes_dangling: Dangling_injection_count +=1 if y0 !=adv_y0: num_success +=1 time_end=time() init_num_query.append(init[2]) num_query.append(query) init_attack_time.append(init[3]) attack_time.append(time_end-time_start) if success: init_perturb, init_dis, init_query, init_time, s_type=init init_ratio=init_perturb \/ space init_perturbation.append(init_perturb) init_distortion.append(init_dis) search_type.append(s_type) init_perturbation_ratio.append(init_ratio) perturb=distance(adv_x0, x0) perturbation.append(perturb) perturbation_ratio.append(perturb\/space) distortion.append(dis) add_edge, delete_edge=count_edges(adv_x0, x0) num_delete_edge.append(delete_edge) num_add_edge.append(add_edge) adv_x0.y=x0.y detect_test_advers.append(adv_x0) detect_test_normal.append(x0) else: detect_test_advers.append(x0) detect_test_normal.append(x0) init_distortion.append(-1) init_perturbation.append(-1) init_perturbation_ratio.append(-1) search_type.append(-1) perturbation.append(-1) perturbation_ratio.append(-1) distortion.append(-1) else: print('instance{} is wrongly classified, No Need to Attack'.format(i)) no_need_count +=1 num_query.append(0) attack_time.append(0) perturbation.append(0) perturbation_ratio.append(0) distortion.append(0) init_perturbation.append(0) init_distortion.append(0) init_num_query.append(0) init_attack_time.append(0) search_type.append(0) init_perturbation_ratio.append(0) print(f\"\\nattack loop: \\ \\n\\t{num_success} out of{i+1 -no_need_count} instance success rate, \\ \\n\\t{Dangling_injection_count} out of{num_success} has dangling injected nodes \\ \\n\\t{no_need_count} instances need no attack, \\ \\n\\t{num_success_via_injection} instances successed with only node injection\") success_ratio=num_success \/(num_test -no_need_count)*100 avg_perturbation=sum(perturbation) \/ num_success print(\"Sign-Opt: the success rate of black-box attack is{}\/{}={:.4f}\".format(num_success,num_test-no_need_count, success_ratio)) print('Sign-Opt: the average perturbation is{:.4f}'.format(avg_perturbation)) print('Sign-Opt: the average perturbation ratio is{:.4f}'.format(sum(perturbation_ratio) \/ num_success*100)) print('Sign-Opt: the average query count is{:.4f}'.format(sum(num_query)\/(num_test-no_need_count))) print('Sign-Opt: the average attacking time is{:.4f}'.format(sum(attack_time)\/(num_test-no_need_count))) print('Sign-Opt: the average distortion is{:.4f}'.format(sum(distortion)\/num_success)) print('dataset:{}'.format(dataset_name)) if args.search==1 and args.effective==1 and args.id==1: detect_test_path='.\/defense\/'+dataset_name+'_'+model_name+'_Our_' torch.save(detect_test_normal, detect_test_path+'test_normal.pt') torch.save(detect_test_advers, detect_test_path+'test_advers.pt') print('test dataset for defense saved!') init_path='.\/out1\/init_{}_{}_{}_{}_'.format(dataset_name, args.id, args.effective, args.search) with open(init_path+'search_type.txt', 'w') as f: f.write(str(search_type)) with open(init_path+'P.txt', 'w') as f: f.write(str(init_perturbation)) with open(init_path+'PR.txt', 'w') as f: f.write(str(init_perturbation_ratio)) with open(init_path+'D.txt', 'w') as f: f.write(str(init_distortion)) with open(init_path+'Q.txt', 'w') as f: f.write(str(init_num_query)) with open(init_path+'T.txt', 'w') as f: f.write(str(init_attack_time)) our_path='.\/out1\/our_{}_{}_{}_{}_'.format(dataset_name, args.id, args.effective, args.search) with open(our_path+'Q.txt', 'w') as f: f.write(str(num_query)) with open(our_path+'T.txt', 'w') as f: f.write(str(attack_time)) with open(our_path+'P.txt', 'w') as f: f.write(str(perturbation)) with open(our_path+'PR.txt', 'w') as f: f.write(str(perturbation_ratio)) with open(our_path+'D.txt', 'w') as f: f.write(str(distortion)) with open(our_path+'ADD.txt', 'w') as f: f.write(str(num_delete_edge)) with open(our_path+'DEL.txt', 'w') as f: f.write(str(num_add_edge)) print(\"the numbers of deleted edges are:\", num_delete_edge) print(\"the numbers od added edges are:\", num_add_edge) print(\"the average number of deleted edges for %s: %d\"%(dataset_name, float(sum(num_delete_edge)\/len(num_delete_edge)))) print(\"the average number of added edges for %s: %d\"%(dataset_name, float(sum(num_add_edge)\/len(num_add_edge)))) ''' out_path='.\/out\/{}_Opt_{}.txt'.format(dataset_name, bound) with open(out_path, 'w') as f: f.write('{} instances don\\'t need to be attacked\\n'.format(no_need_count)) f.write('Sign-Opt fails to attack{} instance\\n'.format(fail_count)) f.write(\"Sign-Opt: the success rate of black-box attack is{}\/{}={:.4f}\\n\".format(success_count,num_test-no_need_count, success_ratio)) f.write('Sign-Opt: the average perturbation is{:.4f}\\n'.format(avg_perturbation)) f.write('Sign-Opt: the average perturbation ratio is{:.4f}\\n'.format(sum(perturbation_ratio) \/ success_count*100)) f.write('Sign-Opt: the average query count is{:.4f}\\n'.format(sum(num_query)\/(num_test-no_need_count))) f.write('Sign-Opt: the average attacking time is{:.4f}\\n'.format(sum(attack_time)\/(num_test-no_need_count))) f.write('Sign-Opt: the average distortion is{:.4f}\\n'.format(sum(distortion)\/success_count)) f.write('Sign-Opt: detail perturbation are:{}\\n'.format(perturbation)) f.write('Sign-Opt: detail perturbation ratio are:{}\\n'.format(perturbation_ratio)) ''' ","sourceWithComments":"import torch\nimport torch_geometric #torch_geometric == 1.6.1\n# import community\nimport community.community_louvain\nimport numpy as np\nimport networkx\nimport argparse\nfrom torch_geometric.datasets import TUDataset\nfrom torch_geometric.data import DataLoader\nfrom torch_geometric.data import Batch\nfrom Sign_OPT import *\nfrom torch_geometric.utils import to_networkx, from_networkx\nimport torch_geometric.transforms as T\nfrom Gin import GIN, SAG, GUNet\nfrom time import time\n\n# this part is added:\n#-----------------------------------------------------------------------\ndef inject_node(x, num_inject, initialization=\"node_mean\", Gaussian_mean=0, Gaussian_std=1):\n    # exception check\n    print(f\"Initialization: {initialization}\")\n    assert initialization!=\"random\" or (initialization==\"random\" and Gaussian_mean and Gaussian_std)\n    node_feature_dim = x.x.shape[1]\n    injected_feature = torch.zeros(node_feature_dim)\n    if initialization == \"zero\":\n        pass\n    elif initialization == \"one\":\n        injected_feature = torch.ones(node_feature_dim)\n    elif initialization == \"random\":\n        injected_feature = torch.empty(node_feature_dim).normal_(mean=Gaussian_mean, std=Gaussian_std)\n    elif initialization == \"node_mean\":\n        injected_feature = torch.mean(x.x, dim=0)\n    else:\n        print(f\"Unsupported Initialization method: {initialization}\")\n        exit()\n    # inject new nodes into x\n    x.x = torch.cat((x.x, torch.tensor(np.array([injected_feature.cpu().numpy() for i in range(num_inject)])).cuda()))\n    x.num_nodes = len(x.x)\n    return x\n\n\n\n\n\n#-----------------------------------------------------------------------\n\ndef get_args():\n    parser = argparse.ArgumentParser(description='Pytorch graph isomorphism network for graph classification')\n    #these are parameters for attack model\n\n\n    # this part is added:\n    #-----------------------------------------------------------------------\n    parser.add_argument('--initialization', type=str, default='node_mean')\n    parser.add_argument('--injection_percentage', type=float, default='0.1')\n    \n    #-----------------------------------------------------------------------\n\n    parser.add_argument('--effective', type=int, default=1)\n    parser.add_argument('--max_query', type=int, default=40000)\n    parser.add_argument('--id', type= int, default=1)\n    parser.add_argument('--search', type=int, default=1)\n    #these are parameters for GIN model\n    parser.add_argument('--dataset', type=str, default=\"IMDB-BINARY\")\n    parser.add_argument('--device', type=int, default=0)\n    parser.add_argument('--batch_size', type=int, default=32, help='social dataset:64 bio dataset:32')\n    parser.add_argument('--hidden_dim', type=int, default=64)\n    parser.add_argument('--dropout', type=float, default=0.5)\n    parser.add_argument('--model_path', type=str, default='.\/trained_model\/')\n    parser.add_argument('--model', type=str, default='GUN')\n    args = parser.parse_args()\n    return args\n\ndef distance(x_adv, x):\n    adj_adv = nx.adjacency_matrix(to_networkx(x_adv, to_undirected=True))\n    adj_x = nx.adjacency_matrix(to_networkx(x, to_undirected=True))\n    return np.sum(np.abs(adj_adv-adj_x)) \/ 2\n    \ndef count_edges(x_adv, x):\n    adj_adv = nx.adjacency_matrix(to_networkx(x_adv, to_undirected=True)).todense().A\n    adj_x = nx.adjacency_matrix(to_networkx(x, to_undirected=True)).todense().A\n    difference = adj_adv - adj_x\n    num_add = sum(sum(difference==1)) \/ 2\n    num_delete = sum(sum(difference==-1)) \/ 2\n    return num_add, num_delete\n\nTUD = {'NCI1':0,'COIL-DEL':0,'IMDB-BINARY':1}\n\nif __name__ == '__main__':\n\n    args = get_args()\n    dataset_name = args.dataset\n    device = torch.device(\"cuda:\"+str(args.device) if torch.cuda.is_available() else torch.device(\"cpu\"))\n    batch_size = args.batch_size\n    hidden_dim = args.hidden_dim\n    dropout = args.dropout\n    model_path = args.model_path\n    model_name = args.model\n    \n    if dataset_name in TUD.keys():\n        degree_as_attr = TUD[dataset_name]\n    else:\n        print('invalid dataset!')\n        raise(ValueError)\n\n    if degree_as_attr:\n        dataset = TUDataset(root='.\/dataset',name=dataset_name,use_edge_attr='False', use_node_attr=True,\n        pre_transform=T.Constant(1, True))\n    else:\n        dataset = TUDataset(root='.\/dataset',name=dataset_name,use_edge_attr='False',use_node_attr=True)\n    \n    index_path = '.\/data_split\/' + dataset_name + '_'\n    with open(index_path+'test_index.txt', 'r') as f:\n        test_index = eval(f.read())\n    test_dataset = dataset[test_index]\n    input_dim = dataset.num_node_features\n    output_dim = dataset.num_classes\n    print('input dim: ', input_dim)\n    print('output dim: ', output_dim)\n    if model_name=='SAG':\n        model = SAG(5,input_dim,hidden_dim,output_dim,0.8,dropout).to(device)\n        load_path = model_path + '{}_{}.pt'.format(dataset_name, model_name)\n    elif model_name=='GIN':\n        model = GIN(5,2,input_dim,hidden_dim,output_dim,dropout).to(device)\n        load_path = model_path + '{}_{}.pt'.format(dataset_name, model_name)\n        # load_path = model_path + '{}.pt'.format(dataset_name)\n    elif model_name=='GUN':\n        model = GUNet(input_dim,hidden_dim,output_dim,0.8,3,dropout).to(device)\n        load_path = model_path + '{}_{}.pt'.format(dataset_name, model_name)\n    model.load_state_dict(torch.load(load_path, map_location=device))\n    model.eval()\n\n    attacker = OPT_attack_sign_SGD(model, device, args.effective)\n    num_test = len(test_dataset)\n    perturbation = [] #perturbation for each poisoned graph\n    perturbation_ratio = [] #perturbation ratio for each poisoned graph\n\n    Dangling_injection_count = 0\n\n    no_need_count = 0\n    num_query = []    \n    fail_count = 0\n    distortion = []\n    attack_time = []\n\n    init_perturbation = [] #perturbation for each poisoned graph\n    init_perturbation_ratio = [] #perturbation ratio for each poisoned graph\n    init_num_query = []    \n    init_distortion = []\n    init_attack_time = []\n    search_type = []\n\n    detect_test_normal = []\n    detect_test_advers = []\n\n    # this part is added:\n    #-----------------------------------------------------------------------\n    num_success = 0\n    num_success_via_injection = 0\n    #-----------------------------------------------------------------------\n\n    num_add_edge, num_delete_edge = [], []\n    for i in range(num_test):\n\n        print(\"\\n \\n \\n \\n \\n \\n \\n \\n\")\n\n        print('begin to attack instance {}'.format(i))\n        x0 = test_dataset[i].to(device)\n\n\n        # this part is added:\n        #-----------------------------------------------------------------------\n        # print(\"\\n \\n \\n \\n \\n\")\n        print(\"---------------------------instance\",i,\"basic info-----------------------------------\")\n        print(\"x0 before node injection is:\", x0)\n        print(\"number of nodes before: \", len(x0.x))\n        # print(\"the information in x0 is:\", x0.x)\n\n        G0 = to_networkx(x0, to_undirected=True)\n        # print(\"nodes before injection:\",list(G0.nodes))\n        print(\"edges before injection:\",list(G0.edges))\n        # print(\"the information in G0 is:\", G0.nodes.data())\n\n\n\n        #-----------------------------------------------------------------------\n        y0 = x0.y[0]\n        y1 = model.predict(x0, device)\n        #-----------------------------------------------------------------------\n\n\n\n        # inject node\n        num_injected = max(1, int(x0.num_nodes*args.injection_percentage))\n        x0 = inject_node(x0, initialization=args.initialization, num_inject=num_injected)\n        print(\"x0 after node injection is:\", x0)\n        print(\"number of nodes after: \", len(x0.x))\n        G1 = to_networkx(x0, to_undirected=True)\n        # print(\"the information in x0 is:\", x0.x)\n        # print(\"nodes after injection:\",list(G1.nodes))\n        # print(\"edges after injection:\",list(G1.edges))\n        # print(\"the information in G1 is:\", G1.nodes.data())\n\n\n        # print(\"x0 edge index is:\",x0.edge_index)\n        \n        #-----------------------------------------------------------------------\n\n\n        y2 = model.predict(x0, device)\n\n\n        # this part is added:\n        #-----------------------------------------------------------------------\n\n\n        print(\"the ground truth y0 is:\", y0.item())\n        print(\"the model predict y1 is:\", y1.item())\n        print(\"\\n-----------------------------------------------------------------------------------\")\n\n        '''\n        please note here that y0 is the ground truth label for graph x0, y1 is the model prediction for x0 without any add-ons,\n        y2 is the model prediction for x0 when nodes are inserted but no edge purturbations are in place\n        we commence edge attack only when y2 != y0\n        '''\n\n        \n\n\n\n        #-----------------------------------------------------------------------\n\n        num_nodes = x0.num_nodes\n        space = num_nodes * (num_nodes - 1) \/ 2\n\n        # model predicts correctly before node injection\n        if y0 == y1: \n          # after injection, prediction is already wrong\n          if y0 != y2: \n            num_success_via_injection += 1\n            num_success += 1\n            print(\"instance {} is successfully attacked via node injection.\".format(i))\n\n          # after injection, prediction y2 is still correct\n          elif y0 == y2: \n            time_start = time()\n            # attack the model\n            adv_x0, adv_y0, query, success, dis, init, is_injected_nodes_dangling = attacker.attack_untargeted(x0, y0, num_injected, query_limit=args.max_query)\n            \n            print(\"before adv attack, the model predicts:\", y0.item())\n            print(\"after adv attack, the model predicts:\", adv_y0.item())\n            \n            if is_injected_nodes_dangling:\n              Dangling_injection_count += 1\n\n            # if successfully changed the prediction\n            if y0 != adv_y0:\n                num_success += 1\n            \n            time_end = time()\n            init_num_query.append(init[2])\n            num_query.append(query)\n            init_attack_time.append(init[3])\n            attack_time.append(time_end-time_start)\n            if success:\n                #process results in Stage 1\n                init_perturb, init_dis, init_query, init_time, s_type = init\n                init_ratio = init_perturb \/ space\n                init_perturbation.append(init_perturb)\n                init_distortion.append(init_dis)\n                search_type.append(s_type)\n                init_perturbation_ratio.append(init_ratio)\n\n                #process results in Stage 2\n                perturb = distance(adv_x0, x0)\n                perturbation.append(perturb)\n                perturbation_ratio.append(perturb\/space)\n                distortion.append(dis)\n                \n                add_edge, delete_edge = count_edges(adv_x0, x0)\n                num_delete_edge.append(delete_edge)\n                num_add_edge.append(add_edge)\n\n                #test dataset for defense\n                #x0.y = torch.tensor([0])\n                #adv_x0.y = torch.tensor([1])\n                adv_x0.y = x0.y\n                detect_test_advers.append(adv_x0)\n                detect_test_normal.append(x0)\n            else:\n                detect_test_advers.append(x0)\n                detect_test_normal.append(x0)\n                init_distortion.append(-1)\n                init_perturbation.append(-1)\n                init_perturbation_ratio.append(-1)\n                search_type.append(-1)\n\n                perturbation.append(-1)\n                perturbation_ratio.append(-1)\n                distortion.append(-1) \n\n        # y0 != y1 the model prediction is wrong\n        else: \n            print('instance {} is wrongly classified, No Need to Attack'.format(i))\n            no_need_count += 1\n            num_query.append(0)\n            attack_time.append(0)\n            perturbation.append(0)\n            perturbation_ratio.append(0)\n            distortion.append(0)\n            \n            init_perturbation.append(0)\n            init_distortion.append(0)\n            init_num_query.append(0)\n            init_attack_time.append(0)\n            search_type.append(0)\n            init_perturbation_ratio.append(0)\n\n    \n        # this part is changed:\n        # -----------------------------------------------------------------------\n        print(f\"\\nattack loop: \\\n        \\n\\t {num_success} out of {i+1 - no_need_count} instance success rate, \\\n        \\n\\t {Dangling_injection_count} out of {num_success} has dangling injected nodes \\\n        \\n\\t {no_need_count} instances need no attack, \\\n        \\n\\t {num_success_via_injection} instances successed with only node injection\")\n    \n    \n\n\n    success_ratio = num_success \/ (num_test - no_need_count)*100\n    avg_perturbation = sum(perturbation) \/ num_success\n    print(\"Sign-Opt: the success rate of black-box attack is {}\/{} = {:.4f}\".format(num_success,num_test-no_need_count, success_ratio))\n    print('Sign-Opt: the average perturbation is {:.4f}'.format(avg_perturbation))\n    print('Sign-Opt: the average perturbation ratio is {:.4f}'.format(sum(perturbation_ratio) \/ num_success*100))\n    print('Sign-Opt: the average query count is {:.4f}'.format(sum(num_query)\/(num_test-no_need_count)))\n    print('Sign-Opt: the average attacking time is {:.4f}'.format(sum(attack_time)\/(num_test-no_need_count)))\n    print('Sign-Opt: the average distortion is {:.4f}'.format(sum(distortion)\/num_success))\n    print('dataset: {}'.format(dataset_name))\n\n    # -----------------------------------------------------------------------\n\n\n    if args.search == 1 and args.effective == 1 and args.id ==1: \n        detect_test_path = '.\/defense\/'+dataset_name+'_'+model_name+'_Our_'\n        torch.save(detect_test_normal, detect_test_path+'test_normal.pt')\n        torch.save(detect_test_advers, detect_test_path+'test_advers.pt')\n        print('test dataset for defense saved!')\n  \n    \n    init_path = '.\/out1\/init_{}_{}_{}_{}_'.format(dataset_name, args.id, args.effective , args.search)\n    with open(init_path+'search_type.txt', 'w') as f:\n        f.write(str(search_type))\n    with open(init_path+'P.txt', 'w') as f:\n        f.write(str(init_perturbation))\n    with open(init_path+'PR.txt', 'w') as f:\n        f.write(str(init_perturbation_ratio))\n    with open(init_path+'D.txt', 'w') as f:\n        f.write(str(init_distortion))\n    with open(init_path+'Q.txt', 'w') as f:\n        f.write(str(init_num_query))\n    with open(init_path+'T.txt', 'w') as f:\n        f.write(str(init_attack_time))  \n    \n    \n    our_path = '.\/out1\/our_{}_{}_{}_{}_'.format(dataset_name, args.id, args.effective , args.search)\n    with open(our_path+'Q.txt', 'w') as f:\n        f.write(str(num_query))\n    with open(our_path+'T.txt', 'w') as f:\n        f.write(str(attack_time))\n    with open(our_path+'P.txt', 'w') as f:\n        f.write(str(perturbation))\n    with open(our_path+'PR.txt', 'w') as f:\n        f.write(str(perturbation_ratio))\n    with open(our_path+'D.txt', 'w') as f:\n        f.write(str(distortion))\n    with open(our_path+'ADD.txt', 'w') as f:\n        f.write(str(num_delete_edge))\n    with open(our_path+'DEL.txt', 'w') as f:\n        f.write(str(num_add_edge))\n            \n    print(\"the numbers of deleted edges are:\", num_delete_edge)\n    print(\"the numbers od added edges are:\", num_add_edge)\n    print(\"the average number of deleted edges for %s: %d\"%(dataset_name, float(sum(num_delete_edge)\/len(num_delete_edge))))\n    print(\"the average number of added edges for %s: %d\"%(dataset_name, float(sum(num_add_edge)\/len(num_add_edge))))\n    '''\n    out_path = '.\/out\/{}_Opt_{}.txt'.format(dataset_name, bound)  \n    with open(out_path, 'w') as f:\n        f.write('{} instances don\\'t need to be attacked\\n'.format(no_need_count))\n        f.write('Sign-Opt fails to attack {} instance\\n'.format(fail_count))\n        f.write(\"Sign-Opt: the success rate of black-box attack is {}\/{} = {:.4f}\\n\".format(success_count,num_test-no_need_count, success_ratio))\n        f.write('Sign-Opt: the average perturbation is {:.4f}\\n'.format(avg_perturbation))\n        f.write('Sign-Opt: the average perturbation ratio is {:.4f}\\n'.format(sum(perturbation_ratio) \/ success_count*100))\n        f.write('Sign-Opt: the average query count is {:.4f}\\n'.format(sum(num_query)\/(num_test-no_need_count)))\n        f.write('Sign-Opt: the average attacking time is {:.4f}\\n'.format(sum(attack_time)\/(num_test-no_need_count)))\n        f.write('Sign-Opt: the average distortion is {:.4f}\\n'.format(sum(distortion)\/success_count))\n        f.write('Sign-Opt: detail perturbation are: {}\\n'.format(perturbation))\n        f.write('Sign-Opt: detail perturbation ratio are: {}\\n'.format(perturbation_ratio))\n    '''\n    \n"}},"msg":"update injection function"}},"https:\/\/github.com\/ML4GW\/PE":{"caa535e73bcf996f4aa7d2d4374c7a017c21b192":{"url":"https:\/\/api.github.com\/repos\/ML4GW\/PE\/commits\/caa535e73bcf996f4aa7d2d4374c7a017c21b192","html_url":"https:\/\/github.com\/ML4GW\/PE\/commit\/caa535e73bcf996f4aa7d2d4374c7a017c21b192","message":"generate gaussian background, and injections in gaussian noise (#60)\n\n* add toggle for gaussian noise\r\n\r\n* update doocstring\r\n\r\n* add logging statement\r\n\r\n* allow psd from file\r\n\r\n* add is_psd argument\r\n\r\n* remove pycbc dep\r\n\r\n* begin unit tessts for oise_from_psd\r\n\r\n* invert timeseries and check for constant power\r\n\r\n* incorporate new noise_fom_psd function\r\n\r\n---------\r\n\r\nCo-authored-by: Deep Chatterjee <deep.chatterjee@ligo.org>","sha":"caa535e73bcf996f4aa7d2d4374c7a017c21b192","keyword":"function injection update","diff":"diff --git a\/projects\/sandbox\/data_generation\/data_generation\/generate_background.py b\/projects\/sandbox\/data_generation\/data_generation\/generate_background.py\nindex e74dfa5..4f75fc5 100644\n--- a\/projects\/sandbox\/data_generation\/data_generation\/generate_background.py\n+++ b\/projects\/sandbox\/data_generation\/data_generation\/generate_background.py\n@@ -1,14 +1,17 @@\n import logging\n from pathlib import Path\n-from typing import List\n+from typing import List, Optional\n \n import h5py\n import numpy as np\n+from data_generation.utils import noise_from_psd\n from gwdatafind import find_urls\n+from gwpy.frequencyseries import FrequencySeries\n from gwpy.segments import DataQualityDict\n from gwpy.timeseries import TimeSeries\n from typeo import scriptify\n \n+from ml4gw.spectral import normalize_psd\n from mlpe.logging import configure_logging\n \n \n@@ -24,6 +27,9 @@ def main(\n     minimum_length: float,\n     datadir: Path,\n     logdir: Path,\n+    gaussian: bool = False,\n+    psd_file: Optional[Path] = None,\n+    is_psd: bool = True,\n     force_generation: bool = False,\n     verbose: bool = False,\n ):\n@@ -36,6 +42,12 @@ def main(\n         outdir: where to store data\n     \"\"\"\n \n+    if psd_file is not None and not gaussian:\n+        raise ValueError(\n+            \"Cannot generate gaussian noise from\"\n+            \" requested PSD when gaussian is False\"\n+        )\n+\n     # make logdir dir\n     logdir.mkdir(exist_ok=True, parents=True)\n     datadir.mkdir(exist_ok=True, parents=True)\n@@ -88,33 +100,64 @@ def main(\n         \"from {} to {}\".format(*segment)\n     )\n \n-    with h5py.File(datadir \/ \"background.h5\", \"w\") as f:\n-        for ifo in ifos:\n+    background_data = {}\n+    for ifo in ifos:\n \n-            # find frame files\n-            files = find_urls(\n-                site=ifo.strip(\"1\"),\n-                frametype=f\"{ifo}_{frame_type}\",\n-                gpsstart=start,\n-                gpsend=stop,\n-                urltype=\"file\",\n-            )\n-            data = TimeSeries.read(\n-                files,\n-                channel=f\"{ifo}:{channel}\",\n-                start=segment[0],\n-                end=segment[1],\n+        # find frame files\n+        files = find_urls(\n+            site=ifo.strip(\"1\"),\n+            frametype=f\"{ifo}_{frame_type}\",\n+            gpsstart=start,\n+            gpsend=stop,\n+            urltype=\"file\",\n+        )\n+        data = TimeSeries.read(\n+            files,\n+            channel=f\"{ifo}:{channel}\",\n+            start=segment[0],\n+            end=segment[1],\n+        )\n+\n+        # resample\n+        data = data.resample(sample_rate)\n+\n+        if np.isnan(data).any():\n+            raise ValueError(\n+                f\"The background for ifo {ifo} contains NaN values\"\n             )\n \n-            # resample\n-            data = data.resample(sample_rate)\n+        if gaussian:\n+            logging.info(f\"Generating gaussian noise from psd for ifo {ifo}\")\n+\n+            # TODO: what should determine this?\n+            df = 1 \/ 8.0\n+            if psd_file is not None:\n+                # if user passed a psd file, load it into a FrequencySeries\n+                # that ml4gw.spectral.normalize_psd can handle\n+                frequencies, psd = np.loadtxt(psd_file, unpack=True)\n+                if not is_psd:\n+                    psd = psd**2\n+\n+                # ml4gw expects psd to start at 0 Hz, so lets check for that\n+                # TODO: implement logic that prepends 0 to psd\n+                # if the passed psd doesn't start at 0\n+                if frequencies[0] != 0:\n+                    raise ValueError(\n+                        \"PSD must start at 0 Hz, not {}\".format(frequencies[0])\n+                    )\n+                data = FrequencySeries(psd, frequencies=frequencies)\n+\n+            # normalize_psd can handle FrequencySeries or TimeSeries\n+            psd = normalize_psd(data, df, sample_rate)\n+            data = noise_from_psd(\n+                psd, df, len(data) \/ sample_rate, sample_rate\n+            )\n \n-            if np.isnan(data).any():\n-                raise ValueError(\n-                    f\"The background for ifo {ifo} contains NaN values\"\n-                )\n+        background_data[ifo] = data\n \n-            f.create_dataset(f\"{ifo}\", data=data)\n-            f.attrs.update({\"t0\": float(segment[0])})\n+    with h5py.File(background_file, \"w\") as f:\n+        for ifo, data in background_data.items():\n+            f.create_dataset(ifo, data=data)\n+        f.attrs.update({\"t0\": float(segment[0])})\n \n     return background_file\ndiff --git a\/projects\/sandbox\/data_generation\/data_generation\/generate_testing_set.py b\/projects\/sandbox\/data_generation\/data_generation\/generate_testing_set.py\nindex 4744f96..c687797 100644\n--- a\/projects\/sandbox\/data_generation\/data_generation\/generate_testing_set.py\n+++ b\/projects\/sandbox\/data_generation\/data_generation\/generate_testing_set.py\n@@ -5,11 +5,17 @@\n import h5py\n import numpy as np\n import torch\n-from data_generation.utils import download_data, inject_into_background\n+from data_generation.utils import (\n+    download_data,\n+    inject_into_background,\n+    noise_from_psd,\n+)\n+from gwpy.timeseries import TimeSeries\n from mldatafind.segments import query_segments\n from typeo import scriptify\n \n from ml4gw.gw import compute_observed_strain, get_ifo_geometry\n+from ml4gw.spectral import normalize_psd\n from mlpe.injection import generate_gw\n from mlpe.logging import configure_logging\n \n@@ -33,6 +39,7 @@ def main(\n     min_duration: float = 0,\n     waveform_arguments: Optional[Dict] = None,\n     parameter_conversion: Optional[Callable] = None,\n+    gaussian: bool = False,\n     force_generation: bool = False,\n     verbose: bool = False,\n ):\n@@ -47,21 +54,36 @@ def main(\n     and injected into kernels sampled non-coincidentally from the background.\n \n     Args:\n-        ifos: List of interferometers\n-        state_flag: Flag used to find times with good data quality.\n-        frame_type: Frame type used for discovering data\n-        channel: Channel for reading data\n-        start: Start time for finding data\n-        stop: Stop time for finding data\n-        sample_rate: Rate at which timeseries are sampled\n-        prior: Callable that instantiates a bilby prior\n-        waveform: A callable compatible with bilby waveform generator\n-        n_samples: Number of waveforms to sample and inject\n-        kernel_length: Length in seconds of kernels produced\n-        waveform_duration: length of injected waveforms\n-        datadir: Path to store data\n-        logdir: Path to store logs\n-        min_duration: Minimum duration of segments\n+        ifos:\n+            List of interferometers\n+        state_flag:\n+            Flag used to find times with good data quality.\n+        frame_type:\n+            Frame type used for discovering data\n+        channel:\n+            Channel for reading data\n+        start:\n+            Start time for finding data\n+        stop:\n+            Stop time for finding data\n+        sample_rate:\n+            Rate at which timeseries are sampled\n+        prior:\n+            Callable that instantiates a bilby prior\n+        waveform:\n+            A callable compatible with bilby waveform generator\n+        n_samples:\n+            Number of waveforms to sample and inject\n+        kernel_length:\n+            Length in seconds of kernels produced\n+        waveform_duration:\n+            length of injected waveforms\n+        datadir:\n+            Path to store data\n+        logdir:\n+            Path to store logs\n+        min_duration:\n+            Minimum duration of segments\n         waveform_arguments:\n             Additional arguments to pass to waveform generator,\n             that will ultimately get passed\n@@ -72,8 +94,14 @@ def main(\n             Parameter conversion to pass the bilby waveform generator.\n             Typically used for converting between bilby and lalsimulation\n             BBH parameters\n-        force_generation: Force generation of data\n-        verbose: Log verbosely\n+        gaussian:\n+            If True, generate gaussian noise from a psd calculated using\n+            the requested period of data. This will be used as the background\n+            to inject the waveforms into.\n+        force_generation:\n+            Force generation of data\n+        verbose:\n+            Log verbosely\n \n     Returns signal file containiing injections and parameters\n     \"\"\"\n@@ -106,6 +134,18 @@ def main(\n         ifos, frame_type, channel, sample_rate, segment_start, segment_stop\n     )\n \n+    if gaussian:\n+        logging.info(\n+            \"Generating gaussian noise from psd for injection background\"\n+        )\n+        df = 1 \/ waveform_duration\n+\n+        for ifo in ifos:\n+            duration = len(background_dict[ifo]) \/ sample_rate\n+            psd = normalize_psd(background_dict[ifo], df, sample_rate)\n+            data = noise_from_psd(psd, df, duration, sample_rate)\n+            background_dict[ifo] = TimeSeries(data, dt=1 \/ sample_rate)\n+\n     background = np.stack([ts.value for ts in background_dict.values()])\n     background = torch.as_tensor(background)\n \ndiff --git a\/projects\/sandbox\/data_generation\/data_generation\/utils.py b\/projects\/sandbox\/data_generation\/data_generation\/utils.py\nindex e7ff1cf..e60ae9b 100644\n--- a\/projects\/sandbox\/data_generation\/data_generation\/utils.py\n+++ b\/projects\/sandbox\/data_generation\/data_generation\/utils.py\n@@ -1,6 +1,8 @@\n from typing import Iterable\n \n import gwdatafind\n+import lal\n+import lalsimulation\n import numpy as np\n import torch\n from gwpy.timeseries import TimeSeries, TimeSeriesDict\n@@ -8,6 +10,66 @@\n from ml4gw.utils.slicing import slice_kernels\n \n \n+def noise_from_psd(\n+    psd: np.ndarray,\n+    df: float,\n+    duration: float,\n+    sample_rate: float,\n+):\n+    \"\"\"\n+    Generate noise from a given PSD. See\n+    https:\/\/pycbc.org\/pycbc\/latest\/html\/_modules\/pycbc\/noise\/gaussian.html#noise_from_psd\n+    for inspiration.\n+\n+    Args:\n+        psd: np.ndarray of the PSD to generate noise from\n+        df: frequency resolution of `psd` in Hz\n+        duration: duration of the noise to generate in seconds\n+        sample_rate: sample rate of the noise to generate in Hz\n+    \"\"\"\n+\n+    # calculate length of requested timeseries in samples\n+    # and create empty array to store data\n+    length = int(duration * sample_rate)\n+    noise_ts = np.empty(length)\n+\n+    # create epcoh and random seed\n+    epoch = lal.LIGOTimeGPS(0, 0)\n+    rng = lal.gsl_rng(\"ranlux\", 0)\n+\n+    # calculate number of samples we'll generate\n+    # per segment, and initialize the segment\n+    delta_t = 1 \/ sample_rate\n+    N = int(1 \/ delta_t \/ df)\n+    stride = N \/\/ 2\n+    segment = lal.CreateREAL8TimeSeries(\n+        \"\", epoch, 0.0, 1.0 \/ sample_rate, lal.StrainUnit, N\n+    )\n+\n+    psd_lal = lal.CreateREAL8FrequencySeries(\n+        \"\", epoch, 0.0, df, lal.SecondUnit, len(psd)\n+    )\n+    psd_lal.data.data[:] = psd\n+    psd_lal.data.data[0] = 0\n+    psd_lal.data.data[-1] = 0\n+\n+    length_generated = 0\n+    lalsimulation.SimNoise(segment, 0, psd_lal, rng)\n+    while length_generated < length:\n+        if (length_generated + stride) < length:\n+            noise_ts[\n+                length_generated : length_generated + stride\n+            ] = segment.data.data[0:stride]\n+        else:\n+            noise_ts[length_generated:length] = segment.data.data[\n+                0 : length - length_generated\n+            ]\n+\n+        length_generated += stride\n+        lalsimulation.SimNoise(segment, stride, psd_lal, rng)\n+    return noise_ts\n+\n+\n def download_data(\n     ifos: Iterable[str],\n     frame_type: str,\ndiff --git a\/projects\/sandbox\/data_generation\/poetry.lock b\/projects\/sandbox\/data_generation\/poetry.lock\nindex 118c406..86729e5 100644\n--- a\/projects\/sandbox\/data_generation\/poetry.lock\n+++ b\/projects\/sandbox\/data_generation\/poetry.lock\n@@ -159,7 +159,7 @@ test = [\"black\", \"isort\", \"pytest (>=3.6)\", \"pytest-cov (>=2.6.1)\", \"toml\"]\n \n [[package]]\n name = \"cryptography\"\n-version = \"39.0.1\"\n+version = \"39.0.2\"\n description = \"cryptography is a package which provides cryptographic recipes and primitives to Python developers.\"\n category = \"main\"\n optional = false\n@@ -503,7 +503,7 @@ source = [\"Cython (>=0.29.7)\"]\n \n [[package]]\n name = \"matplotlib\"\n-version = \"3.7.0\"\n+version = \"3.7.1\"\n description = \"Python plotting package\"\n category = \"main\"\n optional = false\n@@ -524,8 +524,8 @@ setuptools_scm = \">=7\"\n \n [[package]]\n name = \"ml4gw\"\n-version = \"0.0.1\"\n-description = \"Tools for building training torch models on gravitational wave data\"\n+version = \"0.1.1\"\n+description = \"Tools for training torch models on gravitational wave data\"\n category = \"main\"\n optional = false\n python-versions = \">=3.8,<3.11\"\n@@ -551,7 +551,6 @@ python-versions = \">=3.8,<3.11\"\n develop = false\n \n [package.dependencies]\n-astropy = \"<5.0.0\"\n ciecplib = \"^0.7.1\"\n gwpy = \"^2.1\"\n numpy = \">=1.22.0,<1.22.4\"\n@@ -562,7 +561,7 @@ scipy = \"<1.8\"\n type = \"git\"\n url = \"https:\/\/github.com\/ML4GW\/mldatafind.git\"\n reference = \"HEAD\"\n-resolved_reference = \"a87609cd90090cf69384587fb1746deb04929fb7\"\n+resolved_reference = \"3e71f581bb92ce84e69cb3f547d8fc3ff25530a8\"\n \n [[package]]\n name = \"mlpe-base\"\n@@ -841,7 +840,7 @@ python-versions = \">=3.6, <4\"\n \n [[package]]\n name = \"pytest\"\n-version = \"7.2.1\"\n+version = \"7.2.2\"\n description = \"pytest: simple powerful testing with Python\"\n category = \"dev\"\n optional = false\n@@ -965,7 +964,7 @@ docs = [\"Sphinx\"]\n \n [[package]]\n name = \"setuptools\"\n-version = \"67.3.2\"\n+version = \"67.4.0\"\n description = \"Easily download, build, install, upgrade, and uninstall Python packages\"\n category = \"main\"\n optional = false\n@@ -1069,11 +1068,11 @@ typeguard = \">=2.11.1\"\n \n [[package]]\n name = \"tqdm\"\n-version = \"4.64.1\"\n+version = \"4.65.0\"\n description = \"Fast, Extensible Progress Meter\"\n category = \"main\"\n optional = false\n-python-versions = \"!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*,>=2.7\"\n+python-versions = \">=3.7\"\n \n [package.dependencies]\n colorama = {version = \"*\", markers = \"platform_system == \\\"Windows\\\"\"}\n@@ -1148,7 +1147,7 @@ test = [\"pytest (>=3.0.0)\"]\n \n [[package]]\n name = \"zipp\"\n-version = \"3.14.0\"\n+version = \"3.15.0\"\n description = \"Backport of pathlib-compatible object wrapper for zip files\"\n category = \"main\"\n optional = false\n@@ -1156,7 +1155,7 @@ python-versions = \">=3.7\"\n \n [package.extras]\n docs = [\"furo\", \"jaraco.packaging (>=9)\", \"jaraco.tidelift (>=1.4)\", \"rst.linker (>=1.9)\", \"sphinx (>=3.5)\", \"sphinx-lint\"]\n-testing = [\"flake8 (<5)\", \"func-timeout\", \"jaraco.functools\", \"jaraco.itertools\", \"more-itertools\", \"pytest (>=6)\", \"pytest-black (>=0.3.7)\", \"pytest-checkdocs (>=2.4)\", \"pytest-cov\", \"pytest-enabler (>=1.3)\", \"pytest-flake8\", \"pytest-mypy (>=0.9.1)\"]\n+testing = [\"big-O\", \"flake8 (<5)\", \"jaraco.functools\", \"jaraco.itertools\", \"more-itertools\", \"pytest (>=6)\", \"pytest-black (>=0.3.7)\", \"pytest-checkdocs (>=2.4)\", \"pytest-cov\", \"pytest-enabler (>=1.3)\", \"pytest-flake8\", \"pytest-mypy (>=0.9.1)\"]\n \n [metadata]\n lock-version = \"1.1\"\n@@ -1442,29 +1441,29 @@ corner = [\n     {file = \"corner-2.2.1.tar.gz\", hash = \"sha256:d39f1d9ad31093bb2aa06d80b23573e51ad7130af4c4af0931fca745a64751e5\"},\n ]\n cryptography = [\n-    {file = \"cryptography-39.0.1-cp36-abi3-macosx_10_12_universal2.whl\", hash = \"sha256:6687ef6d0a6497e2b58e7c5b852b53f62142cfa7cd1555795758934da363a965\"},\n-    {file = \"cryptography-39.0.1-cp36-abi3-macosx_10_12_x86_64.whl\", hash = \"sha256:706843b48f9a3f9b9911979761c91541e3d90db1ca905fd63fee540a217698bc\"},\n-    {file = \"cryptography-39.0.1-cp36-abi3-manylinux_2_17_aarch64.manylinux2014_aarch64.manylinux_2_24_aarch64.whl\", hash = \"sha256:5d2d8b87a490bfcd407ed9d49093793d0f75198a35e6eb1a923ce1ee86c62b41\"},\n-    {file = \"cryptography-39.0.1-cp36-abi3-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:83e17b26de248c33f3acffb922748151d71827d6021d98c70e6c1a25ddd78505\"},\n-    {file = \"cryptography-39.0.1-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:e124352fd3db36a9d4a21c1aa27fd5d051e621845cb87fb851c08f4f75ce8be6\"},\n-    {file = \"cryptography-39.0.1-cp36-abi3-manylinux_2_24_x86_64.whl\", hash = \"sha256:5aa67414fcdfa22cf052e640cb5ddc461924a045cacf325cd164e65312d99502\"},\n-    {file = \"cryptography-39.0.1-cp36-abi3-manylinux_2_28_aarch64.whl\", hash = \"sha256:35f7c7d015d474f4011e859e93e789c87d21f6f4880ebdc29896a60403328f1f\"},\n-    {file = \"cryptography-39.0.1-cp36-abi3-manylinux_2_28_x86_64.whl\", hash = \"sha256:f24077a3b5298a5a06a8e0536e3ea9ec60e4c7ac486755e5fb6e6ea9b3500106\"},\n-    {file = \"cryptography-39.0.1-cp36-abi3-musllinux_1_1_aarch64.whl\", hash = \"sha256:f0c64d1bd842ca2633e74a1a28033d139368ad959872533b1bab8c80e8240a0c\"},\n-    {file = \"cryptography-39.0.1-cp36-abi3-musllinux_1_1_x86_64.whl\", hash = \"sha256:0f8da300b5c8af9f98111ffd512910bc792b4c77392a9523624680f7956a99d4\"},\n-    {file = \"cryptography-39.0.1-cp36-abi3-win32.whl\", hash = \"sha256:fe913f20024eb2cb2f323e42a64bdf2911bb9738a15dba7d3cce48151034e3a8\"},\n-    {file = \"cryptography-39.0.1-cp36-abi3-win_amd64.whl\", hash = \"sha256:ced4e447ae29ca194449a3f1ce132ded8fcab06971ef5f618605aacaa612beac\"},\n-    {file = \"cryptography-39.0.1-pp38-pypy38_pp73-macosx_10_12_x86_64.whl\", hash = \"sha256:807ce09d4434881ca3a7594733669bd834f5b2c6d5c7e36f8c00f691887042ad\"},\n-    {file = \"cryptography-39.0.1-pp38-pypy38_pp73-manylinux_2_28_aarch64.whl\", hash = \"sha256:c5caeb8188c24888c90b5108a441c106f7faa4c4c075a2bcae438c6e8ca73cef\"},\n-    {file = \"cryptography-39.0.1-pp38-pypy38_pp73-manylinux_2_28_x86_64.whl\", hash = \"sha256:4789d1e3e257965e960232345002262ede4d094d1a19f4d3b52e48d4d8f3b885\"},\n-    {file = \"cryptography-39.0.1-pp38-pypy38_pp73-win_amd64.whl\", hash = \"sha256:96f1157a7c08b5b189b16b47bc9db2332269d6680a196341bf30046330d15388\"},\n-    {file = \"cryptography-39.0.1-pp39-pypy39_pp73-macosx_10_12_x86_64.whl\", hash = \"sha256:e422abdec8b5fa8462aa016786680720d78bdce7a30c652b7fadf83a4ba35336\"},\n-    {file = \"cryptography-39.0.1-pp39-pypy39_pp73-manylinux_2_17_aarch64.manylinux2014_aarch64.manylinux_2_24_aarch64.whl\", hash = \"sha256:b0afd054cd42f3d213bf82c629efb1ee5f22eba35bf0eec88ea9ea7304f511a2\"},\n-    {file = \"cryptography-39.0.1-pp39-pypy39_pp73-manylinux_2_24_x86_64.whl\", hash = \"sha256:6f8ba7f0328b79f08bdacc3e4e66fb4d7aab0c3584e0bd41328dce5262e26b2e\"},\n-    {file = \"cryptography-39.0.1-pp39-pypy39_pp73-manylinux_2_28_aarch64.whl\", hash = \"sha256:ef8b72fa70b348724ff1218267e7f7375b8de4e8194d1636ee60510aae104cd0\"},\n-    {file = \"cryptography-39.0.1-pp39-pypy39_pp73-manylinux_2_28_x86_64.whl\", hash = \"sha256:aec5a6c9864be7df2240c382740fcf3b96928c46604eaa7f3091f58b878c0bb6\"},\n-    {file = \"cryptography-39.0.1-pp39-pypy39_pp73-win_amd64.whl\", hash = \"sha256:fdd188c8a6ef8769f148f88f859884507b954cc64db6b52f66ef199bb9ad660a\"},\n-    {file = \"cryptography-39.0.1.tar.gz\", hash = \"sha256:d1f6198ee6d9148405e49887803907fe8962a23e6c6f83ea7d98f1c0de375695\"},\n+    {file = \"cryptography-39.0.2-cp36-abi3-macosx_10_12_universal2.whl\", hash = \"sha256:2725672bb53bb92dc7b4150d233cd4b8c59615cd8288d495eaa86db00d4e5c06\"},\n+    {file = \"cryptography-39.0.2-cp36-abi3-macosx_10_12_x86_64.whl\", hash = \"sha256:23df8ca3f24699167daf3e23e51f7ba7334d504af63a94af468f468b975b7dd7\"},\n+    {file = \"cryptography-39.0.2-cp36-abi3-manylinux_2_17_aarch64.manylinux2014_aarch64.manylinux_2_24_aarch64.whl\", hash = \"sha256:eb40fe69cfc6f5cdab9a5ebd022131ba21453cf7b8a7fd3631f45bbf52bed612\"},\n+    {file = \"cryptography-39.0.2-cp36-abi3-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:bc0521cce2c1d541634b19f3ac661d7a64f9555135e9d8af3980965be717fd4a\"},\n+    {file = \"cryptography-39.0.2-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:ffd394c7896ed7821a6d13b24657c6a34b6e2650bd84ae063cf11ccffa4f1a97\"},\n+    {file = \"cryptography-39.0.2-cp36-abi3-manylinux_2_24_x86_64.whl\", hash = \"sha256:e8a0772016feeb106efd28d4a328e77dc2edae84dfbac06061319fdb669ff828\"},\n+    {file = \"cryptography-39.0.2-cp36-abi3-manylinux_2_28_aarch64.whl\", hash = \"sha256:8f35c17bd4faed2bc7797d2a66cbb4f986242ce2e30340ab832e5d99ae60e011\"},\n+    {file = \"cryptography-39.0.2-cp36-abi3-manylinux_2_28_x86_64.whl\", hash = \"sha256:b49a88ff802e1993b7f749b1eeb31134f03c8d5c956e3c125c75558955cda536\"},\n+    {file = \"cryptography-39.0.2-cp36-abi3-musllinux_1_1_aarch64.whl\", hash = \"sha256:5f8c682e736513db7d04349b4f6693690170f95aac449c56f97415c6980edef5\"},\n+    {file = \"cryptography-39.0.2-cp36-abi3-musllinux_1_1_x86_64.whl\", hash = \"sha256:d7d84a512a59f4412ca8549b01f94be4161c94efc598bf09d027d67826beddc0\"},\n+    {file = \"cryptography-39.0.2-cp36-abi3-win32.whl\", hash = \"sha256:c43ac224aabcbf83a947eeb8b17eaf1547bce3767ee2d70093b461f31729a480\"},\n+    {file = \"cryptography-39.0.2-cp36-abi3-win_amd64.whl\", hash = \"sha256:788b3921d763ee35dfdb04248d0e3de11e3ca8eb22e2e48fef880c42e1f3c8f9\"},\n+    {file = \"cryptography-39.0.2-pp38-pypy38_pp73-macosx_10_12_x86_64.whl\", hash = \"sha256:d15809e0dbdad486f4ad0979753518f47980020b7a34e9fc56e8be4f60702fac\"},\n+    {file = \"cryptography-39.0.2-pp38-pypy38_pp73-manylinux_2_28_aarch64.whl\", hash = \"sha256:50cadb9b2f961757e712a9737ef33d89b8190c3ea34d0fb6675e00edbe35d074\"},\n+    {file = \"cryptography-39.0.2-pp38-pypy38_pp73-manylinux_2_28_x86_64.whl\", hash = \"sha256:103e8f7155f3ce2ffa0049fe60169878d47a4364b277906386f8de21c9234aa1\"},\n+    {file = \"cryptography-39.0.2-pp38-pypy38_pp73-win_amd64.whl\", hash = \"sha256:6236a9610c912b129610eb1a274bdc1350b5df834d124fa84729ebeaf7da42c3\"},\n+    {file = \"cryptography-39.0.2-pp39-pypy39_pp73-macosx_10_12_x86_64.whl\", hash = \"sha256:e944fe07b6f229f4c1a06a7ef906a19652bdd9fd54c761b0ff87e83ae7a30354\"},\n+    {file = \"cryptography-39.0.2-pp39-pypy39_pp73-manylinux_2_17_aarch64.manylinux2014_aarch64.manylinux_2_24_aarch64.whl\", hash = \"sha256:35d658536b0a4117c885728d1a7032bdc9a5974722ae298d6c533755a6ee3915\"},\n+    {file = \"cryptography-39.0.2-pp39-pypy39_pp73-manylinux_2_24_x86_64.whl\", hash = \"sha256:30b1d1bfd00f6fc80d11300a29f1d8ab2b8d9febb6ed4a38a76880ec564fae84\"},\n+    {file = \"cryptography-39.0.2-pp39-pypy39_pp73-manylinux_2_28_aarch64.whl\", hash = \"sha256:e029b844c21116564b8b61216befabca4b500e6816fa9f0ba49527653cae2108\"},\n+    {file = \"cryptography-39.0.2-pp39-pypy39_pp73-manylinux_2_28_x86_64.whl\", hash = \"sha256:fa507318e427169ade4e9eccef39e9011cdc19534f55ca2f36ec3f388c1f70f3\"},\n+    {file = \"cryptography-39.0.2-pp39-pypy39_pp73-win_amd64.whl\", hash = \"sha256:8bc0008ef798231fac03fe7d26e82d601d15bd16f3afaad1c6113771566570f3\"},\n+    {file = \"cryptography-39.0.2.tar.gz\", hash = \"sha256:bc5b871e977c8ee5a1bbc42fa8d19bcc08baf0c51cbf1586b0e87a2694dde42f\"},\n ]\n cycler = [\n     {file = \"cycler-0.11.0-py3-none-any.whl\", hash = \"sha256:3a27e95f763a428a739d2add979fa7494c912a32c17c4c38c4d5f082cad165a3\"},\n@@ -1797,47 +1796,47 @@ lxml = [\n     {file = \"lxml-4.9.2.tar.gz\", hash = \"sha256:2455cfaeb7ac70338b3257f41e21f0724f4b5b0c0e7702da67ee6c3640835b67\"},\n ]\n matplotlib = [\n-    {file = \"matplotlib-3.7.0-cp310-cp310-macosx_10_12_universal2.whl\", hash = \"sha256:3da8b9618188346239e51f1ea6c0f8f05c6e218cfcc30b399dd7dd7f52e8bceb\"},\n-    {file = \"matplotlib-3.7.0-cp310-cp310-macosx_10_12_x86_64.whl\", hash = \"sha256:c0592ba57217c22987b7322df10f75ef95bc44dce781692b4b7524085de66019\"},\n-    {file = \"matplotlib-3.7.0-cp310-cp310-macosx_11_0_arm64.whl\", hash = \"sha256:21269450243d6928da81a9bed201f0909432a74e7d0d65db5545b9fa8a0d0223\"},\n-    {file = \"matplotlib-3.7.0-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:eb2e76cd429058d8954121c334dddfcd11a6186c6975bca61f3f248c99031b05\"},\n-    {file = \"matplotlib-3.7.0-cp310-cp310-manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:de20eb1247725a2f889173d391a6d9e7e0f2540feda24030748283108b0478ec\"},\n-    {file = \"matplotlib-3.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:c5465735eaaafd1cfaec3fed60aee776aeb3fd3992aa2e49f4635339c931d443\"},\n-    {file = \"matplotlib-3.7.0-cp310-cp310-win32.whl\", hash = \"sha256:092e6abc80cdf8a95f7d1813e16c0e99ceda8d5b195a3ab859c680f3487b80a2\"},\n-    {file = \"matplotlib-3.7.0-cp310-cp310-win_amd64.whl\", hash = \"sha256:4f640534ec2760e270801056bc0d8a10777c48b30966eef78a7c35d8590915ba\"},\n-    {file = \"matplotlib-3.7.0-cp311-cp311-macosx_10_12_universal2.whl\", hash = \"sha256:f336e7014889c38c59029ebacc35c59236a852e4b23836708cfd3f43d1eaeed5\"},\n-    {file = \"matplotlib-3.7.0-cp311-cp311-macosx_10_12_x86_64.whl\", hash = \"sha256:3a10428d4f8d1a478ceabd652e61a175b2fdeed4175ab48da4a7b8deb561e3fa\"},\n-    {file = \"matplotlib-3.7.0-cp311-cp311-macosx_11_0_arm64.whl\", hash = \"sha256:46ca923e980f76d34c1c633343a72bb042d6ba690ecc649aababf5317997171d\"},\n-    {file = \"matplotlib-3.7.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:c849aa94ff2a70fb71f318f48a61076d1205c6013b9d3885ade7f992093ac434\"},\n-    {file = \"matplotlib-3.7.0-cp311-cp311-manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:827e78239292e561cfb70abf356a9d7eaf5bf6a85c97877f254009f20b892f89\"},\n-    {file = \"matplotlib-3.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:691ef1f15360e439886186d0db77b5345b24da12cbc4fc57b26c4826db4d6cab\"},\n-    {file = \"matplotlib-3.7.0-cp311-cp311-win32.whl\", hash = \"sha256:21a8aeac39b4a795e697265d800ce52ab59bdeb6bb23082e2d971f3041074f02\"},\n-    {file = \"matplotlib-3.7.0-cp311-cp311-win_amd64.whl\", hash = \"sha256:01681566e95b9423021b49dea6a2395c16fa054604eacb87f0f4c439750f9114\"},\n-    {file = \"matplotlib-3.7.0-cp38-cp38-macosx_10_12_universal2.whl\", hash = \"sha256:cf119eee4e57389fba5ac8b816934e95c256535e55f0b21628b4205737d1de85\"},\n-    {file = \"matplotlib-3.7.0-cp38-cp38-macosx_10_12_x86_64.whl\", hash = \"sha256:21bd4033c40b95abd5b8453f036ed5aa70856e56ecbd887705c37dce007a4c21\"},\n-    {file = \"matplotlib-3.7.0-cp38-cp38-macosx_11_0_arm64.whl\", hash = \"sha256:111ef351f28fd823ed7177632070a6badd6f475607122bc9002a526f2502a0b5\"},\n-    {file = \"matplotlib-3.7.0-cp38-cp38-manylinux_2_12_i686.manylinux2010_i686.whl\", hash = \"sha256:f91d35b3ef51d29d9c661069b9e4ba431ce283ffc533b981506889e144b5b40e\"},\n-    {file = \"matplotlib-3.7.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl\", hash = \"sha256:0a776462a4a63c0bfc9df106c15a0897aa2dbab6795c693aa366e8e283958854\"},\n-    {file = \"matplotlib-3.7.0-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:0dfd4a0cbd151f6439e6d7f8dca5292839ca311e7e650596d073774847ca2e4f\"},\n-    {file = \"matplotlib-3.7.0-cp38-cp38-win32.whl\", hash = \"sha256:56b7b79488209041a9bf7ddc34f1b069274489ce69e34dc63ae241d0d6b4b736\"},\n-    {file = \"matplotlib-3.7.0-cp38-cp38-win_amd64.whl\", hash = \"sha256:8665855f3919c80551f377bc16df618ceabf3ef65270bc14b60302dce88ca9ab\"},\n-    {file = \"matplotlib-3.7.0-cp39-cp39-macosx_10_12_universal2.whl\", hash = \"sha256:f910d924da8b9fb066b5beae0b85e34ed1b6293014892baadcf2a51da1c65807\"},\n-    {file = \"matplotlib-3.7.0-cp39-cp39-macosx_10_12_x86_64.whl\", hash = \"sha256:cf6346644e8fe234dc847e6232145dac199a650d3d8025b3ef65107221584ba4\"},\n-    {file = \"matplotlib-3.7.0-cp39-cp39-macosx_11_0_arm64.whl\", hash = \"sha256:3d1e52365d8d5af699f04581ca191112e1d1220a9ce4386b57d807124d8b55e6\"},\n-    {file = \"matplotlib-3.7.0-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:c869b646489c6a94375714032e5cec08e3aa8d3f7d4e8ef2b0fb50a52b317ce6\"},\n-    {file = \"matplotlib-3.7.0-cp39-cp39-manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:f4ddac5f59e78d04b20469bc43853a8e619bb6505c7eac8ffb343ff2c516d72f\"},\n-    {file = \"matplotlib-3.7.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:fb0304c1cd802e9a25743414c887e8a7cd51d96c9ec96d388625d2cd1c137ae3\"},\n-    {file = \"matplotlib-3.7.0-cp39-cp39-win32.whl\", hash = \"sha256:a06a6c9822e80f323549c6bc9da96d4f233178212ad9a5f4ab87fd153077a507\"},\n-    {file = \"matplotlib-3.7.0-cp39-cp39-win_amd64.whl\", hash = \"sha256:cb52aa97b92acdee090edfb65d1cb84ea60ab38e871ba8321a10bbcebc2a3540\"},\n-    {file = \"matplotlib-3.7.0-pp38-pypy38_pp73-macosx_10_12_x86_64.whl\", hash = \"sha256:3493b48e56468c39bd9c1532566dff3b8062952721b7521e1f394eb6791495f4\"},\n-    {file = \"matplotlib-3.7.0-pp38-pypy38_pp73-manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:7d0dcd1a0bf8d56551e8617d6dc3881d8a1c7fb37d14e5ec12cbb293f3e6170a\"},\n-    {file = \"matplotlib-3.7.0-pp38-pypy38_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:51fb664c37714cbaac69c16d6b3719f517a13c96c3f76f4caadd5a0aa7ed0329\"},\n-    {file = \"matplotlib-3.7.0-pp38-pypy38_pp73-win_amd64.whl\", hash = \"sha256:4497d88c559b76da320b7759d64db442178beeea06a52dc0c629086982082dcd\"},\n-    {file = \"matplotlib-3.7.0-pp39-pypy39_pp73-macosx_10_12_x86_64.whl\", hash = \"sha256:9d85355c48ef8b9994293eb7c00f44aa8a43cad7a297fbf0770a25cdb2244b91\"},\n-    {file = \"matplotlib-3.7.0-pp39-pypy39_pp73-manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:03eb2c8ff8d85da679b71e14c7c95d16d014c48e0c0bfa14db85f6cdc5c92aad\"},\n-    {file = \"matplotlib-3.7.0-pp39-pypy39_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:71b751d06b2ed1fd017de512d7439c0259822864ea16731522b251a27c0b2ede\"},\n-    {file = \"matplotlib-3.7.0-pp39-pypy39_pp73-win_amd64.whl\", hash = \"sha256:b51ab8a5d5d3bbd4527af633a638325f492e09e45e78afdf816ef55217a09664\"},\n-    {file = \"matplotlib-3.7.0.tar.gz\", hash = \"sha256:8f6efd313430d7ef70a38a3276281cb2e8646b3a22b3b21eb227da20e15e6813\"},\n+    {file = \"matplotlib-3.7.1-cp310-cp310-macosx_10_12_universal2.whl\", hash = \"sha256:95cbc13c1fc6844ab8812a525bbc237fa1470863ff3dace7352e910519e194b1\"},\n+    {file = \"matplotlib-3.7.1-cp310-cp310-macosx_10_12_x86_64.whl\", hash = \"sha256:08308bae9e91aca1ec6fd6dda66237eef9f6294ddb17f0d0b3c863169bf82353\"},\n+    {file = \"matplotlib-3.7.1-cp310-cp310-macosx_11_0_arm64.whl\", hash = \"sha256:544764ba51900da4639c0f983b323d288f94f65f4024dc40ecb1542d74dc0500\"},\n+    {file = \"matplotlib-3.7.1-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:56d94989191de3fcc4e002f93f7f1be5da476385dde410ddafbb70686acf00ea\"},\n+    {file = \"matplotlib-3.7.1-cp310-cp310-manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:e99bc9e65901bb9a7ce5e7bb24af03675cbd7c70b30ac670aa263240635999a4\"},\n+    {file = \"matplotlib-3.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:eb7d248c34a341cd4c31a06fd34d64306624c8cd8d0def7abb08792a5abfd556\"},\n+    {file = \"matplotlib-3.7.1-cp310-cp310-win32.whl\", hash = \"sha256:ce463ce590f3825b52e9fe5c19a3c6a69fd7675a39d589e8b5fbe772272b3a24\"},\n+    {file = \"matplotlib-3.7.1-cp310-cp310-win_amd64.whl\", hash = \"sha256:3d7bc90727351fb841e4d8ae620d2d86d8ed92b50473cd2b42ce9186104ecbba\"},\n+    {file = \"matplotlib-3.7.1-cp311-cp311-macosx_10_12_universal2.whl\", hash = \"sha256:770a205966d641627fd5cf9d3cb4b6280a716522cd36b8b284a8eb1581310f61\"},\n+    {file = \"matplotlib-3.7.1-cp311-cp311-macosx_10_12_x86_64.whl\", hash = \"sha256:f67bfdb83a8232cb7a92b869f9355d677bce24485c460b19d01970b64b2ed476\"},\n+    {file = \"matplotlib-3.7.1-cp311-cp311-macosx_11_0_arm64.whl\", hash = \"sha256:2bf092f9210e105f414a043b92af583c98f50050559616930d884387d0772aba\"},\n+    {file = \"matplotlib-3.7.1-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:89768d84187f31717349c6bfadc0e0d8c321e8eb34522acec8a67b1236a66332\"},\n+    {file = \"matplotlib-3.7.1-cp311-cp311-manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:83111e6388dec67822e2534e13b243cc644c7494a4bb60584edbff91585a83c6\"},\n+    {file = \"matplotlib-3.7.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:a867bf73a7eb808ef2afbca03bcdb785dae09595fbe550e1bab0cd023eba3de0\"},\n+    {file = \"matplotlib-3.7.1-cp311-cp311-win32.whl\", hash = \"sha256:fbdeeb58c0cf0595efe89c05c224e0a502d1aa6a8696e68a73c3efc6bc354304\"},\n+    {file = \"matplotlib-3.7.1-cp311-cp311-win_amd64.whl\", hash = \"sha256:c0bd19c72ae53e6ab979f0ac6a3fafceb02d2ecafa023c5cca47acd934d10be7\"},\n+    {file = \"matplotlib-3.7.1-cp38-cp38-macosx_10_12_universal2.whl\", hash = \"sha256:6eb88d87cb2c49af00d3bbc33a003f89fd9f78d318848da029383bfc08ecfbfb\"},\n+    {file = \"matplotlib-3.7.1-cp38-cp38-macosx_10_12_x86_64.whl\", hash = \"sha256:cf0e4f727534b7b1457898c4f4ae838af1ef87c359b76dcd5330fa31893a3ac7\"},\n+    {file = \"matplotlib-3.7.1-cp38-cp38-macosx_11_0_arm64.whl\", hash = \"sha256:46a561d23b91f30bccfd25429c3c706afe7d73a5cc64ef2dfaf2b2ac47c1a5dc\"},\n+    {file = \"matplotlib-3.7.1-cp38-cp38-manylinux_2_12_i686.manylinux2010_i686.whl\", hash = \"sha256:8704726d33e9aa8a6d5215044b8d00804561971163563e6e6591f9dcf64340cc\"},\n+    {file = \"matplotlib-3.7.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl\", hash = \"sha256:4cf327e98ecf08fcbb82685acaf1939d3338548620ab8dfa02828706402c34de\"},\n+    {file = \"matplotlib-3.7.1-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:617f14ae9d53292ece33f45cba8503494ee199a75b44de7717964f70637a36aa\"},\n+    {file = \"matplotlib-3.7.1-cp38-cp38-win32.whl\", hash = \"sha256:7c9a4b2da6fac77bcc41b1ea95fadb314e92508bf5493ceff058e727e7ecf5b0\"},\n+    {file = \"matplotlib-3.7.1-cp38-cp38-win_amd64.whl\", hash = \"sha256:14645aad967684e92fc349493fa10c08a6da514b3d03a5931a1bac26e6792bd1\"},\n+    {file = \"matplotlib-3.7.1-cp39-cp39-macosx_10_12_universal2.whl\", hash = \"sha256:81a6b377ea444336538638d31fdb39af6be1a043ca5e343fe18d0f17e098770b\"},\n+    {file = \"matplotlib-3.7.1-cp39-cp39-macosx_10_12_x86_64.whl\", hash = \"sha256:28506a03bd7f3fe59cd3cd4ceb2a8d8a2b1db41afede01f66c42561b9be7b4b7\"},\n+    {file = \"matplotlib-3.7.1-cp39-cp39-macosx_11_0_arm64.whl\", hash = \"sha256:8c587963b85ce41e0a8af53b9b2de8dddbf5ece4c34553f7bd9d066148dc719c\"},\n+    {file = \"matplotlib-3.7.1-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:8bf26ade3ff0f27668989d98c8435ce9327d24cffb7f07d24ef609e33d582439\"},\n+    {file = \"matplotlib-3.7.1-cp39-cp39-manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:def58098f96a05f90af7e92fd127d21a287068202aa43b2a93476170ebd99e87\"},\n+    {file = \"matplotlib-3.7.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:f883a22a56a84dba3b588696a2b8a1ab0d2c3d41be53264115c71b0a942d8fdb\"},\n+    {file = \"matplotlib-3.7.1-cp39-cp39-win32.whl\", hash = \"sha256:4f99e1b234c30c1e9714610eb0c6d2f11809c9c78c984a613ae539ea2ad2eb4b\"},\n+    {file = \"matplotlib-3.7.1-cp39-cp39-win_amd64.whl\", hash = \"sha256:3ba2af245e36990facf67fde840a760128ddd71210b2ab6406e640188d69d136\"},\n+    {file = \"matplotlib-3.7.1-pp38-pypy38_pp73-macosx_10_12_x86_64.whl\", hash = \"sha256:3032884084f541163f295db8a6536e0abb0db464008fadca6c98aaf84ccf4717\"},\n+    {file = \"matplotlib-3.7.1-pp38-pypy38_pp73-manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:3a2cb34336110e0ed8bb4f650e817eed61fa064acbefeb3591f1b33e3a84fd96\"},\n+    {file = \"matplotlib-3.7.1-pp38-pypy38_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:b867e2f952ed592237a1828f027d332d8ee219ad722345b79a001f49df0936eb\"},\n+    {file = \"matplotlib-3.7.1-pp38-pypy38_pp73-win_amd64.whl\", hash = \"sha256:57bfb8c8ea253be947ccb2bc2d1bb3862c2bccc662ad1b4626e1f5e004557042\"},\n+    {file = \"matplotlib-3.7.1-pp39-pypy39_pp73-macosx_10_12_x86_64.whl\", hash = \"sha256:438196cdf5dc8d39b50a45cb6e3f6274edbcf2254f85fa9b895bf85851c3a613\"},\n+    {file = \"matplotlib-3.7.1-pp39-pypy39_pp73-manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:21e9cff1a58d42e74d01153360de92b326708fb205250150018a52c70f43c290\"},\n+    {file = \"matplotlib-3.7.1-pp39-pypy39_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:75d4725d70b7c03e082bbb8a34639ede17f333d7247f56caceb3801cb6ff703d\"},\n+    {file = \"matplotlib-3.7.1-pp39-pypy39_pp73-win_amd64.whl\", hash = \"sha256:97cc368a7268141afb5690760921765ed34867ffb9655dd325ed207af85c7529\"},\n+    {file = \"matplotlib-3.7.1.tar.gz\", hash = \"sha256:7b73305f25eab4541bd7ee0b96d87e53ae9c9f1823be5659b806cd85786fe882\"},\n ]\n ml4gw = []\n mldatafind = []\n@@ -2187,8 +2186,8 @@ pyrxp = [\n     {file = \"pyRXP-3.0.1.tar.gz\", hash = \"sha256:f5563dee342f329f840380aafccf786b504d6a82e91611defb27f3f51043be9a\"},\n ]\n pytest = [\n-    {file = \"pytest-7.2.1-py3-none-any.whl\", hash = \"sha256:c7c6ca206e93355074ae32f7403e8ea12163b1163c976fee7d4d84027c162be5\"},\n-    {file = \"pytest-7.2.1.tar.gz\", hash = \"sha256:d45e0952f3727241918b8fd0f376f5ff6b301cc0777c6f9a556935c92d8a7d42\"},\n+    {file = \"pytest-7.2.2-py3-none-any.whl\", hash = \"sha256:130328f552dcfac0b1cec75c12e3f005619dc5f874f0a06e8ff7263f0ee6225e\"},\n+    {file = \"pytest-7.2.2.tar.gz\", hash = \"sha256:c99ab0c73aceb050f68929bc93af19ab6db0558791c6a0715723abe9d0ade9d4\"},\n ]\n python-dateutil = [\n     {file = \"python-dateutil-2.8.2.tar.gz\", hash = \"sha256:0123cacc1627ae19ddf3c27a5de5bd67ee4586fbdd6440d9748f8abb483d3e86\"},\n@@ -2249,8 +2248,8 @@ scitokens = [\n     {file = \"scitokens-1.7.4.tar.gz\", hash = \"sha256:d215091b5a66d8cf37d386602495e93f347646f132469091ca44148683193c4e\"},\n ]\n setuptools = [\n-    {file = \"setuptools-67.3.2-py3-none-any.whl\", hash = \"sha256:bb6d8e508de562768f2027902929f8523932fcd1fb784e6d573d2cafac995a48\"},\n-    {file = \"setuptools-67.3.2.tar.gz\", hash = \"sha256:95f00380ef2ffa41d9bba85d95b27689d923c93dfbafed4aecd7cf988a25e012\"},\n+    {file = \"setuptools-67.4.0-py3-none-any.whl\", hash = \"sha256:f106dee1b506dee5102cc3f3e9e68137bbad6d47b616be7991714b0c62204251\"},\n+    {file = \"setuptools-67.4.0.tar.gz\", hash = \"sha256:e5fd0a713141a4a105412233c63dc4e17ba0090c8e8334594ac790ec97792330\"},\n ]\n setuptools-scm = [\n     {file = \"setuptools_scm-7.1.0-py3-none-any.whl\", hash = \"sha256:73988b6d848709e2af142aa48c986ea29592bbcfca5375678064708205253d8e\"},\n@@ -2315,8 +2314,8 @@ torchtyping = [\n     {file = \"torchtyping-0.1.4.tar.gz\", hash = \"sha256:4763375d17752641bd1bff0faaddade29be3c125fca6355e3cee7700e975fdb5\"},\n ]\n tqdm = [\n-    {file = \"tqdm-4.64.1-py2.py3-none-any.whl\", hash = \"sha256:6fee160d6ffcd1b1c68c65f14c829c22832bc401726335ce92c52d395944a6a1\"},\n-    {file = \"tqdm-4.64.1.tar.gz\", hash = \"sha256:5f4f682a004951c1b450bc753c710e9280c5746ce6ffedee253ddbcbf54cf1e4\"},\n+    {file = \"tqdm-4.65.0-py3-none-any.whl\", hash = \"sha256:c4f53a17fe37e132815abceec022631be8ffe1b9381c2e6e30aa70edc99e9671\"},\n+    {file = \"tqdm-4.65.0.tar.gz\", hash = \"sha256:1871fb68a86b8fb3b59ca4cdd3dcccbc7e6d613eeed31f4c332531977b89beb5\"},\n ]\n typeguard = [\n     {file = \"typeguard-2.13.3-py3-none-any.whl\", hash = \"sha256:5e3e3be01e887e7eafae5af63d1f36c849aaa94e3a0112097312aabfa16284f1\"},\n@@ -2336,6 +2335,6 @@ wheel = [\n     {file = \"wheel-0.38.4.tar.gz\", hash = \"sha256:965f5259b566725405b05e7cf774052044b1ed30119b5d586b2703aafe8719ac\"},\n ]\n zipp = [\n-    {file = \"zipp-3.14.0-py3-none-any.whl\", hash = \"sha256:188834565033387710d046e3fe96acfc9b5e86cbca7f39ff69cf21a4128198b7\"},\n-    {file = \"zipp-3.14.0.tar.gz\", hash = \"sha256:9e5421e176ef5ab4c0ad896624e87a7b2f07aca746c9b2aa305952800cb8eecb\"},\n+    {file = \"zipp-3.15.0-py3-none-any.whl\", hash = \"sha256:48904fc76a60e542af151aded95726c1a5c34ed43ab4134b597665c86d7ad556\"},\n+    {file = \"zipp-3.15.0.tar.gz\", hash = \"sha256:112929ad649da941c23de50f356a2b5570c954b65150642bccdd66bf194d224b\"},\n ]\ndiff --git a\/projects\/sandbox\/data_generation\/pyproject.toml b\/projects\/sandbox\/data_generation\/pyproject.toml\nindex 695dfd3..6b842f1 100644\n--- a\/projects\/sandbox\/data_generation\/pyproject.toml\n+++ b\/projects\/sandbox\/data_generation\/pyproject.toml\n@@ -6,6 +6,9 @@ authors = [\"ethan.marx <ethan.marx@ligo.org>\"]\n license = \"MIT\"\n include = [\"data_generation\/\", \"tests\/\"]\n \n+[tool.pinto]\n+conda = {append_base_ld_library_path = true}\n+\n [tool.poetry.dependencies]\n python = \">=3.8,<3.11\"\n \"ml4gw\" = {path = \"..\/..\/..\/ml4gw\/\", develop = true}\ndiff --git a\/projects\/sandbox\/data_generation\/tests\/test_utils.py b\/projects\/sandbox\/data_generation\/tests\/test_utils.py\nnew file mode 100644\nindex 0000000..527586b\n--- \/dev\/null\n+++ b\/projects\/sandbox\/data_generation\/tests\/test_utils.py\n@@ -0,0 +1,42 @@\n+import numpy as np\n+import pytest\n+from data_generation.utils import noise_from_psd\n+\n+\n+@pytest.fixture(params=[1024, 2048])\n+def sample_rate(request):\n+    return request.param\n+\n+\n+@pytest.fixture(params=[0.5, 1])\n+def df(request):\n+    return request.param\n+\n+\n+@pytest.fixture(params=[1, 10])\n+def duration(request):\n+    return request.param\n+\n+\n+def test_noise_from_psd(\n+    sample_rate,\n+    df,\n+    duration,\n+):\n+    fmax = sample_rate \/\/ 2\n+    frequencies = np.arange(0, fmax + df, df)\n+    psd = np.ones(len(frequencies)) * 1e-46\n+\n+    noise = noise_from_psd(psd, df, duration, sample_rate)\n+    assert len(noise) == int(duration * sample_rate)\n+\n+    # inverse fourier transform noise, and check for\n+    # consistent power as psd\n+    from scipy.fft import fft\n+\n+    num_samples = len(noise)\n+    dt = duration \/ num_samples\n+    noise_fft = fft(noise) * dt\n+    noise_fft_abs = np.abs(noise_fft) ** 2\n+\n+    assert np.log10(noise_fft_abs).mean() == pytest.approx(-46, abs=1)\n","files":{"\/projects\/sandbox\/data_generation\/data_generation\/generate_background.py":{"changes":[{"diff":"\n import logging\n from pathlib import Path\n-from typing import List\n+from typing import List, Optional\n \n import h5py\n import numpy as np\n+from data_generation.utils import noise_from_psd\n from gwdatafind import find_urls\n+from gwpy.frequencyseries import FrequencySeries\n from gwpy.segments import DataQualityDict\n from gwpy.timeseries import TimeSeries\n from typeo import scriptify\n \n+from ml4gw.spectral import normalize_psd\n from mlpe.logging import configure_logging\n \n \n","add":4,"remove":1,"filename":"\/projects\/sandbox\/data_generation\/data_generation\/generate_background.py","badparts":["from typing import List"],"goodparts":["from typing import List, Optional","from data_generation.utils import noise_from_psd","from gwpy.frequencyseries import FrequencySeries","from ml4gw.spectral import normalize_psd"]},{"diff":"\n         \"from {} to {}\".format(*segment)\n     )\n \n-    with h5py.File(datadir \/ \"background.h5\", \"w\") as f:\n-        for ifo in ifos:\n+    background_data = {}\n+    for ifo in ifos:\n \n-            # find frame files\n-            files = find_urls(\n-                site=ifo.strip(\"1\"),\n-                frametype=f\"{ifo}_{frame_type}\",\n-                gpsstart=start,\n-                gpsend=stop,\n-                urltype=\"file\",\n-            )\n-            data = TimeSeries.read(\n-                files,\n-                channel=f\"{ifo}:{channel}\",\n-                start=segment[0],\n-                end=segment[1],\n+        # find frame files\n+        files = find_urls(\n+            site=ifo.strip(\"1\"),\n+            frametype=f\"{ifo}_{frame_type}\",\n+            gpsstart=start,\n+            gpsend=stop,\n+            urltype=\"file\",\n+        )\n+        data = TimeSeries.read(\n+            files,\n+            channel=f\"{ifo}:{channel}\",\n+            start=segment[0],\n+            end=segment[1],\n+        )\n+\n+        # resample\n+        data = data.resample(sample_rate)\n+\n+        if np.isnan(data).any():\n+            raise ValueError(\n+                f\"The background for ifo {ifo} contains NaN values\"\n             )\n \n-            # resample\n-            data = data.resample(sample_rate)\n+        if gaussian:\n+            logging.info(f\"Generating gaussian noise from psd for ifo {ifo}\")\n+\n+            # TODO: what should determine this?\n+            df = 1 \/ 8.0\n+            if psd_file is not None:\n+                # if user passed a psd file, load it into a FrequencySeries\n+                # that ml4gw.spectral.normalize_psd can handle\n+                frequencies, psd = np.loadtxt(psd_file, unpack=True)\n+                if not is_psd:\n+                    psd = psd**2\n+\n+                # ml4gw expects psd to start at 0 Hz, so lets check for that\n+                # TODO: implement logic that prepends 0 to psd\n+                # if the passed psd doesn't start at 0\n+                if frequencies[0] != 0:\n+                    raise ValueError(\n+                        \"PSD must start at 0 Hz, not {}\".format(frequencies[0])\n+                    )\n+                data = FrequencySeries(psd, frequencies=frequencies)\n+\n+            # normalize_psd can handle FrequencySeries or TimeSeries\n+            psd = normalize_psd(data, df, sample_rate)\n+            data = noise_from_psd(\n+                psd, df, len(data) \/ sample_rate, sample_rate\n+            )\n \n-            if np.isnan(data).any():\n-                raise ValueError(\n-                    f\"The background for ifo {ifo} contains NaN values\"\n-                )\n+        background_data[ifo] = data\n \n-            f.create_dataset(f\"{ifo}\", data=data)\n-            f.attrs.update({\"t0\": float(segment[0])})\n+    with h5py.File(background_file, \"w\") as f:\n+        for ifo, data in background_data.items():\n+            f.create_dataset(ifo, data=data)\n+        f.attrs.update({\"t0\": float(segment[0])})\n \n     return background_file","add":54,"remove":23,"filename":"\/projects\/sandbox\/data_generation\/data_generation\/generate_background.py","badparts":["    with h5py.File(datadir \/ \"background.h5\", \"w\") as f:","        for ifo in ifos:","            files = find_urls(","                site=ifo.strip(\"1\"),","                frametype=f\"{ifo}_{frame_type}\",","                gpsstart=start,","                gpsend=stop,","                urltype=\"file\",","            )","            data = TimeSeries.read(","                files,","                channel=f\"{ifo}:{channel}\",","                start=segment[0],","                end=segment[1],","            data = data.resample(sample_rate)","            if np.isnan(data).any():","                raise ValueError(","                    f\"The background for ifo {ifo} contains NaN values\"","                )","            f.create_dataset(f\"{ifo}\", data=data)","            f.attrs.update({\"t0\": float(segment[0])})"],"goodparts":["    background_data = {}","    for ifo in ifos:","        files = find_urls(","            site=ifo.strip(\"1\"),","            frametype=f\"{ifo}_{frame_type}\",","            gpsstart=start,","            gpsend=stop,","            urltype=\"file\",","        )","        data = TimeSeries.read(","            files,","            channel=f\"{ifo}:{channel}\",","            start=segment[0],","            end=segment[1],","        )","        data = data.resample(sample_rate)","        if np.isnan(data).any():","            raise ValueError(","                f\"The background for ifo {ifo} contains NaN values\"","        if gaussian:","            logging.info(f\"Generating gaussian noise from psd for ifo {ifo}\")","            df = 1 \/ 8.0","            if psd_file is not None:","                frequencies, psd = np.loadtxt(psd_file, unpack=True)","                if not is_psd:","                    psd = psd**2","                if frequencies[0] != 0:","                    raise ValueError(","                        \"PSD must start at 0 Hz, not {}\".format(frequencies[0])","                    )","                data = FrequencySeries(psd, frequencies=frequencies)","            psd = normalize_psd(data, df, sample_rate)","            data = noise_from_psd(","                psd, df, len(data) \/ sample_rate, sample_rate","            )","        background_data[ifo] = data","    with h5py.File(background_file, \"w\") as f:","        for ifo, data in background_data.items():","            f.create_dataset(ifo, data=data)","        f.attrs.update({\"t0\": float(segment[0])})"]}],"source":"\nimport logging from pathlib import Path from typing import List import h5py import numpy as np from gwdatafind import find_urls from gwpy.segments import DataQualityDict from gwpy.timeseries import TimeSeries from typeo import scriptify from mlpe.logging import configure_logging @scriptify def main( start: float, stop: float, ifos: List[str], sample_rate: float, channel: str, frame_type: str, state_flag: str, minimum_length: float, datadir: Path, logdir: Path, force_generation: bool=False, verbose: bool=False, ): \"\"\"Generates background data for training BBHnet Args: start: start gpstime stop: stop gpstime ifos: which ifos to query data for outdir: where to store data \"\"\" logdir.mkdir(exist_ok=True, parents=True) datadir.mkdir(exist_ok=True, parents=True) configure_logging(logdir \/ \"generate_background.log\", verbose) background_file=datadir \/ \"background.h5\" if background_file.exists() and not force_generation: logging.info( \"Background data already exists\" \" and forced generation is off. Not generating background\" ) return background_file segments=DataQualityDict.query_dqsegdb( [f\"{ifo}:{state_flag}\" for ifo in ifos], start, stop, ) intersection=segments[f\"{ifos[0]}:{state_flag}\"].active.copy() for ifo in ifos: intersection &=segments[f\"{ifo}:{state_flag}\"].active segment_lengths=np.array( [float(seg[1] -seg[0]) for seg in intersection] ) continuous_segments=np.where(segment_lengths >=minimum_length)[0] if len(continuous_segments)==0: raise ValueError( \"No segments of minimum length, not producing background\" ) segment=intersection[continuous_segments[0]] logging.info( \"Querying coincident, continuous segment \" \"from{} to{}\".format(*segment) ) with h5py.File(datadir \/ \"background.h5\", \"w\") as f: for ifo in ifos: files=find_urls( site=ifo.strip(\"1\"), frametype=f\"{ifo}_{frame_type}\", gpsstart=start, gpsend=stop, urltype=\"file\", ) data=TimeSeries.read( files, channel=f\"{ifo}:{channel}\", start=segment[0], end=segment[1], ) data=data.resample(sample_rate) if np.isnan(data).any(): raise ValueError( f\"The background for ifo{ifo} contains NaN values\" ) f.create_dataset(f\"{ifo}\", data=data) f.attrs.update({\"t0\": float(segment[0])}) return background_file ","sourceWithComments":"import logging\nfrom pathlib import Path\nfrom typing import List\n\nimport h5py\nimport numpy as np\nfrom gwdatafind import find_urls\nfrom gwpy.segments import DataQualityDict\nfrom gwpy.timeseries import TimeSeries\nfrom typeo import scriptify\n\nfrom mlpe.logging import configure_logging\n\n\n@scriptify\ndef main(\n    start: float,\n    stop: float,\n    ifos: List[str],\n    sample_rate: float,\n    channel: str,\n    frame_type: str,\n    state_flag: str,\n    minimum_length: float,\n    datadir: Path,\n    logdir: Path,\n    force_generation: bool = False,\n    verbose: bool = False,\n):\n    \"\"\"Generates background data for training BBHnet\n\n    Args:\n        start: start gpstime\n        stop: stop gpstime\n        ifos: which ifos to query data for\n        outdir: where to store data\n    \"\"\"\n\n    # make logdir dir\n    logdir.mkdir(exist_ok=True, parents=True)\n    datadir.mkdir(exist_ok=True, parents=True)\n\n    # configure logging output file\n    configure_logging(logdir \/ \"generate_background.log\", verbose)\n\n    # check if paths already exist\n    # TODO: maybe put all background in one path\n    background_file = datadir \/ \"background.h5\"\n\n    if background_file.exists() and not force_generation:\n        logging.info(\n            \"Background data already exists\"\n            \" and forced generation is off. Not generating background\"\n        )\n        return background_file\n\n    # query segments for each ifo\n    # I think a certificate is needed for this\n    segments = DataQualityDict.query_dqsegdb(\n        [f\"{ifo}:{state_flag}\" for ifo in ifos],\n        start,\n        stop,\n    )\n\n    # create copy of first ifo segment list to start\n    intersection = segments[f\"{ifos[0]}:{state_flag}\"].active.copy()\n\n    # loop over ifos finding segment intersection\n    for ifo in ifos:\n        intersection &= segments[f\"{ifo}:{state_flag}\"].active\n\n    # find first continuous segment of minimum length\n    segment_lengths = np.array(\n        [float(seg[1] - seg[0]) for seg in intersection]\n    )\n    continuous_segments = np.where(segment_lengths >= minimum_length)[0]\n\n    if len(continuous_segments) == 0:\n        raise ValueError(\n            \"No segments of minimum length, not producing background\"\n        )\n\n    # choose first of such segments\n    segment = intersection[continuous_segments[0]]\n\n    logging.info(\n        \"Querying coincident, continuous segment \"\n        \"from {} to {}\".format(*segment)\n    )\n\n    with h5py.File(datadir \/ \"background.h5\", \"w\") as f:\n        for ifo in ifos:\n\n            # find frame files\n            files = find_urls(\n                site=ifo.strip(\"1\"),\n                frametype=f\"{ifo}_{frame_type}\",\n                gpsstart=start,\n                gpsend=stop,\n                urltype=\"file\",\n            )\n            data = TimeSeries.read(\n                files,\n                channel=f\"{ifo}:{channel}\",\n                start=segment[0],\n                end=segment[1],\n            )\n\n            # resample\n            data = data.resample(sample_rate)\n\n            if np.isnan(data).any():\n                raise ValueError(\n                    f\"The background for ifo {ifo} contains NaN values\"\n                )\n\n            f.create_dataset(f\"{ifo}\", data=data)\n            f.attrs.update({\"t0\": float(segment[0])})\n\n    return background_file\n"},"\/projects\/sandbox\/data_generation\/data_generation\/generate_testing_set.py":{"changes":[{"diff":"\n import h5py\n import numpy as np\n import torch\n-from data_generation.utils import download_data, inject_into_background\n+from data_generation.utils import (\n+    download_data,\n+    inject_into_background,\n+    noise_from_psd,\n+)\n+from gwpy.timeseries import TimeSeries\n from mldatafind.segments import query_segments\n from typeo import scriptify\n \n from ml4gw.gw import compute_observed_strain, get_ifo_geometry\n+from ml4gw.spectral import normalize_psd\n from mlpe.injection import generate_gw\n from mlpe.logging import configure_logging\n \n","add":7,"remove":1,"filename":"\/projects\/sandbox\/data_generation\/data_generation\/generate_testing_set.py","badparts":["from data_generation.utils import download_data, inject_into_background"],"goodparts":["from data_generation.utils import (","    download_data,","    inject_into_background,","    noise_from_psd,",")","from gwpy.timeseries import TimeSeries","from ml4gw.spectral import normalize_psd"]},{"diff":"\n     and injected into kernels sampled non-coincidentally from the background.\n \n     Args:\n-        ifos: List of interferometers\n-        state_flag: Flag used to find times with good data quality.\n-        frame_type: Frame type used for discovering data\n-        channel: Channel for reading data\n-        start: Start time for finding data\n-        stop: Stop time for finding data\n-        sample_rate: Rate at which timeseries are sampled\n-        prior: Callable that instantiates a bilby prior\n-        waveform: A callable compatible with bilby waveform generator\n-        n_samples: Number of waveforms to sample and inject\n-        kernel_length: Length in seconds of kernels produced\n-        waveform_duration: length of injected waveforms\n-        datadir: Path to store data\n-        logdir: Path to store logs\n-        min_duration: Minimum duration of segments\n+        ifos:\n+            List of interferometers\n+        state_flag:\n+            Flag used to find times with good data quality.\n+        frame_type:\n+            Frame type used for discovering data\n+        channel:\n+            Channel for reading data\n+        start:\n+            Start time for finding data\n+        stop:\n+            Stop time for finding data\n+        sample_rate:\n+            Rate at which timeseries are sampled\n+        prior:\n+            Callable that instantiates a bilby prior\n+        waveform:\n+            A callable compatible with bilby waveform generator\n+        n_samples:\n+            Number of waveforms to sample and inject\n+        kernel_length:\n+            Length in seconds of kernels produced\n+        waveform_duration:\n+            length of injected waveforms\n+        datadir:\n+            Path to store data\n+        logdir:\n+            Path to store logs\n+        min_duration:\n+            Minimum duration of segments\n         waveform_arguments:\n             Additional arguments to pass to waveform generator,\n             that will ultimately get passed\n","add":30,"remove":15,"filename":"\/projects\/sandbox\/data_generation\/data_generation\/generate_testing_set.py","badparts":["        ifos: List of interferometers","        state_flag: Flag used to find times with good data quality.","        frame_type: Frame type used for discovering data","        channel: Channel for reading data","        start: Start time for finding data","        stop: Stop time for finding data","        sample_rate: Rate at which timeseries are sampled","        prior: Callable that instantiates a bilby prior","        waveform: A callable compatible with bilby waveform generator","        n_samples: Number of waveforms to sample and inject","        kernel_length: Length in seconds of kernels produced","        waveform_duration: length of injected waveforms","        datadir: Path to store data","        logdir: Path to store logs","        min_duration: Minimum duration of segments"],"goodparts":["        ifos:","            List of interferometers","        state_flag:","            Flag used to find times with good data quality.","        frame_type:","            Frame type used for discovering data","        channel:","            Channel for reading data","        start:","            Start time for finding data","        stop:","            Stop time for finding data","        sample_rate:","            Rate at which timeseries are sampled","        prior:","            Callable that instantiates a bilby prior","        waveform:","            A callable compatible with bilby waveform generator","        n_samples:","            Number of waveforms to sample and inject","        kernel_length:","            Length in seconds of kernels produced","        waveform_duration:","            length of injected waveforms","        datadir:","            Path to store data","        logdir:","            Path to store logs","        min_duration:","            Minimum duration of segments"]},{"diff":"\n             Parameter conversion to pass the bilby waveform generator.\n             Typically used for converting between bilby and lalsimulation\n             BBH parameters\n-        force_generation: Force generation of data\n-        verbose: Log verbosely\n+        gaussian:\n+            If True, generate gaussian noise from a psd calculated using\n+            the requested period of data. This will be used as the background\n+            to inject the waveforms into.\n+        force_generation:\n+            Force generation of data\n+        verbose:\n+            Log verbosely\n \n     Returns signal file containiing injections and parameters\n     \"\"\"\n","add":8,"remove":2,"filename":"\/projects\/sandbox\/data_generation\/data_generation\/generate_testing_set.py","badparts":["        force_generation: Force generation of data","        verbose: Log verbosely"],"goodparts":["        gaussian:","            If True, generate gaussian noise from a psd calculated using","            the requested period of data. This will be used as the background","            to inject the waveforms into.","        force_generation:","            Force generation of data","        verbose:","            Log verbosely"]}],"source":"\nimport logging from pathlib import Path from typing import Callable, Dict, List, Optional import h5py import numpy as np import torch from data_generation.utils import download_data, inject_into_background from mldatafind.segments import query_segments from typeo import scriptify from ml4gw.gw import compute_observed_strain, get_ifo_geometry from mlpe.injection import generate_gw from mlpe.logging import configure_logging @scriptify def main( ifos: List[str], state_flag: str, frame_type: str, channel: str, start: float, stop: float, sample_rate: float, prior: Callable, waveform: Callable, n_samples: int, kernel_length: float, waveform_duration: float, datadir: Path, logdir: Path, min_duration: float=0, waveform_arguments: Optional[Dict]=None, parameter_conversion: Optional[Callable]=None, force_generation: bool=False, verbose: bool=False, ): \"\"\" Generate test dataset of kernels by injecting waveforms into random, non-coincident background kernels Queries first contiguous segment of background between start and stop that satisifies `min_duration` and `state_flag` requirements. Then, a dataset of waveforms is generated from the given prior, and injected into kernels sampled non-coincidentally from the background. Args: ifos: List of interferometers state_flag: Flag used to find times with good data quality. frame_type: Frame type used for discovering data channel: Channel for reading data start: Start time for finding data stop: Stop time for finding data sample_rate: Rate at which timeseries are sampled prior: Callable that instantiates a bilby prior waveform: A callable compatible with bilby waveform generator n_samples: Number of waveforms to sample and inject kernel_length: Length in seconds of kernels produced waveform_duration: length of injected waveforms datadir: Path to store data logdir: Path to store logs min_duration: Minimum duration of segments waveform_arguments: Additional arguments to pass to waveform generator, that will ultimately get passed to the waveform callable specified. For example, generating BBH waveforms requires the specification of a waveform_approximant parameter_conversion: Parameter conversion to pass the bilby waveform generator. Typically used for converting between bilby and lalsimulation BBH parameters force_generation: Force generation of data verbose: Log verbosely Returns signal file containiing injections and parameters \"\"\" configure_logging(logdir \/ \"generate_testing_set.log\", verbose) datadir.mkdir(exist_ok=True, parents=True) logdir.mkdir(exist_ok=True, parents=True) signal_file=datadir \/ \"test_injections.h5\" if signal_file.exists() and not force_generation: logging.info( \"Signal data already exists and forced generation is off. \" \"Not generating testing signals.\" ) return signal_file segment_names=[f\"{ifo}:{state_flag}\" for ifo in ifos] segment_start, segment_stop=query_segments( segment_names, start, stop, min_duration, )[0] background_dict=download_data( ifos, frame_type, channel, sample_rate, segment_start, segment_stop ) background=np.stack([ts.value for ts in background_dict.values()]) background=torch.as_tensor(background) prior=prior() parameters=prior.sample(n_samples) signals=generate_gw( parameters, sample_rate, waveform_duration, waveform, waveform_arguments=waveform_arguments, parameter_conversion=parameter_conversion, ) plus, cross=signals.transpose(1, 0, 2) plus=torch.Tensor(plus) cross=torch.Tensor(cross) kernel_size=int(kernel_length * sample_rate) tensors, vertices=get_ifo_geometry(*ifos) dec=torch.Tensor(parameters[\"dec\"]) psi=torch.Tensor(parameters[\"psi\"]) phi=torch.Tensor(parameters[\"phi\"]) waveforms=compute_observed_strain( dec, psi, phi, tensors, vertices, sample_rate, plus=plus, cross=cross, ) injections=inject_into_background( background, waveforms, kernel_size, ) with h5py.File(signal_file, \"w\") as f: f.create_dataset(\"injections\", data=injections) for name, value in parameters.items(): f.create_dataset(name, data=value) f.attrs.update( { \"size\": n_samples, \"sample_rate\": sample_rate, \"waveform_duration\": waveform_duration, \"waveform\": waveform.__name__, } ) if waveform_arguments is not None: f.attrs.update(waveform_arguments) return signal_file if __name__==\"__main__\": main() ","sourceWithComments":"import logging\nfrom pathlib import Path\nfrom typing import Callable, Dict, List, Optional\n\nimport h5py\nimport numpy as np\nimport torch\nfrom data_generation.utils import download_data, inject_into_background\nfrom mldatafind.segments import query_segments\nfrom typeo import scriptify\n\nfrom ml4gw.gw import compute_observed_strain, get_ifo_geometry\nfrom mlpe.injection import generate_gw\nfrom mlpe.logging import configure_logging\n\n\n@scriptify\ndef main(\n    ifos: List[str],\n    state_flag: str,\n    frame_type: str,\n    channel: str,\n    start: float,\n    stop: float,\n    sample_rate: float,\n    prior: Callable,\n    waveform: Callable,\n    n_samples: int,\n    kernel_length: float,\n    waveform_duration: float,\n    datadir: Path,\n    logdir: Path,\n    min_duration: float = 0,\n    waveform_arguments: Optional[Dict] = None,\n    parameter_conversion: Optional[Callable] = None,\n    force_generation: bool = False,\n    verbose: bool = False,\n):\n    \"\"\"\n    Generate test dataset of kernels by injecting waveforms\n    into random, non-coincident background kernels\n\n    Queries first contiguous segment of background between start and stop\n    that satisifies `min_duration` and `state_flag` requirements.\n\n    Then, a dataset of waveforms is generated from the given prior,\n    and injected into kernels sampled non-coincidentally from the background.\n\n    Args:\n        ifos: List of interferometers\n        state_flag: Flag used to find times with good data quality.\n        frame_type: Frame type used for discovering data\n        channel: Channel for reading data\n        start: Start time for finding data\n        stop: Stop time for finding data\n        sample_rate: Rate at which timeseries are sampled\n        prior: Callable that instantiates a bilby prior\n        waveform: A callable compatible with bilby waveform generator\n        n_samples: Number of waveforms to sample and inject\n        kernel_length: Length in seconds of kernels produced\n        waveform_duration: length of injected waveforms\n        datadir: Path to store data\n        logdir: Path to store logs\n        min_duration: Minimum duration of segments\n        waveform_arguments:\n            Additional arguments to pass to waveform generator,\n            that will ultimately get passed\n            to the waveform callable specified. For example,\n            generating BBH waveforms requires the specification of a\n            waveform_approximant\n        parameter_conversion:\n            Parameter conversion to pass the bilby waveform generator.\n            Typically used for converting between bilby and lalsimulation\n            BBH parameters\n        force_generation: Force generation of data\n        verbose: Log verbosely\n\n    Returns signal file containiing injections and parameters\n    \"\"\"\n\n    configure_logging(logdir \/ \"generate_testing_set.log\", verbose)\n\n    datadir.mkdir(exist_ok=True, parents=True)\n    logdir.mkdir(exist_ok=True, parents=True)\n\n    signal_file = datadir \/ \"test_injections.h5\"\n\n    if signal_file.exists() and not force_generation:\n        logging.info(\n            \"Signal data already exists and forced generation is off. \"\n            \"Not generating testing signals.\"\n        )\n        return signal_file\n\n    # query the first coincident contiguous segment\n    # of required minimum duration and download the data\n    segment_names = [f\"{ifo}:{state_flag}\" for ifo in ifos]\n    segment_start, segment_stop = query_segments(\n        segment_names,\n        start,\n        stop,\n        min_duration,\n    )[0]\n\n    background_dict = download_data(\n        ifos, frame_type, channel, sample_rate, segment_start, segment_stop\n    )\n\n    background = np.stack([ts.value for ts in background_dict.values()])\n    background = torch.as_tensor(background)\n\n    # instantiate prior, sample, and generate signals\n    prior = prior()\n    parameters = prior.sample(n_samples)\n\n    signals = generate_gw(\n        parameters,\n        sample_rate,\n        waveform_duration,\n        waveform,\n        waveform_arguments=waveform_arguments,\n        parameter_conversion=parameter_conversion,\n    )\n\n    plus, cross = signals.transpose(1, 0, 2)\n    plus = torch.Tensor(plus)\n    cross = torch.Tensor(cross)\n\n    kernel_size = int(kernel_length * sample_rate)\n\n    # project raw polarizations onto interferometers\n    # with sampled sky localizations\n    tensors, vertices = get_ifo_geometry(*ifos)\n\n    # dec is declination\n    # psi is polarization angle\n    # phi is relative azimuthal angle between source and earth\n    dec = torch.Tensor(parameters[\"dec\"])\n    psi = torch.Tensor(parameters[\"psi\"])\n    phi = torch.Tensor(parameters[\"phi\"])\n\n    waveforms = compute_observed_strain(\n        dec,\n        psi,\n        phi,\n        tensors,\n        vertices,\n        sample_rate,\n        plus=plus,\n        cross=cross,\n    )\n\n    # inject signals into randomly sampled kernels of background\n    injections = inject_into_background(\n        background,\n        waveforms,\n        kernel_size,\n    )\n\n    # write signals and parameters used to generate them\n    with h5py.File(signal_file, \"w\") as f:\n\n        f.create_dataset(\"injections\", data=injections)\n\n        for name, value in parameters.items():\n            f.create_dataset(name, data=value)\n\n        # write attributes\n        f.attrs.update(\n            {\n                \"size\": n_samples,\n                \"sample_rate\": sample_rate,\n                \"waveform_duration\": waveform_duration,\n                \"waveform\": waveform.__name__,\n            }\n        )\n        if waveform_arguments is not None:\n            f.attrs.update(waveform_arguments)\n\n    return signal_file\n\n\nif __name__ == \"__main__\":\n    main()\n"}},"msg":"generate gaussian background, and injections in gaussian noise (#60)\n\n* add toggle for gaussian noise\r\n\r\n* update doocstring\r\n\r\n* add logging statement\r\n\r\n* allow psd from file\r\n\r\n* add is_psd argument\r\n\r\n* remove pycbc dep\r\n\r\n* begin unit tessts for oise_from_psd\r\n\r\n* invert timeseries and check for constant power\r\n\r\n* incorporate new noise_fom_psd function\r\n\r\n---------\r\n\r\nCo-authored-by: Deep Chatterjee <deep.chatterjee@ligo.org>"}}}