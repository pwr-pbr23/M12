{"https:\/\/github.com\/vanvalenlab\/deepcell-label":{"418a3c63ad444c60d3a33242d270dfe173dda592":{"url":"https:\/\/api.github.com\/repos\/vanvalenlab\/deepcell-label\/commits\/418a3c63ad444c60d3a33242d270dfe173dda592","html_url":"https:\/\/github.com\/vanvalenlab\/deepcell-label\/commit\/418a3c63ad444c60d3a33242d270dfe173dda592","sha":"418a3c63ad444c60d3a33242d270dfe173dda592","keyword":"flooding fix","diff":"diff --git a\/deepcell_label\/label.py b\/deepcell_label\/label.py\nindex a38afe775..5453015d7 100644\n--- a\/deepcell_label\/label.py\n+++ b\/deepcell_label\/label.py\n@@ -248,16 +248,9 @@ def action_flood(self, foreground, background, x, y):\n             x (int): x coordinate of region to flood\n             y (int): y coordinate of region to flood\n         \"\"\"\n-        if background == 0:\n-            mask = self.get_mask(background)\n-            # Lower connectivity helps prevent flooding whole image\n-            flooded = flood(mask, (y, x), connectivity=1)\n-            self.add_mask(flooded, foreground)\n-        else:\n-            mask = self.get_mask(background)\n-            flooded = flood(mask, (y, x), connectivity=2) & mask\n-            self.remove_mask(flooded, background)\n-            self.add_mask(flooded, foreground)\n+        mask = self.get_mask(background)\n+        flooded = flood(mask, (y, x), connectivity=2)\n+        self.add_mask(flooded, foreground)\n \n     def action_watershed(self, cell, x1, y1, x2, y2):\n         \"\"\"Use watershed to segment different objects\"\"\"\n","message":"","files":{"\/deepcell_label\/label.py":{"changes":[{"diff":"\n             x (int): x coordinate of region to flood\n             y (int): y coordinate of region to flood\n         \"\"\"\n-        if background == 0:\n-            mask = self.get_mask(background)\n-            # Lower connectivity helps prevent flooding whole image\n-            flooded = flood(mask, (y, x), connectivity=1)\n-            self.add_mask(flooded, foreground)\n-        else:\n-            mask = self.get_mask(background)\n-            flooded = flood(mask, (y, x), connectivity=2) & mask\n-            self.remove_mask(flooded, background)\n-            self.add_mask(flooded, foreground)\n+        mask = self.get_mask(background)\n+        flooded = flood(mask, (y, x), connectivity=2)\n+        self.add_mask(flooded, foreground)\n \n     def action_watershed(self, cell, x1, y1, x2, y2):\n         \"\"\"Use watershed to segment different objects\"\"\"\n","add":3,"remove":10,"filename":"\/deepcell_label\/label.py","badparts":["        if background == 0:","            mask = self.get_mask(background)","            flooded = flood(mask, (y, x), connectivity=1)","            self.add_mask(flooded, foreground)","        else:","            mask = self.get_mask(background)","            flooded = flood(mask, (y, x), connectivity=2) & mask","            self.remove_mask(flooded, background)","            self.add_mask(flooded, foreground)"],"goodparts":["        mask = self.get_mask(background)","        flooded = flood(mask, (y, x), connectivity=2)","        self.add_mask(flooded, foreground)"]}],"source":"\n\"\"\"Classes to view and edit DeepCell Label Projects\"\"\" from __future__ import absolute_import, division, print_function import io import json import zipfile import numpy as np import skimage from matplotlib.colors import Normalize from skimage import filters from skimage.exposure import rescale_intensity from skimage.measure import regionprops from skimage.morphology import dilation, disk, erosion, flood, square from skimage.segmentation import morphological_chan_vese, watershed class Edit(object): \"\"\" Loads labeled data from a zip file, edits the labels according to edit.json in the zip, and writes the edited labels to a new zip file. \"\"\" def __init__(self, labels_zip): self.valid_modes=['overlap', 'overwrite', 'exclude'] self.raw_required=['watershed', 'active_contour', 'threshold'] self.load(labels_zip) self.dispatch_action() self.write_response_zip() @property def new_value(self): \"\"\"Returns a value not in the segmentation.\"\"\" if len(self.cells)==0: return 1 return max(map(lambda c: c['value'], self.cells)) +1 @property def new_cell(self): \"\"\"Returns a cell not in the segmentation.\"\"\" if len(self.cells)==0: return 1 return max(map(lambda c: c['cell'], self.cells)) +1 def load(self, labels_zip): \"\"\" Load the project data to edit from a zip file. \"\"\" if not zipfile.is_zipfile(labels_zip): raise ValueError('Attached labels.zip is not a zip file.') zf=zipfile.ZipFile(labels_zip) if 'edit.json' not in zf.namelist(): raise ValueError('Attached labels.zip must contain edit.json.') with zf.open('edit.json') as f: edit=json.load(f) if 'action' not in edit: raise ValueError('No action specified in edit.json.') self.action=edit['action'] self.height=edit['height'] self.width=edit['width'] self.args=edit.get('args', None) self.write_mode=edit.get('writeMode', 'overlap') if self.write_mode not in self.valid_modes: raise ValueError( f'Invalid writeMode{self.write_mode} in edit.json. Choose from cell, overwrite, or exclude.' ) if 'labeled.dat' not in zf.namelist(): raise ValueError('zip must contain labeled.dat.') with zf.open('labeled.dat') as f: labels=np.frombuffer(f.read(), np.int32) self.initial_labels=np.reshape(labels,(self.width, self.height)) self.labels=self.initial_labels.copy() if 'cells.json' not in zf.namelist(): raise ValueError('zip must contain cells.json.') with zf.open('cells.json') as f: self.cells=json.load(f) if 'raw.dat' in zf.namelist(): with zf.open('raw.dat') as f: raw=np.frombuffer(f.read(), np.uint8) self.raw=np.reshape(raw,(self.width, self.height)) elif self.action in self.raw_required: raise ValueError( f'Include raw array in raw.json to use action{self.action}.' ) def write_response_zip(self): \"\"\"Write edited segmentation to zip.\"\"\" f=io.BytesIO() with zipfile.ZipFile(f, 'w', compression=zipfile.ZIP_DEFLATED) as zf: zf.writestr('labeled.dat', self.labels.tobytes()) zf.writestr('cells.json', json.dumps(self.cells)) f.seek(0) self.response_zip=f def get_cells(self, value): \"\"\" Returns a list of cells encoded by the value \"\"\" return list( map(lambda c: c['cell'], filter(lambda c: c['value']==value, self.cells)) ) def get_values(self, cell): \"\"\" Returns a list of values that encode a cell \"\"\" return list( map(lambda c: c['value'], filter(lambda c: c['cell']==cell, self.cells)) ) def get_value(self, cells): \"\"\" Returns the value that encodes the list of cells \"\"\" if cells==[]: return 0 values=set(map(lambda c: c['value'], self.cells)) for cell in cells: values=values & set(self.get_values(cell)) if len(values)==0: value=self.new_value for cell in cells: self.cells.append({'value': value, 'cell': cell}) return value return values.pop() def get_mask(self, cell): \"\"\" Returns a boolean mask of the cell(or the background when cell==0) \"\"\" if cell==0: return self.labels==0 mask=np.zeros(self.labels.shape, dtype=bool) for value in self.get_values(cell): mask[self.labels==value]=True return mask def add_mask(self, mask, cell): if self.write_mode=='overwrite': self.labels[mask]=self.get_value([cell]) elif self.write_mode=='exclude': mask=mask &(self.labels==0) self.labels[mask]=self.get_value([cell]) else: self.overlap_mask(mask, cell) def remove_mask(self, mask, cell): self.overlap_mask(mask, cell, remove=True) def overlap_mask(self, mask, cell, remove=False): \"\"\" Adds the cell to the segmentation in the mask area, overlapping with existing cells. \"\"\" values=np.unique(self.labels[mask]) for value in values: cells=self.get_cells(value) if remove: if cell in cells: cells.remove(cell) else: cells.append(cell) new_value=self.get_value(cells) self.labels[mask &(self.labels==value)]=new_value def clean_cell(self, cell): \"\"\"Ensures that a cell is a positive integer\"\"\" return int(max(0, cell)) def dispatch_action(self): \"\"\" Call an action method based on an action type. Args: action(str): name of action method after \"action_\" e.g. \"draw\" to call \"action_draw\" info(dict): key value pairs with arguments for action \"\"\" attr_name='action_{}'.format(self.action) try: action_fn=getattr(self, attr_name) except AttributeError: raise ValueError('Invalid action \"{}\"'.format(self.action)) action_fn(**self.args) def action_draw(self, trace, brush_size, cell, erase=False): \"\"\" Use a \"brush\" to draw in the brush value along trace locations of the annotated data. Args: trace(list): list of(x, y) coordinates where the brush has painted brush_size(int): radius of the brush in pixels cell(int): cell to edit with the brush erase(bool): whether to add or remove label from brush stroke area \"\"\" trace=json.loads(trace) brush_mask=np.zeros(self.labels.shape, dtype=bool) for loc in trace: x=loc[0] y=loc[1] disk=skimage.draw.disk((y, x), brush_size, shape=self.labels.shape) brush_mask[disk]=True if erase: self.remove_mask(brush_mask, cell) else: self.add_mask(brush_mask, cell) def action_trim_pixels(self, cell, x, y): \"\"\" Removes parts of cell not connected to(x, y). Args: cell(int): cell to trim x(int): x position of seed y(int): y position of seed \"\"\" mask=self.get_mask(cell) if mask[y, x]: connected_mask=flood(mask,(y, x)) self.remove_mask(~connected_mask, cell) def action_flood(self, foreground, background, x, y): \"\"\" Floods the connected component of the background label at(x, y) with the foreground label. When the background label is 0, does not flood diagonally connected pixels. Args: foreground(int): label to flood with bacgkround(int): label to flood x(int): x coordinate of region to flood y(int): y coordinate of region to flood \"\"\" if background==0: mask=self.get_mask(background) flooded=flood(mask,(y, x), connectivity=1) self.add_mask(flooded, foreground) else: mask=self.get_mask(background) flooded=flood(mask,(y, x), connectivity=2) & mask self.remove_mask(flooded, background) self.add_mask(flooded, foreground) def action_watershed(self, cell, x1, y1, x2, y2): \"\"\"Use watershed to segment different objects\"\"\" new_cell=self.new_cell markers=np.zeros(self.labels.shape) markers[y1, x1]=cell markers[y2, x2]=new_cell mask=self.get_mask(cell) props=regionprops(mask.astype(np.uint8)) top, left, bottom, right=props[0].bbox raw=np.copy(self.raw[top:bottom, left:right]) markers=np.copy(markers[top:bottom, left:right]) mask=np.copy(mask[top:bottom, left:right]) raw=-rescale_intensity(raw) results=watershed(raw, markers, mask=mask) if np.sum(results==new_cell) < 5: dilated=dilation(results==new_cell, disk(3)) results[dilated]=new_cell if np.sum(results==cell) < 5: dilated=dilation(results==cell, disk(3)) results[dilated]=cell new_cell_mask=np.zeros(self.labels.shape, dtype=bool) cell_mask=np.zeros(self.labels.shape, dtype=bool) new_cell_mask[top:bottom, left:right]=results==new_cell cell_mask[top:bottom, left:right]=results==cell self.remove_mask(self.get_mask(cell), cell) self.add_mask(cell_mask, cell) self.add_mask(new_cell_mask, new_cell) def action_threshold(self, y1, x1, y2, x2, cell): \"\"\" Threshold the raw image for annotation prediction within the user-determined bounding box. Args: y1(int): first y coordinate to bound threshold area x1(int): first x coordinate to bound threshold area y2(int): second y coordinate to bound threshold area x2(int): second x coordinate to bound threshold area cell(int): cell drawn in threshold area \"\"\" cell=self.clean_cell(cell) top=min(y1, y2) bottom=max(y1, y2) +1 left=min(x1, x2) right=max(x1, x2) +1 image=self.raw[top:bottom, left:right].astype('float64') low=filters.threshold_triangle(image=image) high=1.10 * low thresholded=filters.apply_hysteresis_threshold(image, low, high) mask=np.zeros(self.labels.shape, dtype=bool) mask[top:bottom, left:right]=thresholded self.add_mask(mask, cell) def action_active_contour(self, cell, min_pixels=20, iterations=100, dilate=0): \"\"\" Uses active contouring to reshape a cell to match the raw image. \"\"\" mask=self.get_mask(cell) props=regionprops(mask.astype(np.uint8))[0] top, left, bottom, right=props.bbox cell_height=bottom -top cell_width=right -left height, width=self.labels.shape top=max(0, top -height \/\/ 2) bottom=min(height, bottom +cell_height \/\/ 2) left=max(0, left -width \/\/ 2) right=min(width, right +cell_width \/\/ 2) init_level_set=mask[top:bottom, left:right] image=Normalize()(self.raw)[top:bottom, left:right] contoured=morphological_chan_vese( image, iterations, init_level_set=init_level_set ) contoured=dilation(contoured, disk(dilate)) regions=skimage.measure.label(contoured) if np.any(regions): largest_component=regions==( np.argmax(np.bincount(regions.flat)[1:]) +1 ) mask=np.zeros(self.labels.shape, dtype=bool) mask[top:bottom, left:right]=largest_component if np.count_nonzero(mask) >=min_pixels: self.remove_mask(~mask, cell) self.add_mask(mask, cell) def action_erode(self, cell): \"\"\" Shrink the selected cell. \"\"\" mask=self.get_mask(cell) eroded=erosion(mask, square(3)) self.remove_mask(mask & ~eroded, cell) def action_dilate(self, cell): \"\"\" Expand the selected cell. \"\"\" mask=self.get_mask(cell) dilated=dilation(mask, square(3)) self.add_mask(dilated, cell) ","sourceWithComments":"\"\"\"Classes to view and edit DeepCell Label Projects\"\"\"\nfrom __future__ import absolute_import, division, print_function\n\nimport io\nimport json\nimport zipfile\n\nimport numpy as np\nimport skimage\nfrom matplotlib.colors import Normalize\nfrom skimage import filters\nfrom skimage.exposure import rescale_intensity\nfrom skimage.measure import regionprops\nfrom skimage.morphology import dilation, disk, erosion, flood, square\nfrom skimage.segmentation import morphological_chan_vese, watershed\n\n\nclass Edit(object):\n    \"\"\"\n    Loads labeled data from a zip file,\n    edits the labels according to edit.json in the zip,\n    and writes the edited labels to a new zip file.\n    \"\"\"\n\n    def __init__(self, labels_zip):\n\n        self.valid_modes = ['overlap', 'overwrite', 'exclude']\n        self.raw_required = ['watershed', 'active_contour', 'threshold']\n\n        self.load(labels_zip)\n        self.dispatch_action()\n        self.write_response_zip()\n\n    @property\n    def new_value(self):\n        \"\"\"Returns a value not in the segmentation.\"\"\"\n        if len(self.cells) == 0:\n            return 1\n        return max(map(lambda c: c['value'], self.cells)) + 1\n\n    @property\n    def new_cell(self):\n        \"\"\"Returns a cell not in the segmentation.\"\"\"\n        if len(self.cells) == 0:\n            return 1\n        return max(map(lambda c: c['cell'], self.cells)) + 1\n\n    def load(self, labels_zip):\n        \"\"\"\n        Load the project data to edit from a zip file.\n        \"\"\"\n        if not zipfile.is_zipfile(labels_zip):\n            raise ValueError('Attached labels.zip is not a zip file.')\n        zf = zipfile.ZipFile(labels_zip)\n\n        # Load edit args\n        if 'edit.json' not in zf.namelist():\n            raise ValueError('Attached labels.zip must contain edit.json.')\n        with zf.open('edit.json') as f:\n            edit = json.load(f)\n            if 'action' not in edit:\n                raise ValueError('No action specified in edit.json.')\n            self.action = edit['action']\n            self.height = edit['height']\n            self.width = edit['width']\n            self.args = edit.get('args', None)\n            # TODO: specify write mode per cell?\n            self.write_mode = edit.get('writeMode', 'overlap')\n            if self.write_mode not in self.valid_modes:\n                raise ValueError(\n                    f'Invalid writeMode {self.write_mode} in edit.json. Choose from cell, overwrite, or exclude.'\n                )\n\n        # Load label array\n        if 'labeled.dat' not in zf.namelist():\n            raise ValueError('zip must contain labeled.dat.')\n        with zf.open('labeled.dat') as f:\n            labels = np.frombuffer(f.read(), np.int32)\n            self.initial_labels = np.reshape(labels, (self.width, self.height))\n            self.labels = self.initial_labels.copy()\n\n        # Load cells array\n        if 'cells.json' not in zf.namelist():\n            raise ValueError('zip must contain cells.json.')\n        with zf.open('cells.json') as f:\n            self.cells = json.load(f)\n\n        # Load raw image\n        if 'raw.dat' in zf.namelist():\n            with zf.open('raw.dat') as f:\n                raw = np.frombuffer(f.read(), np.uint8)\n                self.raw = np.reshape(raw, (self.width, self.height))\n        elif self.action in self.raw_required:\n            raise ValueError(\n                f'Include raw array in raw.json to use action {self.action}.'\n            )\n\n    def write_response_zip(self):\n        \"\"\"Write edited segmentation to zip.\"\"\"\n        f = io.BytesIO()\n        with zipfile.ZipFile(f, 'w', compression=zipfile.ZIP_DEFLATED) as zf:\n            zf.writestr('labeled.dat', self.labels.tobytes())\n            zf.writestr('cells.json', json.dumps(self.cells))\n        f.seek(0)\n        self.response_zip = f\n\n    def get_cells(self, value):\n        \"\"\"\n        Returns a list of cells encoded by the value\n        \"\"\"\n        return list(\n            map(lambda c: c['cell'], filter(lambda c: c['value'] == value, self.cells))\n        )\n\n    def get_values(self, cell):\n        \"\"\"\n        Returns a list of values that encode a cell\n        \"\"\"\n        return list(\n            map(lambda c: c['value'], filter(lambda c: c['cell'] == cell, self.cells))\n        )\n\n    def get_value(self, cells):\n        \"\"\"\n        Returns the value that encodes the list of cells\n        \"\"\"\n        if cells == []:\n            return 0\n        values = set(map(lambda c: c['value'], self.cells))\n        for cell in cells:\n            values = values & set(self.get_values(cell))\n        if len(values) == 0:\n            value = self.new_value\n            for cell in cells:\n                self.cells.append({'value': value, 'cell': cell})\n            return value\n        return values.pop()\n\n    def get_mask(self, cell):\n        \"\"\"\n        Returns a boolean mask of the cell (or the background when cell == 0)\n        \"\"\"\n        if cell == 0:\n            return self.labels == 0\n        mask = np.zeros(self.labels.shape, dtype=bool)\n        for value in self.get_values(cell):\n            mask[self.labels == value] = True\n        return mask\n\n    def add_mask(self, mask, cell):\n        if self.write_mode == 'overwrite':\n            self.labels[mask] = self.get_value([cell])\n        elif self.write_mode == 'exclude':\n            mask = mask & (self.labels == 0)\n            self.labels[mask] = self.get_value([cell])\n        else:  # self.write_mode == 'overlap'\n            self.overlap_mask(mask, cell)\n\n    def remove_mask(self, mask, cell):\n        self.overlap_mask(mask, cell, remove=True)\n\n    def overlap_mask(self, mask, cell, remove=False):\n        \"\"\"\n        Adds the cell to the segmentation in the mask area,\n        overlapping with existing cells.\n        \"\"\"\n        # Rewrite values inside mask to encode label\n        values = np.unique(self.labels[mask])\n        for value in values:\n            # Get value to encode new set of labels\n            cells = self.get_cells(value)\n            if remove:\n                if cell in cells:\n                    cells.remove(cell)\n            else:\n                cells.append(cell)\n            new_value = self.get_value(cells)\n            self.labels[mask & (self.labels == value)] = new_value\n\n    def clean_cell(self, cell):\n        \"\"\"Ensures that a cell is a positive integer\"\"\"\n        return int(max(0, cell))\n\n    def dispatch_action(self):\n        \"\"\"\n        Call an action method based on an action type.\n\n        Args:\n            action (str): name of action method after \"action_\"\n                          e.g. \"draw\" to call \"action_draw\"\n            info (dict): key value pairs with arguments for action\n        \"\"\"\n        attr_name = 'action_{}'.format(self.action)\n        try:\n            action_fn = getattr(self, attr_name)\n        except AttributeError:\n            raise ValueError('Invalid action \"{}\"'.format(self.action))\n        action_fn(**self.args)\n\n    def action_draw(self, trace, brush_size, cell, erase=False):\n        \"\"\"\n        Use a \"brush\" to draw in the brush value along trace locations of\n        the annotated data.\n\n        Args:\n            trace (list): list of (x, y) coordinates where the brush has painted\n            brush_size (int): radius of the brush in pixels\n            cell (int): cell to edit with the brush\n            erase (bool): whether to add or remove label from brush stroke area\n        \"\"\"\n        trace = json.loads(trace)\n        # Create mask for brush stroke\n        brush_mask = np.zeros(self.labels.shape, dtype=bool)\n        for loc in trace:\n            x = loc[0]\n            y = loc[1]\n            disk = skimage.draw.disk((y, x), brush_size, shape=self.labels.shape)\n            brush_mask[disk] = True\n\n        if erase:\n            self.remove_mask(brush_mask, cell)\n        else:\n            self.add_mask(brush_mask, cell)\n\n    def action_trim_pixels(self, cell, x, y):\n        \"\"\"\n        Removes parts of cell not connected to (x, y).\n\n        Args:\n            cell (int): cell to trim\n            x (int): x position of seed\n            y (int): y position of seed\n        \"\"\"\n        mask = self.get_mask(cell)\n        if mask[y, x]:\n            connected_mask = flood(mask, (y, x))\n            self.remove_mask(~connected_mask, cell)\n\n    # TODO: come back to flooding with overlaps...\n    def action_flood(self, foreground, background, x, y):\n        \"\"\"\n        Floods the connected component of the background label at (x, y) with the foreground label.\n        When the background label is 0, does not flood diagonally connected pixels.\n\n        Args:\n            foreground (int): label to flood with\n            bacgkround (int): label to flood\n            x (int): x coordinate of region to flood\n            y (int): y coordinate of region to flood\n        \"\"\"\n        if background == 0:\n            mask = self.get_mask(background)\n            # Lower connectivity helps prevent flooding whole image\n            flooded = flood(mask, (y, x), connectivity=1)\n            self.add_mask(flooded, foreground)\n        else:\n            mask = self.get_mask(background)\n            flooded = flood(mask, (y, x), connectivity=2) & mask\n            self.remove_mask(flooded, background)\n            self.add_mask(flooded, foreground)\n\n    def action_watershed(self, cell, x1, y1, x2, y2):\n        \"\"\"Use watershed to segment different objects\"\"\"\n        new_cell = self.new_cell\n        # Create markers for to seed watershed labels\n        markers = np.zeros(self.labels.shape)\n        markers[y1, x1] = cell\n        markers[y2, x2] = new_cell\n\n        # Cut images to cell bounding box\n        mask = self.get_mask(cell)\n        props = regionprops(mask.astype(np.uint8))\n        top, left, bottom, right = props[0].bbox\n        raw = np.copy(self.raw[top:bottom, left:right])\n        markers = np.copy(markers[top:bottom, left:right])\n        mask = np.copy(mask[top:bottom, left:right])\n\n        # Contrast adjust and invert the raw image\n        raw = -rescale_intensity(raw)\n        # Apply watershed\n        results = watershed(raw, markers, mask=mask)\n\n        # Dilate small cells to prevent \"dimmer\" cell from being eroded by the \"brighter\" cell\n        if np.sum(results == new_cell) < 5:\n            dilated = dilation(results == new_cell, disk(3))\n            results[dilated] = new_cell\n        if np.sum(results == cell) < 5:\n            dilated = dilation(results == cell, disk(3))\n            results[dilated] = cell\n\n        # Update cells where watershed changed cell\n        new_cell_mask = np.zeros(self.labels.shape, dtype=bool)\n        cell_mask = np.zeros(self.labels.shape, dtype=bool)\n        new_cell_mask[top:bottom, left:right] = results == new_cell\n        cell_mask[top:bottom, left:right] = results == cell\n        self.remove_mask(self.get_mask(cell), cell)\n        self.add_mask(cell_mask, cell)\n        self.add_mask(new_cell_mask, new_cell)\n\n    def action_threshold(self, y1, x1, y2, x2, cell):\n        \"\"\"\n        Threshold the raw image for annotation prediction within the\n        user-determined bounding box.\n\n        Args:\n            y1 (int): first y coordinate to bound threshold area\n            x1 (int): first x coordinate to bound threshold area\n            y2 (int): second y coordinate to bound threshold area\n            x2 (int): second x coordinate to bound threshold area\n            cell (int): cell drawn in threshold area\n        \"\"\"\n        cell = self.clean_cell(cell)\n        # Make bounding box from coordinates\n        top = min(y1, y2)\n        bottom = max(y1, y2) + 1\n        left = min(x1, x2)\n        right = max(x1, x2) + 1\n        image = self.raw[top:bottom, left:right].astype('float64')\n        # Hysteresis thresholding strategy needs two thresholds\n        # triangle threshold picked after trying a few on one dataset\n        # it may not be the best approach for other datasets!\n        low = filters.threshold_triangle(image=image)\n        high = 1.10 * low\n        # Limit stray pixelst\n        thresholded = filters.apply_hysteresis_threshold(image, low, high)\n        mask = np.zeros(self.labels.shape, dtype=bool)\n        mask[top:bottom, left:right] = thresholded\n        self.add_mask(mask, cell)\n\n    def action_active_contour(self, cell, min_pixels=20, iterations=100, dilate=0):\n        \"\"\"\n        Uses active contouring to reshape a cell to match the raw image.\n        \"\"\"\n        mask = self.get_mask(cell)\n        # Limit contouring to a bounding box twice the size of the cell\n        props = regionprops(mask.astype(np.uint8))[0]\n        top, left, bottom, right = props.bbox\n        cell_height = bottom - top\n        cell_width = right - left\n        # Double size of bounding box\n        height, width = self.labels.shape\n        top = max(0, top - height \/\/ 2)\n        bottom = min(height, bottom + cell_height \/\/ 2)\n        left = max(0, left - width \/\/ 2)\n        right = min(width, right + cell_width \/\/ 2)\n\n        # Contour the cell\n        init_level_set = mask[top:bottom, left:right]\n        image = Normalize()(self.raw)[top:bottom, left:right]\n        contoured = morphological_chan_vese(\n            image, iterations, init_level_set=init_level_set\n        )\n\n        # Dilate to adjust for tight fit\n        contoured = dilation(contoured, disk(dilate))\n\n        # Keep only the largest connected component\n        regions = skimage.measure.label(contoured)\n        if np.any(regions):\n            largest_component = regions == (\n                np.argmax(np.bincount(regions.flat)[1:]) + 1\n            )\n            mask = np.zeros(self.labels.shape, dtype=bool)\n            mask[top:bottom, left:right] = largest_component\n\n        # Throw away small contoured cells\n        if np.count_nonzero(mask) >= min_pixels:\n            self.remove_mask(~mask, cell)\n            self.add_mask(mask, cell)\n\n    def action_erode(self, cell):\n        \"\"\"\n        Shrink the selected cell.\n        \"\"\"\n        mask = self.get_mask(cell)\n        eroded = erosion(mask, square(3))\n        self.remove_mask(mask & ~eroded, cell)\n\n    def action_dilate(self, cell):\n        \"\"\"\n        Expand the selected cell.\n        \"\"\"\n        mask = self.get_mask(cell)\n        dilated = dilation(mask, square(3))\n        self.add_mask(dilated, cell)\n"}},"msg":"Fix flooding holes in selected label"}},"https:\/\/github.com\/Armandov\/diffusion-ui":{"60f7c73c8aecb027e2bce4fa5dd8baef976c39f4":{"url":"https:\/\/api.github.com\/repos\/Armandov\/diffusion-ui\/commits\/60f7c73c8aecb027e2bce4fa5dd8baef976c39f4","html_url":"https:\/\/github.com\/Armandov\/diffusion-ui\/commit\/60f7c73c8aecb027e2bce4fa5dd8baef976c39f4","message":"prevent flooding the log with warnings for GPU<3GB","sha":"60f7c73c8aecb027e2bce4fa5dd8baef976c39f4","keyword":"flooding prevent","diff":"diff --git a\/ui\/easydiffusion\/device_manager.py b\/ui\/easydiffusion\/device_manager.py\nindex b7406408..a261a62f 100644\n--- a\/ui\/easydiffusion\/device_manager.py\n+++ b\/ui\/easydiffusion\/device_manager.py\n@@ -156,6 +156,8 @@ def is_device_compatible(device):\n     '''\n     Returns True\/False, and prints any compatibility errors\n     '''\n+    # static variable \"history\". \n+    is_device_compatible.history = getattr(is_device_compatible, 'history', {})\n     try:\n         validate_device_id(device, log_prefix='is_device_compatible')\n     except:\n@@ -168,7 +170,9 @@ def is_device_compatible(device):\n         _, mem_total = torch.cuda.mem_get_info(device)\n         mem_total \/= float(10**9)\n         if mem_total < 3.0:\n-            log.warn(f'GPU {device} with less than 3 GB of VRAM is not compatible with Stable Diffusion')\n+            if is_device_compatible.history.get(device) == None:\n+               log.warn(f'GPU {device} with less than 3 GB of VRAM is not compatible with Stable Diffusion')\n+               is_device_compatible.history[device] = 1\n             return False\n     except RuntimeError as e:\n         log.error(str(e))\n","files":{"\/ui\/easydiffusion\/device_manager.py":{"changes":[{"diff":"\n         _, mem_total = torch.cuda.mem_get_info(device)\n         mem_total \/= float(10**9)\n         if mem_total < 3.0:\n-            log.warn(f'GPU {device} with less than 3 GB of VRAM is not compatible with Stable Diffusion')\n+            if is_device_compatible.history.get(device) == None:\n+               log.warn(f'GPU {device} with less than 3 GB of VRAM is not compatible with Stable Diffusion')\n+               is_device_compatible.history[device] = 1\n             return False\n     except RuntimeError as e:\n         log.error(str(e))\n","add":3,"remove":1,"filename":"\/ui\/easydiffusion\/device_manager.py","badparts":["            log.warn(f'GPU {device} with less than 3 GB of VRAM is not compatible with Stable Diffusion')"],"goodparts":["            if is_device_compatible.history.get(device) == None:","               log.warn(f'GPU {device} with less than 3 GB of VRAM is not compatible with Stable Diffusion')","               is_device_compatible.history[device] = 1"]}],"source":"\nimport os import torch import traceback import re from easydiffusion.utils import log ''' Set `FORCE_FULL_PRECISION` in the environment variables, or in `config.bat`\/`config.sh` to set full precision(i.e. float32). Otherwise the models will load at half-precision(i.e. float16). Half-precision is fine most of the time. Full precision is only needed for working around GPU bugs(like NVIDIA 16xx GPUs). ''' COMPARABLE_GPU_PERCENTILE=0.65 mem_free_threshold=0 def get_device_delta(render_devices, active_devices): ''' render_devices: 'cpu', or 'auto' or['cuda:N'...] active_devices:['cpu', 'cuda:N'...] ''' if render_devices in('cpu', 'auto'): render_devices=[render_devices] elif render_devices is not None: if isinstance(render_devices, str): render_devices=[render_devices] if isinstance(render_devices, list) and len(render_devices) > 0: render_devices=list(filter(lambda x: x.startswith('cuda:'), render_devices)) if len(render_devices)==0: raise Exception('Invalid render_devices value in config.json. Valid:{\"render_devices\":[\"cuda:0\", \"cuda:1\"...]}, or{\"render_devices\": \"cpu\"} or{\"render_devices\": \"auto\"}') render_devices=list(filter(lambda x: is_device_compatible(x), render_devices)) if len(render_devices)==0: raise Exception('Sorry, none of the render_devices configured in config.json are compatible with Stable Diffusion') else: raise Exception('Invalid render_devices value in config.json. Valid:{\"render_devices\":[\"cuda:0\", \"cuda:1\"...]}, or{\"render_devices\": \"cpu\"} or{\"render_devices\": \"auto\"}') else: render_devices=['auto'] if 'auto' in render_devices: render_devices=auto_pick_devices(active_devices) if 'cpu' in render_devices: log.warn('WARNING: Could not find a compatible GPU. Using the CPU, but this will be very slow!') active_devices=set(active_devices) render_devices=set(render_devices) devices_to_start=render_devices -active_devices devices_to_stop=active_devices -render_devices return devices_to_start, devices_to_stop def auto_pick_devices(currently_active_devices): global mem_free_threshold if not torch.cuda.is_available(): return['cpu'] device_count=torch.cuda.device_count() if device_count==1: return['cuda:0'] if is_device_compatible('cuda:0') else['cpu'] log.debug('Autoselecting GPU. Using most free memory.') devices=[] for device in range(device_count): device=f'cuda:{device}' if not is_device_compatible(device): continue mem_free, mem_total=torch.cuda.mem_get_info(device) mem_free \/=float(10**9) mem_total \/=float(10**9) device_name=torch.cuda.get_device_name(device) log.debug(f'{device} detected:{device_name} -Memory(free\/total):{round(mem_free, 2)}Gb \/{round(mem_total, 2)}Gb') devices.append({'device': device, 'device_name': device_name, 'mem_free': mem_free}) devices.sort(key=lambda x:x['mem_free'], reverse=True) max_mem_free=devices[0]['mem_free'] curr_mem_free_threshold=COMPARABLE_GPU_PERCENTILE * max_mem_free mem_free_threshold=max(curr_mem_free_threshold, mem_free_threshold) devices=list(filter((lambda x: x['mem_free'] > mem_free_threshold or x['device'] in currently_active_devices), devices)) devices=list(map(lambda x: x['device'], devices)) return devices def device_init(context, device): ''' This function assumes the 'device' has already been verified to be compatible. `get_device_delta()` has already filtered out incompatible devices. ''' validate_device_id(device, log_prefix='device_init') if device=='cpu': context.device='cpu' context.device_name=get_processor_name() context.half_precision=False log.debug(f'Render device CPU available as{context.device_name}') return context.device_name=torch.cuda.get_device_name(device) context.device=device if needs_to_force_full_precision(context): log.warn(f'forcing full precision on this GPU, to avoid green images. GPU detected:{context.device_name}') context.half_precision=False log.info(f'Setting{device} as active, with precision:{\"half\" if context.half_precision else \"full\"}') torch.cuda.device(device) return def needs_to_force_full_precision(context): if 'FORCE_FULL_PRECISION' in os.environ: return True device_name=context.device_name.lower() return(('nvidia' in device_name or 'geforce' in device_name) and(' 1660' in device_name or ' 1650' in device_name or ' t400' in device_name)) or('Quadro T2000' in device_name) def get_max_vram_usage_level(device): if device !='cpu': _, mem_total=torch.cuda.mem_get_info(device) mem_total \/=float(10**9) if mem_total < 4.5: return 'low' elif mem_total < 6.5: return 'balanced' return 'high' def validate_device_id(device, log_prefix=''): def is_valid(): if not isinstance(device, str): return False if device=='cpu': return True if not device.startswith('cuda:') or not device[5:].isnumeric(): return False return True if not is_valid(): raise EnvironmentError(f\"{log_prefix}: device id should be 'cpu', or 'cuda:N'(where N is an integer index for the GPU). Got:{device}\") def is_device_compatible(device): ''' Returns True\/False, and prints any compatibility errors ''' try: validate_device_id(device, log_prefix='is_device_compatible') except: log.error(str(e)) return False if device=='cpu': return True try: _, mem_total=torch.cuda.mem_get_info(device) mem_total \/=float(10**9) if mem_total < 3.0: log.warn(f'GPU{device} with less than 3 GB of VRAM is not compatible with Stable Diffusion') return False except RuntimeError as e: log.error(str(e)) return False return True def get_processor_name(): try: import platform, subprocess if platform.system()==\"Windows\": return platform.processor() elif platform.system()==\"Darwin\": os.environ['PATH']=os.environ['PATH'] +os.pathsep +'\/usr\/sbin' command=\"sysctl -n machdep.cpu.brand_string\" return subprocess.check_output(command).strip() elif platform.system()==\"Linux\": command=\"cat \/proc\/cpuinfo\" all_info=subprocess.check_output(command, shell=True).decode().strip() for line in all_info.split(\"\\n\"): if \"model name\" in line: return re.sub(\".*model name.*:\", \"\", line, 1).strip() except: log.error(traceback.format_exc()) return \"cpu\" ","sourceWithComments":"import os\nimport torch\nimport traceback\nimport re\n\nfrom easydiffusion.utils import log\n\n'''\nSet `FORCE_FULL_PRECISION` in the environment variables, or in `config.bat`\/`config.sh` to set full precision (i.e. float32).\nOtherwise the models will load at half-precision (i.e. float16).\n\nHalf-precision is fine most of the time. Full precision is only needed for working around GPU bugs (like NVIDIA 16xx GPUs).\n'''\n\nCOMPARABLE_GPU_PERCENTILE = 0.65 # if a GPU's free_mem is within this % of the GPU with the most free_mem, it will be picked\n\nmem_free_threshold = 0\n\ndef get_device_delta(render_devices, active_devices):\n    '''\n    render_devices: 'cpu', or 'auto' or ['cuda:N'...]\n    active_devices: ['cpu', 'cuda:N'...]\n    '''\n\n    if render_devices in ('cpu', 'auto'):\n        render_devices = [render_devices]\n    elif render_devices is not None:\n        if isinstance(render_devices, str):\n            render_devices = [render_devices]\n        if isinstance(render_devices, list) and len(render_devices) > 0:\n            render_devices = list(filter(lambda x: x.startswith('cuda:'), render_devices))\n            if len(render_devices) == 0:\n                raise Exception('Invalid render_devices value in config.json. Valid: {\"render_devices\": [\"cuda:0\", \"cuda:1\"...]}, or {\"render_devices\": \"cpu\"} or {\"render_devices\": \"auto\"}')\n\n            render_devices = list(filter(lambda x: is_device_compatible(x), render_devices))\n            if len(render_devices) == 0:\n                raise Exception('Sorry, none of the render_devices configured in config.json are compatible with Stable Diffusion')\n        else:\n            raise Exception('Invalid render_devices value in config.json. Valid: {\"render_devices\": [\"cuda:0\", \"cuda:1\"...]}, or {\"render_devices\": \"cpu\"} or {\"render_devices\": \"auto\"}')\n    else:\n        render_devices = ['auto']\n\n    if 'auto' in render_devices:\n        render_devices = auto_pick_devices(active_devices)\n        if 'cpu' in render_devices:\n            log.warn('WARNING: Could not find a compatible GPU. Using the CPU, but this will be very slow!')\n\n    active_devices = set(active_devices)\n    render_devices = set(render_devices)\n\n    devices_to_start = render_devices - active_devices\n    devices_to_stop = active_devices - render_devices\n\n    return devices_to_start, devices_to_stop\n\ndef auto_pick_devices(currently_active_devices):\n    global mem_free_threshold\n\n    if not torch.cuda.is_available(): return ['cpu']\n\n    device_count = torch.cuda.device_count()\n    if device_count == 1:\n        return ['cuda:0'] if is_device_compatible('cuda:0') else ['cpu']\n\n    log.debug('Autoselecting GPU. Using most free memory.')\n    devices = []\n    for device in range(device_count):\n        device = f'cuda:{device}'\n        if not is_device_compatible(device):\n            continue\n\n        mem_free, mem_total = torch.cuda.mem_get_info(device)\n        mem_free \/= float(10**9)\n        mem_total \/= float(10**9)\n        device_name = torch.cuda.get_device_name(device)\n        log.debug(f'{device} detected: {device_name} - Memory (free\/total): {round(mem_free, 2)}Gb \/ {round(mem_total, 2)}Gb')\n        devices.append({'device': device, 'device_name': device_name, 'mem_free': mem_free})\n\n    devices.sort(key=lambda x:x['mem_free'], reverse=True)\n    max_mem_free = devices[0]['mem_free']\n    curr_mem_free_threshold = COMPARABLE_GPU_PERCENTILE * max_mem_free\n    mem_free_threshold = max(curr_mem_free_threshold, mem_free_threshold)\n\n    # Auto-pick algorithm:\n    # 1. Pick the top 75 percentile of the GPUs, sorted by free_mem.\n    # 2. Also include already-running devices (GPU-only), otherwise their free_mem will\n    #    always be very low (since their VRAM contains the model).\n    #    These already-running devices probably aren't terrible, since they were picked in the past.\n    #    Worst case, the user can restart the program and that'll get rid of them.\n    devices = list(filter((lambda x: x['mem_free'] > mem_free_threshold or x['device'] in currently_active_devices), devices))\n    devices = list(map(lambda x: x['device'], devices))\n    return devices\n\ndef device_init(context, device):\n    '''\n    This function assumes the 'device' has already been verified to be compatible.\n    `get_device_delta()` has already filtered out incompatible devices.\n    '''\n\n    validate_device_id(device, log_prefix='device_init')\n\n    if device == 'cpu':\n        context.device = 'cpu'\n        context.device_name = get_processor_name()\n        context.half_precision = False\n        log.debug(f'Render device CPU available as {context.device_name}')\n        return\n\n    context.device_name = torch.cuda.get_device_name(device)\n    context.device = device\n\n    # Force full precision on 1660 and 1650 NVIDIA cards to avoid creating green images\n    if needs_to_force_full_precision(context):\n        log.warn(f'forcing full precision on this GPU, to avoid green images. GPU detected: {context.device_name}')\n        # Apply force_full_precision now before models are loaded.\n        context.half_precision = False\n\n    log.info(f'Setting {device} as active, with precision: {\"half\" if context.half_precision else \"full\"}')\n    torch.cuda.device(device)\n\n    return\n\ndef needs_to_force_full_precision(context):\n    if 'FORCE_FULL_PRECISION' in os.environ:\n        return True\n\n    device_name = context.device_name.lower()\n    return (('nvidia' in device_name or 'geforce' in device_name) and (' 1660' in device_name or ' 1650' in device_name or ' t400' in device_name)) or ('Quadro T2000' in device_name)\n\ndef get_max_vram_usage_level(device):\n    if device != 'cpu':\n        _, mem_total = torch.cuda.mem_get_info(device)\n        mem_total \/= float(10**9)\n\n        if mem_total < 4.5:\n            return 'low'\n        elif mem_total < 6.5:\n            return 'balanced'\n\n    return 'high'\n\ndef validate_device_id(device, log_prefix=''):\n    def is_valid():\n        if not isinstance(device, str):\n            return False\n        if device == 'cpu':\n            return True\n        if not device.startswith('cuda:') or not device[5:].isnumeric():\n            return False\n        return True\n\n    if not is_valid():\n        raise EnvironmentError(f\"{log_prefix}: device id should be 'cpu', or 'cuda:N' (where N is an integer index for the GPU). Got: {device}\")\n\ndef is_device_compatible(device):\n    '''\n    Returns True\/False, and prints any compatibility errors\n    '''\n    try:\n        validate_device_id(device, log_prefix='is_device_compatible')\n    except:\n        log.error(str(e))\n        return False\n\n    if device == 'cpu': return True\n    # Memory check\n    try:\n        _, mem_total = torch.cuda.mem_get_info(device)\n        mem_total \/= float(10**9)\n        if mem_total < 3.0:\n            log.warn(f'GPU {device} with less than 3 GB of VRAM is not compatible with Stable Diffusion')\n            return False\n    except RuntimeError as e:\n        log.error(str(e))\n        return False\n    return True\n\ndef get_processor_name():\n    try:\n        import platform, subprocess\n        if platform.system() == \"Windows\":\n            return platform.processor()\n        elif platform.system() == \"Darwin\":\n            os.environ['PATH'] = os.environ['PATH'] + os.pathsep + '\/usr\/sbin'\n            command = \"sysctl -n machdep.cpu.brand_string\"\n            return subprocess.check_output(command).strip()\n        elif platform.system() == \"Linux\":\n            command = \"cat \/proc\/cpuinfo\"\n            all_info = subprocess.check_output(command, shell=True).decode().strip()\n            for line in all_info.split(\"\\n\"):\n                if \"model name\" in line:\n                    return re.sub(\".*model name.*:\", \"\", line, 1).strip()\n    except:\n        log.error(traceback.format_exc())\n        return \"cpu\"\n"}},"msg":"prevent flooding the log with warnings for GPU<3GB"}},"https:\/\/github.com\/Darven8\/stable-diffusion-ui":{"60f7c73c8aecb027e2bce4fa5dd8baef976c39f4":{"url":"https:\/\/api.github.com\/repos\/Darven8\/stable-diffusion-ui\/commits\/60f7c73c8aecb027e2bce4fa5dd8baef976c39f4","html_url":"https:\/\/github.com\/Darven8\/stable-diffusion-ui\/commit\/60f7c73c8aecb027e2bce4fa5dd8baef976c39f4","message":"prevent flooding the log with warnings for GPU<3GB","sha":"60f7c73c8aecb027e2bce4fa5dd8baef976c39f4","keyword":"flooding prevent","diff":"diff --git a\/ui\/easydiffusion\/device_manager.py b\/ui\/easydiffusion\/device_manager.py\nindex b7406408..a261a62f 100644\n--- a\/ui\/easydiffusion\/device_manager.py\n+++ b\/ui\/easydiffusion\/device_manager.py\n@@ -156,6 +156,8 @@ def is_device_compatible(device):\n     '''\n     Returns True\/False, and prints any compatibility errors\n     '''\n+    # static variable \"history\". \n+    is_device_compatible.history = getattr(is_device_compatible, 'history', {})\n     try:\n         validate_device_id(device, log_prefix='is_device_compatible')\n     except:\n@@ -168,7 +170,9 @@ def is_device_compatible(device):\n         _, mem_total = torch.cuda.mem_get_info(device)\n         mem_total \/= float(10**9)\n         if mem_total < 3.0:\n-            log.warn(f'GPU {device} with less than 3 GB of VRAM is not compatible with Stable Diffusion')\n+            if is_device_compatible.history.get(device) == None:\n+               log.warn(f'GPU {device} with less than 3 GB of VRAM is not compatible with Stable Diffusion')\n+               is_device_compatible.history[device] = 1\n             return False\n     except RuntimeError as e:\n         log.error(str(e))\n","files":{"\/ui\/easydiffusion\/device_manager.py":{"changes":[{"diff":"\n         _, mem_total = torch.cuda.mem_get_info(device)\n         mem_total \/= float(10**9)\n         if mem_total < 3.0:\n-            log.warn(f'GPU {device} with less than 3 GB of VRAM is not compatible with Stable Diffusion')\n+            if is_device_compatible.history.get(device) == None:\n+               log.warn(f'GPU {device} with less than 3 GB of VRAM is not compatible with Stable Diffusion')\n+               is_device_compatible.history[device] = 1\n             return False\n     except RuntimeError as e:\n         log.error(str(e))\n","add":3,"remove":1,"filename":"\/ui\/easydiffusion\/device_manager.py","badparts":["            log.warn(f'GPU {device} with less than 3 GB of VRAM is not compatible with Stable Diffusion')"],"goodparts":["            if is_device_compatible.history.get(device) == None:","               log.warn(f'GPU {device} with less than 3 GB of VRAM is not compatible with Stable Diffusion')","               is_device_compatible.history[device] = 1"]}],"source":"\nimport os import torch import traceback import re from easydiffusion.utils import log ''' Set `FORCE_FULL_PRECISION` in the environment variables, or in `config.bat`\/`config.sh` to set full precision(i.e. float32). Otherwise the models will load at half-precision(i.e. float16). Half-precision is fine most of the time. Full precision is only needed for working around GPU bugs(like NVIDIA 16xx GPUs). ''' COMPARABLE_GPU_PERCENTILE=0.65 mem_free_threshold=0 def get_device_delta(render_devices, active_devices): ''' render_devices: 'cpu', or 'auto' or['cuda:N'...] active_devices:['cpu', 'cuda:N'...] ''' if render_devices in('cpu', 'auto'): render_devices=[render_devices] elif render_devices is not None: if isinstance(render_devices, str): render_devices=[render_devices] if isinstance(render_devices, list) and len(render_devices) > 0: render_devices=list(filter(lambda x: x.startswith('cuda:'), render_devices)) if len(render_devices)==0: raise Exception('Invalid render_devices value in config.json. Valid:{\"render_devices\":[\"cuda:0\", \"cuda:1\"...]}, or{\"render_devices\": \"cpu\"} or{\"render_devices\": \"auto\"}') render_devices=list(filter(lambda x: is_device_compatible(x), render_devices)) if len(render_devices)==0: raise Exception('Sorry, none of the render_devices configured in config.json are compatible with Stable Diffusion') else: raise Exception('Invalid render_devices value in config.json. Valid:{\"render_devices\":[\"cuda:0\", \"cuda:1\"...]}, or{\"render_devices\": \"cpu\"} or{\"render_devices\": \"auto\"}') else: render_devices=['auto'] if 'auto' in render_devices: render_devices=auto_pick_devices(active_devices) if 'cpu' in render_devices: log.warn('WARNING: Could not find a compatible GPU. Using the CPU, but this will be very slow!') active_devices=set(active_devices) render_devices=set(render_devices) devices_to_start=render_devices -active_devices devices_to_stop=active_devices -render_devices return devices_to_start, devices_to_stop def auto_pick_devices(currently_active_devices): global mem_free_threshold if not torch.cuda.is_available(): return['cpu'] device_count=torch.cuda.device_count() if device_count==1: return['cuda:0'] if is_device_compatible('cuda:0') else['cpu'] log.debug('Autoselecting GPU. Using most free memory.') devices=[] for device in range(device_count): device=f'cuda:{device}' if not is_device_compatible(device): continue mem_free, mem_total=torch.cuda.mem_get_info(device) mem_free \/=float(10**9) mem_total \/=float(10**9) device_name=torch.cuda.get_device_name(device) log.debug(f'{device} detected:{device_name} -Memory(free\/total):{round(mem_free, 2)}Gb \/{round(mem_total, 2)}Gb') devices.append({'device': device, 'device_name': device_name, 'mem_free': mem_free}) devices.sort(key=lambda x:x['mem_free'], reverse=True) max_mem_free=devices[0]['mem_free'] curr_mem_free_threshold=COMPARABLE_GPU_PERCENTILE * max_mem_free mem_free_threshold=max(curr_mem_free_threshold, mem_free_threshold) devices=list(filter((lambda x: x['mem_free'] > mem_free_threshold or x['device'] in currently_active_devices), devices)) devices=list(map(lambda x: x['device'], devices)) return devices def device_init(context, device): ''' This function assumes the 'device' has already been verified to be compatible. `get_device_delta()` has already filtered out incompatible devices. ''' validate_device_id(device, log_prefix='device_init') if device=='cpu': context.device='cpu' context.device_name=get_processor_name() context.half_precision=False log.debug(f'Render device CPU available as{context.device_name}') return context.device_name=torch.cuda.get_device_name(device) context.device=device if needs_to_force_full_precision(context): log.warn(f'forcing full precision on this GPU, to avoid green images. GPU detected:{context.device_name}') context.half_precision=False log.info(f'Setting{device} as active, with precision:{\"half\" if context.half_precision else \"full\"}') torch.cuda.device(device) return def needs_to_force_full_precision(context): if 'FORCE_FULL_PRECISION' in os.environ: return True device_name=context.device_name.lower() return(('nvidia' in device_name or 'geforce' in device_name) and(' 1660' in device_name or ' 1650' in device_name or ' t400' in device_name)) or('Quadro T2000' in device_name) def get_max_vram_usage_level(device): if device !='cpu': _, mem_total=torch.cuda.mem_get_info(device) mem_total \/=float(10**9) if mem_total < 4.5: return 'low' elif mem_total < 6.5: return 'balanced' return 'high' def validate_device_id(device, log_prefix=''): def is_valid(): if not isinstance(device, str): return False if device=='cpu': return True if not device.startswith('cuda:') or not device[5:].isnumeric(): return False return True if not is_valid(): raise EnvironmentError(f\"{log_prefix}: device id should be 'cpu', or 'cuda:N'(where N is an integer index for the GPU). Got:{device}\") def is_device_compatible(device): ''' Returns True\/False, and prints any compatibility errors ''' try: validate_device_id(device, log_prefix='is_device_compatible') except: log.error(str(e)) return False if device=='cpu': return True try: _, mem_total=torch.cuda.mem_get_info(device) mem_total \/=float(10**9) if mem_total < 3.0: log.warn(f'GPU{device} with less than 3 GB of VRAM is not compatible with Stable Diffusion') return False except RuntimeError as e: log.error(str(e)) return False return True def get_processor_name(): try: import platform, subprocess if platform.system()==\"Windows\": return platform.processor() elif platform.system()==\"Darwin\": os.environ['PATH']=os.environ['PATH'] +os.pathsep +'\/usr\/sbin' command=\"sysctl -n machdep.cpu.brand_string\" return subprocess.check_output(command).strip() elif platform.system()==\"Linux\": command=\"cat \/proc\/cpuinfo\" all_info=subprocess.check_output(command, shell=True).decode().strip() for line in all_info.split(\"\\n\"): if \"model name\" in line: return re.sub(\".*model name.*:\", \"\", line, 1).strip() except: log.error(traceback.format_exc()) return \"cpu\" ","sourceWithComments":"import os\nimport torch\nimport traceback\nimport re\n\nfrom easydiffusion.utils import log\n\n'''\nSet `FORCE_FULL_PRECISION` in the environment variables, or in `config.bat`\/`config.sh` to set full precision (i.e. float32).\nOtherwise the models will load at half-precision (i.e. float16).\n\nHalf-precision is fine most of the time. Full precision is only needed for working around GPU bugs (like NVIDIA 16xx GPUs).\n'''\n\nCOMPARABLE_GPU_PERCENTILE = 0.65 # if a GPU's free_mem is within this % of the GPU with the most free_mem, it will be picked\n\nmem_free_threshold = 0\n\ndef get_device_delta(render_devices, active_devices):\n    '''\n    render_devices: 'cpu', or 'auto' or ['cuda:N'...]\n    active_devices: ['cpu', 'cuda:N'...]\n    '''\n\n    if render_devices in ('cpu', 'auto'):\n        render_devices = [render_devices]\n    elif render_devices is not None:\n        if isinstance(render_devices, str):\n            render_devices = [render_devices]\n        if isinstance(render_devices, list) and len(render_devices) > 0:\n            render_devices = list(filter(lambda x: x.startswith('cuda:'), render_devices))\n            if len(render_devices) == 0:\n                raise Exception('Invalid render_devices value in config.json. Valid: {\"render_devices\": [\"cuda:0\", \"cuda:1\"...]}, or {\"render_devices\": \"cpu\"} or {\"render_devices\": \"auto\"}')\n\n            render_devices = list(filter(lambda x: is_device_compatible(x), render_devices))\n            if len(render_devices) == 0:\n                raise Exception('Sorry, none of the render_devices configured in config.json are compatible with Stable Diffusion')\n        else:\n            raise Exception('Invalid render_devices value in config.json. Valid: {\"render_devices\": [\"cuda:0\", \"cuda:1\"...]}, or {\"render_devices\": \"cpu\"} or {\"render_devices\": \"auto\"}')\n    else:\n        render_devices = ['auto']\n\n    if 'auto' in render_devices:\n        render_devices = auto_pick_devices(active_devices)\n        if 'cpu' in render_devices:\n            log.warn('WARNING: Could not find a compatible GPU. Using the CPU, but this will be very slow!')\n\n    active_devices = set(active_devices)\n    render_devices = set(render_devices)\n\n    devices_to_start = render_devices - active_devices\n    devices_to_stop = active_devices - render_devices\n\n    return devices_to_start, devices_to_stop\n\ndef auto_pick_devices(currently_active_devices):\n    global mem_free_threshold\n\n    if not torch.cuda.is_available(): return ['cpu']\n\n    device_count = torch.cuda.device_count()\n    if device_count == 1:\n        return ['cuda:0'] if is_device_compatible('cuda:0') else ['cpu']\n\n    log.debug('Autoselecting GPU. Using most free memory.')\n    devices = []\n    for device in range(device_count):\n        device = f'cuda:{device}'\n        if not is_device_compatible(device):\n            continue\n\n        mem_free, mem_total = torch.cuda.mem_get_info(device)\n        mem_free \/= float(10**9)\n        mem_total \/= float(10**9)\n        device_name = torch.cuda.get_device_name(device)\n        log.debug(f'{device} detected: {device_name} - Memory (free\/total): {round(mem_free, 2)}Gb \/ {round(mem_total, 2)}Gb')\n        devices.append({'device': device, 'device_name': device_name, 'mem_free': mem_free})\n\n    devices.sort(key=lambda x:x['mem_free'], reverse=True)\n    max_mem_free = devices[0]['mem_free']\n    curr_mem_free_threshold = COMPARABLE_GPU_PERCENTILE * max_mem_free\n    mem_free_threshold = max(curr_mem_free_threshold, mem_free_threshold)\n\n    # Auto-pick algorithm:\n    # 1. Pick the top 75 percentile of the GPUs, sorted by free_mem.\n    # 2. Also include already-running devices (GPU-only), otherwise their free_mem will\n    #    always be very low (since their VRAM contains the model).\n    #    These already-running devices probably aren't terrible, since they were picked in the past.\n    #    Worst case, the user can restart the program and that'll get rid of them.\n    devices = list(filter((lambda x: x['mem_free'] > mem_free_threshold or x['device'] in currently_active_devices), devices))\n    devices = list(map(lambda x: x['device'], devices))\n    return devices\n\ndef device_init(context, device):\n    '''\n    This function assumes the 'device' has already been verified to be compatible.\n    `get_device_delta()` has already filtered out incompatible devices.\n    '''\n\n    validate_device_id(device, log_prefix='device_init')\n\n    if device == 'cpu':\n        context.device = 'cpu'\n        context.device_name = get_processor_name()\n        context.half_precision = False\n        log.debug(f'Render device CPU available as {context.device_name}')\n        return\n\n    context.device_name = torch.cuda.get_device_name(device)\n    context.device = device\n\n    # Force full precision on 1660 and 1650 NVIDIA cards to avoid creating green images\n    if needs_to_force_full_precision(context):\n        log.warn(f'forcing full precision on this GPU, to avoid green images. GPU detected: {context.device_name}')\n        # Apply force_full_precision now before models are loaded.\n        context.half_precision = False\n\n    log.info(f'Setting {device} as active, with precision: {\"half\" if context.half_precision else \"full\"}')\n    torch.cuda.device(device)\n\n    return\n\ndef needs_to_force_full_precision(context):\n    if 'FORCE_FULL_PRECISION' in os.environ:\n        return True\n\n    device_name = context.device_name.lower()\n    return (('nvidia' in device_name or 'geforce' in device_name) and (' 1660' in device_name or ' 1650' in device_name or ' t400' in device_name)) or ('Quadro T2000' in device_name)\n\ndef get_max_vram_usage_level(device):\n    if device != 'cpu':\n        _, mem_total = torch.cuda.mem_get_info(device)\n        mem_total \/= float(10**9)\n\n        if mem_total < 4.5:\n            return 'low'\n        elif mem_total < 6.5:\n            return 'balanced'\n\n    return 'high'\n\ndef validate_device_id(device, log_prefix=''):\n    def is_valid():\n        if not isinstance(device, str):\n            return False\n        if device == 'cpu':\n            return True\n        if not device.startswith('cuda:') or not device[5:].isnumeric():\n            return False\n        return True\n\n    if not is_valid():\n        raise EnvironmentError(f\"{log_prefix}: device id should be 'cpu', or 'cuda:N' (where N is an integer index for the GPU). Got: {device}\")\n\ndef is_device_compatible(device):\n    '''\n    Returns True\/False, and prints any compatibility errors\n    '''\n    try:\n        validate_device_id(device, log_prefix='is_device_compatible')\n    except:\n        log.error(str(e))\n        return False\n\n    if device == 'cpu': return True\n    # Memory check\n    try:\n        _, mem_total = torch.cuda.mem_get_info(device)\n        mem_total \/= float(10**9)\n        if mem_total < 3.0:\n            log.warn(f'GPU {device} with less than 3 GB of VRAM is not compatible with Stable Diffusion')\n            return False\n    except RuntimeError as e:\n        log.error(str(e))\n        return False\n    return True\n\ndef get_processor_name():\n    try:\n        import platform, subprocess\n        if platform.system() == \"Windows\":\n            return platform.processor()\n        elif platform.system() == \"Darwin\":\n            os.environ['PATH'] = os.environ['PATH'] + os.pathsep + '\/usr\/sbin'\n            command = \"sysctl -n machdep.cpu.brand_string\"\n            return subprocess.check_output(command).strip()\n        elif platform.system() == \"Linux\":\n            command = \"cat \/proc\/cpuinfo\"\n            all_info = subprocess.check_output(command, shell=True).decode().strip()\n            for line in all_info.split(\"\\n\"):\n                if \"model name\" in line:\n                    return re.sub(\".*model name.*:\", \"\", line, 1).strip()\n    except:\n        log.error(traceback.format_exc())\n        return \"cpu\"\n"}},"msg":"prevent flooding the log with warnings for GPU<3GB"}},"https:\/\/github.com\/ssrando\/ssrando":{"fc9513bdb94e006170802434edd0a6b668371ae1":{"url":"https:\/\/api.github.com\/repos\/ssrando\/ssrando\/commits\/fc9513bdb94e006170802434edd0a6b668371ae1","html_url":"https:\/\/github.com\/ssrando\/ssrando\/commit\/fc9513bdb94e006170802434edd0a6b668371ae1","message":"Prevent Flooded Faron Woods being hinted barren","sha":"fc9513bdb94e006170802434edd0a6b668371ae1","keyword":"flooding prevent","diff":"diff --git a\/hints\/hint_distribution.py b\/hints\/hint_distribution.py\nindex 7dc117f0..b316a7a6 100644\n--- a\/hints\/hint_distribution.py\n+++ b\/hints\/hint_distribution.py\n@@ -235,7 +235,10 @@ def start(self, logic: Logic, always_hints: list, sometimes_hints: list):\n \n         region_barren, nonprogress = self.logic.get_barren_regions()\n         for zone in region_barren:\n-            if \"Silent Realm\" in zone:\n+            if (\n+                \"Silent Realm\" in zone\n+                or \"Flooded Faron Woods\"\n+            ):\n                 continue  # don't hint barren silent realms since they are an always hint\n             if self.logic.rando.options[\"empty-unrequired-dungeons\"]:\n                 # avoid placing barren hints for unrequired dungeons in race mode\n","files":{"\/hints\/hint_distribution.py":{"changes":[{"diff":"\n \n         region_barren, nonprogress = self.logic.get_barren_regions()\n         for zone in region_barren:\n-            if \"Silent Realm\" in zone:\n+            if (\n+                \"Silent Realm\" in zone\n+                or \"Flooded Faron Woods\"\n+            ):\n                 continue  # don't hint barren silent realms since they are an always hint\n             if self.logic.rando.options[\"empty-unrequired-dungeons\"]:\n                 # avoid placing barren hints for unrequired dungeons in race mode\n","add":4,"remove":1,"filename":"\/hints\/hint_distribution.py","badparts":["            if \"Silent Realm\" in zone:"],"goodparts":["            if (","                \"Silent Realm\" in zone","                or \"Flooded Faron Woods\"","            ):"]}],"source":"\nfrom collections import defaultdict import json from random import Random from hints.hint_types import * from logic.constants import( POTENTIALLY_REQUIRED_DUNGEONS, ALL_DUNGEON_AREAS, SILENT_REALM_CHECKS, ) from logic.logic import Logic from paths import RANDO_ROOT_PATH HINTABLE_ITEMS=( [\"Clawshots\"] +[\"Progressive Beetle\"] * 2 +[\"Progressive Sword\"] * 6 +[\"Emerald Tablet\"] * 1 +[\"Ruby Tablet\"] * 1 +[\"Amber Tablet\"] * 1 +[\"Goddess Harp\"] * 1 +[\"Water Scale\"] * 1 +[\"Fireshield Earrings\"] * 1 ) JUNK_TEXT=[ \"They say that crashing in BiT is easy.\", \"They say that bookshelves can talk\", \"They say that people who love the Bug Net also like Trains\", \"They say that there is a Gossip Stone by the Temple of Time\", \"They say there's a 35% chance for FS Boss Key to be Heetle Locked\", \"They say 64bit left Fire Sanctuary without learning Ballad of the Goddess\", \"They say that Ancient Cistern is haunted by the ghosts of softlocked Links\", \"They say the Potion Lady is still holding onto a Spiral Charge for CJ\", \"They say there is a chest underneath the party wheel in Lanayru\", \"They say that you need the hero's tunic to sleep on the main part of Skyloft\", \"They say that you need to Hot the Spile to defeat Imprisoned 2\", \"They say whenever Spiral Charge is on a trial, a seed roller goes mysteriously missing\", \"They say that Eldin Trial is vanilla whenever it is required\", \"They say that gymnast86 won the first randomizer tournament and retired immediately after\", \"They say that Mogmas don't understand Minesweeper\", \"They say that you can win a race by abandoning Lanayru to check Cawlin's Letter\", \"They say that tornados spawn frequently in the Sky\", \"They say Scrapper gets easily tilted\", \"They say there is a chest on the cliffs by the Goddess Statue\", \"They say that entering Ancient Cistern with no B items has a 1% chance of success\", \"They say that Glittering Spores are the best bird drugs\", \"They say that the Ancient Automaton fears danger darts\", \"They say the single tumbling plant is required every seed\", \"They say that your battery is low\", \"They say that you just have to get the right checks to win\", \"They say that rushing Peatrice is the play\", \"They say there is a 0.0000001164% chance your RNG won't change\", \"If only we could go Back in Time and name the glitch properly...\", 'They say that there is something called a \"hash\" that makes it easier for people to verify that they are playing the right seed', \"They say that the bad seed rollers are still in the car, seeking for a safe refugee\", \"Have you heard the tragedy of Darth Kolok the Pause? I thought not, it's not a story the admins would tell you\", \"Sand Sea is the most hated region in the game, because Sand is coarse, rough and gets everywhere\", \"They say that rice has magical properties when visiting Yerbal\", \"They say that Jannon is still jammin to this day\", \"They say that there is only one place where the Slingshot beats the Bow\", \"They say that Koloktos waiting caused a civil war among players\", \"They say that there is a settings combination which needs 0 checks to be completed\", \"They say that avoiding Fledge's item from a fresh file is impossible\", \"... astronomically...\", \"They say that you can open the chest behind bars in LMF after raising said bars\", \"They say that you look like you have a Questions\", \"They say that HD randomizer development is delayed by a day every time someone asks about it in the Discord\", \"The disc could not be read. Refer to the Wii Operations Manual for details.\", \"They say that a massive storm brews over the Lanayru Sand Sea due to Tentalus' immense size\", ] class InvalidHintDistribution(Exception): pass class HintDistribution: def __init__(self): self.banned_stones=[] self.added_locations=[] self.removed_locations=[] self.added_items=[] self.removed_items=[] self.dungeon_sots_limit=0 self.sots_dungeon_placed=0 self.dungeon_barren_limit=0 self.distribution={} self.rng: Random=None self.logic=None self.hints=[] self.weighted_types=[] self.weights=[] self.sots_locations=[] self.goal_locations=[] self.goals=[] self.goal_index=0 self.barren_overworld_zones=[] self.placed_ow_barren=0 self.barren_dungeons=[] self.placed_dungeon_barren=0 self.prev_barren_type=None self.hinted_locations=[] self.hintable_items=[] self.junk_hints=[] self.sometimes_hints=[] self.barren_hinted_areas=set() self.counts_by_type=defaultdict(int) self.hintfuncs={ \"sometimes\": self._create_sometimes_hint, \"sots\": self._create_sots_hint, \"goal\": self._create_goal_hint, \"barren\": self._create_barren_hint, \"item\": self._create_item_hint, \"random\": self._create_random_hint, \"junk\": self._create_junk_hint, \"bk\": self._create_bk_hint, } def read_from_file(self, f): self._read_from_json(json.load(f)) def read_from_str(self, s): self._read_from_json(json.loads(s)) def _read_from_json(self, jsn): self.banned_stones=jsn[\"banned_stones\"] self.added_locations=jsn[\"added_locations\"] self.removed_locations=jsn[\"removed_locations\"] self.added_items=jsn[\"added_items\"] self.removed_items=jsn[\"removed_items\"] self.dungeon_sots_limit=jsn[\"dungeon_sots_limit\"] self.dungeon_barren_limit=jsn[\"dungeon_barren_limit\"] self.distribution=jsn[\"distribution\"] \"\"\" Performs initial calculations and populates the distributions internal tracking mechanisms for hint generation \"\"\" def start(self, logic: Logic, always_hints: list, sometimes_hints: list): self.rng=logic.rando.rng self.logic=logic for loc in self.added_locations: location=loc[\"location\"] if loc[\"type\"]==\"always\": if location in always_hints: continue always_hints.append(location) if location in sometimes_hints: sometimes_hints.remove(location) elif loc[\"type\"]==\"sometimes\": if location in sometimes_hints: continue sometimes_hints.append(location) if location in always_hints: always_hints.remove(location) for loc in self.removed_locations: if loc in always_hints: always_hints.remove(loc) if loc in sometimes_hints: sometimes_hints.remove(loc) for hint in always_hints: self.hinted_locations.append(hint) if hint in SILENT_REALM_CHECKS.keys(): loc_trial_gate=SILENT_REALM_CHECKS[hint] trial_gate_dest=self.logic.trial_connections[loc_trial_gate] trial_gate_dest_loc=[ trial for trial in SILENT_REALM_CHECKS.keys() if trial_gate_dest in trial ].pop() trial_item=self.logic.done_item_locations[trial_gate_dest_loc] self.hints.extend( [TrialGateGossipStoneHint(hint, trial_item, True, loc_trial_gate)] * self.distribution[\"always\"][\"copies\"] ) else: self.hints.extend( [ LocationGossipStoneHint( hint, self.logic.done_item_locations[hint], True, self.logic.item_locations[hint].get(\"text\"), \"always\", ) ] * self.distribution[\"always\"][\"copies\"] ) self.rng.shuffle(self.hints) self.rng.shuffle(sometimes_hints) self.sometimes_hints=sometimes_hints self.hinted_locations.extend( ( loc for loc in self.logic.prerandomization_item_locations.keys() if not self.logic.is_restricted_placement_item( self.logic.done_item_locations[loc] ) ) ) self.required_boss_key_locations=[ loc for loc, item in self.logic.done_item_locations.items() if(\"Boss Key\" in item) and Logic.split_location_name_by_zone(loc)[0] in self.logic.required_dungeons ] self.rng.shuffle(self.required_boss_key_locations) self.sots_locations=self.loc_dict_filter(self.logic.rando.sots_locations) self.rng.shuffle(self.sots_locations) self.goals=list(self.logic.rando.goal_locations.keys()) self.rng.shuffle(self.goals) self.goal_locations=[ (self.loc_dict_filter(self.logic.rando.goal_locations[goal_name])) for goal_name in self.goals ] for locations in self.goal_locations: self.rng.shuffle(locations) region_barren, nonprogress=self.logic.get_barren_regions() for zone in region_barren: if \"Silent Realm\" in zone: continue if self.logic.rando.options[\"empty-unrequired-dungeons\"]: if( not self.logic.rando.options[\"triforce-required\"] or self.logic.rando.options[\"triforce-shuffle\"]==\"Anywhere\" ) and(zone==\"Sky Keep\"): continue if( zone in POTENTIALLY_REQUIRED_DUNGEONS and zone not in self.logic.required_dungeons ): continue if zone==\"Sky Keep\": if self.logic.rando.options[\"map-mode\"] not in[ \"Removed\", \"Anywhere\", ] or self.logic.rando.options[\"small-key-mode\"] not in[\"Anywhere\"]: continue if zone in ALL_DUNGEON_AREAS: self.barren_dungeons.append(zone) else: self.barren_overworld_zones.append(zone) self.hintable_items=HINTABLE_ITEMS.copy() for item in self.added_items: self.hintable_items.extend([item[\"name\"]] * item[\"amount\"]) if \"Sea Chart\" in self.logic.all_progress_items: self.hintable_items.append(\"Sea Chart\") for item in self.removed_items: if item in self.hintable_items: self.hintable_items.remove(item) for item in self.logic.starting_items: if item in self.hintable_items: self.hintable_items.remove(item) self.logic.rando.rng.shuffle(self.hintable_items) needed_fixed=[] if \"goal\" in self.distribution.keys(): self.distribution[\"goal\"][\"fixed\"] *=len(self.logic.required_dungeons) for type in self.distribution.keys(): if self.distribution[type][\"fixed\"] > 0: needed_fixed.append(type) needed_fixed.sort(key=lambda type: self.distribution[type][\"order\"]) self.junk_hints=JUNK_TEXT.copy() self.rng.shuffle(self.junk_hints) for type in needed_fixed: curr_type=self.distribution[type] func=self.hintfuncs[type] for _ in range(curr_type[\"fixed\"]): if hint:=func(): self.counts_by_type[type] +=1 self.hints.extend([hint] * curr_type[\"copies\"]) self.hints.reverse() for hint_type in self.distribution.keys(): self.weighted_types.append(hint_type) self.weights.append(self.distribution[hint_type][\"weight\"]) \"\"\" Method to filter out keys from SotS and Goal item location dictionaries and return a list of tuples of zones, locations, and items \"\"\" def loc_dict_filter(self, loc_dict): filtered_locations=[] for loc, item in loc_dict.items(): if item in self.removed_items: continue if self.logic.is_restricted_placement_item(item): continue zone, specific_loc=Logic.split_location_name_by_zone(loc) filtered_locations.append((zone, loc, item)) return filtered_locations \"\"\" Uses the distribution to calculate all the hints \"\"\" def get_hints(self, count) -> List[GossipStoneHint]: hints=self.hints while len(hints) < count: [next_type]=self.rng.choices(self.weighted_types, self.weights) if(limit:=self.distribution[next_type].get(\"max\")) is not None: if self.counts_by_type[next_type] >=limit: continue if hint:=self.hintfuncs[next_type](): self.counts_by_type[next_type] +=1 hints.extend([hint] * self.distribution[next_type][\"copies\"]) hints=hints[:count] return hints def _create_sometimes_hint(self): if not self.sometimes_hints: return None hint=self.sometimes_hints.pop() if hint in self.hinted_locations: return self._create_sometimes_hint() self.hinted_locations.append(hint) return LocationGossipStoneHint( hint, self.logic.done_item_locations[hint], True, self.logic.item_locations[hint].get(\"text\"), \"sometimes\", ) def _create_sots_hint(self): if not self.sots_locations: return None zone, loc, item=self.sots_locations.pop() if loc in self.hinted_locations: return self._create_sots_hint() if( self.sots_dungeon_placed >=self.dungeon_sots_limit and zone in ALL_DUNGEON_AREAS ): return self._create_sots_hint() if zone in ALL_DUNGEON_AREAS: self.sots_dungeon_placed +=1 self.hinted_locations.append(loc) if \"Goddess Chest\" in loc: zone=self.logic.rando.item_locations[loc][\"cube_region\"] if self.logic.rando.options[\"cube-sots\"]: if zone==\"Skyview\": zone=\"Faron Woods\" elif zone==\"Mogma Turf\": zone=\"Eldin Volcano\" elif zone==\"Lanayru Mines\": zone=\"Lanayru Desert\" elif zone==\"Lanayru Gorge\": zone=\"Lanayru Sand Sea\" return CubeSotsGoalGossipStoneHint(loc, item, True, zone, None) return SotsGoalGossipStoneHint(loc, item, True, zone, None) def _create_goal_hint(self): if not self.goal_locations[self.goal_index]: if not any(self.goal_locations): return None self.goal_index +=1 self.goal_index %=len(self.goals) return self._create_goal_hint() zone, loc, item=self.goal_locations[self.goal_index].pop() if loc in self.hinted_locations: return self._create_goal_hint() if( self.sots_dungeon_placed >=self.dungeon_sots_limit and zone in ALL_DUNGEON_AREAS ): return self._create_goal_hint() if zone in ALL_DUNGEON_AREAS: self.sots_dungeon_placed +=1 self.hinted_locations.append(loc) self.goal_index +=1 self.goal_index %=len(self.goals) goal=self.goals[self.goal_index -1] if \"Goddess Chest\" in loc: zone=self.logic.rando.item_locations[loc][\"cube_region\"] if self.logic.rando.options[\"cube-sots\"]: if zone==\"Skyview\": zone=\"Faron Woods\" elif zone==\"Mogma Turf\": zone=\"Eldin Volcano\" elif zone==\"Lanayru Mines\": zone=\"Lanayru Desert\" elif zone==\"Lanayru Gorge\": zone=\"Lanayru Sand Sea\" return CubeSotsGoalGossipStoneHint(loc, item, True, zone, goal) return SotsGoalGossipStoneHint(loc, item, True, zone, goal) def _create_barren_hint(self): if self.prev_barren_type is None: self.prev_barren_type=self.rng.choices( [\"dungeon\", \"overworld\"],[0.5, 0.5] )[0] elif self.prev_barren_type==\"dungeon\": self.prev_barren_type=self.rng.choices( [\"dungeon\", \"overworld\"],[0.25, 0.75] )[0] elif self.prev_barren_type==\"overworld\": self.prev_barren_type=self.rng.choices( [\"dungeon\", \"overworld\"],[0.75, 0.25] )[0] if self.prev_barren_type==\"dungeon\": if self.placed_dungeon_barren > self.dungeon_barren_limit: self.prev_barren_type=\"overworld\" if len(self.barren_dungeons)==0 and self.prev_barren_type==\"dungeon\": self.prev_barren_type=\"overworld\" if len(self.barren_overworld_zones)==0: return None if( len(self.barren_overworld_zones)==0 and self.prev_barren_type==\"overworld\" ): self.prev_barren_type=\"dungeon\" if len(self.barren_dungeons)==0: return None if self.prev_barren_type==\"dungeon\": barren_area_list=self.barren_dungeons else: barren_area_list=self.barren_overworld_zones weights=[ len(self.logic.prog_locations_by_zone_name[area]) for area in barren_area_list ] area=self.rng.choices(barren_area_list, weights)[0] barren_area_list.remove(area) self.barren_hinted_areas.add(area) return BarrenGossipStoneHint(None, None, False, area) def _create_item_hint(self): if not self.hintable_items: return None hinted_item=self.hintable_items.pop() locs=[ (location, item) for location, item in self.logic.done_item_locations.items() if item==hinted_item and location not in self.hinted_locations ] if not locs: return None location, item=self.rng.choice(locs) self.hinted_locations.append(location) if self.logic.rando.options[\"precise-item\"]: return LocationGossipStoneHint( location, item, True, self.logic.item_locations[location].get(\"text\"), \"precise_item\", ) zone_override, _=self.logic.split_location_name_by_zone(location) if \"Goddess Chest\" in location: zone_override=self.logic.rando.item_locations[location][\"cube_region\"] return ZoneItemGossipStoneHint(location, item, True, zone_override) def _create_random_hint(self): all_locations_without_hint=self.logic.filter_locations_for_progression( ( loc for loc in self.logic.done_item_locations if not loc in self.hinted_locations and not loc in self.logic.prerandomization_item_locations and Logic.split_location_name_by_zone(loc)[0] not in self.barren_hinted_areas ) ) loc=self.rng.choice(all_locations_without_hint) self.hinted_locations.append(loc) return LocationGossipStoneHint( loc, self.logic.done_item_locations[loc], True, self.logic.item_locations[loc].get(\"text\"), \"random\", ) def _create_bk_hint(self): if not self.required_boss_key_locations: return None loc=self.required_boss_key_locations.pop() if loc in self.hinted_locations: return self._create_bk_hint() self.hinted_locations.append(loc) return LocationGossipStoneHint( loc, self.logic.done_item_locations[loc], True, self.logic.item_locations[loc].get(\"text\"), \"boss_key\", ) def _create_junk_hint(self): return EmptyGossipStoneHint(None, None, False, self.junk_hints.pop()) def get_junk_text(self): return self.junk_hints.pop() ","sourceWithComments":"from collections import defaultdict\nimport json\nfrom random import Random\n\nfrom hints.hint_types import *\nfrom logic.constants import (\n    POTENTIALLY_REQUIRED_DUNGEONS,\n    ALL_DUNGEON_AREAS,\n    SILENT_REALM_CHECKS,\n)\nfrom logic.logic import Logic\nfrom paths import RANDO_ROOT_PATH\n\n\nHINTABLE_ITEMS = (\n    [\"Clawshots\"]\n    + [\"Progressive Beetle\"] * 2\n    + [\"Progressive Sword\"] * 6\n    + [\"Emerald Tablet\"] * 1\n    + [\"Ruby Tablet\"] * 1\n    + [\"Amber Tablet\"] * 1\n    + [\"Goddess Harp\"] * 1\n    + [\"Water Scale\"] * 1\n    + [\"Fireshield Earrings\"] * 1\n)\n\nJUNK_TEXT = [\n    \"They say that crashing in BiT is easy.\",\n    \"They say that bookshelves can talk\",\n    \"They say that people who love the Bug Net also like Trains\",\n    \"They say that there is a Gossip Stone by the Temple of Time\",\n    \"They say there's a 35% chance for FS Boss Key to be Heetle Locked\",\n    \"They say 64bit left Fire Sanctuary without learning Ballad of the Goddess\",\n    \"They say that Ancient Cistern is haunted by the ghosts of softlocked Links\",\n    \"They say the Potion Lady is still holding onto a Spiral Charge for CJ\",\n    \"They say there is a chest underneath the party wheel in Lanayru\",\n    \"They say that you need the hero's tunic to sleep on the main part of Skyloft\",\n    \"They say that you need to Hot the Spile to defeat Imprisoned 2\",\n    \"They say whenever Spiral Charge is on a trial, a seed roller goes mysteriously missing\",\n    \"They say that Eldin Trial is vanilla whenever it is required\",\n    \"They say that gymnast86 won the first randomizer tournament and retired immediately after\",\n    \"They say that Mogmas don't understand Minesweeper\",\n    \"They say that you can win a race by abandoning Lanayru to check Cawlin's Letter\",\n    \"They say that tornados spawn frequently in the Sky\",\n    \"They say Scrapper gets easily tilted\",\n    \"They say there is a chest on the cliffs by the Goddess Statue\",\n    \"They say that entering Ancient Cistern with no B items has a 1% chance of success\",\n    \"They say that Glittering Spores are the best bird drugs\",\n    \"They say that the Ancient Automaton fears danger darts\",\n    \"They say the single tumbling plant is required every seed\",\n    \"They say that your battery is low\",\n    \"They say that you just have to get the right checks to win\",\n    \"They say that rushing Peatrice is the play\",\n    \"They say there is a 0.0000001164% chance your RNG won't change\",\n    \"If only we could go Back in Time and name the glitch properly...\",\n    'They say that there is something called a \"hash\" that makes it easier for people to verify that they are playing the right seed',\n    \"They say that the bad seed rollers are still in the car, seeking for a safe refugee\",\n    \"Have you heard the tragedy of Darth Kolok the Pause? I thought not, it's not a story the admins would tell you\",\n    \"Sand Sea is the most hated region in the game, because Sand is coarse, rough and gets everywhere\",\n    \"They say that rice has magical properties when visiting Yerbal\",\n    \"They say that Jannon is still jammin to this day\",\n    \"They say that there is only one place where the Slingshot beats the Bow\",\n    \"They say that Koloktos waiting caused a civil war among players\",\n    \"They say that there is a settings combination which needs 0 checks to be completed\",\n    \"They say that avoiding Fledge's item from a fresh file is impossible\",\n    \"... astronomically ...\",\n    \"They say that you can open the chest behind bars in LMF after raising said bars\",\n    \"They say that you look like you have a Questions\",\n    \"They say that HD randomizer development is delayed by a day every time someone asks about it in the Discord\",\n    \"The disc could not be read. Refer to the Wii Operations Manual for details.\",\n    \"They say that a massive storm brews over the Lanayru Sand Sea due to Tentalus' immense size\",\n]\n\n\nclass InvalidHintDistribution(Exception):\n    pass\n\n\nclass HintDistribution:\n    def __init__(self):\n        self.banned_stones = []\n        self.added_locations = []\n        self.removed_locations = []\n        self.added_items = []\n        self.removed_items = []\n        self.dungeon_sots_limit = 0\n        self.sots_dungeon_placed = 0\n        self.dungeon_barren_limit = 0\n        self.distribution = {}\n        self.rng: Random = None\n        self.logic = None\n        self.hints = []\n        self.weighted_types = []\n        self.weights = []\n        self.sots_locations = []\n        self.goal_locations = []\n        self.goals = []\n        self.goal_index = 0\n        self.barren_overworld_zones = []\n        self.placed_ow_barren = 0\n        self.barren_dungeons = []\n        self.placed_dungeon_barren = 0\n        self.prev_barren_type = None\n        self.hinted_locations = []\n        self.hintable_items = []\n        self.junk_hints = []\n        self.sometimes_hints = []\n        self.barren_hinted_areas = set()\n        self.counts_by_type = defaultdict(int)\n\n        self.hintfuncs = {\n            \"sometimes\": self._create_sometimes_hint,\n            \"sots\": self._create_sots_hint,\n            \"goal\": self._create_goal_hint,\n            \"barren\": self._create_barren_hint,\n            \"item\": self._create_item_hint,\n            \"random\": self._create_random_hint,\n            \"junk\": self._create_junk_hint,\n            \"bk\": self._create_bk_hint,\n        }\n\n    def read_from_file(self, f):\n        self._read_from_json(json.load(f))\n\n    def read_from_str(self, s):\n        self._read_from_json(json.loads(s))\n\n    def _read_from_json(self, jsn):\n        self.banned_stones = jsn[\"banned_stones\"]\n        self.added_locations = jsn[\"added_locations\"]\n        self.removed_locations = jsn[\"removed_locations\"]\n        self.added_items = jsn[\"added_items\"]\n        self.removed_items = jsn[\"removed_items\"]\n        self.dungeon_sots_limit = jsn[\"dungeon_sots_limit\"]\n        self.dungeon_barren_limit = jsn[\"dungeon_barren_limit\"]\n        self.distribution = jsn[\"distribution\"]\n\n    \"\"\"\n    Performs initial calculations and populates the distributions internal\n    tracking mechanisms for hint generation\n    \"\"\"\n\n    def start(self, logic: Logic, always_hints: list, sometimes_hints: list):\n        self.rng = logic.rando.rng\n        self.logic = logic\n\n        for loc in self.added_locations:\n            location = loc[\"location\"]\n            if loc[\"type\"] == \"always\":\n                if location in always_hints:\n                    continue\n                always_hints.append(location)\n                if location in sometimes_hints:\n                    sometimes_hints.remove(location)\n            elif loc[\"type\"] == \"sometimes\":\n                if location in sometimes_hints:\n                    continue\n                sometimes_hints.append(location)\n                if location in always_hints:\n                    always_hints.remove(location)\n\n        for loc in self.removed_locations:\n            if loc in always_hints:\n                always_hints.remove(loc)\n            if loc in sometimes_hints:\n                sometimes_hints.remove(loc)\n\n        # all always hints are always hinted\n        for hint in always_hints:\n            self.hinted_locations.append(hint)\n            if hint in SILENT_REALM_CHECKS.keys():\n                loc_trial_gate = SILENT_REALM_CHECKS[hint]\n                trial_gate_dest = self.logic.trial_connections[loc_trial_gate]\n                trial_gate_dest_loc = [\n                    trial\n                    for trial in SILENT_REALM_CHECKS.keys()\n                    if trial_gate_dest in trial\n                ].pop()\n                trial_item = self.logic.done_item_locations[trial_gate_dest_loc]\n                self.hints.extend(\n                    [TrialGateGossipStoneHint(hint, trial_item, True, loc_trial_gate)]\n                    * self.distribution[\"always\"][\"copies\"]\n                )\n            else:\n                self.hints.extend(\n                    [\n                        LocationGossipStoneHint(\n                            hint,\n                            self.logic.done_item_locations[hint],\n                            True,\n                            self.logic.item_locations[hint].get(\"text\"),\n                            \"always\",\n                        )\n                    ]\n                    * self.distribution[\"always\"][\"copies\"]\n                )\n        self.rng.shuffle(self.hints)\n        self.rng.shuffle(sometimes_hints)\n        self.sometimes_hints = sometimes_hints\n\n        # ensure prerandomized locations cannot be hinted\n        self.hinted_locations.extend(\n            (\n                loc\n                for loc in self.logic.prerandomization_item_locations.keys()\n                if not self.logic.is_restricted_placement_item(\n                    self.logic.done_item_locations[loc]\n                )\n            )\n        )\n\n        # creates a list of boss key locations for required dungeons\n        self.required_boss_key_locations = [\n            loc\n            for loc, item in self.logic.done_item_locations.items()\n            if (\"Boss Key\" in item)\n            and Logic.split_location_name_by_zone(loc)[0]\n            in self.logic.required_dungeons\n        ]\n        self.rng.shuffle(self.required_boss_key_locations)\n\n        # populate our internal list copies for later manipulation\n        self.sots_locations = self.loc_dict_filter(self.logic.rando.sots_locations)\n        self.rng.shuffle(self.sots_locations)\n        self.goals = list(self.logic.rando.goal_locations.keys())\n        # shuffle the goal names that will be chosen in sequence when goal hints are placed to try to ensure one is placed for each goal\n        self.rng.shuffle(self.goals)\n        # create corresponding list of shuffled goal items\n        self.goal_locations = [\n            (self.loc_dict_filter(self.logic.rando.goal_locations[goal_name]))\n            for goal_name in self.goals\n        ]\n        for locations in self.goal_locations:\n            self.rng.shuffle(locations)\n\n        region_barren, nonprogress = self.logic.get_barren_regions()\n        for zone in region_barren:\n            if \"Silent Realm\" in zone:\n                continue  # don't hint barren silent realms since they are an always hint\n            if self.logic.rando.options[\"empty-unrequired-dungeons\"]:\n                # avoid placing barren hints for unrequired dungeons in race mode\n                if (\n                    not self.logic.rando.options[\"triforce-required\"]\n                    or self.logic.rando.options[\"triforce-shuffle\"] == \"Anywhere\"\n                ) and (zone == \"Sky Keep\"):\n                    # skykeep is always barren when race mode is on and Sky Keep is skipped\n                    continue\n                if (\n                    zone in POTENTIALLY_REQUIRED_DUNGEONS\n                    and zone not in self.logic.required_dungeons\n                ):\n                    # unrequired dungeons are always barren in race mode\n                    continue\n            if zone == \"Sky Keep\":\n                # exclude Sky Keep from the eligible barren locations if it has no open checks\n                if self.logic.rando.options[\"map-mode\"] not in [\n                    \"Removed\",\n                    \"Anywhere\",\n                ] or self.logic.rando.options[\"small-key-mode\"] not in [\"Anywhere\"]:\n                    continue\n            if zone in ALL_DUNGEON_AREAS:\n                self.barren_dungeons.append(zone)\n            else:\n                self.barren_overworld_zones.append(zone)\n\n        self.hintable_items = HINTABLE_ITEMS.copy()\n        for item in self.added_items:\n            self.hintable_items.extend([item[\"name\"]] * item[\"amount\"])\n        if \"Sea Chart\" in self.logic.all_progress_items:\n            self.hintable_items.append(\"Sea Chart\")\n        for item in self.removed_items:\n            if item in self.hintable_items:\n                self.hintable_items.remove(item)\n        for item in self.logic.starting_items:\n            if item in self.hintable_items:\n                self.hintable_items.remove(item)\n        self.logic.rando.rng.shuffle(self.hintable_items)\n\n        needed_fixed = []\n\n        # for each fixed goal hint, place one for each required dungeon\n        if \"goal\" in self.distribution.keys():\n            self.distribution[\"goal\"][\"fixed\"] *= len(self.logic.required_dungeons)\n\n        for type in self.distribution.keys():\n            if self.distribution[type][\"fixed\"] > 0:\n                needed_fixed.append(type)\n        needed_fixed.sort(key=lambda type: self.distribution[type][\"order\"])\n\n        self.junk_hints = JUNK_TEXT.copy()\n        self.rng.shuffle(self.junk_hints)\n\n        for type in needed_fixed:\n            curr_type = self.distribution[type]\n            func = self.hintfuncs[type]\n            for _ in range(curr_type[\"fixed\"]):\n                if hint := func():\n                    self.counts_by_type[type] += 1\n                    self.hints.extend([hint] * curr_type[\"copies\"])\n\n        # reverse the list of hints to we can pop off the back in O(1)\n        # this also preserves the order they were added as they are removed so that order parameter is repsected\n        self.hints.reverse()\n\n        for hint_type in self.distribution.keys():\n            self.weighted_types.append(hint_type)\n            self.weights.append(self.distribution[hint_type][\"weight\"])\n\n    \"\"\"\n    Method to filter out keys from SotS and Goal item location dictionaries and return a list of tuples of zones, locations, and items\n    \"\"\"\n\n    def loc_dict_filter(self, loc_dict):\n        filtered_locations = []\n        for loc, item in loc_dict.items():\n            if item in self.removed_items:\n                continue\n            if self.logic.is_restricted_placement_item(item):\n                continue\n\n            zone, specific_loc = Logic.split_location_name_by_zone(loc)\n            filtered_locations.append((zone, loc, item))\n        return filtered_locations\n\n    \"\"\"\n    Uses the distribution to calculate all the hints\n    \"\"\"\n\n    def get_hints(self, count) -> List[GossipStoneHint]:\n        hints = self.hints\n        while len(hints) < count:\n            [next_type] = self.rng.choices(self.weighted_types, self.weights)\n            if (limit := self.distribution[next_type].get(\"max\")) is not None:\n                if self.counts_by_type[next_type] >= limit:\n                    continue\n            if hint := self.hintfuncs[next_type]():\n                self.counts_by_type[next_type] += 1\n                hints.extend([hint] * self.distribution[next_type][\"copies\"])\n        hints = hints[:count]\n        return hints\n\n    def _create_sometimes_hint(self):\n        if not self.sometimes_hints:\n            return None\n        hint = self.sometimes_hints.pop()\n        if hint in self.hinted_locations:\n            return self._create_sometimes_hint()\n        self.hinted_locations.append(hint)\n        return LocationGossipStoneHint(\n            hint,\n            self.logic.done_item_locations[hint],\n            True,\n            self.logic.item_locations[hint].get(\"text\"),\n            \"sometimes\",\n        )\n\n    def _create_sots_hint(self):\n        if not self.sots_locations:\n            return None\n        zone, loc, item = self.sots_locations.pop()\n        if loc in self.hinted_locations:\n            return self._create_sots_hint()\n        if (\n            self.sots_dungeon_placed >= self.dungeon_sots_limit\n            and zone in ALL_DUNGEON_AREAS\n        ):\n            return self._create_sots_hint()\n        if zone in ALL_DUNGEON_AREAS:\n            self.sots_dungeon_placed += 1\n        self.hinted_locations.append(loc)\n        if \"Goddess Chest\" in loc:\n            zone = self.logic.rando.item_locations[loc][\"cube_region\"]\n            # place cube sots hint & catch specific zones and fit them into their general zone (as seen in the cube progress options)\n            if self.logic.rando.options[\"cube-sots\"]:\n                if zone == \"Skyview\":\n                    zone = \"Faron Woods\"\n                elif zone == \"Mogma Turf\":\n                    zone = \"Eldin Volcano\"\n                elif zone == \"Lanayru Mines\":\n                    zone = \"Lanayru Desert\"\n                elif zone == \"Lanayru Gorge\":\n                    zone = \"Lanayru Sand Sea\"\n                return CubeSotsGoalGossipStoneHint(loc, item, True, zone, None)\n        return SotsGoalGossipStoneHint(loc, item, True, zone, None)\n\n    def _create_goal_hint(self):\n        if not self.goal_locations[self.goal_index]:\n            # if there aren't applicable locations for any goal, return None\n            if not any(self.goal_locations):\n                return None\n            # go to next goal if no locations are left for this goal\n            self.goal_index += 1\n            self.goal_index %= len(self.goals)\n            return self._create_goal_hint()\n        zone, loc, item = self.goal_locations[self.goal_index].pop()\n        if loc in self.hinted_locations:\n            return self._create_goal_hint()\n        if (\n            self.sots_dungeon_placed >= self.dungeon_sots_limit\n            and zone in ALL_DUNGEON_AREAS\n        ):\n            return self._create_goal_hint()\n        if zone in ALL_DUNGEON_AREAS:\n            # goal hints will use the same dungeon limits as sots hints\n            self.sots_dungeon_placed += 1\n        self.hinted_locations.append(loc)\n        # move to next goal boss for next goal hint\n        self.goal_index += 1\n        self.goal_index %= len(self.goals)\n        goal = self.goals[self.goal_index - 1]\n        if \"Goddess Chest\" in loc:\n            zone = self.logic.rando.item_locations[loc][\"cube_region\"]\n            # place cube sots hint & catch specific zones and fit them into their general zone (as seen in the cube progress options)\n            if self.logic.rando.options[\"cube-sots\"]:\n                if zone == \"Skyview\":\n                    zone = \"Faron Woods\"\n                elif zone == \"Mogma Turf\":\n                    zone = \"Eldin Volcano\"\n                elif zone == \"Lanayru Mines\":\n                    zone = \"Lanayru Desert\"\n                elif zone == \"Lanayru Gorge\":\n                    zone = \"Lanayru Sand Sea\"\n                return CubeSotsGoalGossipStoneHint(loc, item, True, zone, goal)\n        return SotsGoalGossipStoneHint(loc, item, True, zone, goal)\n\n    def _create_barren_hint(self):\n        if self.prev_barren_type is None:\n            # 50\/50 between dungeon and overworld on the first hint\n            self.prev_barren_type = self.rng.choices(\n                [\"dungeon\", \"overworld\"], [0.5, 0.5]\n            )[0]\n        elif self.prev_barren_type == \"dungeon\":\n            self.prev_barren_type = self.rng.choices(\n                [\"dungeon\", \"overworld\"], [0.25, 0.75]\n            )[0]\n        elif self.prev_barren_type == \"overworld\":\n            self.prev_barren_type = self.rng.choices(\n                [\"dungeon\", \"overworld\"], [0.75, 0.25]\n            )[0]\n\n        # Check against caps\n        if self.prev_barren_type == \"dungeon\":\n            if self.placed_dungeon_barren > self.dungeon_barren_limit:\n                self.prev_barren_type = \"overworld\"\n\n        # Failsafes if there are not enough barren hints to fill out the generated hint\n        if len(self.barren_dungeons) == 0 and self.prev_barren_type == \"dungeon\":\n            self.prev_barren_type = \"overworld\"\n            if len(self.barren_overworld_zones) == 0:\n                return None\n        if (\n            len(self.barren_overworld_zones) == 0\n            and self.prev_barren_type == \"overworld\"\n        ):\n            self.prev_barren_type = \"dungeon\"\n            if len(self.barren_dungeons) == 0:\n                return None\n\n        # generate a hint and remove it from the lists\n        if self.prev_barren_type == \"dungeon\":\n            barren_area_list = self.barren_dungeons\n        else:\n            barren_area_list = self.barren_overworld_zones\n        weights = [\n            len(self.logic.prog_locations_by_zone_name[area])\n            for area in barren_area_list\n        ]\n        area = self.rng.choices(barren_area_list, weights)[0]\n        barren_area_list.remove(area)\n        self.barren_hinted_areas.add(area)\n        return BarrenGossipStoneHint(None, None, False, area)\n\n    def _create_item_hint(self):\n        if not self.hintable_items:\n            return None\n        hinted_item = self.hintable_items.pop()\n        locs = [\n            (location, item)\n            for location, item in self.logic.done_item_locations.items()\n            if item == hinted_item and location not in self.hinted_locations\n        ]\n        if not locs:\n            return None\n        location, item = self.rng.choice(locs)\n        self.hinted_locations.append(location)\n        if self.logic.rando.options[\"precise-item\"]:\n            return LocationGossipStoneHint(\n                location,\n                item,\n                True,\n                self.logic.item_locations[location].get(\"text\"),\n                \"precise_item\",\n            )\n        zone_override, _ = self.logic.split_location_name_by_zone(location)\n        if \"Goddess Chest\" in location:\n            zone_override = self.logic.rando.item_locations[location][\"cube_region\"]\n        return ZoneItemGossipStoneHint(location, item, True, zone_override)\n\n    def _create_random_hint(self):\n        all_locations_without_hint = self.logic.filter_locations_for_progression(\n            (\n                loc\n                for loc in self.logic.done_item_locations\n                if not loc in self.hinted_locations\n                and not loc in self.logic.prerandomization_item_locations\n                and Logic.split_location_name_by_zone(loc)[0]\n                not in self.barren_hinted_areas\n            )\n        )\n        loc = self.rng.choice(all_locations_without_hint)\n        self.hinted_locations.append(loc)\n        return LocationGossipStoneHint(\n            loc,\n            self.logic.done_item_locations[loc],\n            True,\n            self.logic.item_locations[loc].get(\"text\"),\n            \"random\",\n        )\n\n    def _create_bk_hint(self):\n        if not self.required_boss_key_locations:\n            return None\n        loc = self.required_boss_key_locations.pop()\n        if loc in self.hinted_locations:\n            return self._create_bk_hint()\n        self.hinted_locations.append(loc)\n        return LocationGossipStoneHint(\n            loc,\n            self.logic.done_item_locations[loc],\n            True,\n            self.logic.item_locations[loc].get(\"text\"),\n            \"boss_key\",\n        )\n\n    def _create_junk_hint(self):\n        return EmptyGossipStoneHint(None, None, False, self.junk_hints.pop())\n\n    def get_junk_text(self):\n        return self.junk_hints.pop()\n"}},"msg":"Prevent Flooded Faron Woods being hinted barren"}},"https:\/\/github.com\/oskemarkup\/koronapay_spread_bot":{"0e37453251375b7f375b3879c2e5c771dee989fb":{"url":"https:\/\/api.github.com\/repos\/oskemarkup\/koronapay_spread_bot\/commits\/0e37453251375b7f375b3879c2e5c771dee989fb","html_url":"https:\/\/github.com\/oskemarkup\/koronapay_spread_bot\/commit\/0e37453251375b7f375b3879c2e5c771dee989fb","message":"changed condition for send message to prevent flood","sha":"0e37453251375b7f375b3879c2e5c771dee989fb","keyword":"flooding prevent","diff":"diff --git a\/main.py b\/main.py\nindex 593cb88..9b8b466 100644\n--- a\/main.py\n+++ b\/main.py\n@@ -137,8 +137,6 @@ def turkey_try_to_usdt(rate, fee):\n     korona_params = bundle.get(\"korona_params\", {})\r\n     fee_function = bundle.get(\"fee_function\", add_fee)\r\n \r\n-    old = data.get(tag + \"_rate\", 0)\r\n-    old_with_fee = data.get(tag + \"_rate_with_fee\", 0)\r\n     fee = fees_data.get(tag, config.default_fee)\r\n     rate = get_korona_rate(korona_params)\r\n     rate_with_fee = fee_function(rate, fee)\r\n@@ -148,10 +146,10 @@ def turkey_try_to_usdt(rate, fee):\n     rates += f\"\\U0001F451{name} {rate} ({rate_with_fee})\\n\"\r\n     fees += f\"\\U0001F4B1{name} {fee}%\\n\"\r\n \r\n-    new_data[tag + \"_rate\"] = rate\r\n-    new_data[tag + \"_rate_with_fee\"] = rate_with_fee\r\n+    new_data[tag] = spread\r\n+    old_spread = data.get(tag, 0)\r\n \r\n-    if (old != rate or old_with_fee != rate_with_fee):\r\n+    if (spread > 0 and abs(old_spread - spread) >= 0.5):\r\n         is_changed = True\r\n \r\n if (is_changed is True):\r\n","files":{"\/main.py":{"changes":[{"diff":"\n     korona_params = bundle.get(\"korona_params\", {})\r\n     fee_function = bundle.get(\"fee_function\", add_fee)\r\n \r\n-    old = data.get(tag + \"_rate\", 0)\r\n-    old_with_fee = data.get(tag + \"_rate_with_fee\", 0)\r\n     fee = fees_data.get(tag, config.default_fee)\r\n     rate = get_korona_rate(korona_params)\r\n     rate_with_fee = fee_function(rate, fee)\r\n","add":0,"remove":2,"filename":"\/main.py","badparts":["    old = data.get(tag + \"_rate\", 0)\r","    old_with_fee = data.get(tag + \"_rate_with_fee\", 0)\r"],"goodparts":[]},{"diff":"\n     rates += f\"\\U0001F451{name} {rate} ({rate_with_fee})\\n\"\r\n     fees += f\"\\U0001F4B1{name} {fee}%\\n\"\r\n \r\n-    new_data[tag + \"_rate\"] = rate\r\n-    new_data[tag + \"_rate_with_fee\"] = rate_with_fee\r\n+    new_data[tag] = spread\r\n+    old_spread = data.get(tag, 0)\r\n \r\n-    if (old != rate or old_with_fee != rate_with_fee):\r\n+    if (spread > 0 and abs(old_spread - spread) >= 0.5):\r\n         is_changed = True\r\n \r\n if (is_changed is True):\r\n","add":3,"remove":3,"filename":"\/main.py","badparts":["    new_data[tag + \"_rate\"] = rate\r","    new_data[tag + \"_rate_with_fee\"] = rate_with_fee\r","    if (old != rate or old_with_fee != rate_with_fee):\r"],"goodparts":["    new_data[tag] = spread\r","    old_spread = data.get(tag, 0)\r","    if (spread > 0 and abs(old_spread - spread) >= 0.5):\r"]}],"source":"\nimport math\r import requests\r import json\r \r import config\r \r def get_binance_rate(params):\r url=\"https:\/\/p2p.binance.com\/bapi\/c2c\/v2\/friendly\/c2c\/adv\/search\"\r binance_params={\r \"asset\": \"USDT\",\r \"page\": 1,\r \"rows\": 1,\r }\r binance_params.update(params)\r headers={\r \"Content-Type\": \"application\/json\",\r }\r response=requests.request(\"POST\", url, data=json.dumps(binance_params), headers=headers)\r \r return float(response.json().get(\"data\")[0].get(\"adv\").get(\"price\"))\r \r def get_korona_rate(params):\r korona_url=\"https:\/\/koronapay.com\/transfers\/online\/api\/transfers\/tariffs\"\r korona_params={\r \"sendingCountryId\": \"RUS\",\r \"sendingCurrencyId\": \"810\",\r \"paymentMethod\": \"debitCard\",\r \"receivingAmount\": \"10000\",\r \"receivingMethod\": \"cash\",\r \"paidNotificationEnabled\": \"false\",\r }\r korona_params.update(params)\r headers={\r \"User-Agent\": \"Mozilla\/5.0(iPad; CPU OS 12_2 like Mac OS X) AppleWebKit\/605.1.15(KHTML, like Gecko) Mobile\/15E148\",\r }\r request=requests.get(korona_url, params=korona_params, headers=headers)\r \r return request.json()[0].get(\"exchangeRate\")\r \r def send_msg(text):\r params={\r \"chat_id\": config.chat_id,\r \"text\": text,\r \"parse_mode\": \"html\",\r }\r url_req=\"https:\/\/api.telegram.org\/bot\" +config.token +\"\/sendMessage\"\r \r requests.get(url_req, params=params)\r \r def add_fee(sum, fee):\r return math.ceil(10000 * sum \/(1 -fee \/ 100)) \/ 10000\r \r def calc_spread(buy, sell):\r return math.ceil(10000 *(sell \/ buy -1)) \/ 100\r \r def print_spread(spread):\r if(spread > 1):\r return f\"<b>{spread}%<\/b>\"\r \r if(spread >=0):\r return f\"{spread}%\"\r \r return f\"<s>{spread}%<\/s>\"\r \r def turkey_try_to_usdt(rate, fee):\r usdt_rate=get_binance_rate({\r \"fiat\": \"TRY\",\r \"tradeType\": \"Buy\",\r \"payTypes\":[\"KoronaPay\"]\r }) \r \r return add_fee(rate * usdt_rate, fee)\r \r bundles=[\r {\r \"name\": \"\\U0001f1f9\\U0001f1f7 \\u20BA\",\r \"tag\": \"turkey_try\",\r \"korona_params\":{\r \"receivingCountryId\": \"TUR\",\r \"receivingCurrencyId\": \"949\",\r },\r \"fee_function\": turkey_try_to_usdt,\r },\r {\r \"name\": \"\\U0001f1f9\\U0001f1f7 $\",\r \"tag\": \"turkey_usd\",\r \"korona_params\":{\r \"receivingCountryId\": \"TUR\",\r \"receivingCurrencyId\": \"840\",\r },\r },\r {\r \"name\": \"\\U0001f1f9\\U0001f1f7 \\u20AC\",\r \"tag\": \"turkey_eur\",\r \"korona_params\":{\r \"receivingCountryId\": \"TUR\",\r \"receivingCurrencyId\": \"978\",\r },\r },\r {\r \"name\": \"\\U0001F1EC\\U0001F1EA $\",\r \"tag\": \"georgia_usd\",\r \"korona_params\":{\r \"receivingCountryId\": \"GEO\",\r \"receivingCurrencyId\": \"840\",\r },\r },\r ]\r \r try:\r data=json.load(open(config.file_name, \"r\"))\r except:\r data={}\r \r fees_data=requests.get(config.fees_url).json()\r \r usdt_sell=get_binance_rate({\r \"fiat\": \"RUB\",\r \"tradeType\": \"Sell\",\r \"payTypes\":[\"TinkoffNew\", \"RosBankNew\", \"RaiffeisenBank\"]\r })\r \r spreads=\"\"\r rates=\"\"\r fees=\"\"\r new_data={\r \"usdt_sell\": usdt_sell,\r }\r \r is_changed=False\r \r for bundle in bundles:\r name=bundle.get(\"name\", \"\")\r tag=bundle.get(\"tag\", \"\")\r korona_params=bundle.get(\"korona_params\",{})\r fee_function=bundle.get(\"fee_function\", add_fee)\r \r old=data.get(tag +\"_rate\", 0)\r old_with_fee=data.get(tag +\"_rate_with_fee\", 0)\r fee=fees_data.get(tag, config.default_fee)\r rate=get_korona_rate(korona_params)\r rate_with_fee=fee_function(rate, fee)\r spread=calc_spread(rate_with_fee, usdt_sell)\r \r spreads +=f\"{name}{print_spread(spread)}\\n\"\r rates +=f\"\\U0001F451{name}{rate}({rate_with_fee})\\n\"\r fees +=f\"\\U0001F4B1{name}{fee}%\\n\"\r \r new_data[tag +\"_rate\"]=rate\r new_data[tag +\"_rate_with_fee\"]=rate_with_fee\r \r if(old !=rate or old_with_fee !=rate_with_fee):\r is_changed=True\r \r if(is_changed is True):\r send_msg(spreads +\"\\n\" +rates +\"\\n\" +fees +\"\\n\" +f\"\\u2B05{usdt_sell} \\u20BD\")\r \r json.dump(new_data, open(config.file_name, \"w+\"))\r ","sourceWithComments":"import math\r\nimport requests\r\nimport json\r\n\r\nimport config\r\n\r\n# Functions\r\ndef get_binance_rate(params):\r\n    url = \"https:\/\/p2p.binance.com\/bapi\/c2c\/v2\/friendly\/c2c\/adv\/search\"\r\n    binance_params = {\r\n        \"asset\": \"USDT\",\r\n        \"page\": 1,\r\n        \"rows\": 1,\r\n    }\r\n    binance_params.update(params)\r\n    headers = {\r\n        \"Content-Type\": \"application\/json\",\r\n    }\r\n    response = requests.request(\"POST\", url, data=json.dumps(binance_params), headers=headers)\r\n\r\n    return float(response.json().get(\"data\")[0].get(\"adv\").get(\"price\"))\r\n\r\ndef get_korona_rate(params):\r\n    korona_url = \"https:\/\/koronapay.com\/transfers\/online\/api\/transfers\/tariffs\"\r\n    korona_params = {\r\n        \"sendingCountryId\": \"RUS\",\r\n        \"sendingCurrencyId\": \"810\",\r\n        \"paymentMethod\": \"debitCard\",\r\n        \"receivingAmount\": \"10000\",\r\n        \"receivingMethod\": \"cash\",\r\n        \"paidNotificationEnabled\": \"false\",\r\n    }\r\n    korona_params.update(params)\r\n    headers = {\r\n        \"User-Agent\": \"Mozilla\/5.0 (iPad; CPU OS 12_2 like Mac OS X) AppleWebKit\/605.1.15 (KHTML, like Gecko) Mobile\/15E148\",\r\n    }\r\n    request = requests.get(korona_url, params=korona_params, headers=headers)\r\n\r\n    return request.json()[0].get(\"exchangeRate\")\r\n\r\ndef send_msg(text):\r\n    params = {\r\n        \"chat_id\": config.chat_id,\r\n        \"text\": text,\r\n        \"parse_mode\": \"html\",\r\n    }\r\n    url_req = \"https:\/\/api.telegram.org\/bot\" + config.token + \"\/sendMessage\"\r\n\r\n    requests.get(url_req, params=params)\r\n\r\ndef add_fee(sum, fee):\r\n    return math.ceil(10000 * sum \/ (1 - fee \/ 100)) \/ 10000\r\n\r\ndef calc_spread(buy, sell):\r\n    return math.ceil(10000 * (sell \/ buy - 1)) \/ 100\r\n\r\ndef print_spread(spread):\r\n    if (spread > 1):\r\n        return f\"<b>{spread}%<\/b>\"\r\n\r\n    if (spread >= 0):\r\n        return f\"{spread}%\"\r\n\r\n    return f\"<s>{spread}%<\/s>\"\r\n\r\ndef turkey_try_to_usdt(rate, fee):\r\n    usdt_rate = get_binance_rate({\r\n        \"fiat\": \"TRY\",\r\n        \"tradeType\": \"Buy\",\r\n        \"payTypes\": [\"KoronaPay\"]\r\n    })    \r\n\r\n    return add_fee(rate * usdt_rate, fee)\r\n\r\nbundles = [\r\n    {\r\n        \"name\": \"\\U0001f1f9\\U0001f1f7 \\u20BA\",\r\n        \"tag\": \"turkey_try\",\r\n        \"korona_params\": {\r\n            \"receivingCountryId\": \"TUR\",\r\n            \"receivingCurrencyId\": \"949\",\r\n        },\r\n        \"fee_function\": turkey_try_to_usdt,\r\n    },\r\n    {\r\n        \"name\": \"\\U0001f1f9\\U0001f1f7 $\",\r\n        \"tag\": \"turkey_usd\",\r\n        \"korona_params\": {\r\n            \"receivingCountryId\": \"TUR\",\r\n            \"receivingCurrencyId\": \"840\",\r\n        },\r\n    },\r\n    {\r\n        \"name\": \"\\U0001f1f9\\U0001f1f7 \\u20AC\",\r\n        \"tag\": \"turkey_eur\",\r\n        \"korona_params\": {\r\n            \"receivingCountryId\": \"TUR\",\r\n            \"receivingCurrencyId\": \"978\",\r\n        },\r\n    },\r\n    {\r\n        \"name\": \"\\U0001F1EC\\U0001F1EA $\",\r\n        \"tag\": \"georgia_usd\",\r\n        \"korona_params\": {\r\n            \"receivingCountryId\": \"GEO\",\r\n            \"receivingCurrencyId\": \"840\",\r\n        },\r\n    },\r\n]\r\n\r\n# Load data\r\ntry:\r\n  data = json.load(open(config.file_name, \"r\"))\r\nexcept:\r\n  data = {}\r\n\r\nfees_data = requests.get(config.fees_url).json()\r\n\r\nusdt_sell = get_binance_rate({\r\n    \"fiat\": \"RUB\",\r\n    \"tradeType\": \"Sell\",\r\n    \"payTypes\": [\"TinkoffNew\", \"RosBankNew\", \"RaiffeisenBank\"]\r\n})\r\n\r\nspreads = \"\"\r\nrates = \"\"\r\nfees = \"\"\r\nnew_data = {\r\n    \"usdt_sell\": usdt_sell,\r\n}\r\n\r\nis_changed = False\r\n\r\nfor bundle in bundles:\r\n    name = bundle.get(\"name\", \"\")\r\n    tag = bundle.get(\"tag\", \"\")\r\n    korona_params = bundle.get(\"korona_params\", {})\r\n    fee_function = bundle.get(\"fee_function\", add_fee)\r\n\r\n    old = data.get(tag + \"_rate\", 0)\r\n    old_with_fee = data.get(tag + \"_rate_with_fee\", 0)\r\n    fee = fees_data.get(tag, config.default_fee)\r\n    rate = get_korona_rate(korona_params)\r\n    rate_with_fee = fee_function(rate, fee)\r\n    spread = calc_spread(rate_with_fee, usdt_sell)\r\n\r\n    spreads += f\"{name} {print_spread(spread)}\\n\"\r\n    rates += f\"\\U0001F451{name} {rate} ({rate_with_fee})\\n\"\r\n    fees += f\"\\U0001F4B1{name} {fee}%\\n\"\r\n\r\n    new_data[tag + \"_rate\"] = rate\r\n    new_data[tag + \"_rate_with_fee\"] = rate_with_fee\r\n\r\n    if (old != rate or old_with_fee != rate_with_fee):\r\n        is_changed = True\r\n\r\nif (is_changed is True):\r\n    send_msg(spreads + \"\\n\" + rates + \"\\n\" + fees + \"\\n\" + f\"\\u2B05 {usdt_sell} \\u20BD\")\r\n\r\njson.dump(new_data, open(config.file_name, \"w+\"))\r\n"}},"msg":"changed condition for send message to prevent flood"}},"https:\/\/github.com\/DanielVenter\/RestaurantRater":{"d07688f643d2e9b47764a59a9e1ab2f1e2fddda6":{"url":"https:\/\/api.github.com\/repos\/DanielVenter\/RestaurantRater\/commits\/d07688f643d2e9b47764a59a9e1ab2f1e2fddda6","html_url":"https:\/\/github.com\/DanielVenter\/RestaurantRater\/commit\/d07688f643d2e9b47764a59a9e1ab2f1e2fddda6","message":"Added Clear() to prevent flooding of media folder.","sha":"d07688f643d2e9b47764a59a9e1ab2f1e2fddda6","keyword":"flooding prevent","diff":"diff --git a\/populate_database.py b\/populate_database.py\nindex f4956ef..08bf845 100644\n--- a\/populate_database.py\n+++ b\/populate_database.py\n@@ -10,7 +10,17 @@\n from RestaurantRaterApp.models import user_client, Restaurant\n \n \n-def add_restaurant(name: str, street_number: int, street: str, city: str, description: str, restaurant_id: str, comments: dict):\n+def clear():\n+    media_dir = f\"{current_dir}\/media\"\n+    for folder in os.listdir(media_dir):\n+        for file in os.listdir(f\"{media_dir}\/{folder}\"):\n+            os.remove(f\"{media_dir}\/{folder}\/{file}\")\n+        os.rmdir(f\"{media_dir}\/{folder}\")\n+    print(\"Media Folder Cleared\")\n+\n+\n+def add_restaurant(name: str, street_number: int, street: str, city: str, description: str, restaurant_id: str,\n+                   comments: dict):\n     r = Restaurant.objects.get_or_create(name=name, restaurant_id=restaurant_id, street_number=street_number,\n                                          street=street,\n                                          city=city, description=description, comments=comments)[0]\n@@ -26,7 +36,8 @@ def add_restaurant(name: str, street_number: int, street: str, city: str, descri\n     return r\n \n \n-def add_user(username: str, street_number: int, street: str, city: str, liked_restaurants: list, rated_restaurants: dict, password: str, email: str, name: str,\n+def add_user(username: str, street_number: int, street: str, city: str, liked_restaurants: list,\n+             rated_restaurants: dict, password: str, email: str, name: str,\n              surname: str,\n              owner_status=False, owned_restaurants=[]):\n     u = user_client.objects.get_or_create(username=username, street_number=street_number, street=street, city=city,\n@@ -528,6 +539,7 @@ def populate():\n \n if __name__ == \"__main__\":\n     print(\"Starting Rango population script\")\n+    clear()\n     populate()\n     for u in user_client.objects.all():\n         print(f\"Created user {u}\")\n","files":{"\/populate_database.py":{"changes":[{"diff":"\n from RestaurantRaterApp.models import user_client, Restaurant\n \n \n-def add_restaurant(name: str, street_number: int, street: str, city: str, description: str, restaurant_id: str, comments: dict):\n+def clear():\n+    media_dir = f\"{current_dir}\/media\"\n+    for folder in os.listdir(media_dir):\n+        for file in os.listdir(f\"{media_dir}\/{folder}\"):\n+            os.remove(f\"{media_dir}\/{folder}\/{file}\")\n+        os.rmdir(f\"{media_dir}\/{folder}\")\n+    print(\"Media Folder Cleared\")\n+\n+\n+def add_restaurant(name: str, street_number: int, street: str, city: str, description: str, restaurant_id: str,\n+                   comments: dict):\n     r = Restaurant.objects.get_or_create(name=name, restaurant_id=restaurant_id, street_number=street_number,\n                                          street=street,\n                                          city=city, description=description, comments=comments)[0]\n","add":11,"remove":1,"filename":"\/populate_database.py","badparts":["def add_restaurant(name: str, street_number: int, street: str, city: str, description: str, restaurant_id: str, comments: dict):"],"goodparts":["def clear():","    media_dir = f\"{current_dir}\/media\"","    for folder in os.listdir(media_dir):","        for file in os.listdir(f\"{media_dir}\/{folder}\"):","            os.remove(f\"{media_dir}\/{folder}\/{file}\")","        os.rmdir(f\"{media_dir}\/{folder}\")","    print(\"Media Folder Cleared\")","def add_restaurant(name: str, street_number: int, street: str, city: str, description: str, restaurant_id: str,","                   comments: dict):"]},{"diff":"\n     return r\n \n \n-def add_user(username: str, street_number: int, street: str, city: str, liked_restaurants: list, rated_restaurants: dict, password: str, email: str, name: str,\n+def add_user(username: str, street_number: int, street: str, city: str, liked_restaurants: list,\n+             rated_restaurants: dict, password: str, email: str, name: str,\n              surname: str,\n              owner_status=False, owned_restaurants=[]):\n     u = user_client.objects.get_or_create(username=username, street_number=street_number, street=street, city=city,\n","add":2,"remove":1,"filename":"\/populate_database.py","badparts":["def add_user(username: str, street_number: int, street: str, city: str, liked_restaurants: list, rated_restaurants: dict, password: str, email: str, name: str,"],"goodparts":["def add_user(username: str, street_number: int, street: str, city: str, liked_restaurants: list,","             rated_restaurants: dict, password: str, email: str, name: str,"]}],"source":"\nimport os os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'RestaurantRater.settings') current_dir=os.getcwd() import django from django.core.files import File django.setup() from RestaurantRaterApp.models import user_client, Restaurant def add_restaurant(name: str, street_number: int, street: str, city: str, description: str, restaurant_id: str, comments: dict): r=Restaurant.objects.get_or_create(name=name, restaurant_id=restaurant_id, street_number=street_number, street=street, city=city, description=description, comments=comments)[0] images=os.listdir(f\"{current_dir}\\\\PopulateData\\\\images\\\\{restaurant_id}\") r.img1.save(f\"{name}\\\\img1.jpg\", File(open(f\"{current_dir}\\\\PopulateData\\\\images\\\\{restaurant_id}\\\\{images[0]}\", \"rb\"))) r.img2.save(f\"{name}\\\\img2.jpg\", File(open(f\"{current_dir}\\\\PopulateData\\\\images\\\\{restaurant_id}\\\\{images[1]}\", \"rb\"))) r.img3.save(f\"{name}\\\\img3.jpg\", File(open(f\"{current_dir}\\\\PopulateData\\\\images\\\\{restaurant_id}\\\\{images[2]}\", \"rb\"))) r.save() return r def add_user(username: str, street_number: int, street: str, city: str, liked_restaurants: list, rated_restaurants: dict, password: str, email: str, name: str, surname: str, owner_status=False, owned_restaurants=[]): u=user_client.objects.get_or_create(username=username, street_number=street_number, street=street, city=city, rated_restaurants=rated_restaurants, password=password, email=email, name=name, surname=surname, owner_status=owner_status)[0] u.save() for restaurant in rated_restaurants.keys(): rates(name, restaurant) for restaurant in liked_restaurants: likes(name, restaurant) if owner_status: for restaurant in owned_restaurants: owns(name, restaurant) return u def rates(user: str, restaurant: str): user_obj=user_client.objects.get(name=user) restaurant_obj=Restaurant.objects.get(restaurant_id=restaurant) user_obj.rates.add(restaurant_obj) restaurant_obj.ratings.append(user_obj.rated_restaurants[restaurant_obj.restaurant_id]) restaurant_obj.save() def owns(user: str, restaurant: str): user_obj=user_client.objects.get(name=user) restaurant_obj=Restaurant.objects.get(restaurant_id=restaurant) user_obj.owned_restaurants.add(restaurant_obj) def likes(user: str, restaurant: str): user_obj=user_client.objects.get(name=user) restaurant_obj=Restaurant.objects.get(restaurant_id=restaurant) user_obj.liked_restaurants.add(restaurant_obj) def populate(): restaurant_data=[ {\"name\": \"Alchemilla\", \"street_number\": 1126, \"street\": \"Argyle Street\", \"city\": \"Glasgow\", \"description\": \"Seasonal Mediterranean plates and natural wine.\", \"id\": \"ALC\", \"comments\":{ \"Mark.E\": \"\"\"The restaurant is a cute little intimate location in the interesting area of finnieston. The lighting, ambiance and staff were great and serve the restaurant well. \"\"\", \"Matt.W\": \"\"\"Out-of-the-ordinary small plates. Tasty food and friendly service in a hip setting. Plenty of dishes to choose from. A couple of items on the menu may seem shocking at first. You will be rewarded for being adventurous. \"\"\", \"Danny.M\": \"\"\"Service is wonderful, and the ingredients were fresh and nicely presented, and there is a lot of attention going into the dishes, but it's just not my cup of tea as I found the flavours really bland. Portions are also small, so keep that in mind. I wouldn't come back for the food, but it's worth checking out for yourself \"\"\" } }, {\"name\": \"Julie's Kopitiam\", \"street_number\": 556, \"street\": \"Dumbarton Road\", \"city\": \"Glasgow\", \"description\": \"Comfort food done right\", \"id\": \"JK\", \"comments\":{ \"Thom.O\": \"\"\"I've enjoyed eating here in the past but \u00a340 for rice, an egg, a chicken leg, some cucumber and 3 chickpea fritters is excessive. There is just no attempt to compete at all with the great curry houses et al of Glasgow.\"\"\", \"Andy.P\": \"\"\"Fantastic, food was authentic in flavour. The small restaurant has a great vibe and the staff were friendly. Good prices. What a gem of a place.\"\"\", \"Danny.M\": \"\"\"Have had some good meals from here but also some pretty disappointing ones. Takeaway this evening was overpriced, small in proportion and a let-down across the board. You would hope for more consistency. \"\"\" } }, {\"name\": \"Kimchi Cult\", \"street_number\": 14, \"street\": \"Chancellor St\", \"city\": \"Glasgow\", \"description\": \"Korean-style fast food in Glasgow\u2019s West End.\", \"id\": \"KC\", \"comments\":{\"Nicola.H\": \"\"\"My wife and I tried this tonight after hearing good things and got the Korean Chicken burger, bibimbap and tofu. The food was generally tasty their crispy chicken and sauce was good!\"\"\", \"Michael.G\": \"\"\"Working in Glasgow, went 3 times in a week. What more can I say. Outstanding\"\"\", \"Andy.P\": \"\"\"Portion sizes are much smaller now -it is a depressing story for many such places. Two months ago, their portions were generous and now they are absolutely the opposite-I won\u2019t be back \"\"\" } }, {\"name\": \"Ox and Finch\", \"street_number\": 920, \"street\": \"Sauchiehall Street\", \"city\": \"Glasgow\", \"description\": \"The small plates trend is done very well at this slick Sauchiehall Street restaurant.\", \"id\": \"OnF\", \"comments\":{\"Michael.G\": \"\"\"The food served at Ox and Finch, Glasgow is immensely delicious, of the right portion and reasonably priced. You must not give amiss to this place. The service is just superb and there is a variety of meat, vegetables to choose from. You will surely enjoy your meal as I did.\"\"\", \"Andy.P\": \"\"\"I just love this place and visit on every occasion I'm in Glasgow. The food and wines are 1st class foods done extremely well. I've been going here for 6+years and I have never once felt disappointed. Staff and venue are great,will help you with any diet query, same goes for the wines for matching foods.\"\"\"} }, {\"name\": \"Bilson Eleven\", \"street_number\": 10, \"street\": \"Annfield Place\", \"city\": \"Glasgow\", \"description\": \"A five-or even eight-course fine-dining odyssey.\", \"id\": \"BE\", \"comments\":{\"Thom.O\": \"\"\"What an unbelievable night that was last night at this restaurant. The food, the service and the wine was absolutely sublime. Just an absolutely unbelievable experience and I certainly would recommend this place to anyone who loves their wine and their fine dining experiences.\"\"\", \"Michael.G\": \"\"\"Our favourite restaurant in Glasgow. Thank you Nick for a absolute creative menu visually, sensory & fabulously tasting. Mark, your front of house was all that we come to expect, informative, interesting and totally engrossed in your knowledge of the wines & the menu, thanks gents\"\"\"} }, {\"name\": \"Cail Bruich\", \"street_number\": 725, \"street\": \"great Western\", \"city\": \"Glasgow\", \"description\": \"Tr\u00e8s bon Franco-Scottish cooking.\", \"id\": \"CB\", \"comments\":{\"Jeremy.S\": \"\"\"This was our first visit to Cail Bruich. Food and service was excellent, what you'd expect from a Michelin star restaurant. Staff are very knowledgeable about the dishes they serve. We had the chefs tasting menu and was great. Very disappointed by the manner in which a dessert was served, far from Michelin standard or any acceptable standard. This was dealt with at the time with a manager.\"\"\", \"Matt.W\": \"\"\"Came here for my birthday. I had the tasting menu with wines to match. The wines did not pair well with the food at all and were particularly expensive and very small measures (\u00a372 for four very small glasses). You could drive home from this meal without being over the limit. I've eaten in some wonderful Michelin Starred restaurants and some great AA Rosette places. This is neither. The restaurant was cold when I visited and the champagne was the worst I have ever had anywhere. I don't often leave bad reviews but this was a meal for two that cost close to \u00a3500 and wasn't worth it at all. I've spent more in other places but always felt that I got value for money. I wouldn't go back again.\"\"\"} }, {\"name\": \"The Hanoi Bike Shop\", \"street_number\": 8, \"street\": \"Ruthven Ln\", \"city\": \"Glasgow\", \"description\": \"A fresh, casual, canteen-style Vietnamese restaurant.\", \"id\": \"HBS\", \"comments\":{\"Matt.W\": \"\"\"We arrived at the Hanoi Bike Shop with much anticipation, joined by our Vietnamese friend. The decor is superb, beautifully decorated. Ordered the spring rolls -delish, followed by a beef pho, average, and my wife got the red duck curry -excellent. I asked for a Vietnamese iced coffee with condensed milk and got a classic iced coffee -disappointed. Our food arrived quickly and the service was good.\"\"\", \"Mark.E\": \"\"\"Nice cosy interior with an upstairs seating. The shop is easy to find thanks to the bright signs. The decor of the shop is very nicely done. However the food is quite disappointing, from an Asian\u2019s point of view. The pho was very disappointing, the portion was big and all but the taste and correct pho noodles was not satisfactory. They have a lot of side dishes to eat with their drinks, maybe it\u2019s better for it\u2019s drinks instead of a dinner.\"\"\"} }, {\"name\": \"The Gannet\", \"street_number\": 1155, \"street\": \"Argyle Street\", \"city\": \"Glasgow\", \"description\": \"A slice of Brooklyn-esque cool on the Finnieston \u2018strip\u2019.\", \"id\": \"TG\", \"comments\":{\"Mark.E\": \"\"\"Friendly service. A menu that reads better than it delivers for some courses. Disappointing lack of stornoway black pudding on what was a well cooked duck scotch egg. The fillet of beef was tough and served with 2 small rectangles of mushroom and a trickle of well cooked sauce...it could have been a great dish. The salted caramel fondant however was amazing. I'd expect better from a 3 AA rosette restaurant to be honest.\"\"\", \"Jeff.D\": \"\"\"So I was little torn about this review. The service was very good, the ambience was lovely, the drinks selection was excellent and the food was good. The problem lies in the pricing, when you are charging \u00a324 a main the food needs to be better than good; it needs to be excellent. They hit the mark with both the starters and the deserts, but the found the mains a little lacking. All in all if you are going to treat yourself there are better options than the Gannet.\"\"\", \"Andy.P\": \"\"\"Main served 50 minutes after starter and luke warm on a roasting plate(how can that be you ask? Cause they replated it I assume). Dessert took another 45 mins and they didnt bring our coffee as requested. Then they put the balance of our gift voucher onto a new voucher but made the valid until date today! It's probably a superb restaurant, but we had a very poor version of it.\"\"\"} }, {\"name\": \"The Finnieston\", \"street_number\": 1125, \"street\": \"Argyle Street\", \"city\": \"Glasgow\", \"description\": \"Proudly sourced Scottish seafood and gins at a suitably rustic Argyle Street location.\", \"id\": \"FM\", \"comments\":{\"Matt.W\": \"\"\"Expensive for what you get. Carafe of house wine, half litre at \u00a314 is cheaky. Pleasant enough atmosphere but really stuffy responses when it came to trying to get food. Only permitted food at designated tables. No snacks or small plates served anywhere else. Doesn't make sense.\"\"\", \"Jeff.D\": \"\"\"This was the first place I visited when I moved to Glasgow a year ago. It is our favourite place to catch up with friends since then. The food is really good and I'm really missing their bay leaf elderflower gin on these cold October nights. Can't wait for them to reopen so we can visit again. If someone ever asks me to recommend a good gastropub in Glasgow this is the place to go.\"\"\", \"Andy.P\": \"\"\"The worst bar I've ever been to. The rudest staff who cannot even pour a pint. I never complain but it was so bad that I decided to tell the manager who was even ruder and totally unprofessional. Really awful pretentious place. Just wish I could give zero stars!\"\"\"} }, {\"name\": \"Stravaigin\", \"street_number\": 28, \"street\": \"Gibson Street\", \"city\": \"Glasgow\", \"description\": \"Pub grub staples done very well at a hip West End restaurant.\", \"id\": \"ST\", \"comments\":{\"Michael.G\": \"\"\"Great staff, very attentive and polite waitress but the food omg. Thai curry was full of fish sauce, veggie haggis tasted sour( as it was off). Not happy, would not recommend and will not be back.\"\"\", \"Rose.S\": \"\"\"\u00a325 for 2 breakfasts and 2 small coffees, feel a bit ripped off as there was only 1 bacon rasher, 1 egg, potato scone, square sausage, link sausage, black pudding, mushrooms and very small portion of beans, wasn't keen on the sausage and the egg had that horrible undercooked white stuff on it. Fast service and nice pub.\"\"\", \"Danny.M\": \"\"\"Good God in heaven. I didn't think it was possible to describe haggis as juicy, but Stravaigin have done something magical. It was like an umami punch in the tongue and mama liked it. Best plate of haggis I've ever had in my life. I literally made myself keep eating past the point that I was full. Nice staff and cool atmosphere as well, but honestly if they served this haggis behind a dumpster I would come back.\"\"\"} }, {\"name\": \"Patrick Duck Club\", \"street_number\": 27, \"street\": \"Hyndland Street\", \"city\": \"Glasgow\", \"description\": \"A quirky diner proving you can cook duck in A LOT of different ways.\", \"id\": \"PDC\", \"comments\":{\"Thom.O\": \"\"\"First time visit, booking in advance particularly at weekends is advised. The food and service were excellent. I had a starter, main and dessert(shown in pics) and all courses were delicious. Highly recommended!\"\"\", \"Colin\": \"\"\"Burger was tasteless unfortunately, Ndjua sauce tasted like cold tinned tomatoes with no seasoning... Not a good plate of food, real shame as I wanted to like this place, lovely setting and staff friendly, food just not up to scratch at all.\"\"\"} }, {\"name\": \"Number 16\", \"street_number\": 16, \"street\": \"Byres Road\", \"city\": \"Glasgow\", \"description\": \"A Euro-bistro in a Byres Road bolthole.\", \"id\": \"N16\", \"comments\":{\"Rose.S\": \"\"\"Absolutely sensational. Fantastic food, really creative, delicious and not dainty for the quality. Service was excellent too.\"\"\", \"Jeremy.S\": \"\"\"The food was presented nicely and the restaurant was cosy hence the star but was very let down by the flavours which were so bland they resembled hospital food. I booked the table for my boyfriends birthday after seeing all the positive reviews. We were both dissapointed and the waitress didn't bother asking us how we found the food.\"\"\"} }, {\"name\": \"Spanish Butcher\", \"street_number\": 1055, \"street\": \"Sauchiehall Street\", \"city\": \"Glasgow\", \"description\": \"Premium Spanish meat served in New York loft-style interiors.\", \"id\": \"SB\", \"comments\":{\"Mark.E\": \"\"\"It does steaks and it does steaks very well indeed. Staff are very friendly and helpful, and take great care in explaining the complexities of ordering your steak. Lighting is subdued, the atmosphere is relaxed. It is not cheap but you're getting prime cuts of beef your money.\"\"\", \"Jeremy.S\": \"\"\"Absolutely brilliant restaurant, one of my favourites in Glasgow! Great cocktails, the steaks are fantastic, but the pork cheek is next level\"\"\", \"Colin\": \"\"\"Beautiful interior, service outstanding. Let down by food. Ordered burgers. Less than average, very dry, a teaspoon of relish, chips were like frozen skinny fries. Would I eat here again? Definitely not.\"\"\"} }, {\"name\": \"Beat 6\", \"street_number\": 10, \"street\": \"Whitehall Street\", \"city\": \"Glasgow\", \"description\": \"\"\"A new venture from the team behind Six by Nico, which donates 100% of its profits to the \" Beatson Cancer Charity.\"\"\", \"id\": \"B6\", \"comments\":{\"Jeff.D\": \"\"\"Had the vegetarian tasting menu and it was so good. Super friendly staff in a place with a lovely atmosphere. Will definitely go back\"\"\", \"Colin\": \"\"\"Food was delicious and plated well. The only dish I wasn't a fan of was the beef tartare -not the fault of the staff. They apologised and offered me the veggie option which was kind. Wine pairing is highly recommended.\"\"\", \"Matt.W\": \"\"\"Great experience, fantastic staff and lovely atmosphere. The food was just as tasty as when we've tried it at Six by Nico before. Highly recommended.\"\"\", \"Danny.M\": \"\"\"Every plate was a delight. The Tartare was brilliant. Star of the show for me was the Fregola Sarda. Amazing value for such great food. Lovely staff who pace things just right. A great experience\"\"\"} }, {\"name\": \"Glorisa\", \"street_number\": 1321, \"street\": \"Argyle Street123\", \"city\": \"Glasgow\", \"description\": \"For fresh Mediterranean flavours from the chef who brought us Alchemilla.\", \"id\": \"GL\", \"comments\":{\"Thom.O\": \"\"\"Small plates sharing menu. Short list of options, all done very well with interesting interpretations of some of my favourite tapas dishes. Menu changes frequently, which means I'm highly likely to return several times!\"\"\", \"Rose.S\": \"\"\"Just simply one of the best meals we've had in Glasgow for some time. Standout dish was the pork chop...service was spot on too. Man we are really spoilt for choice in the west end now for 10\/10 places.\"\"\", \"Jeremy.S\": \"\"\"The venue and staff are nice but the food simply wasn't for me. Not my style and for me it wasn't tasty.\"\"\"} }, ] user_data=[ {\"username\": \"Mark.E\", \"street_number\": 21, \"street\": \"Beith Street\", \"city\": \"Glasgow\", \"liked_restaurants\":[\"ALC\", \"SB\", \"B6\"], \"rated_restaurants\":{\"ALC\": 4, \"JK\": 2, \"ST\": 3, \"PDC\": 2, \"N16\": 1, \"SB\": 4, \"B6\": 5, \"GL\": 3, \"CB\": 1, \"FM\": 2, \"TG\": 3, \"HBS\": 1}, \"password\": \"Mark123\", \"email\": \"mark@gmail.com\", \"name\": \"Mark\", \"surname\": \"Edwards\", \"owner_status\": False, \"owned_restaurants\":[] }, {\"username\": \"Matt.W\", \"street_number\": 164, \"street\": \"Buchanan St\", \"city\": \"Glasgow\", \"liked_restaurants\":[\"ALC\", \"JK\", \"HBS\"], \"rated_restaurants\":{\"ALC\": 5, \"JK\": 5, \"KC\": 3, \"OnF\": 1, \"SB\": 2, \"B6\": 5, \"GL\": 1, \"CB\": 2, \"FM\": 2, \"TG\": 3, \"HBS\": 4}, \"password\": \"Matt123\", \"email\": \"matt@gmail.com\", \"name\": \"Matthew\", \"surname\": \"Wainwright\", \"owner_status\": False, \"owned_restaurants\":[] }, {\"username\": \"Thom.O\", \"street_number\": 161, \"street\": \"Duke St\", \"city\": \"Glasgow\", \"liked_restaurants\":[\"ALC\", \"PDC\", \"B6\", \"GL\"], \"rated_restaurants\":{\"ALC\": 4, \"JK\": 3, \"BE\": 5, \"ST\": 3, \"PDC\": 4, \"N16\": 2, \"SB\": 1, \"B6\": 4, \"GL\": 5, \"TG\": 2, \"HBS\": 3}, \"password\": \"Thom123\", \"email\": \"thom@gmail.com\", \"name\": \"Thomas\", \"surname\": \"Oldman\", \"owner_status\": False, \"owned_restaurants\":[] }, {\"username\": \"Michael.G\", \"street_number\": 477, \"street\": \"Duke St\", \"city\": \"Glasgow\", \"liked_restaurants\":[\"ALC\"], \"rated_restaurants\":{\"ALC\": 4, \"JK\": 2, \"KC\": 4, \"OnF\": 5, \"BE\": 4, \"ST\": 2, \"PDC\": 3}, \"password\": \"Matt123\", \"email\": \"matt@gmail.com\", \"name\": \"Michael\", \"surname\": \"Gunning\", \"owner_status\": False, \"owned_restaurants\":[] }, {\"username\": \"Andy.P\", \"street_number\": 394, \"street\": \"Great Western Rd\", \"city\": \"Glasgow\", \"liked_restaurants\":[\"FM\"], \"rated_restaurants\":{\"KC\": 3, \"OnF\": 5, \"BE\": 4, \"ST\": 2, \"PDC\": 4, \"CB\": 3, \"FM\": 2, \"TG\": 1, \"HBS\": 2}, \"password\": \"Andy123\", \"email\": \"andy@gmail.com\", \"name\": \"Andy\", \"surname\": \"Peterson\", \"owner_status\": False, \"owned_restaurants\":[] }, {\"username\": \"Rose.S\", \"street_number\": 8, \"street\": \"Cresswell Ln\", \"city\": \"Glasgow\", \"liked_restaurants\":[\"ALC\", \"N16\", \"B6\", \"FM\"], \"rated_restaurants\":{\"ALC\": 4, \"JK\": 4, \"ST\": 3, \"PDC\": 2, \"N16\": 4, \"SB\": 3, \"B6\": 4, \"GL\": 2, \"FM\": 4, \"TG\": 3, \"HBS\": 2}, \"password\": \"Rose123\", \"email\": \"rose@gmail.com\", \"name\": \"Rose\", \"surname\": \"Street\", \"owner_status\": False, \"owned_restaurants\":[] }, {\"username\": \"Jeremy.S\", \"street_number\": 1620, \"street\": \"Great Western Rd\", \"city\": \"Glasgow\", \"liked_restaurants\":[\"SB\"], \"rated_restaurants\":{\"ST\": 2, \"PDC\": 4, \"N16\": 2, \"SB\": 5, \"B6\": 5, \"GL\": 1, \"CB\": 4, \"FM\": 3, \"HBS\": 2}, \"password\": \"Jem123\", \"email\": \"jeremy@gmail.com\", \"name\": \"Jeremy\", \"surname\": \"Stevenson\", \"owner_status\": False, \"owned_restaurants\":[] }, {\"username\": \"Jeff.D\", \"street_number\": 108, \"street\": \"Queen Margaret Dr\", \"city\": \"Glasgow\", \"liked_restaurants\":[\"ALC\", \"JK\", \"B6\", \"GL\", \"FM\"], \"rated_restaurants\":{\"ALC\": 5, \"JK\": 4, \"KC\": 3, \"OnF\": 1, \"ST\": 3, \"PDC\": 2, \"B6\": 5, \"GL\": 4, \"CB\": 3, \"FM\": 4, \"TG\": 3, \"HBS\": 3}, \"password\": \"Jeff123\", \"email\": \"jeff@gmail.com\", \"name\": \"Jeff\", \"surname\": \"Dalton\", \"owner_status\": False, \"owned_restaurants\":[] }, {\"username\": \"Colin\", \"street_number\": 1, \"street\": \"Cathcard Rd\", \"city\": \"Glasgow\", \"liked_restaurants\":[\"PDC\", \"GL\"], \"rated_restaurants\":{\"PDC\": 3, \"N16\": 3, \"SB\": 3, \"B6\": 4, \"GL\": 5}, \"password\": \"Colin123\", \"email\": \"colin@gmail.com\", \"name\": \"Colin\", \"surname\": \"McNair\", \"owner_status\": True, \"owned_restaurants\":[\"ALC\", \"JK\", \"KC\", \"OnF\", \"BE\"] }, {\"username\": \"Nicola.H\", \"street_number\": 530, \"street\": \"Victoria Rd\", \"city\": \"Glasgow\", \"liked_restaurants\":[\"ALC\"], \"rated_restaurants\":{\"ALC\": 4, \"JK\": 3, \"KC\": 3, \"HBS\": 2}, \"password\": \"Nichola123\", \"email\": \"nichola@gmail.com\", \"name\": \"Nicola\", \"surname\": \"Hamill\", \"owner_status\": True, \"owned_restaurants\":[\"ST\", \"PDC\", \"N16\", \"SB\", \"B6\"] }, {\"username\": \"Danny.M\", \"street_number\": 316, \"street\": \"Calder St\", \"city\": \"Glasgow\", \"liked_restaurants\":[\"ST\", \"SB\", \"GL\"], \"rated_restaurants\":{\"ST\": 4, \"PDC\": 3, \"N16\": 3, \"SB\": 4, \"B6\": 5, \"GL\": 5}, \"password\": \"Danny123\", \"email\": \"danny@gmail.com\", \"name\": \"Danny\", \"surname\": \"Macpherson\", \"owner_status\": True, \"owned_restaurants\":[\"GL\", \"CB\", \"FM\", \"TG\", \"HBS\"] }] for restaurant in restaurant_data: add_restaurant(restaurant[\"name\"], restaurant[\"street_number\"], restaurant[\"street\"], restaurant[\"city\"], restaurant[\"description\"], restaurant[\"id\"], restaurant[\"comments\"]) for user in user_data: add_user(user[\"username\"], user[\"street_number\"], user[\"street\"], user[\"city\"], user[\"liked_restaurants\"], user[\"rated_restaurants\"], user[\"password\"], user[\"email\"], user[\"name\"], user[\"surname\"], user[\"owner_status\"], user[\"owned_restaurants\"]) if __name__==\"__main__\": print(\"Starting Rango population script\") populate() for u in user_client.objects.all(): print(f\"Created user{u}\") if u.owner_status: print(f\"User owns:{u.owned_restaurants_list}\") print(\"\\n\") for r in Restaurant.objects.all(): print(f\"Created restaurant{r} with{r.rating}\") print(\"Population finished\") ","sourceWithComments":"import os\n\nos.environ.setdefault('DJANGO_SETTINGS_MODULE', 'RestaurantRater.settings')\ncurrent_dir = os.getcwd()\n\nimport django\nfrom django.core.files import File\n\ndjango.setup()\nfrom RestaurantRaterApp.models import user_client, Restaurant\n\n\ndef add_restaurant(name: str, street_number: int, street: str, city: str, description: str, restaurant_id: str, comments: dict):\n    r = Restaurant.objects.get_or_create(name=name, restaurant_id=restaurant_id, street_number=street_number,\n                                         street=street,\n                                         city=city, description=description, comments=comments)[0]\n    # Adds images from lib to restaurant\n    images = os.listdir(f\"{current_dir}\\\\PopulateData\\\\images\\\\{restaurant_id}\")\n    r.img1.save(f\"{name}\\\\img1.jpg\",\n                File(open(f\"{current_dir}\\\\PopulateData\\\\images\\\\{restaurant_id}\\\\{images[0]}\", \"rb\")))\n    r.img2.save(f\"{name}\\\\img2.jpg\",\n                File(open(f\"{current_dir}\\\\PopulateData\\\\images\\\\{restaurant_id}\\\\{images[1]}\", \"rb\")))\n    r.img3.save(f\"{name}\\\\img3.jpg\",\n                File(open(f\"{current_dir}\\\\PopulateData\\\\images\\\\{restaurant_id}\\\\{images[2]}\", \"rb\")))\n    r.save()\n    return r\n\n\ndef add_user(username: str, street_number: int, street: str, city: str, liked_restaurants: list, rated_restaurants: dict, password: str, email: str, name: str,\n             surname: str,\n             owner_status=False, owned_restaurants=[]):\n    u = user_client.objects.get_or_create(username=username, street_number=street_number, street=street, city=city,\n                                          rated_restaurants=rated_restaurants, password=password, email=email,\n                                          name=name, surname=surname, owner_status=owner_status)[0]\n    u.save()\n\n    # Adds ratings\n    for restaurant in rated_restaurants.keys():\n        rates(name, restaurant)\n\n    # Adds liked restaurants\n    for restaurant in liked_restaurants:\n        likes(name, restaurant)\n\n    # Adds Owner's Restaurants\n    if owner_status:\n        for restaurant in owned_restaurants:\n            owns(name, restaurant)\n\n    return u\n\n\ndef rates(user: str, restaurant: str):\n    # Creates link between user and restaurant\n    user_obj = user_client.objects.get(name=user)\n    restaurant_obj = Restaurant.objects.get(restaurant_id=restaurant)\n    user_obj.rates.add(restaurant_obj)\n    # Appends rating to ratings list which overall rating calculated from\n    restaurant_obj.ratings.append(user_obj.rated_restaurants[restaurant_obj.restaurant_id])\n    restaurant_obj.save()\n\n\ndef owns(user: str, restaurant: str):\n    # Creates link between user and restaurant\n    user_obj = user_client.objects.get(name=user)\n    restaurant_obj = Restaurant.objects.get(restaurant_id=restaurant)\n    user_obj.owned_restaurants.add(restaurant_obj)\n\n\ndef likes(user: str, restaurant: str):\n    # Creates link between user and restaurant\n    user_obj = user_client.objects.get(name=user)\n    restaurant_obj = Restaurant.objects.get(restaurant_id=restaurant)\n    user_obj.liked_restaurants.add(restaurant_obj)\n\n\ndef populate():\n    # Restaurant data, list of dictionaries\n    restaurant_data = [\n        # Alchemilla - 1\n        {\"name\": \"Alchemilla\",\n         \"street_number\": 1126,\n         \"street\": \"Argyle Street\",\n         \"city\": \"Glasgow\",\n         \"description\": \"Seasonal Mediterranean plates and natural wine.\",\n         \"id\": \"ALC\",\n         \"comments\": {\n             \"Mark.E\": \"\"\"The restaurant is a cute little intimate location in the interesting area of \n             finnieston. The lighting, ambiance and staff were great and serve the restaurant \n             well. \"\"\",\n             \"Matt.W\": \"\"\"Out-of-the-ordinary small plates. Tasty food and friendly service in a hip \n            setting. Plenty of dishes to choose from. A couple of items on the menu may seem shocking at \n            first. You will be rewarded for being adventurous. \"\"\",\n             \"Danny.M\": \"\"\"Service is wonderful, and the ingredients were fresh and nicely presented, and there is a \n             lot of attention going into the dishes, but it's just not my cup of tea as I found the flavours really \n             bland. Portions are also small, so keep that in mind. I wouldn't come back for the food, but it's worth \n             checking out for yourself \"\"\"\n\n         }\n         },\n        # Julie's Kopitiam - 2\n        {\"name\": \"Julie's Kopitiam\",\n         \"street_number\": 556,\n         \"street\": \"Dumbarton Road\",\n         \"city\": \"Glasgow\",\n         \"description\": \"Comfort food done right\",\n         \"id\": \"JK\",\n         \"comments\": {\n             \"Thom.O\": \"\"\"I've enjoyed eating here in the past but \u00a340 for rice, an egg, a chicken leg,\n            some cucumber and 3 chickpea fritters is excessive. There is just no attempt to compete at all with the\n            great curry houses et al of Glasgow.\"\"\",\n             \"Andy.P\": \"\"\"Fantastic, food was authentic in flavour. The small restaurant has a great vibe\n            and the staff were friendly. Good prices. What a gem of a place.\"\"\",\n             \"Danny.M\": \"\"\"Have had some good meals from here but also some pretty disappointing ones. Takeaway this\n            evening was overpriced, small in proportion and a let-down across the board. You would hope for more\n            consistency. \"\"\"\n         }\n         },\n        # Kimchi Cult - 3\n        {\"name\": \"Kimchi Cult\",\n         \"street_number\": 14,\n         \"street\": \"Chancellor St\",\n         \"city\": \"Glasgow\",\n         \"description\": \"Korean-style fast food in Glasgow\u2019s West End.\",\n         \"id\": \"KC\",\n         \"comments\": {\"Nicola.H\": \"\"\"My wife and I tried this tonight after hearing good things and got the Korean \n         Chicken burger, bibimbap and tofu. The food was generally tasty their crispy chicken and sauce was good!\"\"\",\n                      \"Michael.G\": \"\"\"Working in Glasgow, went 3 times in a week. What more can I say. Outstanding\"\"\",\n                      \"Andy.P\": \"\"\"Portion sizes are much smaller now - it is a depressing story for many such \n                        places. Two months ago, their portions were generous and now they are absolutely the \n                        opposite- I won\u2019t be back \"\"\"\n                      }\n         },\n        # Ox and Finch - 4\n        {\"name\": \"Ox and Finch\",\n         \"street_number\": 920,\n         \"street\": \"Sauchiehall Street\",\n         \"city\": \"Glasgow\",\n         \"description\": \"The small plates trend is done very well at this slick Sauchiehall Street restaurant.\",\n         \"id\": \"OnF\",\n         \"comments\": {\"Michael.G\": \"\"\"The food served at Ox and Finch, Glasgow is immensely delicious, of the right \n         portion and reasonably priced. You must not give amiss to this place. The service is just superb and there \n         is a variety of meat, vegetables to choose from. You will surely enjoy your meal as I did.\"\"\",\n                      \"Andy.P\": \"\"\"I just love this place and visit on every occasion I'm in Glasgow. The food and \n                      wines are 1st class foods done extremely well. I've been going here for 6+years and I have \n                      never once felt disappointed. Staff and venue are great,will help you with any diet query,\n                      same goes for the wines for matching foods.\"\"\"}\n         },\n        # Bilson Eleven - 5\n        {\"name\": \"Bilson Eleven\",\n         \"street_number\": 10,\n         \"street\": \"Annfield Place\",\n         \"city\": \"Glasgow\",\n         \"description\": \"A five- or even eight-course fine-dining odyssey.\",\n         \"id\": \"BE\",\n         \"comments\": {\"Thom.O\": \"\"\"What an unbelievable night that was last night at this restaurant. The food, \n         the service and the wine was absolutely sublime. Just an absolutely unbelievable experience and I certainly \n         would recommend this place to anyone who loves their wine and their fine dining experiences.\"\"\",\n\n                      \"Michael.G\": \"\"\"Our favourite restaurant in Glasgow . Thank you Nick for a absolute creative \n                      menu visually , sensory & fabulously tasting. Mark,  your front of house was all that we come \n                      to expect, informative, interesting and totally  engrossed in your knowledge of the wines & the \n                      menu, thanks gents\"\"\"}\n         },\n        # Cail Bruich - 6\n        {\"name\": \"Cail Bruich\",\n         \"street_number\": 725,\n         \"street\": \"great Western\",\n         \"city\": \"Glasgow\",\n         \"description\": \"Tr\u00e8s bon Franco-Scottish cooking.\",\n         \"id\": \"CB\",\n         \"comments\": {\"Jeremy.S\": \"\"\"This was our first visit to Cail Bruich. Food and service was excellent, \n         what you'd expect from a Michelin star restaurant. Staff are very knowledgeable about the dishes they serve. \n         We had the chefs tasting menu and was great. Very disappointed by the manner in which a dessert was served, \n         far from Michelin standard or any acceptable standard. This was dealt with at the time with a manager.\"\"\",\n\n                      \"Matt.W\": \"\"\"Came here for my birthday. I had the tasting menu with wines to match. The wines \n                      did not pair well with the food at all and were particularly expensive and very small measures \n                      (\u00a372 for four very small glasses). You could drive home from this meal without being over the \n                      limit. I've eaten in some wonderful Michelin Starred restaurants and some great AA Rosette \n                      places. This is neither. The restaurant was cold when I visited and the champagne was the worst \n                      I have ever had anywhere. I don't often leave bad reviews but this was a meal for two that cost \n                      close to \u00a3500 and wasn't worth it at all. I've spent more in other places but always felt that \n                      I got value for money. I wouldn't go back again.\"\"\"}\n         },\n        # Hanoi Bike Shop -7\n        {\"name\": \"The Hanoi Bike Shop\",\n         \"street_number\": 8,\n         \"street\": \"Ruthven Ln\",\n         \"city\": \"Glasgow\",\n         \"description\": \"A fresh, casual, canteen-style Vietnamese restaurant.\",\n         \"id\": \"HBS\",\n         \"comments\": {\"Matt.W\": \"\"\"We arrived at the Hanoi Bike Shop with much anticipation, joined by our Vietnamese \n         friend. The decor is superb, beautifully decorated. Ordered the spring rolls - delish, followed by a beef \n         pho, average, and my wife got the red duck curry - excellent. I asked for a Vietnamese iced coffee with \n         condensed milk and got a classic iced coffee - disappointed. Our food arrived quickly and the service was \n         good.\"\"\",\n                      \"Mark.E\": \"\"\"Nice cosy interior with an upstairs seating. The shop is easy to find thanks to \n                      the bright signs. The decor of the shop is very nicely done. However the food is quite \n                      disappointing, from an Asian\u2019s point of view. The pho was very disappointing, the portion was \n                      big and all but the taste and correct pho noodles was not satisfactory. They have a lot of side \n                      dishes to eat with their drinks, maybe it\u2019s better for it\u2019s drinks instead of a dinner.\"\"\"}\n         },\n        # The Gannet - 8\n        {\"name\": \"The Gannet\",\n         \"street_number\": 1155,\n         \"street\": \"Argyle Street\",\n         \"city\": \"Glasgow\",\n         \"description\": \"A slice of Brooklyn-esque cool on the Finnieston \u2018strip\u2019.\",\n         \"id\": \"TG\",\n         \"comments\": {\"Mark.E\": \"\"\"Friendly service. A menu that reads better than it delivers for some courses. \n         Disappointing lack of stornoway black pudding on what was a well cooked duck scotch egg. The fillet of beef \n         was tough and served with 2 small rectangles of mushroom and a trickle of well cooked sauce...it could have \n         been a great dish. The salted caramel fondant however was amazing.  I'd expect better from a 3 AA rosette \n         restaurant to be honest.\"\"\",\n\n                      \"Jeff.D\": \"\"\"So I was little torn about this review. The service was very good, the ambience \n                      was lovely, the drinks selection was excellent and the food was good. The problem lies in the \n                      pricing, when you are charging \u00a324 a main the food needs to be better than good; it needs to be \n                      excellent. They hit the mark with both the starters and the deserts, but the found the mains a \n                      little lacking. All in all if you are going to treat yourself there are better options than the \n                      Gannet.\"\"\",\n\n                      \"Andy.P\": \"\"\"Main served 50 minutes after starter and luke warm on a roasting plate (how can \n                      that be you ask? Cause they replated it I assume).  Dessert took another 45 mins and they didnt \n                      bring our coffee as requested. Then they put the balance of our gift voucher onto a new voucher \n                      but made the valid until date today! It's probably a superb restaurant, but we had a very poor \n                      version of it.\"\"\"}\n         },\n        # The Finnieston - 9\n        {\"name\": \"The Finnieston\",\n         \"street_number\": 1125,\n         \"street\": \"Argyle Street\",\n         \"city\": \"Glasgow\",\n         \"description\": \"Proudly sourced Scottish seafood and gins at a suitably rustic Argyle Street location.\",\n         \"id\": \"FM\",\n         \"comments\": {\"Matt.W\": \"\"\"Expensive for what you get. Carafe of house wine, half litre at \u00a314 is cheaky. \n         Pleasant enough atmosphere but really stuffy responses when it came to trying to get food. Only permitted \n         food at designated tables. No snacks or small plates served anywhere else. Doesn't make sense.\"\"\",\n\n                      \"Jeff.D\": \"\"\"This was the first place I visited when I moved to Glasgow a year ago. It is our \n                      favourite place to catch up with friends since then. The food is really good and I'm really \n                      missing their bay leaf elderflower gin on these cold October nights. Can't wait for them to \n                      reopen so we can visit again. If someone ever asks me to recommend a good gastropub  in Glasgow \n                      this is the place to go.\"\"\",\n\n                      \"Andy.P\": \"\"\"The worst bar I've ever been to. The rudest staff who cannot even pour a pint. I \n                      never complain but it was so bad that I decided to tell the manager who was even ruder and \n                      totally unprofessional. Really awful pretentious place. Just wish I could give zero stars!\"\"\"}\n         },\n        # Stravaigin - 10\n        {\"name\": \"Stravaigin\",\n         \"street_number\": 28,\n         \"street\": \"Gibson Street\",\n         \"city\": \"Glasgow\",\n         \"description\": \"Pub grub staples done very well at a hip West End restaurant.\",\n         \"id\": \"ST\",\n         \"comments\": {\"Michael.G\": \"\"\"Great staff, very attentive and polite waitress but the food omg. Thai curry \n         was full of fish sauce, veggie haggis tasted sour ( as it was off ). Not happy, would not recommend and will \n         not be back.\"\"\",\n\n                      \"Rose.S\": \"\"\"\u00a325 for 2 breakfasts and 2 small coffees, feel a bit ripped off as there was only \n                      1 bacon rasher, 1 egg, potato scone, square sausage, link sausage, black pudding, mushrooms and \n                      very small portion of beans, wasn't keen on the sausage and the egg had that horrible \n                      undercooked white stuff on it. Fast service and nice pub.\"\"\",\n\n                      \"Danny.M\": \"\"\"Good God in heaven. I didn't think it was possible to describe haggis as juicy, \n                      but Stravaigin have done something magical. It was like an umami punch in the tongue and mama \n                      liked it. Best plate of haggis I've ever had in my life. I literally made myself keep eating \n                      past the point that I was full. Nice staff and cool atmosphere as well, but honestly if they \n                      served this haggis behind a dumpster I would come back.\"\"\"}\n         },\n        # The Patric Duck Club - 11\n        {\"name\": \"Patrick Duck Club\",\n         \"street_number\": 27,\n         \"street\": \"Hyndland Street\",\n         \"city\": \"Glasgow\",\n         \"description\": \"A quirky diner proving you can cook duck in A LOT of different ways.\",\n         \"id\": \"PDC\",\n         \"comments\": {\"Thom.O\": \"\"\"First time visit, booking in advance particularly at weekends is advised. The food \n         and service were excellent. I had a starter, main and dessert (shown in pics) and all courses were delicious.\n         Highly recommended!\"\"\",\n                      \"Colin\": \"\"\"Burger was tasteless unfortunately, Ndjua sauce tasted like cold tinned tomatoes \n                      with no seasoning... Not a good plate of food, real shame as I wanted to like this place, \n                      lovely setting and staff friendly, food just not up to scratch at all.\"\"\"}\n         },\n        # Number 16 - 12\n        {\"name\": \"Number 16\",\n         \"street_number\": 16,\n         \"street\": \"Byres Road\",\n         \"city\": \"Glasgow\",\n         \"description\": \"A Euro-bistro in a Byres Road bolthole.\",\n         \"id\": \"N16\",\n         \"comments\": {\"Rose.S\": \"\"\"Absolutely sensational. Fantastic food, really creative, delicious and not dainty \n         for the quality. Service was excellent too.\"\"\",\n                      \"Jeremy.S\": \"\"\"The food was presented nicely and the restaurant was cosy hence the star but was \n                      very let down by the flavours which were so bland they resembled hospital food. I booked the table\n                       for my boyfriends birthday after seeing all the positive reviews. We were both dissapointed and \n                       the waitress didn't bother asking us how we found the food.\"\"\"}\n         },\n        # Spanish Butcher - 13\n        {\"name\": \"Spanish Butcher\",\n         \"street_number\": 1055,\n         \"street\": \"Sauchiehall Street\",\n         \"city\": \"Glasgow\",\n         \"description\": \"Premium Spanish meat served in New York loft-style interiors.\",\n         \"id\": \"SB\",\n         \"comments\": {\"Mark.E\": \"\"\"It does steaks and it does steaks very well indeed. Staff are very friendly and \n         helpful, and take great care in explaining the complexities of ordering your steak. Lighting is subdued, \n         the atmosphere is relaxed. It is not cheap but you're getting prime cuts of beef your money.\"\"\",\n                      \"Jeremy.S\": \"\"\"Absolutely brilliant restaurant, one of my favourites in Glasgow! Great \n                      cocktails, the steaks are fantastic, but the pork cheek is next level\"\"\",\n                      \"Colin\": \"\"\"Beautiful interior, service outstanding. Let down by food. Ordered burgers. Less \n                      than average, very dry, a teaspoon of relish, chips were like frozen skinny fries. Would I eat \n                      here again? Definitely not.\"\"\"}\n         },\n        # Beat 6 - 14\n        {\"name\": \"Beat 6\",\n         \"street_number\": 10,\n         \"street\": \"Whitehall Street\",\n         \"city\": \"Glasgow\",\n         \"description\": \"\"\"A new venture from the team behind Six by Nico, which donates 100% of its profits to the \"\n                        Beatson Cancer Charity.\"\"\",\n         \"id\": \"B6\",\n         \"comments\": {\"Jeff.D\": \"\"\"Had the vegetarian tasting menu and it was so good. Super friendly staff in a \n         place with a lovely atmosphere.  Will definitely go back\"\"\",\n\n                      \"Colin\": \"\"\"Food was delicious and plated well. The only dish I wasn't a fan of was the beef \n                      tartare - not the fault of the staff. They apologised and offered me the veggie option which \n                      was kind. Wine pairing is highly recommended.\"\"\",\n                      \"Matt.W\": \"\"\"Great experience, fantastic staff and lovely atmosphere. The food was just as \n                      tasty as when we've tried it at Six by Nico before. Highly recommended.\"\"\",\n\n                      \"Danny.M\": \"\"\"Every plate was a delight. The Tartare was brilliant. Star of the show for me was \n                      the Fregola Sarda. Amazing value for such great food. Lovely staff who pace things just right. \n                      A great experience\"\"\"}\n         },\n        # Glorisa -15\n        {\"name\": \"Glorisa\",\n         \"street_number\": 1321,\n         \"street\": \"Argyle Street123\",\n         \"city\": \"Glasgow\",\n         \"description\": \"For fresh Mediterranean flavours from the chef who brought us Alchemilla.\",\n         \"id\": \"GL\",\n         \"comments\": {\"Thom.O\": \"\"\"Small plates sharing menu. Short list of options, all done very well with \n         interesting interpretations of some of my favourite tapas dishes. Menu changes frequently, which means I'm \n         highly likely to return several times!\"\"\",\n                      \"Rose.S\": \"\"\"Just simply one of the best meals we've had in Glasgow for some time. Standout \n                      dish was the pork chop...service was spot on too. Man we are really spoilt for choice in the \n                      west end now for 10\/10 places.\"\"\",\n                      \"Jeremy.S\": \"\"\"The venue and staff are nice but the food simply wasn't for me. Not my style and \n                      for me it wasn't tasty.\"\"\"}\n         },\n\n    ]\n    # User data, list of dictionaries\n    user_data = [\n        # Mark Edwards\n        {\"username\": \"Mark.E\",\n         \"street_number\": 21,\n         \"street\": \"Beith Street\",\n         \"city\": \"Glasgow\",\n         \"liked_restaurants\": [\"ALC\", \"SB\", \"B6\"],\n         \"rated_restaurants\": {\"ALC\": 4, \"JK\": 2, \"ST\": 3, \"PDC\": 2, \"N16\": 1, \"SB\": 4, \"B6\": 5, \"GL\": 3, \"CB\": 1,\n                               \"FM\": 2, \"TG\": 3, \"HBS\": 1},\n         \"password\": \"Mark123\",\n         \"email\": \"mark@gmail.com\",\n         \"name\": \"Mark\",\n         \"surname\": \"Edwards\",\n         \"owner_status\": False,\n         \"owned_restaurants\": []\n         },\n        # Matthew Wainwright\n        {\"username\": \"Matt.W\",\n         \"street_number\": 164,\n         \"street\": \"Buchanan St\",\n         \"city\": \"Glasgow\",\n         \"liked_restaurants\": [\"ALC\", \"JK\", \"HBS\"],\n         \"rated_restaurants\": {\"ALC\": 5, \"JK\": 5, \"KC\": 3, \"OnF\": 1, \"SB\": 2, \"B6\": 5, \"GL\": 1, \"CB\": 2, \"FM\": 2,\n                               \"TG\": 3, \"HBS\": 4},\n         \"password\": \"Matt123\",\n         \"email\": \"matt@gmail.com\",\n         \"name\": \"Matthew\",\n         \"surname\": \"Wainwright\",\n         \"owner_status\": False,\n         \"owned_restaurants\": []\n         },\n        # Thomas Oldman\n        {\"username\": \"Thom.O\",\n         \"street_number\": 161,\n         \"street\": \"Duke St\",\n         \"city\": \"Glasgow\",\n         \"liked_restaurants\": [\"ALC\", \"PDC\", \"B6\", \"GL\"],\n         \"rated_restaurants\": {\"ALC\": 4, \"JK\": 3, \"BE\": 5, \"ST\": 3, \"PDC\": 4, \"N16\": 2, \"SB\": 1, \"B6\": 4, \"GL\": 5,\n                               \"TG\": 2, \"HBS\": 3},\n         \"password\": \"Thom123\",\n         \"email\": \"thom@gmail.com\",\n         \"name\": \"Thomas\",\n         \"surname\": \"Oldman\",\n         \"owner_status\": False,\n         \"owned_restaurants\": []\n         },\n        # Michael Gunning\n        {\"username\": \"Michael.G\",\n         \"street_number\": 477,\n         \"street\": \"Duke St\",\n         \"city\": \"Glasgow\",\n         \"liked_restaurants\": [\"ALC\"],\n         \"rated_restaurants\": {\"ALC\": 4, \"JK\": 2, \"KC\": 4, \"OnF\": 5, \"BE\": 4, \"ST\": 2, \"PDC\": 3},\n         \"password\": \"Matt123\",\n         \"email\": \"matt@gmail.com\",\n         \"name\": \"Michael\",\n         \"surname\": \"Gunning\",\n         \"owner_status\": False,\n         \"owned_restaurants\": []\n         },\n        # Andy Peterson\n        {\"username\": \"Andy.P\",\n         \"street_number\": 394,\n         \"street\": \"Great Western Rd\",\n         \"city\": \"Glasgow\",\n         \"liked_restaurants\": [\"FM\"],\n         \"rated_restaurants\": {\"KC\": 3, \"OnF\": 5, \"BE\": 4, \"ST\": 2, \"PDC\": 4, \"CB\": 3, \"FM\": 2, \"TG\": 1, \"HBS\": 2},\n         \"password\": \"Andy123\",\n         \"email\": \"andy@gmail.com\",\n         \"name\": \"Andy\",\n         \"surname\": \"Peterson\",\n         \"owner_status\": False,\n         \"owned_restaurants\": []\n         },\n        # Rose Street\n        {\"username\": \"Rose.S\",\n         \"street_number\": 8,\n         \"street\": \"Cresswell Ln\",\n         \"city\": \"Glasgow\",\n         \"liked_restaurants\": [\"ALC\", \"N16\", \"B6\", \"FM\"],\n         \"rated_restaurants\": {\"ALC\": 4, \"JK\": 4, \"ST\": 3, \"PDC\": 2, \"N16\": 4, \"SB\": 3, \"B6\": 4, \"GL\": 2, \"FM\": 4,\n                               \"TG\": 3, \"HBS\": 2},\n         \"password\": \"Rose123\",\n         \"email\": \"rose@gmail.com\",\n         \"name\": \"Rose\",\n         \"surname\": \"Street\",\n         \"owner_status\": False,\n         \"owned_restaurants\": []\n         },\n        # Jeremy Stevenson\n        {\"username\": \"Jeremy.S\",\n         \"street_number\": 1620,\n         \"street\": \"Great Western Rd\",\n         \"city\": \"Glasgow\",\n         \"liked_restaurants\": [\"SB\"],\n         \"rated_restaurants\": {\"ST\": 2, \"PDC\": 4, \"N16\": 2, \"SB\": 5, \"B6\": 5, \"GL\": 1, \"CB\": 4, \"FM\": 3, \"HBS\": 2},\n         \"password\": \"Jem123\",\n         \"email\": \"jeremy@gmail.com\",\n         \"name\": \"Jeremy\",\n         \"surname\": \"Stevenson\",\n         \"owner_status\": False,\n         \"owned_restaurants\": []\n         },\n        # Jeff Dalton\n        {\"username\": \"Jeff.D\",\n         \"street_number\": 108,\n         \"street\": \"Queen Margaret Dr\",\n         \"city\": \"Glasgow\",\n         \"liked_restaurants\": [\"ALC\", \"JK\", \"B6\", \"GL\", \"FM\"],\n         \"rated_restaurants\": {\"ALC\": 5, \"JK\": 4, \"KC\": 3, \"OnF\": 1, \"ST\": 3, \"PDC\": 2, \"B6\": 5, \"GL\": 4, \"CB\": 3,\n                               \"FM\": 4, \"TG\": 3, \"HBS\": 3},\n         \"password\": \"Jeff123\",\n         \"email\": \"jeff@gmail.com\",\n         \"name\": \"Jeff\",\n         \"surname\": \"Dalton\",\n         \"owner_status\": False,\n         \"owned_restaurants\": []\n         },\n        # Colin McNair - Owner\n        {\"username\": \"Colin\",\n         \"street_number\": 1,\n         \"street\": \"Cathcard Rd\",\n         \"city\": \"Glasgow\",\n         \"liked_restaurants\": [\"PDC\", \"GL\"],\n         \"rated_restaurants\": {\"PDC\": 3, \"N16\": 3, \"SB\": 3, \"B6\": 4, \"GL\": 5},\n         \"password\": \"Colin123\",\n         \"email\": \"colin@gmail.com\",\n         \"name\": \"Colin\",\n         \"surname\": \"McNair\",\n         \"owner_status\": True,\n         \"owned_restaurants\": [\"ALC\", \"JK\", \"KC\", \"OnF\", \"BE\"]\n         },\n        # Nicola Hamill - Owner\n        {\"username\": \"Nicola.H\",\n         \"street_number\": 530,\n         \"street\": \"Victoria Rd\",\n         \"city\": \"Glasgow\",\n         \"liked_restaurants\": [\"ALC\"],\n         \"rated_restaurants\": {\"ALC\": 4, \"JK\": 3, \"KC\": 3, \"HBS\": 2},\n         \"password\": \"Nichola123\",\n         \"email\": \"nichola@gmail.com\",\n         \"name\": \"Nicola\",\n         \"surname\": \"Hamill\",\n         \"owner_status\": True,\n         \"owned_restaurants\": [\"ST\", \"PDC\", \"N16\", \"SB\", \"B6\"]\n         },\n        # Danny Macpherson - Owner\n        {\"username\": \"Danny.M\",\n         \"street_number\": 316,\n         \"street\": \"Calder St\",\n         \"city\": \"Glasgow\",\n         \"liked_restaurants\": [\"ST\", \"SB\", \"GL\"],\n         \"rated_restaurants\": {\"ST\": 4, \"PDC\": 3, \"N16\": 3, \"SB\": 4, \"B6\": 5, \"GL\": 5},\n         \"password\": \"Danny123\",\n         \"email\": \"danny@gmail.com\",\n         \"name\": \"Danny\",\n         \"surname\": \"Macpherson\",\n         \"owner_status\": True,\n         \"owned_restaurants\": [\"GL\", \"CB\", \"FM\", \"TG\", \"HBS\"]\n\n         }]\n\n    for restaurant in restaurant_data:\n        add_restaurant(restaurant[\"name\"], restaurant[\"street_number\"], restaurant[\"street\"], restaurant[\"city\"],\n                       restaurant[\"description\"], restaurant[\"id\"], restaurant[\"comments\"])\n    for user in user_data:\n        add_user(user[\"username\"], user[\"street_number\"], user[\"street\"], user[\"city\"], user[\"liked_restaurants\"],\n                 user[\"rated_restaurants\"],\n                 user[\"password\"], user[\"email\"], user[\"name\"], user[\"surname\"], user[\"owner_status\"],\n                 user[\"owned_restaurants\"])\n\n\nif __name__ == \"__main__\":\n    print(\"Starting Rango population script\")\n    populate()\n    for u in user_client.objects.all():\n        print(f\"Created user {u}\")\n        if u.owner_status:\n            print(f\"User owns: {u.owned_restaurants_list}\")\n    print(\"\\n\")\n    for r in Restaurant.objects.all():\n        print(f\"Created restaurant {r} with {r.rating}\")\n\n    print(\"Population finished\")\n"}},"msg":"Added Clear() to prevent flooding of media folder."}},"https:\/\/github.com\/City-of-Helsinki\/kaavapino":{"914566d3060efa89c99cab55e79eb71a4f302278":{"url":"https:\/\/api.github.com\/repos\/City-of-Helsinki\/kaavapino\/commits\/914566d3060efa89c99cab55e79eb71a4f302278","html_url":"https:\/\/github.com\/City-of-Helsinki\/kaavapino\/commit\/914566d3060efa89c99cab55e79eb71a4f302278","message":"Prevent flooding task queue when importing deadlines","sha":"914566d3060efa89c99cab55e79eb71a4f302278","keyword":"flooding prevent","diff":"diff --git a\/projects\/signals\/handlers.py b\/projects\/signals\/handlers.py\nindex 45b80220..8fba59dc 100644\n--- a\/projects\/signals\/handlers.py\n+++ b\/projects\/signals\/handlers.py\n@@ -10,6 +10,7 @@\n )\n from django.dispatch import receiver\n from django_q.tasks import async_task\n+from django_q.models import OrmQ\n \n from projects.models import (\n     ProjectAttributeFile,\n@@ -81,4 +82,11 @@ def save_attribute_data_subtype(sender, instance, *args, **kwargs):\n \n @receiver([post_save, post_delete, m2m_changed], sender=Deadline)\n def refresh_project_schedule_cache(sender, instance, *args, **kwargs):\n-    async_task(refresh_project_schedule_cache_task)\n+    for task in OrmQ.objects.all():\n+        if task.name() == \"refresh_project_schedule_cache\":\n+            task.delete()\n+\n+    async_task(\n+        refresh_project_schedule_cache_task,\n+        task_name=\"refresh_project_schedule_cache\",\n+    )\n","files":{"\/projects\/signals\/handlers.py":{"changes":[{"diff":"\n \n @receiver([post_save, post_delete, m2m_changed], sender=Deadline)\n def refresh_project_schedule_cache(sender, instance, *args, **kwargs):\n-    async_task(refresh_project_schedule_cache_task)\n+    for task in OrmQ.objects.all():\n+        if task.name() == \"refresh_project_schedule_cache\":\n+            task.delete()\n+\n+    async_task(\n+        refresh_project_schedule_cache_task,\n+        task_name=\"refresh_project_schedule_cache\",\n+    )\n","add":8,"remove":1,"filename":"\/projects\/signals\/handlers.py","badparts":["    async_task(refresh_project_schedule_cache_task)"],"goodparts":["    for task in OrmQ.objects.all():","        if task.name() == \"refresh_project_schedule_cache\":","            task.delete()","    async_task(","        refresh_project_schedule_cache_task,","        task_name=\"refresh_project_schedule_cache\",","    )"]}],"source":"\nimport os from django.core.cache import cache from django.db.models.signals import( pre_delete, pre_save, post_save, post_delete, m2m_changed, ) from django.dispatch import receiver from django_q.tasks import async_task from projects.models import( ProjectAttributeFile, Attribute, DataRetentionPlan, AttributeValueChoice, FieldSetAttribute, ProjectType, ProjectSubtype, ProjectFloorAreaSection, ProjectFloorAreaSectionAttribute, ProjectFloorAreaSectionAttributeMatrixStructure, ProjectFloorAreaSectionAttributeMatrixCell, ProjectPhase, ProjectPhaseSection, ProjectPhaseSectionAttribute, ProjectPhaseFieldSetAttributeIndex, PhaseAttributeMatrixStructure, PhaseAttributeMatrixCell, ProjectPhaseDeadlineSection, ProjectPhaseDeadlineSectionAttribute, Deadline, Project, ) from projects.tasks import refresh_project_schedule_cache \\ as refresh_project_schedule_cache_task @receiver([post_save, post_delete, m2m_changed], sender=Attribute) @receiver([post_save, post_delete, m2m_changed], sender=DataRetentionPlan) @receiver([post_save, post_delete, m2m_changed], sender=AttributeValueChoice) @receiver([post_save, post_delete, m2m_changed], sender=FieldSetAttribute) @receiver([post_save, post_delete, m2m_changed], sender=ProjectType) @receiver([post_save, post_delete, m2m_changed], sender=ProjectSubtype) @receiver([post_save, post_delete, m2m_changed], sender=ProjectFloorAreaSection) @receiver([post_save, post_delete, m2m_changed], sender=ProjectFloorAreaSectionAttribute) @receiver([post_save, post_delete, m2m_changed], sender=ProjectFloorAreaSectionAttributeMatrixStructure) @receiver([post_save, post_delete, m2m_changed], sender=ProjectFloorAreaSectionAttributeMatrixCell) @receiver([post_save, post_delete, m2m_changed], sender=ProjectPhase) @receiver([post_save, post_delete, m2m_changed], sender=ProjectPhaseSection) @receiver([post_save, post_delete, m2m_changed], sender=ProjectPhaseSectionAttribute) @receiver([post_save, post_delete, m2m_changed], sender=ProjectPhaseFieldSetAttributeIndex) @receiver([post_save, post_delete, m2m_changed], sender=PhaseAttributeMatrixStructure) @receiver([post_save, post_delete, m2m_changed], sender=PhaseAttributeMatrixCell) @receiver([post_save, post_delete, m2m_changed], sender=ProjectPhaseDeadlineSection) @receiver([post_save, post_delete, m2m_changed], sender=ProjectPhaseDeadlineSectionAttribute) @receiver([post_save, post_delete, m2m_changed], sender=Deadline) def delete_cached_sections(*args, **kwargs): cache.delete(\"serialized_phase_sections\") cache.delete(\"serialized_deadline_sections\") @receiver([pre_save], sender=Project) def save_attribute_data_subtype(sender, instance, *args, **kwargs): instance.attribute_data[\"kaavaprosessin_kokoluokka\"]=\\ instance.phase.project_subtype.name instance.attribute_data[\"kaavan_vaihe\"]=\\ instance.phase.prefixed_name for attr in Attribute.objects.filter(static_property__isnull=False): value=getattr(instance, attr.static_property) if attr.value_type==Attribute.TYPE_USER: value=value.uuid instance.attribute_data[attr.identifier]=value @receiver([post_save, post_delete, m2m_changed], sender=Deadline) def refresh_project_schedule_cache(sender, instance, *args, **kwargs): async_task(refresh_project_schedule_cache_task) ","sourceWithComments":"import os\n\nfrom django.core.cache import cache\nfrom django.db.models.signals import (\n    pre_delete,\n    pre_save,\n    post_save,\n    post_delete,\n    m2m_changed,\n)\nfrom django.dispatch import receiver\nfrom django_q.tasks import async_task\n\nfrom projects.models import (\n    ProjectAttributeFile,\n    Attribute,\n    DataRetentionPlan,\n    AttributeValueChoice,\n    FieldSetAttribute,\n    ProjectType,\n    ProjectSubtype,\n    ProjectFloorAreaSection,\n    ProjectFloorAreaSectionAttribute,\n    ProjectFloorAreaSectionAttributeMatrixStructure,\n    ProjectFloorAreaSectionAttributeMatrixCell,\n    ProjectPhase,\n    ProjectPhaseSection,\n    ProjectPhaseSectionAttribute,\n    ProjectPhaseFieldSetAttributeIndex,\n    PhaseAttributeMatrixStructure,\n    PhaseAttributeMatrixCell,\n    ProjectPhaseDeadlineSection,\n    ProjectPhaseDeadlineSectionAttribute,\n    Deadline,\n    Project,\n)\nfrom projects.tasks import refresh_project_schedule_cache \\\n    as refresh_project_schedule_cache_task\n\n\n@receiver([post_save, post_delete, m2m_changed], sender=Attribute)\n@receiver([post_save, post_delete, m2m_changed], sender=DataRetentionPlan)\n@receiver([post_save, post_delete, m2m_changed], sender=AttributeValueChoice)\n@receiver([post_save, post_delete, m2m_changed], sender=FieldSetAttribute)\n@receiver([post_save, post_delete, m2m_changed], sender=ProjectType)\n@receiver([post_save, post_delete, m2m_changed], sender=ProjectSubtype)\n@receiver([post_save, post_delete, m2m_changed], sender=ProjectFloorAreaSection)\n@receiver([post_save, post_delete, m2m_changed], sender=ProjectFloorAreaSectionAttribute)\n@receiver([post_save, post_delete, m2m_changed], sender=ProjectFloorAreaSectionAttributeMatrixStructure)\n@receiver([post_save, post_delete, m2m_changed], sender=ProjectFloorAreaSectionAttributeMatrixCell)\n@receiver([post_save, post_delete, m2m_changed], sender=ProjectPhase)\n@receiver([post_save, post_delete, m2m_changed], sender=ProjectPhaseSection)\n@receiver([post_save, post_delete, m2m_changed], sender=ProjectPhaseSectionAttribute)\n@receiver([post_save, post_delete, m2m_changed], sender=ProjectPhaseFieldSetAttributeIndex)\n@receiver([post_save, post_delete, m2m_changed], sender=PhaseAttributeMatrixStructure)\n@receiver([post_save, post_delete, m2m_changed], sender=PhaseAttributeMatrixCell)\n@receiver([post_save, post_delete, m2m_changed], sender=ProjectPhaseDeadlineSection)\n@receiver([post_save, post_delete, m2m_changed], sender=ProjectPhaseDeadlineSectionAttribute)\n@receiver([post_save, post_delete, m2m_changed], sender=Deadline)\ndef delete_cached_sections(*args, **kwargs):\n    cache.delete(\"serialized_phase_sections\")\n    cache.delete(\"serialized_deadline_sections\")\n\n@receiver([pre_save], sender=Project)\ndef save_attribute_data_subtype(sender, instance, *args, **kwargs):\n    # TODO: hard-coded attribute identifiers are not ideal\n    instance.attribute_data[\"kaavaprosessin_kokoluokka\"] = \\\n        instance.phase.project_subtype.name\n\n    instance.attribute_data[\"kaavan_vaihe\"] = \\\n        instance.phase.prefixed_name\n\n    for attr in Attribute.objects.filter(static_property__isnull=False):\n        value = getattr(instance, attr.static_property)\n\n        # make this a model field if more options are needed\n        if attr.value_type == Attribute.TYPE_USER:\n            value = value.uuid\n\n        instance.attribute_data[attr.identifier] = value\n\n@receiver([post_save, post_delete, m2m_changed], sender=Deadline)\ndef refresh_project_schedule_cache(sender, instance, *args, **kwargs):\n    async_task(refresh_project_schedule_cache_task)\n"}},"msg":"Prevent flooding task queue when importing deadlines"}},"https:\/\/github.com\/doug-holtsinger\/WirelessSensor":{"a722857ec8ee2537bf5031f60226eb1c121cb839":{"url":"https:\/\/api.github.com\/repos\/doug-holtsinger\/WirelessSensor\/commits\/a722857ec8ee2537bf5031f60226eb1c121cb839","html_url":"https:\/\/github.com\/doug-holtsinger\/WirelessSensor\/commit\/a722857ec8ee2537bf5031f60226eb1c121cb839","message":"Prevent app from getting flooded with notifications","sha":"a722857ec8ee2537bf5031f60226eb1c121cb839","keyword":"flooding prevent","diff":"diff --git a\/Compute\/src\/AHRS-VISUAL\/ahrs-console.py b\/Compute\/src\/AHRS-VISUAL\/ahrs-console.py\nindex 4e1bf14..74af2e6 100755\n--- a\/Compute\/src\/AHRS-VISUAL\/ahrs-console.py\n+++ b\/Compute\/src\/AHRS-VISUAL\/ahrs-console.py\n@@ -359,7 +359,6 @@ def connectPeripheral(self):\n                 self.peripheral.waitForNotifications(0.1)\n             except:\n                 pass\n-            print(\"NO NOTIFICATIONS\")\n         self.peripheral.disconnect()\n         self.peripheral = None\n         print(\"Disconnected\")\n@@ -375,6 +374,8 @@ def writeCmd(self,cmd):\n \n     def disconnect(self):\n         print(\"Disconnect...\")\n+        # disable delegate from flooding app with notifications\n+        self.peripheral.setDelegate(None)\n         self.connected.set(False)\n \n     def appExit(self):\n","files":{"\/Compute\/src\/AHRS-VISUAL\/ahrs-console.py":{"changes":[{"diff":"\n                 self.peripheral.waitForNotifications(0.1)\n             except:\n                 pass\n-            print(\"NO NOTIFICATIONS\")\n         self.peripheral.disconnect()\n         self.peripheral = None\n         print(\"Disconnected\")\n","add":0,"remove":1,"filename":"\/Compute\/src\/AHRS-VISUAL\/ahrs-console.py","badparts":["            print(\"NO NOTIFICATIONS\")"],"goodparts":[]}],"source":"\n \"\"\" Provide control of the Wireless Sensor device using a GUI. \"\"\" import sys import binascii from bluepy.btle import Scanner, DefaultDelegate, Peripheral, BTLEException import tkinter as tk import threading import matplotlib as mpl import matplotlib.pyplot as plt from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg import numpy as np class GUIApplication(tk.Frame): def __init__(self, master=None): tk.Frame.__init__(self, master) self.grid() self.connected=tk.BooleanVar() self.connected.set(False) self.connect_thread=None self.connect_button=None self.calibrate=tk.IntVar() self.calibrate.set(0) self.calibrate_prev=0 self.twoKp=tk.DoubleVar() self.twoKi=tk.DoubleVar() self.sampleFreq=tk.DoubleVar() self.gyroSens=tk.IntVar() self.resetCalibration() self.ahrs_data=[] for i in range(17): self.ahrs_data.append(tk.StringVar()) for i in range(17): self.ahrs_data[i].set(0) self.dataplot_cnv=None self.line=[] self.euler_angles=[] self.dataplot=[] self.dataplotidx=0 for i in range(3): self.line.append(None) self.euler_angles.append(tk.StringVar()) self.dataplot.append(np.zeros(50)) for i in range(3): self.euler_angles[i].set(0) self.createWidgets() self.peripheral=None def resetCalibration(self): self.twoKp.set(1.0) self.twoKpSave=1.0 self.twoKi.set(0.0) self.twoKiSave=0.0 self.sampleFreq.set(416.0) self.sampleFreqSave=416.0 self.gyroSens.set(16) self.gyroSensSave=16 def scalePlot(self): self.ax.relim() self.ax.autoscale_view(tight=False, scaley=True, scalex=False) def setAHRSData(self, idx, data): if idx >=0 and idx <=16: self.ahrs_data[idx].set(data) if idx==0 and self.dataplot_cnv is not None: print(\"Data idx %d %s\" %( idx, data)) for i in range(3): self.euler_angles[i].set(data[i]) self.dataplot[i][self.dataplotidx]=float(data[i]) self.line[i].set_data(np.arange(50), self.dataplot[i]) if self.dataplotidx==0: self.scalePlot() self.dataplotidx=( self.dataplotidx +1) % 50 self.dataplot_cnv.draw_idle() def createWidgetPlot(self, row_num, col_num, row_span): mpl.use(\"TkAgg\") paddingx=5 paddingy=5 self.fig, self.ax=plt.subplots(figsize=(5, 2.7), constrained_layout=True) for i in range(3): self.line[i],=self.ax.plot(np.arange(50), self.dataplot[i]) self.ax.set_xlabel('Time') self.ax.set_ylabel('Value'); self.ax.relim() self.ax.autoscale_view(tight=False, scaley=True, scalex=False) self.dataplot_cnv=FigureCanvasTkAgg(self.fig, master=self) self.dataplot_cnv.draw() self.dataplot_cnv.get_tk_widget().grid(column=col_num, row=row_num, padx=paddingx, pady=paddingy, rowspan=row_span) def createWidgets(self): row_num=0 col_num=0 paddingx=5 paddingy=5 self.connect_button=tk.Checkbutton(self, text=\"Connect\", command=self.connectButton, variable=self.connected, onvalue=True, offvalue=False) self.connect_button.grid(column=col_num, row=row_num) col_num=col_num +1 tk.Button(self, text=\"Quit\", command=self.appExit).grid(column=col_num, row=row_num) row_num=row_num +1 data_row_num=row_num col_num=0 lf=tk.LabelFrame(self, text=\"Calibration\") lf.grid(column=col_num, row=row_num, padx=paddingx, pady=paddingy) first_ctrl_row_num=row_num self.cb=[] self.cb.append(tk.Radiobutton(lf, text=\"Normalized\", command=self.calibrateButton, variable=self.calibrate, value=0)) self.cb.append(tk.Radiobutton(lf, text=\"Zero Offset\", command=self.calibrateButton, variable=self.calibrate, value=1)) self.cb.append(tk.Radiobutton(lf, text=\"Magnetometer\", command=self.calibrateButton, variable=self.calibrate, value=2)) for cb in self.cb: cb.pack(anchor=\"w\") tk.Button(lf, text=\"Reset\", command=self.calibrateResetButton).pack(anchor=\"w\") col_num=col_num +1 lf2=tk.LabelFrame(self, text=\"Euler Angles\") lf2.grid(column=col_num, row=row_num, padx=paddingx, pady=paddingy) tk.Label(lf2, text=\"Roll\", justify=tk.LEFT, padx=20).grid(column=0, row=0, padx=paddingx, pady=paddingy) tk.Label(lf2, relief=tk.SUNKEN, textvariable=self.euler_angles[0]).grid(column=1, row=0, padx=paddingx, pady=paddingy) tk.Label(lf2, text=\"Pitch\", justify=tk.LEFT, padx=20).grid(column=0, row=1, padx=paddingx, pady=paddingy) tk.Label(lf2, relief=tk.SUNKEN, textvariable=self.euler_angles[1]).grid(column=1, row=1, padx=paddingx, pady=paddingy) tk.Label(lf2, text=\"Yaw\", justify=tk.LEFT, padx=20).grid(column=0, row=2, padx=paddingx, pady=paddingy) tk.Label(lf2, relief=tk.SUNKEN, textvariable=self.euler_angles[2]).grid(column=1, row=2, padx=paddingx, pady=paddingy) col_num=0 row_num=row_num +1 tk.Label(self, text=\"Proportional Gain\").grid(column=0, row=row_num, padx=paddingx, pady=paddingy) tk.Spinbox(self, text=\"Spinbox\", command=self.proportionalGain, from_=0.0, to_=5.0, increment=0.1, format=\"%1.2f\", textvariable=self.twoKp).grid(column=1, row=row_num, padx=paddingx, pady=paddingy) row_num=row_num +1 tk.Label(self, text=\"Integral Gain\").grid(column=0, row=row_num, padx=paddingx, pady=paddingy) tk.Spinbox(self, text=\"Spinbox\", command=self.integralGain, from_=0.0, to_=5.0, increment=0.1, format=\"%1.2f\", textvariable=self.twoKi).grid(column=1, row=row_num, padx=paddingx, pady=paddingy) row_num=row_num +1 tk.Label(self, text=\"Sample Frequency\").grid(column=0, row=row_num, padx=paddingx, pady=paddingy) tk.Spinbox(self, text=\"Spinbox\", command=self.sampleFrequency, from_=0.0, to_=1600.0, increment=32.0, format=\"%4.1f\", textvariable=self.sampleFreq).grid(column=1, row=row_num, padx=paddingx, pady=paddingy) row_num=row_num +1 tk.Label(self, text=\"Gyroscope Sensitivity\").grid(column=0, row=row_num, padx=paddingx, pady=paddingy) tk.Spinbox(self, text=\"Spinbox\", command=self.gyroSensitivity, from_=1, to_=24, increment=1, textvariable=self.gyroSens).grid(column=1, row=row_num, padx=paddingx, pady=paddingy) row_num=row_num +1 lf3=tk.LabelFrame(self, text=\"Accelerometer\") lf3.grid(column=col_num, row=row_num, padx=paddingx, pady=paddingy) tk.Label(lf3, text=\"Normalized\").grid(column=0, row=0, padx=paddingx, pady=paddingy) tk.Label(lf3, relief=tk.SUNKEN, textvariable=self.ahrs_data[1]).grid(column=1, row=0, padx=paddingx, pady=paddingy) tk.Label(lf3, text=\"Calibrated\").grid(column=0, row=1, padx=paddingx, pady=paddingy) tk.Label(lf3, relief=tk.SUNKEN, textvariable=self.ahrs_data[2]).grid(column=1, row=1, padx=paddingx, pady=paddingy) tk.Label(lf3, text=\"Uncalibrated\").grid(column=0, row=2, padx=paddingx, pady=paddingy) tk.Label(lf3, relief=tk.SUNKEN, textvariable=self.ahrs_data[3]).grid(column=1, row=2, padx=paddingx, pady=paddingy) tk.Label(lf3, text=\"Min Threshold\").grid(column=0, row=3, padx=paddingx, pady=paddingy) tk.Label(lf3, relief=tk.SUNKEN, textvariable=self.ahrs_data[4]).grid(column=1, row=3, padx=paddingx, pady=paddingy) col_num=col_num +1 lf4=tk.LabelFrame(self, text=\"Gyroscope\") lf4.grid(column=col_num, row=row_num, padx=paddingx, pady=paddingy) tk.Label(lf4, text=\"Normalized\").grid(column=0, row=0, padx=paddingx, pady=paddingy) tk.Label(lf4, relief=tk.SUNKEN, textvariable=self.ahrs_data[5]).grid(column=1, row=0, padx=paddingx, pady=paddingy) tk.Label(lf4, text=\"Calibrated\").grid(column=0, row=1, padx=paddingx, pady=paddingy) tk.Label(lf4, relief=tk.SUNKEN, textvariable=self.ahrs_data[6]).grid(column=1, row=1, padx=paddingx, pady=paddingy) tk.Label(lf4, text=\"Uncalibrated\").grid(column=0, row=2, padx=paddingx, pady=paddingy) tk.Label(lf4, relief=tk.SUNKEN, textvariable=self.ahrs_data[7]).grid(column=1, row=2, padx=paddingx, pady=paddingy) tk.Label(lf4, text=\"Min Threshold\").grid(column=0, row=3, padx=paddingx, pady=paddingy) tk.Label(lf4, relief=tk.SUNKEN, textvariable=self.ahrs_data[8]).grid(column=1, row=3, padx=paddingx, pady=paddingy) row_num=row_num +1 col_num=col_num -1 lf5=tk.LabelFrame(self, text=\"Magnetometer\") lf5.grid(column=col_num, row=row_num, padx=paddingx, pady=paddingy) tk.Label(lf5, text=\"Normalized\").grid(column=0, row=0, padx=paddingx, pady=paddingy) tk.Label(lf5, relief=tk.SUNKEN, textvariable=self.ahrs_data[9]).grid(column=1, row=0, padx=paddingx, pady=paddingy) tk.Label(lf5, text=\"Calibrated\").grid(column=0, row=1, padx=paddingx, pady=paddingy) tk.Label(lf5, relief=tk.SUNKEN, textvariable=self.ahrs_data[10]).grid(column=1, row=1, padx=paddingx, pady=paddingy) tk.Label(lf5, text=\"Uncalibrated\").grid(column=0, row=2, padx=paddingx, pady=paddingy) tk.Label(lf5, relief=tk.SUNKEN, textvariable=self.ahrs_data[11]).grid(column=1, row=2, padx=paddingx, pady=paddingy) tk.Label(lf5, text=\"Min Threshold\").grid(column=0, row=3, padx=paddingx, pady=paddingy) tk.Label(lf5, relief=tk.SUNKEN, textvariable=self.ahrs_data[12]).grid(column=1, row=3, padx=paddingx, pady=paddingy) col_num=col_num +1 last_ctrl_row_num=row_num lf6=tk.LabelFrame(self, text=\"Quaternion\") lf6.grid(column=col_num, row=row_num, padx=paddingx, pady=paddingy) tk.Label(lf6, text=\"Q0\").grid(column=0, row=0, padx=paddingx, pady=paddingy) tk.Label(lf6, relief=tk.SUNKEN, textvariable=self.ahrs_data[13]).grid(column=1, row=0, padx=paddingx, pady=paddingy) tk.Label(lf6, text=\"Q1\").grid(column=0, row=1, padx=paddingx, pady=paddingy) tk.Label(lf6, relief=tk.SUNKEN, textvariable=self.ahrs_data[14]).grid(column=1, row=1, padx=paddingx, pady=paddingy) tk.Label(lf6, text=\"Q2\").grid(column=0, row=2, padx=paddingx, pady=paddingy) tk.Label(lf6, relief=tk.SUNKEN, textvariable=self.ahrs_data[15]).grid(column=1, row=2, padx=paddingx, pady=paddingy) tk.Label(lf6, text=\"Q3\").grid(column=0, row=3, padx=paddingx, pady=paddingy) tk.Label(lf6, relief=tk.SUNKEN, textvariable=self.ahrs_data[16]).grid(column=1, row=3, padx=paddingx, pady=paddingy) col_num=col_num +1 row_span=last_ctrl_row_num -first_ctrl_row_num +1 self.createWidgetPlot(data_row_num, col_num, row_span) def proportionalGain(self): print(\"Proportional Gain %f\" %( self.twoKp.get())) if self.twoKpSave < self.twoKp.get(): print(\"Prop Gain Up from %f\" %( self.twoKpSave)) self.writeCmd(b\"h\") self.twoKpSave=self.twoKpSave +0.1 else: print(\"Prop Gain Down from %f\" %( self.twoKpSave)) self.writeCmd(b\"j\") self.twoKpSave=self.twoKpSave -0.1 print(\"twoKp=%f\" %( self.twoKpSave)) def integralGain(self): print(\"Integral Gain %f\" %( self.twoKi.get())) if self.twoKiSave < self.twoKi.get(): print(\"Integral Gain Up from %f\" %( self.twoKiSave)) self.writeCmd(b\"l\") self.twoKiSave=self.twoKiSave +0.1 else: print(\"Integral Gain Down from %f\" %( self.twoKiSave)) self.writeCmd(b\"n\") self.twoKiSave=self.twoKiSave -0.1 print(\"twoKi=%f\" %( self.twoKiSave)) def sampleFrequency(self): print(\"Sample Frequency %f\" %( self.sampleFreq.get())) if self.sampleFreqSave < self.sampleFreq.get(): print(\"Sample Frequency Up from %f\" %( self.sampleFreqSave)) self.writeCmd(b\"s\") self.sampleFreqSave=self.sampleFreqSave +32.0 else: print(\"Sample Frequency Down from %f\" %( self.sampleFreqSave)) self.writeCmd(b\"o\") self.sampleFreqSave=self.sampleFreqSave -32.0 print(\"sampleFreq=%f\" %( self.sampleFreqSave)) def gyroSensitivity(self): print(\"Gyroscope Sensitivity %d\" %( self.gyroSens.get())) if self.gyroSensSave < self.gyroSens.get(): print(\"Gyroscope Sensitivity Up from %f\" %( self.gyroSensSave)) self.writeCmd(b\"u\") self.gyroSensSave=self.gyroSensSave +1 else: print(\"Gyroscope Sensitivity Down from %f\" %( self.gyroSensSave)) self.writeCmd(b\"t\") self.gyroSensSave=self.gyroSensSave -1 print(\"gyroSens=%d\" %( self.gyroSensSave)) def calibrateResetButton(self): print(\"Calibrate Reset\") self.writeCmd(b\"e\") self.resetCalibration() def calibrateButton(self): if self.connected.get(): print(\"Calibrate Button %d\" %( self.calibrate.get())) while self.calibrate_prev !=self.calibrate.get(): self.writeCmd(b\"c\") self.calibrate_prev=( self.calibrate_prev +1) % 3 else: self.calibrate.set(self.calibrate_prev) def connectButton(self): if self.connected.get(): self.connect_thread=threading.Thread(target=self.connectPeripheral) self.connect_thread.start() else: self.disconnect() def getBLEState(self): if self.peripheral: try: ble_state=self.peripheral.getState() print(\"Current State %s\" %( ble_state)) except BTLEException as e: self.connected.set(False) print(e) return return ble_state return def connectPeripheral(self): print(\"Connect...\") dev_addr='F1:68:47:7C:AD:E3' dev_addr='CC:43:80:8D:F8:46' self.peripheral=None try: self.peripheral=Peripheral(deviceAddr=dev_addr, addrType='random').withDelegate(NotifyDelegate()) except BTLEException as e: self.connected.set(False) print(e) return print(\"Connected\") srv=self.peripheral.getServiceByUUID('6e400001-b5a3-f393-e0a9-e50e24dcca9e') ch=srv.getCharacteristics() for c in ch: for d in c.getDescriptors(): val=d.read() print(\" Value: \", binascii.b2a_hex(val).decode('utf-8')) d.write(b\"\\x01\\x00\",withResponse=True) val=d.read() print(\" Value: \", binascii.b2a_hex(val).decode('utf-8')) print(\"Wait for notifications\") while self.connected.get(): try: self.peripheral.waitForNotifications(0.1) except: pass print(\"NO NOTIFICATIONS\") self.peripheral.disconnect() self.peripheral=None print(\"Disconnected\") def writeCmd(self,cmd): try: srv=self.peripheral.getServiceByUUID('6e400001-b5a3-f393-e0a9-e50e24dcca9e') uuid_write='6e400002-b5a3-f393-e0a9-e50e24dcca9e' uart_write=srv.getCharacteristics(forUUID=uuid_write) uart_write[0].write(cmd, withResponse=True) except BTLEException as e: self.disconnect() def disconnect(self): print(\"Disconnect...\") self.connected.set(False) def appExit(self): if self.connected.get(): self.connect_button.invoke() if self.peripheral is None: self.quit() sys.exit(0) class NotifyDelegate(DefaultDelegate): def __init__(self): DefaultDelegate.__init__(self) def handleNotification(self, cHandle, data): str_data=str(data, encoding='utf-8').split() print(\"Notif: %s\" %( str_data)) try: idx=int(str_data[0]) app.setAHRSData( idx, str_data[1:]) except: print(\"Notif: %s\" %( str_data)) app=GUIApplication() app.master.title('AHRS Command') app.mainloop() ","sourceWithComments":"#!\/usr\/bin\/env python3\n\n\"\"\" Provide control of the Wireless Sensor device using a GUI.\n\"\"\"\n\nimport sys\nimport binascii\nfrom bluepy.btle import Scanner, DefaultDelegate, Peripheral, BTLEException\n\nimport tkinter as tk\nimport threading\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\nimport numpy as np\n\nclass GUIApplication(tk.Frame):\n    def __init__(self, master=None):\n        tk.Frame.__init__(self, master)\n        self.grid()\n\n        self.connected = tk.BooleanVar()\n        self.connected.set(False)\n        self.connect_thread = None\n        self.connect_button = None\n\n        self.calibrate = tk.IntVar()\n        self.calibrate.set(0)\n        self.calibrate_prev = 0\n\n        self.twoKp = tk.DoubleVar()\n        self.twoKi = tk.DoubleVar()\n        self.sampleFreq = tk.DoubleVar()\n        self.gyroSens = tk.IntVar()\n\n        self.resetCalibration()\n\n        self.ahrs_data = []\n        for i in range(17):\n            self.ahrs_data.append(tk.StringVar())\n        for i in range(17):\n            self.ahrs_data[i].set(0)\n\n        self.dataplot_cnv = None\n        self.line = []\n        self.euler_angles = []\n        self.dataplot = []\n        self.dataplotidx = 0\n        for i in range(3):\n            self.line.append(None)\n            self.euler_angles.append(tk.StringVar())\n            self.dataplot.append(np.zeros(50))\n        for i in range(3):\n            self.euler_angles[i].set(0)\n\n        self.createWidgets()\n        self.peripheral = None\n\n    def resetCalibration(self):\n        self.twoKp.set(1.0)\n        self.twoKpSave = 1.0\n        self.twoKi.set(0.0)\n        self.twoKiSave = 0.0\n        self.sampleFreq.set(416.0)\n        self.sampleFreqSave = 416.0\n        self.gyroSens.set(16)\n        self.gyroSensSave = 16\n\n    def scalePlot(self):\n        #print(\"ybound %s\" % ( str(self.ax.get_ybound()) ) )\n        self.ax.relim()\n        self.ax.autoscale_view(tight=False, scaley=True, scalex=False)\n        #print(\"ybound %s\" % ( str(self.ax.get_ybound()) ) )\n\n    def setAHRSData(self, idx, data):\n        if idx >= 0 and idx <= 16:\n            # print(\"Data idx %d %s\" % ( idx , data ))\n            self.ahrs_data[idx].set(data)\n        if idx == 0 and self.dataplot_cnv is not None:\n            print(\"Data idx %d %s\" % ( idx , data ))\n            for i in range(3):\n                self.euler_angles[i].set(data[i])\n                self.dataplot[i][self.dataplotidx] = float(data[i])\n                self.line[i].set_data(np.arange(50), self.dataplot[i])\n            if self.dataplotidx == 0:\n                self.scalePlot()\n            self.dataplotidx = ( self.dataplotidx + 1 ) % 50\n            self.dataplot_cnv.draw_idle()\n\n    def createWidgetPlot(self, row_num, col_num, row_span):\n        # Canvas\n        mpl.use(\"TkAgg\")\n\n        paddingx = 5\n        paddingy = 5\n        self.fig, self.ax = plt.subplots(figsize=(5, 2.7), constrained_layout=True)\n        for i in range(3):\n            self.line[i], = self.ax.plot(np.arange(50), self.dataplot[i])\n        self.ax.set_xlabel('Time')\n        self.ax.set_ylabel('Value');\n\n        #self.ax.set_ylim(bottom=-90.0, top=90.0)\n        self.ax.relim()\n        self.ax.autoscale_view(tight=False, scaley=True, scalex=False)\n\n        #self.ax.autoscale(enable=True, axis='both')\n        # self.ax.set_autoscaley_on(True)\n        # self.ax.set_autoscaley_on(True)\n        # self.ax.autoscale(enable=True, axis='both')\n        # self.ax.set_ylim(auto=True)\n\n        # print( self.ax.format_ydata(0.1) )\n        # print( self.ax.get_children() )\n\n        self.dataplot_cnv = FigureCanvasTkAgg(self.fig, master=self)\n        self.dataplot_cnv.draw()\n        # cnv.get_tk_widget().grid(column=col_num, row=row_num, padx=paddingx, pady=paddingy, rowspan=last_ctrl_row_num - first_ctrl_row_num + 1)\n        self.dataplot_cnv.get_tk_widget().grid(column=col_num, row=row_num, padx=paddingx, pady=paddingy, rowspan=row_span)\n\n    def createWidgets(self):\n        row_num = 0\n        col_num = 0\n\n        paddingx = 5\n        paddingy = 5\n\n        self.connect_button = tk.Checkbutton(self, text=\"Connect\", command=self.connectButton, variable=self.connected, onvalue=True, offvalue=False)\n        self.connect_button.grid(column=col_num, row=row_num)\n        col_num = col_num + 1\n        tk.Button(self, text=\"Quit\", command=self.appExit).grid(column=col_num, row=row_num)\n\n        row_num = row_num + 1\n        data_row_num = row_num\n        col_num = 0\n\n        lf = tk.LabelFrame(self, text=\"Calibration\")\n        lf.grid(column=col_num, row=row_num, padx=paddingx, pady=paddingy)\n        first_ctrl_row_num = row_num\n\n        self.cb = []\n        self.cb.append(tk.Radiobutton(lf, text=\"Normalized\", command=self.calibrateButton, variable=self.calibrate, value=0))\n        self.cb.append(tk.Radiobutton(lf, text=\"Zero Offset\", command=self.calibrateButton, variable=self.calibrate, value=1))\n        self.cb.append(tk.Radiobutton(lf, text=\"Magnetometer\", command=self.calibrateButton, variable=self.calibrate, value=2))\n        for cb in self.cb:\n            cb.pack(anchor=\"w\")\n\n        tk.Button(lf, text=\"Reset\", command=self.calibrateResetButton).pack(anchor=\"w\")\n\n        # Euler Angles\n        col_num = col_num + 1\n        lf2 = tk.LabelFrame(self, text=\"Euler Angles\")\n        lf2.grid(column=col_num, row=row_num, padx=paddingx, pady=paddingy)\n\n        tk.Label(lf2, text=\"Roll\", justify=tk.LEFT, padx=20).grid(column=0, row=0, padx=paddingx, pady=paddingy)\n        tk.Label(lf2, relief=tk.SUNKEN, textvariable=self.euler_angles[0]).grid(column=1, row=0, padx=paddingx, pady=paddingy)\n        tk.Label(lf2, text=\"Pitch\", justify=tk.LEFT, padx=20).grid(column=0, row=1, padx=paddingx, pady=paddingy)\n        tk.Label(lf2, relief=tk.SUNKEN, textvariable=self.euler_angles[1]).grid(column=1, row=1, padx=paddingx, pady=paddingy)\n        tk.Label(lf2, text=\"Yaw\", justify=tk.LEFT, padx=20).grid(column=0, row=2, padx=paddingx, pady=paddingy)\n        tk.Label(lf2, relief=tk.SUNKEN, textvariable=self.euler_angles[2]).grid(column=1, row=2, padx=paddingx, pady=paddingy)\n\n        col_num = 0\n        row_num = row_num + 1\n\n        tk.Label(self, text=\"Proportional Gain\").grid(column=0, row=row_num, padx=paddingx, pady=paddingy)\n        tk.Spinbox(self, text=\"Spinbox\", command=self.proportionalGain, from_=0.0 , to_=5.0, increment=0.1, format=\"%1.2f\", textvariable=self.twoKp).grid(column=1, row=row_num, padx=paddingx, pady=paddingy)\n\n        row_num = row_num + 1\n        tk.Label(self, text=\"Integral Gain\").grid(column=0, row=row_num, padx=paddingx, pady=paddingy)\n        tk.Spinbox(self, text=\"Spinbox\", command=self.integralGain, from_=0.0, to_=5.0, increment=0.1, format=\"%1.2f\", textvariable=self.twoKi).grid(column=1, row=row_num, padx=paddingx, pady=paddingy)\n\n        row_num = row_num + 1\n        tk.Label(self, text=\"Sample Frequency\").grid(column=0, row=row_num, padx=paddingx, pady=paddingy)\n        tk.Spinbox(self, text=\"Spinbox\", command=self.sampleFrequency, from_=0.0, to_=1600.0, increment=32.0, format=\"%4.1f\", textvariable=self.sampleFreq).grid(column=1, row=row_num, padx=paddingx, pady=paddingy)\n\n        row_num = row_num + 1\n        tk.Label(self, text=\"Gyroscope Sensitivity\").grid(column=0, row=row_num, padx=paddingx, pady=paddingy)\n        tk.Spinbox(self, text=\"Spinbox\", command=self.gyroSensitivity, from_=1, to_=24, increment=1, textvariable=self.gyroSens).grid(column=1, row=row_num, padx=paddingx, pady=paddingy)\n\n        # Accelerometer Data\n        row_num = row_num + 1\n        lf3 = tk.LabelFrame(self, text=\"Accelerometer\")\n        lf3.grid(column=col_num, row=row_num, padx=paddingx, pady=paddingy)\n\n        tk.Label(lf3, text=\"Normalized\").grid(column=0, row=0, padx=paddingx, pady=paddingy)\n        tk.Label(lf3, relief=tk.SUNKEN, textvariable=self.ahrs_data[1]).grid(column=1, row=0, padx=paddingx, pady=paddingy)\n        tk.Label(lf3, text=\"Calibrated\").grid(column=0, row=1, padx=paddingx, pady=paddingy)\n        tk.Label(lf3, relief=tk.SUNKEN, textvariable=self.ahrs_data[2]).grid(column=1, row=1, padx=paddingx, pady=paddingy)\n        tk.Label(lf3, text=\"Uncalibrated\").grid(column=0, row=2, padx=paddingx, pady=paddingy)\n        tk.Label(lf3, relief=tk.SUNKEN, textvariable=self.ahrs_data[3]).grid(column=1, row=2, padx=paddingx, pady=paddingy)\n        tk.Label(lf3, text=\"Min Threshold\").grid(column=0, row=3, padx=paddingx, pady=paddingy)\n        tk.Label(lf3, relief=tk.SUNKEN, textvariable=self.ahrs_data[4]).grid(column=1, row=3, padx=paddingx, pady=paddingy)\n\n        # Gyroscope Data\n        col_num = col_num + 1\n        lf4 = tk.LabelFrame(self, text=\"Gyroscope\")\n        lf4.grid(column=col_num, row=row_num, padx=paddingx, pady=paddingy)\n\n        tk.Label(lf4, text=\"Normalized\").grid(column=0, row=0, padx=paddingx, pady=paddingy)\n        tk.Label(lf4, relief=tk.SUNKEN, textvariable=self.ahrs_data[5]).grid(column=1, row=0, padx=paddingx, pady=paddingy)\n        tk.Label(lf4, text=\"Calibrated\").grid(column=0, row=1, padx=paddingx, pady=paddingy)\n        tk.Label(lf4, relief=tk.SUNKEN, textvariable=self.ahrs_data[6]).grid(column=1, row=1, padx=paddingx, pady=paddingy)\n        tk.Label(lf4, text=\"Uncalibrated\").grid(column=0, row=2, padx=paddingx, pady=paddingy)\n        tk.Label(lf4, relief=tk.SUNKEN, textvariable=self.ahrs_data[7]).grid(column=1, row=2, padx=paddingx, pady=paddingy)\n        tk.Label(lf4, text=\"Min Threshold\").grid(column=0, row=3, padx=paddingx, pady=paddingy)\n        tk.Label(lf4, relief=tk.SUNKEN, textvariable=self.ahrs_data[8]).grid(column=1, row=3, padx=paddingx, pady=paddingy)\n\n        # Magnetometer Data\n        row_num = row_num + 1\n        col_num = col_num - 1\n        lf5 = tk.LabelFrame(self, text=\"Magnetometer\")\n        lf5.grid(column=col_num, row=row_num, padx=paddingx, pady=paddingy)\n\n        tk.Label(lf5, text=\"Normalized\").grid(column=0, row=0, padx=paddingx, pady=paddingy)\n        tk.Label(lf5, relief=tk.SUNKEN, textvariable=self.ahrs_data[9]).grid(column=1, row=0, padx=paddingx, pady=paddingy)\n        tk.Label(lf5, text=\"Calibrated\").grid(column=0, row=1, padx=paddingx, pady=paddingy)\n        tk.Label(lf5, relief=tk.SUNKEN, textvariable=self.ahrs_data[10]).grid(column=1, row=1, padx=paddingx, pady=paddingy)\n        tk.Label(lf5, text=\"Uncalibrated\").grid(column=0, row=2, padx=paddingx, pady=paddingy)\n        tk.Label(lf5, relief=tk.SUNKEN, textvariable=self.ahrs_data[11]).grid(column=1, row=2, padx=paddingx, pady=paddingy)\n        tk.Label(lf5, text=\"Min Threshold\").grid(column=0, row=3, padx=paddingx, pady=paddingy)\n        tk.Label(lf5, relief=tk.SUNKEN, textvariable=self.ahrs_data[12]).grid(column=1, row=3, padx=paddingx, pady=paddingy)\n\n        # Quaternion Data\n        col_num = col_num + 1\n        last_ctrl_row_num = row_num\n        lf6 = tk.LabelFrame(self, text=\"Quaternion\")\n        lf6.grid(column=col_num, row=row_num, padx=paddingx, pady=paddingy)\n\n        tk.Label(lf6, text=\"Q0\").grid(column=0, row=0, padx=paddingx, pady=paddingy)\n        tk.Label(lf6, relief=tk.SUNKEN, textvariable=self.ahrs_data[13]).grid(column=1, row=0, padx=paddingx, pady=paddingy)\n        tk.Label(lf6, text=\"Q1\").grid(column=0, row=1, padx=paddingx, pady=paddingy)\n        tk.Label(lf6, relief=tk.SUNKEN, textvariable=self.ahrs_data[14]).grid(column=1, row=1, padx=paddingx, pady=paddingy)\n        tk.Label(lf6, text=\"Q2\").grid(column=0, row=2, padx=paddingx, pady=paddingy)\n        tk.Label(lf6, relief=tk.SUNKEN, textvariable=self.ahrs_data[15]).grid(column=1, row=2, padx=paddingx, pady=paddingy)\n        tk.Label(lf6, text=\"Q3\").grid(column=0, row=3, padx=paddingx, pady=paddingy)\n        tk.Label(lf6, relief=tk.SUNKEN, textvariable=self.ahrs_data[16]).grid(column=1, row=3, padx=paddingx, pady=paddingy)\n\n        col_num = col_num + 1\n        row_span=last_ctrl_row_num - first_ctrl_row_num + 1\n        self.createWidgetPlot(data_row_num, col_num, row_span)\n\n    def proportionalGain(self):\n        print(\"Proportional Gain %f\" % ( self.twoKp.get()) )\n        if self.twoKpSave < self.twoKp.get():\n            # Send up\n            print(\"Prop Gain Up from %f\" % ( self.twoKpSave) )\n            self.writeCmd(b\"h\")\n            self.twoKpSave = self.twoKpSave + 0.1\n        else:\n            # Send down\n            print(\"Prop Gain Down from %f\" % ( self.twoKpSave) )\n            self.writeCmd(b\"j\")\n            self.twoKpSave = self.twoKpSave - 0.1\n        print(\"twoKp = %f\" % ( self.twoKpSave ))\n\n    def integralGain(self):\n        print(\"Integral Gain %f\" % ( self.twoKi.get()) )\n        if self.twoKiSave < self.twoKi.get():\n            # Send up\n            print(\"Integral Gain Up from %f\" % ( self.twoKiSave) )\n            self.writeCmd(b\"l\")\n            self.twoKiSave = self.twoKiSave + 0.1\n        else:\n            # Send down\n            print(\"Integral Gain Down from %f\" % ( self.twoKiSave) )\n            self.writeCmd(b\"n\")\n            self.twoKiSave = self.twoKiSave - 0.1\n        print(\"twoKi = %f\" % ( self.twoKiSave ))\n\n    def sampleFrequency(self):\n        print(\"Sample Frequency %f\" % ( self.sampleFreq.get()) )\n        if self.sampleFreqSave < self.sampleFreq.get():\n            # Send up\n            print(\"Sample Frequency Up from %f\" % ( self.sampleFreqSave ) )\n            self.writeCmd(b\"s\")\n            self.sampleFreqSave = self.sampleFreqSave + 32.0\n        else:\n            # Send down\n            print(\"Sample Frequency Down from %f\" % ( self.sampleFreqSave) )\n            self.writeCmd(b\"o\")\n            self.sampleFreqSave = self.sampleFreqSave - 32.0\n        print(\"sampleFreq = %f\" % ( self.sampleFreqSave ))\n\n    def gyroSensitivity(self):\n        print(\"Gyroscope Sensitivity %d\" % ( self.gyroSens.get()) )\n        if self.gyroSensSave < self.gyroSens.get():\n            # Send up\n            print(\"Gyroscope Sensitivity Up from %f\" % ( self.gyroSensSave ) )\n            self.writeCmd(b\"u\")\n            self.gyroSensSave = self.gyroSensSave + 1\n        else:\n            # Send down\n            print(\"Gyroscope Sensitivity Down from %f\" % ( self.gyroSensSave ) )\n            self.writeCmd(b\"t\")\n            self.gyroSensSave = self.gyroSensSave - 1\n        print(\"gyroSens = %d\" % ( self.gyroSensSave ))\n\n    def calibrateResetButton(self):\n        print(\"Calibrate Reset\")\n        self.writeCmd(b\"e\")\n        self.resetCalibration()\n\n    def calibrateButton(self):\n        if self.connected.get():\n            print(\"Calibrate Button %d\" % ( self.calibrate.get() ))\n            while self.calibrate_prev != self.calibrate.get():\n                self.writeCmd(b\"c\")\n                self.calibrate_prev = ( self.calibrate_prev + 1 ) % 3\n        else:\n            self.calibrate.set(self.calibrate_prev)\n\n    def connectButton(self):\n        if self.connected.get():\n            self.connect_thread = threading.Thread(target=self.connectPeripheral)\n            self.connect_thread.start()\n        else:\n            self.disconnect()\n\n    def getBLEState(self):\n        if self.peripheral:\n            try:\n                ble_state = self.peripheral.getState()\n                print(\"Current State %s\" % ( ble_state ))\n            except BTLEException as e:\n                self.connected.set(False)\n                print(e)\n                return\n            return ble_state\n        return\n\n    def connectPeripheral(self):\n        print(\"Connect...\")\n        # FIXME\n        # battery-powered device.\n        dev_addr = 'F1:68:47:7C:AD:E3'\n        # USB-powered device.\n        dev_addr = 'CC:43:80:8D:F8:46'\n        self.peripheral = None\n\n        try:\n            self.peripheral = Peripheral(deviceAddr = dev_addr, addrType = 'random').withDelegate(NotifyDelegate())\n        except BTLEException as e:\n            self.connected.set(False)\n            print(e)\n            return\n\n        print(\"Connected\")\n        srv = self.peripheral.getServiceByUUID('6e400001-b5a3-f393-e0a9-e50e24dcca9e')\n        ch = srv.getCharacteristics()\n        for c in ch:\n            for d in c.getDescriptors():\n                val = d.read()\n                print(\"    Value:  \", binascii.b2a_hex(val).decode('utf-8'))\n                d.write(b\"\\x01\\x00\",withResponse=True)\n                val = d.read()\n                print(\"    Value:  \", binascii.b2a_hex(val).decode('utf-8'))\n        print(\"Wait for notifications\")\n        while self.connected.get():\n            try:\n                self.peripheral.waitForNotifications(0.1)\n            except:\n                pass\n            print(\"NO NOTIFICATIONS\")\n        self.peripheral.disconnect()\n        self.peripheral = None\n        print(\"Disconnected\")\n\n    def writeCmd(self,cmd):\n        try:\n            srv = self.peripheral.getServiceByUUID('6e400001-b5a3-f393-e0a9-e50e24dcca9e')\n            uuid_write = '6e400002-b5a3-f393-e0a9-e50e24dcca9e'\n            uart_write = srv.getCharacteristics(forUUID = uuid_write)\n            uart_write[0].write(cmd, withResponse=True)\n        except BTLEException as e:\n            self.disconnect()\n\n    def disconnect(self):\n        print(\"Disconnect...\")\n        self.connected.set(False)\n\n    def appExit(self):\n        if self.connected.get():\n            self.connect_button.invoke()\n        if self.peripheral is None:\n            self.quit()\n            sys.exit(0)\n\nclass NotifyDelegate(DefaultDelegate):\n    def __init__(self):\n        DefaultDelegate.__init__(self)\n    def handleNotification(self, cHandle, data):\n        str_data = str(data, encoding='utf-8').split()\n        print(\"Notif: %s\" % ( str_data ))\n        try:\n            idx = int(str_data[0])\n            app.setAHRSData( idx, str_data[1:] )\n        except:\n            # raise\n            print(\"Notif: %s\" % ( str_data ))\n\n\napp = GUIApplication()\napp.master.title('AHRS Command')\napp.mainloop()\n\n\n"}},"msg":"Prevent app from getting flooded with notifications"}},"https:\/\/github.com\/FigureHook\/figure_hook":{"2229fb71ca657d99664b506b071317ace995ba20":{"url":"https:\/\/api.github.com\/repos\/FigureHook\/figure_hook\/commits\/2229fb71ca657d99664b506b071317ace995ba20","html_url":"https:\/\/github.com\/FigureHook\/figure_hook\/commit\/2229fb71ca657d99664b506b071317ace995ba20","message":"add: prevent anti-flood and log it","sha":"2229fb71ca657d99664b506b071317ace995ba20","keyword":"flooding prevent","diff":"diff --git a\/figure_hook\/Tasks\/periodic.py b\/figure_hook\/Tasks\/periodic.py\nindex 763250f..3a9bdaf 100644\n--- a\/figure_hook\/Tasks\/periodic.py\n+++ b\/figure_hook\/Tasks\/periodic.py\n@@ -1,9 +1,12 @@\n+import logging\n+import time\n from abc import ABC, abstractmethod\n \n from discord.webhook import RequestsWebhookAdapter\n from sqlalchemy import select, update\n \n from figure_hook.constants import PeriodicTask\n+from figure_hook.extension_class import ReleaseFeed\n from figure_hook.Factory.publish_factory.discord_embed_factory import \\\n     DiscordEmbedFactory\n from figure_hook.Factory.publish_factory.plurk_content_factory import \\\n@@ -12,8 +15,11 @@\n from figure_hook.Models import Task, Webhook\n from figure_hook.Publishers.dispatchers import \\\n     DiscordNewReleaseEmbedsDispatcher\n+from figure_hook.Publishers.exceptions import PlurkPublishException\n from figure_hook.Publishers.plurk import Plurker\n \n+logger = logging.getLogger(__name__)\n+\n \n class NewReleasePush(ABC):\n     __task_id__: PeriodicTask\n@@ -102,13 +108,21 @@ def _update_webhook_status(self, webhook_status):\n class PlurkNewReleasePush(NewReleasePush):\n     __task_id__ = PeriodicTask.PLURK_NEW_RELEASE_PUSH\n \n-    def execute(self):\n-        plurker = Plurker()\n+    def __init__(self, session):\n+        super().__init__(session)\n+        self.plurker = Plurker()\n+\n+    def execute(self, logger: logging.Logger = logger):\n         new_releases = self._fetch_new_releases()\n         self._update_execution_time()\n \n         for release in new_releases:\n             content = PlurkContentFactory.create_new_release(release)\n-            plurker.publish(content=content)\n-\n-        return plurker.stats\n+            try:\n+                self.plurker.publish(content=content)\n+            except PlurkPublishException as err:\n+                logger.error(err)\n+            finally:\n+                time.sleep(3)\n+\n+        return self.plurker.stats\n","files":{"\/figure_hook\/Tasks\/periodic.py":{"changes":[{"diff":"\n class PlurkNewReleasePush(NewReleasePush):\n     __task_id__ = PeriodicTask.PLURK_NEW_RELEASE_PUSH\n \n-    def execute(self):\n-        plurker = Plurker()\n+    def __init__(self, session):\n+        super().__init__(session)\n+        self.plurker = Plurker()\n+\n+    def execute(self, logger: logging.Logger = logger):\n         new_releases = self._fetch_new_releases()\n         self._update_execution_time()\n \n         for release in new_releases:\n             content = PlurkContentFactory.create_new_release(release)\n-            plurker.publish(content=content)\n-\n-        return plurker.stats\n+            try:\n+                self.plurker.publish(content=content)\n+            except PlurkPublishException as err:\n+                logger.error(err)\n+            finally:\n+                time.sleep(3)\n+\n+        return self.plurker.stats\n","add":13,"remove":5,"filename":"\/figure_hook\/Tasks\/periodic.py","badparts":["    def execute(self):","        plurker = Plurker()","            plurker.publish(content=content)","        return plurker.stats"],"goodparts":["    def __init__(self, session):","        super().__init__(session)","        self.plurker = Plurker()","    def execute(self, logger: logging.Logger = logger):","            try:","                self.plurker.publish(content=content)","            except PlurkPublishException as err:","                logger.error(err)","            finally:","                time.sleep(3)","        return self.plurker.stats"]}],"source":"\nfrom abc import ABC, abstractmethod from discord.webhook import RequestsWebhookAdapter from sqlalchemy import select, update from figure_hook.constants import PeriodicTask from figure_hook.Factory.publish_factory.discord_embed_factory import \\ DiscordEmbedFactory from figure_hook.Factory.publish_factory.plurk_content_factory import \\ PlurkContentFactory from figure_hook.Helpers.db_helper import ReleaseHelper from figure_hook.Models import Task, Webhook from figure_hook.Publishers.dispatchers import \\ DiscordNewReleaseEmbedsDispatcher from figure_hook.Publishers.plurk import Plurker class NewReleasePush(ABC): __task_id__: PeriodicTask def __init__(self, session): self._session=session self._model=self._fetch_model() or Task.create(name=self.name) def _fetch_model(self): stmt=select(Task).where(Task.name==self.name) result=self.session.execute(stmt) the_task=result.scalar() return the_task @property def executed_at(self): return self._model.executed_at @property def session(self): return self._session @property def name(self): return self.task_id.name @property def task_id(self): return self.__task_id__ def _fetch_new_releases(self): releases=ReleaseHelper.fetch_new_releases( self.session, self.executed_at ) return releases def _update_execution_time(self): self._model.update() @abstractmethod def execute(self): raise NotImplementedError class DiscordNewReleasePush(NewReleasePush): __task_id__=PeriodicTask.DISCORD_NEW_RELEASE_PUSH def execute(self): new_releases=self._fetch_new_releases() self._update_execution_time() raw_embeds=[] for release in new_releases: embed=DiscordEmbedFactory.create_new_release(release) raw_embeds.append(embed) webhooks=Webhook.all() webhook_adapter=RequestsWebhookAdapter() dispatcher=DiscordNewReleaseEmbedsDispatcher( webhooks=webhooks, raw_embeds=raw_embeds, adapter=webhook_adapter ) dispatcher.dispatch() self._update_webhook_status(dispatcher.webhook_status) return dispatcher.stats def _update_webhook_status(self, webhook_status): for webhook_id, is_existed in webhook_status.items(): stmt=update(Webhook).where( Webhook.id==webhook_id ).values( is_existed=is_existed ).execution_options( synchronize_session=\"fetch\" ) self.session.execute(stmt) class PlurkNewReleasePush(NewReleasePush): __task_id__=PeriodicTask.PLURK_NEW_RELEASE_PUSH def execute(self): plurker=Plurker() new_releases=self._fetch_new_releases() self._update_execution_time() for release in new_releases: content=PlurkContentFactory.create_new_release(release) plurker.publish(content=content) return plurker.stats ","sourceWithComments":"from abc import ABC, abstractmethod\n\nfrom discord.webhook import RequestsWebhookAdapter\nfrom sqlalchemy import select, update\n\nfrom figure_hook.constants import PeriodicTask\nfrom figure_hook.Factory.publish_factory.discord_embed_factory import \\\n    DiscordEmbedFactory\nfrom figure_hook.Factory.publish_factory.plurk_content_factory import \\\n    PlurkContentFactory\nfrom figure_hook.Helpers.db_helper import ReleaseHelper\nfrom figure_hook.Models import Task, Webhook\nfrom figure_hook.Publishers.dispatchers import \\\n    DiscordNewReleaseEmbedsDispatcher\nfrom figure_hook.Publishers.plurk import Plurker\n\n\nclass NewReleasePush(ABC):\n    __task_id__: PeriodicTask\n\n    def __init__(self, session):\n        self._session = session\n        self._model = self._fetch_model() or Task.create(name=self.name)\n\n    def _fetch_model(self):\n        stmt = select(Task).where(Task.name == self.name)\n        result = self.session.execute(stmt)\n        the_task = result.scalar()\n\n        return the_task\n\n    @property\n    def executed_at(self):\n        return self._model.executed_at\n\n    @property\n    def session(self):\n        return self._session\n\n    @property\n    def name(self):\n        return self.task_id.name\n\n    @property\n    def task_id(self):\n        return self.__task_id__\n\n    def _fetch_new_releases(self):\n        releases = ReleaseHelper.fetch_new_releases(\n            self.session,\n            self.executed_at\n        )\n        return releases\n\n    def _update_execution_time(self):\n        self._model.update()\n\n    @abstractmethod\n    def execute(self):\n        raise NotImplementedError\n\n\nclass DiscordNewReleasePush(NewReleasePush):\n    __task_id__ = PeriodicTask.DISCORD_NEW_RELEASE_PUSH\n\n    def execute(self):\n        new_releases = self._fetch_new_releases()\n        self._update_execution_time()\n        raw_embeds = []\n\n        for release in new_releases:\n            embed = DiscordEmbedFactory.create_new_release(release)\n            raw_embeds.append(embed)\n\n        webhooks = Webhook.all()\n\n        webhook_adapter = RequestsWebhookAdapter()\n        dispatcher = DiscordNewReleaseEmbedsDispatcher(\n            webhooks=webhooks,\n            raw_embeds=raw_embeds,\n            adapter=webhook_adapter\n        )\n        dispatcher.dispatch()\n\n        self._update_webhook_status(dispatcher.webhook_status)\n\n        return dispatcher.stats\n\n    def _update_webhook_status(self, webhook_status):\n        for webhook_id, is_existed in webhook_status.items():\n            stmt = update(Webhook).where(\n                Webhook.id == webhook_id\n            ).values(\n                is_existed=is_existed\n            ).execution_options(\n                synchronize_session=\"fetch\"\n            )\n\n            self.session.execute(stmt)\n\n\nclass PlurkNewReleasePush(NewReleasePush):\n    __task_id__ = PeriodicTask.PLURK_NEW_RELEASE_PUSH\n\n    def execute(self):\n        plurker = Plurker()\n        new_releases = self._fetch_new_releases()\n        self._update_execution_time()\n\n        for release in new_releases:\n            content = PlurkContentFactory.create_new_release(release)\n            plurker.publish(content=content)\n\n        return plurker.stats\n"}},"msg":"add: prevent anti-flood and log it"}},"https:\/\/github.com\/Dustella\/Forwardbot":{"749829d248f87d86bbd9a08880605fcfaa240536":{"url":"https:\/\/api.github.com\/repos\/Dustella\/Forwardbot\/commits\/749829d248f87d86bbd9a08880605fcfaa240536","html_url":"https:\/\/github.com\/Dustella\/Forwardbot\/commit\/749829d248f87d86bbd9a08880605fcfaa240536","message":"prevent flood messaging","sha":"749829d248f87d86bbd9a08880605fcfaa240536","keyword":"flooding prevent","diff":"diff --git a\/src\/plugins\/antispam\/__init__.py b\/src\/plugins\/antispam\/__init__.py\nindex 2bfdab8..0bc3f36 100755\n--- a\/src\/plugins\/antispam\/__init__.py\n+++ b\/src\/plugins\/antispam\/__init__.py\n@@ -1,20 +1,23 @@\n # import nonebot\n-from nonebot import get_driver,on_keyword\n+from nonebot import get_driver, on_keyword\n from nonebot.exception import ActionFailed\n-from nonebot.adapters.cqhttp import Bot,MessageEvent\n+from nonebot.adapters.cqhttp import Bot, MessageEvent\n from .config import Config\n \n global_config = get_driver().config\n config = Config(**global_config.dict())\n \n-spam=on_keyword({\"\u8fd8\u6ca1\u8fdb\u7fa4\u7684\u540c\u5b66\",\"\u5b66\u6821\u6709\u91cd\u8981\u4e8b\u5b9c\u901a\u77e5\"})\n+spam = on_keyword({\"\u8fd8\u6ca1\u8fdb\u7fa4\u7684\u540c\u5b66\", \"\u5b66\u6821\u6709\u91cd\u8981\u4e8b\u5b9c\u901a\u77e5\"})\n+\n \n @spam.handle()\n-async def anti_spam(bot:Bot,event:MessageEvent):\n+async def anti_spam(bot: Bot, event: MessageEvent):\n     this_message = event.message\n+    if str(event.sender.user_id) == str(bot.self_id):\n+        spam.finish()\n     try:\n         await bot.delete_msg(message_id=event.message_id)\n         await spam.finish(f\"\u53ef\u80fd\u7684\u8bc8\u9a97\u4fe1\u606f\u5df2\u8bc6\u522b\uff0c\u8bf7\u52ff\u4e0a\u5f53\u3002\\n\u539f\u6d88\u606f\u5df2\u64a4\u56de,\u5185\u5bb9\uff1a\\n{str(this_message)}\")\n     except ActionFailed as err:\n         await spam.finish(f\"\u53ef\u80fd\u7684\u8bc8\u9a97\u4fe1\u606f\u5df2\u8bc6\u522b\uff0c\u8bf7\u52ff\u4e0a\u5f53\u3002\\n\u539f\u6d88\u606f\u65e0\u6cd5\u64a4\u56de,\u5185\u5bb9\uff1a\\n{str(this_message)}\\n\u65e0\u6cd5\u64a4\u56de\u539f\u56e0\uff1a{str(err)}\")\n-    pass\n\\ No newline at end of file\n+    pass\n","files":{"\/src\/plugins\/antispam\/__init__.py":{"changes":[{"diff":"\n # import nonebot\n-from nonebot import get_driver,on_keyword\n+from nonebot import get_driver, on_keyword\n from nonebot.exception import ActionFailed\n-from nonebot.adapters.cqhttp import Bot,MessageEvent\n+from nonebot.adapters.cqhttp import Bot, MessageEvent\n from .config import Config\n \n global_config = get_driver().config\n config = Config(**global_config.dict())\n \n-spam=on_keyword({\"\u8fd8\u6ca1\u8fdb\u7fa4\u7684\u540c\u5b66\",\"\u5b66\u6821\u6709\u91cd\u8981\u4e8b\u5b9c\u901a\u77e5\"})\n+spam = on_keyword({\"\u8fd8\u6ca1\u8fdb\u7fa4\u7684\u540c\u5b66\", \"\u5b66\u6821\u6709\u91cd\u8981\u4e8b\u5b9c\u901a\u77e5\"})\n+\n \n @spam.handle()\n-async def anti_spam(bot:Bot,event:MessageEvent):\n+async def anti_spam(bot: Bot, event: MessageEvent):\n     this_message = event.message\n+    if str(event.sender.user_id) == str(bot.self_id):\n+        spam.finish()\n     try:\n         await bot.delete_msg(message_id=event.message_id)\n         await spam.finish(f\"\u53ef\u80fd\u7684\u8bc8\u9a97\u4fe1\u606f\u5df2\u8bc6\u522b\uff0c\u8bf7\u52ff\u4e0a\u5f53\u3002\\n\u539f\u6d88\u606f\u5df2\u64a4\u56de,\u5185\u5bb9\uff1a\\n{str(this_message)}\")\n     except ActionFailed as err:\n         await spam.finish(f\"\u53ef\u80fd\u7684\u8bc8\u9a97\u4fe1\u606f\u5df2\u8bc6\u522b\uff0c\u8bf7\u52ff\u4e0a\u5f53\u3002\\n\u539f\u6d88\u606f\u65e0\u6cd5\u64a4\u56de,\u5185\u5bb9\uff1a\\n{str(this_message)}\\n\u65e0\u6cd5\u64a4\u56de\u539f\u56e0\uff1a{str(err)}\")\n-    pass\n\\ No newline at end of file\n+    pass\n","add":8,"remove":5,"filename":"\/src\/plugins\/antispam\/__init__.py","badparts":["from nonebot import get_driver,on_keyword","from nonebot.adapters.cqhttp import Bot,MessageEvent","spam=on_keyword({\"\u8fd8\u6ca1\u8fdb\u7fa4\u7684\u540c\u5b66\",\"\u5b66\u6821\u6709\u91cd\u8981\u4e8b\u5b9c\u901a\u77e5\"})","async def anti_spam(bot:Bot,event:MessageEvent):","    pass"],"goodparts":["from nonebot import get_driver, on_keyword","from nonebot.adapters.cqhttp import Bot, MessageEvent","spam = on_keyword({\"\u8fd8\u6ca1\u8fdb\u7fa4\u7684\u540c\u5b66\", \"\u5b66\u6821\u6709\u91cd\u8981\u4e8b\u5b9c\u901a\u77e5\"})","async def anti_spam(bot: Bot, event: MessageEvent):","    if str(event.sender.user_id) == str(bot.self_id):","        spam.finish()","    pass"]}],"source":"\n\nfrom nonebot import get_driver,on_keyword from nonebot.exception import ActionFailed from nonebot.adapters.cqhttp import Bot,MessageEvent from.config import Config global_config=get_driver().config config=Config(**global_config.dict()) spam=on_keyword({\"\u8fd8\u6ca1\u8fdb\u7fa4\u7684\u540c\u5b66\",\"\u5b66\u6821\u6709\u91cd\u8981\u4e8b\u5b9c\u901a\u77e5\"}) @spam.handle() async def anti_spam(bot:Bot,event:MessageEvent): this_message=event.message try: await bot.delete_msg(message_id=event.message_id) await spam.finish(f\"\u53ef\u80fd\u7684\u8bc8\u9a97\u4fe1\u606f\u5df2\u8bc6\u522b\uff0c\u8bf7\u52ff\u4e0a\u5f53\u3002\\n\u539f\u6d88\u606f\u5df2\u64a4\u56de,\u5185\u5bb9\uff1a\\n{str(this_message)}\") except ActionFailed as err: await spam.finish(f\"\u53ef\u80fd\u7684\u8bc8\u9a97\u4fe1\u606f\u5df2\u8bc6\u522b\uff0c\u8bf7\u52ff\u4e0a\u5f53\u3002\\n\u539f\u6d88\u606f\u65e0\u6cd5\u64a4\u56de,\u5185\u5bb9\uff1a\\n{str(this_message)}\\n\u65e0\u6cd5\u64a4\u56de\u539f\u56e0\uff1a{str(err)}\") pass ","sourceWithComments":"# import nonebot\nfrom nonebot import get_driver,on_keyword\nfrom nonebot.exception import ActionFailed\nfrom nonebot.adapters.cqhttp import Bot,MessageEvent\nfrom .config import Config\n\nglobal_config = get_driver().config\nconfig = Config(**global_config.dict())\n\nspam=on_keyword({\"\u8fd8\u6ca1\u8fdb\u7fa4\u7684\u540c\u5b66\",\"\u5b66\u6821\u6709\u91cd\u8981\u4e8b\u5b9c\u901a\u77e5\"})\n\n@spam.handle()\nasync def anti_spam(bot:Bot,event:MessageEvent):\n    this_message = event.message\n    try:\n        await bot.delete_msg(message_id=event.message_id)\n        await spam.finish(f\"\u53ef\u80fd\u7684\u8bc8\u9a97\u4fe1\u606f\u5df2\u8bc6\u522b\uff0c\u8bf7\u52ff\u4e0a\u5f53\u3002\\n\u539f\u6d88\u606f\u5df2\u64a4\u56de,\u5185\u5bb9\uff1a\\n{str(this_message)}\")\n    except ActionFailed as err:\n        await spam.finish(f\"\u53ef\u80fd\u7684\u8bc8\u9a97\u4fe1\u606f\u5df2\u8bc6\u522b\uff0c\u8bf7\u52ff\u4e0a\u5f53\u3002\\n\u539f\u6d88\u606f\u65e0\u6cd5\u64a4\u56de,\u5185\u5bb9\uff1a\\n{str(this_message)}\\n\u65e0\u6cd5\u64a4\u56de\u539f\u56e0\uff1a{str(err)}\")\n    pass"}},"msg":"prevent flood messaging"}},"https:\/\/github.com\/dcmeglio\/pykevoplus":{"c652ec64c3a8a2c9031d9613fe9a61d55b5e7b0a":{"url":"https:\/\/api.github.com\/repos\/dcmeglio\/pykevoplus\/commits\/c652ec64c3a8a2c9031d9613fe9a61d55b5e7b0a","html_url":"https:\/\/github.com\/dcmeglio\/pykevoplus\/commit\/c652ec64c3a8a2c9031d9613fe9a61d55b5e7b0a","message":"API Rate Limiting and fault tolerance\n\nAdded re-login logic for errors and added delays for waiting for state changes to prevent api floods","sha":"c652ec64c3a8a2c9031d9613fe9a61d55b5e7b0a","keyword":"flooding prevent","diff":"diff --git a\/pykevoplus\/__init__.py b\/pykevoplus\/__init__.py\nindex c2567e1..0c6fa04 100644\n--- a\/pykevoplus\/__init__.py\n+++ b\/pykevoplus\/__init__.py\n@@ -174,7 +174,14 @@ def __str__(self):\n     def __repr__(self):\n         return \"KevoLock(name={}, id={}, state={})\".format(self.name, self.lockID, self.state)\n \n+    def __DoApiCall(url):\n+        api_result = self.session.get(url, cookies = {'_kevoweb_sessions': self.cookie})\n+        if api_result.status >= 400:\n+            self.cookie = Kevo.Login(requests.Session(), self.username, self.password)\n+            api_result = self.session.get(url, cookies = {'_kevoweb_sessions': self.cookie})\n \n+        return api_result\n+\t\n     @_manage_session\n     def _WaitForState(self, state, timeout=20):\n         \"\"\"\n@@ -185,13 +192,14 @@ def _WaitForState(self, state, timeout=20):\n             timeout:    how long to wait before giving up, in seconds (int)\n         \"\"\"\n         start_time = time.time()\n+\t\ttime.sleep(2)\n         while True:\n             self.Refresh()\n             if self.data[\"bolt_state\"].lower() == state.lower():\n                 break\n             if time.time() - start_time > timeout:\n                 raise KevoError(\"Timeout waiting for {}\".format(state.lower()))\n-            time.sleep(1)\n+            time.sleep(5)\n \n     def StartSession(self):\n         \"\"\"\n@@ -212,7 +220,7 @@ def Refresh(self):\n         Refresh the internal state of this lock object with the state from the real lock\n         \"\"\"\n         lock_info_url = Kevo.COMMANDS_URL_BASE + \"\/lock.json?arguments={}\".format(self.lockID)\n-        info_result = self.session.get(lock_info_url, cookies = {'_kevoweb_sessions': self.cookie})\n+        info_result = __DoApiCall(lock_info_url)\n         if info_result.status_code != 200:\n             raise KevoError(\"Error getting lock info: {}\".format(info_result.text))\n         self.data = json.loads(info_result.text)\n@@ -243,7 +251,7 @@ def Lock(self):\n         Lock this lock.  If the lock is already locked, this method has no effect.\n         \"\"\"\n         command_url = Kevo.COMMANDS_URL_BASE + \"\/remote_lock.json?arguments={}\".format(self.lockID)\n-        self.session.get(command_url, cookies = {'_kevoweb_sessions': self.cookie})\n+        __DoApiCall(command_url)\n         self.WaitForLocked()\n \n     @_manage_session\n@@ -252,7 +260,7 @@ def Unlock(self):\n         Unlock this lock.  If the lock is already unlocked, this method has no effect.\n         \"\"\"\n         command_url = Kevo.COMMANDS_URL_BASE + \"\/remote_unlock.json?arguments={}\".format(self.lockID)\n-        self.session.get(command_url, cookies = {'_kevoweb_sessions': self.cookie})\n+        __DoApiCall(command_url)\n         self.WaitForUnlocked()\n \n     @_manage_session\n","files":{"\/pykevoplus\/__init__.py":{"changes":[{"diff":"\n             timeout:    how long to wait before giving up, in seconds (int)\n         \"\"\"\n         start_time = time.time()\n+\t\ttime.sleep(2)\n         while True:\n             self.Refresh()\n             if self.data[\"bolt_state\"].lower() == state.lower():\n                 break\n             if time.time() - start_time > timeout:\n                 raise KevoError(\"Timeout waiting for {}\".format(state.lower()))\n-            time.sleep(1)\n+            time.sleep(5)\n \n     def StartSession(self):\n         \"\"\"\n","add":2,"remove":1,"filename":"\/pykevoplus\/__init__.py","badparts":["            time.sleep(1)"],"goodparts":["\t\ttime.sleep(2)","            time.sleep(5)"]},{"diff":"\n         Refresh the internal state of this lock object with the state from the real lock\n         \"\"\"\n         lock_info_url = Kevo.COMMANDS_URL_BASE + \"\/lock.json?arguments={}\".format(self.lockID)\n-        info_result = self.session.get(lock_info_url, cookies = {'_kevoweb_sessions': self.cookie})\n+        info_result = __DoApiCall(lock_info_url)\n         if info_result.status_code != 200:\n             raise KevoError(\"Error getting lock info: {}\".format(info_result.text))\n         self.data = json.loads(info_result.text)\n","add":1,"remove":1,"filename":"\/pykevoplus\/__init__.py","badparts":["        info_result = self.session.get(lock_info_url, cookies = {'_kevoweb_sessions': self.cookie})"],"goodparts":["        info_result = __DoApiCall(lock_info_url)"]},{"diff":"\n         Lock this lock.  If the lock is already locked, this method has no effect.\n         \"\"\"\n         command_url = Kevo.COMMANDS_URL_BASE + \"\/remote_lock.json?arguments={}\".format(self.lockID)\n-        self.session.get(command_url, cookies = {'_kevoweb_sessions': self.cookie})\n+        __DoApiCall(command_url)\n         self.WaitForLocked()\n \n     @_manage_session\n","add":1,"remove":1,"filename":"\/pykevoplus\/__init__.py","badparts":["        self.session.get(command_url, cookies = {'_kevoweb_sessions': self.cookie})"],"goodparts":["        __DoApiCall(command_url)"]},{"diff":"\n         Unlock this lock.  If the lock is already unlocked, this method has no effect.\n         \"\"\"\n         command_url = Kevo.COMMANDS_URL_BASE + \"\/remote_unlock.json?arguments={}\".format(self.lockID)\n-        self.session.get(command_url, cookies = {'_kevoweb_sessions': self.cookie})\n+        __DoApiCall(command_url)\n         self.WaitForUnlocked()\n \n     @_manage_session\n","add":1,"remove":1,"filename":"\/pykevoplus\/__init__.py","badparts":["        self.session.get(command_url, cookies = {'_kevoweb_sessions': self.cookie})"],"goodparts":["        __DoApiCall(command_url)"]}],"source":"\n \"\"\" This module provides convenient wrappers for controlling Kwikset Kevo locks \"\"\" from bs4 import BeautifulSoup import functools import json import requests import time class KevoError(Exception): \"\"\"Base exception for all Kevo errors\"\"\" pass class Kevo(object): \"\"\" Common mykevo.com operations \"\"\" KEVO_URL_BASE=\"https:\/\/www.mykevo.com\" COMMANDS_URL_BASE=KEVO_URL_BASE +\"\/user\/remote_locks\/command\" START_URL=KEVO_URL_BASE +\"\/login\" LOGIN_URL=KEVO_URL_BASE +\"\/signin\" @staticmethod def GetAuthToken(session): \"\"\" Get a mykevo.com auth token from a logged-in session Args: session: the session to use(requests.Session) Returns: An auth token(str) \"\"\" token=None result=session.get(Kevo.START_URL) login_page=BeautifulSoup(result.text, \"html.parser\") for field in login_page.find_all(\"input\"): if field.get(\"name\")==\"authenticity_token\": token=field.get(\"value\") break if not token: raise KevoError(\"Could not find auth token on signin page\") return token @staticmethod def Login(session, username, password): \"\"\" Login a session to mykevo.com Args: session: the session to use(requests.Session) username: your mykevo.com username(str) password: your mykevo.com password(str) \"\"\" token=Kevo.GetAuthToken(session) login_payload={ \"user[username]\": username, \"user[password]\": password, \"authenticity_token\": token } result=session.post(Kevo.LOGIN_URL, login_payload) return result.cookies.get_dict()['_kevoweb_sessions'] @staticmethod def GetLocks(username, password): \"\"\" Get a list of Kevo locks in a mykevo.com account Args: username: your mykevo.com username(str) password: your mykevo.com password(str) Returns: A list of locks(list of KevoLock) \"\"\" locks=[] with requests.Session() as session: result=Kevo.Login(session, username, password) lock_page=BeautifulSoup(result.text, \"html.parser\") for lock in lock_page.find_all(\"ul\", \"lock\"): lock_info=lock.find(\"div\", class_=\"lock_unlock_container\") lock_id=lock_info.get(\"data-lock-id\") lock_detail_url=Kevo.COMMANDS_URL_BASE +\"\/lock.json?arguments={}\".format(lock_id) detail_result=session.get(lock_detail_url) lock_details=json.loads(detail_result.text) locks.append(KevoLock.FromJSON(lock_details, username, password)) return locks def _manage_session(method): \"\"\" Decorator to handle the HTTP session to mykevo.com This allows methods in KevoLock to not have to manage auth sessions themselves \"\"\" @functools.wraps(method) def _wrapped(self, *args, **kwargs): if not self.cookie: self.cookie=Kevo.Login(requests.Session(), self.username, self.password) try: return method(self, *args, **kwargs) finally: pass return _wrapped class KevoLock(object): \"\"\" Object to represent a Kwikset Kevo lock \"\"\" @staticmethod def FromLockID(lockID, username, password): \"\"\" Create a KevoLock from the ID of the lock Args: lockID: the UUID of the lock(str) username: your mykevo.com username(str) password: your mykevo.com password(str) Returns: A ready to use lock object(KevoLock) \"\"\" lock=KevoLock() lock.lockID=lockID lock.username=username lock.password=password lock.Refresh() return lock @staticmethod def FromJSON(lockJSON, username, password): \"\"\" Create a KevoLock from the JSON metadata of the lock Args: lockJSON: the JSON data of the lock(dict) username: your mykevo.com username(str) password: your mykevo.com password(str) Returns: A ready to use lock object(KevoLock) \"\"\" lock=KevoLock() lock.username=username lock.password=password lock.data=lockJSON lock.lockID=lock.data[\"id\"] lock.name=lock.data[\"name\"] lock.state=lock.data[\"bolt_state\"] return lock def __init__(self): \"\"\" FromLockID factory constructor should be used instead of directly instantiating a KevoLock object \"\"\" self.data=None self.lockID=None self.name=None self.password=None self.cookie=None self.state=None self.username=None self.session=requests.Session() def __str__(self): return \"{}:{}\".format(self.name, self.state) def __repr__(self): return \"KevoLock(name={}, id={}, state={})\".format(self.name, self.lockID, self.state) @_manage_session def _WaitForState(self, state, timeout=20): \"\"\" Internal function to wait for the lock to achieve a given bolt state Args: state: the bolt state to wait for(str) timeout: how long to wait before giving up, in seconds(int) \"\"\" start_time=time.time() while True: self.Refresh() if self.data[\"bolt_state\"].lower()==state.lower(): break if time.time() -start_time > timeout: raise KevoError(\"Timeout waiting for{}\".format(state.lower())) time.sleep(1) def StartSession(self): \"\"\" Start an auth session for this lock, so that multiple commands can be executed without re-authorizing each command \"\"\" self.cookie=Kevo.Login(requests.Session(), self.username, self.password) def EndSession(self): \"\"\" Finish an auth session for this lock, so that any further commands will re-authorize \"\"\" if self.cookie: self.cookie=None @_manage_session def Refresh(self): \"\"\" Refresh the internal state of this lock object with the state from the real lock \"\"\" lock_info_url=Kevo.COMMANDS_URL_BASE +\"\/lock.json?arguments={}\".format(self.lockID) info_result=self.session.get(lock_info_url, cookies={'_kevoweb_sessions': self.cookie}) if info_result.status_code !=200: raise KevoError(\"Error getting lock info:{}\".format(info_result.text)) self.data=json.loads(info_result.text) self.name=self.data[\"name\"] self.state=self.data[\"bolt_state\"] def WaitForLocked(self, timeout=20): \"\"\" Wait or this lock to be in the locked bolt state Args: timeout: how long to wait before giving up, in seconds(int) \"\"\" self._WaitForState(\"locked\", timeout) def WaitForUnlocked(self, timeout=20): \"\"\" Wait or this lock to be in the unlocked bolt state Args: timeout: how long to wait before giving up, in seconds(int) \"\"\" self._WaitForState(\"unlocked\", timeout) @_manage_session def Lock(self): \"\"\" Lock this lock. If the lock is already locked, this method has no effect. \"\"\" command_url=Kevo.COMMANDS_URL_BASE +\"\/remote_lock.json?arguments={}\".format(self.lockID) self.session.get(command_url, cookies={'_kevoweb_sessions': self.cookie}) self.WaitForLocked() @_manage_session def Unlock(self): \"\"\" Unlock this lock. If the lock is already unlocked, this method has no effect. \"\"\" command_url=Kevo.COMMANDS_URL_BASE +\"\/remote_unlock.json?arguments={}\".format(self.lockID) self.session.get(command_url, cookies={'_kevoweb_sessions': self.cookie}) self.WaitForUnlocked() @_manage_session def GetBoltState(self): \"\"\" Retrieve the current bolt state of this lock Returns: The bolt state(str) \"\"\" self.Refresh() return self.data[\"bolt_state\"] def IsLocked(self): \"\"\" Determine if this lock's bolt state is locked Returns: True if locked, false otherwise(bool) \"\"\" return self.GetBoltState().lower()==\"locked\" def IsUnlocked(self): \"\"\" Determine if this lock's bolt state is unlocked Returns: True if unlocked, false otherwise(bool) \"\"\" return self.GetBoltState().lower()==\"unlocked\" class KevoLockSession(object): \"\"\" Context manager for kevo auth sessions \"\"\" def __init__(self, kevoLock): self.lock=kevoLock def __enter__(self): self.lock.StartSession() def __exit__(self, *exc): self.lock.EndSession() if __name__==\"__main__\": from getpass import getpass user=input(\"Username: \") passwd=getpass(\"Password: \") for kevolock in Kevo.GetLocks(user, passwd): print(repr(kevolock)) ","sourceWithComments":"#!\/usr\/bin\/env python3.6\n\"\"\"\nThis module provides convenient wrappers for controlling Kwikset Kevo locks\n\"\"\"\n\nfrom bs4 import BeautifulSoup\nimport functools\nimport json\nimport requests\nimport time\n\nclass KevoError(Exception):\n    \"\"\"Base exception for all Kevo errors\"\"\"\n    pass\n\nclass Kevo(object):\n    \"\"\"\n    Common mykevo.com operations\n    \"\"\"\n\n    KEVO_URL_BASE = \"https:\/\/www.mykevo.com\"\n    COMMANDS_URL_BASE = KEVO_URL_BASE + \"\/user\/remote_locks\/command\"\n\n    START_URL = KEVO_URL_BASE + \"\/login\"\n    LOGIN_URL = KEVO_URL_BASE + \"\/signin\"\n\n    @staticmethod\n    def GetAuthToken(session):\n        \"\"\"\n        Get a mykevo.com auth token from a logged-in session\n\n        Args:\n            session:    the session to use (requests.Session)\n\n        Returns:\n            An auth token (str)\n        \"\"\"\n        token = None\n        result = session.get(Kevo.START_URL)\n        login_page = BeautifulSoup(result.text, \"html.parser\")\n        for field in login_page.find_all(\"input\"):\n            if field.get(\"name\") == \"authenticity_token\":\n                token = field.get(\"value\")\n                break\n        if not token:\n            raise KevoError(\"Could not find auth token on signin page\")\n        return token\n\n    @staticmethod\n    def Login(session, username, password):\n        \"\"\"\n        Login a session to mykevo.com\n\n        Args:\n            session:    the session to use (requests.Session)\n            username:   your mykevo.com username (str)\n            password:   your mykevo.com password (str)\n        \"\"\"\n        token = Kevo.GetAuthToken(session)\n        login_payload = {\n            \"user[username]\" : username,\n            \"user[password]\" : password,\n            \"authenticity_token\" : token\n        }\n        result = session.post(Kevo.LOGIN_URL, login_payload)\n#        print result.status_code\n#        print result.text\n        return result.cookies.get_dict()['_kevoweb_sessions']\n\n    @staticmethod\n    def GetLocks(username, password):\n        \"\"\"\n        Get a list of Kevo locks in a mykevo.com account\n\n        Args:\n            username:   your mykevo.com username (str)\n            password:   your mykevo.com password (str)\n\n        Returns:\n            A list of locks (list of KevoLock)\n        \"\"\"\n        locks = []\n        with requests.Session() as session:\n            result = Kevo.Login(session, username, password)\n            lock_page = BeautifulSoup(result.text, \"html.parser\")\n            for lock in lock_page.find_all(\"ul\", \"lock\"):\n                lock_info = lock.find(\"div\", class_=\"lock_unlock_container\")\n                lock_id = lock_info.get(\"data-lock-id\")\n                lock_detail_url = Kevo.COMMANDS_URL_BASE + \"\/lock.json?arguments={}\".format(lock_id)\n                detail_result = session.get(lock_detail_url)\n                lock_details = json.loads(detail_result.text)\n                locks.append(KevoLock.FromJSON(lock_details, username, password))\n        return locks\n\ndef _manage_session(method):\n    \"\"\"\n    Decorator to handle the HTTP session to mykevo.com\n    This allows methods in KevoLock to not have to manage auth sessions themselves\n    \"\"\"\n    @functools.wraps(method)\n    def _wrapped(self, *args, **kwargs):\n        if not self.cookie:\n            self.cookie = Kevo.Login(requests.Session(), self.username, self.password)\n        try:\n            return method(self, *args, **kwargs)\n        finally:\n            pass\n    return _wrapped\n\nclass KevoLock(object):\n    \"\"\"\n    Object to represent a Kwikset Kevo lock\n    \"\"\"\n\n    @staticmethod\n    def FromLockID(lockID, username, password):\n        \"\"\"\n        Create a KevoLock from the ID of the lock\n\n        Args:\n            lockID:     the UUID of the lock (str)\n            username:   your mykevo.com username (str)\n            password:   your mykevo.com password (str)\n\n        Returns:\n            A ready to use lock object (KevoLock)\n        \"\"\"\n        lock = KevoLock()\n        lock.lockID = lockID\n        lock.username = username\n        lock.password = password\n        lock.Refresh()\n        return lock\n\n    @staticmethod\n    def FromJSON(lockJSON, username, password):\n        \"\"\"\n        Create a KevoLock from the JSON metadata of the lock\n\n        Args:\n            lockJSON:   the JSON data of the lock (dict)\n            username:   your mykevo.com username (str)\n            password:   your mykevo.com password (str)\n\n        Returns:\n            A ready to use lock object (KevoLock)\n        \"\"\"\n        lock = KevoLock()\n        lock.username = username\n        lock.password = password\n        lock.data = lockJSON\n        lock.lockID = lock.data[\"id\"]\n        lock.name = lock.data[\"name\"]\n        lock.state = lock.data[\"bolt_state\"]\n        return lock\n\n\n    def __init__(self):\n        \"\"\"\n        FromLockID factory constructor should be used instead of directly instantiating a KevoLock object\n        \"\"\"\n        self.data = None\n        self.lockID = None\n        self.name = None\n        self.password = None\n        self.cookie = None\n        self.state = None\n        self.username = None\n        self.session = requests.Session() \n\n    def __str__(self):\n        return \"{}: {}\".format(self.name, self.state)\n\n    def __repr__(self):\n        return \"KevoLock(name={}, id={}, state={})\".format(self.name, self.lockID, self.state)\n\n\n    @_manage_session\n    def _WaitForState(self, state, timeout=20):\n        \"\"\"\n        Internal function to wait for the lock to achieve a given bolt state\n\n        Args:\n            state:      the bolt state to wait for (str)\n            timeout:    how long to wait before giving up, in seconds (int)\n        \"\"\"\n        start_time = time.time()\n        while True:\n            self.Refresh()\n            if self.data[\"bolt_state\"].lower() == state.lower():\n                break\n            if time.time() - start_time > timeout:\n                raise KevoError(\"Timeout waiting for {}\".format(state.lower()))\n            time.sleep(1)\n\n    def StartSession(self):\n        \"\"\"\n        Start an auth session for this lock, so that multiple commands can be executed without re-authorizing each command\n        \"\"\"\n        self.cookie = Kevo.Login(requests.Session(), self.username, self.password)\n\n    def EndSession(self):\n        \"\"\"\n        Finish an auth session for this lock, so that any further commands will re-authorize\n        \"\"\"\n        if self.cookie:\n            self.cookie = None\n\n    @_manage_session\n    def Refresh(self):\n        \"\"\"\n        Refresh the internal state of this lock object with the state from the real lock\n        \"\"\"\n        lock_info_url = Kevo.COMMANDS_URL_BASE + \"\/lock.json?arguments={}\".format(self.lockID)\n        info_result = self.session.get(lock_info_url, cookies = {'_kevoweb_sessions': self.cookie})\n        if info_result.status_code != 200:\n            raise KevoError(\"Error getting lock info: {}\".format(info_result.text))\n        self.data = json.loads(info_result.text)\n        self.name = self.data[\"name\"]\n        self.state = self.data[\"bolt_state\"]\n\n    def WaitForLocked(self, timeout=20):\n        \"\"\"\n        Wait or this lock to be in the locked bolt state\n\n        Args:\n            timeout:    how long to wait before giving up, in seconds (int)\n        \"\"\"\n        self._WaitForState(\"locked\", timeout)\n\n    def WaitForUnlocked(self, timeout=20):\n        \"\"\"\n        Wait or this lock to be in the unlocked bolt state\n\n        Args:\n            timeout:    how long to wait before giving up, in seconds (int)\n        \"\"\"\n        self._WaitForState(\"unlocked\", timeout)\n\n    @_manage_session\n    def Lock(self):\n        \"\"\"\n        Lock this lock.  If the lock is already locked, this method has no effect.\n        \"\"\"\n        command_url = Kevo.COMMANDS_URL_BASE + \"\/remote_lock.json?arguments={}\".format(self.lockID)\n        self.session.get(command_url, cookies = {'_kevoweb_sessions': self.cookie})\n        self.WaitForLocked()\n\n    @_manage_session\n    def Unlock(self):\n        \"\"\"\n        Unlock this lock.  If the lock is already unlocked, this method has no effect.\n        \"\"\"\n        command_url = Kevo.COMMANDS_URL_BASE + \"\/remote_unlock.json?arguments={}\".format(self.lockID)\n        self.session.get(command_url, cookies = {'_kevoweb_sessions': self.cookie})\n        self.WaitForUnlocked()\n\n    @_manage_session\n    def GetBoltState(self):\n        \"\"\"\n        Retrieve the current bolt state of this lock\n\n        Returns:\n            The bolt state (str)\n        \"\"\"\n        self.Refresh()\n        return self.data[\"bolt_state\"]\n\n    def IsLocked(self):\n        \"\"\"\n        Determine if this lock's bolt state is locked\n\n        Returns:\n            True if locked, false otherwise (bool)\n        \"\"\"\n        return self.GetBoltState().lower() == \"locked\"\n\n    def IsUnlocked(self):\n        \"\"\"\n        Determine if this lock's bolt state is unlocked\n\n        Returns:\n            True if unlocked, false otherwise (bool)\n        \"\"\"\n        return self.GetBoltState().lower() == \"unlocked\"\n\n\nclass KevoLockSession(object):\n    \"\"\"\n    Context manager for kevo auth sessions\n    \"\"\"\n\n    def __init__(self, kevoLock):\n        self.lock = kevoLock\n\n    def __enter__(self):\n        self.lock.StartSession()\n\n    def __exit__(self, *exc):\n        self.lock.EndSession()\n\n\nif __name__ == \"__main__\":\n    from getpass import getpass\n\n    user = input(\"Username: \")\n    passwd = getpass(\"Password: \")\n\n    # Scrape the mykevo.com site to find the locks\n    for kevolock in Kevo.GetLocks(user, passwd):\n        print(repr(kevolock))\n\n    # Instantiate locks from IDs\n    # Get the lock IDs by logging into mykevo.com, click Details for the lock, click Settings, the lock ID is on the right\n#    front_door_id = \"cca7cd1d-c1d5-43ce-a087-c73b974b3529\"\n#    back_door_id = \"c60130cd-8139-4688-8ba3-199276a65ad6\"\n#    for lock_id in [front_door_id, back_door_id]:\n#        kevolock = KevoLock.FromLockID(lock_id, user, passwd)\n#        print str(kevolock)\n\n    # Do multiple operations on a lock using a single session\n#    kevolock = KevoLock.FromLockID(garage_door_id, user, passwd)\n#    with KevoLockSession(kevolock):\n#        kevolock.Unlock()\n#        kevolock.Lock()\n"}},"msg":"API Rate Limiting and fault tolerance\n\nAdded re-login logic for errors and added delays for waiting for state changes to prevent api floods"}},"https:\/\/github.com\/OldCodersClub\/LariskaBot":{"b91fc5c5a093d5822145cd1fdef03a19e4e8b3b8":{"url":"https:\/\/api.github.com\/repos\/OldCodersClub\/LariskaBot\/commits\/b91fc5c5a093d5822145cd1fdef03a19e4e8b3b8","html_url":"https:\/\/github.com\/OldCodersClub\/LariskaBot\/commit\/b91fc5c5a093d5822145cd1fdef03a19e4e8b3b8","sha":"b91fc5c5a093d5822145cd1fdef03a19e4e8b3b8","keyword":"flooding fix","diff":"diff --git a\/lariska_bot\/handlers\/throttling.py b\/lariska_bot\/handlers\/throttling.py\nindex 6ee703b..93914d1 100644\n--- a\/lariska_bot\/handlers\/throttling.py\n+++ b\/lariska_bot\/handlers\/throttling.py\n@@ -1,5 +1,6 @@\n async def flood_controlling(*args, **kwargs):\n     await args[0].reply(\n-        \"\u041f\u043e\u043c\u0435\u0434\u043b\u0435\u043d\u043d\u0435\u0435, \u0434\u043e\u0440\u043e\u0433\u0443\u0448\u0430!\"\n-        \"\\n\u042f \u0437\u0430\u043f\u0438\u0441\u044b\u0432\u0430\u044e...\"\n+        '\u041f\u043e\u043c\u0435\u0434\u043b\u0435\u043d\u043d\u0435\u0435, \u0434\u043e\u0440\u043e\u0433\u0443\u0448\u0430!'\n+        '\\n\u042f \u0437\u0430\u043f\u0438\u0441\u044b\u0432\u0430\u044e...'\n+        '\\n\u0413\u0440\u0443\u043f\u043f\u0438\u0440\u0443\u0439 \u043f\u0436\u043b\u0441\u0442 \u043c\u0435\u043b\u043a\u0438\u0435 \u0441\u043e\u043e\u0431\u0449\u0435\u043d\u0438\u044f \u0432 \u043e\u0434\u043d\u043e \u043a\u0440\u0443\u043f\u043d\u043e\u0435. \u0422\u0430\u043a \u0447\u0438\u0442\u0430\u0442\u044c \u043b\u0435\u0433\u0447\u0435.'\n     )\n","message":"","files":{"\/lariska_bot\/handlers\/throttling.py":{"changes":[{"diff":"\n async def flood_controlling(*args, **kwargs):\n     await args[0].reply(\n-        \"\u041f\u043e\u043c\u0435\u0434\u043b\u0435\u043d\u043d\u0435\u0435, \u0434\u043e\u0440\u043e\u0433\u0443\u0448\u0430!\"\n-        \"\\n\u042f \u0437\u0430\u043f\u0438\u0441\u044b\u0432\u0430\u044e...\"\n+        '\u041f\u043e\u043c\u0435\u0434\u043b\u0435\u043d\u043d\u0435\u0435, \u0434\u043e\u0440\u043e\u0433\u0443\u0448\u0430!'\n+        '\\n\u042f \u0437\u0430\u043f\u0438\u0441\u044b\u0432\u0430\u044e...'\n+        '\\n\u0413\u0440\u0443\u043f\u043f\u0438\u0440\u0443\u0439 \u043f\u0436\u043b\u0441\u0442 \u043c\u0435\u043b\u043a\u0438\u0435 \u0441\u043e\u043e\u0431\u0449\u0435\u043d\u0438\u044f \u0432 \u043e\u0434\u043d\u043e \u043a\u0440\u0443\u043f\u043d\u043e\u0435. \u0422\u0430\u043a \u0447\u0438\u0442\u0430\u0442\u044c \u043b\u0435\u0433\u0447\u0435.'\n     )\n","add":3,"remove":2,"filename":"\/lariska_bot\/handlers\/throttling.py","badparts":["        \"\u041f\u043e\u043c\u0435\u0434\u043b\u0435\u043d\u043d\u0435\u0435, \u0434\u043e\u0440\u043e\u0433\u0443\u0448\u0430!\"","        \"\\n\u042f \u0437\u0430\u043f\u0438\u0441\u044b\u0432\u0430\u044e...\""],"goodparts":["        '\u041f\u043e\u043c\u0435\u0434\u043b\u0435\u043d\u043d\u0435\u0435, \u0434\u043e\u0440\u043e\u0433\u0443\u0448\u0430!'","        '\\n\u042f \u0437\u0430\u043f\u0438\u0441\u044b\u0432\u0430\u044e...'","        '\\n\u0413\u0440\u0443\u043f\u043f\u0438\u0440\u0443\u0439 \u043f\u0436\u043b\u0441\u0442 \u043c\u0435\u043b\u043a\u0438\u0435 \u0441\u043e\u043e\u0431\u0449\u0435\u043d\u0438\u044f \u0432 \u043e\u0434\u043d\u043e \u043a\u0440\u0443\u043f\u043d\u043e\u0435. \u0422\u0430\u043a \u0447\u0438\u0442\u0430\u0442\u044c \u043b\u0435\u0433\u0447\u0435.'"]}],"source":"\nasync def flood_controlling(*args, **kwargs): await args[0].reply( \"\u041f\u043e\u043c\u0435\u0434\u043b\u0435\u043d\u043d\u0435\u0435, \u0434\u043e\u0440\u043e\u0433\u0443\u0448\u0430!\" \"\\n\u042f \u0437\u0430\u043f\u0438\u0441\u044b\u0432\u0430\u044e...\" ) ","sourceWithComments":"async def flood_controlling(*args, **kwargs):\n    await args[0].reply(\n        \"\u041f\u043e\u043c\u0435\u0434\u043b\u0435\u043d\u043d\u0435\u0435, \u0434\u043e\u0440\u043e\u0433\u0443\u0448\u0430!\"\n        \"\\n\u042f \u0437\u0430\u043f\u0438\u0441\u044b\u0432\u0430\u044e...\"\n    )\n"}},"msg":"fix(lariska_bot\/handlers): fix 'flood_controlling'"},"7dfc9a5ca61219646771722a461e19148f70ef9b":{"url":"https:\/\/api.github.com\/repos\/OldCodersClub\/LariskaBot\/commits\/7dfc9a5ca61219646771722a461e19148f70ef9b","html_url":"https:\/\/github.com\/OldCodersClub\/LariskaBot\/commit\/7dfc9a5ca61219646771722a461e19148f70ef9b","message":"fix(lariska_bot\/handlers): fix 'dont_flood'","sha":"7dfc9a5ca61219646771722a461e19148f70ef9b","keyword":"flooding fix","diff":"diff --git a\/lariska_bot\/handlers\/handler.py b\/lariska_bot\/handlers\/handler.py\nindex 21d74ce..c53a46f 100644\n--- a\/lariska_bot\/handlers\/handler.py\n+++ b\/lariska_bot\/handlers\/handler.py\n@@ -59,7 +59,7 @@ async def send_welcome(message: types.Message):\n \n @dp.message_handler(content_types=types.ContentTypes.TEXT)\n @dp.throttled(flood_controlling, rate=5)\n-async def main(message: types.Message):\n+async def dont_flood(message: types.Message):\n     pass\n \n \ndiff --git a\/lariska_bot\/handlers\/throttling.py b\/lariska_bot\/handlers\/throttling.py\nindex 93914d1..dd1f3af 100644\n--- a\/lariska_bot\/handlers\/throttling.py\n+++ b\/lariska_bot\/handlers\/throttling.py\n@@ -2,5 +2,4 @@ async def flood_controlling(*args, **kwargs):\n     await args[0].reply(\n         '\u041f\u043e\u043c\u0435\u0434\u043b\u0435\u043d\u043d\u0435\u0435, \u0434\u043e\u0440\u043e\u0433\u0443\u0448\u0430!'\n         '\\n\u042f \u0437\u0430\u043f\u0438\u0441\u044b\u0432\u0430\u044e...'\n-        '\\n\u0413\u0440\u0443\u043f\u043f\u0438\u0440\u0443\u0439 \u043f\u0436\u043b\u0441\u0442 \u043c\u0435\u043b\u043a\u0438\u0435 \u0441\u043e\u043e\u0431\u0449\u0435\u043d\u0438\u044f \u0432 \u043e\u0434\u043d\u043e \u043a\u0440\u0443\u043f\u043d\u043e\u0435. \u0422\u0430\u043a \u0447\u0438\u0442\u0430\u0442\u044c \u043b\u0435\u0433\u0447\u0435.'\n     )\n","files":{"\/lariska_bot\/handlers\/throttling.py":{"changes":[{"diff":"\n     await args[0].reply(\n         '\u041f\u043e\u043c\u0435\u0434\u043b\u0435\u043d\u043d\u0435\u0435, \u0434\u043e\u0440\u043e\u0433\u0443\u0448\u0430!'\n         '\\n\u042f \u0437\u0430\u043f\u0438\u0441\u044b\u0432\u0430\u044e...'\n-        '\\n\u0413\u0440\u0443\u043f\u043f\u0438\u0440\u0443\u0439 \u043f\u0436\u043b\u0441\u0442 \u043c\u0435\u043b\u043a\u0438\u0435 \u0441\u043e\u043e\u0431\u0449\u0435\u043d\u0438\u044f \u0432 \u043e\u0434\u043d\u043e \u043a\u0440\u0443\u043f\u043d\u043e\u0435. \u0422\u0430\u043a \u0447\u0438\u0442\u0430\u0442\u044c \u043b\u0435\u0433\u0447\u0435.'\n     )\n","add":0,"remove":1,"filename":"\/lariska_bot\/handlers\/throttling.py","badparts":["        '\\n\u0413\u0440\u0443\u043f\u043f\u0438\u0440\u0443\u0439 \u043f\u0436\u043b\u0441\u0442 \u043c\u0435\u043b\u043a\u0438\u0435 \u0441\u043e\u043e\u0431\u0449\u0435\u043d\u0438\u044f \u0432 \u043e\u0434\u043d\u043e \u043a\u0440\u0443\u043f\u043d\u043e\u0435. \u0422\u0430\u043a \u0447\u0438\u0442\u0430\u0442\u044c \u043b\u0435\u0433\u0447\u0435.'"],"goodparts":[]}],"source":"\nasync def flood_controlling(*args, **kwargs): await args[0].reply( '\u041f\u043e\u043c\u0435\u0434\u043b\u0435\u043d\u043d\u0435\u0435, \u0434\u043e\u0440\u043e\u0433\u0443\u0448\u0430!' '\\n\u042f \u0437\u0430\u043f\u0438\u0441\u044b\u0432\u0430\u044e...' '\\n\u0413\u0440\u0443\u043f\u043f\u0438\u0440\u0443\u0439 \u043f\u0436\u043b\u0441\u0442 \u043c\u0435\u043b\u043a\u0438\u0435 \u0441\u043e\u043e\u0431\u0449\u0435\u043d\u0438\u044f \u0432 \u043e\u0434\u043d\u043e \u043a\u0440\u0443\u043f\u043d\u043e\u0435. \u0422\u0430\u043a \u0447\u0438\u0442\u0430\u0442\u044c \u043b\u0435\u0433\u0447\u0435.' ) ","sourceWithComments":"async def flood_controlling(*args, **kwargs):\n    await args[0].reply(\n        '\u041f\u043e\u043c\u0435\u0434\u043b\u0435\u043d\u043d\u0435\u0435, \u0434\u043e\u0440\u043e\u0433\u0443\u0448\u0430!'\n        '\\n\u042f \u0437\u0430\u043f\u0438\u0441\u044b\u0432\u0430\u044e...'\n        '\\n\u0413\u0440\u0443\u043f\u043f\u0438\u0440\u0443\u0439 \u043f\u0436\u043b\u0441\u0442 \u043c\u0435\u043b\u043a\u0438\u0435 \u0441\u043e\u043e\u0431\u0449\u0435\u043d\u0438\u044f \u0432 \u043e\u0434\u043d\u043e \u043a\u0440\u0443\u043f\u043d\u043e\u0435. \u0422\u0430\u043a \u0447\u0438\u0442\u0430\u0442\u044c \u043b\u0435\u0433\u0447\u0435.'\n    )\n"}},"msg":"fix(lariska_bot\/handlers): fix 'dont_flood'"}},"https:\/\/github.com\/babyboydaprince\/NukeTown":{"6f1abd7be0e00919e28ac61d682a0de1a3ff71ec":{"url":"https:\/\/api.github.com\/repos\/babyboydaprince\/NukeTown\/commits\/6f1abd7be0e00919e28ac61d682a0de1a3ff71ec","html_url":"https:\/\/github.com\/babyboydaprince\/NukeTown\/commit\/6f1abd7be0e00919e28ac61d682a0de1a3ff71ec","message":"(fix) func calling & flooding","sha":"6f1abd7be0e00919e28ac61d682a0de1a3ff71ec","keyword":"flooding fix","diff":"diff --git a\/modules\/protocols\/udp.py b\/modules\/protocols\/udp.py\nindex 47a37af..e6547d5 100644\n--- a\/modules\/protocols\/udp.py\n+++ b\/modules\/protocols\/udp.py\n@@ -1,5 +1,6 @@\n import random\n import socket\n+from os import urandom as urandom\n from colorama import Fore\n \n # Create socket\n@@ -9,7 +10,7 @@\n def flood(target):\n     for _ in range(16):\n         try:\n-            payload = random._urandom(random.randint(1, 60))\n+            payload = urandom(random.randint(1, 60))\n             sock.sendto(payload, (target[0], target[1]))\n         except Exception as e:\n             print(\n@@ -18,6 +19,6 @@ def flood(target):\n             )\n         else:\n             print(\n-                f\"{Fore.GREEN}[+] {Fore.YELLOW}UDP random packet sent! Payload\"\n-                \"size: {len(payload)}. {Fore.RESET}\"\n+                f\"{Fore.GREEN}[+] {Fore.YELLOW}UDP random packet sent! Payload \\\n+                size: {len(payload)}. {Fore.RESET}\"\n             )\n","files":{"\/modules\/protocols\/udp.py":{"changes":[{"diff":"\n     for _ in range(16):\n         try:\n-            payload = random._urandom(random.randint(1, 60))\n+            payload = urandom(random.randint(1, 60))\n             sock.sendto(payload, (target[0], target[1]))\n         except Exception as e:\n             print(\n","add":1,"remove":1,"filename":"\/modules\/protocols\/udp.py","badparts":["            payload = random._urandom(random.randint(1, 60))"],"goodparts":["            payload = urandom(random.randint(1, 60))"]},{"diff":"\n             )\n         else:\n             print(\n-                f\"{Fore.GREEN}[+] {Fore.YELLOW}UDP random packet sent! Payload\"\n-                \"size: {len(payload)}. {Fore.RESET}\"\n+                f\"{Fore.GREEN}[+] {Fore.YELLOW}UDP random packet sent! Payload \\\n+                size: {len(payload)}. {Fore.RESET}\"\n             )\n","add":2,"remove":2,"filename":"\/modules\/protocols\/udp.py","badparts":["                f\"{Fore.GREEN}[+] {Fore.YELLOW}UDP random packet sent! Payload\"","                \"size: {len(payload)}. {Fore.RESET}\""],"goodparts":["                f\"{Fore.GREEN}[+] {Fore.YELLOW}UDP random packet sent! Payload \\","                size: {len(payload)}. {Fore.RESET}\""]}],"source":"\nimport random import socket from colorama import Fore sock=socket.socket(socket.AF_INET, socket.SOCK_DGRAM) def flood(target): for _ in range(16): try: payload=random._urandom(random.randint(1, 60)) sock.sendto(payload,(target[0], target[1])) except Exception as e: print( f'{Fore.MAGENTA}Error \\ while sending UDP packets\\n{Fore.MAGENTA}{e}{Fore.RESET}' ) else: print( f\"{Fore.GREEN}[+]{Fore.YELLOW}UDP random packet sent! Payload\" \"size:{len(payload)}.{Fore.RESET}\" ) ","sourceWithComments":"import random\nimport socket\nfrom colorama import Fore\n\n# Create socket\nsock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n\n\ndef flood(target):\n    for _ in range(16):\n        try:\n            payload = random._urandom(random.randint(1, 60))\n            sock.sendto(payload, (target[0], target[1]))\n        except Exception as e:\n            print(\n                f'{Fore.MAGENTA}Error \\\n                    while sending UDP packets\\n{Fore.MAGENTA}{e}{Fore.RESET}'\n            )\n        else:\n            print(\n                f\"{Fore.GREEN}[+] {Fore.YELLOW}UDP random packet sent! Payload\"\n                \"size: {len(payload)}. {Fore.RESET}\"\n            )\n"}},"msg":"(fix) func calling & flooding"}},"https:\/\/github.com\/Dra-Sama\/mangabot":{"8314e8e4cbd34a856172065eda5871dac57d26b5":{"url":"https:\/\/api.github.com\/repos\/Dra-Sama\/mangabot\/commits\/8314e8e4cbd34a856172065eda5871dac57d26b5","html_url":"https:\/\/github.com\/Dra-Sama\/mangabot\/commit\/8314e8e4cbd34a856172065eda5871dac57d26b5","message":"Fix flood handling","sha":"8314e8e4cbd34a856172065eda5871dac57d26b5","keyword":"flooding fix","diff":"diff --git a\/bot.py b\/bot.py\nindex d528584..540d241 100644\n--- a\/bot.py\n+++ b\/bot.py\n@@ -379,10 +379,10 @@ async def chapter_click(client, data, chat_id):\n             cbz = fld2cbz(pictures_folder, ch_name)\n             telegraph_url = await img2tph(chapter, clean(f'{chapter.manga.name} {chapter.name}'))\n \n-            messages: List[Message] = await retry_on_flood(bot.send_media_group(cache_channel, [\n+            messages: List[Message] = await retry_on_flood(bot.send_media_group)(cache_channel, [\n                 InputMediaDocument(pdf, thumb=thumb_path),\n                 InputMediaDocument(cbz, thumb=thumb_path, caption=f'{telegraph_url}')\n-            ]))\n+            ])\n \n             pdf_m, cbz_m = messages\n \n@@ -412,12 +412,12 @@ async def chapter_click(client, data, chat_id):\n             media_docs.append(InputMediaDocument(chapterFile.cbz_id))\n \n         if len(media_docs) == 0:\n-            await retry_on_flood(bot.send_message(chat_id, caption))\n+            await retry_on_flood(bot.send_message)(chat_id, caption)\n         elif len(media_docs) == 1:\n-            await retry_on_flood(bot.send_document(chat_id, media_docs[0].media, caption=caption))\n+            await retry_on_flood(bot.send_document)(chat_id, media_docs[0].media, caption=caption)\n         else:\n             media_docs[-1].caption = caption\n-            await retry_on_flood(bot.send_media_group(chat_id, media_docs))\n+            await retry_on_flood(bot.send_media_group)(chat_id, media_docs)\n         await asyncio.sleep(1)\n \n \ndiff --git a\/tools\/flood.py b\/tools\/flood.py\nindex fdd66dc..de4723f 100644\n--- a\/tools\/flood.py\n+++ b\/tools\/flood.py\n@@ -1,22 +1,25 @@\n import asyncio\n+from typing import Callable, Awaitable, Any\n \n import pyrogram.errors\n \n \n # retries an async awaitable as long as it raises FloodWait, and waits for err.x time\n-async def retry_on_flood(awaitable):\n-    while True:\n-        try:\n-            return await awaitable\n-        except pyrogram.errors.FloodWait as err:\n-            print(f'FloodWait, waiting {err.x} seconds')\n-            await asyncio.sleep(err.x)\n-            continue\n-        except pyrogram.errors.RPCError as err:\n-            if err.MESSAGE == 'FloodWait':\n+def retry_on_flood(function: Callable[[Any], Awaitable]):\n+    async def wrapper(*args, **kwargs):\n+        while True:\n+            try:\n+                return await function(*args, **kwargs)\n+            except pyrogram.errors.FloodWait as err:\n+                print(f'FloodWait, waiting {err.x} seconds')\n                 await asyncio.sleep(err.x)\n                 continue\n-            else:\n+            except pyrogram.errors.RPCError as err:\n+                if err.MESSAGE == 'FloodWait':\n+                    await asyncio.sleep(err.x)\n+                    continue\n+                else:\n+                    raise err\n+            except Exception as err:\n                 raise err\n-        except Exception as err:\n-            raise err\n+    return wrapper\n","files":{"\/bot.py":{"changes":[{"diff":"\n             cbz = fld2cbz(pictures_folder, ch_name)\n             telegraph_url = await img2tph(chapter, clean(f'{chapter.manga.name} {chapter.name}'))\n \n-            messages: List[Message] = await retry_on_flood(bot.send_media_group(cache_channel, [\n+            messages: List[Message] = await retry_on_flood(bot.send_media_group)(cache_channel, [\n                 InputMediaDocument(pdf, thumb=thumb_path),\n                 InputMediaDocument(cbz, thumb=thumb_path, caption=f'{telegraph_url}')\n-            ]))\n+            ])\n \n             pdf_m, cbz_m = messages\n \n","add":2,"remove":2,"filename":"\/bot.py","badparts":["            messages: List[Message] = await retry_on_flood(bot.send_media_group(cache_channel, [","            ]))"],"goodparts":["            messages: List[Message] = await retry_on_flood(bot.send_media_group)(cache_channel, [","            ])"]},{"diff":"\n             media_docs.append(InputMediaDocument(chapterFile.cbz_id))\n \n         if len(media_docs) == 0:\n-            await retry_on_flood(bot.send_message(chat_id, caption))\n+            await retry_on_flood(bot.send_message)(chat_id, caption)\n         elif len(media_docs) == 1:\n-            await retry_on_flood(bot.send_document(chat_id, media_docs[0].media, caption=caption))\n+            await retry_on_flood(bot.send_document)(chat_id, media_docs[0].media, caption=caption)\n         else:\n             media_docs[-1].caption = caption\n-            await retry_on_flood(bot.send_media_group(chat_id, media_docs))\n+            await retry_on_flood(bot.send_media_group)(chat_id, media_docs)\n         await asyncio.sleep(1)\n \n ","add":3,"remove":3,"filename":"\/bot.py","badparts":["            await retry_on_flood(bot.send_message(chat_id, caption))","            await retry_on_flood(bot.send_document(chat_id, media_docs[0].media, caption=caption))","            await retry_on_flood(bot.send_media_group(chat_id, media_docs))"],"goodparts":["            await retry_on_flood(bot.send_message)(chat_id, caption)","            await retry_on_flood(bot.send_document)(chat_id, media_docs[0].media, caption=caption)","            await retry_on_flood(bot.send_media_group)(chat_id, media_docs)"]}],"source":"\nimport enum import shutil from ast import arg import asyncio import re from dataclasses import dataclass import datetime as dt import json import pyrogram.errors from pyrogram.types import Message, InlineKeyboardMarkup, InlineKeyboardButton, CallbackQuery, InputMediaDocument from img2cbz.core import fld2cbz from img2pdf.core import fld2pdf from img2tph.core import img2tph from plugins import MangaClient, ManhuaKoClient, MangaCard, MangaChapter, ManhuaPlusClient, TMOClient, MangaDexClient, \\ MangaSeeClient, MangasInClient, McReaderClient, MangaKakalotClient, ManganeloClient, ManganatoClient, \\ KissMangaClient, MangatigreClient, MangaHasuClient import os from pyrogram import Client, filters from typing import Dict, Tuple, List, TypedDict from models.db import DB, ChapterFile, Subscription, LastChapter, MangaName, MangaOutput from pagination import Pagination from plugins.client import clean from tools.flood import retry_on_flood mangas: Dict[str, MangaCard]=dict() chapters: Dict[str, MangaChapter]=dict() pdfs: Dict[str, str]=dict() paginations: Dict[int, Pagination]=dict() queries: Dict[str, Tuple[MangaClient, str]]=dict() full_pages: Dict[str, List[str]]=dict() favourites: Dict[str, MangaCard]=dict() language_query: Dict[str, Tuple[str, str]]=dict() users_in_channel: Dict[int, dt.datetime]=dict() locks: Dict[int, asyncio.Lock]=dict() plugin_dicts: Dict[str, Dict[str, MangaClient]]={ \"\ud83c\uddec\ud83c\udde7 EN\":{ \"MangaDex\": MangaDexClient(), \"Manhuaplus\": ManhuaPlusClient(), \"Mangasee\": MangaSeeClient(), \"McReader\": McReaderClient(), \"MagaKakalot\": MangaKakalotClient(), \"Manganelo\": ManganeloClient(), \"Manganato\": ManganatoClient(), \"KissManga\": KissMangaClient(), \"MangaHasu\": MangaHasuClient() }, \"\ud83c\uddea\ud83c\uddf8 ES\":{ \"MangaDex\": MangaDexClient(language=(\"es-la\", \"es\")), \"ManhuaKo\": ManhuaKoClient(), \"TMO\": TMOClient(), \"Mangatigre\": MangatigreClient() } } class OutputOptions(enum.IntEnum): PDF=1 CBZ=2 Telegraph=4 def __and__(self, other): return self.value & other def __xor__(self, other): return self.value ^ other def __or__(self, other): return self.value | other disabled=[\"[\ud83c\uddec\ud83c\udde7 EN] McReader\"] plugins=dict() for lang, plugin_dict in plugin_dicts.items(): for name, plugin in plugin_dict.items(): identifier=f'[{lang}]{name}' if identifier in disabled: continue plugins[identifier]=plugin subsPaused=disabled +[] def split_list(li): return[li[x: x +2] for x in range(0, len(li), 2)] def get_buttons_for_options(user_options: int): buttons=[] for option in OutputOptions: checked=\"\u2705\" if option & user_options else \"\u274c\" text=f'{checked}{option.name}' buttons.append([InlineKeyboardButton(text, f\"options_{option.value}\")]) return InlineKeyboardMarkup(buttons) env_file=\"env.json\" if os.path.exists(env_file): with open(env_file) as f: env_vars=json.loads(f.read()) else: env_vars=dict(os.environ) bot=Client('bot', api_id=int(env_vars.get('API_ID')), api_hash=env_vars.get('API_HASH'), bot_token=env_vars.get('BOT_TOKEN')) @bot.on_message(filters=~(filters.private & filters.incoming)) async def on_chat_or_channel_message(client: Client, message: Message): pass @bot.on_message() async def on_private_message(client: Client, message: Message): channel=env_vars.get('CHANNEL') if not channel: return message.continue_propagation() if in_channel_cached:=users_in_channel.get(message.from_user.id): if dt.datetime.now() -in_channel_cached < dt.timedelta(days=1): return message.continue_propagation() try: if await client.get_chat_member(channel, message.from_user.id): users_in_channel[message.from_user.id]=dt.datetime.now() return message.continue_propagation() except pyrogram.errors.UsernameNotOccupied: print(\"Channel does not exist, therefore bot will continue to operate normally\") return message.continue_propagation() except pyrogram.errors.ChatAdminRequired: print(\"Bot is not admin of the channel, therefore bot will continue to operate normally\") return message.continue_propagation() except pyrogram.errors.UserNotParticipant: await message.reply(\"In order to use the bot you must join it's update channel.\", reply_markup=InlineKeyboardMarkup( [[InlineKeyboardButton('Join!', url=f't.me\/{channel}')]] )) @bot.on_message(filters=filters.command(['start'])) async def on_start(client: Client, message: Message): await message.reply(\"Welcome to the best manga pdf bot in telegram!!\\n\" \"\\n\" \"How to use? Just type the name of some manga you want to keep up to date.\\n\" \"\\n\" \"For example:\\n\" \"`Fire Force`\") @bot.on_message(filters=filters.command(['refresh'])) async def on_refresh(client: Client, message: Message): text=message.reply_to_message.text or message.reply_to_message.caption if text: regex=re.compile(r'\\[Read on telegraph]\\((.*)\\)') match=regex.search(text.markdown) else: match=None document=message.reply_to_message.document if not(message.reply_to_message and message.reply_to_message.outgoing and ((document and document.file_name[-4:].lower() in['.pdf', '.cbz']) or match)): return await message.reply(\"This command only works when it replies to a manga file that bot sent to you\") db=DB() if document: chapter=await db.get_chapter_file_by_id(document.file_unique_id) else: chapter=await db.get_chapter_file_by_id(match.group(1)) if not chapter: return await message.reply(\"This file was already refreshed\") await db.erase(chapter) return await message.reply(\"File refreshed successfully!\") @bot.on_message(filters=filters.command(['subs'])) async def on_subs(client: Client, message: Message): db=DB() subs=await db.get_subs(str(message.from_user.id)) lines=[] for sub in subs: lines.append(f'<a href=\"{sub.url}\">{sub.name}<\/a>') lines.append(f'`\/cancel{sub.url}`') lines.append('') if not lines: return await message.reply(\"You have no subscriptions yet.\") body=\"\\n\".join(lines) await message.reply(f'Your subscriptions:\\n\\n{body}', disable_web_page_preview=True) @bot.on_message(filters=filters.regex(r'^\/cancel([^]+)$')) async def on_cancel_command(client: Client, message: Message): db=DB() sub=await db.get(Subscription,(message.matches[0].group(1), str(message.from_user.id))) if not sub: return await message.reply(\"You were not subscribed to that manga.\") await db.erase(sub) return await message.reply(\"You will no longer receive updates for that manga.\") @bot.on_message(filters=filters.command(['options'])) async def on_options_command(client: Client, message: Message): db=DB() user_options=await db.get(MangaOutput, str(message.from_user.id)) user_options=user_options.output if user_options else(1 << 30) -1 buttons=get_buttons_for_options(user_options) return await message.reply(\"Select the desired output format.\", reply_markup=buttons) @bot.on_message(filters=filters.regex(r'^\/')) async def on_unknown_command(client: Client, message: Message): await message.reply(\"Unknown command\") @bot.on_message(filters=filters.text) async def on_message(client, message: Message): language_query[f\"lang_None_{hash(message.text)}\"]=(None, message.text) for language in plugin_dicts.keys(): language_query[f\"lang_{language}_{hash(message.text)}\"]=(language, message.text) await bot.send_message(message.chat.id, \"Select search languages.\", reply_markup=InlineKeyboardMarkup( split_list([InlineKeyboardButton(language, callback_data=f\"lang_{language}_{hash(message.text)}\") for language in plugin_dicts.keys()]) )) async def options_click(client, callback: CallbackQuery): db=DB() user_options=await db.get(MangaOutput, str(callback.from_user.id)) if not user_options: user_options=MangaOutput(user_id=str(callback.from_user.id), output=(2 << 30) -1) option=int(callback.data.split('_')[-1]) user_options.output ^=option buttons=get_buttons_for_options(user_options.output) await db.add(user_options) return await callback.message.edit_reply_markup(reply_markup=buttons) async def language_click(client, callback: CallbackQuery): lang, query=language_query[callback.data] if not lang: return await callback.message.edit(\"Select search languages.\", reply_markup=InlineKeyboardMarkup( split_list([InlineKeyboardButton(language, callback_data=f\"lang_{language}_{hash(query)}\") for language in plugin_dicts.keys()]) )) for identifier, manga_client in plugin_dicts[lang].items(): queries[f\"query_{lang}_{identifier}_{hash(query)}\"]=(manga_client, query) await callback.message.edit(f\"Language:{lang}\\n\\nSelect search plugin.\", reply_markup=InlineKeyboardMarkup( split_list([InlineKeyboardButton(identifier, callback_data=f\"query_{lang}_{identifier}_{hash(query)}\") for identifier in plugin_dicts[lang].keys() if f'[{lang}]{identifier}' not in disabled]) +[ [InlineKeyboardButton(\"\u25c0\ufe0f Back\", callback_data=f\"lang_None_{hash(query)}\")]] )) async def plugin_click(client, callback: CallbackQuery): manga_client, query=queries[callback.data] results=await manga_client.search(query) if not results: await bot.send_message(callback.from_user.id, \"No manga found for given query.\") return for result in results: mangas[result.unique()]=result await bot.send_message(callback.from_user.id, \"This is the result of your search\", reply_markup=InlineKeyboardMarkup([ [InlineKeyboardButton(result.name, callback_data=result.unique())] for result in results ])) async def manga_click(client, callback: CallbackQuery, pagination: Pagination=None): if pagination is None: pagination=Pagination() paginations[pagination.id]=pagination if pagination.manga is None: manga=mangas[callback.data] pagination.manga=manga results=await pagination.manga.client.get_chapters(pagination.manga, pagination.page) if not results: await callback.answer(\"Ups, no chapters there.\", show_alert=True) return full_page_key=f'full_page_{hash(\"\".join([result.unique() for result in results]))}' full_pages[full_page_key]=[] for result in results: chapters[result.unique()]=result full_pages[full_page_key].append(result.unique()) db=DB() subs=await db.get(Subscription,(pagination.manga.url, str(callback.from_user.id))) prev=[InlineKeyboardButton('<<', f'{pagination.id}_{pagination.page -1}')] next_=[InlineKeyboardButton('>>', f'{pagination.id}_{pagination.page +1}')] footer=[prev +next_] if pagination.page > 1 else[next_] fav=[[InlineKeyboardButton( \"Unsubscribe\" if subs else \"Subscribe\", f\"{'unfav' if subs else 'fav'}_{pagination.manga.unique()}\" )]] favourites[f\"fav_{pagination.manga.unique()}\"]=pagination.manga favourites[f\"unfav_{pagination.manga.unique()}\"]=pagination.manga full_page=[[InlineKeyboardButton('Full Page', full_page_key)]] buttons=InlineKeyboardMarkup(fav +footer +[ [InlineKeyboardButton(result.name, result.unique())] for result in results ] +full_page +footer) if pagination.message is None: try: message=await bot.send_photo(callback.from_user.id, pagination.manga.picture_url, f'{pagination.manga.name}\\n' f'{pagination.manga.get_url()}', reply_markup=buttons) pagination.message=message except pyrogram.errors.BadRequest as e: file_name=f'pictures\/{pagination.manga.unique()}.jpg' await pagination.manga.client.get_url(pagination.manga.picture_url, cache=True, file_name=file_name) message=await bot.send_photo(callback.from_user.id, f'.\/cache\/{pagination.manga.client.name}\/{file_name}', f'{pagination.manga.name}\\n' f'{pagination.manga.get_url()}', reply_markup=buttons) pagination.message=message else: await bot.edit_message_reply_markup( callback.from_user.id, pagination.message.message_id, reply_markup=buttons ) async def chapter_click(client, data, chat_id): lock=locks.get(chat_id) if not lock: locks[chat_id]=asyncio.Lock() async with locks[chat_id]: cache_channel=env_vars.get(\"CACHE_CHANNEL\") if not cache_channel: return await bot.send_message(chat_id, \"Bot cache channel is not configured correctly.\") try: cache_channel=int(cache_channel) except ValueError: pass chapter=chapters[data] db=DB() chapterFile=await db.get(ChapterFile, chapter.url) options=await db.get(MangaOutput, str(chat_id)) options=options.output if options else(1 << 30) -1 caption='\\n'.join([ f'{chapter.manga.name} -{chapter.name}', f'{chapter.get_url()}' ]) download=not chapterFile download=download or options & OutputOptions.PDF and not chapterFile.file_id download=download or options & OutputOptions.CBZ and not chapterFile.cbz_id download=download or options & OutputOptions.Telegraph and not chapterFile.telegraph_url download=download and options &((1 << len(OutputOptions)) -1) !=0 if download: pictures_folder=await chapter.client.download_pictures(chapter) if not chapter.pictures: return await bot.send_message(chat_id, f'There was an error parsing this chapter or chapter is missing' + f', please check the chapter at the web\\n\\n{caption}') ch_name=clean(f'{clean(chapter.manga.name, 25)} -{chapter.name}', 45) pdf, thumb_path=fld2pdf(pictures_folder, ch_name) cbz=fld2cbz(pictures_folder, ch_name) telegraph_url=await img2tph(chapter, clean(f'{chapter.manga.name}{chapter.name}')) messages: List[Message]=await retry_on_flood(bot.send_media_group(cache_channel,[ InputMediaDocument(pdf, thumb=thumb_path), InputMediaDocument(cbz, thumb=thumb_path, caption=f'{telegraph_url}') ])) pdf_m, cbz_m=messages if not chapterFile: await db.add(ChapterFile(url=chapter.url, file_id=pdf_m.document.file_id, file_unique_id=pdf_m.document.file_unique_id, cbz_id=cbz_m.document.file_id, cbz_unique_id=cbz_m.document.file_unique_id, telegraph_url=telegraph_url)) else: chapterFile.file_id, chapterFile.file_unique_id, chapterFile.cbz_id, \\ chapterFile.cbz_unique_id, chapterFile.telegraph_url=\\ pdf_m.document.file_id, pdf_m.document.file_unique_id, cbz_m.document.file_id, \\ cbz_m.document.file_unique_id, telegraph_url await db.add(chapterFile) shutil.rmtree(pictures_folder) chapterFile=await db.get(ChapterFile, chapter.url) caption=f'{chapter.manga.name} -{chapter.name}\\n' if options & OutputOptions.Telegraph: caption +=f'[Read on telegraph]({chapterFile.telegraph_url})\\n' caption +=f'[Read on website]({chapter.get_url()})' media_docs=[] if options & OutputOptions.PDF: media_docs.append(InputMediaDocument(chapterFile.file_id)) if options & OutputOptions.CBZ: media_docs.append(InputMediaDocument(chapterFile.cbz_id)) if len(media_docs)==0: await retry_on_flood(bot.send_message(chat_id, caption)) elif len(media_docs)==1: await retry_on_flood(bot.send_document(chat_id, media_docs[0].media, caption=caption)) else: media_docs[-1].caption=caption await retry_on_flood(bot.send_media_group(chat_id, media_docs)) await asyncio.sleep(1) async def pagination_click(client: Client, callback: CallbackQuery): pagination_id, page=map(int, callback.data.split('_')) pagination=paginations[pagination_id] pagination.page=page await manga_click(client, callback, pagination) async def full_page_click(client: Client, callback: CallbackQuery): chapters_data=full_pages[callback.data] for chapter_data in reversed(chapters_data): try: await chapter_click(client, chapter_data, callback.from_user.id) except Exception as e: print(e) await asyncio.sleep(0.5) async def favourite_click(client: Client, callback: CallbackQuery): action, data=callback.data.split('_') fav=action=='fav' manga=favourites[callback.data] db=DB() subs=await db.get(Subscription,(manga.url, str(callback.from_user.id))) if not subs and fav: await db.add(Subscription(url=manga.url, user_id=str(callback.from_user.id))) if subs and not fav: await db.erase(subs) if subs and fav: await callback.answer(\"You are already subscribed\", show_alert=True) if not subs and not fav: await callback.answer(\"You are not subscribed\", show_alert=True) reply_markup=callback.message.reply_markup keyboard=reply_markup.inline_keyboard keyboard[0]=[InlineKeyboardButton( \"Unsubscribe\" if fav else \"Subscribe\", f\"{'unfav' if fav else 'fav'}_{data}\" )] await bot.edit_message_reply_markup(callback.from_user.id, callback.message.message_id, InlineKeyboardMarkup(keyboard)) db_manga=await db.get(MangaName, manga.url) if not db_manga: await db.add(MangaName(url=manga.url, name=manga.name)) def is_pagination_data(callback: CallbackQuery): data=callback.data match=re.match(r'\\d+_\\d+', data) if not match: return False pagination_id=int(data.split('_')[0]) if pagination_id not in paginations: return False pagination=paginations[pagination_id] if not pagination.message: return False if pagination.message.chat.id !=callback.from_user.id: return False if pagination.message.message_id !=callback.message.message_id: return False return True @bot.on_callback_query() async def on_callback_query(client, callback: CallbackQuery): if callback.data in queries: await plugin_click(client, callback) elif callback.data in mangas: await manga_click(client, callback) elif callback.data in chapters: await chapter_click(client, callback.data, callback.from_user.id) elif callback.data in full_pages: await full_page_click(client, callback) elif callback.data in favourites: await favourite_click(client, callback) elif is_pagination_data(callback): await pagination_click(client, callback) elif callback.data in language_query: await language_click(client, callback) elif callback.data.startswith('options'): await options_click(client, callback) else: await bot.answer_callback_query(callback.id, 'This is an old button, please redo the search', show_alert=True) return try: await callback.answer() except BaseException as e: print(e) async def remove_subscriptions(sub: str): db=DB() await db.erase_subs(sub) async def update_mangas(): print(\"Updating mangas\") db=DB() subscriptions=await db.get_all(Subscription) last_chapters=await db.get_all(LastChapter) manga_names=await db.get_all(MangaName) subs_dictionary=dict() chapters_dictionary=dict() url_client_dictionary=dict() client_url_dictionary={client: set() for client in plugins.values()} manga_dict=dict() for subscription in subscriptions: if subscription.url not in subs_dictionary: subs_dictionary[subscription.url]=[] subs_dictionary[subscription.url].append(subscription.user_id) for last_chapter in last_chapters: chapters_dictionary[last_chapter.url]=last_chapter for manga in manga_names: manga_dict[manga.url]=manga for url in subs_dictionary: for ident, client in plugins.items(): if ident in subsPaused: continue if await client.contains_url(url): url_client_dictionary[url]=client client_url_dictionary[client].add(url) for client, urls in client_url_dictionary.items(): to_check=[chapters_dictionary[url] for url in urls if chapters_dictionary.get(url)] if len(to_check)==0: continue try: updated, not_updated=await client.check_updated_urls(to_check) except BaseException as e: print(f\"Error while checking updates for site:{client.name}, err: \", e) not_updated=list(urls) for url in not_updated: del url_client_dictionary[url] updated=dict() for url, client in url_client_dictionary.items(): try: if url not in manga_dict: continue manga_name=manga_dict[url].name if url not in chapters_dictionary: agen=client.iter_chapters(url, manga_name) last_chapter=await anext(agen) await db.add(LastChapter(url=url, chapter_url=last_chapter.url)) await asyncio.sleep(10) else: last_chapter=chapters_dictionary[url] new_chapters: List[MangaChapter]=[] counter=0 async for chapter in client.iter_chapters(url, manga_name): if chapter.url==last_chapter.chapter_url: break new_chapters.append(chapter) counter +=1 if counter==20: break if new_chapters: last_chapter.chapter_url=new_chapters[0].url await db.add(last_chapter) updated[url]=list(reversed(new_chapters)) for chapter in new_chapters: if chapter.unique() not in chapters: chapters[chapter.unique()]=chapter await asyncio.sleep(1) except BaseException as e: print(f'An exception occurred getting new chapters for url{url}:{e}') blocked=set() for url, chapter_list in updated.items(): for chapter in chapter_list: print(f'{chapter.manga.name} -{chapter.name}') for sub in subs_dictionary[url]: if sub in blocked: continue try: await chapter_click(bot, chapter.unique(), int(sub)) except pyrogram.errors.UserIsBlocked: print(f'User{sub} blocked the bot') await remove_subscriptions(sub) blocked.add(sub) except BaseException as e: print(f'An exception occurred sending new chapter:{e}') await asyncio.sleep(0.5) await asyncio.sleep(1) async def manga_updater(): minutes=5 while True: wait_time=minutes * 60 try: start=dt.datetime.now() await update_mangas() elapsed=dt.datetime.now() -start wait_time=max((dt.timedelta(seconds=wait_time) -elapsed).total_seconds(), 0) print(f'Time elapsed updating mangas:{elapsed}, waiting for{wait_time}') except BaseException as e: print(f'An exception occurred during chapters update:{e}') if wait_time: await asyncio.sleep(wait_time) ","sourceWithComments":"import enum\nimport shutil\nfrom ast import arg\nimport asyncio\nimport re\nfrom dataclasses import dataclass\nimport datetime as dt\nimport json\n\nimport pyrogram.errors\nfrom pyrogram.types import Message, InlineKeyboardMarkup, InlineKeyboardButton, CallbackQuery, InputMediaDocument\n\nfrom img2cbz.core import fld2cbz\nfrom img2pdf.core import fld2pdf\nfrom img2tph.core import img2tph\nfrom plugins import MangaClient, ManhuaKoClient, MangaCard, MangaChapter, ManhuaPlusClient, TMOClient, MangaDexClient, \\\n    MangaSeeClient, MangasInClient, McReaderClient, MangaKakalotClient, ManganeloClient, ManganatoClient, \\\n    KissMangaClient, MangatigreClient, MangaHasuClient\nimport os\n\nfrom pyrogram import Client, filters\nfrom typing import Dict, Tuple, List, TypedDict\n\nfrom models.db import DB, ChapterFile, Subscription, LastChapter, MangaName, MangaOutput\nfrom pagination import Pagination\nfrom plugins.client import clean\nfrom tools.flood import retry_on_flood\n\nmangas: Dict[str, MangaCard] = dict()\nchapters: Dict[str, MangaChapter] = dict()\npdfs: Dict[str, str] = dict()\npaginations: Dict[int, Pagination] = dict()\nqueries: Dict[str, Tuple[MangaClient, str]] = dict()\nfull_pages: Dict[str, List[str]] = dict()\nfavourites: Dict[str, MangaCard] = dict()\nlanguage_query: Dict[str, Tuple[str, str]] = dict()\nusers_in_channel: Dict[int, dt.datetime] = dict()\nlocks: Dict[int, asyncio.Lock] = dict()\n\nplugin_dicts: Dict[str, Dict[str, MangaClient]] = {\n    \"\ud83c\uddec\ud83c\udde7 EN\": {\n        \"MangaDex\": MangaDexClient(),\n        \"Manhuaplus\": ManhuaPlusClient(),\n        \"Mangasee\": MangaSeeClient(),\n        \"McReader\": McReaderClient(),\n        \"MagaKakalot\": MangaKakalotClient(),\n        \"Manganelo\": ManganeloClient(),\n        \"Manganato\": ManganatoClient(),\n        \"KissManga\": KissMangaClient(),\n        \"MangaHasu\": MangaHasuClient()\n    },\n    \"\ud83c\uddea\ud83c\uddf8 ES\": {\n        \"MangaDex\": MangaDexClient(language=(\"es-la\", \"es\")),\n        \"ManhuaKo\": ManhuaKoClient(),\n        \"TMO\": TMOClient(),\n        \"Mangatigre\": MangatigreClient()\n    }\n}\n\n\nclass OutputOptions(enum.IntEnum):\n    PDF = 1\n    CBZ = 2\n    Telegraph = 4\n\n    def __and__(self, other):\n        return self.value & other\n\n    def __xor__(self, other):\n        return self.value ^ other\n\n    def __or__(self, other):\n        return self.value | other\n\n\ndisabled = [\"[\ud83c\uddec\ud83c\udde7 EN] McReader\"]\n\nplugins = dict()\nfor lang, plugin_dict in plugin_dicts.items():\n    for name, plugin in plugin_dict.items():\n        identifier = f'[{lang}] {name}'\n        if identifier in disabled:\n            continue\n        plugins[identifier] = plugin\n\n# subsPaused = [\"[\ud83c\uddea\ud83c\uddf8 ES] TMO\"]\nsubsPaused = disabled + []\n\n\ndef split_list(li):\n    return [li[x: x + 2] for x in range(0, len(li), 2)]\n\n\ndef get_buttons_for_options(user_options: int):\n    buttons = []\n    for option in OutputOptions:\n        checked = \"\u2705\" if option & user_options else \"\u274c\"\n        text = f'{checked} {option.name}'\n        buttons.append([InlineKeyboardButton(text, f\"options_{option.value}\")])\n    return InlineKeyboardMarkup(buttons)\n\n\nenv_file = \"env.json\"\nif os.path.exists(env_file):\n    with open(env_file) as f:\n        env_vars = json.loads(f.read())\nelse:\n    env_vars = dict(os.environ)\n\nbot = Client('bot',\n             api_id=int(env_vars.get('API_ID')),\n             api_hash=env_vars.get('API_HASH'),\n             bot_token=env_vars.get('BOT_TOKEN'))\n\n\n@bot.on_message(filters=~(filters.private & filters.incoming))\nasync def on_chat_or_channel_message(client: Client, message: Message):\n    pass\n\n\n@bot.on_message()\nasync def on_private_message(client: Client, message: Message):\n    channel = env_vars.get('CHANNEL')\n    if not channel:\n        return message.continue_propagation()\n    if in_channel_cached := users_in_channel.get(message.from_user.id):\n        if dt.datetime.now() - in_channel_cached < dt.timedelta(days=1):\n            return message.continue_propagation()\n    try:\n        if await client.get_chat_member(channel, message.from_user.id):\n            users_in_channel[message.from_user.id] = dt.datetime.now()\n            return message.continue_propagation()\n    except pyrogram.errors.UsernameNotOccupied:\n        print(\"Channel does not exist, therefore bot will continue to operate normally\")\n        return message.continue_propagation()\n    except pyrogram.errors.ChatAdminRequired:\n        print(\"Bot is not admin of the channel, therefore bot will continue to operate normally\")\n        return message.continue_propagation()\n    except pyrogram.errors.UserNotParticipant:\n        await message.reply(\"In order to use the bot you must join it's update channel.\",\n                            reply_markup=InlineKeyboardMarkup(\n                                [[InlineKeyboardButton('Join!', url=f't.me\/{channel}')]]\n                            ))\n\n\n@bot.on_message(filters=filters.command(['start']))\nasync def on_start(client: Client, message: Message):\n    await message.reply(\"Welcome to the best manga pdf bot in telegram!!\\n\"\n                        \"\\n\"\n                        \"How to use? Just type the name of some manga you want to keep up to date.\\n\"\n                        \"\\n\"\n                        \"For example:\\n\"\n                        \"`Fire Force`\")\n\n\n@bot.on_message(filters=filters.command(['refresh']))\nasync def on_refresh(client: Client, message: Message):\n    text = message.reply_to_message.text or message.reply_to_message.caption\n    if text:\n        regex = re.compile(r'\\[Read on telegraph]\\((.*)\\)')\n        match = regex.search(text.markdown)\n    else:\n        match = None\n    document = message.reply_to_message.document\n    if not (message.reply_to_message and message.reply_to_message.outgoing and\n            ((document and document.file_name[-4:].lower() in ['.pdf', '.cbz']) or match)):\n        return await message.reply(\"This command only works when it replies to a manga file that bot sent to you\")\n    db = DB()\n    if document:\n        chapter = await db.get_chapter_file_by_id(document.file_unique_id)\n    else:\n        chapter = await db.get_chapter_file_by_id(match.group(1))\n    if not chapter:\n        return await message.reply(\"This file was already refreshed\")\n    await db.erase(chapter)\n    return await message.reply(\"File refreshed successfully!\")\n\n\n@bot.on_message(filters=filters.command(['subs']))\nasync def on_subs(client: Client, message: Message):\n    db = DB()\n    subs = await db.get_subs(str(message.from_user.id))\n    lines = []\n    for sub in subs:\n        lines.append(f'<a href=\"{sub.url}\">{sub.name}<\/a>')\n        lines.append(f'`\/cancel {sub.url}`')\n        lines.append('')\n\n    if not lines:\n        return await message.reply(\"You have no subscriptions yet.\")\n    body = \"\\n\".join(lines)\n    await message.reply(f'Your subscriptions:\\n\\n{body}', disable_web_page_preview=True)\n\n\n@bot.on_message(filters=filters.regex(r'^\/cancel ([^ ]+)$'))\nasync def on_cancel_command(client: Client, message: Message):\n    db = DB()\n    sub = await db.get(Subscription, (message.matches[0].group(1), str(message.from_user.id)))\n    if not sub:\n        return await message.reply(\"You were not subscribed to that manga.\")\n    await db.erase(sub)\n    return await message.reply(\"You will no longer receive updates for that manga.\")\n\n\n@bot.on_message(filters=filters.command(['options']))\nasync def on_options_command(client: Client, message: Message):\n    db = DB()\n    user_options = await db.get(MangaOutput, str(message.from_user.id))\n    user_options = user_options.output if user_options else (1 << 30) - 1\n    buttons = get_buttons_for_options(user_options)\n    return await message.reply(\"Select the desired output format.\", reply_markup=buttons)\n\n\n@bot.on_message(filters=filters.regex(r'^\/'))\nasync def on_unknown_command(client: Client, message: Message):\n    await message.reply(\"Unknown command\")\n\n\n@bot.on_message(filters=filters.text)\nasync def on_message(client, message: Message):\n    language_query[f\"lang_None_{hash(message.text)}\"] = (None, message.text)\n    for language in plugin_dicts.keys():\n        language_query[f\"lang_{language}_{hash(message.text)}\"] = (language, message.text)\n    await bot.send_message(message.chat.id, \"Select search languages.\", reply_markup=InlineKeyboardMarkup(\n        split_list([InlineKeyboardButton(language, callback_data=f\"lang_{language}_{hash(message.text)}\")\n                    for language in plugin_dicts.keys()])\n    ))\n\n\nasync def options_click(client, callback: CallbackQuery):\n    db = DB()\n    user_options = await db.get(MangaOutput, str(callback.from_user.id))\n    if not user_options:\n        user_options = MangaOutput(user_id=str(callback.from_user.id), output=(2 << 30) - 1)\n    option = int(callback.data.split('_')[-1])\n    user_options.output ^= option\n    buttons = get_buttons_for_options(user_options.output)\n    await db.add(user_options)\n    return await callback.message.edit_reply_markup(reply_markup=buttons)\n\n\nasync def language_click(client, callback: CallbackQuery):\n    lang, query = language_query[callback.data]\n    if not lang:\n        return await callback.message.edit(\"Select search languages.\", reply_markup=InlineKeyboardMarkup(\n            split_list([InlineKeyboardButton(language, callback_data=f\"lang_{language}_{hash(query)}\")\n                        for language in plugin_dicts.keys()])\n        ))\n    for identifier, manga_client in plugin_dicts[lang].items():\n        queries[f\"query_{lang}_{identifier}_{hash(query)}\"] = (manga_client, query)\n    await callback.message.edit(f\"Language: {lang}\\n\\nSelect search plugin.\", reply_markup=InlineKeyboardMarkup(\n        split_list([InlineKeyboardButton(identifier, callback_data=f\"query_{lang}_{identifier}_{hash(query)}\")\n                    for identifier in plugin_dicts[lang].keys() if f'[{lang}] {identifier}' not in disabled]) + [\n            [InlineKeyboardButton(\"\u25c0\ufe0f Back\", callback_data=f\"lang_None_{hash(query)}\")]]\n    ))\n\n\nasync def plugin_click(client, callback: CallbackQuery):\n    manga_client, query = queries[callback.data]\n    results = await manga_client.search(query)\n    if not results:\n        await bot.send_message(callback.from_user.id, \"No manga found for given query.\")\n        return\n    for result in results:\n        mangas[result.unique()] = result\n    await bot.send_message(callback.from_user.id,\n                           \"This is the result of your search\",\n                           reply_markup=InlineKeyboardMarkup([\n                               [InlineKeyboardButton(result.name, callback_data=result.unique())] for result in results\n                           ]))\n\n\nasync def manga_click(client, callback: CallbackQuery, pagination: Pagination = None):\n    if pagination is None:\n        pagination = Pagination()\n        paginations[pagination.id] = pagination\n\n    if pagination.manga is None:\n        manga = mangas[callback.data]\n        pagination.manga = manga\n\n    results = await pagination.manga.client.get_chapters(pagination.manga, pagination.page)\n\n    if not results:\n        await callback.answer(\"Ups, no chapters there.\", show_alert=True)\n        return\n\n    full_page_key = f'full_page_{hash(\"\".join([result.unique() for result in results]))}'\n    full_pages[full_page_key] = []\n    for result in results:\n        chapters[result.unique()] = result\n        full_pages[full_page_key].append(result.unique())\n\n    db = DB()\n    subs = await db.get(Subscription, (pagination.manga.url, str(callback.from_user.id)))\n\n    prev = [InlineKeyboardButton('<<', f'{pagination.id}_{pagination.page - 1}')]\n    next_ = [InlineKeyboardButton('>>', f'{pagination.id}_{pagination.page + 1}')]\n    footer = [prev + next_] if pagination.page > 1 else [next_]\n\n    fav = [[InlineKeyboardButton(\n        \"Unsubscribe\" if subs else \"Subscribe\",\n        f\"{'unfav' if subs else 'fav'}_{pagination.manga.unique()}\"\n    )]]\n    favourites[f\"fav_{pagination.manga.unique()}\"] = pagination.manga\n    favourites[f\"unfav_{pagination.manga.unique()}\"] = pagination.manga\n\n    full_page = [[InlineKeyboardButton('Full Page', full_page_key)]]\n\n    buttons = InlineKeyboardMarkup(fav + footer + [\n        [InlineKeyboardButton(result.name, result.unique())] for result in results\n    ] + full_page + footer)\n\n    if pagination.message is None:\n        try:\n            message = await bot.send_photo(callback.from_user.id,\n                                           pagination.manga.picture_url,\n                                           f'{pagination.manga.name}\\n'\n                                           f'{pagination.manga.get_url()}', reply_markup=buttons)\n            pagination.message = message\n        except pyrogram.errors.BadRequest as e:\n            file_name = f'pictures\/{pagination.manga.unique()}.jpg'\n            await pagination.manga.client.get_url(pagination.manga.picture_url, cache=True, file_name=file_name)\n            message = await bot.send_photo(callback.from_user.id,\n                                           f'.\/cache\/{pagination.manga.client.name}\/{file_name}',\n                                           f'{pagination.manga.name}\\n'\n                                           f'{pagination.manga.get_url()}', reply_markup=buttons)\n            pagination.message = message\n    else:\n        await bot.edit_message_reply_markup(\n            callback.from_user.id,\n            pagination.message.message_id,\n            reply_markup=buttons\n        )\n\n\nasync def chapter_click(client, data, chat_id):\n    lock = locks.get(chat_id)\n    if not lock:\n        locks[chat_id] = asyncio.Lock()\n\n    async with locks[chat_id]:\n        cache_channel = env_vars.get(\"CACHE_CHANNEL\")\n        if not cache_channel:\n            return await bot.send_message(chat_id, \"Bot cache channel is not configured correctly.\")\n\n        # Try convert to int cache_channel, because it can be id or username\n        try:\n            cache_channel = int(cache_channel)\n        except ValueError:\n            pass\n\n        chapter = chapters[data]\n\n        db = DB()\n\n        chapterFile = await db.get(ChapterFile, chapter.url)\n        options = await db.get(MangaOutput, str(chat_id))\n        options = options.output if options else (1 << 30) - 1\n\n        caption = '\\n'.join([\n            f'{chapter.manga.name} - {chapter.name}',\n            f'{chapter.get_url()}'\n        ])\n\n        download = not chapterFile\n        download = download or options & OutputOptions.PDF and not chapterFile.file_id\n        download = download or options & OutputOptions.CBZ and not chapterFile.cbz_id\n        download = download or options & OutputOptions.Telegraph and not chapterFile.telegraph_url\n        download = download and options & ((1 << len(OutputOptions)) - 1) != 0\n\n        if download:\n            pictures_folder = await chapter.client.download_pictures(chapter)\n            if not chapter.pictures:\n                return await bot.send_message(chat_id, f'There was an error parsing this chapter or chapter is missing' +\n                                              f', please check the chapter at the web\\n\\n{caption}')\n            ch_name = clean(f'{clean(chapter.manga.name, 25)} - {chapter.name}', 45)\n            pdf, thumb_path = fld2pdf(pictures_folder, ch_name)\n            cbz = fld2cbz(pictures_folder, ch_name)\n            telegraph_url = await img2tph(chapter, clean(f'{chapter.manga.name} {chapter.name}'))\n\n            messages: List[Message] = await retry_on_flood(bot.send_media_group(cache_channel, [\n                InputMediaDocument(pdf, thumb=thumb_path),\n                InputMediaDocument(cbz, thumb=thumb_path, caption=f'{telegraph_url}')\n            ]))\n\n            pdf_m, cbz_m = messages\n\n            if not chapterFile:\n                await db.add(ChapterFile(url=chapter.url, file_id=pdf_m.document.file_id,\n                                         file_unique_id=pdf_m.document.file_unique_id, cbz_id=cbz_m.document.file_id,\n                                         cbz_unique_id=cbz_m.document.file_unique_id, telegraph_url=telegraph_url))\n            else:\n                chapterFile.file_id, chapterFile.file_unique_id, chapterFile.cbz_id, \\\n                chapterFile.cbz_unique_id, chapterFile.telegraph_url = \\\n                    pdf_m.document.file_id, pdf_m.document.file_unique_id, cbz_m.document.file_id, \\\n                    cbz_m.document.file_unique_id, telegraph_url\n                await db.add(chapterFile)\n\n            shutil.rmtree(pictures_folder)\n\n        chapterFile = await db.get(ChapterFile, chapter.url)\n\n        caption = f'{chapter.manga.name} - {chapter.name}\\n'\n        if options & OutputOptions.Telegraph:\n            caption += f'[Read on telegraph]({chapterFile.telegraph_url})\\n'\n        caption += f'[Read on website]({chapter.get_url()})'\n        media_docs = []\n        if options & OutputOptions.PDF:\n            media_docs.append(InputMediaDocument(chapterFile.file_id))\n        if options & OutputOptions.CBZ:\n            media_docs.append(InputMediaDocument(chapterFile.cbz_id))\n\n        if len(media_docs) == 0:\n            await retry_on_flood(bot.send_message(chat_id, caption))\n        elif len(media_docs) == 1:\n            await retry_on_flood(bot.send_document(chat_id, media_docs[0].media, caption=caption))\n        else:\n            media_docs[-1].caption = caption\n            await retry_on_flood(bot.send_media_group(chat_id, media_docs))\n        await asyncio.sleep(1)\n\n\nasync def pagination_click(client: Client, callback: CallbackQuery):\n    pagination_id, page = map(int, callback.data.split('_'))\n    pagination = paginations[pagination_id]\n    pagination.page = page\n    await manga_click(client, callback, pagination)\n\n\nasync def full_page_click(client: Client, callback: CallbackQuery):\n    chapters_data = full_pages[callback.data]\n    for chapter_data in reversed(chapters_data):\n        try:\n            await chapter_click(client, chapter_data, callback.from_user.id)\n        except Exception as e:\n            print(e)\n        await asyncio.sleep(0.5)\n\n\nasync def favourite_click(client: Client, callback: CallbackQuery):\n    action, data = callback.data.split('_')\n    fav = action == 'fav'\n    manga = favourites[callback.data]\n    db = DB()\n    subs = await db.get(Subscription, (manga.url, str(callback.from_user.id)))\n    if not subs and fav:\n        await db.add(Subscription(url=manga.url, user_id=str(callback.from_user.id)))\n    if subs and not fav:\n        await db.erase(subs)\n    if subs and fav:\n        await callback.answer(\"You are already subscribed\", show_alert=True)\n    if not subs and not fav:\n        await callback.answer(\"You are not subscribed\", show_alert=True)\n    reply_markup = callback.message.reply_markup\n    keyboard = reply_markup.inline_keyboard\n    keyboard[0] = [InlineKeyboardButton(\n        \"Unsubscribe\" if fav else \"Subscribe\",\n        f\"{'unfav' if fav else 'fav'}_{data}\"\n    )]\n    await bot.edit_message_reply_markup(callback.from_user.id, callback.message.message_id,\n                                        InlineKeyboardMarkup(keyboard))\n    db_manga = await db.get(MangaName, manga.url)\n    if not db_manga:\n        await db.add(MangaName(url=manga.url, name=manga.name))\n\n\ndef is_pagination_data(callback: CallbackQuery):\n    data = callback.data\n    match = re.match(r'\\d+_\\d+', data)\n    if not match:\n        return False\n    pagination_id = int(data.split('_')[0])\n    if pagination_id not in paginations:\n        return False\n    pagination = paginations[pagination_id]\n    if not pagination.message:\n        return False\n    if pagination.message.chat.id != callback.from_user.id:\n        return False\n    if pagination.message.message_id != callback.message.message_id:\n        return False\n    return True\n\n\n@bot.on_callback_query()\nasync def on_callback_query(client, callback: CallbackQuery):\n    if callback.data in queries:\n        await plugin_click(client, callback)\n    elif callback.data in mangas:\n        await manga_click(client, callback)\n    elif callback.data in chapters:\n        await chapter_click(client, callback.data, callback.from_user.id)\n    elif callback.data in full_pages:\n        await full_page_click(client, callback)\n    elif callback.data in favourites:\n        await favourite_click(client, callback)\n    elif is_pagination_data(callback):\n        await pagination_click(client, callback)\n    elif callback.data in language_query:\n        await language_click(client, callback)\n    elif callback.data.startswith('options'):\n        await options_click(client, callback)\n    else:\n        await bot.answer_callback_query(callback.id, 'This is an old button, please redo the search', show_alert=True)\n        return\n    try:\n        await callback.answer()\n    except BaseException as e:\n        print(e)\n\n\nasync def remove_subscriptions(sub: str):\n    db = DB()\n\n    await db.erase_subs(sub)\n\n\nasync def update_mangas():\n    print(\"Updating mangas\")\n    db = DB()\n    subscriptions = await db.get_all(Subscription)\n    last_chapters = await db.get_all(LastChapter)\n    manga_names = await db.get_all(MangaName)\n\n    subs_dictionary = dict()\n    chapters_dictionary = dict()\n    url_client_dictionary = dict()\n    client_url_dictionary = {client: set() for client in plugins.values()}\n    manga_dict = dict()\n\n    for subscription in subscriptions:\n        if subscription.url not in subs_dictionary:\n            subs_dictionary[subscription.url] = []\n        subs_dictionary[subscription.url].append(subscription.user_id)\n\n    for last_chapter in last_chapters:\n        chapters_dictionary[last_chapter.url] = last_chapter\n\n    for manga in manga_names:\n        manga_dict[manga.url] = manga\n\n    for url in subs_dictionary:\n        for ident, client in plugins.items():\n            if ident in subsPaused:\n                continue\n            if await client.contains_url(url):\n                url_client_dictionary[url] = client\n                client_url_dictionary[client].add(url)\n\n    for client, urls in client_url_dictionary.items():\n        # print('')\n        # print(f'Updating {client.name}')\n        # print(f'Urls:\\t{list(urls)}')\n        # new_urls = [url for url in urls if not chapters_dictionary.get(url)]\n        # print(f'New Urls:\\t{new_urls}')\n        to_check = [chapters_dictionary[url] for url in urls if chapters_dictionary.get(url)]\n        if len(to_check) == 0:\n            continue\n        try:\n            updated, not_updated = await client.check_updated_urls(to_check)\n        except BaseException as e:\n            print(f\"Error while checking updates for site: {client.name}, err: \", e)\n            not_updated = list(urls)\n        for url in not_updated:\n            del url_client_dictionary[url]\n        # print(f'Updated:\\t{list(updated)}')\n        # print(f'Not Updated:\\t{list(not_updated)}')\n\n    updated = dict()\n\n    for url, client in url_client_dictionary.items():\n        try:\n            if url not in manga_dict:\n                continue\n            manga_name = manga_dict[url].name\n            if url not in chapters_dictionary:\n                agen = client.iter_chapters(url, manga_name)\n                last_chapter = await anext(agen)\n                await db.add(LastChapter(url=url, chapter_url=last_chapter.url))\n                await asyncio.sleep(10)\n            else:\n                last_chapter = chapters_dictionary[url]\n                new_chapters: List[MangaChapter] = []\n                counter = 0\n                async for chapter in client.iter_chapters(url, manga_name):\n                    if chapter.url == last_chapter.chapter_url:\n                        break\n                    new_chapters.append(chapter)\n                    counter += 1\n                    if counter == 20:\n                        break\n                if new_chapters:\n                    last_chapter.chapter_url = new_chapters[0].url\n                    await db.add(last_chapter)\n                    updated[url] = list(reversed(new_chapters))\n                    for chapter in new_chapters:\n                        if chapter.unique() not in chapters:\n                            chapters[chapter.unique()] = chapter\n                await asyncio.sleep(1)\n        except BaseException as e:\n            print(f'An exception occurred getting new chapters for url {url}: {e}')\n\n    blocked = set()\n    for url, chapter_list in updated.items():\n        for chapter in chapter_list:\n            print(f'{chapter.manga.name} - {chapter.name}')\n            for sub in subs_dictionary[url]:\n                if sub in blocked:\n                    continue\n                try:\n                    await chapter_click(bot, chapter.unique(), int(sub))\n                except pyrogram.errors.UserIsBlocked:\n                    print(f'User {sub} blocked the bot')\n                    await remove_subscriptions(sub)\n                    blocked.add(sub)\n                except BaseException as e:\n                    print(f'An exception occurred sending new chapter: {e}')\n                await asyncio.sleep(0.5)\n            await asyncio.sleep(1)\n\n\nasync def manga_updater():\n    minutes = 5\n    while True:\n        wait_time = minutes * 60\n        try:\n            start = dt.datetime.now()\n            await update_mangas()\n            elapsed = dt.datetime.now() - start\n            wait_time = max((dt.timedelta(seconds=wait_time) - elapsed).total_seconds(), 0)\n            print(f'Time elapsed updating mangas: {elapsed}, waiting for {wait_time}')\n        except BaseException as e:\n            print(f'An exception occurred during chapters update: {e}')\n        if wait_time:\n            await asyncio.sleep(wait_time)\n"},"\/tools\/flood.py":{"changes":[{"diff":"\n import asyncio\n+from typing import Callable, Awaitable, Any\n \n import pyrogram.errors\n \n \n # retries an async awaitable as long as it raises FloodWait, and waits for err.x time\n-async def retry_on_flood(awaitable):\n-    while True:\n-        try:\n-            return await awaitable\n-        except pyrogram.errors.FloodWait as err:\n-            print(f'FloodWait, waiting {err.x} seconds')\n-            await asyncio.sleep(err.x)\n-            continue\n-        except pyrogram.errors.RPCError as err:\n-            if err.MESSAGE == 'FloodWait':\n+def retry_on_flood(function: Callable[[Any], Awaitable]):\n+    async def wrapper(*args, **kwargs):\n+        while True:\n+            try:\n+                return await function(*args, **kwargs)\n+            except pyrogram.errors.FloodWait as err:\n+                print(f'FloodWait, waiting {err.x} seconds')\n                 await asyncio.sleep(err.x)\n                 continue\n-            else:\n+            except pyrogram.errors.RPCError as err:\n+                if err.MESSAGE == 'FloodWait':\n+                    await asyncio.sleep(err.x)\n+                    continue\n+                else:\n+                    raise err\n+            except Exception as err:\n                 raise err\n-        except Exception as err:\n-            raise err\n+    return wrapper\n","add":16,"remove":13,"filename":"\/tools\/flood.py","badparts":["async def retry_on_flood(awaitable):","    while True:","        try:","            return await awaitable","        except pyrogram.errors.FloodWait as err:","            print(f'FloodWait, waiting {err.x} seconds')","            await asyncio.sleep(err.x)","            continue","        except pyrogram.errors.RPCError as err:","            if err.MESSAGE == 'FloodWait':","            else:","        except Exception as err:","            raise err"],"goodparts":["from typing import Callable, Awaitable, Any","def retry_on_flood(function: Callable[[Any], Awaitable]):","    async def wrapper(*args, **kwargs):","        while True:","            try:","                return await function(*args, **kwargs)","            except pyrogram.errors.FloodWait as err:","                print(f'FloodWait, waiting {err.x} seconds')","            except pyrogram.errors.RPCError as err:","                if err.MESSAGE == 'FloodWait':","                    await asyncio.sleep(err.x)","                    continue","                else:","                    raise err","            except Exception as err:","    return wrapper"]}],"source":"\nimport asyncio import pyrogram.errors async def retry_on_flood(awaitable): while True: try: return await awaitable except pyrogram.errors.FloodWait as err: print(f'FloodWait, waiting{err.x} seconds') await asyncio.sleep(err.x) continue except pyrogram.errors.RPCError as err: if err.MESSAGE=='FloodWait': await asyncio.sleep(err.x) continue else: raise err except Exception as err: raise err ","sourceWithComments":"import asyncio\n\nimport pyrogram.errors\n\n\n# retries an async awaitable as long as it raises FloodWait, and waits for err.x time\nasync def retry_on_flood(awaitable):\n    while True:\n        try:\n            return await awaitable\n        except pyrogram.errors.FloodWait as err:\n            print(f'FloodWait, waiting {err.x} seconds')\n            await asyncio.sleep(err.x)\n            continue\n        except pyrogram.errors.RPCError as err:\n            if err.MESSAGE == 'FloodWait':\n                await asyncio.sleep(err.x)\n                continue\n            else:\n                raise err\n        except Exception as err:\n            raise err\n"}},"msg":"Fix flood handling"}},"https:\/\/github.com\/DurhamARC\/ManyFEWS":{"20e3ac6331bf1cd2464baf49326ad0d193ce2a6c":{"url":"https:\/\/api.github.com\/repos\/DurhamARC\/ManyFEWS\/commits\/20e3ac6331bf1cd2464baf49326ad0d193ce2a6c","html_url":"https:\/\/github.com\/DurhamARC\/ManyFEWS\/commit\/20e3ac6331bf1cd2464baf49326ad0d193ce2a6c","sha":"20e3ac6331bf1cd2464baf49326ad0d193ce2a6c","keyword":"flooding fix","diff":"diff --git a\/manyfews\/calculations\/flood_risk.py b\/manyfews\/calculations\/flood_risk.py\nindex 13e642d6..142965d1 100644\n--- a\/manyfews\/calculations\/flood_risk.py\n+++ b\/manyfews\/calculations\/flood_risk.py\n@@ -38,6 +38,12 @@ def run_all_flood_models():\n         prediction_date=latest_prediction_date,\n         forecast_time__lte=today + timedelta(days=16),\n     )\n+    # Raise an error and stop the program if outputs_by_time is empty\n+    if len(outputs_by_time) == 0:\n+        raise Exception(\n+            \"sorry, no River Flow result, please check the task dailyModelUpdate run properly.\"\n+        )\n+\n     logger.info(f\"Found {len(outputs_by_time)} sets of output data.\")\n     for output in outputs_by_time:\n         run_flood_model_for_time.delay(latest_prediction_date, output.forecast_time)\n@@ -58,6 +64,11 @@ def run_flood_model_for_time(prediction_date, forecast_time):\n     latest_model_id = ModelVersion.get_current_id()\n     params = FloodModelParameters.objects.filter(model_version_id=latest_model_id).all()\n \n+    if not len(params):\n+        raise Exception(\n+            \"There are no catchment model parameters populated in the database\"\n+        )\n+\n     # FIXME: this is slow (both with celery in batches of 1000, and running in series\n     # (took several hours for 1 time))\n     predict_depths(forecast_time, [p.id for p in params], flow_values)\n@@ -113,10 +124,23 @@ def predict_depths(forecast_time, param_ids, flow_values):\n def predict_depth(flow_values, param):\n     beta_values = [getattr(param, f\"beta{i}\", 0) for i in range(12)]\n     beta_values = [0 if b is None else b for b in beta_values]\n-    polynomial = np.polynomial.Polynomial(beta_values)\n-    depths = polynomial(flow_values)\n+\n+    if beta_values[3] > flow_values.all():\n+        depths = 0\n+        logger.info(\n+            f\"depths are set to Zero, stop higher-order polynomials create flood water at low flows.\"\n+        )\n+\n+    else:\n+        beta_values[3] = 0\n+        polynomial = np.polynomial.Polynomial(beta_values)\n+        depths = polynomial(flow_values)\n     depths[depths < 0] = 0\n \n+    # polynomial = np.polynomial.Polynomial(beta_values)\n+    # depths = polynomial(flow_values)\n+    # depths[depths < 0] = 0\n+\n     # Get median and centiles\n     median = np.median(depths)\n     lower_centile = np.percentile(depths, 10)\n","message":"","files":{"\/manyfews\/calculations\/flood_risk.py":{"changes":[{"diff":"\n def predict_depth(flow_values, param):\n     beta_values = [getattr(param, f\"beta{i}\", 0) for i in range(12)]\n     beta_values = [0 if b is None else b for b in beta_values]\n-    polynomial = np.polynomial.Polynomial(beta_values)\n-    depths = polynomial(flow_values)\n+\n+    if beta_values[3] > flow_values.all():\n+        depths = 0\n+        logger.info(\n+            f\"depths are set to Zero, stop higher-order polynomials create flood water at low flows.\"\n+        )\n+\n+    else:\n+        beta_values[3] = 0\n+        polynomial = np.polynomial.Polynomial(beta_values)\n+        depths = polynomial(flow_values)\n     depths[depths < 0] = 0\n \n+    # polynomial = np.polynomial.Polynomial(beta_values)\n+    # depths = polynomial(flow_values)\n+    # depths[depths < 0] = 0\n+\n     # Get median and centiles\n     median = np.median(depths)\n     lower_centile = np.percentile(depths, 10)\n","add":15,"remove":2,"filename":"\/manyfews\/calculations\/flood_risk.py","badparts":["    polynomial = np.polynomial.Polynomial(beta_values)","    depths = polynomial(flow_values)"],"goodparts":["    if beta_values[3] > flow_values.all():","        depths = 0","        logger.info(","            f\"depths are set to Zero, stop higher-order polynomials create flood water at low flows.\"","        )","    else:","        beta_values[3] = 0","        polynomial = np.polynomial.Polynomial(beta_values)","        depths = polynomial(flow_values)"]}],"source":"\nfrom datetime import timedelta import logging from celery import Celery, shared_task from django.conf import settings from django.contrib.gis.db.models import Extent from django.contrib.gis.geos import Polygon from django.db.models import Avg, Count, Max from django.utils import timezone import numpy as np from.models import( AggregatedDepthPrediction, DepthPrediction, FloodModelParameters, ModelVersion, PercentageFloodRisk, RiverFlowCalculationOutput, ) logger=logging.getLogger(__name__) def run_all_flood_models(): date_aggregation=RiverFlowCalculationOutput.objects.aggregate( Max(\"prediction_date\") ) latest_prediction_date=date_aggregation[\"prediction_date__max\"] today=timezone.now().replace(hour=0, minute=0, second=0, microsecond=0) outputs_by_time=RiverFlowCalculationOutput.objects.filter( prediction_date=latest_prediction_date, forecast_time__lte=today +timedelta(days=16), ) logger.info(f\"Found{len(outputs_by_time)} sets of output data.\") for output in outputs_by_time: run_flood_model_for_time.delay(latest_prediction_date, output.forecast_time) @shared_task(name=\"Run flood model for time\") def run_flood_model_for_time(prediction_date, forecast_time): logger.info(f\"Running flood model for{forecast_time}\") output=RiverFlowCalculationOutput.objects.filter( prediction_date=prediction_date, forecast_time=forecast_time ).first() flow_values_iter=output.riverflowprediction_set.values_list( \"river_flow\", flat=True ) flow_values=np.fromiter(flow_values_iter, np.dtype(\"float_\")) logger.info(f\"Got river flow values:{flow_values}\") latest_model_id=ModelVersion.get_current_id() params=FloodModelParameters.objects.filter(model_version_id=latest_model_id).all() predict_depths(forecast_time,[p.id for p in params], flow_values) aggregate_flood_models(forecast_time) @shared_task(name=\"Predict depths for batch of cells\") def predict_depths(forecast_time, param_ids, flow_values): for i, param_id in enumerate(param_ids): param=FloodModelParameters.objects.get(id=param_id) ( lower_centile, mid_lower_centile, median, upper_centile, )=predict_depth(flow_values, param) prediction=DepthPrediction.objects.filter( date=forecast_time, parameters_id=param_id ).first() if upper_centile > 0: if not prediction: prediction=DepthPrediction(date=forecast_time, parameters_id=param_id) prediction.model_version=param.model_version prediction.median_depth=median prediction.lower_centile=lower_centile prediction.mid_lower_centile=mid_lower_centile prediction.upper_centile=upper_centile prediction.save() else: if prediction: prediction.delete() if i % 1000==0: logger.info( f\"Calculated{i} of{len(param_ids)} pixels({(i \/ len(param_ids)) * 100:.1f}%)\" ) def predict_depth(flow_values, param): beta_values=[getattr(param, f\"beta{i}\", 0) for i in range(12)] beta_values=[0 if b is None else b for b in beta_values] polynomial=np.polynomial.Polynomial(beta_values) depths=polynomial(flow_values) depths[depths < 0]=0 median=np.median(depths) lower_centile=np.percentile(depths, 10) mid_lower_centile=np.percentile(depths, 30) upper_centile=np.percentile(depths, 90) return lower_centile, mid_lower_centile, median, upper_centile @shared_task(name=\"aggregate_flood_models\") def aggregate_flood_models(date): logger.info(f\"Aggregating flood model results for responsive tiling\") current_model_version_id=ModelVersion.get_current_id() result=DepthPrediction.objects.filter( date=date, model_version_id=current_model_version_id ).aggregate(Extent(\"parameters__bounding_box\")) extent=result[\"parameters__bounding_box__extent\"] if extent is None: raise Exception( \"Extent is None \u2013 no bounding box defined in Flood Model Parameters!\" ) for i in[32, 64, 128, 256]: aggregate_flood_models_by_size.delay(date, current_model_version_id, extent, i) @shared_task(name=\"aggregate_flood_models_by_size\") def aggregate_flood_models_by_size(date, model_version_id, extent, i): logger.info(f\"Aggregating for date{date} level{i}\") total_width=extent[2] -extent[0] total_height=extent[3] -extent[1] block_size=min(total_height, total_width) \/ i x=extent[0] y=extent[1] while y < extent[3]: while x < extent[2]: x_max=x +block_size y_max=y +block_size new_bb=Polygon.from_bbox((x, y, x_max, y_max)) q=DepthPrediction.objects.filter( date=date, parameters__bounding_box__within=new_bb, model_version_id=model_version_id, ) values=q.aggregate( Avg(\"median_depth\"), Avg(\"lower_centile\"), Avg(\"mid_lower_centile\"), Avg(\"upper_centile\"), ) if values[\"median_depth__avg\"]: agg=AggregatedDepthPrediction.objects.filter( date=date, bounding_box=new_bb ).first() if not agg: agg=AggregatedDepthPrediction(date=date, bounding_box=new_bb) agg.model_version_id=model_version_id agg.median_depth=values[\"median_depth__avg\"] agg.lower_centile=values[\"lower_centile__avg\"] agg.mid_lower_centile=values[\"mid_lower_centile__avg\"] agg.upper_centile=values[\"upper_centile__avg\"] agg.aggregation_level=i agg.save() x +=block_size x=extent[0] y +=block_size @shared_task(name=\"calculate_risk_percentages\") def calculate_risk_percentages(): today=timezone.now().replace(hour=0, minute=0, second=0, microsecond=0) prediction_counts=( DepthPrediction.objects.filter( date__gte=today, median_depth__gt=0, ) .values(\"date\") .annotate(non_zero_count=Count(\"median_depth\")) .all() ) logger.info(\"Prediction counts total:{}\".format(len(prediction_counts))) for p in prediction_counts: n=p[\"non_zero_count\"] risk=0 if n < settings.CHANNEL_CELL_COUNT: risk=0 elif n > settings.LARGE_FLOOD_COUNT: risk=1 else: risk=n \/(settings.LARGE_FLOOD_COUNT -settings.CHANNEL_CELL_COUNT) PercentageFloodRisk.objects.filter(date=p[\"date\"]).delete() PercentageFloodRisk(date=p[\"date\"], risk=risk).save() ","sourceWithComments":"from datetime import timedelta\nimport logging\n\nfrom celery import Celery, shared_task\nfrom django.conf import settings\nfrom django.contrib.gis.db.models import Extent\nfrom django.contrib.gis.geos import Polygon\nfrom django.db.models import Avg, Count, Max\nfrom django.utils import timezone\nimport numpy as np\n\nfrom .models import (\n    AggregatedDepthPrediction,\n    DepthPrediction,\n    FloodModelParameters,\n    ModelVersion,\n    PercentageFloodRisk,\n    RiverFlowCalculationOutput,\n)\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef run_all_flood_models():\n    # Run flood model over latest outputs from river flow\n\n    # First find latest prediction to run model\n    date_aggregation = RiverFlowCalculationOutput.objects.aggregate(\n        Max(\"prediction_date\")\n    )\n    latest_prediction_date = date_aggregation[\"prediction_date__max\"]\n\n    today = timezone.now().replace(hour=0, minute=0, second=0, microsecond=0)\n\n    # Next find all calculations with that date, in the next 16 days\n    outputs_by_time = RiverFlowCalculationOutput.objects.filter(\n        prediction_date=latest_prediction_date,\n        forecast_time__lte=today + timedelta(days=16),\n    )\n    logger.info(f\"Found {len(outputs_by_time)} sets of output data.\")\n    for output in outputs_by_time:\n        run_flood_model_for_time.delay(latest_prediction_date, output.forecast_time)\n\n\n@shared_task(name=\"Run flood model for time\")\ndef run_flood_model_for_time(prediction_date, forecast_time):\n    logger.info(f\"Running flood model for {forecast_time}\")\n    output = RiverFlowCalculationOutput.objects.filter(\n        prediction_date=prediction_date, forecast_time=forecast_time\n    ).first()\n    flow_values_iter = output.riverflowprediction_set.values_list(\n        \"river_flow\", flat=True\n    )\n    flow_values = np.fromiter(flow_values_iter, np.dtype(\"float_\"))\n    logger.info(f\"Got river flow values: {flow_values}\")\n\n    latest_model_id = ModelVersion.get_current_id()\n    params = FloodModelParameters.objects.filter(model_version_id=latest_model_id).all()\n\n    # FIXME: this is slow (both with celery in batches of 1000, and running in series\n    # (took several hours for 1 time))\n    predict_depths(forecast_time, [p.id for p in params], flow_values)\n    aggregate_flood_models(forecast_time)\n    # batch_size = 1000\n    # i = 0\n    #\n    # while i < len(params):\n    #     end = min(i + batch_size, len(params))\n    #     param_ids = [p.id for p in params[i:end]]\n    #     predict_depths.delay(forecast_time, param_ids, flow_values)\n    #     i += batch_size\n    # TODO: join results to call aggregate_flood_models\n\n\n@shared_task(name=\"Predict depths for batch of cells\")\ndef predict_depths(forecast_time, param_ids, flow_values):\n    for i, param_id in enumerate(param_ids):\n        param = FloodModelParameters.objects.get(id=param_id)\n\n        (\n            lower_centile,\n            mid_lower_centile,\n            median,\n            upper_centile,\n        ) = predict_depth(flow_values, param)\n\n        # Replace current object if there is one\n        prediction = DepthPrediction.objects.filter(\n            date=forecast_time, parameters_id=param_id\n        ).first()\n\n        if upper_centile > 0:\n            if not prediction:\n                prediction = DepthPrediction(date=forecast_time, parameters_id=param_id)\n\n            prediction.model_version = param.model_version\n            prediction.median_depth = median\n            prediction.lower_centile = lower_centile\n            prediction.mid_lower_centile = mid_lower_centile\n            prediction.upper_centile = upper_centile\n            prediction.save()\n        else:\n            if prediction:\n                prediction.delete()\n\n        if i % 1000 == 0:\n            logger.info(\n                f\"Calculated {i} of {len(param_ids)} pixels ({(i \/ len(param_ids)) * 100 :.1f}%)\"\n            )\n\n\ndef predict_depth(flow_values, param):\n    beta_values = [getattr(param, f\"beta{i}\", 0) for i in range(12)]\n    beta_values = [0 if b is None else b for b in beta_values]\n    polynomial = np.polynomial.Polynomial(beta_values)\n    depths = polynomial(flow_values)\n    depths[depths < 0] = 0\n\n    # Get median and centiles\n    median = np.median(depths)\n    lower_centile = np.percentile(depths, 10)\n    mid_lower_centile = np.percentile(depths, 30)\n    upper_centile = np.percentile(depths, 90)\n\n    return lower_centile, mid_lower_centile, median, upper_centile\n\n\n@shared_task(name=\"aggregate_flood_models\")\ndef aggregate_flood_models(date):\n    logger.info(f\"Aggregating flood model results for responsive tiling\")\n    current_model_version_id = ModelVersion.get_current_id()\n    result = DepthPrediction.objects.filter(\n        date=date, model_version_id=current_model_version_id\n    ).aggregate(Extent(\"parameters__bounding_box\"))\n\n    extent = result[\"parameters__bounding_box__extent\"]\n\n    if extent is None:\n        raise Exception(\n            \"Extent is None \u2013 no bounding box defined in Flood Model Parameters!\"\n        )\n\n    for i in [32, 64, 128, 256]:\n        aggregate_flood_models_by_size.delay(date, current_model_version_id, extent, i)\n\n\n@shared_task(name=\"aggregate_flood_models_by_size\")\ndef aggregate_flood_models_by_size(date, model_version_id, extent, i):\n    logger.info(f\"Aggregating for date {date} level {i}\")\n    total_width = extent[2] - extent[0]\n    total_height = extent[3] - extent[1]\n    block_size = min(total_height, total_width) \/ i\n\n    x = extent[0]\n    y = extent[1]\n\n    while y < extent[3]:\n        while x < extent[2]:\n            x_max = x + block_size\n            y_max = y + block_size\n            new_bb = Polygon.from_bbox((x, y, x_max, y_max))\n\n            q = DepthPrediction.objects.filter(\n                date=date,\n                parameters__bounding_box__within=new_bb,\n                model_version_id=model_version_id,\n            )\n            values = q.aggregate(\n                Avg(\"median_depth\"),\n                Avg(\"lower_centile\"),\n                Avg(\"mid_lower_centile\"),\n                Avg(\"upper_centile\"),\n            )\n\n            if values[\"median_depth__avg\"]:\n                agg = AggregatedDepthPrediction.objects.filter(\n                    date=date, bounding_box=new_bb\n                ).first()\n\n                if not agg:\n                    agg = AggregatedDepthPrediction(date=date, bounding_box=new_bb)\n\n                agg.model_version_id = model_version_id\n                agg.median_depth = values[\"median_depth__avg\"]\n                agg.lower_centile = values[\"lower_centile__avg\"]\n                agg.mid_lower_centile = values[\"mid_lower_centile__avg\"]\n                agg.upper_centile = values[\"upper_centile__avg\"]\n                agg.aggregation_level = i\n                agg.save()\n\n            x += block_size\n\n        x = extent[0]\n        y += block_size\n\n\n@shared_task(name=\"calculate_risk_percentages\")\ndef calculate_risk_percentages():\n    # Convert aggregated depths to % risk based on number of cells\n    # with non-zero median depth\n    today = timezone.now().replace(hour=0, minute=0, second=0, microsecond=0)\n    prediction_counts = (\n        DepthPrediction.objects.filter(\n            date__gte=today,\n            median_depth__gt=0,\n        )\n        .values(\"date\")\n        .annotate(non_zero_count=Count(\"median_depth\"))\n        .all()\n    )\n\n    logger.info(\"Prediction counts total: {}\".format(len(prediction_counts)))\n\n    for p in prediction_counts:\n        n = p[\"non_zero_count\"]\n        risk = 0\n        # No risk if number of cells with 'flood' is less than number of cells\n        # that are in the river channel\n        if n < settings.CHANNEL_CELL_COUNT:\n            risk = 0\n        elif n > settings.LARGE_FLOOD_COUNT:\n            risk = 1\n        else:\n            risk = n \/ (settings.LARGE_FLOOD_COUNT - settings.CHANNEL_CELL_COUNT)\n\n        # Delete existing row for this date before creating new\n        PercentageFloodRisk.objects.filter(date=p[\"date\"]).delete()\n        PercentageFloodRisk(date=p[\"date\"], risk=risk).save()\n"}},"msg":"1. fixed a bug of depth calculation. (This stops an issue with higher order polynominals create flood water at low flows) 2. Add check to fix #93."},"6e38d37feb55f94b9598c91106e3a870e1804646":{"url":"https:\/\/api.github.com\/repos\/DurhamARC\/ManyFEWS\/commits\/6e38d37feb55f94b9598c91106e3a870e1804646","html_url":"https:\/\/github.com\/DurhamARC\/ManyFEWS\/commit\/6e38d37feb55f94b9598c91106e3a870e1804646","message":"1. fixed a bug of depth calculation. (This stops an issue with higher order polynominals create flood water at low flows) 2. Add check to fix #93.","sha":"6e38d37feb55f94b9598c91106e3a870e1804646","keyword":"flooding fix","diff":"diff --git a\/manyfews\/calculations\/flood_risk.py b\/manyfews\/calculations\/flood_risk.py\nindex 13e642d6..142965d1 100644\n--- a\/manyfews\/calculations\/flood_risk.py\n+++ b\/manyfews\/calculations\/flood_risk.py\n@@ -38,6 +38,12 @@ def run_all_flood_models():\n         prediction_date=latest_prediction_date,\n         forecast_time__lte=today + timedelta(days=16),\n     )\n+    # Raise an error and stop the program if outputs_by_time is empty\n+    if len(outputs_by_time) == 0:\n+        raise Exception(\n+            \"sorry, no River Flow result, please check the task dailyModelUpdate run properly.\"\n+        )\n+\n     logger.info(f\"Found {len(outputs_by_time)} sets of output data.\")\n     for output in outputs_by_time:\n         run_flood_model_for_time.delay(latest_prediction_date, output.forecast_time)\n@@ -58,6 +64,11 @@ def run_flood_model_for_time(prediction_date, forecast_time):\n     latest_model_id = ModelVersion.get_current_id()\n     params = FloodModelParameters.objects.filter(model_version_id=latest_model_id).all()\n \n+    if not len(params):\n+        raise Exception(\n+            \"There are no catchment model parameters populated in the database\"\n+        )\n+\n     # FIXME: this is slow (both with celery in batches of 1000, and running in series\n     # (took several hours for 1 time))\n     predict_depths(forecast_time, [p.id for p in params], flow_values)\n@@ -113,10 +124,23 @@ def predict_depths(forecast_time, param_ids, flow_values):\n def predict_depth(flow_values, param):\n     beta_values = [getattr(param, f\"beta{i}\", 0) for i in range(12)]\n     beta_values = [0 if b is None else b for b in beta_values]\n-    polynomial = np.polynomial.Polynomial(beta_values)\n-    depths = polynomial(flow_values)\n+\n+    if beta_values[3] > flow_values.all():\n+        depths = 0\n+        logger.info(\n+            f\"depths are set to Zero, stop higher-order polynomials create flood water at low flows.\"\n+        )\n+\n+    else:\n+        beta_values[3] = 0\n+        polynomial = np.polynomial.Polynomial(beta_values)\n+        depths = polynomial(flow_values)\n     depths[depths < 0] = 0\n \n+    # polynomial = np.polynomial.Polynomial(beta_values)\n+    # depths = polynomial(flow_values)\n+    # depths[depths < 0] = 0\n+\n     # Get median and centiles\n     median = np.median(depths)\n     lower_centile = np.percentile(depths, 10)\n","files":{"\/manyfews\/calculations\/flood_risk.py":{"changes":[{"diff":"\n def predict_depth(flow_values, param):\n     beta_values = [getattr(param, f\"beta{i}\", 0) for i in range(12)]\n     beta_values = [0 if b is None else b for b in beta_values]\n-    polynomial = np.polynomial.Polynomial(beta_values)\n-    depths = polynomial(flow_values)\n+\n+    if beta_values[3] > flow_values.all():\n+        depths = 0\n+        logger.info(\n+            f\"depths are set to Zero, stop higher-order polynomials create flood water at low flows.\"\n+        )\n+\n+    else:\n+        beta_values[3] = 0\n+        polynomial = np.polynomial.Polynomial(beta_values)\n+        depths = polynomial(flow_values)\n     depths[depths < 0] = 0\n \n+    # polynomial = np.polynomial.Polynomial(beta_values)\n+    # depths = polynomial(flow_values)\n+    # depths[depths < 0] = 0\n+\n     # Get median and centiles\n     median = np.median(depths)\n     lower_centile = np.percentile(depths, 10)\n","add":15,"remove":2,"filename":"\/manyfews\/calculations\/flood_risk.py","badparts":["    polynomial = np.polynomial.Polynomial(beta_values)","    depths = polynomial(flow_values)"],"goodparts":["    if beta_values[3] > flow_values.all():","        depths = 0","        logger.info(","            f\"depths are set to Zero, stop higher-order polynomials create flood water at low flows.\"","        )","    else:","        beta_values[3] = 0","        polynomial = np.polynomial.Polynomial(beta_values)","        depths = polynomial(flow_values)"]}],"source":"\nfrom datetime import timedelta import logging from celery import Celery, shared_task from django.conf import settings from django.contrib.gis.db.models import Extent from django.contrib.gis.geos import Polygon from django.db.models import Avg, Count, Max from django.utils import timezone import numpy as np from.models import( AggregatedDepthPrediction, DepthPrediction, FloodModelParameters, ModelVersion, PercentageFloodRisk, RiverFlowCalculationOutput, ) logger=logging.getLogger(__name__) def run_all_flood_models(): date_aggregation=RiverFlowCalculationOutput.objects.aggregate( Max(\"prediction_date\") ) latest_prediction_date=date_aggregation[\"prediction_date__max\"] today=timezone.now().replace(hour=0, minute=0, second=0, microsecond=0) outputs_by_time=RiverFlowCalculationOutput.objects.filter( prediction_date=latest_prediction_date, forecast_time__lte=today +timedelta(days=16), ) logger.info(f\"Found{len(outputs_by_time)} sets of output data.\") for output in outputs_by_time: run_flood_model_for_time.delay(latest_prediction_date, output.forecast_time) @shared_task(name=\"Run flood model for time\") def run_flood_model_for_time(prediction_date, forecast_time): logger.info(f\"Running flood model for{forecast_time}\") output=RiverFlowCalculationOutput.objects.filter( prediction_date=prediction_date, forecast_time=forecast_time ).first() flow_values_iter=output.riverflowprediction_set.values_list( \"river_flow\", flat=True ) flow_values=np.fromiter(flow_values_iter, np.dtype(\"float_\")) logger.info(f\"Got river flow values:{flow_values}\") latest_model_id=ModelVersion.get_current_id() params=FloodModelParameters.objects.filter(model_version_id=latest_model_id).all() predict_depths(forecast_time,[p.id for p in params], flow_values) aggregate_flood_models(forecast_time) @shared_task(name=\"Predict depths for batch of cells\") def predict_depths(forecast_time, param_ids, flow_values): for i, param_id in enumerate(param_ids): param=FloodModelParameters.objects.get(id=param_id) ( lower_centile, mid_lower_centile, median, upper_centile, )=predict_depth(flow_values, param) prediction=DepthPrediction.objects.filter( date=forecast_time, parameters_id=param_id ).first() if upper_centile > 0: if not prediction: prediction=DepthPrediction(date=forecast_time, parameters_id=param_id) prediction.model_version=param.model_version prediction.median_depth=median prediction.lower_centile=lower_centile prediction.mid_lower_centile=mid_lower_centile prediction.upper_centile=upper_centile prediction.save() else: if prediction: prediction.delete() if i % 1000==0: logger.info( f\"Calculated{i} of{len(param_ids)} pixels({(i \/ len(param_ids)) * 100:.1f}%)\" ) def predict_depth(flow_values, param): beta_values=[getattr(param, f\"beta{i}\", 0) for i in range(12)] beta_values=[0 if b is None else b for b in beta_values] polynomial=np.polynomial.Polynomial(beta_values) depths=polynomial(flow_values) depths[depths < 0]=0 median=np.median(depths) lower_centile=np.percentile(depths, 10) mid_lower_centile=np.percentile(depths, 30) upper_centile=np.percentile(depths, 90) return lower_centile, mid_lower_centile, median, upper_centile @shared_task(name=\"aggregate_flood_models\") def aggregate_flood_models(date): logger.info(f\"Aggregating flood model results for responsive tiling\") current_model_version_id=ModelVersion.get_current_id() result=DepthPrediction.objects.filter( date=date, model_version_id=current_model_version_id ).aggregate(Extent(\"parameters__bounding_box\")) extent=result[\"parameters__bounding_box__extent\"] if extent is None: raise Exception( \"Extent is None \u2013 no bounding box defined in Flood Model Parameters!\" ) for i in[32, 64, 128, 256]: aggregate_flood_models_by_size.delay(date, current_model_version_id, extent, i) @shared_task(name=\"aggregate_flood_models_by_size\") def aggregate_flood_models_by_size(date, model_version_id, extent, i): logger.info(f\"Aggregating for date{date} level{i}\") total_width=extent[2] -extent[0] total_height=extent[3] -extent[1] block_size=min(total_height, total_width) \/ i x=extent[0] y=extent[1] while y < extent[3]: while x < extent[2]: x_max=x +block_size y_max=y +block_size new_bb=Polygon.from_bbox((x, y, x_max, y_max)) q=DepthPrediction.objects.filter( date=date, parameters__bounding_box__within=new_bb, model_version_id=model_version_id, ) values=q.aggregate( Avg(\"median_depth\"), Avg(\"lower_centile\"), Avg(\"mid_lower_centile\"), Avg(\"upper_centile\"), ) if values[\"median_depth__avg\"]: agg=AggregatedDepthPrediction.objects.filter( date=date, bounding_box=new_bb ).first() if not agg: agg=AggregatedDepthPrediction(date=date, bounding_box=new_bb) agg.model_version_id=model_version_id agg.median_depth=values[\"median_depth__avg\"] agg.lower_centile=values[\"lower_centile__avg\"] agg.mid_lower_centile=values[\"mid_lower_centile__avg\"] agg.upper_centile=values[\"upper_centile__avg\"] agg.aggregation_level=i agg.save() x +=block_size x=extent[0] y +=block_size @shared_task(name=\"calculate_risk_percentages\") def calculate_risk_percentages(): today=timezone.now().replace(hour=0, minute=0, second=0, microsecond=0) prediction_counts=( DepthPrediction.objects.filter( date__gte=today, median_depth__gt=0, ) .values(\"date\") .annotate(non_zero_count=Count(\"median_depth\")) .all() ) logger.info(\"Prediction counts total:{}\".format(len(prediction_counts))) for p in prediction_counts: n=p[\"non_zero_count\"] risk=0 if n < settings.CHANNEL_CELL_COUNT: risk=0 elif n > settings.LARGE_FLOOD_COUNT: risk=1 else: risk=n \/(settings.LARGE_FLOOD_COUNT -settings.CHANNEL_CELL_COUNT) PercentageFloodRisk.objects.filter(date=p[\"date\"]).delete() PercentageFloodRisk(date=p[\"date\"], risk=risk).save() ","sourceWithComments":"from datetime import timedelta\nimport logging\n\nfrom celery import Celery, shared_task\nfrom django.conf import settings\nfrom django.contrib.gis.db.models import Extent\nfrom django.contrib.gis.geos import Polygon\nfrom django.db.models import Avg, Count, Max\nfrom django.utils import timezone\nimport numpy as np\n\nfrom .models import (\n    AggregatedDepthPrediction,\n    DepthPrediction,\n    FloodModelParameters,\n    ModelVersion,\n    PercentageFloodRisk,\n    RiverFlowCalculationOutput,\n)\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef run_all_flood_models():\n    # Run flood model over latest outputs from river flow\n\n    # First find latest prediction to run model\n    date_aggregation = RiverFlowCalculationOutput.objects.aggregate(\n        Max(\"prediction_date\")\n    )\n    latest_prediction_date = date_aggregation[\"prediction_date__max\"]\n\n    today = timezone.now().replace(hour=0, minute=0, second=0, microsecond=0)\n\n    # Next find all calculations with that date, in the next 16 days\n    outputs_by_time = RiverFlowCalculationOutput.objects.filter(\n        prediction_date=latest_prediction_date,\n        forecast_time__lte=today + timedelta(days=16),\n    )\n    logger.info(f\"Found {len(outputs_by_time)} sets of output data.\")\n    for output in outputs_by_time:\n        run_flood_model_for_time.delay(latest_prediction_date, output.forecast_time)\n\n\n@shared_task(name=\"Run flood model for time\")\ndef run_flood_model_for_time(prediction_date, forecast_time):\n    logger.info(f\"Running flood model for {forecast_time}\")\n    output = RiverFlowCalculationOutput.objects.filter(\n        prediction_date=prediction_date, forecast_time=forecast_time\n    ).first()\n    flow_values_iter = output.riverflowprediction_set.values_list(\n        \"river_flow\", flat=True\n    )\n    flow_values = np.fromiter(flow_values_iter, np.dtype(\"float_\"))\n    logger.info(f\"Got river flow values: {flow_values}\")\n\n    latest_model_id = ModelVersion.get_current_id()\n    params = FloodModelParameters.objects.filter(model_version_id=latest_model_id).all()\n\n    # FIXME: this is slow (both with celery in batches of 1000, and running in series\n    # (took several hours for 1 time))\n    predict_depths(forecast_time, [p.id for p in params], flow_values)\n    aggregate_flood_models(forecast_time)\n    # batch_size = 1000\n    # i = 0\n    #\n    # while i < len(params):\n    #     end = min(i + batch_size, len(params))\n    #     param_ids = [p.id for p in params[i:end]]\n    #     predict_depths.delay(forecast_time, param_ids, flow_values)\n    #     i += batch_size\n    # TODO: join results to call aggregate_flood_models\n\n\n@shared_task(name=\"Predict depths for batch of cells\")\ndef predict_depths(forecast_time, param_ids, flow_values):\n    for i, param_id in enumerate(param_ids):\n        param = FloodModelParameters.objects.get(id=param_id)\n\n        (\n            lower_centile,\n            mid_lower_centile,\n            median,\n            upper_centile,\n        ) = predict_depth(flow_values, param)\n\n        # Replace current object if there is one\n        prediction = DepthPrediction.objects.filter(\n            date=forecast_time, parameters_id=param_id\n        ).first()\n\n        if upper_centile > 0:\n            if not prediction:\n                prediction = DepthPrediction(date=forecast_time, parameters_id=param_id)\n\n            prediction.model_version = param.model_version\n            prediction.median_depth = median\n            prediction.lower_centile = lower_centile\n            prediction.mid_lower_centile = mid_lower_centile\n            prediction.upper_centile = upper_centile\n            prediction.save()\n        else:\n            if prediction:\n                prediction.delete()\n\n        if i % 1000 == 0:\n            logger.info(\n                f\"Calculated {i} of {len(param_ids)} pixels ({(i \/ len(param_ids)) * 100 :.1f}%)\"\n            )\n\n\ndef predict_depth(flow_values, param):\n    beta_values = [getattr(param, f\"beta{i}\", 0) for i in range(12)]\n    beta_values = [0 if b is None else b for b in beta_values]\n    polynomial = np.polynomial.Polynomial(beta_values)\n    depths = polynomial(flow_values)\n    depths[depths < 0] = 0\n\n    # Get median and centiles\n    median = np.median(depths)\n    lower_centile = np.percentile(depths, 10)\n    mid_lower_centile = np.percentile(depths, 30)\n    upper_centile = np.percentile(depths, 90)\n\n    return lower_centile, mid_lower_centile, median, upper_centile\n\n\n@shared_task(name=\"aggregate_flood_models\")\ndef aggregate_flood_models(date):\n    logger.info(f\"Aggregating flood model results for responsive tiling\")\n    current_model_version_id = ModelVersion.get_current_id()\n    result = DepthPrediction.objects.filter(\n        date=date, model_version_id=current_model_version_id\n    ).aggregate(Extent(\"parameters__bounding_box\"))\n\n    extent = result[\"parameters__bounding_box__extent\"]\n\n    if extent is None:\n        raise Exception(\n            \"Extent is None \u2013 no bounding box defined in Flood Model Parameters!\"\n        )\n\n    for i in [32, 64, 128, 256]:\n        aggregate_flood_models_by_size.delay(date, current_model_version_id, extent, i)\n\n\n@shared_task(name=\"aggregate_flood_models_by_size\")\ndef aggregate_flood_models_by_size(date, model_version_id, extent, i):\n    logger.info(f\"Aggregating for date {date} level {i}\")\n    total_width = extent[2] - extent[0]\n    total_height = extent[3] - extent[1]\n    block_size = min(total_height, total_width) \/ i\n\n    x = extent[0]\n    y = extent[1]\n\n    while y < extent[3]:\n        while x < extent[2]:\n            x_max = x + block_size\n            y_max = y + block_size\n            new_bb = Polygon.from_bbox((x, y, x_max, y_max))\n\n            q = DepthPrediction.objects.filter(\n                date=date,\n                parameters__bounding_box__within=new_bb,\n                model_version_id=model_version_id,\n            )\n            values = q.aggregate(\n                Avg(\"median_depth\"),\n                Avg(\"lower_centile\"),\n                Avg(\"mid_lower_centile\"),\n                Avg(\"upper_centile\"),\n            )\n\n            if values[\"median_depth__avg\"]:\n                agg = AggregatedDepthPrediction.objects.filter(\n                    date=date, bounding_box=new_bb\n                ).first()\n\n                if not agg:\n                    agg = AggregatedDepthPrediction(date=date, bounding_box=new_bb)\n\n                agg.model_version_id = model_version_id\n                agg.median_depth = values[\"median_depth__avg\"]\n                agg.lower_centile = values[\"lower_centile__avg\"]\n                agg.mid_lower_centile = values[\"mid_lower_centile__avg\"]\n                agg.upper_centile = values[\"upper_centile__avg\"]\n                agg.aggregation_level = i\n                agg.save()\n\n            x += block_size\n\n        x = extent[0]\n        y += block_size\n\n\n@shared_task(name=\"calculate_risk_percentages\")\ndef calculate_risk_percentages():\n    # Convert aggregated depths to % risk based on number of cells\n    # with non-zero median depth\n    today = timezone.now().replace(hour=0, minute=0, second=0, microsecond=0)\n    prediction_counts = (\n        DepthPrediction.objects.filter(\n            date__gte=today,\n            median_depth__gt=0,\n        )\n        .values(\"date\")\n        .annotate(non_zero_count=Count(\"median_depth\"))\n        .all()\n    )\n\n    logger.info(\"Prediction counts total: {}\".format(len(prediction_counts)))\n\n    for p in prediction_counts:\n        n = p[\"non_zero_count\"]\n        risk = 0\n        # No risk if number of cells with 'flood' is less than number of cells\n        # that are in the river channel\n        if n < settings.CHANNEL_CELL_COUNT:\n            risk = 0\n        elif n > settings.LARGE_FLOOD_COUNT:\n            risk = 1\n        else:\n            risk = n \/ (settings.LARGE_FLOOD_COUNT - settings.CHANNEL_CELL_COUNT)\n\n        # Delete existing row for this date before creating new\n        PercentageFloodRisk.objects.filter(date=p[\"date\"]).delete()\n        PercentageFloodRisk(date=p[\"date\"], risk=risk).save()\n"}},"msg":"1. fixed a bug of depth calculation. (This stops an issue with higher order polynominals create flood water at low flows) 2. Add check to fix #93."}},"https:\/\/github.com\/fom-big-data-bike-path-quality\/fom-big-data-bike-path-quality-analytics":{"42b2633f514e16887272bcef8163750279ee0f76":{"url":"https:\/\/api.github.com\/repos\/fom-big-data-bike-path-quality\/fom-big-data-bike-path-quality-analytics\/commits\/42b2633f514e16887272bcef8163750279ee0f76","html_url":"https:\/\/github.com\/fom-big-data-bike-path-quality\/fom-big-data-bike-path-quality-analytics\/commit\/42b2633f514e16887272bcef8163750279ee0f76","message":"fix: Telegram flooding","sha":"42b2633f514e16887272bcef8163750279ee0f76","keyword":"flooding fix","diff":"diff --git a\/lib\/models\/base_model_knn_dtw\/knn_dtw_base_model.py b\/lib\/models\/base_model_knn_dtw\/knn_dtw_base_model.py\nindex e8417f0..912130a 100644\n--- a\/lib\/models\/base_model_knn_dtw\/knn_dtw_base_model.py\n+++ b\/lib\/models\/base_model_knn_dtw\/knn_dtw_base_model.py\n@@ -278,7 +278,7 @@ def validate_split(self, split_index, k_folds, train_ids, validation_ids, datafr\n                 f1_score=round(validation_f1_score, 2),\n                 cohen_kappa_score=round(validation_cohen_kappa_score, 2),\n                 matthews_correlation_coefficient=round(validation_matthews_correlation_coefficient, 2),\n-                telegram=not quiet and not dry_run\n+                telegram=not quiet and not dry_run and k == 1\n             )\n \n             validation_k_list.append(k)\n@@ -385,7 +385,7 @@ def evaluate(self, clean=False, quiet=False, dry_run=False):\n                 test_f1_score=test_f1_score,\n                 test_cohen_kappa_score=test_cohen_kappa_score,\n                 test_matthews_correlation_coefficient=test_matthews_correlation_coefficient,\n-                telegram=not quiet and not dry_run,\n+                telegram=not quiet and not dry_run and k == 1,\n                 confusion_matrix_file_name=f\"confusion_matrix_k{str(k)}\",\n             )\n \n","files":{"\/lib\/models\/base_model_knn_dtw\/knn_dtw_base_model.py":{"changes":[{"diff":"\n                 f1_score=round(validation_f1_score, 2),\n                 cohen_kappa_score=round(validation_cohen_kappa_score, 2),\n                 matthews_correlation_coefficient=round(validation_matthews_correlation_coefficient, 2),\n-                telegram=not quiet and not dry_run\n+                telegram=not quiet and not dry_run and k == 1\n             )\n \n             validation_k_list.append(k)\n","add":1,"remove":1,"filename":"\/lib\/models\/base_model_knn_dtw\/knn_dtw_base_model.py","badparts":["                telegram=not quiet and not dry_run"],"goodparts":["                telegram=not quiet and not dry_run and k == 1"]},{"diff":"\n                 test_f1_score=test_f1_score,\n                 test_cohen_kappa_score=test_cohen_kappa_score,\n                 test_matthews_correlation_coefficient=test_matthews_correlation_coefficient,\n-                telegram=not quiet and not dry_run,\n+                telegram=not quiet and not dry_run and k == 1,\n                 confusion_matrix_file_name=f\"confusion_matrix_k{str(k)}\",\n             )\n \n","add":1,"remove":1,"filename":"\/lib\/models\/base_model_knn_dtw\/knn_dtw_base_model.py","badparts":["                telegram=not quiet and not dry_run,"],"goodparts":["                telegram=not quiet and not dry_run and k == 1,"]}],"source":"\nimport os import random import warnings from datetime import datetime import numpy as np import pandas as pd import torch from confusion_matrix_plotter import ConfusionMatrixPlotter from label_encoder import LabelEncoder from model_evaluator import ModelEvaluator from model_logger import ModelLogger from model_plotter import ModelPlotter from model_preparator import ModelPreparator from sklearn.model_selection import KFold from tracking_decorator import TrackingDecorator from knn_dtw_classifier import KnnDtwClassifier warnings.filterwarnings(\"ignore\", category=DeprecationWarning) warnings.filterwarnings(\"ignore\", category=FutureWarning) warnings.filterwarnings(\"ignore\", category=RuntimeWarning) device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") torch.manual_seed(0) random.seed(0) np.random.seed(0) num_classes=LabelEncoder().num_classes() def get_confusion_matrix_dataframe(classifier, input, target, k): targets=[] predictions=[] confusion_matrix=np.zeros((num_classes, num_classes)) prediction, probability=classifier.predict(input, k) targets.extend(target.astype(int).tolist()) predictions.extend(prediction.astype(int).tolist()) for t, p in zip(target.astype(int), prediction.astype(int)): confusion_matrix[t, p] +=1 confusion_matrix_dataframe=pd.DataFrame(confusion_matrix, index=LabelEncoder().classes, columns=LabelEncoder().classes).astype(\"int64\") used_columns=(confusion_matrix_dataframe !=0).any(axis=0).where(lambda x: x==True).dropna().keys().tolist() used_rows=(confusion_matrix_dataframe !=0).any(axis=1).where(lambda x: x==True).dropna().keys().tolist() used_classes=list(dict.fromkeys(used_columns +used_rows)) confusion_matrix_dataframe=confusion_matrix_dataframe.filter(items=used_classes, axis=0).filter( items=used_classes, axis=1) return confusion_matrix_dataframe, targets, predictions def get_metrics(classifier, data, labels, k): confusion_matrix_dataframe, targets, predictions=get_confusion_matrix_dataframe( classifier=classifier, input=data, target=labels, k=k ) model_evaluator=ModelEvaluator() accuracy=model_evaluator.get_accuracy(confusion_matrix_dataframe) precision=model_evaluator.get_precision(confusion_matrix_dataframe) recall=model_evaluator.get_recall(confusion_matrix_dataframe) f1_score=model_evaluator.get_f1_score(confusion_matrix_dataframe) cohen_kappa_score=model_evaluator.get_cohen_kappa_score(targets, predictions) matthews_correlation_coefficient=model_evaluator.get_matthews_corrcoef_score(targets, predictions) return accuracy, precision, recall, f1_score, cohen_kappa_score, matthews_correlation_coefficient class KnnDtwBaseModel: def __init__(self, logger, log_path_modelling, log_path_evaluation, train_dataframes, test_dataframes, k_nearest_neighbors, subsample_step, max_warping_window, slice_width): self.logger=logger self.log_path_modelling=log_path_modelling self.log_path_evaluation=log_path_evaluation self.train_dataframes=train_dataframes self.test_dataframes=test_dataframes self.k_nearest_neighbors=k_nearest_neighbors self.subsample_step=subsample_step self.max_warping_window=max_warping_window self.slice_width=slice_width self.model_logger=ModelLogger() self.model_plotter=ModelPlotter() self.model_preparator=ModelPreparator() self.model_evaluator=ModelEvaluator() @TrackingDecorator.track_time def validate(self, k_folds, random_state=0, quiet=False, dry_run=False): \"\"\" Validates the model by folding all train dataframes \"\"\" start_time=datetime.now() os.makedirs(self.log_path_modelling, exist_ok=True) kf=KFold(n_splits=k_folds, shuffle=True, random_state=random_state) split_index=0 split_labels=[] overall_validation_accuracy_history=[] overall_validation_precision_history=[] overall_validation_recall_history=[] overall_validation_f1_score_history=[] overall_validation_cohen_kappa_score_history=[] overall_validation_matthews_correlation_coefficient_history=[] ids=sorted(list(self.train_dataframes.keys())) for train_ids, validation_ids in kf.split(ids): split_index +=1 split_labels.append(\"Split \" +str(split_index)) k, validation_accuracy, validation_precision, validation_recall, validation_f1_score, \\ validation_cohen_kappa_score, validation_matthews_correlation_coefficient=self.validate_split( split_index=split_index, k_folds=k_folds, dataframes=self.train_dataframes, slice_width=self.slice_width, train_ids=train_ids, validation_ids=validation_ids, quiet=quiet, dry_run=dry_run ) self.logger.log_line(f\"best validation with k={k}:{validation_matthews_correlation_coefficient}\") overall_validation_accuracy_history.append(validation_accuracy) overall_validation_precision_history.append(validation_precision) overall_validation_recall_history.append(validation_recall) overall_validation_f1_score_history.append(validation_f1_score) overall_validation_cohen_kappa_score_history.append(validation_cohen_kappa_score) overall_validation_matthews_correlation_coefficient_history.append( validation_matthews_correlation_coefficient) self.model_plotter.plot_split_results_hist( logger=self.logger, log_path=self.log_path_modelling, split_labels=split_labels, overall_validation_accuracy_history=overall_validation_accuracy_history, overall_validation_precision_history=overall_validation_precision_history, overall_validation_recall_history=overall_validation_recall_history, overall_validation_f1_score_history=overall_validation_f1_score_history, overall_validation_cohen_kappa_score_history=overall_validation_cohen_kappa_score_history, overall_validation_matthews_correlation_coefficient_history=overall_validation_matthews_correlation_coefficient_history, quiet=quiet ) self.model_logger.log_split_results( logger=self.logger, overall_validation_accuracy_history=overall_validation_accuracy_history, overall_validation_precision_history=overall_validation_precision_history, overall_validation_recall_history=overall_validation_recall_history, overall_validation_f1_score_history=overall_validation_f1_score_history, overall_validation_cohen_kappa_score_history=overall_validation_cohen_kappa_score_history, overall_validation_matthews_correlation_coefficient_history=overall_validation_matthews_correlation_coefficient_history, quiet=quiet) self.logger.log_validation( time_elapsed=\"{}\".format(datetime.now() -start_time), log_path_modelling=self.log_path_modelling, telegram=not quiet and not dry_run ) return 0 @TrackingDecorator.track_time def validate_split(self, split_index, k_folds, train_ids, validation_ids, dataframes, slice_width, quiet, dry_run): \"\"\" Validates a single split \"\"\" start_time=datetime.now() os.makedirs(os.path.join(self.log_path_modelling, \"split-\" +str(split_index)), exist_ok=True) self.logger.log_line(\"\\n Split train_dataframes={id: list(dataframes.values())[id] for id in train_ids} validation_dataframes={id: list(dataframes.values())[id] for id in validation_ids} train_array=self.model_preparator.create_array(train_dataframes) train_data, train_labels=self.model_preparator.split_data_and_labels(train_array) validation_array=self.model_preparator.create_array(validation_dataframes) validation_data, validation_labels=self.model_preparator.split_data_and_labels(validation_array) self.model_plotter.plot_split_distribution( logger=self.logger, log_path=os.path.join(self.log_path_modelling, \"split-\" +str(split_index)), train_dataframes=train_dataframes, validation_dataframes=validation_dataframes, split_index=split_index, slice_width=slice_width, quiet=quiet ) classifier=KnnDtwClassifier(k=self.k_nearest_neighbors, subsample_step=1, max_warping_window=10, use_pruning=True) classifier.fit(train_data, train_labels) validation_k_list=[] validation_accuracy_list=[] validation_precision_list=[] validation_recall_list=[] validation_f1_score_list=[] validation_cohen_kappa_score_list=[] validation_matthews_correlation_coefficient_list=[] for k in range(1, self.k_nearest_neighbors +1): validation_accuracy, \\ validation_precision, \\ validation_recall, \\ validation_f1_score, \\ validation_cohen_kappa_score, \\ validation_matthews_correlation_coefficient=get_metrics( classifier=classifier, data=validation_data, labels=validation_labels, k=k ) np.save(os.path.join(self.log_path_modelling, \"split-\" +str(split_index), \"model\"), classifier.distance_matrix) self.logger.log_split( time_elapsed=\"{}\".format(datetime.now() -start_time), k_split=split_index, k_folds=k_folds, epochs=None, accuracy=round(validation_accuracy, 2), precision=round(validation_precision, 2), recall=round(validation_recall, 2), f1_score=round(validation_f1_score, 2), cohen_kappa_score=round(validation_cohen_kappa_score, 2), matthews_correlation_coefficient=round(validation_matthews_correlation_coefficient, 2), telegram=not quiet and not dry_run ) validation_k_list.append(k) validation_accuracy_list.append(validation_accuracy) validation_precision_list.append(validation_precision) validation_recall_list.append(validation_recall) validation_f1_score_list.append(validation_f1_score) validation_cohen_kappa_score_list.append(validation_cohen_kappa_score) validation_matthews_correlation_coefficient_list.append(validation_matthews_correlation_coefficient) index_best=validation_matthews_correlation_coefficient_list.index( max(validation_matthews_correlation_coefficient_list)) k_best=index_best +1 return k_best, validation_accuracy_list[index_best], validation_precision_list[index_best], \\ validation_recall_list[index_best], validation_f1_score_list[index_best], \\ validation_cohen_kappa_score_list[index_best], validation_matthews_correlation_coefficient_list[ index_best] @TrackingDecorator.track_time def finalize(self, epochs, average_epochs, quiet=False, dry_run=False): \"\"\" Trains a final model by using all train dataframes(not necessary since kNN is instance based) \"\"\" pass @TrackingDecorator.track_time def evaluate(self, clean=False, quiet=False, dry_run=False): \"\"\" Evaluates finalized model against test dataframes \"\"\" start_time=datetime.now() os.makedirs(self.log_path_modelling, exist_ok=True) os.makedirs(self.log_path_evaluation, exist_ok=True) train_array=self.model_preparator.create_array(self.train_dataframes) train_data, train_labels=self.model_preparator.split_data_and_labels(train_array) test_array=self.model_preparator.create_array(self.test_dataframes) test_data, test_labels=self.model_preparator.split_data_and_labels(test_array) classifier=KnnDtwClassifier(k=self.k_nearest_neighbors, subsample_step=1, max_warping_window=10, use_pruning=True) classifier.fit(train_data, train_labels) test_k_list=[] test_accuracy_list=[] test_precision_list=[] test_recall_list=[] test_f1_score_list=[] test_cohen_kappa_score_list=[] test_matthews_correlation_coefficient_list=[] for k in range(1, self.k_nearest_neighbors): test_accuracy, \\ test_precision, \\ test_recall, \\ test_f1_score, \\ test_cohen_kappa_score, \\ test_matthews_correlation_coefficient=get_metrics(classifier, test_data, test_labels, k) np.save(os.path.join(self.log_path_modelling, \"model\"), classifier.distance_matrix) test_confusion_matrix_dataframe, targets, predictions=get_confusion_matrix_dataframe( classifier=classifier, input=test_data, target=test_labels, k=k ) ConfusionMatrixPlotter().run( logger=self.logger, results_path=os.path.join(self.log_path_evaluation), confusion_matrix_dataframe=test_confusion_matrix_dataframe, file_name=f\"confusion_matrix_k{str(k)}\", clean=clean ) self.logger.log_evaluation( time_elapsed=\"{}\".format(datetime.now() -start_time), log_path_evaluation=self.log_path_evaluation, test_accuracy=test_accuracy, test_precision=test_precision, test_recall=test_recall, test_f1_score=test_f1_score, test_cohen_kappa_score=test_cohen_kappa_score, test_matthews_correlation_coefficient=test_matthews_correlation_coefficient, telegram=not quiet and not dry_run, confusion_matrix_file_name=f\"confusion_matrix_k{str(k)}\", ) test_k_list.append(k) test_accuracy_list.append(test_accuracy) test_precision_list.append(test_precision) test_recall_list.append(test_recall) test_f1_score_list.append(test_f1_score) test_cohen_kappa_score_list.append(test_cohen_kappa_score) test_matthews_correlation_coefficient_list.append(test_matthews_correlation_coefficient) return test_k_list, test_accuracy_list, test_precision_list, test_recall_list, test_f1_score_list, \\ test_cohen_kappa_score_list, test_matthews_correlation_coefficient_list ","sourceWithComments":"import os\nimport random\nimport warnings\nfrom datetime import datetime\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom confusion_matrix_plotter import ConfusionMatrixPlotter\nfrom label_encoder import LabelEncoder\nfrom model_evaluator import ModelEvaluator\nfrom model_logger import ModelLogger\nfrom model_plotter import ModelPlotter\nfrom model_preparator import ModelPreparator\nfrom sklearn.model_selection import KFold\nfrom tracking_decorator import TrackingDecorator\n\nfrom knn_dtw_classifier import KnnDtwClassifier\n\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n\n# Set up device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Control sources of randomness\ntorch.manual_seed(0)\nrandom.seed(0)\nnp.random.seed(0)\n\n# Number of classes\nnum_classes = LabelEncoder().num_classes()\n\n\n#\n# Evaluation\n#\n\ndef get_confusion_matrix_dataframe(classifier, input, target, k):\n    targets = []\n    predictions = []\n    confusion_matrix = np.zeros((num_classes, num_classes))\n\n    prediction, probability = classifier.predict(input, k)\n\n    targets.extend(target.astype(int).tolist())\n    predictions.extend(prediction.astype(int).tolist())\n\n    for t, p in zip(target.astype(int), prediction.astype(int)):\n        confusion_matrix[t, p] += 1\n\n    # Build confusion matrix, limit to classes actually used\n    confusion_matrix_dataframe = pd.DataFrame(confusion_matrix, index=LabelEncoder().classes,\n                                              columns=LabelEncoder().classes).astype(\"int64\")\n    used_columns = (confusion_matrix_dataframe != 0).any(axis=0).where(lambda x: x == True).dropna().keys().tolist()\n    used_rows = (confusion_matrix_dataframe != 0).any(axis=1).where(lambda x: x == True).dropna().keys().tolist()\n    used_classes = list(dict.fromkeys(used_columns + used_rows))\n    confusion_matrix_dataframe = confusion_matrix_dataframe.filter(items=used_classes, axis=0).filter(\n        items=used_classes, axis=1)\n\n    return confusion_matrix_dataframe, targets, predictions\n\n\ndef get_metrics(classifier, data, labels, k):\n    confusion_matrix_dataframe, targets, predictions = get_confusion_matrix_dataframe(\n        classifier=classifier,\n        input=data,\n        target=labels,\n        k=k\n    )\n\n    model_evaluator = ModelEvaluator()\n\n    accuracy = model_evaluator.get_accuracy(confusion_matrix_dataframe)\n    precision = model_evaluator.get_precision(confusion_matrix_dataframe)\n    recall = model_evaluator.get_recall(confusion_matrix_dataframe)\n    f1_score = model_evaluator.get_f1_score(confusion_matrix_dataframe)\n    cohen_kappa_score = model_evaluator.get_cohen_kappa_score(targets, predictions)\n    matthews_correlation_coefficient = model_evaluator.get_matthews_corrcoef_score(targets, predictions)\n\n    return accuracy, precision, recall, f1_score, cohen_kappa_score, matthews_correlation_coefficient\n\n\n#\n# Main\n#\n\nclass KnnDtwBaseModel:\n\n    def __init__(self, logger, log_path_modelling, log_path_evaluation, train_dataframes, test_dataframes,\n                 k_nearest_neighbors, subsample_step, max_warping_window, slice_width):\n        self.logger = logger\n        self.log_path_modelling = log_path_modelling\n        self.log_path_evaluation = log_path_evaluation\n\n        self.train_dataframes = train_dataframes\n        self.test_dataframes = test_dataframes\n\n        self.k_nearest_neighbors = k_nearest_neighbors\n        self.subsample_step = subsample_step\n        self.max_warping_window = max_warping_window\n        self.slice_width = slice_width\n\n        self.model_logger = ModelLogger()\n        self.model_plotter = ModelPlotter()\n        self.model_preparator = ModelPreparator()\n        self.model_evaluator = ModelEvaluator()\n\n    @TrackingDecorator.track_time\n    def validate(self, k_folds, random_state=0, quiet=False, dry_run=False):\n        \"\"\"\n        Validates the model by folding all train dataframes\n        \"\"\"\n\n        start_time = datetime.now()\n\n        # Make results path\n        os.makedirs(self.log_path_modelling, exist_ok=True)\n\n        kf = KFold(n_splits=k_folds, shuffle=True, random_state=random_state)\n        split_index = 0\n        split_labels = []\n\n        overall_validation_accuracy_history = []\n        overall_validation_precision_history = []\n        overall_validation_recall_history = []\n        overall_validation_f1_score_history = []\n        overall_validation_cohen_kappa_score_history = []\n        overall_validation_matthews_correlation_coefficient_history = []\n\n        ids = sorted(list(self.train_dataframes.keys()))\n\n        for train_ids, validation_ids in kf.split(ids):\n            # Increment split index\n            split_index += 1\n            split_labels.append(\"Split \" + str(split_index))\n\n            # Validate split\n            k, validation_accuracy, validation_precision, validation_recall, validation_f1_score, \\\n            validation_cohen_kappa_score, validation_matthews_correlation_coefficient = self.validate_split(\n                split_index=split_index,\n                k_folds=k_folds,\n                dataframes=self.train_dataframes,\n                slice_width=self.slice_width,\n                train_ids=train_ids,\n                validation_ids=validation_ids,\n                quiet=quiet,\n                dry_run=dry_run\n            )\n\n            self.logger.log_line(f\"best validation with k={k} : {validation_matthews_correlation_coefficient}\")\n\n            # Aggregate split results\n            overall_validation_accuracy_history.append(validation_accuracy)\n            overall_validation_precision_history.append(validation_precision)\n            overall_validation_recall_history.append(validation_recall)\n            overall_validation_f1_score_history.append(validation_f1_score)\n            overall_validation_cohen_kappa_score_history.append(validation_cohen_kappa_score)\n            overall_validation_matthews_correlation_coefficient_history.append(\n                validation_matthews_correlation_coefficient)\n\n        self.model_plotter.plot_split_results_hist(\n            logger=self.logger,\n            log_path=self.log_path_modelling,\n            split_labels=split_labels,\n            overall_validation_accuracy_history=overall_validation_accuracy_history,\n            overall_validation_precision_history=overall_validation_precision_history,\n            overall_validation_recall_history=overall_validation_recall_history,\n            overall_validation_f1_score_history=overall_validation_f1_score_history,\n            overall_validation_cohen_kappa_score_history=overall_validation_cohen_kappa_score_history,\n            overall_validation_matthews_correlation_coefficient_history=overall_validation_matthews_correlation_coefficient_history,\n            quiet=quiet\n        )\n\n        self.model_logger.log_split_results(\n            logger=self.logger,\n            overall_validation_accuracy_history=overall_validation_accuracy_history,\n            overall_validation_precision_history=overall_validation_precision_history,\n            overall_validation_recall_history=overall_validation_recall_history,\n            overall_validation_f1_score_history=overall_validation_f1_score_history,\n            overall_validation_cohen_kappa_score_history=overall_validation_cohen_kappa_score_history,\n            overall_validation_matthews_correlation_coefficient_history=overall_validation_matthews_correlation_coefficient_history,\n            quiet=quiet)\n\n        self.logger.log_validation(\n            time_elapsed=\"{}\".format(datetime.now() - start_time),\n            log_path_modelling=self.log_path_modelling,\n            telegram=not quiet and not dry_run\n        )\n\n        return 0\n\n    @TrackingDecorator.track_time\n    def validate_split(self, split_index, k_folds, train_ids, validation_ids, dataframes, slice_width, quiet, dry_run):\n        \"\"\"\n        Validates a single split\n        \"\"\"\n\n        start_time = datetime.now()\n\n        # Make results path\n        os.makedirs(os.path.join(self.log_path_modelling, \"split-\" + str(split_index)), exist_ok=True)\n\n        self.logger.log_line(\"\\n Split # \" + str(split_index) + \"\/\" + str(k_folds))\n\n        train_dataframes = {id: list(dataframes.values())[id] for id in train_ids}\n        validation_dataframes = {id: list(dataframes.values())[id] for id in validation_ids}\n\n        # Split data and labels for train\n        train_array = self.model_preparator.create_array(train_dataframes)\n        train_data, train_labels = self.model_preparator.split_data_and_labels(train_array)\n\n        # Split data and labels for validation\n        validation_array = self.model_preparator.create_array(validation_dataframes)\n        validation_data, validation_labels = self.model_preparator.split_data_and_labels(validation_array)\n\n        # Plot target variable distribution\n        self.model_plotter.plot_split_distribution(\n            logger=self.logger,\n            log_path=os.path.join(self.log_path_modelling, \"split-\" + str(split_index)),\n            train_dataframes=train_dataframes,\n            validation_dataframes=validation_dataframes,\n            split_index=split_index,\n            slice_width=slice_width,\n            quiet=quiet\n        )\n\n        # Define classifier\n        classifier = KnnDtwClassifier(k=self.k_nearest_neighbors, subsample_step=1, max_warping_window=10,\n                                      use_pruning=True)\n        classifier.fit(train_data, train_labels)\n\n        validation_k_list = []\n        validation_accuracy_list = []\n        validation_precision_list = []\n        validation_recall_list = []\n        validation_f1_score_list = []\n        validation_cohen_kappa_score_list = []\n        validation_matthews_correlation_coefficient_list = []\n\n        # Iterate over hyper-parameter configurations\n        for k in range(1, self.k_nearest_neighbors + 1):\n            # Get metrics for validation data\n            validation_accuracy, \\\n            validation_precision, \\\n            validation_recall, \\\n            validation_f1_score, \\\n            validation_cohen_kappa_score, \\\n            validation_matthews_correlation_coefficient = get_metrics(\n                classifier=classifier,\n                data=validation_data,\n                labels=validation_labels,\n                k=k\n            )\n\n            # # Plot distance matrix\n            # distance_matrix_dataframe = pd.DataFrame(data=classifier.distance_matrix.astype(int))\n            # DistanceMatrixPlotter().run(\n            #     logger=self.logger,\n            #     results_path=os.path.join(self.log_path_modelling, \"split-\" + str(split_index)),\n            #     distance_matrix_dataframe=distance_matrix_dataframe,\n            #     clean=False,\n            #     quiet=False\n            # )\n\n            np.save(os.path.join(self.log_path_modelling, \"split-\" + str(split_index), \"model\"),\n                    classifier.distance_matrix)\n\n            self.logger.log_split(\n                time_elapsed=\"{}\".format(datetime.now() - start_time),\n                k_split=split_index,\n                k_folds=k_folds,\n                epochs=None,\n                accuracy=round(validation_accuracy, 2),\n                precision=round(validation_precision, 2),\n                recall=round(validation_recall, 2),\n                f1_score=round(validation_f1_score, 2),\n                cohen_kappa_score=round(validation_cohen_kappa_score, 2),\n                matthews_correlation_coefficient=round(validation_matthews_correlation_coefficient, 2),\n                telegram=not quiet and not dry_run\n            )\n\n            validation_k_list.append(k)\n            validation_accuracy_list.append(validation_accuracy)\n            validation_precision_list.append(validation_precision)\n            validation_recall_list.append(validation_recall)\n            validation_f1_score_list.append(validation_f1_score)\n            validation_cohen_kappa_score_list.append(validation_cohen_kappa_score)\n            validation_matthews_correlation_coefficient_list.append(validation_matthews_correlation_coefficient)\n\n        # Determine with which k the best result has been created\n        index_best = validation_matthews_correlation_coefficient_list.index(\n            max(validation_matthews_correlation_coefficient_list))\n        k_best = index_best + 1\n\n        return k_best, validation_accuracy_list[index_best], validation_precision_list[index_best], \\\n               validation_recall_list[index_best], validation_f1_score_list[index_best], \\\n               validation_cohen_kappa_score_list[index_best], validation_matthews_correlation_coefficient_list[\n                   index_best]\n\n    @TrackingDecorator.track_time\n    def finalize(self, epochs, average_epochs, quiet=False, dry_run=False):\n        \"\"\"\n        Trains a final model by using all train dataframes (not necessary since kNN is instance based)\n        \"\"\"\n        pass\n\n    @TrackingDecorator.track_time\n    def evaluate(self, clean=False, quiet=False, dry_run=False):\n        \"\"\"\n        Evaluates finalized model against test dataframes\n        \"\"\"\n\n        start_time = datetime.now()\n\n        # Make results path\n        os.makedirs(self.log_path_modelling, exist_ok=True)\n        os.makedirs(self.log_path_evaluation, exist_ok=True)\n\n        # Split data and labels for train\n        train_array = self.model_preparator.create_array(self.train_dataframes)\n        train_data, train_labels = self.model_preparator.split_data_and_labels(train_array)\n\n        # Split data and labels for validation\n        test_array = self.model_preparator.create_array(self.test_dataframes)\n        test_data, test_labels = self.model_preparator.split_data_and_labels(test_array)\n\n        # Define classifier\n        classifier = KnnDtwClassifier(k=self.k_nearest_neighbors, subsample_step=1, max_warping_window=10,\n                                      use_pruning=True)\n        classifier.fit(train_data, train_labels)\n\n        test_k_list = []\n        test_accuracy_list = []\n        test_precision_list = []\n        test_recall_list = []\n        test_f1_score_list = []\n        test_cohen_kappa_score_list = []\n        test_matthews_correlation_coefficient_list = []\n\n        # Iterate over hyper-parameter configurations\n        for k in range(1, self.k_nearest_neighbors):\n            # Get metrics for test data\n            test_accuracy, \\\n            test_precision, \\\n            test_recall, \\\n            test_f1_score, \\\n            test_cohen_kappa_score, \\\n            test_matthews_correlation_coefficient = get_metrics(classifier, test_data, test_labels, k)\n\n            # # Plot distance matrix\n            # distance_matrix_dataframe = pd.DataFrame(data=classifier.distance_matrix.astype(int))\n            # DistanceMatrixPlotter().run(\n            #     logger=self.logger,\n            #     results_path=self.log_path_modelling,\n            #     distance_matrix_dataframe=distance_matrix_dataframe,\n            #     clean=False,\n            #     quiet=False\n            # )\n\n            np.save(os.path.join(self.log_path_modelling, \"model\"), classifier.distance_matrix)\n\n            # Plot confusion matrix\n            test_confusion_matrix_dataframe, targets, predictions = get_confusion_matrix_dataframe(\n                classifier=classifier,\n                input=test_data,\n                target=test_labels,\n                k=k\n            )\n            ConfusionMatrixPlotter().run(\n                logger=self.logger,\n                results_path=os.path.join(self.log_path_evaluation),\n                confusion_matrix_dataframe=test_confusion_matrix_dataframe,\n                file_name=f\"confusion_matrix_k{str(k)}\",\n                clean=clean\n            )\n\n            self.logger.log_evaluation(\n                time_elapsed=\"{}\".format(datetime.now() - start_time),\n                log_path_evaluation=self.log_path_evaluation,\n                test_accuracy=test_accuracy,\n                test_precision=test_precision,\n                test_recall=test_recall,\n                test_f1_score=test_f1_score,\n                test_cohen_kappa_score=test_cohen_kappa_score,\n                test_matthews_correlation_coefficient=test_matthews_correlation_coefficient,\n                telegram=not quiet and not dry_run,\n                confusion_matrix_file_name=f\"confusion_matrix_k{str(k)}\",\n            )\n\n            test_k_list.append(k)\n            test_accuracy_list.append(test_accuracy)\n            test_precision_list.append(test_precision)\n            test_recall_list.append(test_recall)\n            test_f1_score_list.append(test_f1_score)\n            test_cohen_kappa_score_list.append(test_cohen_kappa_score)\n            test_matthews_correlation_coefficient_list.append(test_matthews_correlation_coefficient)\n\n        return test_k_list, test_accuracy_list, test_precision_list, test_recall_list, test_f1_score_list, \\\n               test_cohen_kappa_score_list, test_matthews_correlation_coefficient_list\n"}},"msg":"fix: Telegram flooding"}},"https:\/\/github.com\/MPAS-Dev\/compass":{"94bbfaaa9ef90803c7ffe04ae372beaba6fa9aee":{"url":"https:\/\/api.github.com\/repos\/MPAS-Dev\/compass\/commits\/94bbfaaa9ef90803c7ffe04ae372beaba6fa9aee","html_url":"https:\/\/github.com\/MPAS-Dev\/compass\/commit\/94bbfaaa9ef90803c7ffe04ae372beaba6fa9aee","message":"Fix second flood fill for bed topography spacing\n\nUse 2 * min_spac to define flood fill mask, instead of min_spac.\nThe factor of 2 works well for 1-10km Greenland, but will likely\nbe problem-specific.","sha":"94bbfaaa9ef90803c7ffe04ae372beaba6fa9aee","keyword":"flooding fix","diff":"diff --git a\/compass\/landice\/mesh.py b\/compass\/landice\/mesh.py\nindex 3a860ff9a..5881137eb 100644\n--- a\/compass\/landice\/mesh.py\n+++ b\/compass\/landice\/mesh.py\n@@ -215,7 +215,7 @@ def set_cell_width(self, section, thk, bed=None, vx=None, vy=None,\n             # spacing_bed[dist_to_grounding_line >= high_dist_bed] = max_spac\n             in_mask2 = (bed <= low_bed)\n             in_mask2[np.logical_and(\n-                       thk > 0, spacing_bed > min_spac)] = 0\n+                       thk > 0, spacing_bed > (2. * min_spac))] = 0\n             low_bed_mask2 = gridded_flood_fill(in_mask2,\n                                               iStart=flood_fill_iStart,\n                                               jStart=flood_fill_jStart)\n","files":{"\/compass\/landice\/mesh.py":{"changes":[{"diff":"\n             # spacing_bed[dist_to_grounding_line >= high_dist_bed] = max_spac\n             in_mask2 = (bed <= low_bed)\n             in_mask2[np.logical_and(\n-                       thk > 0, spacing_bed > min_spac)] = 0\n+                       thk > 0, spacing_bed > (2. * min_spac))] = 0\n             low_bed_mask2 = gridded_flood_fill(in_mask2,\n                                               iStart=flood_fill_iStart,\n                                               jStart=flood_fill_jStart)\n","add":1,"remove":1,"filename":"\/compass\/landice\/mesh.py","badparts":["                       thk > 0, spacing_bed > min_spac)] = 0"],"goodparts":["                       thk > 0, spacing_bed > (2. * min_spac))] = 0"]}],"source":"\nimport numpy as np import jigsawpy import time def gridded_flood_fill(field, iStart=None, jStart=None): \"\"\" Generic flood-fill routine to create mask of connected elements in the desired input array(field) from a gridded dataset. This is generally used to remove glaciers and ice-fields that are not connected to the ice sheet. Note that there may be more efficient algorithms. Parameters ---------- field: numpy.ndarray Array from gridded dataset to use for flood-fill. Usually ice thickness. Returns ------- flood_mask: numpy.ndarray _mask calculated by the flood fill routine, where cells connected to the ice sheet(or main feature) are 1 and everything else is 0. \"\"\" sz=field.shape searched_mask=np.zeros(sz) flood_mask=np.zeros(sz) if iStart==None and jStart==None: iStart=sz[0] \/\/ 2 jStart=sz[1] \/\/ 2 flood_mask[iStart, jStart]=1 neighbors=np.array([[1, 0],[-1, 0],[0, 1],[0, -1]]) lastSearchList=np.ravel_multi_index([[iStart],[jStart]], sz, order='F') cnt=0 while len(lastSearchList) > 0: cnt +=1 newSearchList=np.array([], dtype='i') for iii in range(len(lastSearchList)): [i, j]=np.unravel_index(lastSearchList[iii], sz, order='F') for n in neighbors: ii=min(i +n[0], sz[0] -1) jj=min(j +n[1], sz[1] -1) if searched_mask[ii, jj]==0: searched_mask[ii, jj]=1 if field[ii, jj] > 0.0: flood_mask[ii, jj]=1 newSearchList=np.append(newSearchList, np.ravel_multi_index( [[ii],[jj]], sz, mode='clip', order='F')[0]) lastSearchList=newSearchList return flood_mask def set_rectangular_geom_points_and_edges(xmin, xmax, ymin, ymax): \"\"\" Set node and edge coordinates to pass to :py:func:`mpas_tools.mesh.creation.build_mesh.build_planar_mesh()`. Parameters ---------- xmin: int or float Left-most x-coordinate in region to mesh xmax: int or float Right-most x-coordinate in region to mesh ymin: int or float Bottom-most y-coordinate in region to mesh ymax: int or float Top-most y-coordinate in region to mesh Returns ------- geom_points: jigsawpy.jigsaw_msh_t.VERT2_t xy node coordinates to pass to build_planar_mesh() geom_edges: jigsawpy.jigsaw_msh_t.EDGE2_t xy edge coordinates between nodes to pass to build_planar_mesh() \"\"\" geom_points=np.array([ ((xmin, ymin), 0), ((xmax, ymin), 0), ((xmax, ymax), 0), ((xmin, ymax), 0)], dtype=jigsawpy.jigsaw_msh_t.VERT2_t) geom_edges=np.array([ ((0, 1), 0), ((1, 2), 0), ((2, 3), 0), ((3, 0), 0)], dtype=jigsawpy.jigsaw_msh_t.EDGE2_t) return geom_points, geom_edges def set_cell_width(self, section, thk, bed=None, vx=None, vy=None, dist_to_edge=None, dist_to_grounding_line=None, flood_fill_iStart=None, flood_fill_jStart=None): \"\"\" Set cell widths based on settings in config file to pass to :py:func:`mpas_tools.mesh.creation.build_mesh.build_planar_mesh()`. Requires the following options to be set in the given config section: ``min_spac``, ``max_spac``, ``high_log_speed``, ``low_log_speed``, ``high_dist``, ``low_dist``,``cull_distance``, ``use_speed``, ``use_dist_to_edge``, and ``use_dist_to_grounding_line``. Parameters ---------- section: str Section of the config file from which to read parameters thk: numpy.ndarray Ice thickness field from gridded dataset, usually after trimming to flood fill mask bed: numpy.ndarray Bed topography from gridded dataset vx: numpy.ndarray, optional x-component of ice velocity from gridded dataset, usually after trimming to flood fill mask. Can be set to ``None`` if ``use_speed==False`` in config file. vy: numpy.ndarray, optional y-component of ice velocity from gridded dataset, usually after trimming to flood fill mask. Can be set to ``None`` if ``use_speed==False`` in config file. dist_to_edge: numpy.ndarray, optional Distance from each cell to ice edge, calculated in separate function. Can be set to ``None`` if ``use_dist_to_edge==False`` in config file and you do not want to set large ``cell_width`` where cells will be culled anyway, but this is not recommended. dist_to_grounding_line: numpy.ndarray, optional Distance from each cell to grounding line, calculated in separate function. Can be set to ``None`` if ``use_dist_to_grounding_line==False`` in config file. flood_fill_iStart: int, optional x-index location to start flood-fill when using bed topography flood_fill_jStart: int, optional y-index location to start flood-fill when using bed topography Returns ------- cell_width: numpy.ndarray Desired width of MPAS cells based on mesh desnity functions to pass to :py:func:`mpas_tools.mesh.creation.build_mesh.build_planar_mesh()`. \"\"\" logger=self.logger section=self.config[section] min_spac=float(section.get('min_spac')) max_spac=float(section.get('max_spac')) high_log_speed=float(section.get('high_log_speed')) low_log_speed=float(section.get('low_log_speed')) high_dist=float(section.get('high_dist')) low_dist=float(section.get('low_dist')) high_dist_bed=float(section.get('high_dist_bed')) low_dist_bed=float(section.get('low_dist_bed')) low_bed=float(section.get('low_bed')) high_bed=float(section.get('high_bed')) cull_distance=float(section.get('cull_distance')) * 1.e3 if section.get('use_bed')=='True': logger.info('Using bed elevation for spacing.') if flood_fill_iStart is not None and flood_fill_jStart is not None: logger.info('calling gridded_flood_fill to find \\ bedTopography <=low_bed connected to the ocean.') tic=time.time() in_mask=(bed <=low_bed) in_mask[np.logical_and( thk > 0, dist_to_grounding_line >=high_dist_bed)]=0 low_bed_mask=gridded_flood_fill(in_mask, iStart=flood_fill_iStart, jStart=flood_fill_jStart) toc=time.time() logger.info(f'Flood fill finished in{toc -tic} seconds.') k=0.05 spacing_bed=min_spac +(max_spac -min_spac) \/(1.0 +np.exp( -k *( bed -np.mean([high_bed, low_bed])))) spacing_bed[dist_to_grounding_line >=low_dist_bed]=( ( 1.0 -(dist_to_grounding_line[ dist_to_grounding_line >=low_dist_bed] -low_dist_bed) \/(high_dist_bed -low_dist_bed)) * spacing_bed[dist_to_grounding_line >=low_dist_bed] + (dist_to_grounding_line[dist_to_grounding_line >= low_dist_bed] -low_dist_bed) \/ (high_dist_bed -low_dist_bed) * max_spac) spacing_bed[dist_to_grounding_line >=high_dist_bed]=max_spac if flood_fill_iStart is not None and flood_fill_jStart is not None: spacing_bed[low_bed_mask==0]=max_spac in_mask2=(bed <=low_bed) in_mask2[np.logical_and( thk > 0, spacing_bed > min_spac)]=0 low_bed_mask2=gridded_flood_fill(in_mask2, iStart=flood_fill_iStart, jStart=flood_fill_jStart) spacing_bed[low_bed_mask2==0]=max_spac else: spacing_bed=max_spac * np.ones_like(thk) if section.get('use_speed')=='True': logger.info('Using speed for cell spacing') speed=(vx ** 2 +vy ** 2) ** 0.5 lspd=np.log10(speed) spacing_speed=np.interp(lspd,[low_log_speed, high_log_speed], [max_spac, min_spac], left=max_spac, right=min_spac) missing_data_mask=np.logical_or( np.logical_or(np.isnan(vx), np.isnan(vy)), np.logical_or(np.abs(vx) > 1.e5, np.abs(vy) > 1.e5)) spacing_speed[missing_data_mask]=max_spac logger.info(f'Found{np.sum(missing_data_mask)} points in input ' f'dataset with missing velocity values. Setting ' f'velocity-based spacing to maximum value.') spacing_speed[thk==0.0]=min_spac else: spacing_speed=max_spac * np.ones_like(thk) if section.get('use_dist_to_edge')=='True': logger.info('Using distance to ice edge for cell spacing') spacing_edge=np.interp(dist_to_edge,[low_dist, high_dist], [min_spac, max_spac], left=min_spac, right=max_spac) spacing_edge[thk==0.0]=min_spac else: spacing_edge=max_spac * np.ones_like(thk) if section.get('use_dist_to_grounding_line')=='True': logger.info('Using distance to grounding line for cell spacing') spacing_gl=np.interp(dist_to_grounding_line,[low_dist, high_dist], [min_spac, max_spac], left=min_spac, right=max_spac) spacing_gl[thk==0.0]=min_spac else: spacing_gl=max_spac * np.ones_like(thk) cell_width=max_spac * np.ones_like(thk) for width in[spacing_bed, spacing_speed, spacing_edge, spacing_gl]: cell_width=np.minimum(cell_width, width) if dist_to_edge is not None: mask=np.logical_and( thk==0.0, dist_to_edge >(3. * cull_distance)) cell_width[mask]=max_spac return cell_width def get_dist_to_edge_and_GL(self, thk, topg, x, y, section, window_size=None): \"\"\" Calculate distance from each point to ice edge and grounding line, to be used in mesh density functions in :py:func:`compass.landice.mesh.set_cell_width()`. In future development, this should be updated to use a faster package such as `scikit-fmm`. Parameters ---------- thk: numpy.ndarray Ice thickness field from gridded dataset, usually after trimming to flood fill mask topg: numpy.ndarray Bed topography field from gridded dataset x: numpy.ndarray x coordinates from gridded dataset y: numpy.ndarray y coordinates from gridded dataset window_size: int or float Size(in meters) of a search 'box'(one-directional) to use to calculate the distance from each cell to the ice margin. Bigger number makes search slower, but if too small, the transition zone could get truncated. We usually want this calculated as the maximum of high_dist and high_dist_bed, but there may be cases in which it is useful to set it manually. However, it should never be smaller than either high_dist or high_dist_bed. Returns ------- dist_to_edge: numpy.ndarray Distance from each cell to the ice edge dist_to_grounding_line: numpy.ndarray Distance from each cell to the grounding line \"\"\" logger=self.logger section=self.config[section] tic=time.time() high_dist=float(section.get('high_dist')) high_dist_bed=float(section.get('high_dist_bed')) if window_size is None: window_size=max(high_dist, high_dist_bed) elif window_size < min(high_dist, high_dist_bed): logger.info('WARNING: window_size was set to a value smaller' ' than high_dist and\/or high_dist_bed. Resetting' f' window_size to{max(high_dist, high_dist_bed)},' ' which is max(high_dist, high_dist_bed)') window_size=max(high_dist, high_dist_bed) dx=x[1] -x[0] nx=len(x) ny=len(y) sz=thk.shape neighbors=np.array([[1, 0],[-1, 0],[0, 1],[0, -1], [1, 1],[-1, 1],[1, -1],[-1, -1]]) ice_mask=thk > 0.0 grounded_mask=thk >(-1028.0 \/ 910.0 * topg) floating_mask=np.logical_and(thk <(-1028.0 \/ 910.0 * topg), thk > 0.0) margin_mask=np.zeros(sz, dtype='i') grounding_line_mask=np.zeros(sz, dtype='i') for n in neighbors: not_ice_mask=np.logical_not(np.roll(ice_mask, n, axis=[0, 1])) margin_mask=np.logical_or(margin_mask, not_ice_mask) not_grounded_mask=np.logical_not(np.roll(grounded_mask, n, axis=[0, 1])) grounding_line_mask=np.logical_or(grounding_line_mask, not_grounded_mask) margin_mask=np.logical_and(margin_mask, ice_mask) [XPOS, YPOS]=np.meshgrid(x, y) dist_to_edge=np.zeros(sz) dist_to_grounding_line=np.zeros(sz) d=int(np.ceil(window_size \/ dx)) rng=np.arange(-1 * d, d, dtype='i') max_dist=float(d) * dx ind=np.where(np.ravel(thk, order='F') >=0)[0] for iii in range(len(ind)): [i, j]=np.unravel_index(ind[iii], sz, order='F') irng=i +rng jrng=j +rng irng=irng[np.nonzero(np.logical_and(irng >=0, irng < ny))] jrng=jrng[np.nonzero(np.logical_and(jrng >=0, jrng < nx))] dist_to_here=((XPOS[np.ix_(irng, jrng)] -x[j]) ** 2 + (YPOS[np.ix_(irng, jrng)] -y[i]) ** 2) ** 0.5 dist_to_here_edge=dist_to_here.copy() dist_to_here_grounding_line=dist_to_here.copy() dist_to_here_edge[margin_mask[np.ix_(irng, jrng)]==0]=max_dist dist_to_here_grounding_line[grounding_line_mask [np.ix_(irng, jrng)]==0]=max_dist dist_to_edge[i, j]=dist_to_here_edge.min() dist_to_grounding_line[i, j]=dist_to_here_grounding_line.min() toc=time.time() logger.info('compass.landice.mesh.get_dist_to_edge_and_GL() took{:0.2f} ' 'seconds'.format(toc -tic)) return dist_to_edge, dist_to_grounding_line ","sourceWithComments":"import numpy as np\nimport jigsawpy\nimport time\n\n\ndef gridded_flood_fill(field, iStart=None, jStart=None):\n    \"\"\"\n    Generic flood-fill routine to create mask of connected elements\n    in the desired input array (field) from a gridded dataset. This\n    is generally used to remove glaciers and ice-fields that are not\n    connected to the ice sheet. Note that there may be more efficient\n    algorithms.\n\n    Parameters\n    ----------\n    field : numpy.ndarray\n        Array from gridded dataset to use for flood-fill.\n        Usually ice thickness.\n\n    Returns\n    -------\n    flood_mask : numpy.ndarray\n        _mask calculated by the flood fill routine,\n        where cells connected to the ice sheet (or main feature)\n        are 1 and everything else is 0.\n    \"\"\"\n\n    sz = field.shape\n    searched_mask = np.zeros(sz)\n    flood_mask = np.zeros(sz)\n    if iStart == None and jStart == None:\n        iStart = sz[0] \/\/ 2\n        jStart = sz[1] \/\/ 2\n    flood_mask[iStart, jStart] = 1\n\n    neighbors = np.array([[1, 0], [-1, 0], [0, 1], [0, -1]])\n\n    lastSearchList = np.ravel_multi_index([[iStart], [jStart]],\n                                          sz, order='F')\n\n    cnt = 0\n    while len(lastSearchList) > 0:\n        cnt += 1\n        newSearchList = np.array([], dtype='i')\n\n        for iii in range(len(lastSearchList)):\n            [i, j] = np.unravel_index(lastSearchList[iii], sz, order='F')\n            # search neighbors\n            for n in neighbors:\n                ii = min(i + n[0], sz[0] - 1)  # don't go out of bounds\n                jj = min(j + n[1], sz[1] - 1)  # subscripts to neighbor\n                # only consider unsearched neighbors\n                if searched_mask[ii, jj] == 0:\n                    searched_mask[ii, jj] = 1  # mark as searched\n\n                    if field[ii, jj] > 0.0:\n                        flood_mask[ii, jj] = 1  # mark as ice\n                        # add to list of newly found  cells\n                        newSearchList = np.append(newSearchList,\n                                                  np.ravel_multi_index(\n                                                      [[ii], [jj]], sz,\n                                                      mode='clip',\n                                                      order='F')[0])\n        lastSearchList = newSearchList\n\n    return flood_mask\n\n\ndef set_rectangular_geom_points_and_edges(xmin, xmax, ymin, ymax):\n    \"\"\"\n    Set node and edge coordinates to pass to\n    :py:func:`mpas_tools.mesh.creation.build_mesh.build_planar_mesh()`.\n\n    Parameters\n    ----------\n    xmin : int or float\n        Left-most x-coordinate in region to mesh\n    xmax : int or float\n        Right-most x-coordinate in region to mesh\n    ymin : int or float\n        Bottom-most y-coordinate in region to mesh\n    ymax : int or float\n        Top-most y-coordinate in region to mesh\n\n    Returns\n    -------\n    geom_points : jigsawpy.jigsaw_msh_t.VERT2_t\n        xy node coordinates to pass to build_planar_mesh()\n    geom_edges : jigsawpy.jigsaw_msh_t.EDGE2_t\n        xy edge coordinates between nodes to pass to build_planar_mesh()\n    \"\"\"\n\n    geom_points = np.array([  # list of xy \"node\" coordinates\n        ((xmin, ymin), 0),\n        ((xmax, ymin), 0),\n        ((xmax, ymax), 0),\n        ((xmin, ymax), 0)],\n        dtype=jigsawpy.jigsaw_msh_t.VERT2_t)\n\n    geom_edges = np.array([  # list of \"edges\" between nodes\n        ((0, 1), 0),\n        ((1, 2), 0),\n        ((2, 3), 0),\n        ((3, 0), 0)],\n        dtype=jigsawpy.jigsaw_msh_t.EDGE2_t)\n\n    return geom_points, geom_edges\n\n\ndef set_cell_width(self, section, thk, bed=None, vx=None, vy=None,\n                   dist_to_edge=None, dist_to_grounding_line=None,\n                   flood_fill_iStart=None, flood_fill_jStart=None):\n    \"\"\"\n    Set cell widths based on settings in config file to pass to\n    :py:func:`mpas_tools.mesh.creation.build_mesh.build_planar_mesh()`.\n    Requires the following options to be set in the given config section:\n    ``min_spac``, ``max_spac``, ``high_log_speed``, ``low_log_speed``,\n    ``high_dist``, ``low_dist``,``cull_distance``, ``use_speed``,\n    ``use_dist_to_edge``, and ``use_dist_to_grounding_line``.\n\n    Parameters\n    ----------\n    section : str\n        Section of the config file from which to read parameters\n    thk : numpy.ndarray\n        Ice thickness field from gridded dataset,\n        usually after trimming to flood fill mask\n    bed : numpy.ndarray\n        Bed topography from gridded dataset\n    vx : numpy.ndarray, optional\n        x-component of ice velocity from gridded dataset,\n        usually after trimming to flood fill mask. Can be set to ``None``\n        if ``use_speed == False`` in config file.\n    vy : numpy.ndarray, optional\n        y-component of ice velocity from gridded dataset,\n        usually after trimming to flood fill mask. Can be set to ``None``\n        if ``use_speed == False`` in config file.\n    dist_to_edge : numpy.ndarray, optional\n        Distance from each cell to ice edge, calculated in separate function.\n        Can be set to ``None`` if ``use_dist_to_edge == False`` in config file\n        and you do not want to set large ``cell_width`` where cells will be\n        culled anyway, but this is not recommended.\n    dist_to_grounding_line : numpy.ndarray, optional\n        Distance from each cell to grounding line, calculated in separate\n        function.  Can be set to ``None`` if\n        ``use_dist_to_grounding_line == False`` in config file.\n    flood_fill_iStart : int, optional\n        x-index location to start flood-fill when using bed topography\n    flood_fill_jStart : int, optional\n        y-index location to start flood-fill when using bed topography\n\n    Returns\n    -------\n    cell_width : numpy.ndarray\n        Desired width of MPAS cells based on mesh desnity functions to pass to\n        :py:func:`mpas_tools.mesh.creation.build_mesh.build_planar_mesh()`.\n    \"\"\"\n\n    logger = self.logger\n    section = self.config[section]\n\n    # Get config inputs for cell spacing functions\n    min_spac = float(section.get('min_spac'))\n    max_spac = float(section.get('max_spac'))\n    high_log_speed = float(section.get('high_log_speed'))\n    low_log_speed = float(section.get('low_log_speed'))\n    high_dist = float(section.get('high_dist'))\n    low_dist = float(section.get('low_dist'))\n    high_dist_bed = float(section.get('high_dist_bed'))\n    low_dist_bed = float(section.get('low_dist_bed'))\n    low_bed = float(section.get('low_bed'))\n    high_bed = float(section.get('high_bed'))\n\n    # convert km to m\n    cull_distance = float(section.get('cull_distance')) * 1.e3\n\n    # Cell spacking function based on union of masks\n    if section.get('use_bed') == 'True':\n        logger.info('Using bed elevation for spacing.')\n        if flood_fill_iStart is not None and flood_fill_jStart is not None:\n            logger.info('calling gridded_flood_fill to find \\\n                        bedTopography <= low_bed connected to the ocean.')\n            tic = time.time()\n            # initialize mask to low bed topography\n            in_mask = (bed <= low_bed)\n            # Do not let flood fill reach further than high_dist_bed into\n            # the ice sheet interior.\n            in_mask[np.logical_and(\n                       thk > 0, dist_to_grounding_line >= high_dist_bed)] = 0\n            low_bed_mask = gridded_flood_fill(in_mask,\n                                              iStart=flood_fill_iStart,\n                                              jStart=flood_fill_jStart)\n            toc = time.time()\n            logger.info(f'Flood fill finished in {toc - tic} seconds.')\n        # Use a logistics curve for bed topography spacing.\n        k = 0.05  # This works well, but could try other values\n        spacing_bed = min_spac + (max_spac - min_spac) \/ (1.0 + np.exp(\n                      -k * ( bed - np.mean([high_bed, low_bed]) ) ) )\n        # We only want bed topography to influence spacing within high_dist_bed\n        # from the ice margin. In the region between high_dist_bed and\n        # low_dist_bed, use a linear ramp to damp influence of bed topo.\n        spacing_bed[dist_to_grounding_line >= low_dist_bed] = (\n            ( 1.0 - (dist_to_grounding_line[\n             dist_to_grounding_line >= low_dist_bed] \n             - low_dist_bed) \/ (high_dist_bed - low_dist_bed) ) *\n            spacing_bed[dist_to_grounding_line >= low_dist_bed] +\n            (dist_to_grounding_line[dist_to_grounding_line >= \n             low_dist_bed] - low_dist_bed) \/\n            (high_dist_bed - low_dist_bed) * max_spac )\n        spacing_bed[dist_to_grounding_line >= high_dist_bed] = max_spac\n        if flood_fill_iStart is not None and flood_fill_jStart is not None:\n            spacing_bed[low_bed_mask == 0] = max_spac\n            # Do one more flood fill to eliminate isolated pockets\n            # of high resolution that were separated when we set\n            # spacing_bed[dist_to_grounding_line >= high_dist_bed] = max_spac\n            in_mask2 = (bed <= low_bed)\n            in_mask2[np.logical_and(\n                       thk > 0, spacing_bed > min_spac)] = 0\n            low_bed_mask2 = gridded_flood_fill(in_mask2,\n                                              iStart=flood_fill_iStart,\n                                              jStart=flood_fill_jStart)\n            spacing_bed[low_bed_mask2 == 0] = max_spac\n    else:\n        spacing_bed = max_spac * np.ones_like(thk)\n\n    # Make cell spacing function mapping from log speed to cell spacing\n    if section.get('use_speed') == 'True':\n        logger.info('Using speed for cell spacing')\n        speed = (vx ** 2 + vy ** 2) ** 0.5\n        lspd = np.log10(speed)\n        spacing_speed = np.interp(lspd, [low_log_speed, high_log_speed],\n                            [max_spac, min_spac], left=max_spac,\n                            right=min_spac)\n\n        # Clean up where we have missing velocities. These are usually nans\n        # or the default netCDF _FillValue of ~10.e36\n        missing_data_mask = np.logical_or(\n                               np.logical_or(np.isnan(vx), np.isnan(vy)),\n                               np.logical_or(np.abs(vx) > 1.e5,\n                                             np.abs(vy) > 1.e5))\n        spacing_speed[missing_data_mask] = max_spac\n        logger.info(f'Found {np.sum(missing_data_mask)} points in input '\n                    f'dataset with missing velocity values. Setting '\n                    f'velocity-based spacing to maximum value.')\n\n        spacing_speed[thk == 0.0] = min_spac\n    else:\n        spacing_speed = max_spac * np.ones_like(thk)\n\n    # Make cell spacing function mapping from distance to ice edge\n    if section.get('use_dist_to_edge') == 'True':\n        logger.info('Using distance to ice edge for cell spacing')\n        spacing_edge = np.interp(dist_to_edge, [low_dist, high_dist],\n                             [min_spac, max_spac], left=min_spac,\n                             right=max_spac)\n        spacing_edge[thk == 0.0] = min_spac\n    else:\n        spacing_edge = max_spac * np.ones_like(thk)\n\n    # Make cell spacing function mapping from distance to grounding line\n    if section.get('use_dist_to_grounding_line') == 'True':\n        logger.info('Using distance to grounding line for cell spacing')\n        spacing_gl = np.interp(dist_to_grounding_line, [low_dist, high_dist],\n                             [min_spac, max_spac], left=min_spac,\n                             right=max_spac)\n        spacing_gl[thk == 0.0] = min_spac\n    else:\n        spacing_gl = max_spac * np.ones_like(thk)\n\n    # Merge cell spacing methods\n    cell_width = max_spac * np.ones_like(thk)\n    for width in [spacing_bed, spacing_speed, spacing_edge, spacing_gl]:\n        cell_width = np.minimum(cell_width, width)\n\n    # Set large cell_width in areas we are going to cull anyway (speeds up\n    # whole process). Use 3x the cull_distance to avoid this affecting\n    # cell size in the final mesh. There may be a more rigorous way to set\n    # that distance.\n    if dist_to_edge is not None:\n        mask = np.logical_and(\n            thk == 0.0, dist_to_edge > (3. * cull_distance))\n        cell_width[mask] = max_spac\n\n    return cell_width\n\n\ndef get_dist_to_edge_and_GL(self, thk, topg, x, y, section, window_size=None):\n    \"\"\"\n    Calculate distance from each point to ice edge and grounding line,\n    to be used in mesh density functions in\n    :py:func:`compass.landice.mesh.set_cell_width()`. In future development,\n    this should be updated to use a faster package such as `scikit-fmm`.\n\n    Parameters\n    ----------\n    thk : numpy.ndarray\n        Ice thickness field from gridded dataset,\n        usually after trimming to flood fill mask\n    topg : numpy.ndarray\n        Bed topography field from gridded dataset\n    x : numpy.ndarray\n        x coordinates from gridded dataset\n    y : numpy.ndarray\n        y coordinates from gridded dataset\n    window_size : int or float\n        Size (in meters) of a search 'box' (one-directional) to use\n        to calculate the distance from each cell to the ice margin.\n        Bigger number makes search slower, but if too small, the transition\n        zone could get truncated. We usually want this calculated as the maximum\n        of high_dist and high_dist_bed, but there may be cases in which it is useful\n        to set it manually. However, it should never be smaller than either high_dist\n        or high_dist_bed.\n\n    Returns\n    -------\n    dist_to_edge : numpy.ndarray\n        Distance from each cell to the ice edge\n    dist_to_grounding_line : numpy.ndarray\n        Distance from each cell to the grounding line\n    \"\"\"\n    logger = self.logger\n    section = self.config[section]\n    tic = time.time()\n\n    high_dist = float(section.get('high_dist'))\n    high_dist_bed = float(section.get('high_dist_bed'))\n\n    if window_size is None:\n        window_size = max(high_dist, high_dist_bed)\n    elif window_size < min(high_dist, high_dist_bed):\n        logger.info('WARNING: window_size was set to a value smaller'\n                    ' than high_dist and\/or high_dist_bed. Resetting'\n                    f' window_size to {max(high_dist, high_dist_bed)},'\n                    ' which is  max(high_dist, high_dist_bed)')\n        window_size = max(high_dist, high_dist_bed)\n\n    dx = x[1] - x[0]  # assumed constant and equal in x and y\n    nx = len(x)\n    ny = len(y)\n    sz = thk.shape\n\n    # Create masks to define ice edge and grounding line\n    neighbors = np.array([[1, 0], [-1, 0], [0, 1], [0, -1],\n                          [1, 1], [-1, 1], [1, -1], [-1, -1]])\n\n    ice_mask = thk > 0.0\n    grounded_mask = thk > (-1028.0 \/ 910.0 * topg)\n    floating_mask = np.logical_and(thk < (-1028.0 \/\n                                          910.0 * topg), thk > 0.0)\n    margin_mask = np.zeros(sz, dtype='i')\n    grounding_line_mask = np.zeros(sz, dtype='i')\n\n    for n in neighbors:\n        not_ice_mask = np.logical_not(np.roll(ice_mask, n, axis=[0, 1]))\n        margin_mask = np.logical_or(margin_mask, not_ice_mask)\n\n        not_grounded_mask = np.logical_not(np.roll(grounded_mask,\n                                                   n, axis=[0, 1]))\n        grounding_line_mask = np.logical_or(grounding_line_mask,\n                                            not_grounded_mask)\n\n    # where ice exists and neighbors non-ice locations\n    margin_mask = np.logical_and(margin_mask, ice_mask)\n    # optional - plot mask\n    # plt.pcolor(margin_mask); plt.show()\n\n    # Calculate dist to margin and grounding line\n    [XPOS, YPOS] = np.meshgrid(x, y)\n    dist_to_edge = np.zeros(sz)\n    dist_to_grounding_line = np.zeros(sz)\n\n    d = int(np.ceil(window_size \/ dx))\n    rng = np.arange(-1 * d, d, dtype='i')\n    max_dist = float(d) * dx\n\n    # just look over areas with ice\n    # ind = np.where(np.ravel(thk, order='F') > 0)[0]\n    ind = np.where(np.ravel(thk, order='F') >= 0)[0]  # do it everywhere\n    for iii in range(len(ind)):\n        [i, j] = np.unravel_index(ind[iii], sz, order='F')\n\n        irng = i + rng\n        jrng = j + rng\n\n        # only keep indices in the grid\n        irng = irng[np.nonzero(np.logical_and(irng >= 0, irng < ny))]\n        jrng = jrng[np.nonzero(np.logical_and(jrng >= 0, jrng < nx))]\n\n        dist_to_here = ((XPOS[np.ix_(irng, jrng)] - x[j]) ** 2 +\n                        (YPOS[np.ix_(irng, jrng)] - y[i]) ** 2) ** 0.5\n\n        dist_to_here_edge = dist_to_here.copy()\n        dist_to_here_grounding_line = dist_to_here.copy()\n\n        dist_to_here_edge[margin_mask[np.ix_(irng, jrng)] == 0] = max_dist\n        dist_to_here_grounding_line[grounding_line_mask\n                                    [np.ix_(irng, jrng)] == 0] = max_dist\n\n        dist_to_edge[i, j] = dist_to_here_edge.min()\n        dist_to_grounding_line[i, j] = dist_to_here_grounding_line.min()\n\n    toc = time.time()\n    logger.info('compass.landice.mesh.get_dist_to_edge_and_GL() took {:0.2f} '\n                'seconds'.format(toc - tic))\n\n    return dist_to_edge, dist_to_grounding_line\n"}},"msg":"Fix second flood fill for bed topography spacing\n\nUse 2 * min_spac to define flood fill mask, instead of min_spac.\nThe factor of 2 works well for 1-10km Greenland, but will likely\nbe problem-specific."}},"https:\/\/github.com\/calendulish\/steam-tools-ng":{"c275f53fc105c7516b6117e6d561cae540e4b1fc":{"url":"https:\/\/api.github.com\/repos\/calendulish\/steam-tools-ng\/commits\/c275f53fc105c7516b6117e6d561cae540e4b1fc","html_url":"https:\/\/github.com\/calendulish\/steam-tools-ng\/commit\/c275f53fc105c7516b6117e6d561cae540e4b1fc","message":"Fix log flood on timed events","sha":"c275f53fc105c7516b6117e6d561cae540e4b1fc","keyword":"flooding fix","diff":"diff --git a\/src\/steam_tools_ng\/config.py b\/src\/steam_tools_ng\/config.py\nindex a56d7be..6fdeef3 100644\n--- a\/src\/steam_tools_ng\/config.py\n+++ b\/src\/steam_tools_ng\/config.py\n@@ -259,7 +259,7 @@ def init_logger() -> None:\n     log_file_handler = logger_handlers.RotatingFileHandler(log_directory \/ 'steam-tools-ng.log',\n                                                            backupCount=1,\n                                                            encoding='utf-8')\n-    log_file_handler.setFormatter(logging.Formatter('%(module)s:%(levelname)s (%(funcName)s) => %(message)s'))\n+    log_file_handler.setFormatter(logging.Formatter('%(name)s:%(levelname)s (%(funcName)s) => %(message)s'))\n     log_file_handler.setLevel(getattr(logging, log_level.upper()))\n \n     try:\ndiff --git a\/src\/steam_tools_ng\/console\/utils.py b\/src\/steam_tools_ng\/console\/utils.py\nindex de49c28..af7e0f7 100644\n--- a\/src\/steam_tools_ng\/console\/utils.py\n+++ b\/src\/steam_tools_ng\/console\/utils.py\n@@ -86,21 +86,31 @@ def set_console(\n         info: str = '',\n         error: str = '',\n         level: Tuple[int, int] = (0, 0),\n+        suppress_logging: bool = False,\n ) -> None:\n     for std in (sys.stdout, sys.stderr):\n         print(' ' * (os.get_terminal_size().columns - 1), end='\\r', file=std)\n \n     if not module_data:\n-        module_data = core.utils.ModuleData(display, status, info, error, level)\n+        module_data = core.utils.ModuleData(display, status, info, error, level, suppress_logging=suppress_logging)\n \n     if module_data.error:\n-        log.error(module_data.error)\n+        if not module_data.suppress_logging:\n+            log.error(module_data.error)\n+\n+        print(module_data.error)\n         return\n \n     if module_data.status:\n+        if not module_data.suppress_logging:\n+            log.debug(f\"status data: {module_data.status}\")\n+\n         print(module_data.status, end=' ')\n \n     if module_data.display:\n+        if not module_data.suppress_logging:\n+            log.debug(f\"display data: {module_data.display}\")\n+\n         print(module_data.display, end=' ')\n \n     if module_data.level:\n@@ -116,6 +126,9 @@ def set_console(\n         print(f\"\u250c{'\u2588' * total:{bar_size}}\u2510\", end=' ')\n \n     if module_data.info:\n+        if not module_data.suppress_logging:\n+            log.info(module_data.info)\n+\n         print(module_data.info, sep=' ', end=' ')\n \n     print('', end='\\r')\ndiff --git a\/src\/steam_tools_ng\/core\/steamguard.py b\/src\/steam_tools_ng\/core\/steamguard.py\nindex dae7e2c..a81cdcf 100644\n--- a\/src\/steam_tools_ng\/core\/steamguard.py\n+++ b\/src\/steam_tools_ng\/core\/steamguard.py\n@@ -18,6 +18,7 @@\n import aiohttp\n import asyncio\n import binascii\n+import logging\n from typing import AsyncGenerator\n \n from stlib import universe, webapi\n@@ -29,6 +30,7 @@\n except ImportError as exception:\n     client = None\n \n+log = logging.getLogger(__name__)\n _ = i18n.get_translation\n \n \n@@ -77,8 +79,7 @@ async def main() -> AsyncGenerator[utils.ModuleData, None]:\n         yield utils.ModuleData(status=_(\"Steam Client is not running\"), info=_(\"Waiting Changes\"))\n         await asyncio.sleep(10)\n     else:\n-        yield utils.ModuleData(status=_(\"Loading...\"))\n-\n+        log.info(_(\"New code in 30 seconds\"))\n         seconds = 30 - (server_time % 30)\n \n         for past_time in range(seconds * 8):\n@@ -87,6 +88,7 @@ async def main() -> AsyncGenerator[utils.ModuleData, None]:\n                 status=_(\"Running\"),\n                 info=_(\"New code in {} seconds\").format(seconds - round(past_time \/ 8)),\n                 level=(past_time, seconds * 8),\n+                suppress_logging=True,\n             )\n \n             await asyncio.sleep(0.125)\ndiff --git a\/src\/steam_tools_ng\/core\/utils.py b\/src\/steam_tools_ng\/core\/utils.py\nindex 6812353..16255a4 100644\n--- a\/src\/steam_tools_ng\/core\/utils.py\n+++ b\/src\/steam_tools_ng\/core\/utils.py\n@@ -15,8 +15,10 @@\n # You should have received a copy of the GNU General Public License\n # along with this program. If not, see http:\/\/www.gnu.org\/licenses\/.\n #\n-import asyncio\n \n+import asyncio\n+import inspect\n+import logging\n import time\n from dataclasses import dataclass\n from functools import cache, wraps\n@@ -32,12 +34,18 @@ class ModuleData:\n     level: Tuple[int, int] = (0, 0)\n     action: str = ''\n     raw_data: Any = None\n+    suppress_logging: bool = False\n \n \n async def timed_module_data(wait_offset: int, module_data: ModuleData) -> AsyncGenerator[ModuleData, None]:\n     info = module_data.info\n     assert module_data.level == (0, 0), \"level should not be used here\"\n \n+    module_data.suppress_logging = True\n+    caller = inspect.currentframe().f_back\n+    log = logging.getLogger(caller.f_globals['__name__'])\n+    log.info(info)\n+\n     for past_time in range(wait_offset):\n         current_time = round((wait_offset - past_time) \/ 60)\n         current_time_size = 'm'\ndiff --git a\/src\/steam_tools_ng\/gtk\/window.py b\/src\/steam_tools_ng\/gtk\/window.py\nindex b10ab2f..445ed41 100644\n--- a\/src\/steam_tools_ng\/gtk\/window.py\n+++ b\/src\/steam_tools_ng\/gtk\/window.py\n@@ -454,28 +454,37 @@ def set_status(\n             info: str = '',\n             error: str = '',\n             level: Tuple[int, int] = (0, 0),\n+            suppress_logging: bool = False,\n     ) -> None:\n         _status = getattr(self, f'{module}_status')\n \n         if not module_data:\n-            module_data = core.utils.ModuleData(display, status, info, error, level)\n+            module_data = core.utils.ModuleData(display, status, info, error, level, suppress_logging=suppress_logging)\n \n         if module_data.display:\n-            # log.debug(f\"display data: {module_data.display}\")\n+            if not module_data.suppress_logging:\n+                log.debug(f\"display data: {module_data.display}\")\n+\n             _status.set_display(module_data.display)\n         else:\n             _status.unset_display()\n \n         if module_data.status:\n-            # log.debug(f\"status data: {module_data.status}\")\n+            if not module_data.suppress_logging:\n+                log.debug(f\"status data: {module_data.status}\")\n+\n             _status.set_status(module_data.status)\n \n         if module_data.info:\n-            # log.debug(f\"info data: {module_data.info}\")\n+            if not module_data.suppress_logging:\n+                log.info(module_data.info)\n+\n             _status.set_info(module_data.info)\n \n         if module_data.error:\n-            log.error(module_data.error)\n+            if not module_data.suppress_logging:\n+                log.error(module_data.error)\n+\n             _status.set_error(module_data.error)\n \n         if module_data.level:\n","files":{"\/src\/steam_tools_ng\/config.py":{"changes":[{"diff":"\n     log_file_handler = logger_handlers.RotatingFileHandler(log_directory \/ 'steam-tools-ng.log',\n                                                            backupCount=1,\n                                                            encoding='utf-8')\n-    log_file_handler.setFormatter(logging.Formatter('%(module)s:%(levelname)s (%(funcName)s) => %(message)s'))\n+    log_file_handler.setFormatter(logging.Formatter('%(name)s:%(levelname)s (%(funcName)s) => %(message)s'))\n     log_file_handler.setLevel(getattr(logging, log_level.upper()))\n \n     try:","add":1,"remove":1,"filename":"\/src\/steam_tools_ng\/config.py","badparts":["    log_file_handler.setFormatter(logging.Formatter('%(module)s:%(levelname)s (%(funcName)s) => %(message)s'))"],"goodparts":["    log_file_handler.setFormatter(logging.Formatter('%(name)s:%(levelname)s (%(funcName)s) => %(message)s'))"]}],"source":"\n from collections import OrderedDict import asyncio import configparser import locale import logging import os import site import sys from pathlib import Path from typing import Any, Mapping from stlib import plugins as stlib_plugins from. import i18n, logger_handlers parser=configparser.RawConfigParser() log=logging.getLogger(__name__) if Path('src').is_dir(): data_dir=Path('config') elif hasattr(sys, 'frozen') or sys.platform=='win32': data_dir=Path(os.environ['LOCALAPPDATA']) else: data_dir=Path(os.getenv('XDG_CONFIG_HOME', Path.home() \/ '.config')) config_file_directory=data_dir \/ 'steam-tools-ng' config_file_name='steam-tools-ng.config' config_file=config_file_directory \/ config_file_name try: from stlib import client except ImportError as exception: log.error(str(exception)) client=None def _(message): return message gtk_themes=OrderedDict([ ('light', _(\"Light\")), ('dark', _(\"Dark\")), ]) log_levels=OrderedDict([ ('critical', _(\"Critical\")), ('error', _(\"Error\")), ('warning', _(\"Warning\")), ('info', _(\"Info\")), ('debug', _(\"Debug\")), ]) translations=OrderedDict([ ('en', _(\"English\")), ('pt_BR', _(\"Portuguese(Brazil)\")), ('fr', _(\"French\")), ]) giveaway_types=OrderedDict([ ('main', _(\"Main Giveaways\")), ('new', _(\"New Giveaways\")), ('recommended', _(\"Recommended\")), ('wishlist', _(\"Wishlist Only\")), ('group', _('Group Only')), ]) giveaway_sort_types=OrderedDict([ ('name', _(\"Name\")), ('copies', _(\"Copies\")), ('points', _(\"Points\")), ('level', _(\"Level\")), ]) plugins=OrderedDict([ (\"coupons\", _(\"Free Coupons\")), (\"confirmations\", _(\"Confirmations\")), (\"steamtrades\", _(\"Steam Trades\")), (\"steamgifts\", _(\"Steam Gifts\")), (\"steamguard\", _(\"Steam Guard\")), (\"cardfarming\", _(\"Card Farming\")), ]) _=i18n.get_translation if sys.platform=='win32': event_loop=asyncio.ProactorEventLoop() file_manager='explorer' else: file_manager='xdg-open' event_loop=asyncio.new_event_loop() asyncio.set_event_loop(event_loop) default_config: Mapping[str, Mapping[str, Any]]={ 'logger':{ 'log_directory': data_dir \/ 'steam-tools-ng', 'log_level': 'debug', 'log_console_level': 'info', 'log_color': True, }, 'steam':{ 'api_url': 'https:\/\/api.steampowered.com', }, 'coupons':{ 'enable': True, 'botid_to_donate': '76561198018370992', 'botids': '76561198018370992', 'appid': '753', 'contextid': '3', 'token_to_donate': '6Z6Xn5NM', 'tokens': '6Z6Xn5NM', 'blacklist': '', 'last_trade_time': 0, }, 'confirmations':{ 'enable': True, }, 'steamguard':{ 'enable': True, }, 'steamtrades':{ 'enable': True, 'wait_for_bump': 3700, 'trade_ids': '', }, 'steamgifts':{ 'enable': True, 'wait_for_giveaways': 3700, 'giveaway_type': 'main', 'developer_giveaways': 'True', 'sort': 'name', 'reverse_sorting': False, }, 'cardfarming':{ 'enable': True, 'reverse_sorting': False, 'mandatory_waiting': 7200, 'wait_while_running': 300, 'wait_for_drops': 120, 'max_concurrency': 50, 'invisible': True, }, 'fakerun':{ 'cakes': '', }, 'general':{ 'theme': 'light', 'show_close_button': True, 'language': str(locale.getdefaultlocale()[0]), }, 'login':{ 'steamid': 0, 'deviceid': '', 'token': '', 'token_secure': '', 'oauth_token': '', 'account_name': '', 'shared_secret': '', 'identity_secret': '', 'password': '', }, } def update_log_level(type_: str, level_string: str) -> None: level=getattr(logging, level_string.upper()) file_handler, console_handler, *extra_handlers=logging.root.handlers if type_==\"console\": console_handler.setLevel(level) else: file_handler.setLevel(level) def validate_config(section: str, option: str, defaults: OrderedDict[str, str]) -> None: value=parser.get(section, option) if value and value not in defaults.keys(): if option=='language': log.error(_(\"Unsupported language requested. Fallbacking to English.\")) new('general', 'language', 'en') return raise configparser.Error(_(\"Please, fix your config file. Available values for{}:\\n{}\").format( option, ', '.join(defaults.keys()), )) def init() -> None: config_file_directory.mkdir(parents=True, exist_ok=True) parser.read_dict(default_config) if config_file.is_file(): parser.read(config_file) if(parser.get('steam', 'api_url') in[ 'https:\/\/api.lara.monster', 'https:\/\/api.lara.click', ]): new('steam', 'api_url', default_config['steam']['api_url']) log_directory=Path(parser.get(\"logger\", \"log_directory\")) if not log_directory.is_dir(): log.error(_(\"Incorrect log directory. Fallbacking to default.\")) log_directory=data_dir \/ 'steam-tools-ng' new(\"logger\", \"log_directory\", log_directory) validate_config(\"logger\", \"log_level\", log_levels) validate_config(\"logger\", \"log_console_level\", log_levels) validate_config(\"general\", \"theme\", gtk_themes) validate_config(\"general\", \"language\", translations) validate_config(\"steamgifts\", \"giveaway_type\", giveaway_types) validate_config(\"steamgifts\", \"sort\", giveaway_sort_types) log_directory.mkdir(parents=True, exist_ok=True) stlib_plugins.add_search_paths( str(Path(os.getcwd(), 'lib', 'stlib-plugins')), *[str(Path(site_, 'stlib-plugins')) for site_ in site.getsitepackages()], str(Path(site.getusersitepackages(), 'stlib-plugins')), ) if not stlib_plugins.has_plugin(\"steamtrades\"): new(\"steamtrades\", \"enable\", False) if not stlib_plugins.has_plugin(\"steamgifts\"): new(\"steamgifts\", \"enable\", False) if not client: new(\"cardfarming\", \"enable\", False) def init_logger() -> None: log_directory=Path(parser.get(\"logger\", \"log_directory\")) log_level=parser.get(\"logger\", \"log_level\") log_console_level=parser.get(\"logger\", \"log_console_level\") log_file_handler=logger_handlers.RotatingFileHandler(log_directory \/ 'steam-tools-ng.log', backupCount=1, encoding='utf-8') log_file_handler.setFormatter(logging.Formatter('%(module)s:%(levelname)s(%(funcName)s)=> %(message)s')) log_file_handler.setLevel(getattr(logging, log_level.upper())) try: log_file_handler.doRollover() except PermissionError: log.debug(_(\"Unable to open steam-tools-ng.log\")) log_file_handler.close() log_file_handler=logger_handlers.NullHandler() log_console_handler=logger_handlers.ColoredStreamHandler() log_console_handler.setLevel(getattr(logging, log_console_level.upper())) logging.basicConfig(level=logging.DEBUG, handlers=[log_file_handler, log_console_handler]) def new(section: str, option: str, value: Any) -> None: if option==\"log_level\": update_log_level(\"file\", value) elif option==\"log_console_level\": update_log_level(\"console\", value) if parser.get(section, option, fallback='') !=str(value): log.debug(_('Saving{}:{} on config file').format(section, option)) parser.set(section, option, str(value)) with open(config_file, 'w', encoding=\"utf8\") as config_file_object: parser.write(config_file_object) else: log.debug(_('Not saving{}:{} because values are already updated').format(section, option)) def remove(section: str, option: str) -> None: new(section, option, '') ","sourceWithComments":"#!\/usr\/bin\/env python\n#\n# Lara Maia <dev@lara.monster> 2015 ~ 2023\n#\n# The Steam Tools NG is free software: you can redistribute it and\/or\n# modify it under the terms of the GNU General Public License as\n# published by the Free Software Foundation, either version 3 of\n# the License, or (at your option) any later version.\n#\n# The Steam Tools NG is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n# See the GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program. If not, see http:\/\/www.gnu.org\/licenses\/.\n#\nfrom collections import OrderedDict\n\nimport asyncio\nimport configparser\nimport locale\nimport logging\nimport os\nimport site\nimport sys\nfrom pathlib import Path\nfrom typing import Any, Mapping\n\nfrom stlib import plugins as stlib_plugins\nfrom . import i18n, logger_handlers\n\nparser = configparser.RawConfigParser()\nlog = logging.getLogger(__name__)\n\nif Path('src').is_dir():\n    # development mode\n    data_dir = Path('config')\nelif hasattr(sys, 'frozen') or sys.platform == 'win32':\n    data_dir = Path(os.environ['LOCALAPPDATA'])\nelse:\n    data_dir = Path(os.getenv('XDG_CONFIG_HOME', Path.home() \/ '.config'))\n\nconfig_file_directory = data_dir \/ 'steam-tools-ng'\nconfig_file_name = 'steam-tools-ng.config'\nconfig_file = config_file_directory \/ config_file_name\n\ntry:\n    from stlib import client\nexcept ImportError as exception:\n    log.error(str(exception))\n    client = None\n\n\ndef _(message):\n    return message\n\n\ngtk_themes = OrderedDict([\n    ('light', _(\"Light\")),\n    ('dark', _(\"Dark\")),\n])\n\nlog_levels = OrderedDict([\n    ('critical', _(\"Critical\")),\n    ('error', _(\"Error\")),\n    ('warning', _(\"Warning\")),\n    ('info', _(\"Info\")),\n    ('debug', _(\"Debug\")),\n])\n\ntranslations = OrderedDict([\n    ('en', _(\"English\")),\n    ('pt_BR', _(\"Portuguese (Brazil)\")),\n    ('fr', _(\"French\")),\n])\n\ngiveaway_types = OrderedDict([\n    ('main', _(\"Main Giveaways\")),\n    ('new', _(\"New Giveaways\")),\n    ('recommended', _(\"Recommended\")),\n    ('wishlist', _(\"Wishlist Only\")),\n    ('group', _('Group Only')),\n])\n\ngiveaway_sort_types = OrderedDict([\n    ('name', _(\"Name\")),\n    ('copies', _(\"Copies\")),\n    ('points', _(\"Points\")),\n    ('level', _(\"Level\")),\n])\n\nplugins = OrderedDict([\n    (\"coupons\", _(\"Free Coupons\")),\n    (\"confirmations\", _(\"Confirmations\")),\n    (\"steamtrades\", _(\"Steam Trades\")),\n    (\"steamgifts\", _(\"Steam Gifts\")),\n    (\"steamguard\", _(\"Steam Guard\")),\n    (\"cardfarming\", _(\"Card Farming\")),\n])\n\n_ = i18n.get_translation\n\nif sys.platform == 'win32':\n    event_loop = asyncio.ProactorEventLoop()\n    file_manager = 'explorer'\nelse:\n    file_manager = 'xdg-open'\n    event_loop = asyncio.new_event_loop()\n\nasyncio.set_event_loop(event_loop)\n\ndefault_config: Mapping[str, Mapping[str, Any]] = {\n    'logger': {\n        'log_directory': data_dir \/ 'steam-tools-ng',\n        'log_level': 'debug',\n        'log_console_level': 'info',\n        'log_color': True,\n    },\n    'steam': {\n        'api_url': 'https:\/\/api.steampowered.com',\n    },\n    'coupons': {\n        'enable': True,\n        'botid_to_donate': '76561198018370992',\n        'botids': '76561198018370992',\n        'appid': '753',\n        'contextid': '3',\n        'token_to_donate': '6Z6Xn5NM',\n        'tokens': '6Z6Xn5NM',\n        'blacklist': '',\n        'last_trade_time': 0,\n    },\n    'confirmations': {\n        'enable': True,\n    },\n    'steamguard': {\n        'enable': True,\n    },\n    'steamtrades': {\n        'enable': True,\n        'wait_for_bump': 3700,\n        'trade_ids': '',\n    },\n    'steamgifts': {\n        'enable': True,\n        'wait_for_giveaways': 3700,\n        'giveaway_type': 'main',\n        'developer_giveaways': 'True',\n        'sort': 'name',\n        'reverse_sorting': False,\n    },\n    'cardfarming': {\n        'enable': True,\n        'reverse_sorting': False,\n        'mandatory_waiting': 7200,\n        'wait_while_running': 300,\n        'wait_for_drops': 120,\n        'max_concurrency': 50,\n        'invisible': True,\n    },\n    'fakerun': {\n        'cakes': '',\n    },\n    'general': {\n        'theme': 'light',\n        'show_close_button': True,\n        'language': str(locale.getdefaultlocale()[0]),\n    },\n    'login': {\n        'steamid': 0,\n        'deviceid': '',\n        'token': '',\n        'token_secure': '',\n        'oauth_token': '',\n        'account_name': '',\n        'shared_secret': '',\n        'identity_secret': '',\n        'password': '',\n    },\n}\n\n\ndef update_log_level(type_: str, level_string: str) -> None:\n    level = getattr(logging, level_string.upper())\n    file_handler, console_handler, *extra_handlers = logging.root.handlers\n\n    if type_ == \"console\":\n        console_handler.setLevel(level)\n    else:\n        file_handler.setLevel(level)\n\n\ndef validate_config(section: str, option: str, defaults: OrderedDict[str, str]) -> None:\n    value = parser.get(section, option)\n\n    if value and value not in defaults.keys():\n        if option == 'language':\n            log.error(_(\"Unsupported language requested. Fallbacking to English.\"))\n            new('general', 'language', 'en')\n            return\n\n        raise configparser.Error(_(\"Please, fix your config file. Available values for {}:\\n{}\").format(\n            option,\n            ', '.join(defaults.keys()),\n        ))\n\n\ndef init() -> None:\n    config_file_directory.mkdir(parents=True, exist_ok=True)\n    parser.read_dict(default_config)\n\n    if config_file.is_file():\n        parser.read(config_file)\n\n    # fallback deprecated values\n    if (parser.get('steam', 'api_url') in [\n        'https:\/\/api.lara.monster', 'https:\/\/api.lara.click',\n    ]):\n        new('steam', 'api_url', default_config['steam']['api_url'])\n\n    log_directory = Path(parser.get(\"logger\", \"log_directory\"))\n\n    if not log_directory.is_dir():\n        log.error(_(\"Incorrect log directory. Fallbacking to default.\"))\n        log_directory = data_dir \/ 'steam-tools-ng'\n        new(\"logger\", \"log_directory\", log_directory)\n\n    validate_config(\"logger\", \"log_level\", log_levels)\n    validate_config(\"logger\", \"log_console_level\", log_levels)\n    validate_config(\"general\", \"theme\", gtk_themes)\n    validate_config(\"general\", \"language\", translations)\n    validate_config(\"steamgifts\", \"giveaway_type\", giveaway_types)\n    validate_config(\"steamgifts\", \"sort\", giveaway_sort_types)\n\n    log_directory.mkdir(parents=True, exist_ok=True)\n\n    stlib_plugins.add_search_paths(\n        str(Path(os.getcwd(), 'lib', 'stlib-plugins')),\n        *[str(Path(site_, 'stlib-plugins')) for site_ in site.getsitepackages()],\n        str(Path(site.getusersitepackages(), 'stlib-plugins')),\n    )\n\n    if not stlib_plugins.has_plugin(\"steamtrades\"):\n        new(\"steamtrades\", \"enable\", False)\n\n    if not stlib_plugins.has_plugin(\"steamgifts\"):\n        new(\"steamgifts\", \"enable\", False)\n\n    if not client:\n        new(\"cardfarming\", \"enable\", False)\n\n\ndef init_logger() -> None:\n    log_directory = Path(parser.get(\"logger\", \"log_directory\"))\n    log_level = parser.get(\"logger\", \"log_level\")\n    log_console_level = parser.get(\"logger\", \"log_console_level\")\n\n    log_file_handler = logger_handlers.RotatingFileHandler(log_directory \/ 'steam-tools-ng.log',\n                                                           backupCount=1,\n                                                           encoding='utf-8')\n    log_file_handler.setFormatter(logging.Formatter('%(module)s:%(levelname)s (%(funcName)s) => %(message)s'))\n    log_file_handler.setLevel(getattr(logging, log_level.upper()))\n\n    try:\n        log_file_handler.doRollover()\n    except PermissionError:\n        log.debug(_(\"Unable to open steam-tools-ng.log\"))\n        log_file_handler.close()\n        log_file_handler = logger_handlers.NullHandler()  # type: ignore\n\n    log_console_handler = logger_handlers.ColoredStreamHandler()\n    log_console_handler.setLevel(getattr(logging, log_console_level.upper()))\n\n    # noinspection PyArgumentList\n    logging.basicConfig(level=logging.DEBUG, handlers=[log_file_handler, log_console_handler])\n\n\ndef new(section: str, option: str, value: Any) -> None:\n    if option == \"log_level\":\n        update_log_level(\"file\", value)\n    elif option == \"log_console_level\":\n        update_log_level(\"console\", value)\n\n    if parser.get(section, option, fallback='') != str(value):\n        log.debug(_('Saving {}:{} on config file').format(section, option))\n        parser.set(section, option, str(value))\n\n        with open(config_file, 'w', encoding=\"utf8\") as config_file_object:\n            parser.write(config_file_object)\n    else:\n        log.debug(_('Not saving {}:{} because values are already updated').format(section, option))\n\n\ndef remove(section: str, option: str) -> None:\n    # Some GUI checks will fail if option doesn't exists\n    new(section, option, '')\n    # parser.remove_option(section, option)\n\n    # with open(config_file, 'w', encoding=\"utf8\") as config_file_object:\n    #    parser.write(config_file_object)\n"},"\/src\/steam_tools_ng\/console\/utils.py":{"changes":[{"diff":"\n         info: str = '',\n         error: str = '',\n         level: Tuple[int, int] = (0, 0),\n+        suppress_logging: bool = False,\n ) -> None:\n     for std in (sys.stdout, sys.stderr):\n         print(' ' * (os.get_terminal_size().columns - 1), end='\\r', file=std)\n \n     if not module_data:\n-        module_data = core.utils.ModuleData(display, status, info, error, level)\n+        module_data = core.utils.ModuleData(display, status, info, error, level, suppress_logging=suppress_logging)\n \n     if module_data.error:\n-        log.error(module_data.error)\n+        if not module_data.suppress_logging:\n+            log.error(module_data.error)\n+\n+        print(module_data.error)\n         return\n \n     if module_data.status:\n+        if not module_data.suppress_logging:\n+            log.debug(f\"status data: {module_data.status}\")\n+\n         print(module_data.status, end=' ')\n \n     if module_data.display:\n+        if not module_data.suppress_logging:\n+            log.debug(f\"display data: {module_data.display}\")\n+\n         print(module_data.display, end=' ')\n \n     if module_data.level:\n","add":12,"remove":2,"filename":"\/src\/steam_tools_ng\/console\/utils.py","badparts":["        module_data = core.utils.ModuleData(display, status, info, error, level)","        log.error(module_data.error)"],"goodparts":["        suppress_logging: bool = False,","        module_data = core.utils.ModuleData(display, status, info, error, level, suppress_logging=suppress_logging)","        if not module_data.suppress_logging:","            log.error(module_data.error)","        print(module_data.error)","        if not module_data.suppress_logging:","            log.debug(f\"status data: {module_data.status}\")","        if not module_data.suppress_logging:","            log.debug(f\"display data: {module_data.display}\")"]}],"source":"\n import asyncio import logging import os import sys from typing import Optional, Union, List, Tuple, Any from.. import i18n, core log=logging.getLogger(__name__) _=i18n.get_translation def safe_input( msg: str, default_response: Optional[bool]=None, custom_choices: Optional[List[str]]=None, ) -> Union[bool, str]: if default_response and custom_choices: raise AttributeError(\"You can not use both default_response and custom_choices\") if default_response is True: options=_('[Y\/n]') elif default_response is False: options=_('[y\/N]') elif custom_choices: options=f\"Your choice[{'\/'.join(custom_choices)}]\" else: options='' while True: try: user_input=input(f'{msg}{options}: ') if custom_choices: if not user_input: raise ValueError(_('Invalid response from user')) if user_input.lower() in custom_choices: return user_input.lower() raise ValueError(_('{} is not an accepted value').format(user_input)) if default_response is None: if len(user_input) > 2: return user_input raise ValueError(_('Invalid response from user')) if not user_input: return default_response if user_input.lower()==_('y'): return True if user_input.lower()==_('n'): return False raise ValueError(_('{} is not an accepted value').format(user_input)) except ValueError as exception: log.error(exception.args[0]) log.error(_('Please, try again.')) def set_console( module_data: Optional[core.utils.ModuleData]=None, *, display: str='', status: str='', info: str='', error: str='', level: Tuple[int, int]=(0, 0), ) -> None: for std in(sys.stdout, sys.stderr): print(' ' *(os.get_terminal_size().columns -1), end='\\r', file=std) if not module_data: module_data=core.utils.ModuleData(display, status, info, error, level) if module_data.error: log.error(module_data.error) return if module_data.status: print(module_data.status, end=' ') if module_data.display: print(module_data.display, end=' ') if module_data.level: progress=module_data.level[0] +1 total=module_data.level[1] bar_size=20 if total > 0: total=int(progress * bar_size \/ total) else: total=bar_size print(f\"\u250c{'\u2588' * total:{bar_size}}\u2510\", end=' ') if module_data.info: print(module_data.info, sep=' ', end=' ') print('', end='\\r') def safe_task_callback(task: asyncio.Task[Any]) -> None: if task.cancelled(): log.debug(_(\"%s has been stopped due user request\"), task.get_coro()) return exception=task.exception() if exception and not isinstance(exception, asyncio.CancelledError): stack=task.get_stack() for frame in stack: log.critical(\"%s at %s\", type(exception).__name__, frame) log.critical(\"Fatal Error: %s\", str(exception)) loop=asyncio.get_running_loop() loop.stop() sys.exit(1) ","sourceWithComments":"#!\/usr\/bin\/env python\n#\n# Lara Maia <dev@lara.monster> 2015 ~ 2023\n#\n# The Steam Tools NG is free software: you can redistribute it and\/or\n# modify it under the terms of the GNU General Public License as\n# published by the Free Software Foundation, either version 3 of\n# the License, or (at your option) any later version.\n#\n# The Steam Tools NG is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n# See the GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program. If not, see http:\/\/www.gnu.org\/licenses\/.\n#\nimport asyncio\nimport logging\nimport os\nimport sys\nfrom typing import Optional, Union, List, Tuple, Any\n\nfrom .. import i18n, core\n\nlog = logging.getLogger(__name__)\n_ = i18n.get_translation\n\n\ndef safe_input(\n        msg: str,\n        default_response: Optional[bool] = None,\n        custom_choices: Optional[List[str]] = None,\n) -> Union[bool, str]:\n    if default_response and custom_choices:\n        raise AttributeError(\"You can not use both default_response and custom_choices\")\n\n    if default_response is True:\n        options = _('[Y\/n]')\n    elif default_response is False:\n        options = _('[y\/N]')\n    elif custom_choices:\n        options = f\"Your choice [{'\/'.join(custom_choices)}]\"\n    else:\n        options = ''\n\n    while True:\n        try:\n            user_input = input(f'{msg} {options}: ')\n\n            if custom_choices:\n                if not user_input:\n                    raise ValueError(_('Invalid response from user'))\n\n                if user_input.lower() in custom_choices:\n                    return user_input.lower()\n\n                raise ValueError(_('{} is not an accepted value').format(user_input))\n\n            if default_response is None:\n                if len(user_input) > 2:\n                    return user_input\n\n                raise ValueError(_('Invalid response from user'))\n\n            if not user_input:\n                return default_response\n\n            if user_input.lower() == _('y'):\n                return True\n\n            if user_input.lower() == _('n'):\n                return False\n\n            raise ValueError(_('{} is not an accepted value').format(user_input))\n        except ValueError as exception:\n            log.error(exception.args[0])\n            log.error(_('Please, try again.'))\n\n\ndef set_console(\n        module_data: Optional[core.utils.ModuleData] = None,\n        *,\n        display: str = '',\n        status: str = '',\n        info: str = '',\n        error: str = '',\n        level: Tuple[int, int] = (0, 0),\n) -> None:\n    for std in (sys.stdout, sys.stderr):\n        print(' ' * (os.get_terminal_size().columns - 1), end='\\r', file=std)\n\n    if not module_data:\n        module_data = core.utils.ModuleData(display, status, info, error, level)\n\n    if module_data.error:\n        log.error(module_data.error)\n        return\n\n    if module_data.status:\n        print(module_data.status, end=' ')\n\n    if module_data.display:\n        print(module_data.display, end=' ')\n\n    if module_data.level:\n        progress = module_data.level[0] + 1\n        total = module_data.level[1]\n        bar_size = 20\n\n        if total > 0:\n            total = int(progress * bar_size \/ total)\n        else:\n            total = bar_size\n\n        print(f\"\u250c{'\u2588' * total:{bar_size}}\u2510\", end=' ')\n\n    if module_data.info:\n        print(module_data.info, sep=' ', end=' ')\n\n    print('', end='\\r')\n\n\ndef safe_task_callback(task: asyncio.Task[Any]) -> None:\n    if task.cancelled():\n        log.debug(_(\"%s has been stopped due user request\"), task.get_coro())\n        return\n\n    exception = task.exception()\n\n    if exception and not isinstance(exception, asyncio.CancelledError):\n        stack = task.get_stack()\n\n        for frame in stack:\n            log.critical(\"%s at %s\", type(exception).__name__, frame)\n\n        log.critical(\"Fatal Error: %s\", str(exception))\n        loop = asyncio.get_running_loop()\n        loop.stop()\n        sys.exit(1)\n"},"\/src\/steam_tools_ng\/core\/steamguard.py":{"changes":[{"diff":"\n         yield utils.ModuleData(status=_(\"Steam Client is not running\"), info=_(\"Waiting Changes\"))\n         await asyncio.sleep(10)\n     else:\n-        yield utils.ModuleData(status=_(\"Loading...\"))\n-\n+        log.info(_(\"New code in 30 seconds\"))\n         seconds = 30 - (server_time % 30)\n \n         for past_time in range(seconds * 8):\n","add":1,"remove":2,"filename":"\/src\/steam_tools_ng\/core\/steamguard.py","badparts":["        yield utils.ModuleData(status=_(\"Loading...\"))"],"goodparts":["        log.info(_(\"New code in 30 seconds\"))"]}],"source":"\n import aiohttp import asyncio import binascii from typing import AsyncGenerator from stlib import universe, webapi from. import utils from.. import i18n, config try: from stlib import client except ImportError as exception: client=None _=i18n.get_translation @utils.time_offset_cache(ttl=3600) def cached_server_time() -> int: if not client: raise ProcessLookupError with client.SteamGameServer() as server: real_time=server.get_server_real_time() assert isinstance(real_time, int) return real_time async def main() -> AsyncGenerator[utils.ModuleData, None]: shared_secret=config.parser.get(\"login\", \"shared_secret\") webapi_session=webapi.SteamWebAPI.get_session(0) try: server_time=cached_server_time() except ProcessLookupError: yield utils.ModuleData(error=_(\"Steam is not running.\"), info=_(\"Fallbacking server time to WebAPI\")) try: server_time=await webapi_session.get_server_time() except aiohttp.ClientError: raise aiohttp.ClientError( _( \"Unable to Connect. You can try these things:\\n\" \"1. Check your connection\\n\" \"2. Check if Steam Server isn't down\\n\" \"3. Check if Steam Client is running\\n\" ) ) try: if not shared_secret: config.new(\"steamguard\", \"enable\", \"false\") raise ValueError auth_code=universe.generate_steam_code(server_time, shared_secret) except(ValueError, binascii.Error): yield utils.ModuleData(error=_(\"The current shared secret is invalid.\"), info=_(\"Waiting Changes\")) await asyncio.sleep(10) except ProcessLookupError: yield utils.ModuleData(status=_(\"Steam Client is not running\"), info=_(\"Waiting Changes\")) await asyncio.sleep(10) else: yield utils.ModuleData(status=_(\"Loading...\")) seconds=30 -(server_time % 30) for past_time in range(seconds * 8): yield utils.ModuleData( display=auth_code, status=_(\"Running\"), info=_(\"New code in{} seconds\").format(seconds -round(past_time \/ 8)), level=(past_time, seconds * 8), ) await asyncio.sleep(0.125) ","sourceWithComments":"#!\/usr\/bin\/env python\n#\n# Lara Maia <dev@lara.monster> 2015 ~ 2023\n#\n# The Steam Tools NG is free software: you can redistribute it and\/or\n# modify it under the terms of the GNU General Public License as\n# published by the Free Software Foundation, either version 3 of\n# the License, or (at your option) any later version.\n#\n# The Steam Tools NG is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n# See the GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program. If not, see http:\/\/www.gnu.org\/licenses\/.\n#\nimport aiohttp\nimport asyncio\nimport binascii\nfrom typing import AsyncGenerator\n\nfrom stlib import universe, webapi\nfrom . import utils\nfrom .. import i18n, config\n\ntry:\n    from stlib import client\nexcept ImportError as exception:\n    client = None\n\n_ = i18n.get_translation\n\n\n@utils.time_offset_cache(ttl=3600)\ndef cached_server_time() -> int:\n    if not client:\n        raise ProcessLookupError\n\n    with client.SteamGameServer() as server:\n        real_time = server.get_server_real_time()\n        assert isinstance(real_time, int)\n        return real_time\n\n\nasync def main() -> AsyncGenerator[utils.ModuleData, None]:\n    shared_secret = config.parser.get(\"login\", \"shared_secret\")\n    webapi_session = webapi.SteamWebAPI.get_session(0)\n\n    try:\n        server_time = cached_server_time()\n    except ProcessLookupError:\n        yield utils.ModuleData(error=_(\"Steam is not running.\"), info=_(\"Fallbacking server time to WebAPI\"))\n\n        try:\n            server_time = await webapi_session.get_server_time()\n        except aiohttp.ClientError:\n            raise aiohttp.ClientError(\n                _(\n                    \"Unable to Connect. You can try these things:\\n\"\n                    \"1. Check your connection\\n\"\n                    \"2. Check if Steam Server isn't down\\n\"\n                    \"3. Check if Steam Client is running\\n\"\n                )\n            )\n\n    try:\n        if not shared_secret:\n            config.new(\"steamguard\", \"enable\", \"false\")\n            raise ValueError\n\n        auth_code = universe.generate_steam_code(server_time, shared_secret)\n    except (ValueError, binascii.Error):\n        yield utils.ModuleData(error=_(\"The current shared secret is invalid.\"), info=_(\"Waiting Changes\"))\n        await asyncio.sleep(10)\n    except ProcessLookupError:\n        yield utils.ModuleData(status=_(\"Steam Client is not running\"), info=_(\"Waiting Changes\"))\n        await asyncio.sleep(10)\n    else:\n        yield utils.ModuleData(status=_(\"Loading...\"))\n\n        seconds = 30 - (server_time % 30)\n\n        for past_time in range(seconds * 8):\n            yield utils.ModuleData(\n                display=auth_code,\n                status=_(\"Running\"),\n                info=_(\"New code in {} seconds\").format(seconds - round(past_time \/ 8)),\n                level=(past_time, seconds * 8),\n            )\n\n            await asyncio.sleep(0.125)\n"},"\/src\/steam_tools_ng\/core\/utils.py":{"changes":[{"diff":"\n # You should have received a copy of the GNU General Public License\n # along with this program. If not, see http:\/\/www.gnu.org\/licenses\/.\n #\n-import asyncio\n \n+import asyncio\n+import inspect\n+import logging\n import time\n from dataclasses import dataclass\n from functools import cache, wraps\n","add":3,"remove":1,"filename":"\/src\/steam_tools_ng\/core\/utils.py","badparts":["import asyncio"],"goodparts":["import asyncio","import inspect","import logging"]}],"source":"\n import asyncio import time from dataclasses import dataclass from functools import cache, wraps from typing import Tuple, Any, Callable, AsyncGenerator @dataclass class ModuleData: display: str='' status: str='' info: str='' error: str='' level: Tuple[int, int]=(0, 0) action: str='' raw_data: Any=None async def timed_module_data(wait_offset: int, module_data: ModuleData) -> AsyncGenerator[ModuleData, None]: info=module_data.info assert module_data.level==(0, 0), \"level should not be used here\" for past_time in range(wait_offset): current_time=round((wait_offset -past_time) \/ 60) current_time_size='m' if current_time <=1: current_time=wait_offset -past_time current_time_size='s' module_data.level=(past_time, wait_offset) module_data.info=f'{info}({current_time}{current_time_size})' yield module_data await asyncio.sleep(1) def time_offset_cache(ttl: int=60) -> Callable[[Callable[[], int]], Callable[[], int]]: def wrapper(function_: Any) -> Callable[[], int]: function_=cache(function_) function_.time_base=time.time() @wraps(function_) def wrapped() -> int: if time.time() >=function_.time_base +ttl: function_.cache_clear() function_.time_base=time.time() time_raw=function_() if function_.cache_info().currsize==0: assert isinstance(time_raw, int) return time_raw function_.time_offset=function_.time_base -time_raw return round(time.time() +function_.time_offset) return wrapped return wrapper ","sourceWithComments":"#!\/usr\/bin\/env python\n#\n# Lara Maia <dev@lara.monster> 2015 ~ 2023\n#\n# The Steam Tools NG is free software: you can redistribute it and\/or\n# modify it under the terms of the GNU General Public License as\n# published by the Free Software Foundation, either version 3 of\n# the License, or (at your option) any later version.\n#\n# The Steam Tools NG is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n# See the GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program. If not, see http:\/\/www.gnu.org\/licenses\/.\n#\nimport asyncio\n\nimport time\nfrom dataclasses import dataclass\nfrom functools import cache, wraps\nfrom typing import Tuple, Any, Callable, AsyncGenerator\n\n\n@dataclass\nclass ModuleData:\n    display: str = ''\n    status: str = ''\n    info: str = ''\n    error: str = ''\n    level: Tuple[int, int] = (0, 0)\n    action: str = ''\n    raw_data: Any = None\n\n\nasync def timed_module_data(wait_offset: int, module_data: ModuleData) -> AsyncGenerator[ModuleData, None]:\n    info = module_data.info\n    assert module_data.level == (0, 0), \"level should not be used here\"\n\n    for past_time in range(wait_offset):\n        current_time = round((wait_offset - past_time) \/ 60)\n        current_time_size = 'm'\n\n        if current_time <= 1:\n            current_time = wait_offset - past_time\n            current_time_size = 's'\n\n        module_data.level = (past_time, wait_offset)\n        module_data.info = f'{info} ({current_time}{current_time_size})'\n\n        yield module_data\n        await asyncio.sleep(1)\n\n\ndef time_offset_cache(ttl: int = 60) -> Callable[[Callable[[], int]], Callable[[], int]]:\n    def wrapper(function_: Any) -> Callable[[], int]:\n        function_ = cache(function_)\n        function_.time_base = time.time()\n\n        @wraps(function_)\n        def wrapped() -> int:\n            if time.time() >= function_.time_base + ttl:\n                function_.cache_clear()\n                function_.time_base = time.time()\n\n            time_raw = function_()\n\n            if function_.cache_info().currsize == 0:\n                assert isinstance(time_raw, int)\n                return time_raw\n\n            function_.time_offset = function_.time_base - time_raw\n            return round(time.time() + function_.time_offset)\n\n        return wrapped\n\n    return wrapper\n"},"\/src\/steam_tools_ng\/gtk\/window.py":{"changes":[{"diff":"\n             info: str = '',\n             error: str = '',\n             level: Tuple[int, int] = (0, 0),\n+            suppress_logging: bool = False,\n     ) -> None:\n         _status = getattr(self, f'{module}_status')\n \n         if not module_data:\n-            module_data = core.utils.ModuleData(display, status, info, error, level)\n+            module_data = core.utils.ModuleData(display, status, info, error, level, suppress_logging=suppress_logging)\n \n         if module_data.display:\n-            # log.debug(f\"display data: {module_data.display}\")\n+            if not module_data.suppress_logging:\n+                log.debug(f\"display data: {module_data.display}\")\n+\n             _status.set_display(module_data.display)\n         else:\n             _status.unset_display()\n \n         if module_data.status:\n-            # log.debug(f\"status data: {module_data.status}\")\n+            if not module_data.suppress_logging:\n+                log.debug(f\"status data: {module_data.status}\")\n+\n             _status.set_status(module_data.status)\n \n         if module_data.info:\n-            # log.debug(f\"info data: {module_data.info}\")\n+            if not module_data.suppress_logging:\n+                log.info(module_data.info)\n+\n             _status.set_info(module_data.info)\n \n         if module_data.error:\n-            log.error(module_data.error)\n+            if not module_data.suppress_logging:\n+                log.error(module_data.error)\n+\n             _status.set_error(module_data.error)\n \n         if module_data.level:\n","add":14,"remove":5,"filename":"\/src\/steam_tools_ng\/gtk\/window.py","badparts":["            module_data = core.utils.ModuleData(display, status, info, error, level)","            log.error(module_data.error)"],"goodparts":["            suppress_logging: bool = False,","            module_data = core.utils.ModuleData(display, status, info, error, level, suppress_logging=suppress_logging)","            if not module_data.suppress_logging:","                log.debug(f\"display data: {module_data.display}\")","            if not module_data.suppress_logging:","                log.debug(f\"status data: {module_data.status}\")","            if not module_data.suppress_logging:","                log.info(module_data.info)","            if not module_data.suppress_logging:","                log.error(module_data.error)"]}],"source":"\n import asyncio import contextlib import logging from gi.repository import Gio, Gtk from subprocess import call from typing import Union, Optional, Tuple, Any, List import stlib from stlib import login, universe from. import confirmation, utils, coupon from.. import config, i18n, core _=i18n.get_translation log=logging.getLogger(__name__) if stlib.steamworks_available: from stlib import client class Main(Gtk.ApplicationWindow): def __init__(self, application: Gtk.Application, title: str) -> None: super().__init__(application=application, title=title) self.application=application header_bar=Gtk.HeaderBar() menu=Gio.Menu() menu.append(_(\"Settings\"), \"app.settings\") menu.append(_(\"About\"), \"app.about\") menu.append(_(\"Exit\"), \"app.exit\") menu_button=Gtk.MenuButton() menu_button.set_icon_name(\"open-menu\") menu_button.set_menu_model(menu) header_bar.pack_end(menu_button) self.set_default_size(650, 10) self.set_resizable(False) if config.parser.getboolean(\"general\", \"show_close_button\"): self.set_deletable(True) else: self.set_deletable(False) self.set_titlebar(header_bar) self.set_title('Steam Tools NG') main_grid=Gtk.Grid() main_grid.set_margin_start(10) main_grid.set_margin_end(10) main_grid.set_margin_top(10) main_grid.set_margin_bottom(10) self.set_child(main_grid) self.user_info_label=Gtk.Label() self.user_info_label.set_halign(Gtk.Align.END) header_bar.pack_start(self.user_info_label) stack=Gtk.Stack() stack.set_hhomogeneous(True) main_grid.attach(stack, 1, 2, 1, 1) switcher=Gtk.StackSwitcher() switcher.set_stack(stack) main_grid.attach(switcher, 1, 1, 1, 1) general_section=utils.Section(\"general\", _(\"General\")) general_section.stackup_section(stack) coupons_section=utils.Section(\"coupons\", _(\"Free Coupons\")) coupons_section.stackup_section(stack) self.status_grid=Gtk.Grid() self.status_grid.set_row_spacing(10) self.status_grid.set_column_spacing(10) self.status_grid.set_column_homogeneous(True) general_section.grid.attach(self.status_grid, 0, 0, 4, 1) self.steamtrades_status=utils.Status(5, config.plugins['steamtrades']) self.steamgifts_status=utils.Status(5, config.plugins['steamgifts']) self.steamguard_status=utils.Status(4, config.plugins['steamguard']) self.cardfarming_status=utils.Status(6, config.plugins['cardfarming']) self.confirmations_grid=Gtk.Grid() self.confirmations_grid.set_size_request(655, 250) self.confirmations_grid.set_row_spacing(10) general_section.grid.attach(self.confirmations_grid, 0, 1, 4, 1) self.confirmation_tree=utils.SimpleTextTree( _('confid'), _('creatorid'), _('key'), _('give'), _('to'), _('receive'), overlay_scrolling=False, ) self.confirmations_grid.attach(self.confirmation_tree, 0, 0, 4, 1) for index, column in enumerate(self.confirmation_tree.view.get_columns()): if index in(0, 1, 2): column.set_visible(False) if index==4: column.set_fixed_width(140) else: column.set_fixed_width(220) self.confirmation_tree.view.set_has_tooltip(True) self.confirmation_tree.view.connect('query-tooltip', self.on_query_confirmations_tooltip) confirmation_tree_selection=self.confirmation_tree.view.get_selection() confirmation_tree_selection.connect(\"changed\", self.on_tree_selection_changed) accept_button=Gtk.Button() accept_button.set_margin_start(5) accept_button.set_margin_end(5) accept_button.set_label(_('Accept selected')) accept_button.connect('clicked', self.on_validate_confirmations, \"allow\", confirmation_tree_selection) self.confirmations_grid.attach(accept_button, 0, 1, 1, 1) cancel_button=Gtk.Button() cancel_button.set_margin_start(5) cancel_button.set_margin_end(5) cancel_button.set_label(_('Cancel selected')) cancel_button.connect('clicked', self.on_validate_confirmations, \"cancel\", confirmation_tree_selection) self.confirmations_grid.attach(cancel_button, 1, 1, 1, 1) accept_all_button=Gtk.Button() accept_all_button.set_margin_start(5) accept_all_button.set_margin_end(5) accept_all_button.set_label(_('Accept all')) accept_all_button.connect('clicked', self.on_validate_confirmations, \"allow\", self.confirmation_tree.store) self.confirmations_grid.attach(accept_all_button, 2, 1, 1, 1) cancel_all_button=Gtk.Button() cancel_all_button.set_margin_start(5) cancel_all_button.set_margin_end(5) cancel_all_button.set_label(_('Cancel all')) cancel_all_button.connect('clicked', self.on_validate_confirmations, \"cancel\", self.confirmation_tree.store) self.confirmations_grid.attach(cancel_all_button, 3, 1, 1, 1) self.coupon_grid=Gtk.Grid() self.coupon_grid.set_size_request(655, 300) self.coupon_grid.set_row_spacing(10) coupons_section.grid.attach(self.coupon_grid, 0, 1, 1, 1) self.coupon_warning=Gtk.Label() self.coupon_warning.set_markup(utils.markup( _(\"Warning: It's a heavy uncached operation. Fetch only once a day or you will be blocked.\"), color='red', )) coupons_section.grid.attach(self.coupon_warning, 0, 0, 1, 1) self.coupon_tree=utils.SimpleTextTree( _('price'), _('name'), 'link', 'botid', 'token', 'assetid', overlay_scrolling=False, model=Gtk.ListStore, ) self.coupon_tree.store.set_sort_column_id(0, Gtk.SortType.ASCENDING) self.coupon_tree.store.set_sort_func(0, self.coupon_sorting) self.coupon_grid.attach(self.coupon_tree, 0, 0, 4, 2) for index, column in enumerate(self.coupon_tree.view.get_columns()): if index in(2, 3, 4, 5): column.set_visible(False) self.coupon_tree.view.connect('row-activated', self.on_coupon_double_clicked) coupon_tree_selection=self.coupon_tree.view.get_selection() coupon_tree_selection.connect(\"changed\", self.on_tree_selection_changed) self.coupon_progress=Gtk.LevelBar() self.coupon_grid.attach(self.coupon_progress, 0, 3, 4, 1) get_coupon_button=Gtk.Button() get_coupon_button.set_margin_start(5) get_coupon_button.set_margin_end(5) get_coupon_button.set_label(_('Get selected coupon')) get_coupon_button.connect('clicked', self.on_coupon_action, coupon_tree_selection) self.coupon_grid.attach(get_coupon_button, 2, 4, 1, 1) give_coupon_button=Gtk.Button() give_coupon_button.set_margin_start(5) give_coupon_button.set_margin_end(5) give_coupon_button.set_label(_('Giveaway your coupons')) give_coupon_button.connect('clicked', self.on_coupon_action) self.coupon_grid.attach(give_coupon_button, 3, 4, 1, 1) fetch_coupons_button=Gtk.Button() fetch_coupons_button.set_margin_start(5) fetch_coupons_button.set_margin_end(5) fetch_coupons_button.set_label(_('Fetch coupons')) fetch_coupons_button.connect('clicked', self.on_fetch_coupons) self.coupon_grid.attach(fetch_coupons_button, 0, 4, 1, 1) self.fetch_coupon_event=asyncio.Event() stop_fetching_coupons_button=Gtk.Button() stop_fetching_coupons_button.set_margin_start(5) stop_fetching_coupons_button.set_margin_end(5) stop_fetching_coupons_button.set_label(_('Stop fetching coupons')) stop_fetching_coupons_button.connect('clicked', self.on_stop_fetching_coupons) self.coupon_grid.attach(stop_fetching_coupons_button, 1, 4, 1, 1) self.statusbar=utils.StatusBar() main_grid.attach(self.statusbar, 1, 3, 1, 1) self.connect(\"destroy\", self.application.on_exit_activate) self.connect(\"close-request\", self.application.on_exit_activate) loop=asyncio.get_event_loop() plugin_switch_task=loop.create_task(self.plugin_switch()) plugin_switch_task.add_done_callback(utils.safe_task_callback) user_info_task=loop.create_task(self.user_info()) user_info_task.add_done_callback(utils.safe_task_callback) async def user_info(self) -> None: while self.get_realized(): theme=config.parser.get('general', 'theme') account_name=config.parser.get('login', 'account_name') steamid_raw=config.parser.get('login', 'steamid') steamid=universe.generate_steamid(steamid_raw) login_session=None with contextlib.suppress(IndexError): login_session=login.Login.get_session(0) if not login_session or not await login_session.is_logged_in(steamid): self.application.main_window.user_info_label.set_markup( utils.markup( _('Not logged in'), color='darkred' if theme=='light' else 'red', size='small', ) ) await asyncio.sleep(5) continue self.application.main_window.user_info_label.set_markup( utils.markup( _('You are logged in as:\\n'), color='darkgreen' if theme=='light' else 'green', size='small', ) + utils.markup( account_name, color='darkblue' if theme=='light' else 'blue', size='small', ) + utils.markup( f\"{self.application.steamid.id3_string}\", color='grey', size='small', ) ) await asyncio.sleep(30) async def plugin_switch(self) -> None: plugins_enabled: List[str]=[] plugins_status: List[utils.Status]=[] for plugin_name in config.plugins.keys(): if plugin_name in[\"confirmations\", \"coupons\"]: continue plugins_status.append(getattr(self, f'{plugin_name}_status')) while self.get_realized(): plugins: List[str]=[] for plugin_name in config.plugins.keys(): enabled=config.parser.getboolean(plugin_name, \"enable\") if plugin_name==\"confirmations\": if enabled: self.confirmations_grid.show() else: self.confirmations_grid.hide() continue if plugin_name==\"coupons\": if enabled: self.coupon_grid.show() else: self.coupon_grid.hide() continue if enabled: plugins.append(plugin_name) if plugins==plugins_enabled: self.status_grid.show() await asyncio.sleep(1) continue plugins_enabled=plugins for status in plugins_status: self.status_grid.remove(status) for index, plugin_name in enumerate(plugins_enabled): status=getattr(self, f'{plugin_name}_status') if index==0: if len(plugins_enabled) >=2: self.status_grid.attach(status, 0, 0, 1, 1) else: self.status_grid.attach(status, 0, 0, 2, 1) if index==1 and len(plugins_enabled) >=2: self.status_grid.attach(status, 1, 0, 1, 1) if index==2: if len(plugins_enabled)==3: self.status_grid.attach(status, 0, 1, 2, 1) else: self.status_grid.attach(status, 0, 1, 1, 1) if index==3 and len(plugins_enabled)==4: self.status_grid.attach(status, 1, 1, 1, 1) status.show() @staticmethod def on_query_confirmations_tooltip( tree_view: Gtk.TreeView, x_coord: int, y_coord: int, keyboard_tip: bool, tooltip: Gtk.Tooltip, ) -> bool: context=tree_view.get_tooltip_context(x_coord, y_coord, keyboard_tip) if context[0]: if context.model.iter_depth(context.iter) !=0: return False tooltip.set_text( f'ConfID:{context.model.get_value(context.iter, 0)}\\n' f'CreatorID{context.model.get_value(context.iter, 1)}\\n' f'Key:{context.model.get_value(context.iter, 2)}' ) return True return False def on_fetch_coupons(self, button: Gtk.Button) -> None: self.fetch_coupon_event.set() def on_stop_fetching_coupons(self, button: Gtk.Button) -> None: self.fetch_coupon_event.clear() def on_coupon_action(self, button: Gtk.Button, model: Union[Gtk.TreeModel, Gtk.TreeSelection]=None) -> None: if model: coupon_dialog=coupon.CouponDialog(self, self.application, *model.get_selected()) else: coupon_dialog=coupon.CouponDialog(self, self.application) coupon_dialog.show() def on_validate_confirmations( self, button: Gtk.Button, action: str, model: Union[Gtk.TreeModel, Gtk.TreeSelection]) -> None: if isinstance(model, Gtk.TreeModel): finalize_dialog=confirmation.FinalizeDialog( self, self.application, action, model, False ) else: finalize_dialog=confirmation.FinalizeDialog( self, self.application, action, *model.get_selected() ) finalize_dialog.show() @staticmethod def on_coupon_double_clicked(view: Gtk.TreeView, path: Gtk.TreePath, column: Gtk.TreeViewColumn) -> None: model=view.get_model() url=model[path][2] steam_running=False if stlib.steamworks_available: with contextlib.suppress(ProcessLookupError): with client.SteamGameServer() as server: steam_running=True if steam_running: url=f\"steam:\/\/openurl\/{url}\" call(f'{config.file_manager} \"{url}\"') @staticmethod def on_tree_selection_changed(selection: Gtk.TreeSelection) -> None: model, iter_=selection.get_selected() if iter_: parent=model.iter_parent(iter_) if parent: selection.select_iter(parent) @staticmethod def coupon_sorting(model: Gtk.TreeModel, iter1: Gtk.TreeIter, iter2: Gtk.TreeIter, user_data: Any) -> Any: column, _=model.get_sort_column_id() price1=model.get_value(iter1, column) price2=model.get_value(iter2, column) if float(price1) < float(price2): return -1 if price1==price2: return 0 return 1 def set_status( self, module: str, module_data: Optional[core.utils.ModuleData]=None, *, display: str='', status: str='', info: str='', error: str='', level: Tuple[int, int]=(0, 0), ) -> None: _status=getattr(self, f'{module}_status') if not module_data: module_data=core.utils.ModuleData(display, status, info, error, level) if module_data.display: _status.set_display(module_data.display) else: _status.unset_display() if module_data.status: _status.set_status(module_data.status) if module_data.info: _status.set_info(module_data.info) if module_data.error: log.error(module_data.error) _status.set_error(module_data.error) if module_data.level: _status.set_level(*module_data.level) def get_play_event(self, module: str) -> asyncio.Event: _status=getattr(self, f'{module}_status') assert isinstance(_status, utils.Status) return _status.play_event ","sourceWithComments":"#!\/usr\/bin\/env python\n#\n# Lara Maia <dev@lara.monster> 2015 ~ 2023\n#\n# The Steam Tools NG is free software: you can redistribute it and\/or\n# modify it under the terms of the GNU General Public License as\n# published by the Free Software Foundation, either version 3 of\n# the License, or (at your option) any later version.\n#\n# The Steam Tools NG is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n# See the GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program. If not, see http:\/\/www.gnu.org\/licenses\/.\n#\nimport asyncio\nimport contextlib\nimport logging\nfrom gi.repository import Gio, Gtk\nfrom subprocess import call\nfrom typing import Union, Optional, Tuple, Any, List\n\nimport stlib\nfrom stlib import login, universe\nfrom . import confirmation, utils, coupon\nfrom .. import config, i18n, core\n\n_ = i18n.get_translation\nlog = logging.getLogger(__name__)\n\nif stlib.steamworks_available:\n    from stlib import client\n\n\n# noinspection PyUnusedLocal\nclass Main(Gtk.ApplicationWindow):\n    def __init__(self, application: Gtk.Application, title: str) -> None:\n        super().__init__(application=application, title=title)\n        self.application = application\n        header_bar = Gtk.HeaderBar()\n\n        menu = Gio.Menu()\n        menu.append(_(\"Settings\"), \"app.settings\")\n        menu.append(_(\"About\"), \"app.about\")\n        menu.append(_(\"Exit\"), \"app.exit\")\n\n        menu_button = Gtk.MenuButton()\n        menu_button.set_icon_name(\"open-menu\")\n        menu_button.set_menu_model(menu)\n        header_bar.pack_end(menu_button)\n\n        self.set_default_size(650, 10)\n        self.set_resizable(False)\n\n        if config.parser.getboolean(\"general\", \"show_close_button\"):\n            self.set_deletable(True)\n        else:\n            self.set_deletable(False)\n\n        self.set_titlebar(header_bar)\n        self.set_title('Steam Tools NG')\n\n        main_grid = Gtk.Grid()\n        main_grid.set_margin_start(10)\n        main_grid.set_margin_end(10)\n        main_grid.set_margin_top(10)\n        main_grid.set_margin_bottom(10)\n        self.set_child(main_grid)\n\n        self.user_info_label = Gtk.Label()\n        self.user_info_label.set_halign(Gtk.Align.END)\n        header_bar.pack_start(self.user_info_label)\n\n        stack = Gtk.Stack()\n        stack.set_hhomogeneous(True)\n        main_grid.attach(stack, 1, 2, 1, 1)\n\n        switcher = Gtk.StackSwitcher()\n        switcher.set_stack(stack)\n        main_grid.attach(switcher, 1, 1, 1, 1)\n\n        general_section = utils.Section(\"general\", _(\"General\"))\n        general_section.stackup_section(stack)\n\n        coupons_section = utils.Section(\"coupons\", _(\"Free Coupons\"))\n        coupons_section.stackup_section(stack)\n\n        self.status_grid = Gtk.Grid()\n        self.status_grid.set_row_spacing(10)\n        self.status_grid.set_column_spacing(10)\n        self.status_grid.set_column_homogeneous(True)\n        general_section.grid.attach(self.status_grid, 0, 0, 4, 1)\n\n        # grid managed by plugin switch\n        self.steamtrades_status = utils.Status(5, config.plugins['steamtrades'])\n        self.steamgifts_status = utils.Status(5, config.plugins['steamgifts'])\n        self.steamguard_status = utils.Status(4, config.plugins['steamguard'])\n        self.cardfarming_status = utils.Status(6, config.plugins['cardfarming'])\n\n        self.confirmations_grid = Gtk.Grid()\n        self.confirmations_grid.set_size_request(655, 250)\n        self.confirmations_grid.set_row_spacing(10)\n        general_section.grid.attach(self.confirmations_grid, 0, 1, 4, 1)\n\n        self.confirmation_tree = utils.SimpleTextTree(\n            _('confid'), _('creatorid'), _('key'), _('give'), _('to'), _('receive'),\n            overlay_scrolling=False,\n        )\n\n        self.confirmations_grid.attach(self.confirmation_tree, 0, 0, 4, 1)\n\n        for index, column in enumerate(self.confirmation_tree.view.get_columns()):\n            if index in (0, 1, 2):\n                column.set_visible(False)\n\n            if index == 4:\n                column.set_fixed_width(140)\n            else:\n                column.set_fixed_width(220)\n\n        self.confirmation_tree.view.set_has_tooltip(True)\n        self.confirmation_tree.view.connect('query-tooltip', self.on_query_confirmations_tooltip)\n\n        confirmation_tree_selection = self.confirmation_tree.view.get_selection()\n        confirmation_tree_selection.connect(\"changed\", self.on_tree_selection_changed)\n\n        accept_button = Gtk.Button()\n        accept_button.set_margin_start(5)\n        accept_button.set_margin_end(5)\n        accept_button.set_label(_('Accept selected'))\n        accept_button.connect('clicked', self.on_validate_confirmations, \"allow\", confirmation_tree_selection)\n        self.confirmations_grid.attach(accept_button, 0, 1, 1, 1)\n\n        cancel_button = Gtk.Button()\n        cancel_button.set_margin_start(5)\n        cancel_button.set_margin_end(5)\n        cancel_button.set_label(_('Cancel selected'))\n        cancel_button.connect('clicked', self.on_validate_confirmations, \"cancel\", confirmation_tree_selection)\n        self.confirmations_grid.attach(cancel_button, 1, 1, 1, 1)\n\n        accept_all_button = Gtk.Button()\n        accept_all_button.set_margin_start(5)\n        accept_all_button.set_margin_end(5)\n        accept_all_button.set_label(_('Accept all'))\n        accept_all_button.connect('clicked', self.on_validate_confirmations, \"allow\", self.confirmation_tree.store)\n        self.confirmations_grid.attach(accept_all_button, 2, 1, 1, 1)\n\n        cancel_all_button = Gtk.Button()\n        cancel_all_button.set_margin_start(5)\n        cancel_all_button.set_margin_end(5)\n        cancel_all_button.set_label(_('Cancel all'))\n        cancel_all_button.connect('clicked', self.on_validate_confirmations, \"cancel\", self.confirmation_tree.store)\n        self.confirmations_grid.attach(cancel_all_button, 3, 1, 1, 1)\n\n        self.coupon_grid = Gtk.Grid()\n        self.coupon_grid.set_size_request(655, 300)\n        self.coupon_grid.set_row_spacing(10)\n        coupons_section.grid.attach(self.coupon_grid, 0, 1, 1, 1)\n\n        self.coupon_warning = Gtk.Label()\n        self.coupon_warning.set_markup(utils.markup(\n            _(\"Warning: It's a heavy uncached operation. Fetch only once a day or you will be blocked.\"),\n            color='red',\n        ))\n        coupons_section.grid.attach(self.coupon_warning, 0, 0, 1, 1)\n\n        self.coupon_tree = utils.SimpleTextTree(\n            _('price'), _('name'), 'link', 'botid', 'token', 'assetid',\n            overlay_scrolling=False,\n            model=Gtk.ListStore,\n        )\n\n        self.coupon_tree.store.set_sort_column_id(0, Gtk.SortType.ASCENDING)\n        self.coupon_tree.store.set_sort_func(0, self.coupon_sorting)\n        self.coupon_grid.attach(self.coupon_tree, 0, 0, 4, 2)\n\n        for index, column in enumerate(self.coupon_tree.view.get_columns()):\n            if index in (2, 3, 4, 5):\n                column.set_visible(False)\n\n        self.coupon_tree.view.connect('row-activated', self.on_coupon_double_clicked)\n\n        coupon_tree_selection = self.coupon_tree.view.get_selection()\n        coupon_tree_selection.connect(\"changed\", self.on_tree_selection_changed)\n\n        self.coupon_progress = Gtk.LevelBar()\n        self.coupon_grid.attach(self.coupon_progress, 0, 3, 4, 1)\n\n        get_coupon_button = Gtk.Button()\n        get_coupon_button.set_margin_start(5)\n        get_coupon_button.set_margin_end(5)\n        get_coupon_button.set_label(_('Get selected coupon'))\n        get_coupon_button.connect('clicked', self.on_coupon_action, coupon_tree_selection)\n        self.coupon_grid.attach(get_coupon_button, 2, 4, 1, 1)\n\n        give_coupon_button = Gtk.Button()\n        give_coupon_button.set_margin_start(5)\n        give_coupon_button.set_margin_end(5)\n        give_coupon_button.set_label(_('Giveaway your coupons'))\n        give_coupon_button.connect('clicked', self.on_coupon_action)\n        self.coupon_grid.attach(give_coupon_button, 3, 4, 1, 1)\n\n        fetch_coupons_button = Gtk.Button()\n        fetch_coupons_button.set_margin_start(5)\n        fetch_coupons_button.set_margin_end(5)\n        fetch_coupons_button.set_label(_('Fetch coupons'))\n        fetch_coupons_button.connect('clicked', self.on_fetch_coupons)\n        self.coupon_grid.attach(fetch_coupons_button, 0, 4, 1, 1)\n\n        self.fetch_coupon_event = asyncio.Event()\n\n        stop_fetching_coupons_button = Gtk.Button()\n        stop_fetching_coupons_button.set_margin_start(5)\n        stop_fetching_coupons_button.set_margin_end(5)\n        stop_fetching_coupons_button.set_label(_('Stop fetching coupons'))\n        stop_fetching_coupons_button.connect('clicked', self.on_stop_fetching_coupons)\n        self.coupon_grid.attach(stop_fetching_coupons_button, 1, 4, 1, 1)\n\n        self.statusbar = utils.StatusBar()\n        main_grid.attach(self.statusbar, 1, 3, 1, 1)\n\n        self.connect(\"destroy\", self.application.on_exit_activate)\n        self.connect(\"close-request\", self.application.on_exit_activate)\n\n        loop = asyncio.get_event_loop()\n\n        plugin_switch_task = loop.create_task(self.plugin_switch())\n        plugin_switch_task.add_done_callback(utils.safe_task_callback)\n\n        user_info_task = loop.create_task(self.user_info())\n        user_info_task.add_done_callback(utils.safe_task_callback)\n\n    async def user_info(self) -> None:\n        while self.get_realized():\n            theme = config.parser.get('general', 'theme')\n            account_name = config.parser.get('login', 'account_name')\n            steamid_raw = config.parser.get('login', 'steamid')\n            steamid = universe.generate_steamid(steamid_raw)\n            login_session = None\n\n            with contextlib.suppress(IndexError):\n                login_session = login.Login.get_session(0)\n\n            if not login_session or not await login_session.is_logged_in(steamid):\n                self.application.main_window.user_info_label.set_markup(\n                    utils.markup(\n                        _('Not logged in'),\n                        color='darkred' if theme == 'light' else 'red',\n                        size='small',\n                    )\n                )\n\n                await asyncio.sleep(5)\n                continue\n\n            self.application.main_window.user_info_label.set_markup(\n                utils.markup(\n                    _('You are logged in as:\\n'),\n                    color='darkgreen' if theme == 'light' else 'green',\n                    size='small',\n                ) +\n                utils.markup(\n                    account_name,\n                    color='darkblue' if theme == 'light' else 'blue',\n                    size='small',\n                ) +\n                utils.markup(\n                    f\" {self.application.steamid.id3_string}\",\n                    color='grey',\n                    size='small',\n                )\n            )\n\n            await asyncio.sleep(30)\n\n    async def plugin_switch(self) -> None:\n        plugins_enabled: List[str] = []\n        plugins_status: List[utils.Status] = []\n\n        for plugin_name in config.plugins.keys():\n            if plugin_name in [\"confirmations\", \"coupons\"]:\n                continue\n\n            plugins_status.append(getattr(self, f'{plugin_name}_status'))\n\n        while self.get_realized():\n            plugins: List[str] = []\n\n            for plugin_name in config.plugins.keys():\n                enabled = config.parser.getboolean(plugin_name, \"enable\")\n\n                if plugin_name == \"confirmations\":\n                    if enabled:\n                        self.confirmations_grid.show()\n                    else:\n                        self.confirmations_grid.hide()\n\n                    continue\n\n                if plugin_name == \"coupons\":\n                    if enabled:\n                        self.coupon_grid.show()\n                    else:\n                        self.coupon_grid.hide()\n\n                    continue\n\n                if enabled:\n                    plugins.append(plugin_name)\n\n            if plugins == plugins_enabled:\n                self.status_grid.show()\n                await asyncio.sleep(1)\n                continue\n\n            plugins_enabled = plugins\n\n            for status in plugins_status:\n                self.status_grid.remove(status)\n\n            for index, plugin_name in enumerate(plugins_enabled):\n                status = getattr(self, f'{plugin_name}_status')\n\n                if index == 0:\n                    if len(plugins_enabled) >= 2:\n                        self.status_grid.attach(status, 0, 0, 1, 1)\n                    else:\n                        self.status_grid.attach(status, 0, 0, 2, 1)\n\n                if index == 1 and len(plugins_enabled) >= 2:\n                    self.status_grid.attach(status, 1, 0, 1, 1)\n\n                if index == 2:\n                    if len(plugins_enabled) == 3:\n                        self.status_grid.attach(status, 0, 1, 2, 1)\n                    else:\n                        self.status_grid.attach(status, 0, 1, 1, 1)\n\n                if index == 3 and len(plugins_enabled) == 4:\n                    self.status_grid.attach(status, 1, 1, 1, 1)\n\n                status.show()\n\n    @staticmethod\n    def on_query_confirmations_tooltip(\n            tree_view: Gtk.TreeView,\n            x_coord: int,\n            y_coord: int,\n            keyboard_tip: bool,\n            tooltip: Gtk.Tooltip,\n    ) -> bool:\n        context = tree_view.get_tooltip_context(x_coord, y_coord, keyboard_tip)\n\n        if context[0]:\n            if context.model.iter_depth(context.iter) != 0:\n                return False\n\n            tooltip.set_text(\n                f'ConfID:{context.model.get_value(context.iter, 0)}\\n'\n                f'CreatorID{context.model.get_value(context.iter, 1)}\\n'\n                f'Key:{context.model.get_value(context.iter, 2)}'\n            )\n\n            return True\n\n        return False\n\n    def on_fetch_coupons(self, button: Gtk.Button) -> None:\n        self.fetch_coupon_event.set()\n\n    def on_stop_fetching_coupons(self, button: Gtk.Button) -> None:\n        self.fetch_coupon_event.clear()\n\n    def on_coupon_action(self, button: Gtk.Button, model: Union[Gtk.TreeModel, Gtk.TreeSelection] = None) -> None:\n        if model:\n            coupon_dialog = coupon.CouponDialog(self, self.application, *model.get_selected())\n        else:\n            coupon_dialog = coupon.CouponDialog(self, self.application)\n\n        coupon_dialog.show()\n\n    def on_validate_confirmations(\n            self,\n            button: Gtk.Button,\n            action: str,\n            model: Union[Gtk.TreeModel, Gtk.TreeSelection]) -> None:\n        if isinstance(model, Gtk.TreeModel):\n            finalize_dialog = confirmation.FinalizeDialog(\n                self,\n                self.application,\n                action,\n                model,\n                False\n            )\n        else:\n            finalize_dialog = confirmation.FinalizeDialog(\n                self,\n                self.application,\n                action,\n                *model.get_selected()\n            )\n\n        finalize_dialog.show()\n\n    @staticmethod\n    def on_coupon_double_clicked(view: Gtk.TreeView, path: Gtk.TreePath, column: Gtk.TreeViewColumn) -> None:\n        model = view.get_model()\n        url = model[path][2]\n        steam_running = False\n\n        if stlib.steamworks_available:\n            with contextlib.suppress(ProcessLookupError):\n                with client.SteamGameServer() as server:\n                    steam_running = True\n\n        if steam_running:\n            url = f\"steam:\/\/openurl\/{url}\"\n\n        call(f'{config.file_manager} \"{url}\"')\n\n    @staticmethod\n    def on_tree_selection_changed(selection: Gtk.TreeSelection) -> None:\n        model, iter_ = selection.get_selected()\n\n        if iter_:\n            parent = model.iter_parent(iter_)\n\n            if parent:\n                selection.select_iter(parent)\n\n    @staticmethod\n    def coupon_sorting(model: Gtk.TreeModel, iter1: Gtk.TreeIter, iter2: Gtk.TreeIter, user_data: Any) -> Any:\n        column, _ = model.get_sort_column_id()\n        price1 = model.get_value(iter1, column)\n        price2 = model.get_value(iter2, column)\n\n        if float(price1) < float(price2):\n            return -1\n\n        if price1 == price2:\n            return 0\n\n        return 1\n\n    def set_status(\n            self,\n            module: str,\n            module_data: Optional[core.utils.ModuleData] = None,\n            *,\n            display: str = '',\n            status: str = '',\n            info: str = '',\n            error: str = '',\n            level: Tuple[int, int] = (0, 0),\n    ) -> None:\n        _status = getattr(self, f'{module}_status')\n\n        if not module_data:\n            module_data = core.utils.ModuleData(display, status, info, error, level)\n\n        if module_data.display:\n            # log.debug(f\"display data: {module_data.display}\")\n            _status.set_display(module_data.display)\n        else:\n            _status.unset_display()\n\n        if module_data.status:\n            # log.debug(f\"status data: {module_data.status}\")\n            _status.set_status(module_data.status)\n\n        if module_data.info:\n            # log.debug(f\"info data: {module_data.info}\")\n            _status.set_info(module_data.info)\n\n        if module_data.error:\n            log.error(module_data.error)\n            _status.set_error(module_data.error)\n\n        if module_data.level:\n            _status.set_level(*module_data.level)\n\n    def get_play_event(self, module: str) -> asyncio.Event:\n        _status = getattr(self, f'{module}_status')\n        assert isinstance(_status, utils.Status)\n        return _status.play_event\n"}},"msg":"Fix log flood on timed events"}},"https:\/\/github.com\/richardkhillah\/server-herd":{"0f7e907c79362596fdc18b52dfaf7cfb8faa2374":{"url":"https:\/\/api.github.com\/repos\/richardkhillah\/server-herd\/commits\/0f7e907c79362596fdc18b52dfaf7cfb8faa2374","html_url":"https:\/\/github.com\/richardkhillah\/server-herd\/commit\/0f7e907c79362596fdc18b52dfaf7cfb8faa2374","message":"Fix flooding cycle issues\n\nAdd generic api_calling handle\n\nUpdate graph adjacency list\n\nUpdate parsing issue of IAM requests where ast.literal_eval() was\nparsing an eof. It is not clear whether the eof issue was related to\nasync io between servers or in the str(self.nodes_visited) conversion of\na Record objects nodes_visited.","sha":"0f7e907c79362596fdc18b52dfaf7cfb8faa2374","keyword":"flooding fix","diff":"diff --git a\/client.py b\/client.py\nindex 98da8ed..983179a 100644\n--- a\/client.py\n+++ b\/client.py\n@@ -1,4 +1,4 @@\n-from time import time, strftime\n+from time import time, strftime, sleep\n \n import asyncio\n import json\n@@ -63,40 +63,88 @@ async def time_after(n):\n     return time()\n \n async def main():\n-    hosts = ['kiwi.cs.ucla.edu', 'plum.cs.ucla.edu', 'watermelon.cs.ucla.edu']\n-    coordinates = ['+34.068930-118.445127', '+55.555555-666.666666', '+77.777777-888.888888']\n-    times = ['']\n+    addrs = ['kiwi.cs.ucla.edu', 'plum.cs.ucla.edu', 'watermelon.cs.ucla.edu']\n+    hosts = {k.split('.')[0]: k for k in addrs}\n+    coordinates = {\n+        'ucla': '+34.069072065913495-118.44518110210453',\n+        'alcatraz': '+37.827160394635804-122.4229566155028',\n+        'gordon': '+42.8066597474685-102.20244258652899',\n+        'dc': '+38.89787667717704 -77.03651907314573',\n+        'wounded': '+43.14272554411939-102.36505934705043',\n+        'minneapolis': '+44.9481292661016-93.27610373604308',\n+        'test': '+34.068930-118.445127',\n+    }\n+    # coordinates = ['+34.068930-118.445127', '+55.555555-666.666666', '+77.777777-888.888888']\n+    # times = await asyncio.gather(\n+    #     *(time_after(i) for i in range(6)))\n \n-    h, c, t = 'kiwi.cs.ucla.edu', '+34.068930-118.445127', '1621464827.959498503'\n-    h2, c2, t2 = 'plum.cs.ucla.edu', '+77.770077-999.99999', time()\n-    r, p = 10, 5\n     try:\n-        # t = await asyncio.gather(\n-        #     *(time_after(i) for i in range(6))\n-        # )\n-        # t = await time_after(5)\n-        r = await asyncio.gather(\n-            tcp_echo_client(iamat(h, c), herd['Bailey']),\n-            tcp_echo_client(whatsat(h, r, p), herd['Bailey']),\n-            # tcp_echo_client(whatsat(h, r, 3), herd['Bailey']),\n-            # tcp_echo_client(whatsat(h, r, 6), herd['Bailey']),\n-            # tcp_echo_client(whatsat(h2, r, p), herd['Bailey']),\n-            # tcp_echo_client(iamat(h2, c2, t=t2, skew=1000000000), herd['Bailey']),\n-            # tcp_echo_client(iamat(h2, c2), herd['Bailey']),\n-            # tcp_echo_client(iamat(h2, c), herd['Bailey']),\n-            # tcp_echo_client(whatsat(h, r, p), herd['Campbell']),\n-            # tcp_echo_client(whatsat(h, r, 3), herd['Campbell'])\n+        # Single Server Test\n+        # await tcp_echo_client(iamat(hosts['kiwi'], coordinates['test']), herd['Bailey'])\n+        # await asyncio.sleep(1)\n+        # await tcp_echo_client(whatsat(hosts['kiwi'], 1, 1), herd['Bona'])\n+        # await tcp_echo_client(iamat(hosts['kiwi'], coordinates['alcatraz']), herd['Bona'])\n+\n+        \n+        await tcp_echo_client(iamat(hosts['kiwi'], coordinates['ucla']), herd['Clark'])\n+        await time_after(1)\n+        await tcp_echo_client(whatsat(hosts['kiwi'], 1, 2), herd['Jaquez'])\n+\n+        await tcp_echo_client(iamat(hosts['kiwi'], coordinates['ucla']), herd['Bailey'])\n+        await time_after(2)\n+        await tcp_echo_client(whatsat(hosts['kiwi'], 1, 2), herd['Clark'])\n+\n+        await time_after(5)\n+\n+        await asyncio.gather(\n+            tcp_echo_client(iamat(hosts['kiwi'], coordinates['gordon']), herd['Jaquez']),\n+            tcp_echo_client(whatsat(hosts['kiwi'], 2, 2), herd['Bailey']),\n+        )\n+        \n+        await time_after(5)\n+\n+        await asyncio.gather(\n+            tcp_echo_client(whatsat(hosts['watermelon'], 2, 2), herd['Bailey']),\n+            tcp_echo_client(whatsat(hosts['watermelon'], 2, 2), herd['Bona']),\n+            tcp_echo_client(whatsat(hosts['watermelon'], 2, 2), herd['Campbell']),\n+            tcp_echo_client(whatsat(hosts['watermelon'], 2, 2), herd['Clark']),\n+            tcp_echo_client(whatsat(hosts['watermelon'], 2, 2), herd['Jaquez']),\n+            tcp_echo_client(iamat(hosts['watermelon'], coordinates['dc']), herd['Clark']),\n+            tcp_echo_client(whatsat(hosts['watermelon'], 2, 2), herd['Bailey']),\n         )\n-\n-        print(f'{t=}')\n-        print(f'{r=}')\n-\n-\n \n     except ConnectionRefusedError as cre:\n         print(\"Connection Refused\")\n         print(cre)\n         sys.exit(-1)\n \n+    # sys.exit()\n+    # try:\n+    #     h, c, t = 'kiwi.cs.ucla.edu', '+34.068930-118.445127', '1621464827.959498503'\n+    #     h2, c2, t2 = 'plum.cs.ucla.edu', '+77.770077-999.99999', time()\n+    #     r, p = 10, 5\n+    #     r = await asyncio.gather(\n+    #         tcp_echo_client(iamat(h, c), herd['Bailey']),\n+    #         tcp_echo_client(whatsat(h, r, p), herd['Bailey']),\n+    #         # tcp_echo_client(whatsat(h, r, 3), herd['Bailey']),\n+    #         # tcp_echo_client(whatsat(h, r, 6), herd['Bailey']),\n+    #         # tcp_echo_client(whatsat(h2, r, p), herd['Bailey']),\n+    #         # tcp_echo_client(iamat(h2, c2, t=t2, skew=1000000000), herd['Bailey']),\n+    #         # tcp_echo_client(iamat(h2, c2), herd['Bailey']),\n+    #         # tcp_echo_client(iamat(h2, c), herd['Bailey']),\n+    #         # tcp_echo_client(whatsat(h, r, p), herd['Campbell']),\n+    #         # tcp_echo_client(whatsat(h, r, 3), herd['Campbell'])\n+    #     )\n+\n+    #     print(f'{t=}')\n+    #     print(f'{r=}')\n+\n+\n+\n+    # except ConnectionRefusedError as cre:\n+    #     print(\"Connection Refused\")\n+    #     print(cre)\n+    #     sys.exit(-1)\n+\n if __name__ == '__main__':\n     asyncio.run(main())\n\\ No newline at end of file\ndiff --git a\/record.py b\/record.py\nindex cbcbe91..a2cefee 100644\n--- a\/record.py\n+++ b\/record.py\n@@ -120,7 +120,7 @@ def pagination(self, size):\n             self._pagination = None\n \n     def __str__(self):\n-        return ','.join([self.lat, self.lon])\n+        return ''.join([self.lat, self.lon])\n     \n     \n     \n@@ -156,4 +156,8 @@ def deserialize(cls, dct):\n     \n     @classmethod\n     def coords(cls, lat, lon):\n-        return \",\".join([\"\".join(lat), \"\".join(lon)])\n\\ No newline at end of file\n+        return \"\".join([\"\".join(lat), \"\".join(lon)])\n+    \n+    @property\n+    def api_location(self):\n+        return \",\".join([\"\".join(self.lat), \"\".join(self.lon)])\n\\ No newline at end of file\ndiff --git a\/request.py b\/request.py\nindex 6512f83..c813c35 100644\n--- a\/request.py\n+++ b\/request.py\n@@ -29,6 +29,7 @@ def __init__(self, message, received_time=None, payload=None):\n                 coords = self.crude_coord_split(m[2])\n                 self.lat = (coords[0], coords[1]) # ['+\/-', 'floatstring']\n                 self.lon = (coords[2], coords[3]) # ['+\/-', 'floatstring']\n+\n                 self.client_time = m[3]\n \n                 if received_time is None:\n@@ -50,6 +51,7 @@ def __init__(self, message, received_time=None, payload=None):\n                     pass\n \n             elif type == 'IAM':\n+                # print(f'FULL IAM:: {m=}')\n                 self.type = type\n                 self.sender = m[1]\n                 self.skew = m[2]\n@@ -59,11 +61,16 @@ def __init__(self, message, received_time=None, payload=None):\n                 self.lon = (coords[2], coords[3]) # ['+\/-', 'floatstring']\n                 self.client_time = m[5]\n                 try:\n-                    print(f'{m[6]=}')\n-                    self.nodes_visisted = ast.literal_eval(m[6])\n+                    # print(f'{m[6]=}')\n+                    # self.nodes_visisted = ast.literal_eval(m[6])\n+                    fix = \"\".join(m[6:])\n+                    self.nodes_visisted = ast.literal_eval(fix)\n                 except Exception as e:\n                     # TODO: how should this be handled?\n-                    print(f'ISSUE IN REQUEST.py: {e}')\n+                    before_fix= m[6:]\n+                    fix=\"\".join(m[6:])\n+                    print(f'ISSUE IN REQUEST.py: {e}: {m[6]=} \\n{message=}\\n{m=}\\n{before_fix=}\\n{fix=}')\n+                    raise SystemError(\"F@*!\")\n                     self.nodes_visisted = []\n \n     def get_visited(self):\n@@ -73,7 +80,7 @@ def mark_invalid(self):\n         self.valid = False\n \n     def mark_visited(self, name):\n-        self.nodes_visisted.append(name)\n+        self.nodes_visisted.append(name.strip())\n \n     def was_visited_by(self, name):\n         return name in self.nodes_visisted\n@@ -87,10 +94,18 @@ def __str__(self):\n         radius = self.radius\n         pagination = self.pagination\n         client_time = self.client_time\n-        return f\"{type=} {skew=} {addr=} {lon=} {lat=} {radius=} {pagination=} {client_time=}\"\n+        nodes_visited = self.nodes_visisted\n+        return f\"{type=} {skew=} {addr=} {lon=} {lat=} {radius=} {pagination=} {client_time=} {nodes_visited=}\"\n     \n     @property\n     def location(self):\n+        if self.lat is not None and self.lon is not None:\n+            return \"\".join([*self.lat, *self.lon])\n+        else:\n+            return \"\"\n+    \n+    @property\n+    def api_location(self):\n         if self.lat is not None and self.lon is not None:\n             return \",\".join([*self.lat, *self.lon])\n         else:\n@@ -111,7 +126,7 @@ def flood_response(self, at):\n         try:\n             a = self.skew\n             b = self.addr\n-            c = self.location\n+            c = str(self.location)\n             d = self.client_time\n             e = str(self.nodes_visisted)\n             r = f\"IAM {at} {a} {b} {c} {d} {e}\"\ndiff --git a\/server.py b\/server.py\nindex 9a3f101..c9ee774 100644\n--- a\/server.py\n+++ b\/server.py\n@@ -1,4 +1,5 @@\n import aiohttp\n+import aiofiles\n import asyncio\n import collections\n import json\n@@ -51,12 +52,19 @@ def init_logger(filename):\n # Change this to seas herd when runnin on seas\n herd = my_herd\n \n+# graph = {\n+#     'Bailey': ['Campbell', 'Bona'],\n+#     'Bona': ['Clark', 'Jaquez', 'Campbell', 'Bailey'],\n+#     'Campbell': ['Bailey', 'Bona', 'Jaquez'],\n+#     'Clark': ['Jaquez', 'Bona'],\n+#     'Jaquez': ['Clark', 'Bona', 'Campbell'],\n+# }\n graph = {\n     'Bailey': ['Campbell', 'Bona'],\n-    'Bona': ['Clark', 'Jaquez', 'Campbell', 'Bailey'],\n+    'Bona': ['Clark', 'Campbell', 'Bailey'],\n     'Campbell': ['Bailey', 'Bona', 'Jaquez'],\n     'Clark': ['Jaquez', 'Bona'],\n-    'Jaquez': ['Clark', 'Bona', 'Campbell'],\n+    'Jaquez': ['Clark', 'Campbell'],\n }\n \n # Clark talks with Jaquez and Bona.\n@@ -135,8 +143,9 @@ async def handle_echo(reader, writer):\n     data = await reader.read() # Want this to be as many as needed\n     \n     message = data.decode()\n-    addr = writer.get_extra_info('peername')\n-    logger.info(f\"Received {message} at {addr}\")\n+    # addr = writer.get_extra_info('peername')\n+    # logger.info(f\"Received {message} at {addr}\")\n+    logger.info(f\"Received {message}\")\n \n     # Parse the message. If we have a client \n     request = Request(message, time.time())\n@@ -145,9 +154,19 @@ async def handle_echo(reader, writer):\n     flood = False\n \n     # Flood throught network\n-    if request.is_iam() and not request.was_visited_by(MYNAME):\n-        records[rec.addr] = rec if is_new else make_record(request)\n-        flood = True\n+    # if request.is_iam() and not request.was_visited_by(MYNAME):\n+    if request.is_iam():\n+        if is_new:\n+            # print(f'NEW from {request.sender} {str(rec.position)}')\n+            records[rec.addr] = rec\n+            flood = True\n+        elif str(rec.position) != Position.coords(request.lat, request.lon):\n+            # print(f'UPDATE {str(rec.position)=} to {Position.coords(request.lat, request.lon)=}')\n+            records[rec.addr] =  make_record(request)\n+            flood = True\n+        else:\n+            # print(f'END PROP of {str(request)} at {str(rec.position)}')\n+            pass\n \n     # Do stuff with the record\n     elif is_new:\n@@ -183,7 +202,7 @@ async def handle_echo(reader, writer):\n \n                 # Do an API call to get more results\n                 elif rec.position.pagination <= request.pagination:\n-                    loc = request.location\n+                    loc = request.api_location\n                     rad = request.radius\n                     pag = request.pagination\n                     logger.debug(f'{request=}')\n@@ -205,7 +224,7 @@ async def handle_echo(reader, writer):\n                 #     json.dump(api_response, f)\n \n                 # FIXME: location hack. fix this\n-                loc = str(rec.position)\n+                loc = rec.position.api_location\n                 rad = request.radius\n                 pag = request.pagination\n                 logger.debug(f'{str(request)=}')\n@@ -221,6 +240,7 @@ async def handle_echo(reader, writer):\n             # TODO: Do I need to do something here?\n             pass\n     \n+    # Respond to Client\n     if not request.is_iam():\n         resp = request.response(MYNAME, rec, payload=payload)\n         \n@@ -233,6 +253,7 @@ async def handle_echo(reader, writer):\n         writer.close()\n         await writer.wait_closed()\n \n+    # Propigate to neighbors\n     if request.is_iam() or flood:\n         await propagate(request)\n         \n@@ -241,6 +262,7 @@ async def propagate(request: Request):\n     request.mark_visited(MYNAME)\n     visited = request.get_visited()\n     to_visit = [x for x in graph[MYNAME] if x not in visited]\n+    # to_visit = graph[MYNAME]\n     resp = request.flood_response(MYNAME)\n \n     for neighbor in to_visit:\n@@ -257,13 +279,14 @@ async def propagate(request: Request):\n         except:\n             logger.info(f\"Unable to connect to {neighbor}\")\n \n-def dummy_api_call(location, radius, pagination):\n-    with open('places_raw.json', 'r') as rf:\n-        data = json.load(rf)\n-        data['results'] = data['results'][:pagination]\n-        return data\n+async def dummy_api_call(location, radius, pagination):\n+    async with aiofiles.open('places_raw.json', mode='r') as rf:\n+        read_data = await rf.read()\n+    data = json.loads(read_data)\n+    data['results'] = data['results'][:pagination]\n+    return data\n \n-async def api_call(location, radius, pag):\n+async def places_api_call(location, radius, pag):\n     key = env.PLACES_API_KEY\n     url=(\n         f'https:\/\/maps.googleapis.com\/maps\/api\/place\/nearbysearch\/json' +\n@@ -289,6 +312,8 @@ async def api_call(location, radius, pag):\n             # return json.loads(await resp.text())\n             return json_data\n \n+api_call = dummy_api_call\n+\n async def main():\n     fname, port = parse(sys.argv[1:])\n     if not fname:\n","files":{"\/client.py":{"changes":[{"diff":"\n-from time import time, strftime\n+from time import time, strftime, sleep\n \n import asyncio\n import json\n","add":1,"remove":1,"filename":"\/client.py","badparts":["from time import time, strftime"],"goodparts":["from time import time, strftime, sleep"]},{"diff":"\n     return time()\n \n async def main():\n-    hosts = ['kiwi.cs.ucla.edu', 'plum.cs.ucla.edu', 'watermelon.cs.ucla.edu']\n-    coordinates = ['+34.068930-118.445127', '+55.555555-666.666666', '+77.777777-888.888888']\n-    times = ['']\n+    addrs = ['kiwi.cs.ucla.edu', 'plum.cs.ucla.edu', 'watermelon.cs.ucla.edu']\n+    hosts = {k.split('.')[0]: k for k in addrs}\n+    coordinates = {\n+        'ucla': '+34.069072065913495-118.44518110210453',\n+        'alcatraz': '+37.827160394635804-122.4229566155028',\n+        'gordon': '+42.8066597474685-102.20244258652899',\n+        'dc': '+38.89787667717704 -77.03651907314573',\n+        'wounded': '+43.14272554411939-102.36505934705043',\n+        'minneapolis': '+44.9481292661016-93.27610373604308',\n+        'test': '+34.068930-118.445127',\n+    }\n+    # coordinates = ['+34.068930-118.445127', '+55.555555-666.666666', '+77.777777-888.888888']\n+    # times = await asyncio.gather(\n+    #     *(time_after(i) for i in range(6)))\n \n-    h, c, t = 'kiwi.cs.ucla.edu', '+34.068930-118.445127', '1621464827.959498503'\n-    h2, c2, t2 = 'plum.cs.ucla.edu', '+77.770077-999.99999', time()\n-    r, p = 10, 5\n     try:\n-        # t = await asyncio.gather(\n-        #     *(time_after(i) for i in range(6))\n-        # )\n-        # t = await time_after(5)\n-        r = await asyncio.gather(\n-            tcp_echo_client(iamat(h, c), herd['Bailey']),\n-            tcp_echo_client(whatsat(h, r, p), herd['Bailey']),\n-            # tcp_echo_client(whatsat(h, r, 3), herd['Bailey']),\n-            # tcp_echo_client(whatsat(h, r, 6), herd['Bailey']),\n-            # tcp_echo_client(whatsat(h2, r, p), herd['Bailey']),\n-            # tcp_echo_client(iamat(h2, c2, t=t2, skew=1000000000), herd['Bailey']),\n-            # tcp_echo_client(iamat(h2, c2), herd['Bailey']),\n-            # tcp_echo_client(iamat(h2, c), herd['Bailey']),\n-            # tcp_echo_client(whatsat(h, r, p), herd['Campbell']),\n-            # tcp_echo_client(whatsat(h, r, 3), herd['Campbell'])\n+        # Single Server Test\n+        # await tcp_echo_client(iamat(hosts['kiwi'], coordinates['test']), herd['Bailey'])\n+        # await asyncio.sleep(1)\n+        # await tcp_echo_client(whatsat(hosts['kiwi'], 1, 1), herd['Bona'])\n+        # await tcp_echo_client(iamat(hosts['kiwi'], coordinates['alcatraz']), herd['Bona'])\n+\n+        \n+        await tcp_echo_client(iamat(hosts['kiwi'], coordinates['ucla']), herd['Clark'])\n+        await time_after(1)\n+        await tcp_echo_client(whatsat(hosts['kiwi'], 1, 2), herd['Jaquez'])\n+\n+        await tcp_echo_client(iamat(hosts['kiwi'], coordinates['ucla']), herd['Bailey'])\n+        await time_after(2)\n+        await tcp_echo_client(whatsat(hosts['kiwi'], 1, 2), herd['Clark'])\n+\n+        await time_after(5)\n+\n+        await asyncio.gather(\n+            tcp_echo_client(iamat(hosts['kiwi'], coordinates['gordon']), herd['Jaquez']),\n+            tcp_echo_client(whatsat(hosts['kiwi'], 2, 2), herd['Bailey']),\n+        )\n+        \n+        await time_after(5)\n+\n+        await asyncio.gather(\n+            tcp_echo_client(whatsat(hosts['watermelon'], 2, 2), herd['Bailey']),\n+            tcp_echo_client(whatsat(hosts['watermelon'], 2, 2), herd['Bona']),\n+            tcp_echo_client(whatsat(hosts['watermelon'], 2, 2), herd['Campbell']),\n+            tcp_echo_client(whatsat(hosts['watermelon'], 2, 2), herd['Clark']),\n+            tcp_echo_client(whatsat(hosts['watermelon'], 2, 2), herd['Jaquez']),\n+            tcp_echo_client(iamat(hosts['watermelon'], coordinates['dc']), herd['Clark']),\n+            tcp_echo_client(whatsat(hosts['watermelon'], 2, 2), herd['Bailey']),\n         )\n-\n-        print(f'{t=}')\n-        print(f'{r=}')\n-\n-\n \n     except ConnectionRefusedError as cre:\n         print(\"Connection Refused\")\n         print(cre)\n         sys.exit(-1)\n \n+    # sys.exit()\n+    # try:\n+    #     h, c, t = 'kiwi.cs.ucla.edu', '+34.068930-118.445127', '1621464827.959498503'\n+    #     h2, c2, t2 = 'plum.cs.ucla.edu', '+77.770077-999.99999', time()\n+    #     r, p = 10, 5\n+    #     r = await asyncio.gather(\n+    #         tcp_echo_client(iamat(h, c), herd['Bailey']),\n+    #         tcp_echo_client(whatsat(h, r, p), herd['Bailey']),\n+    #         # tcp_echo_client(whatsat(h, r, 3), herd['Bailey']),\n+    #         # tcp_echo_client(whatsat(h, r, 6), herd['Bailey']),\n+    #         # tcp_echo_client(whatsat(h2, r, p), herd['Bailey']),\n+    #         # tcp_echo_client(iamat(h2, c2, t=t2, skew=1000000000), herd['Bailey']),\n+    #         # tcp_echo_client(iamat(h2, c2), herd['Bailey']),\n+    #         # tcp_echo_client(iamat(h2, c), herd['Bailey']),\n+    #         # tcp_echo_client(whatsat(h, r, p), herd['Campbell']),\n+    #         # tcp_echo_client(whatsat(h, r, 3), herd['Campbell'])\n+    #     )\n+\n+    #     print(f'{t=}')\n+    #     print(f'{r=}')\n+\n+\n+\n+    # except ConnectionRefusedError as cre:\n+    #     print(\"Connection Refused\")\n+    #     print(cre)\n+    #     sys.exit(-1)\n+\n if __name__ == '__main__':\n     asyncio.run(main())\n\\ No newline at end of file","add":74,"remove":26,"filename":"\/client.py","badparts":["    hosts = ['kiwi.cs.ucla.edu', 'plum.cs.ucla.edu', 'watermelon.cs.ucla.edu']","    coordinates = ['+34.068930-118.445127', '+55.555555-666.666666', '+77.777777-888.888888']","    times = ['']","    h, c, t = 'kiwi.cs.ucla.edu', '+34.068930-118.445127', '1621464827.959498503'","    h2, c2, t2 = 'plum.cs.ucla.edu', '+77.770077-999.99999', time()","    r, p = 10, 5","        r = await asyncio.gather(","            tcp_echo_client(iamat(h, c), herd['Bailey']),","            tcp_echo_client(whatsat(h, r, p), herd['Bailey']),","        print(f'{t=}')","        print(f'{r=}')"],"goodparts":["    addrs = ['kiwi.cs.ucla.edu', 'plum.cs.ucla.edu', 'watermelon.cs.ucla.edu']","    hosts = {k.split('.')[0]: k for k in addrs}","    coordinates = {","        'ucla': '+34.069072065913495-118.44518110210453',","        'alcatraz': '+37.827160394635804-122.4229566155028',","        'gordon': '+42.8066597474685-102.20244258652899',","        'dc': '+38.89787667717704 -77.03651907314573',","        'wounded': '+43.14272554411939-102.36505934705043',","        'minneapolis': '+44.9481292661016-93.27610373604308',","        'test': '+34.068930-118.445127',","    }","        await tcp_echo_client(iamat(hosts['kiwi'], coordinates['ucla']), herd['Clark'])","        await time_after(1)","        await tcp_echo_client(whatsat(hosts['kiwi'], 1, 2), herd['Jaquez'])","        await tcp_echo_client(iamat(hosts['kiwi'], coordinates['ucla']), herd['Bailey'])","        await time_after(2)","        await tcp_echo_client(whatsat(hosts['kiwi'], 1, 2), herd['Clark'])","        await time_after(5)","        await asyncio.gather(","            tcp_echo_client(iamat(hosts['kiwi'], coordinates['gordon']), herd['Jaquez']),","            tcp_echo_client(whatsat(hosts['kiwi'], 2, 2), herd['Bailey']),","        )","        await time_after(5)","        await asyncio.gather(","            tcp_echo_client(whatsat(hosts['watermelon'], 2, 2), herd['Bailey']),","            tcp_echo_client(whatsat(hosts['watermelon'], 2, 2), herd['Bona']),","            tcp_echo_client(whatsat(hosts['watermelon'], 2, 2), herd['Campbell']),","            tcp_echo_client(whatsat(hosts['watermelon'], 2, 2), herd['Clark']),","            tcp_echo_client(whatsat(hosts['watermelon'], 2, 2), herd['Jaquez']),","            tcp_echo_client(iamat(hosts['watermelon'], coordinates['dc']), herd['Clark']),","            tcp_echo_client(whatsat(hosts['watermelon'], 2, 2), herd['Bailey']),"]}],"source":"\nfrom time import time, strftime import asyncio import json import sys ipaddr='127.0.0.1' herd={ 'Bailey': 17800, 'Bona': 17801, 'Campbell': 17802, 'Clark': 17803, 'Jaquez': 17804, } def iamat(host, coord, t=None, skew=None): if t is None: t=time() if skew is not None: t +skew print(t+skew) formatted=f\"IAMAT{host}{coord}{t}\" return formatted def whatsat(host, radius, page): formatted=f\"WHATISAT{host}{radius}{page}\" return formatted async def tcp_echo_client(message, port): reader, writer=await asyncio.open_connection( ipaddr, port) print(f'Send:{message!r}') writer.write(message.encode()) await writer.drain() writer.write_eof() if 'WHATISAT' in message: while not reader.at_eof(): data=await reader.readline() if decoded:=data.decode(): if decoded.startswith('AT'): print(f\"decoded:{decoded}\") elif not decoded.startswith('?'): json_data=json.loads(decoded) print( f'{len(json_data[\"results\"])=}') elif decoded.startswith('?'): print(decoded) else: data=await reader.read() decoded_data=data.decode() print(f'Received:{decoded_data!r}') print('Close the connection') writer.close() await writer.wait_closed() async def time_after(n): print(f'sleep for{n=}') await asyncio.sleep(n) print(f'wokeup') return time() async def main(): hosts=['kiwi.cs.ucla.edu', 'plum.cs.ucla.edu', 'watermelon.cs.ucla.edu'] coordinates=['+34.068930-118.445127', '+55.555555-666.666666', '+77.777777-888.888888'] times=[''] h, c, t='kiwi.cs.ucla.edu', '+34.068930-118.445127', '1621464827.959498503' h2, c2, t2='plum.cs.ucla.edu', '+77.770077-999.99999', time() r, p=10, 5 try: r=await asyncio.gather( tcp_echo_client(iamat(h, c), herd['Bailey']), tcp_echo_client(whatsat(h, r, p), herd['Bailey']), ) print(f'{t=}') print(f'{r=}') except ConnectionRefusedError as cre: print(\"Connection Refused\") print(cre) sys.exit(-1) if __name__=='__main__': asyncio.run(main()) ","sourceWithComments":"from time import time, strftime\n\nimport asyncio\nimport json\nimport sys\n\nipaddr = '127.0.0.1'\n\nherd = {\n    'Bailey': 17800,\n    'Bona': 17801,\n    'Campbell': 17802,\n    'Clark': 17803,\n    'Jaquez': 17804,\n}\n\ndef iamat(host, coord, t=None, skew=None):\n    if t is None:\n        t = time()\n    if skew is not None:\n        t + skew\n        print(t+skew)\n    formatted = f\"IAMAT {host} {coord} {t}\"\n    return formatted\n\ndef whatsat(host, radius, page):\n    formatted = f\"WHATISAT {host} {radius} {page}\"\n    return formatted\n\nasync def tcp_echo_client(message, port):\n    reader, writer = await asyncio.open_connection(\n        ipaddr, port)\n\n    print(f'Send: {message!r}')\n    writer.write(message.encode())\n    await writer.drain()\n    writer.write_eof()\n\n    if 'WHATISAT' in message:\n        while not reader.at_eof():\n            data = await reader.readline()\n            if decoded := data.decode():\n                if decoded.startswith('AT'):\n                    print(f\"decoded: {decoded}\")\n                elif not decoded.startswith('?'):\n                    json_data = json.loads(decoded)\n                    print( f'{len(json_data[\"results\"])=}')\n                elif decoded.startswith('?'):\n                    print(decoded)\n    else:\n        data = await reader.read()\n        decoded_data = data.decode()\n        print(f'Received: {decoded_data!r}')\n\n    print('Close the connection')\n    writer.close()\n    await writer.wait_closed()\n\nasync def time_after(n):\n    print(f'sleep for {n=}')\n    await asyncio.sleep(n)\n    print(f'wokeup')\n    return time()\n\nasync def main():\n    hosts = ['kiwi.cs.ucla.edu', 'plum.cs.ucla.edu', 'watermelon.cs.ucla.edu']\n    coordinates = ['+34.068930-118.445127', '+55.555555-666.666666', '+77.777777-888.888888']\n    times = ['']\n\n    h, c, t = 'kiwi.cs.ucla.edu', '+34.068930-118.445127', '1621464827.959498503'\n    h2, c2, t2 = 'plum.cs.ucla.edu', '+77.770077-999.99999', time()\n    r, p = 10, 5\n    try:\n        # t = await asyncio.gather(\n        #     *(time_after(i) for i in range(6))\n        # )\n        # t = await time_after(5)\n        r = await asyncio.gather(\n            tcp_echo_client(iamat(h, c), herd['Bailey']),\n            tcp_echo_client(whatsat(h, r, p), herd['Bailey']),\n            # tcp_echo_client(whatsat(h, r, 3), herd['Bailey']),\n            # tcp_echo_client(whatsat(h, r, 6), herd['Bailey']),\n            # tcp_echo_client(whatsat(h2, r, p), herd['Bailey']),\n            # tcp_echo_client(iamat(h2, c2, t=t2, skew=1000000000), herd['Bailey']),\n            # tcp_echo_client(iamat(h2, c2), herd['Bailey']),\n            # tcp_echo_client(iamat(h2, c), herd['Bailey']),\n            # tcp_echo_client(whatsat(h, r, p), herd['Campbell']),\n            # tcp_echo_client(whatsat(h, r, 3), herd['Campbell'])\n        )\n\n        print(f'{t=}')\n        print(f'{r=}')\n\n\n\n    except ConnectionRefusedError as cre:\n        print(\"Connection Refused\")\n        print(cre)\n        sys.exit(-1)\n\nif __name__ == '__main__':\n    asyncio.run(main())"},"\/record.py":{"changes":[{"diff":"\n             self._pagination = None\n \n     def __str__(self):\n-        return ','.join([self.lat, self.lon])\n+        return ''.join([self.lat, self.lon])\n     \n     \n     \n","add":1,"remove":1,"filename":"\/record.py","badparts":["        return ','.join([self.lat, self.lon])"],"goodparts":["        return ''.join([self.lat, self.lon])"]},{"diff":"\n     \n     @classmethod\n     def coords(cls, lat, lon):\n-        return \",\".join([\"\".join(lat), \"\".join(lon)])\n\\ No newline at end of file\n+        return \"\".join([\"\".join(lat), \"\".join(lon)])\n+    \n+    @property\n+    def api_location(self):\n+        return \",\".join([\"\".join(self.lat), \"\".join(self.lon)])\n\\ No newline at end of fil","add":5,"remove":1,"filename":"\/record.py","badparts":["        return \",\".join([\"\".join(lat), \"\".join(lon)])"],"goodparts":["        return \"\".join([\"\".join(lat), \"\".join(lon)])","    @property","    def api_location(self):","        return \",\".join([\"\".join(self.lat), \"\".join(self.lon)])"]}],"source":"\nimport json class Record: def __init__(self, addr, skew, client_time, position): self.addr=addr self.skew=skew self.client_time=client_time self.position=position def __str__(self): return f'{self.addr}{self.position}' def __eq__(self, other): return( self.addr==other.addr and self.skew==other.skew and self.client_time==other.client_time and self.position==other.position ) def update(self, message): pass def serialize(self): return{ '__record__': True, 'addr': self.addr, 'skew': self.skew, 'client_time': self.client_time, 'position': self.position.serialize(), } @classmethod def deserialize(cls, dct): if '__record__' in dct: return cls( dct['addr'], dct['skew'], dct['client_time'], Position.deserialize(dct['position']), ) def to_message(self, name): pass def has_changed(self, msg): pass class Position: def __init__(self, lat, lon, radius=None, pagination=None, payload=None): self.lat=lat self.lon=lon self.radius=radius self.pagination=pagination self.payload=payload def __repr__(self): return self.__str__() @property def lat(self): return self._lat @lat.setter def lat(self, args): try: if isinstance(args, str): if not args.startswith(('+', '-')): raise ValueError(f'lat error: invalid coordinate string.') self._lat=args else: if len(args) !=2: raise ValueError(f'lat error: expected 2, got{len(args)}.') self._lat=''.join(args) except TypeError: self._lat=None @property def lon(self): return self._lon @lon.setter def lon(self, args): try: if isinstance(args, str): if not args.startswith(('+', '-')): raise ValueError(f'lon error: invalid coordinate string.') self._lon=args else: if len(args) !=2: raise ValueError(f'lon error: expected 2, got{len(args)}.') self._lon=''.join(args) except TypeError: self._lon=None @property def radius(self): return self._radius @radius.setter def radius(self, r): try: self._radius=int(r) if int(r) <=1500 else 1500 except TypeError as te: if r is not None: raise TypeError('Expected radius of coercable int type.') self._radius=None @property def pagination(self): return self._pagination @pagination.setter def pagination(self, size): try: self._pagination=int(size) if int(size) <=20 else 20 except(TypeError, ValueError) as e: if size is not None: raise e self._pagination=None def __str__(self): return ','.join([self.lat, self.lon]) def __eq__(self, other): return( self.lat==other.lat and self.lon==other.lon and self.radius==other.radius and self.pagination==other.pagination and self.payload==other.payload ) def serialize(self): return{ '__position__': True, 'lat': self.lat, 'lon': self.lon, 'radius': self.radius, 'pagination': self.pagination, 'payload': self.payload, } @classmethod def deserialize(cls, dct): if '__position__' in dct: return cls( dct['lat'], dct['lon'], radius=dct['radius'], pagination=dct['pagination'], payload=dct['payload'], ) @classmethod def coords(cls, lat, lon): return \",\".join([\"\".join(lat), \"\".join(lon)]) ","sourceWithComments":"import json\n\nclass Record:\n    def __init__(self, addr, skew, client_time, position):\n        self.addr = addr\n        self.skew = skew\n        self.client_time = client_time\n        self.position = position\n\n    def __str__(self):\n        return f'{self.addr} {self.position}'\n    \n    def __eq__(self, other):\n        return (\n            self.addr == other.addr and\n            self.skew == other.skew and\n            self.client_time == other.client_time and\n            self.position == other.position\n        )\n\n    def update(self, message):\n        pass\n    \n    def serialize(self):\n        return {\n            '__record__': True,\n            'addr': self.addr, \n            'skew': self.skew, \n            'client_time': self.client_time, \n            'position': self.position.serialize(), \n        }\n    \n    @classmethod\n    def deserialize(cls, dct):\n        if '__record__' in dct:\n            return cls(\n                dct['addr'],\n                dct['skew'],\n                dct['client_time'],\n                Position.deserialize(dct['position']),\n            )\n    \n    def to_message(self, name):\n        pass\n\n    def has_changed(self, msg):\n        pass\n\nclass Position:\n    def __init__(self, lat, lon, radius=None, pagination=None, payload=None):\n        self.lat = lat\n        self.lon = lon\n        self.radius=radius\n        self.pagination=pagination\n        self.payload=payload\n    \n    def __repr__(self):\n        return self.__str__()\n    \n    @property\n    def lat(self):\n        return self._lat\n    \n    @lat.setter\n    def lat(self, args):\n        try:\n            if isinstance(args, str):\n                if not args.startswith(('+', '-')):\n                    raise ValueError(f'lat error: invalid coordinate string.')\n                self._lat = args\n            else:\n                if len(args) != 2:\n                    raise ValueError(f'lat error: expected 2, got {len(args)}.')\n                self._lat = ''.join(args)\n        except TypeError:\n            self._lat = None\n    \n    @property\n    def lon(self):\n        return self._lon\n    \n    @lon.setter\n    def lon(self, args):\n        try:\n            if isinstance(args, str):\n                if not args.startswith(('+', '-')):\n                    raise ValueError(f'lon error: invalid coordinate string.')\n                self._lon = args\n            else:\n                if len(args) != 2:\n                    raise ValueError(f'lon error: expected 2, got {len(args)}.')\n                self._lon = ''.join(args)\n        except TypeError:\n            self._lon = None\n\n    @property\n    def radius(self):\n        return self._radius\n\n    @radius.setter\n    def radius(self, r):\n        try:\n            self._radius = int(r) if int(r) <= 1500 else 1500\n        except TypeError as te:\n            if r is not None:\n                raise TypeError('Expected radius of coercable int type.')\n            self._radius = None\n\n    @property\n    def pagination(self):\n        return self._pagination\n\n    @pagination.setter\n    def pagination(self, size):\n        try:\n            self._pagination = int(size) if int(size) <= 20 else 20\n        except (TypeError, ValueError) as e:\n            if size is not None:\n                raise e\n            self._pagination = None\n\n    def __str__(self):\n        return ','.join([self.lat, self.lon])\n    \n    \n    \n    def __eq__(self, other):\n        return (\n            self.lat == other.lat and\n            self.lon == other.lon and\n            self.radius == other.radius and\n            self.pagination == other.pagination and\n            self.payload == other.payload\n        )\n    \n    def serialize(self):\n        return {\n            '__position__': True, \n            'lat': self.lat,\n            'lon': self.lon,\n            'radius': self.radius,\n            'pagination': self.pagination,\n            'payload': self.payload,\n        }\n    \n    @classmethod\n    def deserialize(cls, dct):\n        if '__position__' in dct:\n            return cls(\n                dct['lat'],\n                dct['lon'],\n                radius=dct['radius'],\n                pagination=dct['pagination'],\n                payload=dct['payload'],\n            )\n    \n    @classmethod\n    def coords(cls, lat, lon):\n        return \",\".join([\"\".join(lat), \"\".join(lon)])"},"\/request.py":{"changes":[{"diff":"\n                 self.lon = (coords[2], coords[3]) # ['+\/-', 'floatstring']\n                 self.client_time = m[5]\n                 try:\n-                    print(f'{m[6]=}')\n-                    self.nodes_visisted = ast.literal_eval(m[6])\n+                    # print(f'{m[6]=}')\n+                    # self.nodes_visisted = ast.literal_eval(m[6])\n+                    fix = \"\".join(m[6:])\n+                    self.nodes_visisted = ast.literal_eval(fix)\n                 except Exception as e:\n                     # TODO: how should this be handled?\n-                    print(f'ISSUE IN REQUEST.py: {e}')\n+                    before_fix= m[6:]\n+                    fix=\"\".join(m[6:])\n+                    print(f'ISSUE IN REQUEST.py: {e}: {m[6]=} \\n{message=}\\n{m=}\\n{before_fix=}\\n{fix=}')\n+                    raise SystemError(\"F@*!\")\n                     self.nodes_visisted = []\n \n     def get_visited(self):\n","add":8,"remove":3,"filename":"\/request.py","badparts":["                    print(f'{m[6]=}')","                    self.nodes_visisted = ast.literal_eval(m[6])","                    print(f'ISSUE IN REQUEST.py: {e}')"],"goodparts":["                    fix = \"\".join(m[6:])","                    self.nodes_visisted = ast.literal_eval(fix)","                    before_fix= m[6:]","                    fix=\"\".join(m[6:])","                    print(f'ISSUE IN REQUEST.py: {e}: {m[6]=} \\n{message=}\\n{m=}\\n{before_fix=}\\n{fix=}')","                    raise SystemError(\"F@*!\")"]},{"diff":"\n         self.valid = False\n \n     def mark_visited(self, name):\n-        self.nodes_visisted.append(name)\n+        self.nodes_visisted.append(name.strip())\n \n     def was_visited_by(self, name):\n         return name in self.nodes_visisted\n","add":1,"remove":1,"filename":"\/request.py","badparts":["        self.nodes_visisted.append(name)"],"goodparts":["        self.nodes_visisted.append(name.strip())"]},{"diff":"\n         radius = self.radius\n         pagination = self.pagination\n         client_time = self.client_time\n-        return f\"{type=} {skew=} {addr=} {lon=} {lat=} {radius=} {pagination=} {client_time=}\"\n+        nodes_visited = self.nodes_visisted\n+        return f\"{type=} {skew=} {addr=} {lon=} {lat=} {radius=} {pagination=} {client_time=} {nodes_visited=}\"\n     \n     @property\n     def location(self):\n+        if self.lat is not None and self.lon is not None:\n+            return \"\".join([*self.lat, *self.lon])\n+        else:\n+            return \"\"\n+    \n+    @property\n+    def api_location(self):\n         if self.lat is not None and self.lon is not None:\n             return \",\".join([*self.lat, *self.lon])\n         else:\n","add":9,"remove":1,"filename":"\/request.py","badparts":["        return f\"{type=} {skew=} {addr=} {lon=} {lat=} {radius=} {pagination=} {client_time=}\""],"goodparts":["        nodes_visited = self.nodes_visisted","        return f\"{type=} {skew=} {addr=} {lon=} {lat=} {radius=} {pagination=} {client_time=} {nodes_visited=}\"","        if self.lat is not None and self.lon is not None:","            return \"\".join([*self.lat, *self.lon])","        else:","            return \"\"","    @property","    def api_location(self):"]},{"diff":"\n         try:\n             a = self.skew\n             b = self.addr\n-            c = self.location\n+            c = str(self.location)\n             d = self.client_time\n             e = str(self.nodes_visisted)\n             r = f\"IAM {at} {a} {b} {c} {d} {e","add":1,"remove":1,"filename":"\/request.py","badparts":["            c = self.location"],"goodparts":["            c = str(self.location)"]}],"source":"\nimport ast import json import time class Request: def __init__(self, message, received_time=None, payload=None): self.type=None self.skew=None self.addr=None self.lon=None self.lat=None self.radius=None self.pagination=None self.client_time=None self._message=message self._payload=payload self.nodes_visisted=[] self.valid=True if message.startswith(('IAMAT', 'WHATISAT', 'IAM')): m=message.split() type=m[0] if type=='IAMAT' and len(m)==4: self.type=type self.addr=m[1] coords=self.crude_coord_split(m[2]) self.lat=(coords[0], coords[1]) self.lon=(coords[2], coords[3]) self.client_time=m[3] if received_time is None: received_time=time.time() s=received_time -float(self.client_time) self.skew='+' if s > 0 else '-' self.skew +=str(s) elif type=='WHATISAT' and len(m)==4: self.type=type self.addr=m[1] try: r=int(m[2]) p=int(m[3]) self.radius=r*1000 if r <=50 else 50*1000 self.pagination=p if p <=20 else 20 except: pass elif type=='IAM': self.type=type self.sender=m[1] self.skew=m[2] self.addr=m[3] coords=self.crude_coord_split(m[4]) self.lat=(coords[0], coords[1]) self.lon=(coords[2], coords[3]) self.client_time=m[5] try: print(f'{m[6]=}') self.nodes_visisted=ast.literal_eval(m[6]) except Exception as e: print(f'ISSUE IN REQUEST.py:{e}') self.nodes_visisted=[] def get_visited(self): return self.nodes_visisted def mark_invalid(self): self.valid=False def mark_visited(self, name): self.nodes_visisted.append(name) def was_visited_by(self, name): return name in self.nodes_visisted def __str__(self): type=self.type skew=self.skew addr=self.addr lon=self.lon lat=self.lat radius=self.radius pagination=self.pagination client_time=self.client_time return f\"{type=}{skew=}{addr=}{lon=}{lat=}{radius=}{pagination=}{client_time=}\" @property def location(self): if self.lat is not None and self.lon is not None: return \",\".join([*self.lat, *self.lon]) else: return \"\" def response(self, at, rec, herd=False, payload=None): if not self.valid: return f\"?{self._message}\" s=rec.skew a=rec.addr b=str(rec.position) if self.type=='IAMAT' else rec.position.radius c=str(rec.client_time) if self.type=='IAMAT' else rec.position.pagination r=f\"AT{at}{s}{a}{b}{c}\" return r if not payload else(r +f\"\\n{payload}\\n\") def flood_response(self, at): try: a=self.skew b=self.addr c=self.location d=self.client_time e=str(self.nodes_visisted) r=f\"IAM{at}{a}{b}{c}{d}{e}\" return r except Exception as e: print(f'ISSUE IN REQUEST.PY:{e}') pass def crude_coord_split(self, coordstr): ret=[] temp=\"\" for i in coordstr: if i in['+', '-']: if temp !=\"\": ret.append(temp) ret.append(i) temp=\"\" continue temp +=i ret.append(temp) return ret def is_whatisat(self): return self.type=='WHATISAT' def is_iamat(self): return self.type=='IAMAT' def is_iam(self): return self.type=='IAM' ","sourceWithComments":"import ast\nimport json\nimport time\n\nclass Request:\n    def __init__(self, message, received_time=None, payload=None):\n        self.type = None\n        self.skew = None\n        self.addr = None\n        self.lon = None\n        self.lat = None\n        self.radius = None\n        self.pagination = None\n        self.client_time = None\n        self._message = message\n        self._payload = payload\n        self.nodes_visisted = []\n\n        # for shrinking of variables\n        self.valid = True # Assume valid unless set otherwise\n\n        if message.startswith(('IAMAT', 'WHATISAT', 'IAM')):\n            m = message.split()\n            type=m[0]\n            if type == 'IAMAT' and len(m) == 4:\n                # IAMAT kiwi.cs.ucla.edu +34.068930-118.445127 1621464827.959498503\n                self.type = type\n                self.addr = m[1]\n                coords = self.crude_coord_split(m[2])\n                self.lat = (coords[0], coords[1]) # ['+\/-', 'floatstring']\n                self.lon = (coords[2], coords[3]) # ['+\/-', 'floatstring']\n                self.client_time = m[3]\n\n                if received_time is None:\n                    received_time = time.time()\n                s = received_time - float(self.client_time)\n                self.skew = '+' if s > 0 else '-'\n                self.skew += str(s)\n\n            elif type == 'WHATISAT' and len(m) == 4:\n                # WHATSAT kiwi.cs.ucla.edu 10 5\n                self.type = type\n                self.addr = m[1]\n                try:\n                    r = int(m[2])\n                    p = int(m[3])\n                    self.radius = r*1000 if r <=50 else 50*1000\n                    self.pagination = p if p <= 20 else 20\n                except:\n                    pass\n\n            elif type == 'IAM':\n                self.type = type\n                self.sender = m[1]\n                self.skew = m[2]\n                self.addr = m[3]\n                coords = self.crude_coord_split(m[4])\n                self.lat = (coords[0], coords[1]) # ['+\/-', 'floatstring']\n                self.lon = (coords[2], coords[3]) # ['+\/-', 'floatstring']\n                self.client_time = m[5]\n                try:\n                    print(f'{m[6]=}')\n                    self.nodes_visisted = ast.literal_eval(m[6])\n                except Exception as e:\n                    # TODO: how should this be handled?\n                    print(f'ISSUE IN REQUEST.py: {e}')\n                    self.nodes_visisted = []\n\n    def get_visited(self):\n        return self.nodes_visisted\n    \n    def mark_invalid(self):\n        self.valid = False\n\n    def mark_visited(self, name):\n        self.nodes_visisted.append(name)\n\n    def was_visited_by(self, name):\n        return name in self.nodes_visisted\n    \n    def __str__(self):\n        type = self.type\n        skew = self.skew\n        addr = self.addr\n        lon = self.lon\n        lat = self.lat\n        radius = self.radius\n        pagination = self.pagination\n        client_time = self.client_time\n        return f\"{type=} {skew=} {addr=} {lon=} {lat=} {radius=} {pagination=} {client_time=}\"\n    \n    @property\n    def location(self):\n        if self.lat is not None and self.lon is not None:\n            return \",\".join([*self.lat, *self.lon])\n        else:\n            return \"\"\n\n    def response(self, at, rec, herd=False, payload=None):\n        if not self.valid:\n            return f\"? {self._message}\"\n\n        s = rec.skew\n        a = rec.addr\n        b = str(rec.position) if self.type == 'IAMAT' else rec.position.radius\n        c = str(rec.client_time) if self.type == 'IAMAT' else rec.position.pagination\n        r = f\"AT {at} {s} {a} {b} {c}\"\n        return r if not payload else (r + f\"\\n{payload}\\n\")\n    \n    def flood_response(self, at):\n        try:\n            a = self.skew\n            b = self.addr\n            c = self.location\n            d = self.client_time\n            e = str(self.nodes_visisted)\n            r = f\"IAM {at} {a} {b} {c} {d} {e}\"\n            return r\n        except Exception as e:\n            # TODO: How should this be handled?\n            print(f'ISSUE IN REQUEST.PY: {e}')\n            pass\n\n    def crude_coord_split(self, coordstr):\n        # s = '+34.068930-118.445127'\n        ret = []\n        temp=\"\"\n        for i in coordstr:\n            if i in ['+', '-']:\n                if temp != \"\":\n                    ret.append(temp)\n                ret.append(i)\n                temp=\"\"\n                continue\n            temp += i\n        ret.append(temp)\n        # FIX ME Ensure coordinates have the above form\n        return ret\n\n    def is_whatisat(self):\n        return self.type == 'WHATISAT'\n    def is_iamat(self):\n        return self.type == 'IAMAT'\n    def is_iam(self):\n        return self.type == 'IAM'"},"\/server.py":{"changes":[{"diff":"\n # Change this to seas herd when runnin on seas\n herd = my_herd\n \n+# graph = {\n+#     'Bailey': ['Campbell', 'Bona'],\n+#     'Bona': ['Clark', 'Jaquez', 'Campbell', 'Bailey'],\n+#     'Campbell': ['Bailey', 'Bona', 'Jaquez'],\n+#     'Clark': ['Jaquez', 'Bona'],\n+#     'Jaquez': ['Clark', 'Bona', 'Campbell'],\n+# }\n graph = {\n     'Bailey': ['Campbell', 'Bona'],\n-    'Bona': ['Clark', 'Jaquez', 'Campbell', 'Bailey'],\n+    'Bona': ['Clark', 'Campbell', 'Bailey'],\n     'Campbell': ['Bailey', 'Bona', 'Jaquez'],\n     'Clark': ['Jaquez', 'Bona'],\n-    'Jaquez': ['Clark', 'Bona', 'Campbell'],\n+    'Jaquez': ['Clark', 'Campbell'],\n }\n \n # Clark talks with Jaquez and Bona.\n","add":9,"remove":2,"filename":"\/server.py","badparts":["    'Bona': ['Clark', 'Jaquez', 'Campbell', 'Bailey'],","    'Jaquez': ['Clark', 'Bona', 'Campbell'],"],"goodparts":["    'Bona': ['Clark', 'Campbell', 'Bailey'],","    'Jaquez': ['Clark', 'Campbell'],"]},{"diff":"\n     data = await reader.read() # Want this to be as many as needed\n     \n     message = data.decode()\n-    addr = writer.get_extra_info('peername')\n-    logger.info(f\"Received {message} at {addr}\")\n+    # addr = writer.get_extra_info('peername')\n+    # logger.info(f\"Received {message} at {addr}\")\n+    logger.info(f\"Received {message}\")\n \n     # Parse the message. If we have a client \n     request = Request(message, time.time())\n","add":3,"remove":2,"filename":"\/server.py","badparts":["    addr = writer.get_extra_info('peername')","    logger.info(f\"Received {message} at {addr}\")"],"goodparts":["    logger.info(f\"Received {message}\")"]},{"diff":"\n     flood = False\n \n     # Flood throught network\n-    if request.is_iam() and not request.was_visited_by(MYNAME):\n-        records[rec.addr] = rec if is_new else make_record(request)\n-        flood = True\n+    # if request.is_iam() and not request.was_visited_by(MYNAME):\n+    if request.is_iam():\n+        if is_new:\n+            # print(f'NEW from {request.sender} {str(rec.position)}')\n+            records[rec.addr] = rec\n+            flood = True\n+        elif str(rec.position) != Position.coords(request.lat, request.lon):\n+            # print(f'UPDATE {str(rec.position)=} to {Position.coords(request.lat, request.lon)=}')\n+            records[rec.addr] =  make_record(request)\n+            flood = True\n+        else:\n+            # print(f'END PROP of {str(request)} at {str(rec.position)}')\n+            pass\n \n     # Do stuff with the record\n     elif is_new:\n","add":13,"remove":3,"filename":"\/server.py","badparts":["    if request.is_iam() and not request.was_visited_by(MYNAME):","        records[rec.addr] = rec if is_new else make_record(request)","        flood = True"],"goodparts":["    if request.is_iam():","        if is_new:","            records[rec.addr] = rec","            flood = True","        elif str(rec.position) != Position.coords(request.lat, request.lon):","            records[rec.addr] =  make_record(request)","            flood = True","        else:","            pass"]},{"diff":"\n \n                 # Do an API call to get more results\n                 elif rec.position.pagination <= request.pagination:\n-                    loc = request.location\n+                    loc = request.api_location\n                     rad = request.radius\n                     pag = request.pagination\n                     logger.debug(f'{request=}')\n","add":1,"remove":1,"filename":"\/server.py","badparts":["                    loc = request.location"],"goodparts":["                    loc = request.api_location"]},{"diff":"\n                 #     json.dump(api_response, f)\n \n                 # FIXME: location hack. fix this\n-                loc = str(rec.position)\n+                loc = rec.position.api_location\n                 rad = request.radius\n                 pag = request.pagination\n                 logger.debug(f'{str(request)=}')\n","add":1,"remove":1,"filename":"\/server.py","badparts":["                loc = str(rec.position)"],"goodparts":["                loc = rec.position.api_location"]},{"diff":"\n         except:\n             logger.info(f\"Unable to connect to {neighbor}\")\n \n-def dummy_api_call(location, radius, pagination):\n-    with open('places_raw.json', 'r') as rf:\n-        data = json.load(rf)\n-        data['results'] = data['results'][:pagination]\n-        return data\n+async def dummy_api_call(location, radius, pagination):\n+    async with aiofiles.open('places_raw.json', mode='r') as rf:\n+        read_data = await rf.read()\n+    data = json.loads(read_data)\n+    data['results'] = data['results'][:pagination]\n+    return data\n \n-async def api_call(location, radius, pag):\n+async def places_api_call(location, radius, pag):\n     key = env.PLACES_API_KEY\n     url=(\n         f'https:\/\/maps.googleapis.com\/maps\/api\/place\/nearbysearch\/json' +\n","add":7,"remove":6,"filename":"\/server.py","badparts":["def dummy_api_call(location, radius, pagination):","    with open('places_raw.json', 'r') as rf:","        data = json.load(rf)","        data['results'] = data['results'][:pagination]","        return data","async def api_call(location, radius, pag):"],"goodparts":["async def dummy_api_call(location, radius, pagination):","    async with aiofiles.open('places_raw.json', mode='r') as rf:","        read_data = await rf.read()","    data = json.loads(read_data)","    data['results'] = data['results'][:pagination]","    return data","async def places_api_call(location, radius, pag):"]}],"source":"\nimport aiohttp import asyncio import collections import json import logging import sys import time from typing import List, Tuple from urllib.parse import quote_plus as safe_url import env from request import Request from record import Record, Position logger=logging.getLogger(__name__) def init_logger(filename): logger.setLevel(logging.DEBUG) c_handler=logging.StreamHandler() f_handler=logging.FileHandler(filename+\".log\", mode=\"w\") c_handler.setLevel(logging.DEBUG) f_handler.setLevel(logging.INFO) c_format=logging.Formatter('%(message)s') f_format=logging.Formatter('%(asctime)s -%(message)s') c_handler.setFormatter(c_format) f_handler.setFormatter(f_format) logger.addHandler(c_handler) logger.addHandler(f_handler) my_herd={ 'Bailey': 17800, 'Bona': 17801, 'Campbell': 17802, 'Clark': 17803, 'Jaquez': 17804, } seas_herd={ 'Bailey': 10000, 'Bona': 10001, 'Campbell': 10002, 'Clark': 10003, 'Jaquez': 10004, } herd=my_herd graph={ 'Bailey':['Campbell', 'Bona'], 'Bona':['Clark', 'Jaquez', 'Campbell', 'Bailey'], 'Campbell':['Bailey', 'Bona', 'Jaquez'], 'Clark':['Jaquez', 'Bona'], 'Jaquez':['Clark', 'Bona', 'Campbell'], } MYNAME=\"\" ipaddr='127.0.0.1' USAGE=( f\"Usage{sys.argv[0]}[srvname]\" ) records={} def parse(args: List[str]) -> Tuple[str, int]: arguments=collections.deque(args) name=None while arguments: current=arguments.popleft() if name is None: if current in[\"-h\", \"--help\"]: print(USAGE) sys.exit(0) else: name=current else: print(USAGE) sys.exit(0) try: port=herd[name] except: print(f\"\\\"{name}\\\" not a valid server.\") sys.exit(0) return name, port def make_position(req): return Position(req.lat, req.lon, radius=req.radius, pagination=req.pagination, payload=req._payload) def update_record(record, req, is_peer=False): record.skew=req.skew record.client_time=req.client_time record.position=make_position(req) return record def make_record(req): return Record( req.addr, req.skew, req.client_time, make_position(req), ) def get_or_create_client_record(req): is_new=False try: rec=records[req.addr] except: rec=make_record(req) is_new=True return is_new, rec TYPE={ 'IAMAT': None, 'WHATISAT': None, } async def handle_echo(reader, writer): data=await reader.read() message=data.decode() addr=writer.get_extra_info('peername') logger.info(f\"Received{message} at{addr}\") request=Request(message, time.time()) is_new, rec=get_or_create_client_record(request) payload=None flood=False if request.is_iam() and not request.was_visited_by(MYNAME): records[rec.addr]=rec if is_new else make_record(request) flood=True elif is_new: if request.is_whatisat(): request.mark_invalid() elif request.is_iamat(): records[rec.addr]=rec flood=True else: if request.is_iamat(): if str(rec.position) !=Position.coords(request.lat, request.lon): rec=update_record(rec, request) flood=True elif request.is_whatisat(): if rec.position.radius==request.radius: if request.pagination <=rec.position.pagination: payload=json.loads(rec.position.payload) payload['results']=payload['results'][:request.pagination] payload=json.dumps(payload) elif rec.position.pagination <=request.pagination: loc=request.location rad=request.radius pag=request.pagination logger.debug(f'{request=}') api_response=await api_call(loc, rad, pag) rec.position.pagination=request.pagination rec.position.payload=json.dumps(api_response) payload=rec.position.payload else: request.mark_invalid() raise Exception('Received an invalid exception') else: loc=str(rec.position) rad=request.radius pag=request.pagination logger.debug(f'{str(request)=}') api_response=await api_call(loc, rad, pag) rec.position.radius=rad rec.position.pagination=pag rec.position.payload=json.dumps(api_response) payload=rec.position.payload else: pass if not request.is_iam(): resp=request.response(MYNAME, rec, payload=payload) logger.info(f\"Send:{resp}\") writer.write(resp.encode()) await writer.drain() logger.debug(\"Close the connection\") writer.close() await writer.wait_closed() if request.is_iam() or flood: await propagate(request) async def propagate(request: Request): request.mark_visited(MYNAME) visited=request.get_visited() to_visit=[x for x in graph[MYNAME] if x not in visited] resp=request.flood_response(MYNAME) for neighbor in to_visit: try: _, writer=await asyncio.open_connection(ipaddr, herd[neighbor]) logger.info(f'Sent{neighbor}:{resp}') writer.write(resp.encode()) await writer.drain() writer.close() await writer.wait_closed() except: logger.info(f\"Unable to connect to{neighbor}\") def dummy_api_call(location, radius, pagination): with open('places_raw.json', 'r') as rf: data=json.load(rf) data['results']=data['results'][:pagination] return data async def api_call(location, radius, pag): key=env.PLACES_API_KEY url=( f'https:\/\/maps.googleapis.com\/maps\/api\/place\/nearbysearch\/json' + f'?location={location}' + f'&radius={radius}' + f'&key={key}' ) ret=None async with aiohttp.ClientSession() as session: async with session.get(url) as resp: data=await resp.text() json_data=json.loads(data) if len(results:=json_data['results']) >=pag: results=results[:pag] return json_data async def main(): fname, port=parse(sys.argv[1:]) if not fname: raise SystemExit(USAGE) global MYNAME MYNAME=fname init_logger(MYNAME) logger.info(f'Starting{MYNAME} on port{port}') server=await asyncio.start_server( handle_echo, ipaddr, port=port) async with server: await server.serve_forever() if __name__=='__main__': try: asyncio.run(main()) except KeyboardInterrupt: logging.info(f\"Keyboard Interrupt. Shutting down.\\n\") sys.exit(0) ","sourceWithComments":"import aiohttp\nimport asyncio\nimport collections\nimport json\nimport logging\nimport sys\nimport time\n\nfrom typing import List, Tuple\nfrom urllib.parse import quote_plus as safe_url\n\nimport env\nfrom request import Request\nfrom record import Record, Position\n\nlogger = logging.getLogger(__name__)\n\ndef init_logger(filename):\n    logger.setLevel(logging.DEBUG)\n\n    c_handler = logging.StreamHandler()\n    f_handler = logging.FileHandler(filename+\".log\", mode=\"w\")\n    c_handler.setLevel(logging.DEBUG)\n    f_handler.setLevel(logging.INFO)\n\n    c_format = logging.Formatter('%(message)s')\n    f_format = logging.Formatter('%(asctime)s - %(message)s')\n    c_handler.setFormatter(c_format)\n    f_handler.setFormatter(f_format)\n\n    logger.addHandler(c_handler)\n    logger.addHandler(f_handler)\n\n\nmy_herd = {\n    'Bailey': 17800,\n    'Bona': 17801,\n    'Campbell': 17802,\n    'Clark': 17803,\n    'Jaquez': 17804,\n}\n\nseas_herd = {\n    'Bailey': 10000,\n    'Bona': 10001,\n    'Campbell': 10002,\n    'Clark': 10003,\n    'Jaquez': 10004,\n}\n\n# Change this to seas herd when runnin on seas\nherd = my_herd\n\ngraph = {\n    'Bailey': ['Campbell', 'Bona'],\n    'Bona': ['Clark', 'Jaquez', 'Campbell', 'Bailey'],\n    'Campbell': ['Bailey', 'Bona', 'Jaquez'],\n    'Clark': ['Jaquez', 'Bona'],\n    'Jaquez': ['Clark', 'Bona', 'Campbell'],\n}\n\n# Clark talks with Jaquez and Bona.\n# Campbell talks with everyone else but Clark.\n# Bona talks with Bailey.\n\nMYNAME=\"\"\n\nipaddr='127.0.0.1'\n\nUSAGE = (\n    f\"Usage {sys.argv[0]} [srvname]\"\n)\n\nrecords = {}\n\ndef parse(args: List[str]) -> Tuple[str, int]:\n    arguments = collections.deque(args)\n    name=None\n    while arguments:\n        current = arguments.popleft()\n        if name is None:\n            if current in [\"-h\", \"--help\"]:\n                print(USAGE)\n                sys.exit(0)\n            else:\n                name = current\n        else:\n            print(USAGE)\n            sys.exit(0)\n    try:\n        port = herd[name]\n    except:\n        print(f\"\\\"{name}\\\" not a valid server.\")\n        # logger.error(f\"\\\"{name}\\\" not a valid server.\")\n        sys.exit(0)\n    return name, port\n\ndef make_position(req):\n    return Position(req.lat, req.lon, \n                     radius=req.radius, \n                     pagination=req.pagination, \n                     payload=req._payload)\n\ndef update_record(record, req, is_peer=False):\n    record.skew = req.skew\n    record.client_time = req.client_time\n    record.position = make_position(req)\n    return record\n\ndef make_record(req):\n    return Record(\n            req.addr,\n            req.skew,\n            req.client_time,\n            make_position(req),\n        )\n\ndef get_or_create_client_record(req):\n    is_new = False\n    try:\n        rec = records[req.addr]\n    except:\n        rec = make_record(req)\n        is_new = True\n    return is_new, rec\n\nTYPE = {\n    'IAMAT': None,\n    'WHATISAT': None,\n    # 'IAM': None,\n}\n\nasync def handle_echo(reader, writer):\n    # Read info from sender\n    data = await reader.read() # Want this to be as many as needed\n    \n    message = data.decode()\n    addr = writer.get_extra_info('peername')\n    logger.info(f\"Received {message} at {addr}\")\n\n    # Parse the message. If we have a client \n    request = Request(message, time.time())\n    is_new, rec = get_or_create_client_record(request)\n    payload = None\n    flood = False\n\n    # Flood throught network\n    if request.is_iam() and not request.was_visited_by(MYNAME):\n        records[rec.addr] = rec if is_new else make_record(request)\n        flood = True\n\n    # Do stuff with the record\n    elif is_new:\n        # Invalid Request by new Client\n        if request.is_whatisat():\n            # Need a location before we can answer whatisat\n            request.mark_invalid()\n\n        # New Client\n        elif request.is_iamat(): \n            records[rec.addr] = rec\n            flood = True\n    \n    # Update an existing Client\n    else:    \n        if request.is_iamat():\n            # if iamat and location is same, reply to client only\n            if str(rec.position) != Position.coords(request.lat, request.lon):\n                # FIXME I don't like how this update is occuring. just make direct like the rest of the code\n                rec = update_record(rec, request)\n\n                flood = True\n\n        # Existing Client Query\n        elif request.is_whatisat():\n            if rec.position.radius == request.radius:\n                # serve a subset of previously queried data\n                if request.pagination <= rec.position.pagination:\n                    # construct client response with payload\n                    payload = json.loads(rec.position.payload)\n                    payload['results'] = payload['results'][:request.pagination]\n                    payload = json.dumps(payload)\n\n                # Do an API call to get more results\n                elif rec.position.pagination <= request.pagination:\n                    loc = request.location\n                    rad = request.radius\n                    pag = request.pagination\n                    logger.debug(f'{request=}')\n                    api_response = await api_call(loc, rad, pag)\n                    \n                    # update record with new pagesize and payload\n                    rec.position.pagination = request.pagination\n                    rec.position.payload = json.dumps(api_response)\n                    payload = rec.position.payload\n                # Invalid response\n                else:\n                    request.mark_invalid()\n                    raise Exception('Received an invalid exception')\n\n            else:\n                # perform api query\n                # api_response = await api_call(rec.position, rec.position.radius)\n                # with open('places_raw.json', 'w') as f:\n                #     json.dump(api_response, f)\n\n                # FIXME: location hack. fix this\n                loc = str(rec.position)\n                rad = request.radius\n                pag = request.pagination\n                logger.debug(f'{str(request)=}')\n                api_response = await api_call(loc, rad, pag)\n\n                # update record\n                # print(\"updating record\")\n                rec.position.radius = rad\n                rec.position.pagination = pag\n                rec.position.payload = json.dumps(api_response)\n                payload = rec.position.payload\n        else:\n            # TODO: Do I need to do something here?\n            pass\n    \n    if not request.is_iam():\n        resp = request.response(MYNAME, rec, payload=payload)\n        \n        # Reply to sender\n        logger.info(f\"Send: {resp}\")\n        writer.write(resp.encode())\n        await writer.drain()\n\n        logger.debug(\"Close the connection\")\n        writer.close()\n        await writer.wait_closed()\n\n    if request.is_iam() or flood:\n        await propagate(request)\n        \n\nasync def propagate(request: Request):\n    request.mark_visited(MYNAME)\n    visited = request.get_visited()\n    to_visit = [x for x in graph[MYNAME] if x not in visited]\n    resp = request.flood_response(MYNAME)\n\n    for neighbor in to_visit:\n        try:\n            _, writer = await asyncio.open_connection(ipaddr, herd[neighbor])\n\n            logger.info(f'Sent {neighbor}: {resp}')\n\n            writer.write(resp.encode())\n            await writer.drain()\n\n            writer.close()\n            await writer.wait_closed()\n        except:\n            logger.info(f\"Unable to connect to {neighbor}\")\n\ndef dummy_api_call(location, radius, pagination):\n    with open('places_raw.json', 'r') as rf:\n        data = json.load(rf)\n        data['results'] = data['results'][:pagination]\n        return data\n\nasync def api_call(location, radius, pag):\n    key = env.PLACES_API_KEY\n    url=(\n        f'https:\/\/maps.googleapis.com\/maps\/api\/place\/nearbysearch\/json' +\n        f'?location={location}' +\n        f'&radius={radius}' +\n        f'&key={key}'\n    )\n    # logger.debug(f\"api_call({location}, {radius}, {pag}) {url=}\")\n    # url = (\n    #     # f\"https:\/\/maps.googleapis.com\/maps\/api\/place\/nearbysearch\/json?location=-33.8670522%2C151.1957362&radius=50&type=restaurant&keyword=cruise&key={key}\"\n    #     f\"https:\/\/maps.googleapis.com\/maps\/api\/place\/nearbysearch\/json?location=-33.8670522%2C151.1957362&radius=50&key={key}\"\n    # )\n    ret = None\n    async with aiohttp.ClientSession() as session:\n        async with session.get(url) as resp:\n            data = await resp.text()\n            # print(f'{data=}')\n            json_data = json.loads(data)\n            # print(f'{json_data=}')\/\n            if len(results := json_data['results']) >= pag:\n                results = results[:pag]\n\n            # return json.loads(await resp.text())\n            return json_data\n\nasync def main():\n    fname, port = parse(sys.argv[1:])\n    if not fname:\n        raise SystemExit(USAGE)\n\n    global MYNAME \n    MYNAME = fname\n    init_logger(MYNAME)\n\n    # should I open global connections here?\n    \n    logger.info(f'Starting {MYNAME} on port {port}')\n    server = await asyncio.start_server(\n        handle_echo, ipaddr, port=port)\n\n    # addrs = ', '.join(str(sock.getsockname()) for sock in server.sockets)\n    # logger.debug(f'Serving on {addrs}')\n\n    async with server:\n        await server.serve_forever()\n\nif __name__=='__main__':\n    try:\n        asyncio.run(main())\n    except KeyboardInterrupt:\n        logging.info(f\"Keyboard Interrupt. Shutting down.\\n\")\n        sys.exit(0)"}},"msg":"Fix flooding cycle issues\n\nAdd generic api_calling handle\n\nUpdate graph adjacency list\n\nUpdate parsing issue of IAM requests where ast.literal_eval() was\nparsing an eof. It is not clear whether the eof issue was related to\nasync io between servers or in the str(self.nodes_visited) conversion of\na Record objects nodes_visited."}},"https:\/\/github.com\/bruno-manoel-dbki\/tantalum":{"03295aa8d149e3bb75135bc90d7dd42f0fd51ed6":{"url":"https:\/\/api.github.com\/repos\/bruno-manoel-dbki\/tantalum\/commits\/03295aa8d149e3bb75135bc90d7dd42f0fd51ed6","html_url":"https:\/\/github.com\/bruno-manoel-dbki\/tantalum\/commit\/03295aa8d149e3bb75135bc90d7dd42f0fd51ed6","message":"ml_sets generated. Minor mistakes to fix in flood method in gb_remake files","sha":"03295aa8d149e3bb75135bc90d7dd42f0fd51ed6","keyword":"flooding fix","diff":"diff --git a\/source\/Divide_et_Vince.py b\/source\/Divide_et_Vince.py\nindex 27dd185..27a389e 100644\n--- a\/source\/Divide_et_Vince.py\n+++ b\/source\/Divide_et_Vince.py\n@@ -1,6 +1,3 @@\n-\n-\n-\n import pandas as pd\n import numpy as np\n from skimage import draw,io\n@@ -8,6 +5,7 @@\n import cv2\n import math\n from matplotlib import pyplot as plt\n+\n from IPython.display import set_matplotlib_formats\n plt.rcParams['figure.dpi'] = 400\n plt.rcParams['savefig.dpi'] = 400\n@@ -57,7 +55,8 @@ def clockwiseangle_and_distance(point):\n \n #%%\n \n-sample = np.loadtxt(path+ \".txt\")\n+#sample = np.loadtxt(path+ \".txt\")\n+sample = pd.read_pickle(\"output\/\" + file + \"_remake.pkl\")\n \n \n \n@@ -97,6 +96,7 @@ def clockwiseangle_and_distance(point):\n overflood = np.sum(flooded_grains==0) * 0.8\n over = []\n out = []\n+print(\"Dataframe sucessfully imported\")\n \n # In[5]:\n \n@@ -122,9 +122,10 @@ def clockwiseangle_and_distance(point):\n \n df_grains_norm = (df_grains - df_grains.min()) \/ (df_grains.max() - df_grains.min())\n \n+print(\"ETL in Dataframe sucessfully done\")\n \n-\n-for grain in df_grains.index:\n+print(\"Running flood method\")\n+for grain in df_grains.index.dropna():\n         One_grain = df[(df[\"grain_right\"] == grain) | (df[\"grain_left\"] == grain)]\n         grain_info = df_grains_norm.loc[grain,:]\n         np_img = np.zeros([height, width, 3])\n@@ -184,34 +185,34 @@ def clockwiseangle_and_distance(point):\n             #cv2.putText(flooded_grains, text=str(int(grain)), org=(x_center,y_center),fontFace=2, fontScale=0.2, color=(255,255,255), thickness=1)\n \n \n-\n+#%%\n+print(\"Flood method done\")\n ## PART 2 - SLICE METHOD\n \n+\n+print(\"Running Slice method\")\n width = int(max([max(df.x_end),max(df.x_start)]))+1\n height = int(max([max(df.y_end),max(df.y_start)]))+1\n \n N = width\/\/4\n M = height\/\/4\n \n-tiles = [flooded_grains[x:x+M,y:y+N] for x in range(0,flooded_grains.shape[0],M) for y in range(0,flooded_grains.shape[1],N)]\n \n-for tile in tiles:\n-#    centers, radii, vheight, image, drawing = fv.find_voids_2(tile)\n-    cv2.imshow(\"Tile\",tile)\n \n+grey_img = cv2.imread(path+ '.jpg', 0)\n+grey_img = cv2.resize(grey_img,(width,height),interpolation = cv2.INTER_AREA)\n+\n+tiles = [flooded_grains[x:x+M,y:y+N] for x in range(0,flooded_grains.shape[0],M) for y in range(0,flooded_grains.shape[1],N)]\n \n+tiles_grey = [grey_img[x:x+M,y:y+N] for x in range(0,grey_img.shape[0],M) for y in range(0,grey_img.shape[1],N)]\n \n+n_voids = []\n \n+for idx in range(len(tiles_grey)):\n+    centers, radii, vheight, image, drawing = fv.find_voids_2(tiles_grey[idx])\n+    n_voids.append([idx,len(centers)])\n+    io.imsave(\"ml_sets\/\"+ file + '_'+ str(idx) + '_' + str(len(centers)) + 'void.png',tiles[idx])\n \n-grey_img = cv2.imread(path+ '.jpg', 0)\n-grey_img = cv2.resize(grey_img,(width,height),interpolation = cv2.INTER_AREA)\n+#save n_voids as csv\n \n-tiles_grey = [grey_img[x:x+M,y:y+N] for x in range(0,grey_img.shape[0],M) for y in range(0,grey_img.shape[1],N)]\n-i=0\n-for tile in tiles_grey:\n-    print(\"tile: \"+str(i))\n-    i+=1\n-    centers, radii, vheight, image, drawing = fv.find_voids_2(tile)\n-    #plt.imshow(tile)\n-\n-plt.imshow(tiles_grey[14])\n+print(\"Done\")\n\\ No newline at end of file\ndiff --git a\/source\/GB_reconstruction.py b\/source\/GB_reconstruction.py\nindex b85edad..dea4f99 100644\n--- a\/source\/GB_reconstruction.py\n+++ b\/source\/GB_reconstruction.py\n@@ -110,8 +110,8 @@ def find_voids_2(original):\n \n #%%\n \n-folder = \"..\/data\/\"\n-file = \"1_007\"\n+folder = \"data\/\"\n+file = \"1_001\"\n path = folder + file\n \n #%%\n@@ -285,7 +285,7 @@ def find_voids_2(original):\n    \n     \n         \n-        \n+        start_points = bd_to_keep[[\"x_start\",\"y_start\"]].dropna().values.astype(\"int32\").tolist()\n         end_points = bd_to_keep[[\"x_end\",\"y_end\"]].dropna().values.astype(\"int32\").tolist()\n     \n         \n@@ -331,116 +331,117 @@ def find_voids_2(original):\n \n void_clean = np.zeros([height+1,width+1, 3])\n \n-for idx, row in bd_clean.iterrows():\n-    rr,cc = draw.line(row.x_start,row.y_start,row.x_end,row.y_end)\n+for idx, row in bd_new.iterrows():\n+    rr,cc,a = draw.line_aa(row.x_start.astype(\"uint16\"),row.y_start.astype(\"uint16\"),row.x_end.astype(\"uint16\"),row.y_end.astype(\"uint16\"))\n \n     void_clean[cc,rr] = (0,255,0)\n \n+\n         \n plt.figure(10)\n plt.imshow(void_clean)\n #plt.imsave(\"Boundaries.png\",void_clean.astype(\"uint8\"))\n plt.show()\n-\n+bd_new.to_pickle(\"output\/\"+ file + \"_remake.pkl\")  \n     \n     \n-#%%\n+# #%%\n \n \n-# fig, axs = plt.subplots(2,2)\n-# fig.suptitle('GB Layers')\n-# axs[0,0].set_title(\"Original\")\n-# axs[0,0].imshow(np_img)\n-# axs[0,1].imshow(void[:,:,0])\n-# axs[1,0].imshow(void[:,:,2])\n-# axs[1,1].imshow(void[:,:,1])\n-#%%\n+# # fig, axs = plt.subplots(2,2)\n+# # fig.suptitle('GB Layers')\n+# # axs[0,0].set_title(\"Original\")\n+# # axs[0,0].imshow(np_img)\n+# # axs[0,1].imshow(void[:,:,0])\n+# # axs[1,0].imshow(void[:,:,2])\n+# # axs[1,1].imshow(void[:,:,1])\n+# #%%\n \n-# new bds = (0,255,255) # Light blue\n-# removed bds = (255,0,0) # Red\n-# bds_keeped = ()\n+# # new bds = (0,255,255) # Light blue\n+# # removed bds = (255,0,0) # Red\n+# # bds_keeped = ()\n \n \n \n-# plt.figure(5)\n-# plt.imshow(void)\n-# plt.title(\"Final\")\n+# # plt.figure(5)\n+# # plt.imshow(void)\n+# # plt.title(\"Final\")\n \n-# plt.figure(4)\n-# plt.imshow(void[:,:,2])\n-# plt.title(\"New Lines\")\n+# # plt.figure(4)\n+# # plt.imshow(void[:,:,2])\n+# # plt.title(\"New Lines\")\n \n-# plt.figure(3)\n-# plt.imshow(void[:,:,0])\n-# plt.title(\"Dropped\")\n+# # plt.figure(3)\n+# # plt.imshow(void[:,:,0])\n+# # plt.title(\"Dropped\")\n \n-# plt.figure(2)\n-# plt.imshow(voids_detected)\n-# plt.title(\"Voids Detected\")\n+# # plt.figure(2)\n+# # plt.imshow(voids_detected)\n+# # plt.title(\"Voids Detected\")\n \n \n-# plt.figure(1)\n-# plt.imshow(np_img)\n-# plt.title(\"Original\")\n+# # plt.figure(1)\n+# # plt.imshow(np_img)\n+# # plt.title(\"Original\")\n \n \n \n-# plt.show()\n+# # plt.show()\n \n-#%%\n-#      FOR ALL VOID, CREATE A SLICE OF ORIGINAL DF CONSIDERING ONLY BOUNDARIES \n-#      AROUND THE VOID. SAVE IT AS A GOOD DF\n+# #%%\n+# #      FOR ALL VOID, CREATE A SLICE OF ORIGINAL DF CONSIDERING ONLY BOUNDARIES \n+# #      AROUND THE VOID. SAVE IT AS A GOOD DF\n \n-for idx,useful in useful_void:\n+# for idx,useful in useful_void:\n     \n-    if (useful is True):\n-        radi_0 = radii[idx]*5\n-        center_0 = centers[idx]\n-        radi_0 = round(radi_0)\n+#     if (useful is True):\n+#         radi_0 = radii[idx]*5\n+#         center_0 = centers[idx]\n+#         radi_0 = round(radi_0)\n     \n     \n-        # x_start, y_start = center_0[0] - radi_0 , center_0[1] - radi_0 \n-        # x_end, y_end = center_0[0] + radi_0 , center_0[1] + radi_0 \n-        x_start, y_start = center_0[0] - 50, center_0[1] - 50 \n-        x_end, y_end = center_0[0] + 50 , center_0[1] + 50\n+#         # x_start, y_start = center_0[0] - radi_0 , center_0[1] - radi_0 \n+#         # x_end, y_end = center_0[0] + radi_0 , center_0[1] + radi_0 \n+#         x_start, y_start = center_0[0] - 50, center_0[1] - 50 \n+#         x_end, y_end = center_0[0] + 50 , center_0[1] + 50\n         \n         \n-        bd_start_in_area = bd_new[\n-                                (bd_new[\"x_start\"]>x_start) & (bd_new[\"x_start\"]<x_end) \n-                                &\n-                                (bd_new[\"y_start\"]>y_start) & (bd_new[\"y_start\"]<y_end)]\n+#         bd_start_in_area = bd_new[\n+#                                 (bd_new[\"x_start\"]>x_start) & (bd_new[\"x_start\"]<x_end) \n+#                                 &\n+#                                 (bd_new[\"y_start\"]>y_start) & (bd_new[\"y_start\"]<y_end)]\n         \n         \n         \n-        bd_end_in_area = bd_new[\n-                                (bd_new[\"x_end\"]>x_start) & (bd_new[\"x_end\"]<x_end) \n-                                &\n-                                (bd_new[\"y_end\"]>y_start) & (bd_new[\"y_end\"]<y_end)]\n+#         bd_end_in_area = bd_new[\n+#                                 (bd_new[\"x_end\"]>x_start) & (bd_new[\"x_end\"]<x_end) \n+#                                 &\n+#                                 (bd_new[\"y_end\"]>y_start) & (bd_new[\"y_end\"]<y_end)]\n         \n         \n        \n-        bd_inside = pd.concat([bd_start_in_area, bd_end_in_area])\n-        bd_inside = bd_inside[~bd_inside.index.duplicated()]\n+#         bd_inside = pd.concat([bd_start_in_area, bd_end_in_area])\n+#         bd_inside = bd_inside[~bd_inside.index.duplicated()]\n \n          \n-        bd_inside.loc[:,\"x_start\"] = bd_inside.x_start - x_start\n-        bd_inside.loc[:,\"x_end\"] = bd_inside.x_end - x_start\n+#         bd_inside.loc[:,\"x_start\"] = bd_inside.x_start - x_start\n+#         bd_inside.loc[:,\"x_end\"] = bd_inside.x_end - x_start\n         \n-        bd_inside.loc[:,\"y_start\"] = bd_inside.y_start - y_start\n-        bd_inside.loc[:,\"y_end\"] = bd_inside.y_end - y_start\n+#         bd_inside.loc[:,\"y_start\"] = bd_inside.y_start - y_start\n+#         bd_inside.loc[:,\"y_end\"] = bd_inside.y_end - y_start\n         \n-        x_min = bd_inside[[\"x_start\",\"x_end\"]].values.min()\n-        y_min = bd_inside[[\"y_start\",\"y_end\"]].values.min()\n+#         x_min = bd_inside[[\"x_start\",\"x_end\"]].values.min()\n+#         y_min = bd_inside[[\"y_start\",\"y_end\"]].values.min()\n         \n-        bd_inside.loc[:,\"x_start\"] = bd_inside.x_start - x_min\n-        bd_inside.loc[:,\"x_end\"] = bd_inside.x_end - x_min\n+#         bd_inside.loc[:,\"x_start\"] = bd_inside.x_start - x_min\n+#         bd_inside.loc[:,\"x_end\"] = bd_inside.x_end - x_min\n         \n-        bd_inside.loc[:,\"y_start\"] = bd_inside.y_start - y_min\n-        bd_inside.loc[:,\"y_end\"] = bd_inside.y_end -y_min\n+#         bd_inside.loc[:,\"y_start\"] = bd_inside.y_start - y_min\n+#         bd_inside.loc[:,\"y_end\"] = bd_inside.y_end -y_min\n         \n         \n-        x_size = bd_inside[[\"x_start\",\"x_end\"]].values.max()\n-        y_size = bd_inside[[\"y_start\",\"y_end\"]].values.max()\n+#         x_size = bd_inside[[\"x_start\",\"x_end\"]].values.max()\n+#         y_size = bd_inside[[\"y_start\",\"y_end\"]].values.max()\n         \n         \n         \n@@ -448,106 +449,106 @@ def find_voids_2(original):\n         \n         \n         \n-        void_new = np.zeros([y_size+1,x_size+1, 3])\n-        # void_new = np.zeros([height+1,width+1, 3])\n+#         void_new = np.zeros([y_size+1,x_size+1, 3])\n+#         # void_new = np.zeros([height+1,width+1, 3])\n \n-        for aux, row in bd_inside.iterrows():\n-            rr,cc = draw.line(row.x_start.astype(\"uint8\"),row.y_start.astype(\"uint8\"),row.x_end.astype(\"uint8\"),row.y_end.astype(\"uint8\"))\n-            void_new[cc,rr] = (0,255,0)\n+#         for aux, row in bd_inside.iterrows():\n+#             rr,cc = draw.line(row.x_start.astype(\"uint8\"),row.y_start.astype(\"uint8\"),row.x_end.astype(\"uint8\"),row.y_end.astype(\"uint8\"))\n+#             void_new[cc,rr] = (0,255,0)\n             \n-            # void_new[cc-y_start,rr-x_start] = (0,255,0)\n+#             # void_new[cc-y_start,rr-x_start] = (0,255,0)\n         \n         \n         \n         \n-        # Pickle File    \n+#         # Pickle File    \n         \n         \n-        if( not os.path.exists(\"..\/output\")):\n-            print(\"Creating folder\")\n-            os.mkdir(\"..\/output\")\n+#         if( not os.path.exists(\"..\/output\")):\n+#             print(\"Creating folder\")\n+#             os.mkdir(\"..\/output\")\n             \n             \n             \n-        plt.imsave(\"..\/output\/\"+ file+ \"_void_\"+ str(idx) + \"_new.jpg\", void_new.astype(\"uint8\"))\n-        bd_inside.to_pickle(\"..\/output\/\"+ file+ \"_void_\"+ str(idx) + \"_new.pkl\")  \n+#         plt.imsave(\"..\/output\/\"+ file+ \"_void_\"+ str(idx) + \"_new.jpg\", void_new.astype(\"uint8\"))\n+#         bd_inside.to_pickle(\"..\/output\/\"+ file+ \"_void_\"+ str(idx) + \"_new.pkl\")  \n         \n         \n         \n         \n-#%%\n+# #%%\n \n-for idx,useful in useful_void:\n+# for idx,useful in useful_void:\n     \n-    if (useful is True):\n-        radi_0 = radii[idx]*5\n-        center_0 = centers[idx]\n-        radi_0 = round(radi_0)\n+#     if (useful is True):\n+#         radi_0 = radii[idx]*5\n+#         center_0 = centers[idx]\n+#         radi_0 = round(radi_0)\n     \n     \n-        # x_start, y_start = center_0[0] - radi_0 , center_0[1] - radi_0 \n-        # x_end, y_end = center_0[0] + radi_0 , center_0[1] + radi_0 \n-        x_start, y_start = center_0[0] - 50, center_0[1] - 50 \n-        x_end, y_end = center_0[0] + 50 , center_0[1] + 50\n+#         # x_start, y_start = center_0[0] - radi_0 , center_0[1] - radi_0 \n+#         # x_end, y_end = center_0[0] + radi_0 , center_0[1] + radi_0 \n+#         x_start, y_start = center_0[0] - 50, center_0[1] - 50 \n+#         x_end, y_end = center_0[0] + 50 , center_0[1] + 50\n         \n         \n-        bd_start_in_area = bd_clean[\n-                                (bd_clean[\"x_start\"]>x_start) & (bd_clean[\"x_start\"]<x_end) \n-                                &\n-                                (bd_clean[\"y_start\"]>y_start) & (bd_clean[\"y_start\"]<y_end)]\n+#         bd_start_in_area = bd_clean[\n+#                                 (bd_clean[\"x_start\"]>x_start) & (bd_clean[\"x_start\"]<x_end) \n+#                                 &\n+#                                 (bd_clean[\"y_start\"]>y_start) & (bd_clean[\"y_start\"]<y_end)]\n         \n         \n         \n-        bd_end_in_area = bd_clean[\n-                                (bd_clean[\"x_end\"]>x_start) & (bd_clean[\"x_end\"]<x_end) \n-                                &\n-                                (bd_clean[\"y_end\"]>y_start) & (bd_clean[\"y_end\"]<y_end)]\n+#         bd_end_in_area = bd_clean[\n+#                                 (bd_clean[\"x_end\"]>x_start) & (bd_clean[\"x_end\"]<x_end) \n+#                                 &\n+#                                 (bd_clean[\"y_end\"]>y_start) & (bd_clean[\"y_end\"]<y_end)]\n         \n         \n        \n-        bd_inside = pd.concat([bd_start_in_area, bd_end_in_area])\n-        bd_inside = bd_inside[~bd_inside.index.duplicated()]\n+#         bd_inside = pd.concat([bd_start_in_area, bd_end_in_area])\n+#         bd_inside = bd_inside[~bd_inside.index.duplicated()]\n \n          \n-        bd_inside.loc[:,\"x_start\"] = bd_inside.x_start - x_start\n-        bd_inside.loc[:,\"x_end\"] = bd_inside.x_end - x_start\n+#         bd_inside.loc[:,\"x_start\"] = bd_inside.x_start - x_start\n+#         bd_inside.loc[:,\"x_end\"] = bd_inside.x_end - x_start\n         \n-        bd_inside.loc[:,\"y_start\"] = bd_inside.y_start - y_start\n-        bd_inside.loc[:,\"y_end\"] = bd_inside.y_end - y_start\n+#         bd_inside.loc[:,\"y_start\"] = bd_inside.y_start - y_start\n+#         bd_inside.loc[:,\"y_end\"] = bd_inside.y_end - y_start\n         \n-        x_min = bd_inside[[\"x_start\",\"x_end\"]].values.min()\n-        y_min = bd_inside[[\"y_start\",\"y_end\"]].values.min()\n+#         x_min = bd_inside[[\"x_start\",\"x_end\"]].values.min()\n+#         y_min = bd_inside[[\"y_start\",\"y_end\"]].values.min()\n         \n-        bd_inside.loc[:,\"x_start\"] = bd_inside.x_start - x_min\n-        bd_inside.loc[:,\"x_end\"] = bd_inside.x_end - x_min\n+#         bd_inside.loc[:,\"x_start\"] = bd_inside.x_start - x_min\n+#         bd_inside.loc[:,\"x_end\"] = bd_inside.x_end - x_min\n         \n-        bd_inside.loc[:,\"y_start\"] = bd_inside.y_start - y_min\n-        bd_inside.loc[:,\"y_end\"] = bd_inside.y_end -y_min\n+#         bd_inside.loc[:,\"y_start\"] = bd_inside.y_start - y_min\n+#         bd_inside.loc[:,\"y_end\"] = bd_inside.y_end -y_min\n         \n         \n-        x_size = bd_inside[[\"x_start\",\"x_end\"]].values.max()\n-        y_size = bd_inside[[\"y_start\",\"y_end\"]].values.max()\n+#         x_size = bd_inside[[\"x_start\",\"x_end\"]].values.max()\n+#         y_size = bd_inside[[\"y_start\",\"y_end\"]].values.max()\n         \n         \n         \n         \n         \n         \n-        # Pickle File    \n+#         # Pickle File    \n         \n         \n-        if( not os.path.exists(\"..\/output\")):\n-            print(\"Creating folder\")\n-            os.mkdir(\"..\/output\")\n+#         if( not os.path.exists(\"..\/output\")):\n+#             print(\"Creating folder\")\n+#             os.mkdir(\"..\/output\")\n             \n             \n             \n-        #plt.imsave(\"..\/output\/\"+ file+ \"_void_\"+ str(idx) + \"_base.jpg\", void_new.astype(\"uint8\"))\n-        bd_inside.to_pickle(\"..\/output\/\"+ file+ \"_void_\"+ str(idx) + \"_base.pkl\")  \n+#         #plt.imsave(\"..\/output\/\"+ file+ \"_void_\"+ str(idx) + \"_base.jpg\", void_new.astype(\"uint8\"))\n+#         bd_inside.to_pickle(\"..\/output\/\"+ file+ \"_void_\"+ str(idx) + \"_base.pkl\")  \n         \n        \n         \n         \n \n-# TODO: MAYBE WE HAVE INTEREST IN CONSIDER THE BEHAVIOR OF BIG HOLES, TO DO\n-#       WE'LL START WITH ALL FALSES ELEMENTS IN useful_voids\n\\ No newline at end of file\n+# # TODO: MAYBE WE HAVE INTEREST IN CONSIDER THE BEHAVIOR OF BIG HOLES, TO DO\n+# #       WE'LL START WITH ALL FALSES ELEMENTS IN useful_voids\n\\ No newline at end of file\n","files":{"\/source\/Divide_et_Vince.py":{"changes":[{"diff":"\n \n #%%\n \n-sample = np.loadtxt(path+ \".txt\")\n+#sample = np.loadtxt(path+ \".txt\")\n+sample = pd.read_pickle(\"output\/\" + file + \"_remake.pkl\")\n \n \n \n","add":2,"remove":1,"filename":"\/source\/Divide_et_Vince.py","badparts":["sample = np.loadtxt(path+ \".txt\")"],"goodparts":["sample = pd.read_pickle(\"output\/\" + file + \"_remake.pkl\")"]},{"diff":"\n \n df_grains_norm = (df_grains - df_grains.min()) \/ (df_grains.max() - df_grains.min())\n \n+print(\"ETL in Dataframe sucessfully done\")\n \n-\n-for grain in df_grains.index:\n+print(\"Running flood method\")\n+for grain in df_grains.index.dropna():\n         One_grain = df[(df[\"grain_right\"] == grain) | (df[\"grain_left\"] == grain)]\n         grain_info = df_grains_norm.loc[grain,:]\n         np_img = np.zeros([height, width, 3])\n","add":3,"remove":2,"filename":"\/source\/Divide_et_Vince.py","badparts":["for grain in df_grains.index:"],"goodparts":["print(\"ETL in Dataframe sucessfully done\")","print(\"Running flood method\")","for grain in df_grains.index.dropna():"]},{"diff":"\n             #cv2.putText(flooded_grains, text=str(int(grain)), org=(x_center,y_center),fontFace=2, fontScale=0.2, color=(255,255,255), thickness=1)\n \n \n-\n+#%%\n+print(\"Flood method done\")\n ## PART 2 - SLICE METHOD\n \n+\n+print(\"Running Slice method\")\n width = int(max([max(df.x_end),max(df.x_start)]))+1\n height = int(max([max(df.y_end),max(df.y_start)]))+1\n \n N = width\/\/4\n M = height\/\/4\n \n-tiles = [flooded_grains[x:x+M,y:y+N] for x in range(0,flooded_grains.shape[0],M) for y in range(0,flooded_grains.shape[1],N)]\n \n-for tile in tiles:\n-#    centers, radii, vheight, image, drawing = fv.find_voids_2(tile)\n-    cv2.imshow(\"Tile\",tile)\n \n+grey_img = cv2.imread(path+ '.jpg', 0)\n+grey_img = cv2.resize(grey_img,(width,height),interpolation = cv2.INTER_AREA)\n+\n+tiles = [flooded_grains[x:x+M,y:y+N] for x in range(0,flooded_grains.shape[0],M) for y in range(0,flooded_grains.shape[1],N)]\n \n+tiles_grey = [grey_img[x:x+M,y:y+N] for x in range(0,grey_img.shape[0],M) for y in range(0,grey_img.shape[1],N)]\n \n+n_voids = []\n \n+for idx in range(len(tiles_grey)):\n+    centers, radii, vheight, image, drawing = fv.find_voids_2(tiles_grey[idx])\n+    n_voids.append([idx,len(centers)])\n+    io.imsave(\"ml_sets\/\"+ file + '_'+ str(idx) + '_' + str(len(centers)) + 'void.png',tiles[idx])\n \n-grey_img = cv2.imread(path+ '.jpg', 0)\n-grey_img = cv2.resize(grey_img,(width,height),interpolation = cv2.INTER_AREA)\n+#save n_voids as csv\n \n-tiles_grey = [grey_img[x:x+M,y:y+N] for x in range(0,grey_img.shape[0],M) for y in range(0,grey_img.shape[1],N)]\n-i=0\n-for tile in tiles_grey:\n-    print(\"tile: \"+str(i))\n-    i+=1\n-    centers, radii, vheight, image, drawing = fv.find_voids_2(tile)\n-    #plt.imshow(tile)\n-\n-plt.imshow(tiles_grey[14])\n+print(\"Done\")\n\\ No newline at end of file","add":16,"remove":16,"filename":"\/source\/Divide_et_Vince.py","badparts":["tiles = [flooded_grains[x:x+M,y:y+N] for x in range(0,flooded_grains.shape[0],M) for y in range(0,flooded_grains.shape[1],N)]","for tile in tiles:","    cv2.imshow(\"Tile\",tile)","grey_img = cv2.imread(path+ '.jpg', 0)","grey_img = cv2.resize(grey_img,(width,height),interpolation = cv2.INTER_AREA)","tiles_grey = [grey_img[x:x+M,y:y+N] for x in range(0,grey_img.shape[0],M) for y in range(0,grey_img.shape[1],N)]","i=0","for tile in tiles_grey:","    print(\"tile: \"+str(i))","    i+=1","    centers, radii, vheight, image, drawing = fv.find_voids_2(tile)","plt.imshow(tiles_grey[14])"],"goodparts":["print(\"Flood method done\")","print(\"Running Slice method\")","grey_img = cv2.imread(path+ '.jpg', 0)","grey_img = cv2.resize(grey_img,(width,height),interpolation = cv2.INTER_AREA)","tiles = [flooded_grains[x:x+M,y:y+N] for x in range(0,flooded_grains.shape[0],M) for y in range(0,flooded_grains.shape[1],N)]","tiles_grey = [grey_img[x:x+M,y:y+N] for x in range(0,grey_img.shape[0],M) for y in range(0,grey_img.shape[1],N)]","n_voids = []","for idx in range(len(tiles_grey)):","    centers, radii, vheight, image, drawing = fv.find_voids_2(tiles_grey[idx])","    n_voids.append([idx,len(centers)])","    io.imsave(\"ml_sets\/\"+ file + '_'+ str(idx) + '_' + str(len(centers)) + 'void.png',tiles[idx])","print(\"Done\")"]}],"source":"\n import pandas as pd import numpy as np from skimage import draw,io from skimage.segmentation import flood, flood_fill import cv2 import math from matplotlib import pyplot as plt from IPython.display import set_matplotlib_formats plt.rcParams['figure.dpi']=400 plt.rcParams['savefig.dpi']=400 import find_voids as fv origin=[0, 0] refvec=[0, 1] def clockwiseangle_and_distance(point): vector=[point[0]-origin[0], point[1]-origin[1]] lenvector=math.hypot(vector[0], vector[1]) if lenvector==0: return -math.pi, 0 normalized=[vector[0]\/lenvector, vector[1]\/lenvector] dotprod =normalized[0]*refvec[0] +normalized[1]*refvec[1] diffprod=refvec[1]*normalized[0] -refvec[0]*normalized[1] angle=math.atan2(diffprod, dotprod) if angle < 0: return 2*math.pi+angle, lenvector return angle, lenvector folder=\"data\/\" file=\"1_001\" path=folder +file sample=np.loadtxt(path+\".txt\") ''' ''' df=pd.DataFrame( data=sample, columns=[\"right_phi1\",\"right_PHI\",\"right_phi2\", \"left_phi1\",\"left_PHI\",\"left_phi2\", \"ori_angle\", \"right_ori_x\",\"right_ori_y\",\"right_ori_z\", \"left_ori_x\",\"left_ori_y\",\"left_ori_z\", \"length\", \"trace_angle\", \"x_start\", \"y_start\", \"x_end\", \"y_end\", \"grain_right\",\"grain_left\" ] ) width=int(max([max(df.x_end),max(df.x_start)]))+1 height=int(max([max(df.y_end),max(df.y_start)]))+1 flooded_grains=np.zeros([height, width, 3]) overflood=np.sum(flooded_grains==0) * 0.8 over=[] out=[] df_left=df[['left_phi1','left_PHI','left_phi2','grain_left']] df_left=df_left.rename(columns={\"grain_left\": \"grain\"}) df_left=df_left[~df_left.grain.duplicated()].sort_values('grain') df_left=df_left.set_index('grain') df_right=df[['right_phi1','right_PHI','right_phi2','grain_right']] df_right=df_right.rename(columns={\"grain_right\": \"grain\"}) df_right=df_right[~df_right.grain.duplicated()].sort_values('grain') df_right=df_right.set_index('grain') df_grains=df_left.join(df_right) df_grains_norm=(df_grains -df_grains.min()) \/(df_grains.max() -df_grains.min()) for grain in df_grains.index: One_grain=df[(df[\"grain_right\"]==grain) |(df[\"grain_left\"]==grain)] grain_info=df_grains_norm.loc[grain,:] np_img=np.zeros([height, width, 3]) x_center=math.ceil(One_grain[['x_start','x_end']].mean().mean()) y_center=math.ceil(One_grain[['y_start','y_end']].mean().mean()) phi1,Phi,phi2=grain_info[[\"right_phi1\",\"right_PHI\",\"right_phi2\"]] for idx, row in One_grain.iterrows(): rr,cc,a=draw.line_aa(row.x_start.astype(\"uint16\"),row.y_start.astype(\"uint16\"),row.x_end.astype(\"uint16\"),row.y_end.astype(\"uint16\")) np_img[cc,rr]=(1,1,1) mask=flood(np_img,(y_center, x_center,0)) np_img[np_img[:,:,1] !=0]=[phi1,Phi,phi2] if(np.sum(mask==1)<overflood): flooded_grains[mask[:,:,1] !=0]=[phi1,Phi,phi2] flooded_grains[np_img[:,:,1] !=0]=[phi1,Phi,phi2] else: over.append(grain) One_grain=One_grain[One_grain[\"length\"]>2] start=pd.DataFrame(columns=[\"x\",\"y\"]) end=pd.DataFrame(columns=[\"x\",\"y\"]) start[[\"x\",\"y\"]]=One_grain[['x_start','y_start']] end[[\"x\",\"y\"]]=One_grain[['x_end','y_end']] points=pd.concat([start,end]) p=points.drop_duplicates() p1=p.to_numpy() origin=[x_center,y_center] sort=sorted(p1, key=clockwiseangle_and_distance) a=[] for b in sort: a.append(tuple((int(b[0]),int(b[1])))) cv2.polylines(np_img, np.array([a]), True,(phi1,Phi,phi2), 2) mask=flood(np_img,(y_center, x_center,0)) if(np.sum(mask==1)<overflood): flooded_grains[mask[:,:,1] !=0]=[phi1,Phi,phi2] flooded_grains[np_img[:,:,1] !=0]=[phi1,Phi,phi2] else: out.append(grain) width=int(max([max(df.x_end),max(df.x_start)]))+1 height=int(max([max(df.y_end),max(df.y_start)]))+1 N=width\/\/4 M=height\/\/4 tiles=[flooded_grains[x:x+M,y:y+N] for x in range(0,flooded_grains.shape[0],M) for y in range(0,flooded_grains.shape[1],N)] for tile in tiles: cv2.imshow(\"Tile\",tile) grey_img=cv2.imread(path+'.jpg', 0) grey_img=cv2.resize(grey_img,(width,height),interpolation=cv2.INTER_AREA) tiles_grey=[grey_img[x:x+M,y:y+N] for x in range(0,grey_img.shape[0],M) for y in range(0,grey_img.shape[1],N)] i=0 for tile in tiles_grey: print(\"tile: \"+str(i)) i+=1 centers, radii, vheight, image, drawing=fv.find_voids_2(tile) plt.imshow(tiles_grey[14]) ","sourceWithComments":"\n\n\nimport pandas as pd\nimport numpy as np\nfrom skimage import draw,io\nfrom skimage.segmentation import flood, flood_fill\nimport cv2\nimport math\nfrom matplotlib import pyplot as plt\nfrom IPython.display import set_matplotlib_formats\nplt.rcParams['figure.dpi'] = 400\nplt.rcParams['savefig.dpi'] = 400\n\nimport find_voids as fv\n\n\n#import os\n#path = os.getcwd()\n\n#TODO: remove this global variables from here. Actually we can't do that because origin changes for each grain and I don't know how to pass parameters when use sorted() function\n#import clockwise_segments as clockwise\norigin = [0, 0]\nrefvec = [0, 1]\n\ndef clockwiseangle_and_distance(point):\n    # Vector between point and the origin: v = p - o\n    vector = [point[0]-origin[0], point[1]-origin[1]]\n    \n    # Length of vector: ||v||\n    lenvector = math.hypot(vector[0], vector[1])\n   \n    # If length is zero there is no angle\n    if lenvector == 0:\n        return -math.pi, 0\n    # Normalize vector: v\/||v||\n    normalized = [vector[0]\/lenvector, vector[1]\/lenvector]\n    dotprod  = normalized[0]*refvec[0] + normalized[1]*refvec[1]     # x1*x2 + y1*y2\n    diffprod = refvec[1]*normalized[0] - refvec[0]*normalized[1]     # x1*y2 - y1*x2\n    angle = math.atan2(diffprod, dotprod)\n    # Negative angles represent counter-clockwise angles so we need to subtract them \n    # from 2*pi (360 degrees)\n    if angle < 0:\n        return 2*math.pi+angle, lenvector\n    # I return first the angle because that's the primary sorting criterium\n    # but if two vectors have the same angle then the shorter distance should come first.\n    return angle, lenvector\n\n\n# In[4]:\n\n\n\nfolder = \"data\/\"\nfile = \"1_001\"\npath = folder + file\n\n#%%\n\nsample = np.loadtxt(path+ \".txt\")\n\n\n\n\n'''\n# Column 1-3:   right hand average orientation (phi1, PHI, phi2 in radians)\n# Column 4-6:   left hand average orientation (phi1, PHI, phi2 in radians)\n# Column 7:     Misorientation Angle\n# Column 8-10:  Misorientation Axis in Right Hand grain\n# Column 11-13: Misorientation Axis in Left Hand grain\n# Column 14:    length (in microns)\n# Column 15:    trace angle (in degrees)\n# Column 16-19: x,y coordinates of endpoints (in microns)\n# Column 20-21: IDs of right hand and left hand grains\n\n'''\n\n\ndf = pd.DataFrame(  data = sample, \n                    columns = [\"right_phi1\",\"right_PHI\",\"right_phi2\",                 #1-3\n                               \"left_phi1\",\"left_PHI\",\"left_phi2\",                    #4-6 \n                               \"ori_angle\",                                           #7\n                               \"right_ori_x\",\"right_ori_y\",\"right_ori_z\",              #8-10\n                               \"left_ori_x\",\"left_ori_y\",\"left_ori_z\",                 #11-13  \n                               \"length\",                                              #14\n                               \"trace_angle\",                                         #15\n                               \"x_start\", \"y_start\", \"x_end\", \"y_end\",                #16-19\n                               \"grain_right\",\"grain_left\"                             #20-21\n                               ]                    \n                 )\n\n\nwidth = int(max([max(df.x_end),max(df.x_start)]))+1\nheight = int(max([max(df.y_end),max(df.y_start)]))+1\n\nflooded_grains = np.zeros([height, width, 3])\noverflood = np.sum(flooded_grains==0) * 0.8\nover = []\nout = []\n\n# In[5]:\n\n\n\ndf_left = df[['left_phi1','left_PHI','left_phi2','grain_left']]\ndf_left = df_left.rename(columns={\"grain_left\": \"grain\"})\n\ndf_left = df_left[~df_left.grain.duplicated()].sort_values('grain')\ndf_left = df_left.set_index('grain')\n\n\n\ndf_right = df[['right_phi1','right_PHI','right_phi2','grain_right']]\ndf_right = df_right.rename(columns={\"grain_right\": \"grain\"})\n\ndf_right = df_right[~df_right.grain.duplicated()].sort_values('grain')\ndf_right = df_right.set_index('grain')\n\n#TODO: Check the completeness of this join\n\ndf_grains = df_left.join(df_right)\n\ndf_grains_norm = (df_grains - df_grains.min()) \/ (df_grains.max() - df_grains.min())\n\n\n\nfor grain in df_grains.index:\n        One_grain = df[(df[\"grain_right\"] == grain) | (df[\"grain_left\"] == grain)]\n        grain_info = df_grains_norm.loc[grain,:]\n        np_img = np.zeros([height, width, 3])\n\n        x_center = math.ceil(One_grain[['x_start','x_end']].mean().mean())\n        y_center = math.ceil(One_grain[['y_start','y_end']].mean().mean())\n\n        phi1,Phi,phi2 = grain_info[[\"right_phi1\",\"right_PHI\",\"right_phi2\"]]\n\n        for idx, row in One_grain.iterrows():\n\n            rr,cc,a = draw.line_aa(row.x_start.astype(\"uint16\"),row.y_start.astype(\"uint16\"),row.x_end.astype(\"uint16\"),row.y_end.astype(\"uint16\"))\n            np_img[cc,rr] = (1,1,1)\n\n        mask = flood(np_img, (y_center, x_center,0))\n        #        print(str(grain) + \" len \"+ str(np.count_nonzero(mask)))\n        #        print(str(grain) + \" len 0 \"+ str(np.sum(mask==1)))\n        np_img[np_img[:,:,1] !=0] =  [phi1,Phi,phi2]\n\n        if (np.sum(mask==1)<overflood):\n            flooded_grains[mask[:,:,1] !=0] = [phi1,Phi,phi2]\n            flooded_grains[np_img[:,:,1] !=0] =  [phi1,Phi,phi2]\n\n        else:\n            over.append(grain)\n            One_grain = One_grain[One_grain[\"length\"]>2]\n\n            start = pd.DataFrame(columns=[\"x\",\"y\"])\n            end = pd.DataFrame(columns=[\"x\",\"y\"])\n            start[[\"x\",\"y\"]] = One_grain[['x_start','y_start']]\n            end[[\"x\",\"y\"]] = One_grain[['x_end','y_end']]\n            points = pd.concat([start,end])\n\n            p = points.drop_duplicates()\n            p1 = p.to_numpy()\n\n            origin = [x_center,y_center]\n            \n            sort = sorted(p1, key=clockwiseangle_and_distance)\n            a = []\n            for b in sort:\n                a.append(tuple((int(b[0]),int(b[1]))))\n\n            cv2.polylines(np_img, np.array([a]), True, (phi1,Phi,phi2), 2)\n\n\n            mask = flood(np_img, (y_center, x_center,0))\n            if(np.sum(mask==1)<overflood):\n#                 cv2.imshow('f',flood_grains)\n#                 cv2.waitKey(0)\n#                 cv2.destroyAllWindows()\n                \n                flooded_grains[mask[:,:,1] !=0] = [phi1,Phi,phi2]\n                flooded_grains[np_img[:,:,1] !=0] =  [phi1,Phi,phi2]\n            else:\n                out.append(grain)\n            #cv2.putText(flooded_grains, text=str(int(grain)), org=(x_center,y_center),fontFace=2, fontScale=0.2, color=(255,255,255), thickness=1)\n\n\n\n## PART 2 - SLICE METHOD\n\nwidth = int(max([max(df.x_end),max(df.x_start)]))+1\nheight = int(max([max(df.y_end),max(df.y_start)]))+1\n\nN = width\/\/4\nM = height\/\/4\n\ntiles = [flooded_grains[x:x+M,y:y+N] for x in range(0,flooded_grains.shape[0],M) for y in range(0,flooded_grains.shape[1],N)]\n\nfor tile in tiles:\n#    centers, radii, vheight, image, drawing = fv.find_voids_2(tile)\n    cv2.imshow(\"Tile\",tile)\n\n\n\n\n\ngrey_img = cv2.imread(path+ '.jpg', 0)\ngrey_img = cv2.resize(grey_img,(width,height),interpolation = cv2.INTER_AREA)\n\ntiles_grey = [grey_img[x:x+M,y:y+N] for x in range(0,grey_img.shape[0],M) for y in range(0,grey_img.shape[1],N)]\ni=0\nfor tile in tiles_grey:\n    print(\"tile: \"+str(i))\n    i+=1\n    centers, radii, vheight, image, drawing = fv.find_voids_2(tile)\n    #plt.imshow(tile)\n\nplt.imshow(tiles_grey[14])\n"},"\/source\/GB_reconstruction.py":{"changes":[{"diff":"\n \n #%%\n \n-folder = \"..\/data\/\"\n-file = \"1_007\"\n+folder = \"data\/\"\n+file = \"1_001\"\n path = folder + file\n \n #%%\n","add":2,"remove":2,"filename":"\/source\/GB_reconstruction.py","badparts":["folder = \"..\/data\/\"","file = \"1_007\""],"goodparts":["folder = \"data\/\"","file = \"1_001\""]},{"diff":"\n \n void_clean = np.zeros([height+1,width+1, 3])\n \n-for idx, row in bd_clean.iterrows():\n-    rr,cc = draw.line(row.x_start,row.y_start,row.x_end,row.y_end)\n+for idx, row in bd_new.iterrows():\n+    rr,cc,a = draw.line_aa(row.x_start.astype(\"uint16\"),row.y_start.astype(\"uint16\"),row.x_end.astype(\"uint16\"),row.y_end.astype(\"uint16\"))\n \n     void_clean[cc,rr] = (0,255,0)\n \n+\n         \n plt.figure(10)\n plt.imshow(void_clean)\n #plt.imsave(\"Boundaries.png\",void_clean.astype(\"uint8\"))\n plt.show()\n-\n+bd_new.to_pickle(\"output\/\"+ file + \"_remake.pkl\")  \n     \n     \n-#%%\n+# #%%\n \n \n-# fig, axs = plt.subplots(2,2)\n-# fig.suptitle('GB Layers')\n-# axs[0,0].set_title(\"Original\")\n-# axs[0,0].imshow(np_img)\n-# axs[0,1].imshow(void[:,:,0])\n-# axs[1,0].imshow(void[:,:,2])\n-# axs[1,1].imshow(void[:,:,1])\n-#%%\n+# # fig, axs = plt.subplots(2,2)\n+# # fig.suptitle('GB Layers')\n+# # axs[0,0].set_title(\"Original\")\n+# # axs[0,0].imshow(np_img)\n+# # axs[0,1].imshow(void[:,:,0])\n+# # axs[1,0].imshow(void[:,:,2])\n+# # axs[1,1].imshow(void[:,:,1])\n+# #%%\n \n-# new bds = (0,255,255) # Light blue\n-# removed bds = (255,0,0) # Red\n-# bds_keeped = ()\n+# # new bds = (0,255,255) # Light blue\n+# # removed bds = (255,0,0) # Red\n+# # bds_keeped = ()\n \n \n \n-# plt.figure(5)\n-# plt.imshow(void)\n-# plt.title(\"Final\")\n+# # plt.figure(5)\n+# # plt.imshow(void)\n+# # plt.title(\"Final\")\n \n-# plt.figure(4)\n-# plt.imshow(void[:,:,2])\n-# plt.title(\"New Lines\")\n+# # plt.figure(4)\n+# # plt.imshow(void[:,:,2])\n+# # plt.title(\"New Lines\")\n \n-# plt.figure(3)\n-# plt.imshow(void[:,:,0])\n-# plt.title(\"Dropped\")\n+# # plt.figure(3)\n+# # plt.imshow(void[:,:,0])\n+# # plt.title(\"Dropped\")\n \n-# plt.figure(2)\n-# plt.imshow(voids_detected)\n-# plt.title(\"Voids Detected\")\n+# # plt.figure(2)\n+# # plt.imshow(voids_detected)\n+# # plt.title(\"Voids Detected\")\n \n \n-# plt.figure(1)\n-# plt.imshow(np_img)\n-# plt.title(\"Original\")\n+# # plt.figure(1)\n+# # plt.imshow(np_img)\n+# # plt.title(\"Original\")\n \n \n \n-# plt.show()\n+# # plt.show()\n \n-#%%\n-#      FOR ALL VOID, CREATE A SLICE OF ORIGINAL DF CONSIDERING ONLY BOUNDARIES \n-#      AROUND THE VOID. SAVE IT AS A GOOD DF\n+# #%%\n+# #      FOR ALL VOID, CREATE A SLICE OF ORIGINAL DF CONSIDERING ONLY BOUNDARIES \n+# #      AROUND THE VOID. SAVE IT AS A GOOD DF\n \n-for idx,useful in useful_void:\n+# for idx,useful in useful_void:\n     \n-    if (useful is True):\n-        radi_0 = radii[idx]*5\n-        center_0 = centers[idx]\n-        radi_0 = round(radi_0)\n+#     if (useful is True):\n+#         radi_0 = radii[idx]*5\n+#         center_0 = centers[idx]\n+#         radi_0 = round(radi_0)\n     \n     \n-        # x_start, y_start = center_0[0] - radi_0 , center_0[1] - radi_0 \n-        # x_end, y_end = center_0[0] + radi_0 , center_0[1] + radi_0 \n-        x_start, y_start = center_0[0] - 50, center_0[1] - 50 \n-        x_end, y_end = center_0[0] + 50 , center_0[1] + 50\n+#         # x_start, y_start = center_0[0] - radi_0 , center_0[1] - radi_0 \n+#         # x_end, y_end = center_0[0] + radi_0 , center_0[1] + radi_0 \n+#         x_start, y_start = center_0[0] - 50, center_0[1] - 50 \n+#         x_end, y_end = center_0[0] + 50 , center_0[1] + 50\n         \n         \n-        bd_start_in_area = bd_new[\n-                                (bd_new[\"x_start\"]>x_start) & (bd_new[\"x_start\"]<x_end) \n-                                &\n-                                (bd_new[\"y_start\"]>y_start) & (bd_new[\"y_start\"]<y_end)]\n+#         bd_start_in_area = bd_new[\n+#                                 (bd_new[\"x_start\"]>x_start) & (bd_new[\"x_start\"]<x_end) \n+#                                 &\n+#                                 (bd_new[\"y_start\"]>y_start) & (bd_new[\"y_start\"]<y_end)]\n         \n         \n         \n-        bd_end_in_area = bd_new[\n-                                (bd_new[\"x_end\"]>x_start) & (bd_new[\"x_end\"]<x_end) \n-                                &\n-                                (bd_new[\"y_end\"]>y_start) & (bd_new[\"y_end\"]<y_end)]\n+#         bd_end_in_area = bd_new[\n+#                                 (bd_new[\"x_end\"]>x_start) & (bd_new[\"x_end\"]<x_end) \n+#                                 &\n+#                                 (bd_new[\"y_end\"]>y_start) & (bd_new[\"y_end\"]<y_end)]\n         \n         \n        \n-        bd_inside = pd.concat([bd_start_in_area, bd_end_in_area])\n-        bd_inside = bd_inside[~bd_inside.index.duplicated()]\n+#         bd_inside = pd.concat([bd_start_in_area, bd_end_in_area])\n+#         bd_inside = bd_inside[~bd_inside.index.duplicated()]\n \n          \n-        bd_inside.loc[:,\"x_start\"] = bd_inside.x_start - x_start\n-        bd_inside.loc[:,\"x_end\"] = bd_inside.x_end - x_start\n+#         bd_inside.loc[:,\"x_start\"] = bd_inside.x_start - x_start\n+#         bd_inside.loc[:,\"x_end\"] = bd_inside.x_end - x_start\n         \n-        bd_inside.loc[:,\"y_start\"] = bd_inside.y_start - y_start\n-        bd_inside.loc[:,\"y_end\"] = bd_inside.y_end - y_start\n+#         bd_inside.loc[:,\"y_start\"] = bd_inside.y_start - y_start\n+#         bd_inside.loc[:,\"y_end\"] = bd_inside.y_end - y_start\n         \n-        x_min = bd_inside[[\"x_start\",\"x_end\"]].values.min()\n-        y_min = bd_inside[[\"y_start\",\"y_end\"]].values.min()\n+#         x_min = bd_inside[[\"x_start\",\"x_end\"]].values.min()\n+#         y_min = bd_inside[[\"y_start\",\"y_end\"]].values.min()\n         \n-        bd_inside.loc[:,\"x_start\"] = bd_inside.x_start - x_min\n-        bd_inside.loc[:,\"x_end\"] = bd_inside.x_end - x_min\n+#         bd_inside.loc[:,\"x_start\"] = bd_inside.x_start - x_min\n+#         bd_inside.loc[:,\"x_end\"] = bd_inside.x_end - x_min\n         \n-        bd_inside.loc[:,\"y_start\"] = bd_inside.y_start - y_min\n-        bd_inside.loc[:,\"y_end\"] = bd_inside.y_end -y_min\n+#         bd_inside.loc[:,\"y_start\"] = bd_inside.y_start - y_min\n+#         bd_inside.loc[:,\"y_end\"] = bd_inside.y_end -y_min\n         \n         \n-        x_size = bd_inside[[\"x_start\",\"x_end\"]].values.max()\n-        y_size = bd_inside[[\"y_start\",\"y_end\"]].values.max()\n+#         x_size = bd_inside[[\"x_start\",\"x_end\"]].values.max()\n+#         y_size = bd_inside[[\"y_start\",\"y_end\"]].values.max()\n         \n         \n         \n","add":66,"remove":65,"filename":"\/source\/GB_reconstruction.py","badparts":["for idx, row in bd_clean.iterrows():","    rr,cc = draw.line(row.x_start,row.y_start,row.x_end,row.y_end)","for idx,useful in useful_void:","    if (useful is True):","        radi_0 = radii[idx]*5","        center_0 = centers[idx]","        radi_0 = round(radi_0)","        x_start, y_start = center_0[0] - 50, center_0[1] - 50 ","        x_end, y_end = center_0[0] + 50 , center_0[1] + 50","        bd_start_in_area = bd_new[","                                (bd_new[\"x_start\"]>x_start) & (bd_new[\"x_start\"]<x_end) ","                                &","                                (bd_new[\"y_start\"]>y_start) & (bd_new[\"y_start\"]<y_end)]","        bd_end_in_area = bd_new[","                                (bd_new[\"x_end\"]>x_start) & (bd_new[\"x_end\"]<x_end) ","                                &","                                (bd_new[\"y_end\"]>y_start) & (bd_new[\"y_end\"]<y_end)]","        bd_inside = pd.concat([bd_start_in_area, bd_end_in_area])","        bd_inside = bd_inside[~bd_inside.index.duplicated()]","        bd_inside.loc[:,\"x_start\"] = bd_inside.x_start - x_start","        bd_inside.loc[:,\"x_end\"] = bd_inside.x_end - x_start","        bd_inside.loc[:,\"y_start\"] = bd_inside.y_start - y_start","        bd_inside.loc[:,\"y_end\"] = bd_inside.y_end - y_start","        x_min = bd_inside[[\"x_start\",\"x_end\"]].values.min()","        y_min = bd_inside[[\"y_start\",\"y_end\"]].values.min()","        bd_inside.loc[:,\"x_start\"] = bd_inside.x_start - x_min","        bd_inside.loc[:,\"x_end\"] = bd_inside.x_end - x_min","        bd_inside.loc[:,\"y_start\"] = bd_inside.y_start - y_min","        bd_inside.loc[:,\"y_end\"] = bd_inside.y_end -y_min","        x_size = bd_inside[[\"x_start\",\"x_end\"]].values.max()","        y_size = bd_inside[[\"y_start\",\"y_end\"]].values.max()"],"goodparts":["for idx, row in bd_new.iterrows():","    rr,cc,a = draw.line_aa(row.x_start.astype(\"uint16\"),row.y_start.astype(\"uint16\"),row.x_end.astype(\"uint16\"),row.y_end.astype(\"uint16\"))","bd_new.to_pickle(\"output\/\"+ file + \"_remake.pkl\")  "]},{"diff":"\n         \n         \n         \n-        void_new = np.zeros([y_size+1,x_size+1, 3])\n-        # void_new = np.zeros([height+1,width+1, 3])\n+#         void_new = np.zeros([y_size+1,x_size+1, 3])\n+#         # void_new = np.zeros([height+1,width+1, 3])\n \n-        for aux, row in bd_inside.iterrows():\n-            rr,cc = draw.line(row.x_start.astype(\"uint8\"),row.y_start.astype(\"uint8\"),row.x_end.astype(\"uint8\"),row.y_end.astype(\"uint8\"))\n-            void_new[cc,rr] = (0,255,0)\n+#         for aux, row in bd_inside.iterrows():\n+#             rr,cc = draw.line(row.x_start.astype(\"uint8\"),row.y_start.astype(\"uint8\"),row.x_end.astype(\"uint8\"),row.y_end.astype(\"uint8\"))\n+#             void_new[cc,rr] = (0,255,0)\n             \n-            # void_new[cc-y_start,rr-x_start] = (0,255,0)\n+#             # void_new[cc-y_start,rr-x_start] = (0,255,0)\n         \n         \n         \n         \n-        # Pickle File    \n+#         # Pickle File    \n         \n         \n-        if( not os.path.exists(\"..\/output\")):\n-            print(\"Creating folder\")\n-            os.mkdir(\"..\/output\")\n+#         if( not os.path.exists(\"..\/output\")):\n+#             print(\"Creating folder\")\n+#             os.mkdir(\"..\/output\")\n             \n             \n             \n-        plt.imsave(\"..\/output\/\"+ file+ \"_void_\"+ str(idx) + \"_new.jpg\", void_new.astype(\"uint8\"))\n-        bd_inside.to_pickle(\"..\/output\/\"+ file+ \"_void_\"+ str(idx) + \"_new.pkl\")  \n+#         plt.imsave(\"..\/output\/\"+ file+ \"_void_\"+ str(idx) + \"_new.jpg\", void_new.astype(\"uint8\"))\n+#         bd_inside.to_pickle(\"..\/output\/\"+ file+ \"_void_\"+ str(idx) + \"_new.pkl\")  \n         \n         \n         \n         \n-#%%\n+# #%%\n \n-for idx,useful in useful_void:\n+# for idx,useful in useful_void:\n     \n-    if (useful is True):\n-        radi_0 = radii[idx]*5\n-        center_0 = centers[idx]\n-        radi_0 = round(radi_0)\n+#     if (useful is True):\n+#         radi_0 = radii[idx]*5\n+#         center_0 = centers[idx]\n+#         radi_0 = round(radi_0)\n     \n     \n-        # x_start, y_start = center_0[0] - radi_0 , center_0[1] - radi_0 \n-        # x_end, y_end = center_0[0] + radi_0 , center_0[1] + radi_0 \n-        x_start, y_start = center_0[0] - 50, center_0[1] - 50 \n-        x_end, y_end = center_0[0] + 50 , center_0[1] + 50\n+#         # x_start, y_start = center_0[0] - radi_0 , center_0[1] - radi_0 \n+#         # x_end, y_end = center_0[0] + radi_0 , center_0[1] + radi_0 \n+#         x_start, y_start = center_0[0] - 50, center_0[1] - 50 \n+#         x_end, y_end = center_0[0] + 50 , center_0[1] + 50\n         \n         \n-        bd_start_in_area = bd_clean[\n-                                (bd_clean[\"x_start\"]>x_start) & (bd_clean[\"x_start\"]<x_end) \n-                                &\n-                                (bd_clean[\"y_start\"]>y_start) & (bd_clean[\"y_start\"]<y_end)]\n+#         bd_start_in_area = bd_clean[\n+#                                 (bd_clean[\"x_start\"]>x_start) & (bd_clean[\"x_start\"]<x_end) \n+#                                 &\n+#                                 (bd_clean[\"y_start\"]>y_start) & (bd_clean[\"y_start\"]<y_end)]\n         \n         \n         \n-        bd_end_in_area = bd_clean[\n-                                (bd_clean[\"x_end\"]>x_start) & (bd_clean[\"x_end\"]<x_end) \n-                                &\n-                                (bd_clean[\"y_end\"]>y_start) & (bd_clean[\"y_end\"]<y_end)]\n+#         bd_end_in_area = bd_clean[\n+#                                 (bd_clean[\"x_end\"]>x_start) & (bd_clean[\"x_end\"]<x_end) \n+#                                 &\n+#                                 (bd_clean[\"y_end\"]>y_start) & (bd_clean[\"y_end\"]<y_end)]\n         \n         \n        \n-        bd_inside = pd.concat([bd_start_in_area, bd_end_in_area])\n-        bd_inside = bd_inside[~bd_inside.index.duplicated()]\n+#         bd_inside = pd.concat([bd_start_in_area, bd_end_in_area])\n+#         bd_inside = bd_inside[~bd_inside.index.duplicated()]\n \n          \n-        bd_inside.loc[:,\"x_start\"] = bd_inside.x_start - x_start\n-        bd_inside.loc[:,\"x_end\"] = bd_inside.x_end - x_start\n+#         bd_inside.loc[:,\"x_start\"] = bd_inside.x_start - x_start\n+#         bd_inside.loc[:,\"x_end\"] = bd_inside.x_end - x_start\n         \n-        bd_inside.loc[:,\"y_start\"] = bd_inside.y_start - y_start\n-        bd_inside.loc[:,\"y_end\"] = bd_inside.y_end - y_start\n+#         bd_inside.loc[:,\"y_start\"] = bd_inside.y_start - y_start\n+#         bd_inside.loc[:,\"y_end\"] = bd_inside.y_end - y_start\n         \n-        x_min = bd_inside[[\"x_start\",\"x_end\"]].values.min()\n-        y_min = bd_inside[[\"y_start\",\"y_end\"]].values.min()\n+#         x_min = bd_inside[[\"x_start\",\"x_end\"]].values.min()\n+#         y_min = bd_inside[[\"y_start\",\"y_end\"]].values.min()\n         \n-        bd_inside.loc[:,\"x_start\"] = bd_inside.x_start - x_min\n-        bd_inside.loc[:,\"x_end\"] = bd_inside.x_end - x_min\n+#         bd_inside.loc[:,\"x_start\"] = bd_inside.x_start - x_min\n+#         bd_inside.loc[:,\"x_end\"] = bd_inside.x_end - x_min\n         \n-        bd_inside.loc[:,\"y_start\"] = bd_inside.y_start - y_min\n-        bd_inside.loc[:,\"y_end\"] = bd_inside.y_end -y_min\n+#         bd_inside.loc[:,\"y_start\"] = bd_inside.y_start - y_min\n+#         bd_inside.loc[:,\"y_end\"] = bd_inside.y_end -y_min\n         \n         \n-        x_size = bd_inside[[\"x_start\",\"x_end\"]].values.max()\n-        y_size = bd_inside[[\"y_start\",\"y_end\"]].values.max()\n+#         x_size = bd_inside[[\"x_start\",\"x_end\"]].values.max()\n+#         y_size = bd_inside[[\"y_start\",\"y_end\"]].values.max()\n         \n         \n         \n         \n         \n         \n-        # Pickle File    \n+#         # Pickle File    \n         \n         \n-        if( not os.path.exists(\"..\/output\")):\n-            print(\"Creating folder\")\n-            os.mkdir(\"..\/output\")\n+#         if( not os.path.exists(\"..\/output\")):\n+#             print(\"Creating folder\")\n+#             os.mkdir(\"..\/output\")\n             \n             \n             \n-        #plt.imsave(\"..\/output\/\"+ file+ \"_void_\"+ str(idx) + \"_base.jpg\", void_new.astype(\"uint8\"))\n-        bd_inside.to_pickle(\"..\/output\/\"+ file+ \"_void_\"+ str(idx) + \"_base.pkl\")  \n+#         #plt.imsave(\"..\/output\/\"+ file+ \"_void_\"+ str(idx) + \"_base.jpg\", void_new.astype(\"uint8\"))\n+#         bd_inside.to_pickle(\"..\/output\/\"+ file+ \"_void_\"+ str(idx) + \"_base.pkl\")  \n         \n        \n         \n         \n \n-# TODO: MAYBE WE HAVE INTEREST IN CONSIDER THE BEHAVIOR OF BIG HOLES, TO DO\n-#       WE'LL START WITH ALL FALSES ELEMENTS IN useful_voids\n\\ No newline at end of file\n+# # TODO: MAYBE WE HAVE INTEREST IN CONSIDER THE BEHAVIOR OF BIG HOLES, TO DO\n+# #       WE'LL START WITH ALL FALSES ELEMENTS IN useful_voids\n\\ No newline at end of file\n","add":52,"remove":52,"filename":"\/source\/GB_reconstruction.py","badparts":["        void_new = np.zeros([y_size+1,x_size+1, 3])","        for aux, row in bd_inside.iterrows():","            rr,cc = draw.line(row.x_start.astype(\"uint8\"),row.y_start.astype(\"uint8\"),row.x_end.astype(\"uint8\"),row.y_end.astype(\"uint8\"))","            void_new[cc,rr] = (0,255,0)","        if( not os.path.exists(\"..\/output\")):","            print(\"Creating folder\")","            os.mkdir(\"..\/output\")","        plt.imsave(\"..\/output\/\"+ file+ \"_void_\"+ str(idx) + \"_new.jpg\", void_new.astype(\"uint8\"))","        bd_inside.to_pickle(\"..\/output\/\"+ file+ \"_void_\"+ str(idx) + \"_new.pkl\")  ","for idx,useful in useful_void:","    if (useful is True):","        radi_0 = radii[idx]*5","        center_0 = centers[idx]","        radi_0 = round(radi_0)","        x_start, y_start = center_0[0] - 50, center_0[1] - 50 ","        x_end, y_end = center_0[0] + 50 , center_0[1] + 50","        bd_start_in_area = bd_clean[","                                (bd_clean[\"x_start\"]>x_start) & (bd_clean[\"x_start\"]<x_end) ","                                &","                                (bd_clean[\"y_start\"]>y_start) & (bd_clean[\"y_start\"]<y_end)]","        bd_end_in_area = bd_clean[","                                (bd_clean[\"x_end\"]>x_start) & (bd_clean[\"x_end\"]<x_end) ","                                &","                                (bd_clean[\"y_end\"]>y_start) & (bd_clean[\"y_end\"]<y_end)]","        bd_inside = pd.concat([bd_start_in_area, bd_end_in_area])","        bd_inside = bd_inside[~bd_inside.index.duplicated()]","        bd_inside.loc[:,\"x_start\"] = bd_inside.x_start - x_start","        bd_inside.loc[:,\"x_end\"] = bd_inside.x_end - x_start","        bd_inside.loc[:,\"y_start\"] = bd_inside.y_start - y_start","        bd_inside.loc[:,\"y_end\"] = bd_inside.y_end - y_start","        x_min = bd_inside[[\"x_start\",\"x_end\"]].values.min()","        y_min = bd_inside[[\"y_start\",\"y_end\"]].values.min()","        bd_inside.loc[:,\"x_start\"] = bd_inside.x_start - x_min","        bd_inside.loc[:,\"x_end\"] = bd_inside.x_end - x_min","        bd_inside.loc[:,\"y_start\"] = bd_inside.y_start - y_min","        bd_inside.loc[:,\"y_end\"] = bd_inside.y_end -y_min","        x_size = bd_inside[[\"x_start\",\"x_end\"]].values.max()","        y_size = bd_inside[[\"y_start\",\"y_end\"]].values.max()","        if( not os.path.exists(\"..\/output\")):","            print(\"Creating folder\")","            os.mkdir(\"..\/output\")","        bd_inside.to_pickle(\"..\/output\/\"+ file+ \"_void_\"+ str(idx) + \"_base.pkl\")  "],"goodparts":[]}],"source":"\n import numpy as np import pandas as pd from skimage import draw from matplotlib import pyplot as plt from matplotlib import image import cv2 import os def find_voids_2(original): \t \t \t \t \t \t \t \t original=cv2.bilateralFilter(original,9,75,75) \t \t \t \t \t retval, image=cv2.threshold(original, 60, 255, cv2.THRESH_BINARY) \t \t \t \t \t \t \t contours, hierarchy=cv2.findContours(image, \t cv2.RETR_LIST, \t cv2.CHAIN_APPROX_SIMPLE \t) \t \t drawing=original \t vheight=len(drawing) \t centers=[] radii=[] \t for contour in contours: area=cv2.contourArea(contour) \t\t \t\t if area >(original.size*0.8): continue \t\t \t\t br=cv2.boundingRect(contour) \t\t \t\t \t\t \t\t \t\t \t\t radius=(br[2]\/2)*1.4 radii.append(radius) \t\t \t\t m=cv2.moments(contour) \t\t \t\t if m['m00']==0.0: m['m00']=1.0 \t\t\t\t\t center=(int(m['m10'] \/ m['m00']), int(m['m01'] \/ m['m00'])) centers.append(center) \t\t cv2.circle(drawing, center, int(radius),(0, 0, 255), 3) \t print(\"The program has detected{} voids\".format(len(centers))) \t i=0 for center in centers: cv2.circle(drawing, center, 3,(255, 0, 0), -1) cv2.circle(drawing, center, int(radii[i]),(0, 255, 0), 1) i=i +1 \t \t return centers, radii, vheight, image, drawing folder=\"..\/data\/\" file=\"1_007\" path=folder +file sample=np.loadtxt(path+\".txt\") ''' ''' df=pd.DataFrame( data=sample, columns=[\"right_phi1\",\"right_PHI\",\"right_phi2\", \"left_phi1\",\"left_PHI\",\"left_phi2\", \"ori_angle\", \"right_ori_x\",\"right_ori_y\",\"right_ori_z\", \"left_ori_x\",\"leff_ori_y\",\"left_ori_z\", \"length\", \"trace_angle\", \"x_start\", \"y_start\", \"x_end\", \"y_end\", \"grain_right\",\"grain_left\" ] ) width=int(df.x_end.max())+1 height=int(df.y_end.max())+1 ''' Drawing the image using numpy and scikit The 3rd dimension is an index of the boundarie, so after I can refer to the df. ''' gb_img=cv2.imread(path+'.jpg', 0) gb_img=cv2.resize(gb_img,(width,height),interpolation=cv2.INTER_AREA) centers, radii, vheight, image, drawing=find_voids_2(gb_img) plt.imshow(drawing) bd_info=df.copy() bd_info=bd_info.astype('int32') np_img=np.zeros([height+1, width+1, 3]) for idx, row in bd_info.iterrows(): rr,cc=draw.line(row.x_start,row.y_start,row.x_end,row.y_end) np_img[cc,rr,1]=255 void=np_img.copy() voids_detected=np_img.copy() to_drop=[] bd_new=bd_info.copy() useful_void=[] for idx,num in enumerate(range(len(centers))): radi_0=radii[num]*1.05 center_0=centers[num] radi_0=round(radi_0) x_start, y_start=center_0[0] -radi_0, center_0[1] -radi_0 x_end, y_end=center_0[0] +radi_0, center_0[1] +radi_0 bd_start_in_void=bd_info[[\"x_start\",\"y_start\"]][ (bd_info[\"x_start\"]>x_start) &(bd_info[\"x_start\"]<x_end) & (bd_info[\"y_start\"]>y_start) &(bd_info[\"y_start\"]<y_end)] bd_end_in_void=bd_info[[\"x_end\",\"y_end\"]][ (bd_info[\"x_end\"]>x_start) &(bd_info[\"x_end\"]<x_end) & (bd_info[\"y_end\"]>y_start) &(bd_info[\"y_end\"]<y_end)] void_view=void[y_start-50: y_end+50, x_start-50: x_end+50] bd_inside=pd.concat([bd_start_in_void, bd_end_in_void]) bd_to_drop=bd_inside[bd_inside.index.duplicated()] to_drop +=bd_to_drop.index.tolist() bd_to_keep=bd_inside[~bd_inside.index.duplicated(keep=False)] if(len(bd_to_keep) <5) &(len(bd_to_keep) >0): useful_void +=[[idx,True]] cv2.rectangle(voids_detected,(x_start-25,y_start-25),(x_end+25,y_end+25),(255,255,255), 2) cv2.circle(voids_detected, center_0, int(radi_0),(255, 255, 255), 1) end_points=bd_to_keep[[\"x_end\",\"y_end\"]].dropna().values.astype(\"int32\").tolist() for s in(start_points): for e in(end_points): cv2.line(void, s, e,(255, 255, 255), 2) new_line=[{'x_start':s[0], 'y_start':s[1], 'x_end':e[0], 'y_end':e[1]}] df_new_line=pd.DataFrame.from_records(new_line) bd_new=pd.concat([bd_new,df_new_line], ignore_index=True) if( not os.path.exists(\"..\/output\")): print(\"Creating folder\") os.mkdir(\"..\/output\") plt.imsave(\"..\/output\/\"+file+\"_void_\"+str(idx) +\"_base.jpg\", void_view.astype(\"uint8\")) ''' Creating a new DF without any boundary that only exists near a void and is not conected to other boundaries ''' bd_clean=bd_info.drop(index=to_drop) bd_new=bd_new.drop(index=to_drop) void_clean=np.zeros([height+1,width+1, 3]) for idx, row in bd_clean.iterrows(): rr,cc=draw.line(row.x_start,row.y_start,row.x_end,row.y_end) void_clean[cc,rr]=(0,255,0) plt.figure(10) plt.imshow(void_clean) plt.show() for idx,useful in useful_void: if(useful is True): radi_0=radii[idx]*5 center_0=centers[idx] radi_0=round(radi_0) x_start, y_start=center_0[0] -50, center_0[1] -50 x_end, y_end=center_0[0] +50, center_0[1] +50 bd_start_in_area=bd_new[ (bd_new[\"x_start\"]>x_start) &(bd_new[\"x_start\"]<x_end) & (bd_new[\"y_start\"]>y_start) &(bd_new[\"y_start\"]<y_end)] bd_end_in_area=bd_new[ (bd_new[\"x_end\"]>x_start) &(bd_new[\"x_end\"]<x_end) & (bd_new[\"y_end\"]>y_start) &(bd_new[\"y_end\"]<y_end)] bd_inside=pd.concat([bd_start_in_area, bd_end_in_area]) bd_inside=bd_inside[~bd_inside.index.duplicated()] bd_inside.loc[:,\"x_start\"]=bd_inside.x_start -x_start bd_inside.loc[:,\"x_end\"]=bd_inside.x_end -x_start bd_inside.loc[:,\"y_start\"]=bd_inside.y_start -y_start bd_inside.loc[:,\"y_end\"]=bd_inside.y_end -y_start x_min=bd_inside[[\"x_start\",\"x_end\"]].values.min() y_min=bd_inside[[\"y_start\",\"y_end\"]].values.min() bd_inside.loc[:,\"x_start\"]=bd_inside.x_start -x_min bd_inside.loc[:,\"x_end\"]=bd_inside.x_end -x_min bd_inside.loc[:,\"y_start\"]=bd_inside.y_start -y_min bd_inside.loc[:,\"y_end\"]=bd_inside.y_end -y_min x_size=bd_inside[[\"x_start\",\"x_end\"]].values.max() y_size=bd_inside[[\"y_start\",\"y_end\"]].values.max() void_new=np.zeros([y_size+1,x_size+1, 3]) for aux, row in bd_inside.iterrows(): rr,cc=draw.line(row.x_start.astype(\"uint8\"),row.y_start.astype(\"uint8\"),row.x_end.astype(\"uint8\"),row.y_end.astype(\"uint8\")) void_new[cc,rr]=(0,255,0) if( not os.path.exists(\"..\/output\")): print(\"Creating folder\") os.mkdir(\"..\/output\") plt.imsave(\"..\/output\/\"+file+\"_void_\"+str(idx) +\"_new.jpg\", void_new.astype(\"uint8\")) bd_inside.to_pickle(\"..\/output\/\"+file+\"_void_\"+str(idx) +\"_new.pkl\") for idx,useful in useful_void: if(useful is True): radi_0=radii[idx]*5 center_0=centers[idx] radi_0=round(radi_0) x_start, y_start=center_0[0] -50, center_0[1] -50 x_end, y_end=center_0[0] +50, center_0[1] +50 bd_start_in_area=bd_clean[ (bd_clean[\"x_start\"]>x_start) &(bd_clean[\"x_start\"]<x_end) & (bd_clean[\"y_start\"]>y_start) &(bd_clean[\"y_start\"]<y_end)] bd_end_in_area=bd_clean[ (bd_clean[\"x_end\"]>x_start) &(bd_clean[\"x_end\"]<x_end) & (bd_clean[\"y_end\"]>y_start) &(bd_clean[\"y_end\"]<y_end)] bd_inside=pd.concat([bd_start_in_area, bd_end_in_area]) bd_inside=bd_inside[~bd_inside.index.duplicated()] bd_inside.loc[:,\"x_start\"]=bd_inside.x_start -x_start bd_inside.loc[:,\"x_end\"]=bd_inside.x_end -x_start bd_inside.loc[:,\"y_start\"]=bd_inside.y_start -y_start bd_inside.loc[:,\"y_end\"]=bd_inside.y_end -y_start x_min=bd_inside[[\"x_start\",\"x_end\"]].values.min() y_min=bd_inside[[\"y_start\",\"y_end\"]].values.min() bd_inside.loc[:,\"x_start\"]=bd_inside.x_start -x_min bd_inside.loc[:,\"x_end\"]=bd_inside.x_end -x_min bd_inside.loc[:,\"y_start\"]=bd_inside.y_start -y_min bd_inside.loc[:,\"y_end\"]=bd_inside.y_end -y_min x_size=bd_inside[[\"x_start\",\"x_end\"]].values.max() y_size=bd_inside[[\"y_start\",\"y_end\"]].values.max() if( not os.path.exists(\"..\/output\")): print(\"Creating folder\") os.mkdir(\"..\/output\") bd_inside.to_pickle(\"..\/output\/\"+file+\"_void_\"+str(idx) +\"_base.pkl\") ","sourceWithComments":"# -*- coding: utf-8 -*-\n#%%\n\nimport numpy as np\nimport pandas as pd\nfrom skimage import draw\nfrom matplotlib import pyplot as plt\nfrom matplotlib import image\nimport cv2\nimport os\n\n#from matplotlib import cm\n\n\n#%%\n\n\ndef find_voids_2(original):\n\t### Read image\n\n\t\n\t### Save temp file for comparison\n\t#cv2.imwrite(picname+\"_original.png\", original)\n\t\n\t### Run several filter over image to achieve a more distinct void structure.\n\t### Voids will be more of a round shape this way, which reduces error.\n\t### Depending on grain structure, some filters may have no effect on particular samples.\n    original = cv2.bilateralFilter(original,9,75,75)\n    \n\t#original = cv2.medianBlur(original,5)\n\t#original = cv2.GaussianBlur(original,(5,5),0)\n\t\n\t### TO CHANGE:\n\t### Determine treshold for binary image. First value is gray-value used for cut-off, second is white.\n    retval, image = cv2.threshold(original, 60, 255, cv2.THRESH_BINARY)\n\n\t\n\t### Find eliptical shapes with a minimum size and dilate picutre\n    #el = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (1, 1))\n    #image = cv2.dilate(image, el, iterations=1)\n\n\t### Save picture temporarily for comparison\n\t#cv2.imwrite(pa_pic+\"_dilated.png\", image)\n\t#cv2.imshow('dilated', image)\n\t\n\t### Read contours from dilated binary picture\n    contours, hierarchy = cv2.findContours(image,\n\t    cv2.RETR_LIST,\n\t    cv2.CHAIN_APPROX_SIMPLE\n\t) ### Simplify contour \n\n\t\n\t### Load bakcground image for contour plot\n    drawing = original\n\n\t### Get height of the picture to set up an approximate area (height*height)\n    vheight = len(drawing)\n\t\n    centers = []\n    radii = []\n\t\n    for contour in contours:\n        area = cv2.contourArea(contour)\n\t\t### there is one contour that contains all others (perimeter of image), filter it out considering 80% of full image size\n \t\t### do not run the rest of the loop, jump straight to next contour\n        if area > (original.size*0.8):\n                continue\n\t\t\n\t\t### bound contour with a rectangle. \t\n        br = cv2.boundingRect(contour)\n\t\t\n\t\t#el2 = cv2.fitEllipse(contour)\n\n\t\t### TO CHANGE:\n\t\t### Determine radius of circle by taking half of a side (which one??) \n\t\t### of the bounding Rectangle, then multiply by a factor if desired to make \n\t\t### up for mismatch due to color treshold in line 14.\n        radius = (br[2]\/2)*1.4\n        radii.append(radius)\n\t\t\n\t\t### Find moments of countour\n        m = cv2.moments(contour)\n\t\t\n\t\t### Avoid error due to division by zero\n        if m['m00'] == 0.0:\n            m['m00'] = 1.0\n\t\t\t\t\t\n        center = (int(m['m10'] \/ m['m00']), int(m['m01'] \/ m['m00']))\n        centers.append(center)\n\t\t#cv2.circle(drawing, center, 3, (255, 0, 0), -1)\n        cv2.circle(drawing, center, int(radius), (0, 0, 255), 3)\n\t\n    print(\"The program has detected {} voids\".format(len(centers)))\n\t\n    i = 0\n    for center in centers:\n        cv2.circle(drawing, center, 3, (255, 0, 0), -1)\n        cv2.circle(drawing, center, int(radii[i]), (0, 255, 0), 1)\n        i = i + 1\n\n\t### Save image with recognized voids\n   # cv2.imwrite(\"_drawing.png\", drawing)\n   # cv2.imshow('finished', drawing)\n   # cv2.waitKey()\n   # cv2.destroyAllWindows()\n    \t\n    return centers, radii, vheight, image, drawing\n\n\n\n#%%\n\nfolder = \"..\/data\/\"\nfile = \"1_007\"\npath = folder + file\n\n#%%\n\nsample = np.loadtxt(path+ \".txt\")\n\n'''\n# Column 1-3:   right hand average orientation (phi1, PHI, phi2 in radians)\n# Column 4-6:   left hand average orientation (phi1, PHI, phi2 in radians)\n# Column 7:     Misorientation Angle\n# Column 8-10:  Misorientation Axis in Right Hand grain\n# Column 11-13: Misorientation Axis in Left Hand grain\n# Column 14:    length (in microns)\n# Column 15:    trace angle (in degrees)\n# Column 16-19: x,y coordinates of endpoints (in microns)\n# Column 20-21: IDs of right hand and left hand grains\n\n'''\n\n\ndf = pd.DataFrame(  data = sample, \n                    columns = [\"right_phi1\",\"right_PHI\",\"right_phi2\",                 #1-3\n                               \"left_phi1\",\"left_PHI\",\"left_phi2\",                    #4-6 \n                               \"ori_angle\",                                           #7\n                               \"right_ori_x\",\"right_ori_y\",\"right_ori_z\",              #8-10\n                               \"left_ori_x\",\"leff_ori_y\",\"left_ori_z\",                 #11-13  \n                               \"length\",                                              #14\n                               \"trace_angle\",                                         #15\n                               \"x_start\", \"y_start\", \"x_end\", \"y_end\",                #16-19\n                               \"grain_right\",\"grain_left\"                             #20-21\n                               ]                    \n                 )\n\n#df.head()\n#%%\n# Creating image\n\n\n# To increase the image resolution just multiply width, height and \n# coordinates_array and bd_info (before casting) by the necessary increase. \n# \n\n\nwidth = int(df.x_end.max())+1 #multply here\nheight = int(df.y_end.max())+1 #multply here\n\n\n#%%\n\n'''\n\n    Drawing the image using numpy and scikit\n    \n    The 3rd dimension is an index of the boundarie, so after I can refer to the df.\n    \n'''\n\n\n#%%\n\n\ngb_img = cv2.imread(path+ '.jpg', 0)\ngb_img = cv2.resize(gb_img,(width,height),interpolation = cv2.INTER_AREA)\n\ncenters, radii, vheight, image, drawing = find_voids_2(gb_img)\n\nplt.imshow(drawing)\n\n#%%\n# bd_info = df[[\"x_start\",\"y_start\",\"x_end\",\"y_end\"]].copy() # if want to increase resolution, multiply here\nbd_info = df.copy() \nbd_info = bd_info.astype('int32')\n\n\n\n\nnp_img = np.zeros([height+1, width+1, 3])\n\nfor idx, row in bd_info.iterrows():\n    rr,cc = draw.line(row.x_start,row.y_start,row.x_end,row.y_end)\n    np_img[cc,rr,1] = 255\n\n\n\n#plt.imshow(np_img)\n\n\n#\n#\n#   HOW TO CONSIDER THE VOID AS A CIRCLE AND NOT A RECTANGLE?\n#       We need a mask to compare if the start\/end point is inside the void\n# \n#\n#\n#  void_0 = np_img[y_start : y_end , x_start : x_end]\n#  mask = np.zeros([width,height], dtype=\"uint8\")\n#  cv2.circle(mask, center_0, radi_0, 255, -1)\n# \n#  TODO: NEXT STEP IS FIND A WAY TO COMPARE THE CIRCLE COORDINATES WITH THE DATASET.\n#    MAYBE BITWISE WORKS, BUT ONLY IF DO NOT NEED TO CREATE AN IMAGE WITH POINTS\n#\n\n# num = 8\nvoid = np_img.copy()\nvoids_detected = np_img.copy()\nto_drop = []\nbd_new = bd_info.copy()\n\n\nuseful_void = []\n\n# for num in [num]:\nfor idx,num in enumerate(range(len(centers))):\n\n    radi_0 = radii[num]*1.05\n    center_0 = centers[num]\n    radi_0 = round(radi_0)\n\n    #HOW TO LOOK INSIDE A CIRCLE AND NOT INSIDE A SQUARE?\n\n\n    x_start, y_start = center_0[0] - radi_0 , center_0[1] - radi_0 \n    x_end, y_end = center_0[0] + radi_0 , center_0[1] + radi_0 \n    \n  \n    \n    bd_start_in_void = bd_info[[\"x_start\",\"y_start\"]][\n                            (bd_info[\"x_start\"]>x_start) & (bd_info[\"x_start\"]<x_end) \n                            &\n                            (bd_info[\"y_start\"]>y_start) & (bd_info[\"y_start\"]<y_end)]\n    \n    \n    \n    bd_end_in_void = bd_info[[\"x_end\",\"y_end\"]][\n                            (bd_info[\"x_end\"]>x_start) & (bd_info[\"x_end\"]<x_end) \n                            &\n                            (bd_info[\"y_end\"]>y_start) & (bd_info[\"y_end\"]<y_end)]\n    \n    \n    \n    #bd_to_drop = bd_start_in_void.merge(bd_end_in_void, how =\"inner\")\n    #bd_to_keep = bd_start_in_void.merge(bd_end_in_void, how =\"outer\")\n\n# Another method to keep index\n    \n    void_view = void[y_start-50 : y_end+50 , x_start-50 : x_end+50]\n\n    \n    bd_inside = pd.concat([bd_start_in_void, bd_end_in_void])\n    \n#TODO: use pd.concat([bd_start_in_void, bd_end_in_void],axis=1) insted of \n#      operation above could reduce operations and make code cleaner\n      \n\n\n#    print(bd_to_keep)   \n#    bd_to_keep = bd_to_keep.drop_duplicates(keep = False)\n    bd_to_drop = bd_inside[bd_inside.index.duplicated()]\n    \n    to_drop += bd_to_drop.index.tolist()\n    \n    bd_to_keep = bd_inside[~bd_inside.index.duplicated(keep=False)]\n   # print(bd_to_keep)\n    \n    \n    if (len(bd_to_keep) <5) & (len(bd_to_keep) >0):\n        useful_void += [[idx,True]]\n        cv2.rectangle(voids_detected,(x_start-25,y_start-25),(x_end+25,y_end+25), (255,255,255), 2)\n        cv2.circle(voids_detected, center_0, int(radi_0), (255, 255, 255), 1)\n    \n   \n    \n        \n        \n        end_points = bd_to_keep[[\"x_end\",\"y_end\"]].dropna().values.astype(\"int32\").tolist()\n    \n        \n    \n        for s in (start_points):\n            for e in (end_points):\n                cv2.line(void, s, e, (255, 255, 255), 2)\n              \n                new_line = [{'x_start':s[0],\n                                   'y_start':s[1], \n                                   'x_end':e[0], \n                                   'y_end':e[1]}]\n                df_new_line = pd.DataFrame.from_records(new_line)   \n                bd_new = pd.concat([bd_new,df_new_line] , ignore_index=True)\n                \n       \n        \n        if( not os.path.exists(\"..\/output\")):\n            print(\"Creating folder\")\n            os.mkdir(\"..\/output\")\n            \n        plt.imsave(\"..\/output\/\"+ file+ \"_void_\"+ str(idx) + \"_base.jpg\", void_view.astype(\"uint8\"))\n   \n    \n   \n    \n   #else:\n    #    useful_void += [[idx,False]]\n    #    cv2.rectangle(voids_detected,(x_start,y_start),(x_end,y_end), (255,255), 1)\n\n\n\n'''\nCreating a new DF without any boundary that only exists near a void and is not \nconected to other boundaries\n\n'''\n\n\nbd_clean = bd_info.drop(index = to_drop)\n\nbd_new = bd_new.drop(index = to_drop)\n\nvoid_clean = np.zeros([height+1,width+1, 3])\n\nfor idx, row in bd_clean.iterrows():\n    rr,cc = draw.line(row.x_start,row.y_start,row.x_end,row.y_end)\n\n    void_clean[cc,rr] = (0,255,0)\n\n        \nplt.figure(10)\nplt.imshow(void_clean)\n#plt.imsave(\"Boundaries.png\",void_clean.astype(\"uint8\"))\nplt.show()\n\n    \n    \n#%%\n\n\n# fig, axs = plt.subplots(2,2)\n# fig.suptitle('GB Layers')\n# axs[0,0].set_title(\"Original\")\n# axs[0,0].imshow(np_img)\n# axs[0,1].imshow(void[:,:,0])\n# axs[1,0].imshow(void[:,:,2])\n# axs[1,1].imshow(void[:,:,1])\n#%%\n\n# new bds = (0,255,255) # Light blue\n# removed bds = (255,0,0) # Red\n# bds_keeped = ()\n\n\n\n# plt.figure(5)\n# plt.imshow(void)\n# plt.title(\"Final\")\n\n# plt.figure(4)\n# plt.imshow(void[:,:,2])\n# plt.title(\"New Lines\")\n\n# plt.figure(3)\n# plt.imshow(void[:,:,0])\n# plt.title(\"Dropped\")\n\n# plt.figure(2)\n# plt.imshow(voids_detected)\n# plt.title(\"Voids Detected\")\n\n\n# plt.figure(1)\n# plt.imshow(np_img)\n# plt.title(\"Original\")\n\n\n\n# plt.show()\n\n#%%\n#      FOR ALL VOID, CREATE A SLICE OF ORIGINAL DF CONSIDERING ONLY BOUNDARIES \n#      AROUND THE VOID. SAVE IT AS A GOOD DF\n\nfor idx,useful in useful_void:\n    \n    if (useful is True):\n        radi_0 = radii[idx]*5\n        center_0 = centers[idx]\n        radi_0 = round(radi_0)\n    \n    \n        # x_start, y_start = center_0[0] - radi_0 , center_0[1] - radi_0 \n        # x_end, y_end = center_0[0] + radi_0 , center_0[1] + radi_0 \n        x_start, y_start = center_0[0] - 50, center_0[1] - 50 \n        x_end, y_end = center_0[0] + 50 , center_0[1] + 50\n        \n        \n        bd_start_in_area = bd_new[\n                                (bd_new[\"x_start\"]>x_start) & (bd_new[\"x_start\"]<x_end) \n                                &\n                                (bd_new[\"y_start\"]>y_start) & (bd_new[\"y_start\"]<y_end)]\n        \n        \n        \n        bd_end_in_area = bd_new[\n                                (bd_new[\"x_end\"]>x_start) & (bd_new[\"x_end\"]<x_end) \n                                &\n                                (bd_new[\"y_end\"]>y_start) & (bd_new[\"y_end\"]<y_end)]\n        \n        \n       \n        bd_inside = pd.concat([bd_start_in_area, bd_end_in_area])\n        bd_inside = bd_inside[~bd_inside.index.duplicated()]\n\n         \n        bd_inside.loc[:,\"x_start\"] = bd_inside.x_start - x_start\n        bd_inside.loc[:,\"x_end\"] = bd_inside.x_end - x_start\n        \n        bd_inside.loc[:,\"y_start\"] = bd_inside.y_start - y_start\n        bd_inside.loc[:,\"y_end\"] = bd_inside.y_end - y_start\n        \n        x_min = bd_inside[[\"x_start\",\"x_end\"]].values.min()\n        y_min = bd_inside[[\"y_start\",\"y_end\"]].values.min()\n        \n        bd_inside.loc[:,\"x_start\"] = bd_inside.x_start - x_min\n        bd_inside.loc[:,\"x_end\"] = bd_inside.x_end - x_min\n        \n        bd_inside.loc[:,\"y_start\"] = bd_inside.y_start - y_min\n        bd_inside.loc[:,\"y_end\"] = bd_inside.y_end -y_min\n        \n        \n        x_size = bd_inside[[\"x_start\",\"x_end\"]].values.max()\n        y_size = bd_inside[[\"y_start\",\"y_end\"]].values.max()\n        \n        \n        \n        \n        \n        \n        \n        void_new = np.zeros([y_size+1,x_size+1, 3])\n        # void_new = np.zeros([height+1,width+1, 3])\n\n        for aux, row in bd_inside.iterrows():\n            rr,cc = draw.line(row.x_start.astype(\"uint8\"),row.y_start.astype(\"uint8\"),row.x_end.astype(\"uint8\"),row.y_end.astype(\"uint8\"))\n            void_new[cc,rr] = (0,255,0)\n            \n            # void_new[cc-y_start,rr-x_start] = (0,255,0)\n        \n        \n        \n        \n        # Pickle File    \n        \n        \n        if( not os.path.exists(\"..\/output\")):\n            print(\"Creating folder\")\n            os.mkdir(\"..\/output\")\n            \n            \n            \n        plt.imsave(\"..\/output\/\"+ file+ \"_void_\"+ str(idx) + \"_new.jpg\", void_new.astype(\"uint8\"))\n        bd_inside.to_pickle(\"..\/output\/\"+ file+ \"_void_\"+ str(idx) + \"_new.pkl\")  \n        \n        \n        \n        \n#%%\n\nfor idx,useful in useful_void:\n    \n    if (useful is True):\n        radi_0 = radii[idx]*5\n        center_0 = centers[idx]\n        radi_0 = round(radi_0)\n    \n    \n        # x_start, y_start = center_0[0] - radi_0 , center_0[1] - radi_0 \n        # x_end, y_end = center_0[0] + radi_0 , center_0[1] + radi_0 \n        x_start, y_start = center_0[0] - 50, center_0[1] - 50 \n        x_end, y_end = center_0[0] + 50 , center_0[1] + 50\n        \n        \n        bd_start_in_area = bd_clean[\n                                (bd_clean[\"x_start\"]>x_start) & (bd_clean[\"x_start\"]<x_end) \n                                &\n                                (bd_clean[\"y_start\"]>y_start) & (bd_clean[\"y_start\"]<y_end)]\n        \n        \n        \n        bd_end_in_area = bd_clean[\n                                (bd_clean[\"x_end\"]>x_start) & (bd_clean[\"x_end\"]<x_end) \n                                &\n                                (bd_clean[\"y_end\"]>y_start) & (bd_clean[\"y_end\"]<y_end)]\n        \n        \n       \n        bd_inside = pd.concat([bd_start_in_area, bd_end_in_area])\n        bd_inside = bd_inside[~bd_inside.index.duplicated()]\n\n         \n        bd_inside.loc[:,\"x_start\"] = bd_inside.x_start - x_start\n        bd_inside.loc[:,\"x_end\"] = bd_inside.x_end - x_start\n        \n        bd_inside.loc[:,\"y_start\"] = bd_inside.y_start - y_start\n        bd_inside.loc[:,\"y_end\"] = bd_inside.y_end - y_start\n        \n        x_min = bd_inside[[\"x_start\",\"x_end\"]].values.min()\n        y_min = bd_inside[[\"y_start\",\"y_end\"]].values.min()\n        \n        bd_inside.loc[:,\"x_start\"] = bd_inside.x_start - x_min\n        bd_inside.loc[:,\"x_end\"] = bd_inside.x_end - x_min\n        \n        bd_inside.loc[:,\"y_start\"] = bd_inside.y_start - y_min\n        bd_inside.loc[:,\"y_end\"] = bd_inside.y_end -y_min\n        \n        \n        x_size = bd_inside[[\"x_start\",\"x_end\"]].values.max()\n        y_size = bd_inside[[\"y_start\",\"y_end\"]].values.max()\n        \n        \n        \n        \n        \n        \n        # Pickle File    \n        \n        \n        if( not os.path.exists(\"..\/output\")):\n            print(\"Creating folder\")\n            os.mkdir(\"..\/output\")\n            \n            \n            \n        #plt.imsave(\"..\/output\/\"+ file+ \"_void_\"+ str(idx) + \"_base.jpg\", void_new.astype(\"uint8\"))\n        bd_inside.to_pickle(\"..\/output\/\"+ file+ \"_void_\"+ str(idx) + \"_base.pkl\")  \n        \n       \n        \n        \n\n# TODO: MAYBE WE HAVE INTEREST IN CONSIDER THE BEHAVIOR OF BIG HOLES, TO DO\n#       WE'LL START WITH ALL FALSES ELEMENTS IN useful_voids"}},"msg":"ml_sets generated. Minor mistakes to fix in flood method in gb_remake files"}},"https:\/\/github.com\/sonic-net\/sonic-mgmt":{"f54c7e25a1aeaf89c47a218a053cc4bb12fe2909":{"url":"https:\/\/api.github.com\/repos\/sonic-net\/sonic-mgmt\/commits\/f54c7e25a1aeaf89c47a218a053cc4bb12fe2909","html_url":"https:\/\/github.com\/sonic-net\/sonic-mgmt\/commit\/f54c7e25a1aeaf89c47a218a053cc4bb12fe2909","message":"Fix dualtor mac move test for Mellanox platforms (#7647)\n\nSkip the check on active tor after fdb is flushed on Mellanox platforms, because the check expects the traffic should be dropped when there is no fdb entry. But on Mellanox platforms the traffic will be flooded in the vlan, not dropped.\r\n\r\nChange-Id: Ibb6edfe552d0623f297c65301bcbc61465786fb6","sha":"f54c7e25a1aeaf89c47a218a053cc4bb12fe2909","keyword":"flooding fix","diff":"diff --git a\/tests\/dualtor\/test_orchagent_mac_move.py b\/tests\/dualtor\/test_orchagent_mac_move.py\nindex 3dd9025c799..6c4878ca60e 100644\n--- a\/tests\/dualtor\/test_orchagent_mac_move.py\n+++ b\/tests\/dualtor\/test_orchagent_mac_move.py\n@@ -139,10 +139,12 @@ def test_mac_move(\n         testutils.send(ptfadapter, ptf_t1_intf_index, pkt, count=10)\n \n     # active forwarding check after fdb ageout\/flush\n-    tor.shell(\"fdbclear\")\n-    server_traffic_monitor = ServerTrafficMonitor(\n-        tor, ptfhost, vmhost, tbinfo, test_port,\n-        conn_graph_facts, exp_pkt, existing=False, is_mocked=is_mocked_dualtor(tbinfo)  # noqa F405\n-    )\n-    with crm_neighbor_checker(tor), tunnel_monitor, server_traffic_monitor:\n-        testutils.send(ptfadapter, ptf_t1_intf_index, pkt, count=10)\n+    # skip Mellanox platforms for the traffic will be flooded in the vlan when there is no fdb entries\n+    if not tor.facts['asic_type'] == 'mellanox':\n+        tor.shell(\"fdbclear\")\n+        server_traffic_monitor = ServerTrafficMonitor(\n+            tor, ptfhost, vmhost, tbinfo, test_port,\n+            conn_graph_facts, exp_pkt, existing=False, is_mocked=is_mocked_dualtor(tbinfo)  # noqa F405\n+        )\n+        with crm_neighbor_checker(tor), tunnel_monitor, server_traffic_monitor:\n+            testutils.send(ptfadapter, ptf_t1_intf_index, pkt, count=10)\n","files":{"\/tests\/dualtor\/test_orchagent_mac_move.py":{"changes":[{"diff":"\n         testutils.send(ptfadapter, ptf_t1_intf_index, pkt, count=10)\n \n     # active forwarding check after fdb ageout\/flush\n-    tor.shell(\"fdbclear\")\n-    server_traffic_monitor = ServerTrafficMonitor(\n-        tor, ptfhost, vmhost, tbinfo, test_port,\n-        conn_graph_facts, exp_pkt, existing=False, is_mocked=is_mocked_dualtor(tbinfo)  # noqa F405\n-    )\n-    with crm_neighbor_checker(tor), tunnel_monitor, server_traffic_monitor:\n-        testutils.send(ptfadapter, ptf_t1_intf_index, pkt, count=10)\n+    # skip Mellanox platforms for the traffic will be flooded in the vlan when there is no fdb entries\n+    if not tor.facts['asic_type'] == 'mellanox':\n+        tor.shell(\"fdbclear\")\n+        server_traffic_monitor = ServerTrafficMonitor(\n+            tor, ptfhost, vmhost, tbinfo, test_port,\n+            conn_graph_facts, exp_pkt, existing=False, is_mocked=is_mocked_dualtor(tbinfo)  # noqa F405\n+        )\n+        with crm_neighbor_checker(tor), tunnel_monitor, server_traffic_monitor:\n+            testutils.send(ptfadapter, ptf_t1_intf_index, pkt, count=10)\n","add":9,"remove":7,"filename":"\/tests\/dualtor\/test_orchagent_mac_move.py","badparts":["    tor.shell(\"fdbclear\")","    server_traffic_monitor = ServerTrafficMonitor(","        tor, ptfhost, vmhost, tbinfo, test_port,","        conn_graph_facts, exp_pkt, existing=False, is_mocked=is_mocked_dualtor(tbinfo)  # noqa F405","    )","    with crm_neighbor_checker(tor), tunnel_monitor, server_traffic_monitor:","        testutils.send(ptfadapter, ptf_t1_intf_index, pkt, count=10)"],"goodparts":["    if not tor.facts['asic_type'] == 'mellanox':","        tor.shell(\"fdbclear\")","        server_traffic_monitor = ServerTrafficMonitor(","            tor, ptfhost, vmhost, tbinfo, test_port,","            conn_graph_facts, exp_pkt, existing=False, is_mocked=is_mocked_dualtor(tbinfo)  # noqa F405","        )","        with crm_neighbor_checker(tor), tunnel_monitor, server_traffic_monitor:","            testutils.send(ptfadapter, ptf_t1_intf_index, pkt, count=10)"]}],"source":"\nimport logging import pytest import random from ptf import testutils from tests.common.dualtor.dual_tor_mock import * from tests.common.dualtor.dual_tor_utils import get_t1_ptf_ports from tests.common.dualtor.dual_tor_utils import crm_neighbor_checker from tests.common.dualtor.dual_tor_utils import build_packet_to_server from tests.common.dualtor.dual_tor_utils import mux_cable_server_ip from tests.common.dualtor.server_traffic_utils import ServerTrafficMonitor from tests.common.dualtor.tunnel_traffic_utils import tunnel_traffic_monitor from tests.common.fixtures.ptfhost_utils import run_icmp_responder from tests.common.fixtures.ptfhost_utils import run_garp_service from tests.common.fixtures.ptfhost_utils import change_mac_addresses from tests.common.utilities import dump_scapy_packet_show_output pytestmark=[ pytest.mark.topology('t0'), pytest.mark.usefixtures('apply_mock_dual_tor_tables', 'apply_mock_dual_tor_kernel_configs', 'run_garp_service', 'run_icmp_responder') ] NEW_NEIGHBOR_IPV4_ADDR=\"192.168.0.250\" NEW_NEIGHBOR_HWADDR=\"02:AA:BB:CC:DD:EE\" @pytest.fixture(scope=\"function\") def announce_new_neighbor(ptfadapter, rand_selected_dut, tbinfo): \"\"\"Utility fixture to announce new neighbor from a mux port.\"\"\" def _announce_new_neighbor_gen(): \"\"\"Generator to announce the neighbor to a different interface at each iteration.\"\"\" for dut_iface in dut_ifaces: update_iface_func=yield dut_iface if callable(update_iface_func): update_iface_func(dut_iface) ptf_iface=dut_to_ptf_intf_map[dut_iface] garp_packet=testutils.simple_arp_packet( eth_src=NEW_NEIGHBOR_HWADDR, hw_snd=NEW_NEIGHBOR_HWADDR, ip_snd=NEW_NEIGHBOR_IPV4_ADDR, ip_tgt=NEW_NEIGHBOR_IPV4_ADDR, arp_op=2 ) logging.info( \"GARP packet to announce new neighbor %s to mux interface %s:\\n%s\", NEW_NEIGHBOR_IPV4_ADDR, dut_iface, dump_scapy_packet_show_output(garp_packet) ) testutils.send(ptfadapter, int(ptf_iface), garp_packet, count=5) yield dut_to_ptf_intf_map=rand_selected_dut.get_extended_minigraph_facts(tbinfo)['minigraph_ptf_indices'] mux_configs=mux_cable_server_ip(rand_selected_dut) dut_ifaces=mux_configs.keys() random.shuffle(dut_ifaces) return _announce_new_neighbor_gen() @pytest.fixture(autouse=True) def cleanup_arp(duthosts): \"\"\"Cleanup arp entries after test.\"\"\" yield for duthost in duthosts: duthost.shell(\"sonic-clear arp\") @pytest.fixture(autouse=True) def enable_garp(duthost): \"\"\"Enable creating arp table entry for gratuitous ARP.\"\"\" vlan_intf=duthost.get_running_config_facts()[\"VLAN_MEMBER\"].keys()[0] cmd=\"echo %s > \/proc\/sys\/net\/ipv4\/conf\/\" +vlan_intf +\"\/arp_accept\" duthost.shell(cmd % 1) yield duthost.shell(cmd % 0) def test_mac_move( announce_new_neighbor, apply_active_state_to_orchagent, conn_graph_facts, ptfadapter, ptfhost, rand_selected_dut, set_crm_polling_interval, tbinfo, tunnel_traffic_monitor, vmhost ): tor=rand_selected_dut ptf_t1_intf=random.choice(get_t1_ptf_ports(tor, tbinfo)) ptf_t1_intf_index=int(ptf_t1_intf.strip(\"eth\")) test_port=next(announce_new_neighbor) announce_new_neighbor.send(None) logging.info(\"let new neighbor learnt on active port %s\", test_port) pkt, exp_pkt=build_packet_to_server(tor, ptfadapter, NEW_NEIGHBOR_IPV4_ADDR) tunnel_monitor=tunnel_traffic_monitor(tor, existing=False) server_traffic_monitor=ServerTrafficMonitor( tor, ptfhost, vmhost, tbinfo, test_port, conn_graph_facts, exp_pkt, existing=True, is_mocked=is_mocked_dualtor(tbinfo) ) with crm_neighbor_checker(tor), tunnel_monitor, server_traffic_monitor: testutils.send(ptfadapter, ptf_t1_intf_index, pkt, count=10) test_port=next(announce_new_neighbor) announce_new_neighbor.send(lambda iface: set_dual_tor_state_to_orchagent(tor, \"standby\",[iface])) logging.info(\"mac move to a standby port %s\", test_port) pkt, exp_pkt=build_packet_to_server(tor, ptfadapter, NEW_NEIGHBOR_IPV4_ADDR) tunnel_monitor=tunnel_traffic_monitor(tor, existing=True) server_traffic_monitor=ServerTrafficMonitor( tor, ptfhost, vmhost, tbinfo, test_port, conn_graph_facts, exp_pkt, existing=False, is_mocked=is_mocked_dualtor(tbinfo) ) with crm_neighbor_checker(tor), tunnel_monitor, server_traffic_monitor: testutils.send(ptfadapter, ptf_t1_intf_index, pkt, count=10) tor.shell(\"fdbclear\") server_traffic_monitor=ServerTrafficMonitor( tor, ptfhost, vmhost, tbinfo, test_port, conn_graph_facts, exp_pkt, existing=False, is_mocked=is_mocked_dualtor(tbinfo) ) with crm_neighbor_checker(tor), tunnel_monitor, server_traffic_monitor: testutils.send(ptfadapter, ptf_t1_intf_index, pkt, count=10) test_port=next(announce_new_neighbor) announce_new_neighbor.send(None) logging.info(\"mac move to another active port %s\", test_port) pkt, exp_pkt=build_packet_to_server(tor, ptfadapter, NEW_NEIGHBOR_IPV4_ADDR) tunnel_monitor=tunnel_traffic_monitor(tor, existing=False) server_traffic_monitor=ServerTrafficMonitor( tor, ptfhost, vmhost, tbinfo, test_port, conn_graph_facts, exp_pkt, existing=True, is_mocked=is_mocked_dualtor(tbinfo) ) with crm_neighbor_checker(tor), tunnel_monitor, server_traffic_monitor: testutils.send(ptfadapter, ptf_t1_intf_index, pkt, count=10) tor.shell(\"fdbclear\") server_traffic_monitor=ServerTrafficMonitor( tor, ptfhost, vmhost, tbinfo, test_port, conn_graph_facts, exp_pkt, existing=False, is_mocked=is_mocked_dualtor(tbinfo) ) with crm_neighbor_checker(tor), tunnel_monitor, server_traffic_monitor: testutils.send(ptfadapter, ptf_t1_intf_index, pkt, count=10) ","sourceWithComments":"import logging\nimport pytest\nimport random\n\nfrom ptf import testutils\nfrom tests.common.dualtor.dual_tor_mock import *                                # noqa F403\nfrom tests.common.dualtor.dual_tor_utils import get_t1_ptf_ports\nfrom tests.common.dualtor.dual_tor_utils import crm_neighbor_checker\nfrom tests.common.dualtor.dual_tor_utils import build_packet_to_server\nfrom tests.common.dualtor.dual_tor_utils import mux_cable_server_ip\nfrom tests.common.dualtor.server_traffic_utils import ServerTrafficMonitor\nfrom tests.common.dualtor.tunnel_traffic_utils import tunnel_traffic_monitor    # noqa F401\nfrom tests.common.fixtures.ptfhost_utils import run_icmp_responder              # noqa F401\nfrom tests.common.fixtures.ptfhost_utils import run_garp_service                # noqa F401\nfrom tests.common.fixtures.ptfhost_utils import change_mac_addresses            # noqa F401\nfrom tests.common.utilities import dump_scapy_packet_show_output\n\n\npytestmark = [\n    pytest.mark.topology('t0'),\n    pytest.mark.usefixtures('apply_mock_dual_tor_tables',\n                            'apply_mock_dual_tor_kernel_configs',\n                            'run_garp_service',\n                            'run_icmp_responder')\n]\n\n\nNEW_NEIGHBOR_IPV4_ADDR = \"192.168.0.250\"\nNEW_NEIGHBOR_HWADDR = \"02:AA:BB:CC:DD:EE\"\n\n\n@pytest.fixture(scope=\"function\")\ndef announce_new_neighbor(ptfadapter, rand_selected_dut, tbinfo):\n    \"\"\"Utility fixture to announce new neighbor from a mux port.\"\"\"\n\n    def _announce_new_neighbor_gen():\n        \"\"\"Generator to announce the neighbor to a different interface at each iteration.\"\"\"\n        for dut_iface in dut_ifaces:\n            update_iface_func = yield dut_iface\n            if callable(update_iface_func):\n                update_iface_func(dut_iface)\n            ptf_iface = dut_to_ptf_intf_map[dut_iface]\n            garp_packet = testutils.simple_arp_packet(\n                eth_src=NEW_NEIGHBOR_HWADDR,\n                hw_snd=NEW_NEIGHBOR_HWADDR,\n                ip_snd=NEW_NEIGHBOR_IPV4_ADDR,\n                ip_tgt=NEW_NEIGHBOR_IPV4_ADDR,\n                arp_op=2\n            )\n            logging.info(\n                \"GARP packet to announce new neighbor %s to mux interface %s:\\n%s\",\n                NEW_NEIGHBOR_IPV4_ADDR, dut_iface, dump_scapy_packet_show_output(garp_packet)\n            )\n            testutils.send(ptfadapter, int(ptf_iface), garp_packet, count=5)\n            # let the generator stops here to allow the caller to execute testings\n            yield\n\n    dut_to_ptf_intf_map = rand_selected_dut.get_extended_minigraph_facts(tbinfo)['minigraph_ptf_indices']\n    mux_configs = mux_cable_server_ip(rand_selected_dut)\n    dut_ifaces = mux_configs.keys()\n    random.shuffle(dut_ifaces)\n    return _announce_new_neighbor_gen()\n\n\n@pytest.fixture(autouse=True)\ndef cleanup_arp(duthosts):\n    \"\"\"Cleanup arp entries after test.\"\"\"\n    yield\n    for duthost in duthosts:\n        duthost.shell(\"sonic-clear arp\")\n\n\n@pytest.fixture(autouse=True)\ndef enable_garp(duthost):\n    \"\"\"Enable creating arp table entry for gratuitous ARP.\"\"\"\n    vlan_intf = duthost.get_running_config_facts()[\"VLAN_MEMBER\"].keys()[0]\n    cmd = \"echo %s > \/proc\/sys\/net\/ipv4\/conf\/\" + vlan_intf + \"\/arp_accept\"\n    duthost.shell(cmd % 1)\n    yield\n    duthost.shell(cmd % 0)\n\n\ndef test_mac_move(\n    announce_new_neighbor, apply_active_state_to_orchagent,\n    conn_graph_facts, ptfadapter, ptfhost,\n    rand_selected_dut, set_crm_polling_interval,\n    tbinfo, tunnel_traffic_monitor, vmhost          # noqa F811\n):\n    tor = rand_selected_dut\n    ptf_t1_intf = random.choice(get_t1_ptf_ports(tor, tbinfo))\n    ptf_t1_intf_index = int(ptf_t1_intf.strip(\"eth\"))\n\n    # new neighbor learnt on an active port\n    test_port = next(announce_new_neighbor)\n    announce_new_neighbor.send(None)\n    logging.info(\"let new neighbor learnt on active port %s\", test_port)\n    pkt, exp_pkt = build_packet_to_server(tor, ptfadapter, NEW_NEIGHBOR_IPV4_ADDR)\n    tunnel_monitor = tunnel_traffic_monitor(tor, existing=False)\n    server_traffic_monitor = ServerTrafficMonitor(\n        tor, ptfhost, vmhost, tbinfo, test_port,\n        conn_graph_facts, exp_pkt, existing=True, is_mocked=is_mocked_dualtor(tbinfo)   # noqa F405\n    )\n    with crm_neighbor_checker(tor), tunnel_monitor, server_traffic_monitor:\n        testutils.send(ptfadapter, ptf_t1_intf_index, pkt, count=10)\n\n    # mac move to a standby port\n    test_port = next(announce_new_neighbor)\n    announce_new_neighbor.send(lambda iface: set_dual_tor_state_to_orchagent(tor, \"standby\", [iface]))  # noqa F405\n    logging.info(\"mac move to a standby port %s\", test_port)\n    pkt, exp_pkt = build_packet_to_server(tor, ptfadapter, NEW_NEIGHBOR_IPV4_ADDR)\n    tunnel_monitor = tunnel_traffic_monitor(tor, existing=True)\n    server_traffic_monitor = ServerTrafficMonitor(\n        tor, ptfhost, vmhost, tbinfo, test_port,\n        conn_graph_facts, exp_pkt, existing=False, is_mocked=is_mocked_dualtor(tbinfo)  # noqa F405\n    )\n    with crm_neighbor_checker(tor), tunnel_monitor, server_traffic_monitor:\n        testutils.send(ptfadapter, ptf_t1_intf_index, pkt, count=10)\n\n    # standby forwarding check after fdb ageout\/flush\n    tor.shell(\"fdbclear\")\n    server_traffic_monitor = ServerTrafficMonitor(\n        tor, ptfhost, vmhost, tbinfo, test_port,\n        conn_graph_facts, exp_pkt, existing=False, is_mocked=is_mocked_dualtor(tbinfo)  # noqa F405\n    )\n    with crm_neighbor_checker(tor), tunnel_monitor, server_traffic_monitor:\n        testutils.send(ptfadapter, ptf_t1_intf_index, pkt, count=10)\n\n    # mac move to another active port\n    test_port = next(announce_new_neighbor)\n    announce_new_neighbor.send(None)\n    logging.info(\"mac move to another active port %s\", test_port)\n    pkt, exp_pkt = build_packet_to_server(tor, ptfadapter, NEW_NEIGHBOR_IPV4_ADDR)\n    tunnel_monitor = tunnel_traffic_monitor(tor, existing=False)\n    server_traffic_monitor = ServerTrafficMonitor(\n        tor, ptfhost, vmhost, tbinfo, test_port,\n        conn_graph_facts, exp_pkt, existing=True, is_mocked=is_mocked_dualtor(tbinfo)   # noqa F405\n    )\n    with crm_neighbor_checker(tor), tunnel_monitor, server_traffic_monitor:\n        testutils.send(ptfadapter, ptf_t1_intf_index, pkt, count=10)\n\n    # active forwarding check after fdb ageout\/flush\n    tor.shell(\"fdbclear\")\n    server_traffic_monitor = ServerTrafficMonitor(\n        tor, ptfhost, vmhost, tbinfo, test_port,\n        conn_graph_facts, exp_pkt, existing=False, is_mocked=is_mocked_dualtor(tbinfo)  # noqa F405\n    )\n    with crm_neighbor_checker(tor), tunnel_monitor, server_traffic_monitor:\n        testutils.send(ptfadapter, ptf_t1_intf_index, pkt, count=10)\n"}},"msg":"Fix dualtor mac move test for Mellanox platforms (#7647)\n\nSkip the check on active tor after fdb is flushed on Mellanox platforms, because the check expects the traffic should be dropped when there is no fdb entry. But on Mellanox platforms the traffic will be flooded in the vlan, not dropped.\r\n\r\nChange-Id: Ibb6edfe552d0623f297c65301bcbc61465786fb6"}},"https:\/\/github.com\/achoolucgust\/python-image-to-minecraft":{"ddf946d4f0e5eb42bbabb041bd19dbc578d0366b":{"url":"https:\/\/api.github.com\/repos\/achoolucgust\/python-image-to-minecraft\/commits\/ddf946d4f0e5eb42bbabb041bd19dbc578d0366b","html_url":"https:\/\/github.com\/achoolucgust\/python-image-to-minecraft\/commit\/ddf946d4f0e5eb42bbabb041bd19dbc578d0366b","message":"Same thing with the last commit\n\nsome fixes, and no longer floods your console","sha":"ddf946d4f0e5eb42bbabb041bd19dbc578d0366b","keyword":"flooding fix","diff":"diff --git a\/init.py b\/init.py\nindex 46d29b4..2d60652 100644\n--- a\/init.py\n+++ b\/init.py\n@@ -1,6 +1,7 @@\n import zipfile\r\n import pathlib\r\n import os\r\n+import sys\r\n import shutil\r\n from PIL import Image, ImageStat\r\n \r\n@@ -16,21 +17,21 @@\n for archive_item in archive.namelist():\r\n     if archive_item.startswith(PREFIX):\r\n         #try:\r\n-            print(archive_item)\r\n+            sys.stdout.write(\"\\r\" + archive_item)\r\n             destpath = out.joinpath(archive_item[len(PREFIX):])\r\n             os.makedirs(\"blocks\/s\", exist_ok=True)\r\n             with archive.open(archive_item) as source:\r\n                 shutil.copyfileobj(source, open(f\"blocks\/s\/{destpath}\",\"wb\"))\r\n         #except:\r\n         #    print(\"couldn't get \" + archive_item + \" to copy\")\r\n-\r\n+print(\"\\nFinished! Getting average colors...\")\r\n for item_name in os.listdir('blocks\/s\/'):\r\n     item = pathlib.Path('blocks\/s\/' + item_name)\r\n     if item_name.endswith(\".png\") and not (\"debug\" in item_name): \r\n         img = Image.open(item)\r\n         w,h = img.size\r\n         if w == h:\r\n-            print(\"-=\" + item_name)\r\n+            sys.stdout.write(\"\\r\" + archive_item)\r\n             av = ImageStat.Stat(img).median\r\n             if len(av) > 3:\r\n                 notignored = av[3] > 150\r\n","files":{"\/init.py":{"changes":[{"diff":" for archive_item in archive.namelist():\r\n     if archive_item.startswith(PREFIX):\r\n         #try:\r\n-            print(archive_item)\r\n+            sys.stdout.write(\"\\r\" + archive_item)\r\n             destpath = out.joinpath(archive_item[len(PREFIX):])\r\n             os.makedirs(\"blocks\/s\", exist_ok=True)\r\n             with archive.open(archive_item) as source:\r\n                 shutil.copyfileobj(source, open(f\"blocks\/s\/{destpath}\",\"wb\"))\r\n         #except:\r\n         #    print(\"couldn't get \" + archive_item + \" to copy\")\r\n-\r\n+print(\"\\nFinished! Getting average colors...\")\r\n for item_name in os.listdir('blocks\/s\/'):\r\n     item = pathlib.Path('blocks\/s\/' + item_name)\r\n     if item_name.endswith(\".png\") and not (\"debug\" in item_name): \r\n         img = Image.open(item)\r\n         w,h = img.size\r\n         if w == h:\r\n-            print(\"-=\" + item_name)\r\n+            sys.stdout.write(\"\\r\" + archive_item)\r\n             av = ImageStat.Stat(img).median\r\n             if len(av) > 3:\r\n                 notignored = av[3] > 150\r\n","add":3,"remove":3,"filename":"\/init.py","badparts":["            print(archive_item)\r","\r","            print(\"-=\" + item_name)\r"],"goodparts":["            sys.stdout.write(\"\\r\" + archive_item)\r","print(\"\\nFinished! Getting average colors...\")\r","            sys.stdout.write(\"\\r\" + archive_item)\r"]}],"source":"\nimport zipfile\r import pathlib\r import os\r import shutil\r from PIL import Image, ImageStat\r \r colors=[]\r \r input(\"\\n\\n1. Find a version in your.minecraft folder. By default:\\n%%appdata%%\/Roaming\/.minecraft\\nAnd find a '.jar' file.\\n\\n2. if that doesn't exist, your picking a snapshot.\\nTry using a flat version like 1.19.2 instead of 1.19.2-pre\\n\\n3. Copy and Paste that file to this folder imagetoblock\\n\\n4. Rename this JAR file to version.zip\\n\\n5. Press[Enter]\\n\\n\")\r \r print(\"Opening version.zip...\")\r archive=zipfile.ZipFile('version.zip')\r print(\"Copying Block Textures...\")\r PREFIX='assets\/minecraft\/textures\/block'\r out=pathlib.Path('blocks\/')\r for archive_item in archive.namelist():\r if archive_item.startswith(PREFIX):\r print(archive_item)\r destpath=out.joinpath(archive_item[len(PREFIX):])\r os.makedirs(\"blocks\/s\", exist_ok=True)\r with archive.open(archive_item) as source:\r shutil.copyfileobj(source, open(f\"blocks\/s\/{destpath}\",\"wb\"))\r \r for item_name in os.listdir('blocks\/s\/'):\r item=pathlib.Path('blocks\/s\/' +item_name)\r if item_name.endswith(\".png\") and not(\"debug\" in item_name): \r img=Image.open(item)\r w,h=img.size\r if w==h:\r print(\"-=\" +item_name)\r av=ImageStat.Stat(img).median\r if len(av) > 3:\r notignored=av[3] > 150\r else:\r notignored=False\r if notignored:\r new_image=Image.new(\"RGB\", img.size, tuple(av))\r colors.append([av,item_name])\r new_image.save(f\"blocks\/final\/{item_name}\")\r print(colors)\r \r f=open(\"all_of_the_blocks_list_dont_touch.py\", \"w+\")\r f.write(\"all=\" +str(colors))\r f.close()\r input(\"\\n\\n\\n\\n\\n\\n\\nYou may now run main.py.\\n[Enter] to exit\")\r ","sourceWithComments":"import zipfile\r\nimport pathlib\r\nimport os\r\nimport shutil\r\nfrom PIL import Image, ImageStat\r\n\r\ncolors = []\r\n\r\ninput(\"\\n\\n1. Find a version in your .minecraft folder. By default:\\n%%appdata%%\/Roaming\/.minecraft\\nAnd find a '.jar' file.\\n\\n2. if that doesn't exist, your picking a snapshot.\\nTry using a flat version like 1.19.2 instead of 1.19.2-pre\\n\\n3. Copy and Paste that file to this folder imagetoblock\\n\\n4. Rename this JAR file to version.zip\\n\\n5. Press [Enter]\\n\\n\")\r\n\r\nprint(\"Opening version.zip...\")\r\narchive = zipfile.ZipFile('version.zip')\r\nprint(\"Copying Block Textures...\")\r\nPREFIX = 'assets\/minecraft\/textures\/block'\r\nout = pathlib.Path('blocks\/')\r\nfor archive_item in archive.namelist():\r\n    if archive_item.startswith(PREFIX):\r\n        #try:\r\n            print(archive_item)\r\n            destpath = out.joinpath(archive_item[len(PREFIX):])\r\n            os.makedirs(\"blocks\/s\", exist_ok=True)\r\n            with archive.open(archive_item) as source:\r\n                shutil.copyfileobj(source, open(f\"blocks\/s\/{destpath}\",\"wb\"))\r\n        #except:\r\n        #    print(\"couldn't get \" + archive_item + \" to copy\")\r\n\r\nfor item_name in os.listdir('blocks\/s\/'):\r\n    item = pathlib.Path('blocks\/s\/' + item_name)\r\n    if item_name.endswith(\".png\") and not (\"debug\" in item_name): \r\n        img = Image.open(item)\r\n        w,h = img.size\r\n        if w == h:\r\n            print(\"-=\" + item_name)\r\n            av = ImageStat.Stat(img).median\r\n            if len(av) > 3:\r\n                notignored = av[3] > 150\r\n            else:\r\n                notignored = False\r\n            if notignored:\r\n                new_image = Image.new(\"RGB\", img.size, tuple(av))\r\n                colors.append([av,item_name])\r\n                new_image.save(f\"blocks\/final\/{item_name}\")\r\nprint(colors)\r\n\r\nf = open(\"all_of_the_blocks_list_dont_touch.py\", \"w+\")\r\nf.write(\"all = \" + str(colors))\r\nf.close()\r\ninput(\"\\n\\n\\n\\n\\n\\n\\nYou may now run main.py.\\n[Enter] to exit\")\r\n"}},"msg":"Same thing with the last commit\n\nsome fixes, and no longer floods your console"}},"https:\/\/github.com\/userbotindo\/Anjani":{"7ae9331aab6f3d13fa6da6fe383ad493c21950f9":{"url":"https:\/\/api.github.com\/repos\/userbotindo\/Anjani\/commits\/7ae9331aab6f3d13fa6da6fe383ad493c21950f9","html_url":"https:\/\/github.com\/userbotindo\/Anjani\/commit\/7ae9331aab6f3d13fa6da6fe383ad493c21950f9","message":"SpamPrediction: fix usernotparticipant\n\n-> also fix flood.x > flood.value\n\nChange-Id: I365f47e61b973f07150d23fc3c94035887dbc987","sha":"7ae9331aab6f3d13fa6da6fe383ad493c21950f9","keyword":"flooding fix","diff":"diff --git a\/anjani\/plugins\/spam_prediction.py b\/anjani\/plugins\/spam_prediction.py\nindex 2bbc8ef6e..7dae4f3c3 100644\n--- a\/anjani\/plugins\/spam_prediction.py\n+++ b\/anjani\/plugins\/spam_prediction.py\n@@ -31,7 +31,7 @@\n     MessageNotModified,\n     QueryIdInvalid,\n     UserAdminInvalid,\n-    UserNotParticipant\n+    UserNotParticipant,\n )\n from pyrogram.types import (\n     CallbackQuery,\n@@ -134,7 +134,7 @@ def _build_hex(self, id: Optional[int]) -> str:\n \n     @staticmethod\n     def prob_to_string(value: float) -> str:\n-        return str(value * 10 ** 2)[0:7]\n+        return str(value * 10**2)[0:7]\n \n     async def _predict(self, text: str) -> util.types.NDArray[float]:\n         return await util.run_sync(self.model.predict_proba, [text])\n@@ -155,13 +155,7 @@ async def run_ocr(self, message: Message) -> Optional[str]:\n \n         try:\n             stdout, _, exitCode = await util.system.run_command(\n-                \"tesseract\",\n-                str(image),\n-                \"stdout\",\n-                \"-l\",\n-                \"eng+ind\",\n-                \"--psm\",\n-                \"6\"\n+                \"tesseract\", str(image), \"stdout\", \"-l\", \"eng+ind\", \"--psm\", \"6\"\n             )\n         except Exception as e:  # skipcq: PYL-W0703\n             return self.log.error(\"Unexpected error occured when running OCR\", exc_info=e)\n@@ -191,8 +185,7 @@ async def _spam_ban_handler(self, query: CallbackQuery, user: str) -> None:\n             invoker = await chat.get_member(query.from_user.id)\n         except UserNotParticipant:\n             return await query.answer(\n-                await self.get_text(chat.id, \"error-no-rights\"),\n-                show_alert=True\n+                await self.get_text(chat.id, \"error-no-rights\"), show_alert=True\n             )\n \n         if not invoker.privileges or not invoker.privileges.can_restrict_members:\n@@ -205,9 +198,7 @@ async def _spam_ban_handler(self, query: CallbackQuery, user: str) -> None:\n         await chat.ban_member(target.id)\n         await query.answer(\n             await self.get_text(\n-                chat.id,\n-                \"spampredict-ban\",\n-                user=target.username or target.first_name\n+                chat.id, \"spampredict-ban\", user=target.username or target.first_name\n             )\n         )\n \n@@ -311,7 +302,7 @@ async def _spam_vote_handler(self, query: CallbackQuery, value: str) -> None:\n                             \"Please wait i'm updating the content for you.\",\n                             show_alert=True,\n                         )\n-                        await asyncio.sleep(flood.x)  # type: ignore\n+                        await asyncio.sleep(flood.value)  # type: ignore\n                         continue\n \n                     await asyncio.sleep(0.1)\n@@ -424,7 +415,7 @@ async def spam_check(self, message: Message, text: str, *, from_ocr: bool = Fals\n                     reply_markup=InlineKeyboardMarkup(keyb),\n                 )\n             except FloodWait as flood:\n-                await asyncio.sleep(flood.x)  # type: ignore\n+                await asyncio.sleep(flood.value)  # type: ignore\n                 continue\n \n             await asyncio.sleep(0.1)\n@@ -451,9 +442,13 @@ async def spam_check(self, message: Message, text: str, *, from_ocr: bool = Fals\n             if user is None or message.sender_chat:\n                 return\n \n-            target = await message.chat.get_member(user)\n-            if util.tg.is_staff_or_admin(target):\n-                return\n+            try:\n+                target = await message.chat.get_member(user)\n+            except UserNotParticipant:\n+                target = None\n+            else:\n+                if util.tg.is_staff_or_admin(target):\n+                    return\n \n             if from_ocr:\n                 alert = (\n@@ -484,7 +479,7 @@ async def spam_check(self, message: Message, text: str, *, from_ocr: bool = Fals\n             chat = message.chat\n             me = await chat.get_member(self.bot.uid)\n             button = [[InlineKeyboardButton(\"View Message\", url=msg.link)]]\n-            if me.privileges and me.privileges.can_restrict_members:\n+            if me.privileges and me.privileges.can_restrict_members and target is not None:\n                 button.append(\n                     [\n                         InlineKeyboardButton(\n","files":{"\/anjani\/plugins\/spam_prediction.py":{"changes":[{"diff":"\n     MessageNotModified,\n     QueryIdInvalid,\n     UserAdminInvalid,\n-    UserNotParticipant\n+    UserNotParticipant,\n )\n from pyrogram.types import (\n     CallbackQuery,\n","add":1,"remove":1,"filename":"\/anjani\/plugins\/spam_prediction.py","badparts":["    UserNotParticipant"],"goodparts":["    UserNotParticipant,"]},{"diff":"\n \n     @staticmethod\n     def prob_to_string(value: float) -> str:\n-        return str(value * 10 ** 2)[0:7]\n+        return str(value * 10**2)[0:7]\n \n     async def _predict(self, text: str) -> util.types.NDArray[float]:\n         return await util.run_sync(self.model.predict_proba, [text])\n","add":1,"remove":1,"filename":"\/anjani\/plugins\/spam_prediction.py","badparts":["        return str(value * 10 ** 2)[0:7]"],"goodparts":["        return str(value * 10**2)[0:7]"]},{"diff":"\n \n         try:\n             stdout, _, exitCode = await util.system.run_command(\n-                \"tesseract\",\n-                str(image),\n-                \"stdout\",\n-                \"-l\",\n-                \"eng+ind\",\n-                \"--psm\",\n-                \"6\"\n+                \"tesseract\", str(image), \"stdout\", \"-l\", \"eng+ind\", \"--psm\", \"6\"\n             )\n         except Exception as e:  # skipcq: PYL-W0703\n             return self.log.error(\"Unexpected error occured when running OCR\", exc_info=e)\n","add":1,"remove":7,"filename":"\/anjani\/plugins\/spam_prediction.py","badparts":["                \"tesseract\",","                str(image),","                \"stdout\",","                \"-l\",","                \"eng+ind\",","                \"--psm\",","                \"6\""],"goodparts":["                \"tesseract\", str(image), \"stdout\", \"-l\", \"eng+ind\", \"--psm\", \"6\""]},{"diff":"\n             invoker = await chat.get_member(query.from_user.id)\n         except UserNotParticipant:\n             return await query.answer(\n-                await self.get_text(chat.id, \"error-no-rights\"),\n-                show_alert=True\n+                await self.get_text(chat.id, \"error-no-rights\"), show_alert=True\n             )\n \n         if not invoker.privileges or not invoker.privileges.can_restrict_members:\n","add":1,"remove":2,"filename":"\/anjani\/plugins\/spam_prediction.py","badparts":["                await self.get_text(chat.id, \"error-no-rights\"),","                show_alert=True"],"goodparts":["                await self.get_text(chat.id, \"error-no-rights\"), show_alert=True"]},{"diff":"\n         await chat.ban_member(target.id)\n         await query.answer(\n             await self.get_text(\n-                chat.id,\n-                \"spampredict-ban\",\n-                user=target.username or target.first_name\n+                chat.id, \"spampredict-ban\", user=target.username or target.first_name\n             )\n         )\n \n","add":1,"remove":3,"filename":"\/anjani\/plugins\/spam_prediction.py","badparts":["                chat.id,","                \"spampredict-ban\",","                user=target.username or target.first_name"],"goodparts":["                chat.id, \"spampredict-ban\", user=target.username or target.first_name"]},{"diff":"\n                             \"Please wait i'm updating the content for you.\",\n                             show_alert=True,\n                         )\n-                        await asyncio.sleep(flood.x)  # type: ignore\n+                        await asyncio.sleep(flood.value)  # type: ignore\n                         continue\n \n                     await asyncio.sleep(0.1)\n","add":1,"remove":1,"filename":"\/anjani\/plugins\/spam_prediction.py","badparts":["                        await asyncio.sleep(flood.x)  # type: ignore"],"goodparts":["                        await asyncio.sleep(flood.value)  # type: ignore"]},{"diff":"\n                     reply_markup=InlineKeyboardMarkup(keyb),\n                 )\n             except FloodWait as flood:\n-                await asyncio.sleep(flood.x)  # type: ignore\n+                await asyncio.sleep(flood.value)  # type: ignore\n                 continue\n \n             await asyncio.sleep(0.1)\n","add":1,"remove":1,"filename":"\/anjani\/plugins\/spam_prediction.py","badparts":["                await asyncio.sleep(flood.x)  # type: ignore"],"goodparts":["                await asyncio.sleep(flood.value)  # type: ignore"]},{"diff":"\n             if user is None or message.sender_chat:\n                 return\n \n-            target = await message.chat.get_member(user)\n-            if util.tg.is_staff_or_admin(target):\n-                return\n+            try:\n+                target = await message.chat.get_member(user)\n+            except UserNotParticipant:\n+                target = None\n+            else:\n+                if util.tg.is_staff_or_admin(target):\n+                    return\n \n             if from_ocr:\n                 alert = (\n","add":7,"remove":3,"filename":"\/anjani\/plugins\/spam_prediction.py","badparts":["            target = await message.chat.get_member(user)","            if util.tg.is_staff_or_admin(target):","                return"],"goodparts":["            try:","                target = await message.chat.get_member(user)","            except UserNotParticipant:","                target = None","            else:","                if util.tg.is_staff_or_admin(target):","                    return"]},{"diff":"\n             chat = message.chat\n             me = await chat.get_member(self.bot.uid)\n             button = [[InlineKeyboardButton(\"View Message\", url=msg.link)]]\n-            if me.privileges and me.privileges.can_restrict_members:\n+            if me.privileges and me.privileges.can_restrict_members and target is not None:\n                 button.append(\n                     [\n                         InlineKeyboardButton(\n","add":1,"remove":1,"filename":"\/anjani\/plugins\/spam_prediction.py","badparts":["            if me.privileges and me.privileges.can_restrict_members:"],"goodparts":["            if me.privileges and me.privileges.can_restrict_members and target is not None:"]}],"source":"\n\"\"\"Spam Prediction plugin\"\"\" import asyncio import pickle import re from functools import partial from hashlib import md5, sha256 from typing import Any, Callable, ClassVar, MutableMapping, Optional from aiopath import AsyncPath from pymongo.errors import DuplicateKeyError from pyrogram.errors import( ChatAdminRequired, FloodWait, MessageDeleteForbidden, MessageIdInvalid, MessageNotModified, QueryIdInvalid, UserAdminInvalid, UserNotParticipant ) from pyrogram.types import( CallbackQuery, InlineKeyboardButton, InlineKeyboardMarkup, Message, ) try: from sklearn.pipeline import Pipeline _run_predict=True except ImportError: from anjani.util.types import Pipeline _run_predict=False from anjani import command, filters, listener, plugin, util class SpamPrediction(plugin.Plugin): name: ClassVar[str]=\"SpamPredict\" helpable: ClassVar[bool]=True disabled: ClassVar[bool]=not _run_predict db: util.db.AsyncCollection user_db: util.db.AsyncCollection setting_db: util.db.AsyncCollection model: Pipeline async def on_load(self) -> None: token=self.bot.config.get(\"sp_token\") url=self.bot.config.get(\"sp_url\") if not(token and url): return self.bot.unload_plugin(self) self.db=self.bot.db.get_collection(\"SPAM_DUMP\") self.user_db=self.bot.db.get_collection(\"USERS\") self.setting_db=self.bot.db.get_collection(\"SPAM_PREDICT_SETTING\") async with asyncio.Lock(): await self.__load_model(token, url) async def on_chat_migrate(self, message: Message) -> None: await self.db.update_one( {\"chat_id\": message.migrate_from_chat_id}, {\"$set\":{\"chat_id\": message.chat.id}}, ) async def on_plugin_backup(self, chat_id: int) -> MutableMapping[str, Any]: setting=await self.setting_db.find_one({\"chat_id\": chat_id}) return{self.name: setting} if setting else{} async def on_plugin_restore(self, chat_id: int, data: MutableMapping[str, Any]) -> None: await self.setting_db.update_one( {\"chat_id\": chat_id},{\"$set\": data[self.name]}, upsert=True ) async def __load_model(self, token: str, url: str) -> None: self.log.info(\"Downloading spam prediction model!\") async with self.bot.http.get( url, headers={ \"Authorization\": f\"token{token}\", \"Accept\": \"application\/vnd.github.v3.raw\", }, ) as res: if res.status==200: self.model=await util.run_sync(pickle.loads, await res.read()) else: self.log.warning(\"Failed to download prediction model!\") self.bot.unload_plugin(self) def _check_spam_results_ocr( self, message: Message, future: asyncio.Future[Optional[str]] ) -> None: def done(fut: asyncio.Future[None]) -> None: try: fut.result() except Exception as e: self.log.error(\"Unexpected error occured when checking OCR results\", exc_info=e) text=future.result() if not text: return f=self.bot.loop.create_task(self.spam_check(message, text, from_ocr=True)) f.add_done_callback(done) @staticmethod def _build_hash(content: str) -> str: return sha256(content.strip().encode()).hexdigest() def _build_hex(self, id: Optional[int]) -> str: if not id: id=self.bot.uid return md5((str(id) +self.bot.user.username).encode()).hexdigest() @staticmethod def prob_to_string(value: float) -> str: return str(value * 10 ** 2)[0:7] async def _predict(self, text: str) -> util.types.NDArray[float]: return await util.run_sync(self.model.predict_proba,[text]) async def _is_spam(self, text: str) -> bool: return(await util.run_sync(self.model.predict,[text]))[0]==\"spam\" async def run_ocr(self, message: Message) -> Optional[str]: \"\"\"Run tesseract\"\"\" try: image=AsyncPath(await message.download()) except Exception: return self.log.warning( \"Failed to download image from MessageID %s in Chat %s\", message.id, message.chat.id, ) try: stdout, _, exitCode=await util.system.run_command( \"tesseract\", str(image), \"stdout\", \"-l\", \"eng+ind\", \"--psm\", \"6\" ) except Exception as e: return self.log.error(\"Unexpected error occured when running OCR\", exc_info=e) finally: await image.unlink() if exitCode !=0: return self.log.warning(\"tesseract returned code '%s', %s\", exitCode, stdout) return stdout.strip() @listener.filters( filters.regex(r\"spam_check_(?P<value>t|f)\") | filters.regex(r\"spam_ban_(?P<user>.*)\") ) async def on_callback_query(self, query: CallbackQuery) -> None: data=query.matches[0].groupdict() handler: MutableMapping[str, Callable]={ \"value\": self._spam_vote_handler, \"user\": self._spam_ban_handler, } for handle in data.keys(): await handler[handle](query, data[handle]) async def _spam_ban_handler(self, query: CallbackQuery, user: str) -> None: chat=query.message.chat try: invoker=await chat.get_member(query.from_user.id) except UserNotParticipant: return await query.answer( await self.get_text(chat.id, \"error-no-rights\"), show_alert=True ) if not invoker.privileges or not invoker.privileges.can_restrict_members: return await query.answer(await self.get_text(chat.id, \"spampredict-ban-no-perm\")) target=await self.bot.client.get_users(int(user)) if isinstance(target, list): target=target[0] await chat.ban_member(target.id) await query.answer( await self.get_text( chat.id, \"spampredict-ban\", user=target.username or target.first_name ) ) keyboard=query.message.reply_markup if not isinstance(keyboard, InlineKeyboardMarkup): raise ValueError(\"Reply markup must be an InlineKeyboardMarkup\") await query.edit_message_reply_markup( reply_markup=InlineKeyboardMarkup(keyboard.inline_keyboard[:-1]) ) async def _spam_vote_handler(self, query: CallbackQuery, value: str) -> None: message=query.message content=re.compile(r\"([A-Fa-f0-9]{64})\").search(message.text) author=query.from_user.id if not content: return await query.answer(f\"Can't get hash from MessageID: '{message.id}'\") content_hash=content[0] data=await self.db.find_one({\"_id\": content_hash}) if not data: return await query.answer(\"The voting poll for this message has ended!\") users_on_correct=data[\"spam\"] users_on_incorrect=data[\"ham\"] if value==\"t\": try: if author in users_on_incorrect: users_on_incorrect.remove(author) if author in users_on_correct: users_on_correct.remove(author) else: users_on_correct.append(author) except TypeError: return await query.answer( \"You can't vote this anymore, because this was marked as a spam by our staff\", show_alert=True, ) elif value==\"f\": try: if author in users_on_correct: users_on_correct.remove(author) if author in users_on_incorrect: users_on_incorrect.remove(author) else: users_on_incorrect.append(author) except TypeError: return await query.answer( \"You can't vote this anymore, because this was marked as a spam by our staff\", show_alert=True, ) else: return await query.answer(\"Invalid keyboard method!\", show_alert=True) await self.db.update_one( {\"_id\": content_hash},{\"$set\":{\"spam\": users_on_correct, \"ham\": users_on_incorrect}} ) total_correct, total_incorrect=len(users_on_correct), len(users_on_incorrect) button=[ [ InlineKeyboardButton( text=f\"\u2705 Correct({total_correct})\", callback_data=\"spam_check_t\", ), InlineKeyboardButton( text=f\"\u274c Incorrect({total_incorrect})\", callback_data=\"spam_check_f\", ), ], ] if isinstance(query.message.reply_markup, InlineKeyboardMarkup): old_btn=query.message.reply_markup.inline_keyboard if len(old_btn) > 1: button.append(old_btn[1]) for i in data[\"msg_id\"]: try: while True: try: await self.bot.client.edit_message_reply_markup( -1001314588569, i, InlineKeyboardMarkup(button) ) except MessageIdInvalid: pass except MessageNotModified: await query.answer( \"You already voted this content, \" \"this happened because there are multiple same of contents exists.\", show_alert=True, ) except FloodWait as flood: await query.answer( \"Please wait i'm updating the content for you.\", show_alert=True, ) await asyncio.sleep(flood.x) continue await asyncio.sleep(0.1) break except QueryIdInvalid: self.log.debug(\"Can't edit message, invalid query id '%s'\", query.id) continue try: await query.answer() except QueryIdInvalid: pass @listener.filters(filters.group) async def on_message(self, message: Message) -> None: \"\"\"Checker service for message\"\"\" setting=await self.setting_db.find_one({\"chat_id\": message.chat.id}) if setting and not setting.get(\"setting\"): return chat=message.chat user=message.from_user text=( message.text.strip() if message.text else(message.caption.strip() if message.media and message.caption else None) ) if message.photo: future=self.bot.loop.create_task(self.run_ocr(message)) future.add_done_callback( partial(self.bot.loop.call_soon_threadsafe, self._check_spam_results_ocr, message) ) if not chat or message.left_chat_member or not user or not text: return return await self.spam_check(message, text) async def spam_check(self, message: Message, text: str, *, from_ocr: bool=False) -> None: try: user=message.from_user.id except AttributeError: user=None response=await self._predict(text.strip()) if response.size==0: return probability=response[0][1] if probability <=0.5: return content_hash=self._build_hash(text) identifier=self._build_hex(user) proba_str=self.prob_to_string(probability) notice=( \" f\"**Prediction Result**:{proba_str}\\n\" f\"**Identifier**: `{identifier}`\\n\" ) if ch:=message.forward_from_chat: notice +=f\"**Channel ID**: `{self._build_hex(ch.id)}`\\n\" if from_ocr: notice +=( f\"**Photo Text Hash**: `{content_hash}`\\n\\n**======CONTENT=======**\\n\\n{text}\" ) else: notice +=( f\"**Message Text Hash**: `{content_hash}`\\n\\n**======CONTENT=======**\\n\\n{text}\" ) l_spam, l_ham=0, 0 _, data=await asyncio.gather( self.bot.log_stat(\"predicted\"), self.db.find_one({\"_id\": content_hash}) ) if data: l_spam=len(data[\"spam\"]) l_ham=len(data[\"ham\"]) keyb=[ [ InlineKeyboardButton(text=f\"\u2705 Correct({l_spam})\", callback_data=\"spam_check_t\"), InlineKeyboardButton(text=f\"\u274c Incorrect({l_ham})\", callback_data=\"spam_check_f\"), ] ] if message.chat.username: keyb.append( [InlineKeyboardButton(text=\"Chat\", url=f\"https:\/\/t.me\/{message.chat.username}\")] ) if message.forward_from_chat and message.forward_from_chat.username: raw_btn=InlineKeyboardButton( text=\"Channel\", url=f\"https:\/\/t.me\/{message.forward_from_chat.username}\" ) if message.chat.username: keyb[1].append(raw_btn) else: keyb.append([raw_btn]) while True: try: msg=await self.bot.client.send_message( chat_id=-1001314588569, text=notice, disable_web_page_preview=True, reply_markup=InlineKeyboardMarkup(keyb), ) except FloodWait as flood: await asyncio.sleep(flood.x) continue await asyncio.sleep(0.1) break try: async with asyncio.Lock(): await self.db.insert_one( { \"_id\": content_hash, \"text\": text, \"spam\":[], \"ham\":[], \"proba\": probability, \"msg_id\":[msg.id], \"date\": util.time.sec(), } ) except DuplicateKeyError: await self.db.update_one({\"_id\": content_hash},{\"$push\":{\"msg_id\": msg.id}}) if probability >=0.8: if user is None or message.sender_chat: return target=await message.chat.get_member(user) if util.tg.is_staff_or_admin(target): return if from_ocr: alert=( f\"\u2757\ufe0f**PHOTO SPAM ALERT**\u2757\ufe0f\\n\\n\" f\"**User**: `{identifier}`\\n\" f\"**Photo Text Hash**: `{content_hash}`\\n\" f\"**Spam Probability**: `{proba_str}%`\" ) else: alert=( f\"\u2757\ufe0f**MESSAGE SPAM ALERT**\u2757\ufe0f\\n\\n\" f\"**User**: `{identifier}`\\n\" f\"**Message Text Hash**: `{content_hash}`\\n\" f\"**Spam Probability**: `{proba_str}%`\" ) await self.bot.log_stat(\"spam_detected\") try: await message.delete() except(MessageDeleteForbidden, ChatAdminRequired, UserAdminInvalid): alert +=\"\\n\\nNot enough permission to delete message.\" reply_id=message.id else: await self.bot.log_stat(\"spam_deleted\") alert +=\"\\n\\nThe message has been deleted.\" reply_id=0 chat=message.chat me=await chat.get_member(self.bot.uid) button=[[InlineKeyboardButton(\"View Message\", url=msg.link)]] if me.privileges and me.privileges.can_restrict_members: button.append( [ InlineKeyboardButton( \"Ban User(*admin only)\", callback_data=f\"spam_ban_{user}\" ) ], ) await self.bot.client.send_message( chat.id, alert, reply_to_message_id=reply_id, reply_markup=InlineKeyboardMarkup(button), ) async def mark_spam_ocr( self, content: str, user_id: Optional[int], chat_id: int, message_id: int ) -> bool: identifier=self._build_hex(user_id) content_hash=self._build_hash(content) pred=await self._predict(content.strip()) if pred.size==0: return False proba=pred[0][1] text=f\" if identifier: text +=f\"**Identifier**: `{identifier}`\\n\" text +=f\"**Photo Text Hash**: `{content_hash}`\\n\\n**=======CONTENT=======**\\n\\n{content}\" res=await asyncio.gather( self.bot.client.send_message( chat_id=-1001314588569, text=text, disable_web_page_preview=True, ), self.db.update_one( {\"_id\": content_hash}, { \"$set\":{ \"text\": content.strip(), \"spam\": 1, \"ham\": 0, } }, upsert=True, ), self.bot.log_stat(\"spam_detected\"), self.bot.log_stat(\"predicted\"), ) await self.bot.client.send_message( chat_id, \"Message photo logged as spam!\", reply_markup=InlineKeyboardMarkup( [[InlineKeyboardButton(\"View Message\", url=res[0].link)]] ), reply_to_message_id=message_id, ) return True @command.filters(filters.staff_only) async def cmd_update_model(self, ctx: command.Context) -> Optional[str]: token=self.bot.config.get(\"sp_token\") url=self.bot.config.get(\"sp_url\") if not(token and url): return \"No token provided!\" await self.__load_model(token, url) await ctx.respond(\"Done\", delete_after=5) @command.filters(filters.staff_only) async def cmd_spam(self, ctx: command.Context) -> Optional[str]: \"\"\"Manual spam detection by bot staff\"\"\" user_id=None reply_msg=ctx.msg.reply_to_message if reply_msg: content=reply_msg.text or reply_msg.caption if reply_msg.from_user and reply_msg.from_user.id !=ctx.author.id: user_id=reply_msg.from_user.id elif reply_msg.forward_from: user_id=reply_msg.forward_from.id else: content=ctx.input if reply_msg and reply_msg.photo: ocr_result=await self.run_ocr(reply_msg) if ocr_result: try: await self.mark_spam_ocr(ocr_result, user_id, ctx.chat.id, reply_msg.id) except Exception as e: self.log.error(\"Failed to marked OCR results as spam\", exc_info=e) if not content: return None if not content: return \"Give me a text or reply to a message \/ forwarded message\" identifier=self._build_hex(user_id) content_hash=self._build_hash(content) pred=await self._predict(content.strip()) if pred.size==0: return \"Prediction failed\" proba=pred[0][1] text=f\" if identifier: text +=f\"**Identifier**: `{identifier}`\\n\" text +=f\"**Message Hash**: `{content_hash}`\\n\\n**=======CONTENT=======**\\n\\n{content}\" _, msg, __, ___=await asyncio.gather( self.db.update_one( {\"_id\": content_hash}, { \"$set\":{ \"text\": content.strip(), \"spam\": 1, \"ham\": 0, } }, upsert=True, ), self.bot.client.send_message( chat_id=-1001314588569, text=text, disable_web_page_preview=True, ), self.bot.log_stat(\"spam_detected\"), self.bot.log_stat(\"predicted\"), ) await ctx.respond( \"Message logged as spam!\", reply_markup=InlineKeyboardMarkup( [[InlineKeyboardButton(\"View Message\", url=msg.link)]] ), ) return None @command.filters(aliases=[\"prediction\"]) async def cmd_predict(self, ctx: command.Context) -> Optional[str]: \"\"\"Look a prediction for a replied message\"\"\" chat=ctx.chat user=await self.user_db.find_one({\"_id\": ctx.author.id}) if not user: return None if user[\"reputation\"] < 100: return await self.text(chat.id, \"spampredict-unauthorized\", user[\"reputation\"]) replied=ctx.msg.reply_to_message if not replied: await ctx.respond( await self.get_text(chat.id, \"error-reply-to-message\"), delete_after=5 ) return None content=replied.text or replied.caption photoPrediction=None if replied.photo: await ctx.respond( await self.get_text(chat.id, \"spampredict-photo\"), reply_to_message_id=replied.id, ) ocr_result=await self.run_ocr(replied) if ocr_result: ocr_prediction=await self._predict(ocr_result) if ocr_prediction.size !=0: photoPrediction=( \"**Result Photo Text**\\n\\n\" f\"**Is Spam**:{await self._is_spam(ocr_result)}\\n\" f\"**Spam Prediction**: `{self.prob_to_string(ocr_prediction[0][1])}`\\n\" f\"**Ham Prediction**: `{self.prob_to_string(ocr_prediction[0][0])}`\\n\\n\" ) if not content: await asyncio.gather( self.bot.log_stat(\"predicted\"), ctx.respond(photoPrediction), ) return None else: photoPrediction=\"__Failed to read text from photo__\\n\\n\" if not content: return await self.get_text(chat.id, \"spampredict-empty\") pred=await self._predict(content) if pred.size==0: return await self.get_text(chat.id, \"spampredict-failed\") textPrediction=( f\"**Is Spam**:{await self._is_spam(content)}\\n\" f\"**Spam Prediction**: `{self.prob_to_string(pred[0][1])}`\\n\" f\"**Ham Prediction**: `{self.prob_to_string(pred[0][0])}`\" ) await asyncio.gather( self.bot.log_stat(\"predicted\"), ctx.respond( photoPrediction +\"**Result Caption Text**\\n\\n\" +textPrediction if photoPrediction else \"**Result**\\n\\n\" +textPrediction, reply_to_message_id=None if replied.photo else replied.id, ), ) return None async def setting(self, chat_id: int, setting: bool) -> None: \"\"\"Turn on\/off spam prediction in chats\"\"\" if setting: await self.setting_db.update_one( {\"chat_id\": chat_id},{\"$set\":{\"setting\": True}}, upsert=True ) else: await self.setting_db.delete_one({\"chat_id\": chat_id}) async def is_active(self, chat_id: int) -> bool: \"\"\"Return SpamShield setting\"\"\" data=await self.setting_db.find_one({\"chat_id\": chat_id}) return data[\"setting\"] if data else True @command.filters(filters.admin_only, aliases=[\"spampredict\", \"spam_predict\"]) async def cmd_spam_prediction(self, ctx: command.Context, enable: Optional[bool]=None) -> str: \"\"\"Set spam prediction setting\"\"\" chat=ctx.chat if not ctx.input: return await self.text(chat.id, \"spampredict-view\", await self.is_active(chat.id)) if enable is None: return await self.text(chat.id, \"err-invalid-option\") ret, _=await asyncio.gather( self.text(chat.id, \"spampredict-set\", \"on\" if enable else \"off\"), self.setting(chat.id, enable), ) return ret ","sourceWithComments":"\"\"\"Spam Prediction plugin\"\"\"\n# Copyright (C) 2020 - 2022  UserbotIndo Team, <https:\/\/github.com\/userbotindo.git>\n#\n# This program is free software: you can redistribute it and\/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http:\/\/www.gnu.org\/licenses\/>.\n\nimport asyncio\nimport pickle\nimport re\nfrom functools import partial\nfrom hashlib import md5, sha256\nfrom typing import Any, Callable, ClassVar, MutableMapping, Optional\n\nfrom aiopath import AsyncPath\nfrom pymongo.errors import DuplicateKeyError\nfrom pyrogram.errors import (\n    ChatAdminRequired,\n    FloodWait,\n    MessageDeleteForbidden,\n    MessageIdInvalid,\n    MessageNotModified,\n    QueryIdInvalid,\n    UserAdminInvalid,\n    UserNotParticipant\n)\nfrom pyrogram.types import (\n    CallbackQuery,\n    InlineKeyboardButton,\n    InlineKeyboardMarkup,\n    Message,\n)\n\ntry:\n    from sklearn.pipeline import Pipeline\n\n    _run_predict = True\nexcept ImportError:\n    from anjani.util.types import Pipeline\n\n    _run_predict = False\n\nfrom anjani import command, filters, listener, plugin, util\n\n\nclass SpamPrediction(plugin.Plugin):\n    name: ClassVar[str] = \"SpamPredict\"\n    helpable: ClassVar[bool] = True\n    disabled: ClassVar[bool] = not _run_predict\n\n    db: util.db.AsyncCollection\n    user_db: util.db.AsyncCollection\n    setting_db: util.db.AsyncCollection\n    model: Pipeline\n\n    async def on_load(self) -> None:\n        token = self.bot.config.get(\"sp_token\")\n        url = self.bot.config.get(\"sp_url\")\n        if not (token and url):\n            return self.bot.unload_plugin(self)\n\n        self.db = self.bot.db.get_collection(\"SPAM_DUMP\")\n        self.user_db = self.bot.db.get_collection(\"USERS\")\n        self.setting_db = self.bot.db.get_collection(\"SPAM_PREDICT_SETTING\")\n\n        # Avoid race conditions with on_message listener\n        async with asyncio.Lock():\n            await self.__load_model(token, url)\n\n    async def on_chat_migrate(self, message: Message) -> None:\n        await self.db.update_one(\n            {\"chat_id\": message.migrate_from_chat_id},\n            {\"$set\": {\"chat_id\": message.chat.id}},\n        )\n\n    async def on_plugin_backup(self, chat_id: int) -> MutableMapping[str, Any]:\n        setting = await self.setting_db.find_one({\"chat_id\": chat_id})\n        return {self.name: setting} if setting else {}\n\n    async def on_plugin_restore(self, chat_id: int, data: MutableMapping[str, Any]) -> None:\n        await self.setting_db.update_one(\n            {\"chat_id\": chat_id}, {\"$set\": data[self.name]}, upsert=True\n        )\n\n    async def __load_model(self, token: str, url: str) -> None:\n        self.log.info(\"Downloading spam prediction model!\")\n        async with self.bot.http.get(\n            url,\n            headers={\n                \"Authorization\": f\"token {token}\",\n                \"Accept\": \"application\/vnd.github.v3.raw\",\n            },\n        ) as res:\n            if res.status == 200:\n                self.model = await util.run_sync(pickle.loads, await res.read())\n            else:\n                self.log.warning(\"Failed to download prediction model!\")\n                self.bot.unload_plugin(self)\n\n    def _check_spam_results_ocr(\n        self, message: Message, future: asyncio.Future[Optional[str]]\n    ) -> None:\n        def done(fut: asyncio.Future[None]) -> None:\n            try:\n                fut.result()\n            except Exception as e:  # skipcq: PYL-W0703\n                self.log.error(\"Unexpected error occured when checking OCR results\", exc_info=e)\n\n        text = future.result()\n        if not text:\n            return\n\n        f = self.bot.loop.create_task(self.spam_check(message, text, from_ocr=True))\n        f.add_done_callback(done)\n\n    @staticmethod\n    def _build_hash(content: str) -> str:\n        return sha256(content.strip().encode()).hexdigest()\n\n    def _build_hex(self, id: Optional[int]) -> str:\n        if not id:\n            id = self.bot.uid\n        # skipcq: PTC-W1003\n        return md5((str(id) + self.bot.user.username).encode()).hexdigest()  # skipcq: BAN-B324\n\n    @staticmethod\n    def prob_to_string(value: float) -> str:\n        return str(value * 10 ** 2)[0:7]\n\n    async def _predict(self, text: str) -> util.types.NDArray[float]:\n        return await util.run_sync(self.model.predict_proba, [text])\n\n    async def _is_spam(self, text: str) -> bool:\n        return (await util.run_sync(self.model.predict, [text]))[0] == \"spam\"\n\n    async def run_ocr(self, message: Message) -> Optional[str]:\n        \"\"\"Run tesseract\"\"\"\n        try:\n            image = AsyncPath(await message.download())\n        except Exception:  # skipcq: PYL-W0703\n            return self.log.warning(\n                \"Failed to download image from MessageID %s in Chat %s\",\n                message.id,\n                message.chat.id,\n            )\n\n        try:\n            stdout, _, exitCode = await util.system.run_command(\n                \"tesseract\",\n                str(image),\n                \"stdout\",\n                \"-l\",\n                \"eng+ind\",\n                \"--psm\",\n                \"6\"\n            )\n        except Exception as e:  # skipcq: PYL-W0703\n            return self.log.error(\"Unexpected error occured when running OCR\", exc_info=e)\n        finally:\n            await image.unlink()\n\n        if exitCode != 0:\n            return self.log.warning(\"tesseract returned code '%s', %s\", exitCode, stdout)\n\n        return stdout.strip()\n\n    @listener.filters(\n        filters.regex(r\"spam_check_(?P<value>t|f)\") | filters.regex(r\"spam_ban_(?P<user>.*)\")\n    )\n    async def on_callback_query(self, query: CallbackQuery) -> None:\n        data = query.matches[0].groupdict()\n        handler: MutableMapping[str, Callable] = {\n            \"value\": self._spam_vote_handler,\n            \"user\": self._spam_ban_handler,\n        }\n        for handle in data.keys():\n            await handler[handle](query, data[handle])\n\n    async def _spam_ban_handler(self, query: CallbackQuery, user: str) -> None:\n        chat = query.message.chat\n        try:\n            invoker = await chat.get_member(query.from_user.id)\n        except UserNotParticipant:\n            return await query.answer(\n                await self.get_text(chat.id, \"error-no-rights\"),\n                show_alert=True\n            )\n\n        if not invoker.privileges or not invoker.privileges.can_restrict_members:\n            return await query.answer(await self.get_text(chat.id, \"spampredict-ban-no-perm\"))\n\n        target = await self.bot.client.get_users(int(user))\n        if isinstance(target, list):\n            target = target[0]\n\n        await chat.ban_member(target.id)\n        await query.answer(\n            await self.get_text(\n                chat.id,\n                \"spampredict-ban\",\n                user=target.username or target.first_name\n            )\n        )\n\n        keyboard = query.message.reply_markup\n        if not isinstance(keyboard, InlineKeyboardMarkup):\n            raise ValueError(\"Reply markup must be an InlineKeyboardMarkup\")\n\n        await query.edit_message_reply_markup(\n            reply_markup=InlineKeyboardMarkup(keyboard.inline_keyboard[:-1])\n        )\n\n    async def _spam_vote_handler(self, query: CallbackQuery, value: str) -> None:\n        message = query.message\n        content = re.compile(r\"([A-Fa-f0-9]{64})\").search(message.text)\n        author = query.from_user.id\n\n        if not content:\n            return await query.answer(f\"Can't get hash from MessageID: '{message.id}'\")\n\n        content_hash = content[0]\n\n        data = await self.db.find_one({\"_id\": content_hash})\n        if not data:\n            return await query.answer(\"The voting poll for this message has ended!\")\n\n        users_on_correct = data[\"spam\"]\n        users_on_incorrect = data[\"ham\"]\n        if value == \"t\":\n            try:\n                # Check user in incorrect data\n                if author in users_on_incorrect:\n                    users_on_incorrect.remove(author)\n\n                if author in users_on_correct:\n                    users_on_correct.remove(author)\n                else:\n                    users_on_correct.append(author)\n            except TypeError:\n                return await query.answer(\n                    \"You can't vote this anymore, because this was marked as a spam by our staff\",\n                    show_alert=True,\n                )\n        elif value == \"f\":\n            try:\n                # Check user in correct data\n                if author in users_on_correct:\n                    users_on_correct.remove(author)\n\n                if author in users_on_incorrect:\n                    users_on_incorrect.remove(author)\n                else:\n                    users_on_incorrect.append(author)\n            except TypeError:\n                return await query.answer(\n                    \"You can't vote this anymore, because this was marked as a spam by our staff\",\n                    show_alert=True,\n                )\n        else:\n            return await query.answer(\"Invalid keyboard method!\", show_alert=True)\n\n        await self.db.update_one(\n            {\"_id\": content_hash}, {\"$set\": {\"spam\": users_on_correct, \"ham\": users_on_incorrect}}\n        )\n\n        total_correct, total_incorrect = len(users_on_correct), len(users_on_incorrect)\n        button = [\n            [\n                InlineKeyboardButton(\n                    text=f\"\u2705 Correct ({total_correct})\",\n                    callback_data=\"spam_check_t\",\n                ),\n                InlineKeyboardButton(\n                    text=f\"\u274c Incorrect ({total_incorrect})\",\n                    callback_data=\"spam_check_f\",\n                ),\n            ],\n        ]\n\n        if isinstance(query.message.reply_markup, InlineKeyboardMarkup):\n            old_btn = query.message.reply_markup.inline_keyboard\n            if len(old_btn) > 1:\n                button.append(old_btn[1])\n\n        for i in data[\"msg_id\"]:\n            try:\n                while True:\n                    try:\n                        await self.bot.client.edit_message_reply_markup(\n                            -1001314588569, i, InlineKeyboardMarkup(button)\n                        )\n                    except MessageIdInvalid:\n                        pass\n                    except MessageNotModified:\n                        await query.answer(\n                            \"You already voted this content, \"\n                            \"this happened because there are multiple same of contents exists.\",\n                            show_alert=True,\n                        )\n                    except FloodWait as flood:\n                        await query.answer(\n                            \"Please wait i'm updating the content for you.\",\n                            show_alert=True,\n                        )\n                        await asyncio.sleep(flood.x)  # type: ignore\n                        continue\n\n                    await asyncio.sleep(0.1)\n                    break\n            except QueryIdInvalid:\n                self.log.debug(\"Can't edit message, invalid query id '%s'\", query.id)\n                continue\n\n        try:\n            await query.answer()\n        except QueryIdInvalid:\n            pass\n\n    @listener.filters(filters.group)\n    async def on_message(self, message: Message) -> None:\n        \"\"\"Checker service for message\"\"\"\n        setting = await self.setting_db.find_one({\"chat_id\": message.chat.id})\n        if setting and not setting.get(\"setting\"):\n            return\n\n        chat = message.chat\n        user = message.from_user\n        text = (\n            message.text.strip()\n            if message.text\n            else (message.caption.strip() if message.media and message.caption else None)\n        )\n        if message.photo:\n            future = self.bot.loop.create_task(self.run_ocr(message))\n            future.add_done_callback(\n                partial(self.bot.loop.call_soon_threadsafe, self._check_spam_results_ocr, message)\n            )\n\n        if not chat or message.left_chat_member or not user or not text:\n            return\n\n        # Always check the spam probability\n        return await self.spam_check(message, text)\n\n    async def spam_check(self, message: Message, text: str, *, from_ocr: bool = False) -> None:\n        try:\n            user = message.from_user.id\n        except AttributeError:\n            user = None\n\n        response = await self._predict(text.strip())\n        if response.size == 0:\n            return\n\n        probability = response[0][1]\n        if probability <= 0.5:\n            return\n\n        content_hash = self._build_hash(text)\n        identifier = self._build_hex(user)\n        proba_str = self.prob_to_string(probability)\n\n        notice = (\n            \"#SPAM_PREDICTION\\n\\n\"\n            f\"**Prediction Result**: {proba_str}\\n\"\n            f\"**Identifier**: `{identifier}`\\n\"\n        )\n        if ch := message.forward_from_chat:\n            notice += f\"**Channel ID**: `{self._build_hex(ch.id)}`\\n\"\n\n        if from_ocr:\n            notice += (\n                f\"**Photo Text Hash**: `{content_hash}`\\n\\n**====== CONTENT =======**\\n\\n{text}\"\n            )\n        else:\n            notice += (\n                f\"**Message Text Hash**: `{content_hash}`\\n\\n**====== CONTENT =======**\\n\\n{text}\"\n            )\n\n        l_spam, l_ham = 0, 0\n        _, data = await asyncio.gather(\n            self.bot.log_stat(\"predicted\"), self.db.find_one({\"_id\": content_hash})\n        )\n        if data:\n            l_spam = len(data[\"spam\"])\n            l_ham = len(data[\"ham\"])\n\n        keyb = [\n            [\n                InlineKeyboardButton(text=f\"\u2705 Correct ({l_spam})\", callback_data=\"spam_check_t\"),\n                InlineKeyboardButton(text=f\"\u274c Incorrect ({l_ham})\", callback_data=\"spam_check_f\"),\n            ]\n        ]\n\n        if message.chat.username:\n            keyb.append(\n                [InlineKeyboardButton(text=\"Chat\", url=f\"https:\/\/t.me\/{message.chat.username}\")]\n            )\n\n        if message.forward_from_chat and message.forward_from_chat.username:\n            raw_btn = InlineKeyboardButton(\n                text=\"Channel\", url=f\"https:\/\/t.me\/{message.forward_from_chat.username}\"\n            )\n            if message.chat.username:\n                keyb[1].append(raw_btn)\n            else:\n                keyb.append([raw_btn])\n\n        while True:\n            try:\n                msg = await self.bot.client.send_message(\n                    chat_id=-1001314588569,\n                    text=notice,\n                    disable_web_page_preview=True,\n                    reply_markup=InlineKeyboardMarkup(keyb),\n                )\n            except FloodWait as flood:\n                await asyncio.sleep(flood.x)  # type: ignore\n                continue\n\n            await asyncio.sleep(0.1)\n            break\n\n        try:\n            async with asyncio.Lock():\n                await self.db.insert_one(\n                    {\n                        \"_id\": content_hash,\n                        \"text\": text,\n                        \"spam\": [],\n                        \"ham\": [],\n                        \"proba\": probability,\n                        \"msg_id\": [msg.id],\n                        \"date\": util.time.sec(),\n                    }\n                )\n        except DuplicateKeyError:\n            await self.db.update_one({\"_id\": content_hash}, {\"$push\": {\"msg_id\": msg.id}})\n\n        if probability >= 0.8:\n            # Empty user big chances are anonymous admins\n            if user is None or message.sender_chat:\n                return\n\n            target = await message.chat.get_member(user)\n            if util.tg.is_staff_or_admin(target):\n                return\n\n            if from_ocr:\n                alert = (\n                    f\"\u2757\ufe0f**PHOTO SPAM ALERT**\u2757\ufe0f\\n\\n\"\n                    f\"**User**: `{identifier}`\\n\"\n                    f\"**Photo Text Hash**: `{content_hash}`\\n\"\n                    f\"**Spam Probability**: `{proba_str}%`\"\n                )\n            else:\n                alert = (\n                    f\"\u2757\ufe0f**MESSAGE SPAM ALERT**\u2757\ufe0f\\n\\n\"\n                    f\"**User**: `{identifier}`\\n\"\n                    f\"**Message Text Hash**: `{content_hash}`\\n\"\n                    f\"**Spam Probability**: `{proba_str}%`\"\n                )\n\n            await self.bot.log_stat(\"spam_detected\")\n            try:\n                await message.delete()\n            except (MessageDeleteForbidden, ChatAdminRequired, UserAdminInvalid):\n                alert += \"\\n\\nNot enough permission to delete message.\"\n                reply_id = message.id\n            else:\n                await self.bot.log_stat(\"spam_deleted\")\n                alert += \"\\n\\nThe message has been deleted.\"\n                reply_id = 0\n\n            chat = message.chat\n            me = await chat.get_member(self.bot.uid)\n            button = [[InlineKeyboardButton(\"View Message\", url=msg.link)]]\n            if me.privileges and me.privileges.can_restrict_members:\n                button.append(\n                    [\n                        InlineKeyboardButton(\n                            \"Ban User (*admin only)\", callback_data=f\"spam_ban_{user}\"\n                        )\n                    ],\n                )\n\n            await self.bot.client.send_message(\n                chat.id,\n                alert,\n                reply_to_message_id=reply_id,\n                reply_markup=InlineKeyboardMarkup(button),\n            )\n\n    async def mark_spam_ocr(\n        self, content: str, user_id: Optional[int], chat_id: int, message_id: int\n    ) -> bool:\n        identifier = self._build_hex(user_id)\n        content_hash = self._build_hash(content)\n        pred = await self._predict(content.strip())\n        if pred.size == 0:\n            return False\n\n        proba = pred[0][1]\n        text = f\"#SPAM\\n\\n**CPU Prediction**: `{self.prob_to_string(proba)}`\\n\"\n        if identifier:\n            text += f\"**Identifier**: `{identifier}`\\n\"\n\n        text += f\"**Photo Text Hash**: `{content_hash}`\\n\\n**======= CONTENT =======**\\n\\n{content}\"\n        res = await asyncio.gather(\n            self.bot.client.send_message(\n                chat_id=-1001314588569,\n                text=text,\n                disable_web_page_preview=True,\n            ),\n            self.db.update_one(\n                {\"_id\": content_hash},\n                {\n                    \"$set\": {\n                        \"text\": content.strip(),\n                        \"spam\": 1,\n                        \"ham\": 0,\n                    }\n                },\n                upsert=True,\n            ),\n            self.bot.log_stat(\"spam_detected\"),\n            self.bot.log_stat(\"predicted\"),\n        )\n        await self.bot.client.send_message(\n            chat_id,\n            \"Message photo logged as spam!\",\n            reply_markup=InlineKeyboardMarkup(\n                [[InlineKeyboardButton(\"View Message\", url=res[0].link)]]\n            ),\n            reply_to_message_id=message_id,\n        )\n        return True\n\n    @command.filters(filters.staff_only)\n    async def cmd_update_model(self, ctx: command.Context) -> Optional[str]:\n        token = self.bot.config.get(\"sp_token\")\n        url = self.bot.config.get(\"sp_url\")\n        if not (token and url):\n            return \"No token provided!\"\n\n        await self.__load_model(token, url)\n        await ctx.respond(\"Done\", delete_after=5)\n\n    @command.filters(filters.staff_only)\n    async def cmd_spam(self, ctx: command.Context) -> Optional[str]:\n        \"\"\"Manual spam detection by bot staff\"\"\"\n        user_id = None\n        reply_msg = ctx.msg.reply_to_message\n        if reply_msg:\n            content = reply_msg.text or reply_msg.caption\n            if reply_msg.from_user and reply_msg.from_user.id != ctx.author.id:\n                user_id = reply_msg.from_user.id\n            elif reply_msg.forward_from:\n                user_id = reply_msg.forward_from.id\n        else:\n            content = ctx.input\n\n        if reply_msg and reply_msg.photo:\n            ocr_result = await self.run_ocr(reply_msg)\n            if ocr_result:\n                try:\n                    await self.mark_spam_ocr(ocr_result, user_id, ctx.chat.id, reply_msg.id)\n                except Exception as e:  # skipcq: PYL-W0703\n                    self.log.error(\"Failed to marked OCR results as spam\", exc_info=e)\n\n                # Return early if content is empty, so error message not shown\n                if not content:\n                    return None\n\n        if not content:\n            return \"Give me a text or reply to a message \/ forwarded message\"\n\n        identifier = self._build_hex(user_id)\n        content_hash = self._build_hash(content)\n        pred = await self._predict(content.strip())\n        if pred.size == 0:\n            return \"Prediction failed\"\n\n        proba = pred[0][1]\n        text = f\"#SPAM\\n\\n**CPU Prediction**: `{self.prob_to_string(proba)}`\\n\"\n        if identifier:\n            text += f\"**Identifier**: `{identifier}`\\n\"\n\n        text += f\"**Message Hash**: `{content_hash}`\\n\\n**======= CONTENT =======**\\n\\n{content}\"\n        _, msg, __, ___ = await asyncio.gather(\n            self.db.update_one(\n                {\"_id\": content_hash},\n                {\n                    \"$set\": {\n                        \"text\": content.strip(),\n                        \"spam\": 1,\n                        \"ham\": 0,\n                    }\n                },\n                upsert=True,\n            ),\n            self.bot.client.send_message(\n                chat_id=-1001314588569,\n                text=text,\n                disable_web_page_preview=True,\n            ),\n            self.bot.log_stat(\"spam_detected\"),\n            self.bot.log_stat(\"predicted\"),\n        )\n        await ctx.respond(\n            \"Message logged as spam!\",\n            reply_markup=InlineKeyboardMarkup(\n                [[InlineKeyboardButton(\"View Message\", url=msg.link)]]\n            ),\n        )\n        return None\n\n    @command.filters(aliases=[\"prediction\"])\n    async def cmd_predict(self, ctx: command.Context) -> Optional[str]:\n        \"\"\"Look a prediction for a replied message\"\"\"\n        chat = ctx.chat\n        user = await self.user_db.find_one({\"_id\": ctx.author.id})\n        if not user:\n            return None\n\n        if user[\"reputation\"] < 100:\n            return await self.text(chat.id, \"spampredict-unauthorized\", user[\"reputation\"])\n\n        replied = ctx.msg.reply_to_message\n        if not replied:\n            await ctx.respond(\n                await self.get_text(chat.id, \"error-reply-to-message\"), delete_after=5\n            )\n            return None\n\n        content = replied.text or replied.caption\n\n        photoPrediction = None\n        if replied.photo:\n            await ctx.respond(\n                await self.get_text(chat.id, \"spampredict-photo\"),\n                reply_to_message_id=replied.id,\n            )\n\n            ocr_result = await self.run_ocr(replied)\n            if ocr_result:\n                ocr_prediction = await self._predict(ocr_result)\n                if ocr_prediction.size != 0:\n                    photoPrediction = (\n                        \"**Result Photo Text**\\n\\n\"\n                        f\"**Is Spam**: {await self._is_spam(ocr_result)}\\n\"\n                        f\"**Spam Prediction**: `{self.prob_to_string(ocr_prediction[0][1])}`\\n\"\n                        f\"**Ham Prediction**: `{self.prob_to_string(ocr_prediction[0][0])}`\\n\\n\"\n                    )\n                    # Return early if content is empty, so error message not shown\n                    if not content:\n                        await asyncio.gather(\n                            self.bot.log_stat(\"predicted\"),\n                            ctx.respond(photoPrediction),\n                        )\n                        return None\n            else:\n                photoPrediction = \"__Failed to read text from photo__\\n\\n\"\n\n        if not content:\n            return await self.get_text(chat.id, \"spampredict-empty\")\n\n        pred = await self._predict(content)\n        if pred.size == 0:\n            return await self.get_text(chat.id, \"spampredict-failed\")\n\n        textPrediction = (\n            f\"**Is Spam**: {await self._is_spam(content)}\\n\"\n            f\"**Spam Prediction**: `{self.prob_to_string(pred[0][1])}`\\n\"\n            f\"**Ham Prediction**: `{self.prob_to_string(pred[0][0])}`\"\n        )\n        await asyncio.gather(\n            self.bot.log_stat(\"predicted\"),\n            ctx.respond(\n                photoPrediction + \"**Result Caption Text**\\n\\n\" + textPrediction\n                if photoPrediction\n                else \"**Result**\\n\\n\" + textPrediction,\n                reply_to_message_id=None if replied.photo else replied.id,\n            ),\n        )\n        return None\n\n    async def setting(self, chat_id: int, setting: bool) -> None:\n        \"\"\"Turn on\/off spam prediction in chats\"\"\"\n        if setting:\n            await self.setting_db.update_one(\n                {\"chat_id\": chat_id}, {\"$set\": {\"setting\": True}}, upsert=True\n            )\n        else:\n            await self.setting_db.delete_one({\"chat_id\": chat_id})\n\n    async def is_active(self, chat_id: int) -> bool:\n        \"\"\"Return SpamShield setting\"\"\"\n        data = await self.setting_db.find_one({\"chat_id\": chat_id})\n        return data[\"setting\"] if data else True\n\n    @command.filters(filters.admin_only, aliases=[\"spampredict\", \"spam_predict\"])\n    async def cmd_spam_prediction(self, ctx: command.Context, enable: Optional[bool] = None) -> str:\n        \"\"\"Set spam prediction setting\"\"\"\n        chat = ctx.chat\n        if not ctx.input:\n            return await self.text(chat.id, \"spampredict-view\", await self.is_active(chat.id))\n\n        if enable is None:\n            return await self.text(chat.id, \"err-invalid-option\")\n\n        ret, _ = await asyncio.gather(\n            self.text(chat.id, \"spampredict-set\", \"on\" if enable else \"off\"),\n            self.setting(chat.id, enable),\n        )\n        return ret\n"}},"msg":"SpamPrediction: fix usernotparticipant\n\n-> also fix flood.x > flood.value\n\nChange-Id: I365f47e61b973f07150d23fc3c94035887dbc987"}},"https:\/\/github.com\/mriale\/PyDPainter":{"8cbba5eacdd73a0c90edcf9168a18b635398e2a3":{"url":"https:\/\/api.github.com\/repos\/mriale\/PyDPainter\/commits\/8cbba5eacdd73a0c90edcf9168a18b635398e2a3","html_url":"https:\/\/github.com\/mriale\/PyDPainter\/commit\/8cbba5eacdd73a0c90edcf9168a18b635398e2a3","message":"Implement resize in Screen Format. Fix flood fill.","sha":"8cbba5eacdd73a0c90edcf9168a18b635398e2a3","keyword":"flooding fix","diff":"diff --git a\/libs\/menureq.py b\/libs\/menureq.py\nindex 4337405..524890c 100644\n--- a\/libs\/menureq.py\n+++ b\/libs\/menureq.py\n@@ -467,7 +467,7 @@ def get_top_pal(pal, colorlist, num_colors, halfbright):\n                     if halfbright:\n                         dmode |= config.MODE_EXTRA_HALFBRIGHT\n \n-                    if not new_clicked and (px != config.pixel_width or py != config.pixel_height):\n+                    if not new_clicked and resize_page and (px != config.pixel_width or py != config.pixel_height):\n                         new_pixel_canvas = pygame.transform.scale(config.pixel_canvas, (px, py))\n                         new_pixel_canvas.set_palette(config.pal)\n                         config.pixel_canvas = new_pixel_canvas\ndiff --git a\/libs\/prim.py b\/libs\/prim.py\nindex b7f51b5..5405ae9 100644\n--- a\/libs\/prim.py\n+++ b\/libs\/prim.py\n@@ -1686,7 +1686,7 @@ def floodfill(surface, fill_color, position):\n         config.fillmode.bounds = copy.copy(FillMode.NOBOUNDS)\n         if onscreen((x,y)):\n             surf_array = pygame.surfarray.pixels2d(surface)  # Create an array from the surface.\n-            maxx, maxy = config.screen_width, config.screen_height\n+            maxx, maxy = config.pixel_width, config.pixel_height\n             current_color = surf_array[x,y]\n             if fill_color == current_color:\n                 if config.fillmode.value == FillMode.SOLID:\n","files":{"\/libs\/menureq.py":{"changes":[{"diff":"\n                     if halfbright:\n                         dmode |= config.MODE_EXTRA_HALFBRIGHT\n \n-                    if not new_clicked and (px != config.pixel_width or py != config.pixel_height):\n+                    if not new_clicked and resize_page and (px != config.pixel_width or py != config.pixel_height):\n                         new_pixel_canvas = pygame.transform.scale(config.pixel_canvas, (px, py))\n                         new_pixel_canvas.set_palette(config.pal)\n                         config.pixel_canvas = new_pixel_canvas","add":1,"remove":1,"filename":"\/libs\/menureq.py","badparts":["                    if not new_clicked and (px != config.pixel_width or py != config.pixel_height):"],"goodparts":["                    if not new_clicked and resize_page and (px != config.pixel_width or py != config.pixel_height):"]}],"source":"\n import os.path, colorsys import gadget from gadget import * from prim import * import contextlib with contextlib.redirect_stdout(None): import pygame from pygame.locals import * config=None def menureq_set_config(config_in): global config config=config_in def get_dir(path): filelist=[] dirlist=[\"..(parent dir)\"] try: with os.scandir(path) as it: for entry in it: if not entry.name.startswith('.'): if entry.is_file(): filelist.append(entry.name) elif entry.is_dir(): dirlist.append(\"\\x92\\x93\" +entry.name) filelist.sort(key=str.casefold) dirlist.sort(key=str.casefold) except FileNotFoundError: dirlist=[\"<Not found>\"] return dirlist +filelist def file_req(screen, title, action_label, filepath, filename): req=str2req(title, \"\"\" Path:_________________________ File:_________________________ [%s][Cancel] \"\"\"%(action_label), \" req.center(screen) config.pixel_req_rect=req.get_screen_rect() retval=\"\" list_itemsg=req.gadget_id(\"0_1\") list_itemsg.items=get_dir(filepath) list_itemsg.top_item=0 list_itemsg.value=list_itemsg.top_item list_upg=req.gadget_id(\"28_1\") list_upg.value=-1 list_downg=req.gadget_id(\"28_10\") list_downg.value=1 list_sliderg=req.gadget_id(\"28_2\") list_sliderg.value=list_itemsg.top_item listg_list=[list_itemsg, list_upg, list_downg, list_sliderg] list_itemsg.listgadgets=listg_list list_upg.listgadgets=listg_list list_downg.listgadgets=listg_list list_sliderg.listgadgets=listg_list file_pathg=req.gadget_id(\"5_0\") file_pathg.value=filepath file_pathg.maxvalue=255 filename=os.path.basename(filename) file_nameg=req.gadget_id(\"5_11\") file_nameg.value=filename file_nameg.maxvalue=255 fontmult=1 if config.aspectX !=config.aspectY: fontmult=2 req.draw(screen) config.recompose() last_click_ms=pygame.time.get_ticks() running=1 wait_for_mouseup=0 while running or wait_for_mouseup: event=pygame.event.wait() gevents=req.process_event(screen, event) if event.type==KEYDOWN and event.key==K_ESCAPE: running=0 for ge in gevents: if ge.gadget.type==Gadget.TYPE_BOOL: if ge.gadget.label==action_label: if file_nameg.value !=\"\": retval=os.path.join(filepath, file_nameg.value) running=0 elif ge.gadget.label==\"Cancel\": running=0 if ge.gadget.type==Gadget.TYPE_STRING: if ge.type==ge.TYPE_GADGETUP and ge.gadget==file_pathg: filepath=file_pathg.value list_itemsg.items=get_dir(filepath) list_itemsg.top_item=0 list_itemsg.value=list_itemsg.top_item list_itemsg.need_redraw=True if not pygame.event.peek((KEYDOWN, KEYUP, MOUSEBUTTONDOWN, MOUSEBUTTONUP, MOUSEMOTION, VIDEORESIZE)): if event.type==MOUSEBUTTONDOWN and event.button==1 and list_itemsg.pointin(config.get_mouse_pixel_pos(event), list_itemsg.screenrect): filename=list_itemsg.items[list_itemsg.value] if len(filename) > 2 and(filename[0:2]==\"\\x92\\x93\" or filename[0:2]==\"..\"): if filename[0:2]==\"\\x92\\x93\": filepath=os.path.join(filepath, filename[2:]) elif filename[0:2]==\"..\": filepath=os.path.abspath(os.path.join(filepath, \"..\")) list_itemsg.items=get_dir(filepath) list_itemsg.top_item=0 list_itemsg.value=list_itemsg.top_item list_itemsg.need_redraw=True file_pathg.value=filepath file_pathg.need_redraw=True else: file_nameg.value=filename file_nameg.need_redraw=True if pygame.time.get_ticks() -last_click_ms < 500: if file_nameg.value !=\"\": retval=os.path.join(filepath, file_nameg.value) running=0 wait_for_mouseup=1 else: last_click_ms=pygame.time.get_ticks() elif event.type==MOUSEBUTTONUP and event.button==1: wait_for_mouseup=0 req.draw(screen) config.recompose() config.pixel_req_rect=None config.recompose() return retval def screen_format_req(screen, new_clicked=False): req=str2req(\"Choose Screen Format\", \"\"\" Number of Format: Colors: [Lo-Res 320x200][ 2][ 32] [Med-Res 640x200][ 4][ 64] [Interlace 320x400][ 8][128] [Hi-Res 640x400][ 16][256] Out of: [NTSC~PAL] [4096~16M] Resize Page:[Yes~No] [Cancel][OK][Make Default] \"\"\", \"\", mouse_pixel_mapper=config.get_mouse_pixel_pos, font=config.font) if config.display_mode & config.PAL_MONITOR_ID==config.PAL_MONITOR_ID: aspect=2 else: aspect=1 cdepth=config.color_depth global bdepth bdepth=int(math.log(config.NUM_COLORS,2)) gDepth=[] for g in req.gadgets: if g.label.lstrip() in['2','4','8','16','32','64','128','256']: gDepth.append(g) gDepth.sort(key=lambda g: g.label) gNTSC=req.gadget_id(\"0_7\") gPAL=req.gadget_id(\"5_7\") g12bit=req.gadget_id(\"21_7\") g24bit=req.gadget_id(\"26_7\") if config.display_mode & config.MODE_HIRES: if config.display_mode & config.MODE_LACE: res=3 else: res=1 else: if config.display_mode & config.MODE_LACE: res=2 else: res=0 gres=[] for g in req.gadgets: if re.search(r'\\dx\\d\\d\\d$', g.label): gres.append(g) gDepth[bdepth-1].state=1 resize_page=False gResize=[None, None] gResize[1]=req.gadget_id(\"13_9\") gResize[0]=req.gadget_id(\"17_9\") if new_clicked: gResize[0].enabled=False gResize[1].enabled=False for i in range(0,2): gResize[i].state=(resize_page==(i==1)) gResize[i].need_redraw=True def apply_aspect(): if aspect==1: gNTSC.state=1 for g in gres: g.label=g.label.replace(\"256\", \"200\") g.label=g.label.replace(\"512\", \"400\") g.need_redraw=True else: gPAL.state=1 for g in gres: g.label=g.label.replace(\"200\", \"256\") g.label=g.label.replace(\"400\", \"512\") g.need_redraw=True apply_aspect() def apply_cdepth(): if cdepth==16: g12bit.state=1 gDepth[5].label=\"EHB\" gDepth[5].need_redraw=True gDepth[6].enabled=False gDepth[6].need_redraw=True gDepth[7].enabled=False gDepth[7].need_redraw=True else: g24bit.state=1 gDepth[4].enabled=True gDepth[4].need_redraw=True gDepth[5].enabled=True gDepth[5].need_redraw=True gDepth[5].label=\" 64\" gDepth[5].need_redraw=True gDepth[6].enabled=True gDepth[6].need_redraw=True gDepth[7].enabled=True gDepth[7].need_redraw=True apply_cdepth() def apply_bdepth(): gDepth[bdepth-1].state=1 gDepth[bdepth-1].need_redraw=True def apply_mode(): global bdepth if cdepth==16 and res in[1,3]: if bdepth > 4: bdepth=4 apply_bdepth() gDepth[4].enabled=False gDepth[4].need_redraw=True gDepth[5].enabled=False gDepth[5].need_redraw=True elif cdepth==16 and res in[0,2]: if bdepth > 6: bdepth=6 apply_bdepth() gDepth[4].enabled=True gDepth[4].need_redraw=True gDepth[5].enabled=True gDepth[5].need_redraw=True gres[res].state=1 apply_mode() def get_top_colors(num_colors): surf_array=pygame.surfarray.pixels2d(config.pixel_canvas) unique_colors, counts_colors=np.unique(surf_array, return_counts=True) surf_array=None hist_array=np.asarray((unique_colors, counts_colors)).transpose() sorted_hist_array=hist_array[np.argsort(-hist_array[:, 1])] colorlist=np.sort(sorted_hist_array[0:num_colors, 0]) if colorlist[0] !=0: colorlist=np.sort(sorted_hist_array[0:num_colors-1, 0]) colorlist=np.insert(colorlist, 0, 0) return colorlist def get_top_pal(pal, colorlist, num_colors, halfbright): num_top_colors=num_colors if halfbright: num_top_colors=32 newpal=[] for i in range(0,num_top_colors): if i < len(colorlist): newpal.append(pal[colorlist[i]]) else: newpal.append(pal[i]) if halfbright: for i in range(0,32): newpal.append(((newpal[i][0] & 0xee) \/\/ 2, (newpal[i][1] & 0xee) \/\/ 2, (newpal[i][2] & 0xee) \/\/ 2)) for i in range(num_colors,256): newpal.append(newpal[0]) return newpal req.center(screen) config.pixel_req_rect=req.get_screen_rect() req.draw(screen) config.recompose() running=1 reinit=False ok_clicked=False while running: event=pygame.event.wait() gevents=req.process_event(screen, event) if event.type==KEYDOWN and event.key==K_ESCAPE: running=0 for ge in gevents: if ge.gadget==gNTSC: aspect=1 apply_aspect() elif ge.gadget==gPAL: aspect=2 apply_aspect() elif ge.gadget in gres: for i in range(len(gres)): if ge.gadget==gres[i]: res=i apply_mode() elif ge.gadget in gDepth: for i in range(len(gDepth)): if ge.gadget==gDepth[i]: bdepth=i+1 elif ge.gadget==g12bit: cdepth=16 apply_cdepth() apply_mode() elif ge.gadget==g24bit: cdepth=256 apply_cdepth() apply_mode() elif ge.gadget in gResize: resize_page=(gResize.index(ge.gadget)==1) if ge.gadget.type==Gadget.TYPE_BOOL: if ge.gadget.label in[\"OK\",\"Make Default\"] and not req.has_error(): ok_clicked=True num_colors=2**bdepth if bdepth==6 and cdepth==16: halfbright=True else: halfbright=False if new_clicked: reinit=True elif num_colors < config.NUM_COLORS: reinit=False num_top_colors=num_colors if halfbright: num_top_colors=32 colorlist=get_top_colors(num_top_colors) newpal=get_top_pal(config.pal, colorlist, num_colors, halfbright) new_pixel_canvas=pygame.Surface((config.pixel_width, config.pixel_height),0,8) new_pixel_canvas.set_palette(newpal) new_pixel_canvas.blit(config.pixel_canvas,(0,0)) config.pixel_canvas=new_pixel_canvas config.truepal=get_top_pal(config.truepal, colorlist, num_colors, halfbright)[0:num_colors] config.truepal=config.quantize_palette(config.truepal, cdepth) config.pal=list(config.truepal) config.pal=config.unique_palette(config.pal) config.backuppal=list(config.pal) config.pixel_canvas.set_palette(config.pal) elif num_colors==config.NUM_COLORS: reinit=False elif num_colors > config.NUM_COLORS: reinit=False config.truepal=config.quantize_palette(config.truepal, cdepth) newpal=config.get_default_palette(num_colors) config.truepal.extend(newpal[config.NUM_COLORS:num_colors]) if halfbright: for i in range(0,32): config.truepal[i+32]=\\ ((config.truepal[i][0] & 0xee) \/\/ 2, (config.truepal[i][1] & 0xee) \/\/ 2, (config.truepal[i][2] & 0xee) \/\/ 2) config.pal=list(config.truepal) config.pal=config.unique_palette(config.pal) config.backuppal=list(config.pal) config.pixel_canvas.set_palette(config.pal) dmode=0 px=320 py=200 if res in[1,3]: dmode |=config.MODE_HIRES px *=2 if res in[2,3]: dmode |=config.MODE_LACE py *=2 if aspect==1: dmode |=config.NTSC_MONITOR_ID else: dmode |=config.PAL_MONITOR_ID py=py * 128 \/\/ 100 if halfbright: dmode |=config.MODE_EXTRA_HALFBRIGHT if not new_clicked and(px !=config.pixel_width or py !=config.pixel_height): new_pixel_canvas=pygame.transform.scale(config.pixel_canvas,(px, py)) new_pixel_canvas.set_palette(config.pal) config.pixel_canvas=new_pixel_canvas reinit=False config.display_mode=dmode config.pixel_width=px config.pixel_height=py config.color_depth=cdepth config.NUM_COLORS=num_colors if ge.gadget.label==\"Make Default\": config.saveConfig() running=0 elif ge.gadget.label==\"Cancel\": running=0 apply_bdepth() if aspect==1: gNTSC.state=1 else: gPAL.state=1 gres[res].state=1 if cdepth==16: g12bit.state=1 else: g24bit.state=1 if resize_page: gResize[1].state=1 else: gResize[0].state=1 if running and not pygame.event.peek((KEYDOWN, KEYUP, MOUSEBUTTONDOWN, MOUSEBUTTONUP, MOUSEMOTION, VIDEORESIZE)): req.draw(screen) config.recompose() config.pixel_req_rect=None if ok_clicked: config.initialize_surfaces(reinit=reinit) else: config.recompose() return def page_size_req(screen): req=str2req(\"Set Page Size\", \"\"\" Type in size: Width:_____ Height:_____ Or select one: [Standard xxx x yyy] [Full Page xxx x yyy] [Overscan xxx x yyy] [Cancel][OK] \"\"\", \"\", mouse_pixel_mapper=config.get_mouse_pixel_pos, font=config.font) req.center(screen) config.pixel_req_rect=req.get_screen_rect() widthg=req.gadget_id(\"7_1\") heightg=req.gadget_id(\"20_1\") standardg=req.gadget_id(\"1_4\") fullpageg=req.gadget_id(\"1_5\") overscang=req.gadget_id(\"1_6\") pageg=[standardg, fullpageg, overscang] if config.display_mode & config.PAL_MONITOR_ID==config.PAL_MONITOR_ID: page_size=[[320,256],[320,435],[368,283]] else: page_size=[[320,200],[320,340],[362,241]] widthg.value=str(config.pixel_width) widthg.numonly=True heightg.value=str(config.pixel_height) heightg.numonly=True if config.display_mode & config.MODE_HIRES: for i in range(0,3): page_size[i][0] *=2 if config.display_mode & config.MODE_LACE: for i in range(0,3): page_size[i][1] *=2 for i in range(0,3): pageg[i].label=pageg[i].label.replace(\"xxx\", str(page_size[i][0])) pageg[i].label=pageg[i].label.replace(\"yyy\", str(page_size[i][1])) req.draw(screen) config.recompose() running=1 while running: event=pygame.event.wait() gevents=req.process_event(screen, event) if event.type==KEYDOWN and event.key==K_ESCAPE: running=0 for ge in gevents: if ge.gadget.type==Gadget.TYPE_BOOL: if ge.gadget.label==\"OK\" and not req.has_error(): config.resize_canvas(int(widthg.value), int(heightg.value)) running=0 elif ge.gadget.label==\"Cancel\": running=0 elif ge.gadget in pageg: i=pageg.index(ge.gadget) widthg.value=str(page_size[i][0]) widthg.need_redraw=True heightg.value=str(page_size[i][1]) heightg.need_redraw=True if not pygame.event.peek((KEYDOWN, KEYUP, MOUSEBUTTONDOWN, MOUSEBUTTONUP, MOUSEMOTION, VIDEORESIZE)): req.draw(screen) config.recompose() config.pixel_req_rect=None config.recompose() return class PreviewPic(Gadget): def __init__(self, type, label, rect, value=None, maxvalue=None, id=None): self.pic=config.pixel_canvas.convert() super(PreviewPic, self).__init__(type, label, rect, value, maxvalue, id) def draw(self, screen, font, offset=(0,0), fgcolor=(0,0,0), bgcolor=(160,160,160), hcolor=(208,208,224)): self.visible=True x,y,w,h=self.rect xo, yo=offset self.offsetx=xo self.offsety=yo self.screenrect=(x+xo,y+yo,w,h) if self.type==Gadget.TYPE_CUSTOM: if not self.need_redraw: return self.need_redraw=False if self.label==\" if config.pixel_width \/ w > config.pixel_height \/ h: newh=int(config.pixel_height * w \/ config.pixel_width) yo +=(h -newh) \/\/ 2 h=newh else: neww=int(config.pixel_width * h \/ config.pixel_height) xo +=(w -neww) \/\/ 2 w=neww screen.set_clip(self.screenrect) screen.blit(pygame.transform.smoothscale(self.pic,(w, h)),(x+xo,y+yo)) else: super(PreviewPic, self).draw(screen, font, offset) def page_preview_req(screen): req=str2req(\"Page Preview\", \"\"\" [OK] \"\"\", \" req.center(screen) config.pixel_req_rect=req.get_screen_rect() req.draw(screen) config.recompose() running=1 while running: event=pygame.event.wait() gevents=req.process_event(screen, event) if event.type==KEYDOWN and event.key==K_ESCAPE: running=0 for ge in gevents: if ge.gadget.type==Gadget.TYPE_BOOL: if ge.gadget.label==\"OK\": running=0 if running and not pygame.event.peek((KEYDOWN, KEYUP, MOUSEBUTTONDOWN, MOUSEBUTTONUP, MOUSEMOTION, VIDEORESIZE)): req.draw(screen) config.recompose() config.pixel_req_rect=None config.recompose() return class PPpic(Gadget): def __init__(self, type, label, rect, value=None, maxvalue=None, id=None): self.pic=imgload('logo.png') super(PPpic, self).__init__(type, label, rect, value, maxvalue, id) def draw(self, screen, font, offset=(0,0), fgcolor=(0,0,0), bgcolor=(160,160,160), hcolor=(208,208,224)): self.visible=True x,y,w,h=self.rect xo, yo=offset self.offsetx=xo self.offsety=yo self.screenrect=(x+xo,y+yo,w,h) if self.type==Gadget.TYPE_CUSTOM: if not self.need_redraw: return self.need_redraw=False if self.label==\" screen.set_clip(self.screenrect) screen.blit(pygame.transform.smoothscale(self.pic,(w, h)),(x+xo,y+yo)) else: super(PPpic, self).draw(screen, font, offset) def about_req(screen): req=str2req(\"About\", \"\"\" PyDPainter \\xA92022 Mark Riale Version %-8s Licensed under GPL 3 or later. See LICENSE for more details. [OK] \"\"\" %(config.version), \" req.center(screen) config.pixel_req_rect=req.get_screen_rect() req.draw(screen) config.recompose() running=1 while running: event=pygame.event.wait() gevents=req.process_event(screen, event) if event.type==KEYDOWN and event.key==K_ESCAPE: running=0 for ge in gevents: if ge.gadget.type==Gadget.TYPE_BOOL: if ge.gadget.label==\"OK\": running=0 if running and not pygame.event.peek((KEYDOWN, KEYUP, MOUSEBUTTONDOWN, MOUSEBUTTONUP, MOUSEMOTION, VIDEORESIZE)): req.draw(screen) config.recompose() config.pixel_req_rect=None config.recompose() return ","sourceWithComments":"#!\/usr\/bin\/python3\n# vim: tabstop=8 expandtab shiftwidth=4 softtabstop=4\n\nimport os.path, colorsys\n\nimport gadget\nfrom gadget import *\n\nfrom prim import *\n\nimport contextlib\nwith contextlib.redirect_stdout(None):\n    import pygame\n    from pygame.locals import *\n\nconfig = None\n\ndef menureq_set_config(config_in):\n    global config\n    config = config_in\n\n#Read in directory and return sorted list\ndef get_dir(path):\n    filelist = []\n    dirlist = [\".. (parent dir)\"]\n    try:\n        with os.scandir(path) as it:\n            for entry in it:\n                if not entry.name.startswith('.'):\n                    if entry.is_file():\n                        filelist.append(entry.name)\n                    elif entry.is_dir():\n                        dirlist.append(\"\\x92\\x93\" + entry.name)\n        filelist.sort(key=str.casefold)\n        dirlist.sort(key=str.casefold)\n    except FileNotFoundError:\n        dirlist = [\"<Not found>\"]\n    return dirlist + filelist\n\ndef file_req(screen, title, action_label, filepath, filename):\n    req = str2req(title, \"\"\"\nPath:_________________________\n############################^^\n############################@@\n############################@@\n############################@@\n############################@@\n############################@@\n############################@@\n############################@@\n############################@@\n############################^^\nFile:_________________________\n[%s][Cancel]\n\"\"\"%(action_label), \"#^@\", mouse_pixel_mapper=config.get_mouse_pixel_pos, custom_gadget_type=ListGadget, font=config.font)\n    req.center(screen)\n    config.pixel_req_rect = req.get_screen_rect()\n\n    retval = \"\"\n\n    #list items\n    list_itemsg = req.gadget_id(\"0_1\")\n    list_itemsg.items = get_dir(filepath)\n    list_itemsg.top_item = 0\n    list_itemsg.value = list_itemsg.top_item\n\n    #list up\/down arrows\n    list_upg = req.gadget_id(\"28_1\")\n    list_upg.value = -1\n    list_downg = req.gadget_id(\"28_10\")\n    list_downg.value = 1\n\n    #list slider\n    list_sliderg = req.gadget_id(\"28_2\")\n    list_sliderg.value = list_itemsg.top_item\n\n    #all list item gadgets\n    listg_list = [list_itemsg, list_upg, list_downg, list_sliderg]\n    list_itemsg.listgadgets = listg_list\n    list_upg.listgadgets = listg_list\n    list_downg.listgadgets = listg_list\n    list_sliderg.listgadgets = listg_list\n\n    #File path\n    file_pathg = req.gadget_id(\"5_0\")\n    file_pathg.value = filepath\n    file_pathg.maxvalue = 255\n\n    #File name\n    filename = os.path.basename(filename)\n    file_nameg = req.gadget_id(\"5_11\")\n    file_nameg.value = filename\n    file_nameg.maxvalue = 255\n\n    #take care of non-square pixels\n    fontmult = 1\n    if config.aspectX != config.aspectY:\n        fontmult = 2\n\n    req.draw(screen)\n    config.recompose()\n\n    last_click_ms = pygame.time.get_ticks()\n\n    running = 1\n    wait_for_mouseup = 0\n\n    while running or wait_for_mouseup:\n        event = pygame.event.wait()\n        gevents = req.process_event(screen, event)\n\n        if event.type == KEYDOWN and event.key == K_ESCAPE:\n            running = 0\n\n        for ge in gevents:\n            if ge.gadget.type == Gadget.TYPE_BOOL:\n                if ge.gadget.label == action_label:\n                    if file_nameg.value != \"\":\n                        retval = os.path.join(filepath, file_nameg.value)\n                    running = 0\n                elif ge.gadget.label == \"Cancel\":\n                    running = 0\n            if ge.gadget.type == Gadget.TYPE_STRING:\n                if ge.type == ge.TYPE_GADGETUP and ge.gadget == file_pathg:\n                    filepath = file_pathg.value\n                    list_itemsg.items = get_dir(filepath)\n                    list_itemsg.top_item = 0\n                    list_itemsg.value = list_itemsg.top_item\n                    list_itemsg.need_redraw = True\n\n        if not pygame.event.peek((KEYDOWN, KEYUP, MOUSEBUTTONDOWN, MOUSEBUTTONUP, MOUSEMOTION, VIDEORESIZE)):\n            if event.type == MOUSEBUTTONDOWN and event.button == 1 and list_itemsg.pointin(config.get_mouse_pixel_pos(event), list_itemsg.screenrect):\n                filename = list_itemsg.items[list_itemsg.value]\n                if len(filename) > 2 and (filename[0:2] == \"\\x92\\x93\" or filename[0:2] == \"..\"):\n                    if filename[0:2] == \"\\x92\\x93\":\n                        filepath = os.path.join(filepath, filename[2:])\n                    elif filename[0:2] == \"..\":\n                        filepath = os.path.abspath(os.path.join(filepath, \"..\"))\n                    list_itemsg.items = get_dir(filepath)\n                    list_itemsg.top_item = 0\n                    list_itemsg.value = list_itemsg.top_item\n                    list_itemsg.need_redraw = True\n                    file_pathg.value = filepath\n                    file_pathg.need_redraw = True\n                else:\n                    file_nameg.value = filename\n                    file_nameg.need_redraw = True\n                    if pygame.time.get_ticks() - last_click_ms < 500:\n                        if file_nameg.value != \"\":\n                            retval = os.path.join(filepath, file_nameg.value)\n                        running = 0\n                        wait_for_mouseup = 1\n                    else:\n                        last_click_ms = pygame.time.get_ticks()\n            elif event.type == MOUSEBUTTONUP and event.button == 1:\n                wait_for_mouseup = 0\n\n            req.draw(screen)\n            config.recompose()\n\n    config.pixel_req_rect = None\n    config.recompose()\n\n    return retval\n\n\ndef screen_format_req(screen, new_clicked=False):\n    req = str2req(\"Choose Screen Format\", \"\"\"\n                     Number of\n      Format:         Colors:\n[Lo-Res     320x200] [  2][ 32]\n[Med-Res    640x200] [  4][ 64]\n[Interlace  320x400] [  8][128]\n[Hi-Res     640x400] [ 16][256]\n                      Out of:\n[NTSC~PAL]           [4096~16M]\n\nResize Page: [Yes~No]\n\n[Cancel][OK][Make Default]\n\"\"\", \"\", mouse_pixel_mapper=config.get_mouse_pixel_pos, font=config.font)\n\n    #Get PAL\/NTSC from current display mode\n    if config.display_mode & config.PAL_MONITOR_ID == config.PAL_MONITOR_ID:\n        aspect = 2\n    else:\n        aspect = 1\n\n    #Get color depth (16 = 4096 colors and 256 = 16M colors)\n    cdepth = config.color_depth\n\n    #Get bitplane depth from current number of colors\n    global bdepth\n    bdepth = int(math.log(config.NUM_COLORS,2))\n\n    #Gather bitplane depth gadgets and sort in numeric order\n    gDepth = []\n    for g in req.gadgets:\n        if g.label.lstrip() in ['2','4','8','16','32','64','128','256']:\n            gDepth.append(g)\n    gDepth.sort(key=lambda g: g.label)\n\n    #Gather NTSC\/PAL gadgets\n    gNTSC = req.gadget_id(\"0_7\")\n    gPAL = req.gadget_id(\"5_7\")\n\n    #Gather (OCS)ECS\/AGA gadgets\n    g12bit = req.gadget_id(\"21_7\")\n    g24bit = req.gadget_id(\"26_7\")\n\n    #Get resolution from current screen mode\n    if config.display_mode & config.MODE_HIRES:\n        if config.display_mode & config.MODE_LACE:\n            res = 3\n        else:\n            res = 1\n    else:\n        if config.display_mode & config.MODE_LACE:\n            res = 2\n        else:\n            res = 0\n\n    #Gather screen mode gadgets\n    gres = []\n    for g in req.gadgets:\n        if re.search(r'\\dx\\d\\d\\d$', g.label):\n            gres.append(g)\n\n    gDepth[bdepth-1].state = 1\n\n    #Gather page resize gadgets\n    resize_page = False\n    gResize = [None, None]\n    gResize[1] = req.gadget_id(\"13_9\") #Yes\n    gResize[0] = req.gadget_id(\"17_9\") #No\n    if new_clicked:\n        gResize[0].enabled = False\n        gResize[1].enabled = False\n\n    for i in range(0,2):\n        gResize[i].state = (resize_page == (i == 1))\n        gResize[i].need_redraw = True\n\n    def apply_aspect():\n        if aspect == 1:\n            gNTSC.state = 1\n            for g in gres:\n                g.label = g.label.replace(\"256\", \"200\")\n                g.label = g.label.replace(\"512\", \"400\")\n                g.need_redraw = True\n        else:\n            gPAL.state = 1\n            for g in gres:\n                g.label = g.label.replace(\"200\", \"256\")\n                g.label = g.label.replace(\"400\", \"512\")\n                g.need_redraw = True\n    apply_aspect()\n\n    def apply_cdepth():\n        if cdepth == 16:\n            g12bit.state = 1\n            gDepth[5].label = \"EHB\"\n            gDepth[5].need_redraw = True\n            gDepth[6].enabled = False\n            gDepth[6].need_redraw = True\n            gDepth[7].enabled = False\n            gDepth[7].need_redraw = True\n        else:\n            g24bit.state = 1\n            gDepth[4].enabled = True\n            gDepth[4].need_redraw = True\n            gDepth[5].enabled = True\n            gDepth[5].need_redraw = True\n            gDepth[5].label = \" 64\"\n            gDepth[5].need_redraw = True\n            gDepth[6].enabled = True\n            gDepth[6].need_redraw = True\n            gDepth[7].enabled = True\n            gDepth[7].need_redraw = True\n    apply_cdepth()\n\n    def apply_bdepth():\n        gDepth[bdepth-1].state = 1\n        gDepth[bdepth-1].need_redraw = True\n\n    def apply_mode():\n        global bdepth\n        #Apply limits to screen mode\/bitplane depth\n        if cdepth == 16 and res in [1,3]:\n            if bdepth > 4:\n                bdepth = 4\n                apply_bdepth()\n            gDepth[4].enabled = False\n            gDepth[4].need_redraw = True\n            gDepth[5].enabled = False\n            gDepth[5].need_redraw = True\n        elif cdepth == 16 and res in [0,2]:\n            if bdepth > 6:\n                bdepth = 6\n                apply_bdepth()\n            gDepth[4].enabled = True\n            gDepth[4].need_redraw = True\n            gDepth[5].enabled = True\n            gDepth[5].need_redraw = True\n        gres[res].state = 1\n    apply_mode()\n\n    def get_top_colors(num_colors):\n        surf_array = pygame.surfarray.pixels2d(config.pixel_canvas)\n        #find unique color indexes and counts of color indexes in pic\n        unique_colors, counts_colors = np.unique(surf_array, return_counts=True)\n        surf_array = None\n        #put counts and color indexes into matrix together into histogram\n        hist_array = np.asarray((unique_colors, counts_colors)).transpose()\n        #print(hist_array)\n        #sort histogram descending by frequency\n        sorted_hist_array = hist_array[np.argsort(-hist_array[:, 1])]\n        #print(sorted_hist_array)\n        #take first num_colors indexes\n        colorlist = np.sort(sorted_hist_array[0:num_colors, 0])\n        #make sure to preserve color 0\n        if colorlist[0] != 0:\n            colorlist = np.sort(sorted_hist_array[0:num_colors-1, 0])\n            colorlist = np.insert(colorlist, 0, 0)\n        #print(colorlist)\n        return colorlist\n\n    def get_top_pal(pal, colorlist, num_colors, halfbright):\n        num_top_colors = num_colors\n        if halfbright:\n            num_top_colors = 32\n\n        #assign top colors to new palette\n        newpal = []\n        for i in range(0,num_top_colors):\n            if i < len(colorlist):\n                newpal.append(pal[colorlist[i]])\n            else:\n                newpal.append(pal[i])\n\n        #adjust halfbright colors\n        if halfbright:\n            for i in range(0,32):\n                newpal.append(((newpal[i][0] & 0xee) \/\/ 2,\n                               (newpal[i][1] & 0xee) \/\/ 2,\n                               (newpal[i][2] & 0xee) \/\/ 2))\n\n        #fill in upper palette with background color\n        for i in range(num_colors,256):\n            newpal.append(newpal[0])\n\n        return newpal\n\n\n    req.center(screen)\n    config.pixel_req_rect = req.get_screen_rect()\n    req.draw(screen)\n    config.recompose()\n\n    running = 1\n    reinit = False\n    ok_clicked = False\n    while running:\n        event = pygame.event.wait()\n        gevents = req.process_event(screen, event)\n\n        if event.type == KEYDOWN and event.key == K_ESCAPE:\n            running = 0 \n\n        for ge in gevents:\n            if ge.gadget == gNTSC:\n                aspect = 1\n                apply_aspect()\n            elif ge.gadget == gPAL:\n                aspect = 2\n                apply_aspect()\n            elif ge.gadget in gres:\n                for i in range(len(gres)):\n                    if ge.gadget == gres[i]:\n                        res = i\n                apply_mode()\n            elif ge.gadget in gDepth:\n                for i in range(len(gDepth)):\n                    if ge.gadget == gDepth[i]:\n                        bdepth = i+1\n            elif ge.gadget == g12bit:\n                cdepth = 16\n                apply_cdepth()\n                apply_mode()\n            elif ge.gadget == g24bit:\n                cdepth = 256\n                apply_cdepth()\n                apply_mode()\n            elif ge.gadget in gResize:\n                resize_page = (gResize.index(ge.gadget) == 1)\n            if ge.gadget.type == Gadget.TYPE_BOOL:\n                if ge.gadget.label in [\"OK\",\"Make Default\"] and not req.has_error():\n                    ok_clicked = True\n                    num_colors = 2**bdepth\n                    if bdepth == 6 and cdepth == 16:\n                        halfbright = True\n                    else:\n                        halfbright = False\n\n                    if new_clicked:\n                        reinit = True\n                    elif num_colors < config.NUM_COLORS:\n                        #reduce palette to higest frequency color indexes\n                        reinit = False\n\n                        num_top_colors = num_colors\n                        if halfbright:\n                            num_top_colors = 32\n\n                        colorlist = get_top_colors(num_top_colors)\n\n                        newpal = get_top_pal(config.pal, colorlist, num_colors, halfbright)\n\n                        #convert colors to reduced palette using blit\n                        new_pixel_canvas = pygame.Surface((config.pixel_width, config.pixel_height),0,8)\n                        new_pixel_canvas.set_palette(newpal)\n                        new_pixel_canvas.blit(config.pixel_canvas, (0,0))\n\n                        #substitute new canvas for the higher color one\n                        config.pixel_canvas = new_pixel_canvas\n                        config.truepal = get_top_pal(config.truepal, colorlist, num_colors, halfbright)[0:num_colors]\n                        config.truepal = config.quantize_palette(config.truepal, cdepth)\n                        config.pal = list(config.truepal)\n                        config.pal = config.unique_palette(config.pal)\n                        config.backuppal = list(config.pal)\n                        config.pixel_canvas.set_palette(config.pal)\n                    elif num_colors == config.NUM_COLORS:\n                        reinit = False\n                    elif num_colors > config.NUM_COLORS:\n                        reinit = False\n\n                        config.truepal = config.quantize_palette(config.truepal, cdepth)\n                        newpal = config.get_default_palette(num_colors)\n                        config.truepal.extend(newpal[config.NUM_COLORS:num_colors])\n                        if halfbright:\n                            for i in range(0,32):\n                                config.truepal[i+32] = \\\n                                          ((config.truepal[i][0] & 0xee) \/\/ 2,\n                                           (config.truepal[i][1] & 0xee) \/\/ 2,\n                                           (config.truepal[i][2] & 0xee) \/\/ 2)\n                        config.pal = list(config.truepal)\n                        config.pal = config.unique_palette(config.pal)\n                        config.backuppal = list(config.pal)\n                        config.pixel_canvas.set_palette(config.pal)\n\n                    dmode = 0\n                    px = 320\n                    py = 200\n                    if res in [1,3]:\n                        dmode |= config.MODE_HIRES\n                        px *= 2\n                    if res in [2,3]:\n                        dmode |= config.MODE_LACE\n                        py *= 2\n\n                    if aspect == 1:\n                        dmode |= config.NTSC_MONITOR_ID\n                    else:\n                        dmode |= config.PAL_MONITOR_ID\n                        py = py * 128 \/\/ 100\n\n                    if halfbright:\n                        dmode |= config.MODE_EXTRA_HALFBRIGHT\n\n                    if not new_clicked and (px != config.pixel_width or py != config.pixel_height):\n                        new_pixel_canvas = pygame.transform.scale(config.pixel_canvas, (px, py))\n                        new_pixel_canvas.set_palette(config.pal)\n                        config.pixel_canvas = new_pixel_canvas\n                        reinit = False\n\n                    config.display_mode = dmode\n                    config.pixel_width = px\n                    config.pixel_height = py\n                    config.color_depth = cdepth\n                    config.NUM_COLORS = num_colors\n                    if ge.gadget.label == \"Make Default\":\n                        config.saveConfig()\n                    running = 0\n                elif ge.gadget.label == \"Cancel\":\n                    running = 0 \n\n        apply_bdepth()\n\n        if aspect == 1:\n            gNTSC.state = 1\n        else:\n            gPAL.state = 1\n\n        gres[res].state = 1\n\n        if cdepth == 16:\n            g12bit.state = 1\n        else:\n            g24bit.state = 1\n\n        if resize_page:\n            gResize[1].state = 1\n        else:\n            gResize[0].state = 1\n\n        if running and not pygame.event.peek((KEYDOWN, KEYUP, MOUSEBUTTONDOWN, MOUSEBUTTONUP, MOUSEMOTION, VIDEORESIZE)):\n            req.draw(screen)\n            config.recompose()\n\n    config.pixel_req_rect = None\n    if ok_clicked:\n        config.initialize_surfaces(reinit=reinit)\n    else:\n        config.recompose()\n\n    return\n\n\ndef page_size_req(screen):\n    req = str2req(\"Set Page Size\", \"\"\"\nType in size:\n Width:_____ Height:_____\n\nOr select one:\n [Standard     xxx x yyy]\n [Full Page    xxx x yyy]\n [Overscan     xxx x yyy]\n\n[Cancel][OK]\n\"\"\", \"\", mouse_pixel_mapper=config.get_mouse_pixel_pos, font=config.font)\n    req.center(screen)\n    config.pixel_req_rect = req.get_screen_rect()\n\n    widthg = req.gadget_id(\"7_1\")\n    heightg = req.gadget_id(\"20_1\")\n    standardg = req.gadget_id(\"1_4\")\n    fullpageg = req.gadget_id(\"1_5\")\n    overscang = req.gadget_id(\"1_6\")\n\n    pageg = [standardg, fullpageg, overscang]\n\n    if config.display_mode & config.PAL_MONITOR_ID == config.PAL_MONITOR_ID:\n        page_size = [[320,256],[320,435],[368,283]]\n    else:\n        page_size = [[320,200],[320,340],[362,241]]\n\n    widthg.value = str(config.pixel_width)\n    widthg.numonly = True\n    heightg.value = str(config.pixel_height)\n    heightg.numonly = True\n\n    if config.display_mode & config.MODE_HIRES:\n        for i in range(0,3):\n            page_size[i][0] *= 2\n\n    if config.display_mode & config.MODE_LACE:\n        for i in range(0,3):\n            page_size[i][1] *= 2\n\n    # Populate buttons from page_size array\n    for i in range(0,3):\n        pageg[i].label = pageg[i].label.replace(\"xxx\", str(page_size[i][0]))\n        pageg[i].label = pageg[i].label.replace(\"yyy\", str(page_size[i][1]))\n\n    req.draw(screen)\n    config.recompose()\n\n    running = 1\n    while running:\n        event = pygame.event.wait()\n        gevents = req.process_event(screen, event)\n\n        if event.type == KEYDOWN and event.key == K_ESCAPE:\n            running = 0\n\n        for ge in gevents:\n            if ge.gadget.type == Gadget.TYPE_BOOL:\n                if ge.gadget.label == \"OK\" and not req.has_error():\n                    config.resize_canvas(int(widthg.value), int(heightg.value))\n                    running = 0\n                elif ge.gadget.label == \"Cancel\":\n                    running = 0\n                elif ge.gadget in pageg:\n                    i = pageg.index(ge.gadget)\n                    widthg.value = str(page_size[i][0])\n                    widthg.need_redraw = True\n                    heightg.value = str(page_size[i][1])\n                    heightg.need_redraw = True\n\n        if not pygame.event.peek((KEYDOWN, KEYUP, MOUSEBUTTONDOWN, MOUSEBUTTONUP, MOUSEMOTION, VIDEORESIZE)):\n            req.draw(screen)\n            config.recompose()\n\n    config.pixel_req_rect = None\n    config.recompose()\n\n    return\n\nclass PreviewPic(Gadget):\n    def __init__(self, type, label, rect, value=None, maxvalue=None, id=None):\n        self.pic = config.pixel_canvas.convert()\n        super(PreviewPic, self).__init__(type, label, rect, value, maxvalue, id)\n \n    def draw(self, screen, font, offset=(0,0), fgcolor=(0,0,0), bgcolor=(160,160,160), hcolor=(208,208,224)):\n        self.visible = True\n        x,y,w,h = self.rect\n        xo, yo = offset\n        self.offsetx = xo\n        self.offsety = yo\n        self.screenrect = (x+xo,y+yo,w,h)\n\n        if self.type == Gadget.TYPE_CUSTOM:\n            if not self.need_redraw:\n                return\n\n            self.need_redraw = False\n\n            if self.label == \"#\":\n                if config.pixel_width \/ w > config.pixel_height \/ h:\n                    # Scale to max width and center height\n                    newh = int(config.pixel_height * w \/ config.pixel_width)\n                    yo += (h - newh) \/\/ 2\n                    h = newh\n                else:\n                    # Scale to max height and center width\n                    neww = int(config.pixel_width * h \/ config.pixel_height)\n                    xo += (w - neww) \/\/ 2\n                    w = neww\n                screen.set_clip(self.screenrect)\n                screen.blit(pygame.transform.smoothscale(self.pic, (w, h)), (x+xo,y+yo))\n        else:\n            super(PreviewPic, self).draw(screen, font, offset)\n\ndef page_preview_req(screen):\n    req = str2req(\"Page Preview\", \"\"\"\n###############################\n###############################\n###############################\n###############################\n###############################\n###############################\n###############################\n###############################\n###############################\n###############################\n###############################\n###############################\n              [OK]\n\"\"\", \"#\", mouse_pixel_mapper=config.get_mouse_pixel_pos, custom_gadget_type=PreviewPic, font=config.font)\n\n    req.center(screen)\n    config.pixel_req_rect = req.get_screen_rect()\n    req.draw(screen)\n    config.recompose()\n\n    running = 1\n    while running:\n        event = pygame.event.wait()\n        gevents = req.process_event(screen, event)\n\n        if event.type == KEYDOWN and event.key == K_ESCAPE:\n            running = 0\n\n        for ge in gevents:\n            if ge.gadget.type == Gadget.TYPE_BOOL:\n                if ge.gadget.label == \"OK\":\n                    running = 0\n\n        if running and not pygame.event.peek((KEYDOWN, KEYUP, MOUSEBUTTONDOWN, MOUSEBUTTONUP, MOUSEMOTION, VIDEORESIZE)):\n            req.draw(screen)\n            config.recompose()\n\n    config.pixel_req_rect = None\n    config.recompose()\n\n    return\n\n\nclass PPpic(Gadget):\n    def __init__(self, type, label, rect, value=None, maxvalue=None, id=None):\n        self.pic = imgload('logo.png')\n        super(PPpic, self).__init__(type, label, rect, value, maxvalue, id)\n        \n    def draw(self, screen, font, offset=(0,0), fgcolor=(0,0,0), bgcolor=(160,160,160), hcolor=(208,208,224)):\n        self.visible = True\n        x,y,w,h = self.rect\n        xo, yo = offset\n        self.offsetx = xo\n        self.offsety = yo\n        self.screenrect = (x+xo,y+yo,w,h)\n\n        if self.type == Gadget.TYPE_CUSTOM:\n            if not self.need_redraw:\n                return\n\n            self.need_redraw = False\n\n            if self.label == \"#\":\n                screen.set_clip(self.screenrect)\n                screen.blit(pygame.transform.smoothscale(self.pic, (w, h)), (x+xo,y+yo))\n        else:\n            super(PPpic, self).draw(screen, font, offset)\n\ndef about_req(screen):\n    req = str2req(\"About\", \"\"\"\nPyDPainter       ############\n\\xA92022 Mark Riale ############\nVersion %-8s ############\n                 ############\nLicensed under   ############\nGPL 3 or later.  ############\nSee LICENSE for  ############\nmore details.    ############\n             [OK]\n\"\"\" % (config.version), \"#\", mouse_pixel_mapper=config.get_mouse_pixel_pos, custom_gadget_type=PPpic, font=config.font)\n\n    req.center(screen)\n    config.pixel_req_rect = req.get_screen_rect()\n    req.draw(screen)\n    config.recompose()\n\n    running = 1\n    while running:\n        event = pygame.event.wait()\n        gevents = req.process_event(screen, event)\n\n        if event.type == KEYDOWN and event.key == K_ESCAPE:\n            running = 0 \n\n        for ge in gevents:\n            if ge.gadget.type == Gadget.TYPE_BOOL:\n                if ge.gadget.label == \"OK\":\n                    running = 0 \n\n        if running and not pygame.event.peek((KEYDOWN, KEYUP, MOUSEBUTTONDOWN, MOUSEBUTTONUP, MOUSEMOTION, VIDEORESIZE)):\n            req.draw(screen)\n            config.recompose()\n\n    config.pixel_req_rect = None\n    config.recompose()\n\n    return\n"},"\/libs\/prim.py":{"changes":[{"diff":"\n         config.fillmode.bounds = copy.copy(FillMode.NOBOUNDS)\n         if onscreen((x,y)):\n             surf_array = pygame.surfarray.pixels2d(surface)  # Create an array from the surface.\n-            maxx, maxy = config.screen_width, config.screen_height\n+            maxx, maxy = config.pixel_width, config.pixel_height\n             current_color = surf_array[x,y]\n             if fill_color == current_color:\n                 if config.fillmode.value == FillMode.SOLID:\n","add":1,"remove":1,"filename":"\/libs\/prim.py","badparts":["            maxx, maxy = config.screen_width, config.screen_height"],"goodparts":["            maxx, maxy = config.pixel_width, config.pixel_height"]}]}},"msg":"Implement resize in Screen Format. Fix flood fill."}},"https:\/\/github.com\/natohutch\/floodme":{"8662f5c1696db9902ab974fdf0f7fb8573caea2a":{"url":"https:\/\/api.github.com\/repos\/natohutch\/floodme\/commits\/8662f5c1696db9902ab974fdf0f7fb8573caea2a","html_url":"https:\/\/github.com\/natohutch\/floodme\/commit\/8662f5c1696db9902ab974fdf0f7fb8573caea2a","message":"fix to flood rendering","sha":"8662f5c1696db9902ab974fdf0f7fb8573caea2a","keyword":"flooding fix","diff":"diff --git a\/app\/app.py b\/app\/app.py\nindex 6e3b218..39dca3d 100644\n--- a\/app\/app.py\n+++ b\/app\/app.py\n@@ -257,7 +257,7 @@ def infrastructure():\n  SELECT st_value((SELECT st_setsrid(rast, 4326) FROM hydraulics WHERE filename='PMF'), geom) AS flood_z, * FROM road_aep WHERE current_aep = 'PMF'\r\n  UNION\r\n SELECT null AS flood_z, * FROM road_aep WHERE current_aep IS null)\r\n-SELECT *, flood_z - ground_z, sensor AS flood_depth FROM points_with_levels WHERE flood_z > ground_z) AS t\r\n+SELECT *, flood_z - ground_z AS flood_depth, sensor FROM points_with_levels WHERE flood_z > ground_z) AS t\r\n \"\"\"\r\n         ), {\"time\": time})\r\n         results = result.all()[0][0]\r\n","files":{"\/app\/app.py":{"changes":[{"diff":"\n  SELECT st_value((SELECT st_setsrid(rast, 4326) FROM hydraulics WHERE filename='PMF'), geom) AS flood_z, * FROM road_aep WHERE current_aep = 'PMF'\r\n  UNION\r\n SELECT null AS flood_z, * FROM road_aep WHERE current_aep IS null)\r\n-SELECT *, flood_z - ground_z, sensor AS flood_depth FROM points_with_levels WHERE flood_z > ground_z) AS t\r\n+SELECT *, flood_z - ground_z AS flood_depth, sensor FROM points_with_levels WHERE flood_z > ground_z) AS t\r\n \"\"\"\r\n         ), {\"time\": time})\r\n         results = result.all()[0][0]\r\n","add":1,"remove":1,"filename":"\/app\/app.py","badparts":["SELECT *, flood_z - ground_z, sensor AS flood_depth FROM points_with_levels WHERE flood_z > ground_z) AS t\r"],"goodparts":["SELECT *, flood_z - ground_z AS flood_depth, sensor FROM points_with_levels WHERE flood_z > ground_z) AS t\r"]}],"source":"\nfrom flask import Flask, request, jsonify, render_template\r from flask_cors import CORS\r from sqlalchemy import create_engine, text\r import os, json\r \r dbhost=os.environ.get(\"DBHOST\")\r pword=os.environ.get(\"DBPWORD\")\r \r app=Flask(__name__,\r static_url_path=\"\",\r static_folder=\"static\")\r \r CORS(app)\r \r engine=create_engine(f\"postgresql:\/\/postgres:{pword}@{dbhost}:5432\/floodaware\", echo=False, future=True)\r \r @app.route(\"\/\")\r def home():\r \"\"\"a\"\"\"\r islive=False\r startdate=20200207\r enddate=20200208\r daysback=1\r if \"live\" in list(request.args):\r islive=bool(request.args[\"live\"])\r if \"daysback\" in list(request.args):\r daysback=str(request.args[\"daysback\"])\r if \"startdate\" in list(request.args):\r startdate=str(request.args[\"startdate\"])\r if \"enddate\" in list(request.args):\r enddate=str(request.args[\"enddate\"])\r \r return render_template(\"index.html\", flaskislive=int(islive), flaskdaysback=daysback, flaskstartdate=startdate, flaskenddate=enddate)\r \r @app.route(\"\/mb\")\r def map():\r \"\"\"a\"\"\"\r return render_template(\"map.html\")\r \r @app.route(\"\/api\/catchment\")\r def catchment():\r \"\"\"a\"\"\"\r with engine.connect() as conn:\r result=conn.execute(text(\"\"\"\r SELECT json_build_object(\r 'type', 'FeatureCollection',\r 'features', json_agg(st_asgeojson(t.*)::json)\r )\r FROM\r (SELECT geom, id FROM catchment) AS t(geom, id)\r \"\"\"))\r return jsonify(result.all()[0][0])\r \r @app.route(\"\/api\/rainfall\")\r def rainfall():\r \"\"\"a\"\"\"\r startdate=str(request.args[\"startdate\"])\r enddate=str(request.args[\"enddate\"])\r \r with engine.connect() as conn:\r result=conn.execute(text(\"\"\"\r SELECT json_build_object(\r 'type', 'FeatureCollection',\r 'features', json_agg(st_asgeojson(t.*)::json)\r )\r FROM\r (SELECT st_forcepolygoncw(geom) AS geom, val, stamp FROM rainfall_raster,\r st_dumpaspolygons(\r st_clip(rast,\r (SELECT st_expand(st_envelope(st_collect(geom)), 0.01) FROM catchment)\r )\r )\r WHERE val > 0 AND stamp BETWEEN:start AND:end) AS t(geom, val, stamp)\r \"\"\"),{\"start\": startdate, \"end\": enddate})\r \r results=result.all()[0][0]\r if(results[\"features\"]==None):\r results[\"features\"]=[]\r return jsonify(results)\r \r @app.route(\"\/api\/rainfall\/avg\")\r def rainfall_avg():\r \"\"\"a\"\"\"\r startdate=str(request.args[\"startdate\"])\r enddate=str(request.args[\"enddate\"])\r with engine.connect() as conn:\r result=conn.execute(text(\"\"\"\r SELECT json_build_object(\r 'type', 'FeatureCollection',\r 'features', json_agg(t.*)\r )\r FROM\r (SELECT\r (st_summarystats(\r st_clip(rast,\r (SELECT st_expand(st_envelope(st_collect(geom)), 0.01) FROM catchment)\r )\r )).mean as avg, stamp FROM rainfall_raster \r WHERE stamp BETWEEN:start AND:end) AS t(avg, stamp)\r \"\"\"),{\"start\": startdate, \"end\": enddate})\r \r results=result.all()[0][0]\r if(results[\"features\"]==None):\r results[\"features\"]=[]\r return jsonify(results)\r \r \r @app.route(\"\/api\/sensors\")\r def sensors():\r \"\"\"a\"\"\"\r with engine.connect() as conn:\r result=conn.execute(text(\"\"\"\r SELECT json_build_object(\r 'type', 'FeatureCollection',\r 'features', json_agg(st_asgeojson(t.*)::json)\r )\r FROM\r (SELECT geom, id, ahd FROM sensors WHERE id NOT IN(10, 21)) AS t(geom, id, ahd)\r \"\"\"))\r \r return jsonify(result.all()[0][0])\r \r @app.route(\"\/api\/sensors\/data\")\r def sensors_data():\r \"\"\"a\"\"\"\r startdate=str(request.args[\"startdate\"])\r enddate=str(request.args[\"enddate\"])\r \r with engine.connect() as conn:\r result=conn.execute(text(\"\"\"\r SELECT json_build_object(\r 'type', 'FeatureCollection',\r 'features', json_agg(t.*)\r )\r FROM\r (SELECT id, level, stamp FROM sensor_levels WHERE stamp BETWEEN:start AND:end ORDER BY stamp ASC) as t(id, level, stamp)\r \"\"\"),{\"start\": startdate, \"end\": enddate})\r \r results=result.all()[0][0]\r if(results[\"features\"]==None):\r results[\"features\"]=[]\r return jsonify(results)\r \r @app.route(\"\/api\/hotspots\")\r def hotspot():\r \"\"\"a\"\"\"\r with engine.connect() as conn:\r result=conn.execute(text(\"\"\"\r SELECT json_build_object(\r 'type', 'FeatureCollection',\r 'features', json_agg(st_asgeojson(t.*)::json)\r )\r FROM\r (SELECT st_force2d(geom) AS geom, current_flood_z -floor_z AS flood_depth FROM properties WHERE floor_z -0.5 < current_flood_z) AS t\r \"\"\"))\r \r return jsonify(result.all()[0][0])\r \r @app.route(\"\/api\/hotspots\/at\")\r def hotspots_at():\r \"\"\"a\"\"\"\r time=str(request.args[\"time\"])\r with engine.connect() as conn:\r result=conn.execute(text(\"\"\"\r SELECT json_build_object(\r 'type', 'FeatureCollection',\r 'features', json_agg(st_asgeojson(t.*)::json)\r )\r FROM\r (\r WITH\r time_selection AS(SELECT(:time)::timestamp AS time),\r latest AS(SELECT DISTINCT ON(id) stamp, id, level FROM sensor_levels WHERE stamp BETWEEN(SELECT time -INTERVAL '30min' FROM time_selection) AND(SELECT time FROM time_selection) ORDER BY id, stamp DESC),\r joined AS(SELECT stamp, id, name, ahd-level::float\/1000 AS level, ahd, aep, geom FROM latest JOIN sensors using(id)),\r current_aeps AS\r (SELECT\r *,\r CASE\r \tWHEN level > aep[6] THEN 'PMF'\r \tWHEN level > aep[5] THEN '1pct'\r \tWHEN level > aep[4] THEN '2pct'\r \tWHEN level > aep[3] THEN '5pct'\r \tWHEN level > aep[2] THEN '10pct'\r \tWHEN level >=aep[1] THEN '20pct'\r END\r AS current_aep\r FROM joined),\r properties_with_aep AS(SELECT stamp, gid, current_aep, floor_z, ground_z, linked_sensor, st_force2d(properties.geom) AS geom FROM properties JOIN current_aeps ON linked_sensor=id),\r properties_with_levels AS\r (SELECT st_value((SELECT st_setsrid(rast, 4326) FROM hydraulics WHERE filename='PMF'), geom) AS flood_z, * FROM properties_with_aep WHERE current_aep='PMF'\r UNION\r SELECT st_value((SELECT st_setsrid(rast, 4326) FROM hydraulics WHERE filename='1pct'), geom) AS flood_z, * FROM properties_with_aep WHERE current_aep='1pct'\r UNION\r SELECT st_value((SELECT st_setsrid(rast, 4326) FROM hydraulics WHERE filename='2pct'), geom) AS flood_z, * FROM properties_with_aep WHERE current_aep='2pct'\r UNION\r SELECT st_value((SELECT st_setsrid(rast, 4326) FROM hydraulics WHERE filename='5pct'), geom) AS flood_z, * FROM properties_with_aep WHERE current_aep='5pct'\r UNION\r SELECT st_value((SELECT st_setsrid(rast, 4326) FROM hydraulics WHERE filename='10pct'), geom) AS flood_z, * FROM properties_with_aep WHERE current_aep='10pct'\r UNION\r SELECT st_value((SELECT st_setsrid(rast, 4326) FROM hydraulics WHERE filename='20pct'), geom) AS flood_z, * FROM properties_with_aep WHERE current_aep='20pct'\r UNION\r SELECT null AS flood_z, * FROM properties_with_aep WHERE current_aep IS null)\r SELECT stamp, ground_z, floor_z, COALESCE(flood_z, -20) AS flood_z, COALESCE(flood_z -floor_z, -20) AS flood_depth, linked_sensor, geom FROM properties_with_levels WHERE geom IS NOT NULL--WHERE flood_z -floor_z > -0.5\r ) AS t\r \"\"\"),{\"time\": time})\r \r results=result.all()[0][0]\r if(results[\"features\"]==None):\r results[\"features\"]=[]\r return jsonify(results)\r \r @app.route(\"\/api\/infrastructure\")\r def infrastructure():\r \"\"\"a\"\"\"\r time=str(request.args[\"time\"])\r with engine.connect() as conn:\r result=conn.execute(text(\r \"\"\"\r SELECT json_build_object(\r 'type', 'FeatureCollection',\r 'features', json_agg(st_asgeojson(t.*)::json)\r )\r FROM\r (\r WITH\r time_selection AS(SELECT(:time)::timestamp AS time),\r latest AS(SELECT DISTINCT ON(id) stamp, id, level FROM sensor_levels WHERE stamp BETWEEN(SELECT time -INTERVAL '30min' FROM time_selection) AND(SELECT time FROM time_selection) ORDER BY id, stamp DESC),\r joined AS(SELECT stamp, id, name, ahd-level::float\/1000 AS level, ahd, aep, geom FROM latest JOIN sensors using(id)),\r linked_sensors AS(SELECT * FROM(VALUES(1,12),(2,12),(4,12),(3,13),(7,13),(5,6),(6,6),(8,6),(35,6),(36,6),(10,14),(12,14),(15,14),(11,4),(9,11),(13,11),(33,11),(34,11),(44,11),(14,2),(23,2),(38,2),(16,39),(17,39),(18,39),(21,39),(32,41),(42,41),(24,38),(26,38),(40,38),(19,5),(20,5),(25,5),(43,5),(37,19),(29,9),(22,10),(27,10),(28,10),(47,10),(31,16),(39,16),(41,16),(45,16),(30,37),(46,37)) AS mapping(catchment, sensor)),\r current_aeps AS\r (SELECT\r *,\r CASE\r \tWHEN 2*level > aep[6] THEN 'PMF'\r \tWHEN level > aep[5] THEN '1pct'\r \tWHEN level > aep[4] THEN '2pct'\r \tWHEN level > aep[3] THEN '5pct'\r \tWHEN level > aep[2] THEN '10pct'\r \tWHEN level >=aep[1] THEN '20pct'\r END\r AS current_aep\r FROM joined),\r road AS(SELECT road_points.geom, road_points.catchment_id, linked_sensors.sensor, st_value((SELECT st_setsrid(rast, 4326) FROM dem), road_points.geom) AS ground_z FROM road_points JOIN linked_sensors ON road_points.catchment_id=linked_sensors.catchment),\r road_aep AS(SELECT road.geom, current_aeps.current_aep, road.ground_z, sensor, stamp FROM road JOIN current_aeps ON road.sensor=current_aeps.id),\r points_with_levels AS\r (SELECT st_value((SELECT st_setsrid(rast, 4326) FROM hydraulics WHERE filename='20pct'), geom) AS flood_z, * FROM road_aep WHERE current_aep='20pct'\r UNION\r SELECT st_value((SELECT st_setsrid(rast, 4326) FROM hydraulics WHERE filename='10pct'), geom) AS flood_z, * FROM road_aep WHERE current_aep='10pct'\r UNION\r SELECT st_value((SELECT st_setsrid(rast, 4326) FROM hydraulics WHERE filename='5pct'), geom) AS flood_z, * FROM road_aep WHERE current_aep='5pct'\r UNION\r SELECT st_value((SELECT st_setsrid(rast, 4326) FROM hydraulics WHERE filename='2pct'), geom) AS flood_z, * FROM road_aep WHERE current_aep='2pct'\r UNION\r SELECT st_value((SELECT st_setsrid(rast, 4326) FROM hydraulics WHERE filename='1pct'), geom) AS flood_z, * FROM road_aep WHERE current_aep='1pct'\r UNION\r SELECT st_value((SELECT st_setsrid(rast, 4326) FROM hydraulics WHERE filename='PMF'), geom) AS flood_z, * FROM road_aep WHERE current_aep='PMF'\r UNION\r SELECT null AS flood_z, * FROM road_aep WHERE current_aep IS null)\r SELECT *, flood_z -ground_z, sensor AS flood_depth FROM points_with_levels WHERE flood_z > ground_z) AS t\r \"\"\"\r ),{\"time\": time})\r results=result.all()[0][0]\r if(results[\"features\"]==None):\r results[\"features\"]=[]\r return jsonify(results)\r \r \r \r \r \r @app.route(\"\/api\/hotspots\/dummy\")\r def hotty():\r \"\"\"a\"\"\"\r sensors=tuple(json.loads(request.args[\"sensors\"]))\r level=request.args[\"level\"]\r \r with engine.connect() as conn:\r result=conn.execute(text(\"\"\"\r WITH flood_areas AS(SELECT id, AHD, st_buffer(geom, 0.003) AS geom FROM sensors WHERE id IN:sensors),\r props AS(SELECT bc_id, ground_level, floor_level -ground_level AS floor_height, geom FROM properties)\r SELECT json_build_object(\r 'type', 'FeatureCollection',\r 'features', json_agg(st_asgeojson(t.*)::json)\r )\r FROM\r (SELECT bc_id, ground_level, geom FROM props WHERE st_contains((SELECT st_collect(geom) FROM flood_areas), geom) AND floor_height <:level) AS t\r \"\"\"),{\"sensors\": sensors, \"level\": level})\r \r properties=result.all()[0][0]\r \r with engine.connect() as conn:\r result=conn.execute(text(\"\"\"\r SELECT json_build_object(\r 'type', 'FeatureCollection',\r 'features', json_agg(st_asgeojson(t.*)::json)\r )\r FROM\r (SELECT id, AHD, st_buffer(geom, 0.003) AS geom FROM sensors WHERE id IN:sensors) AS t\r \"\"\"),{\"sensors\": sensors})\r \r hotspots=result.all()[0][0]\r \r return jsonify({\"properties\": properties, \"hotspots\": hotspots})\r \r \r if(__name__==\"__main__\"):\r app.run(host=\"0.0.0.0\", port=5000) ","sourceWithComments":"from flask import Flask, request, jsonify, render_template\r\nfrom flask_cors import CORS\r\nfrom sqlalchemy import create_engine, text\r\nimport os, json\r\n\r\ndbhost = os.environ.get(\"DBHOST\")\r\npword = os.environ.get(\"DBPWORD\")\r\n\r\napp = Flask(__name__,\r\n            static_url_path=\"\",\r\n            static_folder=\"static\")\r\n\r\nCORS(app)\r\n\r\n# conn = psycopg2.connect(database=\"floodaware\", user=\"postgres\", host=dbhost, password=pword)\r\nengine = create_engine(f\"postgresql:\/\/postgres:{pword}@{dbhost}:5432\/floodaware\", echo=False, future=True)\r\n\r\n@app.route(\"\/\")\r\ndef home():\r\n    \"\"\"a\"\"\"\r\n    islive = False\r\n    startdate = 20200207\r\n    enddate = 20200208\r\n    daysback = 1\r\n    if \"live\" in list(request.args):\r\n        islive = bool(request.args[\"live\"])\r\n    if \"daysback\" in list(request.args):\r\n        daysback = str(request.args[\"daysback\"])\r\n    if \"startdate\" in list(request.args):\r\n        startdate = str(request.args[\"startdate\"])\r\n    if \"enddate\" in list(request.args):\r\n        enddate = str(request.args[\"enddate\"])\r\n    \r\n    return render_template(\"index.html\", flaskislive=int(islive), flaskdaysback=daysback, flaskstartdate=startdate, flaskenddate=enddate)\r\n\r\n@app.route(\"\/mb\")\r\ndef map():\r\n    \"\"\"a\"\"\"\r\n    return render_template(\"map.html\")\r\n\r\n@app.route(\"\/api\/catchment\")\r\ndef catchment():\r\n    \"\"\"a\"\"\"\r\n    with engine.connect() as conn:\r\n        result = conn.execute(text(\"\"\"\r\n            SELECT json_build_object(\r\n                'type', 'FeatureCollection',\r\n                'features', json_agg(st_asgeojson(t.*)::json)\r\n            )\r\n            FROM\r\n            (SELECT geom, id FROM catchment) AS t(geom, id)\r\n        \"\"\"))\r\n        return jsonify(result.all()[0][0])\r\n\r\n@app.route(\"\/api\/rainfall\")\r\ndef rainfall():\r\n    \"\"\"a\"\"\"\r\n    startdate = str(request.args[\"startdate\"])\r\n    enddate = str(request.args[\"enddate\"])\r\n\r\n    with engine.connect() as conn:\r\n        result = conn.execute(text(\"\"\"\r\n            SELECT json_build_object(\r\n                'type', 'FeatureCollection',\r\n                'features', json_agg(st_asgeojson(t.*)::json)\r\n            )\r\n            FROM\r\n            (SELECT st_forcepolygoncw(geom) AS geom, val, stamp FROM rainfall_raster,\r\n            st_dumpaspolygons(\r\n                st_clip(rast,\r\n                    (SELECT st_expand(st_envelope(st_collect(geom)), 0.01) FROM catchment)\r\n                )\r\n            )\r\n            WHERE val > 0 AND stamp BETWEEN :start AND :end) AS t(geom, val, stamp)\r\n        \"\"\"), {\"start\": startdate, \"end\": enddate})\r\n\r\n        results = result.all()[0][0]\r\n        if (results[\"features\"] == None):\r\n            results[\"features\"] = []\r\n        return jsonify(results)\r\n\r\n@app.route(\"\/api\/rainfall\/avg\")\r\ndef rainfall_avg():\r\n    \"\"\"a\"\"\"\r\n    startdate = str(request.args[\"startdate\"])\r\n    enddate = str(request.args[\"enddate\"])\r\n    with engine.connect() as conn:\r\n        result = conn.execute(text(\"\"\"\r\n            SELECT json_build_object(\r\n                'type', 'FeatureCollection',\r\n                'features', json_agg(t.*)\r\n            )\r\n            FROM\r\n            (SELECT\r\n            (st_summarystats(\r\n                st_clip(rast,\r\n                    (SELECT st_expand(st_envelope(st_collect(geom)), 0.01) FROM catchment)\r\n                )\r\n            )).mean as avg, stamp FROM rainfall_raster            \r\n            WHERE stamp BETWEEN :start AND :end) AS t(avg, stamp)\r\n        \"\"\"), {\"start\": startdate, \"end\": enddate})\r\n    \r\n        results = result.all()[0][0]\r\n        if (results[\"features\"] == None):\r\n            results[\"features\"] = []\r\n        return jsonify(results)\r\n\r\n\r\n@app.route(\"\/api\/sensors\")\r\ndef sensors():\r\n    \"\"\"a\"\"\"\r\n    with engine.connect() as conn:\r\n        result = conn.execute(text(\"\"\"\r\n            SELECT json_build_object(\r\n                'type', 'FeatureCollection',\r\n                'features', json_agg(st_asgeojson(t.*)::json)\r\n            )\r\n            FROM\r\n            (SELECT geom, id, ahd FROM sensors WHERE id NOT IN (10, 21)) AS t(geom, id, ahd)\r\n        \"\"\"))\r\n\r\n        return jsonify(result.all()[0][0])\r\n\r\n@app.route(\"\/api\/sensors\/data\")\r\ndef sensors_data():\r\n    \"\"\"a\"\"\"\r\n    startdate = str(request.args[\"startdate\"])\r\n    enddate = str(request.args[\"enddate\"])\r\n\r\n    with engine.connect() as conn:\r\n        result = conn.execute(text(\"\"\"\r\n            SELECT json_build_object(\r\n                'type', 'FeatureCollection',\r\n                'features', json_agg(t.*)\r\n            )\r\n            FROM\r\n            (SELECT id, level, stamp FROM sensor_levels WHERE stamp BETWEEN :start AND :end ORDER BY stamp ASC) as t(id, level, stamp)\r\n        \"\"\"),{\"start\": startdate, \"end\": enddate})\r\n\r\n        results = result.all()[0][0]\r\n        if (results[\"features\"] == None):\r\n            results[\"features\"] = []\r\n        return jsonify(results)\r\n\r\n@app.route(\"\/api\/hotspots\")\r\ndef hotspot():\r\n    \"\"\"a\"\"\"\r\n    with engine.connect() as conn:\r\n        result = conn.execute(text(\"\"\"\r\n        SELECT json_build_object(\r\n            'type', 'FeatureCollection',\r\n            'features', json_agg(st_asgeojson(t.*)::json)\r\n        )\r\n        FROM\r\n        (SELECT st_force2d(geom) AS geom, current_flood_z - floor_z AS flood_depth FROM properties WHERE floor_z - 0.5 < current_flood_z) AS t\r\n        \"\"\"))\r\n\r\n    return jsonify(result.all()[0][0])\r\n\r\n@app.route(\"\/api\/hotspots\/at\")\r\ndef hotspots_at():\r\n    \"\"\"a\"\"\"\r\n    time = str(request.args[\"time\"])\r\n    with engine.connect() as conn:\r\n        result = conn.execute(text(\"\"\"\r\n        SELECT json_build_object(\r\n            'type', 'FeatureCollection',\r\n            'features', json_agg(st_asgeojson(t.*)::json)\r\n        )\r\n        FROM\r\n        (\r\n            WITH\r\ntime_selection AS (SELECT (:time)::timestamp AS time),\r\nlatest AS (SELECT DISTINCT ON (id) stamp, id, level FROM sensor_levels WHERE stamp BETWEEN (SELECT time - INTERVAL '30min' FROM time_selection) AND (SELECT time FROM time_selection) ORDER BY id, stamp DESC),\r\njoined AS (SELECT stamp, id, name, ahd-level::float\/1000 AS level, ahd, aep, geom FROM latest JOIN sensors using(id)),\r\ncurrent_aeps AS\r\n(SELECT\r\n*,\r\nCASE\r\n\tWHEN level > aep[6] THEN 'PMF'\r\n\tWHEN level > aep[5] THEN '1pct'\r\n\tWHEN level > aep[4] THEN '2pct'\r\n\tWHEN level > aep[3] THEN '5pct'\r\n\tWHEN level > aep[2] THEN '10pct'\r\n\tWHEN level >= aep[1] THEN '20pct'\r\nEND\r\nAS current_aep\r\nFROM joined),\r\nproperties_with_aep AS (SELECT stamp, gid, current_aep, floor_z, ground_z, linked_sensor, st_force2d(properties.geom) AS geom FROM properties JOIN current_aeps ON linked_sensor = id),\r\nproperties_with_levels AS\r\n(SELECT st_value((SELECT st_setsrid(rast, 4326) FROM hydraulics WHERE filename='PMF'), geom) AS flood_z, * FROM properties_with_aep WHERE current_aep = 'PMF'\r\nUNION\r\nSELECT st_value((SELECT st_setsrid(rast, 4326) FROM hydraulics WHERE filename='1pct'), geom) AS flood_z, * FROM properties_with_aep WHERE current_aep = '1pct'\r\nUNION\r\nSELECT st_value((SELECT st_setsrid(rast, 4326) FROM hydraulics WHERE filename='2pct'), geom) AS flood_z, * FROM properties_with_aep WHERE current_aep = '2pct'\r\nUNION\r\nSELECT st_value((SELECT st_setsrid(rast, 4326) FROM hydraulics WHERE filename='5pct'), geom) AS flood_z, * FROM properties_with_aep WHERE current_aep = '5pct'\r\nUNION\r\nSELECT st_value((SELECT st_setsrid(rast, 4326) FROM hydraulics WHERE filename='10pct'), geom) AS flood_z, * FROM properties_with_aep WHERE current_aep = '10pct'\r\nUNION\r\nSELECT st_value((SELECT st_setsrid(rast, 4326) FROM hydraulics WHERE filename='20pct'), geom) AS flood_z, * FROM properties_with_aep WHERE current_aep = '20pct'\r\nUNION\r\nSELECT null AS flood_z, * FROM properties_with_aep WHERE current_aep IS null)\r\nSELECT stamp, ground_z, floor_z, COALESCE(flood_z, -20) AS flood_z, COALESCE(flood_z - floor_z, -20) AS flood_depth, linked_sensor, geom FROM properties_with_levels WHERE geom IS NOT NULL--WHERE flood_z - floor_z > -0.5\r\n        ) AS t\r\n        \"\"\"), {\"time\": time})\r\n\r\n    results = result.all()[0][0]\r\n    if (results[\"features\"] == None):\r\n        results[\"features\"] = []\r\n    return jsonify(results)\r\n\r\n@app.route(\"\/api\/infrastructure\")\r\ndef infrastructure():\r\n    \"\"\"a\"\"\"\r\n    time = str(request.args[\"time\"])\r\n    with engine.connect() as conn:\r\n        result = conn.execute(text(\r\n            \"\"\"\r\n            SELECT json_build_object(\r\n            'type', 'FeatureCollection',\r\n            'features', json_agg(st_asgeojson(t.*)::json)\r\n        )\r\n        FROM\r\n        (\r\nWITH\r\ntime_selection AS (SELECT (:time)::timestamp AS time),\r\nlatest AS (SELECT DISTINCT ON (id) stamp, id, level FROM sensor_levels WHERE stamp BETWEEN (SELECT time - INTERVAL '30min' FROM time_selection) AND (SELECT time FROM time_selection) ORDER BY id, stamp DESC),\r\njoined AS (SELECT stamp, id, name, ahd-level::float\/1000 AS level, ahd, aep, geom FROM latest JOIN sensors using(id)),\r\nlinked_sensors AS (SELECT * FROM (VALUES (1,12), (2,12), (4,12), (3,13), (7,13), (5,6), (6,6), (8,6), (35,6), (36,6), (10,14), (12,14), (15,14), (11,4), (9,11), (13,11), (33,11), (34,11), (44,11), (14,2), (23,2), (38,2), (16,39), (17,39), (18,39), (21,39), (32,41), (42,41), (24,38), (26,38), (40,38), (19,5), (20,5), (25,5), (43,5), (37,19), (29,9), (22,10), (27,10), (28,10), (47,10), (31,16), (39,16), (41,16), (45,16), (30,37), (46,37)) AS mapping (catchment, sensor)),\r\ncurrent_aeps AS\r\n(SELECT\r\n*,\r\nCASE\r\n\tWHEN 2*level > aep[6] THEN 'PMF'\r\n\tWHEN level > aep[5] THEN '1pct'\r\n\tWHEN level > aep[4] THEN '2pct'\r\n\tWHEN level > aep[3] THEN '5pct'\r\n\tWHEN level > aep[2] THEN '10pct'\r\n\tWHEN level >= aep[1] THEN '20pct'\r\nEND\r\nAS current_aep\r\nFROM joined),\r\nroad AS (SELECT road_points.geom, road_points.catchment_id, linked_sensors.sensor, st_value((SELECT st_setsrid(rast, 4326) FROM dem), road_points.geom) AS ground_z FROM road_points JOIN linked_sensors ON road_points.catchment_id = linked_sensors.catchment),\r\nroad_aep AS (SELECT road.geom, current_aeps.current_aep, road.ground_z, sensor, stamp FROM road JOIN current_aeps ON road.sensor = current_aeps.id),\r\npoints_with_levels AS\r\n(SELECT st_value((SELECT st_setsrid(rast, 4326) FROM hydraulics WHERE filename='20pct'), geom) AS flood_z, * FROM road_aep WHERE current_aep = '20pct'\r\nUNION\r\n SELECT st_value((SELECT st_setsrid(rast, 4326) FROM hydraulics WHERE filename='10pct'), geom) AS flood_z, * FROM road_aep WHERE current_aep = '10pct'\r\n UNION\r\n SELECT st_value((SELECT st_setsrid(rast, 4326) FROM hydraulics WHERE filename='5pct'), geom) AS flood_z, * FROM road_aep WHERE current_aep = '5pct'\r\n UNION\r\n SELECT st_value((SELECT st_setsrid(rast, 4326) FROM hydraulics WHERE filename='2pct'), geom) AS flood_z, * FROM road_aep WHERE current_aep = '2pct'\r\n UNION\r\n SELECT st_value((SELECT st_setsrid(rast, 4326) FROM hydraulics WHERE filename='1pct'), geom) AS flood_z, * FROM road_aep WHERE current_aep = '1pct'\r\n UNION\r\n SELECT st_value((SELECT st_setsrid(rast, 4326) FROM hydraulics WHERE filename='PMF'), geom) AS flood_z, * FROM road_aep WHERE current_aep = 'PMF'\r\n UNION\r\nSELECT null AS flood_z, * FROM road_aep WHERE current_aep IS null)\r\nSELECT *, flood_z - ground_z, sensor AS flood_depth FROM points_with_levels WHERE flood_z > ground_z) AS t\r\n\"\"\"\r\n        ), {\"time\": time})\r\n        results = result.all()[0][0]\r\n        if (results[\"features\"] == None):\r\n            results[\"features\"] = []\r\n    return jsonify(results)\r\n\r\n\r\n    \r\n\r\n\r\n@app.route(\"\/api\/hotspots\/dummy\")\r\ndef hotty():\r\n    \"\"\"a\"\"\"\r\n    sensors = tuple(json.loads(request.args[\"sensors\"]))\r\n    level = request.args[\"level\"]\r\n\r\n    with engine.connect() as conn:\r\n        result = conn.execute(text(\"\"\"\r\n            WITH flood_areas AS (SELECT id, AHD, st_buffer(geom, 0.003) AS geom FROM sensors WHERE id IN :sensors),\r\n            props AS (SELECT bc_id, ground_level, floor_level - ground_level AS floor_height, geom FROM properties)\r\n            SELECT json_build_object(\r\n                'type', 'FeatureCollection',\r\n                'features', json_agg(st_asgeojson(t.*)::json)\r\n            )\r\n            FROM\r\n            (SELECT bc_id, ground_level, geom FROM props WHERE st_contains((SELECT st_collect(geom) FROM flood_areas), geom) AND floor_height < :level) AS t\r\n        \"\"\"),{\"sensors\": sensors, \"level\": level})\r\n\r\n    properties = result.all()[0][0]\r\n\r\n    with engine.connect() as conn:\r\n        result = conn.execute(text(\"\"\"\r\n            SELECT json_build_object(\r\n                'type', 'FeatureCollection',\r\n                'features', json_agg(st_asgeojson(t.*)::json)\r\n            )\r\n            FROM\r\n            (SELECT id, AHD, st_buffer(geom, 0.003) AS geom FROM sensors WHERE id IN :sensors) AS t\r\n        \"\"\"), {\"sensors\": sensors})\r\n\r\n    hotspots = result.all()[0][0]\r\n\r\n    return jsonify({\"properties\": properties, \"hotspots\": hotspots})\r\n\r\n\r\nif (__name__ == \"__main__\"):\r\n    app.run(host=\"0.0.0.0\", port=5000)"}},"msg":"fix to flood rendering"}},"https:\/\/github.com\/driverog\/tg-manga-bot":{"8314e8e4cbd34a856172065eda5871dac57d26b5":{"url":"https:\/\/api.github.com\/repos\/driverog\/tg-manga-bot\/commits\/8314e8e4cbd34a856172065eda5871dac57d26b5","html_url":"https:\/\/github.com\/driverog\/tg-manga-bot\/commit\/8314e8e4cbd34a856172065eda5871dac57d26b5","message":"Fix flood handling","sha":"8314e8e4cbd34a856172065eda5871dac57d26b5","keyword":"flooding fix","diff":"diff --git a\/bot.py b\/bot.py\nindex d528584..540d241 100644\n--- a\/bot.py\n+++ b\/bot.py\n@@ -379,10 +379,10 @@ async def chapter_click(client, data, chat_id):\n             cbz = fld2cbz(pictures_folder, ch_name)\n             telegraph_url = await img2tph(chapter, clean(f'{chapter.manga.name} {chapter.name}'))\n \n-            messages: List[Message] = await retry_on_flood(bot.send_media_group(cache_channel, [\n+            messages: List[Message] = await retry_on_flood(bot.send_media_group)(cache_channel, [\n                 InputMediaDocument(pdf, thumb=thumb_path),\n                 InputMediaDocument(cbz, thumb=thumb_path, caption=f'{telegraph_url}')\n-            ]))\n+            ])\n \n             pdf_m, cbz_m = messages\n \n@@ -412,12 +412,12 @@ async def chapter_click(client, data, chat_id):\n             media_docs.append(InputMediaDocument(chapterFile.cbz_id))\n \n         if len(media_docs) == 0:\n-            await retry_on_flood(bot.send_message(chat_id, caption))\n+            await retry_on_flood(bot.send_message)(chat_id, caption)\n         elif len(media_docs) == 1:\n-            await retry_on_flood(bot.send_document(chat_id, media_docs[0].media, caption=caption))\n+            await retry_on_flood(bot.send_document)(chat_id, media_docs[0].media, caption=caption)\n         else:\n             media_docs[-1].caption = caption\n-            await retry_on_flood(bot.send_media_group(chat_id, media_docs))\n+            await retry_on_flood(bot.send_media_group)(chat_id, media_docs)\n         await asyncio.sleep(1)\n \n \ndiff --git a\/tools\/flood.py b\/tools\/flood.py\nindex fdd66dc..de4723f 100644\n--- a\/tools\/flood.py\n+++ b\/tools\/flood.py\n@@ -1,22 +1,25 @@\n import asyncio\n+from typing import Callable, Awaitable, Any\n \n import pyrogram.errors\n \n \n # retries an async awaitable as long as it raises FloodWait, and waits for err.x time\n-async def retry_on_flood(awaitable):\n-    while True:\n-        try:\n-            return await awaitable\n-        except pyrogram.errors.FloodWait as err:\n-            print(f'FloodWait, waiting {err.x} seconds')\n-            await asyncio.sleep(err.x)\n-            continue\n-        except pyrogram.errors.RPCError as err:\n-            if err.MESSAGE == 'FloodWait':\n+def retry_on_flood(function: Callable[[Any], Awaitable]):\n+    async def wrapper(*args, **kwargs):\n+        while True:\n+            try:\n+                return await function(*args, **kwargs)\n+            except pyrogram.errors.FloodWait as err:\n+                print(f'FloodWait, waiting {err.x} seconds')\n                 await asyncio.sleep(err.x)\n                 continue\n-            else:\n+            except pyrogram.errors.RPCError as err:\n+                if err.MESSAGE == 'FloodWait':\n+                    await asyncio.sleep(err.x)\n+                    continue\n+                else:\n+                    raise err\n+            except Exception as err:\n                 raise err\n-        except Exception as err:\n-            raise err\n+    return wrapper\n","files":{"\/bot.py":{"changes":[{"diff":"\n             cbz = fld2cbz(pictures_folder, ch_name)\n             telegraph_url = await img2tph(chapter, clean(f'{chapter.manga.name} {chapter.name}'))\n \n-            messages: List[Message] = await retry_on_flood(bot.send_media_group(cache_channel, [\n+            messages: List[Message] = await retry_on_flood(bot.send_media_group)(cache_channel, [\n                 InputMediaDocument(pdf, thumb=thumb_path),\n                 InputMediaDocument(cbz, thumb=thumb_path, caption=f'{telegraph_url}')\n-            ]))\n+            ])\n \n             pdf_m, cbz_m = messages\n \n","add":2,"remove":2,"filename":"\/bot.py","badparts":["            messages: List[Message] = await retry_on_flood(bot.send_media_group(cache_channel, [","            ]))"],"goodparts":["            messages: List[Message] = await retry_on_flood(bot.send_media_group)(cache_channel, [","            ])"]},{"diff":"\n             media_docs.append(InputMediaDocument(chapterFile.cbz_id))\n \n         if len(media_docs) == 0:\n-            await retry_on_flood(bot.send_message(chat_id, caption))\n+            await retry_on_flood(bot.send_message)(chat_id, caption)\n         elif len(media_docs) == 1:\n-            await retry_on_flood(bot.send_document(chat_id, media_docs[0].media, caption=caption))\n+            await retry_on_flood(bot.send_document)(chat_id, media_docs[0].media, caption=caption)\n         else:\n             media_docs[-1].caption = caption\n-            await retry_on_flood(bot.send_media_group(chat_id, media_docs))\n+            await retry_on_flood(bot.send_media_group)(chat_id, media_docs)\n         await asyncio.sleep(1)\n \n ","add":3,"remove":3,"filename":"\/bot.py","badparts":["            await retry_on_flood(bot.send_message(chat_id, caption))","            await retry_on_flood(bot.send_document(chat_id, media_docs[0].media, caption=caption))","            await retry_on_flood(bot.send_media_group(chat_id, media_docs))"],"goodparts":["            await retry_on_flood(bot.send_message)(chat_id, caption)","            await retry_on_flood(bot.send_document)(chat_id, media_docs[0].media, caption=caption)","            await retry_on_flood(bot.send_media_group)(chat_id, media_docs)"]}],"source":"\nimport enum import shutil from ast import arg import asyncio import re from dataclasses import dataclass import datetime as dt import json import pyrogram.errors from pyrogram.types import Message, InlineKeyboardMarkup, InlineKeyboardButton, CallbackQuery, InputMediaDocument from img2cbz.core import fld2cbz from img2pdf.core import fld2pdf from img2tph.core import img2tph from plugins import MangaClient, ManhuaKoClient, MangaCard, MangaChapter, ManhuaPlusClient, TMOClient, MangaDexClient, \\ MangaSeeClient, MangasInClient, McReaderClient, MangaKakalotClient, ManganeloClient, ManganatoClient, \\ KissMangaClient, MangatigreClient, MangaHasuClient import os from pyrogram import Client, filters from typing import Dict, Tuple, List, TypedDict from models.db import DB, ChapterFile, Subscription, LastChapter, MangaName, MangaOutput from pagination import Pagination from plugins.client import clean from tools.flood import retry_on_flood mangas: Dict[str, MangaCard]=dict() chapters: Dict[str, MangaChapter]=dict() pdfs: Dict[str, str]=dict() paginations: Dict[int, Pagination]=dict() queries: Dict[str, Tuple[MangaClient, str]]=dict() full_pages: Dict[str, List[str]]=dict() favourites: Dict[str, MangaCard]=dict() language_query: Dict[str, Tuple[str, str]]=dict() users_in_channel: Dict[int, dt.datetime]=dict() locks: Dict[int, asyncio.Lock]=dict() plugin_dicts: Dict[str, Dict[str, MangaClient]]={ \"\ud83c\uddec\ud83c\udde7 EN\":{ \"MangaDex\": MangaDexClient(), \"Manhuaplus\": ManhuaPlusClient(), \"Mangasee\": MangaSeeClient(), \"McReader\": McReaderClient(), \"MagaKakalot\": MangaKakalotClient(), \"Manganelo\": ManganeloClient(), \"Manganato\": ManganatoClient(), \"KissManga\": KissMangaClient(), \"MangaHasu\": MangaHasuClient() }, \"\ud83c\uddea\ud83c\uddf8 ES\":{ \"MangaDex\": MangaDexClient(language=(\"es-la\", \"es\")), \"ManhuaKo\": ManhuaKoClient(), \"TMO\": TMOClient(), \"Mangatigre\": MangatigreClient() } } class OutputOptions(enum.IntEnum): PDF=1 CBZ=2 Telegraph=4 def __and__(self, other): return self.value & other def __xor__(self, other): return self.value ^ other def __or__(self, other): return self.value | other disabled=[\"[\ud83c\uddec\ud83c\udde7 EN] McReader\"] plugins=dict() for lang, plugin_dict in plugin_dicts.items(): for name, plugin in plugin_dict.items(): identifier=f'[{lang}]{name}' if identifier in disabled: continue plugins[identifier]=plugin subsPaused=disabled +[] def split_list(li): return[li[x: x +2] for x in range(0, len(li), 2)] def get_buttons_for_options(user_options: int): buttons=[] for option in OutputOptions: checked=\"\u2705\" if option & user_options else \"\u274c\" text=f'{checked}{option.name}' buttons.append([InlineKeyboardButton(text, f\"options_{option.value}\")]) return InlineKeyboardMarkup(buttons) env_file=\"env.json\" if os.path.exists(env_file): with open(env_file) as f: env_vars=json.loads(f.read()) else: env_vars=dict(os.environ) bot=Client('bot', api_id=int(env_vars.get('API_ID')), api_hash=env_vars.get('API_HASH'), bot_token=env_vars.get('BOT_TOKEN')) @bot.on_message(filters=~(filters.private & filters.incoming)) async def on_chat_or_channel_message(client: Client, message: Message): pass @bot.on_message() async def on_private_message(client: Client, message: Message): channel=env_vars.get('CHANNEL') if not channel: return message.continue_propagation() if in_channel_cached:=users_in_channel.get(message.from_user.id): if dt.datetime.now() -in_channel_cached < dt.timedelta(days=1): return message.continue_propagation() try: if await client.get_chat_member(channel, message.from_user.id): users_in_channel[message.from_user.id]=dt.datetime.now() return message.continue_propagation() except pyrogram.errors.UsernameNotOccupied: print(\"Channel does not exist, therefore bot will continue to operate normally\") return message.continue_propagation() except pyrogram.errors.ChatAdminRequired: print(\"Bot is not admin of the channel, therefore bot will continue to operate normally\") return message.continue_propagation() except pyrogram.errors.UserNotParticipant: await message.reply(\"In order to use the bot you must join it's update channel.\", reply_markup=InlineKeyboardMarkup( [[InlineKeyboardButton('Join!', url=f't.me\/{channel}')]] )) @bot.on_message(filters=filters.command(['start'])) async def on_start(client: Client, message: Message): await message.reply(\"Welcome to the best manga pdf bot in telegram!!\\n\" \"\\n\" \"How to use? Just type the name of some manga you want to keep up to date.\\n\" \"\\n\" \"For example:\\n\" \"`Fire Force`\") @bot.on_message(filters=filters.command(['refresh'])) async def on_refresh(client: Client, message: Message): text=message.reply_to_message.text or message.reply_to_message.caption if text: regex=re.compile(r'\\[Read on telegraph]\\((.*)\\)') match=regex.search(text.markdown) else: match=None document=message.reply_to_message.document if not(message.reply_to_message and message.reply_to_message.outgoing and ((document and document.file_name[-4:].lower() in['.pdf', '.cbz']) or match)): return await message.reply(\"This command only works when it replies to a manga file that bot sent to you\") db=DB() if document: chapter=await db.get_chapter_file_by_id(document.file_unique_id) else: chapter=await db.get_chapter_file_by_id(match.group(1)) if not chapter: return await message.reply(\"This file was already refreshed\") await db.erase(chapter) return await message.reply(\"File refreshed successfully!\") @bot.on_message(filters=filters.command(['subs'])) async def on_subs(client: Client, message: Message): db=DB() subs=await db.get_subs(str(message.from_user.id)) lines=[] for sub in subs: lines.append(f'<a href=\"{sub.url}\">{sub.name}<\/a>') lines.append(f'`\/cancel{sub.url}`') lines.append('') if not lines: return await message.reply(\"You have no subscriptions yet.\") body=\"\\n\".join(lines) await message.reply(f'Your subscriptions:\\n\\n{body}', disable_web_page_preview=True) @bot.on_message(filters=filters.regex(r'^\/cancel([^]+)$')) async def on_cancel_command(client: Client, message: Message): db=DB() sub=await db.get(Subscription,(message.matches[0].group(1), str(message.from_user.id))) if not sub: return await message.reply(\"You were not subscribed to that manga.\") await db.erase(sub) return await message.reply(\"You will no longer receive updates for that manga.\") @bot.on_message(filters=filters.command(['options'])) async def on_options_command(client: Client, message: Message): db=DB() user_options=await db.get(MangaOutput, str(message.from_user.id)) user_options=user_options.output if user_options else(1 << 30) -1 buttons=get_buttons_for_options(user_options) return await message.reply(\"Select the desired output format.\", reply_markup=buttons) @bot.on_message(filters=filters.regex(r'^\/')) async def on_unknown_command(client: Client, message: Message): await message.reply(\"Unknown command\") @bot.on_message(filters=filters.text) async def on_message(client, message: Message): language_query[f\"lang_None_{hash(message.text)}\"]=(None, message.text) for language in plugin_dicts.keys(): language_query[f\"lang_{language}_{hash(message.text)}\"]=(language, message.text) await bot.send_message(message.chat.id, \"Select search languages.\", reply_markup=InlineKeyboardMarkup( split_list([InlineKeyboardButton(language, callback_data=f\"lang_{language}_{hash(message.text)}\") for language in plugin_dicts.keys()]) )) async def options_click(client, callback: CallbackQuery): db=DB() user_options=await db.get(MangaOutput, str(callback.from_user.id)) if not user_options: user_options=MangaOutput(user_id=str(callback.from_user.id), output=(2 << 30) -1) option=int(callback.data.split('_')[-1]) user_options.output ^=option buttons=get_buttons_for_options(user_options.output) await db.add(user_options) return await callback.message.edit_reply_markup(reply_markup=buttons) async def language_click(client, callback: CallbackQuery): lang, query=language_query[callback.data] if not lang: return await callback.message.edit(\"Select search languages.\", reply_markup=InlineKeyboardMarkup( split_list([InlineKeyboardButton(language, callback_data=f\"lang_{language}_{hash(query)}\") for language in plugin_dicts.keys()]) )) for identifier, manga_client in plugin_dicts[lang].items(): queries[f\"query_{lang}_{identifier}_{hash(query)}\"]=(manga_client, query) await callback.message.edit(f\"Language:{lang}\\n\\nSelect search plugin.\", reply_markup=InlineKeyboardMarkup( split_list([InlineKeyboardButton(identifier, callback_data=f\"query_{lang}_{identifier}_{hash(query)}\") for identifier in plugin_dicts[lang].keys() if f'[{lang}]{identifier}' not in disabled]) +[ [InlineKeyboardButton(\"\u25c0\ufe0f Back\", callback_data=f\"lang_None_{hash(query)}\")]] )) async def plugin_click(client, callback: CallbackQuery): manga_client, query=queries[callback.data] results=await manga_client.search(query) if not results: await bot.send_message(callback.from_user.id, \"No manga found for given query.\") return for result in results: mangas[result.unique()]=result await bot.send_message(callback.from_user.id, \"This is the result of your search\", reply_markup=InlineKeyboardMarkup([ [InlineKeyboardButton(result.name, callback_data=result.unique())] for result in results ])) async def manga_click(client, callback: CallbackQuery, pagination: Pagination=None): if pagination is None: pagination=Pagination() paginations[pagination.id]=pagination if pagination.manga is None: manga=mangas[callback.data] pagination.manga=manga results=await pagination.manga.client.get_chapters(pagination.manga, pagination.page) if not results: await callback.answer(\"Ups, no chapters there.\", show_alert=True) return full_page_key=f'full_page_{hash(\"\".join([result.unique() for result in results]))}' full_pages[full_page_key]=[] for result in results: chapters[result.unique()]=result full_pages[full_page_key].append(result.unique()) db=DB() subs=await db.get(Subscription,(pagination.manga.url, str(callback.from_user.id))) prev=[InlineKeyboardButton('<<', f'{pagination.id}_{pagination.page -1}')] next_=[InlineKeyboardButton('>>', f'{pagination.id}_{pagination.page +1}')] footer=[prev +next_] if pagination.page > 1 else[next_] fav=[[InlineKeyboardButton( \"Unsubscribe\" if subs else \"Subscribe\", f\"{'unfav' if subs else 'fav'}_{pagination.manga.unique()}\" )]] favourites[f\"fav_{pagination.manga.unique()}\"]=pagination.manga favourites[f\"unfav_{pagination.manga.unique()}\"]=pagination.manga full_page=[[InlineKeyboardButton('Full Page', full_page_key)]] buttons=InlineKeyboardMarkup(fav +footer +[ [InlineKeyboardButton(result.name, result.unique())] for result in results ] +full_page +footer) if pagination.message is None: try: message=await bot.send_photo(callback.from_user.id, pagination.manga.picture_url, f'{pagination.manga.name}\\n' f'{pagination.manga.get_url()}', reply_markup=buttons) pagination.message=message except pyrogram.errors.BadRequest as e: file_name=f'pictures\/{pagination.manga.unique()}.jpg' await pagination.manga.client.get_url(pagination.manga.picture_url, cache=True, file_name=file_name) message=await bot.send_photo(callback.from_user.id, f'.\/cache\/{pagination.manga.client.name}\/{file_name}', f'{pagination.manga.name}\\n' f'{pagination.manga.get_url()}', reply_markup=buttons) pagination.message=message else: await bot.edit_message_reply_markup( callback.from_user.id, pagination.message.message_id, reply_markup=buttons ) async def chapter_click(client, data, chat_id): lock=locks.get(chat_id) if not lock: locks[chat_id]=asyncio.Lock() async with locks[chat_id]: cache_channel=env_vars.get(\"CACHE_CHANNEL\") if not cache_channel: return await bot.send_message(chat_id, \"Bot cache channel is not configured correctly.\") try: cache_channel=int(cache_channel) except ValueError: pass chapter=chapters[data] db=DB() chapterFile=await db.get(ChapterFile, chapter.url) options=await db.get(MangaOutput, str(chat_id)) options=options.output if options else(1 << 30) -1 caption='\\n'.join([ f'{chapter.manga.name} -{chapter.name}', f'{chapter.get_url()}' ]) download=not chapterFile download=download or options & OutputOptions.PDF and not chapterFile.file_id download=download or options & OutputOptions.CBZ and not chapterFile.cbz_id download=download or options & OutputOptions.Telegraph and not chapterFile.telegraph_url download=download and options &((1 << len(OutputOptions)) -1) !=0 if download: pictures_folder=await chapter.client.download_pictures(chapter) if not chapter.pictures: return await bot.send_message(chat_id, f'There was an error parsing this chapter or chapter is missing' + f', please check the chapter at the web\\n\\n{caption}') ch_name=clean(f'{clean(chapter.manga.name, 25)} -{chapter.name}', 45) pdf, thumb_path=fld2pdf(pictures_folder, ch_name) cbz=fld2cbz(pictures_folder, ch_name) telegraph_url=await img2tph(chapter, clean(f'{chapter.manga.name}{chapter.name}')) messages: List[Message]=await retry_on_flood(bot.send_media_group(cache_channel,[ InputMediaDocument(pdf, thumb=thumb_path), InputMediaDocument(cbz, thumb=thumb_path, caption=f'{telegraph_url}') ])) pdf_m, cbz_m=messages if not chapterFile: await db.add(ChapterFile(url=chapter.url, file_id=pdf_m.document.file_id, file_unique_id=pdf_m.document.file_unique_id, cbz_id=cbz_m.document.file_id, cbz_unique_id=cbz_m.document.file_unique_id, telegraph_url=telegraph_url)) else: chapterFile.file_id, chapterFile.file_unique_id, chapterFile.cbz_id, \\ chapterFile.cbz_unique_id, chapterFile.telegraph_url=\\ pdf_m.document.file_id, pdf_m.document.file_unique_id, cbz_m.document.file_id, \\ cbz_m.document.file_unique_id, telegraph_url await db.add(chapterFile) shutil.rmtree(pictures_folder) chapterFile=await db.get(ChapterFile, chapter.url) caption=f'{chapter.manga.name} -{chapter.name}\\n' if options & OutputOptions.Telegraph: caption +=f'[Read on telegraph]({chapterFile.telegraph_url})\\n' caption +=f'[Read on website]({chapter.get_url()})' media_docs=[] if options & OutputOptions.PDF: media_docs.append(InputMediaDocument(chapterFile.file_id)) if options & OutputOptions.CBZ: media_docs.append(InputMediaDocument(chapterFile.cbz_id)) if len(media_docs)==0: await retry_on_flood(bot.send_message(chat_id, caption)) elif len(media_docs)==1: await retry_on_flood(bot.send_document(chat_id, media_docs[0].media, caption=caption)) else: media_docs[-1].caption=caption await retry_on_flood(bot.send_media_group(chat_id, media_docs)) await asyncio.sleep(1) async def pagination_click(client: Client, callback: CallbackQuery): pagination_id, page=map(int, callback.data.split('_')) pagination=paginations[pagination_id] pagination.page=page await manga_click(client, callback, pagination) async def full_page_click(client: Client, callback: CallbackQuery): chapters_data=full_pages[callback.data] for chapter_data in reversed(chapters_data): try: await chapter_click(client, chapter_data, callback.from_user.id) except Exception as e: print(e) await asyncio.sleep(0.5) async def favourite_click(client: Client, callback: CallbackQuery): action, data=callback.data.split('_') fav=action=='fav' manga=favourites[callback.data] db=DB() subs=await db.get(Subscription,(manga.url, str(callback.from_user.id))) if not subs and fav: await db.add(Subscription(url=manga.url, user_id=str(callback.from_user.id))) if subs and not fav: await db.erase(subs) if subs and fav: await callback.answer(\"You are already subscribed\", show_alert=True) if not subs and not fav: await callback.answer(\"You are not subscribed\", show_alert=True) reply_markup=callback.message.reply_markup keyboard=reply_markup.inline_keyboard keyboard[0]=[InlineKeyboardButton( \"Unsubscribe\" if fav else \"Subscribe\", f\"{'unfav' if fav else 'fav'}_{data}\" )] await bot.edit_message_reply_markup(callback.from_user.id, callback.message.message_id, InlineKeyboardMarkup(keyboard)) db_manga=await db.get(MangaName, manga.url) if not db_manga: await db.add(MangaName(url=manga.url, name=manga.name)) def is_pagination_data(callback: CallbackQuery): data=callback.data match=re.match(r'\\d+_\\d+', data) if not match: return False pagination_id=int(data.split('_')[0]) if pagination_id not in paginations: return False pagination=paginations[pagination_id] if not pagination.message: return False if pagination.message.chat.id !=callback.from_user.id: return False if pagination.message.message_id !=callback.message.message_id: return False return True @bot.on_callback_query() async def on_callback_query(client, callback: CallbackQuery): if callback.data in queries: await plugin_click(client, callback) elif callback.data in mangas: await manga_click(client, callback) elif callback.data in chapters: await chapter_click(client, callback.data, callback.from_user.id) elif callback.data in full_pages: await full_page_click(client, callback) elif callback.data in favourites: await favourite_click(client, callback) elif is_pagination_data(callback): await pagination_click(client, callback) elif callback.data in language_query: await language_click(client, callback) elif callback.data.startswith('options'): await options_click(client, callback) else: await bot.answer_callback_query(callback.id, 'This is an old button, please redo the search', show_alert=True) return try: await callback.answer() except BaseException as e: print(e) async def remove_subscriptions(sub: str): db=DB() await db.erase_subs(sub) async def update_mangas(): print(\"Updating mangas\") db=DB() subscriptions=await db.get_all(Subscription) last_chapters=await db.get_all(LastChapter) manga_names=await db.get_all(MangaName) subs_dictionary=dict() chapters_dictionary=dict() url_client_dictionary=dict() client_url_dictionary={client: set() for client in plugins.values()} manga_dict=dict() for subscription in subscriptions: if subscription.url not in subs_dictionary: subs_dictionary[subscription.url]=[] subs_dictionary[subscription.url].append(subscription.user_id) for last_chapter in last_chapters: chapters_dictionary[last_chapter.url]=last_chapter for manga in manga_names: manga_dict[manga.url]=manga for url in subs_dictionary: for ident, client in plugins.items(): if ident in subsPaused: continue if await client.contains_url(url): url_client_dictionary[url]=client client_url_dictionary[client].add(url) for client, urls in client_url_dictionary.items(): to_check=[chapters_dictionary[url] for url in urls if chapters_dictionary.get(url)] if len(to_check)==0: continue try: updated, not_updated=await client.check_updated_urls(to_check) except BaseException as e: print(f\"Error while checking updates for site:{client.name}, err: \", e) not_updated=list(urls) for url in not_updated: del url_client_dictionary[url] updated=dict() for url, client in url_client_dictionary.items(): try: if url not in manga_dict: continue manga_name=manga_dict[url].name if url not in chapters_dictionary: agen=client.iter_chapters(url, manga_name) last_chapter=await anext(agen) await db.add(LastChapter(url=url, chapter_url=last_chapter.url)) await asyncio.sleep(10) else: last_chapter=chapters_dictionary[url] new_chapters: List[MangaChapter]=[] counter=0 async for chapter in client.iter_chapters(url, manga_name): if chapter.url==last_chapter.chapter_url: break new_chapters.append(chapter) counter +=1 if counter==20: break if new_chapters: last_chapter.chapter_url=new_chapters[0].url await db.add(last_chapter) updated[url]=list(reversed(new_chapters)) for chapter in new_chapters: if chapter.unique() not in chapters: chapters[chapter.unique()]=chapter await asyncio.sleep(1) except BaseException as e: print(f'An exception occurred getting new chapters for url{url}:{e}') blocked=set() for url, chapter_list in updated.items(): for chapter in chapter_list: print(f'{chapter.manga.name} -{chapter.name}') for sub in subs_dictionary[url]: if sub in blocked: continue try: await chapter_click(bot, chapter.unique(), int(sub)) except pyrogram.errors.UserIsBlocked: print(f'User{sub} blocked the bot') await remove_subscriptions(sub) blocked.add(sub) except BaseException as e: print(f'An exception occurred sending new chapter:{e}') await asyncio.sleep(0.5) await asyncio.sleep(1) async def manga_updater(): minutes=5 while True: wait_time=minutes * 60 try: start=dt.datetime.now() await update_mangas() elapsed=dt.datetime.now() -start wait_time=max((dt.timedelta(seconds=wait_time) -elapsed).total_seconds(), 0) print(f'Time elapsed updating mangas:{elapsed}, waiting for{wait_time}') except BaseException as e: print(f'An exception occurred during chapters update:{e}') if wait_time: await asyncio.sleep(wait_time) ","sourceWithComments":"import enum\nimport shutil\nfrom ast import arg\nimport asyncio\nimport re\nfrom dataclasses import dataclass\nimport datetime as dt\nimport json\n\nimport pyrogram.errors\nfrom pyrogram.types import Message, InlineKeyboardMarkup, InlineKeyboardButton, CallbackQuery, InputMediaDocument\n\nfrom img2cbz.core import fld2cbz\nfrom img2pdf.core import fld2pdf\nfrom img2tph.core import img2tph\nfrom plugins import MangaClient, ManhuaKoClient, MangaCard, MangaChapter, ManhuaPlusClient, TMOClient, MangaDexClient, \\\n    MangaSeeClient, MangasInClient, McReaderClient, MangaKakalotClient, ManganeloClient, ManganatoClient, \\\n    KissMangaClient, MangatigreClient, MangaHasuClient\nimport os\n\nfrom pyrogram import Client, filters\nfrom typing import Dict, Tuple, List, TypedDict\n\nfrom models.db import DB, ChapterFile, Subscription, LastChapter, MangaName, MangaOutput\nfrom pagination import Pagination\nfrom plugins.client import clean\nfrom tools.flood import retry_on_flood\n\nmangas: Dict[str, MangaCard] = dict()\nchapters: Dict[str, MangaChapter] = dict()\npdfs: Dict[str, str] = dict()\npaginations: Dict[int, Pagination] = dict()\nqueries: Dict[str, Tuple[MangaClient, str]] = dict()\nfull_pages: Dict[str, List[str]] = dict()\nfavourites: Dict[str, MangaCard] = dict()\nlanguage_query: Dict[str, Tuple[str, str]] = dict()\nusers_in_channel: Dict[int, dt.datetime] = dict()\nlocks: Dict[int, asyncio.Lock] = dict()\n\nplugin_dicts: Dict[str, Dict[str, MangaClient]] = {\n    \"\ud83c\uddec\ud83c\udde7 EN\": {\n        \"MangaDex\": MangaDexClient(),\n        \"Manhuaplus\": ManhuaPlusClient(),\n        \"Mangasee\": MangaSeeClient(),\n        \"McReader\": McReaderClient(),\n        \"MagaKakalot\": MangaKakalotClient(),\n        \"Manganelo\": ManganeloClient(),\n        \"Manganato\": ManganatoClient(),\n        \"KissManga\": KissMangaClient(),\n        \"MangaHasu\": MangaHasuClient()\n    },\n    \"\ud83c\uddea\ud83c\uddf8 ES\": {\n        \"MangaDex\": MangaDexClient(language=(\"es-la\", \"es\")),\n        \"ManhuaKo\": ManhuaKoClient(),\n        \"TMO\": TMOClient(),\n        \"Mangatigre\": MangatigreClient()\n    }\n}\n\n\nclass OutputOptions(enum.IntEnum):\n    PDF = 1\n    CBZ = 2\n    Telegraph = 4\n\n    def __and__(self, other):\n        return self.value & other\n\n    def __xor__(self, other):\n        return self.value ^ other\n\n    def __or__(self, other):\n        return self.value | other\n\n\ndisabled = [\"[\ud83c\uddec\ud83c\udde7 EN] McReader\"]\n\nplugins = dict()\nfor lang, plugin_dict in plugin_dicts.items():\n    for name, plugin in plugin_dict.items():\n        identifier = f'[{lang}] {name}'\n        if identifier in disabled:\n            continue\n        plugins[identifier] = plugin\n\n# subsPaused = [\"[\ud83c\uddea\ud83c\uddf8 ES] TMO\"]\nsubsPaused = disabled + []\n\n\ndef split_list(li):\n    return [li[x: x + 2] for x in range(0, len(li), 2)]\n\n\ndef get_buttons_for_options(user_options: int):\n    buttons = []\n    for option in OutputOptions:\n        checked = \"\u2705\" if option & user_options else \"\u274c\"\n        text = f'{checked} {option.name}'\n        buttons.append([InlineKeyboardButton(text, f\"options_{option.value}\")])\n    return InlineKeyboardMarkup(buttons)\n\n\nenv_file = \"env.json\"\nif os.path.exists(env_file):\n    with open(env_file) as f:\n        env_vars = json.loads(f.read())\nelse:\n    env_vars = dict(os.environ)\n\nbot = Client('bot',\n             api_id=int(env_vars.get('API_ID')),\n             api_hash=env_vars.get('API_HASH'),\n             bot_token=env_vars.get('BOT_TOKEN'))\n\n\n@bot.on_message(filters=~(filters.private & filters.incoming))\nasync def on_chat_or_channel_message(client: Client, message: Message):\n    pass\n\n\n@bot.on_message()\nasync def on_private_message(client: Client, message: Message):\n    channel = env_vars.get('CHANNEL')\n    if not channel:\n        return message.continue_propagation()\n    if in_channel_cached := users_in_channel.get(message.from_user.id):\n        if dt.datetime.now() - in_channel_cached < dt.timedelta(days=1):\n            return message.continue_propagation()\n    try:\n        if await client.get_chat_member(channel, message.from_user.id):\n            users_in_channel[message.from_user.id] = dt.datetime.now()\n            return message.continue_propagation()\n    except pyrogram.errors.UsernameNotOccupied:\n        print(\"Channel does not exist, therefore bot will continue to operate normally\")\n        return message.continue_propagation()\n    except pyrogram.errors.ChatAdminRequired:\n        print(\"Bot is not admin of the channel, therefore bot will continue to operate normally\")\n        return message.continue_propagation()\n    except pyrogram.errors.UserNotParticipant:\n        await message.reply(\"In order to use the bot you must join it's update channel.\",\n                            reply_markup=InlineKeyboardMarkup(\n                                [[InlineKeyboardButton('Join!', url=f't.me\/{channel}')]]\n                            ))\n\n\n@bot.on_message(filters=filters.command(['start']))\nasync def on_start(client: Client, message: Message):\n    await message.reply(\"Welcome to the best manga pdf bot in telegram!!\\n\"\n                        \"\\n\"\n                        \"How to use? Just type the name of some manga you want to keep up to date.\\n\"\n                        \"\\n\"\n                        \"For example:\\n\"\n                        \"`Fire Force`\")\n\n\n@bot.on_message(filters=filters.command(['refresh']))\nasync def on_refresh(client: Client, message: Message):\n    text = message.reply_to_message.text or message.reply_to_message.caption\n    if text:\n        regex = re.compile(r'\\[Read on telegraph]\\((.*)\\)')\n        match = regex.search(text.markdown)\n    else:\n        match = None\n    document = message.reply_to_message.document\n    if not (message.reply_to_message and message.reply_to_message.outgoing and\n            ((document and document.file_name[-4:].lower() in ['.pdf', '.cbz']) or match)):\n        return await message.reply(\"This command only works when it replies to a manga file that bot sent to you\")\n    db = DB()\n    if document:\n        chapter = await db.get_chapter_file_by_id(document.file_unique_id)\n    else:\n        chapter = await db.get_chapter_file_by_id(match.group(1))\n    if not chapter:\n        return await message.reply(\"This file was already refreshed\")\n    await db.erase(chapter)\n    return await message.reply(\"File refreshed successfully!\")\n\n\n@bot.on_message(filters=filters.command(['subs']))\nasync def on_subs(client: Client, message: Message):\n    db = DB()\n    subs = await db.get_subs(str(message.from_user.id))\n    lines = []\n    for sub in subs:\n        lines.append(f'<a href=\"{sub.url}\">{sub.name}<\/a>')\n        lines.append(f'`\/cancel {sub.url}`')\n        lines.append('')\n\n    if not lines:\n        return await message.reply(\"You have no subscriptions yet.\")\n    body = \"\\n\".join(lines)\n    await message.reply(f'Your subscriptions:\\n\\n{body}', disable_web_page_preview=True)\n\n\n@bot.on_message(filters=filters.regex(r'^\/cancel ([^ ]+)$'))\nasync def on_cancel_command(client: Client, message: Message):\n    db = DB()\n    sub = await db.get(Subscription, (message.matches[0].group(1), str(message.from_user.id)))\n    if not sub:\n        return await message.reply(\"You were not subscribed to that manga.\")\n    await db.erase(sub)\n    return await message.reply(\"You will no longer receive updates for that manga.\")\n\n\n@bot.on_message(filters=filters.command(['options']))\nasync def on_options_command(client: Client, message: Message):\n    db = DB()\n    user_options = await db.get(MangaOutput, str(message.from_user.id))\n    user_options = user_options.output if user_options else (1 << 30) - 1\n    buttons = get_buttons_for_options(user_options)\n    return await message.reply(\"Select the desired output format.\", reply_markup=buttons)\n\n\n@bot.on_message(filters=filters.regex(r'^\/'))\nasync def on_unknown_command(client: Client, message: Message):\n    await message.reply(\"Unknown command\")\n\n\n@bot.on_message(filters=filters.text)\nasync def on_message(client, message: Message):\n    language_query[f\"lang_None_{hash(message.text)}\"] = (None, message.text)\n    for language in plugin_dicts.keys():\n        language_query[f\"lang_{language}_{hash(message.text)}\"] = (language, message.text)\n    await bot.send_message(message.chat.id, \"Select search languages.\", reply_markup=InlineKeyboardMarkup(\n        split_list([InlineKeyboardButton(language, callback_data=f\"lang_{language}_{hash(message.text)}\")\n                    for language in plugin_dicts.keys()])\n    ))\n\n\nasync def options_click(client, callback: CallbackQuery):\n    db = DB()\n    user_options = await db.get(MangaOutput, str(callback.from_user.id))\n    if not user_options:\n        user_options = MangaOutput(user_id=str(callback.from_user.id), output=(2 << 30) - 1)\n    option = int(callback.data.split('_')[-1])\n    user_options.output ^= option\n    buttons = get_buttons_for_options(user_options.output)\n    await db.add(user_options)\n    return await callback.message.edit_reply_markup(reply_markup=buttons)\n\n\nasync def language_click(client, callback: CallbackQuery):\n    lang, query = language_query[callback.data]\n    if not lang:\n        return await callback.message.edit(\"Select search languages.\", reply_markup=InlineKeyboardMarkup(\n            split_list([InlineKeyboardButton(language, callback_data=f\"lang_{language}_{hash(query)}\")\n                        for language in plugin_dicts.keys()])\n        ))\n    for identifier, manga_client in plugin_dicts[lang].items():\n        queries[f\"query_{lang}_{identifier}_{hash(query)}\"] = (manga_client, query)\n    await callback.message.edit(f\"Language: {lang}\\n\\nSelect search plugin.\", reply_markup=InlineKeyboardMarkup(\n        split_list([InlineKeyboardButton(identifier, callback_data=f\"query_{lang}_{identifier}_{hash(query)}\")\n                    for identifier in plugin_dicts[lang].keys() if f'[{lang}] {identifier}' not in disabled]) + [\n            [InlineKeyboardButton(\"\u25c0\ufe0f Back\", callback_data=f\"lang_None_{hash(query)}\")]]\n    ))\n\n\nasync def plugin_click(client, callback: CallbackQuery):\n    manga_client, query = queries[callback.data]\n    results = await manga_client.search(query)\n    if not results:\n        await bot.send_message(callback.from_user.id, \"No manga found for given query.\")\n        return\n    for result in results:\n        mangas[result.unique()] = result\n    await bot.send_message(callback.from_user.id,\n                           \"This is the result of your search\",\n                           reply_markup=InlineKeyboardMarkup([\n                               [InlineKeyboardButton(result.name, callback_data=result.unique())] for result in results\n                           ]))\n\n\nasync def manga_click(client, callback: CallbackQuery, pagination: Pagination = None):\n    if pagination is None:\n        pagination = Pagination()\n        paginations[pagination.id] = pagination\n\n    if pagination.manga is None:\n        manga = mangas[callback.data]\n        pagination.manga = manga\n\n    results = await pagination.manga.client.get_chapters(pagination.manga, pagination.page)\n\n    if not results:\n        await callback.answer(\"Ups, no chapters there.\", show_alert=True)\n        return\n\n    full_page_key = f'full_page_{hash(\"\".join([result.unique() for result in results]))}'\n    full_pages[full_page_key] = []\n    for result in results:\n        chapters[result.unique()] = result\n        full_pages[full_page_key].append(result.unique())\n\n    db = DB()\n    subs = await db.get(Subscription, (pagination.manga.url, str(callback.from_user.id)))\n\n    prev = [InlineKeyboardButton('<<', f'{pagination.id}_{pagination.page - 1}')]\n    next_ = [InlineKeyboardButton('>>', f'{pagination.id}_{pagination.page + 1}')]\n    footer = [prev + next_] if pagination.page > 1 else [next_]\n\n    fav = [[InlineKeyboardButton(\n        \"Unsubscribe\" if subs else \"Subscribe\",\n        f\"{'unfav' if subs else 'fav'}_{pagination.manga.unique()}\"\n    )]]\n    favourites[f\"fav_{pagination.manga.unique()}\"] = pagination.manga\n    favourites[f\"unfav_{pagination.manga.unique()}\"] = pagination.manga\n\n    full_page = [[InlineKeyboardButton('Full Page', full_page_key)]]\n\n    buttons = InlineKeyboardMarkup(fav + footer + [\n        [InlineKeyboardButton(result.name, result.unique())] for result in results\n    ] + full_page + footer)\n\n    if pagination.message is None:\n        try:\n            message = await bot.send_photo(callback.from_user.id,\n                                           pagination.manga.picture_url,\n                                           f'{pagination.manga.name}\\n'\n                                           f'{pagination.manga.get_url()}', reply_markup=buttons)\n            pagination.message = message\n        except pyrogram.errors.BadRequest as e:\n            file_name = f'pictures\/{pagination.manga.unique()}.jpg'\n            await pagination.manga.client.get_url(pagination.manga.picture_url, cache=True, file_name=file_name)\n            message = await bot.send_photo(callback.from_user.id,\n                                           f'.\/cache\/{pagination.manga.client.name}\/{file_name}',\n                                           f'{pagination.manga.name}\\n'\n                                           f'{pagination.manga.get_url()}', reply_markup=buttons)\n            pagination.message = message\n    else:\n        await bot.edit_message_reply_markup(\n            callback.from_user.id,\n            pagination.message.message_id,\n            reply_markup=buttons\n        )\n\n\nasync def chapter_click(client, data, chat_id):\n    lock = locks.get(chat_id)\n    if not lock:\n        locks[chat_id] = asyncio.Lock()\n\n    async with locks[chat_id]:\n        cache_channel = env_vars.get(\"CACHE_CHANNEL\")\n        if not cache_channel:\n            return await bot.send_message(chat_id, \"Bot cache channel is not configured correctly.\")\n\n        # Try convert to int cache_channel, because it can be id or username\n        try:\n            cache_channel = int(cache_channel)\n        except ValueError:\n            pass\n\n        chapter = chapters[data]\n\n        db = DB()\n\n        chapterFile = await db.get(ChapterFile, chapter.url)\n        options = await db.get(MangaOutput, str(chat_id))\n        options = options.output if options else (1 << 30) - 1\n\n        caption = '\\n'.join([\n            f'{chapter.manga.name} - {chapter.name}',\n            f'{chapter.get_url()}'\n        ])\n\n        download = not chapterFile\n        download = download or options & OutputOptions.PDF and not chapterFile.file_id\n        download = download or options & OutputOptions.CBZ and not chapterFile.cbz_id\n        download = download or options & OutputOptions.Telegraph and not chapterFile.telegraph_url\n        download = download and options & ((1 << len(OutputOptions)) - 1) != 0\n\n        if download:\n            pictures_folder = await chapter.client.download_pictures(chapter)\n            if not chapter.pictures:\n                return await bot.send_message(chat_id, f'There was an error parsing this chapter or chapter is missing' +\n                                              f', please check the chapter at the web\\n\\n{caption}')\n            ch_name = clean(f'{clean(chapter.manga.name, 25)} - {chapter.name}', 45)\n            pdf, thumb_path = fld2pdf(pictures_folder, ch_name)\n            cbz = fld2cbz(pictures_folder, ch_name)\n            telegraph_url = await img2tph(chapter, clean(f'{chapter.manga.name} {chapter.name}'))\n\n            messages: List[Message] = await retry_on_flood(bot.send_media_group(cache_channel, [\n                InputMediaDocument(pdf, thumb=thumb_path),\n                InputMediaDocument(cbz, thumb=thumb_path, caption=f'{telegraph_url}')\n            ]))\n\n            pdf_m, cbz_m = messages\n\n            if not chapterFile:\n                await db.add(ChapterFile(url=chapter.url, file_id=pdf_m.document.file_id,\n                                         file_unique_id=pdf_m.document.file_unique_id, cbz_id=cbz_m.document.file_id,\n                                         cbz_unique_id=cbz_m.document.file_unique_id, telegraph_url=telegraph_url))\n            else:\n                chapterFile.file_id, chapterFile.file_unique_id, chapterFile.cbz_id, \\\n                chapterFile.cbz_unique_id, chapterFile.telegraph_url = \\\n                    pdf_m.document.file_id, pdf_m.document.file_unique_id, cbz_m.document.file_id, \\\n                    cbz_m.document.file_unique_id, telegraph_url\n                await db.add(chapterFile)\n\n            shutil.rmtree(pictures_folder)\n\n        chapterFile = await db.get(ChapterFile, chapter.url)\n\n        caption = f'{chapter.manga.name} - {chapter.name}\\n'\n        if options & OutputOptions.Telegraph:\n            caption += f'[Read on telegraph]({chapterFile.telegraph_url})\\n'\n        caption += f'[Read on website]({chapter.get_url()})'\n        media_docs = []\n        if options & OutputOptions.PDF:\n            media_docs.append(InputMediaDocument(chapterFile.file_id))\n        if options & OutputOptions.CBZ:\n            media_docs.append(InputMediaDocument(chapterFile.cbz_id))\n\n        if len(media_docs) == 0:\n            await retry_on_flood(bot.send_message(chat_id, caption))\n        elif len(media_docs) == 1:\n            await retry_on_flood(bot.send_document(chat_id, media_docs[0].media, caption=caption))\n        else:\n            media_docs[-1].caption = caption\n            await retry_on_flood(bot.send_media_group(chat_id, media_docs))\n        await asyncio.sleep(1)\n\n\nasync def pagination_click(client: Client, callback: CallbackQuery):\n    pagination_id, page = map(int, callback.data.split('_'))\n    pagination = paginations[pagination_id]\n    pagination.page = page\n    await manga_click(client, callback, pagination)\n\n\nasync def full_page_click(client: Client, callback: CallbackQuery):\n    chapters_data = full_pages[callback.data]\n    for chapter_data in reversed(chapters_data):\n        try:\n            await chapter_click(client, chapter_data, callback.from_user.id)\n        except Exception as e:\n            print(e)\n        await asyncio.sleep(0.5)\n\n\nasync def favourite_click(client: Client, callback: CallbackQuery):\n    action, data = callback.data.split('_')\n    fav = action == 'fav'\n    manga = favourites[callback.data]\n    db = DB()\n    subs = await db.get(Subscription, (manga.url, str(callback.from_user.id)))\n    if not subs and fav:\n        await db.add(Subscription(url=manga.url, user_id=str(callback.from_user.id)))\n    if subs and not fav:\n        await db.erase(subs)\n    if subs and fav:\n        await callback.answer(\"You are already subscribed\", show_alert=True)\n    if not subs and not fav:\n        await callback.answer(\"You are not subscribed\", show_alert=True)\n    reply_markup = callback.message.reply_markup\n    keyboard = reply_markup.inline_keyboard\n    keyboard[0] = [InlineKeyboardButton(\n        \"Unsubscribe\" if fav else \"Subscribe\",\n        f\"{'unfav' if fav else 'fav'}_{data}\"\n    )]\n    await bot.edit_message_reply_markup(callback.from_user.id, callback.message.message_id,\n                                        InlineKeyboardMarkup(keyboard))\n    db_manga = await db.get(MangaName, manga.url)\n    if not db_manga:\n        await db.add(MangaName(url=manga.url, name=manga.name))\n\n\ndef is_pagination_data(callback: CallbackQuery):\n    data = callback.data\n    match = re.match(r'\\d+_\\d+', data)\n    if not match:\n        return False\n    pagination_id = int(data.split('_')[0])\n    if pagination_id not in paginations:\n        return False\n    pagination = paginations[pagination_id]\n    if not pagination.message:\n        return False\n    if pagination.message.chat.id != callback.from_user.id:\n        return False\n    if pagination.message.message_id != callback.message.message_id:\n        return False\n    return True\n\n\n@bot.on_callback_query()\nasync def on_callback_query(client, callback: CallbackQuery):\n    if callback.data in queries:\n        await plugin_click(client, callback)\n    elif callback.data in mangas:\n        await manga_click(client, callback)\n    elif callback.data in chapters:\n        await chapter_click(client, callback.data, callback.from_user.id)\n    elif callback.data in full_pages:\n        await full_page_click(client, callback)\n    elif callback.data in favourites:\n        await favourite_click(client, callback)\n    elif is_pagination_data(callback):\n        await pagination_click(client, callback)\n    elif callback.data in language_query:\n        await language_click(client, callback)\n    elif callback.data.startswith('options'):\n        await options_click(client, callback)\n    else:\n        await bot.answer_callback_query(callback.id, 'This is an old button, please redo the search', show_alert=True)\n        return\n    try:\n        await callback.answer()\n    except BaseException as e:\n        print(e)\n\n\nasync def remove_subscriptions(sub: str):\n    db = DB()\n\n    await db.erase_subs(sub)\n\n\nasync def update_mangas():\n    print(\"Updating mangas\")\n    db = DB()\n    subscriptions = await db.get_all(Subscription)\n    last_chapters = await db.get_all(LastChapter)\n    manga_names = await db.get_all(MangaName)\n\n    subs_dictionary = dict()\n    chapters_dictionary = dict()\n    url_client_dictionary = dict()\n    client_url_dictionary = {client: set() for client in plugins.values()}\n    manga_dict = dict()\n\n    for subscription in subscriptions:\n        if subscription.url not in subs_dictionary:\n            subs_dictionary[subscription.url] = []\n        subs_dictionary[subscription.url].append(subscription.user_id)\n\n    for last_chapter in last_chapters:\n        chapters_dictionary[last_chapter.url] = last_chapter\n\n    for manga in manga_names:\n        manga_dict[manga.url] = manga\n\n    for url in subs_dictionary:\n        for ident, client in plugins.items():\n            if ident in subsPaused:\n                continue\n            if await client.contains_url(url):\n                url_client_dictionary[url] = client\n                client_url_dictionary[client].add(url)\n\n    for client, urls in client_url_dictionary.items():\n        # print('')\n        # print(f'Updating {client.name}')\n        # print(f'Urls:\\t{list(urls)}')\n        # new_urls = [url for url in urls if not chapters_dictionary.get(url)]\n        # print(f'New Urls:\\t{new_urls}')\n        to_check = [chapters_dictionary[url] for url in urls if chapters_dictionary.get(url)]\n        if len(to_check) == 0:\n            continue\n        try:\n            updated, not_updated = await client.check_updated_urls(to_check)\n        except BaseException as e:\n            print(f\"Error while checking updates for site: {client.name}, err: \", e)\n            not_updated = list(urls)\n        for url in not_updated:\n            del url_client_dictionary[url]\n        # print(f'Updated:\\t{list(updated)}')\n        # print(f'Not Updated:\\t{list(not_updated)}')\n\n    updated = dict()\n\n    for url, client in url_client_dictionary.items():\n        try:\n            if url not in manga_dict:\n                continue\n            manga_name = manga_dict[url].name\n            if url not in chapters_dictionary:\n                agen = client.iter_chapters(url, manga_name)\n                last_chapter = await anext(agen)\n                await db.add(LastChapter(url=url, chapter_url=last_chapter.url))\n                await asyncio.sleep(10)\n            else:\n                last_chapter = chapters_dictionary[url]\n                new_chapters: List[MangaChapter] = []\n                counter = 0\n                async for chapter in client.iter_chapters(url, manga_name):\n                    if chapter.url == last_chapter.chapter_url:\n                        break\n                    new_chapters.append(chapter)\n                    counter += 1\n                    if counter == 20:\n                        break\n                if new_chapters:\n                    last_chapter.chapter_url = new_chapters[0].url\n                    await db.add(last_chapter)\n                    updated[url] = list(reversed(new_chapters))\n                    for chapter in new_chapters:\n                        if chapter.unique() not in chapters:\n                            chapters[chapter.unique()] = chapter\n                await asyncio.sleep(1)\n        except BaseException as e:\n            print(f'An exception occurred getting new chapters for url {url}: {e}')\n\n    blocked = set()\n    for url, chapter_list in updated.items():\n        for chapter in chapter_list:\n            print(f'{chapter.manga.name} - {chapter.name}')\n            for sub in subs_dictionary[url]:\n                if sub in blocked:\n                    continue\n                try:\n                    await chapter_click(bot, chapter.unique(), int(sub))\n                except pyrogram.errors.UserIsBlocked:\n                    print(f'User {sub} blocked the bot')\n                    await remove_subscriptions(sub)\n                    blocked.add(sub)\n                except BaseException as e:\n                    print(f'An exception occurred sending new chapter: {e}')\n                await asyncio.sleep(0.5)\n            await asyncio.sleep(1)\n\n\nasync def manga_updater():\n    minutes = 5\n    while True:\n        wait_time = minutes * 60\n        try:\n            start = dt.datetime.now()\n            await update_mangas()\n            elapsed = dt.datetime.now() - start\n            wait_time = max((dt.timedelta(seconds=wait_time) - elapsed).total_seconds(), 0)\n            print(f'Time elapsed updating mangas: {elapsed}, waiting for {wait_time}')\n        except BaseException as e:\n            print(f'An exception occurred during chapters update: {e}')\n        if wait_time:\n            await asyncio.sleep(wait_time)\n"},"\/tools\/flood.py":{"changes":[{"diff":"\n import asyncio\n+from typing import Callable, Awaitable, Any\n \n import pyrogram.errors\n \n \n # retries an async awaitable as long as it raises FloodWait, and waits for err.x time\n-async def retry_on_flood(awaitable):\n-    while True:\n-        try:\n-            return await awaitable\n-        except pyrogram.errors.FloodWait as err:\n-            print(f'FloodWait, waiting {err.x} seconds')\n-            await asyncio.sleep(err.x)\n-            continue\n-        except pyrogram.errors.RPCError as err:\n-            if err.MESSAGE == 'FloodWait':\n+def retry_on_flood(function: Callable[[Any], Awaitable]):\n+    async def wrapper(*args, **kwargs):\n+        while True:\n+            try:\n+                return await function(*args, **kwargs)\n+            except pyrogram.errors.FloodWait as err:\n+                print(f'FloodWait, waiting {err.x} seconds')\n                 await asyncio.sleep(err.x)\n                 continue\n-            else:\n+            except pyrogram.errors.RPCError as err:\n+                if err.MESSAGE == 'FloodWait':\n+                    await asyncio.sleep(err.x)\n+                    continue\n+                else:\n+                    raise err\n+            except Exception as err:\n                 raise err\n-        except Exception as err:\n-            raise err\n+    return wrapper\n","add":16,"remove":13,"filename":"\/tools\/flood.py","badparts":["async def retry_on_flood(awaitable):","    while True:","        try:","            return await awaitable","        except pyrogram.errors.FloodWait as err:","            print(f'FloodWait, waiting {err.x} seconds')","            await asyncio.sleep(err.x)","            continue","        except pyrogram.errors.RPCError as err:","            if err.MESSAGE == 'FloodWait':","            else:","        except Exception as err:","            raise err"],"goodparts":["from typing import Callable, Awaitable, Any","def retry_on_flood(function: Callable[[Any], Awaitable]):","    async def wrapper(*args, **kwargs):","        while True:","            try:","                return await function(*args, **kwargs)","            except pyrogram.errors.FloodWait as err:","                print(f'FloodWait, waiting {err.x} seconds')","            except pyrogram.errors.RPCError as err:","                if err.MESSAGE == 'FloodWait':","                    await asyncio.sleep(err.x)","                    continue","                else:","                    raise err","            except Exception as err:","    return wrapper"]}],"source":"\nimport asyncio import pyrogram.errors async def retry_on_flood(awaitable): while True: try: return await awaitable except pyrogram.errors.FloodWait as err: print(f'FloodWait, waiting{err.x} seconds') await asyncio.sleep(err.x) continue except pyrogram.errors.RPCError as err: if err.MESSAGE=='FloodWait': await asyncio.sleep(err.x) continue else: raise err except Exception as err: raise err ","sourceWithComments":"import asyncio\n\nimport pyrogram.errors\n\n\n# retries an async awaitable as long as it raises FloodWait, and waits for err.x time\nasync def retry_on_flood(awaitable):\n    while True:\n        try:\n            return await awaitable\n        except pyrogram.errors.FloodWait as err:\n            print(f'FloodWait, waiting {err.x} seconds')\n            await asyncio.sleep(err.x)\n            continue\n        except pyrogram.errors.RPCError as err:\n            if err.MESSAGE == 'FloodWait':\n                await asyncio.sleep(err.x)\n                continue\n            else:\n                raise err\n        except Exception as err:\n            raise err\n"}},"msg":"Fix flood handling"}},"https:\/\/github.com\/ChristianZernickel\/mangabot":{"8314e8e4cbd34a856172065eda5871dac57d26b5":{"url":"https:\/\/api.github.com\/repos\/ChristianZernickel\/mangabot\/commits\/8314e8e4cbd34a856172065eda5871dac57d26b5","html_url":"https:\/\/github.com\/ChristianZernickel\/mangabot\/commit\/8314e8e4cbd34a856172065eda5871dac57d26b5","message":"Fix flood handling","sha":"8314e8e4cbd34a856172065eda5871dac57d26b5","keyword":"flooding fix","diff":"diff --git a\/bot.py b\/bot.py\nindex d528584..540d241 100644\n--- a\/bot.py\n+++ b\/bot.py\n@@ -379,10 +379,10 @@ async def chapter_click(client, data, chat_id):\n             cbz = fld2cbz(pictures_folder, ch_name)\n             telegraph_url = await img2tph(chapter, clean(f'{chapter.manga.name} {chapter.name}'))\n \n-            messages: List[Message] = await retry_on_flood(bot.send_media_group(cache_channel, [\n+            messages: List[Message] = await retry_on_flood(bot.send_media_group)(cache_channel, [\n                 InputMediaDocument(pdf, thumb=thumb_path),\n                 InputMediaDocument(cbz, thumb=thumb_path, caption=f'{telegraph_url}')\n-            ]))\n+            ])\n \n             pdf_m, cbz_m = messages\n \n@@ -412,12 +412,12 @@ async def chapter_click(client, data, chat_id):\n             media_docs.append(InputMediaDocument(chapterFile.cbz_id))\n \n         if len(media_docs) == 0:\n-            await retry_on_flood(bot.send_message(chat_id, caption))\n+            await retry_on_flood(bot.send_message)(chat_id, caption)\n         elif len(media_docs) == 1:\n-            await retry_on_flood(bot.send_document(chat_id, media_docs[0].media, caption=caption))\n+            await retry_on_flood(bot.send_document)(chat_id, media_docs[0].media, caption=caption)\n         else:\n             media_docs[-1].caption = caption\n-            await retry_on_flood(bot.send_media_group(chat_id, media_docs))\n+            await retry_on_flood(bot.send_media_group)(chat_id, media_docs)\n         await asyncio.sleep(1)\n \n \ndiff --git a\/tools\/flood.py b\/tools\/flood.py\nindex fdd66dc..de4723f 100644\n--- a\/tools\/flood.py\n+++ b\/tools\/flood.py\n@@ -1,22 +1,25 @@\n import asyncio\n+from typing import Callable, Awaitable, Any\n \n import pyrogram.errors\n \n \n # retries an async awaitable as long as it raises FloodWait, and waits for err.x time\n-async def retry_on_flood(awaitable):\n-    while True:\n-        try:\n-            return await awaitable\n-        except pyrogram.errors.FloodWait as err:\n-            print(f'FloodWait, waiting {err.x} seconds')\n-            await asyncio.sleep(err.x)\n-            continue\n-        except pyrogram.errors.RPCError as err:\n-            if err.MESSAGE == 'FloodWait':\n+def retry_on_flood(function: Callable[[Any], Awaitable]):\n+    async def wrapper(*args, **kwargs):\n+        while True:\n+            try:\n+                return await function(*args, **kwargs)\n+            except pyrogram.errors.FloodWait as err:\n+                print(f'FloodWait, waiting {err.x} seconds')\n                 await asyncio.sleep(err.x)\n                 continue\n-            else:\n+            except pyrogram.errors.RPCError as err:\n+                if err.MESSAGE == 'FloodWait':\n+                    await asyncio.sleep(err.x)\n+                    continue\n+                else:\n+                    raise err\n+            except Exception as err:\n                 raise err\n-        except Exception as err:\n-            raise err\n+    return wrapper\n","files":{"\/bot.py":{"changes":[{"diff":"\n             cbz = fld2cbz(pictures_folder, ch_name)\n             telegraph_url = await img2tph(chapter, clean(f'{chapter.manga.name} {chapter.name}'))\n \n-            messages: List[Message] = await retry_on_flood(bot.send_media_group(cache_channel, [\n+            messages: List[Message] = await retry_on_flood(bot.send_media_group)(cache_channel, [\n                 InputMediaDocument(pdf, thumb=thumb_path),\n                 InputMediaDocument(cbz, thumb=thumb_path, caption=f'{telegraph_url}')\n-            ]))\n+            ])\n \n             pdf_m, cbz_m = messages\n \n","add":2,"remove":2,"filename":"\/bot.py","badparts":["            messages: List[Message] = await retry_on_flood(bot.send_media_group(cache_channel, [","            ]))"],"goodparts":["            messages: List[Message] = await retry_on_flood(bot.send_media_group)(cache_channel, [","            ])"]},{"diff":"\n             media_docs.append(InputMediaDocument(chapterFile.cbz_id))\n \n         if len(media_docs) == 0:\n-            await retry_on_flood(bot.send_message(chat_id, caption))\n+            await retry_on_flood(bot.send_message)(chat_id, caption)\n         elif len(media_docs) == 1:\n-            await retry_on_flood(bot.send_document(chat_id, media_docs[0].media, caption=caption))\n+            await retry_on_flood(bot.send_document)(chat_id, media_docs[0].media, caption=caption)\n         else:\n             media_docs[-1].caption = caption\n-            await retry_on_flood(bot.send_media_group(chat_id, media_docs))\n+            await retry_on_flood(bot.send_media_group)(chat_id, media_docs)\n         await asyncio.sleep(1)\n \n ","add":3,"remove":3,"filename":"\/bot.py","badparts":["            await retry_on_flood(bot.send_message(chat_id, caption))","            await retry_on_flood(bot.send_document(chat_id, media_docs[0].media, caption=caption))","            await retry_on_flood(bot.send_media_group(chat_id, media_docs))"],"goodparts":["            await retry_on_flood(bot.send_message)(chat_id, caption)","            await retry_on_flood(bot.send_document)(chat_id, media_docs[0].media, caption=caption)","            await retry_on_flood(bot.send_media_group)(chat_id, media_docs)"]}],"source":"\nimport enum import shutil from ast import arg import asyncio import re from dataclasses import dataclass import datetime as dt import json import pyrogram.errors from pyrogram.types import Message, InlineKeyboardMarkup, InlineKeyboardButton, CallbackQuery, InputMediaDocument from img2cbz.core import fld2cbz from img2pdf.core import fld2pdf from img2tph.core import img2tph from plugins import MangaClient, ManhuaKoClient, MangaCard, MangaChapter, ManhuaPlusClient, TMOClient, MangaDexClient, \\ MangaSeeClient, MangasInClient, McReaderClient, MangaKakalotClient, ManganeloClient, ManganatoClient, \\ KissMangaClient, MangatigreClient, MangaHasuClient import os from pyrogram import Client, filters from typing import Dict, Tuple, List, TypedDict from models.db import DB, ChapterFile, Subscription, LastChapter, MangaName, MangaOutput from pagination import Pagination from plugins.client import clean from tools.flood import retry_on_flood mangas: Dict[str, MangaCard]=dict() chapters: Dict[str, MangaChapter]=dict() pdfs: Dict[str, str]=dict() paginations: Dict[int, Pagination]=dict() queries: Dict[str, Tuple[MangaClient, str]]=dict() full_pages: Dict[str, List[str]]=dict() favourites: Dict[str, MangaCard]=dict() language_query: Dict[str, Tuple[str, str]]=dict() users_in_channel: Dict[int, dt.datetime]=dict() locks: Dict[int, asyncio.Lock]=dict() plugin_dicts: Dict[str, Dict[str, MangaClient]]={ \"\ud83c\uddec\ud83c\udde7 EN\":{ \"MangaDex\": MangaDexClient(), \"Manhuaplus\": ManhuaPlusClient(), \"Mangasee\": MangaSeeClient(), \"McReader\": McReaderClient(), \"MagaKakalot\": MangaKakalotClient(), \"Manganelo\": ManganeloClient(), \"Manganato\": ManganatoClient(), \"KissManga\": KissMangaClient(), \"MangaHasu\": MangaHasuClient() }, \"\ud83c\uddea\ud83c\uddf8 ES\":{ \"MangaDex\": MangaDexClient(language=(\"es-la\", \"es\")), \"ManhuaKo\": ManhuaKoClient(), \"TMO\": TMOClient(), \"Mangatigre\": MangatigreClient() } } class OutputOptions(enum.IntEnum): PDF=1 CBZ=2 Telegraph=4 def __and__(self, other): return self.value & other def __xor__(self, other): return self.value ^ other def __or__(self, other): return self.value | other disabled=[\"[\ud83c\uddec\ud83c\udde7 EN] McReader\"] plugins=dict() for lang, plugin_dict in plugin_dicts.items(): for name, plugin in plugin_dict.items(): identifier=f'[{lang}]{name}' if identifier in disabled: continue plugins[identifier]=plugin subsPaused=disabled +[] def split_list(li): return[li[x: x +2] for x in range(0, len(li), 2)] def get_buttons_for_options(user_options: int): buttons=[] for option in OutputOptions: checked=\"\u2705\" if option & user_options else \"\u274c\" text=f'{checked}{option.name}' buttons.append([InlineKeyboardButton(text, f\"options_{option.value}\")]) return InlineKeyboardMarkup(buttons) env_file=\"env.json\" if os.path.exists(env_file): with open(env_file) as f: env_vars=json.loads(f.read()) else: env_vars=dict(os.environ) bot=Client('bot', api_id=int(env_vars.get('API_ID')), api_hash=env_vars.get('API_HASH'), bot_token=env_vars.get('BOT_TOKEN')) @bot.on_message(filters=~(filters.private & filters.incoming)) async def on_chat_or_channel_message(client: Client, message: Message): pass @bot.on_message() async def on_private_message(client: Client, message: Message): channel=env_vars.get('CHANNEL') if not channel: return message.continue_propagation() if in_channel_cached:=users_in_channel.get(message.from_user.id): if dt.datetime.now() -in_channel_cached < dt.timedelta(days=1): return message.continue_propagation() try: if await client.get_chat_member(channel, message.from_user.id): users_in_channel[message.from_user.id]=dt.datetime.now() return message.continue_propagation() except pyrogram.errors.UsernameNotOccupied: print(\"Channel does not exist, therefore bot will continue to operate normally\") return message.continue_propagation() except pyrogram.errors.ChatAdminRequired: print(\"Bot is not admin of the channel, therefore bot will continue to operate normally\") return message.continue_propagation() except pyrogram.errors.UserNotParticipant: await message.reply(\"In order to use the bot you must join it's update channel.\", reply_markup=InlineKeyboardMarkup( [[InlineKeyboardButton('Join!', url=f't.me\/{channel}')]] )) @bot.on_message(filters=filters.command(['start'])) async def on_start(client: Client, message: Message): await message.reply(\"Welcome to the best manga pdf bot in telegram!!\\n\" \"\\n\" \"How to use? Just type the name of some manga you want to keep up to date.\\n\" \"\\n\" \"For example:\\n\" \"`Fire Force`\") @bot.on_message(filters=filters.command(['refresh'])) async def on_refresh(client: Client, message: Message): text=message.reply_to_message.text or message.reply_to_message.caption if text: regex=re.compile(r'\\[Read on telegraph]\\((.*)\\)') match=regex.search(text.markdown) else: match=None document=message.reply_to_message.document if not(message.reply_to_message and message.reply_to_message.outgoing and ((document and document.file_name[-4:].lower() in['.pdf', '.cbz']) or match)): return await message.reply(\"This command only works when it replies to a manga file that bot sent to you\") db=DB() if document: chapter=await db.get_chapter_file_by_id(document.file_unique_id) else: chapter=await db.get_chapter_file_by_id(match.group(1)) if not chapter: return await message.reply(\"This file was already refreshed\") await db.erase(chapter) return await message.reply(\"File refreshed successfully!\") @bot.on_message(filters=filters.command(['subs'])) async def on_subs(client: Client, message: Message): db=DB() subs=await db.get_subs(str(message.from_user.id)) lines=[] for sub in subs: lines.append(f'<a href=\"{sub.url}\">{sub.name}<\/a>') lines.append(f'`\/cancel{sub.url}`') lines.append('') if not lines: return await message.reply(\"You have no subscriptions yet.\") body=\"\\n\".join(lines) await message.reply(f'Your subscriptions:\\n\\n{body}', disable_web_page_preview=True) @bot.on_message(filters=filters.regex(r'^\/cancel([^]+)$')) async def on_cancel_command(client: Client, message: Message): db=DB() sub=await db.get(Subscription,(message.matches[0].group(1), str(message.from_user.id))) if not sub: return await message.reply(\"You were not subscribed to that manga.\") await db.erase(sub) return await message.reply(\"You will no longer receive updates for that manga.\") @bot.on_message(filters=filters.command(['options'])) async def on_options_command(client: Client, message: Message): db=DB() user_options=await db.get(MangaOutput, str(message.from_user.id)) user_options=user_options.output if user_options else(1 << 30) -1 buttons=get_buttons_for_options(user_options) return await message.reply(\"Select the desired output format.\", reply_markup=buttons) @bot.on_message(filters=filters.regex(r'^\/')) async def on_unknown_command(client: Client, message: Message): await message.reply(\"Unknown command\") @bot.on_message(filters=filters.text) async def on_message(client, message: Message): language_query[f\"lang_None_{hash(message.text)}\"]=(None, message.text) for language in plugin_dicts.keys(): language_query[f\"lang_{language}_{hash(message.text)}\"]=(language, message.text) await bot.send_message(message.chat.id, \"Select search languages.\", reply_markup=InlineKeyboardMarkup( split_list([InlineKeyboardButton(language, callback_data=f\"lang_{language}_{hash(message.text)}\") for language in plugin_dicts.keys()]) )) async def options_click(client, callback: CallbackQuery): db=DB() user_options=await db.get(MangaOutput, str(callback.from_user.id)) if not user_options: user_options=MangaOutput(user_id=str(callback.from_user.id), output=(2 << 30) -1) option=int(callback.data.split('_')[-1]) user_options.output ^=option buttons=get_buttons_for_options(user_options.output) await db.add(user_options) return await callback.message.edit_reply_markup(reply_markup=buttons) async def language_click(client, callback: CallbackQuery): lang, query=language_query[callback.data] if not lang: return await callback.message.edit(\"Select search languages.\", reply_markup=InlineKeyboardMarkup( split_list([InlineKeyboardButton(language, callback_data=f\"lang_{language}_{hash(query)}\") for language in plugin_dicts.keys()]) )) for identifier, manga_client in plugin_dicts[lang].items(): queries[f\"query_{lang}_{identifier}_{hash(query)}\"]=(manga_client, query) await callback.message.edit(f\"Language:{lang}\\n\\nSelect search plugin.\", reply_markup=InlineKeyboardMarkup( split_list([InlineKeyboardButton(identifier, callback_data=f\"query_{lang}_{identifier}_{hash(query)}\") for identifier in plugin_dicts[lang].keys() if f'[{lang}]{identifier}' not in disabled]) +[ [InlineKeyboardButton(\"\u25c0\ufe0f Back\", callback_data=f\"lang_None_{hash(query)}\")]] )) async def plugin_click(client, callback: CallbackQuery): manga_client, query=queries[callback.data] results=await manga_client.search(query) if not results: await bot.send_message(callback.from_user.id, \"No manga found for given query.\") return for result in results: mangas[result.unique()]=result await bot.send_message(callback.from_user.id, \"This is the result of your search\", reply_markup=InlineKeyboardMarkup([ [InlineKeyboardButton(result.name, callback_data=result.unique())] for result in results ])) async def manga_click(client, callback: CallbackQuery, pagination: Pagination=None): if pagination is None: pagination=Pagination() paginations[pagination.id]=pagination if pagination.manga is None: manga=mangas[callback.data] pagination.manga=manga results=await pagination.manga.client.get_chapters(pagination.manga, pagination.page) if not results: await callback.answer(\"Ups, no chapters there.\", show_alert=True) return full_page_key=f'full_page_{hash(\"\".join([result.unique() for result in results]))}' full_pages[full_page_key]=[] for result in results: chapters[result.unique()]=result full_pages[full_page_key].append(result.unique()) db=DB() subs=await db.get(Subscription,(pagination.manga.url, str(callback.from_user.id))) prev=[InlineKeyboardButton('<<', f'{pagination.id}_{pagination.page -1}')] next_=[InlineKeyboardButton('>>', f'{pagination.id}_{pagination.page +1}')] footer=[prev +next_] if pagination.page > 1 else[next_] fav=[[InlineKeyboardButton( \"Unsubscribe\" if subs else \"Subscribe\", f\"{'unfav' if subs else 'fav'}_{pagination.manga.unique()}\" )]] favourites[f\"fav_{pagination.manga.unique()}\"]=pagination.manga favourites[f\"unfav_{pagination.manga.unique()}\"]=pagination.manga full_page=[[InlineKeyboardButton('Full Page', full_page_key)]] buttons=InlineKeyboardMarkup(fav +footer +[ [InlineKeyboardButton(result.name, result.unique())] for result in results ] +full_page +footer) if pagination.message is None: try: message=await bot.send_photo(callback.from_user.id, pagination.manga.picture_url, f'{pagination.manga.name}\\n' f'{pagination.manga.get_url()}', reply_markup=buttons) pagination.message=message except pyrogram.errors.BadRequest as e: file_name=f'pictures\/{pagination.manga.unique()}.jpg' await pagination.manga.client.get_url(pagination.manga.picture_url, cache=True, file_name=file_name) message=await bot.send_photo(callback.from_user.id, f'.\/cache\/{pagination.manga.client.name}\/{file_name}', f'{pagination.manga.name}\\n' f'{pagination.manga.get_url()}', reply_markup=buttons) pagination.message=message else: await bot.edit_message_reply_markup( callback.from_user.id, pagination.message.message_id, reply_markup=buttons ) async def chapter_click(client, data, chat_id): lock=locks.get(chat_id) if not lock: locks[chat_id]=asyncio.Lock() async with locks[chat_id]: cache_channel=env_vars.get(\"CACHE_CHANNEL\") if not cache_channel: return await bot.send_message(chat_id, \"Bot cache channel is not configured correctly.\") try: cache_channel=int(cache_channel) except ValueError: pass chapter=chapters[data] db=DB() chapterFile=await db.get(ChapterFile, chapter.url) options=await db.get(MangaOutput, str(chat_id)) options=options.output if options else(1 << 30) -1 caption='\\n'.join([ f'{chapter.manga.name} -{chapter.name}', f'{chapter.get_url()}' ]) download=not chapterFile download=download or options & OutputOptions.PDF and not chapterFile.file_id download=download or options & OutputOptions.CBZ and not chapterFile.cbz_id download=download or options & OutputOptions.Telegraph and not chapterFile.telegraph_url download=download and options &((1 << len(OutputOptions)) -1) !=0 if download: pictures_folder=await chapter.client.download_pictures(chapter) if not chapter.pictures: return await bot.send_message(chat_id, f'There was an error parsing this chapter or chapter is missing' + f', please check the chapter at the web\\n\\n{caption}') ch_name=clean(f'{clean(chapter.manga.name, 25)} -{chapter.name}', 45) pdf, thumb_path=fld2pdf(pictures_folder, ch_name) cbz=fld2cbz(pictures_folder, ch_name) telegraph_url=await img2tph(chapter, clean(f'{chapter.manga.name}{chapter.name}')) messages: List[Message]=await retry_on_flood(bot.send_media_group(cache_channel,[ InputMediaDocument(pdf, thumb=thumb_path), InputMediaDocument(cbz, thumb=thumb_path, caption=f'{telegraph_url}') ])) pdf_m, cbz_m=messages if not chapterFile: await db.add(ChapterFile(url=chapter.url, file_id=pdf_m.document.file_id, file_unique_id=pdf_m.document.file_unique_id, cbz_id=cbz_m.document.file_id, cbz_unique_id=cbz_m.document.file_unique_id, telegraph_url=telegraph_url)) else: chapterFile.file_id, chapterFile.file_unique_id, chapterFile.cbz_id, \\ chapterFile.cbz_unique_id, chapterFile.telegraph_url=\\ pdf_m.document.file_id, pdf_m.document.file_unique_id, cbz_m.document.file_id, \\ cbz_m.document.file_unique_id, telegraph_url await db.add(chapterFile) shutil.rmtree(pictures_folder) chapterFile=await db.get(ChapterFile, chapter.url) caption=f'{chapter.manga.name} -{chapter.name}\\n' if options & OutputOptions.Telegraph: caption +=f'[Read on telegraph]({chapterFile.telegraph_url})\\n' caption +=f'[Read on website]({chapter.get_url()})' media_docs=[] if options & OutputOptions.PDF: media_docs.append(InputMediaDocument(chapterFile.file_id)) if options & OutputOptions.CBZ: media_docs.append(InputMediaDocument(chapterFile.cbz_id)) if len(media_docs)==0: await retry_on_flood(bot.send_message(chat_id, caption)) elif len(media_docs)==1: await retry_on_flood(bot.send_document(chat_id, media_docs[0].media, caption=caption)) else: media_docs[-1].caption=caption await retry_on_flood(bot.send_media_group(chat_id, media_docs)) await asyncio.sleep(1) async def pagination_click(client: Client, callback: CallbackQuery): pagination_id, page=map(int, callback.data.split('_')) pagination=paginations[pagination_id] pagination.page=page await manga_click(client, callback, pagination) async def full_page_click(client: Client, callback: CallbackQuery): chapters_data=full_pages[callback.data] for chapter_data in reversed(chapters_data): try: await chapter_click(client, chapter_data, callback.from_user.id) except Exception as e: print(e) await asyncio.sleep(0.5) async def favourite_click(client: Client, callback: CallbackQuery): action, data=callback.data.split('_') fav=action=='fav' manga=favourites[callback.data] db=DB() subs=await db.get(Subscription,(manga.url, str(callback.from_user.id))) if not subs and fav: await db.add(Subscription(url=manga.url, user_id=str(callback.from_user.id))) if subs and not fav: await db.erase(subs) if subs and fav: await callback.answer(\"You are already subscribed\", show_alert=True) if not subs and not fav: await callback.answer(\"You are not subscribed\", show_alert=True) reply_markup=callback.message.reply_markup keyboard=reply_markup.inline_keyboard keyboard[0]=[InlineKeyboardButton( \"Unsubscribe\" if fav else \"Subscribe\", f\"{'unfav' if fav else 'fav'}_{data}\" )] await bot.edit_message_reply_markup(callback.from_user.id, callback.message.message_id, InlineKeyboardMarkup(keyboard)) db_manga=await db.get(MangaName, manga.url) if not db_manga: await db.add(MangaName(url=manga.url, name=manga.name)) def is_pagination_data(callback: CallbackQuery): data=callback.data match=re.match(r'\\d+_\\d+', data) if not match: return False pagination_id=int(data.split('_')[0]) if pagination_id not in paginations: return False pagination=paginations[pagination_id] if not pagination.message: return False if pagination.message.chat.id !=callback.from_user.id: return False if pagination.message.message_id !=callback.message.message_id: return False return True @bot.on_callback_query() async def on_callback_query(client, callback: CallbackQuery): if callback.data in queries: await plugin_click(client, callback) elif callback.data in mangas: await manga_click(client, callback) elif callback.data in chapters: await chapter_click(client, callback.data, callback.from_user.id) elif callback.data in full_pages: await full_page_click(client, callback) elif callback.data in favourites: await favourite_click(client, callback) elif is_pagination_data(callback): await pagination_click(client, callback) elif callback.data in language_query: await language_click(client, callback) elif callback.data.startswith('options'): await options_click(client, callback) else: await bot.answer_callback_query(callback.id, 'This is an old button, please redo the search', show_alert=True) return try: await callback.answer() except BaseException as e: print(e) async def remove_subscriptions(sub: str): db=DB() await db.erase_subs(sub) async def update_mangas(): print(\"Updating mangas\") db=DB() subscriptions=await db.get_all(Subscription) last_chapters=await db.get_all(LastChapter) manga_names=await db.get_all(MangaName) subs_dictionary=dict() chapters_dictionary=dict() url_client_dictionary=dict() client_url_dictionary={client: set() for client in plugins.values()} manga_dict=dict() for subscription in subscriptions: if subscription.url not in subs_dictionary: subs_dictionary[subscription.url]=[] subs_dictionary[subscription.url].append(subscription.user_id) for last_chapter in last_chapters: chapters_dictionary[last_chapter.url]=last_chapter for manga in manga_names: manga_dict[manga.url]=manga for url in subs_dictionary: for ident, client in plugins.items(): if ident in subsPaused: continue if await client.contains_url(url): url_client_dictionary[url]=client client_url_dictionary[client].add(url) for client, urls in client_url_dictionary.items(): to_check=[chapters_dictionary[url] for url in urls if chapters_dictionary.get(url)] if len(to_check)==0: continue try: updated, not_updated=await client.check_updated_urls(to_check) except BaseException as e: print(f\"Error while checking updates for site:{client.name}, err: \", e) not_updated=list(urls) for url in not_updated: del url_client_dictionary[url] updated=dict() for url, client in url_client_dictionary.items(): try: if url not in manga_dict: continue manga_name=manga_dict[url].name if url not in chapters_dictionary: agen=client.iter_chapters(url, manga_name) last_chapter=await anext(agen) await db.add(LastChapter(url=url, chapter_url=last_chapter.url)) await asyncio.sleep(10) else: last_chapter=chapters_dictionary[url] new_chapters: List[MangaChapter]=[] counter=0 async for chapter in client.iter_chapters(url, manga_name): if chapter.url==last_chapter.chapter_url: break new_chapters.append(chapter) counter +=1 if counter==20: break if new_chapters: last_chapter.chapter_url=new_chapters[0].url await db.add(last_chapter) updated[url]=list(reversed(new_chapters)) for chapter in new_chapters: if chapter.unique() not in chapters: chapters[chapter.unique()]=chapter await asyncio.sleep(1) except BaseException as e: print(f'An exception occurred getting new chapters for url{url}:{e}') blocked=set() for url, chapter_list in updated.items(): for chapter in chapter_list: print(f'{chapter.manga.name} -{chapter.name}') for sub in subs_dictionary[url]: if sub in blocked: continue try: await chapter_click(bot, chapter.unique(), int(sub)) except pyrogram.errors.UserIsBlocked: print(f'User{sub} blocked the bot') await remove_subscriptions(sub) blocked.add(sub) except BaseException as e: print(f'An exception occurred sending new chapter:{e}') await asyncio.sleep(0.5) await asyncio.sleep(1) async def manga_updater(): minutes=5 while True: wait_time=minutes * 60 try: start=dt.datetime.now() await update_mangas() elapsed=dt.datetime.now() -start wait_time=max((dt.timedelta(seconds=wait_time) -elapsed).total_seconds(), 0) print(f'Time elapsed updating mangas:{elapsed}, waiting for{wait_time}') except BaseException as e: print(f'An exception occurred during chapters update:{e}') if wait_time: await asyncio.sleep(wait_time) ","sourceWithComments":"import enum\nimport shutil\nfrom ast import arg\nimport asyncio\nimport re\nfrom dataclasses import dataclass\nimport datetime as dt\nimport json\n\nimport pyrogram.errors\nfrom pyrogram.types import Message, InlineKeyboardMarkup, InlineKeyboardButton, CallbackQuery, InputMediaDocument\n\nfrom img2cbz.core import fld2cbz\nfrom img2pdf.core import fld2pdf\nfrom img2tph.core import img2tph\nfrom plugins import MangaClient, ManhuaKoClient, MangaCard, MangaChapter, ManhuaPlusClient, TMOClient, MangaDexClient, \\\n    MangaSeeClient, MangasInClient, McReaderClient, MangaKakalotClient, ManganeloClient, ManganatoClient, \\\n    KissMangaClient, MangatigreClient, MangaHasuClient\nimport os\n\nfrom pyrogram import Client, filters\nfrom typing import Dict, Tuple, List, TypedDict\n\nfrom models.db import DB, ChapterFile, Subscription, LastChapter, MangaName, MangaOutput\nfrom pagination import Pagination\nfrom plugins.client import clean\nfrom tools.flood import retry_on_flood\n\nmangas: Dict[str, MangaCard] = dict()\nchapters: Dict[str, MangaChapter] = dict()\npdfs: Dict[str, str] = dict()\npaginations: Dict[int, Pagination] = dict()\nqueries: Dict[str, Tuple[MangaClient, str]] = dict()\nfull_pages: Dict[str, List[str]] = dict()\nfavourites: Dict[str, MangaCard] = dict()\nlanguage_query: Dict[str, Tuple[str, str]] = dict()\nusers_in_channel: Dict[int, dt.datetime] = dict()\nlocks: Dict[int, asyncio.Lock] = dict()\n\nplugin_dicts: Dict[str, Dict[str, MangaClient]] = {\n    \"\ud83c\uddec\ud83c\udde7 EN\": {\n        \"MangaDex\": MangaDexClient(),\n        \"Manhuaplus\": ManhuaPlusClient(),\n        \"Mangasee\": MangaSeeClient(),\n        \"McReader\": McReaderClient(),\n        \"MagaKakalot\": MangaKakalotClient(),\n        \"Manganelo\": ManganeloClient(),\n        \"Manganato\": ManganatoClient(),\n        \"KissManga\": KissMangaClient(),\n        \"MangaHasu\": MangaHasuClient()\n    },\n    \"\ud83c\uddea\ud83c\uddf8 ES\": {\n        \"MangaDex\": MangaDexClient(language=(\"es-la\", \"es\")),\n        \"ManhuaKo\": ManhuaKoClient(),\n        \"TMO\": TMOClient(),\n        \"Mangatigre\": MangatigreClient()\n    }\n}\n\n\nclass OutputOptions(enum.IntEnum):\n    PDF = 1\n    CBZ = 2\n    Telegraph = 4\n\n    def __and__(self, other):\n        return self.value & other\n\n    def __xor__(self, other):\n        return self.value ^ other\n\n    def __or__(self, other):\n        return self.value | other\n\n\ndisabled = [\"[\ud83c\uddec\ud83c\udde7 EN] McReader\"]\n\nplugins = dict()\nfor lang, plugin_dict in plugin_dicts.items():\n    for name, plugin in plugin_dict.items():\n        identifier = f'[{lang}] {name}'\n        if identifier in disabled:\n            continue\n        plugins[identifier] = plugin\n\n# subsPaused = [\"[\ud83c\uddea\ud83c\uddf8 ES] TMO\"]\nsubsPaused = disabled + []\n\n\ndef split_list(li):\n    return [li[x: x + 2] for x in range(0, len(li), 2)]\n\n\ndef get_buttons_for_options(user_options: int):\n    buttons = []\n    for option in OutputOptions:\n        checked = \"\u2705\" if option & user_options else \"\u274c\"\n        text = f'{checked} {option.name}'\n        buttons.append([InlineKeyboardButton(text, f\"options_{option.value}\")])\n    return InlineKeyboardMarkup(buttons)\n\n\nenv_file = \"env.json\"\nif os.path.exists(env_file):\n    with open(env_file) as f:\n        env_vars = json.loads(f.read())\nelse:\n    env_vars = dict(os.environ)\n\nbot = Client('bot',\n             api_id=int(env_vars.get('API_ID')),\n             api_hash=env_vars.get('API_HASH'),\n             bot_token=env_vars.get('BOT_TOKEN'))\n\n\n@bot.on_message(filters=~(filters.private & filters.incoming))\nasync def on_chat_or_channel_message(client: Client, message: Message):\n    pass\n\n\n@bot.on_message()\nasync def on_private_message(client: Client, message: Message):\n    channel = env_vars.get('CHANNEL')\n    if not channel:\n        return message.continue_propagation()\n    if in_channel_cached := users_in_channel.get(message.from_user.id):\n        if dt.datetime.now() - in_channel_cached < dt.timedelta(days=1):\n            return message.continue_propagation()\n    try:\n        if await client.get_chat_member(channel, message.from_user.id):\n            users_in_channel[message.from_user.id] = dt.datetime.now()\n            return message.continue_propagation()\n    except pyrogram.errors.UsernameNotOccupied:\n        print(\"Channel does not exist, therefore bot will continue to operate normally\")\n        return message.continue_propagation()\n    except pyrogram.errors.ChatAdminRequired:\n        print(\"Bot is not admin of the channel, therefore bot will continue to operate normally\")\n        return message.continue_propagation()\n    except pyrogram.errors.UserNotParticipant:\n        await message.reply(\"In order to use the bot you must join it's update channel.\",\n                            reply_markup=InlineKeyboardMarkup(\n                                [[InlineKeyboardButton('Join!', url=f't.me\/{channel}')]]\n                            ))\n\n\n@bot.on_message(filters=filters.command(['start']))\nasync def on_start(client: Client, message: Message):\n    await message.reply(\"Welcome to the best manga pdf bot in telegram!!\\n\"\n                        \"\\n\"\n                        \"How to use? Just type the name of some manga you want to keep up to date.\\n\"\n                        \"\\n\"\n                        \"For example:\\n\"\n                        \"`Fire Force`\")\n\n\n@bot.on_message(filters=filters.command(['refresh']))\nasync def on_refresh(client: Client, message: Message):\n    text = message.reply_to_message.text or message.reply_to_message.caption\n    if text:\n        regex = re.compile(r'\\[Read on telegraph]\\((.*)\\)')\n        match = regex.search(text.markdown)\n    else:\n        match = None\n    document = message.reply_to_message.document\n    if not (message.reply_to_message and message.reply_to_message.outgoing and\n            ((document and document.file_name[-4:].lower() in ['.pdf', '.cbz']) or match)):\n        return await message.reply(\"This command only works when it replies to a manga file that bot sent to you\")\n    db = DB()\n    if document:\n        chapter = await db.get_chapter_file_by_id(document.file_unique_id)\n    else:\n        chapter = await db.get_chapter_file_by_id(match.group(1))\n    if not chapter:\n        return await message.reply(\"This file was already refreshed\")\n    await db.erase(chapter)\n    return await message.reply(\"File refreshed successfully!\")\n\n\n@bot.on_message(filters=filters.command(['subs']))\nasync def on_subs(client: Client, message: Message):\n    db = DB()\n    subs = await db.get_subs(str(message.from_user.id))\n    lines = []\n    for sub in subs:\n        lines.append(f'<a href=\"{sub.url}\">{sub.name}<\/a>')\n        lines.append(f'`\/cancel {sub.url}`')\n        lines.append('')\n\n    if not lines:\n        return await message.reply(\"You have no subscriptions yet.\")\n    body = \"\\n\".join(lines)\n    await message.reply(f'Your subscriptions:\\n\\n{body}', disable_web_page_preview=True)\n\n\n@bot.on_message(filters=filters.regex(r'^\/cancel ([^ ]+)$'))\nasync def on_cancel_command(client: Client, message: Message):\n    db = DB()\n    sub = await db.get(Subscription, (message.matches[0].group(1), str(message.from_user.id)))\n    if not sub:\n        return await message.reply(\"You were not subscribed to that manga.\")\n    await db.erase(sub)\n    return await message.reply(\"You will no longer receive updates for that manga.\")\n\n\n@bot.on_message(filters=filters.command(['options']))\nasync def on_options_command(client: Client, message: Message):\n    db = DB()\n    user_options = await db.get(MangaOutput, str(message.from_user.id))\n    user_options = user_options.output if user_options else (1 << 30) - 1\n    buttons = get_buttons_for_options(user_options)\n    return await message.reply(\"Select the desired output format.\", reply_markup=buttons)\n\n\n@bot.on_message(filters=filters.regex(r'^\/'))\nasync def on_unknown_command(client: Client, message: Message):\n    await message.reply(\"Unknown command\")\n\n\n@bot.on_message(filters=filters.text)\nasync def on_message(client, message: Message):\n    language_query[f\"lang_None_{hash(message.text)}\"] = (None, message.text)\n    for language in plugin_dicts.keys():\n        language_query[f\"lang_{language}_{hash(message.text)}\"] = (language, message.text)\n    await bot.send_message(message.chat.id, \"Select search languages.\", reply_markup=InlineKeyboardMarkup(\n        split_list([InlineKeyboardButton(language, callback_data=f\"lang_{language}_{hash(message.text)}\")\n                    for language in plugin_dicts.keys()])\n    ))\n\n\nasync def options_click(client, callback: CallbackQuery):\n    db = DB()\n    user_options = await db.get(MangaOutput, str(callback.from_user.id))\n    if not user_options:\n        user_options = MangaOutput(user_id=str(callback.from_user.id), output=(2 << 30) - 1)\n    option = int(callback.data.split('_')[-1])\n    user_options.output ^= option\n    buttons = get_buttons_for_options(user_options.output)\n    await db.add(user_options)\n    return await callback.message.edit_reply_markup(reply_markup=buttons)\n\n\nasync def language_click(client, callback: CallbackQuery):\n    lang, query = language_query[callback.data]\n    if not lang:\n        return await callback.message.edit(\"Select search languages.\", reply_markup=InlineKeyboardMarkup(\n            split_list([InlineKeyboardButton(language, callback_data=f\"lang_{language}_{hash(query)}\")\n                        for language in plugin_dicts.keys()])\n        ))\n    for identifier, manga_client in plugin_dicts[lang].items():\n        queries[f\"query_{lang}_{identifier}_{hash(query)}\"] = (manga_client, query)\n    await callback.message.edit(f\"Language: {lang}\\n\\nSelect search plugin.\", reply_markup=InlineKeyboardMarkup(\n        split_list([InlineKeyboardButton(identifier, callback_data=f\"query_{lang}_{identifier}_{hash(query)}\")\n                    for identifier in plugin_dicts[lang].keys() if f'[{lang}] {identifier}' not in disabled]) + [\n            [InlineKeyboardButton(\"\u25c0\ufe0f Back\", callback_data=f\"lang_None_{hash(query)}\")]]\n    ))\n\n\nasync def plugin_click(client, callback: CallbackQuery):\n    manga_client, query = queries[callback.data]\n    results = await manga_client.search(query)\n    if not results:\n        await bot.send_message(callback.from_user.id, \"No manga found for given query.\")\n        return\n    for result in results:\n        mangas[result.unique()] = result\n    await bot.send_message(callback.from_user.id,\n                           \"This is the result of your search\",\n                           reply_markup=InlineKeyboardMarkup([\n                               [InlineKeyboardButton(result.name, callback_data=result.unique())] for result in results\n                           ]))\n\n\nasync def manga_click(client, callback: CallbackQuery, pagination: Pagination = None):\n    if pagination is None:\n        pagination = Pagination()\n        paginations[pagination.id] = pagination\n\n    if pagination.manga is None:\n        manga = mangas[callback.data]\n        pagination.manga = manga\n\n    results = await pagination.manga.client.get_chapters(pagination.manga, pagination.page)\n\n    if not results:\n        await callback.answer(\"Ups, no chapters there.\", show_alert=True)\n        return\n\n    full_page_key = f'full_page_{hash(\"\".join([result.unique() for result in results]))}'\n    full_pages[full_page_key] = []\n    for result in results:\n        chapters[result.unique()] = result\n        full_pages[full_page_key].append(result.unique())\n\n    db = DB()\n    subs = await db.get(Subscription, (pagination.manga.url, str(callback.from_user.id)))\n\n    prev = [InlineKeyboardButton('<<', f'{pagination.id}_{pagination.page - 1}')]\n    next_ = [InlineKeyboardButton('>>', f'{pagination.id}_{pagination.page + 1}')]\n    footer = [prev + next_] if pagination.page > 1 else [next_]\n\n    fav = [[InlineKeyboardButton(\n        \"Unsubscribe\" if subs else \"Subscribe\",\n        f\"{'unfav' if subs else 'fav'}_{pagination.manga.unique()}\"\n    )]]\n    favourites[f\"fav_{pagination.manga.unique()}\"] = pagination.manga\n    favourites[f\"unfav_{pagination.manga.unique()}\"] = pagination.manga\n\n    full_page = [[InlineKeyboardButton('Full Page', full_page_key)]]\n\n    buttons = InlineKeyboardMarkup(fav + footer + [\n        [InlineKeyboardButton(result.name, result.unique())] for result in results\n    ] + full_page + footer)\n\n    if pagination.message is None:\n        try:\n            message = await bot.send_photo(callback.from_user.id,\n                                           pagination.manga.picture_url,\n                                           f'{pagination.manga.name}\\n'\n                                           f'{pagination.manga.get_url()}', reply_markup=buttons)\n            pagination.message = message\n        except pyrogram.errors.BadRequest as e:\n            file_name = f'pictures\/{pagination.manga.unique()}.jpg'\n            await pagination.manga.client.get_url(pagination.manga.picture_url, cache=True, file_name=file_name)\n            message = await bot.send_photo(callback.from_user.id,\n                                           f'.\/cache\/{pagination.manga.client.name}\/{file_name}',\n                                           f'{pagination.manga.name}\\n'\n                                           f'{pagination.manga.get_url()}', reply_markup=buttons)\n            pagination.message = message\n    else:\n        await bot.edit_message_reply_markup(\n            callback.from_user.id,\n            pagination.message.message_id,\n            reply_markup=buttons\n        )\n\n\nasync def chapter_click(client, data, chat_id):\n    lock = locks.get(chat_id)\n    if not lock:\n        locks[chat_id] = asyncio.Lock()\n\n    async with locks[chat_id]:\n        cache_channel = env_vars.get(\"CACHE_CHANNEL\")\n        if not cache_channel:\n            return await bot.send_message(chat_id, \"Bot cache channel is not configured correctly.\")\n\n        # Try convert to int cache_channel, because it can be id or username\n        try:\n            cache_channel = int(cache_channel)\n        except ValueError:\n            pass\n\n        chapter = chapters[data]\n\n        db = DB()\n\n        chapterFile = await db.get(ChapterFile, chapter.url)\n        options = await db.get(MangaOutput, str(chat_id))\n        options = options.output if options else (1 << 30) - 1\n\n        caption = '\\n'.join([\n            f'{chapter.manga.name} - {chapter.name}',\n            f'{chapter.get_url()}'\n        ])\n\n        download = not chapterFile\n        download = download or options & OutputOptions.PDF and not chapterFile.file_id\n        download = download or options & OutputOptions.CBZ and not chapterFile.cbz_id\n        download = download or options & OutputOptions.Telegraph and not chapterFile.telegraph_url\n        download = download and options & ((1 << len(OutputOptions)) - 1) != 0\n\n        if download:\n            pictures_folder = await chapter.client.download_pictures(chapter)\n            if not chapter.pictures:\n                return await bot.send_message(chat_id, f'There was an error parsing this chapter or chapter is missing' +\n                                              f', please check the chapter at the web\\n\\n{caption}')\n            ch_name = clean(f'{clean(chapter.manga.name, 25)} - {chapter.name}', 45)\n            pdf, thumb_path = fld2pdf(pictures_folder, ch_name)\n            cbz = fld2cbz(pictures_folder, ch_name)\n            telegraph_url = await img2tph(chapter, clean(f'{chapter.manga.name} {chapter.name}'))\n\n            messages: List[Message] = await retry_on_flood(bot.send_media_group(cache_channel, [\n                InputMediaDocument(pdf, thumb=thumb_path),\n                InputMediaDocument(cbz, thumb=thumb_path, caption=f'{telegraph_url}')\n            ]))\n\n            pdf_m, cbz_m = messages\n\n            if not chapterFile:\n                await db.add(ChapterFile(url=chapter.url, file_id=pdf_m.document.file_id,\n                                         file_unique_id=pdf_m.document.file_unique_id, cbz_id=cbz_m.document.file_id,\n                                         cbz_unique_id=cbz_m.document.file_unique_id, telegraph_url=telegraph_url))\n            else:\n                chapterFile.file_id, chapterFile.file_unique_id, chapterFile.cbz_id, \\\n                chapterFile.cbz_unique_id, chapterFile.telegraph_url = \\\n                    pdf_m.document.file_id, pdf_m.document.file_unique_id, cbz_m.document.file_id, \\\n                    cbz_m.document.file_unique_id, telegraph_url\n                await db.add(chapterFile)\n\n            shutil.rmtree(pictures_folder)\n\n        chapterFile = await db.get(ChapterFile, chapter.url)\n\n        caption = f'{chapter.manga.name} - {chapter.name}\\n'\n        if options & OutputOptions.Telegraph:\n            caption += f'[Read on telegraph]({chapterFile.telegraph_url})\\n'\n        caption += f'[Read on website]({chapter.get_url()})'\n        media_docs = []\n        if options & OutputOptions.PDF:\n            media_docs.append(InputMediaDocument(chapterFile.file_id))\n        if options & OutputOptions.CBZ:\n            media_docs.append(InputMediaDocument(chapterFile.cbz_id))\n\n        if len(media_docs) == 0:\n            await retry_on_flood(bot.send_message(chat_id, caption))\n        elif len(media_docs) == 1:\n            await retry_on_flood(bot.send_document(chat_id, media_docs[0].media, caption=caption))\n        else:\n            media_docs[-1].caption = caption\n            await retry_on_flood(bot.send_media_group(chat_id, media_docs))\n        await asyncio.sleep(1)\n\n\nasync def pagination_click(client: Client, callback: CallbackQuery):\n    pagination_id, page = map(int, callback.data.split('_'))\n    pagination = paginations[pagination_id]\n    pagination.page = page\n    await manga_click(client, callback, pagination)\n\n\nasync def full_page_click(client: Client, callback: CallbackQuery):\n    chapters_data = full_pages[callback.data]\n    for chapter_data in reversed(chapters_data):\n        try:\n            await chapter_click(client, chapter_data, callback.from_user.id)\n        except Exception as e:\n            print(e)\n        await asyncio.sleep(0.5)\n\n\nasync def favourite_click(client: Client, callback: CallbackQuery):\n    action, data = callback.data.split('_')\n    fav = action == 'fav'\n    manga = favourites[callback.data]\n    db = DB()\n    subs = await db.get(Subscription, (manga.url, str(callback.from_user.id)))\n    if not subs and fav:\n        await db.add(Subscription(url=manga.url, user_id=str(callback.from_user.id)))\n    if subs and not fav:\n        await db.erase(subs)\n    if subs and fav:\n        await callback.answer(\"You are already subscribed\", show_alert=True)\n    if not subs and not fav:\n        await callback.answer(\"You are not subscribed\", show_alert=True)\n    reply_markup = callback.message.reply_markup\n    keyboard = reply_markup.inline_keyboard\n    keyboard[0] = [InlineKeyboardButton(\n        \"Unsubscribe\" if fav else \"Subscribe\",\n        f\"{'unfav' if fav else 'fav'}_{data}\"\n    )]\n    await bot.edit_message_reply_markup(callback.from_user.id, callback.message.message_id,\n                                        InlineKeyboardMarkup(keyboard))\n    db_manga = await db.get(MangaName, manga.url)\n    if not db_manga:\n        await db.add(MangaName(url=manga.url, name=manga.name))\n\n\ndef is_pagination_data(callback: CallbackQuery):\n    data = callback.data\n    match = re.match(r'\\d+_\\d+', data)\n    if not match:\n        return False\n    pagination_id = int(data.split('_')[0])\n    if pagination_id not in paginations:\n        return False\n    pagination = paginations[pagination_id]\n    if not pagination.message:\n        return False\n    if pagination.message.chat.id != callback.from_user.id:\n        return False\n    if pagination.message.message_id != callback.message.message_id:\n        return False\n    return True\n\n\n@bot.on_callback_query()\nasync def on_callback_query(client, callback: CallbackQuery):\n    if callback.data in queries:\n        await plugin_click(client, callback)\n    elif callback.data in mangas:\n        await manga_click(client, callback)\n    elif callback.data in chapters:\n        await chapter_click(client, callback.data, callback.from_user.id)\n    elif callback.data in full_pages:\n        await full_page_click(client, callback)\n    elif callback.data in favourites:\n        await favourite_click(client, callback)\n    elif is_pagination_data(callback):\n        await pagination_click(client, callback)\n    elif callback.data in language_query:\n        await language_click(client, callback)\n    elif callback.data.startswith('options'):\n        await options_click(client, callback)\n    else:\n        await bot.answer_callback_query(callback.id, 'This is an old button, please redo the search', show_alert=True)\n        return\n    try:\n        await callback.answer()\n    except BaseException as e:\n        print(e)\n\n\nasync def remove_subscriptions(sub: str):\n    db = DB()\n\n    await db.erase_subs(sub)\n\n\nasync def update_mangas():\n    print(\"Updating mangas\")\n    db = DB()\n    subscriptions = await db.get_all(Subscription)\n    last_chapters = await db.get_all(LastChapter)\n    manga_names = await db.get_all(MangaName)\n\n    subs_dictionary = dict()\n    chapters_dictionary = dict()\n    url_client_dictionary = dict()\n    client_url_dictionary = {client: set() for client in plugins.values()}\n    manga_dict = dict()\n\n    for subscription in subscriptions:\n        if subscription.url not in subs_dictionary:\n            subs_dictionary[subscription.url] = []\n        subs_dictionary[subscription.url].append(subscription.user_id)\n\n    for last_chapter in last_chapters:\n        chapters_dictionary[last_chapter.url] = last_chapter\n\n    for manga in manga_names:\n        manga_dict[manga.url] = manga\n\n    for url in subs_dictionary:\n        for ident, client in plugins.items():\n            if ident in subsPaused:\n                continue\n            if await client.contains_url(url):\n                url_client_dictionary[url] = client\n                client_url_dictionary[client].add(url)\n\n    for client, urls in client_url_dictionary.items():\n        # print('')\n        # print(f'Updating {client.name}')\n        # print(f'Urls:\\t{list(urls)}')\n        # new_urls = [url for url in urls if not chapters_dictionary.get(url)]\n        # print(f'New Urls:\\t{new_urls}')\n        to_check = [chapters_dictionary[url] for url in urls if chapters_dictionary.get(url)]\n        if len(to_check) == 0:\n            continue\n        try:\n            updated, not_updated = await client.check_updated_urls(to_check)\n        except BaseException as e:\n            print(f\"Error while checking updates for site: {client.name}, err: \", e)\n            not_updated = list(urls)\n        for url in not_updated:\n            del url_client_dictionary[url]\n        # print(f'Updated:\\t{list(updated)}')\n        # print(f'Not Updated:\\t{list(not_updated)}')\n\n    updated = dict()\n\n    for url, client in url_client_dictionary.items():\n        try:\n            if url not in manga_dict:\n                continue\n            manga_name = manga_dict[url].name\n            if url not in chapters_dictionary:\n                agen = client.iter_chapters(url, manga_name)\n                last_chapter = await anext(agen)\n                await db.add(LastChapter(url=url, chapter_url=last_chapter.url))\n                await asyncio.sleep(10)\n            else:\n                last_chapter = chapters_dictionary[url]\n                new_chapters: List[MangaChapter] = []\n                counter = 0\n                async for chapter in client.iter_chapters(url, manga_name):\n                    if chapter.url == last_chapter.chapter_url:\n                        break\n                    new_chapters.append(chapter)\n                    counter += 1\n                    if counter == 20:\n                        break\n                if new_chapters:\n                    last_chapter.chapter_url = new_chapters[0].url\n                    await db.add(last_chapter)\n                    updated[url] = list(reversed(new_chapters))\n                    for chapter in new_chapters:\n                        if chapter.unique() not in chapters:\n                            chapters[chapter.unique()] = chapter\n                await asyncio.sleep(1)\n        except BaseException as e:\n            print(f'An exception occurred getting new chapters for url {url}: {e}')\n\n    blocked = set()\n    for url, chapter_list in updated.items():\n        for chapter in chapter_list:\n            print(f'{chapter.manga.name} - {chapter.name}')\n            for sub in subs_dictionary[url]:\n                if sub in blocked:\n                    continue\n                try:\n                    await chapter_click(bot, chapter.unique(), int(sub))\n                except pyrogram.errors.UserIsBlocked:\n                    print(f'User {sub} blocked the bot')\n                    await remove_subscriptions(sub)\n                    blocked.add(sub)\n                except BaseException as e:\n                    print(f'An exception occurred sending new chapter: {e}')\n                await asyncio.sleep(0.5)\n            await asyncio.sleep(1)\n\n\nasync def manga_updater():\n    minutes = 5\n    while True:\n        wait_time = minutes * 60\n        try:\n            start = dt.datetime.now()\n            await update_mangas()\n            elapsed = dt.datetime.now() - start\n            wait_time = max((dt.timedelta(seconds=wait_time) - elapsed).total_seconds(), 0)\n            print(f'Time elapsed updating mangas: {elapsed}, waiting for {wait_time}')\n        except BaseException as e:\n            print(f'An exception occurred during chapters update: {e}')\n        if wait_time:\n            await asyncio.sleep(wait_time)\n"},"\/tools\/flood.py":{"changes":[{"diff":"\n import asyncio\n+from typing import Callable, Awaitable, Any\n \n import pyrogram.errors\n \n \n # retries an async awaitable as long as it raises FloodWait, and waits for err.x time\n-async def retry_on_flood(awaitable):\n-    while True:\n-        try:\n-            return await awaitable\n-        except pyrogram.errors.FloodWait as err:\n-            print(f'FloodWait, waiting {err.x} seconds')\n-            await asyncio.sleep(err.x)\n-            continue\n-        except pyrogram.errors.RPCError as err:\n-            if err.MESSAGE == 'FloodWait':\n+def retry_on_flood(function: Callable[[Any], Awaitable]):\n+    async def wrapper(*args, **kwargs):\n+        while True:\n+            try:\n+                return await function(*args, **kwargs)\n+            except pyrogram.errors.FloodWait as err:\n+                print(f'FloodWait, waiting {err.x} seconds')\n                 await asyncio.sleep(err.x)\n                 continue\n-            else:\n+            except pyrogram.errors.RPCError as err:\n+                if err.MESSAGE == 'FloodWait':\n+                    await asyncio.sleep(err.x)\n+                    continue\n+                else:\n+                    raise err\n+            except Exception as err:\n                 raise err\n-        except Exception as err:\n-            raise err\n+    return wrapper\n","add":16,"remove":13,"filename":"\/tools\/flood.py","badparts":["async def retry_on_flood(awaitable):","    while True:","        try:","            return await awaitable","        except pyrogram.errors.FloodWait as err:","            print(f'FloodWait, waiting {err.x} seconds')","            await asyncio.sleep(err.x)","            continue","        except pyrogram.errors.RPCError as err:","            if err.MESSAGE == 'FloodWait':","            else:","        except Exception as err:","            raise err"],"goodparts":["from typing import Callable, Awaitable, Any","def retry_on_flood(function: Callable[[Any], Awaitable]):","    async def wrapper(*args, **kwargs):","        while True:","            try:","                return await function(*args, **kwargs)","            except pyrogram.errors.FloodWait as err:","                print(f'FloodWait, waiting {err.x} seconds')","            except pyrogram.errors.RPCError as err:","                if err.MESSAGE == 'FloodWait':","                    await asyncio.sleep(err.x)","                    continue","                else:","                    raise err","            except Exception as err:","    return wrapper"]}],"source":"\nimport asyncio import pyrogram.errors async def retry_on_flood(awaitable): while True: try: return await awaitable except pyrogram.errors.FloodWait as err: print(f'FloodWait, waiting{err.x} seconds') await asyncio.sleep(err.x) continue except pyrogram.errors.RPCError as err: if err.MESSAGE=='FloodWait': await asyncio.sleep(err.x) continue else: raise err except Exception as err: raise err ","sourceWithComments":"import asyncio\n\nimport pyrogram.errors\n\n\n# retries an async awaitable as long as it raises FloodWait, and waits for err.x time\nasync def retry_on_flood(awaitable):\n    while True:\n        try:\n            return await awaitable\n        except pyrogram.errors.FloodWait as err:\n            print(f'FloodWait, waiting {err.x} seconds')\n            await asyncio.sleep(err.x)\n            continue\n        except pyrogram.errors.RPCError as err:\n            if err.MESSAGE == 'FloodWait':\n                await asyncio.sleep(err.x)\n                continue\n            else:\n                raise err\n        except Exception as err:\n            raise err\n"}},"msg":"Fix flood handling"}},"https:\/\/github.com\/tobinatore\/Bachelorarbeit":{"74fcd44567ac6ea2ccb5d63cb31e88629cd85a17":{"url":"https:\/\/api.github.com\/repos\/tobinatore\/Bachelorarbeit\/commits\/74fcd44567ac6ea2ccb5d63cb31e88629cd85a17","html_url":"https:\/\/github.com\/tobinatore\/Bachelorarbeit\/commit\/74fcd44567ac6ea2ccb5d63cb31e88629cd85a17","message":"fixed a security flaw that made the framework itself vulnerable to flooding attacks","sha":"74fcd44567ac6ea2ccb5d63cb31e88629cd85a17","keyword":"flooding vulnerability","diff":"diff --git a\/Szenario_1\/1\/framework.py b\/Szenario_1\/1\/framework.py\nindex 861a4eb..56a5e73 100644\n--- a\/Szenario_1\/1\/framework.py\n+++ b\/Szenario_1\/1\/framework.py\n@@ -62,10 +62,11 @@ def interval_reset(nm: NodeManager) -> None:\n     nm.reset_time_reached()\n \n \n-def wait_for_message(sock: socket.socket, wm: WorkerManager):\n+def wait_for_message(sock: socket.socket, wm: WorkerManager, nm: NodeManager):\n     while True:\n         message, addr = sock.recvfrom(196)\n-        threading.Thread(target=wm.add_bundle, args=[message]).start()\n+        if not util.utils.get_bundle_source(message) in nm.get_banned_nodes():\n+            threading.Thread(target=wm.add_bundle, args=[message]).start()\n \n \n def listen(nm: NodeManager, wm: WorkerManager) -> None:\n@@ -80,7 +81,7 @@ def listen(nm: NodeManager, wm: WorkerManager) -> None:\n             sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n             sock.bind((address[1], 4555))\n             print(\"Bound Socket to interface \" + address[0] + \", IP: \" + address[1])\n-            threading.Thread(target=wait_for_message, args=[sock, wm]).start()\n+            threading.Thread(target=wait_for_message, args=[sock, wm, nm]).start()\n             sockets.append(sock)\n \n \ndiff --git a\/Szenario_1\/1\/util\/nodemanager.py b\/Szenario_1\/1\/util\/nodemanager.py\nindex 8a1ec2c..5e92424 100644\n--- a\/Szenario_1\/1\/util\/nodemanager.py\n+++ b\/Szenario_1\/1\/util\/nodemanager.py\n@@ -4,7 +4,7 @@\n import os\n import subprocess\n import time\n-from typing import Dict, List\n+from typing import Dict, List, Set\n import threading\n \n import pyion\n@@ -42,6 +42,8 @@ class NodeManager:\n     __reset_interval: int\n     __bundles_last_interval: Dict[str, int]\n     reset_flag: bool\n+    __mem_percentage: float\n+    __banned_nodes: Set[str]\n \n     # ------------------------\n     # |    INITIALIZATION    |\n@@ -95,6 +97,8 @@ def __init__(self, path: str) -> None:\n         self.__reset_interval = int(config[\"TRUST_CONFIG\"][\"reset_interval\"])\n         self.__bundles_last_interval = {node: 0 for node in self.__neighbours}\n         self.reset_flag = False\n+        self.__mem_percentage = 0.0\n+        self.__banned_nodes = set()\n \n         # Logging the results\n         self.__logger.info(\n@@ -211,6 +215,7 @@ def update_trust_scores(self, new_scores: Dict[str, float]) -> None:\n             new_scores (Dict[str, float]): A dict containing the new scores.\n         \"\"\"\n         self.__trust_scores = new_scores\n+        self.__logger.info(\"New Trust Scores:\" + str(self.__trust_scores))\n \n     def can_accept_bundle(self, node: str) -> bool:\n         \"\"\"Compares memory usage with the cutoff value\n@@ -227,7 +232,7 @@ def can_accept_bundle(self, node: str) -> bool:\n         \"\"\"\n \n         current_trust = self.__trust_scores[node]\n-\n+        self.__logger.info(\"TRUST FOR NODE \" + node + \": \" + str(current_trust))\n         if current_trust > 0:\n             # find cutoff by evaluating all key of the dict\n             # and choosing the one closest (but still less than)\n@@ -240,36 +245,13 @@ def can_accept_bundle(self, node: str) -> bool:\n                     if key <= current_trust\n                 )\n             ]\n-            nn = self.__node_num\n-\n-            # change directories to the node's directory\n-            # so the SDR can be queried\n-            os.chdir(self.__node_dir)\n \n-            # Query ION's SDR memory\n-            process = subprocess.run(\n-                [\"ip netns exec nns\" + nn + \" sdrwatch n\" + nn],\n-                capture_output=True,\n-                shell=True,\n-            )\n-            stdout = process.stdout\n-            stdout = stdout.decode(\"utf-8\").split(\"\\n\")\n-            for string in stdout:\n-                if \"total now in use\" in string:\n-                    used = string\n-                elif \"total heap size\" in string:\n-                    total = string\n-\n-            # sdrwatch returns a string in the format \"total heap:              40000\"\n-            # -> the number needs to be extracted\n-            total = [int(n) for n in total.split() if n.isdigit()][0]\n-            used = [int(n) for n in used.split() if n.isdigit()][0]\n-            percentage = (float(used) \/ total) * 100\n-\n-            return True if percentage < cutoff else False\n+            return True if self.__mem_percentage < cutoff else False\n         else:\n             # the sender has a trust score of 0\n             # -> do not accept any of its bundles\n+            self.__logger.info(\"NOT ACCEPTING BUNDLE\")\n+            self.__banned_nodes.add(node)\n             return False\n \n     def reset_time_reached(self) -> None:\n@@ -281,4 +263,42 @@ def reset_time_reached(self) -> None:\n             self.__threshold_flooding,\n             self.__penalty_growth_rate,\n             self.__trust_recovery_rate,\n-        )\n\\ No newline at end of file\n+        )\n+        self.__mem_percentage = self.calc_memory_percentage()\n+\n+    def calc_memory_percentage(self) -> float:\n+        \"\"\"Calculates the percentage of unavailable memory space.\n+\n+        Returns:\n+            float: Percentage of memory used\n+        \"\"\"\n+        nn = self.__node_num\n+\n+        # change directories to the node's directory\n+        # so the SDR can be queried\n+        os.chdir(self.__node_dir)\n+\n+        # Query ION's SDR memory\n+        process = subprocess.run(\n+            [\"ip netns exec nns\" + nn + \" sdrwatch n\" + nn],\n+            capture_output=True,\n+            shell=True,\n+        )\n+        stdout = process.stdout\n+        stdout = stdout.decode(\"utf-8\").split(\"\\n\")\n+        for string in stdout:\n+            if \"total now in use\" in string:\n+                used = string\n+            elif \"total heap size\" in string:\n+                total = string\n+\n+        # sdrwatch returns a string in the format \"total heap:              40000\"\n+        # -> the number needs to be extracted\n+        total = [int(n) for n in total.split() if n.isdigit()][0]\n+        used = [int(n) for n in used.split() if n.isdigit()][0]\n+        perc = (float(used) \/ total) * 100\n+        self.__logger.info(\"Memory in use:\" + str(perc))\n+        return perc\n+\n+    def get_banned_nodes(self) -> List[str]:\n+        return self.__banned_nodes\ndiff --git a\/Szenario_1\/1\/util\/utils.py b\/Szenario_1\/1\/util\/utils.py\nindex bcb945e..6603a38 100644\n--- a\/Szenario_1\/1\/util\/utils.py\n+++ b\/Szenario_1\/1\/util\/utils.py\n@@ -46,7 +46,7 @@ def calc_penalty(\n     \"\"\"\n     for node in bundles:\n         value = bundles[node]\n-        logger.info(\"Accepted bundles from node \" + str(node) + \": \" + str(value))\n+        logger.info(\"Bundles from node \" + str(node) + \": \" + str(value))\n         if value > threshold:\n             scores[node] = max(\n                 (scores[node] - (((value - threshold) ** factor) * 0.1)), 0\n@@ -54,5 +54,7 @@ def calc_penalty(\n             logger.info(\"NEW SCORE NODE \" + node + \": \" + str(scores[node]))\n         elif scores[node] < 10 and scores[node] > 0:\n             scores[node] += rec_rate\n-    logger.info(\"Scores node \" + nm.get_node_number() + \": \" + str(scores))\n+\n+        if scores[node] > 10:\n+            scores[node] = 10\n     nm.update_trust_scores(scores)\ndiff --git a\/Szenario_1\/2\/config_2.ini b\/Szenario_1\/2\/config_2.ini\nindex 7fde95e..45392bc 100644\n--- a\/Szenario_1\/2\/config_2.ini\n+++ b\/Szenario_1\/2\/config_2.ini\n@@ -8,7 +8,7 @@ neighbours=1,3\n node_dir=\/home\/tobias\/Desktop\/Bachelorarbeit\/Szenario_1\/2\n \n [TRUST_CONFIG]\n-threshold_flooding=8\n+threshold_flooding=5\n memory_limits=0,5|1,10|2,20|3,30|4,40|5,50|6,60|7,70|8,85|9,95|10,99\n punishment_growth_rate=1.5\n trust_recovery_rate=0.1\ndiff --git a\/Szenario_1\/2\/framework.py b\/Szenario_1\/2\/framework.py\nindex 861a4eb..56a5e73 100644\n--- a\/Szenario_1\/2\/framework.py\n+++ b\/Szenario_1\/2\/framework.py\n@@ -62,10 +62,11 @@ def interval_reset(nm: NodeManager) -> None:\n     nm.reset_time_reached()\n \n \n-def wait_for_message(sock: socket.socket, wm: WorkerManager):\n+def wait_for_message(sock: socket.socket, wm: WorkerManager, nm: NodeManager):\n     while True:\n         message, addr = sock.recvfrom(196)\n-        threading.Thread(target=wm.add_bundle, args=[message]).start()\n+        if not util.utils.get_bundle_source(message) in nm.get_banned_nodes():\n+            threading.Thread(target=wm.add_bundle, args=[message]).start()\n \n \n def listen(nm: NodeManager, wm: WorkerManager) -> None:\n@@ -80,7 +81,7 @@ def listen(nm: NodeManager, wm: WorkerManager) -> None:\n             sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n             sock.bind((address[1], 4555))\n             print(\"Bound Socket to interface \" + address[0] + \", IP: \" + address[1])\n-            threading.Thread(target=wait_for_message, args=[sock, wm]).start()\n+            threading.Thread(target=wait_for_message, args=[sock, wm, nm]).start()\n             sockets.append(sock)\n \n \ndiff --git a\/Szenario_1\/2\/util\/nodemanager.py b\/Szenario_1\/2\/util\/nodemanager.py\nindex 8a1ec2c..5e92424 100644\n--- a\/Szenario_1\/2\/util\/nodemanager.py\n+++ b\/Szenario_1\/2\/util\/nodemanager.py\n@@ -4,7 +4,7 @@\n import os\n import subprocess\n import time\n-from typing import Dict, List\n+from typing import Dict, List, Set\n import threading\n \n import pyion\n@@ -42,6 +42,8 @@ class NodeManager:\n     __reset_interval: int\n     __bundles_last_interval: Dict[str, int]\n     reset_flag: bool\n+    __mem_percentage: float\n+    __banned_nodes: Set[str]\n \n     # ------------------------\n     # |    INITIALIZATION    |\n@@ -95,6 +97,8 @@ def __init__(self, path: str) -> None:\n         self.__reset_interval = int(config[\"TRUST_CONFIG\"][\"reset_interval\"])\n         self.__bundles_last_interval = {node: 0 for node in self.__neighbours}\n         self.reset_flag = False\n+        self.__mem_percentage = 0.0\n+        self.__banned_nodes = set()\n \n         # Logging the results\n         self.__logger.info(\n@@ -211,6 +215,7 @@ def update_trust_scores(self, new_scores: Dict[str, float]) -> None:\n             new_scores (Dict[str, float]): A dict containing the new scores.\n         \"\"\"\n         self.__trust_scores = new_scores\n+        self.__logger.info(\"New Trust Scores:\" + str(self.__trust_scores))\n \n     def can_accept_bundle(self, node: str) -> bool:\n         \"\"\"Compares memory usage with the cutoff value\n@@ -227,7 +232,7 @@ def can_accept_bundle(self, node: str) -> bool:\n         \"\"\"\n \n         current_trust = self.__trust_scores[node]\n-\n+        self.__logger.info(\"TRUST FOR NODE \" + node + \": \" + str(current_trust))\n         if current_trust > 0:\n             # find cutoff by evaluating all key of the dict\n             # and choosing the one closest (but still less than)\n@@ -240,36 +245,13 @@ def can_accept_bundle(self, node: str) -> bool:\n                     if key <= current_trust\n                 )\n             ]\n-            nn = self.__node_num\n-\n-            # change directories to the node's directory\n-            # so the SDR can be queried\n-            os.chdir(self.__node_dir)\n \n-            # Query ION's SDR memory\n-            process = subprocess.run(\n-                [\"ip netns exec nns\" + nn + \" sdrwatch n\" + nn],\n-                capture_output=True,\n-                shell=True,\n-            )\n-            stdout = process.stdout\n-            stdout = stdout.decode(\"utf-8\").split(\"\\n\")\n-            for string in stdout:\n-                if \"total now in use\" in string:\n-                    used = string\n-                elif \"total heap size\" in string:\n-                    total = string\n-\n-            # sdrwatch returns a string in the format \"total heap:              40000\"\n-            # -> the number needs to be extracted\n-            total = [int(n) for n in total.split() if n.isdigit()][0]\n-            used = [int(n) for n in used.split() if n.isdigit()][0]\n-            percentage = (float(used) \/ total) * 100\n-\n-            return True if percentage < cutoff else False\n+            return True if self.__mem_percentage < cutoff else False\n         else:\n             # the sender has a trust score of 0\n             # -> do not accept any of its bundles\n+            self.__logger.info(\"NOT ACCEPTING BUNDLE\")\n+            self.__banned_nodes.add(node)\n             return False\n \n     def reset_time_reached(self) -> None:\n@@ -281,4 +263,42 @@ def reset_time_reached(self) -> None:\n             self.__threshold_flooding,\n             self.__penalty_growth_rate,\n             self.__trust_recovery_rate,\n-        )\n\\ No newline at end of file\n+        )\n+        self.__mem_percentage = self.calc_memory_percentage()\n+\n+    def calc_memory_percentage(self) -> float:\n+        \"\"\"Calculates the percentage of unavailable memory space.\n+\n+        Returns:\n+            float: Percentage of memory used\n+        \"\"\"\n+        nn = self.__node_num\n+\n+        # change directories to the node's directory\n+        # so the SDR can be queried\n+        os.chdir(self.__node_dir)\n+\n+        # Query ION's SDR memory\n+        process = subprocess.run(\n+            [\"ip netns exec nns\" + nn + \" sdrwatch n\" + nn],\n+            capture_output=True,\n+            shell=True,\n+        )\n+        stdout = process.stdout\n+        stdout = stdout.decode(\"utf-8\").split(\"\\n\")\n+        for string in stdout:\n+            if \"total now in use\" in string:\n+                used = string\n+            elif \"total heap size\" in string:\n+                total = string\n+\n+        # sdrwatch returns a string in the format \"total heap:              40000\"\n+        # -> the number needs to be extracted\n+        total = [int(n) for n in total.split() if n.isdigit()][0]\n+        used = [int(n) for n in used.split() if n.isdigit()][0]\n+        perc = (float(used) \/ total) * 100\n+        self.__logger.info(\"Memory in use:\" + str(perc))\n+        return perc\n+\n+    def get_banned_nodes(self) -> List[str]:\n+        return self.__banned_nodes\ndiff --git a\/Szenario_1\/2\/util\/utils.py b\/Szenario_1\/2\/util\/utils.py\nindex bcb945e..6603a38 100644\n--- a\/Szenario_1\/2\/util\/utils.py\n+++ b\/Szenario_1\/2\/util\/utils.py\n@@ -46,7 +46,7 @@ def calc_penalty(\n     \"\"\"\n     for node in bundles:\n         value = bundles[node]\n-        logger.info(\"Accepted bundles from node \" + str(node) + \": \" + str(value))\n+        logger.info(\"Bundles from node \" + str(node) + \": \" + str(value))\n         if value > threshold:\n             scores[node] = max(\n                 (scores[node] - (((value - threshold) ** factor) * 0.1)), 0\n@@ -54,5 +54,7 @@ def calc_penalty(\n             logger.info(\"NEW SCORE NODE \" + node + \": \" + str(scores[node]))\n         elif scores[node] < 10 and scores[node] > 0:\n             scores[node] += rec_rate\n-    logger.info(\"Scores node \" + nm.get_node_number() + \": \" + str(scores))\n+\n+        if scores[node] > 10:\n+            scores[node] = 10\n     nm.update_trust_scores(scores)\ndiff --git a\/Szenario_1\/3\/framework.py b\/Szenario_1\/3\/framework.py\nindex 861a4eb..56a5e73 100644\n--- a\/Szenario_1\/3\/framework.py\n+++ b\/Szenario_1\/3\/framework.py\n@@ -62,10 +62,11 @@ def interval_reset(nm: NodeManager) -> None:\n     nm.reset_time_reached()\n \n \n-def wait_for_message(sock: socket.socket, wm: WorkerManager):\n+def wait_for_message(sock: socket.socket, wm: WorkerManager, nm: NodeManager):\n     while True:\n         message, addr = sock.recvfrom(196)\n-        threading.Thread(target=wm.add_bundle, args=[message]).start()\n+        if not util.utils.get_bundle_source(message) in nm.get_banned_nodes():\n+            threading.Thread(target=wm.add_bundle, args=[message]).start()\n \n \n def listen(nm: NodeManager, wm: WorkerManager) -> None:\n@@ -80,7 +81,7 @@ def listen(nm: NodeManager, wm: WorkerManager) -> None:\n             sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n             sock.bind((address[1], 4555))\n             print(\"Bound Socket to interface \" + address[0] + \", IP: \" + address[1])\n-            threading.Thread(target=wait_for_message, args=[sock, wm]).start()\n+            threading.Thread(target=wait_for_message, args=[sock, wm, nm]).start()\n             sockets.append(sock)\n \n \ndiff --git a\/Szenario_1\/3\/util\/nodemanager.py b\/Szenario_1\/3\/util\/nodemanager.py\nindex 8a1ec2c..5e92424 100644\n--- a\/Szenario_1\/3\/util\/nodemanager.py\n+++ b\/Szenario_1\/3\/util\/nodemanager.py\n@@ -4,7 +4,7 @@\n import os\n import subprocess\n import time\n-from typing import Dict, List\n+from typing import Dict, List, Set\n import threading\n \n import pyion\n@@ -42,6 +42,8 @@ class NodeManager:\n     __reset_interval: int\n     __bundles_last_interval: Dict[str, int]\n     reset_flag: bool\n+    __mem_percentage: float\n+    __banned_nodes: Set[str]\n \n     # ------------------------\n     # |    INITIALIZATION    |\n@@ -95,6 +97,8 @@ def __init__(self, path: str) -> None:\n         self.__reset_interval = int(config[\"TRUST_CONFIG\"][\"reset_interval\"])\n         self.__bundles_last_interval = {node: 0 for node in self.__neighbours}\n         self.reset_flag = False\n+        self.__mem_percentage = 0.0\n+        self.__banned_nodes = set()\n \n         # Logging the results\n         self.__logger.info(\n@@ -211,6 +215,7 @@ def update_trust_scores(self, new_scores: Dict[str, float]) -> None:\n             new_scores (Dict[str, float]): A dict containing the new scores.\n         \"\"\"\n         self.__trust_scores = new_scores\n+        self.__logger.info(\"New Trust Scores:\" + str(self.__trust_scores))\n \n     def can_accept_bundle(self, node: str) -> bool:\n         \"\"\"Compares memory usage with the cutoff value\n@@ -227,7 +232,7 @@ def can_accept_bundle(self, node: str) -> bool:\n         \"\"\"\n \n         current_trust = self.__trust_scores[node]\n-\n+        self.__logger.info(\"TRUST FOR NODE \" + node + \": \" + str(current_trust))\n         if current_trust > 0:\n             # find cutoff by evaluating all key of the dict\n             # and choosing the one closest (but still less than)\n@@ -240,36 +245,13 @@ def can_accept_bundle(self, node: str) -> bool:\n                     if key <= current_trust\n                 )\n             ]\n-            nn = self.__node_num\n-\n-            # change directories to the node's directory\n-            # so the SDR can be queried\n-            os.chdir(self.__node_dir)\n \n-            # Query ION's SDR memory\n-            process = subprocess.run(\n-                [\"ip netns exec nns\" + nn + \" sdrwatch n\" + nn],\n-                capture_output=True,\n-                shell=True,\n-            )\n-            stdout = process.stdout\n-            stdout = stdout.decode(\"utf-8\").split(\"\\n\")\n-            for string in stdout:\n-                if \"total now in use\" in string:\n-                    used = string\n-                elif \"total heap size\" in string:\n-                    total = string\n-\n-            # sdrwatch returns a string in the format \"total heap:              40000\"\n-            # -> the number needs to be extracted\n-            total = [int(n) for n in total.split() if n.isdigit()][0]\n-            used = [int(n) for n in used.split() if n.isdigit()][0]\n-            percentage = (float(used) \/ total) * 100\n-\n-            return True if percentage < cutoff else False\n+            return True if self.__mem_percentage < cutoff else False\n         else:\n             # the sender has a trust score of 0\n             # -> do not accept any of its bundles\n+            self.__logger.info(\"NOT ACCEPTING BUNDLE\")\n+            self.__banned_nodes.add(node)\n             return False\n \n     def reset_time_reached(self) -> None:\n@@ -281,4 +263,42 @@ def reset_time_reached(self) -> None:\n             self.__threshold_flooding,\n             self.__penalty_growth_rate,\n             self.__trust_recovery_rate,\n-        )\n\\ No newline at end of file\n+        )\n+        self.__mem_percentage = self.calc_memory_percentage()\n+\n+    def calc_memory_percentage(self) -> float:\n+        \"\"\"Calculates the percentage of unavailable memory space.\n+\n+        Returns:\n+            float: Percentage of memory used\n+        \"\"\"\n+        nn = self.__node_num\n+\n+        # change directories to the node's directory\n+        # so the SDR can be queried\n+        os.chdir(self.__node_dir)\n+\n+        # Query ION's SDR memory\n+        process = subprocess.run(\n+            [\"ip netns exec nns\" + nn + \" sdrwatch n\" + nn],\n+            capture_output=True,\n+            shell=True,\n+        )\n+        stdout = process.stdout\n+        stdout = stdout.decode(\"utf-8\").split(\"\\n\")\n+        for string in stdout:\n+            if \"total now in use\" in string:\n+                used = string\n+            elif \"total heap size\" in string:\n+                total = string\n+\n+        # sdrwatch returns a string in the format \"total heap:              40000\"\n+        # -> the number needs to be extracted\n+        total = [int(n) for n in total.split() if n.isdigit()][0]\n+        used = [int(n) for n in used.split() if n.isdigit()][0]\n+        perc = (float(used) \/ total) * 100\n+        self.__logger.info(\"Memory in use:\" + str(perc))\n+        return perc\n+\n+    def get_banned_nodes(self) -> List[str]:\n+        return self.__banned_nodes\ndiff --git a\/Szenario_1\/3\/util\/utils.py b\/Szenario_1\/3\/util\/utils.py\nindex bcb945e..6603a38 100644\n--- a\/Szenario_1\/3\/util\/utils.py\n+++ b\/Szenario_1\/3\/util\/utils.py\n@@ -46,7 +46,7 @@ def calc_penalty(\n     \"\"\"\n     for node in bundles:\n         value = bundles[node]\n-        logger.info(\"Accepted bundles from node \" + str(node) + \": \" + str(value))\n+        logger.info(\"Bundles from node \" + str(node) + \": \" + str(value))\n         if value > threshold:\n             scores[node] = max(\n                 (scores[node] - (((value - threshold) ** factor) * 0.1)), 0\n@@ -54,5 +54,7 @@ def calc_penalty(\n             logger.info(\"NEW SCORE NODE \" + node + \": \" + str(scores[node]))\n         elif scores[node] < 10 and scores[node] > 0:\n             scores[node] += rec_rate\n-    logger.info(\"Scores node \" + nm.get_node_number() + \": \" + str(scores))\n+\n+        if scores[node] > 10:\n+            scores[node] = 10\n     nm.update_trust_scores(scores)\ndiff --git a\/Szenario_1\/4\/framework.py b\/Szenario_1\/4\/framework.py\nindex 861a4eb..56a5e73 100644\n--- a\/Szenario_1\/4\/framework.py\n+++ b\/Szenario_1\/4\/framework.py\n@@ -62,10 +62,11 @@ def interval_reset(nm: NodeManager) -> None:\n     nm.reset_time_reached()\n \n \n-def wait_for_message(sock: socket.socket, wm: WorkerManager):\n+def wait_for_message(sock: socket.socket, wm: WorkerManager, nm: NodeManager):\n     while True:\n         message, addr = sock.recvfrom(196)\n-        threading.Thread(target=wm.add_bundle, args=[message]).start()\n+        if not util.utils.get_bundle_source(message) in nm.get_banned_nodes():\n+            threading.Thread(target=wm.add_bundle, args=[message]).start()\n \n \n def listen(nm: NodeManager, wm: WorkerManager) -> None:\n@@ -80,7 +81,7 @@ def listen(nm: NodeManager, wm: WorkerManager) -> None:\n             sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n             sock.bind((address[1], 4555))\n             print(\"Bound Socket to interface \" + address[0] + \", IP: \" + address[1])\n-            threading.Thread(target=wait_for_message, args=[sock, wm]).start()\n+            threading.Thread(target=wait_for_message, args=[sock, wm, nm]).start()\n             sockets.append(sock)\n \n \ndiff --git a\/Szenario_1\/4\/util\/nodemanager.py b\/Szenario_1\/4\/util\/nodemanager.py\nindex 8a1ec2c..5e92424 100644\n--- a\/Szenario_1\/4\/util\/nodemanager.py\n+++ b\/Szenario_1\/4\/util\/nodemanager.py\n@@ -4,7 +4,7 @@\n import os\n import subprocess\n import time\n-from typing import Dict, List\n+from typing import Dict, List, Set\n import threading\n \n import pyion\n@@ -42,6 +42,8 @@ class NodeManager:\n     __reset_interval: int\n     __bundles_last_interval: Dict[str, int]\n     reset_flag: bool\n+    __mem_percentage: float\n+    __banned_nodes: Set[str]\n \n     # ------------------------\n     # |    INITIALIZATION    |\n@@ -95,6 +97,8 @@ def __init__(self, path: str) -> None:\n         self.__reset_interval = int(config[\"TRUST_CONFIG\"][\"reset_interval\"])\n         self.__bundles_last_interval = {node: 0 for node in self.__neighbours}\n         self.reset_flag = False\n+        self.__mem_percentage = 0.0\n+        self.__banned_nodes = set()\n \n         # Logging the results\n         self.__logger.info(\n@@ -211,6 +215,7 @@ def update_trust_scores(self, new_scores: Dict[str, float]) -> None:\n             new_scores (Dict[str, float]): A dict containing the new scores.\n         \"\"\"\n         self.__trust_scores = new_scores\n+        self.__logger.info(\"New Trust Scores:\" + str(self.__trust_scores))\n \n     def can_accept_bundle(self, node: str) -> bool:\n         \"\"\"Compares memory usage with the cutoff value\n@@ -227,7 +232,7 @@ def can_accept_bundle(self, node: str) -> bool:\n         \"\"\"\n \n         current_trust = self.__trust_scores[node]\n-\n+        self.__logger.info(\"TRUST FOR NODE \" + node + \": \" + str(current_trust))\n         if current_trust > 0:\n             # find cutoff by evaluating all key of the dict\n             # and choosing the one closest (but still less than)\n@@ -240,36 +245,13 @@ def can_accept_bundle(self, node: str) -> bool:\n                     if key <= current_trust\n                 )\n             ]\n-            nn = self.__node_num\n-\n-            # change directories to the node's directory\n-            # so the SDR can be queried\n-            os.chdir(self.__node_dir)\n \n-            # Query ION's SDR memory\n-            process = subprocess.run(\n-                [\"ip netns exec nns\" + nn + \" sdrwatch n\" + nn],\n-                capture_output=True,\n-                shell=True,\n-            )\n-            stdout = process.stdout\n-            stdout = stdout.decode(\"utf-8\").split(\"\\n\")\n-            for string in stdout:\n-                if \"total now in use\" in string:\n-                    used = string\n-                elif \"total heap size\" in string:\n-                    total = string\n-\n-            # sdrwatch returns a string in the format \"total heap:              40000\"\n-            # -> the number needs to be extracted\n-            total = [int(n) for n in total.split() if n.isdigit()][0]\n-            used = [int(n) for n in used.split() if n.isdigit()][0]\n-            percentage = (float(used) \/ total) * 100\n-\n-            return True if percentage < cutoff else False\n+            return True if self.__mem_percentage < cutoff else False\n         else:\n             # the sender has a trust score of 0\n             # -> do not accept any of its bundles\n+            self.__logger.info(\"NOT ACCEPTING BUNDLE\")\n+            self.__banned_nodes.add(node)\n             return False\n \n     def reset_time_reached(self) -> None:\n@@ -281,4 +263,42 @@ def reset_time_reached(self) -> None:\n             self.__threshold_flooding,\n             self.__penalty_growth_rate,\n             self.__trust_recovery_rate,\n-        )\n\\ No newline at end of file\n+        )\n+        self.__mem_percentage = self.calc_memory_percentage()\n+\n+    def calc_memory_percentage(self) -> float:\n+        \"\"\"Calculates the percentage of unavailable memory space.\n+\n+        Returns:\n+            float: Percentage of memory used\n+        \"\"\"\n+        nn = self.__node_num\n+\n+        # change directories to the node's directory\n+        # so the SDR can be queried\n+        os.chdir(self.__node_dir)\n+\n+        # Query ION's SDR memory\n+        process = subprocess.run(\n+            [\"ip netns exec nns\" + nn + \" sdrwatch n\" + nn],\n+            capture_output=True,\n+            shell=True,\n+        )\n+        stdout = process.stdout\n+        stdout = stdout.decode(\"utf-8\").split(\"\\n\")\n+        for string in stdout:\n+            if \"total now in use\" in string:\n+                used = string\n+            elif \"total heap size\" in string:\n+                total = string\n+\n+        # sdrwatch returns a string in the format \"total heap:              40000\"\n+        # -> the number needs to be extracted\n+        total = [int(n) for n in total.split() if n.isdigit()][0]\n+        used = [int(n) for n in used.split() if n.isdigit()][0]\n+        perc = (float(used) \/ total) * 100\n+        self.__logger.info(\"Memory in use:\" + str(perc))\n+        return perc\n+\n+    def get_banned_nodes(self) -> List[str]:\n+        return self.__banned_nodes\ndiff --git a\/Szenario_1\/4\/util\/utils.py b\/Szenario_1\/4\/util\/utils.py\nindex bcb945e..6603a38 100644\n--- a\/Szenario_1\/4\/util\/utils.py\n+++ b\/Szenario_1\/4\/util\/utils.py\n@@ -46,7 +46,7 @@ def calc_penalty(\n     \"\"\"\n     for node in bundles:\n         value = bundles[node]\n-        logger.info(\"Accepted bundles from node \" + str(node) + \": \" + str(value))\n+        logger.info(\"Bundles from node \" + str(node) + \": \" + str(value))\n         if value > threshold:\n             scores[node] = max(\n                 (scores[node] - (((value - threshold) ** factor) * 0.1)), 0\n@@ -54,5 +54,7 @@ def calc_penalty(\n             logger.info(\"NEW SCORE NODE \" + node + \": \" + str(scores[node]))\n         elif scores[node] < 10 and scores[node] > 0:\n             scores[node] += rec_rate\n-    logger.info(\"Scores node \" + nm.get_node_number() + \": \" + str(scores))\n+\n+        if scores[node] > 10:\n+            scores[node] = 10\n     nm.update_trust_scores(scores)\ndiff --git a\/Szenario_1\/5\/framework.py b\/Szenario_1\/5\/framework.py\nindex 861a4eb..56a5e73 100644\n--- a\/Szenario_1\/5\/framework.py\n+++ b\/Szenario_1\/5\/framework.py\n@@ -62,10 +62,11 @@ def interval_reset(nm: NodeManager) -> None:\n     nm.reset_time_reached()\n \n \n-def wait_for_message(sock: socket.socket, wm: WorkerManager):\n+def wait_for_message(sock: socket.socket, wm: WorkerManager, nm: NodeManager):\n     while True:\n         message, addr = sock.recvfrom(196)\n-        threading.Thread(target=wm.add_bundle, args=[message]).start()\n+        if not util.utils.get_bundle_source(message) in nm.get_banned_nodes():\n+            threading.Thread(target=wm.add_bundle, args=[message]).start()\n \n \n def listen(nm: NodeManager, wm: WorkerManager) -> None:\n@@ -80,7 +81,7 @@ def listen(nm: NodeManager, wm: WorkerManager) -> None:\n             sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n             sock.bind((address[1], 4555))\n             print(\"Bound Socket to interface \" + address[0] + \", IP: \" + address[1])\n-            threading.Thread(target=wait_for_message, args=[sock, wm]).start()\n+            threading.Thread(target=wait_for_message, args=[sock, wm, nm]).start()\n             sockets.append(sock)\n \n \ndiff --git a\/Szenario_1\/5\/util\/nodemanager.py b\/Szenario_1\/5\/util\/nodemanager.py\nindex 8a1ec2c..5e92424 100644\n--- a\/Szenario_1\/5\/util\/nodemanager.py\n+++ b\/Szenario_1\/5\/util\/nodemanager.py\n@@ -4,7 +4,7 @@\n import os\n import subprocess\n import time\n-from typing import Dict, List\n+from typing import Dict, List, Set\n import threading\n \n import pyion\n@@ -42,6 +42,8 @@ class NodeManager:\n     __reset_interval: int\n     __bundles_last_interval: Dict[str, int]\n     reset_flag: bool\n+    __mem_percentage: float\n+    __banned_nodes: Set[str]\n \n     # ------------------------\n     # |    INITIALIZATION    |\n@@ -95,6 +97,8 @@ def __init__(self, path: str) -> None:\n         self.__reset_interval = int(config[\"TRUST_CONFIG\"][\"reset_interval\"])\n         self.__bundles_last_interval = {node: 0 for node in self.__neighbours}\n         self.reset_flag = False\n+        self.__mem_percentage = 0.0\n+        self.__banned_nodes = set()\n \n         # Logging the results\n         self.__logger.info(\n@@ -211,6 +215,7 @@ def update_trust_scores(self, new_scores: Dict[str, float]) -> None:\n             new_scores (Dict[str, float]): A dict containing the new scores.\n         \"\"\"\n         self.__trust_scores = new_scores\n+        self.__logger.info(\"New Trust Scores:\" + str(self.__trust_scores))\n \n     def can_accept_bundle(self, node: str) -> bool:\n         \"\"\"Compares memory usage with the cutoff value\n@@ -227,7 +232,7 @@ def can_accept_bundle(self, node: str) -> bool:\n         \"\"\"\n \n         current_trust = self.__trust_scores[node]\n-\n+        self.__logger.info(\"TRUST FOR NODE \" + node + \": \" + str(current_trust))\n         if current_trust > 0:\n             # find cutoff by evaluating all key of the dict\n             # and choosing the one closest (but still less than)\n@@ -240,36 +245,13 @@ def can_accept_bundle(self, node: str) -> bool:\n                     if key <= current_trust\n                 )\n             ]\n-            nn = self.__node_num\n-\n-            # change directories to the node's directory\n-            # so the SDR can be queried\n-            os.chdir(self.__node_dir)\n \n-            # Query ION's SDR memory\n-            process = subprocess.run(\n-                [\"ip netns exec nns\" + nn + \" sdrwatch n\" + nn],\n-                capture_output=True,\n-                shell=True,\n-            )\n-            stdout = process.stdout\n-            stdout = stdout.decode(\"utf-8\").split(\"\\n\")\n-            for string in stdout:\n-                if \"total now in use\" in string:\n-                    used = string\n-                elif \"total heap size\" in string:\n-                    total = string\n-\n-            # sdrwatch returns a string in the format \"total heap:              40000\"\n-            # -> the number needs to be extracted\n-            total = [int(n) for n in total.split() if n.isdigit()][0]\n-            used = [int(n) for n in used.split() if n.isdigit()][0]\n-            percentage = (float(used) \/ total) * 100\n-\n-            return True if percentage < cutoff else False\n+            return True if self.__mem_percentage < cutoff else False\n         else:\n             # the sender has a trust score of 0\n             # -> do not accept any of its bundles\n+            self.__logger.info(\"NOT ACCEPTING BUNDLE\")\n+            self.__banned_nodes.add(node)\n             return False\n \n     def reset_time_reached(self) -> None:\n@@ -281,4 +263,42 @@ def reset_time_reached(self) -> None:\n             self.__threshold_flooding,\n             self.__penalty_growth_rate,\n             self.__trust_recovery_rate,\n-        )\n\\ No newline at end of file\n+        )\n+        self.__mem_percentage = self.calc_memory_percentage()\n+\n+    def calc_memory_percentage(self) -> float:\n+        \"\"\"Calculates the percentage of unavailable memory space.\n+\n+        Returns:\n+            float: Percentage of memory used\n+        \"\"\"\n+        nn = self.__node_num\n+\n+        # change directories to the node's directory\n+        # so the SDR can be queried\n+        os.chdir(self.__node_dir)\n+\n+        # Query ION's SDR memory\n+        process = subprocess.run(\n+            [\"ip netns exec nns\" + nn + \" sdrwatch n\" + nn],\n+            capture_output=True,\n+            shell=True,\n+        )\n+        stdout = process.stdout\n+        stdout = stdout.decode(\"utf-8\").split(\"\\n\")\n+        for string in stdout:\n+            if \"total now in use\" in string:\n+                used = string\n+            elif \"total heap size\" in string:\n+                total = string\n+\n+        # sdrwatch returns a string in the format \"total heap:              40000\"\n+        # -> the number needs to be extracted\n+        total = [int(n) for n in total.split() if n.isdigit()][0]\n+        used = [int(n) for n in used.split() if n.isdigit()][0]\n+        perc = (float(used) \/ total) * 100\n+        self.__logger.info(\"Memory in use:\" + str(perc))\n+        return perc\n+\n+    def get_banned_nodes(self) -> List[str]:\n+        return self.__banned_nodes\ndiff --git a\/Szenario_1\/5\/util\/utils.py b\/Szenario_1\/5\/util\/utils.py\nindex bcb945e..6603a38 100644\n--- a\/Szenario_1\/5\/util\/utils.py\n+++ b\/Szenario_1\/5\/util\/utils.py\n@@ -46,7 +46,7 @@ def calc_penalty(\n     \"\"\"\n     for node in bundles:\n         value = bundles[node]\n-        logger.info(\"Accepted bundles from node \" + str(node) + \": \" + str(value))\n+        logger.info(\"Bundles from node \" + str(node) + \": \" + str(value))\n         if value > threshold:\n             scores[node] = max(\n                 (scores[node] - (((value - threshold) ** factor) * 0.1)), 0\n@@ -54,5 +54,7 @@ def calc_penalty(\n             logger.info(\"NEW SCORE NODE \" + node + \": \" + str(scores[node]))\n         elif scores[node] < 10 and scores[node] > 0:\n             scores[node] += rec_rate\n-    logger.info(\"Scores node \" + nm.get_node_number() + \": \" + str(scores))\n+\n+        if scores[node] > 10:\n+            scores[node] = 10\n     nm.update_trust_scores(scores)\ndiff --git a\/Szenario_1\/6\/framework.py b\/Szenario_1\/6\/framework.py\nindex 861a4eb..56a5e73 100644\n--- a\/Szenario_1\/6\/framework.py\n+++ b\/Szenario_1\/6\/framework.py\n@@ -62,10 +62,11 @@ def interval_reset(nm: NodeManager) -> None:\n     nm.reset_time_reached()\n \n \n-def wait_for_message(sock: socket.socket, wm: WorkerManager):\n+def wait_for_message(sock: socket.socket, wm: WorkerManager, nm: NodeManager):\n     while True:\n         message, addr = sock.recvfrom(196)\n-        threading.Thread(target=wm.add_bundle, args=[message]).start()\n+        if not util.utils.get_bundle_source(message) in nm.get_banned_nodes():\n+            threading.Thread(target=wm.add_bundle, args=[message]).start()\n \n \n def listen(nm: NodeManager, wm: WorkerManager) -> None:\n@@ -80,7 +81,7 @@ def listen(nm: NodeManager, wm: WorkerManager) -> None:\n             sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n             sock.bind((address[1], 4555))\n             print(\"Bound Socket to interface \" + address[0] + \", IP: \" + address[1])\n-            threading.Thread(target=wait_for_message, args=[sock, wm]).start()\n+            threading.Thread(target=wait_for_message, args=[sock, wm, nm]).start()\n             sockets.append(sock)\n \n \ndiff --git a\/Szenario_1\/6\/util\/nodemanager.py b\/Szenario_1\/6\/util\/nodemanager.py\nindex 8a1ec2c..5e92424 100644\n--- a\/Szenario_1\/6\/util\/nodemanager.py\n+++ b\/Szenario_1\/6\/util\/nodemanager.py\n@@ -4,7 +4,7 @@\n import os\n import subprocess\n import time\n-from typing import Dict, List\n+from typing import Dict, List, Set\n import threading\n \n import pyion\n@@ -42,6 +42,8 @@ class NodeManager:\n     __reset_interval: int\n     __bundles_last_interval: Dict[str, int]\n     reset_flag: bool\n+    __mem_percentage: float\n+    __banned_nodes: Set[str]\n \n     # ------------------------\n     # |    INITIALIZATION    |\n@@ -95,6 +97,8 @@ def __init__(self, path: str) -> None:\n         self.__reset_interval = int(config[\"TRUST_CONFIG\"][\"reset_interval\"])\n         self.__bundles_last_interval = {node: 0 for node in self.__neighbours}\n         self.reset_flag = False\n+        self.__mem_percentage = 0.0\n+        self.__banned_nodes = set()\n \n         # Logging the results\n         self.__logger.info(\n@@ -211,6 +215,7 @@ def update_trust_scores(self, new_scores: Dict[str, float]) -> None:\n             new_scores (Dict[str, float]): A dict containing the new scores.\n         \"\"\"\n         self.__trust_scores = new_scores\n+        self.__logger.info(\"New Trust Scores:\" + str(self.__trust_scores))\n \n     def can_accept_bundle(self, node: str) -> bool:\n         \"\"\"Compares memory usage with the cutoff value\n@@ -227,7 +232,7 @@ def can_accept_bundle(self, node: str) -> bool:\n         \"\"\"\n \n         current_trust = self.__trust_scores[node]\n-\n+        self.__logger.info(\"TRUST FOR NODE \" + node + \": \" + str(current_trust))\n         if current_trust > 0:\n             # find cutoff by evaluating all key of the dict\n             # and choosing the one closest (but still less than)\n@@ -240,36 +245,13 @@ def can_accept_bundle(self, node: str) -> bool:\n                     if key <= current_trust\n                 )\n             ]\n-            nn = self.__node_num\n-\n-            # change directories to the node's directory\n-            # so the SDR can be queried\n-            os.chdir(self.__node_dir)\n \n-            # Query ION's SDR memory\n-            process = subprocess.run(\n-                [\"ip netns exec nns\" + nn + \" sdrwatch n\" + nn],\n-                capture_output=True,\n-                shell=True,\n-            )\n-            stdout = process.stdout\n-            stdout = stdout.decode(\"utf-8\").split(\"\\n\")\n-            for string in stdout:\n-                if \"total now in use\" in string:\n-                    used = string\n-                elif \"total heap size\" in string:\n-                    total = string\n-\n-            # sdrwatch returns a string in the format \"total heap:              40000\"\n-            # -> the number needs to be extracted\n-            total = [int(n) for n in total.split() if n.isdigit()][0]\n-            used = [int(n) for n in used.split() if n.isdigit()][0]\n-            percentage = (float(used) \/ total) * 100\n-\n-            return True if percentage < cutoff else False\n+            return True if self.__mem_percentage < cutoff else False\n         else:\n             # the sender has a trust score of 0\n             # -> do not accept any of its bundles\n+            self.__logger.info(\"NOT ACCEPTING BUNDLE\")\n+            self.__banned_nodes.add(node)\n             return False\n \n     def reset_time_reached(self) -> None:\n@@ -281,4 +263,42 @@ def reset_time_reached(self) -> None:\n             self.__threshold_flooding,\n             self.__penalty_growth_rate,\n             self.__trust_recovery_rate,\n-        )\n\\ No newline at end of file\n+        )\n+        self.__mem_percentage = self.calc_memory_percentage()\n+\n+    def calc_memory_percentage(self) -> float:\n+        \"\"\"Calculates the percentage of unavailable memory space.\n+\n+        Returns:\n+            float: Percentage of memory used\n+        \"\"\"\n+        nn = self.__node_num\n+\n+        # change directories to the node's directory\n+        # so the SDR can be queried\n+        os.chdir(self.__node_dir)\n+\n+        # Query ION's SDR memory\n+        process = subprocess.run(\n+            [\"ip netns exec nns\" + nn + \" sdrwatch n\" + nn],\n+            capture_output=True,\n+            shell=True,\n+        )\n+        stdout = process.stdout\n+        stdout = stdout.decode(\"utf-8\").split(\"\\n\")\n+        for string in stdout:\n+            if \"total now in use\" in string:\n+                used = string\n+            elif \"total heap size\" in string:\n+                total = string\n+\n+        # sdrwatch returns a string in the format \"total heap:              40000\"\n+        # -> the number needs to be extracted\n+        total = [int(n) for n in total.split() if n.isdigit()][0]\n+        used = [int(n) for n in used.split() if n.isdigit()][0]\n+        perc = (float(used) \/ total) * 100\n+        self.__logger.info(\"Memory in use:\" + str(perc))\n+        return perc\n+\n+    def get_banned_nodes(self) -> List[str]:\n+        return self.__banned_nodes\ndiff --git a\/Szenario_1\/6\/util\/utils.py b\/Szenario_1\/6\/util\/utils.py\nindex bcb945e..6603a38 100644\n--- a\/Szenario_1\/6\/util\/utils.py\n+++ b\/Szenario_1\/6\/util\/utils.py\n@@ -46,7 +46,7 @@ def calc_penalty(\n     \"\"\"\n     for node in bundles:\n         value = bundles[node]\n-        logger.info(\"Accepted bundles from node \" + str(node) + \": \" + str(value))\n+        logger.info(\"Bundles from node \" + str(node) + \": \" + str(value))\n         if value > threshold:\n             scores[node] = max(\n                 (scores[node] - (((value - threshold) ** factor) * 0.1)), 0\n@@ -54,5 +54,7 @@ def calc_penalty(\n             logger.info(\"NEW SCORE NODE \" + node + \": \" + str(scores[node]))\n         elif scores[node] < 10 and scores[node] > 0:\n             scores[node] += rec_rate\n-    logger.info(\"Scores node \" + nm.get_node_number() + \": \" + str(scores))\n+\n+        if scores[node] > 10:\n+            scores[node] = 10\n     nm.update_trust_scores(scores)\ndiff --git a\/Szenario_1\/7\/framework.py b\/Szenario_1\/7\/framework.py\nindex 861a4eb..56a5e73 100644\n--- a\/Szenario_1\/7\/framework.py\n+++ b\/Szenario_1\/7\/framework.py\n@@ -62,10 +62,11 @@ def interval_reset(nm: NodeManager) -> None:\n     nm.reset_time_reached()\n \n \n-def wait_for_message(sock: socket.socket, wm: WorkerManager):\n+def wait_for_message(sock: socket.socket, wm: WorkerManager, nm: NodeManager):\n     while True:\n         message, addr = sock.recvfrom(196)\n-        threading.Thread(target=wm.add_bundle, args=[message]).start()\n+        if not util.utils.get_bundle_source(message) in nm.get_banned_nodes():\n+            threading.Thread(target=wm.add_bundle, args=[message]).start()\n \n \n def listen(nm: NodeManager, wm: WorkerManager) -> None:\n@@ -80,7 +81,7 @@ def listen(nm: NodeManager, wm: WorkerManager) -> None:\n             sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n             sock.bind((address[1], 4555))\n             print(\"Bound Socket to interface \" + address[0] + \", IP: \" + address[1])\n-            threading.Thread(target=wait_for_message, args=[sock, wm]).start()\n+            threading.Thread(target=wait_for_message, args=[sock, wm, nm]).start()\n             sockets.append(sock)\n \n \ndiff --git a\/Szenario_1\/7\/util\/nodemanager.py b\/Szenario_1\/7\/util\/nodemanager.py\nindex 8a1ec2c..5e92424 100644\n--- a\/Szenario_1\/7\/util\/nodemanager.py\n+++ b\/Szenario_1\/7\/util\/nodemanager.py\n@@ -4,7 +4,7 @@\n import os\n import subprocess\n import time\n-from typing import Dict, List\n+from typing import Dict, List, Set\n import threading\n \n import pyion\n@@ -42,6 +42,8 @@ class NodeManager:\n     __reset_interval: int\n     __bundles_last_interval: Dict[str, int]\n     reset_flag: bool\n+    __mem_percentage: float\n+    __banned_nodes: Set[str]\n \n     # ------------------------\n     # |    INITIALIZATION    |\n@@ -95,6 +97,8 @@ def __init__(self, path: str) -> None:\n         self.__reset_interval = int(config[\"TRUST_CONFIG\"][\"reset_interval\"])\n         self.__bundles_last_interval = {node: 0 for node in self.__neighbours}\n         self.reset_flag = False\n+        self.__mem_percentage = 0.0\n+        self.__banned_nodes = set()\n \n         # Logging the results\n         self.__logger.info(\n@@ -211,6 +215,7 @@ def update_trust_scores(self, new_scores: Dict[str, float]) -> None:\n             new_scores (Dict[str, float]): A dict containing the new scores.\n         \"\"\"\n         self.__trust_scores = new_scores\n+        self.__logger.info(\"New Trust Scores:\" + str(self.__trust_scores))\n \n     def can_accept_bundle(self, node: str) -> bool:\n         \"\"\"Compares memory usage with the cutoff value\n@@ -227,7 +232,7 @@ def can_accept_bundle(self, node: str) -> bool:\n         \"\"\"\n \n         current_trust = self.__trust_scores[node]\n-\n+        self.__logger.info(\"TRUST FOR NODE \" + node + \": \" + str(current_trust))\n         if current_trust > 0:\n             # find cutoff by evaluating all key of the dict\n             # and choosing the one closest (but still less than)\n@@ -240,36 +245,13 @@ def can_accept_bundle(self, node: str) -> bool:\n                     if key <= current_trust\n                 )\n             ]\n-            nn = self.__node_num\n-\n-            # change directories to the node's directory\n-            # so the SDR can be queried\n-            os.chdir(self.__node_dir)\n \n-            # Query ION's SDR memory\n-            process = subprocess.run(\n-                [\"ip netns exec nns\" + nn + \" sdrwatch n\" + nn],\n-                capture_output=True,\n-                shell=True,\n-            )\n-            stdout = process.stdout\n-            stdout = stdout.decode(\"utf-8\").split(\"\\n\")\n-            for string in stdout:\n-                if \"total now in use\" in string:\n-                    used = string\n-                elif \"total heap size\" in string:\n-                    total = string\n-\n-            # sdrwatch returns a string in the format \"total heap:              40000\"\n-            # -> the number needs to be extracted\n-            total = [int(n) for n in total.split() if n.isdigit()][0]\n-            used = [int(n) for n in used.split() if n.isdigit()][0]\n-            percentage = (float(used) \/ total) * 100\n-\n-            return True if percentage < cutoff else False\n+            return True if self.__mem_percentage < cutoff else False\n         else:\n             # the sender has a trust score of 0\n             # -> do not accept any of its bundles\n+            self.__logger.info(\"NOT ACCEPTING BUNDLE\")\n+            self.__banned_nodes.add(node)\n             return False\n \n     def reset_time_reached(self) -> None:\n@@ -281,4 +263,42 @@ def reset_time_reached(self) -> None:\n             self.__threshold_flooding,\n             self.__penalty_growth_rate,\n             self.__trust_recovery_rate,\n-        )\n\\ No newline at end of file\n+        )\n+        self.__mem_percentage = self.calc_memory_percentage()\n+\n+    def calc_memory_percentage(self) -> float:\n+        \"\"\"Calculates the percentage of unavailable memory space.\n+\n+        Returns:\n+            float: Percentage of memory used\n+        \"\"\"\n+        nn = self.__node_num\n+\n+        # change directories to the node's directory\n+        # so the SDR can be queried\n+        os.chdir(self.__node_dir)\n+\n+        # Query ION's SDR memory\n+        process = subprocess.run(\n+            [\"ip netns exec nns\" + nn + \" sdrwatch n\" + nn],\n+            capture_output=True,\n+            shell=True,\n+        )\n+        stdout = process.stdout\n+        stdout = stdout.decode(\"utf-8\").split(\"\\n\")\n+        for string in stdout:\n+            if \"total now in use\" in string:\n+                used = string\n+            elif \"total heap size\" in string:\n+                total = string\n+\n+        # sdrwatch returns a string in the format \"total heap:              40000\"\n+        # -> the number needs to be extracted\n+        total = [int(n) for n in total.split() if n.isdigit()][0]\n+        used = [int(n) for n in used.split() if n.isdigit()][0]\n+        perc = (float(used) \/ total) * 100\n+        self.__logger.info(\"Memory in use:\" + str(perc))\n+        return perc\n+\n+    def get_banned_nodes(self) -> List[str]:\n+        return self.__banned_nodes\ndiff --git a\/Szenario_1\/7\/util\/utils.py b\/Szenario_1\/7\/util\/utils.py\nindex bcb945e..6603a38 100644\n--- a\/Szenario_1\/7\/util\/utils.py\n+++ b\/Szenario_1\/7\/util\/utils.py\n@@ -46,7 +46,7 @@ def calc_penalty(\n     \"\"\"\n     for node in bundles:\n         value = bundles[node]\n-        logger.info(\"Accepted bundles from node \" + str(node) + \": \" + str(value))\n+        logger.info(\"Bundles from node \" + str(node) + \": \" + str(value))\n         if value > threshold:\n             scores[node] = max(\n                 (scores[node] - (((value - threshold) ** factor) * 0.1)), 0\n@@ -54,5 +54,7 @@ def calc_penalty(\n             logger.info(\"NEW SCORE NODE \" + node + \": \" + str(scores[node]))\n         elif scores[node] < 10 and scores[node] > 0:\n             scores[node] += rec_rate\n-    logger.info(\"Scores node \" + nm.get_node_number() + \": \" + str(scores))\n+\n+        if scores[node] > 10:\n+            scores[node] = 10\n     nm.update_trust_scores(scores)\ndiff --git a\/Szenario_1\/8\/framework.py b\/Szenario_1\/8\/framework.py\nindex 861a4eb..56a5e73 100644\n--- a\/Szenario_1\/8\/framework.py\n+++ b\/Szenario_1\/8\/framework.py\n@@ -62,10 +62,11 @@ def interval_reset(nm: NodeManager) -> None:\n     nm.reset_time_reached()\n \n \n-def wait_for_message(sock: socket.socket, wm: WorkerManager):\n+def wait_for_message(sock: socket.socket, wm: WorkerManager, nm: NodeManager):\n     while True:\n         message, addr = sock.recvfrom(196)\n-        threading.Thread(target=wm.add_bundle, args=[message]).start()\n+        if not util.utils.get_bundle_source(message) in nm.get_banned_nodes():\n+            threading.Thread(target=wm.add_bundle, args=[message]).start()\n \n \n def listen(nm: NodeManager, wm: WorkerManager) -> None:\n@@ -80,7 +81,7 @@ def listen(nm: NodeManager, wm: WorkerManager) -> None:\n             sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n             sock.bind((address[1], 4555))\n             print(\"Bound Socket to interface \" + address[0] + \", IP: \" + address[1])\n-            threading.Thread(target=wait_for_message, args=[sock, wm]).start()\n+            threading.Thread(target=wait_for_message, args=[sock, wm, nm]).start()\n             sockets.append(sock)\n \n \ndiff --git a\/Szenario_1\/8\/util\/nodemanager.py b\/Szenario_1\/8\/util\/nodemanager.py\nindex 8a1ec2c..5e92424 100644\n--- a\/Szenario_1\/8\/util\/nodemanager.py\n+++ b\/Szenario_1\/8\/util\/nodemanager.py\n@@ -4,7 +4,7 @@\n import os\n import subprocess\n import time\n-from typing import Dict, List\n+from typing import Dict, List, Set\n import threading\n \n import pyion\n@@ -42,6 +42,8 @@ class NodeManager:\n     __reset_interval: int\n     __bundles_last_interval: Dict[str, int]\n     reset_flag: bool\n+    __mem_percentage: float\n+    __banned_nodes: Set[str]\n \n     # ------------------------\n     # |    INITIALIZATION    |\n@@ -95,6 +97,8 @@ def __init__(self, path: str) -> None:\n         self.__reset_interval = int(config[\"TRUST_CONFIG\"][\"reset_interval\"])\n         self.__bundles_last_interval = {node: 0 for node in self.__neighbours}\n         self.reset_flag = False\n+        self.__mem_percentage = 0.0\n+        self.__banned_nodes = set()\n \n         # Logging the results\n         self.__logger.info(\n@@ -211,6 +215,7 @@ def update_trust_scores(self, new_scores: Dict[str, float]) -> None:\n             new_scores (Dict[str, float]): A dict containing the new scores.\n         \"\"\"\n         self.__trust_scores = new_scores\n+        self.__logger.info(\"New Trust Scores:\" + str(self.__trust_scores))\n \n     def can_accept_bundle(self, node: str) -> bool:\n         \"\"\"Compares memory usage with the cutoff value\n@@ -227,7 +232,7 @@ def can_accept_bundle(self, node: str) -> bool:\n         \"\"\"\n \n         current_trust = self.__trust_scores[node]\n-\n+        self.__logger.info(\"TRUST FOR NODE \" + node + \": \" + str(current_trust))\n         if current_trust > 0:\n             # find cutoff by evaluating all key of the dict\n             # and choosing the one closest (but still less than)\n@@ -240,36 +245,13 @@ def can_accept_bundle(self, node: str) -> bool:\n                     if key <= current_trust\n                 )\n             ]\n-            nn = self.__node_num\n-\n-            # change directories to the node's directory\n-            # so the SDR can be queried\n-            os.chdir(self.__node_dir)\n \n-            # Query ION's SDR memory\n-            process = subprocess.run(\n-                [\"ip netns exec nns\" + nn + \" sdrwatch n\" + nn],\n-                capture_output=True,\n-                shell=True,\n-            )\n-            stdout = process.stdout\n-            stdout = stdout.decode(\"utf-8\").split(\"\\n\")\n-            for string in stdout:\n-                if \"total now in use\" in string:\n-                    used = string\n-                elif \"total heap size\" in string:\n-                    total = string\n-\n-            # sdrwatch returns a string in the format \"total heap:              40000\"\n-            # -> the number needs to be extracted\n-            total = [int(n) for n in total.split() if n.isdigit()][0]\n-            used = [int(n) for n in used.split() if n.isdigit()][0]\n-            percentage = (float(used) \/ total) * 100\n-\n-            return True if percentage < cutoff else False\n+            return True if self.__mem_percentage < cutoff else False\n         else:\n             # the sender has a trust score of 0\n             # -> do not accept any of its bundles\n+            self.__logger.info(\"NOT ACCEPTING BUNDLE\")\n+            self.__banned_nodes.add(node)\n             return False\n \n     def reset_time_reached(self) -> None:\n@@ -281,4 +263,42 @@ def reset_time_reached(self) -> None:\n             self.__threshold_flooding,\n             self.__penalty_growth_rate,\n             self.__trust_recovery_rate,\n-        )\n\\ No newline at end of file\n+        )\n+        self.__mem_percentage = self.calc_memory_percentage()\n+\n+    def calc_memory_percentage(self) -> float:\n+        \"\"\"Calculates the percentage of unavailable memory space.\n+\n+        Returns:\n+            float: Percentage of memory used\n+        \"\"\"\n+        nn = self.__node_num\n+\n+        # change directories to the node's directory\n+        # so the SDR can be queried\n+        os.chdir(self.__node_dir)\n+\n+        # Query ION's SDR memory\n+        process = subprocess.run(\n+            [\"ip netns exec nns\" + nn + \" sdrwatch n\" + nn],\n+            capture_output=True,\n+            shell=True,\n+        )\n+        stdout = process.stdout\n+        stdout = stdout.decode(\"utf-8\").split(\"\\n\")\n+        for string in stdout:\n+            if \"total now in use\" in string:\n+                used = string\n+            elif \"total heap size\" in string:\n+                total = string\n+\n+        # sdrwatch returns a string in the format \"total heap:              40000\"\n+        # -> the number needs to be extracted\n+        total = [int(n) for n in total.split() if n.isdigit()][0]\n+        used = [int(n) for n in used.split() if n.isdigit()][0]\n+        perc = (float(used) \/ total) * 100\n+        self.__logger.info(\"Memory in use:\" + str(perc))\n+        return perc\n+\n+    def get_banned_nodes(self) -> List[str]:\n+        return self.__banned_nodes\ndiff --git a\/Szenario_1\/8\/util\/utils.py b\/Szenario_1\/8\/util\/utils.py\nindex bcb945e..6603a38 100644\n--- a\/Szenario_1\/8\/util\/utils.py\n+++ b\/Szenario_1\/8\/util\/utils.py\n@@ -46,7 +46,7 @@ def calc_penalty(\n     \"\"\"\n     for node in bundles:\n         value = bundles[node]\n-        logger.info(\"Accepted bundles from node \" + str(node) + \": \" + str(value))\n+        logger.info(\"Bundles from node \" + str(node) + \": \" + str(value))\n         if value > threshold:\n             scores[node] = max(\n                 (scores[node] - (((value - threshold) ** factor) * 0.1)), 0\n@@ -54,5 +54,7 @@ def calc_penalty(\n             logger.info(\"NEW SCORE NODE \" + node + \": \" + str(scores[node]))\n         elif scores[node] < 10 and scores[node] > 0:\n             scores[node] += rec_rate\n-    logger.info(\"Scores node \" + nm.get_node_number() + \": \" + str(scores))\n+\n+        if scores[node] > 10:\n+            scores[node] = 10\n     nm.update_trust_scores(scores)\ndiff --git a\/Szenario_1\/9\/framework.py b\/Szenario_1\/9\/framework.py\nindex 861a4eb..56a5e73 100644\n--- a\/Szenario_1\/9\/framework.py\n+++ b\/Szenario_1\/9\/framework.py\n@@ -62,10 +62,11 @@ def interval_reset(nm: NodeManager) -> None:\n     nm.reset_time_reached()\n \n \n-def wait_for_message(sock: socket.socket, wm: WorkerManager):\n+def wait_for_message(sock: socket.socket, wm: WorkerManager, nm: NodeManager):\n     while True:\n         message, addr = sock.recvfrom(196)\n-        threading.Thread(target=wm.add_bundle, args=[message]).start()\n+        if not util.utils.get_bundle_source(message) in nm.get_banned_nodes():\n+            threading.Thread(target=wm.add_bundle, args=[message]).start()\n \n \n def listen(nm: NodeManager, wm: WorkerManager) -> None:\n@@ -80,7 +81,7 @@ def listen(nm: NodeManager, wm: WorkerManager) -> None:\n             sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n             sock.bind((address[1], 4555))\n             print(\"Bound Socket to interface \" + address[0] + \", IP: \" + address[1])\n-            threading.Thread(target=wait_for_message, args=[sock, wm]).start()\n+            threading.Thread(target=wait_for_message, args=[sock, wm, nm]).start()\n             sockets.append(sock)\n \n \ndiff --git a\/Szenario_1\/9\/util\/nodemanager.py b\/Szenario_1\/9\/util\/nodemanager.py\nindex 8a1ec2c..5e92424 100644\n--- a\/Szenario_1\/9\/util\/nodemanager.py\n+++ b\/Szenario_1\/9\/util\/nodemanager.py\n@@ -4,7 +4,7 @@\n import os\n import subprocess\n import time\n-from typing import Dict, List\n+from typing import Dict, List, Set\n import threading\n \n import pyion\n@@ -42,6 +42,8 @@ class NodeManager:\n     __reset_interval: int\n     __bundles_last_interval: Dict[str, int]\n     reset_flag: bool\n+    __mem_percentage: float\n+    __banned_nodes: Set[str]\n \n     # ------------------------\n     # |    INITIALIZATION    |\n@@ -95,6 +97,8 @@ def __init__(self, path: str) -> None:\n         self.__reset_interval = int(config[\"TRUST_CONFIG\"][\"reset_interval\"])\n         self.__bundles_last_interval = {node: 0 for node in self.__neighbours}\n         self.reset_flag = False\n+        self.__mem_percentage = 0.0\n+        self.__banned_nodes = set()\n \n         # Logging the results\n         self.__logger.info(\n@@ -211,6 +215,7 @@ def update_trust_scores(self, new_scores: Dict[str, float]) -> None:\n             new_scores (Dict[str, float]): A dict containing the new scores.\n         \"\"\"\n         self.__trust_scores = new_scores\n+        self.__logger.info(\"New Trust Scores:\" + str(self.__trust_scores))\n \n     def can_accept_bundle(self, node: str) -> bool:\n         \"\"\"Compares memory usage with the cutoff value\n@@ -227,7 +232,7 @@ def can_accept_bundle(self, node: str) -> bool:\n         \"\"\"\n \n         current_trust = self.__trust_scores[node]\n-\n+        self.__logger.info(\"TRUST FOR NODE \" + node + \": \" + str(current_trust))\n         if current_trust > 0:\n             # find cutoff by evaluating all key of the dict\n             # and choosing the one closest (but still less than)\n@@ -240,36 +245,13 @@ def can_accept_bundle(self, node: str) -> bool:\n                     if key <= current_trust\n                 )\n             ]\n-            nn = self.__node_num\n-\n-            # change directories to the node's directory\n-            # so the SDR can be queried\n-            os.chdir(self.__node_dir)\n \n-            # Query ION's SDR memory\n-            process = subprocess.run(\n-                [\"ip netns exec nns\" + nn + \" sdrwatch n\" + nn],\n-                capture_output=True,\n-                shell=True,\n-            )\n-            stdout = process.stdout\n-            stdout = stdout.decode(\"utf-8\").split(\"\\n\")\n-            for string in stdout:\n-                if \"total now in use\" in string:\n-                    used = string\n-                elif \"total heap size\" in string:\n-                    total = string\n-\n-            # sdrwatch returns a string in the format \"total heap:              40000\"\n-            # -> the number needs to be extracted\n-            total = [int(n) for n in total.split() if n.isdigit()][0]\n-            used = [int(n) for n in used.split() if n.isdigit()][0]\n-            percentage = (float(used) \/ total) * 100\n-\n-            return True if percentage < cutoff else False\n+            return True if self.__mem_percentage < cutoff else False\n         else:\n             # the sender has a trust score of 0\n             # -> do not accept any of its bundles\n+            self.__logger.info(\"NOT ACCEPTING BUNDLE\")\n+            self.__banned_nodes.add(node)\n             return False\n \n     def reset_time_reached(self) -> None:\n@@ -281,4 +263,42 @@ def reset_time_reached(self) -> None:\n             self.__threshold_flooding,\n             self.__penalty_growth_rate,\n             self.__trust_recovery_rate,\n-        )\n\\ No newline at end of file\n+        )\n+        self.__mem_percentage = self.calc_memory_percentage()\n+\n+    def calc_memory_percentage(self) -> float:\n+        \"\"\"Calculates the percentage of unavailable memory space.\n+\n+        Returns:\n+            float: Percentage of memory used\n+        \"\"\"\n+        nn = self.__node_num\n+\n+        # change directories to the node's directory\n+        # so the SDR can be queried\n+        os.chdir(self.__node_dir)\n+\n+        # Query ION's SDR memory\n+        process = subprocess.run(\n+            [\"ip netns exec nns\" + nn + \" sdrwatch n\" + nn],\n+            capture_output=True,\n+            shell=True,\n+        )\n+        stdout = process.stdout\n+        stdout = stdout.decode(\"utf-8\").split(\"\\n\")\n+        for string in stdout:\n+            if \"total now in use\" in string:\n+                used = string\n+            elif \"total heap size\" in string:\n+                total = string\n+\n+        # sdrwatch returns a string in the format \"total heap:              40000\"\n+        # -> the number needs to be extracted\n+        total = [int(n) for n in total.split() if n.isdigit()][0]\n+        used = [int(n) for n in used.split() if n.isdigit()][0]\n+        perc = (float(used) \/ total) * 100\n+        self.__logger.info(\"Memory in use:\" + str(perc))\n+        return perc\n+\n+    def get_banned_nodes(self) -> List[str]:\n+        return self.__banned_nodes\ndiff --git a\/Szenario_1\/9\/util\/utils.py b\/Szenario_1\/9\/util\/utils.py\nindex bcb945e..6603a38 100644\n--- a\/Szenario_1\/9\/util\/utils.py\n+++ b\/Szenario_1\/9\/util\/utils.py\n@@ -46,7 +46,7 @@ def calc_penalty(\n     \"\"\"\n     for node in bundles:\n         value = bundles[node]\n-        logger.info(\"Accepted bundles from node \" + str(node) + \": \" + str(value))\n+        logger.info(\"Bundles from node \" + str(node) + \": \" + str(value))\n         if value > threshold:\n             scores[node] = max(\n                 (scores[node] - (((value - threshold) ** factor) * 0.1)), 0\n@@ -54,5 +54,7 @@ def calc_penalty(\n             logger.info(\"NEW SCORE NODE \" + node + \": \" + str(scores[node]))\n         elif scores[node] < 10 and scores[node] > 0:\n             scores[node] += rec_rate\n-    logger.info(\"Scores node \" + nm.get_node_number() + \": \" + str(scores))\n+\n+        if scores[node] > 10:\n+            scores[node] = 10\n     nm.update_trust_scores(scores)\ndiff --git a\/Szenario_1\/run_alt.sh b\/Szenario_1\/run_alt.sh\nindex 5e2740e..e8b9d57 100755\n--- a\/Szenario_1\/run_alt.sh\n+++ b\/Szenario_1\/run_alt.sh\n@@ -3,7 +3,7 @@\n for i in 1 2 3 4 5 6 7 8 9\n do\n     cd \".\/$i\/\"\n-    xterm -hold -e \"ip netns exec nns$i ionstart -I n'$i'1.rc\" &\n+    ip netns exec nns$i ionstart -I n\"$i\"1.rc &\n     cd ..\n     sleep 0.1\n done\ndiff --git a\/framework\/framework.py b\/framework\/framework.py\nindex 861a4eb..56a5e73 100644\n--- a\/framework\/framework.py\n+++ b\/framework\/framework.py\n@@ -62,10 +62,11 @@ def interval_reset(nm: NodeManager) -> None:\n     nm.reset_time_reached()\n \n \n-def wait_for_message(sock: socket.socket, wm: WorkerManager):\n+def wait_for_message(sock: socket.socket, wm: WorkerManager, nm: NodeManager):\n     while True:\n         message, addr = sock.recvfrom(196)\n-        threading.Thread(target=wm.add_bundle, args=[message]).start()\n+        if not util.utils.get_bundle_source(message) in nm.get_banned_nodes():\n+            threading.Thread(target=wm.add_bundle, args=[message]).start()\n \n \n def listen(nm: NodeManager, wm: WorkerManager) -> None:\n@@ -80,7 +81,7 @@ def listen(nm: NodeManager, wm: WorkerManager) -> None:\n             sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n             sock.bind((address[1], 4555))\n             print(\"Bound Socket to interface \" + address[0] + \", IP: \" + address[1])\n-            threading.Thread(target=wait_for_message, args=[sock, wm]).start()\n+            threading.Thread(target=wait_for_message, args=[sock, wm, nm]).start()\n             sockets.append(sock)\n \n \ndiff --git a\/framework\/util\/nodemanager.py b\/framework\/util\/nodemanager.py\nindex 8a1ec2c..5e92424 100644\n--- a\/framework\/util\/nodemanager.py\n+++ b\/framework\/util\/nodemanager.py\n@@ -4,7 +4,7 @@\n import os\n import subprocess\n import time\n-from typing import Dict, List\n+from typing import Dict, List, Set\n import threading\n \n import pyion\n@@ -42,6 +42,8 @@ class NodeManager:\n     __reset_interval: int\n     __bundles_last_interval: Dict[str, int]\n     reset_flag: bool\n+    __mem_percentage: float\n+    __banned_nodes: Set[str]\n \n     # ------------------------\n     # |    INITIALIZATION    |\n@@ -95,6 +97,8 @@ def __init__(self, path: str) -> None:\n         self.__reset_interval = int(config[\"TRUST_CONFIG\"][\"reset_interval\"])\n         self.__bundles_last_interval = {node: 0 for node in self.__neighbours}\n         self.reset_flag = False\n+        self.__mem_percentage = 0.0\n+        self.__banned_nodes = set()\n \n         # Logging the results\n         self.__logger.info(\n@@ -211,6 +215,7 @@ def update_trust_scores(self, new_scores: Dict[str, float]) -> None:\n             new_scores (Dict[str, float]): A dict containing the new scores.\n         \"\"\"\n         self.__trust_scores = new_scores\n+        self.__logger.info(\"New Trust Scores:\" + str(self.__trust_scores))\n \n     def can_accept_bundle(self, node: str) -> bool:\n         \"\"\"Compares memory usage with the cutoff value\n@@ -227,7 +232,7 @@ def can_accept_bundle(self, node: str) -> bool:\n         \"\"\"\n \n         current_trust = self.__trust_scores[node]\n-\n+        self.__logger.info(\"TRUST FOR NODE \" + node + \": \" + str(current_trust))\n         if current_trust > 0:\n             # find cutoff by evaluating all key of the dict\n             # and choosing the one closest (but still less than)\n@@ -240,36 +245,13 @@ def can_accept_bundle(self, node: str) -> bool:\n                     if key <= current_trust\n                 )\n             ]\n-            nn = self.__node_num\n-\n-            # change directories to the node's directory\n-            # so the SDR can be queried\n-            os.chdir(self.__node_dir)\n \n-            # Query ION's SDR memory\n-            process = subprocess.run(\n-                [\"ip netns exec nns\" + nn + \" sdrwatch n\" + nn],\n-                capture_output=True,\n-                shell=True,\n-            )\n-            stdout = process.stdout\n-            stdout = stdout.decode(\"utf-8\").split(\"\\n\")\n-            for string in stdout:\n-                if \"total now in use\" in string:\n-                    used = string\n-                elif \"total heap size\" in string:\n-                    total = string\n-\n-            # sdrwatch returns a string in the format \"total heap:              40000\"\n-            # -> the number needs to be extracted\n-            total = [int(n) for n in total.split() if n.isdigit()][0]\n-            used = [int(n) for n in used.split() if n.isdigit()][0]\n-            percentage = (float(used) \/ total) * 100\n-\n-            return True if percentage < cutoff else False\n+            return True if self.__mem_percentage < cutoff else False\n         else:\n             # the sender has a trust score of 0\n             # -> do not accept any of its bundles\n+            self.__logger.info(\"NOT ACCEPTING BUNDLE\")\n+            self.__banned_nodes.add(node)\n             return False\n \n     def reset_time_reached(self) -> None:\n@@ -281,4 +263,42 @@ def reset_time_reached(self) -> None:\n             self.__threshold_flooding,\n             self.__penalty_growth_rate,\n             self.__trust_recovery_rate,\n-        )\n\\ No newline at end of file\n+        )\n+        self.__mem_percentage = self.calc_memory_percentage()\n+\n+    def calc_memory_percentage(self) -> float:\n+        \"\"\"Calculates the percentage of unavailable memory space.\n+\n+        Returns:\n+            float: Percentage of memory used\n+        \"\"\"\n+        nn = self.__node_num\n+\n+        # change directories to the node's directory\n+        # so the SDR can be queried\n+        os.chdir(self.__node_dir)\n+\n+        # Query ION's SDR memory\n+        process = subprocess.run(\n+            [\"ip netns exec nns\" + nn + \" sdrwatch n\" + nn],\n+            capture_output=True,\n+            shell=True,\n+        )\n+        stdout = process.stdout\n+        stdout = stdout.decode(\"utf-8\").split(\"\\n\")\n+        for string in stdout:\n+            if \"total now in use\" in string:\n+                used = string\n+            elif \"total heap size\" in string:\n+                total = string\n+\n+        # sdrwatch returns a string in the format \"total heap:              40000\"\n+        # -> the number needs to be extracted\n+        total = [int(n) for n in total.split() if n.isdigit()][0]\n+        used = [int(n) for n in used.split() if n.isdigit()][0]\n+        perc = (float(used) \/ total) * 100\n+        self.__logger.info(\"Memory in use:\" + str(perc))\n+        return perc\n+\n+    def get_banned_nodes(self) -> List[str]:\n+        return self.__banned_nodes\ndiff --git a\/framework\/util\/utils.py b\/framework\/util\/utils.py\nindex 8a5402b..6603a38 100644\n--- a\/framework\/util\/utils.py\n+++ b\/framework\/util\/utils.py\n@@ -46,7 +46,7 @@ def calc_penalty(\n     \"\"\"\n     for node in bundles:\n         value = bundles[node]\n-        logger.info(\"Accepted bundles from node \" + str(node) + \": \" + str(value))\n+        logger.info(\"Bundles from node \" + str(node) + \": \" + str(value))\n         if value > threshold:\n             scores[node] = max(\n                 (scores[node] - (((value - threshold) ** factor) * 0.1)), 0\n@@ -57,5 +57,4 @@ def calc_penalty(\n \n         if scores[node] > 10:\n             scores[node] = 10\n-    logger.info(\"Scores node \" + nm.get_node_number() + \": \" + str(scores))\n     nm.update_trust_scores(scores)\n","files":{"\/Szenario_1\/1\/framework.py":{"changes":[{"diff":"\n     nm.reset_time_reached()\n \n \n-def wait_for_message(sock: socket.socket, wm: WorkerManager):\n+def wait_for_message(sock: socket.socket, wm: WorkerManager, nm: NodeManager):\n     while True:\n         message, addr = sock.recvfrom(196)\n-        threading.Thread(target=wm.add_bundle, args=[message]).start()\n+        if not util.utils.get_bundle_source(message) in nm.get_banned_nodes():\n+            threading.Thread(target=wm.add_bundle, args=[message]).start()\n \n \n def listen(nm: NodeManager, wm: WorkerManager) -> None:\n","add":3,"remove":2,"filename":"\/Szenario_1\/1\/framework.py","badparts":["def wait_for_message(sock: socket.socket, wm: WorkerManager):","        threading.Thread(target=wm.add_bundle, args=[message]).start()"],"goodparts":["def wait_for_message(sock: socket.socket, wm: WorkerManager, nm: NodeManager):","        if not util.utils.get_bundle_source(message) in nm.get_banned_nodes():","            threading.Thread(target=wm.add_bundle, args=[message]).start()"]},{"diff":"\n             sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n             sock.bind((address[1], 4555))\n             print(\"Bound Socket to interface \" + address[0] + \", IP: \" + address[1])\n-            threading.Thread(target=wait_for_message, args=[sock, wm]).start()\n+            threading.Thread(target=wait_for_message, args=[sock, wm, nm]).start()\n             sockets.append(sock)\n \n ","add":1,"remove":1,"filename":"\/Szenario_1\/1\/framework.py","badparts":["            threading.Thread(target=wait_for_message, args=[sock, wm]).start()"],"goodparts":["            threading.Thread(target=wait_for_message, args=[sock, wm, nm]).start()"]}],"source":"\n\nimport argparse import logging import socket import sys import threading import time import signal from util.workermanager import WorkerManager import psutil from apscheduler.schedulers.background import BackgroundScheduler from apscheduler.triggers.interval import IntervalTrigger import util.utils from util.nodemanager import NodeManager logger=logging.getLogger(__name__) f_socket: socket.socket scheduler: BackgroundScheduler def get_ip_addresses(family: socket.AddressFamily): \"\"\"Returns all network interfaces and their assigned IP addresses. Args: family(socket.AddressFamily): AF_INET for IPv4, AF_INET6 for IPv6 addresses. Yields: List[Tuple(str,str)]: A list of all interfaces and their assigned IP addresses \"\"\" for interface, snics in psutil.net_if_addrs().items(): for snic in snics: if snic.family==family: yield(interface, snic.address) def init_argparse() -> argparse.ArgumentParser: parser=argparse.ArgumentParser( usage=\"%(prog)s config\", description=\"Mitigates Flooding-DDoS-Attacks on DTN-nodes.\", ) parser.add_argument(\"config\", help=\"Path to a configuration file\") return parser def interval_reset(nm: NodeManager) -> None: \"\"\"Instructs NodeManager to reset the counts of the Bundles which were received in the last interval. Args: nm(NodeManager): Management utility of the node for which the counts should be reset. \"\"\" print(\"RESET\") nm.reset_flag=True nm.reset_time_reached() def wait_for_message(sock: socket.socket, wm: WorkerManager): while True: message, addr=sock.recvfrom(196) threading.Thread(target=wm.add_bundle, args=[message]).start() def listen(nm: NodeManager, wm: WorkerManager) -> None: print(\"Listening for incoming bundles on node \" +nm.get_node_number() +\":\") ip_list=get_ip_addresses(socket.AF_INET) global sockets sockets=[] for address in ip_list: if not address[0]==\"lo\": sock=socket.socket(socket.AF_INET, socket.SOCK_DGRAM) sock.bind((address[1], 4555)) print(\"Bound Socket to interface \" +address[0] +\", IP: \" +address[1]) threading.Thread(target=wait_for_message, args=[sock, wm]).start() sockets.append(sock) def main(nm: NodeManager, wm: WorkerManager) -> None: \"\"\"Main event loop. Args: nm(NodeManager):[description] \"\"\" wm.start_workers() logger.info(\"Starting thread listening for incoming messages\") l=threading.Thread(target=listen, args=[nm, wm]) l.daemon=True l.start() global scheduler scheduler=BackgroundScheduler() scheduler.add_job( interval_reset, IntervalTrigger(seconds=nm.get_reset_time()), args=([nm]) ) scheduler.start() logging.getLogger(\"apscheduler\").setLevel(logging.WARNING) while True: time.sleep(100) def cleanup() -> None: \"\"\"Closes the socket and shuts down the scheduler on SIGINT. \"\"\" global sockets global scheduler logger.info(\"---------Shutting down----------\") for sock in sockets: sock.close() logger.info(\"Closed sockets\") scheduler.shutdown() logger.info(\"Shut down scheduler\") def signal_handler(signum, frame) -> None: logger.info(\"Received SIGINT\") cleanup() sys.exit(0) if __name__==\"__main__\": signal.signal(signal.SIGINT, signal_handler) logging.basicConfig( filename=\"framework.log\", level=logging.INFO, format=\"%(asctime)s %(name)s:%(levelname)s: %(message)s\", datefmt=\"[%Y-%m-%d %H:%M:%S]\", ) parser=init_argparse() args=parser.parse_args() if not args.config: print(\"Please provide a configuration file!\") exit(1) else: try: f=open(args.config, \"r\") f.close() logger.info(\"Starting NodeManager\") nm=NodeManager(args.config) except(FileNotFoundError, IsADirectoryError) as err: logger.error(f\"{sys.argv[0]}:{args.config}:{err.strerror}\") print(f\"{sys.argv[0]}:{args.config}:{err.strerror}\", file=sys.stderr) exit(1) logger.info(\"Entering main event loop.\") wm=WorkerManager(nm, 25) main(nm, wm) ","sourceWithComments":"#!\/usr\/bin\/env python\nimport argparse\nimport logging\nimport socket\nimport sys\nimport threading\nimport time\nimport signal\nfrom util.workermanager import WorkerManager\nimport psutil\nfrom apscheduler.schedulers.background import BackgroundScheduler\nfrom apscheduler.triggers.interval import IntervalTrigger\n\nimport util.utils\nfrom util.nodemanager import NodeManager\n\n\nlogger = logging.getLogger(__name__)\nf_socket: socket.socket\nscheduler: BackgroundScheduler\n\n\ndef get_ip_addresses(family: socket.AddressFamily):\n    \"\"\"Returns all network interfaces and their assigned IP addresses.\n\n    Args:\n        family (socket.AddressFamily): AF_INET for IPv4, AF_INET6 for IPv6 addresses.\n\n    Yields:\n        List[Tuple(str,str)]: A list of all interfaces and their assigned IP addresses\n    \"\"\"\n\n    # This snippet was written by StackOverflow user pmav99\n    # Link to answer: https:\/\/stackoverflow.com\/a\/43478599\n    for interface, snics in psutil.net_if_addrs().items():\n        for snic in snics:\n            if snic.family == family:\n                yield (interface, snic.address)\n\n\ndef init_argparse() -> argparse.ArgumentParser:\n\n    parser = argparse.ArgumentParser(\n        usage=\"%(prog)s config\",\n        description=\"Mitigates Flooding-DDoS-Attacks on DTN-nodes.\",\n    )\n\n    parser.add_argument(\"config\", help=\"Path to a configuration file\")\n\n    return parser\n\n\ndef interval_reset(nm: NodeManager) -> None:\n    \"\"\"Instructs NodeManager to reset the counts of the Bundles which were received in the last\n    interval.\n\n    Args:\n        nm (NodeManager): Management utility of the node for which the counts should be reset.\n    \"\"\"\n    print(\"RESET\")\n    nm.reset_flag = True\n    nm.reset_time_reached()\n\n\ndef wait_for_message(sock: socket.socket, wm: WorkerManager):\n    while True:\n        message, addr = sock.recvfrom(196)\n        threading.Thread(target=wm.add_bundle, args=[message]).start()\n\n\ndef listen(nm: NodeManager, wm: WorkerManager) -> None:\n    print(\"Listening for incoming bundles on node \" + nm.get_node_number() + \":\")\n\n    ip_list = get_ip_addresses(socket.AF_INET)\n    global sockets\n    sockets = []\n\n    for address in ip_list:\n        if not address[0] == \"lo\":\n            sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n            sock.bind((address[1], 4555))\n            print(\"Bound Socket to interface \" + address[0] + \", IP: \" + address[1])\n            threading.Thread(target=wait_for_message, args=[sock, wm]).start()\n            sockets.append(sock)\n\n\ndef main(nm: NodeManager, wm: WorkerManager) -> None:\n    \"\"\"Main event loop.\n\n    Args:\n        nm (NodeManager): [description]\n    \"\"\"\n\n    wm.start_workers()\n    # Create a thread for listening to incoming messages\n    logger.info(\"Starting thread listening for incoming messages\")\n    l = threading.Thread(target=listen, args=[nm, wm])\n    l.daemon = True\n    l.start()\n\n    # Schedule resetting the count of the received bundles\n    # every X seconds\n    global scheduler\n    scheduler = BackgroundScheduler()\n    scheduler.add_job(\n        interval_reset, IntervalTrigger(seconds=nm.get_reset_time()), args=([nm])\n    )\n    scheduler.start()\n    logging.getLogger(\"apscheduler\").setLevel(logging.WARNING)\n\n    while True:\n        time.sleep(100)\n\n\ndef cleanup() -> None:\n    \"\"\"Closes the socket and shuts down\n    the scheduler on SIGINT.\n    \"\"\"\n\n    global sockets\n    global scheduler\n\n    logger.info(\"---------Shutting down----------\")\n    for sock in sockets:\n        sock.close()\n    logger.info(\"Closed sockets\")\n    scheduler.shutdown()\n    logger.info(\"Shut down scheduler\")\n\n\ndef signal_handler(signum, frame) -> None:\n    logger.info(\"Received SIGINT\")\n    cleanup()\n    sys.exit(0)\n\n\nif __name__ == \"__main__\":\n\n    # Handling SIGINT\n    signal.signal(signal.SIGINT, signal_handler)\n\n    # Initializing logger\n    logging.basicConfig(\n        filename=\"framework.log\",\n        level=logging.INFO,\n        format=\"%(asctime)s %(name)s:%(levelname)s: %(message)s\",\n        datefmt=\"[%Y-%m-%d %H:%M:%S]\",\n    )\n\n    # Parsing command line arguments\n    parser = init_argparse()\n    args = parser.parse_args()\n\n    if not args.config:\n        print(\"Please provide a configuration file!\")\n        exit(1)\n    else:\n        try:\n            # Quick test to see if file exists\n            f = open(args.config, \"r\")\n            f.close()\n\n            # Initialize NodeManager with config file\n            logger.info(\"Starting NodeManager\")\n            nm = NodeManager(args.config)\n\n        except (FileNotFoundError, IsADirectoryError) as err:\n            logger.error(f\"{sys.argv[0]}: {args.config}: {err.strerror}\")\n            print(f\"{sys.argv[0]}: {args.config}: {err.strerror}\", file=sys.stderr)\n            exit(1)\n\n    logger.info(\"Entering main event loop.\")\n    wm = WorkerManager(nm, 25)\n    main(nm, wm)\n"},"\/Szenario_1\/1\/util\/nodemanager.py":{"changes":[{"diff":"\n import os\n import subprocess\n import time\n-from typing import Dict, List\n+from typing import Dict, List, Set\n import threading\n \n import pyion\n","add":1,"remove":1,"filename":"\/Szenario_1\/1\/util\/nodemanager.py","badparts":["from typing import Dict, List"],"goodparts":["from typing import Dict, List, Set"]},{"diff":"\n                     if key <= current_trust\n                 )\n             ]\n-            nn = self.__node_num\n-\n-            # change directories to the node's directory\n-            # so the SDR can be queried\n-            os.chdir(self.__node_dir)\n \n-            # Query ION's SDR memory\n-            process = subprocess.run(\n-                [\"ip netns exec nns\" + nn + \" sdrwatch n\" + nn],\n-                capture_output=True,\n-                shell=True,\n-            )\n-            stdout = process.stdout\n-            stdout = stdout.decode(\"utf-8\").split(\"\\n\")\n-            for string in stdout:\n-                if \"total now in use\" in string:\n-                    used = string\n-                elif \"total heap size\" in string:\n-                    total = string\n-\n-            # sdrwatch returns a string in the format \"total heap:              40000\"\n-            # -> the number needs to be extracted\n-            total = [int(n) for n in total.split() if n.isdigit()][0]\n-            used = [int(n) for n in used.split() if n.isdigit()][0]\n-            percentage = (float(used) \/ total) * 100\n-\n-            return True if percentage < cutoff else False\n+            return True if self.__mem_percentage < cutoff else False\n         else:\n             # the sender has a trust score of 0\n             # -> do not accept any of its bundles\n+            self.__logger.info(\"NOT ACCEPTING BUNDLE\")\n+            self.__banned_nodes.add(node)\n             return False\n \n     def reset_time_reached(self) -> None:\n","add":3,"remove":26,"filename":"\/Szenario_1\/1\/util\/nodemanager.py","badparts":["            nn = self.__node_num","            os.chdir(self.__node_dir)","            process = subprocess.run(","                [\"ip netns exec nns\" + nn + \" sdrwatch n\" + nn],","                capture_output=True,","                shell=True,","            )","            stdout = process.stdout","            stdout = stdout.decode(\"utf-8\").split(\"\\n\")","            for string in stdout:","                if \"total now in use\" in string:","                    used = string","                elif \"total heap size\" in string:","                    total = string","            total = [int(n) for n in total.split() if n.isdigit()][0]","            used = [int(n) for n in used.split() if n.isdigit()][0]","            percentage = (float(used) \/ total) * 100","            return True if percentage < cutoff else False"],"goodparts":["            return True if self.__mem_percentage < cutoff else False","            self.__logger.info(\"NOT ACCEPTING BUNDLE\")","            self.__banned_nodes.add(node)"]},{"diff":"\n             self.__threshold_flooding,\n             self.__penalty_growth_rate,\n             self.__trust_recovery_rate,\n-        )\n\\ No newline at end of file\n+        )\n+        self.__mem_percentage = self.calc_memory_percentage()\n+\n+    def calc_memory_percentage(self) -> float:\n+        \"\"\"Calculates the percentage of unavailable memory space.\n+\n+        Returns:\n+            float: Percentage of memory used\n+        \"\"\"\n+        nn = self.__node_num\n+\n+        # change directories to the node's directory\n+        # so the SDR can be queried\n+        os.chdir(self.__node_dir)\n+\n+        # Query ION's SDR memory\n+        process = subprocess.run(\n+            [\"ip netns exec nns\" + nn + \" sdrwatch n\" + nn],\n+            capture_output=True,\n+            shell=True,\n+        )\n+        stdout = process.stdout\n+        stdout = stdout.decode(\"utf-8\").split(\"\\n\")\n+        for string in stdout:\n+            if \"total now in use\" in string:\n+                used = string\n+            elif \"total heap size\" in string:\n+                total = string\n+\n+        # sdrwatch returns a string in the format \"total heap:              40000\"\n+        # -> the number needs to be extracted\n+        total = [int(n) for n in total.split() if n.isdigit()][0]\n+        used = [int(n) for n in used.split() if n.isdigit()][0]\n+        perc = (float(used) \/ total) * 100\n+        self.__logger.info(\"Memory in use:\" + str(perc))\n+        return perc\n+\n+    def get_banned_nodes(self) -> List[str]:\n+        return self.__banned_node","add":39,"remove":1,"filename":"\/Szenario_1\/1\/util\/nodemanager.py","badparts":["        )"],"goodparts":["        )","        self.__mem_percentage = self.calc_memory_percentage()","    def calc_memory_percentage(self) -> float:","        \"\"\"Calculates the percentage of unavailable memory space.","        Returns:","            float: Percentage of memory used","        \"\"\"","        nn = self.__node_num","        os.chdir(self.__node_dir)","        process = subprocess.run(","            [\"ip netns exec nns\" + nn + \" sdrwatch n\" + nn],","            capture_output=True,","            shell=True,","        )","        stdout = process.stdout","        stdout = stdout.decode(\"utf-8\").split(\"\\n\")","        for string in stdout:","            if \"total now in use\" in string:","                used = string","            elif \"total heap size\" in string:","                total = string","        total = [int(n) for n in total.split() if n.isdigit()][0]","        used = [int(n) for n in used.split() if n.isdigit()][0]","        perc = (float(used) \/ total) * 100","        self.__logger.info(\"Memory in use:\" + str(perc))","        return perc","    def get_banned_nodes(self) -> List[str]:","        return self.__banned_node"]}],"source":"\nimport configparser import logging import math import os import subprocess import time from typing import Dict, List import threading import pyion import util.utils class NodeManager: \"\"\" Class containing information about a node. \"\"\" __logger: logging.Logger __node_num: str __eid_send: str __eid_recv: str __is_flooder: bool __neighbours: List[str] __node_dir: str __threshold_flooding: int __memory_limits: Dict[float, int] __penalty_growth_rate: float __trust_recovery_rate: float __trust_scores: Dict[str, float] __reset_interval: int __bundles_last_interval: Dict[str, int] reset_flag: bool def __init__(self, path: str) -> None: self.__logger=logging.getLogger(__name__) config=configparser.ConfigParser() config.read(path) self.__logger.info(\"Getting node config\") self.__node_num=config[\"NODE_CONFIG\"][\"node_number\"] self.__eid_send=config[\"NODE_CONFIG\"][\"eid_send\"] self.__eid_recv=config[\"NODE_CONFIG\"][\"eid_receive\"] self.__is_flooder=config.getboolean(\"NODE_CONFIG\", \"is_flooder\") self.__neighbours=config[\"NODE_CONFIG\"][\"neighbours\"].split(\",\") self.__node_dir=config[\"NODE_CONFIG\"][\"node_dir\"] self.__logger.info(\"Node number: \" +str(self.__node_num)) self.__logger.info(\"Sending from EID \" +self.__eid_send) self.__logger.info(\"Receiving critical info on EID \" +self.__eid_recv) self.__logger.info( \"Node is flooder: \" +(\"no\" if not self.__is_flooder else \"yes\") ) self.__logger.info(\"Neighbours: \" +str(self.__neighbours)) self.__logger.info(\"Initialized proxy to ION engine.\") self.__logger.info(\"Initializing trust system\") self.__threshold_flooding=int(config[\"TRUST_CONFIG\"][\"threshold_flooding\"]) l=[ word for line in config[\"TRUST_CONFIG\"][\"memory_limits\"].split(\"|\") for word in line.split(\",\") ] it=iter(l) self.__memory_limits={ float(score): int(limit) for score, limit in zip(it, it) } self.__penalty_growth_rate=float( config[\"TRUST_CONFIG\"][\"punishment_growth_rate\"] ) self.__trust_recovery_rate=float( config[\"TRUST_CONFIG\"][\"trust_recovery_rate\"] ) self.__trust_scores={node: 10.0 for node in self.__neighbours} self.__reset_interval=int(config[\"TRUST_CONFIG\"][\"reset_interval\"]) self.__bundles_last_interval={node: 0 for node in self.__neighbours} self.reset_flag=False self.__logger.info( \"Threshold value for flooding: \" +str(self.__threshold_flooding) +\" bps\" ) self.__logger.info(\"Bundle acceptance cutoff: \" +str(self.__memory_limits)) self.__logger.info( \"Growth of penalty function: \" +( \"linear\" if self.__penalty_growth_rate==1.0 else \"exponential(factor: \" +str(self.__penalty_growth_rate) +\")\" ) ) self.__logger.info( \"Trust recovery rate: \" +( (\"disabled\") if self.__trust_recovery_rate==0 else str(self.__trust_recovery_rate) ) ) self.__logger.info(\"Initialized trust scores: \" +str(self.__trust_scores)) self.__logger.info( \"Bundle count reset interval set to: \" +str(self.__reset_interval) +\" seconds\" ) self.__logger.info( \"Initialized bundles received in the last \" +str(self.__reset_interval) +\" seconds: \" +str(self.__bundles_last_interval) ) def count_recvd_bundle(self, node_nbr: str, no_bundles: int=1) -> int: \"\"\"Updates the __bundles_last_interval variable by adding the number of bundles which were newly received. Args: node_nbr(str): Node number \/ name of the sender. no_bundles(int, optional): Number of bundles node_nbr sent. Defaults to 1. Returns: int: The number of bundles exceeding __threshold_flooding. \"\"\" if self.reset_flag: self.__bundles_last_interval[node_nbr]=no_bundles self.reset_flag=False else: self.__bundles_last_interval[node_nbr] +=no_bundles return( 0 if self.__bundles_last_interval[node_nbr] <=self.__threshold_flooding else self.__bundles_last_interval[node_nbr] -self.__threshold_flooding ) def reset_rcvd_bundles(self) -> None: \"\"\"Resets the count of bundles received in the last second to 0 for each neighbour.\"\"\" self.__bundles_last_interval={node: 0 for node in self.__neighbours} def get_node_number(self) -> str: \"\"\"Returns own node number Returns: str: The node number \/ name of this node \"\"\" return self.__node_num def get_ion_proxy(self) -> object: return self.__ion_proxy def get_neighbours(self) -> List[str]: \"\"\"Returns all neighbours of this node. Returns: List[str]: List of neighbours \"\"\" return self.__neighbours def is_neighbour(self, node_nbr: str) -> bool: \"\"\"Checks if a given node is a neighbour of this node. Args: node_nbr(str): Node number \/ name of the node to check. Returns: bool: True if the node is a neighbour, False else. \"\"\" return node_nbr in self.__neighbours def get_trust_scores(self) -> Dict[str, float]: \"\"\"Returns a dict containing the trust scores for all neighbouring nodes. Returns: Dict[str, float]: Dict in the format{node:trust_score,...} \"\"\" return self.__trust_scores def get_reset_time(self) -> int: \"\"\"Returns the reset interval as specified in the configuration file. Returns: int: The reset interval in seconds \"\"\" return self.__reset_interval def update_trust_scores(self, new_scores: Dict[str, float]) -> None: \"\"\"Sets the trust scores to the supplied values. Args: new_scores(Dict[str, float]): A dict containing the new scores. \"\"\" self.__trust_scores=new_scores def can_accept_bundle(self, node: str) -> bool: \"\"\"Compares memory usage with the cutoff value defined by the senders current trust score and decides whether to accept or decline the bundle. Args: node(str): The node number of the sender Returns: bool: Decision on whether to accept or decline the bundle. True if bundle gets accepted False else \"\"\" current_trust=self.__trust_scores[node] if current_trust > 0: cutoff=self.__memory_limits[ max( key for key in map(int, self.__memory_limits.keys()) if key <=current_trust ) ] nn=self.__node_num os.chdir(self.__node_dir) process=subprocess.run( [\"ip netns exec nns\" +nn +\" sdrwatch n\" +nn], capture_output=True, shell=True, ) stdout=process.stdout stdout=stdout.decode(\"utf-8\").split(\"\\n\") for string in stdout: if \"total now in use\" in string: used=string elif \"total heap size\" in string: total=string total=[int(n) for n in total.split() if n.isdigit()][0] used=[int(n) for n in used.split() if n.isdigit()][0] percentage=(float(used) \/ total) * 100 return True if percentage < cutoff else False else: return False def reset_time_reached(self) -> None: \"\"\"The specified time has elapsed, calculates new trust scores.\"\"\" util.utils.calc_penalty( self, self.__bundles_last_interval, self.__trust_scores, self.__threshold_flooding, self.__penalty_growth_rate, self.__trust_recovery_rate, ) ","sourceWithComments":"import configparser\nimport logging\nimport math\nimport os\nimport subprocess\nimport time\nfrom typing import Dict, List\nimport threading\n\nimport pyion\nimport util.utils\n\n\nclass NodeManager:\n    \"\"\"\n    Class containing information about a node.\n    \"\"\"\n\n    # ------------------------\n    # |    INTERNAL VARS     |\n    # ------------------------\n    __logger: logging.Logger\n\n    # ------------------------\n    # |      NODE VARS       |\n    # ------------------------\n    __node_num: str\n    __eid_send: str\n    __eid_recv: str\n    __is_flooder: bool\n    __neighbours: List[str]\n    __node_dir: str\n\n    # ------------------------\n    # |      TRUST VARS      |\n    # ------------------------\n    __threshold_flooding: int\n    __memory_limits: Dict[float, int]\n    __penalty_growth_rate: float\n    __trust_recovery_rate: float\n    __trust_scores: Dict[str, float]\n    __reset_interval: int\n    __bundles_last_interval: Dict[str, int]\n    reset_flag: bool\n\n    # ------------------------\n    # |    INITIALIZATION    |\n    # ------------------------\n    def __init__(self, path: str) -> None:\n\n        # Initialize Logger\n        self.__logger = logging.getLogger(__name__)\n\n        config = configparser.ConfigParser()\n        config.read(path)\n\n        # Configuring the node\n        self.__logger.info(\"Getting node config\")\n        self.__node_num = config[\"NODE_CONFIG\"][\"node_number\"]\n        self.__eid_send = config[\"NODE_CONFIG\"][\"eid_send\"]\n        self.__eid_recv = config[\"NODE_CONFIG\"][\"eid_receive\"]\n        self.__is_flooder = config.getboolean(\"NODE_CONFIG\", \"is_flooder\")\n        self.__neighbours = config[\"NODE_CONFIG\"][\"neighbours\"].split(\",\")\n        self.__node_dir = config[\"NODE_CONFIG\"][\"node_dir\"]\n\n        # Logging results\n        self.__logger.info(\"Node number: \" + str(self.__node_num))\n        self.__logger.info(\"Sending from EID \" + self.__eid_send)\n        self.__logger.info(\"Receiving critical info on EID \" + self.__eid_recv)\n        self.__logger.info(\n            \"Node is flooder: \" + (\"no\" if not self.__is_flooder else \"yes\")\n        )\n        self.__logger.info(\"Neighbours: \" + str(self.__neighbours))\n        self.__logger.info(\"Initialized proxy to ION engine.\")\n\n        # Initializing the trust system\n        self.__logger.info(\"Initializing trust system\")\n        self.__threshold_flooding = int(config[\"TRUST_CONFIG\"][\"threshold_flooding\"])\n        l = [\n            word\n            for line in config[\"TRUST_CONFIG\"][\"memory_limits\"].split(\"|\")\n            for word in line.split(\",\")\n        ]\n        it = iter(l)\n        self.__memory_limits = {\n            float(score): int(limit) for score, limit in zip(it, it)\n        }\n        self.__penalty_growth_rate = float(\n            config[\"TRUST_CONFIG\"][\"punishment_growth_rate\"]\n        )\n        self.__trust_recovery_rate = float(\n            config[\"TRUST_CONFIG\"][\"trust_recovery_rate\"]\n        )\n        self.__trust_scores = {node: 10.0 for node in self.__neighbours}\n        self.__reset_interval = int(config[\"TRUST_CONFIG\"][\"reset_interval\"])\n        self.__bundles_last_interval = {node: 0 for node in self.__neighbours}\n        self.reset_flag = False\n\n        # Logging the results\n        self.__logger.info(\n            \"Threshold value for flooding: \" + str(self.__threshold_flooding) + \" bps\"\n        )\n        self.__logger.info(\"Bundle acceptance cutoff: \" + str(self.__memory_limits))\n        self.__logger.info(\n            \"Growth of penalty function: \"\n            + (\n                \"linear\"\n                if self.__penalty_growth_rate == 1.0\n                else \"exponential (factor: \" + str(self.__penalty_growth_rate) + \")\"\n            )\n        )\n        self.__logger.info(\n            \"Trust recovery rate: \"\n            + (\n                (\"disabled\")\n                if self.__trust_recovery_rate == 0\n                else str(self.__trust_recovery_rate)\n            )\n        )\n        self.__logger.info(\"Initialized trust scores: \" + str(self.__trust_scores))\n        self.__logger.info(\n            \"Bundle count reset interval set to: \"\n            + str(self.__reset_interval)\n            + \" seconds\"\n        )\n        self.__logger.info(\n            \"Initialized bundles received in the last \"\n            + str(self.__reset_interval)\n            + \" seconds: \"\n            + str(self.__bundles_last_interval)\n        )\n\n    def count_recvd_bundle(self, node_nbr: str, no_bundles: int = 1) -> int:\n        \"\"\"Updates the __bundles_last_interval variable by adding the number of bundles which were newly received.\n\n        Args:\n            node_nbr (str): Node number \/ name of the sender.\n            no_bundles (int, optional): Number of bundles node_nbr sent. Defaults to 1.\n\n        Returns:\n            int: The number of bundles exceeding __threshold_flooding.\n        \"\"\"\n        if self.reset_flag:\n            self.__bundles_last_interval[node_nbr] = no_bundles\n            self.reset_flag = False\n        else:\n            self.__bundles_last_interval[node_nbr] += no_bundles\n        return (\n            0\n            if self.__bundles_last_interval[node_nbr] <= self.__threshold_flooding\n            else self.__bundles_last_interval[node_nbr] - self.__threshold_flooding\n        )\n\n    def reset_rcvd_bundles(self) -> None:\n        \"\"\"Resets the count of bundles received in the last second to 0 for each neighbour.\"\"\"\n        self.__bundles_last_interval = {node: 0 for node in self.__neighbours}\n\n    def get_node_number(self) -> str:\n        \"\"\"Returns own node number\n\n        Returns:\n            str: The node number \/ name of this node\n        \"\"\"\n        return self.__node_num\n\n    def get_ion_proxy(self) -> object:\n        return self.__ion_proxy\n\n    def get_neighbours(self) -> List[str]:\n        \"\"\"Returns all neighbours of this node.\n\n        Returns:\n            List[str]: List of neighbours\n        \"\"\"\n        return self.__neighbours\n\n    def is_neighbour(self, node_nbr: str) -> bool:\n        \"\"\"Checks if a given node is a neighbour of this node.\n\n        Args:\n            node_nbr (str): Node number \/ name of the node to check.\n\n        Returns:\n            bool: True if the node is a neighbour, False else.\n        \"\"\"\n\n        return node_nbr in self.__neighbours\n\n    def get_trust_scores(self) -> Dict[str, float]:\n        \"\"\"Returns a dict containing the trust scores for all\n        neighbouring nodes.\n\n        Returns:\n            Dict[str, float]: Dict in the format {node:trust_score,...}\n        \"\"\"\n        return self.__trust_scores\n\n    def get_reset_time(self) -> int:\n        \"\"\"Returns the reset interval as specified in the\n        configuration file.\n\n        Returns:\n            int: The reset interval in seconds\n        \"\"\"\n        return self.__reset_interval\n\n    def update_trust_scores(self, new_scores: Dict[str, float]) -> None:\n        \"\"\"Sets the trust scores to the supplied values.\n\n        Args:\n            new_scores (Dict[str, float]): A dict containing the new scores.\n        \"\"\"\n        self.__trust_scores = new_scores\n\n    def can_accept_bundle(self, node: str) -> bool:\n        \"\"\"Compares memory usage with the cutoff value\n        defined by the senders current trust score and decides\n        whether to accept or decline the bundle.\n\n        Args:\n            node (str): The node number of the sender\n\n        Returns:\n            bool: Decision on whether to accept or decline the bundle.\n                    True if bundle gets accepted\n                    False else\n        \"\"\"\n\n        current_trust = self.__trust_scores[node]\n\n        if current_trust > 0:\n            # find cutoff by evaluating all key of the dict\n            # and choosing the one closest (but still less than)\n            # or equal to current_trust\n            # Snippet adapted from https:\/\/stackoverflow.com\/a\/37851350\n            cutoff = self.__memory_limits[\n                max(\n                    key\n                    for key in map(int, self.__memory_limits.keys())\n                    if key <= current_trust\n                )\n            ]\n            nn = self.__node_num\n\n            # change directories to the node's directory\n            # so the SDR can be queried\n            os.chdir(self.__node_dir)\n\n            # Query ION's SDR memory\n            process = subprocess.run(\n                [\"ip netns exec nns\" + nn + \" sdrwatch n\" + nn],\n                capture_output=True,\n                shell=True,\n            )\n            stdout = process.stdout\n            stdout = stdout.decode(\"utf-8\").split(\"\\n\")\n            for string in stdout:\n                if \"total now in use\" in string:\n                    used = string\n                elif \"total heap size\" in string:\n                    total = string\n\n            # sdrwatch returns a string in the format \"total heap:              40000\"\n            # -> the number needs to be extracted\n            total = [int(n) for n in total.split() if n.isdigit()][0]\n            used = [int(n) for n in used.split() if n.isdigit()][0]\n            percentage = (float(used) \/ total) * 100\n\n            return True if percentage < cutoff else False\n        else:\n            # the sender has a trust score of 0\n            # -> do not accept any of its bundles\n            return False\n\n    def reset_time_reached(self) -> None:\n        \"\"\"The specified time has elapsed, calculates new trust scores.\"\"\"\n        util.utils.calc_penalty(\n            self,\n            self.__bundles_last_interval,\n            self.__trust_scores,\n            self.__threshold_flooding,\n            self.__penalty_growth_rate,\n            self.__trust_recovery_rate,\n        )"},"\/Szenario_1\/1\/util\/utils.py":{"changes":[{"diff":"\n     \"\"\"\n     for node in bundles:\n         value = bundles[node]\n-        logger.info(\"Accepted bundles from node \" + str(node) + \": \" + str(value))\n+        logger.info(\"Bundles from node \" + str(node) + \": \" + str(value))\n         if value > threshold:\n             scores[node] = max(\n                 (scores[node] - (((value - threshold) ** factor) * 0.1)), 0\n","add":1,"remove":1,"filename":"\/Szenario_1\/1\/util\/utils.py","badparts":["        logger.info(\"Accepted bundles from node \" + str(node) + \": \" + str(value))"],"goodparts":["        logger.info(\"Bundles from node \" + str(node) + \": \" + str(value))"]},{"diff":"\n             logger.info(\"NEW SCORE NODE \" + node + \": \" + str(scores[node]))\n         elif scores[node] < 10 and scores[node] > 0:\n             scores[node] += rec_rate\n-    logger.info(\"Scores node \" + nm.get_node_number() + \": \" + str(scores))\n+\n+        if scores[node] > 10:\n+            scores[node] = 10\n     nm.update_trust_scores(score","add":3,"remove":1,"filename":"\/Szenario_1\/1\/util\/utils.py","badparts":["    logger.info(\"Scores node \" + nm.get_node_number() + \": \" + str(scores))"],"goodparts":["        if scores[node] > 10:","            scores[node] = 10"]}],"source":"\nfrom util.nodemanager import NodeManager from cbor2 import loads from typing import Dict import logging import queue import threading import time logger=logging.getLogger(__name__) def get_bundle_source(recvd_message: bytes) -> str: \"\"\"Parses the received data and extracts the senders node number. Args: recvd_message(bytes): Bundle conforming to the Concise Binary Object Representation. Returns: str: The node number \/ name of the bundles origin. \"\"\" return str(loads(recvd_message)[0][4][1][0]) def calc_penalty( nm: NodeManager, bundles: Dict[str, int], scores: Dict[str, float], threshold: int, factor: float=1.5, rec_rate: float=0.1, ) -> None: \"\"\"Calculates the penalty a node gets for sending too many bundles in a short timeframe. Args: nm(NodeManager): Manager of the node for whose neighbours the trust scores are calculated. bundles(Dict[str, int]): The amount of bundles received from each neighbour in the last interval. scores(Dict[str, float]): The current trust scores for all neighbours. threshold(int): The amount of bundles after which sending more bundles is considered flooding. factor(float, optional): Exponent of the penalty function. Defaults to 1.5. rec_rate(float, optional): The amount of trust a node regenerates if it didn't flood. Defaults to 0.1. \"\"\" for node in bundles: value=bundles[node] logger.info(\"Accepted bundles from node \" +str(node) +\": \" +str(value)) if value > threshold: scores[node]=max( (scores[node] -(((value -threshold) ** factor) * 0.1)), 0 ) logger.info(\"NEW SCORE NODE \" +node +\": \" +str(scores[node])) elif scores[node] < 10 and scores[node] > 0: scores[node] +=rec_rate logger.info(\"Scores node \" +nm.get_node_number() +\": \" +str(scores)) nm.update_trust_scores(scores) ","sourceWithComments":"from util.nodemanager import NodeManager\nfrom cbor2 import loads\nfrom typing import Dict\nimport logging\nimport queue\nimport threading\nimport time\n\nlogger = logging.getLogger(__name__)\n\n\ndef get_bundle_source(recvd_message: bytes) -> str:\n    \"\"\"Parses the received data and extracts the senders node number.\n\n    Args:\n        recvd_message (bytes): Bundle conforming to the Concise Binary Object Representation.\n\n    Returns:\n        str: The node number \/ name of the bundles origin.\n    \"\"\"\n\n    # Decoding the byte-representation and accessing the node number of the sender.\n    # see https:\/\/datatracker.ietf.org\/doc\/html\/draft-ietf-dtn-bpbis-31#section-4.3.1\n    # for the structure of the primary bundle block.\n    return str(loads(recvd_message)[0][4][1][0])\n\n\ndef calc_penalty(\n    nm: NodeManager,\n    bundles: Dict[str, int],\n    scores: Dict[str, float],\n    threshold: int,\n    factor: float = 1.5,\n    rec_rate: float = 0.1,\n) -> None:\n    \"\"\"Calculates the penalty a node gets for sending\n    too many bundles in a short timeframe.\n\n    Args:\n        nm (NodeManager): Manager of the node for whose neighbours the trust scores are calculated.\n        bundles (Dict[str, int]): The amount of bundles received from each neighbour in the last interval.\n        scores (Dict[str, float]): The current trust scores for all neighbours.\n        threshold (int): The amount of bundles after which sending more bundles is considered flooding.\n        factor (float, optional): Exponent of the penalty function. Defaults to 1.5.\n        rec_rate (float, optional): The amount of trust a node regenerates if it didn't flood. Defaults to 0.1.\n    \"\"\"\n    for node in bundles:\n        value = bundles[node]\n        logger.info(\"Accepted bundles from node \" + str(node) + \": \" + str(value))\n        if value > threshold:\n            scores[node] = max(\n                (scores[node] - (((value - threshold) ** factor) * 0.1)), 0\n            )\n            logger.info(\"NEW SCORE NODE \" + node + \": \" + str(scores[node]))\n        elif scores[node] < 10 and scores[node] > 0:\n            scores[node] += rec_rate\n    logger.info(\"Scores node \" + nm.get_node_number() + \": \" + str(scores))\n    nm.update_trust_scores(scores)\n"}},"msg":"fixed a security flaw that made the framework itself vulnerable to flooding attacks"}},"https:\/\/github.com\/taneugene\/oscar":{"12991a4b71008843e58046227ed7c9378e7e77d5":{"url":"https:\/\/api.github.com\/repos\/taneugene\/oscar\/commits\/12991a4b71008843e58046227ed7c9378e7e77d5","html_url":"https:\/\/github.com\/taneugene\/oscar\/commit\/12991a4b71008843e58046227ed7c9378e7e77d5","message":"ssbn and exposure handling, currently a bad vulnerability function (affected by floods)","sha":"12991a4b71008843e58046227ed7c9378e7e77d5","keyword":"flooding vulnerability","diff":"diff --git a\/lib\/ssbn.py b\/lib\/ssbn.py\nindex 078fb63..59b3eae 100644\n--- a\/lib\/ssbn.py\n+++ b\/lib\/ssbn.py\n@@ -91,19 +91,14 @@ def get_basic_info(path):\n     # print(fname)\n     m = re.match(\"^([A-Z]{2})-([FPUM])([DU]{0,1})-([0-9]{1,4})-([0-9])*\\.tif$\", fname)\n     d = {}\n+    d['folder'] = path[:-neg_pos]\n     d['path'] = path\n     d['filename'] = m[0]\n     d['iso2'] = m[1]\n     d['type'] = m[2]\n     d['return_period'] = int(m[4])\n     d['tile'] = int(m[5])\n-    if m[3] == \"D\":\n-        d['defended'] = True\n-    elif m[3] == 'U':\n-        d['defended'] = False\n-    else:\n-        print(\"Regex is not capturing defended-ness from filename correctly\")\n-        assert(False)\n+    d['defended'] = m[3]\n     return d\n def gpw_basic_info(asset_fname):\n     f = asset_fname[-(asset_fname[::-1].find('\/')):]\n@@ -115,6 +110,9 @@ def gpw_basic_info(asset_fname):\n def get_param_info(flist):\n     lis = [get_basic_info(f) for f in flist]\n     d = {}\n+    d['folder'] = lis[0]['folder']\n+    d['flist'] = sorted(pd.Series([d['filename']for d in lis]).unique())\n+    d['defended'] = lis[0]['defended']\n     d['iso2'] = lis[0]['iso2']\n     d['type'] = lis[0]['type']\n     d['rps'] = sorted(pd.Series([d['return_period']for d in lis]).unique())\n@@ -228,22 +226,55 @@ def get_tiles(country, folder, rp):\n     exposures = sorted(glob.glob('data_exposures\/gpw\/{}*'.format(c)))[:-1]\n     return floods, exposures\n def Rasterize(shapefile, inras, outras):\n-    \"\"\"From: https:\/\/gist.github.com\/mhweber\/1a07b0881c2ab88d062e3b32060e5486\"\"\"\n+    if exists(outras):\n+        print('{} already exists'.format(outras))\n+        return\n     with rasterio.open(inras) as src:\n         kwargs = src.meta.copy()\n         kwargs.update({\n             'driver': 'GTiff',\n-            'compress': 'lzw'\n+            'compress': 'DEFLATE'\n         })\n-        windows = src.block_windows(1)\n         with rasterio.open(outras, 'w', **kwargs) as dst:\n-            for idx, window in windows:\n-                out_arr = np.zeros_like(src.read(1, window=window))\n-                # this is where we create a generator of geom, value pairs to use in rasterizing\n-                shapes = ((geom,value) for geom, value in zip(shapefile.geometry, shapefile.index))\n-                burned = features.rasterize(shapes=shapes, fill=0, out=out_arr, transform=src.transform)\n-                dst.write_band(1, burned, window=window)\n+            out_arr = np.zeros_like(src.read(1))\n+            # this is where we create a generator of geom, value pairs to use in rasterizing\n+            shapes = ((geom,value) for geom, value in zip(shapefile.geometry, shapefile.index))\n+            burned = features.rasterize(shapes=shapes, fill=0, out=out_arr, transform=src.transform)\n+            dst.write_band(1, burned)\n \n+def vulnerability(hazard, exposure):\n+    assert hazard.shape == exposure.shape\n+    hazard = hazard.astype(bool)\n+    damage = exposure * hazard\n+    return damage\n+\n+def estimate_affected(row, country, params):\n+    # Loop through tiles for each basin\n+    print(row.name)\n+    for tile in params['tiles']:\n+        # Get the raster of basin == the current basin\n+        fname = '.\/data_exposures\/{}_hybas_raster_{}.tif'.format(params['iso2'], tile)\n+        hybas = gtiff_to_array(fname)\n+        hybas_mask = (hybas == row.name)\n+        # Get the raster of population\n+        exp_fname = '.\/data_exposures\/gpw\/{}_population_count_{}.tif'.format(params['iso2'], tile)\n+        exp = gtiff_to_array(exp_fname)\n+        exp[exp<0] = 0\n+        # Nearest neighbor went from 30s to 3s\n+        exp = exp\/100\n+        # Get the total population by basin and store it\n+        assert hybas_mask.shape == exp.shape\n+        total_pop = (hybas_mask*exp)\n+        row['basin_pop'] += total_pop.sum()\n+        # Loop through return periods\n+        for rp in params['rps']:\n+            # Get the raster of boolean floods by return period\n+            fname = '{}{}-{}{}-{}-{}.tif'.format(params['folder'],c,params['type'],params['defended'], str(int(rp)), tile)\n+            floods = get_ssbn_array(fname)\n+            # Store the population affected for floods\n+            total_affected = vulnerability(floods, total_pop).sum()\n+            row[rp] += total_affected\n+    return row\n \n # Resample assets grids (e.g. gpw) to the tile sizes that ssbn gives\n country = ['NG','AR','PE','CO']\n@@ -260,32 +291,45 @@ def Rasterize(shapefile, inras, outras):\n #     flist = sorted(glob.glob(folders(c)[0]+'*250*'))\n #     mosaic(flist)\n \n-\n-c = 'CO'\n+c = 'AR'\n folder = folders(c)[0]\n fname = 'data_exposures\/gpw\/{}_population_count_all.tif'.format(c)\n df = filter_polygons(fname)\n+df.plot()\n tiles = sorted((glob.glob(folder+'*')))\n params = get_param_info(tiles)\n floods, exposures = get_tiles(c,folder, params['rps'][0])\n a,gt,s = get_ssbn_array(floods[0], True)\n b = get_bounds(floods[0])\n \n-lons = np.array([round(gt[0]+gt[1]*i,5) for i in range(a.shape[1])])\n-lats = np.array([round(gt[3]+gt[5]*i,5) for i in range(a.shape[0])])\n-loni = np.array([i for i in range(a.shape[1])])\n-lati = np.array([i for i in range(a.shape[0])])\n-xx,yy = np.meshgrid(lons, lats)\n+\n+country = ['AR','PE','CO']\n+c = 'PE'\n+for c in country:\n+    fname = 'data_exposures\/gpw\/{}_population_count_all.tif'.format(c)\n+    folder = folders(c)[0]\n+    flist = sorted(glob.glob(folder+'*'))\n+    params = get_param_info(flist)\n+    # Get the df of basins\n+    df = filter_polygons(fname = 'data_exposures\/gpw\/{}_population_count_all.tif'.format(c))\n+    # basins don't change with return period\n+    floods, exposures = get_tiles(c,folder, params['rps'][0])\n+    for tile in floods:\n+        print(tile)\n+        d = get_basic_info(tile)\n+        outras = '.\/data_exposures\/{}_hybas_raster_{}.tif'.format(d['iso2'], d['tile'])\n+        Rasterize(df,tile, outras)\n+    # Initialize to 0\n+    df['basin_pop'] = 0\n+    for rp in params['rps']:\n+        df[rp] = 0\n+    # Apply over basins\n+    df = df.apply(estimate_affected, axis = 1, country = c, params = params)\n+\n+    df.to_file(\"{}_floods_rp_{}.geojson\".format(c,params['type']+params['defended']), driver='GeoJSON')\n \n \n-# Try burning the basins to grid.\n-out_fn = '.\/{}rasterized_basins.tif'.format(c)\n-# Rasterize\n \n-# After rasterizing\n-df.\n-a = Rasterize(df, floods[3], out_fn)\n-b = gtiff_to_array(out_fn)\n \n # SSBN Processing\n     # Convert to boolean\ndiff --git a\/main.py b\/main.py\nindex 8e350b0..0056f68 100644\n--- a\/main.py\n+++ b\/main.py\n@@ -6,6 +6,8 @@\n # user defined libraries\n sys.path.append('lib\/')\n import ssbn\n+import hydrobasins\n+import tifs\n \n # Model parameters\n unzip_all = True\n@@ -26,16 +28,7 @@\n \n \n \n-\n-\n-\n-# Function to look through a directory and merge tifs into one big tif?\n-\n-# Function to convert a gtiff to an array\n-    # Function to get all the relevant details and metadata from a tif\n-\n-\n-\n+# Get to\n \n \n # Ideal Structure of model\n","files":{"\/lib\/ssbn.py":{"changes":[{"diff":"\n     # print(fname)\n     m = re.match(\"^([A-Z]{2})-([FPUM])([DU]{0,1})-([0-9]{1,4})-([0-9])*\\.tif$\", fname)\n     d = {}\n+    d['folder'] = path[:-neg_pos]\n     d['path'] = path\n     d['filename'] = m[0]\n     d['iso2'] = m[1]\n     d['type'] = m[2]\n     d['return_period'] = int(m[4])\n     d['tile'] = int(m[5])\n-    if m[3] == \"D\":\n-        d['defended'] = True\n-    elif m[3] == 'U':\n-        d['defended'] = False\n-    else:\n-        print(\"Regex is not capturing defended-ness from filename correctly\")\n-        assert(False)\n+    d['defended'] = m[3]\n     return d\n def gpw_basic_info(asset_fname):\n     f = asset_fname[-(asset_fname[::-1].find('\/')):]\n","add":2,"remove":7,"filename":"\/lib\/ssbn.py","badparts":["    if m[3] == \"D\":","        d['defended'] = True","    elif m[3] == 'U':","        d['defended'] = False","    else:","        print(\"Regex is not capturing defended-ness from filename correctly\")","        assert(False)"],"goodparts":["    d['folder'] = path[:-neg_pos]","    d['defended'] = m[3]"]},{"diff":"\n     exposures = sorted(glob.glob('data_exposures\/gpw\/{}*'.format(c)))[:-1]\n     return floods, exposures\n def Rasterize(shapefile, inras, outras):\n-    \"\"\"From: https:\/\/gist.github.com\/mhweber\/1a07b0881c2ab88d062e3b32060e5486\"\"\"\n+    if exists(outras):\n+        print('{} already exists'.format(outras))\n+        return\n     with rasterio.open(inras) as src:\n         kwargs = src.meta.copy()\n         kwargs.update({\n             'driver': 'GTiff',\n-            'compress': 'lzw'\n+            'compress': 'DEFLATE'\n         })\n-        windows = src.block_windows(1)\n         with rasterio.open(outras, 'w', **kwargs) as dst:\n-            for idx, window in windows:\n-                out_arr = np.zeros_like(src.read(1, window=window))\n-                # this is where we create a generator of geom, value pairs to use in rasterizing\n-                shapes = ((geom,value) for geom, value in zip(shapefile.geometry, shapefile.index))\n-                burned = features.rasterize(shapes=shapes, fill=0, out=out_arr, transform=src.transform)\n-                dst.write_band(1, burned, window=window)\n+            out_arr = np.zeros_like(src.read(1))\n+            # this is where we create a generator of geom, value pairs to use in rasterizing\n+            shapes = ((geom,value) for geom, value in zip(shapefile.geometry, shapefile.index))\n+            burned = features.rasterize(shapes=shapes, fill=0, out=out_arr, transform=src.transform)\n+            dst.write_band(1, burned)\n \n+def vulnerability(hazard, exposure):\n+    assert hazard.shape == exposure.shape\n+    hazard = hazard.astype(bool)\n+    damage = exposure * hazard\n+    return damage\n+\n+def estimate_affected(row, country, params):\n+    # Loop through tiles for each basin\n+    print(row.name)\n+    for tile in params['tiles']:\n+        # Get the raster of basin == the current basin\n+        fname = '.\/data_exposures\/{}_hybas_raster_{}.tif'.format(params['iso2'], tile)\n+        hybas = gtiff_to_array(fname)\n+        hybas_mask = (hybas == row.name)\n+        # Get the raster of population\n+        exp_fname = '.\/data_exposures\/gpw\/{}_population_count_{}.tif'.format(params['iso2'], tile)\n+        exp = gtiff_to_array(exp_fname)\n+        exp[exp<0] = 0\n+        # Nearest neighbor went from 30s to 3s\n+        exp = exp\/100\n+        # Get the total population by basin and store it\n+        assert hybas_mask.shape == exp.shape\n+        total_pop = (hybas_mask*exp)\n+        row['basin_pop'] += total_pop.sum()\n+        # Loop through return periods\n+        for rp in params['rps']:\n+            # Get the raster of boolean floods by return period\n+            fname = '{}{}-{}{}-{}-{}.tif'.format(params['folder'],c,params['type'],params['defended'], str(int(rp)), tile)\n+            floods = get_ssbn_array(fname)\n+            # Store the population affected for floods\n+            total_affected = vulnerability(floods, total_pop).sum()\n+            row[rp] += total_affected\n+    return row\n \n # Resample assets grids (e.g. gpw) to the tile sizes that ssbn gives\n country = ['NG','AR','PE','CO']\n","add":42,"remove":9,"filename":"\/lib\/ssbn.py","badparts":["    \"\"\"From: https:\/\/gist.github.com\/mhweber\/1a07b0881c2ab88d062e3b32060e5486\"\"\"","            'compress': 'lzw'","        windows = src.block_windows(1)","            for idx, window in windows:","                out_arr = np.zeros_like(src.read(1, window=window))","                shapes = ((geom,value) for geom, value in zip(shapefile.geometry, shapefile.index))","                burned = features.rasterize(shapes=shapes, fill=0, out=out_arr, transform=src.transform)","                dst.write_band(1, burned, window=window)"],"goodparts":["    if exists(outras):","        print('{} already exists'.format(outras))","        return","            'compress': 'DEFLATE'","            out_arr = np.zeros_like(src.read(1))","            shapes = ((geom,value) for geom, value in zip(shapefile.geometry, shapefile.index))","            burned = features.rasterize(shapes=shapes, fill=0, out=out_arr, transform=src.transform)","            dst.write_band(1, burned)","def vulnerability(hazard, exposure):","    assert hazard.shape == exposure.shape","    hazard = hazard.astype(bool)","    damage = exposure * hazard","    return damage","def estimate_affected(row, country, params):","    print(row.name)","    for tile in params['tiles']:","        fname = '.\/data_exposures\/{}_hybas_raster_{}.tif'.format(params['iso2'], tile)","        hybas = gtiff_to_array(fname)","        hybas_mask = (hybas == row.name)","        exp_fname = '.\/data_exposures\/gpw\/{}_population_count_{}.tif'.format(params['iso2'], tile)","        exp = gtiff_to_array(exp_fname)","        exp[exp<0] = 0","        exp = exp\/100","        assert hybas_mask.shape == exp.shape","        total_pop = (hybas_mask*exp)","        row['basin_pop'] += total_pop.sum()","        for rp in params['rps']:","            fname = '{}{}-{}{}-{}-{}.tif'.format(params['folder'],c,params['type'],params['defended'], str(int(rp)), tile)","            floods = get_ssbn_array(fname)","            total_affected = vulnerability(floods, total_pop).sum()","            row[rp] += total_affected","    return row"]},{"diff":"\n #     flist = sorted(glob.glob(folders(c)[0]+'*250*'))\n #     mosaic(flist)\n \n-\n-c = 'CO'\n+c = 'AR'\n folder = folders(c)[0]\n fname = 'data_exposures\/gpw\/{}_population_count_all.tif'.format(c)\n df = filter_polygons(fname)\n+df.plot()\n tiles = sorted((glob.glob(folder+'*')))\n params = get_param_info(tiles)\n floods, exposures = get_tiles(c,folder, params['rps'][0])\n a,gt,s = get_ssbn_array(floods[0], True)\n b = get_bounds(floods[0])\n \n-lons = np.array([round(gt[0]+gt[1]*i,5) for i in range(a.shape[1])])\n-lats = np.array([round(gt[3]+gt[5]*i,5) for i in range(a.shape[0])])\n-loni = np.array([i for i in range(a.shape[1])])\n-lati = np.array([i for i in range(a.shape[0])])\n-xx,yy = np.meshgrid(lons, lats)\n+\n+country = ['AR','PE','CO']\n+c = 'PE'\n+for c in country:\n+    fname = 'data_exposures\/gpw\/{}_population_count_all.tif'.format(c)\n+    folder = folders(c)[0]\n+    flist = sorted(glob.glob(folder+'*'))\n+    params = get_param_info(flist)\n+    # Get the df of basins\n+    df = filter_polygons(fname = 'data_exposures\/gpw\/{}_population_count_all.tif'.format(c))\n+    # basins don't change with return period\n+    floods, exposures = get_tiles(c,folder, params['rps'][0])\n+    for tile in floods:\n+        print(tile)\n+        d = get_basic_info(tile)\n+        outras = '.\/data_exposures\/{}_hybas_raster_{}.tif'.format(d['iso2'], d['tile'])\n+        Rasterize(df,tile, outras)\n+    # Initialize to 0\n+    df['basin_pop'] = 0\n+    for rp in params['rps']:\n+        df[rp] = 0\n+    # Apply over basins\n+    df = df.apply(estimate_affected, axis = 1, country = c, params = params)\n+\n+    df.to_file(\"{}_floods_rp_{}.geojson\".format(c,params['type']+params['defended']), driver='GeoJSON')\n \n \n-# Try burning the basins to grid.\n-out_fn = '.\/{}rasterized_basins.tif'.format(c)\n-# Rasterize\n \n-# After rasterizing\n-df.\n-a = Rasterize(df, floods[3], out_fn)\n-b = gtiff_to_array(out_fn)\n \n # SSBN Processing\n     # Convert to boolean","add":27,"remove":14,"filename":"\/lib\/ssbn.py","badparts":["c = 'CO'","lons = np.array([round(gt[0]+gt[1]*i,5) for i in range(a.shape[1])])","lats = np.array([round(gt[3]+gt[5]*i,5) for i in range(a.shape[0])])","loni = np.array([i for i in range(a.shape[1])])","lati = np.array([i for i in range(a.shape[0])])","xx,yy = np.meshgrid(lons, lats)","out_fn = '.\/{}rasterized_basins.tif'.format(c)","df.","a = Rasterize(df, floods[3], out_fn)","b = gtiff_to_array(out_fn)"],"goodparts":["c = 'AR'","df.plot()","country = ['AR','PE','CO']","c = 'PE'","for c in country:","    fname = 'data_exposures\/gpw\/{}_population_count_all.tif'.format(c)","    folder = folders(c)[0]","    flist = sorted(glob.glob(folder+'*'))","    params = get_param_info(flist)","    df = filter_polygons(fname = 'data_exposures\/gpw\/{}_population_count_all.tif'.format(c))","    floods, exposures = get_tiles(c,folder, params['rps'][0])","    for tile in floods:","        print(tile)","        d = get_basic_info(tile)","        outras = '.\/data_exposures\/{}_hybas_raster_{}.tif'.format(d['iso2'], d['tile'])","        Rasterize(df,tile, outras)","    df['basin_pop'] = 0","    for rp in params['rps']:","        df[rp] = 0","    df = df.apply(estimate_affected, axis = 1, country = c, params = params)","    df.to_file(\"{}_floods_rp_{}.geojson\".format(c,params['type']+params['defended']), driver='GeoJSON')"]}],"source":"\nimport gdal import geopandas as gpd import glob import numpy as np import os from os.path import exists, isfile import pandas as pd import rasterio from rasterio import features from rasterio.merge import merge from rasterio.plot import show import re import seaborn as sns from shapely.geometry import box,Point, Polygon import zipfile def unzip(path, pwd): \"\"\"Unzips a zipfile unless the output folder already exists \"\"\" fl=set(glob.glob(pwd+\"*.zip\")) if not exists(path): raise AssertionError(\"Incorrect Path\") output_path=path[:-4]+'\/' print(\"Extracting{} to{}\".format(path, output_path)) if exists(output_path): print(\"Output folder{} already exists\".format(output_path)) return zip_ref=zipfile.ZipFile(path, 'r') zip_ref.extractall(pwd) print(\"Extracted{}\".format(list((set(glob.glob(pwd+\"*\")) -fl))[0])) zip_ref.close() def folders(c): \"\"\"checks availability of fluvial and pluvial filepaths and returns the folder filepaths. Parameters ---------- c: 2 letter string for country Returns ------- tuple(fluvial, pluvial): filepaths for the fluvial and pluvial undefended folder in the ssbn folder if available, return false otherwise. If folder doesn't exist but zips exist, extract using unzip then return filepaths.\"\"\" folder=\".\/data_hazards\/ssbn\/\" fluvial=folder+c+\"_fluvial_undefended\/\" pluvial=folder+c+\"_pluvial_undefended\/\" if exists(fluvial) and exists(pluvial): return fluvial, pluvial elif(exists(pluvial) and(not exists(fluvial))): print(\"Fluvial Data Folder Missing \\n\") zip=fluvial[:-1]+\".zip\" elif(exists(fluvial) and(not exists(pluvial))): print(\"Pluvial Data Folder Missing \\n\") zip=pluvial[:-1]+\".zip\" else: print(\"Data is missing\") return False if isfile(zip): unzip(zip, folder) return fluvial, pluvial else: print(\"Zip does not exist\") return False def gtiff_to_array(fname, get_global=False): \"\"\"Open a gtiff and convert it to an array. Store coordinates in global variables if toggle is on\"\"\" tif=gdal.Open(fname) a=tif.ReadAsArray() gt=tif.GetGeoTransform() if get_global: print(gdal.Info(tif)) global lons, lats, loni, lati, xx, yy, xi, yi lons=np.array([round(gt[0]+gt[1]*i,5) for i in range(a.shape[1])]) lats=np.array([round(gt[3]+gt[5]*i,5) for i in range(a.shape[0])]) loni=np.array([i for i in range(a.shape[1])]) lati=np.array([i for i in range(a.shape[0])]) xx,yy=np.meshgrid(lons, lats) xi,yi=np.meshgrid(loni,lati) return a def get_basic_info(path): \"\"\"Retrieves information from the SSBN filename convention and stores it as a dictionary. \"\"\" neg_pos=path[::-1].find('\/') fname=path[-neg_pos:] m=re.match(\"^([A-Z]{2})-([FPUM])([DU]{0,1})-([0-9]{1,4})-([0-9])*\\.tif$\", fname) d={} d['path']=path d['filename']=m[0] d['iso2']=m[1] d['type']=m[2] d['return_period']=int(m[4]) d['tile']=int(m[5]) if m[3]==\"D\": d['defended']=True elif m[3]=='U': d['defended']=False else: print(\"Regex is not capturing defended-ness from filename correctly\") assert(False) return d def gpw_basic_info(asset_fname): f=asset_fname[-(asset_fname[::-1].find('\/')):] d={} d['dataset']=f[:f.find('_population')] d['resolution']=f[f.find('30_sec'):] d['type']=f[f.find('pop'):f.find('_rev11')] return d def get_param_info(flist): lis=[get_basic_info(f) for f in flist] d={} d['iso2']=lis[0]['iso2'] d['type']=lis[0]['type'] d['rps']=sorted(pd.Series([d['return_period']for d in lis]).unique()) d['tiles']=sorted(pd.Series([d['tile']for d in lis]).unique()) return d def convert_nulls(ssbn_array): \"\"\"SSBN has several null values corresponding to sea and null value tiles\"\"\" a=ssbn_array a[a==-9999]=np.nan a[a==999]=np.nan return a def get_ssbn_array(fname, return_geotransform=False): \"\"\"Open a gtiff and convert it to an array.\"\"\" tif=gdal.Open(fname) a=tif.ReadAsArray() a=convert_nulls(a) if return_geotransform: gt=tif.GetGeoTransform() return a, gt,(tif.RasterXSize, tif.RasterYSize) return a def get_asset_fname(): s=\"data_exposures\/gpw_v4_population_count_rev11_2015_30_sec.tif\" return s def get_asset_tif(): tif=gdal.Open(get_asset_fname()) return tif def get_asset_array(): tif=get_asset_tif() a=tif.ReadAsArray() gt=tif.GetGeoTransform() shape=(tif.RasterXSize, tif.RasterYSize) return a, gt, shape def resample_assets_to_ssbn_tiles(ssbn_fname, asset_fname=None, resampleAlg='near'): \"\"\"transforms an asset grid to the dimensions(x,y,step) of the ssbn tif. \"\"\" if not asset_fname: asset_fname=get_asset_fname() else: pass ssbn_info=get_basic_info(ssbn_fname) asset_info=gpw_basic_info(asset_fname) outdir='.\/data_exposures\/gpw\/' outfile='{}_{}_{}.tif'.format(ssbn_info['iso2'],asset_info['type'],ssbn_info['tile']) if exists(outdir+outfile): print('skipping{}'.format(outfile)) else: a,gt,ts=get_ssbn_array(ssbn_fname, True) bounds=get_bounds(ssbn_fname) print('creating{}'.format(outfile)) gdal.Translate(outdir +outfile, get_asset_tif(), xRes=gt[1], yRes=gt[5], projWin=bounds, resampleAlg=resampleAlg, creationOptions=[\"COMPRESS=DEFLATE\"]) def mosaic(flist): \"\"\"Mosaics or merges by tiling several files\"\"\" print(\"\\nCombining the following files...\") print(flist) out_fp=flist[0][:-5]+'all.tif' print('Output file is{}'.format(out_fp)) files_to_mosaic=[] for f in flist: src=rasterio.open(f) files_to_mosaic.append(src) out_tif, out_trans=merge(files_to_mosaic) out_meta=src.meta.copy() out_meta.update({'width': out_tif.shape[2], 'height': out_tif.shape[1], 'transform': out_trans, 'compress': \"DEFLATE\" }) with rasterio.open(out_fp, \"w\", **out_meta) as dest: dest.write(out_tif) return True def get_bounds(fname, shp=False): tif=gdal.Open(fname) gt,ts=tif.GetGeoTransform(),(tif.RasterXSize, tif.RasterYSize) if shp: return[Point(gt[0],gt[3]),Point(gt[0]+gt[1]*ts[0], gt[3]+gt[5]*ts[1])] else: return[gt[0],gt[3],gt[0]+gt[1]*ts[0], gt[3]+gt[5]*ts[1]] def filter_polygons(fname, hb='.\/data_exposures\/hydrobasins\/hybas_sa_lev04_v1c.shp'): \"\"\"Filters a shapefile based on whether the polygons intersect with a bounding box based on a geotiff fname.\"\"\" df=gpd.read_file(hb) b=gpd.GeoSeries(box(*get_bounds(fname,False)), crs={'init': 'epsg:4326'}) b=gpd.GeoDataFrame(b,columns=['geometry'], crs=b.crs) return gpd.overlay( b, df,how='intersection') def get_tiles(country, folder, rp): floods=sorted(glob.glob(folder+country+'-*-' +str(rp)+'-*.tif')) exposures=sorted(glob.glob('data_exposures\/gpw\/{}*'.format(c)))[:-1] return floods, exposures def Rasterize(shapefile, inras, outras): \"\"\"From: https:\/\/gist.github.com\/mhweber\/1a07b0881c2ab88d062e3b32060e5486\"\"\" with rasterio.open(inras) as src: kwargs=src.meta.copy() kwargs.update({ 'driver': 'GTiff', 'compress': 'lzw' }) windows=src.block_windows(1) with rasterio.open(outras, 'w', **kwargs) as dst: for idx, window in windows: out_arr=np.zeros_like(src.read(1, window=window)) shapes=((geom,value) for geom, value in zip(shapefile.geometry, shapefile.index)) burned=features.rasterize(shapes=shapes, fill=0, out=out_arr, transform=src.transform) dst.write_band(1, burned, window=window) country=['NG','AR','PE','CO'] [resample_assets_to_ssbn_tiles(f) for c in country for f in sorted(glob.glob(folders(c)[0]+'*'))] basins='.\/data_exposures\/hydrobasins\/' [unzip(f,basins) for f in glob.glob(basins+'*.zip')] for c in country: flist=sorted(glob.glob(\"data_exposures\/gpw\/{}_*\".format(c))) mosaic(flist) c='CO' folder=folders(c)[0] fname='data_exposures\/gpw\/{}_population_count_all.tif'.format(c) df=filter_polygons(fname) tiles=sorted((glob.glob(folder+'*'))) params=get_param_info(tiles) floods, exposures=get_tiles(c,folder, params['rps'][0]) a,gt,s=get_ssbn_array(floods[0], True) b=get_bounds(floods[0]) lons=np.array([round(gt[0]+gt[1]*i,5) for i in range(a.shape[1])]) lats=np.array([round(gt[3]+gt[5]*i,5) for i in range(a.shape[0])]) loni=np.array([i for i in range(a.shape[1])]) lati=np.array([i for i in range(a.shape[0])]) xx,yy=np.meshgrid(lons, lats) out_fn='.\/{}rasterized_basins.tif'.format(c) df. a=Rasterize(df, floods[3], out_fn) b=gtiff_to_array(out_fn) def get_correlation_grid(): \"\"\"A simplification of the flood model is that major basins in a country are perfectly correlated. We can get basin data from the \"\"\" ","sourceWithComments":"import gdal\nimport geopandas as gpd\nimport glob\nimport numpy as np\nimport os\nfrom os.path import exists, isfile\nimport pandas as pd\nimport rasterio\nfrom rasterio import features\nfrom rasterio.merge import merge\nfrom rasterio.plot import show\nimport re\nimport seaborn as sns\nfrom shapely.geometry import box,Point, Polygon\nimport zipfile\n\ndef unzip(path, pwd):\n    \"\"\"Unzips a zipfile unless the output folder already exists\n    \"\"\"\n    # Get a set of current files\n    fl = set(glob.glob(pwd+\"*.zip\"))\n    # If the input pth doesn't exist, try to find the file\n    if not exists(path):\n        raise AssertionError(\"Incorrect Path\")\n    output_path = path[:-4]+'\/'\n    print(\"Extracting {} to {}\".format(path, output_path))\n    if exists(output_path):\n        print(\"Output folder {} already exists\".format(output_path))\n        return\n    zip_ref = zipfile.ZipFile(path, 'r')\n    zip_ref.extractall(pwd)\n    print(\"Extracted {}\".format(list((set(glob.glob(pwd+\"*\")) - fl))[0]))\n    zip_ref.close()\ndef folders(c):\n    \"\"\"checks availability of fluvial and pluvial filepaths and returns the\n    folder filepaths.\n\n    Parameters\n    ----------\n    c : 2 letter string for country\n\n    Returns\n    -------\n    tuple(fluvial, pluvial):\n        filepaths for the fluvial and pluvial undefended folder in\n        the ssbn folder if available, return false otherwise. If folder doesn't exist\n        but zips exist, extract using unzip then return filepaths.\"\"\"\n\n    folder = \".\/data_hazards\/ssbn\/\"\n    fluvial = folder+c+\"_fluvial_undefended\/\"\n    pluvial = folder+c+\"_pluvial_undefended\/\"\n    if exists(fluvial) and exists(pluvial):\n        return fluvial, pluvial\n    elif (exists(pluvial) and (not exists(fluvial))):\n        print(\"Fluvial Data Folder Missing \\n\")\n        zip = fluvial[:-1]+\".zip\"\n    elif (exists(fluvial) and (not exists(pluvial))):\n        print(\"Pluvial Data Folder Missing \\n\")\n        zip = pluvial[:-1]+\".zip\"\n    else:\n        print(\"Data is missing\")\n        return False\n    if isfile(zip):\n        unzip(zip, folder)\n        return fluvial, pluvial\n    else:\n        print(\"Zip does not exist\")\n        return False\ndef gtiff_to_array(fname, get_global = False):\n    \"\"\"Open a gtiff and convert it to an array.  Store coordinates in global variables if toggle is on\"\"\"\n    tif = gdal.Open(fname)\n    a = tif.ReadAsArray()\n    gt = tif.GetGeoTransform()\n    if get_global:\n        print(gdal.Info(tif))\n        global lons, lats, loni, lati, xx, yy, xi, yi\n        lons = np.array([round(gt[0]+gt[1]*i,5) for i in range(a.shape[1])])\n        lats = np.array([round(gt[3]+gt[5]*i,5) for i in range(a.shape[0])])\n        loni = np.array([i for i in range(a.shape[1])])\n        lati = np.array([i for i in range(a.shape[0])])\n        xx,yy = np.meshgrid(lons, lats)\n        xi,yi = np.meshgrid(loni,lati)\n    return a\ndef get_basic_info(path):\n    \"\"\"Retrieves information from the SSBN filename convention and\n    stores it as a dictionary.\n    \"\"\"\n    # print(path)\n    neg_pos = path[::-1].find('\/')\n    fname = path[-neg_pos:]\n    # print(fname)\n    m = re.match(\"^([A-Z]{2})-([FPUM])([DU]{0,1})-([0-9]{1,4})-([0-9])*\\.tif$\", fname)\n    d = {}\n    d['path'] = path\n    d['filename'] = m[0]\n    d['iso2'] = m[1]\n    d['type'] = m[2]\n    d['return_period'] = int(m[4])\n    d['tile'] = int(m[5])\n    if m[3] == \"D\":\n        d['defended'] = True\n    elif m[3] == 'U':\n        d['defended'] = False\n    else:\n        print(\"Regex is not capturing defended-ness from filename correctly\")\n        assert(False)\n    return d\ndef gpw_basic_info(asset_fname):\n    f = asset_fname[-(asset_fname[::-1].find('\/')):]\n    d = {}\n    d['dataset'] = f[:f.find('_population')]\n    d['resolution'] = f[f.find('30_sec'):]\n    d['type'] = f[f.find('pop'):f.find('_rev11')]\n    return d\ndef get_param_info(flist):\n    lis = [get_basic_info(f) for f in flist]\n    d = {}\n    d['iso2'] = lis[0]['iso2']\n    d['type'] = lis[0]['type']\n    d['rps'] = sorted(pd.Series([d['return_period']for d in lis]).unique())\n    d['tiles'] = sorted(pd.Series([d['tile']for d in lis]).unique())\n    return d\ndef convert_nulls(ssbn_array):\n    \"\"\"SSBN has several null values corresponding to sea and null value tiles\"\"\"\n    a = ssbn_array\n    # Null values, no reading\n    a[a == -9999] = np.nan\n    # Null values, sea\/ocean tiles with no possible flooding.\n    a[a == 999] = np.nan\n    return a\ndef get_ssbn_array(fname, return_geotransform = False):\n    \"\"\"Open a gtiff and convert it to an array.\"\"\"\n    tif = gdal.Open(fname)\n    a = tif.ReadAsArray()\n    a = convert_nulls(a)\n    # print(gdal.Info(tif))\n    if return_geotransform:\n        gt = tif.GetGeoTransform()\n        # print(gt)\n        # return a tuple of array, geotransform, xysize\n        return a, gt, (tif.RasterXSize, tif.RasterYSize)\n    return a\n# \"\"\"Deprecated: currently only uses population count tif from GPW - replace to\n# get different forms of exposuure grids. \"\"\"\ndef get_asset_fname():\n    s = \"data_exposures\/gpw_v4_population_count_rev11_2015_30_sec.tif\"\n    return s\ndef get_asset_tif():\n    tif = gdal.Open(get_asset_fname())\n    return tif\ndef get_asset_array():\n    tif = get_asset_tif()\n    a = tif.ReadAsArray()\n    gt = tif.GetGeoTransform()\n    shape = (tif.RasterXSize, tif.RasterYSize)\n    return a, gt, shape\ndef resample_assets_to_ssbn_tiles(ssbn_fname, asset_fname = None, resampleAlg = 'near'):\n    \"\"\"transforms an asset grid to the dimensions (x,y,step) of the ssbn tif.\n    \"\"\"\n    if not asset_fname:\n        asset_fname = get_asset_fname()\n    else:\n        pass\n    ssbn_info = get_basic_info(ssbn_fname)\n    asset_info = gpw_basic_info(asset_fname)\n    # Need to check that file doesn't already exist.\n    outdir = '.\/data_exposures\/gpw\/'\n    outfile = '{}_{}_{}.tif'.format(ssbn_info['iso2'],asset_info['type'],ssbn_info['tile'])\n    if exists(outdir+outfile):\n        print('skipping {}'.format(outfile))\n    # Use nearest neighbor method to create a file\n    else:\n        # a,gt,ts = array, geotransform, xy_tiles\n        a,gt,ts= get_ssbn_array(ssbn_fname, True)\n        # secs = str(int(gt[1]*60*60))\n        bounds = get_bounds(ssbn_fname)\n        print('creating {}'.format(outfile))\n        gdal.Translate(outdir + outfile, get_asset_tif(), xRes = gt[1], yRes = gt[5], projWin = bounds, resampleAlg = resampleAlg, creationOptions = [\"COMPRESS=DEFLATE\"])\ndef mosaic(flist):\n    \"\"\"Mosaics or merges by tiling several files\"\"\"\n    print(\"\\nCombining the following files...\")\n    print(flist)\n    out_fp = flist[0][:-5]+'all.tif'\n    print('Output file is {}'.format(out_fp))\n\n    # Iterates and opens files using rasterio and appends to list\n    files_to_mosaic = []\n    for f in flist:\n        src = rasterio.open(f)\n        files_to_mosaic.append(src)\n    # rasterio.merge.merge over list\n    out_tif, out_trans = merge(files_to_mosaic)\n    # Configure output metadata\n    out_meta = src.meta.copy()\n    # print(src.profile)\n    out_meta.update({'width': out_tif.shape[2],\n                     'height': out_tif.shape[1],\n                     'transform': out_trans,\n                    # Docs say that this should be compression, but 'compress'\n                    # is actually the toggle that works.\n                    # Makes a 10x difference to filesize!\n                     'compress': \"DEFLATE\"\n                     })\n    with rasterio.open(out_fp, \"w\", **out_meta) as dest:\n        dest.write(out_tif)\n    return True\ndef get_bounds(fname, shp = False):\n    tif = gdal.Open(fname)\n    gt,ts = tif.GetGeoTransform(),(tif.RasterXSize, tif.RasterYSize)\n    if shp:\n        return [Point(gt[0],gt[3]),Point(gt[0]+gt[1]*ts[0], gt[3]+gt[5]*ts[1])]\n    else:\n        return [gt[0],gt[3],gt[0]+gt[1]*ts[0], gt[3]+gt[5]*ts[1]]\ndef filter_polygons(fname, hb = '.\/data_exposures\/hydrobasins\/hybas_sa_lev04_v1c.shp'):\n    \"\"\"Filters a shapefile based on whether the polygons intersect with\n    a bounding box based on a geotiff fname.\"\"\"\n    # get south american basins\n    # pfa = sorted(glob.glob(basins+'*sa*.shp'))\n    # Use pfascetter level 4\n    # gpd.read_file(pfa[3])['geometry'].plot()\n    df = gpd.read_file(hb)\n    b = gpd.GeoSeries(box(*get_bounds(fname,False)), crs = {'init': 'epsg:4326'})\n    # Need to forward the crs manually wtf geopandas\n    b = gpd.GeoDataFrame(b,columns = ['geometry'], crs = b.crs)\n    return gpd.overlay( b, df,how = 'intersection')\ndef get_tiles(country, folder, rp):\n    floods = sorted(glob.glob(folder+country+'-*-' + str(rp)+'-*.tif'))\n    exposures = sorted(glob.glob('data_exposures\/gpw\/{}*'.format(c)))[:-1]\n    return floods, exposures\ndef Rasterize(shapefile, inras, outras):\n    \"\"\"From: https:\/\/gist.github.com\/mhweber\/1a07b0881c2ab88d062e3b32060e5486\"\"\"\n    with rasterio.open(inras) as src:\n        kwargs = src.meta.copy()\n        kwargs.update({\n            'driver': 'GTiff',\n            'compress': 'lzw'\n        })\n        windows = src.block_windows(1)\n        with rasterio.open(outras, 'w', **kwargs) as dst:\n            for idx, window in windows:\n                out_arr = np.zeros_like(src.read(1, window=window))\n                # this is where we create a generator of geom, value pairs to use in rasterizing\n                shapes = ((geom,value) for geom, value in zip(shapefile.geometry, shapefile.index))\n                burned = features.rasterize(shapes=shapes, fill=0, out=out_arr, transform=src.transform)\n                dst.write_band(1, burned, window=window)\n\n\n# Resample assets grids (e.g. gpw) to the tile sizes that ssbn gives\ncountry = ['NG','AR','PE','CO']\n[resample_assets_to_ssbn_tiles(f) for c in country for f in sorted(glob.glob(folders(c)[0]+'*'))]\n# Unzip all basin data\nbasins = '.\/data_exposures\/hydrobasins\/'\n[unzip(f,basins) for f in glob.glob(basins+'*.zip')]\n# Mosaic all the GPW data to the country level\nfor c in country:\n    flist = sorted(glob.glob(\"data_exposures\/gpw\/{}_*\".format(c)))\n    mosaic(flist)\n# # Test mosaic on the 250 return periods for fluvial only - it works, but you don't need this\n# for c in country:\n#     flist = sorted(glob.glob(folders(c)[0]+'*250*'))\n#     mosaic(flist)\n\n\nc = 'CO'\nfolder = folders(c)[0]\nfname = 'data_exposures\/gpw\/{}_population_count_all.tif'.format(c)\ndf = filter_polygons(fname)\ntiles = sorted((glob.glob(folder+'*')))\nparams = get_param_info(tiles)\nfloods, exposures = get_tiles(c,folder, params['rps'][0])\na,gt,s = get_ssbn_array(floods[0], True)\nb = get_bounds(floods[0])\n\nlons = np.array([round(gt[0]+gt[1]*i,5) for i in range(a.shape[1])])\nlats = np.array([round(gt[3]+gt[5]*i,5) for i in range(a.shape[0])])\nloni = np.array([i for i in range(a.shape[1])])\nlati = np.array([i for i in range(a.shape[0])])\nxx,yy = np.meshgrid(lons, lats)\n\n\n# Try burning the basins to grid.\nout_fn = '.\/{}rasterized_basins.tif'.format(c)\n# Rasterize\n\n# After rasterizing\ndf.\na = Rasterize(df, floods[3], out_fn)\nb = gtiff_to_array(out_fn)\n\n# SSBN Processing\n    # Convert to boolean\n# GPW Processing\n    # Nearest neighbor to ssbn grid\n# Basin processing`\n    # Rasterizing to SSBN grid\n    # 1\/0\n\n\n\n\n\n\n\n# Loop_through_tifs:\n    # Loop through the rows of the basin GeoDataFrame,\n        # create a separate array for whether each point in a tiff is in the polygon or not\n        # Multiply inPolygon * population * bool_depth = pop_affected\n        # Multiply inPolygon * population = population in basin\n        # Add both counts to the dataframe.\n\n\n# Write function that tells you whether cells in a tiff are inside a polygon or not.\n\ndef get_correlation_grid():\n    \"\"\"A simplification of the flood model is that major basins in a country are\n    perfectly correlated.  We can get basin data from the\n    \"\"\"\n\n# Use gricells_to_adm0 function to simulate floods that are perfectly correlated across hydrobasins\n\n#\n"}},"msg":"ssbn and exposure handling, currently a bad vulnerability function (affected by floods)"}},"https:\/\/github.com\/SFDO-Tooling\/Metecho":{"d0b4aa17ee39b6dddf8df0d876d8fe22b1e00587":{"url":"https:\/\/api.github.com\/repos\/SFDO-Tooling\/Metecho\/commits\/d0b4aa17ee39b6dddf8df0d876d8fe22b1e00587","html_url":"https:\/\/github.com\/SFDO-Tooling\/Metecho\/commit\/d0b4aa17ee39b6dddf8df0d876d8fe22b1e00587","message":"\ud83d\udee2 Track commits on Projects and Branches\n\nAnd update them when we get a GitHub webhook event.\n\nI think that there's a DOS vulnerability here in that we're not\ncurrently requiring that the hook validate that it's from GitHub itself,\nand anyone can just require us to hammer the GitHub API and flood our\nqueues.","sha":"d0b4aa17ee39b6dddf8df0d876d8fe22b1e00587","keyword":"flooding vulnerability","diff":"diff --git a\/metashare\/api\/jobs.py b\/metashare\/api\/jobs.py\nindex 1f24b3c79..71937040a 100644\n--- a\/metashare\/api\/jobs.py\n+++ b\/metashare\/api\/jobs.py\n@@ -2,6 +2,7 @@\n import traceback\n \n from asgiref.sync import async_to_sync\n+from django.db import transaction\n from django.utils.text import slugify\n from django.utils.timezone import now\n from django_rq import job\n@@ -254,3 +255,46 @@ def refresh_github_repositories_for_user(user):\n \n \n refresh_github_repositories_for_user_job = job(refresh_github_repositories_for_user)\n+\n+\n+def _commit_to_json(commit):\n+    return {\n+        \"sha\": commit.sha,\n+        \"author\": {\n+            \"avatar_url\": commit.author.avatar_url if commit.author else \"\",\n+            \"login\": commit.author.login if commit.author else \"\",\n+        },\n+        \"committer\": {\n+            \"avatar_url\": commit.committer.avatar_url if commit.committer else \"\",\n+            \"login\": commit.committer.login if commit.committer else \"\",\n+        },\n+        \"message\": commit.message,\n+    }\n+\n+\n+# This avoids partially-applied saving:\n+@transaction.atomic\n+def refresh_commits(*, user, repository):\n+    repo = get_repo_info(user, repository.repo_id)\n+\n+    projects = repository.projects.filter(branch_name__isnull=False)\n+    for project in projects:\n+        branch = repo.branch(project.branch_name)\n+        project.commits = [\n+            _commit_to_json(commit) for commit in repo.commits(sha=branch.latest_sha())\n+        ]\n+        project.save()\n+        project.finalize_branch_update()\n+\n+        tasks = project.tasks.filter(branch_name__isnull=False)\n+        for task in tasks:\n+            branch = repo.branch(task.branch_name)\n+            task.commits = [\n+                _commit_to_json(commit)\n+                for commit in repo.commits(sha=branch.latest_sha())\n+            ]\n+            task.save()\n+            task.finalize_branch_update()\n+\n+\n+refresh_commits_job = job(refresh_commits)\ndiff --git a\/metashare\/api\/migrations\/0037_auto_20191118_1931.py b\/metashare\/api\/migrations\/0037_auto_20191118_1931.py\nnew file mode 100644\nindex 000000000..fedd92867\n--- \/dev\/null\n+++ b\/metashare\/api\/migrations\/0037_auto_20191118_1931.py\n@@ -0,0 +1,24 @@\n+# Generated by Django 2.2.7 on 2019-11-18 19:31\n+\n+import django.contrib.postgres.fields.jsonb\n+from django.db import migrations\n+\n+\n+class Migration(migrations.Migration):\n+\n+    dependencies = [\n+        (\"api\", \"0036_merge_20191113_1851\"),\n+    ]\n+\n+    operations = [\n+        migrations.AddField(\n+            model_name=\"project\",\n+            name=\"commits\",\n+            field=django.contrib.postgres.fields.jsonb.JSONField(default=list),\n+        ),\n+        migrations.AddField(\n+            model_name=\"task\",\n+            name=\"commits\",\n+            field=django.contrib.postgres.fields.jsonb.JSONField(default=list),\n+        ),\n+    ]\ndiff --git a\/metashare\/api\/models.py b\/metashare\/api\/models.py\nindex b7ac3f40d..953927b38 100644\n--- a\/metashare\/api\/models.py\n+++ b\/metashare\/api\/models.py\n@@ -184,6 +184,21 @@ class Meta:\n         ordering = (\"name\",)\n         unique_together = ((\"repo_owner\", \"repo_name\"),)\n \n+    def get_a_matching_user(self):\n+        github_repository = GitHubRepository.objects.filter(\n+            repo_id=self.repo_id\n+        ).first()\n+\n+        if github_repository:\n+            return github_repository.user\n+\n+        return None\n+\n+    def refresh_commits(self, user):\n+        from .jobs import refresh_commits_job\n+\n+        refresh_commits_job.delay(user=user, repository=self)\n+\n \n class GitHubRepository(mixins.HashIdMixin, models.Model):\n     user = models.ForeignKey(\n@@ -210,8 +225,9 @@ class Project(mixins.HashIdMixin, mixins.TimestampsMixin, SlugMixin, models.Mode\n     name = StringField()\n     description = MarkdownField(blank=True, property_suffix=\"_markdown\")\n     branch_name = models.CharField(\n-        max_length=100, blank=True, null=True, validators=[validate_unicode_branch],\n+        max_length=100, blank=True, null=True, validators=[validate_unicode_branch]\n     )\n+    commits = JSONField(default=list)\n \n     repository = models.ForeignKey(\n         Repository, on_delete=models.PROTECT, related_name=\"projects\"\n@@ -254,8 +270,9 @@ class Task(mixins.HashIdMixin, mixins.TimestampsMixin, SlugMixin, models.Model):\n         related_name=\"assigned_tasks\",\n     )\n     branch_name = models.CharField(\n-        max_length=100, blank=True, null=True, validators=[validate_unicode_branch],\n+        max_length=100, blank=True, null=True, validators=[validate_unicode_branch]\n     )\n+    commits = JSONField(default=list)\n \n     slug_class = TaskSlug\n \ndiff --git a\/metashare\/api\/serializers.py b\/metashare\/api\/serializers.py\nindex 270192382..c7cc725b5 100644\n--- a\/metashare\/api\/serializers.py\n+++ b\/metashare\/api\/serializers.py\n@@ -101,6 +101,7 @@ class Meta:\n             \"old_slugs\",\n             \"repository\",\n             \"branch_url\",\n+            \"commits\",\n         )\n         validators = (\n             CaseInsensitiveUniqueTogetherValidator(\n@@ -144,6 +145,7 @@ class Meta:\n             \"slug\",\n             \"old_slugs\",\n             \"branch_url\",\n+            \"commits\",\n         )\n         validators = (\n             CaseInsensitiveUniqueTogetherValidator(\ndiff --git a\/metashare\/api\/tests\/jobs.py b\/metashare\/api\/tests\/jobs.py\nindex e2a290314..e8b8f9242 100644\n--- a\/metashare\/api\/tests\/jobs.py\n+++ b\/metashare\/api\/tests\/jobs.py\n@@ -12,6 +12,7 @@\n     create_branches_on_github_then_create_scratch_org,\n     delete_scratch_org,\n     get_unsaved_changes,\n+    refresh_commits,\n     refresh_github_repositories_for_user,\n )\n from ..models import SCRATCH_ORG_TYPES\n@@ -257,3 +258,39 @@ def test_commit_changes_from_org(self, scratch_org_factory, user_factory):\n                 commit_changes_from_org(scratch_org, user, {}, \"message\")\n \n             assert async_to_sync.called\n+\n+\n+@pytest.mark.django_db\n+def test_refresh_commits(\n+    user_factory, repository_factory, project_factory, task_factory\n+):\n+    user = user_factory()\n+    repository = repository_factory()\n+    project = project_factory(repository=repository, branch_name=\"project\")\n+    task_factory(project=project, branch_name=\"task\")\n+    with ExitStack() as stack:\n+        commit1 = MagicMock(\n+            **{\n+                \"sha\": \"abcd1234\",\n+                \"author.avatar_url\": \"https:\/\/example.com\/img.png\",\n+                \"author.login\": \"test_user\",\n+                \"committer.avatar_url\": \"https:\/\/example.com\/img.png\",\n+                \"committer.login\": \"test_user\",\n+                \"message\": \"Test message 1\",\n+            }\n+        )\n+        commit2 = MagicMock(\n+            **{\n+                \"sha\": \"1234abcd\",\n+                \"author.avatar_url\": None,\n+                \"author.login\": None,\n+                \"committer.avatar_url\": None,\n+                \"committer.login\": None,\n+                \"message\": \"Test message 2\",\n+            }\n+        )\n+        repo = MagicMock(**{\"commits.return_value\": [commit1, commit2]})\n+        get_repo_info = stack.enter_context(patch(\"metashare.api.jobs.get_repo_info\"))\n+        get_repo_info.return_value = repo\n+\n+        refresh_commits(user=user, repository=repository)\ndiff --git a\/metashare\/api\/tests\/models.py b\/metashare\/api\/tests\/models.py\nindex 4ba8e5336..785c8f3d0 100644\n--- a\/metashare\/api\/tests\/models.py\n+++ b\/metashare\/api\/tests\/models.py\n@@ -31,6 +31,21 @@ def test_get_repo_id(self, repository_factory):\n             assert get_repo_info.called\n             assert gh_repo.repo_id == 123\n \n+    def test_get_a_matching_user__none(self, repository_factory):\n+        repo = repository_factory()\n+        assert repo.get_a_matching_user() is None\n+\n+    def test_get_a_matching_user(self, repository_factory, git_hub_repository_factory):\n+        repo = repository_factory(repo_id=123)\n+        gh_repo = git_hub_repository_factory(repo_id=123)\n+        assert repo.get_a_matching_user() == gh_repo.user\n+\n+    def test_refresh_commits(self, repository_factory, user_factory):\n+        repo = repository_factory()\n+        with patch(\"metashare.api.jobs.refresh_commits_job\") as refresh_commits_job:\n+            repo.refresh_commits(None)\n+            assert refresh_commits_job.delay.called\n+\n \n @pytest.mark.django_db\n class TestProject:\ndiff --git a\/metashare\/api\/tests\/views.py b\/metashare\/api\/tests\/views.py\nindex c10a50312..1ed34f2a3 100644\n--- a\/metashare\/api\/tests\/views.py\n+++ b\/metashare\/api\/tests\/views.py\n@@ -3,6 +3,7 @@\n \n import pytest\n from django.urls import reverse\n+from github3.exceptions import ResponseError\n \n from ..models import SCRATCH_ORG_TYPES\n \n@@ -41,32 +42,85 @@ def test_user_refresh_view(client):\n \n \n @pytest.mark.django_db\n-def test_repository_view(client, repository_factory, git_hub_repository_factory):\n-    git_hub_repository_factory(\n-        user=client.user, repo_id=123, repo_url=\"https:\/\/example.com\/test-repo.git\"\n-    )\n-    repo = repository_factory(repo_name=\"repo\", repo_id=123)\n-    repository_factory(repo_name=\"repo2\", repo_id=456)\n-    repository_factory(repo_name=\"repo3\", repo_id=None)\n-    response = client.get(reverse(\"repository-list\"))\n+class TestRepositoryView:\n+    def test_get_queryset(self, client, repository_factory, git_hub_repository_factory):\n+        git_hub_repository_factory(\n+            user=client.user, repo_id=123, repo_url=\"https:\/\/example.com\/test-repo.git\"\n+        )\n+        repo = repository_factory(repo_name=\"repo\", repo_id=123)\n+        repository_factory(repo_name=\"repo2\", repo_id=456)\n+        repository_factory(repo_name=\"repo3\", repo_id=None)\n+        with patch(\"metashare.api.model_mixins.get_repo_info\") as get_repo_info:\n+            get_repo_info.return_value = MagicMock(id=789)\n+            response = client.get(reverse(\"repository-list\"))\n+\n+        assert response.status_code == 200\n+        assert response.json() == {\n+            \"count\": 1,\n+            \"previous\": None,\n+            \"next\": None,\n+            \"results\": [\n+                {\n+                    \"id\": str(repo.id),\n+                    \"name\": str(repo.name),\n+                    \"description\": \"\",\n+                    \"is_managed\": False,\n+                    \"slug\": str(repo.slug),\n+                    \"old_slugs\": [],\n+                    \"repo_url\": (\n+                        f\"https:\/\/github.com\/{repo.repo_owner}\/{repo.repo_name}\"\n+                    ),\n+                }\n+            ],\n+        }\n+\n+    def test_get_queryset__bad(\n+        self, client, repository_factory, git_hub_repository_factory\n+    ):\n+        git_hub_repository_factory(\n+            user=client.user, repo_id=123, repo_url=\"https:\/\/example.com\/test-repo.git\"\n+        )\n+        repo = repository_factory(repo_name=\"repo\", repo_id=123)\n+        repository_factory(repo_name=\"repo2\", repo_id=456)\n+        repository_factory(repo_name=\"repo3\", repo_id=None)\n+        with patch(\"metashare.api.model_mixins.get_repo_info\") as get_repo_info:\n+            get_repo_info.side_effect = ResponseError(MagicMock())\n+            response = client.get(reverse(\"repository-list\"))\n+\n+        assert response.status_code == 200\n+        assert response.json() == {\n+            \"count\": 1,\n+            \"previous\": None,\n+            \"next\": None,\n+            \"results\": [\n+                {\n+                    \"id\": str(repo.id),\n+                    \"name\": str(repo.name),\n+                    \"description\": \"\",\n+                    \"is_managed\": False,\n+                    \"slug\": str(repo.slug),\n+                    \"old_slugs\": [],\n+                    \"repo_url\": (\n+                        f\"https:\/\/github.com\/{repo.repo_owner}\/{repo.repo_name}\"\n+                    ),\n+                }\n+            ],\n+        }\n+\n+    def test_hook__good(self, client, repository_factory, git_hub_repository_factory):\n+        repo = repository_factory(repo_id=123)\n+        git_hub_repository_factory(repo_id=123)\n+        with patch(\"metashare.api.jobs.refresh_commits_job\") as refresh_commits_job:\n+            response = client.post(\n+                reverse(\"repository-hook\", kwargs={\"pk\": str(repo.id)})\n+            )\n+            assert refresh_commits_job.delay.called\n+            assert response.status_code == 202\n \n-    assert response.status_code == 200\n-    assert response.json() == {\n-        \"count\": 1,\n-        \"previous\": None,\n-        \"next\": None,\n-        \"results\": [\n-            {\n-                \"id\": str(repo.id),\n-                \"name\": str(repo.name),\n-                \"description\": \"\",\n-                \"is_managed\": False,\n-                \"slug\": str(repo.slug),\n-                \"old_slugs\": [],\n-                \"repo_url\": f\"https:\/\/github.com\/{repo.repo_owner}\/{repo.repo_name}\",\n-            }\n-        ],\n-    }\n+    def test_hook__bad(self, client, repository_factory):\n+        repo = repository_factory()\n+        response = client.post(reverse(\"repository-hook\", kwargs={\"pk\": str(repo.id)}))\n+        assert response.status_code == 500\n \n \n @pytest.mark.django_db\ndiff --git a\/metashare\/api\/views.py b\/metashare\/api\/views.py\nindex 48ff00fbf..2ca000b28 100644\n--- a\/metashare\/api\/views.py\n+++ b\/metashare\/api\/views.py\n@@ -10,8 +10,6 @@\n from rest_framework.views import APIView\n \n from .filters import ProjectFilter, RepositoryFilter, ScratchOrgFilter, TaskFilter\n-\n-# from .gh import get_repo_info\n from .models import SCRATCH_ORG_TYPES, Project, Repository, ScratchOrg, Task\n from .paginators import CustomPaginator\n from .serializers import (\n@@ -96,11 +94,15 @@ def get_queryset(self):\n \n     @action(methods=[\"POST\"], detail=True, permission_classes=[AllowAny])\n     def hook(self, request, pk=None):\n-        # repository = self.get_object()\n-        # repo = get_repo_info(self.request.user, repository.repo_id)\n-        return Response(\"\", status=200)\n-        # branch = \"\"\n-        # list(repo.commits(sha=branch.latest_sha()))\n+        repository = self.model.objects.get(pk=pk)\n+        user = repository.get_a_matching_user()\n+        if not user:\n+            return Response(\n+                {\"error\": f\"No matching user for repository {pk}.\"},\n+                status=status.HTTP_500_INTERNAL_SERVER_ERROR,\n+            )\n+        repository.refresh_commits(user)\n+        return Response(status=status.HTTP_202_ACCEPTED)\n \n \n class ProjectViewSet(viewsets.ModelViewSet):\n","files":{"\/metashare\/api\/models.py":{"changes":[{"diff":"\n     name = StringField()\n     description = MarkdownField(blank=True, property_suffix=\"_markdown\")\n     branch_name = models.CharField(\n-        max_length=100, blank=True, null=True, validators=[validate_unicode_branch],\n+        max_length=100, blank=True, null=True, validators=[validate_unicode_branch]\n     )\n+    commits = JSONField(default=list)\n \n     repository = models.ForeignKey(\n         Repository, on_delete=models.PROTECT, related_name=\"projects\"\n","add":2,"remove":1,"filename":"\/metashare\/api\/models.py","badparts":["        max_length=100, blank=True, null=True, validators=[validate_unicode_branch],"],"goodparts":["        max_length=100, blank=True, null=True, validators=[validate_unicode_branch]","    commits = JSONField(default=list)"]},{"diff":"\n         related_name=\"assigned_tasks\",\n     )\n     branch_name = models.CharField(\n-        max_length=100, blank=True, null=True, validators=[validate_unicode_branch],\n+        max_length=100, blank=True, null=True, validators=[validate_unicode_branch]\n     )\n+    commits = JSONField(default=list)\n \n     slug_class = TaskSlug","add":2,"remove":1,"filename":"\/metashare\/api\/models.py","badparts":["        max_length=100, blank=True, null=True, validators=[validate_unicode_branch],"],"goodparts":["        max_length=100, blank=True, null=True, validators=[validate_unicode_branch]","    commits = JSONField(default=list)"]}],"source":"\nfrom allauth.account.signals import user_logged_in from asgiref.sync import async_to_sync from cryptography.fernet import InvalidToken from cumulusci.core.config import OrgConfig from cumulusci.oauth.salesforce import jwt_session from django.conf import settings from django.contrib.auth.models import AbstractUser from django.contrib.auth.models import UserManager as BaseUserManager from django.contrib.postgres.fields import JSONField from django.core.serializers.json import DjangoJSONEncoder from django.db import models from django.db.models.signals import post_save from django.dispatch import receiver from django.utils import timezone from django.utils.functional import cached_property from model_utils import Choices from sfdo_template_helpers.crypto import fernet_decrypt from sfdo_template_helpers.fields import MarkdownField, StringField from sfdo_template_helpers.slugs import AbstractSlug, SlugMixin from simple_salesforce.exceptions import SalesforceError from. import gh from. import model_mixins as mixins from. import push from.constants import ORGANIZATION_DETAILS from.sf_run_flow import get_devhub_api from.validators import validate_unicode_branch ORG_TYPES=Choices(\"Production\", \"Scratch\", \"Sandbox\", \"Developer\") class UserQuerySet(models.QuerySet): pass class UserManager(BaseUserManager.from_queryset(UserQuerySet)): pass class User(mixins.HashIdMixin, AbstractUser): objects=UserManager() currently_fetching_repos=models.BooleanField(default=False) def refresh_repositories(self): repos=gh.get_all_org_repos(self) GitHubRepository.objects.filter(user=self).delete() GitHubRepository.objects.bulk_create( [ GitHubRepository(user=self, repo_id=repo.id, repo_url=repo.html_url) for repo in repos ] ) self.currently_fetching_repos=False self.save() self.notify_repositories_updated() def notify_repositories_updated(self): message={\"type\": \"USER_REPOS_REFRESH\"} async_to_sync(push.push_message_about_instance)(self, message) def invalidate_salesforce_credentials(self): self.socialaccount_set.filter(provider__startswith=\"salesforce-\").delete() def subscribable_by(self, user): return self==user def _get_org_property(self, key): try: return self.salesforce_account.extra_data[ORGANIZATION_DETAILS][key] except(AttributeError, KeyError, TypeError): return None @property def org_id(self): try: return self.salesforce_account.extra_data[\"organization_id\"] except(AttributeError, KeyError, TypeError): return None @property def org_name(self): return self._get_org_property(\"Name\") @property def org_type(self): return self._get_org_property(\"OrganizationType\") @property def full_org_type(self): org_type=self._get_org_property(\"OrganizationType\") is_sandbox=self._get_org_property(\"IsSandbox\") has_expiration=self._get_org_property(\"TrialExpirationDate\") is not None if org_type is None or is_sandbox is None: return None if org_type==\"Developer Edition\" and not is_sandbox: return ORG_TYPES.Developer if org_type !=\"Developer Edition\" and not is_sandbox: return ORG_TYPES.Production if is_sandbox and not has_expiration: return ORG_TYPES.Sandbox if is_sandbox and has_expiration: return ORG_TYPES.Scratch @property def instance_url(self): try: return self.salesforce_account.extra_data[\"instance_url\"] except(AttributeError, KeyError): return None @property def sf_username(self): try: return self.salesforce_account.extra_data[\"preferred_username\"] except(AttributeError, KeyError): return None @property def sf_token(self): try: token=self.salesforce_account.socialtoken_set.first() return( fernet_decrypt(token.token) if token.token else None, token.token_secret if token.token_secret else None, ) except(InvalidToken, AttributeError): return(None, None) @property def salesforce_account(self): return self.socialaccount_set.filter(provider__startswith=\"salesforce-\").first() @property def valid_token_for(self): if all(self.sf_token) and self.org_id: return self.org_id return None @cached_property def is_devhub_enabled(self): if not self.salesforce_account: return False if self.full_org_type in(ORG_TYPES.Scratch, ORG_TYPES.Sandbox): return False client=get_devhub_api(devhub_username=self.sf_username) try: resp=client.restful(\"sobjects\/ScratchOrgInfo\") if resp: return True return False except SalesforceError: return False class RepositorySlug(AbstractSlug): parent=models.ForeignKey( \"Repository\", on_delete=models.PROTECT, related_name=\"slugs\" ) class Repository( mixins.PopulateRepoId, mixins.HashIdMixin, mixins.TimestampsMixin, SlugMixin, models.Model, ): repo_owner=StringField() repo_name=StringField() name=StringField(unique=True) description=MarkdownField(blank=True, property_suffix=\"_markdown\") is_managed=models.BooleanField(default=False) repo_id=models.IntegerField(null=True, blank=True, unique=True) slug_class=RepositorySlug def __str__(self): return self.name class Meta: verbose_name_plural=\"repositories\" ordering=(\"name\",) unique_together=((\"repo_owner\", \"repo_name\"),) class GitHubRepository(mixins.HashIdMixin, models.Model): user=models.ForeignKey( User, on_delete=models.CASCADE, related_name=\"repositories\" ) repo_id=models.IntegerField() repo_url=models.URLField() class Meta: verbose_name_plural=\"GitHub repositories\" unique_together=((\"user\", \"repo_id\"),) def __str__(self): return self.repo_url class ProjectSlug(AbstractSlug): parent=models.ForeignKey( \"Project\", on_delete=models.PROTECT, related_name=\"slugs\" ) class Project(mixins.HashIdMixin, mixins.TimestampsMixin, SlugMixin, models.Model): name=StringField() description=MarkdownField(blank=True, property_suffix=\"_markdown\") branch_name=models.CharField( max_length=100, blank=True, null=True, validators=[validate_unicode_branch], ) repository=models.ForeignKey( Repository, on_delete=models.PROTECT, related_name=\"projects\" ) slug_class=ProjectSlug def __str__(self): return self.name def subscribable_by(self, user): return True def finalize_branch_update(self): from.serializers import ProjectSerializer self.save() payload=ProjectSerializer(self).data message={\"type\": \"PROJECT_UPDATE\", \"payload\": payload} async_to_sync(push.push_message_about_instance)(self, message) class Meta: ordering=(\"-created_at\", \"name\") unique_together=((\"name\", \"repository\"),) class TaskSlug(AbstractSlug): parent=models.ForeignKey(\"Task\", on_delete=models.PROTECT, related_name=\"slugs\") class Task(mixins.HashIdMixin, mixins.TimestampsMixin, SlugMixin, models.Model): name=StringField() project=models.ForeignKey(Project, on_delete=models.PROTECT, related_name=\"tasks\") description=MarkdownField(blank=True, property_suffix=\"_markdown\") assignee=models.ForeignKey( User, on_delete=models.SET_NULL, null=True, blank=True, related_name=\"assigned_tasks\", ) branch_name=models.CharField( max_length=100, blank=True, null=True, validators=[validate_unicode_branch], ) slug_class=TaskSlug def __str__(self): return self.name def subscribable_by(self, user): return True def finalize_branch_update(self): from.serializers import TaskSerializer self.save() payload=TaskSerializer(self).data message={\"type\": \"TASK_UPDATE\", \"payload\": payload} async_to_sync(push.push_message_about_instance)(self, message) class Meta: ordering=(\"-created_at\", \"name\") unique_together=((\"name\", \"project\"),) SCRATCH_ORG_TYPES=Choices(\"Dev\", \"QA\") class ScratchOrg(mixins.HashIdMixin, mixins.TimestampsMixin, models.Model): task=models.ForeignKey(Task, on_delete=models.PROTECT) org_type=StringField(choices=SCRATCH_ORG_TYPES) owner=models.ForeignKey(User, on_delete=models.PROTECT) last_modified_at=models.DateTimeField(null=True, blank=True) expires_at=models.DateTimeField(null=True, blank=True) latest_commit=StringField(blank=True) latest_commit_url=models.URLField(blank=True) latest_commit_at=models.DateTimeField(null=True, blank=True) url=models.URLField(null=True, blank=True) unsaved_changes=JSONField(default=dict, encoder=DjangoJSONEncoder, blank=True) latest_revision_numbers=JSONField( default=dict, encoder=DjangoJSONEncoder, blank=True ) currently_refreshing_changes=models.BooleanField(default=False) currently_capturing_changes=models.BooleanField(default=False) config=JSONField(default=dict, encoder=DjangoJSONEncoder, blank=True) delete_queued_at=models.DateTimeField(null=True, blank=True) owner_sf_id=StringField(blank=True) def subscribable_by(self, user): return True def save(self, *args, **kwargs): is_new=self.id is None ret=super().save(*args, **kwargs) if is_new: self.queue_provision() return ret def get_refreshed_org_config(self): org_config=OrgConfig(self.config, \"dev\") info=jwt_session( settings.SF_CLIENT_ID, settings.SF_CLIENT_KEY, org_config.username, org_config.instance_url, ) org_config.config.update(info) org_config._load_userinfo() org_config._load_orginfo() return org_config def get_login_url(self): org_config=self.get_refreshed_org_config() return org_config.start_url def notify_changed(self): from.serializers import ScratchOrgSerializer payload=ScratchOrgSerializer(self).data message={\"type\": \"SCRATCH_ORG_UPDATE\", \"payload\": payload} async_to_sync(push.push_message_about_instance)(self, message) def queue_delete(self): from.jobs import delete_scratch_org_job if self.last_modified_at: self.delete_queued_at=timezone.now() self.save() self.notify_changed() delete_scratch_org_job.delay(self) def finalize_delete(self): from.serializers import ScratchOrgSerializer payload=ScratchOrgSerializer(self).data message={\"type\": \"SCRATCH_ORG_DELETE\", \"payload\": payload} async_to_sync(push.push_message_about_instance)(self, message) def delete(self, *args, **kwargs): if self.last_modified_at: self.finalize_delete() super().delete(*args, **kwargs) def queue_provision(self): from.jobs import create_branches_on_github_then_create_scratch_org_job create_branches_on_github_then_create_scratch_org_job.delay(scratch_org=self) def finalize_provision(self, error=None): from.serializers import ScratchOrgSerializer if error is None: self.save() async_to_sync(push.push_message_about_instance)( self, { \"type\": \"SCRATCH_ORG_PROVISION\", \"payload\": ScratchOrgSerializer(self).data, }, ) else: async_to_sync(push.report_scratch_org_error)( self, error, \"SCRATCH_ORG_PROVISION_FAILED\" ) if self.url: self.queue_delete() else: self.delete() def queue_get_unsaved_changes(self): from.jobs import get_unsaved_changes_job self.currently_refreshing_changes=True self.save() self.notify_changed() get_unsaved_changes_job.delay(self) def finalize_get_unsaved_changes(self, error=None): self.currently_refreshing_changes=False if error is None: self.save() self.notify_changed() else: self.unsaved_changes={} self.save() async_to_sync(push.report_scratch_org_error)( self, error, \"SCRATCH_ORG_FETCH_CHANGES_FAILED\" ) def queue_commit_changes(self, user, desired_changes, commit_message): from.jobs import commit_changes_from_org_job self.currently_capturing_changes=True self.save() self.notify_changed() commit_changes_from_org_job.delay(self, user, desired_changes, commit_message) def finalize_commit_changes(self, error=None): from.serializers import ScratchOrgSerializer self.currently_capturing_changes=False self.save() if error is None: async_to_sync(push.push_message_about_instance)( self, { \"type\": \"SCRATCH_ORG_COMMIT_CHANGES\", \"payload\": ScratchOrgSerializer(self).data, }, ) else: async_to_sync(push.report_scratch_org_error)( self, error, \"SCRATCH_ORG_COMMIT_CHANGES_FAILED\" ) @receiver(user_logged_in) def user_logged_in_handler(sender, *, user, **kwargs): from.jobs import refresh_github_repositories_for_user_job refresh_github_repositories_for_user_job.delay(user) def ensure_slug_handler(sender, *, created, instance, **kwargs): if created: instance.ensure_slug() post_save.connect(ensure_slug_handler, sender=Repository) post_save.connect(ensure_slug_handler, sender=Project) post_save.connect(ensure_slug_handler, sender=Task) ","sourceWithComments":"from allauth.account.signals import user_logged_in\nfrom asgiref.sync import async_to_sync\nfrom cryptography.fernet import InvalidToken\nfrom cumulusci.core.config import OrgConfig\nfrom cumulusci.oauth.salesforce import jwt_session\nfrom django.conf import settings\nfrom django.contrib.auth.models import AbstractUser\nfrom django.contrib.auth.models import UserManager as BaseUserManager\nfrom django.contrib.postgres.fields import JSONField\nfrom django.core.serializers.json import DjangoJSONEncoder\nfrom django.db import models\nfrom django.db.models.signals import post_save\nfrom django.dispatch import receiver\nfrom django.utils import timezone\nfrom django.utils.functional import cached_property\nfrom model_utils import Choices\nfrom sfdo_template_helpers.crypto import fernet_decrypt\nfrom sfdo_template_helpers.fields import MarkdownField, StringField\nfrom sfdo_template_helpers.slugs import AbstractSlug, SlugMixin\nfrom simple_salesforce.exceptions import SalesforceError\n\nfrom . import gh\nfrom . import model_mixins as mixins\nfrom . import push\nfrom .constants import ORGANIZATION_DETAILS\nfrom .sf_run_flow import get_devhub_api\nfrom .validators import validate_unicode_branch\n\nORG_TYPES = Choices(\"Production\", \"Scratch\", \"Sandbox\", \"Developer\")\n\n\nclass UserQuerySet(models.QuerySet):\n    pass\n\n\nclass UserManager(BaseUserManager.from_queryset(UserQuerySet)):\n    pass\n\n\nclass User(mixins.HashIdMixin, AbstractUser):\n    objects = UserManager()\n    currently_fetching_repos = models.BooleanField(default=False)\n\n    def refresh_repositories(self):\n        repos = gh.get_all_org_repos(self)\n        GitHubRepository.objects.filter(user=self).delete()\n        GitHubRepository.objects.bulk_create(\n            [\n                GitHubRepository(user=self, repo_id=repo.id, repo_url=repo.html_url)\n                for repo in repos\n            ]\n        )\n        self.currently_fetching_repos = False\n        self.save()\n        self.notify_repositories_updated()\n\n    def notify_repositories_updated(self):\n        message = {\"type\": \"USER_REPOS_REFRESH\"}\n        async_to_sync(push.push_message_about_instance)(self, message)\n\n    def invalidate_salesforce_credentials(self):\n        self.socialaccount_set.filter(provider__startswith=\"salesforce-\").delete()\n\n    def subscribable_by(self, user):\n        return self == user\n\n    def _get_org_property(self, key):\n        try:\n            return self.salesforce_account.extra_data[ORGANIZATION_DETAILS][key]\n        except (AttributeError, KeyError, TypeError):\n            return None\n\n    @property\n    def org_id(self):\n        try:\n            return self.salesforce_account.extra_data[\"organization_id\"]\n        except (AttributeError, KeyError, TypeError):\n            return None\n\n    @property\n    def org_name(self):\n        return self._get_org_property(\"Name\")\n\n    @property\n    def org_type(self):\n        return self._get_org_property(\"OrganizationType\")\n\n    @property\n    def full_org_type(self):\n        org_type = self._get_org_property(\"OrganizationType\")\n        is_sandbox = self._get_org_property(\"IsSandbox\")\n        has_expiration = self._get_org_property(\"TrialExpirationDate\") is not None\n        if org_type is None or is_sandbox is None:\n            return None\n        if org_type == \"Developer Edition\" and not is_sandbox:\n            return ORG_TYPES.Developer\n        if org_type != \"Developer Edition\" and not is_sandbox:\n            return ORG_TYPES.Production\n        if is_sandbox and not has_expiration:\n            return ORG_TYPES.Sandbox\n        if is_sandbox and has_expiration:\n            return ORG_TYPES.Scratch\n\n    @property\n    def instance_url(self):\n        try:\n            return self.salesforce_account.extra_data[\"instance_url\"]\n        except (AttributeError, KeyError):\n            return None\n\n    @property\n    def sf_username(self):\n        try:\n            return self.salesforce_account.extra_data[\"preferred_username\"]\n        except (AttributeError, KeyError):\n            return None\n\n    @property\n    def sf_token(self):\n        try:\n            token = self.salesforce_account.socialtoken_set.first()\n            return (\n                fernet_decrypt(token.token) if token.token else None,\n                token.token_secret if token.token_secret else None,\n            )\n        except (InvalidToken, AttributeError):\n            return (None, None)\n\n    @property\n    def salesforce_account(self):\n        return self.socialaccount_set.filter(provider__startswith=\"salesforce-\").first()\n\n    @property\n    def valid_token_for(self):\n        if all(self.sf_token) and self.org_id:\n            return self.org_id\n        return None\n\n    @cached_property\n    def is_devhub_enabled(self):\n        # We can shortcut and avoid making an HTTP request in some cases:\n        if not self.salesforce_account:\n            return False\n        if self.full_org_type in (ORG_TYPES.Scratch, ORG_TYPES.Sandbox):\n            return False\n\n        client = get_devhub_api(devhub_username=self.sf_username)\n        try:\n            resp = client.restful(\"sobjects\/ScratchOrgInfo\")\n            if resp:\n                return True\n            return False\n        except SalesforceError:\n            return False\n\n\nclass RepositorySlug(AbstractSlug):\n    parent = models.ForeignKey(\n        \"Repository\", on_delete=models.PROTECT, related_name=\"slugs\"\n    )\n\n\nclass Repository(\n    mixins.PopulateRepoId,\n    mixins.HashIdMixin,\n    mixins.TimestampsMixin,\n    SlugMixin,\n    models.Model,\n):\n    repo_owner = StringField()\n    repo_name = StringField()\n    name = StringField(unique=True)\n    description = MarkdownField(blank=True, property_suffix=\"_markdown\")\n    is_managed = models.BooleanField(default=False)\n    repo_id = models.IntegerField(null=True, blank=True, unique=True)\n\n    slug_class = RepositorySlug\n\n    def __str__(self):\n        return self.name\n\n    class Meta:\n        verbose_name_plural = \"repositories\"\n        ordering = (\"name\",)\n        unique_together = ((\"repo_owner\", \"repo_name\"),)\n\n\nclass GitHubRepository(mixins.HashIdMixin, models.Model):\n    user = models.ForeignKey(\n        User, on_delete=models.CASCADE, related_name=\"repositories\"\n    )\n    repo_id = models.IntegerField()\n    repo_url = models.URLField()\n\n    class Meta:\n        verbose_name_plural = \"GitHub repositories\"\n        unique_together = ((\"user\", \"repo_id\"),)\n\n    def __str__(self):\n        return self.repo_url\n\n\nclass ProjectSlug(AbstractSlug):\n    parent = models.ForeignKey(\n        \"Project\", on_delete=models.PROTECT, related_name=\"slugs\"\n    )\n\n\nclass Project(mixins.HashIdMixin, mixins.TimestampsMixin, SlugMixin, models.Model):\n    name = StringField()\n    description = MarkdownField(blank=True, property_suffix=\"_markdown\")\n    branch_name = models.CharField(\n        max_length=100, blank=True, null=True, validators=[validate_unicode_branch],\n    )\n\n    repository = models.ForeignKey(\n        Repository, on_delete=models.PROTECT, related_name=\"projects\"\n    )\n\n    slug_class = ProjectSlug\n\n    def __str__(self):\n        return self.name\n\n    def subscribable_by(self, user):  # pragma: nocover\n        return True\n\n    def finalize_branch_update(self):\n        from .serializers import ProjectSerializer\n\n        self.save()\n        payload = ProjectSerializer(self).data\n        message = {\"type\": \"PROJECT_UPDATE\", \"payload\": payload}\n        async_to_sync(push.push_message_about_instance)(self, message)\n\n    class Meta:\n        ordering = (\"-created_at\", \"name\")\n        unique_together = ((\"name\", \"repository\"),)\n\n\nclass TaskSlug(AbstractSlug):\n    parent = models.ForeignKey(\"Task\", on_delete=models.PROTECT, related_name=\"slugs\")\n\n\nclass Task(mixins.HashIdMixin, mixins.TimestampsMixin, SlugMixin, models.Model):\n    name = StringField()\n    project = models.ForeignKey(Project, on_delete=models.PROTECT, related_name=\"tasks\")\n    description = MarkdownField(blank=True, property_suffix=\"_markdown\")\n    assignee = models.ForeignKey(\n        User,\n        on_delete=models.SET_NULL,\n        null=True,\n        blank=True,\n        related_name=\"assigned_tasks\",\n    )\n    branch_name = models.CharField(\n        max_length=100, blank=True, null=True, validators=[validate_unicode_branch],\n    )\n\n    slug_class = TaskSlug\n\n    def __str__(self):\n        return self.name\n\n    def subscribable_by(self, user):  # pragma: nocover\n        return True\n\n    def finalize_branch_update(self):\n        from .serializers import TaskSerializer\n\n        self.save()\n        payload = TaskSerializer(self).data\n        message = {\"type\": \"TASK_UPDATE\", \"payload\": payload}\n        async_to_sync(push.push_message_about_instance)(self, message)\n\n    class Meta:\n        ordering = (\"-created_at\", \"name\")\n        unique_together = ((\"name\", \"project\"),)\n\n\nSCRATCH_ORG_TYPES = Choices(\"Dev\", \"QA\")\n\n\nclass ScratchOrg(mixins.HashIdMixin, mixins.TimestampsMixin, models.Model):\n    task = models.ForeignKey(Task, on_delete=models.PROTECT)\n    org_type = StringField(choices=SCRATCH_ORG_TYPES)\n    owner = models.ForeignKey(User, on_delete=models.PROTECT)\n    last_modified_at = models.DateTimeField(null=True, blank=True)\n    expires_at = models.DateTimeField(null=True, blank=True)\n    latest_commit = StringField(blank=True)\n    latest_commit_url = models.URLField(blank=True)\n    latest_commit_at = models.DateTimeField(null=True, blank=True)\n    url = models.URLField(null=True, blank=True)\n    unsaved_changes = JSONField(default=dict, encoder=DjangoJSONEncoder, blank=True)\n    latest_revision_numbers = JSONField(\n        default=dict, encoder=DjangoJSONEncoder, blank=True\n    )\n    currently_refreshing_changes = models.BooleanField(default=False)\n    currently_capturing_changes = models.BooleanField(default=False)\n    config = JSONField(default=dict, encoder=DjangoJSONEncoder, blank=True)\n    delete_queued_at = models.DateTimeField(null=True, blank=True)\n    owner_sf_id = StringField(blank=True)\n\n    def subscribable_by(self, user):  # pragma: nocover\n        return True\n\n    def save(self, *args, **kwargs):\n        is_new = self.id is None\n        ret = super().save(*args, **kwargs)\n\n        if is_new:\n            self.queue_provision()\n\n        return ret\n\n    def get_refreshed_org_config(self):\n        org_config = OrgConfig(self.config, \"dev\")\n        info = jwt_session(\n            settings.SF_CLIENT_ID,\n            settings.SF_CLIENT_KEY,\n            org_config.username,\n            org_config.instance_url,\n        )\n        org_config.config.update(info)\n        org_config._load_userinfo()\n        org_config._load_orginfo()\n        return org_config\n\n    def get_login_url(self):\n        org_config = self.get_refreshed_org_config()\n        return org_config.start_url\n\n    def notify_changed(self):\n        from .serializers import ScratchOrgSerializer\n\n        payload = ScratchOrgSerializer(self).data\n        message = {\"type\": \"SCRATCH_ORG_UPDATE\", \"payload\": payload}\n\n        async_to_sync(push.push_message_about_instance)(self, message)\n\n    def queue_delete(self):\n        from .jobs import delete_scratch_org_job\n\n        # If the scratch org has no `last_modified_at`, it did not\n        # successfully complete the initial flow run on Salesforce, and\n        # therefore we don't need to notify of its destruction; this\n        # should only happen when it is destroyed during the initial\n        # flow run.\n        if self.last_modified_at:\n            self.delete_queued_at = timezone.now()\n            self.save()\n            self.notify_changed()\n\n        delete_scratch_org_job.delay(self)\n\n    def finalize_delete(self):\n        from .serializers import ScratchOrgSerializer\n\n        payload = ScratchOrgSerializer(self).data\n        message = {\"type\": \"SCRATCH_ORG_DELETE\", \"payload\": payload}\n\n        async_to_sync(push.push_message_about_instance)(self, message)\n\n    def delete(self, *args, **kwargs):\n        # If the scratch org has no `last_modified_at`, it did not\n        # successfully complete the initial flow run on Salesforce, and\n        # therefore we don't need to notify of its destruction; this\n        # should only happen when it is destroyed during provisioning or\n        # the initial flow run.\n        if self.last_modified_at:\n            self.finalize_delete()\n        super().delete(*args, **kwargs)\n\n    def queue_provision(self):\n        from .jobs import create_branches_on_github_then_create_scratch_org_job\n\n        create_branches_on_github_then_create_scratch_org_job.delay(scratch_org=self)\n\n    def finalize_provision(self, error=None):\n        from .serializers import ScratchOrgSerializer\n\n        if error is None:\n            self.save()\n            async_to_sync(push.push_message_about_instance)(\n                self,\n                {\n                    \"type\": \"SCRATCH_ORG_PROVISION\",\n                    \"payload\": ScratchOrgSerializer(self).data,\n                },\n            )\n        else:\n            async_to_sync(push.report_scratch_org_error)(\n                self, error, \"SCRATCH_ORG_PROVISION_FAILED\"\n            )\n            # If the scratch org has already been created on Salesforce,\n            # we need to delete it there as well.\n            if self.url:\n                self.queue_delete()\n            else:\n                self.delete()\n\n    def queue_get_unsaved_changes(self):\n        from .jobs import get_unsaved_changes_job\n\n        self.currently_refreshing_changes = True\n        self.save()\n        self.notify_changed()\n\n        get_unsaved_changes_job.delay(self)\n\n    def finalize_get_unsaved_changes(self, error=None):\n        self.currently_refreshing_changes = False\n        if error is None:\n            self.save()\n            self.notify_changed()\n        else:\n            self.unsaved_changes = {}\n            self.save()\n            async_to_sync(push.report_scratch_org_error)(\n                self, error, \"SCRATCH_ORG_FETCH_CHANGES_FAILED\"\n            )\n\n    def queue_commit_changes(self, user, desired_changes, commit_message):\n        from .jobs import commit_changes_from_org_job\n\n        self.currently_capturing_changes = True\n        self.save()\n        self.notify_changed()\n\n        commit_changes_from_org_job.delay(self, user, desired_changes, commit_message)\n\n    def finalize_commit_changes(self, error=None):\n        from .serializers import ScratchOrgSerializer\n\n        self.currently_capturing_changes = False\n        self.save()\n        if error is None:\n            async_to_sync(push.push_message_about_instance)(\n                self,\n                {\n                    \"type\": \"SCRATCH_ORG_COMMIT_CHANGES\",\n                    \"payload\": ScratchOrgSerializer(self).data,\n                },\n            )\n        else:\n            async_to_sync(push.report_scratch_org_error)(\n                self, error, \"SCRATCH_ORG_COMMIT_CHANGES_FAILED\"\n            )\n\n\n@receiver(user_logged_in)\ndef user_logged_in_handler(sender, *, user, **kwargs):\n    from .jobs import refresh_github_repositories_for_user_job\n\n    refresh_github_repositories_for_user_job.delay(user)\n\n\ndef ensure_slug_handler(sender, *, created, instance, **kwargs):\n    if created:\n        instance.ensure_slug()\n\n\npost_save.connect(ensure_slug_handler, sender=Repository)\npost_save.connect(ensure_slug_handler, sender=Project)\npost_save.connect(ensure_slug_handler, sender=Task)\n"},"\/metashare\/api\/tests\/views.py":{"changes":[{"diff":"\n \n \n @pytest.mark.django_db\n-def test_repository_view(client, repository_factory, git_hub_repository_factory):\n-    git_hub_repository_factory(\n-        user=client.user, repo_id=123, repo_url=\"https:\/\/example.com\/test-repo.git\"\n-    )\n-    repo = repository_factory(repo_name=\"repo\", repo_id=123)\n-    repository_factory(repo_name=\"repo2\", repo_id=456)\n-    repository_factory(repo_name=\"repo3\", repo_id=None)\n-    response = client.get(reverse(\"repository-list\"))\n+class TestRepositoryView:\n+    def test_get_queryset(self, client, repository_factory, git_hub_repository_factory):\n+        git_hub_repository_factory(\n+            user=client.user, repo_id=123, repo_url=\"https:\/\/example.com\/test-repo.git\"\n+        )\n+        repo = repository_factory(repo_name=\"repo\", repo_id=123)\n+        repository_factory(repo_name=\"repo2\", repo_id=456)\n+        repository_factory(repo_name=\"repo3\", repo_id=None)\n+        with patch(\"metashare.api.model_mixins.get_repo_info\") as get_repo_info:\n+            get_repo_info.return_value = MagicMock(id=789)\n+            response = client.get(reverse(\"repository-list\"))\n+\n+        assert response.status_code == 200\n+        assert response.json() == {\n+            \"count\": 1,\n+            \"previous\": None,\n+            \"next\": None,\n+            \"results\": [\n+                {\n+                    \"id\": str(repo.id),\n+                    \"name\": str(repo.name),\n+                    \"description\": \"\",\n+                    \"is_managed\": False,\n+                    \"slug\": str(repo.slug),\n+                    \"old_slugs\": [],\n+                    \"repo_url\": (\n+                        f\"https:\/\/github.com\/{repo.repo_owner}\/{repo.repo_name}\"\n+                    ),\n+                }\n+            ],\n+        }\n+\n+    def test_get_queryset__bad(\n+        self, client, repository_factory, git_hub_repository_factory\n+    ):\n+        git_hub_repository_factory(\n+            user=client.user, repo_id=123, repo_url=\"https:\/\/example.com\/test-repo.git\"\n+        )\n+        repo = repository_factory(repo_name=\"repo\", repo_id=123)\n+        repository_factory(repo_name=\"repo2\", repo_id=456)\n+        repository_factory(repo_name=\"repo3\", repo_id=None)\n+        with patch(\"metashare.api.model_mixins.get_repo_info\") as get_repo_info:\n+            get_repo_info.side_effect = ResponseError(MagicMock())\n+            response = client.get(reverse(\"repository-list\"))\n+\n+        assert response.status_code == 200\n+        assert response.json() == {\n+            \"count\": 1,\n+            \"previous\": None,\n+            \"next\": None,\n+            \"results\": [\n+                {\n+                    \"id\": str(repo.id),\n+                    \"name\": str(repo.name),\n+                    \"description\": \"\",\n+                    \"is_managed\": False,\n+                    \"slug\": str(repo.slug),\n+                    \"old_slugs\": [],\n+                    \"repo_url\": (\n+                        f\"https:\/\/github.com\/{repo.repo_owner}\/{repo.repo_name}\"\n+                    ),\n+                }\n+            ],\n+        }\n+\n+    def test_hook__good(self, client, repository_factory, git_hub_repository_factory):\n+        repo = repository_factory(repo_id=123)\n+        git_hub_repository_factory(repo_id=123)\n+        with patch(\"metashare.api.jobs.refresh_commits_job\") as refresh_commits_job:\n+            response = client.post(\n+                reverse(\"repository-hook\", kwargs={\"pk\": str(repo.id)})\n+            )\n+            assert refresh_commits_job.delay.called\n+            assert response.status_code == 202\n \n-    assert response.status_code == 200\n-    assert response.json() == {\n-        \"count\": 1,\n-        \"previous\": None,\n-        \"next\": None,\n-        \"results\": [\n-            {\n-                \"id\": str(repo.id),\n-                \"name\": str(repo.name),\n-                \"description\": \"\",\n-                \"is_managed\": False,\n-                \"slug\": str(repo.slug),\n-                \"old_slugs\": [],\n-                \"repo_url\": f\"https:\/\/github.com\/{repo.repo_owner}\/{repo.repo_name}\",\n-            }\n-        ],\n-    }\n+    def test_hook__bad(self, client, repository_factory):\n+        repo = repository_factory()\n+        response = client.post(reverse(\"repository-hook\", kwargs={\"pk\": str(repo.id)}))\n+        assert response.status_code == 500\n \n \n @pytest.mark.dja","add":78,"remove":25,"filename":"\/metashare\/api\/tests\/views.py","badparts":["def test_repository_view(client, repository_factory, git_hub_repository_factory):","    git_hub_repository_factory(","        user=client.user, repo_id=123, repo_url=\"https:\/\/example.com\/test-repo.git\"","    )","    repo = repository_factory(repo_name=\"repo\", repo_id=123)","    repository_factory(repo_name=\"repo2\", repo_id=456)","    repository_factory(repo_name=\"repo3\", repo_id=None)","    response = client.get(reverse(\"repository-list\"))","    assert response.status_code == 200","    assert response.json() == {","        \"count\": 1,","        \"previous\": None,","        \"next\": None,","        \"results\": [","            {","                \"id\": str(repo.id),","                \"name\": str(repo.name),","                \"description\": \"\",","                \"is_managed\": False,","                \"slug\": str(repo.slug),","                \"old_slugs\": [],","                \"repo_url\": f\"https:\/\/github.com\/{repo.repo_owner}\/{repo.repo_name}\",","            }","        ],","    }"],"goodparts":["class TestRepositoryView:","    def test_get_queryset(self, client, repository_factory, git_hub_repository_factory):","        git_hub_repository_factory(","            user=client.user, repo_id=123, repo_url=\"https:\/\/example.com\/test-repo.git\"","        )","        repo = repository_factory(repo_name=\"repo\", repo_id=123)","        repository_factory(repo_name=\"repo2\", repo_id=456)","        repository_factory(repo_name=\"repo3\", repo_id=None)","        with patch(\"metashare.api.model_mixins.get_repo_info\") as get_repo_info:","            get_repo_info.return_value = MagicMock(id=789)","            response = client.get(reverse(\"repository-list\"))","        assert response.status_code == 200","        assert response.json() == {","            \"count\": 1,","            \"previous\": None,","            \"next\": None,","            \"results\": [","                {","                    \"id\": str(repo.id),","                    \"name\": str(repo.name),","                    \"description\": \"\",","                    \"is_managed\": False,","                    \"slug\": str(repo.slug),","                    \"old_slugs\": [],","                    \"repo_url\": (","                        f\"https:\/\/github.com\/{repo.repo_owner}\/{repo.repo_name}\"","                    ),","                }","            ],","        }","    def test_get_queryset__bad(","        self, client, repository_factory, git_hub_repository_factory","    ):","        git_hub_repository_factory(","            user=client.user, repo_id=123, repo_url=\"https:\/\/example.com\/test-repo.git\"","        )","        repo = repository_factory(repo_name=\"repo\", repo_id=123)","        repository_factory(repo_name=\"repo2\", repo_id=456)","        repository_factory(repo_name=\"repo3\", repo_id=None)","        with patch(\"metashare.api.model_mixins.get_repo_info\") as get_repo_info:","            get_repo_info.side_effect = ResponseError(MagicMock())","            response = client.get(reverse(\"repository-list\"))","        assert response.status_code == 200","        assert response.json() == {","            \"count\": 1,","            \"previous\": None,","            \"next\": None,","            \"results\": [","                {","                    \"id\": str(repo.id),","                    \"name\": str(repo.name),","                    \"description\": \"\",","                    \"is_managed\": False,","                    \"slug\": str(repo.slug),","                    \"old_slugs\": [],","                    \"repo_url\": (","                        f\"https:\/\/github.com\/{repo.repo_owner}\/{repo.repo_name}\"","                    ),","                }","            ],","        }","    def test_hook__good(self, client, repository_factory, git_hub_repository_factory):","        repo = repository_factory(repo_id=123)","        git_hub_repository_factory(repo_id=123)","        with patch(\"metashare.api.jobs.refresh_commits_job\") as refresh_commits_job:","            response = client.post(","                reverse(\"repository-hook\", kwargs={\"pk\": str(repo.id)})","            )","            assert refresh_commits_job.delay.called","            assert response.status_code == 202","    def test_hook__bad(self, client, repository_factory):","        repo = repository_factory()","        response = client.post(reverse(\"repository-hook\", kwargs={\"pk\": str(repo.id)}))","        assert response.status_code == 500"]}],"source":"\nfrom contextlib import ExitStack from unittest.mock import MagicMock, patch import pytest from django.urls import reverse from..models import SCRATCH_ORG_TYPES @pytest.mark.django_db def test_user_view(client): response=client.get(reverse(\"user\")) assert response.status_code==200 assert response.json()[\"username\"].endswith(\"@example.com\") @pytest.mark.django_db def test_user_disconnect_view(client): response=client.post(reverse(\"user-disconnect-sf\")) assert not client.user.socialaccount_set.filter( provider__startswith=\"salesforce-\" ).exists() assert response.status_code==200 assert response.json()[\"username\"].endswith(\"@example.com\") @pytest.mark.django_db def test_user_refresh_view(client): with patch(\"metashare.api.gh.gh_given_user\") as gh_given_user: repo=MagicMock() repo.url=\"test\" gh=MagicMock() gh.repositories.return_value=[repo] gh_given_user.return_value=gh response=client.post(reverse(\"user-refresh\")) assert response.status_code==202 @pytest.mark.django_db def test_repository_view(client, repository_factory, git_hub_repository_factory): git_hub_repository_factory( user=client.user, repo_id=123, repo_url=\"https:\/\/example.com\/test-repo.git\" ) repo=repository_factory(repo_name=\"repo\", repo_id=123) repository_factory(repo_name=\"repo2\", repo_id=456) repository_factory(repo_name=\"repo3\", repo_id=None) response=client.get(reverse(\"repository-list\")) assert response.status_code==200 assert response.json()=={ \"count\": 1, \"previous\": None, \"next\": None, \"results\":[ { \"id\": str(repo.id), \"name\": str(repo.name), \"description\": \"\", \"is_managed\": False, \"slug\": str(repo.slug), \"old_slugs\":[], \"repo_url\": f\"https:\/\/github.com\/{repo.repo_owner}\/{repo.repo_name}\", } ], } @pytest.mark.django_db class TestScratchOrgView: def test_commit_happy_path(self, client, scratch_org_factory): scratch_org=scratch_org_factory(org_type=\"Dev\", owner=client.user) with patch( \"metashare.api.jobs.commit_changes_from_org_job\" ) as commit_changes_from_org_job: response=client.post( reverse(\"scratch-org-commit\", kwargs={\"pk\": str(scratch_org.id)}), {\"commit_message\": \"Test message\", \"changes\":{}}, format=\"json\", ) assert response.status_code==202 assert commit_changes_from_org_job.delay.called def test_commit_sad_path__422(self, client, scratch_org_factory): scratch_org=scratch_org_factory(org_type=\"Dev\") with patch( \"metashare.api.jobs.commit_changes_from_org_job\" ) as commit_changes_from_org_job: response=client.post( reverse(\"scratch-org-commit\", kwargs={\"pk\": str(scratch_org.id)}), {\"changes\":{}}, format=\"json\", ) assert response.status_code==422 assert not commit_changes_from_org_job.delay.called def test_commit_sad_path__403(self, client, scratch_org_factory): scratch_org=scratch_org_factory(org_type=\"Dev\") with patch( \"metashare.api.jobs.commit_changes_from_org_job\" ) as commit_changes_from_org_job: response=client.post( reverse(\"scratch-org-commit\", kwargs={\"pk\": str(scratch_org.id)}), {\"commit_message\": \"Test message\", \"changes\":{}}, format=\"json\", ) assert response.status_code==403 assert not commit_changes_from_org_job.delay.called def test_list_fetch_changes(self, client, scratch_org_factory): scratch_org_factory( org_type=SCRATCH_ORG_TYPES.Dev, url=\"https:\/\/example.com\", delete_queued_at=None, currently_capturing_changes=False, currently_refreshing_changes=False, owner=client.user, ) with patch( \"metashare.api.jobs.get_unsaved_changes_job\" ) as get_unsaved_changes_job: url=reverse(\"scratch-org-list\") response=client.get(url) assert response.status_code==200 assert get_unsaved_changes_job.delay.called def test_retrieve_fetch_changes(self, client, scratch_org_factory): scratch_org=scratch_org_factory( org_type=SCRATCH_ORG_TYPES.Dev, url=\"https:\/\/example.com\", delete_queued_at=None, currently_capturing_changes=False, currently_refreshing_changes=False, owner=client.user, ) with patch( \"metashare.api.jobs.get_unsaved_changes_job\" ) as get_unsaved_changes_job: url=reverse(\"scratch-org-detail\", kwargs={\"pk\": str(scratch_org.id)}) response=client.get(url) assert response.status_code==200 assert get_unsaved_changes_job.delay.called def test_create(self, client, task_factory, social_account_factory): task=task_factory() social_account_factory( user=client.user, provider=\"salesforce-production\", extra_data={\"preferred_username\": \"test-username\"}, ) url=reverse(\"scratch-org-list\") with ExitStack() as stack: stack.enter_context( patch(\"metashare.api.views.viewsets.ModelViewSet.perform_create\") ) get_devhub_api=stack.enter_context( patch(\"metashare.api.models.get_devhub_api\") ) resp={\"foo\": \"bar\"} sf_client=MagicMock() sf_client.restful.return_value=resp get_devhub_api.return_value=sf_client response=client.post(url,{\"task\": str(task.id), \"org_type\": \"Dev\"}) assert response.status_code==201, response.content def test_create__bad(self, client, task_factory, social_account_factory): task=task_factory() social_account_factory( user=client.user, provider=\"salesforce-production\", extra_data={\"preferred_username\": \"test-username\"}, ) url=reverse(\"scratch-org-list\") with ExitStack() as stack: stack.enter_context( patch(\"metashare.api.views.viewsets.ModelViewSet.perform_create\") ) get_devhub_api=stack.enter_context( patch(\"metashare.api.models.get_devhub_api\") ) sf_client=MagicMock() sf_client.restful.return_value=None get_devhub_api.return_value=sf_client response=client.post(url,{\"task\": str(task.id), \"org_type\": \"Dev\"}) assert response.status_code==403, response.content def test_queue_delete(self, client, scratch_org_factory, social_account_factory): social_account_factory( user=client.user, provider=\"salesforce-production\", extra_data={\"preferred_username\": \"test-username\"}, ) scratch_org=scratch_org_factory(owner_sf_id=\"test-username\") with patch(\"metashare.api.models.ScratchOrg.queue_delete\"): url=reverse(\"scratch-org-detail\", kwargs={\"pk\": str(scratch_org.id)}) response=client.delete(url) assert response.status_code==204 def test_queue_delete__bad( self, client, scratch_org_factory, social_account_factory ): social_account_factory( user=client.user, provider=\"salesforce-production\", extra_data={\"preferred_username\": \"test-username\"}, ) scratch_org=scratch_org_factory(owner_sf_id=\"other-test-username\") with patch(\"metashare.api.models.ScratchOrg.queue_delete\"): url=reverse(\"scratch-org-detail\", kwargs={\"pk\": str(scratch_org.id)}) response=client.delete(url) assert response.status_code==403 def test_redirect__good(self, client, scratch_org_factory): scratch_org=scratch_org_factory(owner=client.user) with patch(\"metashare.api.models.ScratchOrg.get_login_url\") as get_login_url: get_login_url.return_value=\"https:\/\/example.com\" url=reverse(\"scratch-org-redirect\", kwargs={\"pk\": str(scratch_org.id)}) response=client.get(url) assert response.status_code==302 def test_redirect__bad(self, client, scratch_org_factory): scratch_org=scratch_org_factory() with patch(\"metashare.api.models.ScratchOrg.get_login_url\") as get_login_url: get_login_url.return_value=\"https:\/\/example.com\" url=reverse(\"scratch-org-redirect\", kwargs={\"pk\": str(scratch_org.id)}) response=client.get(url) assert response.status_code==403 ","sourceWithComments":"from contextlib import ExitStack\nfrom unittest.mock import MagicMock, patch\n\nimport pytest\nfrom django.urls import reverse\n\nfrom ..models import SCRATCH_ORG_TYPES\n\n\n@pytest.mark.django_db\ndef test_user_view(client):\n    response = client.get(reverse(\"user\"))\n\n    assert response.status_code == 200\n    assert response.json()[\"username\"].endswith(\"@example.com\")\n\n\n@pytest.mark.django_db\ndef test_user_disconnect_view(client):\n    response = client.post(reverse(\"user-disconnect-sf\"))\n\n    assert not client.user.socialaccount_set.filter(\n        provider__startswith=\"salesforce-\"\n    ).exists()\n    assert response.status_code == 200\n    assert response.json()[\"username\"].endswith(\"@example.com\")\n\n\n@pytest.mark.django_db\ndef test_user_refresh_view(client):\n    with patch(\"metashare.api.gh.gh_given_user\") as gh_given_user:\n        repo = MagicMock()\n        repo.url = \"test\"\n        gh = MagicMock()\n        gh.repositories.return_value = [repo]\n        gh_given_user.return_value = gh\n\n        response = client.post(reverse(\"user-refresh\"))\n\n    assert response.status_code == 202\n\n\n@pytest.mark.django_db\ndef test_repository_view(client, repository_factory, git_hub_repository_factory):\n    git_hub_repository_factory(\n        user=client.user, repo_id=123, repo_url=\"https:\/\/example.com\/test-repo.git\"\n    )\n    repo = repository_factory(repo_name=\"repo\", repo_id=123)\n    repository_factory(repo_name=\"repo2\", repo_id=456)\n    repository_factory(repo_name=\"repo3\", repo_id=None)\n    response = client.get(reverse(\"repository-list\"))\n\n    assert response.status_code == 200\n    assert response.json() == {\n        \"count\": 1,\n        \"previous\": None,\n        \"next\": None,\n        \"results\": [\n            {\n                \"id\": str(repo.id),\n                \"name\": str(repo.name),\n                \"description\": \"\",\n                \"is_managed\": False,\n                \"slug\": str(repo.slug),\n                \"old_slugs\": [],\n                \"repo_url\": f\"https:\/\/github.com\/{repo.repo_owner}\/{repo.repo_name}\",\n            }\n        ],\n    }\n\n\n@pytest.mark.django_db\nclass TestScratchOrgView:\n    def test_commit_happy_path(self, client, scratch_org_factory):\n        scratch_org = scratch_org_factory(org_type=\"Dev\", owner=client.user)\n        with patch(\n            \"metashare.api.jobs.commit_changes_from_org_job\"\n        ) as commit_changes_from_org_job:\n            response = client.post(\n                reverse(\"scratch-org-commit\", kwargs={\"pk\": str(scratch_org.id)}),\n                {\"commit_message\": \"Test message\", \"changes\": {}},\n                format=\"json\",\n            )\n            assert response.status_code == 202\n            assert commit_changes_from_org_job.delay.called\n\n    def test_commit_sad_path__422(self, client, scratch_org_factory):\n        scratch_org = scratch_org_factory(org_type=\"Dev\")\n        with patch(\n            \"metashare.api.jobs.commit_changes_from_org_job\"\n        ) as commit_changes_from_org_job:\n            response = client.post(\n                reverse(\"scratch-org-commit\", kwargs={\"pk\": str(scratch_org.id)}),\n                {\"changes\": {}},\n                format=\"json\",\n            )\n            assert response.status_code == 422\n            assert not commit_changes_from_org_job.delay.called\n\n    def test_commit_sad_path__403(self, client, scratch_org_factory):\n        scratch_org = scratch_org_factory(org_type=\"Dev\")\n        with patch(\n            \"metashare.api.jobs.commit_changes_from_org_job\"\n        ) as commit_changes_from_org_job:\n            response = client.post(\n                reverse(\"scratch-org-commit\", kwargs={\"pk\": str(scratch_org.id)}),\n                {\"commit_message\": \"Test message\", \"changes\": {}},\n                format=\"json\",\n            )\n            assert response.status_code == 403\n            assert not commit_changes_from_org_job.delay.called\n\n    def test_list_fetch_changes(self, client, scratch_org_factory):\n        scratch_org_factory(\n            org_type=SCRATCH_ORG_TYPES.Dev,\n            url=\"https:\/\/example.com\",\n            delete_queued_at=None,\n            currently_capturing_changes=False,\n            currently_refreshing_changes=False,\n            owner=client.user,\n        )\n        with patch(\n            \"metashare.api.jobs.get_unsaved_changes_job\"\n        ) as get_unsaved_changes_job:\n            url = reverse(\"scratch-org-list\")\n            response = client.get(url)\n\n            assert response.status_code == 200\n            assert get_unsaved_changes_job.delay.called\n\n    def test_retrieve_fetch_changes(self, client, scratch_org_factory):\n        scratch_org = scratch_org_factory(\n            org_type=SCRATCH_ORG_TYPES.Dev,\n            url=\"https:\/\/example.com\",\n            delete_queued_at=None,\n            currently_capturing_changes=False,\n            currently_refreshing_changes=False,\n            owner=client.user,\n        )\n        with patch(\n            \"metashare.api.jobs.get_unsaved_changes_job\"\n        ) as get_unsaved_changes_job:\n            url = reverse(\"scratch-org-detail\", kwargs={\"pk\": str(scratch_org.id)})\n            response = client.get(url)\n\n            assert response.status_code == 200\n            assert get_unsaved_changes_job.delay.called\n\n    def test_create(self, client, task_factory, social_account_factory):\n        task = task_factory()\n        social_account_factory(\n            user=client.user,\n            provider=\"salesforce-production\",\n            extra_data={\"preferred_username\": \"test-username\"},\n        )\n        url = reverse(\"scratch-org-list\")\n        with ExitStack() as stack:\n            stack.enter_context(\n                patch(\"metashare.api.views.viewsets.ModelViewSet.perform_create\")\n            )\n            get_devhub_api = stack.enter_context(\n                patch(\"metashare.api.models.get_devhub_api\")\n            )\n            resp = {\"foo\": \"bar\"}\n            sf_client = MagicMock()\n            sf_client.restful.return_value = resp\n            get_devhub_api.return_value = sf_client\n\n            response = client.post(url, {\"task\": str(task.id), \"org_type\": \"Dev\"})\n\n        assert response.status_code == 201, response.content\n\n    def test_create__bad(self, client, task_factory, social_account_factory):\n        task = task_factory()\n        social_account_factory(\n            user=client.user,\n            provider=\"salesforce-production\",\n            extra_data={\"preferred_username\": \"test-username\"},\n        )\n        url = reverse(\"scratch-org-list\")\n        with ExitStack() as stack:\n            stack.enter_context(\n                patch(\"metashare.api.views.viewsets.ModelViewSet.perform_create\")\n            )\n            get_devhub_api = stack.enter_context(\n                patch(\"metashare.api.models.get_devhub_api\")\n            )\n            sf_client = MagicMock()\n            sf_client.restful.return_value = None\n            get_devhub_api.return_value = sf_client\n\n            response = client.post(url, {\"task\": str(task.id), \"org_type\": \"Dev\"})\n\n        assert response.status_code == 403, response.content\n\n    def test_queue_delete(self, client, scratch_org_factory, social_account_factory):\n        social_account_factory(\n            user=client.user,\n            provider=\"salesforce-production\",\n            extra_data={\"preferred_username\": \"test-username\"},\n        )\n        scratch_org = scratch_org_factory(owner_sf_id=\"test-username\")\n        with patch(\"metashare.api.models.ScratchOrg.queue_delete\"):\n            url = reverse(\"scratch-org-detail\", kwargs={\"pk\": str(scratch_org.id)})\n            response = client.delete(url)\n\n            assert response.status_code == 204\n\n    def test_queue_delete__bad(\n        self, client, scratch_org_factory, social_account_factory\n    ):\n        social_account_factory(\n            user=client.user,\n            provider=\"salesforce-production\",\n            extra_data={\"preferred_username\": \"test-username\"},\n        )\n        scratch_org = scratch_org_factory(owner_sf_id=\"other-test-username\")\n        with patch(\"metashare.api.models.ScratchOrg.queue_delete\"):\n            url = reverse(\"scratch-org-detail\", kwargs={\"pk\": str(scratch_org.id)})\n            response = client.delete(url)\n\n            assert response.status_code == 403\n\n    def test_redirect__good(self, client, scratch_org_factory):\n        scratch_org = scratch_org_factory(owner=client.user)\n        with patch(\"metashare.api.models.ScratchOrg.get_login_url\") as get_login_url:\n            get_login_url.return_value = \"https:\/\/example.com\"\n            url = reverse(\"scratch-org-redirect\", kwargs={\"pk\": str(scratch_org.id)})\n            response = client.get(url)\n\n            assert response.status_code == 302\n\n    def test_redirect__bad(self, client, scratch_org_factory):\n        scratch_org = scratch_org_factory()\n        with patch(\"metashare.api.models.ScratchOrg.get_login_url\") as get_login_url:\n            get_login_url.return_value = \"https:\/\/example.com\"\n            url = reverse(\"scratch-org-redirect\", kwargs={\"pk\": str(scratch_org.id)})\n            response = client.get(url)\n\n            assert response.status_code == 403\n"},"\/metashare\/api\/views.py":{"changes":[{"diff":"\n \n     @action(methods=[\"POST\"], detail=True, permission_classes=[AllowAny])\n     def hook(self, request, pk=None):\n-        # repository = self.get_object()\n-        # repo = get_repo_info(self.request.user, repository.repo_id)\n-        return Response(\"\", status=200)\n-        # branch = \"\"\n-        # list(repo.commits(sha=branch.latest_sha()))\n+        repository = self.model.objects.get(pk=pk)\n+        user = repository.get_a_matching_user()\n+        if not user:\n+            return Response(\n+                {\"error\": f\"No matching user for repository {pk}.\"},\n+                status=status.HTTP_500_INTERNAL_SERVER_ERROR,\n+            )\n+        repository.refresh_commits(user)\n+        return Response(status=status.HTTP_202_ACCEPTED)\n \n \n class ProjectViewSet(viewsets.ModelViewSet):\n","add":9,"remove":5,"filename":"\/metashare\/api\/views.py","badparts":["        return Response(\"\", status=200)"],"goodparts":["        repository = self.model.objects.get(pk=pk)","        user = repository.get_a_matching_user()","        if not user:","            return Response(","                {\"error\": f\"No matching user for repository {pk}.\"},","                status=status.HTTP_500_INTERNAL_SERVER_ERROR,","            )","        repository.refresh_commits(user)","        return Response(status=status.HTTP_202_ACCEPTED)"]}],"source":"\nfrom django.contrib.auth import get_user_model from django.http import HttpResponseRedirect from django_filters.rest_framework import DjangoFilterBackend from github3.exceptions import ResponseError from rest_framework import generics, status, viewsets from rest_framework.decorators import action from rest_framework.exceptions import PermissionDenied from rest_framework.permissions import AllowAny, IsAuthenticated from rest_framework.response import Response from rest_framework.views import APIView from.filters import ProjectFilter, RepositoryFilter, ScratchOrgFilter, TaskFilter from.models import SCRATCH_ORG_TYPES, Project, Repository, ScratchOrg, Task from.paginators import CustomPaginator from.serializers import( CommitSerializer, FullUserSerializer, MinimalUserSerializer, ProjectSerializer, RepositorySerializer, ScratchOrgSerializer, TaskSerializer, ) User=get_user_model() class CurrentUserObjectMixin: def get_queryset(self): return self.model.objects.filter(id=self.request.user.id) def get_object(self): return self.get_queryset().get() class UserView(CurrentUserObjectMixin, generics.RetrieveAPIView): \"\"\" Shows the current user. \"\"\" model=User serializer_class=FullUserSerializer permission_classes=(IsAuthenticated,) class UserRefreshView(CurrentUserObjectMixin, APIView): model=User permission_classes=(IsAuthenticated,) def post(self, request): from.jobs import refresh_github_repositories_for_user_job user=self.get_object() refresh_github_repositories_for_user_job.delay(user) return Response(status=status.HTTP_202_ACCEPTED) class UserDisconnectSFView(CurrentUserObjectMixin, APIView): model=User permission_classes=(IsAuthenticated,) def post(self, request): user=self.get_object() user.invalidate_salesforce_credentials() serializer=FullUserSerializer(user) return Response(serializer.data, status=status.HTTP_200_OK) class UserViewSet(viewsets.ModelViewSet): permission_classes=(IsAuthenticated,) serializer_class=MinimalUserSerializer pagination_class=CustomPaginator queryset=User.objects.all() class RepositoryViewSet(viewsets.ModelViewSet): permission_classes=(IsAuthenticated,) serializer_class=RepositorySerializer filter_backends=(DjangoFilterBackend,) filterset_class=RepositoryFilter pagination_class=CustomPaginator model=Repository def get_queryset(self): repo_ids=self.request.user.repositories.values_list(\"repo_id\", flat=True) for repo in Repository.objects.filter(repo_id__isnull=True): try: repo.get_repo_id(self.request.user) except ResponseError: pass return Repository.objects.filter(repo_id__isnull=False, repo_id__in=repo_ids) @action(methods=[\"POST\"], detail=True, permission_classes=[AllowAny]) def hook(self, request, pk=None): return Response(\"\", status=200) class ProjectViewSet(viewsets.ModelViewSet): permission_classes=(IsAuthenticated,) serializer_class=ProjectSerializer pagination_class=CustomPaginator queryset=Project.objects.all() filter_backends=(DjangoFilterBackend,) filterset_class=ProjectFilter class TaskViewSet(viewsets.ModelViewSet): permission_classes=(IsAuthenticated,) serializer_class=TaskSerializer queryset=Task.objects.all() filter_backends=(DjangoFilterBackend,) filterset_class=TaskFilter class ScratchOrgViewSet(viewsets.ModelViewSet): permission_classes=(IsAuthenticated,) serializer_class=ScratchOrgSerializer queryset=ScratchOrg.objects.all() filter_backends=(DjangoFilterBackend,) filterset_class=ScratchOrgFilter def perform_create(self, *args, **kwargs): if self.request.user.is_devhub_enabled: super().perform_create(*args, **kwargs) else: raise PermissionDenied( \"User is not connected to a Salesforce organization \" \"with Dev Hub enabled.\" ) def perform_destroy(self, instance): if self.request.user.sf_username==instance.owner_sf_id: instance.queue_delete() else: raise PermissionDenied( \"User is not connected to Salesforce as the same Salesforce user who \" \"created the ScratchOrg.\" ) def list(self, request, *args, **kwargs): queryset=self.filter_queryset(self.get_queryset()) filters={ \"owner\": request.user, \"org_type\": SCRATCH_ORG_TYPES.Dev, \"url__isnull\": False, \"delete_queued_at__isnull\": True, \"currently_capturing_changes\": False, \"currently_refreshing_changes\": False, } for instance in queryset.filter(**filters): instance.queue_get_unsaved_changes() serializer=self.get_serializer(queryset, many=True) return Response(serializer.data) def retrieve(self, request, *args, **kwargs): instance=self.get_object() conditions=[ instance.owner==request.user, instance.org_type==SCRATCH_ORG_TYPES.Dev, instance.url is not None, instance.delete_queued_at is None, not instance.currently_capturing_changes, not instance.currently_refreshing_changes, ] if all(conditions): instance.queue_get_unsaved_changes() serializer=self.get_serializer(instance) return Response(serializer.data) @action(detail=True, methods=[\"POST\"]) def commit(self, request, pk=None): serializer=CommitSerializer(data=request.data) if not serializer.is_valid(): return Response( serializer.errors, status=status.HTTP_422_UNPROCESSABLE_ENTITY ) scratch_org=self.get_object() if not request.user==scratch_org.owner: return Response( {\"error\": \"Requesting user did not create scratch org.\"}, status=status.HTTP_403_FORBIDDEN, ) commit_message=serializer.validated_data[\"commit_message\"] desired_changes=serializer.validated_data[\"changes\"] scratch_org.queue_commit_changes(request.user, desired_changes, commit_message) return Response( self.get_serializer(scratch_org).data, status=status.HTTP_202_ACCEPTED ) @action(detail=True, methods=[\"GET\"]) def redirect(self, request, pk=None): scratch_org=self.get_object() if not request.user==scratch_org.owner: return Response( {\"error\": \"Requesting user did not create scratch org.\"}, status=status.HTTP_403_FORBIDDEN, ) url=scratch_org.get_login_url() return HttpResponseRedirect(redirect_to=url) ","sourceWithComments":"from django.contrib.auth import get_user_model\nfrom django.http import HttpResponseRedirect\nfrom django_filters.rest_framework import DjangoFilterBackend\nfrom github3.exceptions import ResponseError\nfrom rest_framework import generics, status, viewsets\nfrom rest_framework.decorators import action\nfrom rest_framework.exceptions import PermissionDenied\nfrom rest_framework.permissions import AllowAny, IsAuthenticated\nfrom rest_framework.response import Response\nfrom rest_framework.views import APIView\n\nfrom .filters import ProjectFilter, RepositoryFilter, ScratchOrgFilter, TaskFilter\n\n# from .gh import get_repo_info\nfrom .models import SCRATCH_ORG_TYPES, Project, Repository, ScratchOrg, Task\nfrom .paginators import CustomPaginator\nfrom .serializers import (\n    CommitSerializer,\n    FullUserSerializer,\n    MinimalUserSerializer,\n    ProjectSerializer,\n    RepositorySerializer,\n    ScratchOrgSerializer,\n    TaskSerializer,\n)\n\nUser = get_user_model()\n\n\nclass CurrentUserObjectMixin:\n    def get_queryset(self):\n        return self.model.objects.filter(id=self.request.user.id)\n\n    def get_object(self):\n        return self.get_queryset().get()\n\n\nclass UserView(CurrentUserObjectMixin, generics.RetrieveAPIView):\n    \"\"\"\n    Shows the current user.\n    \"\"\"\n\n    model = User\n    serializer_class = FullUserSerializer\n    permission_classes = (IsAuthenticated,)\n\n\nclass UserRefreshView(CurrentUserObjectMixin, APIView):\n    model = User\n    permission_classes = (IsAuthenticated,)\n\n    def post(self, request):\n        from .jobs import refresh_github_repositories_for_user_job\n\n        user = self.get_object()\n        refresh_github_repositories_for_user_job.delay(user)\n        return Response(status=status.HTTP_202_ACCEPTED)\n\n\nclass UserDisconnectSFView(CurrentUserObjectMixin, APIView):\n    model = User\n    permission_classes = (IsAuthenticated,)\n\n    def post(self, request):\n        user = self.get_object()\n        user.invalidate_salesforce_credentials()\n        serializer = FullUserSerializer(user)\n        return Response(serializer.data, status=status.HTTP_200_OK)\n\n\nclass UserViewSet(viewsets.ModelViewSet):\n    permission_classes = (IsAuthenticated,)\n    serializer_class = MinimalUserSerializer\n    pagination_class = CustomPaginator\n    queryset = User.objects.all()\n\n\nclass RepositoryViewSet(viewsets.ModelViewSet):\n    permission_classes = (IsAuthenticated,)\n    serializer_class = RepositorySerializer\n    filter_backends = (DjangoFilterBackend,)\n    filterset_class = RepositoryFilter\n    pagination_class = CustomPaginator\n    model = Repository\n\n    def get_queryset(self):\n        repo_ids = self.request.user.repositories.values_list(\"repo_id\", flat=True)\n\n        for repo in Repository.objects.filter(repo_id__isnull=True):\n            try:\n                repo.get_repo_id(self.request.user)\n            except ResponseError:\n                pass\n\n        return Repository.objects.filter(repo_id__isnull=False, repo_id__in=repo_ids)\n\n    @action(methods=[\"POST\"], detail=True, permission_classes=[AllowAny])\n    def hook(self, request, pk=None):\n        # repository = self.get_object()\n        # repo = get_repo_info(self.request.user, repository.repo_id)\n        return Response(\"\", status=200)\n        # branch = \"\"\n        # list(repo.commits(sha=branch.latest_sha()))\n\n\nclass ProjectViewSet(viewsets.ModelViewSet):\n    permission_classes = (IsAuthenticated,)\n    serializer_class = ProjectSerializer\n    pagination_class = CustomPaginator\n    queryset = Project.objects.all()\n    filter_backends = (DjangoFilterBackend,)\n    filterset_class = ProjectFilter\n\n\nclass TaskViewSet(viewsets.ModelViewSet):\n    permission_classes = (IsAuthenticated,)\n    serializer_class = TaskSerializer\n    queryset = Task.objects.all()\n    filter_backends = (DjangoFilterBackend,)\n    filterset_class = TaskFilter\n\n\nclass ScratchOrgViewSet(viewsets.ModelViewSet):\n    permission_classes = (IsAuthenticated,)\n    serializer_class = ScratchOrgSerializer\n    queryset = ScratchOrg.objects.all()\n    filter_backends = (DjangoFilterBackend,)\n    filterset_class = ScratchOrgFilter\n\n    def perform_create(self, *args, **kwargs):\n        if self.request.user.is_devhub_enabled:\n            super().perform_create(*args, **kwargs)\n        else:\n            raise PermissionDenied(\n                \"User is not connected to a Salesforce organization \"\n                \"with Dev Hub enabled.\"\n            )\n\n    def perform_destroy(self, instance):\n        if self.request.user.sf_username == instance.owner_sf_id:\n            instance.queue_delete()\n        else:\n            raise PermissionDenied(\n                \"User is not connected to Salesforce as the same Salesforce user who \"\n                \"created the ScratchOrg.\"\n            )\n\n    def list(self, request, *args, **kwargs):\n        # XXX: This method is copied verbatim from\n        # rest_framework.mixins.RetrieveModelMixin, because I needed to\n        # insert the get_unsaved_changes line in the middle.\n        queryset = self.filter_queryset(self.get_queryset())\n\n        # XXX: I am apprehensive about the possibility of flooding the\n        # worker queues easily this way:\n        filters = {\n            \"owner\": request.user,\n            \"org_type\": SCRATCH_ORG_TYPES.Dev,\n            \"url__isnull\": False,\n            \"delete_queued_at__isnull\": True,\n            \"currently_capturing_changes\": False,\n            \"currently_refreshing_changes\": False,\n        }\n        for instance in queryset.filter(**filters):\n            instance.queue_get_unsaved_changes()\n\n        # XXX: If we ever paginate this endpoint, we will need to add\n        # pagination logic back in here.\n\n        serializer = self.get_serializer(queryset, many=True)\n        return Response(serializer.data)\n\n    def retrieve(self, request, *args, **kwargs):\n        # XXX: This method is adapted from\n        # rest_framework.mixins.RetrieveModelMixin, but one particular\n        # change: we needed to insert the get_unsaved_changes line in\n        # the middle.\n        instance = self.get_object()\n        conditions = [\n            instance.owner == request.user,\n            instance.org_type == SCRATCH_ORG_TYPES.Dev,\n            instance.url is not None,\n            instance.delete_queued_at is None,\n            not instance.currently_capturing_changes,\n            not instance.currently_refreshing_changes,\n        ]\n        if all(conditions):\n            instance.queue_get_unsaved_changes()\n        serializer = self.get_serializer(instance)\n        return Response(serializer.data)\n\n    @action(detail=True, methods=[\"POST\"])\n    def commit(self, request, pk=None):\n        serializer = CommitSerializer(data=request.data)\n        if not serializer.is_valid():\n            return Response(\n                serializer.errors, status=status.HTTP_422_UNPROCESSABLE_ENTITY\n            )\n\n        scratch_org = self.get_object()\n        if not request.user == scratch_org.owner:\n            return Response(\n                {\"error\": \"Requesting user did not create scratch org.\"},\n                status=status.HTTP_403_FORBIDDEN,\n            )\n        commit_message = serializer.validated_data[\"commit_message\"]\n        desired_changes = serializer.validated_data[\"changes\"]\n        scratch_org.queue_commit_changes(request.user, desired_changes, commit_message)\n        return Response(\n            self.get_serializer(scratch_org).data, status=status.HTTP_202_ACCEPTED\n        )\n\n    @action(detail=True, methods=[\"GET\"])\n    def redirect(self, request, pk=None):\n        scratch_org = self.get_object()\n        if not request.user == scratch_org.owner:\n            return Response(\n                {\"error\": \"Requesting user did not create scratch org.\"},\n                status=status.HTTP_403_FORBIDDEN,\n            )\n        url = scratch_org.get_login_url()\n        return HttpResponseRedirect(redirect_to=url)\n"}},"msg":"\ud83d\udee2 Track commits on Projects and Branches\n\nAnd update them when we get a GitHub webhook event.\n\nI think that there's a DOS vulnerability here in that we're not\ncurrently requiring that the hook validate that it's from GitHub itself,\nand anyone can just require us to hammer the GitHub API and flood our\nqueues."}}}