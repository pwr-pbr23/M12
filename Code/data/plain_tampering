{"https:\/\/github.com\/open-formulieren\/open-forms":{"d0b9e2b001570f578dcbd4897c94520e41ac15b2":{"url":"https:\/\/api.github.com\/repos\/open-formulieren\/open-forms\/commits\/d0b9e2b001570f578dcbd4897c94520e41ac15b2","html_url":"https:\/\/github.com\/open-formulieren\/open-forms\/commit\/d0b9e2b001570f578dcbd4897c94520e41ac15b2","sha":"d0b9e2b001570f578dcbd4897c94520e41ac15b2","keyword":"tampering check","diff":"diff --git a\/src\/openforms\/submissions\/tests\/factories.py b\/src\/openforms\/submissions\/tests\/factories.py\nindex 074048536c..7a10063b40 100644\n--- a\/src\/openforms\/submissions\/tests\/factories.py\n+++ b\/src\/openforms\/submissions\/tests\/factories.py\n@@ -77,7 +77,8 @@ def from_components(\n         **kwargs,\n     ) -> Submission:\n         \"\"\"\n-        generate a complete Form\/FormStep\/FormDefinition + Submission\/SubmissionStep tree from a list of formio components\n+        generate a complete Form\/FormStep\/FormDefinition + Submission\/SubmissionStep\n+        tree from a list of formio components\n \n         remember to generate from privates.test import temp_private_root\n         \"\"\"\n@@ -127,7 +128,9 @@ def from_data(data_dict: dict, **kwargs):\n \n class SubmissionStepFactory(factory.django.DjangoModelFactory):\n     submission = factory.SubFactory(SubmissionFactory)\n-    form_step = factory.SubFactory(FormStepFactory)\n+    form_step = factory.SubFactory(\n+        FormStepFactory, form=factory.SelfAttribute(\"..submission.form\")\n+    )\n \n     class Meta:\n         model = SubmissionStep\ndiff --git a\/src\/openforms\/submissions\/tests\/test_attachment_download_view.py b\/src\/openforms\/submissions\/tests\/test_attachment_download_view.py\nnew file mode 100644\nindex 0000000000..2662cc3af6\n--- \/dev\/null\n+++ b\/src\/openforms\/submissions\/tests\/test_attachment_download_view.py\n@@ -0,0 +1,82 @@\n+from django.test import TestCase, override_settings\n+from django.urls import reverse\n+\n+from furl import furl\n+from privates.test import temp_private_root\n+\n+from .factories import SubmissionFileAttachmentFactory\n+\n+\n+@override_settings(SENDFILE_BACKEND=\"django_sendfile.backends.nginx\")\n+@temp_private_root()\n+class SubmissionAttachmentDownloadTest(TestCase):\n+    def test_incomplete_submission_404(self):\n+        submission_file_attachment = SubmissionFileAttachmentFactory.create(\n+            submission_step__submission__completed=False,\n+        )\n+        path = reverse(\n+            \"submissions:attachment-download\",\n+            kwargs={\"uuid\": submission_file_attachment.uuid},\n+        )\n+        url = furl(path).add({\"hash\": \"dummy-hash\"})\n+\n+        response = self.client.get(url)\n+\n+        self.assertEqual(response.status_code, 404)\n+\n+    def test_submission_not_registered_yet_404(self):\n+        submission_file_attachments = [\n+            SubmissionFileAttachmentFactory.create(\n+                submission_step__submission__completed=True,\n+                submission_step__submission__registration_failed=True,\n+            ),\n+            SubmissionFileAttachmentFactory.create(\n+                submission_step__submission__completed=True,\n+                submission_step__submission__registration_pending=True,\n+            ),\n+            SubmissionFileAttachmentFactory.create(\n+                submission_step__submission__completed=True,\n+                submission_step__submission__registration_in_progress=True,\n+            ),\n+        ]\n+\n+        for submission_file_attachment in submission_file_attachments:\n+            with self.subTest(submission_file_attachment):\n+                path = reverse(\n+                    \"submissions:attachment-download\",\n+                    kwargs={\"uuid\": submission_file_attachment.uuid},\n+                )\n+                url = furl(path).add({\"hash\": \"dummy-hash\"})\n+\n+                response = self.client.get(url)\n+\n+                self.assertEqual(response.status_code, 404)\n+\n+    def test_valid_preconditions_missing_hash_403(self):\n+        submission_file_attachment = SubmissionFileAttachmentFactory.create(\n+            submission_step__submission__completed=True,\n+            submission_step__submission__registration_success=True,\n+        )\n+        path = reverse(\n+            \"submissions:attachment-download\",\n+            kwargs={\"uuid\": submission_file_attachment.uuid},\n+        )\n+\n+        response = self.client.get(path)\n+\n+        self.assertEqual(response.status_code, 403)\n+\n+    def test_valid_preconditions_invalid_hash_403(self):\n+        submission_file_attachment = SubmissionFileAttachmentFactory.create(\n+            submission_step__submission__completed=True,\n+            submission_step__submission__registration_success=True,\n+        )\n+        path = reverse(\n+            \"submissions:attachment-download\",\n+            kwargs={\"uuid\": submission_file_attachment.uuid},\n+        )\n+        url = furl(path).add({\"hash\": \"badhash\"})\n+\n+        response = self.client.get(url)\n+\n+        self.assertEqual(response.status_code, 403)\ndiff --git a\/src\/openforms\/submissions\/urls.py b\/src\/openforms\/submissions\/urls.py\nindex c044c39f91..486e1f1a9e 100644\n--- a\/src\/openforms\/submissions\/urls.py\n+++ b\/src\/openforms\/submissions\/urls.py\n@@ -1,6 +1,6 @@\n from django.urls import path\n \n-from .views import ResumeSubmissionView\n+from .views import ResumeSubmissionView, SubmissionAttachmentDownloadView\n \n app_name = \"submissions\"\n \n@@ -9,5 +9,10 @@\n         \"<uuid:submission_uuid>\/<str:token>\/resume\",\n         ResumeSubmissionView.as_view(),\n         name=\"resume\",\n-    )\n+    ),\n+    path(\n+        \"attachment\/<uuid:uuid>\/download\/\",\n+        SubmissionAttachmentDownloadView.as_view(),\n+        name=\"attachment-download\",\n+    ),\n ]\n","message":"","files":{"\/src\/openforms\/submissions\/tests\/factories.py":{"changes":[{"diff":"\n         **kwargs,\n     ) -> Submission:\n         \"\"\"\n-        generate a complete Form\/FormStep\/FormDefinition + Submission\/SubmissionStep tree from a list of formio components\n+        generate a complete Form\/FormStep\/FormDefinition + Submission\/SubmissionStep\n+        tree from a list of formio components\n \n         remember to generate from privates.test import temp_private_root\n         \"\"\"\n","add":2,"remove":1,"filename":"\/src\/openforms\/submissions\/tests\/factories.py","badparts":["        generate a complete Form\/FormStep\/FormDefinition + Submission\/SubmissionStep tree from a list of formio components"],"goodparts":["        generate a complete Form\/FormStep\/FormDefinition + Submission\/SubmissionStep","        tree from a list of formio components"]},{"diff":"\n \n class SubmissionStepFactory(factory.django.DjangoModelFactory):\n     submission = factory.SubFactory(SubmissionFactory)\n-    form_step = factory.SubFactory(FormStepFactory)\n+    form_step = factory.SubFactory(\n+        FormStepFactory, form=factory.SelfAttribute(\"..submission.form\")\n+    )\n \n     class Meta:\n         model = SubmissionStep","add":3,"remove":1,"filename":"\/src\/openforms\/submissions\/tests\/factories.py","badparts":["    form_step = factory.SubFactory(FormStepFactory)"],"goodparts":["    form_step = factory.SubFactory(","        FormStepFactory, form=factory.SelfAttribute(\"..submission.form\")","    )"]}],"source":"\nimport copy from datetime import timedelta from typing import List from django.utils import timezone import factory from openforms.forms.tests.factories import( FormDefinitionFactory, FormFactory, FormStepFactory, ) from..constants import RegistrationStatuses from..models import( Submission, SubmissionFileAttachment, SubmissionReport, SubmissionStep, TemporaryFileUpload, ) class SubmissionFactory(factory.django.DjangoModelFactory): form=factory.SubFactory(FormFactory) class Meta: model=Submission class Params: completed=factory.Trait( completed_on=factory.Faker(\"date_time_this_month\", tzinfo=timezone.utc), created_on=factory.LazyAttribute( lambda s: s.completed_on -timedelta(hours=4) ), price=factory.PostGenerationMethodCall(\"calculate_price\"), ) registration_failed=factory.Trait( completed=True, last_register_date=factory.LazyFunction(timezone.now), registration_status=RegistrationStatuses.failed, ) registration_success=factory.Trait( completed=True, last_register_date=factory.LazyFunction(timezone.now), registration_status=RegistrationStatuses.success, ) registration_pending=factory.Trait( completed=True, last_register_date=None, registration_status=RegistrationStatuses.pending, ) registration_in_progress=factory.Trait( completed=True, last_register_date=factory.LazyFunction(timezone.now), registration_status=RegistrationStatuses.in_progress, ) has_previous_submission=factory.Trait( previous_submission=factory.SubFactory( \"openforms.submissions.tests.factories.SubmissionFactory\", form=factory.SelfAttribute(\"..form\"), ) ) with_report=factory.Trait( report=factory.RelatedFactory( \"openforms.submissions.tests.factories.SubmissionReportFactory\", factory_related_name=\"submission\", ) ) @classmethod def from_components( cls, components_list: List[dict], submitted_data: dict=None, **kwargs, ) -> Submission: \"\"\" generate a complete Form\/FormStep\/FormDefinition +Submission\/SubmissionStep tree from a list of formio components remember to generate from privates.test import temp_private_root \"\"\" kwargs.setdefault(\"with_report\", True) submission=cls.create(**kwargs) form=submission.form components=list() for _comp in components_list: component=copy.deepcopy(_comp) key=component[\"key\"] if not component.get(\"label\"): component[\"label\"]=key.title() if not component.get(\"type\"): component[\"type\"]=\"text\" components.append(component) configuration={\"components\": components} form_definition=FormDefinitionFactory.create( name=f\"definition-{key}\", configuration=configuration ) form_step=FormStepFactory.create(form=form, form_definition=form_definition) SubmissionStepFactory.create( submission=submission, form_step=form_step, data=submitted_data ) return submission @staticmethod def from_data(data_dict: dict, **kwargs): components=[ { \"key\": key, } for key in data_dict ] return SubmissionFactory.from_components( components, data_dict, **kwargs, ) class SubmissionStepFactory(factory.django.DjangoModelFactory): submission=factory.SubFactory(SubmissionFactory) form_step=factory.SubFactory(FormStepFactory) class Meta: model=SubmissionStep class SubmissionReportFactory(factory.django.DjangoModelFactory): title=factory.Faker(\"bs\") content=factory.django.FileField(filename=\"submission_report.pdf\") submission=factory.SubFactory(SubmissionFactory) class Meta: model=SubmissionReport class TemporaryFileUploadFactory(factory.django.DjangoModelFactory): file_name=factory.Faker(\"file_name\") content_type=factory.Faker(\"mime_type\") content=factory.django.FileField(filename=\"file.dat\", data=b\"content\") class Meta: model=TemporaryFileUpload class SubmissionFileAttachmentFactory(factory.django.DjangoModelFactory): submission_step=factory.SubFactory(SubmissionStepFactory) temporary_file=factory.SubFactory(TemporaryFileUploadFactory) content=factory.django.FileField(filename=\"attachment.pdf\", data=b\"content\") form_key=factory.Faker(\"bs\") file_name=factory.Faker(\"file_name\") original_name=factory.Faker(\"file_name\") content_type=factory.Faker(\"mime_type\") class Meta: model=SubmissionFileAttachment ","sourceWithComments":"import copy\nfrom datetime import timedelta\nfrom typing import List\n\nfrom django.utils import timezone\n\nimport factory\n\nfrom openforms.forms.tests.factories import (\n    FormDefinitionFactory,\n    FormFactory,\n    FormStepFactory,\n)\n\nfrom ..constants import RegistrationStatuses\nfrom ..models import (\n    Submission,\n    SubmissionFileAttachment,\n    SubmissionReport,\n    SubmissionStep,\n    TemporaryFileUpload,\n)\n\n\nclass SubmissionFactory(factory.django.DjangoModelFactory):\n    form = factory.SubFactory(FormFactory)\n\n    class Meta:\n        model = Submission\n\n    class Params:\n        completed = factory.Trait(\n            completed_on=factory.Faker(\"date_time_this_month\", tzinfo=timezone.utc),\n            created_on=factory.LazyAttribute(\n                lambda s: s.completed_on - timedelta(hours=4)\n            ),\n            price=factory.PostGenerationMethodCall(\"calculate_price\"),\n        )\n        registration_failed = factory.Trait(\n            completed=True,\n            last_register_date=factory.LazyFunction(timezone.now),\n            registration_status=RegistrationStatuses.failed,\n        )\n        registration_success = factory.Trait(\n            completed=True,\n            last_register_date=factory.LazyFunction(timezone.now),\n            registration_status=RegistrationStatuses.success,\n        )\n        registration_pending = factory.Trait(\n            completed=True,\n            last_register_date=None,\n            registration_status=RegistrationStatuses.pending,\n        )\n        registration_in_progress = factory.Trait(\n            completed=True,\n            last_register_date=factory.LazyFunction(timezone.now),\n            registration_status=RegistrationStatuses.in_progress,\n        )\n        has_previous_submission = factory.Trait(\n            previous_submission=factory.SubFactory(\n                \"openforms.submissions.tests.factories.SubmissionFactory\",\n                form=factory.SelfAttribute(\"..form\"),\n            )\n        )\n        with_report = factory.Trait(\n            report=factory.RelatedFactory(\n                \"openforms.submissions.tests.factories.SubmissionReportFactory\",\n                factory_related_name=\"submission\",\n            )\n        )\n\n    @classmethod\n    def from_components(\n        cls,\n        components_list: List[dict],\n        submitted_data: dict = None,\n        **kwargs,\n    ) -> Submission:\n        \"\"\"\n        generate a complete Form\/FormStep\/FormDefinition + Submission\/SubmissionStep tree from a list of formio components\n\n        remember to generate from privates.test import temp_private_root\n        \"\"\"\n        kwargs.setdefault(\"with_report\", True)\n        submission = cls.create(**kwargs)\n        form = submission.form\n\n        components = list()\n\n        for _comp in components_list:\n            component = copy.deepcopy(_comp)\n            key = component[\"key\"]\n            # convenience\n            if not component.get(\"label\"):\n                component[\"label\"] = key.title()\n            if not component.get(\"type\"):\n                component[\"type\"] = \"text\"\n\n            components.append(component)\n\n        configuration = {\"components\": components}\n\n        form_definition = FormDefinitionFactory.create(\n            name=f\"definition-{key}\", configuration=configuration\n        )\n        form_step = FormStepFactory.create(form=form, form_definition=form_definition)\n        SubmissionStepFactory.create(\n            submission=submission, form_step=form_step, data=submitted_data\n        )\n\n        return submission\n\n    @staticmethod\n    def from_data(data_dict: dict, **kwargs):\n        components = [\n            {\n                \"key\": key,\n            }\n            for key in data_dict\n        ]\n        return SubmissionFactory.from_components(\n            components,\n            data_dict,\n            **kwargs,\n        )\n\n\nclass SubmissionStepFactory(factory.django.DjangoModelFactory):\n    submission = factory.SubFactory(SubmissionFactory)\n    form_step = factory.SubFactory(FormStepFactory)\n\n    class Meta:\n        model = SubmissionStep\n\n\nclass SubmissionReportFactory(factory.django.DjangoModelFactory):\n    title = factory.Faker(\"bs\")\n    content = factory.django.FileField(filename=\"submission_report.pdf\")\n    submission = factory.SubFactory(SubmissionFactory)\n\n    class Meta:\n        model = SubmissionReport\n\n\nclass TemporaryFileUploadFactory(factory.django.DjangoModelFactory):\n    file_name = factory.Faker(\"file_name\")\n    content_type = factory.Faker(\"mime_type\")\n    content = factory.django.FileField(filename=\"file.dat\", data=b\"content\")\n\n    class Meta:\n        model = TemporaryFileUpload\n\n\nclass SubmissionFileAttachmentFactory(factory.django.DjangoModelFactory):\n    submission_step = factory.SubFactory(SubmissionStepFactory)\n    temporary_file = factory.SubFactory(TemporaryFileUploadFactory)\n    content = factory.django.FileField(filename=\"attachment.pdf\", data=b\"content\")\n    form_key = factory.Faker(\"bs\")\n    file_name = factory.Faker(\"file_name\")\n    original_name = factory.Faker(\"file_name\")\n    content_type = factory.Faker(\"mime_type\")\n\n    class Meta:\n        model = SubmissionFileAttachment\n"},"\/src\/openforms\/submissions\/urls.py":{"changes":[{"diff":"\n from django.urls import path\n \n-from .views import ResumeSubmissionView\n+from .views import ResumeSubmissionView, SubmissionAttachmentDownloadView\n \n app_name = \"submissions\"\n \n","add":1,"remove":1,"filename":"\/src\/openforms\/submissions\/urls.py","badparts":["from .views import ResumeSubmissionView"],"goodparts":["from .views import ResumeSubmissionView, SubmissionAttachmentDownloadView"]},{"diff":"         \"<uuid:submission_uuid>\/<str:token>\/resume\",\n         ResumeSubmissionView.as_view(),\n         name=\"resume\",\n-    )\n+    ),\n+    path(\n+        \"attachment\/<uuid:uuid>\/download\/\",\n+        SubmissionAttachmentDownloadView.as_view(),\n+        name=\"attachment-download\",\n+    ),\n ]\n","add":6,"remove":1,"filename":"\/src\/openforms\/submissions\/urls.py","badparts":["    )"],"goodparts":["    ),","    path(","        \"attachment\/<uuid:uuid>\/download\/\",","        SubmissionAttachmentDownloadView.as_view(),","        name=\"attachment-download\",","    ),"]}],"source":"\nfrom django.urls import path from.views import ResumeSubmissionView app_name=\"submissions\" urlpatterns=[ path( \"<uuid:submission_uuid>\/<str:token>\/resume\", ResumeSubmissionView.as_view(), name=\"resume\", ) ] ","sourceWithComments":"from django.urls import path\n\nfrom .views import ResumeSubmissionView\n\napp_name = \"submissions\"\n\nurlpatterns = [\n    path(\n        \"<uuid:submission_uuid>\/<str:token>\/resume\",\n        ResumeSubmissionView.as_view(),\n        name=\"resume\",\n    )\n]\n"}},"msg":":white_check_mark: [#1193] -- added error-flow tests for file downloads\n\n* Test for valid preconditions\n* Test for missing\/tampered content hashes"}},"https:\/\/github.com\/TWTx\/openForms":{"d0b9e2b001570f578dcbd4897c94520e41ac15b2":{"url":"https:\/\/api.github.com\/repos\/TWTx\/openForms\/commits\/d0b9e2b001570f578dcbd4897c94520e41ac15b2","html_url":"https:\/\/github.com\/TWTx\/openForms\/commit\/d0b9e2b001570f578dcbd4897c94520e41ac15b2","sha":"d0b9e2b001570f578dcbd4897c94520e41ac15b2","keyword":"tampering check","diff":"diff --git a\/src\/openforms\/submissions\/tests\/factories.py b\/src\/openforms\/submissions\/tests\/factories.py\nindex 07404853..7a10063b 100644\n--- a\/src\/openforms\/submissions\/tests\/factories.py\n+++ b\/src\/openforms\/submissions\/tests\/factories.py\n@@ -77,7 +77,8 @@ def from_components(\n         **kwargs,\n     ) -> Submission:\n         \"\"\"\n-        generate a complete Form\/FormStep\/FormDefinition + Submission\/SubmissionStep tree from a list of formio components\n+        generate a complete Form\/FormStep\/FormDefinition + Submission\/SubmissionStep\n+        tree from a list of formio components\n \n         remember to generate from privates.test import temp_private_root\n         \"\"\"\n@@ -127,7 +128,9 @@ def from_data(data_dict: dict, **kwargs):\n \n class SubmissionStepFactory(factory.django.DjangoModelFactory):\n     submission = factory.SubFactory(SubmissionFactory)\n-    form_step = factory.SubFactory(FormStepFactory)\n+    form_step = factory.SubFactory(\n+        FormStepFactory, form=factory.SelfAttribute(\"..submission.form\")\n+    )\n \n     class Meta:\n         model = SubmissionStep\ndiff --git a\/src\/openforms\/submissions\/tests\/test_attachment_download_view.py b\/src\/openforms\/submissions\/tests\/test_attachment_download_view.py\nnew file mode 100644\nindex 00000000..2662cc3a\n--- \/dev\/null\n+++ b\/src\/openforms\/submissions\/tests\/test_attachment_download_view.py\n@@ -0,0 +1,82 @@\n+from django.test import TestCase, override_settings\n+from django.urls import reverse\n+\n+from furl import furl\n+from privates.test import temp_private_root\n+\n+from .factories import SubmissionFileAttachmentFactory\n+\n+\n+@override_settings(SENDFILE_BACKEND=\"django_sendfile.backends.nginx\")\n+@temp_private_root()\n+class SubmissionAttachmentDownloadTest(TestCase):\n+    def test_incomplete_submission_404(self):\n+        submission_file_attachment = SubmissionFileAttachmentFactory.create(\n+            submission_step__submission__completed=False,\n+        )\n+        path = reverse(\n+            \"submissions:attachment-download\",\n+            kwargs={\"uuid\": submission_file_attachment.uuid},\n+        )\n+        url = furl(path).add({\"hash\": \"dummy-hash\"})\n+\n+        response = self.client.get(url)\n+\n+        self.assertEqual(response.status_code, 404)\n+\n+    def test_submission_not_registered_yet_404(self):\n+        submission_file_attachments = [\n+            SubmissionFileAttachmentFactory.create(\n+                submission_step__submission__completed=True,\n+                submission_step__submission__registration_failed=True,\n+            ),\n+            SubmissionFileAttachmentFactory.create(\n+                submission_step__submission__completed=True,\n+                submission_step__submission__registration_pending=True,\n+            ),\n+            SubmissionFileAttachmentFactory.create(\n+                submission_step__submission__completed=True,\n+                submission_step__submission__registration_in_progress=True,\n+            ),\n+        ]\n+\n+        for submission_file_attachment in submission_file_attachments:\n+            with self.subTest(submission_file_attachment):\n+                path = reverse(\n+                    \"submissions:attachment-download\",\n+                    kwargs={\"uuid\": submission_file_attachment.uuid},\n+                )\n+                url = furl(path).add({\"hash\": \"dummy-hash\"})\n+\n+                response = self.client.get(url)\n+\n+                self.assertEqual(response.status_code, 404)\n+\n+    def test_valid_preconditions_missing_hash_403(self):\n+        submission_file_attachment = SubmissionFileAttachmentFactory.create(\n+            submission_step__submission__completed=True,\n+            submission_step__submission__registration_success=True,\n+        )\n+        path = reverse(\n+            \"submissions:attachment-download\",\n+            kwargs={\"uuid\": submission_file_attachment.uuid},\n+        )\n+\n+        response = self.client.get(path)\n+\n+        self.assertEqual(response.status_code, 403)\n+\n+    def test_valid_preconditions_invalid_hash_403(self):\n+        submission_file_attachment = SubmissionFileAttachmentFactory.create(\n+            submission_step__submission__completed=True,\n+            submission_step__submission__registration_success=True,\n+        )\n+        path = reverse(\n+            \"submissions:attachment-download\",\n+            kwargs={\"uuid\": submission_file_attachment.uuid},\n+        )\n+        url = furl(path).add({\"hash\": \"badhash\"})\n+\n+        response = self.client.get(url)\n+\n+        self.assertEqual(response.status_code, 403)\ndiff --git a\/src\/openforms\/submissions\/urls.py b\/src\/openforms\/submissions\/urls.py\nindex c044c39f..486e1f1a 100644\n--- a\/src\/openforms\/submissions\/urls.py\n+++ b\/src\/openforms\/submissions\/urls.py\n@@ -1,6 +1,6 @@\n from django.urls import path\n \n-from .views import ResumeSubmissionView\n+from .views import ResumeSubmissionView, SubmissionAttachmentDownloadView\n \n app_name = \"submissions\"\n \n@@ -9,5 +9,10 @@\n         \"<uuid:submission_uuid>\/<str:token>\/resume\",\n         ResumeSubmissionView.as_view(),\n         name=\"resume\",\n-    )\n+    ),\n+    path(\n+        \"attachment\/<uuid:uuid>\/download\/\",\n+        SubmissionAttachmentDownloadView.as_view(),\n+        name=\"attachment-download\",\n+    ),\n ]\n","message":"","files":{"\/src\/openforms\/submissions\/tests\/factories.py":{"changes":[{"diff":"\n         **kwargs,\n     ) -> Submission:\n         \"\"\"\n-        generate a complete Form\/FormStep\/FormDefinition + Submission\/SubmissionStep tree from a list of formio components\n+        generate a complete Form\/FormStep\/FormDefinition + Submission\/SubmissionStep\n+        tree from a list of formio components\n \n         remember to generate from privates.test import temp_private_root\n         \"\"\"\n","add":2,"remove":1,"filename":"\/src\/openforms\/submissions\/tests\/factories.py","badparts":["        generate a complete Form\/FormStep\/FormDefinition + Submission\/SubmissionStep tree from a list of formio components"],"goodparts":["        generate a complete Form\/FormStep\/FormDefinition + Submission\/SubmissionStep","        tree from a list of formio components"]},{"diff":"\n \n class SubmissionStepFactory(factory.django.DjangoModelFactory):\n     submission = factory.SubFactory(SubmissionFactory)\n-    form_step = factory.SubFactory(FormStepFactory)\n+    form_step = factory.SubFactory(\n+        FormStepFactory, form=factory.SelfAttribute(\"..submission.form\")\n+    )\n \n     class Meta:\n         model = SubmissionStep","add":3,"remove":1,"filename":"\/src\/openforms\/submissions\/tests\/factories.py","badparts":["    form_step = factory.SubFactory(FormStepFactory)"],"goodparts":["    form_step = factory.SubFactory(","        FormStepFactory, form=factory.SelfAttribute(\"..submission.form\")","    )"]}],"source":"\nimport copy from datetime import timedelta from typing import List from django.utils import timezone import factory from openforms.forms.tests.factories import( FormDefinitionFactory, FormFactory, FormStepFactory, ) from..constants import RegistrationStatuses from..models import( Submission, SubmissionFileAttachment, SubmissionReport, SubmissionStep, TemporaryFileUpload, ) class SubmissionFactory(factory.django.DjangoModelFactory): form=factory.SubFactory(FormFactory) class Meta: model=Submission class Params: completed=factory.Trait( completed_on=factory.Faker(\"date_time_this_month\", tzinfo=timezone.utc), created_on=factory.LazyAttribute( lambda s: s.completed_on -timedelta(hours=4) ), price=factory.PostGenerationMethodCall(\"calculate_price\"), ) registration_failed=factory.Trait( completed=True, last_register_date=factory.LazyFunction(timezone.now), registration_status=RegistrationStatuses.failed, ) registration_success=factory.Trait( completed=True, last_register_date=factory.LazyFunction(timezone.now), registration_status=RegistrationStatuses.success, ) registration_pending=factory.Trait( completed=True, last_register_date=None, registration_status=RegistrationStatuses.pending, ) registration_in_progress=factory.Trait( completed=True, last_register_date=factory.LazyFunction(timezone.now), registration_status=RegistrationStatuses.in_progress, ) has_previous_submission=factory.Trait( previous_submission=factory.SubFactory( \"openforms.submissions.tests.factories.SubmissionFactory\", form=factory.SelfAttribute(\"..form\"), ) ) with_report=factory.Trait( report=factory.RelatedFactory( \"openforms.submissions.tests.factories.SubmissionReportFactory\", factory_related_name=\"submission\", ) ) @classmethod def from_components( cls, components_list: List[dict], submitted_data: dict=None, **kwargs, ) -> Submission: \"\"\" generate a complete Form\/FormStep\/FormDefinition +Submission\/SubmissionStep tree from a list of formio components remember to generate from privates.test import temp_private_root \"\"\" kwargs.setdefault(\"with_report\", True) submission=cls.create(**kwargs) form=submission.form components=list() for _comp in components_list: component=copy.deepcopy(_comp) key=component[\"key\"] if not component.get(\"label\"): component[\"label\"]=key.title() if not component.get(\"type\"): component[\"type\"]=\"text\" components.append(component) configuration={\"components\": components} form_definition=FormDefinitionFactory.create( name=f\"definition-{key}\", configuration=configuration ) form_step=FormStepFactory.create(form=form, form_definition=form_definition) SubmissionStepFactory.create( submission=submission, form_step=form_step, data=submitted_data ) return submission @staticmethod def from_data(data_dict: dict, **kwargs): components=[ { \"key\": key, } for key in data_dict ] return SubmissionFactory.from_components( components, data_dict, **kwargs, ) class SubmissionStepFactory(factory.django.DjangoModelFactory): submission=factory.SubFactory(SubmissionFactory) form_step=factory.SubFactory(FormStepFactory) class Meta: model=SubmissionStep class SubmissionReportFactory(factory.django.DjangoModelFactory): title=factory.Faker(\"bs\") content=factory.django.FileField(filename=\"submission_report.pdf\") submission=factory.SubFactory(SubmissionFactory) class Meta: model=SubmissionReport class TemporaryFileUploadFactory(factory.django.DjangoModelFactory): file_name=factory.Faker(\"file_name\") content_type=factory.Faker(\"mime_type\") content=factory.django.FileField(filename=\"file.dat\", data=b\"content\") class Meta: model=TemporaryFileUpload class SubmissionFileAttachmentFactory(factory.django.DjangoModelFactory): submission_step=factory.SubFactory(SubmissionStepFactory) temporary_file=factory.SubFactory(TemporaryFileUploadFactory) content=factory.django.FileField(filename=\"attachment.pdf\", data=b\"content\") form_key=factory.Faker(\"bs\") file_name=factory.Faker(\"file_name\") original_name=factory.Faker(\"file_name\") content_type=factory.Faker(\"mime_type\") class Meta: model=SubmissionFileAttachment ","sourceWithComments":"import copy\nfrom datetime import timedelta\nfrom typing import List\n\nfrom django.utils import timezone\n\nimport factory\n\nfrom openforms.forms.tests.factories import (\n    FormDefinitionFactory,\n    FormFactory,\n    FormStepFactory,\n)\n\nfrom ..constants import RegistrationStatuses\nfrom ..models import (\n    Submission,\n    SubmissionFileAttachment,\n    SubmissionReport,\n    SubmissionStep,\n    TemporaryFileUpload,\n)\n\n\nclass SubmissionFactory(factory.django.DjangoModelFactory):\n    form = factory.SubFactory(FormFactory)\n\n    class Meta:\n        model = Submission\n\n    class Params:\n        completed = factory.Trait(\n            completed_on=factory.Faker(\"date_time_this_month\", tzinfo=timezone.utc),\n            created_on=factory.LazyAttribute(\n                lambda s: s.completed_on - timedelta(hours=4)\n            ),\n            price=factory.PostGenerationMethodCall(\"calculate_price\"),\n        )\n        registration_failed = factory.Trait(\n            completed=True,\n            last_register_date=factory.LazyFunction(timezone.now),\n            registration_status=RegistrationStatuses.failed,\n        )\n        registration_success = factory.Trait(\n            completed=True,\n            last_register_date=factory.LazyFunction(timezone.now),\n            registration_status=RegistrationStatuses.success,\n        )\n        registration_pending = factory.Trait(\n            completed=True,\n            last_register_date=None,\n            registration_status=RegistrationStatuses.pending,\n        )\n        registration_in_progress = factory.Trait(\n            completed=True,\n            last_register_date=factory.LazyFunction(timezone.now),\n            registration_status=RegistrationStatuses.in_progress,\n        )\n        has_previous_submission = factory.Trait(\n            previous_submission=factory.SubFactory(\n                \"openforms.submissions.tests.factories.SubmissionFactory\",\n                form=factory.SelfAttribute(\"..form\"),\n            )\n        )\n        with_report = factory.Trait(\n            report=factory.RelatedFactory(\n                \"openforms.submissions.tests.factories.SubmissionReportFactory\",\n                factory_related_name=\"submission\",\n            )\n        )\n\n    @classmethod\n    def from_components(\n        cls,\n        components_list: List[dict],\n        submitted_data: dict = None,\n        **kwargs,\n    ) -> Submission:\n        \"\"\"\n        generate a complete Form\/FormStep\/FormDefinition + Submission\/SubmissionStep tree from a list of formio components\n\n        remember to generate from privates.test import temp_private_root\n        \"\"\"\n        kwargs.setdefault(\"with_report\", True)\n        submission = cls.create(**kwargs)\n        form = submission.form\n\n        components = list()\n\n        for _comp in components_list:\n            component = copy.deepcopy(_comp)\n            key = component[\"key\"]\n            # convenience\n            if not component.get(\"label\"):\n                component[\"label\"] = key.title()\n            if not component.get(\"type\"):\n                component[\"type\"] = \"text\"\n\n            components.append(component)\n\n        configuration = {\"components\": components}\n\n        form_definition = FormDefinitionFactory.create(\n            name=f\"definition-{key}\", configuration=configuration\n        )\n        form_step = FormStepFactory.create(form=form, form_definition=form_definition)\n        SubmissionStepFactory.create(\n            submission=submission, form_step=form_step, data=submitted_data\n        )\n\n        return submission\n\n    @staticmethod\n    def from_data(data_dict: dict, **kwargs):\n        components = [\n            {\n                \"key\": key,\n            }\n            for key in data_dict\n        ]\n        return SubmissionFactory.from_components(\n            components,\n            data_dict,\n            **kwargs,\n        )\n\n\nclass SubmissionStepFactory(factory.django.DjangoModelFactory):\n    submission = factory.SubFactory(SubmissionFactory)\n    form_step = factory.SubFactory(FormStepFactory)\n\n    class Meta:\n        model = SubmissionStep\n\n\nclass SubmissionReportFactory(factory.django.DjangoModelFactory):\n    title = factory.Faker(\"bs\")\n    content = factory.django.FileField(filename=\"submission_report.pdf\")\n    submission = factory.SubFactory(SubmissionFactory)\n\n    class Meta:\n        model = SubmissionReport\n\n\nclass TemporaryFileUploadFactory(factory.django.DjangoModelFactory):\n    file_name = factory.Faker(\"file_name\")\n    content_type = factory.Faker(\"mime_type\")\n    content = factory.django.FileField(filename=\"file.dat\", data=b\"content\")\n\n    class Meta:\n        model = TemporaryFileUpload\n\n\nclass SubmissionFileAttachmentFactory(factory.django.DjangoModelFactory):\n    submission_step = factory.SubFactory(SubmissionStepFactory)\n    temporary_file = factory.SubFactory(TemporaryFileUploadFactory)\n    content = factory.django.FileField(filename=\"attachment.pdf\", data=b\"content\")\n    form_key = factory.Faker(\"bs\")\n    file_name = factory.Faker(\"file_name\")\n    original_name = factory.Faker(\"file_name\")\n    content_type = factory.Faker(\"mime_type\")\n\n    class Meta:\n        model = SubmissionFileAttachment\n"},"\/src\/openforms\/submissions\/urls.py":{"changes":[{"diff":"\n from django.urls import path\n \n-from .views import ResumeSubmissionView\n+from .views import ResumeSubmissionView, SubmissionAttachmentDownloadView\n \n app_name = \"submissions\"\n \n","add":1,"remove":1,"filename":"\/src\/openforms\/submissions\/urls.py","badparts":["from .views import ResumeSubmissionView"],"goodparts":["from .views import ResumeSubmissionView, SubmissionAttachmentDownloadView"]},{"diff":"         \"<uuid:submission_uuid>\/<str:token>\/resume\",\n         ResumeSubmissionView.as_view(),\n         name=\"resume\",\n-    )\n+    ),\n+    path(\n+        \"attachment\/<uuid:uuid>\/download\/\",\n+        SubmissionAttachmentDownloadView.as_view(),\n+        name=\"attachment-download\",\n+    ),\n ]\n","add":6,"remove":1,"filename":"\/src\/openforms\/submissions\/urls.py","badparts":["    )"],"goodparts":["    ),","    path(","        \"attachment\/<uuid:uuid>\/download\/\",","        SubmissionAttachmentDownloadView.as_view(),","        name=\"attachment-download\",","    ),"]}],"source":"\nfrom django.urls import path from.views import ResumeSubmissionView app_name=\"submissions\" urlpatterns=[ path( \"<uuid:submission_uuid>\/<str:token>\/resume\", ResumeSubmissionView.as_view(), name=\"resume\", ) ] ","sourceWithComments":"from django.urls import path\n\nfrom .views import ResumeSubmissionView\n\napp_name = \"submissions\"\n\nurlpatterns = [\n    path(\n        \"<uuid:submission_uuid>\/<str:token>\/resume\",\n        ResumeSubmissionView.as_view(),\n        name=\"resume\",\n    )\n]\n"}},"msg":":white_check_mark: [#1193] -- added error-flow tests for file downloads\n\n* Test for valid preconditions\n* Test for missing\/tampered content hashes"}},"https:\/\/github.com\/oshogbo\/gramine-wips":{"567790aec46e0714171121318783b8ac8c56adc3":{"url":"https:\/\/api.github.com\/repos\/oshogbo\/gramine-wips\/commits\/567790aec46e0714171121318783b8ac8c56adc3","html_url":"https:\/\/github.com\/oshogbo\/gramine-wips\/commit\/567790aec46e0714171121318783b8ac8c56adc3","sha":"567790aec46e0714171121318783b8ac8c56adc3","keyword":"tampering malicious","diff":"diff --git a\/LibOS\/shim\/test\/fs\/test_pf.py b\/LibOS\/shim\/test\/fs\/test_pf.py\nindex e430e02af..7b1621d7a 100644\n--- a\/LibOS\/shim\/test\/fs\/test_pf.py\n+++ b\/LibOS\/shim\/test\/fs\/test_pf.py\n@@ -193,24 +193,20 @@ def test_210_copy_dir_mounted(self):\n \n     def __corrupt_file(self, input_path, output_path):\n         cmd = [self.PF_TAMPER, '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path]\n-        return self.run_native_binary(cmd)\n+        return self.run_native_binary(cmd, libpath=os.path.join(os.getcwd(), 'lib'))\n \n     # invalid\/corrupted files\n-    @expectedFailureIf(HAS_SGX)\n-    # pylint: disable=fixme\n     def test_500_invalid(self):\n-        # TODO: port these to the new file format\n         invalid_dir = os.path.join(self.TEST_DIR, 'pf_invalid')\n-        # files below should work normally (benign modifications)\n-        should_pass = ['chunk_padding_1_fixed', 'chunk_padding_2_fixed', 'chunk_data_3',\n-                       'chunk_data_3_fixed', 'chunk_data_4', 'chunk_data_4_fixed']\n         if not os.path.exists(invalid_dir):\n             os.mkdir(invalid_dir)\n+\n         # prepare valid encrypted file (largest one for maximum possible corruptions)\n         original_input = self.OUTPUT_FILES[-1]\n         self.__encrypt_file(self.INPUT_FILES[-1], original_input)\n         # generate invalid files based on the above\n         self.__corrupt_file(original_input, invalid_dir)\n+\n         # try to decrypt invalid files\n         for name in os.listdir(invalid_dir):\n             invalid = os.path.join(invalid_dir, name)\n@@ -218,17 +214,13 @@ def test_500_invalid(self):\n             input_path = os.path.join(invalid_dir, os.path.basename(original_input))\n             # copy the file so it has the original file name (for allowed path check)\n             shutil.copy(invalid, input_path)\n-            should_pass = any(s in name for s in should_pass)\n \n             try:\n                 args = ['decrypt', '-V', '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path]\n                 self.__pf_crypt(args)\n             except subprocess.CalledProcessError as exc:\n-                if should_pass:\n-                    self.assertEqual(exc.returncode, 0)\n-                else:\n-                    self.assertNotEqual(exc.returncode, 0)\n+                # decryption of invalid file must fail with -1 (wrapped to 255)\n+                self.assertEqual(exc.returncode, 255)\n             else:\n-                if not should_pass:\n-                    print('[!] Fail: successfully decrypted file: ' + name)\n-                    self.fail()\n+                print('[!] Fail: successfully decrypted file: ' + name)\n+                self.fail()\ndiff --git a\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files.c b\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files.c\nindex 4f0c59ba2..d020a1e24 100644\n--- a\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files.c\n+++ b\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files.c\n@@ -5,6 +5,8 @@\n  *\/\n \n #include \"api.h\"\n+#include \"protected_files.h\"\n+#include \"protected_files_format.h\"\n #include \"protected_files_internal.h\"\n \n #ifndef IN_PAL\n@@ -65,20 +67,6 @@ static pf_random_f          g_cb_random          = NULL;\n static pf_iv_t g_empty_iv = {0};\n static bool g_initialized = false;\n \n-#define METADATA_KEY_NAME \"SGX-PROTECTED-FS-METADATA-KEY\"\n-#define MAX_LABEL_SIZE    64\n-\n-static_assert(sizeof(METADATA_KEY_NAME) <= MAX_LABEL_SIZE, \"label too long\");\n-\n-#pragma pack(push, 1)\n-typedef struct {\n-    uint32_t index;\n-    char label[MAX_LABEL_SIZE]; \/\/ must be NULL terminated\n-    pf_keyid_t nonce;\n-    uint32_t output_len; \/\/ in bits\n-} kdf_input_t;\n-#pragma pack(pop)\n-\n \/\/ The key derivation function follow recommendations from NIST Special Publication 800-108:\n \/\/ Recommendation for Key Derivation Using Pseudorandom Functions\n \/\/ https:\/\/nvlpubs.nist.gov\/nistpubs\/Legacy\/SP\/nistspecialpublication800-108.pdf\ndiff --git a\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files_format.h b\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files_format.h\nnew file mode 100644\nindex 000000000..2394e53ef\n--- \/dev\/null\n+++ b\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files_format.h\n@@ -0,0 +1,139 @@\n+\/* SPDX-License-Identifier: LGPL-3.0-or-later *\/\n+\/* Copyright (C) 2019-2020 Invisible Things Lab\n+ *                         Rafal Wojdyla <omeg@invisiblethingslab.com>\n+ * Copyright (C) 2011-2020 Intel Corporation\n+ *\/\n+\n+#ifndef PROTECTED_FILES_FORMAT_H_\n+#define PROTECTED_FILES_FORMAT_H_\n+\n+#include <limits.h>\n+\n+#include \"assert.h\"\n+#include \"list.h\"\n+#include \"protected_files.h\"\n+\n+#define PF_FILE_ID       0x46505f4850415247 \/* GRAPH_PF *\/\n+#define PF_MAJOR_VERSION 0x01\n+#define PF_MINOR_VERSION 0x00\n+\n+#define METADATA_KEY_NAME \"SGX-PROTECTED-FS-METADATA-KEY\"\n+#define MAX_LABEL_SIZE    64\n+\n+static_assert(sizeof(METADATA_KEY_NAME) <= MAX_LABEL_SIZE, \"label too long\");\n+\n+#pragma pack(push, 1)\n+\n+typedef struct _metadata_plain {\n+    uint64_t   file_id;\n+    uint8_t    major_version;\n+    uint8_t    minor_version;\n+    pf_keyid_t metadata_key_id;\n+    pf_mac_t   metadata_gmac; \/* GCM mac *\/\n+} metadata_plain_t;\n+\n+#define PATH_MAX_SIZE (260 + 512)\n+\n+\/\/ these are all defined as relative to node size, so we can decrease node size in tests\n+\/\/ and have deeper tree\n+#define MD_USER_DATA_SIZE (PF_NODE_SIZE * 3 \/ 4) \/\/ 3072\n+static_assert(MD_USER_DATA_SIZE == 3072, \"bad struct size\");\n+\n+typedef struct _metadata_encrypted {\n+    char     path[PATH_MAX_SIZE];\n+    uint64_t size;\n+    pf_key_t mht_key;\n+    pf_mac_t mht_gmac;\n+    uint8_t  data[MD_USER_DATA_SIZE];\n+} metadata_encrypted_t;\n+\n+typedef uint8_t metadata_encrypted_blob_t[sizeof(metadata_encrypted_t)];\n+\n+#define METADATA_NODE_SIZE PF_NODE_SIZE\n+\n+typedef uint8_t metadata_padding_t[METADATA_NODE_SIZE -\n+                                   (sizeof(metadata_plain_t) + sizeof(metadata_encrypted_blob_t))];\n+\n+typedef struct _metadata_node {\n+    metadata_plain_t          plain_part;\n+    metadata_encrypted_blob_t encrypted_part;\n+    metadata_padding_t        padding;\n+} metadata_node_t;\n+\n+static_assert(sizeof(metadata_node_t) == PF_NODE_SIZE, \"sizeof(metadata_node_t)\");\n+\n+typedef struct _data_node_crypto {\n+    pf_key_t key;\n+    pf_mac_t gmac;\n+} gcm_crypto_data_t;\n+\n+\/\/ for PF_NODE_SIZE == 4096, we have 96 attached data nodes and 32 mht child nodes\n+\/\/ for PF_NODE_SIZE == 2048, we have 48 attached data nodes and 16 mht child nodes\n+\/\/ for PF_NODE_SIZE == 1024, we have 24 attached data nodes and 8 mht child nodes\n+\/\/ 3\/4 of the node size is dedicated to data nodes\n+#define ATTACHED_DATA_NODES_COUNT ((PF_NODE_SIZE \/ sizeof(gcm_crypto_data_t)) * 3 \/ 4)\n+static_assert(ATTACHED_DATA_NODES_COUNT == 96, \"ATTACHED_DATA_NODES_COUNT\");\n+\/\/ 1\/4 of the node size is dedicated to child mht nodes\n+#define CHILD_MHT_NODES_COUNT ((PF_NODE_SIZE \/ sizeof(gcm_crypto_data_t)) * 1 \/ 4)\n+static_assert(CHILD_MHT_NODES_COUNT == 32, \"CHILD_MHT_NODES_COUNT\");\n+\n+typedef struct _mht_node {\n+    gcm_crypto_data_t data_nodes_crypto[ATTACHED_DATA_NODES_COUNT];\n+    gcm_crypto_data_t mht_nodes_crypto[CHILD_MHT_NODES_COUNT];\n+} mht_node_t;\n+\n+static_assert(sizeof(mht_node_t) == PF_NODE_SIZE, \"sizeof(mht_node_t)\");\n+\n+typedef struct _data_node {\n+    uint8_t data[PF_NODE_SIZE];\n+} data_node_t;\n+\n+static_assert(sizeof(data_node_t) == PF_NODE_SIZE, \"sizeof(data_node_t)\");\n+\n+typedef struct _encrypted_node {\n+    uint8_t cipher[PF_NODE_SIZE];\n+} encrypted_node_t;\n+\n+static_assert(sizeof(encrypted_node_t) == PF_NODE_SIZE, \"sizeof(encrypted_node_t)\");\n+\n+#define MAX_PAGES_IN_CACHE 48\n+\n+typedef enum {\n+    FILE_MHT_NODE_TYPE  = 1,\n+    FILE_DATA_NODE_TYPE = 2,\n+} mht_node_type_e;\n+\n+\/\/ make sure these are the same size\n+static_assert(sizeof(mht_node_t) == sizeof(data_node_t),\n+              \"sizeof(mht_node_t) == sizeof(data_node_t)\");\n+\n+DEFINE_LIST(_file_node);\n+typedef struct _file_node {\n+    LIST_TYPE(_file_node) list;\n+    uint8_t type;\n+    uint64_t node_number;\n+    struct _file_node* parent;\n+    bool need_writing;\n+    bool new_node;\n+    struct {\n+        uint64_t physical_node_number;\n+        encrypted_node_t encrypted; \/\/ the actual data from the disk\n+    };\n+    union { \/\/ decrypted data\n+        mht_node_t mht;\n+        data_node_t data;\n+    } decrypted;\n+} file_node_t;\n+DEFINE_LISTP(_file_node);\n+\n+typedef struct {\n+    uint32_t index;\n+    char label[MAX_LABEL_SIZE]; \/\/ must be NULL terminated\n+    pf_keyid_t nonce;\n+    uint32_t output_len; \/\/ in bits\n+} kdf_input_t;\n+\n+#pragma pack(pop)\n+\n+#endif \/* PROTECTED_FILES_FORMAT_H_ *\/\n+\ndiff --git a\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files_internal.h b\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files_internal.h\nindex 4a31261d5..c06d1ecf6 100644\n--- a\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files_internal.h\n+++ b\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files_internal.h\n@@ -13,116 +13,7 @@\n #include \"list.h\"\n #include \"lru_cache.h\"\n #include \"protected_files.h\"\n-\n-#define PF_FILE_ID       0x46505f4850415247 \/* GRAPH_PF *\/\n-#define PF_MAJOR_VERSION 0x01\n-#define PF_MINOR_VERSION 0x00\n-\n-#pragma pack(push, 1)\n-\n-typedef struct _metadata_plain {\n-    uint64_t   file_id;\n-    uint8_t    major_version;\n-    uint8_t    minor_version;\n-    pf_keyid_t metadata_key_id;\n-    pf_mac_t   metadata_gmac; \/* GCM mac *\/\n-} metadata_plain_t;\n-\n-#define PATH_MAX_SIZE (260 + 512)\n-\n-\/\/ these are all defined as relative to node size, so we can decrease node size in tests\n-\/\/ and have deeper tree\n-#define MD_USER_DATA_SIZE (PF_NODE_SIZE * 3 \/ 4) \/\/ 3072\n-static_assert(MD_USER_DATA_SIZE == 3072, \"bad struct size\");\n-\n-typedef struct _metadata_encrypted {\n-    char     path[PATH_MAX_SIZE];\n-    uint64_t size;\n-    pf_key_t mht_key;\n-    pf_mac_t mht_gmac;\n-    uint8_t  data[MD_USER_DATA_SIZE];\n-} metadata_encrypted_t;\n-\n-typedef uint8_t metadata_encrypted_blob_t[sizeof(metadata_encrypted_t)];\n-\n-#define METADATA_NODE_SIZE PF_NODE_SIZE\n-\n-typedef uint8_t metadata_padding_t[METADATA_NODE_SIZE -\n-                                   (sizeof(metadata_plain_t) + sizeof(metadata_encrypted_blob_t))];\n-\n-typedef struct _metadata_node {\n-    metadata_plain_t          plain_part;\n-    metadata_encrypted_blob_t encrypted_part;\n-    metadata_padding_t        padding;\n-} metadata_node_t;\n-\n-static_assert(sizeof(metadata_node_t) == PF_NODE_SIZE, \"sizeof(metadata_node_t)\");\n-\n-typedef struct _data_node_crypto {\n-    pf_key_t key;\n-    pf_mac_t gmac;\n-} gcm_crypto_data_t;\n-\n-\/\/ for PF_NODE_SIZE == 4096, we have 96 attached data nodes and 32 mht child nodes\n-\/\/ for PF_NODE_SIZE == 2048, we have 48 attached data nodes and 16 mht child nodes\n-\/\/ for PF_NODE_SIZE == 1024, we have 24 attached data nodes and 8 mht child nodes\n-\/\/ 3\/4 of the node size is dedicated to data nodes\n-#define ATTACHED_DATA_NODES_COUNT ((PF_NODE_SIZE \/ sizeof(gcm_crypto_data_t)) * 3 \/ 4)\n-static_assert(ATTACHED_DATA_NODES_COUNT == 96, \"ATTACHED_DATA_NODES_COUNT\");\n-\/\/ 1\/4 of the node size is dedicated to child mht nodes\n-#define CHILD_MHT_NODES_COUNT ((PF_NODE_SIZE \/ sizeof(gcm_crypto_data_t)) * 1 \/ 4)\n-static_assert(CHILD_MHT_NODES_COUNT == 32, \"CHILD_MHT_NODES_COUNT\");\n-\n-typedef struct _mht_node {\n-    gcm_crypto_data_t data_nodes_crypto[ATTACHED_DATA_NODES_COUNT];\n-    gcm_crypto_data_t mht_nodes_crypto[CHILD_MHT_NODES_COUNT];\n-} mht_node_t;\n-\n-static_assert(sizeof(mht_node_t) == PF_NODE_SIZE, \"sizeof(mht_node_t)\");\n-\n-typedef struct _data_node {\n-    uint8_t data[PF_NODE_SIZE];\n-} data_node_t;\n-\n-static_assert(sizeof(data_node_t) == PF_NODE_SIZE, \"sizeof(data_node_t)\");\n-\n-typedef struct _encrypted_node {\n-    uint8_t cipher[PF_NODE_SIZE];\n-} encrypted_node_t;\n-\n-static_assert(sizeof(encrypted_node_t) == PF_NODE_SIZE, \"sizeof(encrypted_node_t)\");\n-\n-#define MAX_PAGES_IN_CACHE 48\n-\n-typedef enum {\n-    FILE_MHT_NODE_TYPE  = 1,\n-    FILE_DATA_NODE_TYPE = 2,\n-} mht_node_type_e;\n-\n-\/\/ make sure these are the same size\n-static_assert(sizeof(mht_node_t) == sizeof(data_node_t),\n-              \"sizeof(mht_node_t) == sizeof(data_node_t)\");\n-\n-DEFINE_LIST(_file_node);\n-typedef struct _file_node {\n-    LIST_TYPE(_file_node) list;\n-    uint8_t type;\n-    uint64_t node_number;\n-    struct _file_node* parent;\n-    bool need_writing;\n-    bool new_node;\n-    struct {\n-        uint64_t physical_node_number;\n-        encrypted_node_t encrypted; \/\/ the actual data from the disk\n-    };\n-    union { \/\/ decrypted data\n-        mht_node_t mht;\n-        data_node_t data;\n-    } decrypted;\n-} file_node_t;\n-DEFINE_LISTP(_file_node);\n-\n-#pragma pack(pop)\n+#include \"protected_files_format.h\"\n \n struct pf_context {\n     metadata_node_t file_metadata; \/\/ actual data from disk's meta data node\ndiff --git a\/Pal\/src\/host\/Linux-SGX\/tools\/Makefile b\/Pal\/src\/host\/Linux-SGX\/tools\/Makefile\nindex a5903ad97..87bc78124 100644\n--- a\/Pal\/src\/host\/Linux-SGX\/tools\/Makefile\n+++ b\/Pal\/src\/host\/Linux-SGX\/tools\/Makefile\n@@ -12,3 +12,4 @@ $(targets):\n \t$(MAKE) -C verify-ias-report $@\n \t$(MAKE) -C ra-tls $@\n \t$(MAKE) -C pf_crypt $@\n+\t$(MAKE) -C pf_tamper $@\ndiff --git a\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/.gitignore b\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/.gitignore\nnew file mode 100644\nindex 000000000..a52536d82\n--- \/dev\/null\n+++ b\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/.gitignore\n@@ -0,0 +1 @@\n+\/pf_tamper\ndiff --git a\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/Makefile b\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/Makefile\nnew file mode 100644\nindex 000000000..94c826e4b\n--- \/dev\/null\n+++ b\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/Makefile\n@@ -0,0 +1,31 @@\n+include ..\/..\/..\/..\/..\/..\/Scripts\/Makefile.configs\n+include ..\/..\/..\/..\/..\/..\/Scripts\/Makefile.rules\n+\n+CFLAGS += -I..\/.. \\\n+          -I..\/common \\\n+          -I..\/..\/protected-files \\\n+          -I..\/..\/..\/..\/..\/include\/lib \\\n+          -D_GNU_SOURCE\n+\n+LDLIBS += -L..\/common \\\n+          -L..\/..\/..\/..\/..\/lib\/crypto\/mbedtls\/install\/lib \\\n+          -lsgx_util -lmbedcrypto\n+\n+PREFIX ?= \/usr\/local\n+\n+pf_tamper: pf_tamper.o\n+\t$(call cmd,csingle)\n+\n+.PHONY: all\n+all: pf_tamper\n+\n+.PHONY: install\n+install:\n+\tinstall -D pf_tamper -t ${PREFIX}\/bin\n+\n+.PHONY: clean\n+clean:\n+\t$(RM) *.o pf_tamper\n+\n+.PHONY: distclean\n+distclean: clean\ndiff --git a\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/pf_tamper.c b\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/pf_tamper.c\nnew file mode 100644\nindex 000000000..4d39a0d58\n--- \/dev\/null\n+++ b\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/pf_tamper.c\n@@ -0,0 +1,476 @@\n+\/* SPDX-License-Identifier: LGPL-3.0-or-later *\/\n+\/* Copyright (C) 2019-2020 Invisible Things Lab\n+ *                         Rafal Wojdyla <omeg@invisiblethingslab.com>\n+ *\/\n+\n+#include <fcntl.h>\n+#include <getopt.h>\n+#include <stdlib.h>\n+#include <sys\/mman.h>\n+#include <unistd.h>\n+\n+#include \"pf_util.h\"\n+#include \"protected_files.h\"\n+#include \"protected_files_format.h\"\n+#include \"util.h\"\n+\n+\/* Tamper with a PF in various ways for testing purposes. The PF is assumed to be valid and have at\n+ * least enough data to contain two MHT nodes. *\/\n+\n+\/* Command line options *\/\n+struct option g_options[] = {\n+    { \"input\", required_argument, 0, 'i' },\n+    { \"output\", required_argument, 0, 'o' },\n+    { \"wrap-key\", required_argument, 0, 'w' },\n+    { \"verbose\", no_argument, 0, 'v' },\n+    { \"help\", no_argument, 0, 'h' },\n+    { 0, 0, 0, 0 }\n+};\n+\n+static void usage(void) {\n+    INFO(\"\\nUsage: pf_tamper [options]\\n\");\n+    INFO(\"\\nAvailable options:\\n\");\n+    INFO(\"  --help, -h           Display this help\\n\");\n+    INFO(\"  --verbose, -v        Enable verbose output\\n\");\n+    INFO(\"  --wrap-key, -w PATH  Path to wrap key file\\n\");\n+    INFO(\"  --input, -i PATH     Source file to be tampered with (must be a valid PF)\\n\");\n+    INFO(\"  --output, -o PATH    Directory where modified files will be written to\\n\");\n+}\n+\n+#define FATAL(fmt, ...) do { \\\n+    ERROR(fmt, ##__VA_ARGS__); \\\n+    exit(-1); \\\n+} while (0)\n+\n+ssize_t g_input_size = 0;\n+char* g_input_name = NULL;\n+void* g_input_data = MAP_FAILED;\n+char* g_output_dir = NULL;\n+char* g_output_path = NULL;\n+size_t g_output_path_size = 0;\n+pf_key_t g_wrap_key;\n+pf_key_t g_meta_key;\n+\n+static pf_iv_t g_empty_iv = {0};\n+\n+static void derive_main_key(const pf_key_t* kdk, const pf_keyid_t* key_id, pf_key_t* out_key) {\n+    kdf_input_t buf = {0};\n+    pf_status_t status;\n+\n+    buf.index = 1;\n+    strncpy(buf.label, METADATA_KEY_NAME, MAX_LABEL_SIZE);\n+    memcpy(&buf.nonce, key_id, sizeof(buf.nonce));\n+    buf.output_len = 0x80;\n+\n+    status = mbedtls_aes_gcm_encrypt(kdk, &g_empty_iv, &buf, sizeof(buf), NULL, 0, NULL, out_key);\n+    if (PF_FAILURE(status))\n+        FATAL(\"key derivation failed\\n\");\n+}\n+\n+static void make_output_path(const char* suffix) {\n+    snprintf(g_output_path, g_output_path_size, \"%s\/%s.%s\", g_output_dir, g_input_name, suffix);\n+    INFO(\"[*] %s\\n\", g_output_path);\n+}\n+\n+\/* PF layout (node size is PF_NODE_SIZE):\n+ * - Node 0: metadata (metadata_node_t)\n+ *   - metadata_plain_t\n+ *   - metadata_encrypted_t (may include MD_USER_DATA_SIZE bytes of data)\n+ *   - metadata_padding_t\n+ * - Node 1: MHT (mht_node_t)\n+ * - Node 2-97: data (ATTACHED_DATA_NODES_COUNT == 96)\n+ * - Node 98: MHT\n+ * - Node 99-195: data\n+ * - ...\n+ *\/\n+static void truncate_file(const char* suffix, size_t output_size) {\n+    int ret;\n+\n+    make_output_path(suffix);\n+\n+    if (output_size < g_input_size) {\n+        ret = write_file(g_output_path, output_size, g_input_data);\n+    } else {\n+        ret = write_file(g_output_path, g_input_size, g_input_data);\n+        if (ret < 0)\n+            goto out;\n+        ret = truncate(g_output_path, output_size);\n+    }\n+out:\n+    if (ret < 0)\n+        FATAL(\"truncate_file failed: %d\\n\", ret);\n+}\n+\n+#define FIELD_SIZEOF(t, f) (sizeof(((t*)0)->f))\n+#define FIELD_TRUNCATED(t, f) (offsetof(t, f) + (FIELD_SIZEOF(t, f) \/ 2))\n+#define DATA_CRYPTO_SIZE (FIELD_SIZEOF(mht_node_t, data_nodes_crypto))\n+\n+static void tamper_truncate(void) {\n+    size_t mdps = sizeof(metadata_plain_t);\n+    DBG(\"size(metadata_plain_t)             = 0x%04lx\\n\", sizeof(metadata_plain_t));\n+    DBG(\"metadata_plain_t.file_id           : 0x%04lx (0x%04lx)\\n\",\n+        offsetof(metadata_plain_t, file_id), FIELD_SIZEOF(metadata_plain_t, file_id));\n+    DBG(\"metadata_plain_t.major_version     : 0x%04lx (0x%04lx)\\n\",\n+        offsetof(metadata_plain_t, major_version), FIELD_SIZEOF(metadata_plain_t, major_version));\n+    DBG(\"metadata_plain_t.minor_version     : 0x%04lx (0x%04lx)\\n\",\n+        offsetof(metadata_plain_t, minor_version), FIELD_SIZEOF(metadata_plain_t, minor_version));\n+    DBG(\"metadata_plain_t.metadata_key_id   : 0x%04lx (0x%04lx)\\n\",\n+        offsetof(metadata_plain_t, metadata_key_id),\n+        FIELD_SIZEOF(metadata_plain_t, metadata_key_id));\n+    DBG(\"metadata_plain_t.metadata_gmac     : 0x%04lx (0x%04lx)\\n\",\n+        offsetof(metadata_plain_t, metadata_gmac),\n+        FIELD_SIZEOF(metadata_plain_t, metadata_gmac));\n+\n+    DBG(\"size(metadata_encrypted_t)         = 0x%04lx\\n\", sizeof(metadata_encrypted_t));\n+    DBG(\"metadata_encrypted_t.path          : 0x%04lx (0x%04lx)\\n\",\n+        mdps + offsetof(metadata_encrypted_t, path),\n+        FIELD_SIZEOF(metadata_encrypted_t, path));\n+    DBG(\"metadata_encrypted_t.size          : 0x%04lx (0x%04lx)\\n\",\n+        mdps + offsetof(metadata_encrypted_t, size), FIELD_SIZEOF(metadata_encrypted_t, size));\n+    DBG(\"metadata_encrypted_t.mht_key       : 0x%04lx (0x%04lx)\\n\",\n+        mdps + offsetof(metadata_encrypted_t, mht_key),\n+        FIELD_SIZEOF(metadata_encrypted_t, mht_key));\n+    DBG(\"metadata_encrypted_t.mht_gmac      : 0x%04lx (0x%04lx)\\n\",\n+        mdps + offsetof(metadata_encrypted_t, mht_gmac),\n+        FIELD_SIZEOF(metadata_encrypted_t, mht_gmac));\n+    DBG(\"metadata_encrypted_t.data          : 0x%04lx (0x%04lx)\\n\",\n+        mdps + offsetof(metadata_encrypted_t, data), FIELD_SIZEOF(metadata_encrypted_t, data));\n+\n+    DBG(\"size(metadata_padding_t)           = 0x%04lx\\n\", sizeof(metadata_padding_t));\n+    DBG(\"metadata_padding_t                 : 0x%04lx (0x%04lx)\\n\",\n+        mdps + sizeof(metadata_encrypted_t), sizeof(metadata_padding_t));\n+\n+    \/* node 0: metadata + 3k of user data *\/\n+    \/* plain metadata *\/\n+    truncate_file(\"trunc_meta_plain_0\", 0);\n+    truncate_file(\"trunc_meta_plain_1\", FIELD_TRUNCATED(metadata_plain_t, file_id));\n+    truncate_file(\"trunc_meta_plain_2\", offsetof(metadata_plain_t, major_version));\n+    truncate_file(\"trunc_meta_plain_3\", offsetof(metadata_plain_t, minor_version));\n+    truncate_file(\"trunc_meta_plain_4\", offsetof(metadata_plain_t, metadata_key_id));\n+    truncate_file(\"trunc_meta_plain_5\", FIELD_TRUNCATED(metadata_plain_t, metadata_key_id));\n+    truncate_file(\"trunc_meta_plain_6\", offsetof(metadata_plain_t, metadata_gmac));\n+    truncate_file(\"trunc_meta_plain_7\", FIELD_TRUNCATED(metadata_plain_t, metadata_gmac));\n+\n+    \/* encrypted metadata *\/\n+    truncate_file(\"trunc_meta_enc_0\", mdps + offsetof(metadata_encrypted_t, path));\n+    truncate_file(\"trunc_meta_enc_1\", mdps + FIELD_TRUNCATED(metadata_encrypted_t, path));\n+    truncate_file(\"trunc_meta_enc_2\", mdps + offsetof(metadata_encrypted_t, size));\n+    truncate_file(\"trunc_meta_enc_3\", mdps + FIELD_TRUNCATED(metadata_encrypted_t, size));\n+    truncate_file(\"trunc_meta_enc_4\", mdps + offsetof(metadata_encrypted_t, mht_key));\n+    truncate_file(\"trunc_meta_enc_5\", mdps + FIELD_TRUNCATED(metadata_encrypted_t, mht_key));\n+    truncate_file(\"trunc_meta_enc_6\", mdps + offsetof(metadata_encrypted_t, mht_gmac));\n+    truncate_file(\"trunc_meta_enc_7\", mdps + FIELD_TRUNCATED(metadata_encrypted_t, mht_gmac));\n+    truncate_file(\"trunc_meta_enc_8\", mdps + offsetof(metadata_encrypted_t, data));\n+    truncate_file(\"trunc_meta_enc_9\", mdps + FIELD_TRUNCATED(metadata_encrypted_t, data));\n+\n+    \/* padding *\/\n+    truncate_file(\"trunc_meta_pad_0\", mdps + sizeof(metadata_encrypted_t));\n+    truncate_file(\"trunc_meta_pad_1\", mdps + sizeof(metadata_encrypted_t)\n+                  + sizeof(metadata_padding_t) \/ 2);\n+\n+    \/* node 1: mht root *\/\n+    \/* after node 0 *\/\n+    truncate_file(\"trunc_mht_0\", PF_NODE_SIZE);\n+    \/* middle of data_nodes_crypto[0].key *\/\n+    truncate_file(\"trunc_mht_1\", PF_NODE_SIZE + PF_KEY_SIZE \/ 2);\n+    \/* after data_nodes_crypto[0].key *\/\n+    truncate_file(\"trunc_mht_2\", PF_NODE_SIZE + PF_KEY_SIZE);\n+    \/* middle of data_nodes_crypto[0].gmac *\/\n+    truncate_file(\"trunc_mht_3\", PF_NODE_SIZE + PF_KEY_SIZE + PF_MAC_SIZE \/ 2);\n+    \/* after data_nodes_crypto[0].gmac *\/\n+    truncate_file(\"trunc_mht_4\", PF_NODE_SIZE + PF_KEY_SIZE + PF_MAC_SIZE);\n+    \/* after data_nodes_crypto *\/\n+    truncate_file(\"trunc_mht_5\", PF_NODE_SIZE + DATA_CRYPTO_SIZE);\n+    \/* middle of mht_nodes_crypto[0].key *\/\n+    truncate_file(\"trunc_mht_6\", PF_NODE_SIZE + DATA_CRYPTO_SIZE + PF_KEY_SIZE \/ 2);\n+    \/* after mht_nodes_crypto[0].key *\/\n+    truncate_file(\"trunc_mht_7\", PF_NODE_SIZE + DATA_CRYPTO_SIZE + PF_KEY_SIZE);\n+    \/* middle of mht_nodes_crypto[0].gmac *\/\n+    truncate_file(\"trunc_mht_8\", PF_NODE_SIZE + DATA_CRYPTO_SIZE + PF_KEY_SIZE + PF_MAC_SIZE \/ 2);\n+    \/* after mht_nodes_crypto[0].gmac *\/\n+    truncate_file(\"trunc_mht_9\", PF_NODE_SIZE + DATA_CRYPTO_SIZE + PF_KEY_SIZE + PF_MAC_SIZE);\n+\n+    \/* node 2-3: data #0, #1 *\/\n+    \/* after mht root *\/\n+    truncate_file(\"trunc_data_0\", 2 * PF_NODE_SIZE);\n+    \/* middle of data #0 *\/\n+    truncate_file(\"trunc_data_1\", 2 * PF_NODE_SIZE + PF_NODE_SIZE \/ 2);\n+    \/* after data #0 *\/\n+    truncate_file(\"trunc_data_2\", 3 * PF_NODE_SIZE);\n+    \/* middle of data #1 *\/\n+    truncate_file(\"trunc_data_3\", 3 * PF_NODE_SIZE + PF_NODE_SIZE \/ 2);\n+\n+    \/* extend *\/\n+    truncate_file(\"extend_0\", g_input_size + 1);\n+    truncate_file(\"extend_1\", g_input_size + PF_NODE_SIZE \/ 2);\n+    truncate_file(\"extend_2\", g_input_size + PF_NODE_SIZE);\n+    truncate_file(\"extend_3\", g_input_size + PF_NODE_SIZE + PF_NODE_SIZE \/ 2);\n+}\n+\n+\/* returns mmap'd output contents *\/\n+static void* create_output(const char* path) {\n+    void* mem = MAP_FAILED;\n+    int fd = open(path, O_RDWR|O_CREAT, 0664);\n+    if (fd < 0)\n+        FATAL(\"Failed to open output file '%s': %s\\n\", path, strerror(errno));\n+\n+    if (ftruncate(fd, g_input_size) < 0)\n+        FATAL(\"Failed to ftruncate output file '%s': %s\\n\", path, strerror(errno));\n+\n+    mem = mmap(NULL, g_input_size, PROT_READ|PROT_WRITE, MAP_SHARED, fd, 0);\n+    if (mem == MAP_FAILED)\n+        FATAL(\"Failed to mmap output file '%s': %s\\n\", path, strerror(errno));\n+\n+    memcpy(mem, g_input_data, g_input_size);\n+\n+    close(fd);\n+    return mem;\n+}\n+\n+static void pf_decrypt(const void* encrypted, size_t size, const pf_key_t* key, const pf_mac_t* mac,\n+                       void* decrypted, const char* msg) {\n+    pf_status_t status = mbedtls_aes_gcm_decrypt(key, &g_empty_iv, NULL, 0,\n+                                                 encrypted, size,\n+                                                 decrypted, mac);\n+    if (PF_FAILURE(status))\n+        FATAL(\"decrypting %s failed\\n\", msg);\n+}\n+\n+static void pf_encrypt(const void* decrypted, size_t size, const pf_key_t* key, pf_mac_t* mac,\n+                       void* encrypted, const char* msg) {\n+    pf_status_t status = mbedtls_aes_gcm_encrypt(key, &g_empty_iv, NULL, 0,\n+                                                 decrypted, size,\n+                                                 encrypted, mac);\n+    if (PF_FAILURE(status))\n+        FATAL(\"encrypting %s failed\\n\", msg);\n+}\n+\n+\/* copy input PF and apply some modifications *\/\n+#define __BREAK_PF(suffix, ...) do { \\\n+    make_output_path(suffix); \\\n+    meta = create_output(g_output_path); \\\n+    out = (uint8_t*)meta; \\\n+    pf_decrypt(&meta->encrypted_part, sizeof(meta->encrypted_part), &g_meta_key, \\\n+               &meta->plain_part.metadata_gmac, meta_dec, \"metadata\"); \\\n+    mht_enc = (mht_node_t*)(out + PF_NODE_SIZE); \\\n+    pf_decrypt(mht_enc, sizeof(*mht_enc), &meta_dec->mht_key, &meta_dec->mht_gmac, mht_dec, \\\n+               \"mht\"); \\\n+    __VA_ARGS__ \\\n+    munmap(meta, g_input_size); \\\n+} while (0)\n+\n+\/* if update is true, also create a file with correct metadata MAC *\/\n+#define BREAK_PF(suffix, update, ...) do { \\\n+    __BREAK_PF(suffix, __VA_ARGS__); \\\n+    if (update) { \\\n+        __BREAK_PF(suffix \"_fixed\", __VA_ARGS__ { \\\n+                       pf_encrypt(meta_dec, sizeof(*meta_dec), &g_meta_key, \\\n+                                  &meta->plain_part.metadata_gmac, meta->encrypted_part, \\\n+                                  \"metadata\"); \\\n+                   } ); \\\n+    } \\\n+} while (0)\n+\n+#define BREAK_MHT(suffix, ...) do { \\\n+    __BREAK_PF(suffix, __VA_ARGS__ { \\\n+                   pf_encrypt(mht_dec, sizeof(*mht_dec), &meta_dec->mht_key, &meta_dec->mht_gmac, \\\n+                              mht_enc, \"mht\"); \\\n+               } ); \\\n+} while (0)\n+\n+#define LAST_BYTE(array) (((uint8_t*)&array)[sizeof(array) - 1])\n+\n+static void tamper_modify(void) {\n+    metadata_node_t* meta = NULL;\n+    uint8_t* out = NULL;\n+    metadata_encrypted_t* meta_dec = malloc(sizeof(*meta_dec));\n+    if (!meta_dec)\n+        FATAL(\"Out of memory\\n\");\n+    mht_node_t* mht_enc = NULL;\n+    mht_node_t* mht_dec = malloc(sizeof(*mht_dec));\n+    if (!mht_dec)\n+        FATAL(\"Out of memory\\n\");\n+\n+    \/* plain part of the metadata isn't covered by the MAC so no point updating it *\/\n+    BREAK_PF(\"meta_plain_id_0\", \/*update=*\/false,\n+             { meta->plain_part.file_id = 0; });\n+    BREAK_PF(\"meta_plain_id_1\", \/*update=*\/false,\n+             { meta->plain_part.file_id = UINT64_MAX; });\n+    BREAK_PF(\"meta_plain_version_0\", \/*update=*\/false,\n+             { meta->plain_part.major_version = 0; });\n+    BREAK_PF(\"meta_plain_version_1\", \/*update=*\/false,\n+             { meta->plain_part.major_version = 0xff; });\n+    BREAK_PF(\"meta_plain_version_2\", \/*update=*\/false,\n+             { meta->plain_part.minor_version = 0xff; });\n+\n+    \/* metadata_key_id is the keying material for encrypted metadata key derivation, so create also\n+     * PFs with updated MACs *\/\n+    BREAK_PF(\"meta_plain_keyid_0\", \/*update=*\/true,\n+             { meta->plain_part.metadata_key_id[0] ^= 1; });\n+    BREAK_PF(\"meta_plain_keyid_1\", \/*update=*\/true,\n+             { LAST_BYTE(meta->plain_part.metadata_key_id) ^= 0xfe; });\n+    BREAK_PF(\"meta_plain_mac_0\", \/*update=*\/true,\n+             { meta->plain_part.metadata_gmac[0] ^= 0xfe; });\n+    BREAK_PF(\"meta_plain_mac_1\", \/*update=*\/true,\n+             { LAST_BYTE(meta->plain_part.metadata_gmac) &= 1; });\n+\n+    BREAK_PF(\"meta_enc_filename_0\", \/*update=*\/true,\n+             { meta_dec->path[0] = 0; });\n+    BREAK_PF(\"meta_enc_filename_1\", \/*update=*\/true,\n+             { meta_dec->path[0] ^= 1; });\n+    BREAK_PF(\"meta_enc_filename_2\", \/*update=*\/true,\n+             { LAST_BYTE(meta_dec->path) ^= 0xfe; });\n+    BREAK_PF(\"meta_enc_size_0\", \/*update=*\/true,\n+             { meta_dec->size = 0; });\n+    BREAK_PF(\"meta_enc_size_1\", \/*update=*\/true,\n+             { meta_dec->size = g_input_size - 1; });\n+    BREAK_PF(\"meta_enc_size_2\", \/*update=*\/true,\n+             { meta_dec->size = g_input_size + 1; });\n+    BREAK_PF(\"meta_enc_size_3\", \/*update=*\/true,\n+             { meta_dec->size = UINT64_MAX; });\n+    BREAK_PF(\"meta_enc_mht_key_0\", \/*update=*\/true,\n+             { meta_dec->mht_key[0] ^= 1; });\n+    BREAK_PF(\"meta_enc_mht_key_1\", \/*update=*\/true,\n+             { LAST_BYTE(meta_dec->mht_key) ^= 0xfe; });\n+    BREAK_PF(\"meta_enc_mht_mac_0\", \/*update=*\/true,\n+             { meta_dec->mht_gmac[0] ^= 1; });\n+    BREAK_PF(\"meta_enc_mht_mac_1\", \/*update=*\/true,\n+             { LAST_BYTE(meta_dec->mht_gmac) ^= 0xfe; });\n+    BREAK_PF(\"meta_enc_data_0\", \/*update=*\/true,\n+             { meta_dec->data[0] ^= 0xfe; });\n+    BREAK_PF(\"meta_enc_data_1\", \/*update=*\/true,\n+             { LAST_BYTE(meta_dec->data) ^= 1; });\n+\n+    \/* padding is ignored *\/\n+    BREAK_PF(\"meta_padding_0\", \/*update=*\/false,\n+             { meta->padding[0] ^= 1; });\n+    BREAK_PF(\"meta_padding_1\", \/*update=*\/false,\n+             { LAST_BYTE(meta->padding) ^= 0xfe; });\n+\n+    BREAK_MHT(\"mht_0\", { mht_dec->data_nodes_crypto[0].key[0] ^= 1; });\n+    BREAK_MHT(\"mht_1\", { mht_dec->data_nodes_crypto[0].gmac[0] ^= 1; });\n+    BREAK_MHT(\"mht_2\", { mht_dec->mht_nodes_crypto[0].key[0] ^= 1; });\n+    BREAK_MHT(\"mht_3\", { mht_dec->mht_nodes_crypto[0].gmac[0] ^= 1; });\n+    BREAK_MHT(\"mht_4\", { mht_dec->data_nodes_crypto[ATTACHED_DATA_NODES_COUNT - 1].key[0] ^= 1; });\n+    BREAK_MHT(\"mht_5\", { mht_dec->data_nodes_crypto[ATTACHED_DATA_NODES_COUNT - 1].gmac[0] ^= 1; });\n+    BREAK_MHT(\"mht_6\", { mht_dec->mht_nodes_crypto[CHILD_MHT_NODES_COUNT - 1].key[0] ^= 1; });\n+    BREAK_MHT(\"mht_7\", { mht_dec->mht_nodes_crypto[CHILD_MHT_NODES_COUNT - 1].gmac[0] ^= 1; });\n+    BREAK_MHT(\"mht_8\", {\n+        gcm_crypto_data_t crypto;\n+        memcpy(&crypto, &mht_dec->data_nodes_crypto[0], sizeof(crypto));\n+        memcpy(&mht_dec->data_nodes_crypto[0], &mht_dec->data_nodes_crypto[1], sizeof(crypto));\n+        memcpy(&mht_dec->data_nodes_crypto[1], &crypto, sizeof(crypto));\n+    });\n+    BREAK_MHT(\"mht_9\", {\n+        gcm_crypto_data_t crypto;\n+        memcpy(&crypto, &mht_dec->mht_nodes_crypto[0], sizeof(crypto));\n+        memcpy(&mht_dec->mht_nodes_crypto[0], &mht_dec->mht_nodes_crypto[1], sizeof(crypto));\n+        memcpy(&mht_dec->mht_nodes_crypto[1], &crypto, sizeof(crypto));\n+    });\n+\n+    \/* data nodes start from node #2 *\/\n+    BREAK_PF(\"data_0\", \/*update=*\/false,\n+             { *(out + 2 * PF_NODE_SIZE) ^= 1; });\n+    BREAK_PF(\"data_1\", \/*update=*\/false,\n+             { *(out + 3 * PF_NODE_SIZE - 1) ^= 1; });\n+    BREAK_PF(\"data_2\", \/*update=*\/false, {\n+        \/* swap data nodes *\/\n+        memcpy(out + 2 * PF_NODE_SIZE, g_input_data + 3 * PF_NODE_SIZE, PF_NODE_SIZE);\n+        memcpy(out + 3 * PF_NODE_SIZE, g_input_data + 2 * PF_NODE_SIZE, PF_NODE_SIZE);\n+    });\n+\n+    free(mht_dec);\n+    free(meta_dec);\n+}\n+\n+int main(int argc, char* argv[]) {\n+    int ret = -1;\n+\n+    int option          = 0;\n+    char* input_path    = NULL;\n+    char* wrap_key_path = NULL;\n+    int input_fd        = -1;\n+\n+    while (true) {\n+        option = getopt_long(argc, argv, \"i:o:w:vh\", g_options, NULL);\n+        if (option == -1)\n+            break;\n+\n+        switch (option) {\n+            case 'i':\n+                input_path = optarg;\n+                break;\n+            case 'o':\n+                g_output_dir = optarg;\n+                break;\n+            case 'w':\n+                wrap_key_path = optarg;\n+                break;\n+            case 'v':\n+                set_verbose(true);\n+                break;\n+            case 'h':\n+                usage();\n+                return 0;\n+            default:\n+                ERROR(\"Unknown option: %c\\n\", option);\n+                usage();\n+        }\n+    }\n+\n+    if (!input_path) {\n+        ERROR(\"Input path not specified\\n\");\n+        usage();\n+        goto out;\n+    }\n+\n+    if (!g_output_dir) {\n+        ERROR(\"Output path not specified\\n\");\n+        usage();\n+        goto out;\n+    }\n+\n+    if (!wrap_key_path) {\n+        ERROR(\"Wrap key path not specified\\n\");\n+        usage();\n+        goto out;\n+    }\n+\n+    input_fd = open(input_path, O_RDONLY);\n+    if (input_fd < 0) {\n+        ERROR(\"Failed to open input file '%s': %s\\n\", input_path, strerror(errno));\n+        goto out;\n+    }\n+\n+    g_input_size = get_file_size(input_fd);\n+    if (g_input_size < 0) {\n+        ERROR(\"Failed to stat input file '%s': %s\\n\", input_path, strerror(errno));\n+        goto out;\n+    }\n+\n+    g_input_data = mmap(NULL, g_input_size, PROT_READ, MAP_PRIVATE, input_fd, 0);\n+    if (g_input_data == MAP_FAILED) {\n+        ERROR(\"Failed to mmap input file '%s': %s\\n\", input_path, strerror(errno));\n+        goto out;\n+    }\n+\n+    load_wrap_key(wrap_key_path, &g_wrap_key);\n+    derive_main_key(&g_wrap_key, &((metadata_plain_t*)g_input_data)->metadata_key_id,\n+                    &g_meta_key);\n+\n+    g_input_name = basename(input_path);\n+    g_output_path_size = strlen(g_input_name) + strlen(g_output_dir) + 256;\n+    g_output_path = malloc(g_output_path_size);\n+    if (!g_output_path) {\n+        ERROR(\"No memory\\n\");\n+        goto out;\n+    }\n+\n+    tamper_truncate();\n+    tamper_modify();\n+    ret = 0;\n+\n+out:\n+    \/* skip cleanup as we are in main() *\/\n+    return ret;\n+}\n","message":"","files":{"\/LibOS\/shim\/test\/fs\/test_pf.py":{"changes":[{"diff":"\n \n     def __corrupt_file(self, input_path, output_path):\n         cmd = [self.PF_TAMPER, '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path]\n-        return self.run_native_binary(cmd)\n+        return self.run_native_binary(cmd, libpath=os.path.join(os.getcwd(), 'lib'))\n \n     # invalid\/corrupted files\n-    @expectedFailureIf(HAS_SGX)\n-    # pylint: disable=fixme\n     def test_500_invalid(self):\n-        # TODO: port these to the new file format\n         invalid_dir = os.path.join(self.TEST_DIR, 'pf_invalid')\n-        # files below should work normally (benign modifications)\n-        should_pass = ['chunk_padding_1_fixed', 'chunk_padding_2_fixed', 'chunk_data_3',\n-                       'chunk_data_3_fixed', 'chunk_data_4', 'chunk_data_4_fixed']\n         if not os.path.exists(invalid_dir):\n             os.mkdir(invalid_dir)\n+\n         # prepare valid encrypted file (largest one for maximum possible corruptions)\n         original_input = self.OUTPUT_FILES[-1]\n         self.__encrypt_file(self.INPUT_FILES[-1], original_input)\n         # generate invalid files based on the above\n         self.__corrupt_file(original_input, invalid_dir)\n+\n         # try to decrypt invalid files\n         for name in os.listdir(invalid_dir):\n             invalid = os.path.join(invalid_dir, name)\n","add":3,"remove":7,"filename":"\/LibOS\/shim\/test\/fs\/test_pf.py","badparts":["        return self.run_native_binary(cmd)","    @expectedFailureIf(HAS_SGX)","        should_pass = ['chunk_padding_1_fixed', 'chunk_padding_2_fixed', 'chunk_data_3',","                       'chunk_data_3_fixed', 'chunk_data_4', 'chunk_data_4_fixed']"],"goodparts":["        return self.run_native_binary(cmd, libpath=os.path.join(os.getcwd(), 'lib'))"]},{"diff":"\n             input_path = os.path.join(invalid_dir, os.path.basename(original_input))\n             # copy the file so it has the original file name (for allowed path check)\n             shutil.copy(invalid, input_path)\n-            should_pass = any(s in name for s in should_pass)\n \n             try:\n                 args = ['decrypt', '-V', '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path]\n                 self.__pf_crypt(args)\n             except subprocess.CalledProcessError as exc:\n-                if should_pass:\n-                    self.assertEqual(exc.returncode, 0)\n-                else:\n-                    self.assertNotEqual(exc.returncode, 0)\n+                # decryption of invalid file must fail with -1 (wrapped to 255)\n+                self.assertEqual(exc.returncode, 255)\n             else:\n-                if not should_pass:\n-                    print('[!] Fail: successfully decrypted file: ' + name)\n-                    self.fail()\n+                print('[!] Fail: successfully decrypted file: ' + name)\n+                self.fail()","add":4,"remove":8,"filename":"\/LibOS\/shim\/test\/fs\/test_pf.py","badparts":["            should_pass = any(s in name for s in should_pass)","                if should_pass:","                    self.assertEqual(exc.returncode, 0)","                else:","                    self.assertNotEqual(exc.returncode, 0)","                if not should_pass:","                    print('[!] Fail: successfully decrypted file: ' + name)","                    self.fail()"],"goodparts":["                self.assertEqual(exc.returncode, 255)","                print('[!] Fail: successfully decrypted file: ' + name)","                self.fail()"]}],"source":"\n import filecmp import os import shutil import subprocess import unittest from test_fs import( TC_00_FileSystem, ) from regression import( HAS_SGX, expectedFailureIf, ) @unittest.skipUnless(HAS_SGX, 'Protected files require SGX support') class TC_50_ProtectedFiles(TC_00_FileSystem): @classmethod def setUpClass(cls): cls.PF_CRYPT='bin\/pf_crypt' cls.PF_TAMPER='bin\/pf_tamper' cls.WRAP_KEY=os.path.join(cls.TEST_DIR, 'wrap-key') cls.CONST_WRAP_KEY=[0xff, 0xee, 0xdd, 0xcc, 0xbb, 0xaa, 0x99, 0x88, 0x77, 0x66, 0x55, 0x44, 0x33, 0x22, 0x11, 0x00] cls.ENCRYPTED_DIR=os.path.join(cls.TEST_DIR, 'pf_input') cls.ENCRYPTED_FILES=[os.path.join(cls.ENCRYPTED_DIR, str(v)) for v in cls.FILE_SIZES] cls.LIB_PATH=os.path.join(os.getcwd(), 'lib') super().setUpClass() if not os.path.exists(cls.ENCRYPTED_DIR): os.mkdir(cls.ENCRYPTED_DIR) cls.OUTPUT_DIR=os.path.join(cls.TEST_DIR, 'pf_output') cls.OUTPUT_FILES=[os.path.join(cls.OUTPUT_DIR, str(x)) for x in cls.FILE_SIZES] cls.__set_default_key(cls) for i in cls.INDEXES: cmd=[cls.PF_CRYPT, 'encrypt', '-w', cls.WRAP_KEY, '-i', cls.INPUT_FILES[i], '-o', cls.ENCRYPTED_FILES[i]] cls.run_native_binary(cmd, libpath=os.path.join(os.getcwd(), 'lib')) def __pf_crypt(self, args): args.insert(0, self.PF_CRYPT) return self.run_native_binary(args, libpath=os.path.join(os.getcwd(), 'lib')) def __set_default_key(self): with open(self.WRAP_KEY, 'wb') as file: file.write(bytes(self.CONST_WRAP_KEY)) def copy_input(self, input_path, output_path): self.__encrypt_file(input_path, output_path) def __encrypt_file(self, input_path, output_path): args=['encrypt', '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path] stdout, stderr=self.__pf_crypt(args) return(stdout, stderr) def __decrypt_file(self, input_path, output_path): args=['decrypt', '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path] stdout, stderr=self.__pf_crypt(args) return(stdout, stderr) def test_000_gen_key(self): key_path=os.path.join(self.TEST_DIR, 'tmpkey') args=['gen-key', '-w', key_path] stdout, _=self.__pf_crypt(args) self.assertIn('Wrap key saved to: ' +key_path, stdout) self.assertEqual(os.path.getsize(key_path), 16) os.remove(key_path) def test_010_encrypt_decrypt(self): for i in self.INDEXES: self.__encrypt_file(self.INPUT_FILES[i], self.OUTPUT_FILES[i]) self.assertFalse(filecmp.cmp(self.INPUT_FILES[i], self.OUTPUT_FILES[i], shallow=False)) dec_path=os.path.join(self.OUTPUT_DIR, os.path.basename(self.OUTPUT_FILES[i]) +'.dec') self.__decrypt_file(self.OUTPUT_FILES[i], dec_path) self.assertTrue(filecmp.cmp(self.INPUT_FILES[i], dec_path, shallow=False)) def test_100_open_close(self): input_path=self.ENCRYPTED_FILES[-1] output_path=os.path.join(self.OUTPUT_DIR, 'test_100') stdout, stderr=self.run_binary(['open_close', 'R', input_path]) self.verify_open_close(stdout, stderr, input_path, 'input') try: stdout, stderr=self.run_binary(['open_close', 'W', output_path]) self.assertIn('ERROR: Failed to open output file', stderr) except subprocess.CalledProcessError as exc: self.assertNotEqual(exc.returncode, 0) self.assertTrue(os.path.isfile(output_path)) else: print('[!] Fail: open_close returned 0') self.fail() def test_101_open_flags(self): file_path=os.path.join(self.OUTPUT_DIR, 'test_101') stdout, stderr=self.run_binary(['open_flags', file_path]) self.verify_open_flags(stdout, stderr) def test_115_seek_tell(self): plaintext_path=self.INPUT_FILES[-1] input_path=self.ENCRYPTED_FILES[-1] output_path_1=os.path.join(self.OUTPUT_DIR, 'test_115a') output_path_2=os.path.join(self.OUTPUT_DIR, 'test_115b') self.copy_input(plaintext_path, output_path_1) self.copy_input(plaintext_path, output_path_2) stdout, stderr=self.run_binary(['seek_tell', input_path, output_path_1, output_path_2]) self.verify_seek_tell(stdout, stderr, input_path, output_path_1, output_path_2, self.FILE_SIZES[-1]) def test_130_file_stat(self): for i in self.INDEXES: input_path=self.ENCRYPTED_FILES[i] output_path=self.OUTPUT_FILES[i] size=str(self.FILE_SIZES[i]) self.copy_input(self.INPUT_FILES[i], output_path) stdout, stderr=self.run_binary(['stat', input_path, output_path]) self.verify_stat(stdout, stderr, input_path, output_path, size) def verify_size(self, file, size): dec_path=os.path.join(self.OUTPUT_DIR, os.path.basename(file) +'.dec') self.__decrypt_file(file, dec_path) self.assertEqual(os.stat(dec_path).st_size, size) @expectedFailureIf(HAS_SGX) def test_140_file_truncate(self): self.fail() def test_150_file_rename(self): path1=os.path.join(self.OUTPUT_DIR, 'test_150a') path2=os.path.join(self.OUTPUT_DIR, 'test_150b') self.copy_input(self.ENCRYPTED_FILES[-1], path1) shutil.copy(path1, path2) args=['decrypt', '-V', '-w', self.WRAP_KEY, '-i', path2, '-o', path1] try: self.__pf_crypt(args) except subprocess.CalledProcessError as exc: self.assertNotEqual(exc.returncode, 0) else: print('[!] Fail: successfully decrypted renamed file: ' +path2) self.fail() def verify_copy_content(self, input_path, output_path): dec_path=os.path.join(self.OUTPUT_DIR, os.path.basename(output_path) +'.dec') self.__decrypt_file(output_path, dec_path) self.assertTrue(filecmp.cmp(input_path, dec_path, shallow=False)) def do_copy_test(self, executable, timeout): stdout, stderr=self.run_binary([executable, self.ENCRYPTED_DIR, self.OUTPUT_DIR], timeout=timeout) self.verify_copy(stdout, stderr, self.ENCRYPTED_DIR, executable) def test_204_copy_dir_mmap_whole(self): self.do_copy_test('copy_mmap_whole', 30) def test_205_copy_dir_mmap_seq(self): self.do_copy_test('copy_mmap_seq', 60) def test_206_copy_dir_mmap_rev(self): self.do_copy_test('copy_mmap_rev', 60) def test_210_copy_dir_mounted(self): executable='copy_whole' stdout, stderr=self.run_binary([executable, '\/mounted\/pf_input', '\/mounted\/pf_output'], timeout=30) self.verify_copy(stdout, stderr, '\/mounted\/pf_input', executable) def __corrupt_file(self, input_path, output_path): cmd=[self.PF_TAMPER, '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path] return self.run_native_binary(cmd) @expectedFailureIf(HAS_SGX) def test_500_invalid(self): invalid_dir=os.path.join(self.TEST_DIR, 'pf_invalid') should_pass=['chunk_padding_1_fixed', 'chunk_padding_2_fixed', 'chunk_data_3', 'chunk_data_3_fixed', 'chunk_data_4', 'chunk_data_4_fixed'] if not os.path.exists(invalid_dir): os.mkdir(invalid_dir) original_input=self.OUTPUT_FILES[-1] self.__encrypt_file(self.INPUT_FILES[-1], original_input) self.__corrupt_file(original_input, invalid_dir) for name in os.listdir(invalid_dir): invalid=os.path.join(invalid_dir, name) output_path=os.path.join(self.OUTPUT_DIR, name) input_path=os.path.join(invalid_dir, os.path.basename(original_input)) shutil.copy(invalid, input_path) should_pass=any(s in name for s in should_pass) try: args=['decrypt', '-V', '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path] self.__pf_crypt(args) except subprocess.CalledProcessError as exc: if should_pass: self.assertEqual(exc.returncode, 0) else: self.assertNotEqual(exc.returncode, 0) else: if not should_pass: print('[!] Fail: successfully decrypted file: ' +name) self.fail() ","sourceWithComments":"#!\/usr\/bin\/env python3\n\nimport filecmp\nimport os\nimport shutil\nimport subprocess\nimport unittest\n\nfrom test_fs import (\n    TC_00_FileSystem,\n)\n\nfrom regression import (\n    HAS_SGX,\n    expectedFailureIf,\n)\n\n@unittest.skipUnless(HAS_SGX, 'Protected files require SGX support')\nclass TC_50_ProtectedFiles(TC_00_FileSystem):\n    @classmethod\n    def setUpClass(cls):\n        cls.PF_CRYPT = 'bin\/pf_crypt'\n        cls.PF_TAMPER = 'bin\/pf_tamper'\n        cls.WRAP_KEY = os.path.join(cls.TEST_DIR, 'wrap-key')\n        # CONST_WRAP_KEY must match the one in manifest\n        cls.CONST_WRAP_KEY = [0xff, 0xee, 0xdd, 0xcc, 0xbb, 0xaa, 0x99, 0x88,\n                              0x77, 0x66, 0x55, 0x44, 0x33, 0x22, 0x11, 0x00]\n        cls.ENCRYPTED_DIR = os.path.join(cls.TEST_DIR, 'pf_input')\n        cls.ENCRYPTED_FILES = [os.path.join(cls.ENCRYPTED_DIR, str(v)) for v in cls.FILE_SIZES]\n        cls.LIB_PATH = os.path.join(os.getcwd(), 'lib')\n\n        super().setUpClass()\n        if not os.path.exists(cls.ENCRYPTED_DIR):\n            os.mkdir(cls.ENCRYPTED_DIR)\n        cls.OUTPUT_DIR = os.path.join(cls.TEST_DIR, 'pf_output')\n        cls.OUTPUT_FILES = [os.path.join(cls.OUTPUT_DIR, str(x)) for x in cls.FILE_SIZES]\n        # create encrypted files\n        cls.__set_default_key(cls)\n        for i in cls.INDEXES:\n            cmd = [cls.PF_CRYPT, 'encrypt', '-w', cls.WRAP_KEY, '-i', cls.INPUT_FILES[i], '-o',\n                   cls.ENCRYPTED_FILES[i]]\n\n            cls.run_native_binary(cmd, libpath=os.path.join(os.getcwd(), 'lib'))\n\n    def __pf_crypt(self, args):\n        args.insert(0, self.PF_CRYPT)\n        return self.run_native_binary(args, libpath=os.path.join(os.getcwd(), 'lib'))\n\n    def __set_default_key(self):\n        with open(self.WRAP_KEY, 'wb') as file:\n            file.write(bytes(self.CONST_WRAP_KEY))\n\n    # overrides TC_00_FileSystem to encrypt the file instead of just copying\n    def copy_input(self, input_path, output_path):\n        self.__encrypt_file(input_path, output_path)\n\n    def __encrypt_file(self, input_path, output_path):\n        args = ['encrypt', '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path]\n        stdout, stderr = self.__pf_crypt(args)\n        return (stdout, stderr)\n\n    def __decrypt_file(self, input_path, output_path):\n        args = ['decrypt', '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path]\n        stdout, stderr = self.__pf_crypt(args)\n        return (stdout, stderr)\n\n    def test_000_gen_key(self):\n        # test random key generation\n        key_path = os.path.join(self.TEST_DIR, 'tmpkey')\n        args = ['gen-key', '-w', key_path]\n        stdout, _ = self.__pf_crypt(args)\n        self.assertIn('Wrap key saved to: ' + key_path, stdout)\n        self.assertEqual(os.path.getsize(key_path), 16)\n        os.remove(key_path)\n\n    def test_010_encrypt_decrypt(self):\n        for i in self.INDEXES:\n            self.__encrypt_file(self.INPUT_FILES[i], self.OUTPUT_FILES[i])\n            self.assertFalse(filecmp.cmp(self.INPUT_FILES[i], self.OUTPUT_FILES[i], shallow=False))\n            dec_path = os.path.join(self.OUTPUT_DIR,\n                                    os.path.basename(self.OUTPUT_FILES[i]) + '.dec')\n            self.__decrypt_file(self.OUTPUT_FILES[i], dec_path)\n            self.assertTrue(filecmp.cmp(self.INPUT_FILES[i], dec_path, shallow=False))\n\n    # overrides TC_00_FileSystem to change input dir (from plaintext to encrypted)\n    def test_100_open_close(self):\n        # the test binary expects a path to read-only (existing) file or a path to file that\n        # will get created\n        input_path = self.ENCRYPTED_FILES[-1] # existing file\n        output_path = os.path.join(self.OUTPUT_DIR, 'test_100') # new file\n        stdout, stderr = self.run_binary(['open_close', 'R', input_path])\n        self.verify_open_close(stdout, stderr, input_path, 'input')\n        # the following test tries to open multiple handles to a single writable PF, should fail\n        try:\n            stdout, stderr = self.run_binary(['open_close', 'W', output_path])\n            self.assertIn('ERROR: Failed to open output file', stderr)\n        except subprocess.CalledProcessError as exc:\n            self.assertNotEqual(exc.returncode, 0)\n            self.assertTrue(os.path.isfile(output_path))\n        else:\n            print('[!] Fail: open_close returned 0')\n            self.fail()\n\n    # overrides TC_00_FileSystem to change input dir (from plaintext to encrypted)\n    def test_101_open_flags(self):\n        # the test binary expects a path to file that will get created\n        file_path = os.path.join(self.OUTPUT_DIR, 'test_101') # new file\n        stdout, stderr = self.run_binary(['open_flags', file_path])\n        self.verify_open_flags(stdout, stderr)\n\n    # overrides TC_00_FileSystem to change input dir (from plaintext to encrypted)\n    def test_115_seek_tell(self):\n        # the test binary expects a path to read-only (existing) file and two paths to files that\n        # will get created\n        plaintext_path = self.INPUT_FILES[-1]\n        input_path = self.ENCRYPTED_FILES[-1] # existing file\n        output_path_1 = os.path.join(self.OUTPUT_DIR, 'test_115a') # writable files\n        output_path_2 = os.path.join(self.OUTPUT_DIR, 'test_115b')\n        self.copy_input(plaintext_path, output_path_1) # encrypt\n        self.copy_input(plaintext_path, output_path_2)\n        stdout, stderr = self.run_binary(['seek_tell', input_path, output_path_1, output_path_2])\n        self.verify_seek_tell(stdout, stderr, input_path, output_path_1, output_path_2,\n                              self.FILE_SIZES[-1])\n\n    # overrides TC_00_FileSystem to change input dir (from plaintext to encrypted)\n    def test_130_file_stat(self):\n        # the test binary expects a path to read-only (existing) file and a path to file that\n        # will get created\n        for i in self.INDEXES:\n            input_path = self.ENCRYPTED_FILES[i]\n            output_path = self.OUTPUT_FILES[i]\n            size = str(self.FILE_SIZES[i])\n            self.copy_input(self.INPUT_FILES[i], output_path)\n            stdout, stderr = self.run_binary(['stat', input_path, output_path])\n            self.verify_stat(stdout, stderr, input_path, output_path, size)\n\n    # overrides TC_00_FileSystem to decrypt output\n    def verify_size(self, file, size):\n        dec_path = os.path.join(self.OUTPUT_DIR, os.path.basename(file) + '.dec')\n        self.__decrypt_file(file, dec_path)\n        self.assertEqual(os.stat(dec_path).st_size, size)\n\n    @expectedFailureIf(HAS_SGX)\n    # pylint: disable=fixme\n    def test_140_file_truncate(self):\n        self.fail() # TODO: port these to the new file format\n\n    def test_150_file_rename(self):\n        path1 = os.path.join(self.OUTPUT_DIR, 'test_150a')\n        path2 = os.path.join(self.OUTPUT_DIR, 'test_150b')\n        self.copy_input(self.ENCRYPTED_FILES[-1], path1)\n        shutil.copy(path1, path2)\n        # accessing renamed file should fail\n        args = ['decrypt', '-V', '-w', self.WRAP_KEY, '-i', path2, '-o', path1]\n        try:\n            self.__pf_crypt(args)\n        except subprocess.CalledProcessError as exc:\n            self.assertNotEqual(exc.returncode, 0)\n        else:\n            print('[!] Fail: successfully decrypted renamed file: ' + path2)\n            self.fail()\n\n    # overrides TC_00_FileSystem to decrypt output\n    def verify_copy_content(self, input_path, output_path):\n        dec_path = os.path.join(self.OUTPUT_DIR, os.path.basename(output_path) + '.dec')\n        self.__decrypt_file(output_path, dec_path)\n        self.assertTrue(filecmp.cmp(input_path, dec_path, shallow=False))\n\n    # overrides TC_00_FileSystem to change input dir (from plaintext to encrypted)\n    def do_copy_test(self, executable, timeout):\n        stdout, stderr = self.run_binary([executable, self.ENCRYPTED_DIR, self.OUTPUT_DIR],\n                                         timeout=timeout)\n        self.verify_copy(stdout, stderr, self.ENCRYPTED_DIR, executable)\n\n    # overrides TC_00_FileSystem to not skip this on SGX\n    def test_204_copy_dir_mmap_whole(self):\n        self.do_copy_test('copy_mmap_whole', 30)\n\n    # overrides TC_00_FileSystem to not skip this on SGX\n    def test_205_copy_dir_mmap_seq(self):\n        self.do_copy_test('copy_mmap_seq', 60)\n\n    # overrides TC_00_FileSystem to not skip this on SGX\n    def test_206_copy_dir_mmap_rev(self):\n        self.do_copy_test('copy_mmap_rev', 60)\n\n    # overrides TC_00_FileSystem to change dirs (from plaintext to encrypted)\n    def test_210_copy_dir_mounted(self):\n        executable = 'copy_whole'\n        stdout, stderr = self.run_binary([executable, '\/mounted\/pf_input', '\/mounted\/pf_output'],\n                                         timeout=30)\n        self.verify_copy(stdout, stderr, '\/mounted\/pf_input', executable)\n\n    def __corrupt_file(self, input_path, output_path):\n        cmd = [self.PF_TAMPER, '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path]\n        return self.run_native_binary(cmd)\n\n    # invalid\/corrupted files\n    @expectedFailureIf(HAS_SGX)\n    # pylint: disable=fixme\n    def test_500_invalid(self):\n        # TODO: port these to the new file format\n        invalid_dir = os.path.join(self.TEST_DIR, 'pf_invalid')\n        # files below should work normally (benign modifications)\n        should_pass = ['chunk_padding_1_fixed', 'chunk_padding_2_fixed', 'chunk_data_3',\n                       'chunk_data_3_fixed', 'chunk_data_4', 'chunk_data_4_fixed']\n        if not os.path.exists(invalid_dir):\n            os.mkdir(invalid_dir)\n        # prepare valid encrypted file (largest one for maximum possible corruptions)\n        original_input = self.OUTPUT_FILES[-1]\n        self.__encrypt_file(self.INPUT_FILES[-1], original_input)\n        # generate invalid files based on the above\n        self.__corrupt_file(original_input, invalid_dir)\n        # try to decrypt invalid files\n        for name in os.listdir(invalid_dir):\n            invalid = os.path.join(invalid_dir, name)\n            output_path = os.path.join(self.OUTPUT_DIR, name)\n            input_path = os.path.join(invalid_dir, os.path.basename(original_input))\n            # copy the file so it has the original file name (for allowed path check)\n            shutil.copy(invalid, input_path)\n            should_pass = any(s in name for s in should_pass)\n\n            try:\n                args = ['decrypt', '-V', '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path]\n                self.__pf_crypt(args)\n            except subprocess.CalledProcessError as exc:\n                if should_pass:\n                    self.assertEqual(exc.returncode, 0)\n                else:\n                    self.assertNotEqual(exc.returncode, 0)\n            else:\n                if not should_pass:\n                    print('[!] Fail: successfully decrypted file: ' + name)\n                    self.fail()\n"}},"msg":"[Pal\/Linux-SGX] Test malicious modifications to protected files\n\nThis commit adds a new PF utility `pf_tamper` that tampers with\nvalid protected files and uses this utility to test that the PF\nlogic in Linux-SGX detects such malicious modifications.\n\nThis commit also moves out the PF-format macros and structs from\n`protected_files_internal.h` to `protected_files_format.h` for\nbetter readability.\n\nCo-authored-by: Dmitrii Kuvaiskii <dmitrii.kuvaiskii@intel.com>"}},"https:\/\/github.com\/gramineproject\/gramine":{"567790aec46e0714171121318783b8ac8c56adc3":{"url":"https:\/\/api.github.com\/repos\/gramineproject\/gramine\/commits\/567790aec46e0714171121318783b8ac8c56adc3","html_url":"https:\/\/github.com\/gramineproject\/gramine\/commit\/567790aec46e0714171121318783b8ac8c56adc3","sha":"567790aec46e0714171121318783b8ac8c56adc3","keyword":"tampering malicious","diff":"diff --git a\/LibOS\/shim\/test\/fs\/test_pf.py b\/LibOS\/shim\/test\/fs\/test_pf.py\nindex e430e02af8..7b1621d7ae 100644\n--- a\/LibOS\/shim\/test\/fs\/test_pf.py\n+++ b\/LibOS\/shim\/test\/fs\/test_pf.py\n@@ -193,24 +193,20 @@ def test_210_copy_dir_mounted(self):\n \n     def __corrupt_file(self, input_path, output_path):\n         cmd = [self.PF_TAMPER, '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path]\n-        return self.run_native_binary(cmd)\n+        return self.run_native_binary(cmd, libpath=os.path.join(os.getcwd(), 'lib'))\n \n     # invalid\/corrupted files\n-    @expectedFailureIf(HAS_SGX)\n-    # pylint: disable=fixme\n     def test_500_invalid(self):\n-        # TODO: port these to the new file format\n         invalid_dir = os.path.join(self.TEST_DIR, 'pf_invalid')\n-        # files below should work normally (benign modifications)\n-        should_pass = ['chunk_padding_1_fixed', 'chunk_padding_2_fixed', 'chunk_data_3',\n-                       'chunk_data_3_fixed', 'chunk_data_4', 'chunk_data_4_fixed']\n         if not os.path.exists(invalid_dir):\n             os.mkdir(invalid_dir)\n+\n         # prepare valid encrypted file (largest one for maximum possible corruptions)\n         original_input = self.OUTPUT_FILES[-1]\n         self.__encrypt_file(self.INPUT_FILES[-1], original_input)\n         # generate invalid files based on the above\n         self.__corrupt_file(original_input, invalid_dir)\n+\n         # try to decrypt invalid files\n         for name in os.listdir(invalid_dir):\n             invalid = os.path.join(invalid_dir, name)\n@@ -218,17 +214,13 @@ def test_500_invalid(self):\n             input_path = os.path.join(invalid_dir, os.path.basename(original_input))\n             # copy the file so it has the original file name (for allowed path check)\n             shutil.copy(invalid, input_path)\n-            should_pass = any(s in name for s in should_pass)\n \n             try:\n                 args = ['decrypt', '-V', '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path]\n                 self.__pf_crypt(args)\n             except subprocess.CalledProcessError as exc:\n-                if should_pass:\n-                    self.assertEqual(exc.returncode, 0)\n-                else:\n-                    self.assertNotEqual(exc.returncode, 0)\n+                # decryption of invalid file must fail with -1 (wrapped to 255)\n+                self.assertEqual(exc.returncode, 255)\n             else:\n-                if not should_pass:\n-                    print('[!] Fail: successfully decrypted file: ' + name)\n-                    self.fail()\n+                print('[!] Fail: successfully decrypted file: ' + name)\n+                self.fail()\ndiff --git a\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files.c b\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files.c\nindex 4f0c59ba23..d020a1e24a 100644\n--- a\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files.c\n+++ b\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files.c\n@@ -5,6 +5,8 @@\n  *\/\n \n #include \"api.h\"\n+#include \"protected_files.h\"\n+#include \"protected_files_format.h\"\n #include \"protected_files_internal.h\"\n \n #ifndef IN_PAL\n@@ -65,20 +67,6 @@ static pf_random_f          g_cb_random          = NULL;\n static pf_iv_t g_empty_iv = {0};\n static bool g_initialized = false;\n \n-#define METADATA_KEY_NAME \"SGX-PROTECTED-FS-METADATA-KEY\"\n-#define MAX_LABEL_SIZE    64\n-\n-static_assert(sizeof(METADATA_KEY_NAME) <= MAX_LABEL_SIZE, \"label too long\");\n-\n-#pragma pack(push, 1)\n-typedef struct {\n-    uint32_t index;\n-    char label[MAX_LABEL_SIZE]; \/\/ must be NULL terminated\n-    pf_keyid_t nonce;\n-    uint32_t output_len; \/\/ in bits\n-} kdf_input_t;\n-#pragma pack(pop)\n-\n \/\/ The key derivation function follow recommendations from NIST Special Publication 800-108:\n \/\/ Recommendation for Key Derivation Using Pseudorandom Functions\n \/\/ https:\/\/nvlpubs.nist.gov\/nistpubs\/Legacy\/SP\/nistspecialpublication800-108.pdf\ndiff --git a\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files_format.h b\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files_format.h\nnew file mode 100644\nindex 0000000000..2394e53ef7\n--- \/dev\/null\n+++ b\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files_format.h\n@@ -0,0 +1,139 @@\n+\/* SPDX-License-Identifier: LGPL-3.0-or-later *\/\n+\/* Copyright (C) 2019-2020 Invisible Things Lab\n+ *                         Rafal Wojdyla <omeg@invisiblethingslab.com>\n+ * Copyright (C) 2011-2020 Intel Corporation\n+ *\/\n+\n+#ifndef PROTECTED_FILES_FORMAT_H_\n+#define PROTECTED_FILES_FORMAT_H_\n+\n+#include <limits.h>\n+\n+#include \"assert.h\"\n+#include \"list.h\"\n+#include \"protected_files.h\"\n+\n+#define PF_FILE_ID       0x46505f4850415247 \/* GRAPH_PF *\/\n+#define PF_MAJOR_VERSION 0x01\n+#define PF_MINOR_VERSION 0x00\n+\n+#define METADATA_KEY_NAME \"SGX-PROTECTED-FS-METADATA-KEY\"\n+#define MAX_LABEL_SIZE    64\n+\n+static_assert(sizeof(METADATA_KEY_NAME) <= MAX_LABEL_SIZE, \"label too long\");\n+\n+#pragma pack(push, 1)\n+\n+typedef struct _metadata_plain {\n+    uint64_t   file_id;\n+    uint8_t    major_version;\n+    uint8_t    minor_version;\n+    pf_keyid_t metadata_key_id;\n+    pf_mac_t   metadata_gmac; \/* GCM mac *\/\n+} metadata_plain_t;\n+\n+#define PATH_MAX_SIZE (260 + 512)\n+\n+\/\/ these are all defined as relative to node size, so we can decrease node size in tests\n+\/\/ and have deeper tree\n+#define MD_USER_DATA_SIZE (PF_NODE_SIZE * 3 \/ 4) \/\/ 3072\n+static_assert(MD_USER_DATA_SIZE == 3072, \"bad struct size\");\n+\n+typedef struct _metadata_encrypted {\n+    char     path[PATH_MAX_SIZE];\n+    uint64_t size;\n+    pf_key_t mht_key;\n+    pf_mac_t mht_gmac;\n+    uint8_t  data[MD_USER_DATA_SIZE];\n+} metadata_encrypted_t;\n+\n+typedef uint8_t metadata_encrypted_blob_t[sizeof(metadata_encrypted_t)];\n+\n+#define METADATA_NODE_SIZE PF_NODE_SIZE\n+\n+typedef uint8_t metadata_padding_t[METADATA_NODE_SIZE -\n+                                   (sizeof(metadata_plain_t) + sizeof(metadata_encrypted_blob_t))];\n+\n+typedef struct _metadata_node {\n+    metadata_plain_t          plain_part;\n+    metadata_encrypted_blob_t encrypted_part;\n+    metadata_padding_t        padding;\n+} metadata_node_t;\n+\n+static_assert(sizeof(metadata_node_t) == PF_NODE_SIZE, \"sizeof(metadata_node_t)\");\n+\n+typedef struct _data_node_crypto {\n+    pf_key_t key;\n+    pf_mac_t gmac;\n+} gcm_crypto_data_t;\n+\n+\/\/ for PF_NODE_SIZE == 4096, we have 96 attached data nodes and 32 mht child nodes\n+\/\/ for PF_NODE_SIZE == 2048, we have 48 attached data nodes and 16 mht child nodes\n+\/\/ for PF_NODE_SIZE == 1024, we have 24 attached data nodes and 8 mht child nodes\n+\/\/ 3\/4 of the node size is dedicated to data nodes\n+#define ATTACHED_DATA_NODES_COUNT ((PF_NODE_SIZE \/ sizeof(gcm_crypto_data_t)) * 3 \/ 4)\n+static_assert(ATTACHED_DATA_NODES_COUNT == 96, \"ATTACHED_DATA_NODES_COUNT\");\n+\/\/ 1\/4 of the node size is dedicated to child mht nodes\n+#define CHILD_MHT_NODES_COUNT ((PF_NODE_SIZE \/ sizeof(gcm_crypto_data_t)) * 1 \/ 4)\n+static_assert(CHILD_MHT_NODES_COUNT == 32, \"CHILD_MHT_NODES_COUNT\");\n+\n+typedef struct _mht_node {\n+    gcm_crypto_data_t data_nodes_crypto[ATTACHED_DATA_NODES_COUNT];\n+    gcm_crypto_data_t mht_nodes_crypto[CHILD_MHT_NODES_COUNT];\n+} mht_node_t;\n+\n+static_assert(sizeof(mht_node_t) == PF_NODE_SIZE, \"sizeof(mht_node_t)\");\n+\n+typedef struct _data_node {\n+    uint8_t data[PF_NODE_SIZE];\n+} data_node_t;\n+\n+static_assert(sizeof(data_node_t) == PF_NODE_SIZE, \"sizeof(data_node_t)\");\n+\n+typedef struct _encrypted_node {\n+    uint8_t cipher[PF_NODE_SIZE];\n+} encrypted_node_t;\n+\n+static_assert(sizeof(encrypted_node_t) == PF_NODE_SIZE, \"sizeof(encrypted_node_t)\");\n+\n+#define MAX_PAGES_IN_CACHE 48\n+\n+typedef enum {\n+    FILE_MHT_NODE_TYPE  = 1,\n+    FILE_DATA_NODE_TYPE = 2,\n+} mht_node_type_e;\n+\n+\/\/ make sure these are the same size\n+static_assert(sizeof(mht_node_t) == sizeof(data_node_t),\n+              \"sizeof(mht_node_t) == sizeof(data_node_t)\");\n+\n+DEFINE_LIST(_file_node);\n+typedef struct _file_node {\n+    LIST_TYPE(_file_node) list;\n+    uint8_t type;\n+    uint64_t node_number;\n+    struct _file_node* parent;\n+    bool need_writing;\n+    bool new_node;\n+    struct {\n+        uint64_t physical_node_number;\n+        encrypted_node_t encrypted; \/\/ the actual data from the disk\n+    };\n+    union { \/\/ decrypted data\n+        mht_node_t mht;\n+        data_node_t data;\n+    } decrypted;\n+} file_node_t;\n+DEFINE_LISTP(_file_node);\n+\n+typedef struct {\n+    uint32_t index;\n+    char label[MAX_LABEL_SIZE]; \/\/ must be NULL terminated\n+    pf_keyid_t nonce;\n+    uint32_t output_len; \/\/ in bits\n+} kdf_input_t;\n+\n+#pragma pack(pop)\n+\n+#endif \/* PROTECTED_FILES_FORMAT_H_ *\/\n+\ndiff --git a\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files_internal.h b\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files_internal.h\nindex 4a31261d58..c06d1ecf69 100644\n--- a\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files_internal.h\n+++ b\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files_internal.h\n@@ -13,116 +13,7 @@\n #include \"list.h\"\n #include \"lru_cache.h\"\n #include \"protected_files.h\"\n-\n-#define PF_FILE_ID       0x46505f4850415247 \/* GRAPH_PF *\/\n-#define PF_MAJOR_VERSION 0x01\n-#define PF_MINOR_VERSION 0x00\n-\n-#pragma pack(push, 1)\n-\n-typedef struct _metadata_plain {\n-    uint64_t   file_id;\n-    uint8_t    major_version;\n-    uint8_t    minor_version;\n-    pf_keyid_t metadata_key_id;\n-    pf_mac_t   metadata_gmac; \/* GCM mac *\/\n-} metadata_plain_t;\n-\n-#define PATH_MAX_SIZE (260 + 512)\n-\n-\/\/ these are all defined as relative to node size, so we can decrease node size in tests\n-\/\/ and have deeper tree\n-#define MD_USER_DATA_SIZE (PF_NODE_SIZE * 3 \/ 4) \/\/ 3072\n-static_assert(MD_USER_DATA_SIZE == 3072, \"bad struct size\");\n-\n-typedef struct _metadata_encrypted {\n-    char     path[PATH_MAX_SIZE];\n-    uint64_t size;\n-    pf_key_t mht_key;\n-    pf_mac_t mht_gmac;\n-    uint8_t  data[MD_USER_DATA_SIZE];\n-} metadata_encrypted_t;\n-\n-typedef uint8_t metadata_encrypted_blob_t[sizeof(metadata_encrypted_t)];\n-\n-#define METADATA_NODE_SIZE PF_NODE_SIZE\n-\n-typedef uint8_t metadata_padding_t[METADATA_NODE_SIZE -\n-                                   (sizeof(metadata_plain_t) + sizeof(metadata_encrypted_blob_t))];\n-\n-typedef struct _metadata_node {\n-    metadata_plain_t          plain_part;\n-    metadata_encrypted_blob_t encrypted_part;\n-    metadata_padding_t        padding;\n-} metadata_node_t;\n-\n-static_assert(sizeof(metadata_node_t) == PF_NODE_SIZE, \"sizeof(metadata_node_t)\");\n-\n-typedef struct _data_node_crypto {\n-    pf_key_t key;\n-    pf_mac_t gmac;\n-} gcm_crypto_data_t;\n-\n-\/\/ for PF_NODE_SIZE == 4096, we have 96 attached data nodes and 32 mht child nodes\n-\/\/ for PF_NODE_SIZE == 2048, we have 48 attached data nodes and 16 mht child nodes\n-\/\/ for PF_NODE_SIZE == 1024, we have 24 attached data nodes and 8 mht child nodes\n-\/\/ 3\/4 of the node size is dedicated to data nodes\n-#define ATTACHED_DATA_NODES_COUNT ((PF_NODE_SIZE \/ sizeof(gcm_crypto_data_t)) * 3 \/ 4)\n-static_assert(ATTACHED_DATA_NODES_COUNT == 96, \"ATTACHED_DATA_NODES_COUNT\");\n-\/\/ 1\/4 of the node size is dedicated to child mht nodes\n-#define CHILD_MHT_NODES_COUNT ((PF_NODE_SIZE \/ sizeof(gcm_crypto_data_t)) * 1 \/ 4)\n-static_assert(CHILD_MHT_NODES_COUNT == 32, \"CHILD_MHT_NODES_COUNT\");\n-\n-typedef struct _mht_node {\n-    gcm_crypto_data_t data_nodes_crypto[ATTACHED_DATA_NODES_COUNT];\n-    gcm_crypto_data_t mht_nodes_crypto[CHILD_MHT_NODES_COUNT];\n-} mht_node_t;\n-\n-static_assert(sizeof(mht_node_t) == PF_NODE_SIZE, \"sizeof(mht_node_t)\");\n-\n-typedef struct _data_node {\n-    uint8_t data[PF_NODE_SIZE];\n-} data_node_t;\n-\n-static_assert(sizeof(data_node_t) == PF_NODE_SIZE, \"sizeof(data_node_t)\");\n-\n-typedef struct _encrypted_node {\n-    uint8_t cipher[PF_NODE_SIZE];\n-} encrypted_node_t;\n-\n-static_assert(sizeof(encrypted_node_t) == PF_NODE_SIZE, \"sizeof(encrypted_node_t)\");\n-\n-#define MAX_PAGES_IN_CACHE 48\n-\n-typedef enum {\n-    FILE_MHT_NODE_TYPE  = 1,\n-    FILE_DATA_NODE_TYPE = 2,\n-} mht_node_type_e;\n-\n-\/\/ make sure these are the same size\n-static_assert(sizeof(mht_node_t) == sizeof(data_node_t),\n-              \"sizeof(mht_node_t) == sizeof(data_node_t)\");\n-\n-DEFINE_LIST(_file_node);\n-typedef struct _file_node {\n-    LIST_TYPE(_file_node) list;\n-    uint8_t type;\n-    uint64_t node_number;\n-    struct _file_node* parent;\n-    bool need_writing;\n-    bool new_node;\n-    struct {\n-        uint64_t physical_node_number;\n-        encrypted_node_t encrypted; \/\/ the actual data from the disk\n-    };\n-    union { \/\/ decrypted data\n-        mht_node_t mht;\n-        data_node_t data;\n-    } decrypted;\n-} file_node_t;\n-DEFINE_LISTP(_file_node);\n-\n-#pragma pack(pop)\n+#include \"protected_files_format.h\"\n \n struct pf_context {\n     metadata_node_t file_metadata; \/\/ actual data from disk's meta data node\ndiff --git a\/Pal\/src\/host\/Linux-SGX\/tools\/Makefile b\/Pal\/src\/host\/Linux-SGX\/tools\/Makefile\nindex a5903ad970..87bc78124b 100644\n--- a\/Pal\/src\/host\/Linux-SGX\/tools\/Makefile\n+++ b\/Pal\/src\/host\/Linux-SGX\/tools\/Makefile\n@@ -12,3 +12,4 @@ $(targets):\n \t$(MAKE) -C verify-ias-report $@\n \t$(MAKE) -C ra-tls $@\n \t$(MAKE) -C pf_crypt $@\n+\t$(MAKE) -C pf_tamper $@\ndiff --git a\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/.gitignore b\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/.gitignore\nnew file mode 100644\nindex 0000000000..a52536d82e\n--- \/dev\/null\n+++ b\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/.gitignore\n@@ -0,0 +1 @@\n+\/pf_tamper\ndiff --git a\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/Makefile b\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/Makefile\nnew file mode 100644\nindex 0000000000..94c826e4bd\n--- \/dev\/null\n+++ b\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/Makefile\n@@ -0,0 +1,31 @@\n+include ..\/..\/..\/..\/..\/..\/Scripts\/Makefile.configs\n+include ..\/..\/..\/..\/..\/..\/Scripts\/Makefile.rules\n+\n+CFLAGS += -I..\/.. \\\n+          -I..\/common \\\n+          -I..\/..\/protected-files \\\n+          -I..\/..\/..\/..\/..\/include\/lib \\\n+          -D_GNU_SOURCE\n+\n+LDLIBS += -L..\/common \\\n+          -L..\/..\/..\/..\/..\/lib\/crypto\/mbedtls\/install\/lib \\\n+          -lsgx_util -lmbedcrypto\n+\n+PREFIX ?= \/usr\/local\n+\n+pf_tamper: pf_tamper.o\n+\t$(call cmd,csingle)\n+\n+.PHONY: all\n+all: pf_tamper\n+\n+.PHONY: install\n+install:\n+\tinstall -D pf_tamper -t ${PREFIX}\/bin\n+\n+.PHONY: clean\n+clean:\n+\t$(RM) *.o pf_tamper\n+\n+.PHONY: distclean\n+distclean: clean\ndiff --git a\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/pf_tamper.c b\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/pf_tamper.c\nnew file mode 100644\nindex 0000000000..4d39a0d586\n--- \/dev\/null\n+++ b\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/pf_tamper.c\n@@ -0,0 +1,476 @@\n+\/* SPDX-License-Identifier: LGPL-3.0-or-later *\/\n+\/* Copyright (C) 2019-2020 Invisible Things Lab\n+ *                         Rafal Wojdyla <omeg@invisiblethingslab.com>\n+ *\/\n+\n+#include <fcntl.h>\n+#include <getopt.h>\n+#include <stdlib.h>\n+#include <sys\/mman.h>\n+#include <unistd.h>\n+\n+#include \"pf_util.h\"\n+#include \"protected_files.h\"\n+#include \"protected_files_format.h\"\n+#include \"util.h\"\n+\n+\/* Tamper with a PF in various ways for testing purposes. The PF is assumed to be valid and have at\n+ * least enough data to contain two MHT nodes. *\/\n+\n+\/* Command line options *\/\n+struct option g_options[] = {\n+    { \"input\", required_argument, 0, 'i' },\n+    { \"output\", required_argument, 0, 'o' },\n+    { \"wrap-key\", required_argument, 0, 'w' },\n+    { \"verbose\", no_argument, 0, 'v' },\n+    { \"help\", no_argument, 0, 'h' },\n+    { 0, 0, 0, 0 }\n+};\n+\n+static void usage(void) {\n+    INFO(\"\\nUsage: pf_tamper [options]\\n\");\n+    INFO(\"\\nAvailable options:\\n\");\n+    INFO(\"  --help, -h           Display this help\\n\");\n+    INFO(\"  --verbose, -v        Enable verbose output\\n\");\n+    INFO(\"  --wrap-key, -w PATH  Path to wrap key file\\n\");\n+    INFO(\"  --input, -i PATH     Source file to be tampered with (must be a valid PF)\\n\");\n+    INFO(\"  --output, -o PATH    Directory where modified files will be written to\\n\");\n+}\n+\n+#define FATAL(fmt, ...) do { \\\n+    ERROR(fmt, ##__VA_ARGS__); \\\n+    exit(-1); \\\n+} while (0)\n+\n+ssize_t g_input_size = 0;\n+char* g_input_name = NULL;\n+void* g_input_data = MAP_FAILED;\n+char* g_output_dir = NULL;\n+char* g_output_path = NULL;\n+size_t g_output_path_size = 0;\n+pf_key_t g_wrap_key;\n+pf_key_t g_meta_key;\n+\n+static pf_iv_t g_empty_iv = {0};\n+\n+static void derive_main_key(const pf_key_t* kdk, const pf_keyid_t* key_id, pf_key_t* out_key) {\n+    kdf_input_t buf = {0};\n+    pf_status_t status;\n+\n+    buf.index = 1;\n+    strncpy(buf.label, METADATA_KEY_NAME, MAX_LABEL_SIZE);\n+    memcpy(&buf.nonce, key_id, sizeof(buf.nonce));\n+    buf.output_len = 0x80;\n+\n+    status = mbedtls_aes_gcm_encrypt(kdk, &g_empty_iv, &buf, sizeof(buf), NULL, 0, NULL, out_key);\n+    if (PF_FAILURE(status))\n+        FATAL(\"key derivation failed\\n\");\n+}\n+\n+static void make_output_path(const char* suffix) {\n+    snprintf(g_output_path, g_output_path_size, \"%s\/%s.%s\", g_output_dir, g_input_name, suffix);\n+    INFO(\"[*] %s\\n\", g_output_path);\n+}\n+\n+\/* PF layout (node size is PF_NODE_SIZE):\n+ * - Node 0: metadata (metadata_node_t)\n+ *   - metadata_plain_t\n+ *   - metadata_encrypted_t (may include MD_USER_DATA_SIZE bytes of data)\n+ *   - metadata_padding_t\n+ * - Node 1: MHT (mht_node_t)\n+ * - Node 2-97: data (ATTACHED_DATA_NODES_COUNT == 96)\n+ * - Node 98: MHT\n+ * - Node 99-195: data\n+ * - ...\n+ *\/\n+static void truncate_file(const char* suffix, size_t output_size) {\n+    int ret;\n+\n+    make_output_path(suffix);\n+\n+    if (output_size < g_input_size) {\n+        ret = write_file(g_output_path, output_size, g_input_data);\n+    } else {\n+        ret = write_file(g_output_path, g_input_size, g_input_data);\n+        if (ret < 0)\n+            goto out;\n+        ret = truncate(g_output_path, output_size);\n+    }\n+out:\n+    if (ret < 0)\n+        FATAL(\"truncate_file failed: %d\\n\", ret);\n+}\n+\n+#define FIELD_SIZEOF(t, f) (sizeof(((t*)0)->f))\n+#define FIELD_TRUNCATED(t, f) (offsetof(t, f) + (FIELD_SIZEOF(t, f) \/ 2))\n+#define DATA_CRYPTO_SIZE (FIELD_SIZEOF(mht_node_t, data_nodes_crypto))\n+\n+static void tamper_truncate(void) {\n+    size_t mdps = sizeof(metadata_plain_t);\n+    DBG(\"size(metadata_plain_t)             = 0x%04lx\\n\", sizeof(metadata_plain_t));\n+    DBG(\"metadata_plain_t.file_id           : 0x%04lx (0x%04lx)\\n\",\n+        offsetof(metadata_plain_t, file_id), FIELD_SIZEOF(metadata_plain_t, file_id));\n+    DBG(\"metadata_plain_t.major_version     : 0x%04lx (0x%04lx)\\n\",\n+        offsetof(metadata_plain_t, major_version), FIELD_SIZEOF(metadata_plain_t, major_version));\n+    DBG(\"metadata_plain_t.minor_version     : 0x%04lx (0x%04lx)\\n\",\n+        offsetof(metadata_plain_t, minor_version), FIELD_SIZEOF(metadata_plain_t, minor_version));\n+    DBG(\"metadata_plain_t.metadata_key_id   : 0x%04lx (0x%04lx)\\n\",\n+        offsetof(metadata_plain_t, metadata_key_id),\n+        FIELD_SIZEOF(metadata_plain_t, metadata_key_id));\n+    DBG(\"metadata_plain_t.metadata_gmac     : 0x%04lx (0x%04lx)\\n\",\n+        offsetof(metadata_plain_t, metadata_gmac),\n+        FIELD_SIZEOF(metadata_plain_t, metadata_gmac));\n+\n+    DBG(\"size(metadata_encrypted_t)         = 0x%04lx\\n\", sizeof(metadata_encrypted_t));\n+    DBG(\"metadata_encrypted_t.path          : 0x%04lx (0x%04lx)\\n\",\n+        mdps + offsetof(metadata_encrypted_t, path),\n+        FIELD_SIZEOF(metadata_encrypted_t, path));\n+    DBG(\"metadata_encrypted_t.size          : 0x%04lx (0x%04lx)\\n\",\n+        mdps + offsetof(metadata_encrypted_t, size), FIELD_SIZEOF(metadata_encrypted_t, size));\n+    DBG(\"metadata_encrypted_t.mht_key       : 0x%04lx (0x%04lx)\\n\",\n+        mdps + offsetof(metadata_encrypted_t, mht_key),\n+        FIELD_SIZEOF(metadata_encrypted_t, mht_key));\n+    DBG(\"metadata_encrypted_t.mht_gmac      : 0x%04lx (0x%04lx)\\n\",\n+        mdps + offsetof(metadata_encrypted_t, mht_gmac),\n+        FIELD_SIZEOF(metadata_encrypted_t, mht_gmac));\n+    DBG(\"metadata_encrypted_t.data          : 0x%04lx (0x%04lx)\\n\",\n+        mdps + offsetof(metadata_encrypted_t, data), FIELD_SIZEOF(metadata_encrypted_t, data));\n+\n+    DBG(\"size(metadata_padding_t)           = 0x%04lx\\n\", sizeof(metadata_padding_t));\n+    DBG(\"metadata_padding_t                 : 0x%04lx (0x%04lx)\\n\",\n+        mdps + sizeof(metadata_encrypted_t), sizeof(metadata_padding_t));\n+\n+    \/* node 0: metadata + 3k of user data *\/\n+    \/* plain metadata *\/\n+    truncate_file(\"trunc_meta_plain_0\", 0);\n+    truncate_file(\"trunc_meta_plain_1\", FIELD_TRUNCATED(metadata_plain_t, file_id));\n+    truncate_file(\"trunc_meta_plain_2\", offsetof(metadata_plain_t, major_version));\n+    truncate_file(\"trunc_meta_plain_3\", offsetof(metadata_plain_t, minor_version));\n+    truncate_file(\"trunc_meta_plain_4\", offsetof(metadata_plain_t, metadata_key_id));\n+    truncate_file(\"trunc_meta_plain_5\", FIELD_TRUNCATED(metadata_plain_t, metadata_key_id));\n+    truncate_file(\"trunc_meta_plain_6\", offsetof(metadata_plain_t, metadata_gmac));\n+    truncate_file(\"trunc_meta_plain_7\", FIELD_TRUNCATED(metadata_plain_t, metadata_gmac));\n+\n+    \/* encrypted metadata *\/\n+    truncate_file(\"trunc_meta_enc_0\", mdps + offsetof(metadata_encrypted_t, path));\n+    truncate_file(\"trunc_meta_enc_1\", mdps + FIELD_TRUNCATED(metadata_encrypted_t, path));\n+    truncate_file(\"trunc_meta_enc_2\", mdps + offsetof(metadata_encrypted_t, size));\n+    truncate_file(\"trunc_meta_enc_3\", mdps + FIELD_TRUNCATED(metadata_encrypted_t, size));\n+    truncate_file(\"trunc_meta_enc_4\", mdps + offsetof(metadata_encrypted_t, mht_key));\n+    truncate_file(\"trunc_meta_enc_5\", mdps + FIELD_TRUNCATED(metadata_encrypted_t, mht_key));\n+    truncate_file(\"trunc_meta_enc_6\", mdps + offsetof(metadata_encrypted_t, mht_gmac));\n+    truncate_file(\"trunc_meta_enc_7\", mdps + FIELD_TRUNCATED(metadata_encrypted_t, mht_gmac));\n+    truncate_file(\"trunc_meta_enc_8\", mdps + offsetof(metadata_encrypted_t, data));\n+    truncate_file(\"trunc_meta_enc_9\", mdps + FIELD_TRUNCATED(metadata_encrypted_t, data));\n+\n+    \/* padding *\/\n+    truncate_file(\"trunc_meta_pad_0\", mdps + sizeof(metadata_encrypted_t));\n+    truncate_file(\"trunc_meta_pad_1\", mdps + sizeof(metadata_encrypted_t)\n+                  + sizeof(metadata_padding_t) \/ 2);\n+\n+    \/* node 1: mht root *\/\n+    \/* after node 0 *\/\n+    truncate_file(\"trunc_mht_0\", PF_NODE_SIZE);\n+    \/* middle of data_nodes_crypto[0].key *\/\n+    truncate_file(\"trunc_mht_1\", PF_NODE_SIZE + PF_KEY_SIZE \/ 2);\n+    \/* after data_nodes_crypto[0].key *\/\n+    truncate_file(\"trunc_mht_2\", PF_NODE_SIZE + PF_KEY_SIZE);\n+    \/* middle of data_nodes_crypto[0].gmac *\/\n+    truncate_file(\"trunc_mht_3\", PF_NODE_SIZE + PF_KEY_SIZE + PF_MAC_SIZE \/ 2);\n+    \/* after data_nodes_crypto[0].gmac *\/\n+    truncate_file(\"trunc_mht_4\", PF_NODE_SIZE + PF_KEY_SIZE + PF_MAC_SIZE);\n+    \/* after data_nodes_crypto *\/\n+    truncate_file(\"trunc_mht_5\", PF_NODE_SIZE + DATA_CRYPTO_SIZE);\n+    \/* middle of mht_nodes_crypto[0].key *\/\n+    truncate_file(\"trunc_mht_6\", PF_NODE_SIZE + DATA_CRYPTO_SIZE + PF_KEY_SIZE \/ 2);\n+    \/* after mht_nodes_crypto[0].key *\/\n+    truncate_file(\"trunc_mht_7\", PF_NODE_SIZE + DATA_CRYPTO_SIZE + PF_KEY_SIZE);\n+    \/* middle of mht_nodes_crypto[0].gmac *\/\n+    truncate_file(\"trunc_mht_8\", PF_NODE_SIZE + DATA_CRYPTO_SIZE + PF_KEY_SIZE + PF_MAC_SIZE \/ 2);\n+    \/* after mht_nodes_crypto[0].gmac *\/\n+    truncate_file(\"trunc_mht_9\", PF_NODE_SIZE + DATA_CRYPTO_SIZE + PF_KEY_SIZE + PF_MAC_SIZE);\n+\n+    \/* node 2-3: data #0, #1 *\/\n+    \/* after mht root *\/\n+    truncate_file(\"trunc_data_0\", 2 * PF_NODE_SIZE);\n+    \/* middle of data #0 *\/\n+    truncate_file(\"trunc_data_1\", 2 * PF_NODE_SIZE + PF_NODE_SIZE \/ 2);\n+    \/* after data #0 *\/\n+    truncate_file(\"trunc_data_2\", 3 * PF_NODE_SIZE);\n+    \/* middle of data #1 *\/\n+    truncate_file(\"trunc_data_3\", 3 * PF_NODE_SIZE + PF_NODE_SIZE \/ 2);\n+\n+    \/* extend *\/\n+    truncate_file(\"extend_0\", g_input_size + 1);\n+    truncate_file(\"extend_1\", g_input_size + PF_NODE_SIZE \/ 2);\n+    truncate_file(\"extend_2\", g_input_size + PF_NODE_SIZE);\n+    truncate_file(\"extend_3\", g_input_size + PF_NODE_SIZE + PF_NODE_SIZE \/ 2);\n+}\n+\n+\/* returns mmap'd output contents *\/\n+static void* create_output(const char* path) {\n+    void* mem = MAP_FAILED;\n+    int fd = open(path, O_RDWR|O_CREAT, 0664);\n+    if (fd < 0)\n+        FATAL(\"Failed to open output file '%s': %s\\n\", path, strerror(errno));\n+\n+    if (ftruncate(fd, g_input_size) < 0)\n+        FATAL(\"Failed to ftruncate output file '%s': %s\\n\", path, strerror(errno));\n+\n+    mem = mmap(NULL, g_input_size, PROT_READ|PROT_WRITE, MAP_SHARED, fd, 0);\n+    if (mem == MAP_FAILED)\n+        FATAL(\"Failed to mmap output file '%s': %s\\n\", path, strerror(errno));\n+\n+    memcpy(mem, g_input_data, g_input_size);\n+\n+    close(fd);\n+    return mem;\n+}\n+\n+static void pf_decrypt(const void* encrypted, size_t size, const pf_key_t* key, const pf_mac_t* mac,\n+                       void* decrypted, const char* msg) {\n+    pf_status_t status = mbedtls_aes_gcm_decrypt(key, &g_empty_iv, NULL, 0,\n+                                                 encrypted, size,\n+                                                 decrypted, mac);\n+    if (PF_FAILURE(status))\n+        FATAL(\"decrypting %s failed\\n\", msg);\n+}\n+\n+static void pf_encrypt(const void* decrypted, size_t size, const pf_key_t* key, pf_mac_t* mac,\n+                       void* encrypted, const char* msg) {\n+    pf_status_t status = mbedtls_aes_gcm_encrypt(key, &g_empty_iv, NULL, 0,\n+                                                 decrypted, size,\n+                                                 encrypted, mac);\n+    if (PF_FAILURE(status))\n+        FATAL(\"encrypting %s failed\\n\", msg);\n+}\n+\n+\/* copy input PF and apply some modifications *\/\n+#define __BREAK_PF(suffix, ...) do { \\\n+    make_output_path(suffix); \\\n+    meta = create_output(g_output_path); \\\n+    out = (uint8_t*)meta; \\\n+    pf_decrypt(&meta->encrypted_part, sizeof(meta->encrypted_part), &g_meta_key, \\\n+               &meta->plain_part.metadata_gmac, meta_dec, \"metadata\"); \\\n+    mht_enc = (mht_node_t*)(out + PF_NODE_SIZE); \\\n+    pf_decrypt(mht_enc, sizeof(*mht_enc), &meta_dec->mht_key, &meta_dec->mht_gmac, mht_dec, \\\n+               \"mht\"); \\\n+    __VA_ARGS__ \\\n+    munmap(meta, g_input_size); \\\n+} while (0)\n+\n+\/* if update is true, also create a file with correct metadata MAC *\/\n+#define BREAK_PF(suffix, update, ...) do { \\\n+    __BREAK_PF(suffix, __VA_ARGS__); \\\n+    if (update) { \\\n+        __BREAK_PF(suffix \"_fixed\", __VA_ARGS__ { \\\n+                       pf_encrypt(meta_dec, sizeof(*meta_dec), &g_meta_key, \\\n+                                  &meta->plain_part.metadata_gmac, meta->encrypted_part, \\\n+                                  \"metadata\"); \\\n+                   } ); \\\n+    } \\\n+} while (0)\n+\n+#define BREAK_MHT(suffix, ...) do { \\\n+    __BREAK_PF(suffix, __VA_ARGS__ { \\\n+                   pf_encrypt(mht_dec, sizeof(*mht_dec), &meta_dec->mht_key, &meta_dec->mht_gmac, \\\n+                              mht_enc, \"mht\"); \\\n+               } ); \\\n+} while (0)\n+\n+#define LAST_BYTE(array) (((uint8_t*)&array)[sizeof(array) - 1])\n+\n+static void tamper_modify(void) {\n+    metadata_node_t* meta = NULL;\n+    uint8_t* out = NULL;\n+    metadata_encrypted_t* meta_dec = malloc(sizeof(*meta_dec));\n+    if (!meta_dec)\n+        FATAL(\"Out of memory\\n\");\n+    mht_node_t* mht_enc = NULL;\n+    mht_node_t* mht_dec = malloc(sizeof(*mht_dec));\n+    if (!mht_dec)\n+        FATAL(\"Out of memory\\n\");\n+\n+    \/* plain part of the metadata isn't covered by the MAC so no point updating it *\/\n+    BREAK_PF(\"meta_plain_id_0\", \/*update=*\/false,\n+             { meta->plain_part.file_id = 0; });\n+    BREAK_PF(\"meta_plain_id_1\", \/*update=*\/false,\n+             { meta->plain_part.file_id = UINT64_MAX; });\n+    BREAK_PF(\"meta_plain_version_0\", \/*update=*\/false,\n+             { meta->plain_part.major_version = 0; });\n+    BREAK_PF(\"meta_plain_version_1\", \/*update=*\/false,\n+             { meta->plain_part.major_version = 0xff; });\n+    BREAK_PF(\"meta_plain_version_2\", \/*update=*\/false,\n+             { meta->plain_part.minor_version = 0xff; });\n+\n+    \/* metadata_key_id is the keying material for encrypted metadata key derivation, so create also\n+     * PFs with updated MACs *\/\n+    BREAK_PF(\"meta_plain_keyid_0\", \/*update=*\/true,\n+             { meta->plain_part.metadata_key_id[0] ^= 1; });\n+    BREAK_PF(\"meta_plain_keyid_1\", \/*update=*\/true,\n+             { LAST_BYTE(meta->plain_part.metadata_key_id) ^= 0xfe; });\n+    BREAK_PF(\"meta_plain_mac_0\", \/*update=*\/true,\n+             { meta->plain_part.metadata_gmac[0] ^= 0xfe; });\n+    BREAK_PF(\"meta_plain_mac_1\", \/*update=*\/true,\n+             { LAST_BYTE(meta->plain_part.metadata_gmac) &= 1; });\n+\n+    BREAK_PF(\"meta_enc_filename_0\", \/*update=*\/true,\n+             { meta_dec->path[0] = 0; });\n+    BREAK_PF(\"meta_enc_filename_1\", \/*update=*\/true,\n+             { meta_dec->path[0] ^= 1; });\n+    BREAK_PF(\"meta_enc_filename_2\", \/*update=*\/true,\n+             { LAST_BYTE(meta_dec->path) ^= 0xfe; });\n+    BREAK_PF(\"meta_enc_size_0\", \/*update=*\/true,\n+             { meta_dec->size = 0; });\n+    BREAK_PF(\"meta_enc_size_1\", \/*update=*\/true,\n+             { meta_dec->size = g_input_size - 1; });\n+    BREAK_PF(\"meta_enc_size_2\", \/*update=*\/true,\n+             { meta_dec->size = g_input_size + 1; });\n+    BREAK_PF(\"meta_enc_size_3\", \/*update=*\/true,\n+             { meta_dec->size = UINT64_MAX; });\n+    BREAK_PF(\"meta_enc_mht_key_0\", \/*update=*\/true,\n+             { meta_dec->mht_key[0] ^= 1; });\n+    BREAK_PF(\"meta_enc_mht_key_1\", \/*update=*\/true,\n+             { LAST_BYTE(meta_dec->mht_key) ^= 0xfe; });\n+    BREAK_PF(\"meta_enc_mht_mac_0\", \/*update=*\/true,\n+             { meta_dec->mht_gmac[0] ^= 1; });\n+    BREAK_PF(\"meta_enc_mht_mac_1\", \/*update=*\/true,\n+             { LAST_BYTE(meta_dec->mht_gmac) ^= 0xfe; });\n+    BREAK_PF(\"meta_enc_data_0\", \/*update=*\/true,\n+             { meta_dec->data[0] ^= 0xfe; });\n+    BREAK_PF(\"meta_enc_data_1\", \/*update=*\/true,\n+             { LAST_BYTE(meta_dec->data) ^= 1; });\n+\n+    \/* padding is ignored *\/\n+    BREAK_PF(\"meta_padding_0\", \/*update=*\/false,\n+             { meta->padding[0] ^= 1; });\n+    BREAK_PF(\"meta_padding_1\", \/*update=*\/false,\n+             { LAST_BYTE(meta->padding) ^= 0xfe; });\n+\n+    BREAK_MHT(\"mht_0\", { mht_dec->data_nodes_crypto[0].key[0] ^= 1; });\n+    BREAK_MHT(\"mht_1\", { mht_dec->data_nodes_crypto[0].gmac[0] ^= 1; });\n+    BREAK_MHT(\"mht_2\", { mht_dec->mht_nodes_crypto[0].key[0] ^= 1; });\n+    BREAK_MHT(\"mht_3\", { mht_dec->mht_nodes_crypto[0].gmac[0] ^= 1; });\n+    BREAK_MHT(\"mht_4\", { mht_dec->data_nodes_crypto[ATTACHED_DATA_NODES_COUNT - 1].key[0] ^= 1; });\n+    BREAK_MHT(\"mht_5\", { mht_dec->data_nodes_crypto[ATTACHED_DATA_NODES_COUNT - 1].gmac[0] ^= 1; });\n+    BREAK_MHT(\"mht_6\", { mht_dec->mht_nodes_crypto[CHILD_MHT_NODES_COUNT - 1].key[0] ^= 1; });\n+    BREAK_MHT(\"mht_7\", { mht_dec->mht_nodes_crypto[CHILD_MHT_NODES_COUNT - 1].gmac[0] ^= 1; });\n+    BREAK_MHT(\"mht_8\", {\n+        gcm_crypto_data_t crypto;\n+        memcpy(&crypto, &mht_dec->data_nodes_crypto[0], sizeof(crypto));\n+        memcpy(&mht_dec->data_nodes_crypto[0], &mht_dec->data_nodes_crypto[1], sizeof(crypto));\n+        memcpy(&mht_dec->data_nodes_crypto[1], &crypto, sizeof(crypto));\n+    });\n+    BREAK_MHT(\"mht_9\", {\n+        gcm_crypto_data_t crypto;\n+        memcpy(&crypto, &mht_dec->mht_nodes_crypto[0], sizeof(crypto));\n+        memcpy(&mht_dec->mht_nodes_crypto[0], &mht_dec->mht_nodes_crypto[1], sizeof(crypto));\n+        memcpy(&mht_dec->mht_nodes_crypto[1], &crypto, sizeof(crypto));\n+    });\n+\n+    \/* data nodes start from node #2 *\/\n+    BREAK_PF(\"data_0\", \/*update=*\/false,\n+             { *(out + 2 * PF_NODE_SIZE) ^= 1; });\n+    BREAK_PF(\"data_1\", \/*update=*\/false,\n+             { *(out + 3 * PF_NODE_SIZE - 1) ^= 1; });\n+    BREAK_PF(\"data_2\", \/*update=*\/false, {\n+        \/* swap data nodes *\/\n+        memcpy(out + 2 * PF_NODE_SIZE, g_input_data + 3 * PF_NODE_SIZE, PF_NODE_SIZE);\n+        memcpy(out + 3 * PF_NODE_SIZE, g_input_data + 2 * PF_NODE_SIZE, PF_NODE_SIZE);\n+    });\n+\n+    free(mht_dec);\n+    free(meta_dec);\n+}\n+\n+int main(int argc, char* argv[]) {\n+    int ret = -1;\n+\n+    int option          = 0;\n+    char* input_path    = NULL;\n+    char* wrap_key_path = NULL;\n+    int input_fd        = -1;\n+\n+    while (true) {\n+        option = getopt_long(argc, argv, \"i:o:w:vh\", g_options, NULL);\n+        if (option == -1)\n+            break;\n+\n+        switch (option) {\n+            case 'i':\n+                input_path = optarg;\n+                break;\n+            case 'o':\n+                g_output_dir = optarg;\n+                break;\n+            case 'w':\n+                wrap_key_path = optarg;\n+                break;\n+            case 'v':\n+                set_verbose(true);\n+                break;\n+            case 'h':\n+                usage();\n+                return 0;\n+            default:\n+                ERROR(\"Unknown option: %c\\n\", option);\n+                usage();\n+        }\n+    }\n+\n+    if (!input_path) {\n+        ERROR(\"Input path not specified\\n\");\n+        usage();\n+        goto out;\n+    }\n+\n+    if (!g_output_dir) {\n+        ERROR(\"Output path not specified\\n\");\n+        usage();\n+        goto out;\n+    }\n+\n+    if (!wrap_key_path) {\n+        ERROR(\"Wrap key path not specified\\n\");\n+        usage();\n+        goto out;\n+    }\n+\n+    input_fd = open(input_path, O_RDONLY);\n+    if (input_fd < 0) {\n+        ERROR(\"Failed to open input file '%s': %s\\n\", input_path, strerror(errno));\n+        goto out;\n+    }\n+\n+    g_input_size = get_file_size(input_fd);\n+    if (g_input_size < 0) {\n+        ERROR(\"Failed to stat input file '%s': %s\\n\", input_path, strerror(errno));\n+        goto out;\n+    }\n+\n+    g_input_data = mmap(NULL, g_input_size, PROT_READ, MAP_PRIVATE, input_fd, 0);\n+    if (g_input_data == MAP_FAILED) {\n+        ERROR(\"Failed to mmap input file '%s': %s\\n\", input_path, strerror(errno));\n+        goto out;\n+    }\n+\n+    load_wrap_key(wrap_key_path, &g_wrap_key);\n+    derive_main_key(&g_wrap_key, &((metadata_plain_t*)g_input_data)->metadata_key_id,\n+                    &g_meta_key);\n+\n+    g_input_name = basename(input_path);\n+    g_output_path_size = strlen(g_input_name) + strlen(g_output_dir) + 256;\n+    g_output_path = malloc(g_output_path_size);\n+    if (!g_output_path) {\n+        ERROR(\"No memory\\n\");\n+        goto out;\n+    }\n+\n+    tamper_truncate();\n+    tamper_modify();\n+    ret = 0;\n+\n+out:\n+    \/* skip cleanup as we are in main() *\/\n+    return ret;\n+}\n","message":"","files":{"\/LibOS\/shim\/test\/fs\/test_pf.py":{"changes":[{"diff":"\n \n     def __corrupt_file(self, input_path, output_path):\n         cmd = [self.PF_TAMPER, '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path]\n-        return self.run_native_binary(cmd)\n+        return self.run_native_binary(cmd, libpath=os.path.join(os.getcwd(), 'lib'))\n \n     # invalid\/corrupted files\n-    @expectedFailureIf(HAS_SGX)\n-    # pylint: disable=fixme\n     def test_500_invalid(self):\n-        # TODO: port these to the new file format\n         invalid_dir = os.path.join(self.TEST_DIR, 'pf_invalid')\n-        # files below should work normally (benign modifications)\n-        should_pass = ['chunk_padding_1_fixed', 'chunk_padding_2_fixed', 'chunk_data_3',\n-                       'chunk_data_3_fixed', 'chunk_data_4', 'chunk_data_4_fixed']\n         if not os.path.exists(invalid_dir):\n             os.mkdir(invalid_dir)\n+\n         # prepare valid encrypted file (largest one for maximum possible corruptions)\n         original_input = self.OUTPUT_FILES[-1]\n         self.__encrypt_file(self.INPUT_FILES[-1], original_input)\n         # generate invalid files based on the above\n         self.__corrupt_file(original_input, invalid_dir)\n+\n         # try to decrypt invalid files\n         for name in os.listdir(invalid_dir):\n             invalid = os.path.join(invalid_dir, name)\n","add":3,"remove":7,"filename":"\/LibOS\/shim\/test\/fs\/test_pf.py","badparts":["        return self.run_native_binary(cmd)","    @expectedFailureIf(HAS_SGX)","        should_pass = ['chunk_padding_1_fixed', 'chunk_padding_2_fixed', 'chunk_data_3',","                       'chunk_data_3_fixed', 'chunk_data_4', 'chunk_data_4_fixed']"],"goodparts":["        return self.run_native_binary(cmd, libpath=os.path.join(os.getcwd(), 'lib'))"]},{"diff":"\n             input_path = os.path.join(invalid_dir, os.path.basename(original_input))\n             # copy the file so it has the original file name (for allowed path check)\n             shutil.copy(invalid, input_path)\n-            should_pass = any(s in name for s in should_pass)\n \n             try:\n                 args = ['decrypt', '-V', '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path]\n                 self.__pf_crypt(args)\n             except subprocess.CalledProcessError as exc:\n-                if should_pass:\n-                    self.assertEqual(exc.returncode, 0)\n-                else:\n-                    self.assertNotEqual(exc.returncode, 0)\n+                # decryption of invalid file must fail with -1 (wrapped to 255)\n+                self.assertEqual(exc.returncode, 255)\n             else:\n-                if not should_pass:\n-                    print('[!] Fail: successfully decrypted file: ' + name)\n-                    self.fail()\n+                print('[!] Fail: successfully decrypted file: ' + name)\n+                self.fail()","add":4,"remove":8,"filename":"\/LibOS\/shim\/test\/fs\/test_pf.py","badparts":["            should_pass = any(s in name for s in should_pass)","                if should_pass:","                    self.assertEqual(exc.returncode, 0)","                else:","                    self.assertNotEqual(exc.returncode, 0)","                if not should_pass:","                    print('[!] Fail: successfully decrypted file: ' + name)","                    self.fail()"],"goodparts":["                self.assertEqual(exc.returncode, 255)","                print('[!] Fail: successfully decrypted file: ' + name)","                self.fail()"]}],"source":"\n import filecmp import os import shutil import subprocess import unittest from test_fs import( TC_00_FileSystem, ) from regression import( HAS_SGX, expectedFailureIf, ) @unittest.skipUnless(HAS_SGX, 'Protected files require SGX support') class TC_50_ProtectedFiles(TC_00_FileSystem): @classmethod def setUpClass(cls): cls.PF_CRYPT='bin\/pf_crypt' cls.PF_TAMPER='bin\/pf_tamper' cls.WRAP_KEY=os.path.join(cls.TEST_DIR, 'wrap-key') cls.CONST_WRAP_KEY=[0xff, 0xee, 0xdd, 0xcc, 0xbb, 0xaa, 0x99, 0x88, 0x77, 0x66, 0x55, 0x44, 0x33, 0x22, 0x11, 0x00] cls.ENCRYPTED_DIR=os.path.join(cls.TEST_DIR, 'pf_input') cls.ENCRYPTED_FILES=[os.path.join(cls.ENCRYPTED_DIR, str(v)) for v in cls.FILE_SIZES] cls.LIB_PATH=os.path.join(os.getcwd(), 'lib') super().setUpClass() if not os.path.exists(cls.ENCRYPTED_DIR): os.mkdir(cls.ENCRYPTED_DIR) cls.OUTPUT_DIR=os.path.join(cls.TEST_DIR, 'pf_output') cls.OUTPUT_FILES=[os.path.join(cls.OUTPUT_DIR, str(x)) for x in cls.FILE_SIZES] cls.__set_default_key(cls) for i in cls.INDEXES: cmd=[cls.PF_CRYPT, 'encrypt', '-w', cls.WRAP_KEY, '-i', cls.INPUT_FILES[i], '-o', cls.ENCRYPTED_FILES[i]] cls.run_native_binary(cmd, libpath=os.path.join(os.getcwd(), 'lib')) def __pf_crypt(self, args): args.insert(0, self.PF_CRYPT) return self.run_native_binary(args, libpath=os.path.join(os.getcwd(), 'lib')) def __set_default_key(self): with open(self.WRAP_KEY, 'wb') as file: file.write(bytes(self.CONST_WRAP_KEY)) def copy_input(self, input_path, output_path): self.__encrypt_file(input_path, output_path) def __encrypt_file(self, input_path, output_path): args=['encrypt', '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path] stdout, stderr=self.__pf_crypt(args) return(stdout, stderr) def __decrypt_file(self, input_path, output_path): args=['decrypt', '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path] stdout, stderr=self.__pf_crypt(args) return(stdout, stderr) def test_000_gen_key(self): key_path=os.path.join(self.TEST_DIR, 'tmpkey') args=['gen-key', '-w', key_path] stdout, _=self.__pf_crypt(args) self.assertIn('Wrap key saved to: ' +key_path, stdout) self.assertEqual(os.path.getsize(key_path), 16) os.remove(key_path) def test_010_encrypt_decrypt(self): for i in self.INDEXES: self.__encrypt_file(self.INPUT_FILES[i], self.OUTPUT_FILES[i]) self.assertFalse(filecmp.cmp(self.INPUT_FILES[i], self.OUTPUT_FILES[i], shallow=False)) dec_path=os.path.join(self.OUTPUT_DIR, os.path.basename(self.OUTPUT_FILES[i]) +'.dec') self.__decrypt_file(self.OUTPUT_FILES[i], dec_path) self.assertTrue(filecmp.cmp(self.INPUT_FILES[i], dec_path, shallow=False)) def test_100_open_close(self): input_path=self.ENCRYPTED_FILES[-1] output_path=os.path.join(self.OUTPUT_DIR, 'test_100') stdout, stderr=self.run_binary(['open_close', 'R', input_path]) self.verify_open_close(stdout, stderr, input_path, 'input') try: stdout, stderr=self.run_binary(['open_close', 'W', output_path]) self.assertIn('ERROR: Failed to open output file', stderr) except subprocess.CalledProcessError as exc: self.assertNotEqual(exc.returncode, 0) self.assertTrue(os.path.isfile(output_path)) else: print('[!] Fail: open_close returned 0') self.fail() def test_101_open_flags(self): file_path=os.path.join(self.OUTPUT_DIR, 'test_101') stdout, stderr=self.run_binary(['open_flags', file_path]) self.verify_open_flags(stdout, stderr) def test_115_seek_tell(self): plaintext_path=self.INPUT_FILES[-1] input_path=self.ENCRYPTED_FILES[-1] output_path_1=os.path.join(self.OUTPUT_DIR, 'test_115a') output_path_2=os.path.join(self.OUTPUT_DIR, 'test_115b') self.copy_input(plaintext_path, output_path_1) self.copy_input(plaintext_path, output_path_2) stdout, stderr=self.run_binary(['seek_tell', input_path, output_path_1, output_path_2]) self.verify_seek_tell(stdout, stderr, input_path, output_path_1, output_path_2, self.FILE_SIZES[-1]) def test_130_file_stat(self): for i in self.INDEXES: input_path=self.ENCRYPTED_FILES[i] output_path=self.OUTPUT_FILES[i] size=str(self.FILE_SIZES[i]) self.copy_input(self.INPUT_FILES[i], output_path) stdout, stderr=self.run_binary(['stat', input_path, output_path]) self.verify_stat(stdout, stderr, input_path, output_path, size) def verify_size(self, file, size): dec_path=os.path.join(self.OUTPUT_DIR, os.path.basename(file) +'.dec') self.__decrypt_file(file, dec_path) self.assertEqual(os.stat(dec_path).st_size, size) @expectedFailureIf(HAS_SGX) def test_140_file_truncate(self): self.fail() def test_150_file_rename(self): path1=os.path.join(self.OUTPUT_DIR, 'test_150a') path2=os.path.join(self.OUTPUT_DIR, 'test_150b') self.copy_input(self.ENCRYPTED_FILES[-1], path1) shutil.copy(path1, path2) args=['decrypt', '-V', '-w', self.WRAP_KEY, '-i', path2, '-o', path1] try: self.__pf_crypt(args) except subprocess.CalledProcessError as exc: self.assertNotEqual(exc.returncode, 0) else: print('[!] Fail: successfully decrypted renamed file: ' +path2) self.fail() def verify_copy_content(self, input_path, output_path): dec_path=os.path.join(self.OUTPUT_DIR, os.path.basename(output_path) +'.dec') self.__decrypt_file(output_path, dec_path) self.assertTrue(filecmp.cmp(input_path, dec_path, shallow=False)) def do_copy_test(self, executable, timeout): stdout, stderr=self.run_binary([executable, self.ENCRYPTED_DIR, self.OUTPUT_DIR], timeout=timeout) self.verify_copy(stdout, stderr, self.ENCRYPTED_DIR, executable) def test_204_copy_dir_mmap_whole(self): self.do_copy_test('copy_mmap_whole', 30) def test_205_copy_dir_mmap_seq(self): self.do_copy_test('copy_mmap_seq', 60) def test_206_copy_dir_mmap_rev(self): self.do_copy_test('copy_mmap_rev', 60) def test_210_copy_dir_mounted(self): executable='copy_whole' stdout, stderr=self.run_binary([executable, '\/mounted\/pf_input', '\/mounted\/pf_output'], timeout=30) self.verify_copy(stdout, stderr, '\/mounted\/pf_input', executable) def __corrupt_file(self, input_path, output_path): cmd=[self.PF_TAMPER, '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path] return self.run_native_binary(cmd) @expectedFailureIf(HAS_SGX) def test_500_invalid(self): invalid_dir=os.path.join(self.TEST_DIR, 'pf_invalid') should_pass=['chunk_padding_1_fixed', 'chunk_padding_2_fixed', 'chunk_data_3', 'chunk_data_3_fixed', 'chunk_data_4', 'chunk_data_4_fixed'] if not os.path.exists(invalid_dir): os.mkdir(invalid_dir) original_input=self.OUTPUT_FILES[-1] self.__encrypt_file(self.INPUT_FILES[-1], original_input) self.__corrupt_file(original_input, invalid_dir) for name in os.listdir(invalid_dir): invalid=os.path.join(invalid_dir, name) output_path=os.path.join(self.OUTPUT_DIR, name) input_path=os.path.join(invalid_dir, os.path.basename(original_input)) shutil.copy(invalid, input_path) should_pass=any(s in name for s in should_pass) try: args=['decrypt', '-V', '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path] self.__pf_crypt(args) except subprocess.CalledProcessError as exc: if should_pass: self.assertEqual(exc.returncode, 0) else: self.assertNotEqual(exc.returncode, 0) else: if not should_pass: print('[!] Fail: successfully decrypted file: ' +name) self.fail() ","sourceWithComments":"#!\/usr\/bin\/env python3\n\nimport filecmp\nimport os\nimport shutil\nimport subprocess\nimport unittest\n\nfrom test_fs import (\n    TC_00_FileSystem,\n)\n\nfrom regression import (\n    HAS_SGX,\n    expectedFailureIf,\n)\n\n@unittest.skipUnless(HAS_SGX, 'Protected files require SGX support')\nclass TC_50_ProtectedFiles(TC_00_FileSystem):\n    @classmethod\n    def setUpClass(cls):\n        cls.PF_CRYPT = 'bin\/pf_crypt'\n        cls.PF_TAMPER = 'bin\/pf_tamper'\n        cls.WRAP_KEY = os.path.join(cls.TEST_DIR, 'wrap-key')\n        # CONST_WRAP_KEY must match the one in manifest\n        cls.CONST_WRAP_KEY = [0xff, 0xee, 0xdd, 0xcc, 0xbb, 0xaa, 0x99, 0x88,\n                              0x77, 0x66, 0x55, 0x44, 0x33, 0x22, 0x11, 0x00]\n        cls.ENCRYPTED_DIR = os.path.join(cls.TEST_DIR, 'pf_input')\n        cls.ENCRYPTED_FILES = [os.path.join(cls.ENCRYPTED_DIR, str(v)) for v in cls.FILE_SIZES]\n        cls.LIB_PATH = os.path.join(os.getcwd(), 'lib')\n\n        super().setUpClass()\n        if not os.path.exists(cls.ENCRYPTED_DIR):\n            os.mkdir(cls.ENCRYPTED_DIR)\n        cls.OUTPUT_DIR = os.path.join(cls.TEST_DIR, 'pf_output')\n        cls.OUTPUT_FILES = [os.path.join(cls.OUTPUT_DIR, str(x)) for x in cls.FILE_SIZES]\n        # create encrypted files\n        cls.__set_default_key(cls)\n        for i in cls.INDEXES:\n            cmd = [cls.PF_CRYPT, 'encrypt', '-w', cls.WRAP_KEY, '-i', cls.INPUT_FILES[i], '-o',\n                   cls.ENCRYPTED_FILES[i]]\n\n            cls.run_native_binary(cmd, libpath=os.path.join(os.getcwd(), 'lib'))\n\n    def __pf_crypt(self, args):\n        args.insert(0, self.PF_CRYPT)\n        return self.run_native_binary(args, libpath=os.path.join(os.getcwd(), 'lib'))\n\n    def __set_default_key(self):\n        with open(self.WRAP_KEY, 'wb') as file:\n            file.write(bytes(self.CONST_WRAP_KEY))\n\n    # overrides TC_00_FileSystem to encrypt the file instead of just copying\n    def copy_input(self, input_path, output_path):\n        self.__encrypt_file(input_path, output_path)\n\n    def __encrypt_file(self, input_path, output_path):\n        args = ['encrypt', '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path]\n        stdout, stderr = self.__pf_crypt(args)\n        return (stdout, stderr)\n\n    def __decrypt_file(self, input_path, output_path):\n        args = ['decrypt', '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path]\n        stdout, stderr = self.__pf_crypt(args)\n        return (stdout, stderr)\n\n    def test_000_gen_key(self):\n        # test random key generation\n        key_path = os.path.join(self.TEST_DIR, 'tmpkey')\n        args = ['gen-key', '-w', key_path]\n        stdout, _ = self.__pf_crypt(args)\n        self.assertIn('Wrap key saved to: ' + key_path, stdout)\n        self.assertEqual(os.path.getsize(key_path), 16)\n        os.remove(key_path)\n\n    def test_010_encrypt_decrypt(self):\n        for i in self.INDEXES:\n            self.__encrypt_file(self.INPUT_FILES[i], self.OUTPUT_FILES[i])\n            self.assertFalse(filecmp.cmp(self.INPUT_FILES[i], self.OUTPUT_FILES[i], shallow=False))\n            dec_path = os.path.join(self.OUTPUT_DIR,\n                                    os.path.basename(self.OUTPUT_FILES[i]) + '.dec')\n            self.__decrypt_file(self.OUTPUT_FILES[i], dec_path)\n            self.assertTrue(filecmp.cmp(self.INPUT_FILES[i], dec_path, shallow=False))\n\n    # overrides TC_00_FileSystem to change input dir (from plaintext to encrypted)\n    def test_100_open_close(self):\n        # the test binary expects a path to read-only (existing) file or a path to file that\n        # will get created\n        input_path = self.ENCRYPTED_FILES[-1] # existing file\n        output_path = os.path.join(self.OUTPUT_DIR, 'test_100') # new file\n        stdout, stderr = self.run_binary(['open_close', 'R', input_path])\n        self.verify_open_close(stdout, stderr, input_path, 'input')\n        # the following test tries to open multiple handles to a single writable PF, should fail\n        try:\n            stdout, stderr = self.run_binary(['open_close', 'W', output_path])\n            self.assertIn('ERROR: Failed to open output file', stderr)\n        except subprocess.CalledProcessError as exc:\n            self.assertNotEqual(exc.returncode, 0)\n            self.assertTrue(os.path.isfile(output_path))\n        else:\n            print('[!] Fail: open_close returned 0')\n            self.fail()\n\n    # overrides TC_00_FileSystem to change input dir (from plaintext to encrypted)\n    def test_101_open_flags(self):\n        # the test binary expects a path to file that will get created\n        file_path = os.path.join(self.OUTPUT_DIR, 'test_101') # new file\n        stdout, stderr = self.run_binary(['open_flags', file_path])\n        self.verify_open_flags(stdout, stderr)\n\n    # overrides TC_00_FileSystem to change input dir (from plaintext to encrypted)\n    def test_115_seek_tell(self):\n        # the test binary expects a path to read-only (existing) file and two paths to files that\n        # will get created\n        plaintext_path = self.INPUT_FILES[-1]\n        input_path = self.ENCRYPTED_FILES[-1] # existing file\n        output_path_1 = os.path.join(self.OUTPUT_DIR, 'test_115a') # writable files\n        output_path_2 = os.path.join(self.OUTPUT_DIR, 'test_115b')\n        self.copy_input(plaintext_path, output_path_1) # encrypt\n        self.copy_input(plaintext_path, output_path_2)\n        stdout, stderr = self.run_binary(['seek_tell', input_path, output_path_1, output_path_2])\n        self.verify_seek_tell(stdout, stderr, input_path, output_path_1, output_path_2,\n                              self.FILE_SIZES[-1])\n\n    # overrides TC_00_FileSystem to change input dir (from plaintext to encrypted)\n    def test_130_file_stat(self):\n        # the test binary expects a path to read-only (existing) file and a path to file that\n        # will get created\n        for i in self.INDEXES:\n            input_path = self.ENCRYPTED_FILES[i]\n            output_path = self.OUTPUT_FILES[i]\n            size = str(self.FILE_SIZES[i])\n            self.copy_input(self.INPUT_FILES[i], output_path)\n            stdout, stderr = self.run_binary(['stat', input_path, output_path])\n            self.verify_stat(stdout, stderr, input_path, output_path, size)\n\n    # overrides TC_00_FileSystem to decrypt output\n    def verify_size(self, file, size):\n        dec_path = os.path.join(self.OUTPUT_DIR, os.path.basename(file) + '.dec')\n        self.__decrypt_file(file, dec_path)\n        self.assertEqual(os.stat(dec_path).st_size, size)\n\n    @expectedFailureIf(HAS_SGX)\n    # pylint: disable=fixme\n    def test_140_file_truncate(self):\n        self.fail() # TODO: port these to the new file format\n\n    def test_150_file_rename(self):\n        path1 = os.path.join(self.OUTPUT_DIR, 'test_150a')\n        path2 = os.path.join(self.OUTPUT_DIR, 'test_150b')\n        self.copy_input(self.ENCRYPTED_FILES[-1], path1)\n        shutil.copy(path1, path2)\n        # accessing renamed file should fail\n        args = ['decrypt', '-V', '-w', self.WRAP_KEY, '-i', path2, '-o', path1]\n        try:\n            self.__pf_crypt(args)\n        except subprocess.CalledProcessError as exc:\n            self.assertNotEqual(exc.returncode, 0)\n        else:\n            print('[!] Fail: successfully decrypted renamed file: ' + path2)\n            self.fail()\n\n    # overrides TC_00_FileSystem to decrypt output\n    def verify_copy_content(self, input_path, output_path):\n        dec_path = os.path.join(self.OUTPUT_DIR, os.path.basename(output_path) + '.dec')\n        self.__decrypt_file(output_path, dec_path)\n        self.assertTrue(filecmp.cmp(input_path, dec_path, shallow=False))\n\n    # overrides TC_00_FileSystem to change input dir (from plaintext to encrypted)\n    def do_copy_test(self, executable, timeout):\n        stdout, stderr = self.run_binary([executable, self.ENCRYPTED_DIR, self.OUTPUT_DIR],\n                                         timeout=timeout)\n        self.verify_copy(stdout, stderr, self.ENCRYPTED_DIR, executable)\n\n    # overrides TC_00_FileSystem to not skip this on SGX\n    def test_204_copy_dir_mmap_whole(self):\n        self.do_copy_test('copy_mmap_whole', 30)\n\n    # overrides TC_00_FileSystem to not skip this on SGX\n    def test_205_copy_dir_mmap_seq(self):\n        self.do_copy_test('copy_mmap_seq', 60)\n\n    # overrides TC_00_FileSystem to not skip this on SGX\n    def test_206_copy_dir_mmap_rev(self):\n        self.do_copy_test('copy_mmap_rev', 60)\n\n    # overrides TC_00_FileSystem to change dirs (from plaintext to encrypted)\n    def test_210_copy_dir_mounted(self):\n        executable = 'copy_whole'\n        stdout, stderr = self.run_binary([executable, '\/mounted\/pf_input', '\/mounted\/pf_output'],\n                                         timeout=30)\n        self.verify_copy(stdout, stderr, '\/mounted\/pf_input', executable)\n\n    def __corrupt_file(self, input_path, output_path):\n        cmd = [self.PF_TAMPER, '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path]\n        return self.run_native_binary(cmd)\n\n    # invalid\/corrupted files\n    @expectedFailureIf(HAS_SGX)\n    # pylint: disable=fixme\n    def test_500_invalid(self):\n        # TODO: port these to the new file format\n        invalid_dir = os.path.join(self.TEST_DIR, 'pf_invalid')\n        # files below should work normally (benign modifications)\n        should_pass = ['chunk_padding_1_fixed', 'chunk_padding_2_fixed', 'chunk_data_3',\n                       'chunk_data_3_fixed', 'chunk_data_4', 'chunk_data_4_fixed']\n        if not os.path.exists(invalid_dir):\n            os.mkdir(invalid_dir)\n        # prepare valid encrypted file (largest one for maximum possible corruptions)\n        original_input = self.OUTPUT_FILES[-1]\n        self.__encrypt_file(self.INPUT_FILES[-1], original_input)\n        # generate invalid files based on the above\n        self.__corrupt_file(original_input, invalid_dir)\n        # try to decrypt invalid files\n        for name in os.listdir(invalid_dir):\n            invalid = os.path.join(invalid_dir, name)\n            output_path = os.path.join(self.OUTPUT_DIR, name)\n            input_path = os.path.join(invalid_dir, os.path.basename(original_input))\n            # copy the file so it has the original file name (for allowed path check)\n            shutil.copy(invalid, input_path)\n            should_pass = any(s in name for s in should_pass)\n\n            try:\n                args = ['decrypt', '-V', '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path]\n                self.__pf_crypt(args)\n            except subprocess.CalledProcessError as exc:\n                if should_pass:\n                    self.assertEqual(exc.returncode, 0)\n                else:\n                    self.assertNotEqual(exc.returncode, 0)\n            else:\n                if not should_pass:\n                    print('[!] Fail: successfully decrypted file: ' + name)\n                    self.fail()\n"}},"msg":"[Pal\/Linux-SGX] Test malicious modifications to protected files\n\nThis commit adds a new PF utility `pf_tamper` that tampers with\nvalid protected files and uses this utility to test that the PF\nlogic in Linux-SGX detects such malicious modifications.\n\nThis commit also moves out the PF-format macros and structs from\n`protected_files_internal.h` to `protected_files_format.h` for\nbetter readability.\n\nCo-authored-by: Dmitrii Kuvaiskii <dmitrii.kuvaiskii@intel.com>"}},"https:\/\/github.com\/stefanberger\/gramine4ppc64":{"567790aec46e0714171121318783b8ac8c56adc3":{"url":"https:\/\/api.github.com\/repos\/stefanberger\/gramine4ppc64\/commits\/567790aec46e0714171121318783b8ac8c56adc3","html_url":"https:\/\/github.com\/stefanberger\/gramine4ppc64\/commit\/567790aec46e0714171121318783b8ac8c56adc3","sha":"567790aec46e0714171121318783b8ac8c56adc3","keyword":"tampering malicious","diff":"diff --git a\/LibOS\/shim\/test\/fs\/test_pf.py b\/LibOS\/shim\/test\/fs\/test_pf.py\nindex e430e02af..7b1621d7a 100644\n--- a\/LibOS\/shim\/test\/fs\/test_pf.py\n+++ b\/LibOS\/shim\/test\/fs\/test_pf.py\n@@ -193,24 +193,20 @@ def test_210_copy_dir_mounted(self):\n \n     def __corrupt_file(self, input_path, output_path):\n         cmd = [self.PF_TAMPER, '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path]\n-        return self.run_native_binary(cmd)\n+        return self.run_native_binary(cmd, libpath=os.path.join(os.getcwd(), 'lib'))\n \n     # invalid\/corrupted files\n-    @expectedFailureIf(HAS_SGX)\n-    # pylint: disable=fixme\n     def test_500_invalid(self):\n-        # TODO: port these to the new file format\n         invalid_dir = os.path.join(self.TEST_DIR, 'pf_invalid')\n-        # files below should work normally (benign modifications)\n-        should_pass = ['chunk_padding_1_fixed', 'chunk_padding_2_fixed', 'chunk_data_3',\n-                       'chunk_data_3_fixed', 'chunk_data_4', 'chunk_data_4_fixed']\n         if not os.path.exists(invalid_dir):\n             os.mkdir(invalid_dir)\n+\n         # prepare valid encrypted file (largest one for maximum possible corruptions)\n         original_input = self.OUTPUT_FILES[-1]\n         self.__encrypt_file(self.INPUT_FILES[-1], original_input)\n         # generate invalid files based on the above\n         self.__corrupt_file(original_input, invalid_dir)\n+\n         # try to decrypt invalid files\n         for name in os.listdir(invalid_dir):\n             invalid = os.path.join(invalid_dir, name)\n@@ -218,17 +214,13 @@ def test_500_invalid(self):\n             input_path = os.path.join(invalid_dir, os.path.basename(original_input))\n             # copy the file so it has the original file name (for allowed path check)\n             shutil.copy(invalid, input_path)\n-            should_pass = any(s in name for s in should_pass)\n \n             try:\n                 args = ['decrypt', '-V', '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path]\n                 self.__pf_crypt(args)\n             except subprocess.CalledProcessError as exc:\n-                if should_pass:\n-                    self.assertEqual(exc.returncode, 0)\n-                else:\n-                    self.assertNotEqual(exc.returncode, 0)\n+                # decryption of invalid file must fail with -1 (wrapped to 255)\n+                self.assertEqual(exc.returncode, 255)\n             else:\n-                if not should_pass:\n-                    print('[!] Fail: successfully decrypted file: ' + name)\n-                    self.fail()\n+                print('[!] Fail: successfully decrypted file: ' + name)\n+                self.fail()\ndiff --git a\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files.c b\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files.c\nindex 4f0c59ba2..d020a1e24 100644\n--- a\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files.c\n+++ b\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files.c\n@@ -5,6 +5,8 @@\n  *\/\n \n #include \"api.h\"\n+#include \"protected_files.h\"\n+#include \"protected_files_format.h\"\n #include \"protected_files_internal.h\"\n \n #ifndef IN_PAL\n@@ -65,20 +67,6 @@ static pf_random_f          g_cb_random          = NULL;\n static pf_iv_t g_empty_iv = {0};\n static bool g_initialized = false;\n \n-#define METADATA_KEY_NAME \"SGX-PROTECTED-FS-METADATA-KEY\"\n-#define MAX_LABEL_SIZE    64\n-\n-static_assert(sizeof(METADATA_KEY_NAME) <= MAX_LABEL_SIZE, \"label too long\");\n-\n-#pragma pack(push, 1)\n-typedef struct {\n-    uint32_t index;\n-    char label[MAX_LABEL_SIZE]; \/\/ must be NULL terminated\n-    pf_keyid_t nonce;\n-    uint32_t output_len; \/\/ in bits\n-} kdf_input_t;\n-#pragma pack(pop)\n-\n \/\/ The key derivation function follow recommendations from NIST Special Publication 800-108:\n \/\/ Recommendation for Key Derivation Using Pseudorandom Functions\n \/\/ https:\/\/nvlpubs.nist.gov\/nistpubs\/Legacy\/SP\/nistspecialpublication800-108.pdf\ndiff --git a\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files_format.h b\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files_format.h\nnew file mode 100644\nindex 000000000..2394e53ef\n--- \/dev\/null\n+++ b\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files_format.h\n@@ -0,0 +1,139 @@\n+\/* SPDX-License-Identifier: LGPL-3.0-or-later *\/\n+\/* Copyright (C) 2019-2020 Invisible Things Lab\n+ *                         Rafal Wojdyla <omeg@invisiblethingslab.com>\n+ * Copyright (C) 2011-2020 Intel Corporation\n+ *\/\n+\n+#ifndef PROTECTED_FILES_FORMAT_H_\n+#define PROTECTED_FILES_FORMAT_H_\n+\n+#include <limits.h>\n+\n+#include \"assert.h\"\n+#include \"list.h\"\n+#include \"protected_files.h\"\n+\n+#define PF_FILE_ID       0x46505f4850415247 \/* GRAPH_PF *\/\n+#define PF_MAJOR_VERSION 0x01\n+#define PF_MINOR_VERSION 0x00\n+\n+#define METADATA_KEY_NAME \"SGX-PROTECTED-FS-METADATA-KEY\"\n+#define MAX_LABEL_SIZE    64\n+\n+static_assert(sizeof(METADATA_KEY_NAME) <= MAX_LABEL_SIZE, \"label too long\");\n+\n+#pragma pack(push, 1)\n+\n+typedef struct _metadata_plain {\n+    uint64_t   file_id;\n+    uint8_t    major_version;\n+    uint8_t    minor_version;\n+    pf_keyid_t metadata_key_id;\n+    pf_mac_t   metadata_gmac; \/* GCM mac *\/\n+} metadata_plain_t;\n+\n+#define PATH_MAX_SIZE (260 + 512)\n+\n+\/\/ these are all defined as relative to node size, so we can decrease node size in tests\n+\/\/ and have deeper tree\n+#define MD_USER_DATA_SIZE (PF_NODE_SIZE * 3 \/ 4) \/\/ 3072\n+static_assert(MD_USER_DATA_SIZE == 3072, \"bad struct size\");\n+\n+typedef struct _metadata_encrypted {\n+    char     path[PATH_MAX_SIZE];\n+    uint64_t size;\n+    pf_key_t mht_key;\n+    pf_mac_t mht_gmac;\n+    uint8_t  data[MD_USER_DATA_SIZE];\n+} metadata_encrypted_t;\n+\n+typedef uint8_t metadata_encrypted_blob_t[sizeof(metadata_encrypted_t)];\n+\n+#define METADATA_NODE_SIZE PF_NODE_SIZE\n+\n+typedef uint8_t metadata_padding_t[METADATA_NODE_SIZE -\n+                                   (sizeof(metadata_plain_t) + sizeof(metadata_encrypted_blob_t))];\n+\n+typedef struct _metadata_node {\n+    metadata_plain_t          plain_part;\n+    metadata_encrypted_blob_t encrypted_part;\n+    metadata_padding_t        padding;\n+} metadata_node_t;\n+\n+static_assert(sizeof(metadata_node_t) == PF_NODE_SIZE, \"sizeof(metadata_node_t)\");\n+\n+typedef struct _data_node_crypto {\n+    pf_key_t key;\n+    pf_mac_t gmac;\n+} gcm_crypto_data_t;\n+\n+\/\/ for PF_NODE_SIZE == 4096, we have 96 attached data nodes and 32 mht child nodes\n+\/\/ for PF_NODE_SIZE == 2048, we have 48 attached data nodes and 16 mht child nodes\n+\/\/ for PF_NODE_SIZE == 1024, we have 24 attached data nodes and 8 mht child nodes\n+\/\/ 3\/4 of the node size is dedicated to data nodes\n+#define ATTACHED_DATA_NODES_COUNT ((PF_NODE_SIZE \/ sizeof(gcm_crypto_data_t)) * 3 \/ 4)\n+static_assert(ATTACHED_DATA_NODES_COUNT == 96, \"ATTACHED_DATA_NODES_COUNT\");\n+\/\/ 1\/4 of the node size is dedicated to child mht nodes\n+#define CHILD_MHT_NODES_COUNT ((PF_NODE_SIZE \/ sizeof(gcm_crypto_data_t)) * 1 \/ 4)\n+static_assert(CHILD_MHT_NODES_COUNT == 32, \"CHILD_MHT_NODES_COUNT\");\n+\n+typedef struct _mht_node {\n+    gcm_crypto_data_t data_nodes_crypto[ATTACHED_DATA_NODES_COUNT];\n+    gcm_crypto_data_t mht_nodes_crypto[CHILD_MHT_NODES_COUNT];\n+} mht_node_t;\n+\n+static_assert(sizeof(mht_node_t) == PF_NODE_SIZE, \"sizeof(mht_node_t)\");\n+\n+typedef struct _data_node {\n+    uint8_t data[PF_NODE_SIZE];\n+} data_node_t;\n+\n+static_assert(sizeof(data_node_t) == PF_NODE_SIZE, \"sizeof(data_node_t)\");\n+\n+typedef struct _encrypted_node {\n+    uint8_t cipher[PF_NODE_SIZE];\n+} encrypted_node_t;\n+\n+static_assert(sizeof(encrypted_node_t) == PF_NODE_SIZE, \"sizeof(encrypted_node_t)\");\n+\n+#define MAX_PAGES_IN_CACHE 48\n+\n+typedef enum {\n+    FILE_MHT_NODE_TYPE  = 1,\n+    FILE_DATA_NODE_TYPE = 2,\n+} mht_node_type_e;\n+\n+\/\/ make sure these are the same size\n+static_assert(sizeof(mht_node_t) == sizeof(data_node_t),\n+              \"sizeof(mht_node_t) == sizeof(data_node_t)\");\n+\n+DEFINE_LIST(_file_node);\n+typedef struct _file_node {\n+    LIST_TYPE(_file_node) list;\n+    uint8_t type;\n+    uint64_t node_number;\n+    struct _file_node* parent;\n+    bool need_writing;\n+    bool new_node;\n+    struct {\n+        uint64_t physical_node_number;\n+        encrypted_node_t encrypted; \/\/ the actual data from the disk\n+    };\n+    union { \/\/ decrypted data\n+        mht_node_t mht;\n+        data_node_t data;\n+    } decrypted;\n+} file_node_t;\n+DEFINE_LISTP(_file_node);\n+\n+typedef struct {\n+    uint32_t index;\n+    char label[MAX_LABEL_SIZE]; \/\/ must be NULL terminated\n+    pf_keyid_t nonce;\n+    uint32_t output_len; \/\/ in bits\n+} kdf_input_t;\n+\n+#pragma pack(pop)\n+\n+#endif \/* PROTECTED_FILES_FORMAT_H_ *\/\n+\ndiff --git a\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files_internal.h b\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files_internal.h\nindex 4a31261d5..c06d1ecf6 100644\n--- a\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files_internal.h\n+++ b\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files_internal.h\n@@ -13,116 +13,7 @@\n #include \"list.h\"\n #include \"lru_cache.h\"\n #include \"protected_files.h\"\n-\n-#define PF_FILE_ID       0x46505f4850415247 \/* GRAPH_PF *\/\n-#define PF_MAJOR_VERSION 0x01\n-#define PF_MINOR_VERSION 0x00\n-\n-#pragma pack(push, 1)\n-\n-typedef struct _metadata_plain {\n-    uint64_t   file_id;\n-    uint8_t    major_version;\n-    uint8_t    minor_version;\n-    pf_keyid_t metadata_key_id;\n-    pf_mac_t   metadata_gmac; \/* GCM mac *\/\n-} metadata_plain_t;\n-\n-#define PATH_MAX_SIZE (260 + 512)\n-\n-\/\/ these are all defined as relative to node size, so we can decrease node size in tests\n-\/\/ and have deeper tree\n-#define MD_USER_DATA_SIZE (PF_NODE_SIZE * 3 \/ 4) \/\/ 3072\n-static_assert(MD_USER_DATA_SIZE == 3072, \"bad struct size\");\n-\n-typedef struct _metadata_encrypted {\n-    char     path[PATH_MAX_SIZE];\n-    uint64_t size;\n-    pf_key_t mht_key;\n-    pf_mac_t mht_gmac;\n-    uint8_t  data[MD_USER_DATA_SIZE];\n-} metadata_encrypted_t;\n-\n-typedef uint8_t metadata_encrypted_blob_t[sizeof(metadata_encrypted_t)];\n-\n-#define METADATA_NODE_SIZE PF_NODE_SIZE\n-\n-typedef uint8_t metadata_padding_t[METADATA_NODE_SIZE -\n-                                   (sizeof(metadata_plain_t) + sizeof(metadata_encrypted_blob_t))];\n-\n-typedef struct _metadata_node {\n-    metadata_plain_t          plain_part;\n-    metadata_encrypted_blob_t encrypted_part;\n-    metadata_padding_t        padding;\n-} metadata_node_t;\n-\n-static_assert(sizeof(metadata_node_t) == PF_NODE_SIZE, \"sizeof(metadata_node_t)\");\n-\n-typedef struct _data_node_crypto {\n-    pf_key_t key;\n-    pf_mac_t gmac;\n-} gcm_crypto_data_t;\n-\n-\/\/ for PF_NODE_SIZE == 4096, we have 96 attached data nodes and 32 mht child nodes\n-\/\/ for PF_NODE_SIZE == 2048, we have 48 attached data nodes and 16 mht child nodes\n-\/\/ for PF_NODE_SIZE == 1024, we have 24 attached data nodes and 8 mht child nodes\n-\/\/ 3\/4 of the node size is dedicated to data nodes\n-#define ATTACHED_DATA_NODES_COUNT ((PF_NODE_SIZE \/ sizeof(gcm_crypto_data_t)) * 3 \/ 4)\n-static_assert(ATTACHED_DATA_NODES_COUNT == 96, \"ATTACHED_DATA_NODES_COUNT\");\n-\/\/ 1\/4 of the node size is dedicated to child mht nodes\n-#define CHILD_MHT_NODES_COUNT ((PF_NODE_SIZE \/ sizeof(gcm_crypto_data_t)) * 1 \/ 4)\n-static_assert(CHILD_MHT_NODES_COUNT == 32, \"CHILD_MHT_NODES_COUNT\");\n-\n-typedef struct _mht_node {\n-    gcm_crypto_data_t data_nodes_crypto[ATTACHED_DATA_NODES_COUNT];\n-    gcm_crypto_data_t mht_nodes_crypto[CHILD_MHT_NODES_COUNT];\n-} mht_node_t;\n-\n-static_assert(sizeof(mht_node_t) == PF_NODE_SIZE, \"sizeof(mht_node_t)\");\n-\n-typedef struct _data_node {\n-    uint8_t data[PF_NODE_SIZE];\n-} data_node_t;\n-\n-static_assert(sizeof(data_node_t) == PF_NODE_SIZE, \"sizeof(data_node_t)\");\n-\n-typedef struct _encrypted_node {\n-    uint8_t cipher[PF_NODE_SIZE];\n-} encrypted_node_t;\n-\n-static_assert(sizeof(encrypted_node_t) == PF_NODE_SIZE, \"sizeof(encrypted_node_t)\");\n-\n-#define MAX_PAGES_IN_CACHE 48\n-\n-typedef enum {\n-    FILE_MHT_NODE_TYPE  = 1,\n-    FILE_DATA_NODE_TYPE = 2,\n-} mht_node_type_e;\n-\n-\/\/ make sure these are the same size\n-static_assert(sizeof(mht_node_t) == sizeof(data_node_t),\n-              \"sizeof(mht_node_t) == sizeof(data_node_t)\");\n-\n-DEFINE_LIST(_file_node);\n-typedef struct _file_node {\n-    LIST_TYPE(_file_node) list;\n-    uint8_t type;\n-    uint64_t node_number;\n-    struct _file_node* parent;\n-    bool need_writing;\n-    bool new_node;\n-    struct {\n-        uint64_t physical_node_number;\n-        encrypted_node_t encrypted; \/\/ the actual data from the disk\n-    };\n-    union { \/\/ decrypted data\n-        mht_node_t mht;\n-        data_node_t data;\n-    } decrypted;\n-} file_node_t;\n-DEFINE_LISTP(_file_node);\n-\n-#pragma pack(pop)\n+#include \"protected_files_format.h\"\n \n struct pf_context {\n     metadata_node_t file_metadata; \/\/ actual data from disk's meta data node\ndiff --git a\/Pal\/src\/host\/Linux-SGX\/tools\/Makefile b\/Pal\/src\/host\/Linux-SGX\/tools\/Makefile\nindex a5903ad97..87bc78124 100644\n--- a\/Pal\/src\/host\/Linux-SGX\/tools\/Makefile\n+++ b\/Pal\/src\/host\/Linux-SGX\/tools\/Makefile\n@@ -12,3 +12,4 @@ $(targets):\n \t$(MAKE) -C verify-ias-report $@\n \t$(MAKE) -C ra-tls $@\n \t$(MAKE) -C pf_crypt $@\n+\t$(MAKE) -C pf_tamper $@\ndiff --git a\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/.gitignore b\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/.gitignore\nnew file mode 100644\nindex 000000000..a52536d82\n--- \/dev\/null\n+++ b\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/.gitignore\n@@ -0,0 +1 @@\n+\/pf_tamper\ndiff --git a\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/Makefile b\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/Makefile\nnew file mode 100644\nindex 000000000..94c826e4b\n--- \/dev\/null\n+++ b\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/Makefile\n@@ -0,0 +1,31 @@\n+include ..\/..\/..\/..\/..\/..\/Scripts\/Makefile.configs\n+include ..\/..\/..\/..\/..\/..\/Scripts\/Makefile.rules\n+\n+CFLAGS += -I..\/.. \\\n+          -I..\/common \\\n+          -I..\/..\/protected-files \\\n+          -I..\/..\/..\/..\/..\/include\/lib \\\n+          -D_GNU_SOURCE\n+\n+LDLIBS += -L..\/common \\\n+          -L..\/..\/..\/..\/..\/lib\/crypto\/mbedtls\/install\/lib \\\n+          -lsgx_util -lmbedcrypto\n+\n+PREFIX ?= \/usr\/local\n+\n+pf_tamper: pf_tamper.o\n+\t$(call cmd,csingle)\n+\n+.PHONY: all\n+all: pf_tamper\n+\n+.PHONY: install\n+install:\n+\tinstall -D pf_tamper -t ${PREFIX}\/bin\n+\n+.PHONY: clean\n+clean:\n+\t$(RM) *.o pf_tamper\n+\n+.PHONY: distclean\n+distclean: clean\ndiff --git a\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/pf_tamper.c b\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/pf_tamper.c\nnew file mode 100644\nindex 000000000..4d39a0d58\n--- \/dev\/null\n+++ b\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/pf_tamper.c\n@@ -0,0 +1,476 @@\n+\/* SPDX-License-Identifier: LGPL-3.0-or-later *\/\n+\/* Copyright (C) 2019-2020 Invisible Things Lab\n+ *                         Rafal Wojdyla <omeg@invisiblethingslab.com>\n+ *\/\n+\n+#include <fcntl.h>\n+#include <getopt.h>\n+#include <stdlib.h>\n+#include <sys\/mman.h>\n+#include <unistd.h>\n+\n+#include \"pf_util.h\"\n+#include \"protected_files.h\"\n+#include \"protected_files_format.h\"\n+#include \"util.h\"\n+\n+\/* Tamper with a PF in various ways for testing purposes. The PF is assumed to be valid and have at\n+ * least enough data to contain two MHT nodes. *\/\n+\n+\/* Command line options *\/\n+struct option g_options[] = {\n+    { \"input\", required_argument, 0, 'i' },\n+    { \"output\", required_argument, 0, 'o' },\n+    { \"wrap-key\", required_argument, 0, 'w' },\n+    { \"verbose\", no_argument, 0, 'v' },\n+    { \"help\", no_argument, 0, 'h' },\n+    { 0, 0, 0, 0 }\n+};\n+\n+static void usage(void) {\n+    INFO(\"\\nUsage: pf_tamper [options]\\n\");\n+    INFO(\"\\nAvailable options:\\n\");\n+    INFO(\"  --help, -h           Display this help\\n\");\n+    INFO(\"  --verbose, -v        Enable verbose output\\n\");\n+    INFO(\"  --wrap-key, -w PATH  Path to wrap key file\\n\");\n+    INFO(\"  --input, -i PATH     Source file to be tampered with (must be a valid PF)\\n\");\n+    INFO(\"  --output, -o PATH    Directory where modified files will be written to\\n\");\n+}\n+\n+#define FATAL(fmt, ...) do { \\\n+    ERROR(fmt, ##__VA_ARGS__); \\\n+    exit(-1); \\\n+} while (0)\n+\n+ssize_t g_input_size = 0;\n+char* g_input_name = NULL;\n+void* g_input_data = MAP_FAILED;\n+char* g_output_dir = NULL;\n+char* g_output_path = NULL;\n+size_t g_output_path_size = 0;\n+pf_key_t g_wrap_key;\n+pf_key_t g_meta_key;\n+\n+static pf_iv_t g_empty_iv = {0};\n+\n+static void derive_main_key(const pf_key_t* kdk, const pf_keyid_t* key_id, pf_key_t* out_key) {\n+    kdf_input_t buf = {0};\n+    pf_status_t status;\n+\n+    buf.index = 1;\n+    strncpy(buf.label, METADATA_KEY_NAME, MAX_LABEL_SIZE);\n+    memcpy(&buf.nonce, key_id, sizeof(buf.nonce));\n+    buf.output_len = 0x80;\n+\n+    status = mbedtls_aes_gcm_encrypt(kdk, &g_empty_iv, &buf, sizeof(buf), NULL, 0, NULL, out_key);\n+    if (PF_FAILURE(status))\n+        FATAL(\"key derivation failed\\n\");\n+}\n+\n+static void make_output_path(const char* suffix) {\n+    snprintf(g_output_path, g_output_path_size, \"%s\/%s.%s\", g_output_dir, g_input_name, suffix);\n+    INFO(\"[*] %s\\n\", g_output_path);\n+}\n+\n+\/* PF layout (node size is PF_NODE_SIZE):\n+ * - Node 0: metadata (metadata_node_t)\n+ *   - metadata_plain_t\n+ *   - metadata_encrypted_t (may include MD_USER_DATA_SIZE bytes of data)\n+ *   - metadata_padding_t\n+ * - Node 1: MHT (mht_node_t)\n+ * - Node 2-97: data (ATTACHED_DATA_NODES_COUNT == 96)\n+ * - Node 98: MHT\n+ * - Node 99-195: data\n+ * - ...\n+ *\/\n+static void truncate_file(const char* suffix, size_t output_size) {\n+    int ret;\n+\n+    make_output_path(suffix);\n+\n+    if (output_size < g_input_size) {\n+        ret = write_file(g_output_path, output_size, g_input_data);\n+    } else {\n+        ret = write_file(g_output_path, g_input_size, g_input_data);\n+        if (ret < 0)\n+            goto out;\n+        ret = truncate(g_output_path, output_size);\n+    }\n+out:\n+    if (ret < 0)\n+        FATAL(\"truncate_file failed: %d\\n\", ret);\n+}\n+\n+#define FIELD_SIZEOF(t, f) (sizeof(((t*)0)->f))\n+#define FIELD_TRUNCATED(t, f) (offsetof(t, f) + (FIELD_SIZEOF(t, f) \/ 2))\n+#define DATA_CRYPTO_SIZE (FIELD_SIZEOF(mht_node_t, data_nodes_crypto))\n+\n+static void tamper_truncate(void) {\n+    size_t mdps = sizeof(metadata_plain_t);\n+    DBG(\"size(metadata_plain_t)             = 0x%04lx\\n\", sizeof(metadata_plain_t));\n+    DBG(\"metadata_plain_t.file_id           : 0x%04lx (0x%04lx)\\n\",\n+        offsetof(metadata_plain_t, file_id), FIELD_SIZEOF(metadata_plain_t, file_id));\n+    DBG(\"metadata_plain_t.major_version     : 0x%04lx (0x%04lx)\\n\",\n+        offsetof(metadata_plain_t, major_version), FIELD_SIZEOF(metadata_plain_t, major_version));\n+    DBG(\"metadata_plain_t.minor_version     : 0x%04lx (0x%04lx)\\n\",\n+        offsetof(metadata_plain_t, minor_version), FIELD_SIZEOF(metadata_plain_t, minor_version));\n+    DBG(\"metadata_plain_t.metadata_key_id   : 0x%04lx (0x%04lx)\\n\",\n+        offsetof(metadata_plain_t, metadata_key_id),\n+        FIELD_SIZEOF(metadata_plain_t, metadata_key_id));\n+    DBG(\"metadata_plain_t.metadata_gmac     : 0x%04lx (0x%04lx)\\n\",\n+        offsetof(metadata_plain_t, metadata_gmac),\n+        FIELD_SIZEOF(metadata_plain_t, metadata_gmac));\n+\n+    DBG(\"size(metadata_encrypted_t)         = 0x%04lx\\n\", sizeof(metadata_encrypted_t));\n+    DBG(\"metadata_encrypted_t.path          : 0x%04lx (0x%04lx)\\n\",\n+        mdps + offsetof(metadata_encrypted_t, path),\n+        FIELD_SIZEOF(metadata_encrypted_t, path));\n+    DBG(\"metadata_encrypted_t.size          : 0x%04lx (0x%04lx)\\n\",\n+        mdps + offsetof(metadata_encrypted_t, size), FIELD_SIZEOF(metadata_encrypted_t, size));\n+    DBG(\"metadata_encrypted_t.mht_key       : 0x%04lx (0x%04lx)\\n\",\n+        mdps + offsetof(metadata_encrypted_t, mht_key),\n+        FIELD_SIZEOF(metadata_encrypted_t, mht_key));\n+    DBG(\"metadata_encrypted_t.mht_gmac      : 0x%04lx (0x%04lx)\\n\",\n+        mdps + offsetof(metadata_encrypted_t, mht_gmac),\n+        FIELD_SIZEOF(metadata_encrypted_t, mht_gmac));\n+    DBG(\"metadata_encrypted_t.data          : 0x%04lx (0x%04lx)\\n\",\n+        mdps + offsetof(metadata_encrypted_t, data), FIELD_SIZEOF(metadata_encrypted_t, data));\n+\n+    DBG(\"size(metadata_padding_t)           = 0x%04lx\\n\", sizeof(metadata_padding_t));\n+    DBG(\"metadata_padding_t                 : 0x%04lx (0x%04lx)\\n\",\n+        mdps + sizeof(metadata_encrypted_t), sizeof(metadata_padding_t));\n+\n+    \/* node 0: metadata + 3k of user data *\/\n+    \/* plain metadata *\/\n+    truncate_file(\"trunc_meta_plain_0\", 0);\n+    truncate_file(\"trunc_meta_plain_1\", FIELD_TRUNCATED(metadata_plain_t, file_id));\n+    truncate_file(\"trunc_meta_plain_2\", offsetof(metadata_plain_t, major_version));\n+    truncate_file(\"trunc_meta_plain_3\", offsetof(metadata_plain_t, minor_version));\n+    truncate_file(\"trunc_meta_plain_4\", offsetof(metadata_plain_t, metadata_key_id));\n+    truncate_file(\"trunc_meta_plain_5\", FIELD_TRUNCATED(metadata_plain_t, metadata_key_id));\n+    truncate_file(\"trunc_meta_plain_6\", offsetof(metadata_plain_t, metadata_gmac));\n+    truncate_file(\"trunc_meta_plain_7\", FIELD_TRUNCATED(metadata_plain_t, metadata_gmac));\n+\n+    \/* encrypted metadata *\/\n+    truncate_file(\"trunc_meta_enc_0\", mdps + offsetof(metadata_encrypted_t, path));\n+    truncate_file(\"trunc_meta_enc_1\", mdps + FIELD_TRUNCATED(metadata_encrypted_t, path));\n+    truncate_file(\"trunc_meta_enc_2\", mdps + offsetof(metadata_encrypted_t, size));\n+    truncate_file(\"trunc_meta_enc_3\", mdps + FIELD_TRUNCATED(metadata_encrypted_t, size));\n+    truncate_file(\"trunc_meta_enc_4\", mdps + offsetof(metadata_encrypted_t, mht_key));\n+    truncate_file(\"trunc_meta_enc_5\", mdps + FIELD_TRUNCATED(metadata_encrypted_t, mht_key));\n+    truncate_file(\"trunc_meta_enc_6\", mdps + offsetof(metadata_encrypted_t, mht_gmac));\n+    truncate_file(\"trunc_meta_enc_7\", mdps + FIELD_TRUNCATED(metadata_encrypted_t, mht_gmac));\n+    truncate_file(\"trunc_meta_enc_8\", mdps + offsetof(metadata_encrypted_t, data));\n+    truncate_file(\"trunc_meta_enc_9\", mdps + FIELD_TRUNCATED(metadata_encrypted_t, data));\n+\n+    \/* padding *\/\n+    truncate_file(\"trunc_meta_pad_0\", mdps + sizeof(metadata_encrypted_t));\n+    truncate_file(\"trunc_meta_pad_1\", mdps + sizeof(metadata_encrypted_t)\n+                  + sizeof(metadata_padding_t) \/ 2);\n+\n+    \/* node 1: mht root *\/\n+    \/* after node 0 *\/\n+    truncate_file(\"trunc_mht_0\", PF_NODE_SIZE);\n+    \/* middle of data_nodes_crypto[0].key *\/\n+    truncate_file(\"trunc_mht_1\", PF_NODE_SIZE + PF_KEY_SIZE \/ 2);\n+    \/* after data_nodes_crypto[0].key *\/\n+    truncate_file(\"trunc_mht_2\", PF_NODE_SIZE + PF_KEY_SIZE);\n+    \/* middle of data_nodes_crypto[0].gmac *\/\n+    truncate_file(\"trunc_mht_3\", PF_NODE_SIZE + PF_KEY_SIZE + PF_MAC_SIZE \/ 2);\n+    \/* after data_nodes_crypto[0].gmac *\/\n+    truncate_file(\"trunc_mht_4\", PF_NODE_SIZE + PF_KEY_SIZE + PF_MAC_SIZE);\n+    \/* after data_nodes_crypto *\/\n+    truncate_file(\"trunc_mht_5\", PF_NODE_SIZE + DATA_CRYPTO_SIZE);\n+    \/* middle of mht_nodes_crypto[0].key *\/\n+    truncate_file(\"trunc_mht_6\", PF_NODE_SIZE + DATA_CRYPTO_SIZE + PF_KEY_SIZE \/ 2);\n+    \/* after mht_nodes_crypto[0].key *\/\n+    truncate_file(\"trunc_mht_7\", PF_NODE_SIZE + DATA_CRYPTO_SIZE + PF_KEY_SIZE);\n+    \/* middle of mht_nodes_crypto[0].gmac *\/\n+    truncate_file(\"trunc_mht_8\", PF_NODE_SIZE + DATA_CRYPTO_SIZE + PF_KEY_SIZE + PF_MAC_SIZE \/ 2);\n+    \/* after mht_nodes_crypto[0].gmac *\/\n+    truncate_file(\"trunc_mht_9\", PF_NODE_SIZE + DATA_CRYPTO_SIZE + PF_KEY_SIZE + PF_MAC_SIZE);\n+\n+    \/* node 2-3: data #0, #1 *\/\n+    \/* after mht root *\/\n+    truncate_file(\"trunc_data_0\", 2 * PF_NODE_SIZE);\n+    \/* middle of data #0 *\/\n+    truncate_file(\"trunc_data_1\", 2 * PF_NODE_SIZE + PF_NODE_SIZE \/ 2);\n+    \/* after data #0 *\/\n+    truncate_file(\"trunc_data_2\", 3 * PF_NODE_SIZE);\n+    \/* middle of data #1 *\/\n+    truncate_file(\"trunc_data_3\", 3 * PF_NODE_SIZE + PF_NODE_SIZE \/ 2);\n+\n+    \/* extend *\/\n+    truncate_file(\"extend_0\", g_input_size + 1);\n+    truncate_file(\"extend_1\", g_input_size + PF_NODE_SIZE \/ 2);\n+    truncate_file(\"extend_2\", g_input_size + PF_NODE_SIZE);\n+    truncate_file(\"extend_3\", g_input_size + PF_NODE_SIZE + PF_NODE_SIZE \/ 2);\n+}\n+\n+\/* returns mmap'd output contents *\/\n+static void* create_output(const char* path) {\n+    void* mem = MAP_FAILED;\n+    int fd = open(path, O_RDWR|O_CREAT, 0664);\n+    if (fd < 0)\n+        FATAL(\"Failed to open output file '%s': %s\\n\", path, strerror(errno));\n+\n+    if (ftruncate(fd, g_input_size) < 0)\n+        FATAL(\"Failed to ftruncate output file '%s': %s\\n\", path, strerror(errno));\n+\n+    mem = mmap(NULL, g_input_size, PROT_READ|PROT_WRITE, MAP_SHARED, fd, 0);\n+    if (mem == MAP_FAILED)\n+        FATAL(\"Failed to mmap output file '%s': %s\\n\", path, strerror(errno));\n+\n+    memcpy(mem, g_input_data, g_input_size);\n+\n+    close(fd);\n+    return mem;\n+}\n+\n+static void pf_decrypt(const void* encrypted, size_t size, const pf_key_t* key, const pf_mac_t* mac,\n+                       void* decrypted, const char* msg) {\n+    pf_status_t status = mbedtls_aes_gcm_decrypt(key, &g_empty_iv, NULL, 0,\n+                                                 encrypted, size,\n+                                                 decrypted, mac);\n+    if (PF_FAILURE(status))\n+        FATAL(\"decrypting %s failed\\n\", msg);\n+}\n+\n+static void pf_encrypt(const void* decrypted, size_t size, const pf_key_t* key, pf_mac_t* mac,\n+                       void* encrypted, const char* msg) {\n+    pf_status_t status = mbedtls_aes_gcm_encrypt(key, &g_empty_iv, NULL, 0,\n+                                                 decrypted, size,\n+                                                 encrypted, mac);\n+    if (PF_FAILURE(status))\n+        FATAL(\"encrypting %s failed\\n\", msg);\n+}\n+\n+\/* copy input PF and apply some modifications *\/\n+#define __BREAK_PF(suffix, ...) do { \\\n+    make_output_path(suffix); \\\n+    meta = create_output(g_output_path); \\\n+    out = (uint8_t*)meta; \\\n+    pf_decrypt(&meta->encrypted_part, sizeof(meta->encrypted_part), &g_meta_key, \\\n+               &meta->plain_part.metadata_gmac, meta_dec, \"metadata\"); \\\n+    mht_enc = (mht_node_t*)(out + PF_NODE_SIZE); \\\n+    pf_decrypt(mht_enc, sizeof(*mht_enc), &meta_dec->mht_key, &meta_dec->mht_gmac, mht_dec, \\\n+               \"mht\"); \\\n+    __VA_ARGS__ \\\n+    munmap(meta, g_input_size); \\\n+} while (0)\n+\n+\/* if update is true, also create a file with correct metadata MAC *\/\n+#define BREAK_PF(suffix, update, ...) do { \\\n+    __BREAK_PF(suffix, __VA_ARGS__); \\\n+    if (update) { \\\n+        __BREAK_PF(suffix \"_fixed\", __VA_ARGS__ { \\\n+                       pf_encrypt(meta_dec, sizeof(*meta_dec), &g_meta_key, \\\n+                                  &meta->plain_part.metadata_gmac, meta->encrypted_part, \\\n+                                  \"metadata\"); \\\n+                   } ); \\\n+    } \\\n+} while (0)\n+\n+#define BREAK_MHT(suffix, ...) do { \\\n+    __BREAK_PF(suffix, __VA_ARGS__ { \\\n+                   pf_encrypt(mht_dec, sizeof(*mht_dec), &meta_dec->mht_key, &meta_dec->mht_gmac, \\\n+                              mht_enc, \"mht\"); \\\n+               } ); \\\n+} while (0)\n+\n+#define LAST_BYTE(array) (((uint8_t*)&array)[sizeof(array) - 1])\n+\n+static void tamper_modify(void) {\n+    metadata_node_t* meta = NULL;\n+    uint8_t* out = NULL;\n+    metadata_encrypted_t* meta_dec = malloc(sizeof(*meta_dec));\n+    if (!meta_dec)\n+        FATAL(\"Out of memory\\n\");\n+    mht_node_t* mht_enc = NULL;\n+    mht_node_t* mht_dec = malloc(sizeof(*mht_dec));\n+    if (!mht_dec)\n+        FATAL(\"Out of memory\\n\");\n+\n+    \/* plain part of the metadata isn't covered by the MAC so no point updating it *\/\n+    BREAK_PF(\"meta_plain_id_0\", \/*update=*\/false,\n+             { meta->plain_part.file_id = 0; });\n+    BREAK_PF(\"meta_plain_id_1\", \/*update=*\/false,\n+             { meta->plain_part.file_id = UINT64_MAX; });\n+    BREAK_PF(\"meta_plain_version_0\", \/*update=*\/false,\n+             { meta->plain_part.major_version = 0; });\n+    BREAK_PF(\"meta_plain_version_1\", \/*update=*\/false,\n+             { meta->plain_part.major_version = 0xff; });\n+    BREAK_PF(\"meta_plain_version_2\", \/*update=*\/false,\n+             { meta->plain_part.minor_version = 0xff; });\n+\n+    \/* metadata_key_id is the keying material for encrypted metadata key derivation, so create also\n+     * PFs with updated MACs *\/\n+    BREAK_PF(\"meta_plain_keyid_0\", \/*update=*\/true,\n+             { meta->plain_part.metadata_key_id[0] ^= 1; });\n+    BREAK_PF(\"meta_plain_keyid_1\", \/*update=*\/true,\n+             { LAST_BYTE(meta->plain_part.metadata_key_id) ^= 0xfe; });\n+    BREAK_PF(\"meta_plain_mac_0\", \/*update=*\/true,\n+             { meta->plain_part.metadata_gmac[0] ^= 0xfe; });\n+    BREAK_PF(\"meta_plain_mac_1\", \/*update=*\/true,\n+             { LAST_BYTE(meta->plain_part.metadata_gmac) &= 1; });\n+\n+    BREAK_PF(\"meta_enc_filename_0\", \/*update=*\/true,\n+             { meta_dec->path[0] = 0; });\n+    BREAK_PF(\"meta_enc_filename_1\", \/*update=*\/true,\n+             { meta_dec->path[0] ^= 1; });\n+    BREAK_PF(\"meta_enc_filename_2\", \/*update=*\/true,\n+             { LAST_BYTE(meta_dec->path) ^= 0xfe; });\n+    BREAK_PF(\"meta_enc_size_0\", \/*update=*\/true,\n+             { meta_dec->size = 0; });\n+    BREAK_PF(\"meta_enc_size_1\", \/*update=*\/true,\n+             { meta_dec->size = g_input_size - 1; });\n+    BREAK_PF(\"meta_enc_size_2\", \/*update=*\/true,\n+             { meta_dec->size = g_input_size + 1; });\n+    BREAK_PF(\"meta_enc_size_3\", \/*update=*\/true,\n+             { meta_dec->size = UINT64_MAX; });\n+    BREAK_PF(\"meta_enc_mht_key_0\", \/*update=*\/true,\n+             { meta_dec->mht_key[0] ^= 1; });\n+    BREAK_PF(\"meta_enc_mht_key_1\", \/*update=*\/true,\n+             { LAST_BYTE(meta_dec->mht_key) ^= 0xfe; });\n+    BREAK_PF(\"meta_enc_mht_mac_0\", \/*update=*\/true,\n+             { meta_dec->mht_gmac[0] ^= 1; });\n+    BREAK_PF(\"meta_enc_mht_mac_1\", \/*update=*\/true,\n+             { LAST_BYTE(meta_dec->mht_gmac) ^= 0xfe; });\n+    BREAK_PF(\"meta_enc_data_0\", \/*update=*\/true,\n+             { meta_dec->data[0] ^= 0xfe; });\n+    BREAK_PF(\"meta_enc_data_1\", \/*update=*\/true,\n+             { LAST_BYTE(meta_dec->data) ^= 1; });\n+\n+    \/* padding is ignored *\/\n+    BREAK_PF(\"meta_padding_0\", \/*update=*\/false,\n+             { meta->padding[0] ^= 1; });\n+    BREAK_PF(\"meta_padding_1\", \/*update=*\/false,\n+             { LAST_BYTE(meta->padding) ^= 0xfe; });\n+\n+    BREAK_MHT(\"mht_0\", { mht_dec->data_nodes_crypto[0].key[0] ^= 1; });\n+    BREAK_MHT(\"mht_1\", { mht_dec->data_nodes_crypto[0].gmac[0] ^= 1; });\n+    BREAK_MHT(\"mht_2\", { mht_dec->mht_nodes_crypto[0].key[0] ^= 1; });\n+    BREAK_MHT(\"mht_3\", { mht_dec->mht_nodes_crypto[0].gmac[0] ^= 1; });\n+    BREAK_MHT(\"mht_4\", { mht_dec->data_nodes_crypto[ATTACHED_DATA_NODES_COUNT - 1].key[0] ^= 1; });\n+    BREAK_MHT(\"mht_5\", { mht_dec->data_nodes_crypto[ATTACHED_DATA_NODES_COUNT - 1].gmac[0] ^= 1; });\n+    BREAK_MHT(\"mht_6\", { mht_dec->mht_nodes_crypto[CHILD_MHT_NODES_COUNT - 1].key[0] ^= 1; });\n+    BREAK_MHT(\"mht_7\", { mht_dec->mht_nodes_crypto[CHILD_MHT_NODES_COUNT - 1].gmac[0] ^= 1; });\n+    BREAK_MHT(\"mht_8\", {\n+        gcm_crypto_data_t crypto;\n+        memcpy(&crypto, &mht_dec->data_nodes_crypto[0], sizeof(crypto));\n+        memcpy(&mht_dec->data_nodes_crypto[0], &mht_dec->data_nodes_crypto[1], sizeof(crypto));\n+        memcpy(&mht_dec->data_nodes_crypto[1], &crypto, sizeof(crypto));\n+    });\n+    BREAK_MHT(\"mht_9\", {\n+        gcm_crypto_data_t crypto;\n+        memcpy(&crypto, &mht_dec->mht_nodes_crypto[0], sizeof(crypto));\n+        memcpy(&mht_dec->mht_nodes_crypto[0], &mht_dec->mht_nodes_crypto[1], sizeof(crypto));\n+        memcpy(&mht_dec->mht_nodes_crypto[1], &crypto, sizeof(crypto));\n+    });\n+\n+    \/* data nodes start from node #2 *\/\n+    BREAK_PF(\"data_0\", \/*update=*\/false,\n+             { *(out + 2 * PF_NODE_SIZE) ^= 1; });\n+    BREAK_PF(\"data_1\", \/*update=*\/false,\n+             { *(out + 3 * PF_NODE_SIZE - 1) ^= 1; });\n+    BREAK_PF(\"data_2\", \/*update=*\/false, {\n+        \/* swap data nodes *\/\n+        memcpy(out + 2 * PF_NODE_SIZE, g_input_data + 3 * PF_NODE_SIZE, PF_NODE_SIZE);\n+        memcpy(out + 3 * PF_NODE_SIZE, g_input_data + 2 * PF_NODE_SIZE, PF_NODE_SIZE);\n+    });\n+\n+    free(mht_dec);\n+    free(meta_dec);\n+}\n+\n+int main(int argc, char* argv[]) {\n+    int ret = -1;\n+\n+    int option          = 0;\n+    char* input_path    = NULL;\n+    char* wrap_key_path = NULL;\n+    int input_fd        = -1;\n+\n+    while (true) {\n+        option = getopt_long(argc, argv, \"i:o:w:vh\", g_options, NULL);\n+        if (option == -1)\n+            break;\n+\n+        switch (option) {\n+            case 'i':\n+                input_path = optarg;\n+                break;\n+            case 'o':\n+                g_output_dir = optarg;\n+                break;\n+            case 'w':\n+                wrap_key_path = optarg;\n+                break;\n+            case 'v':\n+                set_verbose(true);\n+                break;\n+            case 'h':\n+                usage();\n+                return 0;\n+            default:\n+                ERROR(\"Unknown option: %c\\n\", option);\n+                usage();\n+        }\n+    }\n+\n+    if (!input_path) {\n+        ERROR(\"Input path not specified\\n\");\n+        usage();\n+        goto out;\n+    }\n+\n+    if (!g_output_dir) {\n+        ERROR(\"Output path not specified\\n\");\n+        usage();\n+        goto out;\n+    }\n+\n+    if (!wrap_key_path) {\n+        ERROR(\"Wrap key path not specified\\n\");\n+        usage();\n+        goto out;\n+    }\n+\n+    input_fd = open(input_path, O_RDONLY);\n+    if (input_fd < 0) {\n+        ERROR(\"Failed to open input file '%s': %s\\n\", input_path, strerror(errno));\n+        goto out;\n+    }\n+\n+    g_input_size = get_file_size(input_fd);\n+    if (g_input_size < 0) {\n+        ERROR(\"Failed to stat input file '%s': %s\\n\", input_path, strerror(errno));\n+        goto out;\n+    }\n+\n+    g_input_data = mmap(NULL, g_input_size, PROT_READ, MAP_PRIVATE, input_fd, 0);\n+    if (g_input_data == MAP_FAILED) {\n+        ERROR(\"Failed to mmap input file '%s': %s\\n\", input_path, strerror(errno));\n+        goto out;\n+    }\n+\n+    load_wrap_key(wrap_key_path, &g_wrap_key);\n+    derive_main_key(&g_wrap_key, &((metadata_plain_t*)g_input_data)->metadata_key_id,\n+                    &g_meta_key);\n+\n+    g_input_name = basename(input_path);\n+    g_output_path_size = strlen(g_input_name) + strlen(g_output_dir) + 256;\n+    g_output_path = malloc(g_output_path_size);\n+    if (!g_output_path) {\n+        ERROR(\"No memory\\n\");\n+        goto out;\n+    }\n+\n+    tamper_truncate();\n+    tamper_modify();\n+    ret = 0;\n+\n+out:\n+    \/* skip cleanup as we are in main() *\/\n+    return ret;\n+}\n","message":"","files":{"\/LibOS\/shim\/test\/fs\/test_pf.py":{"changes":[{"diff":"\n \n     def __corrupt_file(self, input_path, output_path):\n         cmd = [self.PF_TAMPER, '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path]\n-        return self.run_native_binary(cmd)\n+        return self.run_native_binary(cmd, libpath=os.path.join(os.getcwd(), 'lib'))\n \n     # invalid\/corrupted files\n-    @expectedFailureIf(HAS_SGX)\n-    # pylint: disable=fixme\n     def test_500_invalid(self):\n-        # TODO: port these to the new file format\n         invalid_dir = os.path.join(self.TEST_DIR, 'pf_invalid')\n-        # files below should work normally (benign modifications)\n-        should_pass = ['chunk_padding_1_fixed', 'chunk_padding_2_fixed', 'chunk_data_3',\n-                       'chunk_data_3_fixed', 'chunk_data_4', 'chunk_data_4_fixed']\n         if not os.path.exists(invalid_dir):\n             os.mkdir(invalid_dir)\n+\n         # prepare valid encrypted file (largest one for maximum possible corruptions)\n         original_input = self.OUTPUT_FILES[-1]\n         self.__encrypt_file(self.INPUT_FILES[-1], original_input)\n         # generate invalid files based on the above\n         self.__corrupt_file(original_input, invalid_dir)\n+\n         # try to decrypt invalid files\n         for name in os.listdir(invalid_dir):\n             invalid = os.path.join(invalid_dir, name)\n","add":3,"remove":7,"filename":"\/LibOS\/shim\/test\/fs\/test_pf.py","badparts":["        return self.run_native_binary(cmd)","    @expectedFailureIf(HAS_SGX)","        should_pass = ['chunk_padding_1_fixed', 'chunk_padding_2_fixed', 'chunk_data_3',","                       'chunk_data_3_fixed', 'chunk_data_4', 'chunk_data_4_fixed']"],"goodparts":["        return self.run_native_binary(cmd, libpath=os.path.join(os.getcwd(), 'lib'))"]},{"diff":"\n             input_path = os.path.join(invalid_dir, os.path.basename(original_input))\n             # copy the file so it has the original file name (for allowed path check)\n             shutil.copy(invalid, input_path)\n-            should_pass = any(s in name for s in should_pass)\n \n             try:\n                 args = ['decrypt', '-V', '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path]\n                 self.__pf_crypt(args)\n             except subprocess.CalledProcessError as exc:\n-                if should_pass:\n-                    self.assertEqual(exc.returncode, 0)\n-                else:\n-                    self.assertNotEqual(exc.returncode, 0)\n+                # decryption of invalid file must fail with -1 (wrapped to 255)\n+                self.assertEqual(exc.returncode, 255)\n             else:\n-                if not should_pass:\n-                    print('[!] Fail: successfully decrypted file: ' + name)\n-                    self.fail()\n+                print('[!] Fail: successfully decrypted file: ' + name)\n+                self.fail()","add":4,"remove":8,"filename":"\/LibOS\/shim\/test\/fs\/test_pf.py","badparts":["            should_pass = any(s in name for s in should_pass)","                if should_pass:","                    self.assertEqual(exc.returncode, 0)","                else:","                    self.assertNotEqual(exc.returncode, 0)","                if not should_pass:","                    print('[!] Fail: successfully decrypted file: ' + name)","                    self.fail()"],"goodparts":["                self.assertEqual(exc.returncode, 255)","                print('[!] Fail: successfully decrypted file: ' + name)","                self.fail()"]}],"source":"\n import filecmp import os import shutil import subprocess import unittest from test_fs import( TC_00_FileSystem, ) from regression import( HAS_SGX, expectedFailureIf, ) @unittest.skipUnless(HAS_SGX, 'Protected files require SGX support') class TC_50_ProtectedFiles(TC_00_FileSystem): @classmethod def setUpClass(cls): cls.PF_CRYPT='bin\/pf_crypt' cls.PF_TAMPER='bin\/pf_tamper' cls.WRAP_KEY=os.path.join(cls.TEST_DIR, 'wrap-key') cls.CONST_WRAP_KEY=[0xff, 0xee, 0xdd, 0xcc, 0xbb, 0xaa, 0x99, 0x88, 0x77, 0x66, 0x55, 0x44, 0x33, 0x22, 0x11, 0x00] cls.ENCRYPTED_DIR=os.path.join(cls.TEST_DIR, 'pf_input') cls.ENCRYPTED_FILES=[os.path.join(cls.ENCRYPTED_DIR, str(v)) for v in cls.FILE_SIZES] cls.LIB_PATH=os.path.join(os.getcwd(), 'lib') super().setUpClass() if not os.path.exists(cls.ENCRYPTED_DIR): os.mkdir(cls.ENCRYPTED_DIR) cls.OUTPUT_DIR=os.path.join(cls.TEST_DIR, 'pf_output') cls.OUTPUT_FILES=[os.path.join(cls.OUTPUT_DIR, str(x)) for x in cls.FILE_SIZES] cls.__set_default_key(cls) for i in cls.INDEXES: cmd=[cls.PF_CRYPT, 'encrypt', '-w', cls.WRAP_KEY, '-i', cls.INPUT_FILES[i], '-o', cls.ENCRYPTED_FILES[i]] cls.run_native_binary(cmd, libpath=os.path.join(os.getcwd(), 'lib')) def __pf_crypt(self, args): args.insert(0, self.PF_CRYPT) return self.run_native_binary(args, libpath=os.path.join(os.getcwd(), 'lib')) def __set_default_key(self): with open(self.WRAP_KEY, 'wb') as file: file.write(bytes(self.CONST_WRAP_KEY)) def copy_input(self, input_path, output_path): self.__encrypt_file(input_path, output_path) def __encrypt_file(self, input_path, output_path): args=['encrypt', '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path] stdout, stderr=self.__pf_crypt(args) return(stdout, stderr) def __decrypt_file(self, input_path, output_path): args=['decrypt', '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path] stdout, stderr=self.__pf_crypt(args) return(stdout, stderr) def test_000_gen_key(self): key_path=os.path.join(self.TEST_DIR, 'tmpkey') args=['gen-key', '-w', key_path] stdout, _=self.__pf_crypt(args) self.assertIn('Wrap key saved to: ' +key_path, stdout) self.assertEqual(os.path.getsize(key_path), 16) os.remove(key_path) def test_010_encrypt_decrypt(self): for i in self.INDEXES: self.__encrypt_file(self.INPUT_FILES[i], self.OUTPUT_FILES[i]) self.assertFalse(filecmp.cmp(self.INPUT_FILES[i], self.OUTPUT_FILES[i], shallow=False)) dec_path=os.path.join(self.OUTPUT_DIR, os.path.basename(self.OUTPUT_FILES[i]) +'.dec') self.__decrypt_file(self.OUTPUT_FILES[i], dec_path) self.assertTrue(filecmp.cmp(self.INPUT_FILES[i], dec_path, shallow=False)) def test_100_open_close(self): input_path=self.ENCRYPTED_FILES[-1] output_path=os.path.join(self.OUTPUT_DIR, 'test_100') stdout, stderr=self.run_binary(['open_close', 'R', input_path]) self.verify_open_close(stdout, stderr, input_path, 'input') try: stdout, stderr=self.run_binary(['open_close', 'W', output_path]) self.assertIn('ERROR: Failed to open output file', stderr) except subprocess.CalledProcessError as exc: self.assertNotEqual(exc.returncode, 0) self.assertTrue(os.path.isfile(output_path)) else: print('[!] Fail: open_close returned 0') self.fail() def test_101_open_flags(self): file_path=os.path.join(self.OUTPUT_DIR, 'test_101') stdout, stderr=self.run_binary(['open_flags', file_path]) self.verify_open_flags(stdout, stderr) def test_115_seek_tell(self): plaintext_path=self.INPUT_FILES[-1] input_path=self.ENCRYPTED_FILES[-1] output_path_1=os.path.join(self.OUTPUT_DIR, 'test_115a') output_path_2=os.path.join(self.OUTPUT_DIR, 'test_115b') self.copy_input(plaintext_path, output_path_1) self.copy_input(plaintext_path, output_path_2) stdout, stderr=self.run_binary(['seek_tell', input_path, output_path_1, output_path_2]) self.verify_seek_tell(stdout, stderr, input_path, output_path_1, output_path_2, self.FILE_SIZES[-1]) def test_130_file_stat(self): for i in self.INDEXES: input_path=self.ENCRYPTED_FILES[i] output_path=self.OUTPUT_FILES[i] size=str(self.FILE_SIZES[i]) self.copy_input(self.INPUT_FILES[i], output_path) stdout, stderr=self.run_binary(['stat', input_path, output_path]) self.verify_stat(stdout, stderr, input_path, output_path, size) def verify_size(self, file, size): dec_path=os.path.join(self.OUTPUT_DIR, os.path.basename(file) +'.dec') self.__decrypt_file(file, dec_path) self.assertEqual(os.stat(dec_path).st_size, size) @expectedFailureIf(HAS_SGX) def test_140_file_truncate(self): self.fail() def test_150_file_rename(self): path1=os.path.join(self.OUTPUT_DIR, 'test_150a') path2=os.path.join(self.OUTPUT_DIR, 'test_150b') self.copy_input(self.ENCRYPTED_FILES[-1], path1) shutil.copy(path1, path2) args=['decrypt', '-V', '-w', self.WRAP_KEY, '-i', path2, '-o', path1] try: self.__pf_crypt(args) except subprocess.CalledProcessError as exc: self.assertNotEqual(exc.returncode, 0) else: print('[!] Fail: successfully decrypted renamed file: ' +path2) self.fail() def verify_copy_content(self, input_path, output_path): dec_path=os.path.join(self.OUTPUT_DIR, os.path.basename(output_path) +'.dec') self.__decrypt_file(output_path, dec_path) self.assertTrue(filecmp.cmp(input_path, dec_path, shallow=False)) def do_copy_test(self, executable, timeout): stdout, stderr=self.run_binary([executable, self.ENCRYPTED_DIR, self.OUTPUT_DIR], timeout=timeout) self.verify_copy(stdout, stderr, self.ENCRYPTED_DIR, executable) def test_204_copy_dir_mmap_whole(self): self.do_copy_test('copy_mmap_whole', 30) def test_205_copy_dir_mmap_seq(self): self.do_copy_test('copy_mmap_seq', 60) def test_206_copy_dir_mmap_rev(self): self.do_copy_test('copy_mmap_rev', 60) def test_210_copy_dir_mounted(self): executable='copy_whole' stdout, stderr=self.run_binary([executable, '\/mounted\/pf_input', '\/mounted\/pf_output'], timeout=30) self.verify_copy(stdout, stderr, '\/mounted\/pf_input', executable) def __corrupt_file(self, input_path, output_path): cmd=[self.PF_TAMPER, '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path] return self.run_native_binary(cmd) @expectedFailureIf(HAS_SGX) def test_500_invalid(self): invalid_dir=os.path.join(self.TEST_DIR, 'pf_invalid') should_pass=['chunk_padding_1_fixed', 'chunk_padding_2_fixed', 'chunk_data_3', 'chunk_data_3_fixed', 'chunk_data_4', 'chunk_data_4_fixed'] if not os.path.exists(invalid_dir): os.mkdir(invalid_dir) original_input=self.OUTPUT_FILES[-1] self.__encrypt_file(self.INPUT_FILES[-1], original_input) self.__corrupt_file(original_input, invalid_dir) for name in os.listdir(invalid_dir): invalid=os.path.join(invalid_dir, name) output_path=os.path.join(self.OUTPUT_DIR, name) input_path=os.path.join(invalid_dir, os.path.basename(original_input)) shutil.copy(invalid, input_path) should_pass=any(s in name for s in should_pass) try: args=['decrypt', '-V', '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path] self.__pf_crypt(args) except subprocess.CalledProcessError as exc: if should_pass: self.assertEqual(exc.returncode, 0) else: self.assertNotEqual(exc.returncode, 0) else: if not should_pass: print('[!] Fail: successfully decrypted file: ' +name) self.fail() ","sourceWithComments":"#!\/usr\/bin\/env python3\n\nimport filecmp\nimport os\nimport shutil\nimport subprocess\nimport unittest\n\nfrom test_fs import (\n    TC_00_FileSystem,\n)\n\nfrom regression import (\n    HAS_SGX,\n    expectedFailureIf,\n)\n\n@unittest.skipUnless(HAS_SGX, 'Protected files require SGX support')\nclass TC_50_ProtectedFiles(TC_00_FileSystem):\n    @classmethod\n    def setUpClass(cls):\n        cls.PF_CRYPT = 'bin\/pf_crypt'\n        cls.PF_TAMPER = 'bin\/pf_tamper'\n        cls.WRAP_KEY = os.path.join(cls.TEST_DIR, 'wrap-key')\n        # CONST_WRAP_KEY must match the one in manifest\n        cls.CONST_WRAP_KEY = [0xff, 0xee, 0xdd, 0xcc, 0xbb, 0xaa, 0x99, 0x88,\n                              0x77, 0x66, 0x55, 0x44, 0x33, 0x22, 0x11, 0x00]\n        cls.ENCRYPTED_DIR = os.path.join(cls.TEST_DIR, 'pf_input')\n        cls.ENCRYPTED_FILES = [os.path.join(cls.ENCRYPTED_DIR, str(v)) for v in cls.FILE_SIZES]\n        cls.LIB_PATH = os.path.join(os.getcwd(), 'lib')\n\n        super().setUpClass()\n        if not os.path.exists(cls.ENCRYPTED_DIR):\n            os.mkdir(cls.ENCRYPTED_DIR)\n        cls.OUTPUT_DIR = os.path.join(cls.TEST_DIR, 'pf_output')\n        cls.OUTPUT_FILES = [os.path.join(cls.OUTPUT_DIR, str(x)) for x in cls.FILE_SIZES]\n        # create encrypted files\n        cls.__set_default_key(cls)\n        for i in cls.INDEXES:\n            cmd = [cls.PF_CRYPT, 'encrypt', '-w', cls.WRAP_KEY, '-i', cls.INPUT_FILES[i], '-o',\n                   cls.ENCRYPTED_FILES[i]]\n\n            cls.run_native_binary(cmd, libpath=os.path.join(os.getcwd(), 'lib'))\n\n    def __pf_crypt(self, args):\n        args.insert(0, self.PF_CRYPT)\n        return self.run_native_binary(args, libpath=os.path.join(os.getcwd(), 'lib'))\n\n    def __set_default_key(self):\n        with open(self.WRAP_KEY, 'wb') as file:\n            file.write(bytes(self.CONST_WRAP_KEY))\n\n    # overrides TC_00_FileSystem to encrypt the file instead of just copying\n    def copy_input(self, input_path, output_path):\n        self.__encrypt_file(input_path, output_path)\n\n    def __encrypt_file(self, input_path, output_path):\n        args = ['encrypt', '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path]\n        stdout, stderr = self.__pf_crypt(args)\n        return (stdout, stderr)\n\n    def __decrypt_file(self, input_path, output_path):\n        args = ['decrypt', '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path]\n        stdout, stderr = self.__pf_crypt(args)\n        return (stdout, stderr)\n\n    def test_000_gen_key(self):\n        # test random key generation\n        key_path = os.path.join(self.TEST_DIR, 'tmpkey')\n        args = ['gen-key', '-w', key_path]\n        stdout, _ = self.__pf_crypt(args)\n        self.assertIn('Wrap key saved to: ' + key_path, stdout)\n        self.assertEqual(os.path.getsize(key_path), 16)\n        os.remove(key_path)\n\n    def test_010_encrypt_decrypt(self):\n        for i in self.INDEXES:\n            self.__encrypt_file(self.INPUT_FILES[i], self.OUTPUT_FILES[i])\n            self.assertFalse(filecmp.cmp(self.INPUT_FILES[i], self.OUTPUT_FILES[i], shallow=False))\n            dec_path = os.path.join(self.OUTPUT_DIR,\n                                    os.path.basename(self.OUTPUT_FILES[i]) + '.dec')\n            self.__decrypt_file(self.OUTPUT_FILES[i], dec_path)\n            self.assertTrue(filecmp.cmp(self.INPUT_FILES[i], dec_path, shallow=False))\n\n    # overrides TC_00_FileSystem to change input dir (from plaintext to encrypted)\n    def test_100_open_close(self):\n        # the test binary expects a path to read-only (existing) file or a path to file that\n        # will get created\n        input_path = self.ENCRYPTED_FILES[-1] # existing file\n        output_path = os.path.join(self.OUTPUT_DIR, 'test_100') # new file\n        stdout, stderr = self.run_binary(['open_close', 'R', input_path])\n        self.verify_open_close(stdout, stderr, input_path, 'input')\n        # the following test tries to open multiple handles to a single writable PF, should fail\n        try:\n            stdout, stderr = self.run_binary(['open_close', 'W', output_path])\n            self.assertIn('ERROR: Failed to open output file', stderr)\n        except subprocess.CalledProcessError as exc:\n            self.assertNotEqual(exc.returncode, 0)\n            self.assertTrue(os.path.isfile(output_path))\n        else:\n            print('[!] Fail: open_close returned 0')\n            self.fail()\n\n    # overrides TC_00_FileSystem to change input dir (from plaintext to encrypted)\n    def test_101_open_flags(self):\n        # the test binary expects a path to file that will get created\n        file_path = os.path.join(self.OUTPUT_DIR, 'test_101') # new file\n        stdout, stderr = self.run_binary(['open_flags', file_path])\n        self.verify_open_flags(stdout, stderr)\n\n    # overrides TC_00_FileSystem to change input dir (from plaintext to encrypted)\n    def test_115_seek_tell(self):\n        # the test binary expects a path to read-only (existing) file and two paths to files that\n        # will get created\n        plaintext_path = self.INPUT_FILES[-1]\n        input_path = self.ENCRYPTED_FILES[-1] # existing file\n        output_path_1 = os.path.join(self.OUTPUT_DIR, 'test_115a') # writable files\n        output_path_2 = os.path.join(self.OUTPUT_DIR, 'test_115b')\n        self.copy_input(plaintext_path, output_path_1) # encrypt\n        self.copy_input(plaintext_path, output_path_2)\n        stdout, stderr = self.run_binary(['seek_tell', input_path, output_path_1, output_path_2])\n        self.verify_seek_tell(stdout, stderr, input_path, output_path_1, output_path_2,\n                              self.FILE_SIZES[-1])\n\n    # overrides TC_00_FileSystem to change input dir (from plaintext to encrypted)\n    def test_130_file_stat(self):\n        # the test binary expects a path to read-only (existing) file and a path to file that\n        # will get created\n        for i in self.INDEXES:\n            input_path = self.ENCRYPTED_FILES[i]\n            output_path = self.OUTPUT_FILES[i]\n            size = str(self.FILE_SIZES[i])\n            self.copy_input(self.INPUT_FILES[i], output_path)\n            stdout, stderr = self.run_binary(['stat', input_path, output_path])\n            self.verify_stat(stdout, stderr, input_path, output_path, size)\n\n    # overrides TC_00_FileSystem to decrypt output\n    def verify_size(self, file, size):\n        dec_path = os.path.join(self.OUTPUT_DIR, os.path.basename(file) + '.dec')\n        self.__decrypt_file(file, dec_path)\n        self.assertEqual(os.stat(dec_path).st_size, size)\n\n    @expectedFailureIf(HAS_SGX)\n    # pylint: disable=fixme\n    def test_140_file_truncate(self):\n        self.fail() # TODO: port these to the new file format\n\n    def test_150_file_rename(self):\n        path1 = os.path.join(self.OUTPUT_DIR, 'test_150a')\n        path2 = os.path.join(self.OUTPUT_DIR, 'test_150b')\n        self.copy_input(self.ENCRYPTED_FILES[-1], path1)\n        shutil.copy(path1, path2)\n        # accessing renamed file should fail\n        args = ['decrypt', '-V', '-w', self.WRAP_KEY, '-i', path2, '-o', path1]\n        try:\n            self.__pf_crypt(args)\n        except subprocess.CalledProcessError as exc:\n            self.assertNotEqual(exc.returncode, 0)\n        else:\n            print('[!] Fail: successfully decrypted renamed file: ' + path2)\n            self.fail()\n\n    # overrides TC_00_FileSystem to decrypt output\n    def verify_copy_content(self, input_path, output_path):\n        dec_path = os.path.join(self.OUTPUT_DIR, os.path.basename(output_path) + '.dec')\n        self.__decrypt_file(output_path, dec_path)\n        self.assertTrue(filecmp.cmp(input_path, dec_path, shallow=False))\n\n    # overrides TC_00_FileSystem to change input dir (from plaintext to encrypted)\n    def do_copy_test(self, executable, timeout):\n        stdout, stderr = self.run_binary([executable, self.ENCRYPTED_DIR, self.OUTPUT_DIR],\n                                         timeout=timeout)\n        self.verify_copy(stdout, stderr, self.ENCRYPTED_DIR, executable)\n\n    # overrides TC_00_FileSystem to not skip this on SGX\n    def test_204_copy_dir_mmap_whole(self):\n        self.do_copy_test('copy_mmap_whole', 30)\n\n    # overrides TC_00_FileSystem to not skip this on SGX\n    def test_205_copy_dir_mmap_seq(self):\n        self.do_copy_test('copy_mmap_seq', 60)\n\n    # overrides TC_00_FileSystem to not skip this on SGX\n    def test_206_copy_dir_mmap_rev(self):\n        self.do_copy_test('copy_mmap_rev', 60)\n\n    # overrides TC_00_FileSystem to change dirs (from plaintext to encrypted)\n    def test_210_copy_dir_mounted(self):\n        executable = 'copy_whole'\n        stdout, stderr = self.run_binary([executable, '\/mounted\/pf_input', '\/mounted\/pf_output'],\n                                         timeout=30)\n        self.verify_copy(stdout, stderr, '\/mounted\/pf_input', executable)\n\n    def __corrupt_file(self, input_path, output_path):\n        cmd = [self.PF_TAMPER, '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path]\n        return self.run_native_binary(cmd)\n\n    # invalid\/corrupted files\n    @expectedFailureIf(HAS_SGX)\n    # pylint: disable=fixme\n    def test_500_invalid(self):\n        # TODO: port these to the new file format\n        invalid_dir = os.path.join(self.TEST_DIR, 'pf_invalid')\n        # files below should work normally (benign modifications)\n        should_pass = ['chunk_padding_1_fixed', 'chunk_padding_2_fixed', 'chunk_data_3',\n                       'chunk_data_3_fixed', 'chunk_data_4', 'chunk_data_4_fixed']\n        if not os.path.exists(invalid_dir):\n            os.mkdir(invalid_dir)\n        # prepare valid encrypted file (largest one for maximum possible corruptions)\n        original_input = self.OUTPUT_FILES[-1]\n        self.__encrypt_file(self.INPUT_FILES[-1], original_input)\n        # generate invalid files based on the above\n        self.__corrupt_file(original_input, invalid_dir)\n        # try to decrypt invalid files\n        for name in os.listdir(invalid_dir):\n            invalid = os.path.join(invalid_dir, name)\n            output_path = os.path.join(self.OUTPUT_DIR, name)\n            input_path = os.path.join(invalid_dir, os.path.basename(original_input))\n            # copy the file so it has the original file name (for allowed path check)\n            shutil.copy(invalid, input_path)\n            should_pass = any(s in name for s in should_pass)\n\n            try:\n                args = ['decrypt', '-V', '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path]\n                self.__pf_crypt(args)\n            except subprocess.CalledProcessError as exc:\n                if should_pass:\n                    self.assertEqual(exc.returncode, 0)\n                else:\n                    self.assertNotEqual(exc.returncode, 0)\n            else:\n                if not should_pass:\n                    print('[!] Fail: successfully decrypted file: ' + name)\n                    self.fail()\n"}},"msg":"[Pal\/Linux-SGX] Test malicious modifications to protected files\n\nThis commit adds a new PF utility `pf_tamper` that tampers with\nvalid protected files and uses this utility to test that the PF\nlogic in Linux-SGX detects such malicious modifications.\n\nThis commit also moves out the PF-format macros and structs from\n`protected_files_internal.h` to `protected_files_format.h` for\nbetter readability.\n\nCo-authored-by: Dmitrii Kuvaiskii <dmitrii.kuvaiskii@intel.com>"}},"https:\/\/github.com\/webexplo1t\/findXSRF":{"fd3c868314b86a1ffdbc6633672782a2269e850d":{"url":"https:\/\/api.github.com\/repos\/webexplo1t\/findXSRF\/commits\/fd3c868314b86a1ffdbc6633672782a2269e850d","html_url":"https:\/\/github.com\/webexplo1t\/findXSRF\/commit\/fd3c868314b86a1ffdbc6633672782a2269e850d","sha":"fd3c868314b86a1ffdbc6633672782a2269e850d","keyword":"tampering check","diff":"diff --git a\/modules\/Tamper.py b\/modules\/Tamper.py\nindex 7281b96..de933d5 100644\n--- a\/modules\/Tamper.py\n+++ b\/modules\/Tamper.py\n@@ -19,6 +19,7 @@\n # Null char flags (hex)\n flagx1 = 0x00\n flagx2 = 0x00\n+flagx3 = 0x00\n \n def Tamper(url, action, req, body, query, para):\n     '''\n@@ -54,15 +55,14 @@ def Tamper(url, action, req, body, query, para):\n     # we assume that the tamper did not work :( But if there is a 20x\n     # (Accepted) or a 30x (Redirection), then we know it worked.\n     #\n-    # NOTE: This algorithm has lots of room for improvment.\n-    if not str(resp.status_code).startswith('40') or not str(resp.status_code).startswith('50'):\n+    # NOTE: This algorithm has lots of room for improvement.\n+    if str(resp.status_code).startswith('50'):\n+        verbout(color.RED,' [+] Token tamper from request causes a 50x Internal Error!')\n+    if not str(resp.status_code).startswith('40') and not str(resp.status_code).startswith('50'):\n         flagx1 = 0x01\n \n     # [Step 2]: Second we take the token and then remove a char\n     # at a specific position and test the response body.\n-    #\n-    # Required check for checking if string at that position isn't the\n-    # same char we are going to replace with.\n     verbout(GR, 'Tampering Token by index removal...')\n     tampvalx2 = replaceStrIndex(value, 3)\n     verbout(G, 'Tampered Token: '+color.CYAN+tampvalx1)\n@@ -74,12 +74,32 @@ def Tamper(url, action, req, body, query, para):\n     # we assume that the tamper did not work :( But if there is a 20x\n     # (Accepted) or a 30x (Redirection), then we know it worked.\n     #\n-    # NOTE: This algorithm has lots of room for improvment.\n-    if not str(resp.status_code).startswith('40') or not str(resp.status_code).startswith('50'):\n+    # NOTE: This algorithm has lots of room for improvement.\n+    if str(resp.status_code).startswith('50'):\n+        verbout(color.RED,' [+] Token tamper from request causes a 50x Internal Error!')\n+    if not str(resp.status_code).startswith('40') and not str(resp.status_code).startswith('50'):\n+        flagx2 = 0x01\n+\n+    # [Step 3]: Third we take the token and then remove the whole\n+    # anticsrf token and test the response body.\n+    verbout(GR, 'Tampering Token by Token removal...')\n+    del req[query]\n+    verbout(G, 'Removed token from request!')\n+    # Lets build up the request...\n+    resp = Post(url, action, req)\n+\n+    # If there is a 40x (Not Found) or a 50x (Internal Error) error,\n+    # we assume that the tamper did not work :( But if there is a 20x\n+    # (Accepted) or a 30x (Redirection), then we know it worked.\n+    #\n+    # NOTE: This algorithm has lots of room for improvement.\n+    if str(resp.status_code).startswith('50'):\n+        verbout(color.RED,' [+] Token removal from request causes a 50x Internal Error!')\n+    if not str(resp.status_code).startswith('40') and not str(resp.status_code).startswith('50'):\n         flagx2 = 0x01\n \n     # If any of the forgeries worked...\n-    if flagx1 == 0x01 or flagx2 == 0x01:\n+    if flagx1 == 0x01 or flagx2 == 0x01 or flagx3 == 0x01:\n         verbout(color.GREEN,' [+] The tampered token value works!')\n         verbout(color.GREEN,' [-] The Tampered Anti-CSRF Token requested does NOT return a 40x or 50x response! ')\n         print(color.ORANGE+' [-] Endpoint '+color.BR+' CONFIRMED VULNERABLE '+color.END+color.ORANGE+' to Request Forgery Attacks...')\n@@ -92,4 +112,3 @@ def Tamper(url, action, req, body, query, para):\n def replaceStrIndex(text, index=0, replacement=''):\n     ''' This method returns a tampered string by replacement '''\n     return '%s%s%s' % (text[:index], replacement, text[index+1:])\n-\n","message":"","files":{"\/modules\/Tamper.py":{"changes":[{"diff":"\n     # we assume that the tamper did not work :( But if there is a 20x\n     # (Accepted) or a 30x (Redirection), then we know it worked.\n     #\n-    # NOTE: This algorithm has lots of room for improvment.\n-    if not str(resp.status_code).startswith('40') or not str(resp.status_code).startswith('50'):\n+    # NOTE: This algorithm has lots of room for improvement.\n+    if str(resp.status_code).startswith('50'):\n+        verbout(color.RED,' [+] Token tamper from request causes a 50x Internal Error!')\n+    if not str(resp.status_code).startswith('40') and not str(resp.status_code).startswith('50'):\n         flagx1 = 0x01\n \n     # [Step 2]: Second we take the token and then remove a char\n     # at a specific position and test the response body.\n-    #\n-    # Required check for checking if string at that position isn't the\n-    # same char we are going to replace with.\n     verbout(GR, 'Tampering Token by index removal...')\n     tampvalx2 = replaceStrIndex(value, 3)\n     verbout(G, 'Tampered Token: '+color.CYAN+tampvalx1)\n","add":4,"remove":5,"filename":"\/modules\/Tamper.py","badparts":["    if not str(resp.status_code).startswith('40') or not str(resp.status_code).startswith('50'):"],"goodparts":["    if str(resp.status_code).startswith('50'):","        verbout(color.RED,' [+] Token tamper from request causes a 50x Internal Error!')","    if not str(resp.status_code).startswith('40') and not str(resp.status_code).startswith('50'):"]},{"diff":"\n     # we assume that the tamper did not work :( But if there is a 20x\n     # (Accepted) or a 30x (Redirection), then we know it worked.\n     #\n-    # NOTE: This algorithm has lots of room for improvment.\n-    if not str(resp.status_code).startswith('40') or not str(resp.status_code).startswith('50'):\n+    # NOTE: This algorithm has lots of room for improvement.\n+    if str(resp.status_code).startswith('50'):\n+        verbout(color.RED,' [+] Token tamper from request causes a 50x Internal Error!')\n+    if not str(resp.status_code).startswith('40') and not str(resp.status_code).startswith('50'):\n+        flagx2 = 0x01\n+\n+    # [Step 3]: Third we take the token and then remove the whole\n+    # anticsrf token and test the response body.\n+    verbout(GR, 'Tampering Token by Token removal...')\n+    del req[query]\n+    verbout(G, 'Removed token from request!')\n+    # Lets build up the request...\n+    resp = Post(url, action, req)\n+\n+    # If there is a 40x (Not Found) or a 50x (Internal Error) error,\n+    # we assume that the tamper did not work :( But if there is a 20x\n+    # (Accepted) or a 30x (Redirection), then we know it worked.\n+    #\n+    # NOTE: This algorithm has lots of room for improvement.\n+    if str(resp.status_code).startswith('50'):\n+        verbout(color.RED,' [+] Token removal from request causes a 50x Internal Error!')\n+    if not str(resp.status_code).startswith('40') and not str(resp.status_code).startswith('50'):\n         flagx2 = 0x01\n \n     # If any of the forgeries worked...\n-    if flagx1 == 0x01 or flagx2 == 0x01:\n+    if flagx1 == 0x01 or flagx2 == 0x01 or flagx3 == 0x01:\n         verbout(color.GREEN,' [+] The tampered token value works!')\n         verbout(color.GREEN,' [-] The Tampered Anti-CSRF Token requested does NOT return a 40x or 50x response! ')\n         print(color.ORANGE+' [-] Endpoint '+color.BR+' CONFIRMED VULNERABLE '+color.END+color.ORANGE+' to Request Forgery Attacks...')\n","add":23,"remove":3,"filename":"\/modules\/Tamper.py","badparts":["    if not str(resp.status_code).startswith('40') or not str(resp.status_code).startswith('50'):","    if flagx1 == 0x01 or flagx2 == 0x01:"],"goodparts":["    if str(resp.status_code).startswith('50'):","        verbout(color.RED,' [+] Token tamper from request causes a 50x Internal Error!')","    if not str(resp.status_code).startswith('40') and not str(resp.status_code).startswith('50'):","        flagx2 = 0x01","    verbout(GR, 'Tampering Token by Token removal...')","    del req[query]","    verbout(G, 'Removed token from request!')","    resp = Post(url, action, req)","    if str(resp.status_code).startswith('50'):","        verbout(color.RED,' [+] Token removal from request causes a 50x Internal Error!')","    if not str(resp.status_code).startswith('40') and not str(resp.status_code).startswith('50'):","    if flagx1 == 0x01 or flagx2 == 0x01 or flagx3 == 0x01:"]}],"source":"\n from re import search, I from core.colors import * from core.request import Post from files.config import * from core.verbout import verbout from urllib.parse import urlencode, quote flagx1=0x00 flagx2=0x00 def Tamper(url, action, req, body, query, para): ''' The main idea behind this is to tamper the Anti-CSRF tokens found and check the content length for related vulnerabilities. ''' verbout(GR, 'Proceeding for CSRF attack via Anti-CSRF token tampering...') if para=='': return True value=r'%s' % para verbout(GR, 'Tampering Token by index replacement...') if value[3] !='a': tampvalx1=replaceStrIndex(value, 3, 'a') else: tampvalx1=replaceStrIndex(value, 3, 'x') verbout(G, 'Tampered Token: '+color.CYAN+tampvalx1) req[query]=tampvalx1 resp=Post(url, action, req) if not str(resp.status_code).startswith('40') or not str(resp.status_code).startswith('50'): flagx1=0x01 verbout(GR, 'Tampering Token by index removal...') tampvalx2=replaceStrIndex(value, 3) verbout(G, 'Tampered Token: '+color.CYAN+tampvalx1) req[query]=tampvalx2 resp=Post(url, action, req) if not str(resp.status_code).startswith('40') or not str(resp.status_code).startswith('50'): flagx2=0x01 if flagx1==0x01 or flagx2==0x01: verbout(color.GREEN,'[+] The tampered token value works!') verbout(color.GREEN,'[-] The Tampered Anti-CSRF Token requested does NOT return a 40x or 50x response! ') print(color.ORANGE+'[-] Endpoint '+color.BR+' CONFIRMED VULNERABLE '+color.END+color.ORANGE+' to Request Forgery Attacks...') print(color.ORANGE+'[!] Vulnerability Type: '+color.BG+' Non-Unique Anti-CSRF Tokens in Requests '+color.END) else: print(color.RED+'[-] The Tampered Anti-CSRF Token requested returns a 40x or 50x response... ') print(color.ORANGE+'[-] Endpoint '+color.BG+' NOT VULNERABLE '+color.END+color.ORANGE+' to CSRF Attacks...') print(color.ORANGE+'[!] CSRF Mitigation Method: '+color.BG+' Unique Anti-CSRF Tokens '+color.END) def replaceStrIndex(text, index=0, replacement=''): ''' This method returns a tampered string by replacement ''' return '%s%s%s' %(text[:index], replacement, text[index+1:]) ","sourceWithComments":"#!\/usr\/bin\/env python3\n# -*- coding: utf-8 -*-\n\n#-:-:-:-:-:-:-:-:-:#\n#    XSRFProbe     #\n#-:-:-:-:-:-:-:-:-:#\n\n# Author: @_tID\n# This module requires XSRFProbe\n# https:\/\/github.com\/0xInfection\/XSRFProbe\n\nfrom re import search, I\nfrom core.colors import *\nfrom core.request import Post\nfrom files.config import *\nfrom core.verbout import verbout\nfrom urllib.parse import urlencode, quote\n\n# Null char flags (hex)\nflagx1 = 0x00\nflagx2 = 0x00\n\ndef Tamper(url, action, req, body, query, para):\n    '''\n    The main idea behind this is to tamper the Anti-CSRF tokens\n          found and check the content length for related\n                      vulnerabilities.\n    '''\n    verbout(GR, 'Proceeding for CSRF attack via Anti-CSRF token tampering...')\n    # First of all lets get out token from request\n    if para == '':\n        return True\n    # Coverting the token to a raw string, cause some special\n    # chars might fu*k with the Shannon Entropy operation.\n    value = r'%s' % para\n\n    # Alright lets start...\n    # [Step 1]: First we take the token and then replace a char\n    # at a specific position and test the response body.\n    #\n    # Required check for checking if string at that position isn't the\n    # same char we are going to replace with.\n    verbout(GR, 'Tampering Token by index replacement...')\n    if value[3] != 'a':\n        tampvalx1 = replaceStrIndex(value, 3, 'a')\n    else:\n        tampvalx1 = replaceStrIndex(value, 3, 'x')\n    verbout(G, 'Tampered Token: '+color.CYAN+tampvalx1)\n    # Lets build up the request...\n    req[query] = tampvalx1\n    resp = Post(url, action, req)\n\n    # If there is a 40x (Not Found) or a 50x (Internal Error) error,\n    # we assume that the tamper did not work :( But if there is a 20x\n    # (Accepted) or a 30x (Redirection), then we know it worked.\n    #\n    # NOTE: This algorithm has lots of room for improvment.\n    if not str(resp.status_code).startswith('40') or not str(resp.status_code).startswith('50'):\n        flagx1 = 0x01\n\n    # [Step 2]: Second we take the token and then remove a char\n    # at a specific position and test the response body.\n    #\n    # Required check for checking if string at that position isn't the\n    # same char we are going to replace with.\n    verbout(GR, 'Tampering Token by index removal...')\n    tampvalx2 = replaceStrIndex(value, 3)\n    verbout(G, 'Tampered Token: '+color.CYAN+tampvalx1)\n    # Lets build up the request...\n    req[query] = tampvalx2\n    resp = Post(url, action, req)\n\n    # If there is a 40x (Not Found) or a 50x (Internal Error) error,\n    # we assume that the tamper did not work :( But if there is a 20x\n    # (Accepted) or a 30x (Redirection), then we know it worked.\n    #\n    # NOTE: This algorithm has lots of room for improvment.\n    if not str(resp.status_code).startswith('40') or not str(resp.status_code).startswith('50'):\n        flagx2 = 0x01\n\n    # If any of the forgeries worked...\n    if flagx1 == 0x01 or flagx2 == 0x01:\n        verbout(color.GREEN,' [+] The tampered token value works!')\n        verbout(color.GREEN,' [-] The Tampered Anti-CSRF Token requested does NOT return a 40x or 50x response! ')\n        print(color.ORANGE+' [-] Endpoint '+color.BR+' CONFIRMED VULNERABLE '+color.END+color.ORANGE+' to Request Forgery Attacks...')\n        print(color.ORANGE+' [!] Vulnerability Type: '+color.BG+' Non-Unique Anti-CSRF Tokens in Requests '+color.END)\n    else:\n        print(color.RED+' [-] The Tampered Anti-CSRF Token requested returns a 40x or 50x response... ')\n        print(color.ORANGE+' [-] Endpoint '+color.BG+' NOT VULNERABLE '+color.END+color.ORANGE+' to CSRF Attacks...')\n        print(color.ORANGE+' [!] CSRF Mitigation Method: '+color.BG+' Unique Anti-CSRF Tokens '+color.END)\n\ndef replaceStrIndex(text, index=0, replacement=''):\n    ''' This method returns a tampered string by replacement '''\n    return '%s%s%s' % (text[:index], replacement, text[index+1:])\n\n"}},"msg":"Added 3rd check on Token Tampering"}},"https:\/\/github.com\/0xInfection\/XSRFProbe":{"fd3c868314b86a1ffdbc6633672782a2269e850d":{"url":"https:\/\/api.github.com\/repos\/0xInfection\/XSRFProbe\/commits\/fd3c868314b86a1ffdbc6633672782a2269e850d","html_url":"https:\/\/github.com\/0xInfection\/XSRFProbe\/commit\/fd3c868314b86a1ffdbc6633672782a2269e850d","sha":"fd3c868314b86a1ffdbc6633672782a2269e850d","keyword":"tampering check","diff":"diff --git a\/modules\/Tamper.py b\/modules\/Tamper.py\nindex 7281b96..de933d5 100644\n--- a\/modules\/Tamper.py\n+++ b\/modules\/Tamper.py\n@@ -19,6 +19,7 @@\n # Null char flags (hex)\n flagx1 = 0x00\n flagx2 = 0x00\n+flagx3 = 0x00\n \n def Tamper(url, action, req, body, query, para):\n     '''\n@@ -54,15 +55,14 @@ def Tamper(url, action, req, body, query, para):\n     # we assume that the tamper did not work :( But if there is a 20x\n     # (Accepted) or a 30x (Redirection), then we know it worked.\n     #\n-    # NOTE: This algorithm has lots of room for improvment.\n-    if not str(resp.status_code).startswith('40') or not str(resp.status_code).startswith('50'):\n+    # NOTE: This algorithm has lots of room for improvement.\n+    if str(resp.status_code).startswith('50'):\n+        verbout(color.RED,' [+] Token tamper from request causes a 50x Internal Error!')\n+    if not str(resp.status_code).startswith('40') and not str(resp.status_code).startswith('50'):\n         flagx1 = 0x01\n \n     # [Step 2]: Second we take the token and then remove a char\n     # at a specific position and test the response body.\n-    #\n-    # Required check for checking if string at that position isn't the\n-    # same char we are going to replace with.\n     verbout(GR, 'Tampering Token by index removal...')\n     tampvalx2 = replaceStrIndex(value, 3)\n     verbout(G, 'Tampered Token: '+color.CYAN+tampvalx1)\n@@ -74,12 +74,32 @@ def Tamper(url, action, req, body, query, para):\n     # we assume that the tamper did not work :( But if there is a 20x\n     # (Accepted) or a 30x (Redirection), then we know it worked.\n     #\n-    # NOTE: This algorithm has lots of room for improvment.\n-    if not str(resp.status_code).startswith('40') or not str(resp.status_code).startswith('50'):\n+    # NOTE: This algorithm has lots of room for improvement.\n+    if str(resp.status_code).startswith('50'):\n+        verbout(color.RED,' [+] Token tamper from request causes a 50x Internal Error!')\n+    if not str(resp.status_code).startswith('40') and not str(resp.status_code).startswith('50'):\n+        flagx2 = 0x01\n+\n+    # [Step 3]: Third we take the token and then remove the whole\n+    # anticsrf token and test the response body.\n+    verbout(GR, 'Tampering Token by Token removal...')\n+    del req[query]\n+    verbout(G, 'Removed token from request!')\n+    # Lets build up the request...\n+    resp = Post(url, action, req)\n+\n+    # If there is a 40x (Not Found) or a 50x (Internal Error) error,\n+    # we assume that the tamper did not work :( But if there is a 20x\n+    # (Accepted) or a 30x (Redirection), then we know it worked.\n+    #\n+    # NOTE: This algorithm has lots of room for improvement.\n+    if str(resp.status_code).startswith('50'):\n+        verbout(color.RED,' [+] Token removal from request causes a 50x Internal Error!')\n+    if not str(resp.status_code).startswith('40') and not str(resp.status_code).startswith('50'):\n         flagx2 = 0x01\n \n     # If any of the forgeries worked...\n-    if flagx1 == 0x01 or flagx2 == 0x01:\n+    if flagx1 == 0x01 or flagx2 == 0x01 or flagx3 == 0x01:\n         verbout(color.GREEN,' [+] The tampered token value works!')\n         verbout(color.GREEN,' [-] The Tampered Anti-CSRF Token requested does NOT return a 40x or 50x response! ')\n         print(color.ORANGE+' [-] Endpoint '+color.BR+' CONFIRMED VULNERABLE '+color.END+color.ORANGE+' to Request Forgery Attacks...')\n@@ -92,4 +112,3 @@ def Tamper(url, action, req, body, query, para):\n def replaceStrIndex(text, index=0, replacement=''):\n     ''' This method returns a tampered string by replacement '''\n     return '%s%s%s' % (text[:index], replacement, text[index+1:])\n-\n","message":"","files":{"\/modules\/Tamper.py":{"changes":[{"diff":"\n     # we assume that the tamper did not work :( But if there is a 20x\n     # (Accepted) or a 30x (Redirection), then we know it worked.\n     #\n-    # NOTE: This algorithm has lots of room for improvment.\n-    if not str(resp.status_code).startswith('40') or not str(resp.status_code).startswith('50'):\n+    # NOTE: This algorithm has lots of room for improvement.\n+    if str(resp.status_code).startswith('50'):\n+        verbout(color.RED,' [+] Token tamper from request causes a 50x Internal Error!')\n+    if not str(resp.status_code).startswith('40') and not str(resp.status_code).startswith('50'):\n         flagx1 = 0x01\n \n     # [Step 2]: Second we take the token and then remove a char\n     # at a specific position and test the response body.\n-    #\n-    # Required check for checking if string at that position isn't the\n-    # same char we are going to replace with.\n     verbout(GR, 'Tampering Token by index removal...')\n     tampvalx2 = replaceStrIndex(value, 3)\n     verbout(G, 'Tampered Token: '+color.CYAN+tampvalx1)\n","add":4,"remove":5,"filename":"\/modules\/Tamper.py","badparts":["    if not str(resp.status_code).startswith('40') or not str(resp.status_code).startswith('50'):"],"goodparts":["    if str(resp.status_code).startswith('50'):","        verbout(color.RED,' [+] Token tamper from request causes a 50x Internal Error!')","    if not str(resp.status_code).startswith('40') and not str(resp.status_code).startswith('50'):"]},{"diff":"\n     # we assume that the tamper did not work :( But if there is a 20x\n     # (Accepted) or a 30x (Redirection), then we know it worked.\n     #\n-    # NOTE: This algorithm has lots of room for improvment.\n-    if not str(resp.status_code).startswith('40') or not str(resp.status_code).startswith('50'):\n+    # NOTE: This algorithm has lots of room for improvement.\n+    if str(resp.status_code).startswith('50'):\n+        verbout(color.RED,' [+] Token tamper from request causes a 50x Internal Error!')\n+    if not str(resp.status_code).startswith('40') and not str(resp.status_code).startswith('50'):\n+        flagx2 = 0x01\n+\n+    # [Step 3]: Third we take the token and then remove the whole\n+    # anticsrf token and test the response body.\n+    verbout(GR, 'Tampering Token by Token removal...')\n+    del req[query]\n+    verbout(G, 'Removed token from request!')\n+    # Lets build up the request...\n+    resp = Post(url, action, req)\n+\n+    # If there is a 40x (Not Found) or a 50x (Internal Error) error,\n+    # we assume that the tamper did not work :( But if there is a 20x\n+    # (Accepted) or a 30x (Redirection), then we know it worked.\n+    #\n+    # NOTE: This algorithm has lots of room for improvement.\n+    if str(resp.status_code).startswith('50'):\n+        verbout(color.RED,' [+] Token removal from request causes a 50x Internal Error!')\n+    if not str(resp.status_code).startswith('40') and not str(resp.status_code).startswith('50'):\n         flagx2 = 0x01\n \n     # If any of the forgeries worked...\n-    if flagx1 == 0x01 or flagx2 == 0x01:\n+    if flagx1 == 0x01 or flagx2 == 0x01 or flagx3 == 0x01:\n         verbout(color.GREEN,' [+] The tampered token value works!')\n         verbout(color.GREEN,' [-] The Tampered Anti-CSRF Token requested does NOT return a 40x or 50x response! ')\n         print(color.ORANGE+' [-] Endpoint '+color.BR+' CONFIRMED VULNERABLE '+color.END+color.ORANGE+' to Request Forgery Attacks...')\n","add":23,"remove":3,"filename":"\/modules\/Tamper.py","badparts":["    if not str(resp.status_code).startswith('40') or not str(resp.status_code).startswith('50'):","    if flagx1 == 0x01 or flagx2 == 0x01:"],"goodparts":["    if str(resp.status_code).startswith('50'):","        verbout(color.RED,' [+] Token tamper from request causes a 50x Internal Error!')","    if not str(resp.status_code).startswith('40') and not str(resp.status_code).startswith('50'):","        flagx2 = 0x01","    verbout(GR, 'Tampering Token by Token removal...')","    del req[query]","    verbout(G, 'Removed token from request!')","    resp = Post(url, action, req)","    if str(resp.status_code).startswith('50'):","        verbout(color.RED,' [+] Token removal from request causes a 50x Internal Error!')","    if not str(resp.status_code).startswith('40') and not str(resp.status_code).startswith('50'):","    if flagx1 == 0x01 or flagx2 == 0x01 or flagx3 == 0x01:"]}],"source":"\n from re import search, I from core.colors import * from core.request import Post from files.config import * from core.verbout import verbout from urllib.parse import urlencode, quote flagx1=0x00 flagx2=0x00 def Tamper(url, action, req, body, query, para): ''' The main idea behind this is to tamper the Anti-CSRF tokens found and check the content length for related vulnerabilities. ''' verbout(GR, 'Proceeding for CSRF attack via Anti-CSRF token tampering...') if para=='': return True value=r'%s' % para verbout(GR, 'Tampering Token by index replacement...') if value[3] !='a': tampvalx1=replaceStrIndex(value, 3, 'a') else: tampvalx1=replaceStrIndex(value, 3, 'x') verbout(G, 'Tampered Token: '+color.CYAN+tampvalx1) req[query]=tampvalx1 resp=Post(url, action, req) if not str(resp.status_code).startswith('40') or not str(resp.status_code).startswith('50'): flagx1=0x01 verbout(GR, 'Tampering Token by index removal...') tampvalx2=replaceStrIndex(value, 3) verbout(G, 'Tampered Token: '+color.CYAN+tampvalx1) req[query]=tampvalx2 resp=Post(url, action, req) if not str(resp.status_code).startswith('40') or not str(resp.status_code).startswith('50'): flagx2=0x01 if flagx1==0x01 or flagx2==0x01: verbout(color.GREEN,'[+] The tampered token value works!') verbout(color.GREEN,'[-] The Tampered Anti-CSRF Token requested does NOT return a 40x or 50x response! ') print(color.ORANGE+'[-] Endpoint '+color.BR+' CONFIRMED VULNERABLE '+color.END+color.ORANGE+' to Request Forgery Attacks...') print(color.ORANGE+'[!] Vulnerability Type: '+color.BG+' Non-Unique Anti-CSRF Tokens in Requests '+color.END) else: print(color.RED+'[-] The Tampered Anti-CSRF Token requested returns a 40x or 50x response... ') print(color.ORANGE+'[-] Endpoint '+color.BG+' NOT VULNERABLE '+color.END+color.ORANGE+' to CSRF Attacks...') print(color.ORANGE+'[!] CSRF Mitigation Method: '+color.BG+' Unique Anti-CSRF Tokens '+color.END) def replaceStrIndex(text, index=0, replacement=''): ''' This method returns a tampered string by replacement ''' return '%s%s%s' %(text[:index], replacement, text[index+1:]) ","sourceWithComments":"#!\/usr\/bin\/env python3\n# -*- coding: utf-8 -*-\n\n#-:-:-:-:-:-:-:-:-:#\n#    XSRFProbe     #\n#-:-:-:-:-:-:-:-:-:#\n\n# Author: @_tID\n# This module requires XSRFProbe\n# https:\/\/github.com\/0xInfection\/XSRFProbe\n\nfrom re import search, I\nfrom core.colors import *\nfrom core.request import Post\nfrom files.config import *\nfrom core.verbout import verbout\nfrom urllib.parse import urlencode, quote\n\n# Null char flags (hex)\nflagx1 = 0x00\nflagx2 = 0x00\n\ndef Tamper(url, action, req, body, query, para):\n    '''\n    The main idea behind this is to tamper the Anti-CSRF tokens\n          found and check the content length for related\n                      vulnerabilities.\n    '''\n    verbout(GR, 'Proceeding for CSRF attack via Anti-CSRF token tampering...')\n    # First of all lets get out token from request\n    if para == '':\n        return True\n    # Coverting the token to a raw string, cause some special\n    # chars might fu*k with the Shannon Entropy operation.\n    value = r'%s' % para\n\n    # Alright lets start...\n    # [Step 1]: First we take the token and then replace a char\n    # at a specific position and test the response body.\n    #\n    # Required check for checking if string at that position isn't the\n    # same char we are going to replace with.\n    verbout(GR, 'Tampering Token by index replacement...')\n    if value[3] != 'a':\n        tampvalx1 = replaceStrIndex(value, 3, 'a')\n    else:\n        tampvalx1 = replaceStrIndex(value, 3, 'x')\n    verbout(G, 'Tampered Token: '+color.CYAN+tampvalx1)\n    # Lets build up the request...\n    req[query] = tampvalx1\n    resp = Post(url, action, req)\n\n    # If there is a 40x (Not Found) or a 50x (Internal Error) error,\n    # we assume that the tamper did not work :( But if there is a 20x\n    # (Accepted) or a 30x (Redirection), then we know it worked.\n    #\n    # NOTE: This algorithm has lots of room for improvment.\n    if not str(resp.status_code).startswith('40') or not str(resp.status_code).startswith('50'):\n        flagx1 = 0x01\n\n    # [Step 2]: Second we take the token and then remove a char\n    # at a specific position and test the response body.\n    #\n    # Required check for checking if string at that position isn't the\n    # same char we are going to replace with.\n    verbout(GR, 'Tampering Token by index removal...')\n    tampvalx2 = replaceStrIndex(value, 3)\n    verbout(G, 'Tampered Token: '+color.CYAN+tampvalx1)\n    # Lets build up the request...\n    req[query] = tampvalx2\n    resp = Post(url, action, req)\n\n    # If there is a 40x (Not Found) or a 50x (Internal Error) error,\n    # we assume that the tamper did not work :( But if there is a 20x\n    # (Accepted) or a 30x (Redirection), then we know it worked.\n    #\n    # NOTE: This algorithm has lots of room for improvment.\n    if not str(resp.status_code).startswith('40') or not str(resp.status_code).startswith('50'):\n        flagx2 = 0x01\n\n    # If any of the forgeries worked...\n    if flagx1 == 0x01 or flagx2 == 0x01:\n        verbout(color.GREEN,' [+] The tampered token value works!')\n        verbout(color.GREEN,' [-] The Tampered Anti-CSRF Token requested does NOT return a 40x or 50x response! ')\n        print(color.ORANGE+' [-] Endpoint '+color.BR+' CONFIRMED VULNERABLE '+color.END+color.ORANGE+' to Request Forgery Attacks...')\n        print(color.ORANGE+' [!] Vulnerability Type: '+color.BG+' Non-Unique Anti-CSRF Tokens in Requests '+color.END)\n    else:\n        print(color.RED+' [-] The Tampered Anti-CSRF Token requested returns a 40x or 50x response... ')\n        print(color.ORANGE+' [-] Endpoint '+color.BG+' NOT VULNERABLE '+color.END+color.ORANGE+' to CSRF Attacks...')\n        print(color.ORANGE+' [!] CSRF Mitigation Method: '+color.BG+' Unique Anti-CSRF Tokens '+color.END)\n\ndef replaceStrIndex(text, index=0, replacement=''):\n    ''' This method returns a tampered string by replacement '''\n    return '%s%s%s' % (text[:index], replacement, text[index+1:])\n\n"}},"msg":"Added 3rd check on Token Tampering"}},"https:\/\/github.com\/gaelH9\/odoo_test":{"d4c865199e4a01d3fd606d52e152f1c9fe3eb690":{"url":"https:\/\/api.github.com\/repos\/gaelH9\/odoo_test\/commits\/d4c865199e4a01d3fd606d52e152f1c9fe3eb690","html_url":"https:\/\/github.com\/gaelH9\/odoo_test\/commit\/d4c865199e4a01d3fd606d52e152f1c9fe3eb690","sha":"d4c865199e4a01d3fd606d52e152f1c9fe3eb690","keyword":"tampering check","diff":"diff --git a\/addons\/mail\/models\/mail_thread.py b\/addons\/mail\/models\/mail_thread.py\nindex 6ff66d3191dd..7b6e412e1945 100644\n--- a\/addons\/mail\/models\/mail_thread.py\n+++ b\/addons\/mail\/models\/mail_thread.py\n@@ -2996,7 +2996,7 @@ def _notify_get_action_link(self, link_type, **kwargs):\n             token = self._notify_encode_link(base_link, params)\n             params['token'] = token\n \n-        link = '%s?%s' % (base_link, urls.url_encode(params))\n+        link = '%s?%s' % (base_link, urls.url_encode(params, sort=True))\n         if self:\n             link = self[0].get_base_url() + link\n \ndiff --git a\/addons\/test_mail_full\/__init__.py b\/addons\/test_mail_full\/__init__.py\nindex 567548c78f5a..dfccc6b661b1 100644\n--- a\/addons\/test_mail_full\/__init__.py\n+++ b\/addons\/test_mail_full\/__init__.py\n@@ -1,4 +1,5 @@\n # -*- coding: utf-8 -*-\n \n+from . import controllers\n from . import models\n from . import tests\ndiff --git a\/addons\/test_mail_full\/controllers\/__init__.py b\/addons\/test_mail_full\/controllers\/__init__.py\nnew file mode 100644\nindex 000000000000..903b755e71e3\n--- \/dev\/null\n+++ b\/addons\/test_mail_full\/controllers\/__init__.py\n@@ -0,0 +1,4 @@\n+# -*- coding: utf-8 -*-\n+# Part of Odoo. See LICENSE file for full copyright and licensing details.\n+\n+from . import portal\ndiff --git a\/addons\/test_mail_full\/controllers\/portal.py b\/addons\/test_mail_full\/controllers\/portal.py\nnew file mode 100644\nindex 000000000000..336e9d62e960\n--- \/dev\/null\n+++ b\/addons\/test_mail_full\/controllers\/portal.py\n@@ -0,0 +1,14 @@\n+# -*- coding: utf-8 -*-\n+# Part of Odoo. See LICENSE file for full copyright and licensing details.\n+\n+\n+from odoo import http\n+from odoo.http import request\n+\n+\n+class PortalTest(http.Controller):\n+    \"\"\"Implements some test portal routes (ex.: for viewing a record).\"\"\"\n+\n+    @http.route('\/my\/test_portal\/<int:res_id>', type='http', auth='public', methods=['GET'])\n+    def test_portal_record_view(self, res_id, access_token=None, **kwargs):\n+        return request.make_response(f'Record view of test_portal {res_id} ({access_token}, {kwargs})')\ndiff --git a\/addons\/test_mail_full\/models\/test_mail_models_mail.py b\/addons\/test_mail_full\/models\/test_mail_models_mail.py\nindex 926b6808af27..a00bdb046a33 100644\n--- a\/addons\/test_mail_full\/models\/test_mail_models_mail.py\n+++ b\/addons\/test_mail_full\/models\/test_mail_models_mail.py\n@@ -5,20 +5,21 @@\n \n \n class MailTestPortal(models.Model):\n-    \"\"\" A model intheriting from mail.thread with some fields used for portal\n-    sharing, like a partner, ...\"\"\"\n+    \"\"\" A model inheriting from mail.thread and portal.mixin with some fields\n+    used for portal sharing, like a partner, ...\"\"\"\n     _description = 'Chatter Model for Portal'\n     _name = 'mail.test.portal'\n     _inherit = [\n-        'mail.thread',\n         'portal.mixin',\n+        'mail.thread',\n     ]\n \n     name = fields.Char()\n     partner_id = fields.Many2one('res.partner', 'Customer')\n+    user_id = fields.Many2one(comodel_name='res.users', string=\"Salesperson\")\n \n     def _compute_access_url(self):\n-        self.access_url = False\n+        super()._compute_access_url()\n         for record in self.filtered('id'):\n             record.access_url = '\/my\/test_portal\/%s' % self.id\n \ndiff --git a\/addons\/test_mail_full\/security\/ir.model.access.csv b\/addons\/test_mail_full\/security\/ir.model.access.csv\nindex 4fce169a33d4..b5fe967c07a3 100644\n--- a\/addons\/test_mail_full\/security\/ir.model.access.csv\n+++ b\/addons\/test_mail_full\/security\/ir.model.access.csv\n@@ -1,5 +1,5 @@\n id,name,model_id:id,group_id:id,perm_read,perm_write,perm_create,perm_unlink\n-access_mail_test_portal_all,mail.test.portal.all,model_mail_test_portal,,1,0,0,0\n+access_mail_test_portal_all,mail.test.portal.all,model_mail_test_portal,,0,0,0,0\n access_mail_test_portal_user,mail.test.portal.user,model_mail_test_portal,base.group_user,1,1,1,1\n access_mail_test_rating_all,mail.test.rating.all,model_mail_test_rating,,0,0,0,0\n access_mail_test_rating_portal,mail.test.rating.portal,model_mail_test_rating,base.group_portal,1,0,0,0\ndiff --git a\/addons\/test_mail_full\/tests\/test_portal.py b\/addons\/test_mail_full\/tests\/test_portal.py\nindex 1a83d25d3438..44a086a9901d 100644\n--- a\/addons\/test_mail_full\/tests\/test_portal.py\n+++ b\/addons\/test_mail_full\/tests\/test_portal.py\n@@ -1,15 +1,17 @@\n # -*- coding: utf-8 -*-\n # Part of Odoo. See LICENSE file for full copyright and licensing details.\n \n-from werkzeug.urls import url_parse, url_decode\n+from werkzeug.urls import url_parse, url_decode, url_encode, url_unparse\n \n import json\n \n from odoo import http\n+from odoo.addons.mail.tests.common import MailCommon\n from odoo.addons.test_mail_full.tests.common import TestMailFullCommon\n from odoo.addons.test_mail_sms.tests.common import TestSMSRecipients\n from odoo.tests import tagged, users\n from odoo.tests.common import HttpCase\n+from odoo.tools import html_escape\n \n \n @tagged('portal')\n@@ -145,6 +147,191 @@ def test_portal_share_comment(self):\n         self.assertEqual(message.author_id, self.partner_2)\n \n \n+@tagged('portal')\n+class TestPortalFlow(MailCommon, HttpCase):\n+    \"\"\"Share a link by email to a customer without an account for viewing a record through the portal.\n+\n+    The tests consist in sending a mail related to a record to a customer and checking that the record can be viewed\n+    through the embedded link:\n+    - either in the backend if the user is connected and has the right to\n+    - or in the portal otherwise\n+    \"\"\"\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n+        cls.customer = cls.env['res.partner'].create({\n+            'country_id': cls.env.ref('base.fr').id,\n+            'email': 'mdelvaux34@example.com',\n+            'lang': 'en_US',\n+            'mobile': '+33639982325',\n+            'name': 'Mathias Delvaux',\n+            'phone': '+33353011823',\n+        })\n+        cls.record_portal = cls.env['mail.test.portal'].create({\n+            'name': 'Test Portal Record',\n+            'partner_id': cls.customer.id,\n+            'user_id': cls.user_admin.id,\n+        })\n+        cls.mail_template = cls.env['mail.template'].create({\n+            'auto_delete': True,\n+            'body_html': '<p>Hello <t t-out=\"object.partner_id.name\"\/>, your quotation is ready for review.<\/p>',\n+            'email_from': '{{ (object.user_id.email_formatted or user.email_formatted) }}',\n+            'model_id': cls.env.ref('test_mail_full.model_mail_test_portal').id,\n+            'name': 'Quotation template',\n+            'partner_to': '{{ object.partner_id.id }}',\n+            'subject': 'Your quotation \"{{ object.name }}\"',\n+        })\n+        cls._create_portal_user()\n+        for group_name, group_func, group_data in cls.record_portal.sudo()._notify_get_recipients_groups():\n+            if group_name == 'portal_customer' and group_func(cls.customer):\n+                cls.record_access_url = group_data['button_access']['url']\n+                break\n+        else:\n+            raise AssertionError('Record access URL not found')\n+        # Build record_access_url_wrong_token with altered access_token for testing security\n+        parsed_url = url_parse(cls.record_access_url)\n+        query_params = url_decode(parsed_url.query)\n+        cls.record_access_url_wrong_token = url_unparse(\n+            (parsed_url[0], parsed_url[1], parsed_url[2],\n+             url_encode({**query_params,\n+                         'access_token': query_params['access_token'].translate(\n+                             str.maketrans('0123456789abcdef',\n+                                           '9876543210fedcba'))},\n+                        sort=True),\n+             parsed_url[4]))\n+\n+    def assert_URL(self, url, expected_path, expected_fragment_params=None, expected_query=None):\n+        \"\"\"Asserts that the URL has the expected path and if set, the expected fragment parameters and query.\"\"\"\n+        parsed_url = url_parse(url)\n+        fragment_params = url_decode(parsed_url.fragment)\n+        self.assertEqual(parsed_url.path, expected_path)\n+        if expected_fragment_params:\n+            for key, expected_value in expected_fragment_params.items():\n+                self.assertEqual(fragment_params.get(key), expected_value,\n+                                 f'Expected: \"{key}={expected_value}\" (for path: {expected_path})')\n+        if expected_query:\n+            self.assertEqual(expected_query, parsed_url.query,\n+                             f'Expected: query=\"{expected_query}\" (for path: {expected_path})')\n+\n+    def _get_composer_with_context(self, template_id=False):\n+        return self.env['mail.compose.message'].with_context({\n+            'default_composition_mode': 'comment',\n+            'default_email_layout_xmlid': 'mail.mail_notification_layout_with_responsible_signature',\n+            'default_model': self.record_portal._name,\n+            'default_res_id': self.record_portal.id,\n+            'default_template_id': template_id,\n+            'default_use_template': template_id,\n+            'force_email': True,\n+            'lang': '{{ object.partner_id.lang }}',\n+        })\n+\n+    def test_initial_data(self):\n+        \"\"\"Test some initial values.\n+\n+        Test that record_access_url is a valid URL to view the record_portal and that record_access_url_wrong_token\n+        only differs from record_access_url by a different access_token.\n+        \"\"\"\n+        parsed_record_access_url = url_parse(self.record_access_url)\n+        record_access_query_params = url_decode(parsed_record_access_url.query)\n+        parsed_record_access_url_wrong_token = url_parse(self.record_access_url_wrong_token)\n+        record_access_wrong_token_query_params = url_decode(parsed_record_access_url_wrong_token.query)\n+\n+        self.assertEqual(parsed_record_access_url.path, '\/mail\/view')\n+        # Note that pid, hash and auth_signup_token are not tested by this test but may be present in the URL (config).\n+        self.assertEqual(record_access_query_params.get('model'), 'mail.test.portal')\n+        self.assertEqual(int(record_access_query_params.get('res_id')), self.record_portal.id)\n+        self.assertTrue(record_access_query_params.get('access_token'))\n+\n+        self.assertNotEqual(self.record_access_url, self.record_access_url_wrong_token)\n+        self.assertEqual(parsed_record_access_url_wrong_token.path, '\/mail\/view')\n+        self.assertTrue(record_access_wrong_token_query_params['access_token'])\n+        self.assertNotEqual(record_access_query_params['access_token'],\n+                            record_access_wrong_token_query_params['access_token'])\n+        self.assertEqual({k: v for k, v in record_access_query_params.items() if k != 'access_token'},\n+                         {k: v for k, v in record_access_wrong_token_query_params.items() if k != 'access_token'})\n+\n+    @users('portal_test')\n+    def test_customer_access_logged_without_access(self):\n+        \"\"\"Check that the link redirects the customer (without backend access) to the portal for viewing the record.\"\"\"\n+        self.authenticate(self.env.user.login, self.env.user.login)\n+        res = self.url_open(self.record_access_url)\n+        self.assertEqual(res.status_code, 200)\n+        self.assert_URL(res.url, f'\/my\/test_portal\/{self.record_portal.id}')\n+\n+    @users('portal_test')\n+    def test_customer_access_logged_without_access_wrong_token(self):\n+        \"\"\"Check that it redirects to discuss when logged customer has no access to the record and token is invalid.\"\"\"\n+        self.authenticate(self.env.user.login, self.env.user.login)\n+        res = self.url_open(self.record_access_url_wrong_token)\n+        self.assertEqual(res.status_code, 200)\n+        self.assert_URL(res.url, '\/my', {'action': 'mail.action_discuss'})\n+\n+    def test_customer_access_not_logged(self):\n+        \"\"\"Check that the access link redirects the customer (not logged) to the portal for viewing the record.\"\"\"\n+        res = self.url_open(self.record_access_url)\n+        self.assertEqual(res.status_code, 200)\n+        self.assertIn(f'\/my\/test_portal\/{self.record_portal.id}', res.url)\n+        self.assert_URL(res.url, f'\/my\/test_portal\/{self.record_portal.id}')\n+\n+    def test_customer_access_not_logged_wrong_token(self):\n+        \"\"\"Check that the access link redirect the customer to login when the token is invalid.\"\"\"\n+        res = self.url_open(self.record_access_url_wrong_token)\n+        self.assertEqual(res.status_code, 200)\n+        self.assert_URL(res.url, '\/web\/login', {'model': 'mail.test.portal', 'id': str(self.record_portal.id)},\n+                        expected_query='redirect=')\n+\n+    @users('employee')\n+    def test_employee_access(self):\n+        \"\"\"Check that the access link redirects an employee to the backend for viewing the record.\"\"\"\n+        self.authenticate(self.env.user.login, self.env.user.login)\n+        res = self.url_open(self.record_access_url)\n+        self.assertEqual(res.status_code, 200)\n+        self.assert_URL(res.url, '\/web', {'model': 'mail.test.portal', 'id': str(self.record_portal.id)})\n+\n+    @users('employee')\n+    def test_employee_access_wrong_token(self):\n+        \"\"\"Check that the access link redirects an employee to the record even if the token invalid.\"\"\"\n+        self.authenticate(self.env.user.login, self.env.user.login)\n+        res = self.url_open(self.record_access_url_wrong_token)\n+        self.assertEqual(res.status_code, 200)\n+        self.assert_URL(res.url, '\/web', {'model': 'mail.test.portal', 'id': str(self.record_portal.id)})\n+\n+    @users('employee')\n+    def test_send_message_to_customer(self):\n+        \"\"\"Same as test_send_message_to_customer_using_template but without a template.\"\"\"\n+        composer = self._get_composer_with_context().create({\n+            'body': '<p>Hello Mathias Delvaux, your quotation is ready for review.<\/p>',\n+            'partner_ids': self.customer.ids,\n+            'subject': 'Your Quotation \"a white table\"',\n+        })\n+\n+        with self.mock_mail_gateway(mail_unlink_sent=True):\n+            composer._action_send_mail()\n+\n+        self.assertEqual(len(self._mails), 1)\n+        self.assertIn(f'\"{html_escape(self.record_access_url)}\"', self._mails[0].get('body'))\n+        # Check that the template is not used (not the same subject)\n+        self.assertEqual('Your Quotation \"a white table\"', self._mails[0].get('subject'))\n+        self.assertIn('Hello Mathias Delvaux', self._mails[0].get('body'))\n+\n+    @users('employee')\n+    def test_send_message_to_customer_using_template(self):\n+        \"\"\"Send a mail to a customer without an account and check that it contains a link to view the record.\n+\n+        Other tests below check that that same link has the correct behavior.\n+        This test follows the common use case by using a template while the next send the mail without a template.\"\"\"\n+        composer = self._get_composer_with_context(self.mail_template.id).create({})\n+        # currently onchange necessary\n+        composer._onchange_template_id_wrapper()\n+\n+        with self.mock_mail_gateway(mail_unlink_sent=True):\n+            composer._action_send_mail()\n+\n+        self.assertEqual(len(self._mails), 1)\n+        self.assertIn(f'\"{html_escape(self.record_access_url)}\"', self._mails[0].get('body'))\n+        self.assertEqual(f'Your quotation \"{self.record_portal.name}\"', self._mails[0].get('subject'))  # Check that the template is used\n+\n+\n @tagged('portal')\n class TestPortalMixin(TestPortal):\n \n","message":"","files":{"\/addons\/mail\/models\/mail_thread.py":{"changes":[{"diff":"\n             token = self._notify_encode_link(base_link, params)\n             params['token'] = token\n \n-        link = '%s?%s' % (base_link, urls.url_encode(params))\n+        link = '%s?%s' % (base_link, urls.url_encode(params, sort=True))\n         if self:\n             link = self[0].get_base_url() + link\n ","add":1,"remove":1,"filename":"\/addons\/mail\/models\/mail_thread.py","badparts":["        link = '%s?%s' % (base_link, urls.url_encode(params))"],"goodparts":["        link = '%s?%s' % (base_link, urls.url_encode(params, sort=True))"]}]},"\/addons\/test_mail_full\/models\/test_mail_models_mail.py":{"changes":[{"diff":"\n \n \n class MailTestPortal(models.Model):\n-    \"\"\" A model intheriting from mail.thread with some fields used for portal\n-    sharing, like a partner, ...\"\"\"\n+    \"\"\" A model inheriting from mail.thread and portal.mixin with some fields\n+    used for portal sharing, like a partner, ...\"\"\"\n     _description = 'Chatter Model for Portal'\n     _name = 'mail.test.portal'\n     _inherit = [\n-        'mail.thread',\n         'portal.mixin',\n+        'mail.thread',\n     ]\n \n     name = fields.Char()\n     partner_id = fields.Many2one('res.partner', 'Customer')\n+    user_id = fields.Many2one(comodel_name='res.users', string=\"Salesperson\")\n \n     def _compute_access_url(self):\n-        self.access_url = False\n+        super()._compute_access_url()\n         for record in self.filtered('id'):\n             record.access_url = '\/my\/test_portal\/%s' % self.","add":5,"remove":4,"filename":"\/addons\/test_mail_full\/models\/test_mail_models_mail.py","badparts":["    \"\"\" A model intheriting from mail.thread with some fields used for portal","    sharing, like a partner, ...\"\"\"","        'mail.thread',","        self.access_url = False"],"goodparts":["    \"\"\" A model inheriting from mail.thread and portal.mixin with some fields","    used for portal sharing, like a partner, ...\"\"\"","        'mail.thread',","    user_id = fields.Many2one(comodel_name='res.users', string=\"Salesperson\")","        super()._compute_access_url()"]}],"source":"\n from odoo import api, fields, models class MailTestPortal(models.Model): \"\"\" A model intheriting from mail.thread with some fields used for portal sharing, like a partner,...\"\"\" _description='Chatter Model for Portal' _name='mail.test.portal' _inherit=[ 'mail.thread', 'portal.mixin', ] name=fields.Char() partner_id=fields.Many2one('res.partner', 'Customer') def _compute_access_url(self): self.access_url=False for record in self.filtered('id'): record.access_url='\/my\/test_portal\/%s' % self.id class MailTestRating(models.Model): \"\"\" A model inheriting from mail.thread with some fields used for SMS gateway, like a partner, a specific mobile phone,... \"\"\" _description='Rating Model(ticket-like)' _name='mail.test.rating' _inherit=[ 'mail.thread', 'mail.activity.mixin', 'rating.mixin', 'portal.mixin', ] _mailing_enabled=True _order='name asc, id asc' name=fields.Char() subject=fields.Char() company_id=fields.Many2one('res.company', 'Company') customer_id=fields.Many2one('res.partner', 'Customer') email_from=fields.Char(compute='_compute_email_from', precompute=True, readonly=False, store=True) mobile_nbr=fields.Char(compute='_compute_mobile_nbr', precompute=True, readonly=False, store=True) phone_nbr=fields.Char(compute='_compute_phone_nbr', precompute=True, readonly=False, store=True) user_id=fields.Many2one('res.users', 'Responsible', tracking=1) @api.depends('customer_id') def _compute_email_from(self): for rating in self: if rating.customer_id.email_normalized: rating.email_from=rating.customer_id.email_normalized elif not rating.email_from: rating.email_from=False @api.depends('customer_id') def _compute_mobile_nbr(self): for rating in self: if rating.customer_id.mobile: rating.mobile_nbr=rating.customer_id.mobile elif not rating.mobile_nbr: rating.mobile_nbr=False @api.depends('customer_id') def _compute_phone_nbr(self): for rating in self: if rating.customer_id.phone: rating.phone_nbr=rating.customer_id.phone elif not rating.phone_nbr: rating.phone_nbr=False def _rating_apply_get_default_subtype_id(self): return self.env['ir.model.data']._xmlid_to_res_id(\"test_mail_full.mt_mail_test_rating_rating_done\") def _rating_get_partner(self): return self.customer_id ","sourceWithComments":"# -*- coding: utf-8 -*-\n# Part of Odoo. See LICENSE file for full copyright and licensing details.\n\nfrom odoo import api, fields, models\n\n\nclass MailTestPortal(models.Model):\n    \"\"\" A model intheriting from mail.thread with some fields used for portal\n    sharing, like a partner, ...\"\"\"\n    _description = 'Chatter Model for Portal'\n    _name = 'mail.test.portal'\n    _inherit = [\n        'mail.thread',\n        'portal.mixin',\n    ]\n\n    name = fields.Char()\n    partner_id = fields.Many2one('res.partner', 'Customer')\n\n    def _compute_access_url(self):\n        self.access_url = False\n        for record in self.filtered('id'):\n            record.access_url = '\/my\/test_portal\/%s' % self.id\n\n\nclass MailTestRating(models.Model):\n    \"\"\" A model inheriting from mail.thread with some fields used for SMS\n    gateway, like a partner, a specific mobile phone, ... \"\"\"\n    _description = 'Rating Model (ticket-like)'\n    _name = 'mail.test.rating'\n    _inherit = [\n        'mail.thread',\n        'mail.activity.mixin',\n        'rating.mixin',\n        'portal.mixin',\n    ]\n    _mailing_enabled = True\n    _order = 'name asc, id asc'\n\n    name = fields.Char()\n    subject = fields.Char()\n    company_id = fields.Many2one('res.company', 'Company')\n    customer_id = fields.Many2one('res.partner', 'Customer')\n    email_from = fields.Char(compute='_compute_email_from', precompute=True, readonly=False, store=True)\n    mobile_nbr = fields.Char(compute='_compute_mobile_nbr', precompute=True, readonly=False, store=True)\n    phone_nbr = fields.Char(compute='_compute_phone_nbr', precompute=True, readonly=False, store=True)\n    user_id = fields.Many2one('res.users', 'Responsible', tracking=1)\n\n    @api.depends('customer_id')\n    def _compute_email_from(self):\n        for rating in self:\n            if rating.customer_id.email_normalized:\n                rating.email_from = rating.customer_id.email_normalized\n            elif not rating.email_from:\n                rating.email_from = False\n\n    @api.depends('customer_id')\n    def _compute_mobile_nbr(self):\n        for rating in self:\n            if rating.customer_id.mobile:\n                rating.mobile_nbr = rating.customer_id.mobile\n            elif not rating.mobile_nbr:\n                rating.mobile_nbr = False\n\n    @api.depends('customer_id')\n    def _compute_phone_nbr(self):\n        for rating in self:\n            if rating.customer_id.phone:\n                rating.phone_nbr = rating.customer_id.phone\n            elif not rating.phone_nbr:\n                rating.phone_nbr = False\n\n    def _rating_apply_get_default_subtype_id(self):\n        return self.env['ir.model.data']._xmlid_to_res_id(\"test_mail_full.mt_mail_test_rating_rating_done\")\n\n    def _rating_get_partner(self):\n        return self.customer_id\n"}},"msg":"[IMP] mail, test_mail_full: add portal flow test\n\nThe tests consist in sending a mail related to a record to a customer and\nchecking that the record can be viewed through the embedded link:\n- either in the backend if the user is connected and has the right to\n- or in the portal otherwise\n\nIt also test that if the embedded link is tampered, the access is forbidden.\n\nTask-2797311\n\ncloses odoo\/odoo#105405\n\nSigned-off-by: Thibault Delavallee (tde) <tde@openerp.com>"}},"https:\/\/github.com\/shipmunkdev\/blockpy":{"df0c151375fae4a22cddd75172f7f54fbe63f46a":{"url":"https:\/\/api.github.com\/repos\/shipmunkdev\/blockpy\/commits\/df0c151375fae4a22cddd75172f7f54fbe63f46a","html_url":"https:\/\/github.com\/shipmunkdev\/blockpy\/commit\/df0c151375fae4a22cddd75172f7f54fbe63f46a","message":"Add Pytest to validate the is_valid function is working to prevent Block tampering","sha":"df0c151375fae4a22cddd75172f7f54fbe63f46a","keyword":"tampering prevent","diff":"diff --git a\/backend\/tests\/blockchain\/test_block.py b\/backend\/tests\/blockchain\/test_block.py\nindex 9147471..28f7466 100644\n--- a\/backend\/tests\/blockchain\/test_block.py\n+++ b\/backend\/tests\/blockchain\/test_block.py\n@@ -1,3 +1,4 @@\n+import pytest\n import time\n \n from backend.blockchain.block import Block, GENESIS_DATA\n@@ -43,4 +44,42 @@ def test_mined_block_difficulty_limits_at_1():\n \n     mined_block = Block.mine_block(last_block, 'bar')\n \n-    assert mined_block.difficulty == 1\n\\ No newline at end of file\n+    assert mined_block.difficulty == 1\n+\n+@pytest.fixture\n+def last_block():\n+    return Block.genesis()\n+\n+@pytest.fixture\n+def block(last_block):\n+    return Block.mine_block(last_block, 'test_data')\n+\n+def test_is_valid_block(last_block, block):\n+    Block.is_valid_block(last_block, block)\n+\n+\n+def test_is_valid_block_bad_last_hash(last_block, block):\n+    block.last_hash = 'evil_data'\n+\n+    with pytest.raises(Exception, match='last_hash must be correct'):\n+        Block.is_valid_block(last_block, block)\n+\n+def test_is_valid_block_bad_proof_of_work(last_block, block):\n+    block.hash = 'fff'\n+\n+    with pytest.raises(Exception, match='proof of work requirement was not met'):\n+        Block.is_valid_block(last_block, block)\n+\n+def test_is_valid_block_jumped_difficulty(last_block, block):\n+    jumped_difficulty = 10\n+    block.difficulty = jumped_difficulty\n+    block.hash = f'{\"0\" * jumped_difficulty}111abc'\n+\n+    with pytest.raises(Exception, match='difficulty must only adjust by 1'):\n+        Block.is_valid_block(last_block, block)\n+\n+def test_is_valid_block_bad_block_hash(last_block, block):\n+    block.hash = '0000000000000000abcabc'\n+\n+    with pytest.raises(Exception, match='block hash must be correct'):\n+        Block.is_valid_block(last_block, block)\n","files":{"\/backend\/tests\/blockchain\/test_block.py":{"changes":[{"diff":"\n \n     mined_block = Block.mine_block(last_block, 'bar')\n \n-    assert mined_block.difficulty == 1\n\\ No newline at end of file\n+    assert mined_block.difficulty == 1\n+\n+@pytest.fixture\n+def last_block():\n+    return Block.genesis()\n+\n+@pytest.fixture\n+def block(last_block):\n+    return Block.mine_block(last_block, 'test_data')\n+\n+def test_is_valid_block(last_block, block):\n+    Block.is_valid_block(last_block, block)\n+\n+\n+def test_is_valid_block_bad_last_hash(last_block, block):\n+    block.last_hash = 'evil_data'\n+\n+    with pytest.raises(Exception, match='last_hash must be correct'):\n+        Block.is_valid_block(last_block, block)\n+\n+def test_is_valid_block_bad_proof_of_work(last_block, block):\n+    block.hash = 'fff'\n+\n+    with pytest.raises(Exception, match='proof of work requirement was not met'):\n+        Block.is_valid_block(last_block, block)\n+\n+def test_is_valid_block_jumped_difficulty(last_block, block):\n+    jumped_difficulty = 10\n+    block.difficulty = jumped_difficulty\n+    block.hash = f'{\"0\" * jumped_difficulty}111abc'\n+\n+    with pytest.raises(Exception, match='difficulty must only adjust by 1'):\n+        Block.is_valid_block(last_block, block)\n+\n+def test_is_valid_block_bad_block_hash(last_block, block):\n+    block.hash = '0000000000000000abcabc'\n+\n+    with pytest.raises(Exception, match='block hash must be correct'):\n+        Block.is_valid_block(last_block, block)\n","add":39,"remove":1,"filename":"\/backend\/tests\/blockchain\/test_block.py","badparts":["    assert mined_block.difficulty == 1"],"goodparts":["    assert mined_block.difficulty == 1","@pytest.fixture","def last_block():","    return Block.genesis()","@pytest.fixture","def block(last_block):","    return Block.mine_block(last_block, 'test_data')","def test_is_valid_block(last_block, block):","    Block.is_valid_block(last_block, block)","def test_is_valid_block_bad_last_hash(last_block, block):","    block.last_hash = 'evil_data'","    with pytest.raises(Exception, match='last_hash must be correct'):","        Block.is_valid_block(last_block, block)","def test_is_valid_block_bad_proof_of_work(last_block, block):","    block.hash = 'fff'","    with pytest.raises(Exception, match='proof of work requirement was not met'):","        Block.is_valid_block(last_block, block)","def test_is_valid_block_jumped_difficulty(last_block, block):","    jumped_difficulty = 10","    block.difficulty = jumped_difficulty","    block.hash = f'{\"0\" * jumped_difficulty}111abc'","    with pytest.raises(Exception, match='difficulty must only adjust by 1'):","        Block.is_valid_block(last_block, block)","def test_is_valid_block_bad_block_hash(last_block, block):","    block.hash = '0000000000000000abcabc'","    with pytest.raises(Exception, match='block hash must be correct'):","        Block.is_valid_block(last_block, block)"]}],"source":"\nimport time from backend.blockchain.block import Block, GENESIS_DATA from backend.util.hex_to_binary import hex_to_binary from backend.config import MINE_RATE, SECONDS def test_mine_block(): last_block=Block.genesis() data='test_data' block=Block.mine_block(last_block, data) assert isinstance(block, Block) assert block.data==data assert block.last_hash==last_block.hash assert hex_to_binary(block.hash)[0:block.difficulty]=='0' * block.difficulty def test_genesis(): genesis=Block.genesis() assert isinstance(genesis, Block) for key, value in GENESIS_DATA.items(): getattr(genesis, key)==value def test_quickly_mined_block(): last_block=Block.mine_block(Block.genesis(), 'foo') mined_block=Block.mine_block(last_block, 'bar') assert mined_block.difficulty==last_block.difficulty +1 def test_slowly_mined_block(): last_block=Block.mine_block(Block.genesis(), 'foo') time.sleep(MINE_RATE \/ SECONDS) mined_block=Block.mine_block(last_block, 'bar') assert mined_block.difficulty==last_block.difficulty -1 def test_mined_block_difficulty_limits_at_1(): last_block=Block(time.time_ns(), 'test_last_hash', 'test_hash', 'test_data', 1, 0) time.sleep(MINE_RATE \/ SECONDS) mined_block=Block.mine_block(last_block, 'bar') assert mined_block.difficulty==1 ","sourceWithComments":"import time\n\nfrom backend.blockchain.block import Block, GENESIS_DATA\nfrom backend.util.hex_to_binary import hex_to_binary\nfrom backend.config import MINE_RATE, SECONDS\n\ndef test_mine_block():\n    last_block = Block.genesis()\n    data = 'test_data'\n    block = Block.mine_block(last_block, data)\n\n    assert isinstance(block, Block)\n    assert block.data == data\n    assert block.last_hash == last_block.hash\n    assert hex_to_binary(block.hash)[0:block.difficulty] == '0' * block.difficulty\n\ndef test_genesis():\n    genesis = Block.genesis()\n\n    assert isinstance(genesis, Block)\n    for key, value in GENESIS_DATA.items():\n        getattr(genesis, key) == value\n\ndef test_quickly_mined_block():\n    last_block = Block.mine_block(Block.genesis(), 'foo')\n    mined_block = Block.mine_block(last_block, 'bar')\n\n    assert mined_block.difficulty == last_block.difficulty + 1\n\ndef test_slowly_mined_block():\n    last_block = Block.mine_block(Block.genesis(), 'foo')\n    \n    time.sleep(MINE_RATE \/ SECONDS)\n\n    mined_block = Block.mine_block(last_block, 'bar')\n\n    assert mined_block.difficulty == last_block.difficulty - 1\n\ndef test_mined_block_difficulty_limits_at_1():\n    last_block = Block(time.time_ns(), 'test_last_hash', 'test_hash', 'test_data', 1, 0)\n\n    time.sleep(MINE_RATE \/ SECONDS)\n\n    mined_block = Block.mine_block(last_block, 'bar')\n\n    assert mined_block.difficulty == 1"}},"msg":"Add Pytest to validate the is_valid function is working to prevent Block tampering"}},"https:\/\/github.com\/GinoP123\/TaskScheduler":{"c4f4fe8c4fb1cfa26ecbfbc79b8c78ba64a7cddc":{"url":"https:\/\/api.github.com\/repos\/GinoP123\/TaskScheduler\/commits\/c4f4fe8c4fb1cfa26ecbfbc79b8c78ba64a7cddc","html_url":"https:\/\/github.com\/GinoP123\/TaskScheduler\/commit\/c4f4fe8c4fb1cfa26ecbfbc79b8c78ba64a7cddc","message":"Added Encryption of task start time to prevent tampering","sha":"c4f4fe8c4fb1cfa26ecbfbc79b8c78ba64a7cddc","keyword":"tampering prevent","diff":"diff --git a\/encrypt.py b\/encrypt.py\nnew file mode 100644\nindex 0000000..c983710\n--- \/dev\/null\n+++ b\/encrypt.py\n@@ -0,0 +1,35 @@\n+import datetime\n+import settings\n+\n+\n+def to_binary(string):\n+\treturn ''.join(map(lambda x: format(ord(x), settings.binary_format)[2:], string))\n+\n+\n+def to_string(binary):\n+\tstring = \"\"\n+\tchars = len(binary) \/\/ settings.bits_in_byte\n+\tfor char in range(chars):\n+\t\tpos = char * settings.bits_in_byte\n+\t\tchar_bits = binary[pos:pos + settings.bits_in_byte]\n+\t\tstring += chr(int(char_bits, 2))\n+\treturn string\n+\n+\n+def get_current_day():\n+\treturn datetime.datetime.now().weekday() + 1\n+\n+\n+def encrypt(string):\n+\tbinary = to_binary(string)\n+\tshift_amount = get_current_day()\n+\tbinary = binary[-shift_amount:] + binary[:-shift_amount]\n+\treturn to_string(binary)\n+\n+\n+def decrypt(string):\n+\tbinary = to_binary(string)\n+\tshift_amount = get_current_day()\n+\tbinary = binary[shift_amount:] + binary[:shift_amount]\n+\treturn to_string(binary)\n+\ndiff --git a\/main.py b\/main.py\nindex 5dcb989..473c832 100755\n--- a\/main.py\n+++ b\/main.py\n@@ -7,6 +7,7 @@\n import subprocess as sp\n import datetime\n import os\n+import encrypt\n \n \n def get_tasks():\n@@ -20,7 +21,9 @@ def update_tasks(tasks):\n \t\tcsv.writer(tasks_file).writerows(tasks)\n \n \n-def get_sum():\n+def get_sum(tasks):\n+\tif os.path.getmtime(settings.sum_path) < os.path.getmtime(settings.tasks_path):\n+\t\tupdate_sum(sum(map(lambda x: int(x[1]), tasks)))\n \twith open(settings.sum_path) as infile:\n \t\treturn int(infile.read().strip())\n \n@@ -64,7 +67,7 @@ def add_task():\n \ttasks = get_tasks()\n \ttasks.append([name, str(units)])\n \tupdate_tasks(tasks)\n-\tupdate_sum(get_sum() + units)\n+\tupdate_sum(get_sum(tasks) + units)\n \n \n def list_tasks():\n@@ -79,31 +82,32 @@ def list_tasks():\n \t\tprint(settings.task_string.format(task[0]))\n \n \n-\n def pick_task():\n \ttasks = get_tasks()\n-\ttasks_sum = get_sum()\n+\ttasks_sum = get_sum(tasks)\n \ttask = random.randint(1, tasks_sum)\n \n \tpicked_task = -1\n \twhile task > 0:\n \t\tpicked_task += 1\n \t\ttask -= int(tasks[picked_task][1])\n-\t\n+\ttask = decrement_task(tasks, picked_task)\n \tupdate_sum(tasks_sum - 1)\n-\treturn decrement_task(tasks, picked_task)\n+\treturn task\n \n \n def check_time():\n \twith open(settings.time_file) as infile:\n-\t\ttime = datetime.datetime.strptime(infile.read().strip(), settings.time_format)\n+\t\ttime = encrypt.decrypt(infile.read())\n+\ttime = datetime.datetime.strptime(time, settings.time_format)\n \tdiff = (datetime.datetime.now() - time).seconds \/\/ 60\n \treturn diff >= int(settings.time_designated)\n \n \n def set_time():\n \twith open(settings.time_file, 'w') as outfile:\n-\t\toutfile.write(datetime.datetime.now().strftime(settings.time_format))\n+\t\ttime = datetime.datetime.now().strftime(settings.time_format)\n+\t\toutfile.write(encrypt.encrypt(time))\n \n \n def main():\ndiff --git a\/settings.py b\/settings.py\nindex fc2d6cf..a5b4fea 100644\n--- a\/settings.py\n+++ b\/settings.py\n@@ -7,8 +7,8 @@\n \n # Presets\n time_format = \"%m\/%d\/%Y, %H:%M:%S\"\n-time_designated = \"25\"\n-time_until_break = \"20\"\n+time_designated = \"30\"\n+time_until_break = \"25\"\n \n input_name = \"\\n\\tEnter Name: \"\n input_units = \"\\n\\tUnits: \"\n@@ -22,3 +22,9 @@\n \n task_column_labels = \"\\n\\tTASKS REMAINING:\\n\"\n task_string = \"\\t\\t{}\\n\"\n+\n+\n+# Encryption Presets\n+binary_format = '#010b'\n+bits_in_byte = 8\n+\n","files":{"\/main.py":{"changes":[{"diff":"\n \t\tcsv.writer(tasks_file).writerows(tasks)\n \n \n-def get_sum():\n+def get_sum(tasks):\n+\tif os.path.getmtime(settings.sum_path) < os.path.getmtime(settings.tasks_path):\n+\t\tupdate_sum(sum(map(lambda x: int(x[1]), tasks)))\n \twith open(settings.sum_path) as infile:\n \t\treturn int(infile.read().strip())\n \n","add":3,"remove":1,"filename":"\/main.py","badparts":["def get_sum():"],"goodparts":["def get_sum(tasks):","\tif os.path.getmtime(settings.sum_path) < os.path.getmtime(settings.tasks_path):","\t\tupdate_sum(sum(map(lambda x: int(x[1]), tasks)))"]},{"diff":"\n \ttasks = get_tasks()\n \ttasks.append([name, str(units)])\n \tupdate_tasks(tasks)\n-\tupdate_sum(get_sum() + units)\n+\tupdate_sum(get_sum(tasks) + units)\n \n \n def list_tasks():\n","add":1,"remove":1,"filename":"\/main.py","badparts":["\tupdate_sum(get_sum() + units)"],"goodparts":["\tupdate_sum(get_sum(tasks) + units)"]},{"diff":"\n \t\tprint(settings.task_string.format(task[0]))\n \n \n-\n def pick_task():\n \ttasks = get_tasks()\n-\ttasks_sum = get_sum()\n+\ttasks_sum = get_sum(tasks)\n \ttask = random.randint(1, tasks_sum)\n \n \tpicked_task = -1\n \twhile task > 0:\n \t\tpicked_task += 1\n \t\ttask -= int(tasks[picked_task][1])\n-\t\n+\ttask = decrement_task(tasks, picked_task)\n \tupdate_sum(tasks_sum - 1)\n-\treturn decrement_task(tasks, picked_task)\n+\treturn task\n \n \n def check_time():\n \twith open(settings.time_file) as infile:\n-\t\ttime = datetime.datetime.strptime(infile.read().strip(), settings.time_format)\n+\t\ttime = encrypt.decrypt(infile.read())\n+\ttime = datetime.datetime.strptime(time, settings.time_format)\n \tdiff = (datetime.datetime.now() - time).seconds \/\/ 60\n \treturn diff >= int(settings.time_designated)\n \n \n def set_time():\n \twith open(settings.time_file, 'w') as outfile:\n-\t\toutfile.write(datetime.datetime.now().strftime(settings.time_format))\n+\t\ttime = datetime.datetime.now().strftime(settings.time_format)\n+\t\toutfile.write(encrypt.encrypt(time))\n \n \n def main()","add":7,"remove":6,"filename":"\/main.py","badparts":["\ttasks_sum = get_sum()","\t","\treturn decrement_task(tasks, picked_task)","\t\ttime = datetime.datetime.strptime(infile.read().strip(), settings.time_format)","\t\toutfile.write(datetime.datetime.now().strftime(settings.time_format))"],"goodparts":["\ttasks_sum = get_sum(tasks)","\ttask = decrement_task(tasks, picked_task)","\treturn task","\t\ttime = encrypt.decrypt(infile.read())","\ttime = datetime.datetime.strptime(time, settings.time_format)","\t\ttime = datetime.datetime.now().strftime(settings.time_format)","\t\toutfile.write(encrypt.encrypt(time))"]}],"source":"\n import csv import settings import random import sys import subprocess as sp import datetime import os def get_tasks(): \twith open(settings.tasks_path) as tasks_file: \t\ttasks=list(csv.reader(tasks_file)) \treturn tasks def update_tasks(tasks): \twith open(settings.tasks_path, 'w') as tasks_file: \t\tcsv.writer(tasks_file).writerows(tasks) def get_sum(): \twith open(settings.sum_path) as infile: \t\treturn int(infile.read().strip()) def update_sum(new_sum): \twith open(settings.sum_path, 'w') as outfile: \t\toutfile.write(str(new_sum)) def update_gravy(task): \twith open(settings.gravy_path, 'a') as outfile: \t\toutfile.write(f\"{task}\\n\") def decrement_task(tasks, index): \ttask=tasks[index] \tif task[1]=='1': \t\tupdate_gravy(tasks.pop(index)[0]) \telse: \t\ttask[1]=str(int(task[1]) -1) \tupdate_tasks(tasks) \treturn task[0] def get_new_task_info(): \tname=\"\" \tunits=\"\" \tprint() \twhile not name: \t\tname=input(settings.input_name) \twhile not units.isnumeric(): \t\tunits=input(settings.input_units) \tprint(\"\\n\") \treturn name, int(units) def add_task(): \tname, units=get_new_task_info() \ttasks=get_tasks() \ttasks.append([name, str(units)]) \tupdate_tasks(tasks) \tupdate_sum(get_sum() +units) def list_tasks(): \ttasks=get_tasks() \tif not tasks: \t\tprint(settings.no_task_message) \t\texit() \tprint(settings.task_column_labels) \tfor task in tasks: \t\tprint(settings.task_string.format(task[0])) def pick_task(): \ttasks=get_tasks() \ttasks_sum=get_sum() \ttask=random.randint(1, tasks_sum) \tpicked_task=-1 \twhile task > 0: \t\tpicked_task +=1 \t\ttask -=int(tasks[picked_task][1]) \t \tupdate_sum(tasks_sum -1) \treturn decrement_task(tasks, picked_task) def check_time(): \twith open(settings.time_file) as infile: \t\ttime=datetime.datetime.strptime(infile.read().strip(), settings.time_format) \tdiff=(datetime.datetime.now() -time).seconds \/\/ 60 \treturn diff >=int(settings.time_designated) def set_time(): \twith open(settings.time_file, 'w') as outfile: \t\toutfile.write(datetime.datetime.now().strftime(settings.time_format)) def main(): \tif check_time(): \t\ttask=pick_task() \t\tif task: \t\t\tsp.run([settings.alarm_setter_path, \"set\", \t\t\t\tsettings.time_until_break, \t\t\t\tsettings.break_message.format(task)]) \t\t\tsp.run([settings.alarm_setter_path, \"set\", \t\t\t\tsettings.time_designated, \t\t\t\tsettings.break_over_message]) \t\t\tprint(settings.next_task_message.format(task)) \t\t\tset_time() \t\telse: \t\t\tprint(settings.no_task_message) \telse: \t\tprint(settings.keep_working_message) if __name__==\"__main__\": \tassert len(sys.argv) \tos.chdir(os.path.dirname(sys.argv[0])) \tcommand=''.join(sys.argv[1:]) \tif command==\"next\": \t\tmain() \telif command==\"add\": \t\tadd_task() \telif command==\"list\": \t\tlist_tasks() \telse: \t\traise AssertionError(settings.invalid_syntax_message) ","sourceWithComments":"#!\/usr\/bin\/env python3\n\nimport csv\nimport settings\nimport random\nimport sys\nimport subprocess as sp\nimport datetime\nimport os\n\n\ndef get_tasks():\n\twith open(settings.tasks_path) as tasks_file:\n\t\ttasks = list(csv.reader(tasks_file))\n\treturn tasks\n\n\ndef update_tasks(tasks):\n\twith open(settings.tasks_path, 'w') as tasks_file:\n\t\tcsv.writer(tasks_file).writerows(tasks)\n\n\ndef get_sum():\n\twith open(settings.sum_path) as infile:\n\t\treturn int(infile.read().strip())\n\n\ndef update_sum(new_sum):\n\twith open(settings.sum_path, 'w') as outfile:\n\t\toutfile.write(str(new_sum))\n\n\ndef update_gravy(task):\n\twith open(settings.gravy_path, 'a') as outfile:\n\t\toutfile.write(f\"{task}\\n\")\n\n\ndef decrement_task(tasks, index):\n\ttask = tasks[index]\n\tif task[1] == '1':\n\t\tupdate_gravy(tasks.pop(index)[0])\n\telse:\n\t\ttask[1] = str(int(task[1]) - 1)\n\tupdate_tasks(tasks)\n\treturn task[0]\n\n\ndef get_new_task_info():\n\tname = \"\"\n\tunits = \"\"\n\n\tprint()\n\twhile not name:\n\t\tname = input(settings.input_name)\n\twhile not units.isnumeric():\n\t\tunits = input(settings.input_units)\n\tprint(\"\\n\")\n\n\treturn name, int(units)\n\n\ndef add_task():\n\tname, units = get_new_task_info()\n\ttasks = get_tasks()\n\ttasks.append([name, str(units)])\n\tupdate_tasks(tasks)\n\tupdate_sum(get_sum() + units)\n\n\ndef list_tasks():\n\ttasks = get_tasks()\n\n\tif not tasks:\n\t\tprint(settings.no_task_message)\n\t\texit()\n\n\tprint(settings.task_column_labels)\n\tfor task in tasks:\n\t\tprint(settings.task_string.format(task[0]))\n\n\n\ndef pick_task():\n\ttasks = get_tasks()\n\ttasks_sum = get_sum()\n\ttask = random.randint(1, tasks_sum)\n\n\tpicked_task = -1\n\twhile task > 0:\n\t\tpicked_task += 1\n\t\ttask -= int(tasks[picked_task][1])\n\t\n\tupdate_sum(tasks_sum - 1)\n\treturn decrement_task(tasks, picked_task)\n\n\ndef check_time():\n\twith open(settings.time_file) as infile:\n\t\ttime = datetime.datetime.strptime(infile.read().strip(), settings.time_format)\n\tdiff = (datetime.datetime.now() - time).seconds \/\/ 60\n\treturn diff >= int(settings.time_designated)\n\n\ndef set_time():\n\twith open(settings.time_file, 'w') as outfile:\n\t\toutfile.write(datetime.datetime.now().strftime(settings.time_format))\n\n\ndef main():\n\tif check_time():\n\t\ttask = pick_task()\n\t\tif task:\n\t\t\tsp.run([settings.alarm_setter_path, \"set\", \n\t\t\t\tsettings.time_until_break, \n\t\t\t\tsettings.break_message.format(task)])\n\t\t\tsp.run([settings.alarm_setter_path, \"set\", \n\t\t\t\tsettings.time_designated, \n\t\t\t\tsettings.break_over_message])\n\t\t\tprint(settings.next_task_message.format(task))\n\t\t\tset_time()\n\t\telse:\n\t\t\tprint(settings.no_task_message)\n\telse:\n\t\tprint(settings.keep_working_message)\n\n\n\nif __name__ == \"__main__\":\n\tassert len(sys.argv)\n\tos.chdir(os.path.dirname(sys.argv[0]))\n\n\tcommand = ''.join(sys.argv[1:])\n\tif command == \"next\":\n\t\tmain()\n\telif command == \"add\":\n\t\tadd_task()\n\telif command == \"list\":\n\t\tlist_tasks()\n\telse:\n\t\traise AssertionError(settings.invalid_syntax_message)\n\n"},"\/settings.py":{"changes":[{"diff":"\n \n # Presets\n time_format = \"%m\/%d\/%Y, %H:%M:%S\"\n-time_designated = \"25\"\n-time_until_break = \"20\"\n+time_designated = \"30\"\n+time_until_break = \"25\"\n \n input_name = \"\\n\\tEnter Name: \"\n input_units = \"\\n\\tUnits: \"\n","add":2,"remove":2,"filename":"\/settings.py","badparts":["time_designated = \"25\"","time_until_break = \"20\""],"goodparts":["time_designated = \"30\"","time_until_break = \"25\""]}],"source":"\n\nalarm_setter_path=\"\/Users\/ginoprasad\/Scripts\/AlarmSetter\/main.sh\" tasks_path=\"text_files\/tasks.csv\" sum_path=\"text_files\/sum.txt\" gravy_path=\"text_files\/gravy.txt\" time_file=\"text_files\/time.txt\" time_format=\"%m\/%d\/%Y, %H:%M:%S\" time_designated=\"25\" time_until_break=\"20\" input_name=\"\\n\\tEnter Name: \" input_units=\"\\n\\tUnits: \" next_task_message=\"\\n\\tNext Task:{}\\n\" break_message=\"Take a break!!!\" break_over_message=\"Break Over!!!\" no_task_message=\"\\n\\tNo Tasks Left:)\\n\" keep_working_message=\"\\n\\tWoah There Buddy, Still on the Clock\\n\" invalid_syntax_message=\"ERROR: INVALID SYNTAX\" task_column_labels=\"\\n\\tTASKS REMAINING:\\n\" task_string=\"\\t\\t{}\\n\" ","sourceWithComments":"# File Paths\nalarm_setter_path = \"\/Users\/ginoprasad\/Scripts\/AlarmSetter\/main.sh\"\ntasks_path = \"text_files\/tasks.csv\"\nsum_path = \"text_files\/sum.txt\"\ngravy_path = \"text_files\/gravy.txt\"\ntime_file = \"text_files\/time.txt\"\n\n# Presets\ntime_format = \"%m\/%d\/%Y, %H:%M:%S\"\ntime_designated = \"25\"\ntime_until_break = \"20\"\n\ninput_name = \"\\n\\tEnter Name: \"\ninput_units = \"\\n\\tUnits: \"\n\nnext_task_message = \"\\n\\tNext Task: {}\\n\"\nbreak_message = \"Take a break!!!\"\nbreak_over_message = \"Break Over!!!\"\nno_task_message = \"\\n\\tNo Tasks Left :)\\n\"\nkeep_working_message = \"\\n\\tWoah There Buddy, Still on the Clock\\n\"\ninvalid_syntax_message = \"ERROR: INVALID SYNTAX\"\n\ntask_column_labels = \"\\n\\tTASKS REMAINING:\\n\"\ntask_string = \"\\t\\t{}\\n\"\n"}},"msg":"Added Encryption of task start time to prevent tampering"}},"https:\/\/github.com\/swhustla\/n_queens":{"1a984ddb7692ec75312a06ed9b24bf7fc4369695":{"url":"https:\/\/api.github.com\/repos\/swhustla\/n_queens\/commits\/1a984ddb7692ec75312a06ed9b24bf7fc4369695","html_url":"https:\/\/github.com\/swhustla\/n_queens\/commit\/1a984ddb7692ec75312a06ed9b24bf7fc4369695","message":"Return copies of members to prevent tampering","sha":"1a984ddb7692ec75312a06ed9b24bf7fc4369695","keyword":"tampering prevent","diff":"diff --git a\/models\/board.py b\/models\/board.py\nindex 6165404..f50ccda 100644\n--- a\/models\/board.py\n+++ b\/models\/board.py\n@@ -31,12 +31,15 @@ def __init__(self, size: int = None, rows: list[int] = None, shuffled=False):\n             shuffle(self.__rows)\n         self.__colliding_indices = colliding_indices(self.__rows, self.__size)\n         self.__collisions = len(self.__colliding_indices)\n+    \n+    def rows(self):\n+        return copy(self.__rows)\n \n     def size(self):\n         return self.__size\n \n     def colliding_indices(self):\n-        return self.__colliding_indices\n+        return copy(self.__colliding_indices)\n \n     def collisions(self):\n         return self.__collisions\n","files":{"\/models\/board.py":{"changes":[{"diff":"\n             shuffle(self.__rows)\n         self.__colliding_indices = colliding_indices(self.__rows, self.__size)\n         self.__collisions = len(self.__colliding_indices)\n+    \n+    def rows(self):\n+        return copy(self.__rows)\n \n     def size(self):\n         return self.__size\n \n     def colliding_indices(self):\n-        return self.__colliding_indices\n+        return copy(self.__colliding_indices)\n \n     def collisions(self):\n         return self.__collisions\n","add":4,"remove":1,"filename":"\/models\/board.py","badparts":["        return self.__colliding_indices"],"goodparts":["    def rows(self):","        return copy(self.__rows)","        return copy(self.__colliding_indices)"]}],"source":"\nfrom copy import copy from random import shuffle from models.board_utils import colliding_indices, place_queen, swap class Board: def __init__(self, size: int=None, rows: list[int]=None, shuffled=False): \"\"\"Return a board of specified size > 3, or the specified rows (positions of queens in each row), after optionally shuffling them. The size parameter is ignored if rows are provided.\"\"\" if size is None and rows is None: raise if rows is not None: if not isinstance(rows, list): raise size=len(rows) if size <=3: raise if any(row >=size or row < 0 or not isinstance(row, int) for row in rows): raise self.__rows=copy(rows) self.__size=len(rows) else: if not isinstance(size, int): raise if size <=3: raise self.__size=size self.__rows=[row for row in range(size)] if shuffled: shuffle(self.__rows) self.__colliding_indices=colliding_indices(self.__rows, self.__size) self.__collisions=len(self.__colliding_indices) def size(self): return self.__size def colliding_indices(self): return self.__colliding_indices def collisions(self): return self.__collisions def is_valid_until(self, row: int): def until_row(index): x, y=index return x <=row and y <=row return len(list(filter(until_row, self.__colliding_indices)))==0 def is_valid(self): return self.collisions()==0 def queen(self, row: int): return self.__rows[row] def __stringify_row(self, row: int): col_strings=[] for i in range(self.__size): col_strings.append(\"Q\" if i==row else \"_\") return \" \".join(col_strings) def stringify(self): row_strings=[] for row in self.__rows: row_strings.append(self.__stringify_row(row)) return \"\\n\".join(row_strings) +\"\\n\" def place(self, row: int, col: int): return Board(rows=place_queen(self.__rows, row, col)) def swap(self, row_x: int, row_y: int): return Board(rows=(swap(self.__rows, row_x, row_y))) ","sourceWithComments":"from copy import copy\nfrom random import shuffle\nfrom models.board_utils import colliding_indices, place_queen, swap\n\n\nclass Board:\n    def __init__(self, size: int = None, rows: list[int] = None, shuffled=False):\n        \"\"\"Return a board of specified size > 3, or the specified rows\n        (positions of queens in each row), after optionally shuffling them. The\n        size parameter is ignored if rows are provided.\"\"\"\n        if size is None and rows is None:\n            raise\n        if rows is not None:\n            if not isinstance(rows, list):\n                raise\n            size = len(rows)\n            if size <= 3:\n                raise\n            if any(row >= size or row < 0 or not isinstance(row, int) for row in rows):\n                raise\n            self.__rows = copy(rows)\n            self.__size = len(rows)\n        else:\n            if not isinstance(size, int):\n                raise\n            if size <= 3:\n                raise\n            self.__size = size\n            self.__rows = [row for row in range(size)]\n        if shuffled:\n            shuffle(self.__rows)\n        self.__colliding_indices = colliding_indices(self.__rows, self.__size)\n        self.__collisions = len(self.__colliding_indices)\n\n    def size(self):\n        return self.__size\n\n    def colliding_indices(self):\n        return self.__colliding_indices\n\n    def collisions(self):\n        return self.__collisions\n\n    def is_valid_until(self, row: int):\n        def until_row(index):\n            x, y = index\n            return x <= row and y <= row\n\n        return len(list(filter(until_row, self.__colliding_indices))) == 0\n\n    def is_valid(self):\n        return self.collisions() == 0\n\n    def queen(self, row: int):\n        return self.__rows[row]\n\n    def __stringify_row(self, row: int):\n        col_strings = []\n        for i in range(self.__size):\n            col_strings.append(\"Q\" if i == row else \"_\")\n        return \" \".join(col_strings)\n\n    def stringify(self):\n        row_strings = []\n        for row in self.__rows:\n            row_strings.append(self.__stringify_row(row))\n        return \"\\n\".join(row_strings) + \"\\n\"\n\n    def place(self, row: int, col: int):\n        return Board(rows=place_queen(self.__rows, row, col))\n    \n    def swap(self, row_x: int, row_y: int):\n        return Board(rows=(swap(self.__rows, row_x, row_y)))\n"}},"msg":"Return copies of members to prevent tampering"}},"https:\/\/github.com\/bobintetley\/asm3":{"2ceb3f271fe089edd404e2040a9c7f6b908ec7c1":{"url":"https:\/\/api.github.com\/repos\/bobintetley\/asm3\/commits\/2ceb3f271fe089edd404e2040a9c7f6b908ec7c1","html_url":"https:\/\/github.com\/bobintetley\/asm3\/commit\/2ceb3f271fe089edd404e2040a9c7f6b908ec7c1","message":"prevent sql interface tampering with certain tables","sha":"2ceb3f271fe089edd404e2040a9c7f6b908ec7c1","keyword":"tampering prevent","diff":"diff --git a\/src\/asm3\/i18n.py b\/src\/asm3\/i18n.py\nindex e287457ea..705c17008 100644\n--- a\/src\/asm3\/i18n.py\n+++ b\/src\/asm3\/i18n.py\n@@ -6,8 +6,8 @@\n # flake8: noqa - we have a lot of locales and this is convenient\n from asm3.locales import *\n \n-VERSION = \"44u [Thu  8 Oct 10:18:16 BST 2020]\"\n-BUILD = \"10081018\"\n+VERSION = \"44u [Thu  8 Oct 12:41:36 BST 2020]\"\n+BUILD = \"10081241\"\n \n DMY = ( \"%d\/%m\/%Y\", \"%d\/%m\/%y\" )\n HDMY = ( \"%d-%m-%Y\", \"%d-%m-%y\" )\ndiff --git a\/src\/code.py b\/src\/code.py\nindex 5dbc75567..bead9f34b 100644\n--- a\/src\/code.py\n+++ b\/src\/code.py\n@@ -5381,16 +5381,29 @@ def post_execfile(self, o):\n         self.content_type(\"text\/plain\")\n         return self.exec_sql_from_file(o.dbo, o.user, sql)\n \n+    def check_update_query(self, q):\n+        \"\"\" Prevent any kind of update to certain tables to prevent\n+            more savvy malicious users tampering via SQL Interface.\n+            q is already stripped and converted to lower case by the exec_sql caller.\n+            If one of our tamper proofed tables is touched, an Exception is raised\n+            and the query not run.\n+        \"\"\"\n+        for t in ( \"audittrail\", \"deletion\" ):\n+            if q.find(t) != -1:\n+                raise Exception(\"Forbidden: %s\" % q)\n+\n     def exec_sql(self, dbo, user, sql):\n         l = dbo.locale\n         rowsaffected = 0\n         try:\n             for q in dbo.split_queries(sql):\n                 if q == \"\": continue\n+                ql = q.lower()\n                 asm3.al.info(\"%s query: %s\" % (user, q), \"code.sql\", dbo)\n-                if q.lower().startswith(\"select\") or q.lower().startswith(\"show\"):\n+                if ql.startswith(\"select\") or ql.startswith(\"show\"):\n                     return asm3.html.table(dbo.query(q))\n                 else:\n+                    self.check_update_query(ql)\n                     rowsaffected += dbo.execute(q)\n             asm3.configuration.db_view_seq_version(dbo, \"0\")\n             return _(\"{0} rows affected.\", l).format(rowsaffected)\n@@ -5404,10 +5417,12 @@ def exec_sql_from_file(self, dbo, user, sql):\n         for q in dbo.split_queries(sql):\n             try:\n                 if q == \"\": continue\n+                ql = q.lower()\n                 asm3.al.info(\"%s query: %s\" % (user, q), \"code.sql\", dbo)\n-                if q.lower().startswith(\"select\") or q.lower().startswith(\"show\"):\n+                if ql.startswith(\"select\") or ql.startswith(\"show\"):\n                     output.append(str(dbo.query(q)))\n                 else:\n+                    self.check_update_query(ql)\n                     rowsaffected = dbo.execute(q)\n                     output.append(_(\"{0} rows affected.\", l).format(rowsaffected))\n             except Exception as err:\n","files":{"\/src\/asm3\/i18n.py":{"changes":[{"diff":"\n # flake8: noqa - we have a lot of locales and this is convenient\n from asm3.locales import *\n \n-VERSION = \"44u [Thu  8 Oct 10:18:16 BST 2020]\"\n-BUILD = \"10081018\"\n+VERSION = \"44u [Thu  8 Oct 12:41:36 BST 2020]\"\n+BUILD = \"10081241\"\n \n DMY = ( \"%d\/%m\/%Y\", \"%d\/%m\/%y\" )\n HDMY = ( \"%d-%m-%Y\", \"%d-%m-%y\" )","add":2,"remove":2,"filename":"\/src\/asm3\/i18n.py","badparts":["VERSION = \"44u [Thu  8 Oct 10:18:16 BST 2020]\"","BUILD = \"10081018\""],"goodparts":["VERSION = \"44u [Thu  8 Oct 12:41:36 BST 2020]\"","BUILD = \"10081241\""]}],"source":"\n\nimport datetime import json import time from asm3.locales import * VERSION=\"44u[Thu 8 Oct 10:18:16 BST 2020]\" BUILD=\"10081018\" DMY=( \"%d\/%m\/%Y\", \"%d\/%m\/%y\") HDMY=( \"%d-%m-%Y\", \"%d-%m-%y\") DDMY=( \"%d.%m.%Y\", \"%d.%m.%y\") MDY=( \"%m\/%d\/%Y\", \"%m\/%d\/%y\") YMD=( \"%Y\/%m\/%d\", \"%y\/%m\/%d\") DYMD=( \"%Y.%m.%d\", \"%y.%m.%d\") HYMD=( \"%Y-%m-%d\", \"%y-%m-%d\") DOLLAR=\"$\" EURO=\"& POUND=\"&pound;\" YEN=\"&yen;\" CURRENCY_PREFIX=\"p\" CURRENCY_SUFFIX=\"s\" DST_US=\"6-203-111\" DST_UK=\"6-L03-L10\" DST_AU=\"6-110-104\" def PLURAL_ENGLISH(n): \"\"\" gettext plural function for English\/Latin languages \"\"\" if n==1: return 0 return 1 def PLURAL_HUNGARIAN(n): \"\"\" gettext style plural function for Hungarian Hungarian always uses the singular unless the element appears by itself(which it never does for the purposes of ngettext) so always return the singular \"\"\" return 0 def PLURAL_POLISH(n): \"\"\" gettext style plural function for Polish \"\"\" if n==1: return 0 if n % 10 >=2 and n % 10 <=4 and(n % 100 < 10 or n % 100 >=20): return 1 return 2 def PLURAL_SLAVIC(n): \"\"\" gettext style plural function for Slavic languages, Russian, Ukrainian, Belarusian, Serbian, Croatian \"\"\" if n % 10==1 and n % 100 !=11: return 0 if n % 10 >=2 and n % 10 <=4 and(n % 100 < 10 or n % 100 >=20): return 1 return 2 LM_LANGUAGE=0 LM_COUNTRY=1 LM_DATEFORMAT=2 LM_CURRENCY_SYMBOL=3 LM_PLURAL_FUNCTION=4 LM_CURRENCY_POSITION=5 LM_CURRENCY_DECIMAL_PLACES=6 LM_CURRENCY_DECIMAL_MARK=7 LM_CURRENCY_DIGIT_GROUPING=8 LM_DST=9 locale_maps={ \"en\": ( \"English\", \"United States\", MDY, DOLLAR, PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", DST_US), \"en_GB\": ( \"English\", \"Great Britain\", DMY, POUND, PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", DST_UK), \"en_AU\": ( \"English\", \"Australia\", DMY, DOLLAR, PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", DST_AU), \"en_AE\": ( \"English\", \"United Arab Emirates\", DMY, \"& \"en_AW\": ( \"English\", \"Aruba\", DMY, \"Awg.\", PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", \"\"), \"en_BE\": ( \"English\", \"Belgium\", DMY, EURO, PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \",\", \" \", DST_UK), \"en_BM\": ( \"English\", \"Bermuda\", DMY, DOLLAR, PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", \"6-203-111\"), \"en_BG\": ( \"English\", \"Bulgaria\", HYMD, \"& \"en_BH\": ( \"English\", \"Bahrain\", MDY, \"BD\", PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", \"\", \"\"), \"en_BQ\": ( \"English\", \"Bonaire\", DMY, DOLLAR, PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", \"\"), \"en_CA\": ( \"English\", \"Canada\", MDY, DOLLAR, PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", DST_US), \"en_CH\": ( \"English\", \"Switzerland\", DDMY, \"CHF\", PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", DST_UK), \"en_CN\": ( \"English\", \"China\", HYMD, YEN, PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", \"\"), \"en_CR\": ( \"English\", \"Costa Rica\", DMY, \"& \"en_CY\": ( \"English\", \"Cyprus\", DMY, EURO, PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", DST_UK), \"en_ES\": ( \"English\", \"Spain\", DMY, EURO, PLURAL_ENGLISH, CURRENCY_SUFFIX, 2, \",\", \" \", DST_UK), \"en_HK\": ( \"English\", \"Hong Kong\", HDMY, DOLLAR, PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", \"\"), \"en_KH\": ( \"English\", \"Cambodia\", DMY, DOLLAR, PLURAL_ENGLISH, CURRENCY_SUFFIX, 2, \".\", \",\", \"\"), \"en_KW\": ( \"English\", \"Kuwait\", DMY, \"KD\", PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", \"\"), \"en_KY\": ( \"English\", \"Cayman Islands\", DMY, DOLLAR, PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", \"\"), \"en_IE\": ( \"English\", \"Ireland\", DMY, EURO, PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", DST_UK), \"en_IN\": ( \"English\", \"India\", DMY, \"& \"en_IL\": ( \"English\", \"Israel\", DMY, \"& \"en_JP\": ( \"English\", \"Japan\", YMD, \"&yen;\", PLURAL_ENGLISH, CURRENCY_SUFFIX, 0, \".\", \",\", \"\"), \"en_LB\": ( \"English\", \"Lebanon\", MDY, \"L&pound;\", PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", DST_UK), \"en_LU\": ( \"English\", \"Luxembourg\", DDMY, EURO, PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", DST_UK), \"en_MU\": ( \"English\", \"Mauritius\", DMY, \"& \"en_MY\": ( \"English\", \"Malaysia\", DMY, \"RM\", PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", \"\"), \"en_MX\": ( \"English\", \"Mexico\", DMY, DOLLAR, PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", \"6-104-L10\"), \"en_NA\": ( \"English\", \"Namibia\", YMD, DOLLAR, PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \",\", \" \", \"\"), \"en_PH\": ( \"English\", \"Philippines\", MDY, \"& \"en_QA\": ( \"English\", \"Qatar\", DMY, \"QR\", PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", \"\"), \"en_NZ\": ( \"English\", \"New Zealand\", DMY, DOLLAR, PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", \"6-L09-104\"), \"en_TH\": ( \"English\", \"Thailand\", DMY, \"& \"en_TW\": ( \"English\", \"Taiwan\", YMD, DOLLAR, PLURAL_ENGLISH, CURRENCY_PREFIX, 0, \".\", \",\", \"\"), \"en_TW2\": ( \"English\", \"Taiwan $0.00\", YMD, DOLLAR, PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", \"\"), \"en_VN\": ( \"English\", \"Vietnam\", DMY, \"& \"en_ZA\": ( \"English\", \"South Africa\", YMD, \"R\", PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \",\", \" \", \"\"), \"bg\": ( \"Bulgarian\", \"Bulgaria\", HYMD, \"& \"bs\": ( \"Bosnian\", \"Bosnia\", HYMD, \"KM\", PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", DST_UK), \"cs\": ( \"Czech\", \"Czech Republic\", DYMD, \"& \"de\": ( \"German\", \"Germany\", DDMY, EURO, PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \",\", \" \", DST_UK), \"de_AT\": ( \"German\", \"Austria\", DDMY, EURO, PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", DST_UK), \"de_CH\": ( \"German\", \"Switzerland\", DDMY, \"CHF\", PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", DST_UK), \"de_LU\": ( \"German\", \"Luxembourg\", DDMY, EURO, PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", DST_UK), \"el\": ( \"Greek\", \"Greece\", DMY, EURO, PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \",\", \" \", DST_UK), \"es\": ( \"Spanish\", \"Spain\", DMY, EURO, PLURAL_ENGLISH, CURRENCY_SUFFIX, 2, \",\", \" \", DST_UK), \"es_CO\": ( \"Spanish\", \"Columbia\", DMY, DOLLAR, PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \",\", \" \", \"\"), \"es_CR\": ( \"Spanish\", \"Costa Rica\", DMY, \"& \"es_EC\": ( \"Spanish\", \"Ecuador\", DMY, DOLLAR, PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \",\", \" \", \"\"), \"es_MX\": ( \"Spanish\", \"Mexico\", DMY, DOLLAR, PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", \"6-104-L10\"), \"et\": ( \"Estonian\", \"Estonia\", DMY, \"kr\", PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \",\", \" \", DST_UK), \"fi\": ( \"Finnish\", \"Finland\", DDMY, EURO, PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \",\", \" \", DST_UK), \"fr\": ( \"French\", \"France\", DMY, EURO, PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \",\", \" \", DST_UK), \"fr_BE\": ( \"French\", \"Belgium\", DMY, EURO, PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \",\", \" \", DST_UK), \"fr_LU\": ( \"French\", \"Luxembourg\", DDMY, EURO, PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", DST_UK), \"fr_CA\": ( \"French\", \"Canada\", DMY, DOLLAR, PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \",\", \" \", DST_US), \"fr_CH\": ( \"French\", \"Switzerland\", DDMY, \"CHF\", PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", DST_UK), \"he\": ( \"Hebrew\", \"Israel\", DMY, \"& \"hu\": ( \"Hungarian\", \"Hungary\", DYMD, \"Ft\", PLURAL_HUNGARIAN, CURRENCY_PREFIX, 2, \",\", \" \", DST_UK), \"it\": ( \"Italian\", \"Italy\", DMY, EURO, PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \",\", \" \", DST_UK), \"it_CH\": ( \"Italian\", \"Switzerland\", DDMY, \"CHF\", PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", DST_UK), \"lt\": ( \"Lithuanian\", \"Lithuania\", DYMD, EURO, PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \",\", \" \", DST_UK), \"nb\": ( \"Norwegian Bokmal\", \"Norway\", DDMY, \"kr\", PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \",\", \" \", DST_UK), \"nl\": ( \"Dutch\", \"Holland\", HDMY, EURO, PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \",\", \" \", DST_UK), \"nl_AW\": ( \"Dutch\", \"Aruba\", DMY, \"Awg.\", PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", \"\"), \"nl_BE\": ( \"Dutch\", \"Belgium\", DMY, EURO, PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \",\", \" \", DST_UK), \"nl_BQ\": ( \"Dutch\", \"Bonaire\", DMY, DOLLAR, PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", \"\"), \"pl\": ( \"Polish\", \"Poland\", DDMY, \"& \"pt\": ( \"Portuguese\", \"Portugal\", HDMY, EURO, PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \",\", \" \", DST_UK), \"ru\": ( \"Russian\", \"Russia\", DDMY, \"& \"sk\": ( \"Slovakian\", \"Slovakia\", DDMY, EURO, PLURAL_SLAVIC, CURRENCY_PREFIX, 2, \",\", \" \", DST_UK), \"sl\": ( \"Slovenian\", \"Slovenia\", DDMY, EURO, PLURAL_SLAVIC, CURRENCY_PREFIX, 2, \",\", \" \", DST_UK), \"sv\": ( \"Swedish\", \"Sweden\", HYMD, \"kr\", PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \",\", \" \", DST_UK), \"th\": ( \"Thai\", \"Thailand\", DMY, \"& \"tr\": ( \"Turkish\", \"Turkey\", DDMY, \"TL\", PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \",\", \" \", \"\") } def _(english, locale=\"en\"): return translate(english, locale) def real_locale(locale=\"en\"): if locale in(\"en_AE\", \"en_BE\", \"en_BG\", \"en_BM\", \"en_BQ\", \"en_CH\", \"en_CN\", \"en_CY\", \"en_ES\", \"en_HK\", \"en_IE\", \"en_IN\", \"en_JP\", \"en_KH\", \"en_LB\", \"en_LU\", \"en_MU\", \"en_MY\", \"en_NA\", \"en_PH\", \"en_QA\", \"en_TH\", \"en_TW\", \"en_TW2\", \"en_VN\", \"en_ZA\"): locale=\"en_GB\" if locale in(\"en_AW\", \"en_BH\", \"en_CO\", \"en_CR\", \"en_KW\", \"en_KY\", \"en_IL\", \"en_LB\", \"en_MX\"): locale=\"en\" if locale in(\"en_NZ\",): locale=\"en_AU\" if locale in(\"nl_AW\", \"nl_BE\", \"nl_BQ\"): locale=\"nl\" if locale in(\"fr_BE\", \"fr_CH\", \"fr_LU\"): locale=\"fr\" if locale in(\"de_AT\", \"de_CH\", \"de_LU\"): locale=\"de\" if locale in(\"it_CH\",): locale=\"it\" if locale in(\"es_CO\", \"es_CR\", \"es_EC\", \"es_MX\"): locale=\"es\" return locale def translate(english, locale=\"en\"): \"\"\" Returns a translation string for an English phrase in the locale given. \"\"\" locale=real_locale(locale) if locale==\"en\": return english try: lang=globals()[\"locale_\" +locale] except: return english if english not in lang.val: return english s=lang.val[english] if s is None or s==\"\" or s.startswith(\"??\") or s.startswith(\"(??\"): return english else: return lang.val[english] def ntranslate(number, translations, locale=\"en\"): \"\"\" Translates a phrase that deals with a number of something so the correct plural can be used. number: The number of items translations: A list of already translated strings for each plural form locale: The locale the strings are in(which plural function to use) \"\"\" try: pluralfun=get_plural_function(locale) text=translations[pluralfun(number)] text=text.replace(\"{plural0}\", str(number)) text=text.replace(\"{plural1}\", str(number)) text=text.replace(\"{plural2}\", str(number)) text=text.replace(\"{plural3}\", str(number)) text=text.replace(\"{plural4}\", str(number)) return text except Exception as e: return e def get_version(): \"\"\" Returns the version of ASM \"\"\" return VERSION def get_version_number(): \"\"\" Returns the version number of ASM \"\"\" return VERSION[0:VERSION.find(\" \")] def get_locale_map(locale, index): if locale in locale_maps: return locale_maps[locale][index] else: return locale_maps[\"en\"][index] def get_locales(): locales=[] for k, v in locale_maps.items(): if k.find(\"_\") !=-1 and k !=\"en\": locales.append((k, \"%s(%s)\" %(v[LM_LANGUAGE], v[LM_COUNTRY]))) else: locales.append((k, \"%s\" % v[LM_LANGUAGE])) locales=sorted(locales, key=lambda x: x[1]) return locales def get_country(locale): return get_locale_map(locale, LM_COUNTRY) def get_language(locale): return get_locale_map(locale, LM_LANGUAGE) def get_display_date_format(locale, digitsinyear=4): \"\"\" Returns the display date format for a locale \"\"\" if digitsinyear==4: return get_locale_map(locale, LM_DATEFORMAT)[0] else: return get_locale_map(locale, LM_DATEFORMAT)[1] def get_currency_symbol(locale): \"\"\" Returns the currency symbol for a locale \"\"\" return get_locale_map(locale, LM_CURRENCY_SYMBOL) def get_currency_prefix(locale): \"\"\" Returns \"p\" if the currency symbol goes at the beginning, or \"s\" for the end when displaying. \"\"\" return get_locale_map(locale, LM_CURRENCY_POSITION) def get_currency_dp(locale): \"\"\" Returns the number of decimal places for a locale when displaying currency \"\"\" return get_locale_map(locale, LM_CURRENCY_DECIMAL_PLACES) def get_currency_radix(locale): \"\"\" Returns the decimal mark symbol \"\"\" return get_locale_map(locale, LM_CURRENCY_DECIMAL_MARK) def get_currency_digit_grouping(locale): \"\"\" Returns the character used to separate thousands \"\"\" return get_locale_map(locale, LM_CURRENCY_DIGIT_GROUPING) def get_dst(locale): \"\"\" Returns the daylight savings time info for locale \"\"\" return get_locale_map(locale, LM_DST) def get_plural_function(locale): \"\"\" Returns the function for calculating plurals for this locale \"\"\" return get_locale_map(locale, LM_PLURAL_FUNCTION) def format_currency(locale, value, includeSymbol=True): \"\"\" Formats a currency value to the correct number of decimal places and returns it as a string \"\"\" if value is None: value=0 i=0 f=0.0 try: i=int(value) f=float(i) except: pass f=f \/ 100 dp=str(get_currency_dp(locale)) symbol=get_currency_symbol(locale) fstr=\"{:,.\" +dp +\"f}\" if includeSymbol: if get_currency_prefix(locale)==CURRENCY_PREFIX: fstr=symbol +fstr else: fstr +=symbol if f < 0: f=abs(f) fstr=\"(\" +fstr +\")\" s=fstr.format(f) s=s.replace(\",\", \"GRP\").replace(\".\", \"RDX\") s=s.replace(\"GRP\", get_currency_digit_grouping(locale)) s=s.replace(\"RDX\", get_currency_radix(locale)) return s def format_currency_no_symbol(locale, value): \"\"\" Formats a currency value, but leaves off the currency symbol \"\"\" return format_currency(locale, value, includeSymbol=False) def format_time(d, timeformat=\"%H:%M:%S\"): if d is None: return \"\" return time.strftime(timeformat, d.timetuple()) def format_time_now(offset=0.0): return format_time(now(offset)) def http_date(dt): \"\"\" Formats a UTC python date\/time in HTTP(RFC1123) format \"\"\" weekday=[\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"][dt.weekday()] month=[\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"][dt.month -1] return \"%s, %02d %s %04d %02d:%02d:%02d GMT\" %(weekday, dt.day, month, dt.year, dt.hour, dt.minute, dt.second) def python2display(locale, d): \"\"\" Formats a python date as a display string. 'd' is a Python date, return value is a display string. \"\"\" if d is None: return \"\" try: return time.strftime(get_display_date_format(locale), d.timetuple()) except: return \"\" def python2unix(d): \"\"\" Converts a python date to unix. \"\"\" try: return time.mktime(d.timetuple()) except: return 0 def format_date(d, dateformat=\"%Y-%m-%d\"): \"\"\" Formats a python date to the format given(strftime rules) \"\"\" if d is None: return \"\" try: return time.strftime(dateformat, d.timetuple()) except: return \"\" def display2python(locale, d): \"\"\" Parses a display string back to python format. Can cope with 2 or 4 digit years. 'd' is a string. return value is the date or None if it could not be parsed. If an ISO date YYYY-MM-DD is passed by mistake, it will cope with that too(handy for posts from html5 date input) \"\"\" if d is None: return None if len(d)==10 and d[4]==\"-\" and d[7]==\"-\": return datetime.datetime.strptime(d, \"%Y-%m-%d\") try: return datetime.datetime.strptime(d, get_display_date_format(locale, 2)) except: try: return datetime.datetime.strptime(d, get_display_date_format(locale, 4)) except: return None def parse_date(dateformat, d): \"\"\" Parses a python date from the dateformat given \"\"\" try: return datetime.datetime.strptime(d, dateformat) except: return None def parse_time(d, t): \"\"\" Parses the time t and combines it with python date d \"\"\" def cint(s): try: return int(s) except: return 0 if d is None: return None tbits=t.split(\":\") hour=0 minute=0 second=0 if len(tbits) > 0: hour=cint(tbits[0]) if len(tbits) > 1: minute=cint(tbits[1]) if len(tbits) > 2: second=cint(tbits[2]) t=datetime.time(hour, minute, second) d=d.combine(d, t) return d def yes_no(l, condition): if condition: return _(\"Yes\", l) else: return _(\"No\", l) def yes_no_unknown(l, v): if v==0: return _(\"Yes\", l) elif v==1: return _(\"No\", l) else: return _(\"Unknown\", l) def yes_no_unknown_blank(l, v): if v==0: return _(\"Yes\", l) elif v==1: return _(\"No\", l) else: return \"\" def add_months(date, months=1): \"\"\" Adds calendar months to a date, returning a new datetime \"\"\" newmonth=((( date.month -1) +months) % 12) +1 newyear =date.year +((( date.month -1) +months) \/\/ 12) try: return datetime.datetime( newyear, newmonth, date.day) except: return datetime.datetime( newyear, newmonth, 28) def add_years(date, years=1.0): \"\"\" Adds years to a date, returning a new datetime \"\"\" if date is None: return None if date.day==29 and date.month==2: return date +datetime.timedelta(days=int(years * 365.0)) if years==int(years): return date.replace( year=date.year +int(years)) return date +datetime.timedelta(days=int(years * 365.0)) def add_days(date, nodays=1): \"\"\" Adds days to a date, returning a new datetime \"\"\" if date is None: return None return date +datetime.timedelta(days=nodays) def add_hours(date, nohours=1): \"\"\" Add hours to date, returning a new datetime \"\"\" if date is None: return None return date +datetime.timedelta(hours=nohours) def add_minutes(date, nomins=1): \"\"\" Add mins to date, returning a new datetime \"\"\" if date is None: return None return date +datetime.timedelta(minutes=nomins) def add_seconds(date, nosecs=1): \"\"\" Add secs to date, returning a new datetime \"\"\" if date is None: return None return date +datetime.timedelta(seconds=nosecs) def subtract_seconds(date, nosecs=1): \"\"\" Subtract seconds from date, returning a new datetime \"\"\" if date is None: return None return date -datetime.timedelta(seconds=nosecs) def subtract_minutes(date, nomins=1): \"\"\" Subtract minutes from date, returning a new datetime \"\"\" if date is None: return None return date -datetime.timedelta(minutes=nomins) def subtract_hours(date, nohours=1): \"\"\" Subtract hours from date, returning a new datetime \"\"\" if date is None: return None return date -datetime.timedelta(hours=nohours) def subtract_days(date, nodays=1): \"\"\" Subtract days from date, returning a new datetime \"\"\" if date is None: return None return date -datetime.timedelta(days=nodays) def subtract_years(date, years=1.0): \"\"\" Subtracts years from date, returning a new datetime \"\"\" if date is None: return None if date.day==29 and date.month==2: return date -datetime.timedelta(days=int(years * 365.0)) if years==int(years): return date.replace( year=date.year -int(years)) return date -datetime.timedelta(days=int(years * 365.0)) def subtract_months(date, months=1): \"\"\" Subtracts months from a date. Will not work after 11 months. \"\"\" def subtract_one_month(t): one_day=datetime.timedelta(days=1) one_month_earlier=t -one_day while one_month_earlier.month==t.month or one_month_earlier.day > t.day: one_month_earlier -=one_day return one_month_earlier for dummy in range(0, months): date=subtract_one_month(date) return date def monday_of_week(date): \"\"\" Returns the monday of the current week of date. \"\"\" if date is None: return None while True: if date.weekday()==0: return date date=subtract_days(date, 1) def sunday_of_week(date): \"\"\" Returns the sunday of the current week of date. \"\"\" if date is None: return None while True: if date.weekday()==6: return date date=add_days(date, 1) def first_of_month(date): \"\"\" Returns the first of the current month of date. \"\"\" return date.replace(day=1) def first_of_year(date): \"\"\" Returns the first of the current year. \"\"\" return date.replace(day=1, month=1) def last_of_month(date): \"\"\" Returns the last of the current month of date. \"\"\" date=add_months(date, 1) date=first_of_month(date) return subtract_days(date, 1) def last_of_year(date): \"\"\" Returns the last of the current year of date. \"\"\" date=add_years(date, 1) date=first_of_year(date) return subtract_days(date, 1) def after(date1, date2): \"\"\" returns true if date1 is after date2 \"\"\" return date_diff_days(date1, date2) < 0 def date_diff_days(date1, date2): \"\"\" Returns the difference in days between two dates. It's assumed that date2 > date1. We aren't using subtraction for timedeltas because it doesn't seem to work correctly when subtracting date from datetime(and some items in the database come through as date). Instead, we convert to unix time to calculate. (datetime) date1 (datetime) date2 \"\"\" if date1 is None or date2 is None: return 0 try: ux1=time.mktime(date1.timetuple()) ux2=time.mktime(date2.timetuple()) delta=int((ux2 -ux1) \/ 60 \/ 60 \/ 24) return delta except: return 0 def date_diff(l, date1, date2): \"\"\" Returns a string representing the difference between two dates. Eg: 6 weeks, 5 months. It is expected that date2 > date1 (datetime) date1 (datetime) date2 \"\"\" days=int(date_diff_days(date1, date2)) return format_diff(l, days) def format_diff(l, days): \"\"\" Returns a formatted diff from a number of days. Eg: 6 weeks, 5 months. \"\"\" if days < 0: days=0 weeks=int(days \/ 7) months=int(days \/ 30.5) years=int(days \/ 365) if days < 7: return ntranslate(days,[ _(\"{plural0} day.\", l), _(\"{plural1} days.\", l), _(\"{plural2} days.\", l), _(\"{plural3} days.\")], l) elif weeks <=26: return ntranslate(weeks,[ _(\"{plural0} week.\", l), _(\"{plural1} weeks.\", l), _(\"{plural2} weeks.\", l), _(\"{plural3} weeks.\")], l) elif days < 365: return ntranslate(months,[ _(\"{plural0} month.\", l), _(\"{plural1} months.\", l), _(\"{plural2} months.\", l), _(\"{plural3} months.\")], l) else: months=int((days % 365) \/ 30.5) return ntranslate(years,[ _(\"{plural0} year.\", l), _(\"{plural1} years.\", l), _(\"{plural2} years.\", l), _(\"{plural3} years.\")], l).replace(\".\", \"\") +\\ \" \" +ntranslate(months,[ _(\"{plural0} month.\", l), _(\"{plural1} months.\", l), _(\"{plural2} months.\", l), _(\"{plural3} months.\")], l) def parse_dst(c): \"\"\" Parses dst code c and returns values for day of week, start offset in month, start month, end offset in month, end month \"\"\" dow, start, end=c.split(\"-\") return(int(dow), start[0], int(start[1:]), end[0], int(end[1:])) def dst_find_day(dow, x, month, yearoffset=0): \"\"\" Finds the xth dow in month this year. dow: int day of the week(0=mon, 6=sun) x: which day to find, 1-5 or L for the last dow in the month month: int the month we're looking in yearoffset: add to the current year \"\"\" d=today() if yearoffset > 0: d=add_years(d, yearoffset) d=d.replace(month=month, day=1) if x==\"L\": d=last_of_month(d) while d.day > 20: if d.weekday()==dow: break d=subtract_days(d, 1) else: found=0 while d.month==month: if d.weekday()==dow: found +=1 if found==int(x): break d=add_days(d, 1) return d def dst_start_date(l): \"\"\" Calculates the dst start date\/time for locale \"\"\" dow, so, sm, dummy, dummy=parse_dst(get_dst(l)) return dst_find_day( dow, so, sm) def dst_end_date(l): \"\"\" Calculates the dst end date\/time for locale \"\"\" dow, dummy, sm, eo, em=parse_dst(get_dst(l)) return dst_find_day(dow, eo, em, sm > em and 1 or 0) def dst_adjust(l, offset=0.0): \"\"\" Returns 1 if locale l is currently in daylight savings time. offset: Included so that the time now without dst can be calculated first to decide whether or not we're in dst(otherwise we could be wrong for a day) You can add this call to a timezone offset to get the correct adjustment. \"\"\" c=get_dst(l) if c==\"\": return 0 d=now(offset) dsts=dst_start_date(l) dste=dst_end_date(l) if d >=dsts and d <=dste: return 1 return 0 def now(offset=0.0): \"\"\" Returns a python date representing now offset: A UTC offset to apply in hours \"\"\" if offset < 0: return datetime.datetime.now() -datetime.timedelta(hours=abs(offset)) else: return datetime.datetime.now() +datetime.timedelta(hours=offset) def today(): \"\"\" Returns a python datetime set to today, but with time information at midnight. \"\"\" d=datetime.datetime.now() return datetime.datetime(d.year, d.month, d.day) ","sourceWithComments":"\nimport datetime\nimport json\nimport time\n\n# flake8: noqa - we have a lot of locales and this is convenient\nfrom asm3.locales import *\n\nVERSION = \"44u [Thu  8 Oct 10:18:16 BST 2020]\"\nBUILD = \"10081018\"\n\nDMY = ( \"%d\/%m\/%Y\", \"%d\/%m\/%y\" )\nHDMY = ( \"%d-%m-%Y\", \"%d-%m-%y\" )\nDDMY = ( \"%d.%m.%Y\", \"%d.%m.%y\" )\nMDY = ( \"%m\/%d\/%Y\", \"%m\/%d\/%y\" )\nYMD = ( \"%Y\/%m\/%d\", \"%y\/%m\/%d\" )\nDYMD = ( \"%Y.%m.%d\", \"%y.%m.%d\" )\nHYMD = ( \"%Y-%m-%d\", \"%y-%m-%d\" )\nDOLLAR = \"$\"\nEURO = \"&#x20ac;\"\nPOUND = \"&pound;\"\nYEN = \"&yen;\"\nCURRENCY_PREFIX = \"p\"\nCURRENCY_SUFFIX = \"s\"\nDST_US = \"6-203-111\"\nDST_UK = \"6-L03-L10\"\nDST_AU = \"6-110-104\"\n\ndef PLURAL_ENGLISH(n):\n    \"\"\" gettext plural function for English\/Latin languages \"\"\"\n    if n == 1: return 0\n    return 1\n\ndef PLURAL_HUNGARIAN(n):\n    \"\"\" gettext style plural function for Hungarian \n        Hungarian always uses the singular unless the element appears\n        by itself (which it never does for the purposes of ngettext)\n        so always return the singular\n    \"\"\"\n    return 0\n\ndef PLURAL_POLISH(n):\n    \"\"\" gettext style plural function for Polish \"\"\"\n    if n == 1: return 0\n    if n % 10 >= 2 and n % 10 <= 4 and (n % 100 < 10 or n % 100 >= 20): return 1\n    return 2\n\ndef PLURAL_SLAVIC(n):\n    \"\"\" gettext style plural function for Slavic languages,\n        Russian, Ukrainian, Belarusian, Serbian, Croatian\n    \"\"\"\n    if n % 10 == 1 and n % 100 != 11: return 0\n    if n % 10 >= 2 and n % 10 <= 4 and (n % 100 < 10 or n % 100 >= 20): return 1\n    return 2\n\n# Maps of locale to currency\/date format - this is a map of lists instead of maps\n# to try and keep things readable and on one line\nLM_LANGUAGE = 0\nLM_COUNTRY = 1\nLM_DATEFORMAT = 2\nLM_CURRENCY_SYMBOL = 3\nLM_PLURAL_FUNCTION = 4\nLM_CURRENCY_POSITION = 5\nLM_CURRENCY_DECIMAL_PLACES = 6\nLM_CURRENCY_DECIMAL_MARK = 7\nLM_CURRENCY_DIGIT_GROUPING = 8\nLM_DST = 9\nlocale_maps = {\n    \"en\":       ( \"English\", \"United States\", MDY, DOLLAR, PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", DST_US ),\n    \"en_GB\":    ( \"English\", \"Great Britain\", DMY, POUND, PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", DST_UK ),\n    \"en_AU\":    ( \"English\", \"Australia\", DMY, DOLLAR, PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", DST_AU ),\n    \"en_AE\":    ( \"English\", \"United Arab Emirates\", DMY, \"&#x62f;&#x2e;&#x625;\", PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", \"\"),\n    \"en_AW\":    ( \"English\", \"Aruba\", DMY, \"Awg.\", PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", \"\" ),\n    \"en_BE\":    ( \"English\", \"Belgium\", DMY, EURO, PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \",\", \" \", DST_UK ),\n    \"en_BM\":    ( \"English\", \"Bermuda\", DMY, DOLLAR, PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", \"6-203-111\" ),\n    \"en_BG\":    ( \"English\", \"Bulgaria\", HYMD, \"&#x043b;&#x0432;\", PLURAL_ENGLISH, CURRENCY_SUFFIX, 2, \",\", \" \", DST_UK),\n    \"en_BH\":    ( \"English\", \"Bahrain\", MDY, \"BD\", PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", \"\", \"\" ),\n    \"en_BQ\":    ( \"English\", \"Bonaire\", DMY, DOLLAR, PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", \"\" ),\n    \"en_CA\":    ( \"English\", \"Canada\", MDY, DOLLAR, PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", DST_US ),\n    \"en_CH\":    ( \"English\", \"Switzerland\", DDMY, \"CHF\", PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", DST_UK ),\n    \"en_CN\":    ( \"English\", \"China\", HYMD, YEN, PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", \"\" ),\n    \"en_CR\":    ( \"English\", \"Costa Rica\", DMY, \"&#8353;\", PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", \"\" ),\n    \"en_CY\":    ( \"English\", \"Cyprus\", DMY, EURO, PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", DST_UK ),\n    \"en_ES\":    ( \"English\", \"Spain\", DMY, EURO, PLURAL_ENGLISH, CURRENCY_SUFFIX, 2, \",\", \" \", DST_UK ),\n    \"en_HK\":    ( \"English\", \"Hong Kong\", HDMY, DOLLAR, PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", \"\" ),\n    \"en_KH\":    ( \"English\", \"Cambodia\", DMY, DOLLAR, PLURAL_ENGLISH, CURRENCY_SUFFIX, 2, \".\", \",\", \"\" ),\n    \"en_KW\":    ( \"English\", \"Kuwait\", DMY, \"KD\", PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", \"\" ),\n    \"en_KY\":    ( \"English\", \"Cayman Islands\", DMY, DOLLAR, PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", \"\" ),\n    \"en_IE\":    ( \"English\", \"Ireland\", DMY, EURO, PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", DST_UK ),\n    \"en_IN\":    ( \"English\", \"India\", DMY, \"&#8377;\", PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", \"\"),\n    \"en_IL\":    ( \"English\", \"Israel\", DMY, \"&#x20aa;\", PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", DST_UK ),\n    \"en_JP\":    ( \"English\", \"Japan\", YMD, \"&yen;\", PLURAL_ENGLISH, CURRENCY_SUFFIX, 0, \".\", \",\", \"\" ),\n    \"en_LB\":    ( \"English\", \"Lebanon\", MDY, \"L&pound;\", PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", DST_UK ),\n    \"en_LU\":    ( \"English\", \"Luxembourg\", DDMY, EURO, PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", DST_UK ),\n    \"en_MU\":    ( \"English\", \"Mauritius\", DMY, \"&#8360;\", PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", \"\"),\n    \"en_MY\":    ( \"English\", \"Malaysia\", DMY, \"RM\", PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", \"\" ),\n    \"en_MX\":    ( \"English\", \"Mexico\", DMY, DOLLAR, PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", \"6-104-L10\" ),\n    \"en_NA\":    ( \"English\", \"Namibia\", YMD, DOLLAR, PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \",\", \" \", \"\"),\n    \"en_PH\":    ( \"English\", \"Philippines\", MDY, \"&#x20b1;\", PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", \"\"),\n    \"en_QA\":    ( \"English\", \"Qatar\", DMY, \"QR\", PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", \"\"),\n    \"en_NZ\":    ( \"English\", \"New Zealand\", DMY, DOLLAR, PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", \"6-L09-104\" ),\n    \"en_TH\":    ( \"English\", \"Thailand\", DMY, \"&#x0e3f;\", PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", \"\" ),\n    \"en_TW\":    ( \"English\", \"Taiwan\", YMD, DOLLAR, PLURAL_ENGLISH, CURRENCY_PREFIX, 0, \".\", \",\", \"\"),\n    \"en_TW2\":   ( \"English\", \"Taiwan $0.00\", YMD, DOLLAR, PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", \"\" ),\n    \"en_VN\":    ( \"English\", \"Vietnam\", DMY, \"&#8363;\", PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", \"\" ),\n    \"en_ZA\":    ( \"English\", \"South Africa\", YMD, \"R\", PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \",\", \" \", \"\"),\n    \"bg\":       ( \"Bulgarian\", \"Bulgaria\", HYMD, \"&#x043b;&#x0432;\", PLURAL_ENGLISH, CURRENCY_SUFFIX, 2, \",\", \" \", DST_UK),\n    \"bs\":       ( \"Bosnian\", \"Bosnia\", HYMD, \"KM\", PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", DST_UK),\n    \"cs\":       ( \"Czech\", \"Czech Republic\", DYMD, \"&#x004b;&#x010d;\", PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \",\", \" \", DST_UK ),\n    \"de\":       ( \"German\", \"Germany\", DDMY, EURO, PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \",\", \" \", DST_UK ),\n    \"de_AT\":    ( \"German\", \"Austria\", DDMY, EURO, PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", DST_UK ),\n    \"de_CH\":    ( \"German\", \"Switzerland\", DDMY, \"CHF\", PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", DST_UK ),\n    \"de_LU\":    ( \"German\", \"Luxembourg\", DDMY, EURO, PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", DST_UK ),\n    \"el\":       ( \"Greek\", \"Greece\", DMY, EURO, PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \",\", \" \", DST_UK ),\n    \"es\":       ( \"Spanish\", \"Spain\", DMY, EURO, PLURAL_ENGLISH, CURRENCY_SUFFIX, 2, \",\", \" \", DST_UK ),\n    \"es_CO\":    ( \"Spanish\", \"Columbia\", DMY, DOLLAR, PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \",\", \" \", \"\" ),\n    \"es_CR\":    ( \"Spanish\", \"Costa Rica\", DMY, \"&#8353;\", PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \",\", \" \", \"\" ),\n    \"es_EC\":    ( \"Spanish\", \"Ecuador\", DMY, DOLLAR, PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \",\", \" \", \"\" ),\n    \"es_MX\":    ( \"Spanish\", \"Mexico\", DMY, DOLLAR, PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", \"6-104-L10\" ),\n    \"et\":       ( \"Estonian\", \"Estonia\", DMY, \"kr\", PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \",\", \" \", DST_UK ),\n    \"fi\":       ( \"Finnish\", \"Finland\", DDMY, EURO, PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \",\", \" \", DST_UK),\n    \"fr\":       ( \"French\", \"France\", DMY, EURO, PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \",\", \" \", DST_UK ),\n    \"fr_BE\":    ( \"French\", \"Belgium\", DMY, EURO, PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \",\", \" \", DST_UK ),\n    \"fr_LU\":    ( \"French\", \"Luxembourg\", DDMY, EURO, PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", DST_UK ),\n    \"fr_CA\":    ( \"French\", \"Canada\", DMY, DOLLAR, PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \",\", \" \", DST_US ),\n    \"fr_CH\":    ( \"French\", \"Switzerland\", DDMY, \"CHF\", PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", DST_UK ),\n    \"he\":       ( \"Hebrew\", \"Israel\", DMY, \"&#x20aa;\", PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", DST_UK ),\n    \"hu\":       ( \"Hungarian\", \"Hungary\", DYMD, \"Ft\",  PLURAL_HUNGARIAN, CURRENCY_PREFIX, 2, \",\", \" \", DST_UK), \n    \"it\":       ( \"Italian\", \"Italy\", DMY, EURO, PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \",\", \" \", DST_UK ),\n    \"it_CH\":    ( \"Italian\", \"Switzerland\", DDMY, \"CHF\", PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", DST_UK ),\n    \"lt\":       ( \"Lithuanian\", \"Lithuania\", DYMD, EURO, PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \",\", \" \", DST_UK ),\n    \"nb\":       ( \"Norwegian Bokmal\", \"Norway\", DDMY, \"kr\", PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \",\", \" \", DST_UK ),\n    \"nl\":       ( \"Dutch\", \"Holland\", HDMY, EURO, PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \",\", \" \", DST_UK ),\n    \"nl_AW\":    ( \"Dutch\", \"Aruba\", DMY, \"Awg.\", PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", \"\" ),\n    \"nl_BE\":    ( \"Dutch\", \"Belgium\", DMY, EURO, PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \",\", \" \", DST_UK ),\n    \"nl_BQ\":    ( \"Dutch\", \"Bonaire\", DMY, DOLLAR, PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", \"\" ),\n    \"pl\":       ( \"Polish\", \"Poland\", DDMY, \"&#x007a;&#x0142;\", PLURAL_POLISH, CURRENCY_PREFIX, 2, \",\", \" \", DST_UK ),\n    \"pt\":       ( \"Portuguese\", \"Portugal\", HDMY, EURO, PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \",\", \" \", DST_UK ),\n    \"ru\":       ( \"Russian\", \"Russia\", DDMY, \"&#1056;&#1059;&#1041;.\", PLURAL_SLAVIC, CURRENCY_PREFIX, 2, \",\", \" \", \"\" ),\n    \"sk\":       ( \"Slovakian\", \"Slovakia\", DDMY, EURO, PLURAL_SLAVIC, CURRENCY_PREFIX, 2, \",\", \" \", DST_UK ),\n    \"sl\":       ( \"Slovenian\", \"Slovenia\", DDMY, EURO, PLURAL_SLAVIC, CURRENCY_PREFIX, 2, \",\", \" \", DST_UK ),\n    \"sv\":       ( \"Swedish\", \"Sweden\", HYMD, \"kr\", PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \",\", \" \", DST_UK ),\n    \"th\":       ( \"Thai\", \"Thailand\", DMY, \"&#x0e3f;\", PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \".\", \",\", \"\" ),\n    \"tr\":       ( \"Turkish\", \"Turkey\", DDMY, \"TL\", PLURAL_ENGLISH, CURRENCY_PREFIX, 2, \",\", \" \", \"\" )\n}\n\ndef _(english, locale = \"en\"):\n    return translate(english, locale)\n\ndef real_locale(locale = \"en\"):\n    # When translating text strings, treat some locales as pointers \n    # to other locales without the need for a full translation:\n    # Our core English locales (with actual differences) are:\n    #   en    (US)\n    #   en_AU (Australia)\n    #   en_CA (Canada)\n    #   en_GB (UK)\n    if locale in (\"en_AE\", \"en_BE\", \"en_BG\", \"en_BM\", \"en_BQ\", \"en_CH\", \"en_CN\", \"en_CY\", \"en_ES\", \n        \"en_HK\", \"en_IE\", \"en_IN\", \"en_JP\", \"en_KH\", \"en_LB\", \"en_LU\", \"en_MU\", \"en_MY\", \"en_NA\", \n        \"en_PH\", \"en_QA\", \"en_TH\", \"en_TW\", \"en_TW2\", \"en_VN\", \"en_ZA\"):\n        locale = \"en_GB\"\n    if locale in (\"en_AW\", \"en_BH\", \"en_CO\", \"en_CR\", \"en_KW\", \"en_KY\", \"en_IL\", \"en_LB\", \n        \"en_MX\"):\n        locale = \"en\"\n    if locale in (\"en_NZ\",):\n        locale = \"en_AU\"\n    # Dutch locales\n    if locale in (\"nl_AW\", \"nl_BE\", \"nl_BQ\"):\n        locale = \"nl\"\n    # French locales\n    if locale in (\"fr_BE\", \"fr_CH\", \"fr_LU\"):\n        locale = \"fr\"\n    # German locales\n    if locale in (\"de_AT\", \"de_CH\", \"de_LU\"):\n        locale = \"de\"\n    # Italian locales\n    if locale in (\"it_CH\",):\n        locale = \"it\"\n    # Spanish locales\n    if locale in (\"es_CO\", \"es_CR\", \"es_EC\", \"es_MX\"):\n        locale = \"es\"\n    return locale\n\ndef translate(english, locale = \"en\"):\n    \"\"\"\n    Returns a translation string for an English phrase in\n    the locale given.\n    \"\"\"\n    locale = real_locale(locale)\n\n    # If we're dealing with English, then just\n    # return the English phrase. I hate that I'm doing\n    # this, but I'm going with the accepted standard of\n    # US English being default even though we invented\n    # the bloody language.\n    if locale == \"en\":\n        return english\n\n    # Otherwise, look up the phrase in the correct\n    # module for our locale.\n    try:\n        lang = globals()[\"locale_\" + locale]\n    except:\n        # The module doesn't exist for the locale, fall\n        # back to plain English translation\n        return english\n\n    # If the string isn't in our locale dictionary, fall back to English\n    if english not in lang.val: return english\n\n    # If the value hasn't been translated, fall back to English\n    s = lang.val[english]\n    if s is None or s == \"\" or s.startswith(\"??\") or s.startswith(\"(??\"):\n        return english\n    else:\n        return lang.val[english]\n\ndef ntranslate(number, translations, locale = \"en\"):\n    \"\"\" Translates a phrase that deals with a number of something\n        so the correct plural can be used. \n        number: The number of items\n        translations: A list of already translated strings for each plural form\n        locale: The locale the strings are in (which plural function to use)\n    \"\"\"\n    try:\n        pluralfun = get_plural_function(locale)\n        text = translations[pluralfun(number)]\n        text = text.replace(\"{plural0}\", str(number))\n        text = text.replace(\"{plural1}\", str(number))\n        text = text.replace(\"{plural2}\", str(number))\n        text = text.replace(\"{plural3}\", str(number))\n        text = text.replace(\"{plural4}\", str(number))\n        return text\n    except Exception as e:\n        return e\n\ndef get_version():\n    \"\"\"\n    Returns the version of ASM\n    \"\"\"\n    return VERSION\n\ndef get_version_number():\n    \"\"\"\n    Returns the version number of ASM\n    \"\"\"\n    return VERSION[0:VERSION.find(\" \")]\n\ndef get_locale_map(locale, index):\n    if locale in locale_maps:\n        return locale_maps[locale][index]\n    else:\n        return locale_maps[\"en\"][index]\n\ndef get_locales():\n    locales = []\n    # Build a list of locale, display name\n    for k, v in locale_maps.items():\n        if k.find(\"_\") != -1 and k != \"en\":\n            locales.append( (k, \"%s (%s)\" % (v[LM_LANGUAGE], v[LM_COUNTRY])) )\n        else:\n            locales.append( (k, \"%s\" % v[LM_LANGUAGE]) )\n    # Sort on display name alphabetically\n    locales = sorted(locales, key=lambda x: x[1])\n    return locales\n\ndef get_country(locale):\n    return get_locale_map(locale, LM_COUNTRY)\n\ndef get_language(locale):\n    return get_locale_map(locale, LM_LANGUAGE)\n\ndef get_display_date_format(locale, digitsinyear = 4):\n    \"\"\"\n    Returns the display date format for a locale\n    \"\"\"\n    if digitsinyear == 4:\n        return get_locale_map(locale, LM_DATEFORMAT)[0]\n    else:\n        return get_locale_map(locale, LM_DATEFORMAT)[1]\n\ndef get_currency_symbol(locale):\n    \"\"\"\n    Returns the currency symbol for a locale\n    \"\"\"\n    return get_locale_map(locale, LM_CURRENCY_SYMBOL)\n\ndef get_currency_prefix(locale):\n    \"\"\"\n    Returns \"p\" if the currency symbol goes at the beginning, or \"s\" for the end\n    when displaying.\n    \"\"\"\n    return get_locale_map(locale, LM_CURRENCY_POSITION)\n\ndef get_currency_dp(locale):\n    \"\"\"\n    Returns the number of decimal places for a locale when\n    displaying currency\n    \"\"\"\n    return get_locale_map(locale, LM_CURRENCY_DECIMAL_PLACES)\n\ndef get_currency_radix(locale):\n    \"\"\"\n    Returns the decimal mark symbol\n    \"\"\"\n    return get_locale_map(locale, LM_CURRENCY_DECIMAL_MARK)\n\ndef get_currency_digit_grouping(locale):\n    \"\"\"\n    Returns the character used to separate thousands\n    \"\"\"\n    return get_locale_map(locale, LM_CURRENCY_DIGIT_GROUPING)\n\ndef get_dst(locale):\n    \"\"\"\n    Returns the daylight savings time info for locale\n    \"\"\"\n    return get_locale_map(locale, LM_DST)\n\ndef get_plural_function(locale):\n    \"\"\"\n    Returns the function for calculating plurals for this locale\n    \"\"\"\n    return get_locale_map(locale, LM_PLURAL_FUNCTION)\n\ndef format_currency(locale, value, includeSymbol = True):\n    \"\"\"\n    Formats a currency value to the correct number of \n    decimal places and returns it as a string\n    \"\"\"\n    if value is None: value = 0\n    i = 0\n    f = 0.0\n    try:\n        i = int(value)\n        f = float(i)\n    except:\n        pass\n    f = f \/ 100\n    dp = str(get_currency_dp(locale))\n    symbol = get_currency_symbol(locale)\n    # Start with a basic currency format with comma groupings every 3 digits\n    # and the right number of decimal places for the locale\n    fstr = \"{:,.\" + dp + \"f}\"\n    # Add the currency symbol to the format in the correct spot\n    if includeSymbol:\n        if get_currency_prefix(locale) == CURRENCY_PREFIX:\n            fstr = symbol + fstr\n        else:\n            fstr += symbol\n    # If it's negative, wrap brackets around the format\n    if f < 0: \n        f = abs(f)\n        fstr = \"(\" + fstr + \")\"\n    # Do the format to get our value\n    s = fstr.format(f)\n    # Substitute the grouping and radix symbols based on locale\n    s = s.replace(\",\", \"GRP\").replace(\".\", \"RDX\")\n    s = s.replace(\"GRP\", get_currency_digit_grouping(locale))\n    s = s.replace(\"RDX\", get_currency_radix(locale))\n    return s\n\ndef format_currency_no_symbol(locale, value):\n    \"\"\" \n    Formats a currency value, but leaves off the currency symbol\n    \"\"\"\n    return format_currency(locale, value, includeSymbol = False)\n\ndef format_time(d, timeformat=\"%H:%M:%S\"):\n    if d is None: return \"\"\n    return time.strftime(timeformat, d.timetuple())\n\ndef format_time_now(offset = 0.0):\n    return format_time(now(offset))\n\ndef http_date(dt):\n    \"\"\"\n    Formats a UTC python date\/time in HTTP (RFC1123) format\n    \"\"\"\n    weekday = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"][dt.weekday()]\n    month = [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\",\n             \"Oct\", \"Nov\", \"Dec\"][dt.month - 1]\n    return \"%s, %02d %s %04d %02d:%02d:%02d GMT\" % (weekday, dt.day, month,\n        dt.year, dt.hour, dt.minute, dt.second)\n\ndef python2display(locale, d):\n    \"\"\"\n    Formats a python date as a display string. 'd' is\n    a Python date, return value is a display string.\n    \"\"\"\n    if d is None: return \"\"\n    try:\n        return time.strftime(get_display_date_format(locale), d.timetuple())\n    except:\n        return \"\"\n\ndef python2unix(d):\n    \"\"\"\n    Converts a python date to unix.\n    \"\"\"\n    try:\n        return time.mktime(d.timetuple())\n    except:\n        return 0\n\ndef format_date(d, dateformat=\"%Y-%m-%d\"):\n    \"\"\"\n    Formats a python date to the format given (strftime rules)\n    \"\"\"\n    if d is None: return \"\"\n    try:\n        return time.strftime(dateformat, d.timetuple())\n    except:\n        return \"\"\n\ndef display2python(locale, d):\n    \"\"\"\n    Parses a display string back to python format. Can cope with\n    2 or 4 digit years.\n    'd' is a string. return value is the date or None if it\n    could not be parsed.\n    If an ISO date YYYY-MM-DD is passed by mistake, it will cope with that too (handy for posts from html5 date input)\n    \"\"\"\n    if d is None: return None\n    if len(d) == 10 and d[4] == \"-\" and d[7] == \"-\": return datetime.datetime.strptime(d, \"%Y-%m-%d\")\n    try:\n        return datetime.datetime.strptime(d, get_display_date_format(locale, 2))\n    except:\n        try:\n            return datetime.datetime.strptime(d, get_display_date_format(locale, 4))\n        except:\n            return None\n\ndef parse_date(dateformat, d):\n    \"\"\"\n    Parses a python date from the dateformat given\n    \"\"\"\n    try:\n        return datetime.datetime.strptime(d, dateformat)\n    except:\n        return None\n\ndef parse_time(d, t):\n    \"\"\"\n    Parses the time t and combines it with python date d\n    \"\"\"\n    def cint(s):\n        try:\n            return int(s)\n        except:\n            return 0\n    if d is None: return None\n    tbits = t.split(\":\")\n    hour = 0\n    minute = 0\n    second = 0\n    if len(tbits) > 0: hour = cint(tbits[0])\n    if len(tbits) > 1: minute = cint(tbits[1])\n    if len(tbits) > 2: second = cint(tbits[2])\n    t = datetime.time(hour, minute, second)\n    d = d.combine(d, t)\n    return d\n\ndef yes_no(l, condition):\n    if condition:\n        return _(\"Yes\", l)\n    else:\n        return _(\"No\", l)\n\ndef yes_no_unknown(l, v):\n    if v == 0: return _(\"Yes\", l)\n    elif v == 1: return _(\"No\", l)\n    else: return _(\"Unknown\", l)\n\ndef yes_no_unknown_blank(l, v):\n    if v == 0: return _(\"Yes\", l)\n    elif v == 1: return _(\"No\", l)\n    else: return \"\"\n\ndef add_months(date, months = 1):\n    \"\"\"\n    Adds calendar months to a date, returning a new datetime\n    \"\"\"\n    newmonth = ((( date.month - 1) + months ) % 12 ) + 1\n    newyear  = date.year + ((( date.month - 1) + months ) \/\/ 12 )\n    try:\n        return datetime.datetime( newyear, newmonth, date.day )\n    except:\n        return datetime.datetime( newyear, newmonth, 28 )\n\ndef add_years(date, years = 1.0):\n    \"\"\"\n    Adds years to a date, returning a new datetime\n    \"\"\"\n    if date is None: return None\n    if date.day == 29 and date.month == 2: return date + datetime.timedelta(days = int(years * 365.0)) # Leap years break calendar years\n    if years == int(years): return date.replace( year = date.year + int(years))\n    return date + datetime.timedelta(days = int(years * 365.0))\n\ndef add_days(date, nodays = 1):\n    \"\"\"\n    Adds days to a date, returning a new datetime\n    \"\"\"\n    if date is None: return None\n    return date + datetime.timedelta(days = nodays)\n\ndef add_hours(date, nohours = 1):\n    \"\"\"\n    Add hours to date, returning a new datetime\n    \"\"\"\n    if date is None: return None\n    return date + datetime.timedelta(hours = nohours)\n\ndef add_minutes(date, nomins = 1):\n    \"\"\"\n    Add mins to date, returning a new datetime\n    \"\"\"\n    if date is None: return None\n    return date + datetime.timedelta(minutes = nomins)\n\ndef add_seconds(date, nosecs = 1):\n    \"\"\"\n    Add secs to date, returning a new datetime\n    \"\"\"\n    if date is None: return None\n    return date + datetime.timedelta(seconds = nosecs)\n\ndef subtract_seconds(date, nosecs = 1):\n    \"\"\"\n    Subtract seconds from date, returning a new datetime\n    \"\"\"\n    if date is None: return None\n    return date - datetime.timedelta(seconds = nosecs)\n\ndef subtract_minutes(date, nomins = 1):\n    \"\"\"\n    Subtract minutes from date, returning a new datetime\n    \"\"\"\n    if date is None: return None\n    return date - datetime.timedelta(minutes = nomins)\n\ndef subtract_hours(date, nohours = 1):\n    \"\"\"\n    Subtract hours from date, returning a new datetime\n    \"\"\"\n    if date is None: return None\n    return date - datetime.timedelta(hours = nohours)\n\ndef subtract_days(date, nodays = 1):\n    \"\"\"\n    Subtract days from date, returning a new datetime\n    \"\"\"\n    if date is None: return None\n    return date - datetime.timedelta(days = nodays)\n\ndef subtract_years(date, years = 1.0):\n    \"\"\"\n    Subtracts years from date, returning a new datetime\n    \"\"\"\n    if date is None: return None\n    if date.day == 29 and date.month == 2: return date - datetime.timedelta(days = int(years * 365.0)) # Leap years break calendar years\n    if years == int(years): return date.replace( year = date.year - int(years)) # Go back a calendar year if it's a whole year\n    return date - datetime.timedelta(days = int(years * 365.0))\n\ndef subtract_months(date, months = 1):\n    \"\"\"\n    Subtracts months from a date. Will not work after 11 months.\n    \"\"\"\n    def subtract_one_month(t):\n        one_day = datetime.timedelta(days=1)\n        one_month_earlier = t - one_day\n        while one_month_earlier.month == t.month or one_month_earlier.day > t.day:\n            one_month_earlier -= one_day\n        return one_month_earlier\n    for dummy in range(0, months):\n        date = subtract_one_month(date)\n    return date\n    #year, month = divmod(months, 12)\n    #if date.month <= month:\n    #    year = date.year - 1\n    #    month = date.month - month + 12\n    #else:\n    #    year = date.year \n    #    month = date.month - month\n    #return date.replace(year = year, month = month)\n\ndef monday_of_week(date):\n    \"\"\"\n    Returns the monday of the current week of date.\n    \"\"\"\n    if date is None: return None\n    while True:\n        if date.weekday() == 0:\n            return date\n        date = subtract_days(date, 1)\n\ndef sunday_of_week(date):\n    \"\"\"\n    Returns the sunday of the current week of date.\n    \"\"\"\n    if date is None: return None\n    while True:\n        if date.weekday() == 6:\n            return date\n        date = add_days(date, 1)\n\ndef first_of_month(date):\n    \"\"\"\n    Returns the first of the current month of date.\n    \"\"\"\n    return date.replace(day = 1)\n\ndef first_of_year(date):\n    \"\"\"\n    Returns the first of the current year.\n    \"\"\"\n    return date.replace(day = 1, month = 1)\n\ndef last_of_month(date):\n    \"\"\"\n    Returns the last of the current month of date.\n    \"\"\"\n    date = add_months(date, 1)\n    date = first_of_month(date)\n    return subtract_days(date, 1)\n\ndef last_of_year(date):\n    \"\"\"\n    Returns the last of the current year of date.\n    \"\"\"\n    date = add_years(date, 1)\n    date = first_of_year(date)\n    return subtract_days(date, 1)\n\ndef after(date1, date2):\n    \"\"\"\n    returns true if date1 is after date2\n    \"\"\"\n    return date_diff_days(date1, date2) < 0\n\ndef date_diff_days(date1, date2):\n    \"\"\"\n    Returns the difference in days between two dates. It's\n    assumed that date2 > date1. We aren't using subtraction\n    for timedeltas because it doesn't seem to work correctly\n    when subtracting date from datetime (and some items\n    in the database come through as date). Instead, we convert\n    to unix time to calculate.\n    (datetime) date1\n    (datetime) date2\n    \"\"\"\n    if date1 is None or date2 is None: return 0\n    try:\n        ux1 = time.mktime(date1.timetuple())\n        ux2 = time.mktime(date2.timetuple())\n        delta = int((ux2 - ux1) \/ 60 \/ 60 \/ 24)\n        return delta\n    except:\n        return 0\n\ndef date_diff(l, date1, date2):\n    \"\"\"\n    Returns a string representing the difference between two\n    dates. Eg: 6 weeks, 5 months.\n    It is expected that date2 > date1\n    (datetime) date1\n    (datetime) date2\n    \"\"\"\n    days = int(date_diff_days(date1, date2))\n    return format_diff(l, days)\n\ndef format_diff(l, days):\n    \"\"\"\n    Returns a formatted diff from a number of days.\n    Eg: 6 weeks, 5 months.\n    \"\"\"\n    if days < 0: days = 0\n    weeks = int(days \/ 7)\n    months = int(days \/ 30.5)\n    years = int(days \/ 365)\n   \n    # If it's less than a week, show as days\n    if days < 7:\n        return ntranslate(days, [ _(\"{plural0} day.\", l), _(\"{plural1} days.\", l), _(\"{plural2} days.\", l), _(\"{plural3} days.\")], l)\n    # If it's 26 weeks or less, show as weeks\n    elif weeks <= 26:\n        return ntranslate(weeks, [ _(\"{plural0} week.\", l), _(\"{plural1} weeks.\", l), _(\"{plural2} weeks.\", l), _(\"{plural3} weeks.\")], l)\n    # If it's less than a year, show as months\n    elif days < 365:\n        return ntranslate(months, [ _(\"{plural0} month.\", l), _(\"{plural1} months.\", l), _(\"{plural2} months.\", l), _(\"{plural3} months.\")], l)\n    else:\n        # Show as years and months\n        months = int((days % 365) \/ 30.5)\n        return ntranslate(years, [ _(\"{plural0} year.\", l), _(\"{plural1} years.\", l), _(\"{plural2} years.\", l), _(\"{plural3} years.\")], l).replace(\".\", \"\") + \\\n            \" \" + ntranslate(months, [ _(\"{plural0} month.\", l), _(\"{plural1} months.\", l), _(\"{plural2} months.\", l), _(\"{plural3} months.\")], l)\n\ndef parse_dst(c):\n    \"\"\"\n    Parses dst code c and returns values for day of week,\n    start offset in month, start month, end offset in month, end month\n    \"\"\"\n    dow, start, end = c.split(\"-\")\n    return (int(dow), start[0], int(start[1:]), end[0], int(end[1:]))\n\ndef dst_find_day(dow, x, month, yearoffset=0):\n    \"\"\" \n    Finds the xth dow in month this year.\n    dow: int day of the week (0 = mon, 6 = sun)\n    x: which day to find, 1-5 or L for the last dow in the month\n    month: int the month we're looking in\n    yearoffset: add to the current year\n    \"\"\"\n    d = today()\n    if yearoffset > 0: d = add_years(d, yearoffset)\n    d = d.replace(month=month, day=1)\n    if x == \"L\":\n        # Looking for the last dow in month\n        d = last_of_month(d)\n        while d.day > 20:\n            if d.weekday() == dow: break\n            d = subtract_days(d, 1)\n    else:\n        # Looking for X dow in month\n        found = 0\n        while d.month == month:\n            if d.weekday() == dow: found += 1\n            if found == int(x): break\n            d = add_days(d, 1)\n    return d\n\ndef dst_start_date(l):\n    \"\"\" Calculates the dst start date\/time for locale \"\"\"\n    dow, so, sm, dummy, dummy = parse_dst(get_dst(l))\n    return dst_find_day( dow, so, sm)\n\ndef dst_end_date(l):\n    \"\"\" Calculates the dst end date\/time for locale \"\"\"\n    dow, dummy, sm, eo, em = parse_dst(get_dst(l))\n    return dst_find_day(dow, eo, em, sm > em and 1 or 0) # set 1 year offset if start month is later than end month\n\ndef dst_adjust(l, offset = 0.0):\n    \"\"\"\n    Returns 1 if locale l is currently in daylight savings time.\n    offset: Included so that the time now without dst can be calculated first to\n            decide whether or not we're in dst (otherwise we could be wrong for a day)\n    You can add this call to a timezone offset to get the correct adjustment.\n    \"\"\"\n    c = get_dst(l)\n    if c == \"\": return 0 # No dst information for this locale, no adjustment\n    d = now(offset)\n    dsts = dst_start_date(l)\n    dste = dst_end_date(l)\n    if d >= dsts and d <= dste: return 1 # we're in dst\n    return 0\n\ndef now(offset = 0.0):\n    \"\"\"\n    Returns a python date representing now\n    offset: A UTC offset to apply in hours\n    \"\"\"\n    if offset < 0:\n        return datetime.datetime.now() - datetime.timedelta(hours = abs(offset))\n    else:\n        return datetime.datetime.now() + datetime.timedelta(hours = offset)\n\ndef today():\n    \"\"\"\n    Returns a python datetime set to today, but with time information at midnight.\n    \"\"\"\n    d = datetime.datetime.now()\n    return datetime.datetime(d.year, d.month, d.day)\n\n"},"\/src\/code.py":{"changes":[{"diff":"\n         self.content_type(\"text\/plain\")\n         return self.exec_sql_from_file(o.dbo, o.user, sql)\n \n+    def check_update_query(self, q):\n+        \"\"\" Prevent any kind of update to certain tables to prevent\n+            more savvy malicious users tampering via SQL Interface.\n+            q is already stripped and converted to lower case by the exec_sql caller.\n+            If one of our tamper proofed tables is touched, an Exception is raised\n+            and the query not run.\n+        \"\"\"\n+        for t in ( \"audittrail\", \"deletion\" ):\n+            if q.find(t) != -1:\n+                raise Exception(\"Forbidden: %s\" % q)\n+\n     def exec_sql(self, dbo, user, sql):\n         l = dbo.locale\n         rowsaffected = 0\n         try:\n             for q in dbo.split_queries(sql):\n                 if q == \"\": continue\n+                ql = q.lower()\n                 asm3.al.info(\"%s query: %s\" % (user, q), \"code.sql\", dbo)\n-                if q.lower().startswith(\"select\") or q.lower().startswith(\"show\"):\n+                if ql.startswith(\"select\") or ql.startswith(\"show\"):\n                     return asm3.html.table(dbo.query(q))\n                 else:\n+                    self.check_update_query(ql)\n                     rowsaffected += dbo.execute(q)\n             asm3.configuration.db_view_seq_version(dbo, \"0\")\n             return _(\"{0} rows affected.\", l).format(rowsaffected)\n","add":14,"remove":1,"filename":"\/src\/code.py","badparts":["                if q.lower().startswith(\"select\") or q.lower().startswith(\"show\"):"],"goodparts":["    def check_update_query(self, q):","        \"\"\" Prevent any kind of update to certain tables to prevent","            more savvy malicious users tampering via SQL Interface.","            q is already stripped and converted to lower case by the exec_sql caller.","            If one of our tamper proofed tables is touched, an Exception is raised","            and the query not run.","        \"\"\"","        for t in ( \"audittrail\", \"deletion\" ):","            if q.find(t) != -1:","                raise Exception(\"Forbidden: %s\" % q)","                ql = q.lower()","                if ql.startswith(\"select\") or ql.startswith(\"show\"):","                    self.check_update_query(ql)"]},{"diff":"\n         for q in dbo.split_queries(sql):\n             try:\n                 if q == \"\": continue\n+                ql = q.lower()\n                 asm3.al.info(\"%s query: %s\" % (user, q), \"code.sql\", dbo)\n-                if q.lower().startswith(\"select\") or q.lower().startswith(\"show\"):\n+                if ql.startswith(\"select\") or ql.startswith(\"show\"):\n                     output.append(str(dbo.query(q)))\n                 else:\n+                    self.check_update_query(ql)\n                     rowsaffected = dbo.execute(q)\n                     output.append(_(\"{0} rows affected.\", l).format(rowsaffected))\n             except Exception as err:\n","add":3,"remove":1,"filename":"\/src\/code.py","badparts":["                if q.lower().startswith(\"select\") or q.lower().startswith(\"show\"):"],"goodparts":["                ql = q.lower()","                if ql.startswith(\"select\") or ql.startswith(\"show\"):","                    self.check_update_query(ql)"]}]}},"msg":"prevent sql interface tampering with certain tables"}},"https:\/\/github.com\/eshyong\/pp":{"fef74b06a5d6e547c4364f4be0cf60d039795984":{"url":"https:\/\/api.github.com\/repos\/eshyong\/pp\/commits\/fef74b06a5d6e547c4364f4be0cf60d039795984","html_url":"https:\/\/github.com\/eshyong\/pp\/commit\/fef74b06a5d6e547c4364f4be0cf60d039795984","message":"Prevent tampering","sha":"fef74b06a5d6e547c4364f4be0cf60d039795984","keyword":"tampering prevent","diff":"diff --git a\/app.py b\/app.py\nindex 64e6bc7..041417c 100644\n--- a\/app.py\n+++ b\/app.py\n@@ -68,7 +68,9 @@ def handle_message(event):\n \n     text = event.get('text')\n     channel = event.get('channel')\n-    if text is None or channel is None:\n+    user = event.get('user')\n+\n+    if text is None or channel is None or user != 'UB2P4KJJE':\n         return\n \n     # ignore duplicate events\n","files":{"\/app.py":{"changes":[{"diff":"\n \n     text = event.get('text')\n     channel = event.get('channel')\n-    if text is None or channel is None:\n+    user = event.get('user')\n+\n+    if text is None or channel is None or user != 'UB2P4KJJE':\n         return\n \n     # ignore duplicate events\n","add":3,"remove":1,"filename":"\/app.py","badparts":["    if text is None or channel is None:"],"goodparts":["    user = event.get('user')","    if text is None or channel is None or user != 'UB2P4KJJE':"]}],"source":"\nfrom datetime import datetime from operator import itemgetter import os import re import signal import sys from flask import Flask, json, request, jsonify from slackclient import SlackClient if not os.environ.get('SLACK_TOKEN'): print('SLACK_TOKEN must be set when running this program') sys.exit(1) app=Flask(__name__) client=SlackClient(os.environ['SLACK_TOKEN']) incr_regexes=[r'<@(?P<user_name>[a-zA-Z0-9]+)> \\+\\+', r'<@(?P<user_name>[a-zA-Z0-9]+)>\\+\\+'] decr_regexes=[r'<@(?P<user_name>[a-zA-Z0-9]+)> --', r'<@(?P<user_name>[a-zA-Z0-9]+)>--'] last_timestamp=datetime.now() scores={} try: with open('.scores.json') as scores_file: scores=json.load(scores_file) except: pass def handle_sigint(signal, frame): print('\\nsaving scores') with open('.scores.json', mode='w') as scores_file: json.dump(scores, scores_file, indent=2, sort_keys=True) scores_file.write('\\n') print('exiting') sys.exit(0) signal.signal(signal.SIGINT, handle_sigint) @app.route('\/events', methods=['POST']) def respond(): data=json.loads(request.data) print(json.dumps(data, indent=2, sort_keys=True)) if 'challenge' in data: return data['challenge'] event=data.get('event',{}) if 'type' in event: if event['type']=='message': handle_message(event) elif event['type']=='app_mention': handle_mention(event) return '' def handle_message(event): global last_timestamp global scores text=event.get('text') channel=event.get('channel') if text is None or channel is None: return event_timestamp=datetime.fromtimestamp(float(event.get('ts'))) if event_timestamp < last_timestamp: return last_timestamp=event_timestamp pluses=get_matches(text, incr_regexes) minuses=get_matches(text, decr_regexes) for name in pluses: if name not in scores: scores[name]=0 scores[name] +=1 for name in minuses: if name not in scores: scores[name]=0 scores[name] -=1 send_message(channel, create_message(set(pluses +minuses))) def handle_mention(event): global last_timestamp text=event.get('text') channel=event.get('channel') if text is None or channel is None: return event_timestamp=datetime.fromtimestamp(float(event.get('ts'))) if event_timestamp < last_timestamp: return last_timestamp=event_timestamp command=text.split()[1] if command=='leaderboard': sorted_scores=reversed(sorted([(k, v) for k, v in scores.items()], key=itemgetter(1))) names=[name for name, _ in sorted_scores] if not names: send_message(channel, 'sorry, no scores:man-shrugging:') else: send_message(channel, create_message(names)) else: send_message(channel, \"Sorry, pp doesn't know that command\") def get_matches(text, regexes): users=[] for regex in regexes: matches=re.findall(regex, text) for match in matches: real_name=get_real_name_for_user(match) if real_name is not None: users.append(real_name) return users def get_real_name_for_user(user_name): try: response=client.api_call('users.info', user=user_name) if not response.get('ok'): return None return response['user']['profile']['real_name'] except Exception as e: print(e) return None def create_message(names): texts=[\"{}'s score is{}\".format(name, scores[name]) for name in names] return '\\n'.join(texts) def send_message(channel, text): global client try: client.api_call('chat.postMessage', channel=channel, text=text) except Exception as e: print(e) ","sourceWithComments":"from datetime import datetime\nfrom operator import itemgetter\nimport os\nimport re\nimport signal\nimport sys\n\nfrom flask import Flask, json, request, jsonify\nfrom slackclient import SlackClient\n\n\nif not os.environ.get('SLACK_TOKEN'):\n    print('SLACK_TOKEN must be set when running this program')\n    sys.exit(1)\n\napp = Flask(__name__)\n\n# globals, please ignore\nclient = SlackClient(os.environ['SLACK_TOKEN'])\nincr_regexes = [r'<@(?P<user_name>[a-zA-Z0-9 ]+)> \\+\\+', r'<@(?P<user_name>[a-zA-Z0-9 ]+)>\\+\\+']\ndecr_regexes = [r'<@(?P<user_name>[a-zA-Z0-9 ]+)> --', r'<@(?P<user_name>[a-zA-Z0-9 ]+)>--']\nlast_timestamp = datetime.now()\n\n\n# load scores file\nscores = {}\ntry:\n    with open('.scores.json') as scores_file:\n        scores = json.load(scores_file)\nexcept:\n    pass\n\n\n# handle server shutdown gracefully\ndef handle_sigint(signal, frame):\n    print('\\nsaving scores')\n    with open('.scores.json', mode='w') as scores_file:\n        json.dump(scores, scores_file, indent=2, sort_keys=True)\n        scores_file.write('\\n')\n    print('exiting')\n    sys.exit(0)\n\nsignal.signal(signal.SIGINT, handle_sigint)\n\n\n@app.route('\/events', methods=['POST'])\ndef respond():\n    data = json.loads(request.data)\n    print(json.dumps(data, indent=2, sort_keys=True))\n\n    # to let slack verify our connection\n    if 'challenge' in data:\n        return data['challenge']\n\n    # do bot stuff here\n    event = data.get('event', {})\n    if 'type' in event:\n        if event['type'] == 'message':\n            handle_message(event)\n        elif event['type'] == 'app_mention':\n            handle_mention(event)\n\n    return ''\n\ndef handle_message(event):\n    global last_timestamp\n    global scores\n\n    text = event.get('text')\n    channel = event.get('channel')\n    if text is None or channel is None:\n        return\n\n    # ignore duplicate events\n    event_timestamp = datetime.fromtimestamp(float(event.get('ts')))\n    if event_timestamp < last_timestamp:\n        return\n    last_timestamp = event_timestamp\n\n    pluses = get_matches(text, incr_regexes)\n    minuses = get_matches(text, decr_regexes)\n \n    for name in pluses:\n        if name not in scores:\n            scores[name] = 0\n        scores[name] += 1\n\n    for name in minuses:\n        if name not in scores:\n            scores[name] = 0\n        scores[name] -= 1\n\n    send_message(channel, create_message(set(pluses + minuses)))\n\n\ndef handle_mention(event):\n    global last_timestamp\n\n    text = event.get('text')\n    channel = event.get('channel')\n    if text is None or channel is None:\n        return\n\n    # ignore duplicate events\n    event_timestamp = datetime.fromtimestamp(float(event.get('ts')))\n    if event_timestamp < last_timestamp:\n        return\n    last_timestamp = event_timestamp\n\n    command = text.split()[1]\n    if command == 'leaderboard':\n        sorted_scores = reversed(sorted([(k, v) for k, v in scores.items()], key=itemgetter(1)))\n        names = [name for name, _ in sorted_scores]\n        if not names:\n            send_message(channel, 'sorry, no scores :man-shrugging:')\n        else:\n            send_message(channel, create_message(names))\n    else:\n        send_message(channel, \"Sorry, pp doesn't know that command\")\n\n\ndef get_matches(text, regexes):\n    users = []\n    for regex in regexes:\n        matches = re.findall(regex, text)\n        for match in matches:\n            real_name = get_real_name_for_user(match)\n            if real_name is not None:\n                users.append(real_name)\n    return users\n\n\ndef get_real_name_for_user(user_name):\n    try:\n        response = client.api_call('users.info', user=user_name)\n        if not response.get('ok'):\n            return None\n        return response['user']['profile']['real_name']\n    except Exception as e:\n        print(e)\n    return None\n\n\ndef create_message(names):\n    texts = [\"{}'s score is {}\".format(name, scores[name]) for name in names]\n    return '\\n'.join(texts)\n\n\ndef send_message(channel, text):\n    global client\n\n    try:\n        client.api_call('chat.postMessage', channel=channel, text=text)\n    except Exception as e:\n        print(e)\n\n\n"}},"msg":"Prevent tampering"}},"https:\/\/github.com\/jzone3\/Project-Alexandria":{"175e5d04dfb828c15f80f0af4c0b8c4fa8710bb3":{"url":"https:\/\/api.github.com\/repos\/jzone3\/Project-Alexandria\/commits\/175e5d04dfb828c15f80f0af4c0b8c4fa8710bb3","html_url":"https:\/\/github.com\/jzone3\/Project-Alexandria\/commit\/175e5d04dfb828c15f80f0af4c0b8c4fa8710bb3","message":"prevent school cookie tampering","sha":"175e5d04dfb828c15f80f0af4c0b8c4fa8710bb3","keyword":"tampering prevent","diff":"diff --git a\/main.py b\/main.py\nindex e5e91e3..7e3acbb 100644\n--- a\/main.py\n+++ b\/main.py\n@@ -46,10 +46,13 @@ def list_to_str(self, lst):\n \t\treturn to_return\n \n \tdef get_schools_list(self):\n+\t\treturn self.list_to_str(self.get_schools_raw())\n+\n+\tdef get_schools_raw(self):\n \t\tschools_list = get_schools()\n \t\tif schools_list is None:\n \t\t\tschools_list = ['Bergen County Academies']\n-\t\treturn self.list_to_str(schools_list)\n+\t\treturn schools_list\n \n \tdef render(self, template, params={}):\n \t\tparams['signed_in'] = self.logged_in()\n@@ -95,7 +98,6 @@ def render_prefs(self, params={}):\n \t\t\temail_verified = params['email_verified']\n \t\t\tdel params['email_verified']\n \t\tschool = self.get_school_cookie()\n-\t\tlogging.error(school)\n \t\tif 'school' in params.keys():\n \t\t\tdel params['school']\n \t\tnew_params = {'email' : email, 'email_verified' : email_verified, 'school' : school}\n@@ -132,9 +134,11 @@ def get_school_cookie(self):\n \t\tschool = self.request.cookies.get('school', '')\n \t\tif school:\n \t\t\tschool = school.replace('_', ' ')\n-\t\t\treturn school\n-\t\telse:\n-\t\t\treturn None\n+\t\t\tif school in self.get_schools_raw():\n+\t\t\t\treturn school\n+\t\tschool = get_school(self.get_username())\n+\t\tself.set_school_cookie(school)\n+\t\treturn school\n \n \n class MainHandler(BaseHandler):\n@@ -174,14 +178,12 @@ def post(self):\n \n \t\t\tusername, password, verify, school, year, agree, human, email = [self.rget(x) for x in ('username', 'password', 'verify', 'school', 'year', 'agree', 'session_secret', 'email')]\n \t\t\tresults = signup(username=username, password=password, verify=verify, school=school, year=year, agree=agree, human=human, email=email)\n-\t\t\tlogging.error(school)\n \t\t\tif results['success']:\n \t\t\t\tadd_school(school)\n \t\t\t\tself.set_cookie(results['cookie'])\n \t\t\t\tself.set_school_cookie(school)\n \t\t\t\tself.redirect('\/')\t\n \t\t\telse:\n-\t\t\t\tlogging.error(school)\n \t\t\t\tself.render('index.html', {'username': username,\n \t\t\t\t\t\t\t\t\t\t   'school': school,\n \t\t\t\t\t\t\t\t\t\t   'email' : email,\n","files":{"\/main.py":{"changes":[{"diff":"\n \t\treturn to_return\n \n \tdef get_schools_list(self):\n+\t\treturn self.list_to_str(self.get_schools_raw())\n+\n+\tdef get_schools_raw(self):\n \t\tschools_list = get_schools()\n \t\tif schools_list is None:\n \t\t\tschools_list = ['Bergen County Academies']\n-\t\treturn self.list_to_str(schools_list)\n+\t\treturn schools_list\n \n \tdef render(self, template, params={}):\n \t\tparams['signed_in'] = self.logged_in()\n","add":4,"remove":1,"filename":"\/main.py","badparts":["\t\treturn self.list_to_str(schools_list)"],"goodparts":["\t\treturn self.list_to_str(self.get_schools_raw())","\tdef get_schools_raw(self):","\t\treturn schools_list"]},{"diff":"\n \t\t\temail_verified = params['email_verified']\n \t\t\tdel params['email_verified']\n \t\tschool = self.get_school_cookie()\n-\t\tlogging.error(school)\n \t\tif 'school' in params.keys():\n \t\t\tdel params['school']\n \t\tnew_params = {'email' : email, 'email_verified' : email_verified, 'school' : school}\n","add":0,"remove":1,"filename":"\/main.py","badparts":["\t\tlogging.error(school)"],"goodparts":[]},{"diff":"\n \t\tschool = self.request.cookies.get('school', '')\n \t\tif school:\n \t\t\tschool = school.replace('_', ' ')\n-\t\t\treturn school\n-\t\telse:\n-\t\t\treturn None\n+\t\t\tif school in self.get_schools_raw():\n+\t\t\t\treturn school\n+\t\tschool = get_school(self.get_username())\n+\t\tself.set_school_cookie(school)\n+\t\treturn school\n \n \n class MainHandler(BaseHandler):\n","add":5,"remove":3,"filename":"\/main.py","badparts":["\t\t\treturn school","\t\telse:","\t\t\treturn None"],"goodparts":["\t\t\tif school in self.get_schools_raw():","\t\t\t\treturn school","\t\tschool = get_school(self.get_username())","\t\tself.set_school_cookie(school)","\t\treturn school"]},{"diff":"\n \n \t\t\tusername, password, verify, school, year, agree, human, email = [self.rget(x) for x in ('username', 'password', 'verify', 'school', 'year', 'agree', 'session_secret', 'email')]\n \t\t\tresults = signup(username=username, password=password, verify=verify, school=school, year=year, agree=agree, human=human, email=email)\n-\t\t\tlogging.error(school)\n \t\t\tif results['success']:\n \t\t\t\tadd_school(school)\n \t\t\t\tself.set_cookie(results['cookie'])\n \t\t\t\tself.set_school_cookie(school)\n \t\t\t\tself.redirect('\/')\t\n \t\t\telse:\n-\t\t\t\tlogging.error(school)\n \t\t\t\tself.render('index.html', {'username': username,\n \t\t\t\t\t\t\t\t\t\t   'school': school,\n \t\t\t\t\t\t\t\t\t\t   'email' : email,\n","add":0,"remove":2,"filename":"\/main.py","badparts":["\t\t\tlogging.error(school)","\t\t\t\tlogging.error(school)"],"goodparts":[]}],"source":"\nimport hashlib import hmac import jinja2 import logging import os import re import urllib import urllib2 import webapp2 import secret from search import * from utils import * import externals.ayah from google.appengine.api import files from google.appengine.api import urlfetch from google.appengine.ext import blobstore from google.appengine.ext.webapp import blobstore_handlers from google.appengine.ext.webapp.util import run_wsgi_app template_dir=os.path.join(os.path.dirname(__file__), 'templates') jinja_env=jinja2.Environment(loader=jinja2.FileSystemLoader(template_dir), autoescape=True) class BaseHandler(webapp2.RequestHandler): \t'''Parent class for all handlers, shortens functions''' \tdef write(self, content): \t\treturn self.response.out.write(content) \tdef rget(self, name): \t\treturn self.request.get(name) \tdef get_username(self): \t\tusername=self.request.cookies.get(LOGIN_COOKIE_NAME, '') \t\tif username and not username=='': \t\t\treturn username.split(\"|\")[0] \t\treturn None \tdef list_to_str(self, lst): \t\tto_return='[' \t\tfor i in lst: \t\t\tif i==lst[len(lst) -1]: \t\t\t\tto_return +='\"' +i +'\"]' \t\t\telse: \t\t\t\tto_return +='\"' +i +'\",' \t\treturn to_return \tdef get_schools_list(self): \t\tschools_list=get_schools() \t\tif schools_list is None: \t\t\tschools_list=['Bergen County Academies'] \t\treturn self.list_to_str(schools_list) \tdef render(self, template, params={}): \t\tparams['signed_in']=self.logged_in() \t\tif params['signed_in']: \t\t\tparams['username']=self.get_username() \t\telse: \t\t\t \t\t\tparams['all_schools']=self.get_schools_list() \t\t\t \t\t\tif not 'username' in params.keys(): \t\t\t\tparams['username']='' \t\t\t \t\t\texternals.ayah.configure('9ee379aab47a91907b9f9b505204b16494367d56', \t\t\t\t\t\t\t\t\t '7ec7c6561c6dba467095b91dd58778f2c60fbaf2') \t\t\twidget_html=externals.ayah.get_publisher_html() \t\t\tparams['widget_html']=widget_html \t\tif template=='prefs.html': \t\t\tparams['all_schools']=self.get_schools_list() \t\ttemplate=jinja_env.get_template(template) \t\tself.response.out.write(template.render(params)) \tdef render_prefs(self, params={}): \t\tusername=self.get_username() \t\tuser=get_user(username) \t\tif not 'email' in params.keys(): \t\t\ttry: \t\t\t\temail=user.email \t\t\texcept: \t\t\t\temail=None \t\telse: \t\t\temail=params['email'] \t\t\tdel params['email'] \t\tif not 'email_verified' in params.keys(): \t\t\ttry: \t\t\t\temail_verified=user.email_verified \t\t\texcept: \t\t\t\temail_verified=None \t\telse: \t\t\temail_verified=params['email_verified'] \t\t\tdel params['email_verified'] \t\tschool=self.get_school_cookie() \t\tlogging.error(school) \t\tif 'school' in params.keys(): \t\t\tdel params['school'] \t\tnew_params={'email': email, 'email_verified': email_verified, 'school': school} \t\tall_params=dict(new_params) \t\tall_params.update(params) \t\tself.render('prefs.html', all_params) \tdef logged_in(self): \t\tusername=self.request.cookies.get(LOGIN_COOKIE_NAME, '') \t\tif username and not username=='': \t\t\tname, hashed_name=username.split(\"|\") \t\t\tif name and hashed_name and hash_str(name)==hashed_name: \t\t\t\treturn True \t\t\telse: \t\t\t\tself.delete_cookie(LOGIN_COOKIE_NAME) \t\t\t\tself.delete_cookie(school) \t\t\t\treturn False \t\telse: \t\t\treturn False\t\t \tdef set_cookie(self, cookie): \t\tself.response.headers.add_header('Set-Cookie', cookie) \tdef delete_cookie(self, cookie): \t\tself.response.headers.add_header('Set-Cookie', '%s=; Path=\/' % cookie) \tdef set_school_cookie(self, school): \t\t'''sets and formats school cookie''' \t\tschool=str(school).replace(' ', '_') \t\tself.set_cookie('school='+school) \tdef get_school_cookie(self): \t\t'''retrieves school and formats from cookie''' \t\tschool=self.request.cookies.get('school', '') \t\tif school: \t\t\tschool=school.replace('_', ' ') \t\t\treturn school \t\telse: \t\t\treturn None class MainHandler(BaseHandler): \t'''Handles homepage: index.html and dashboard.html''' \tdef get(self): \t\tlogged_in=self.logged_in() \t\tif self.rget('q'): \t\t\tself.redirect('\/search?q=' +self.rget('q')) \t\tif logged_in: \t\t\tself.render('dashboard.html',{'submitted': get_submitted(self.get_username())})\t\t\t \t\telse: \t\t\tself.render('index.html',{'blockbg':True}) \tdef post(self): \t\tformname=self.rget('formname') \t\tif formname=='login': \t\t\tusername=self.rget('username') \t\t\tkey, value=check_login(username, self.rget('password')) \t\t\tif key: \t\t\t\tif self.rget('remember')=='on': \t\t\t\t\tvalue=value +' Expires=' +remember_me() +' Path=\/' \t\t\t\t\tself.set_cookie(value) \t\t\t\telse: \t\t\t\t\tself.set_cookie(value +' Path=\/') \t\t\t\tself.set_school_cookie(get_school(username)) \t\t\t\tself.redirect('\/') \t\t\telse: \t\t\t\tself.render('index.html',{'username': username, 'wrong': value, 'modal': 'login', 'blockbg': True}) \t\telif formname=='signup': \t\t\tusername, password, verify, school, year, agree, human, email=('', '', '', '', '', '', '', '') \t\t\tusername_error, password_error, verify_error, school_error, year_error, agree_error, human_error, email_error=('', '', '', '', '', '', '', '') \t\t\tusername, password, verify, school, year, agree, human, email=[self.rget(x) for x in('username', 'password', 'verify', 'school', 'year', 'agree', 'session_secret', 'email')] \t\t\tresults=signup(username=username, password=password, verify=verify, school=school, year=year, agree=agree, human=human, email=email) \t\t\tlogging.error(school) \t\t\tif results['success']: \t\t\t\tadd_school(school) \t\t\t\tself.set_cookie(results['cookie']) \t\t\t\tself.set_school_cookie(school) \t\t\t\tself.redirect('\/')\t \t\t\telse: \t\t\t\tlogging.error(school) \t\t\t\tself.render('index.html',{'username': username, \t\t\t\t\t\t\t\t\t\t 'school': school, \t\t\t\t\t\t\t\t\t\t 'email': email, \t\t\t\t\t\t\t\t\t\t 'email_error': get_error(results, 'email'), \t\t\t\t\t\t\t\t\t\t 'username_error': get_error(results, 'username'), \t\t\t\t\t\t\t\t\t\t 'password_error': get_error(results, 'password'), \t\t\t\t\t\t\t\t\t\t 'verify_error': get_error(results, 'verify'), \t\t\t\t\t\t\t\t\t\t 'school_error': get_error(results, 'school'), \t\t\t\t\t\t\t\t\t\t 'year_error': get_error(results, 'year'), \t\t\t\t\t\t\t\t\t\t 'agree_error': get_error(results, 'agree'), \t\t\t\t\t\t\t\t\t\t 'human_error': get_error(results, 'human'), \t\t\t\t\t\t\t\t\t\t 'choice': int(year), \t\t\t\t\t\t\t\t\t\t 'blockbg': True, \t\t\t\t\t\t\t\t\t\t 'modal': 'signup'}) \t\telse: \t\t\tself.redirect('\/') class LogoutHandler(BaseHandler): \t'''Handles logging out''' \tdef get(self): \t\tself.delete_cookie(LOGIN_COOKIE_NAME) \t\tself.delete_cookie('ACSID') \t\tself.redirect('\/') class GuidesHandler(BaseHandler): \t'''Handles guides: guides.html''' \tdef get(self): \t\tif self.rget('q'): \t\t\tself.redirect('\/search?q=' +self.rget('q')) \t\ttop_guides=get_top_guides() \t\tself.render('guides.html',{'top_guides':top_guides}) class AboutHandler(BaseHandler): \t'''Handles about: about.html''' \tdef get(self): \t\tself.render('about.html') class ContactHandler(BaseHandler): \t'''Handles contact: contact.html''' \tdef get(self): \t\tself.render('contact.html') class TeamHandler(BaseHandler): \t'''Handles team: team.html''' \tdef get(self): \t\tself.render('team.html') class DashboardHandler(BaseHandler): \t'''Handlers dashboard: dashboard.html''' \tdef get(self): \t\tif self.rget('q'): \t\t\tself.redirect('\/search?q=' +self.rget('q')) \t\tif self.logged_in(): \t\t\tself.render('dashboard.html') \t\telse: \t\t\tself.redirect('\/') class GuidePageHandler(BaseHandler): \t'''Handlers custom guide pages: guide_page.html''' \tdef get(self, url): \t\turl=url[1:] \t\tq=Guides.all() \t\tq.filter('url=', url.lower()) \t\tresult=q.get() \t\tif result: \t\t\tvotes=str_votes(result.votes) \t\t\tdl_link='\/serve\/' +result.blob_key \t\t\tself.render('guide_page.html',{'result':result, 'votes':votes, 'dl_link':dl_link}) \t\telse: \t\t\tself.error(404) \t\t\tself.render('404.html',{'blockbg':True}) class UserPageHandler(BaseHandler): \t'''Handlers custom user pages: user_page.html''' \tdef get(self, url): \t\turl=url[1:] \t\tq=Users.all() \t\tq.filter('username=', url) \t\tresult=q.get() \t\tif result: \t\t\tscore=int(str_votes(result.score)) \t\t\tgrade=str_grade(result.grade) \t\t\tself.render('user_page.html',{'result':result, 'grade':grade, 'score':score}) \t\telse: \t\t\tself.error(404) \t\t\tself.render('404.html',{'blockbg':True}) class UploadHandler(BaseHandler): \tdef get(self): \t\tif self.logged_in(): \t\t\tself.render('upload.html') \t\telse: \t\t\tself.redirect('\/') \tdef post(self): \t\ttitle=self.rget('title') \t\tsubject=self.rget('subject') \t\tteacher=self.rget('teacher') \t\tlocked=self.rget('locked') \t\tedit_url=self.rget('edit_url') \t\ttags=self.rget('tags') \t\tfile_url=self.rget('file') \t\tif file_url: \t\t\t \t\t\tresult=urlfetch.fetch(file_url) \t\t\theaders=result.headers \t\t\tif result.status_code !=200: \t\t\t\tself.write(\"Connection Error.\") \t\t\t\treturn \t\t\terrors=upload_errors(title, subject, teacher, locked, edit_url, headers) \t\telse: \t\t\terrors=upload_errors(title, subject, teacher, locked, edit_url, \t\t\t\t\t\t\t\t {'content-type':'text\/plain', 'content-length':'0'}) \t\t\terrors['file_error']='Please upload a file.' \t\tif any(errors.values()): \t\t\tfields={'title':title, 'subject':subject, 'teacher':teacher, \t\t\t\t\t 'locked':locked, 'edit_url':edit_url, 'tags':tags} \t\t\terrors.update(fields) \t\t\tself.render('\/upload.html', errors) \t\telse: \t\t\ttags=get_tags(tags) +create_tags(title, subject, teacher) \t\t\tusername=self.get_username() \t\t\tfilename=get_filename(title, username) \t\t\tschool=get_school(username) \t\t\tif locked: \t\t\t\tlocked=True \t\t\telse: \t\t\t\tlocked=False \t\t\t \t\t\tf=files.blobstore.create(mime_type=headers['content-type'], _blobinfo_uploaded_filename=filename) \t\t\twith files.open(f, 'a') as data: \t\t\t\tdata.write(result.content) \t\t\tfiles.finalize(f) \t\t\tblob_key=files.blobstore.get_blob_key(f) \t\t\t \t\t\turl=get_url(filename, username) \t\t\t \t\t\tguide=Guides(user_created=username, title=title, subject=subject, \t\t\t\t teacher=teacher, tags=tags, blob_key=str(blob_key), locked=locked, \t\t\t\t votes=0, edit_url=edit_url, school=school, url=url) \t\t\tguide.put() \t\t\t \t\t\tadd_submitted(username,str(blob_key)) \t\t\t \t\t\t \t\t\tkey=str(guide.key()) \t\t\tadd_to_index(school, key, tags) \t\t\tself.redirect('\/guides\/' +url) class ServeHandler(blobstore_handlers.BlobstoreDownloadHandler): \tdef get(self, resource): \t\tresource=str(urllib.unquote(resource)) \t\tblob_info=blobstore.BlobInfo.get(resource) \t\tself.send_blob(blob_info, save_as=blob_info.filename) class NotFoundHandler(BaseHandler): \tdef get(self): \t\tself.error(404) \t\tself.render('404.html',{'blockbg':True}) class ToSHandler(BaseHandler): \tdef get(self): \t\tself.render('tos.html') class SearchHandler(BaseHandler): \tdef get(self): \t\tquery=self.rget('q') \t\tschool=self.get_school_cookie() \t\tif not school: \t\t\tschool='Bergen County Academies' \t\trankings=search(school, query) \t\tresults=[] \t\tfor ranking in rankings: \t\t\t \t\t\tguide=Guides.get(ranking[0]) \t\t\t \t\t\tresult={'url':guide.url, 'title':guide.title, 'subject':guide.subject, \t\t\t\t\t 'teacher':guide.teacher, 'votes':str_votes(guide.votes)} \t\t\tresults.append(result) \t\tif results: \t\t\tself.render('search.html',{'results':results}) \t\telse: \t\t\tself.render('search.html') class PreferencesHandler(BaseHandler): \tdef get(self): \t\tif self.logged_in(): \t\t\tschool_success=self.rget('school_success') \t\t\tif school_success: \t\t\t\tself.render_prefs({'school_success': True}) \t\t\telse: \t\t\t\tself.render_prefs() \t\telse: \t\t\tself.redirect('\/') class ChangeEmailHandler(BaseHandler): \tdef get(self): \t\tself.redirect('\/preferences') \tdef post(self): \t\tif self.logged_in(): \t\t\temail=self.rget('email') \t\t\tresults=new_email(email, self.get_username()) \t\t\tif results[0]: \t\t\t\tself.render_prefs({'email_success': True}) \t\t\telse: \t\t\t\tself.render_prefs({'email_error': results[1]}) \t\telse: \t\t\tself.redirect('\/') class ChangeSchoolHandler(BaseHandler): \tdef get(self): \t\tself.redirect('\/preferences') \tdef post(self): \t\tif self.logged_in(): \t\t\tschool=self.rget('school') \t\t\tresults=change_school(school, self.get_username()) \t\t\tif results[0]: \t\t\t\tself.set_school_cookie(school) \t\t\t\tself.redirect('\/preferences?school_success=True') \t\t\t\t \t\t\telse: \t\t\t\tself.write(results[1]) \t\t\t\tself.render('prefs',{'school_error': results[1]}) \t\telse: \t\t\tself.redirect('\/') class ChangePasswordHandler(BaseHandler): \tdef get(self): \t\tself.redirect('\/preferences') \tdef post(self): \t\tif self.logged_in(): \t\t\tusername=self.get_username() \t\t\told_password, new_password, verify_new_password=[self.rget(x) for x in['current_password', 'new_password', 'verify_new_password']] \t\t\tresults=change_password(old_password, new_password, verify_new_password, username) \t\t\tif results[0]: \t\t\t\tself.set_cookie(results[1]) \t\t\t\tself.render_prefs({'username': username, 'password_success': True}) \t\t\telse: \t\t\t\tself.render_prefs(results[1]) \t\telse: \t\t\tself.redirect('\/') class DeleteAccountHandler(BaseHandler): \tdef get(self): \t\tif self.logged_in(): \t\t\tself.render('delete_account.html') \t\telse: \t\t\tself.redirect('\/') \tdef post(self): \t\tif self.logged_in(): \t\t\tpassword=self.rget('password') \t\t\tif check_login(self.get_username(), password): \t\t\t\tfeedback=self.rget('feedback') \t\t\t\tif feedback: \t\t\t\t\tsave_feedback(feedback, 'delete_account') \t\t\t\tdelete_user_account(self.get_username()) \t\t\t\tself.delete_cookie(LOGIN_COOKIE_NAME) \t\t\t\tself.delete_cookie('school') \t\t\t\tself.redirect('\/') \t\t\telse: \t\t\t\tself.render('\/delete_account') \t\telse: \t\t\tself.redirect('\/') from google.appengine.api import users class GoogleSignupHandler(BaseHandler): def get(self): self.redirect(users.create_login_url(\"\/ext_signup\")) class GoogleLoginHandler(BaseHandler): \tdef google_login(self, user): \t\tq=Users.all() \t\tq.filter('email=', user.email()) \t\taccount=q.get() \t\tif account: \t\t\tusername=account.username \t\t\tcookie=LOGIN_COOKIE_NAME +'=%s|%s; Expires=%s Path=\/' %(str(username), hash_str(username), remember_me()) \t\t\tself.set_cookie(cookie) \t\t\treturn True \t\telse: \t\t\treturn False \tdef get(self): \t\tuser=users.get_current_user() \t\tif user: \t\t\tif self.google_login(user): \t\t\t\tself.redirect('\/') \t\t\telse: \t\t\t\tself.render('index.html',{'blockbg': True, \t\t\t\t\t\t\t\t\t\t 'modal':'login', \t\t\t\t\t\t\t\t\t\t 'google_error':\"\"\"There was no information found for your Google Account. Did you mean to <a href=\" \t\t\t\t\t\t\t\t\t\t }) \t\t\t\treturn \t\telse: \t\t\tself.redirect(users.create_login_url(\"\/google_login\")) class ExternalSignUp(BaseHandler): \tdef get(self): \t\tuser=users.get_current_user() \t\tif not user: \t\t\tself.redirect('\/google_signup') \t\tself.render('external_signup.html') \tdef post(self): \t\tuser=users.get_current_user() \t\tif user: \t\t\tusername=self.rget('username') \t\t\tschool=self.rget('school') \t\t\tyear=self.rget('year') \t\t\tagree=self.rget('agree') \t\t\temail=user.email() \t\t\tresult=signup_ext(username, school, year, agree, email) \t\t\tif result['success']: \t\t\t\tcookie=result['cookie'] \t\t\t\tself.set_cookie(cookie) \t\t\t\tself.redirect('\/') \t\t\telse: \t\t\t\tself.render('external_signup.html',{'username_error':result.get('username'), \t\t\t\t\t\t\t\t\t\t\t\t\t 'school_error':result.get('school'), \t\t\t\t\t\t\t\t\t\t\t\t\t 'year_error':result.get('year'), \t\t\t\t\t\t\t\t\t\t\t\t\t 'agree_error':result.get('agree'), \t\t\t\t\t\t\t\t\t\t\t\t\t 'username':username, \t\t\t\t\t\t\t\t\t\t\t\t\t 'school':school, \t\t\t\t\t\t\t\t\t\t\t\t\t 'choice':year}) \t\telse: \t\t\tself.redirect('\/google_signup') app=webapp2.WSGIApplication([('\/?', MainHandler), \t\t\t\t\t\t\t ('\/about\/?', AboutHandler), \t\t\t\t\t\t\t ('\/logout\/?', LogoutHandler), \t\t\t\t\t\t\t ('\/guides\/?', GuidesHandler), \t\t\t\t\t\t\t ('\/contact\/?', ContactHandler), \t\t\t\t\t\t\t ('\/team\/?', TeamHandler), \t\t\t\t\t\t\t ('\/dashboard\/?', DashboardHandler), \t\t\t\t\t\t\t ('\/guides' +PAGE_RE, GuidePageHandler), \t\t\t\t\t\t\t ('\/user'+PAGE_RE, UserPageHandler), \t\t\t\t\t\t\t ('\/upload\/?', UploadHandler), \t\t\t\t\t\t\t ('\/serve\/([^\/]+)?', ServeHandler), \t\t\t\t\t\t\t ('\/tos\/?', ToSHandler), \t\t\t\t\t\t\t ('\/preferences\/?', PreferencesHandler), \t\t\t\t\t\t\t ('\/search', SearchHandler),\t \t\t\t\t\t\t\t ('\/change_email\/?', ChangeEmailHandler), \t\t\t\t\t\t\t ('\/change_school\/?', ChangeSchoolHandler), \t\t\t\t\t\t\t ('\/change_password\/?', ChangePasswordHandler), \t\t\t\t\t\t\t ('\/delete_account\/?', DeleteAccountHandler), \t\t\t\t\t\t\t ('\/google_signup\/?', GoogleSignupHandler), \t\t\t\t\t\t\t ('\/google_login\/?', GoogleLoginHandler), \t\t\t\t\t\t\t ('\/ext_signup\/?', ExternalSignUp), \t\t\t\t\t\t\t \t\t\t\t\t\t\t ('\/.*', NotFoundHandler), \t\t\t\t\t\t\t ], debug=True) ","sourceWithComments":"import hashlib\nimport hmac\nimport jinja2\nimport logging\nimport os\nimport re\nimport urllib\nimport urllib2\nimport webapp2\n\nimport secret\nfrom search import *\nfrom utils import *\n\nimport externals.ayah\nfrom google.appengine.api import files\nfrom google.appengine.api import urlfetch\nfrom google.appengine.ext import blobstore\nfrom google.appengine.ext.webapp import blobstore_handlers\nfrom google.appengine.ext.webapp.util import run_wsgi_app\n\ntemplate_dir = os.path.join(os.path.dirname(__file__), 'templates')\njinja_env = jinja2.Environment(loader = jinja2.FileSystemLoader(template_dir), autoescape=True)\n\nclass BaseHandler(webapp2.RequestHandler):\n\t'''Parent class for all handlers, shortens functions'''\n\tdef write(self, content):\n\t\treturn self.response.out.write(content)\n\n\tdef rget(self, name):\n\t\treturn self.request.get(name)\n\n\tdef get_username(self):\n\t\tusername = self.request.cookies.get(LOGIN_COOKIE_NAME, '')\n\t\tif username and not username == '':\n\t\t\treturn username.split(\"|\")[0]\n\t\treturn None\n\n\tdef list_to_str(self, lst):\n\t\tto_return = '['\n\t\tfor i in lst:\n\t\t\tif i == lst[len(lst) - 1]:\n\t\t\t\tto_return += '\"' + i + '\"]'\n\t\t\telse:\n\t\t\t\tto_return += '\"' + i + '\",'\n\t\treturn to_return\n\n\tdef get_schools_list(self):\n\t\tschools_list = get_schools()\n\t\tif schools_list is None:\n\t\t\tschools_list = ['Bergen County Academies']\n\t\treturn self.list_to_str(schools_list)\n\n\tdef render(self, template, params={}):\n\t\tparams['signed_in'] = self.logged_in()\n\t\tif params['signed_in']:\n\t\t\tparams['username'] = self.get_username()\n\t\telse:\n\t\t\t# get schools list for typeahead\n\t\t\tparams['all_schools'] = self.get_schools_list()\n\n\t\t\t# set username to blank\n\t\t\tif not 'username' in params.keys():\n\t\t\t\tparams['username'] = ''\n\t\t\t# setup areyouahuman\n\t\t\texternals.ayah.configure('9ee379aab47a91907b9f9b505204b16494367d56', \n\t\t\t\t\t\t\t\t\t '7ec7c6561c6dba467095b91dd58778f2c60fbaf2')\n\t\t\twidget_html = externals.ayah.get_publisher_html()\n\t\t\tparams['widget_html'] = widget_html\n\n\t\tif template == 'prefs.html':\n\t\t\tparams['all_schools'] = self.get_schools_list()\n\n\t\ttemplate = jinja_env.get_template(template)\n\t\tself.response.out.write(template.render(params))\n\n\tdef render_prefs(self, params={}):\n\t\tusername = self.get_username()\n\t\tuser = get_user(username)\n\t\tif not 'email' in params.keys():\n\t\t\ttry:\n\t\t\t\temail = user.email\n\t\t\texcept:\n\t\t\t\temail = None\n\t\telse:\n\t\t\temail = params['email']\n\t\t\tdel params['email']\n\n\t\tif not 'email_verified' in params.keys():\n\t\t\ttry:\n\t\t\t\temail_verified = user.email_verified\n\t\t\texcept:\n\t\t\t\temail_verified = None\n\t\telse:\n\t\t\temail_verified = params['email_verified']\n\t\t\tdel params['email_verified']\n\t\tschool = self.get_school_cookie()\n\t\tlogging.error(school)\n\t\tif 'school' in params.keys():\n\t\t\tdel params['school']\n\t\tnew_params = {'email' : email, 'email_verified' : email_verified, 'school' : school}\n\t\tall_params = dict(new_params)\n\t\tall_params.update(params)\n\t\tself.render('prefs.html', all_params)\n\n\tdef logged_in(self):\n\t\tusername = self.request.cookies.get(LOGIN_COOKIE_NAME, '')\n\t\tif username and not username == '':\n\t\t\tname, hashed_name = username.split(\"|\")\n\t\t\tif name and hashed_name and hash_str(name) == hashed_name:\n\t\t\t\treturn True\n\t\t\telse:\n\t\t\t\tself.delete_cookie(LOGIN_COOKIE_NAME)\n\t\t\t\tself.delete_cookie(school)\n\t\t\t\treturn False\n\t\telse:\n\t\t\treturn False\t\t\n\n\tdef set_cookie(self, cookie):\n\t\tself.response.headers.add_header('Set-Cookie', cookie)\n\n\tdef delete_cookie(self, cookie):\n\t\tself.response.headers.add_header('Set-Cookie', '%s=; Path=\/' % cookie)\n\n\tdef set_school_cookie(self, school):\n\t\t'''sets and formats school cookie'''\n\t\tschool = str(school).replace(' ', '_')\n\t\tself.set_cookie('school='+school)\n\n\tdef get_school_cookie(self):\n\t\t'''retrieves school and formats from cookie'''\n\t\tschool = self.request.cookies.get('school', '')\n\t\tif school:\n\t\t\tschool = school.replace('_', ' ')\n\t\t\treturn school\n\t\telse:\n\t\t\treturn None\n\n\nclass MainHandler(BaseHandler):\n\t'''Handles homepage: index.html and dashboard.html'''\n\tdef get(self):\n\t\tlogged_in = self.logged_in()\n\n\t\tif self.rget('q'):\n\t\t\tself.redirect('\/search?q=' + self.rget('q'))\n\n\t\tif logged_in:\n\t\t\tself.render('dashboard.html', {'submitted' : get_submitted(self.get_username())})\t\t\t\n\t\telse:\n\t\t\tself.render('index.html', {'blockbg':True})\n\n\tdef post(self):\n\t\tformname = self.rget('formname')\n\n\t\tif formname == 'login':\n\t\t\tusername = self.rget('username')\n\t\t\tkey, value = check_login(username, self.rget('password'))\n\n\t\t\tif key:\n\t\t\t\tif self.rget('remember') == 'on':\n\t\t\t\t\tvalue = value + ' Expires=' + remember_me() + ' Path=\/'\n\t\t\t\t\tself.set_cookie(value)\n\t\t\t\telse:\n\t\t\t\t\tself.set_cookie(value + ' Path=\/')\n\t\t\t\tself.set_school_cookie(get_school(username))\n\t\t\t\tself.redirect('\/')\n\t\t\telse:\n\t\t\t\tself.render('index.html', {'username': username, 'wrong': value, 'modal' : 'login', 'blockbg' : True})\n\n\t\telif formname == 'signup':\n\t\t\tusername, password, verify, school, year, agree, human, email = ('', '', '', '', '', '', '', '')\n\t\t\tusername_error, password_error, verify_error, school_error, year_error, agree_error, human_error, email_error = ('', '', '', '', '', '', '', '')\n\n\t\t\tusername, password, verify, school, year, agree, human, email = [self.rget(x) for x in ('username', 'password', 'verify', 'school', 'year', 'agree', 'session_secret', 'email')]\n\t\t\tresults = signup(username=username, password=password, verify=verify, school=school, year=year, agree=agree, human=human, email=email)\n\t\t\tlogging.error(school)\n\t\t\tif results['success']:\n\t\t\t\tadd_school(school)\n\t\t\t\tself.set_cookie(results['cookie'])\n\t\t\t\tself.set_school_cookie(school)\n\t\t\t\tself.redirect('\/')\t\n\t\t\telse:\n\t\t\t\tlogging.error(school)\n\t\t\t\tself.render('index.html', {'username': username,\n\t\t\t\t\t\t\t\t\t\t   'school': school,\n\t\t\t\t\t\t\t\t\t\t   'email' : email,\n\t\t\t\t\t\t\t\t\t\t   'email_error' : get_error(results, 'email'),\n\t\t\t\t\t\t\t\t\t\t   'username_error': get_error(results, 'username'),\n\t\t\t\t\t\t\t\t\t\t   'password_error': get_error(results, 'password'),\n\t\t\t\t\t\t\t\t\t\t   'verify_error': get_error(results, 'verify'),\n\t\t\t\t\t\t\t\t\t\t   'school_error': get_error(results, 'school'),\n\t\t\t\t\t\t\t\t\t\t   'year_error': get_error(results, 'year'),\n\t\t\t\t\t\t\t\t\t\t   'agree_error': get_error(results, 'agree'),\n\t\t\t\t\t\t\t\t\t\t   'human_error': get_error(results, 'human'),\n\t\t\t\t\t\t\t\t\t\t   'choice' : int(year),\n\t\t\t\t\t\t\t\t\t\t   'blockbg' : True,\n\t\t\t\t\t\t\t\t\t\t   'modal': 'signup'})\n\n\t\telse:\n\t\t\tself.redirect('\/')\n\nclass LogoutHandler(BaseHandler):\n\t'''Handles logging out'''\n\tdef get(self):\n\t\tself.delete_cookie(LOGIN_COOKIE_NAME)\n\t\tself.delete_cookie('ACSID')\n\t\tself.redirect('\/')\n\nclass GuidesHandler(BaseHandler):\n\t'''Handles guides: guides.html'''\n\tdef get(self):\n\t\tif self.rget('q'):\n\t\t\tself.redirect('\/search?q=' + self.rget('q'))\n\t\ttop_guides = get_top_guides()\n\t\tself.render('guides.html', {'top_guides':top_guides})\n\nclass AboutHandler(BaseHandler):\n\t'''Handles about: about.html'''\n\tdef get(self):\n\t\tself.render('about.html')\n\nclass ContactHandler(BaseHandler):\n\t'''Handles contact: contact.html'''\n\tdef get(self):\n\t\tself.render('contact.html')\n\nclass TeamHandler(BaseHandler):\n\t'''Handles team: team.html'''\n\tdef get(self):\n\t\tself.render('team.html')\n\nclass DashboardHandler(BaseHandler):\n\t'''Handlers dashboard: dashboard.html'''\n\tdef get(self):\n\t\tif self.rget('q'):\n\t\t\tself.redirect('\/search?q=' + self.rget('q'))\n\n\t\tif self.logged_in():\n\t\t\tself.render('dashboard.html')\n\t\telse:\n\t\t\tself.redirect('\/')\n\nclass GuidePageHandler(BaseHandler):\n\t'''Handlers custom guide pages: guide_page.html'''\n\tdef get(self, url):\n\t\turl = url[1:]\n\t\tq = Guides.all()\n\t\tq.filter('url =', url.lower())\n\t\tresult = q.get()\n\t\tif result:\n\t\t\tvotes = str_votes(result.votes)\n\t\t\tdl_link = '\/serve\/' + result.blob_key\n\t\t\tself.render('guide_page.html', {'result':result, 'votes':votes, 'dl_link':dl_link})\n\t\telse:\n\t\t\tself.error(404)\n\t\t\tself.render('404.html', {'blockbg':True})\n\nclass UserPageHandler(BaseHandler):\n\t'''Handlers custom user pages: user_page.html'''\n\tdef get(self, url):\n\t\turl = url[1:]\n\t\tq = Users.all()\n\t\tq.filter('username =', url)\n\t\tresult = q.get()\n\t\tif result:\n\t\t\tscore = int(str_votes(result.score))\n\t\t\tgrade = str_grade(result.grade)\n\t\t\tself.render('user_page.html', {'result':result, 'grade':grade, 'score':score})\n\t\telse:\n\t\t\tself.error(404)\n\t\t\tself.render('404.html', {'blockbg':True})\n\nclass UploadHandler(BaseHandler):\n\tdef get(self):\n\t\tif self.logged_in():\n\t\t\tself.render('upload.html')\n\t\telse:\n\t\t\tself.redirect('\/')\n\n\tdef post(self):\n\t\ttitle = self.rget('title')\n\t\tsubject = self.rget('subject')\n\t\tteacher = self.rget('teacher')\n\t\tlocked = self.rget('locked')\n\t\tedit_url = self.rget('edit_url')\n\t\ttags = self.rget('tags')\n\t\tfile_url = self.rget('file')\n\n\t\tif file_url:\n\t\t\t# get the file from filepicker.io\n\t\t\tresult = urlfetch.fetch(file_url)\n\t\t\theaders = result.headers\n\t\t\tif result.status_code != 200:\n\t\t\t\tself.write(\"Connection Error.\")\n\t\t\t\treturn\n\t\t\terrors = upload_errors(title, subject, teacher, locked, edit_url, headers)\n\t\telse:\n\t\t\terrors = upload_errors(title, subject, teacher, locked, edit_url, \n\t\t\t\t\t\t\t\t   {'content-type':'text\/plain', 'content-length':'0'})\n\t\t\terrors['file_error'] = 'Please upload a file.'\n\n\t\tif any(errors.values()):\n\t\t\tfields = {'title':title, 'subject':subject, 'teacher':teacher, \n\t\t\t\t\t  'locked':locked, 'edit_url':edit_url, 'tags':tags}\n\t\t\terrors.update(fields)\n\t\t\tself.render('\/upload.html', errors)\n\t\telse:\n\t\t\ttags = get_tags(tags) + create_tags(title, subject, teacher)\n\t\t\tusername = self.get_username()\n\t\t\tfilename = get_filename(title, username)\n\t\t\tschool = get_school(username)\n\t\t\tif locked: \n\t\t\t\tlocked = True\n\t\t\telse: \n\t\t\t\tlocked = False\n\n\t\t\t# write file to blobstore\n\t\t\tf = files.blobstore.create(mime_type=headers['content-type'], _blobinfo_uploaded_filename=filename)\n\t\t\twith files.open(f, 'a') as data:\n\t\t\t\tdata.write(result.content)\n\t\t\tfiles.finalize(f)\n\t\t\tblob_key = files.blobstore.get_blob_key(f)\n\n\t\t\t# construct url for guide page\n\t\t\turl = get_url(filename, username)\n\n\t\t\t# add guide to db\n\t\t\tguide = Guides(user_created=username, title=title, subject=subject,\n\t\t\t\t   teacher=teacher, tags=tags, blob_key=str(blob_key), locked=locked,\n\t\t\t\t   votes=0, edit_url=edit_url, school=school, url=url)\n\t\t\tguide.put()\n\n\t\t\t# add guide to user's submitted guides cache\n\t\t\tadd_submitted(username,str(blob_key))\n\t\t\t\n\t\t\t# add guide to index\n\t\t\tkey = str(guide.key())\n\t\t\tadd_to_index(school, key, tags)\n\n\t\t\tself.redirect('\/guides\/' + url)\n\nclass ServeHandler(blobstore_handlers.BlobstoreDownloadHandler):\n\tdef get(self, resource):\n\t\tresource = str(urllib.unquote(resource))\n\t\tblob_info = blobstore.BlobInfo.get(resource)\n\t\tself.send_blob(blob_info, save_as=blob_info.filename)\n\nclass NotFoundHandler(BaseHandler):\n\tdef get(self):\n\t\tself.error(404)\n\t\tself.render('404.html',{'blockbg':True})\n\nclass ToSHandler(BaseHandler):\n\tdef get(self):\n\t\tself.render('tos.html')\n\nclass SearchHandler(BaseHandler):\n\tdef get(self):\n\t\tquery = self.rget('q')\n\t\tschool = self.get_school_cookie()\n\t\tif not school:\n\t\t\tschool = 'Bergen County Academies'\n\t\trankings = search(school, query)\n\n\t\tresults = []\n\n\t\tfor ranking in rankings:\n\t\t\t# get guides by key\n\t\t\tguide = Guides.get(ranking[0])\n\t\t\t# format results\n\t\t\tresult = {'url':guide.url, 'title':guide.title, 'subject':guide.subject,\n\t\t\t\t\t  'teacher':guide.teacher, 'votes':str_votes(guide.votes)}\n\t\t\tresults.append(result)\n\n\t\tif results:\n\t\t\tself.render('search.html', {'results':results})\n\t\telse:\n\t\t\tself.render('search.html')\n\n# class Test(BaseHandler):\n# \tdef get(self):\n# \t\texternals.ayah.configure('9ee379aab47a91907b9f9b505204b16494367d56', '7ec7c6561c6dba467095b91dd58778f2c60fbaf2')\n# \t\thtml = externals.ayah.get_publisher_html()\n# \t\tself.write('<form method=\"post\"><input type=\"text\">'+html+'<button type=\"submit\"><\/button><\/form>')\n\n# \tdef post(self):\n# \t\tsecret = self.rget('session_secret')\n# \t\texternals.ayah.configure('9ee379aab47a91907b9f9b505204b16494367d56', '7ec7c6561c6dba467095b91dd58778f2c60fbaf2')\n# \t\tif externals.ayah.score_result(secret):\n# \t\t\tself.write(secret)\n# \t\telse:\n# \t\t\tself.write('no')\n\nclass PreferencesHandler(BaseHandler):\n\tdef get(self):\n\t\tif self.logged_in():\n\t\t\tschool_success = self.rget('school_success')\n\t\t\tif school_success:\n\t\t\t\tself.render_prefs({'school_success' : True})\n\t\t\telse:\n\t\t\t\tself.render_prefs()\n\t\telse:\n\t\t\tself.redirect('\/')\n\nclass ChangeEmailHandler(BaseHandler):\n\tdef get(self):\n\t\tself.redirect('\/preferences')\n\n\tdef post(self):\n\t\tif self.logged_in():\n\t\t\temail = self.rget('email')\n\t\t\tresults = new_email(email, self.get_username())\n\t\t\tif results[0]:\n\t\t\t\tself.render_prefs({'email_success' : True})\n\t\t\telse:\n\t\t\t\tself.render_prefs({'email_error' : results[1]})\n\t\telse:\n\t\t\tself.redirect('\/')\n\nclass ChangeSchoolHandler(BaseHandler):\n\tdef get(self):\n\t\tself.redirect('\/preferences')\n\n\tdef post(self):\n\t\tif self.logged_in():\n\t\t\tschool = self.rget('school')\n\t\t\tresults = change_school(school, self.get_username())\n\t\t\tif results[0]:\n\t\t\t\tself.set_school_cookie(school)\n\t\t\t\tself.redirect('\/preferences?school_success=True')\n\t\t\t\t# self.render_prefs({'school_success' : True})\n\t\t\telse:\n\t\t\t\tself.write(results[1])\n\t\t\t\tself.render('prefs', {'school_error' : results[1]})\n\t\telse:\n\t\t\tself.redirect('\/')\n\nclass ChangePasswordHandler(BaseHandler):\n\tdef get(self):\n\t\tself.redirect('\/preferences')\n\n\tdef post(self):\n\t\tif self.logged_in():\n\t\t\tusername = self.get_username()\n\t\t\told_password, new_password, verify_new_password= [self.rget(x) for x in ['current_password', 'new_password', 'verify_new_password']]\n\t\t\tresults = change_password(old_password, new_password, verify_new_password, username)\n\t\t\tif results[0]:\n\t\t\t\tself.set_cookie(results[1])\n\t\t\t\tself.render_prefs({'username' : username, 'password_success' : True})\n\t\t\telse:\n\t\t\t\tself.render_prefs(results[1])\n\t\telse:\n\t\t\tself.redirect('\/')\n\nclass DeleteAccountHandler(BaseHandler):\n\tdef get(self):\n\t\tif self.logged_in():\n\t\t\tself.render('delete_account.html')\n\t\telse:\n\t\t\tself.redirect('\/')\n\n\tdef post(self):\n\t\tif self.logged_in():\n\t\t\tpassword = self.rget('password')\n\t\t\tif check_login(self.get_username(), password):\n\t\t\t\tfeedback = self.rget('feedback')\n\t\t\t\tif feedback:\n\t\t\t\t\tsave_feedback(feedback, 'delete_account')\n\t\t\t\tdelete_user_account(self.get_username())\n\t\t\t\tself.delete_cookie(LOGIN_COOKIE_NAME)\n\t\t\t\tself.delete_cookie('school')\n\t\t\t\tself.redirect('\/')\n\t\t\telse:\n\t\t\t\tself.render('\/delete_account')\n\t\telse:\n\t\t\tself.redirect('\/')\n\nfrom google.appengine.api import users\n\nclass GoogleSignupHandler(BaseHandler):\n    def get(self):\n        self.redirect(users.create_login_url(\"\/ext_signup\"))\n\nclass GoogleLoginHandler(BaseHandler):\n\tdef google_login(self, user):\n\t\tq = Users.all()\n\t\tq.filter('email =', user.email())\n\t\taccount = q.get()\n\t\tif account:\n\t\t\tusername = account.username\n\t\t\tcookie = LOGIN_COOKIE_NAME + '=%s|%s; Expires=%s Path=\/' % (str(username), hash_str(username), remember_me())\n\t\t\tself.set_cookie(cookie)\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\n\tdef get(self):\n\t\tuser = users.get_current_user()\n\t\tif user:\n\t\t\tif self.google_login(user):\n\t\t\t\tself.redirect('\/')\n\t\t\telse:\n\t\t\t\tself.render('index.html', {'blockbg': True,\n\t\t\t\t\t\t\t\t\t\t   'modal':'login',\n\t\t\t\t\t\t\t\t\t\t   'google_error':\"\"\"There was no information found for your Google Account. Did you mean to <a href=\"#signup\" role=\"button\" data-toggle=\"modal\" onclick=\"$('#login').modal('hide')\">sign up<\/a>?\"\"\",\n\t\t\t\t\t\t\t\t\t\t   })\n\t\t\t\treturn\n\n\t\telse:\n\t\t\tself.redirect(users.create_login_url(\"\/google_login\"))\n\nclass ExternalSignUp(BaseHandler):\n\tdef get(self):\n\t\tuser = users.get_current_user()\n\t\tif not user:\n\t\t\tself.redirect('\/google_signup')\n\t\tself.render('external_signup.html')\n\n\tdef post(self):\n\t\tuser = users.get_current_user()\n\t\tif user:\n\t\t\tusername = self.rget('username')\n\t\t\tschool = self.rget('school')\n\t\t\tyear = self.rget('year')\n\t\t\tagree = self.rget('agree')\n\t\t\temail = user.email()\n\n\t\t\tresult = signup_ext(username, school, year, agree, email)\n\n\t\t\tif result['success']:\n\t\t\t\tcookie = result['cookie']\n\t\t\t\tself.set_cookie(cookie)\n\t\t\t\tself.redirect('\/')\n\t\t\telse:\n\t\t\t\tself.render('external_signup.html', {'username_error':result.get('username'),\n\t\t\t\t\t\t\t\t\t\t\t\t\t 'school_error':result.get('school'),\n\t\t\t\t\t\t\t\t\t\t\t\t\t 'year_error':result.get('year'),\n\t\t\t\t\t\t\t\t\t\t\t\t\t 'agree_error':result.get('agree'),\n\t\t\t\t\t\t\t\t\t\t\t\t\t 'username':username,\n\t\t\t\t\t\t\t\t\t\t\t\t\t 'school':school,\n\t\t\t\t\t\t\t\t\t\t\t\t\t 'choice':year})\n\n\t\telse:\n\t\t\tself.redirect('\/google_signup')\n\n\napp = webapp2.WSGIApplication([('\/?', MainHandler),\n\t\t\t\t\t\t\t   ('\/about\/?', AboutHandler),\n\t\t\t\t\t\t\t   ('\/logout\/?', LogoutHandler),\n\t\t\t\t\t\t\t   ('\/guides\/?', GuidesHandler),\n\t\t\t\t\t\t\t   ('\/contact\/?', ContactHandler),\n\t\t\t\t\t\t\t   ('\/team\/?', TeamHandler),\n\t\t\t\t\t\t\t   ('\/dashboard\/?', DashboardHandler),\n\t\t\t\t\t\t\t   ('\/guides' + PAGE_RE, GuidePageHandler),\n\t\t\t\t\t\t\t   ('\/user'+ PAGE_RE, UserPageHandler),\n\t\t\t\t\t\t\t   ('\/upload\/?', UploadHandler),\n\t\t\t\t\t\t\t   ('\/serve\/([^\/]+)?', ServeHandler),\n\t\t\t\t\t\t\t   ('\/tos\/?', ToSHandler),\n\t\t\t\t\t\t\t   ('\/preferences\/?', PreferencesHandler),\n\t\t\t\t\t\t\t   ('\/search', SearchHandler),\t\n\t\t\t\t\t\t\t   ('\/change_email\/?', ChangeEmailHandler),\n\t\t\t\t\t\t\t   ('\/change_school\/?', ChangeSchoolHandler),\n\t\t\t\t\t\t\t   ('\/change_password\/?', ChangePasswordHandler),\n\t\t\t\t\t\t\t   ('\/delete_account\/?', DeleteAccountHandler),\n\t\t\t\t\t\t\t   ('\/google_signup\/?', GoogleSignupHandler),\n\t\t\t\t\t\t\t   ('\/google_login\/?', GoogleLoginHandler),\n\t\t\t\t\t\t\t   ('\/ext_signup\/?', ExternalSignUp),\n\t\t\t\t\t\t\t   # ('\/test', Test),\n\t\t\t\t\t\t\t   ('\/.*', NotFoundHandler),\n\t\t\t\t\t\t\t   ], debug=True)\n"}},"msg":"prevent school cookie tampering"}},"https:\/\/github.com\/onnela-lab\/beiwe-backend":{"543d6fc63e8fab609f14dd31d3cc9a58eb2d367a":{"url":"https:\/\/api.github.com\/repos\/onnela-lab\/beiwe-backend\/commits\/543d6fc63e8fab609f14dd31d3cc9a58eb2d367a","html_url":"https:\/\/github.com\/onnela-lab\/beiwe-backend\/commit\/543d6fc63e8fab609f14dd31d3cc9a58eb2d367a","message":"Edited FileProcessLock to prevent tampering or weirdness","sha":"543d6fc63e8fab609f14dd31d3cc9a58eb2d367a","keyword":"tampering prevent","diff":"diff --git a\/study\/data_access_models.py b\/study\/data_access_models.py\nindex 2680aa160..cbd087910 100644\n--- a\/study\/data_access_models.py\n+++ b\/study\/data_access_models.py\n@@ -100,14 +100,16 @@ def append_file_for_processing(cls, file_path, study_value, **kwargs):\n class FileProcessLock(AbstractModel):\r\n     \r\n     lock_time = models.DateTimeField(null=True)\r\n-    # AJK TODO should we enforce on the database level that there can only be one FPL?\r\n+    \r\n+    # In case of weirdness, ensure that there is only one lock object\r\n+    is_actual_lock = models.BooleanField(unique=True, help_text=\"This is True if the Lock is actually a lock\")\r\n     \r\n     @classmethod\r\n     def lock(cls):\r\n-        if cls.objects.exists():\r\n+        if cls.islocked():\r\n             raise FileProcessingLockedError('File processing already locked')\r\n         else:\r\n-            cls.objects.create(mark=datetime.utcnow())\r\n+            cls.objects.create(lock_time=datetime.utcnow(), is_actual_lock=True)\r\n     \r\n     @classmethod\r\n     def unlock(cls):\r\n@@ -115,8 +117,8 @@ def unlock(cls):\n     \r\n     @classmethod\r\n     def islocked(cls):\r\n-        return cls.objects.exists()\r\n+        return cls.objects.filter(is_actual_lock=True).exists()\r\n     \r\n     @classmethod\r\n     def get_time_since_locked(cls):\r\n-        return datetime.utcnow() - FileProcessLock.objects.first().lock_time\r\n+        return datetime.utcnow() - FileProcessLock.objects.get(is_actual_lock=True).lock_time\r\ndiff --git a\/study\/migrations\/0004_fileprocesslock_is_actual_lock.py b\/study\/migrations\/0004_fileprocesslock_is_actual_lock.py\nnew file mode 100644\nindex 000000000..099ca8adf\n--- \/dev\/null\n+++ b\/study\/migrations\/0004_fileprocesslock_is_actual_lock.py\n@@ -0,0 +1,21 @@\n+# -*- coding: utf-8 -*-\n+# Generated by Django 1.11.5 on 2017-09-13 15:06\n+from __future__ import unicode_literals\n+\n+from django.db import migrations, models\n+\n+\n+class Migration(migrations.Migration):\n+\n+    dependencies = [\n+        ('study', '0003_auto_20170913_1424'),\n+    ]\n+\n+    operations = [\n+        migrations.AddField(\n+            model_name='fileprocesslock',\n+            name='is_actual_lock',\n+            field=models.BooleanField(default=True, help_text=b'This is True if the Lock is actually a lock', unique=True),\n+            preserve_default=False,\n+        ),\n+    ]\n","files":{"\/study\/data_access_models.py":{"changes":[{"diff":"\n class FileProcessLock(AbstractModel):\r\n     \r\n     lock_time = models.DateTimeField(null=True)\r\n-    # AJK TODO should we enforce on the database level that there can only be one FPL?\r\n+    \r\n+    # In case of weirdness, ensure that there is only one lock object\r\n+    is_actual_lock = models.BooleanField(unique=True, help_text=\"This is True if the Lock is actually a lock\")\r\n     \r\n     @classmethod\r\n     def lock(cls):\r\n-        if cls.objects.exists():\r\n+        if cls.islocked():\r\n             raise FileProcessingLockedError('File processing already locked')\r\n         else:\r\n-            cls.objects.create(mark=datetime.utcnow())\r\n+            cls.objects.create(lock_time=datetime.utcnow(), is_actual_lock=True)\r\n     \r\n     @classmethod\r\n     def unlock(cls):\r\n","add":5,"remove":3,"filename":"\/study\/data_access_models.py","badparts":["        if cls.objects.exists():\r","            cls.objects.create(mark=datetime.utcnow())\r"],"goodparts":["    \r","    is_actual_lock = models.BooleanField(unique=True, help_text=\"This is True if the Lock is actually a lock\")\r","        if cls.islocked():\r","            cls.objects.create(lock_time=datetime.utcnow(), is_actual_lock=True)\r"]},{"diff":"\n     \r\n     @classmethod\r\n     def islocked(cls):\r\n-        return cls.objects.exists()\r\n+        return cls.objects.filter(is_actual_lock=True).exists()\r\n     \r\n     @classmethod\r\n     def get_time_since_locked(cls):\r\n-        return datetime.utcnow() - FileProcessLock.objects.first().lock_time\r\n+        return datetime.utcnow() - FileProcessLock.objects.get(is_actual_lock=True).lock_time\r","add":2,"remove":2,"filename":"\/study\/data_access_models.py","badparts":["        return cls.objects.exists()\r","        return datetime.utcnow() - FileProcessLock.objects.first().lock_time\r"],"goodparts":["        return cls.objects.filter(is_actual_lock=True).exists()\r","        return datetime.utcnow() - FileProcessLock.objects.get(is_actual_lock=True).lock_time\r"]}],"source":"\nfrom datetime import datetime\r \r from django.db import models\r \r from config.constants import ALL_DATA_STREAMS, CHUNKABLE_FILES, CHUNK_TIMESLICE_QUANTUM\r from libs.security import chunk_hash, low_memory_chunk_hash\r from study.base_models import AbstractModel\r from study.study_models import Study\r \r \r class FileProcessingLockedError(Exception): pass\r \r \r class ChunkRegistry(AbstractModel):\r \r DATA_TYPE_CHOICES=tuple([(stream_name, stream_name) for stream_name in ALL_DATA_STREAMS])\r \r is_chunkable=models.BooleanField()\r chunk_path=models.CharField(max_length=256)\r chunk_hash=models.CharField(max_length=25, blank=True)\r \r data_type=models.CharField(max_length=32, choices=DATA_TYPE_CHOICES) time_bin=models.DateTimeField() \r study=models.ForeignKey('Study', on_delete=models.PROTECT, related_name='chunk_registries') participant=models.ForeignKey('Participant', on_delete=models.PROTECT, related_name='chunk_registries') survey=models.ForeignKey('Survey', blank=True, null=True, on_delete=models.PROTECT, related_name='chunk_registries') \r @classmethod\r def add_new_chunk(cls, data_type, time_bin, file_contents=None, **kwargs):\r is_chunkable=data_type in CHUNKABLE_FILES\r if is_chunkable:\r time_bin=int(time_bin) * CHUNK_TIMESLICE_QUANTUM\r chunk_hash_str=chunk_hash(file_contents)\r else:\r chunk_hash_str=''\r \r cls.objects.create(\r is_chunkable=is_chunkable,\r chunk_hash=chunk_hash_str,\r data_type=data_type,\r time_bin=datetime.fromtimestamp(time_bin),\r **kwargs\r )\r \r @classmethod\r def get_chunks_time_range(cls, study_id, user_ids=None, data_types=None, start=None, end=None):\r \"\"\"\r This function uses Django query syntax to provide datetimes and have Django do the\r comparison operation, and the 'in' operator to have Django only match the user list\r provided.\r \"\"\"\r \r query={'study_id': study_id}\r if user_ids:\r query['participant__patient_id__in']=user_ids\r if data_types:\r query['data_type__in']=data_types\r if start:\r query['time_bin__gte']=start\r if end:\r query['time_bin__lte']=end\r return cls.objects.filter(**query)\r \r def update_chunk_hash(self, data_to_hash):\r self.chunk_hash=chunk_hash(data_to_hash)\r self.save()\r \r def low_memory_update_chunk_hash(self, list_data_to_hash):\r self.chunk_hash=low_memory_chunk_hash(list_data_to_hash)\r self.save()\r \r \r class FileToProcess(AbstractModel):\r \r s3_file_path=models.CharField(max_length=256, blank=False)\r \r study=models.ForeignKey('Study', on_delete=models.PROTECT, related_name='files_to_process')\r participant=models.ForeignKey('Participant', on_delete=models.PROTECT, related_name='files_to_process')\r \r @classmethod\r def append_file_for_processing(cls, file_path, study_value, **kwargs):\r if Study.objects.filter(object_id=study_value).exists():\r study_pk=Study.objects.filter(object_id=study_value).values_list('pk', flat=True).get()\r study_object_id=study_value\r else:\r study_pk=study_value\r study_object_id=Study.objects.filter(pk=study_value).values_list('object_id', flat=True).get()\r \r if file_path[:24]==study_object_id:\r cls.objects.create(s3_file_path=file_path, study_id=study_pk, **kwargs)\r else:\r cls.objects.create(s3_file_path=study_object_id +'\/' +file_path, study_id=study_pk, **kwargs)\r \r \r class FileProcessLock(AbstractModel):\r \r lock_time=models.DateTimeField(null=True)\r \r @classmethod\r def lock(cls):\r if cls.objects.exists():\r raise FileProcessingLockedError('File processing already locked')\r else:\r cls.objects.create(mark=datetime.utcnow())\r \r @classmethod\r def unlock(cls):\r cls.objects.all().delete()\r \r @classmethod\r def islocked(cls):\r return cls.objects.exists()\r \r @classmethod\r def get_time_since_locked(cls):\r return datetime.utcnow() -FileProcessLock.objects.first().lock_time\r ","sourceWithComments":"from datetime import datetime\r\n\r\nfrom django.db import models\r\n\r\nfrom config.constants import ALL_DATA_STREAMS, CHUNKABLE_FILES, CHUNK_TIMESLICE_QUANTUM\r\nfrom libs.security import chunk_hash, low_memory_chunk_hash\r\nfrom study.base_models import AbstractModel\r\nfrom study.study_models import Study\r\n\r\n\r\nclass FileProcessingLockedError(Exception): pass\r\n\r\n\r\nclass ChunkRegistry(AbstractModel):\r\n\r\n    DATA_TYPE_CHOICES = tuple([(stream_name, stream_name) for stream_name in ALL_DATA_STREAMS])\r\n\r\n    is_chunkable = models.BooleanField()\r\n    chunk_path = models.CharField(max_length=256)\r\n    chunk_hash = models.CharField(max_length=25, blank=True)\r\n\r\n    data_type = models.CharField(max_length=32, choices=DATA_TYPE_CHOICES)  # , db_index=True)\r\n    time_bin = models.DateTimeField()  # db_index=True)\r\n\r\n    study = models.ForeignKey('Study', on_delete=models.PROTECT, related_name='chunk_registries')  # , db_index=True)\r\n    participant = models.ForeignKey('Participant', on_delete=models.PROTECT, related_name='chunk_registries')  # , db_index=True)\r\n    survey = models.ForeignKey('Survey', blank=True, null=True, on_delete=models.PROTECT, related_name='chunk_registries')  # , db_index=True)\r\n\r\n    @classmethod\r\n    def add_new_chunk(cls, data_type, time_bin, file_contents=None, **kwargs):\r\n        is_chunkable = data_type in CHUNKABLE_FILES\r\n        if is_chunkable:\r\n            time_bin = int(time_bin) * CHUNK_TIMESLICE_QUANTUM\r\n            chunk_hash_str = chunk_hash(file_contents)\r\n        else:\r\n            chunk_hash_str = ''\r\n\r\n        cls.objects.create(\r\n            is_chunkable=is_chunkable,\r\n            chunk_hash=chunk_hash_str,\r\n            data_type=data_type,\r\n            time_bin=datetime.fromtimestamp(time_bin),\r\n            **kwargs\r\n        )\r\n\r\n    @classmethod\r\n    def get_chunks_time_range(cls, study_id, user_ids=None, data_types=None, start=None, end=None):\r\n        \"\"\"\r\n        This function uses Django query syntax to provide datetimes and have Django do the\r\n        comparison operation, and the 'in' operator to have Django only match the user list\r\n        provided.\r\n        \"\"\"\r\n\r\n        query = {'study_id': study_id}\r\n        if user_ids:\r\n            query['participant__patient_id__in'] = user_ids\r\n        if data_types:\r\n            query['data_type__in'] = data_types\r\n        if start:\r\n            query['time_bin__gte'] = start\r\n        if end:\r\n            query['time_bin__lte'] = end\r\n        return cls.objects.filter(**query)\r\n\r\n    def update_chunk_hash(self, data_to_hash):\r\n        self.chunk_hash = chunk_hash(data_to_hash)\r\n        self.save()\r\n\r\n    def low_memory_update_chunk_hash(self, list_data_to_hash):\r\n        self.chunk_hash = low_memory_chunk_hash(list_data_to_hash)\r\n        self.save()\r\n\r\n\r\nclass FileToProcess(AbstractModel):\r\n\r\n    s3_file_path = models.CharField(max_length=256, blank=False)\r\n\r\n    study = models.ForeignKey('Study', on_delete=models.PROTECT, related_name='files_to_process')\r\n    participant = models.ForeignKey('Participant', on_delete=models.PROTECT, related_name='files_to_process')\r\n\r\n    @classmethod\r\n    def append_file_for_processing(cls, file_path, study_value, **kwargs):\r\n        # Get the study's ID\r\n        # AJK TODO depending how this is used in the codebase, stick with passing a single type of study_value\r\n        if Study.objects.filter(object_id=study_value).exists():\r\n            # A Study object_id was passed (as a string)\r\n            study_pk = Study.objects.filter(object_id=study_value).values_list('pk', flat=True).get()\r\n            study_object_id = study_value\r\n        else:\r\n            # A Study primary key was passed\r\n            study_pk = study_value\r\n            study_object_id = Study.objects.filter(pk=study_value).values_list('object_id', flat=True).get()\r\n\r\n        if file_path[:24] == study_object_id:\r\n            cls.objects.create(s3_file_path=file_path, study_id=study_pk, **kwargs)\r\n        else:\r\n            cls.objects.create(s3_file_path=study_object_id + '\/' + file_path, study_id=study_pk, **kwargs)\r\n\r\n\r\nclass FileProcessLock(AbstractModel):\r\n    \r\n    lock_time = models.DateTimeField(null=True)\r\n    # AJK TODO should we enforce on the database level that there can only be one FPL?\r\n    \r\n    @classmethod\r\n    def lock(cls):\r\n        if cls.objects.exists():\r\n            raise FileProcessingLockedError('File processing already locked')\r\n        else:\r\n            cls.objects.create(mark=datetime.utcnow())\r\n    \r\n    @classmethod\r\n    def unlock(cls):\r\n        cls.objects.all().delete()\r\n    \r\n    @classmethod\r\n    def islocked(cls):\r\n        return cls.objects.exists()\r\n    \r\n    @classmethod\r\n    def get_time_since_locked(cls):\r\n        return datetime.utcnow() - FileProcessLock.objects.first().lock_time\r\n"}},"msg":"Edited FileProcessLock to prevent tampering or weirdness"}},"https:\/\/github.com\/xuancong84\/beiwe-backend":{"543d6fc63e8fab609f14dd31d3cc9a58eb2d367a":{"url":"https:\/\/api.github.com\/repos\/xuancong84\/beiwe-backend\/commits\/543d6fc63e8fab609f14dd31d3cc9a58eb2d367a","html_url":"https:\/\/github.com\/xuancong84\/beiwe-backend\/commit\/543d6fc63e8fab609f14dd31d3cc9a58eb2d367a","message":"Edited FileProcessLock to prevent tampering or weirdness","sha":"543d6fc63e8fab609f14dd31d3cc9a58eb2d367a","keyword":"tampering prevent","diff":"diff --git a\/study\/data_access_models.py b\/study\/data_access_models.py\nindex 2680aa1..cbd0879 100644\n--- a\/study\/data_access_models.py\n+++ b\/study\/data_access_models.py\n@@ -100,14 +100,16 @@ def append_file_for_processing(cls, file_path, study_value, **kwargs):\n class FileProcessLock(AbstractModel):\r\n     \r\n     lock_time = models.DateTimeField(null=True)\r\n-    # AJK TODO should we enforce on the database level that there can only be one FPL?\r\n+    \r\n+    # In case of weirdness, ensure that there is only one lock object\r\n+    is_actual_lock = models.BooleanField(unique=True, help_text=\"This is True if the Lock is actually a lock\")\r\n     \r\n     @classmethod\r\n     def lock(cls):\r\n-        if cls.objects.exists():\r\n+        if cls.islocked():\r\n             raise FileProcessingLockedError('File processing already locked')\r\n         else:\r\n-            cls.objects.create(mark=datetime.utcnow())\r\n+            cls.objects.create(lock_time=datetime.utcnow(), is_actual_lock=True)\r\n     \r\n     @classmethod\r\n     def unlock(cls):\r\n@@ -115,8 +117,8 @@ def unlock(cls):\n     \r\n     @classmethod\r\n     def islocked(cls):\r\n-        return cls.objects.exists()\r\n+        return cls.objects.filter(is_actual_lock=True).exists()\r\n     \r\n     @classmethod\r\n     def get_time_since_locked(cls):\r\n-        return datetime.utcnow() - FileProcessLock.objects.first().lock_time\r\n+        return datetime.utcnow() - FileProcessLock.objects.get(is_actual_lock=True).lock_time\r\ndiff --git a\/study\/migrations\/0004_fileprocesslock_is_actual_lock.py b\/study\/migrations\/0004_fileprocesslock_is_actual_lock.py\nnew file mode 100644\nindex 0000000..099ca8a\n--- \/dev\/null\n+++ b\/study\/migrations\/0004_fileprocesslock_is_actual_lock.py\n@@ -0,0 +1,21 @@\n+# -*- coding: utf-8 -*-\n+# Generated by Django 1.11.5 on 2017-09-13 15:06\n+from __future__ import unicode_literals\n+\n+from django.db import migrations, models\n+\n+\n+class Migration(migrations.Migration):\n+\n+    dependencies = [\n+        ('study', '0003_auto_20170913_1424'),\n+    ]\n+\n+    operations = [\n+        migrations.AddField(\n+            model_name='fileprocesslock',\n+            name='is_actual_lock',\n+            field=models.BooleanField(default=True, help_text=b'This is True if the Lock is actually a lock', unique=True),\n+            preserve_default=False,\n+        ),\n+    ]\n","files":{"\/study\/data_access_models.py":{"changes":[{"diff":"\n class FileProcessLock(AbstractModel):\r\n     \r\n     lock_time = models.DateTimeField(null=True)\r\n-    # AJK TODO should we enforce on the database level that there can only be one FPL?\r\n+    \r\n+    # In case of weirdness, ensure that there is only one lock object\r\n+    is_actual_lock = models.BooleanField(unique=True, help_text=\"This is True if the Lock is actually a lock\")\r\n     \r\n     @classmethod\r\n     def lock(cls):\r\n-        if cls.objects.exists():\r\n+        if cls.islocked():\r\n             raise FileProcessingLockedError('File processing already locked')\r\n         else:\r\n-            cls.objects.create(mark=datetime.utcnow())\r\n+            cls.objects.create(lock_time=datetime.utcnow(), is_actual_lock=True)\r\n     \r\n     @classmethod\r\n     def unlock(cls):\r\n","add":5,"remove":3,"filename":"\/study\/data_access_models.py","badparts":["        if cls.objects.exists():\r","            cls.objects.create(mark=datetime.utcnow())\r"],"goodparts":["    \r","    is_actual_lock = models.BooleanField(unique=True, help_text=\"This is True if the Lock is actually a lock\")\r","        if cls.islocked():\r","            cls.objects.create(lock_time=datetime.utcnow(), is_actual_lock=True)\r"]},{"diff":"\n     \r\n     @classmethod\r\n     def islocked(cls):\r\n-        return cls.objects.exists()\r\n+        return cls.objects.filter(is_actual_lock=True).exists()\r\n     \r\n     @classmethod\r\n     def get_time_since_locked(cls):\r\n-        return datetime.utcnow() - FileProcessLock.objects.first().lock_time\r\n+        return datetime.utcnow() - FileProcessLock.objects.get(is_actual_lock=True).lock_time\r","add":2,"remove":2,"filename":"\/study\/data_access_models.py","badparts":["        return cls.objects.exists()\r","        return datetime.utcnow() - FileProcessLock.objects.first().lock_time\r"],"goodparts":["        return cls.objects.filter(is_actual_lock=True).exists()\r","        return datetime.utcnow() - FileProcessLock.objects.get(is_actual_lock=True).lock_time\r"]}],"source":"\nfrom datetime import datetime\r \r from django.db import models\r \r from config.constants import ALL_DATA_STREAMS, CHUNKABLE_FILES, CHUNK_TIMESLICE_QUANTUM\r from libs.security import chunk_hash, low_memory_chunk_hash\r from study.base_models import AbstractModel\r from study.study_models import Study\r \r \r class FileProcessingLockedError(Exception): pass\r \r \r class ChunkRegistry(AbstractModel):\r \r DATA_TYPE_CHOICES=tuple([(stream_name, stream_name) for stream_name in ALL_DATA_STREAMS])\r \r is_chunkable=models.BooleanField()\r chunk_path=models.CharField(max_length=256)\r chunk_hash=models.CharField(max_length=25, blank=True)\r \r data_type=models.CharField(max_length=32, choices=DATA_TYPE_CHOICES) time_bin=models.DateTimeField() \r study=models.ForeignKey('Study', on_delete=models.PROTECT, related_name='chunk_registries') participant=models.ForeignKey('Participant', on_delete=models.PROTECT, related_name='chunk_registries') survey=models.ForeignKey('Survey', blank=True, null=True, on_delete=models.PROTECT, related_name='chunk_registries') \r @classmethod\r def add_new_chunk(cls, data_type, time_bin, file_contents=None, **kwargs):\r is_chunkable=data_type in CHUNKABLE_FILES\r if is_chunkable:\r time_bin=int(time_bin) * CHUNK_TIMESLICE_QUANTUM\r chunk_hash_str=chunk_hash(file_contents)\r else:\r chunk_hash_str=''\r \r cls.objects.create(\r is_chunkable=is_chunkable,\r chunk_hash=chunk_hash_str,\r data_type=data_type,\r time_bin=datetime.fromtimestamp(time_bin),\r **kwargs\r )\r \r @classmethod\r def get_chunks_time_range(cls, study_id, user_ids=None, data_types=None, start=None, end=None):\r \"\"\"\r This function uses Django query syntax to provide datetimes and have Django do the\r comparison operation, and the 'in' operator to have Django only match the user list\r provided.\r \"\"\"\r \r query={'study_id': study_id}\r if user_ids:\r query['participant__patient_id__in']=user_ids\r if data_types:\r query['data_type__in']=data_types\r if start:\r query['time_bin__gte']=start\r if end:\r query['time_bin__lte']=end\r return cls.objects.filter(**query)\r \r def update_chunk_hash(self, data_to_hash):\r self.chunk_hash=chunk_hash(data_to_hash)\r self.save()\r \r def low_memory_update_chunk_hash(self, list_data_to_hash):\r self.chunk_hash=low_memory_chunk_hash(list_data_to_hash)\r self.save()\r \r \r class FileToProcess(AbstractModel):\r \r s3_file_path=models.CharField(max_length=256, blank=False)\r \r study=models.ForeignKey('Study', on_delete=models.PROTECT, related_name='files_to_process')\r participant=models.ForeignKey('Participant', on_delete=models.PROTECT, related_name='files_to_process')\r \r @classmethod\r def append_file_for_processing(cls, file_path, study_value, **kwargs):\r if Study.objects.filter(object_id=study_value).exists():\r study_pk=Study.objects.filter(object_id=study_value).values_list('pk', flat=True).get()\r study_object_id=study_value\r else:\r study_pk=study_value\r study_object_id=Study.objects.filter(pk=study_value).values_list('object_id', flat=True).get()\r \r if file_path[:24]==study_object_id:\r cls.objects.create(s3_file_path=file_path, study_id=study_pk, **kwargs)\r else:\r cls.objects.create(s3_file_path=study_object_id +'\/' +file_path, study_id=study_pk, **kwargs)\r \r \r class FileProcessLock(AbstractModel):\r \r lock_time=models.DateTimeField(null=True)\r \r @classmethod\r def lock(cls):\r if cls.objects.exists():\r raise FileProcessingLockedError('File processing already locked')\r else:\r cls.objects.create(mark=datetime.utcnow())\r \r @classmethod\r def unlock(cls):\r cls.objects.all().delete()\r \r @classmethod\r def islocked(cls):\r return cls.objects.exists()\r \r @classmethod\r def get_time_since_locked(cls):\r return datetime.utcnow() -FileProcessLock.objects.first().lock_time\r ","sourceWithComments":"from datetime import datetime\r\n\r\nfrom django.db import models\r\n\r\nfrom config.constants import ALL_DATA_STREAMS, CHUNKABLE_FILES, CHUNK_TIMESLICE_QUANTUM\r\nfrom libs.security import chunk_hash, low_memory_chunk_hash\r\nfrom study.base_models import AbstractModel\r\nfrom study.study_models import Study\r\n\r\n\r\nclass FileProcessingLockedError(Exception): pass\r\n\r\n\r\nclass ChunkRegistry(AbstractModel):\r\n\r\n    DATA_TYPE_CHOICES = tuple([(stream_name, stream_name) for stream_name in ALL_DATA_STREAMS])\r\n\r\n    is_chunkable = models.BooleanField()\r\n    chunk_path = models.CharField(max_length=256)\r\n    chunk_hash = models.CharField(max_length=25, blank=True)\r\n\r\n    data_type = models.CharField(max_length=32, choices=DATA_TYPE_CHOICES)  # , db_index=True)\r\n    time_bin = models.DateTimeField()  # db_index=True)\r\n\r\n    study = models.ForeignKey('Study', on_delete=models.PROTECT, related_name='chunk_registries')  # , db_index=True)\r\n    participant = models.ForeignKey('Participant', on_delete=models.PROTECT, related_name='chunk_registries')  # , db_index=True)\r\n    survey = models.ForeignKey('Survey', blank=True, null=True, on_delete=models.PROTECT, related_name='chunk_registries')  # , db_index=True)\r\n\r\n    @classmethod\r\n    def add_new_chunk(cls, data_type, time_bin, file_contents=None, **kwargs):\r\n        is_chunkable = data_type in CHUNKABLE_FILES\r\n        if is_chunkable:\r\n            time_bin = int(time_bin) * CHUNK_TIMESLICE_QUANTUM\r\n            chunk_hash_str = chunk_hash(file_contents)\r\n        else:\r\n            chunk_hash_str = ''\r\n\r\n        cls.objects.create(\r\n            is_chunkable=is_chunkable,\r\n            chunk_hash=chunk_hash_str,\r\n            data_type=data_type,\r\n            time_bin=datetime.fromtimestamp(time_bin),\r\n            **kwargs\r\n        )\r\n\r\n    @classmethod\r\n    def get_chunks_time_range(cls, study_id, user_ids=None, data_types=None, start=None, end=None):\r\n        \"\"\"\r\n        This function uses Django query syntax to provide datetimes and have Django do the\r\n        comparison operation, and the 'in' operator to have Django only match the user list\r\n        provided.\r\n        \"\"\"\r\n\r\n        query = {'study_id': study_id}\r\n        if user_ids:\r\n            query['participant__patient_id__in'] = user_ids\r\n        if data_types:\r\n            query['data_type__in'] = data_types\r\n        if start:\r\n            query['time_bin__gte'] = start\r\n        if end:\r\n            query['time_bin__lte'] = end\r\n        return cls.objects.filter(**query)\r\n\r\n    def update_chunk_hash(self, data_to_hash):\r\n        self.chunk_hash = chunk_hash(data_to_hash)\r\n        self.save()\r\n\r\n    def low_memory_update_chunk_hash(self, list_data_to_hash):\r\n        self.chunk_hash = low_memory_chunk_hash(list_data_to_hash)\r\n        self.save()\r\n\r\n\r\nclass FileToProcess(AbstractModel):\r\n\r\n    s3_file_path = models.CharField(max_length=256, blank=False)\r\n\r\n    study = models.ForeignKey('Study', on_delete=models.PROTECT, related_name='files_to_process')\r\n    participant = models.ForeignKey('Participant', on_delete=models.PROTECT, related_name='files_to_process')\r\n\r\n    @classmethod\r\n    def append_file_for_processing(cls, file_path, study_value, **kwargs):\r\n        # Get the study's ID\r\n        # AJK TODO depending how this is used in the codebase, stick with passing a single type of study_value\r\n        if Study.objects.filter(object_id=study_value).exists():\r\n            # A Study object_id was passed (as a string)\r\n            study_pk = Study.objects.filter(object_id=study_value).values_list('pk', flat=True).get()\r\n            study_object_id = study_value\r\n        else:\r\n            # A Study primary key was passed\r\n            study_pk = study_value\r\n            study_object_id = Study.objects.filter(pk=study_value).values_list('object_id', flat=True).get()\r\n\r\n        if file_path[:24] == study_object_id:\r\n            cls.objects.create(s3_file_path=file_path, study_id=study_pk, **kwargs)\r\n        else:\r\n            cls.objects.create(s3_file_path=study_object_id + '\/' + file_path, study_id=study_pk, **kwargs)\r\n\r\n\r\nclass FileProcessLock(AbstractModel):\r\n    \r\n    lock_time = models.DateTimeField(null=True)\r\n    # AJK TODO should we enforce on the database level that there can only be one FPL?\r\n    \r\n    @classmethod\r\n    def lock(cls):\r\n        if cls.objects.exists():\r\n            raise FileProcessingLockedError('File processing already locked')\r\n        else:\r\n            cls.objects.create(mark=datetime.utcnow())\r\n    \r\n    @classmethod\r\n    def unlock(cls):\r\n        cls.objects.all().delete()\r\n    \r\n    @classmethod\r\n    def islocked(cls):\r\n        return cls.objects.exists()\r\n    \r\n    @classmethod\r\n    def get_time_since_locked(cls):\r\n        return datetime.utcnow() - FileProcessLock.objects.first().lock_time\r\n"}},"msg":"Edited FileProcessLock to prevent tampering or weirdness"}},"https:\/\/github.com\/heinzfutto\/server-backend":{"543d6fc63e8fab609f14dd31d3cc9a58eb2d367a":{"url":"https:\/\/api.github.com\/repos\/heinzfutto\/server-backend\/commits\/543d6fc63e8fab609f14dd31d3cc9a58eb2d367a","html_url":"https:\/\/github.com\/heinzfutto\/server-backend\/commit\/543d6fc63e8fab609f14dd31d3cc9a58eb2d367a","message":"Edited FileProcessLock to prevent tampering or weirdness","sha":"543d6fc63e8fab609f14dd31d3cc9a58eb2d367a","keyword":"tampering prevent","diff":"diff --git a\/study\/data_access_models.py b\/study\/data_access_models.py\nindex 2680aa1..cbd0879 100644\n--- a\/study\/data_access_models.py\n+++ b\/study\/data_access_models.py\n@@ -100,14 +100,16 @@ def append_file_for_processing(cls, file_path, study_value, **kwargs):\n class FileProcessLock(AbstractModel):\r\n     \r\n     lock_time = models.DateTimeField(null=True)\r\n-    # AJK TODO should we enforce on the database level that there can only be one FPL?\r\n+    \r\n+    # In case of weirdness, ensure that there is only one lock object\r\n+    is_actual_lock = models.BooleanField(unique=True, help_text=\"This is True if the Lock is actually a lock\")\r\n     \r\n     @classmethod\r\n     def lock(cls):\r\n-        if cls.objects.exists():\r\n+        if cls.islocked():\r\n             raise FileProcessingLockedError('File processing already locked')\r\n         else:\r\n-            cls.objects.create(mark=datetime.utcnow())\r\n+            cls.objects.create(lock_time=datetime.utcnow(), is_actual_lock=True)\r\n     \r\n     @classmethod\r\n     def unlock(cls):\r\n@@ -115,8 +117,8 @@ def unlock(cls):\n     \r\n     @classmethod\r\n     def islocked(cls):\r\n-        return cls.objects.exists()\r\n+        return cls.objects.filter(is_actual_lock=True).exists()\r\n     \r\n     @classmethod\r\n     def get_time_since_locked(cls):\r\n-        return datetime.utcnow() - FileProcessLock.objects.first().lock_time\r\n+        return datetime.utcnow() - FileProcessLock.objects.get(is_actual_lock=True).lock_time\r\ndiff --git a\/study\/migrations\/0004_fileprocesslock_is_actual_lock.py b\/study\/migrations\/0004_fileprocesslock_is_actual_lock.py\nnew file mode 100644\nindex 0000000..099ca8a\n--- \/dev\/null\n+++ b\/study\/migrations\/0004_fileprocesslock_is_actual_lock.py\n@@ -0,0 +1,21 @@\n+# -*- coding: utf-8 -*-\n+# Generated by Django 1.11.5 on 2017-09-13 15:06\n+from __future__ import unicode_literals\n+\n+from django.db import migrations, models\n+\n+\n+class Migration(migrations.Migration):\n+\n+    dependencies = [\n+        ('study', '0003_auto_20170913_1424'),\n+    ]\n+\n+    operations = [\n+        migrations.AddField(\n+            model_name='fileprocesslock',\n+            name='is_actual_lock',\n+            field=models.BooleanField(default=True, help_text=b'This is True if the Lock is actually a lock', unique=True),\n+            preserve_default=False,\n+        ),\n+    ]\n","files":{"\/study\/data_access_models.py":{"changes":[{"diff":"\n class FileProcessLock(AbstractModel):\r\n     \r\n     lock_time = models.DateTimeField(null=True)\r\n-    # AJK TODO should we enforce on the database level that there can only be one FPL?\r\n+    \r\n+    # In case of weirdness, ensure that there is only one lock object\r\n+    is_actual_lock = models.BooleanField(unique=True, help_text=\"This is True if the Lock is actually a lock\")\r\n     \r\n     @classmethod\r\n     def lock(cls):\r\n-        if cls.objects.exists():\r\n+        if cls.islocked():\r\n             raise FileProcessingLockedError('File processing already locked')\r\n         else:\r\n-            cls.objects.create(mark=datetime.utcnow())\r\n+            cls.objects.create(lock_time=datetime.utcnow(), is_actual_lock=True)\r\n     \r\n     @classmethod\r\n     def unlock(cls):\r\n","add":5,"remove":3,"filename":"\/study\/data_access_models.py","badparts":["        if cls.objects.exists():\r","            cls.objects.create(mark=datetime.utcnow())\r"],"goodparts":["    \r","    is_actual_lock = models.BooleanField(unique=True, help_text=\"This is True if the Lock is actually a lock\")\r","        if cls.islocked():\r","            cls.objects.create(lock_time=datetime.utcnow(), is_actual_lock=True)\r"]},{"diff":"\n     \r\n     @classmethod\r\n     def islocked(cls):\r\n-        return cls.objects.exists()\r\n+        return cls.objects.filter(is_actual_lock=True).exists()\r\n     \r\n     @classmethod\r\n     def get_time_since_locked(cls):\r\n-        return datetime.utcnow() - FileProcessLock.objects.first().lock_time\r\n+        return datetime.utcnow() - FileProcessLock.objects.get(is_actual_lock=True).lock_time\r","add":2,"remove":2,"filename":"\/study\/data_access_models.py","badparts":["        return cls.objects.exists()\r","        return datetime.utcnow() - FileProcessLock.objects.first().lock_time\r"],"goodparts":["        return cls.objects.filter(is_actual_lock=True).exists()\r","        return datetime.utcnow() - FileProcessLock.objects.get(is_actual_lock=True).lock_time\r"]}],"source":"\nfrom datetime import datetime\r \r from django.db import models\r \r from config.constants import ALL_DATA_STREAMS, CHUNKABLE_FILES, CHUNK_TIMESLICE_QUANTUM\r from libs.security import chunk_hash, low_memory_chunk_hash\r from study.base_models import AbstractModel\r from study.study_models import Study\r \r \r class FileProcessingLockedError(Exception): pass\r \r \r class ChunkRegistry(AbstractModel):\r \r DATA_TYPE_CHOICES=tuple([(stream_name, stream_name) for stream_name in ALL_DATA_STREAMS])\r \r is_chunkable=models.BooleanField()\r chunk_path=models.CharField(max_length=256)\r chunk_hash=models.CharField(max_length=25, blank=True)\r \r data_type=models.CharField(max_length=32, choices=DATA_TYPE_CHOICES) time_bin=models.DateTimeField() \r study=models.ForeignKey('Study', on_delete=models.PROTECT, related_name='chunk_registries') participant=models.ForeignKey('Participant', on_delete=models.PROTECT, related_name='chunk_registries') survey=models.ForeignKey('Survey', blank=True, null=True, on_delete=models.PROTECT, related_name='chunk_registries') \r @classmethod\r def add_new_chunk(cls, data_type, time_bin, file_contents=None, **kwargs):\r is_chunkable=data_type in CHUNKABLE_FILES\r if is_chunkable:\r time_bin=int(time_bin) * CHUNK_TIMESLICE_QUANTUM\r chunk_hash_str=chunk_hash(file_contents)\r else:\r chunk_hash_str=''\r \r cls.objects.create(\r is_chunkable=is_chunkable,\r chunk_hash=chunk_hash_str,\r data_type=data_type,\r time_bin=datetime.fromtimestamp(time_bin),\r **kwargs\r )\r \r @classmethod\r def get_chunks_time_range(cls, study_id, user_ids=None, data_types=None, start=None, end=None):\r \"\"\"\r This function uses Django query syntax to provide datetimes and have Django do the\r comparison operation, and the 'in' operator to have Django only match the user list\r provided.\r \"\"\"\r \r query={'study_id': study_id}\r if user_ids:\r query['participant__patient_id__in']=user_ids\r if data_types:\r query['data_type__in']=data_types\r if start:\r query['time_bin__gte']=start\r if end:\r query['time_bin__lte']=end\r return cls.objects.filter(**query)\r \r def update_chunk_hash(self, data_to_hash):\r self.chunk_hash=chunk_hash(data_to_hash)\r self.save()\r \r def low_memory_update_chunk_hash(self, list_data_to_hash):\r self.chunk_hash=low_memory_chunk_hash(list_data_to_hash)\r self.save()\r \r \r class FileToProcess(AbstractModel):\r \r s3_file_path=models.CharField(max_length=256, blank=False)\r \r study=models.ForeignKey('Study', on_delete=models.PROTECT, related_name='files_to_process')\r participant=models.ForeignKey('Participant', on_delete=models.PROTECT, related_name='files_to_process')\r \r @classmethod\r def append_file_for_processing(cls, file_path, study_value, **kwargs):\r if Study.objects.filter(object_id=study_value).exists():\r study_pk=Study.objects.filter(object_id=study_value).values_list('pk', flat=True).get()\r study_object_id=study_value\r else:\r study_pk=study_value\r study_object_id=Study.objects.filter(pk=study_value).values_list('object_id', flat=True).get()\r \r if file_path[:24]==study_object_id:\r cls.objects.create(s3_file_path=file_path, study_id=study_pk, **kwargs)\r else:\r cls.objects.create(s3_file_path=study_object_id +'\/' +file_path, study_id=study_pk, **kwargs)\r \r \r class FileProcessLock(AbstractModel):\r \r lock_time=models.DateTimeField(null=True)\r \r @classmethod\r def lock(cls):\r if cls.objects.exists():\r raise FileProcessingLockedError('File processing already locked')\r else:\r cls.objects.create(mark=datetime.utcnow())\r \r @classmethod\r def unlock(cls):\r cls.objects.all().delete()\r \r @classmethod\r def islocked(cls):\r return cls.objects.exists()\r \r @classmethod\r def get_time_since_locked(cls):\r return datetime.utcnow() -FileProcessLock.objects.first().lock_time\r ","sourceWithComments":"from datetime import datetime\r\n\r\nfrom django.db import models\r\n\r\nfrom config.constants import ALL_DATA_STREAMS, CHUNKABLE_FILES, CHUNK_TIMESLICE_QUANTUM\r\nfrom libs.security import chunk_hash, low_memory_chunk_hash\r\nfrom study.base_models import AbstractModel\r\nfrom study.study_models import Study\r\n\r\n\r\nclass FileProcessingLockedError(Exception): pass\r\n\r\n\r\nclass ChunkRegistry(AbstractModel):\r\n\r\n    DATA_TYPE_CHOICES = tuple([(stream_name, stream_name) for stream_name in ALL_DATA_STREAMS])\r\n\r\n    is_chunkable = models.BooleanField()\r\n    chunk_path = models.CharField(max_length=256)\r\n    chunk_hash = models.CharField(max_length=25, blank=True)\r\n\r\n    data_type = models.CharField(max_length=32, choices=DATA_TYPE_CHOICES)  # , db_index=True)\r\n    time_bin = models.DateTimeField()  # db_index=True)\r\n\r\n    study = models.ForeignKey('Study', on_delete=models.PROTECT, related_name='chunk_registries')  # , db_index=True)\r\n    participant = models.ForeignKey('Participant', on_delete=models.PROTECT, related_name='chunk_registries')  # , db_index=True)\r\n    survey = models.ForeignKey('Survey', blank=True, null=True, on_delete=models.PROTECT, related_name='chunk_registries')  # , db_index=True)\r\n\r\n    @classmethod\r\n    def add_new_chunk(cls, data_type, time_bin, file_contents=None, **kwargs):\r\n        is_chunkable = data_type in CHUNKABLE_FILES\r\n        if is_chunkable:\r\n            time_bin = int(time_bin) * CHUNK_TIMESLICE_QUANTUM\r\n            chunk_hash_str = chunk_hash(file_contents)\r\n        else:\r\n            chunk_hash_str = ''\r\n\r\n        cls.objects.create(\r\n            is_chunkable=is_chunkable,\r\n            chunk_hash=chunk_hash_str,\r\n            data_type=data_type,\r\n            time_bin=datetime.fromtimestamp(time_bin),\r\n            **kwargs\r\n        )\r\n\r\n    @classmethod\r\n    def get_chunks_time_range(cls, study_id, user_ids=None, data_types=None, start=None, end=None):\r\n        \"\"\"\r\n        This function uses Django query syntax to provide datetimes and have Django do the\r\n        comparison operation, and the 'in' operator to have Django only match the user list\r\n        provided.\r\n        \"\"\"\r\n\r\n        query = {'study_id': study_id}\r\n        if user_ids:\r\n            query['participant__patient_id__in'] = user_ids\r\n        if data_types:\r\n            query['data_type__in'] = data_types\r\n        if start:\r\n            query['time_bin__gte'] = start\r\n        if end:\r\n            query['time_bin__lte'] = end\r\n        return cls.objects.filter(**query)\r\n\r\n    def update_chunk_hash(self, data_to_hash):\r\n        self.chunk_hash = chunk_hash(data_to_hash)\r\n        self.save()\r\n\r\n    def low_memory_update_chunk_hash(self, list_data_to_hash):\r\n        self.chunk_hash = low_memory_chunk_hash(list_data_to_hash)\r\n        self.save()\r\n\r\n\r\nclass FileToProcess(AbstractModel):\r\n\r\n    s3_file_path = models.CharField(max_length=256, blank=False)\r\n\r\n    study = models.ForeignKey('Study', on_delete=models.PROTECT, related_name='files_to_process')\r\n    participant = models.ForeignKey('Participant', on_delete=models.PROTECT, related_name='files_to_process')\r\n\r\n    @classmethod\r\n    def append_file_for_processing(cls, file_path, study_value, **kwargs):\r\n        # Get the study's ID\r\n        # AJK TODO depending how this is used in the codebase, stick with passing a single type of study_value\r\n        if Study.objects.filter(object_id=study_value).exists():\r\n            # A Study object_id was passed (as a string)\r\n            study_pk = Study.objects.filter(object_id=study_value).values_list('pk', flat=True).get()\r\n            study_object_id = study_value\r\n        else:\r\n            # A Study primary key was passed\r\n            study_pk = study_value\r\n            study_object_id = Study.objects.filter(pk=study_value).values_list('object_id', flat=True).get()\r\n\r\n        if file_path[:24] == study_object_id:\r\n            cls.objects.create(s3_file_path=file_path, study_id=study_pk, **kwargs)\r\n        else:\r\n            cls.objects.create(s3_file_path=study_object_id + '\/' + file_path, study_id=study_pk, **kwargs)\r\n\r\n\r\nclass FileProcessLock(AbstractModel):\r\n    \r\n    lock_time = models.DateTimeField(null=True)\r\n    # AJK TODO should we enforce on the database level that there can only be one FPL?\r\n    \r\n    @classmethod\r\n    def lock(cls):\r\n        if cls.objects.exists():\r\n            raise FileProcessingLockedError('File processing already locked')\r\n        else:\r\n            cls.objects.create(mark=datetime.utcnow())\r\n    \r\n    @classmethod\r\n    def unlock(cls):\r\n        cls.objects.all().delete()\r\n    \r\n    @classmethod\r\n    def islocked(cls):\r\n        return cls.objects.exists()\r\n    \r\n    @classmethod\r\n    def get_time_since_locked(cls):\r\n        return datetime.utcnow() - FileProcessLock.objects.first().lock_time\r\n"}},"msg":"Edited FileProcessLock to prevent tampering or weirdness"}},"https:\/\/github.com\/CarlEdman\/godaddy-ddns":{"d5a1ab8674e24bc4b96222d98283d1448baff362":{"url":"https:\/\/api.github.com\/repos\/CarlEdman\/godaddy-ddns\/commits\/d5a1ab8674e24bc4b96222d98283d1448baff362","html_url":"https:\/\/github.com\/CarlEdman\/godaddy-ddns\/commit\/d5a1ab8674e24bc4b96222d98283d1448baff362","message":"tls support for ip lookup to add validation and prevent potential tampering.","sha":"d5a1ab8674e24bc4b96222d98283d1448baff362","keyword":"tampering prevent","diff":"diff --git a\/godaddy_ddns.py b\/godaddy_ddns.py\nindex b017879..cc5172c 100755\n--- a\/godaddy_ddns.py\n+++ b\/godaddy_ddns.py\n@@ -83,7 +83,7 @@ def main():\n \n   if not args.ip:\n     try:\n-      with urlopen(\"http:\/\/ipv4.icanhazip.com\/\") as f: resp=f.read()\n+      with urlopen(\"https:\/\/ipv4.icanhazip.com\/\") as f: resp=f.read()\n       if sys.version_info > (3,): resp = resp.decode('utf-8')\n       args.ip = resp.strip()\n     except URLError:\n","files":{"\/godaddy_ddns.py":{"changes":[{"diff":"\n \n   if not args.ip:\n     try:\n-      with urlopen(\"http:\/\/ipv4.icanhazip.com\/\") as f: resp=f.read()\n+      with urlopen(\"https:\/\/ipv4.icanhazip.com\/\") as f: resp=f.read()\n       if sys.version_info > (3,): resp = resp.decode('utf-8')\n       args.ip = resp.strip()\n     except URLError:\n","add":1,"remove":1,"filename":"\/godaddy_ddns.py","badparts":["      with urlopen(\"http:\/\/ipv4.icanhazip.com\/\") as f: resp=f.read()"],"goodparts":["      with urlopen(\"https:\/\/ipv4.icanhazip.com\/\") as f: resp=f.read()"]}],"source":"\n prog='godaddy-ddns' version='0.3' author='Carl Edman(CarlEdman@gmail.com)' import sys, json, argparse if sys.version_info >(3,): from urllib.request import urlopen, Request from urllib.error import URLError, HTTPError else: from urllib2 import urlopen, Request from urllib2 import URLError, HTTPError parser=argparse.ArgumentParser(description='Update GoDaddy DNS \"A\" Record.', fromfile_prefix_chars='%', epilog=\\ '''GoDaddy customers can obtain values for the KEY and SECRET arguments by creating a production key at https:\/\/developer.godaddy.com\/keys\/. Note that command line arguments may be specified in a FILE, one to a line, by instead giving the argument \"%FILE\". For security reasons, it is particularly recommended to supply the KEY and SECRET arguments in such a file, rather than directly on the command line.''') parser.add_argument('--version', action='version', version='{}{}'.format(prog, version)) parser.add_argument('hostname', type=str, help='DNS fully-qualified host name with an A record. If the hostname consists of only a domain name(i.e., it contains only one period), the record for @ is updated.') parser.add_argument('--ip', type=str, default=None, help='DNS Address(defaults to public WAN address from http:\/\/ipv4.icanhazip.com\/)') parser.add_argument('--key', type=str, default='', help='GoDaddy production key') parser.add_argument('--secret', type=str, default='', help='GoDaddy production secret') parser.add_argument('--ttl', type=int, default=3600, help='DNS TTL.') args=parser.parse_args() def main(): hostnames=args.hostname.split('.') if len(hostnames)<2: msg='Hostname \"{}\" is not a fully-qualified host name of form \"HOST.DOMAIN.TOP\".'.format(args.hostname) raise Exception(msg) elif len(hostnames)<3: hostnames.insert(0,'@') if not args.ip: try: with urlopen(\"http:\/\/ipv4.icanhazip.com\/\") as f: resp=f.read() if sys.version_info >(3,): resp=resp.decode('utf-8') args.ip=resp.strip() except URLError: msg='Unable to automatically obtain IP address from http:\/\/ipv4.icanhazip.com\/.' raise Exception(msg) ipslist=args.ip.split(\",\") for ipsiter in ipslist: ips=ipsiter.split('.') if len(ips)!=4 or \\ not ips[0].isdigit() or not ips[1].isdigit() or not ips[2].isdigit() or not ips[3].isdigit() or \\ int(ips[0])>255 or int(ips[1])>255 or int(ips[2])>255 or int(ips[3])>255: msg='\"{}\" is not valid IP address.'.format(args.ip) raise Exception(msg) url='https:\/\/api.godaddy.com\/v1\/domains\/{}\/records\/A\/{}'.format('.'.join(hostnames[1:]),hostnames[0]) data=json.dumps([{ \"data\": ip, \"ttl\": args.ttl, \"name\": hostnames[0], \"type\": \"A\"} for ip in ipslist]) if sys.version_info >(3,): data=data.encode('utf-8') req=Request(url, method='PUT', data=data) req.add_header(\"Content-Type\",\"application\/json\") req.add_header(\"Accept\",\"application\/json\") if args.key and args.secret: req.add_header(\"Authorization\", \"sso-key{}:{}\".format(args.key,args.secret)) try: with urlopen(req) as f: resp=f.read() if sys.version_info >(3,): resp=resp.decode('utf-8') except HTTPError as e: if e.code==400: msg='Unable to set IP address: GoDaddy API URL({}) was malformed.'.format(req.full_url) elif e.code==401: if args.key and args.secret: msg='''Unable to set IP address: --key or --secret option incorrect. Correct values can be obtained from from https:\/\/developer.godaddy.com\/keys\/ and are ideally placed in a % file.''' else: msg='''Unable to set IP address: --key or --secret option missing. Correct values can be obtained from from https:\/\/developer.godaddy.com\/keys\/ and are ideally placed in a % file.''' elif e.code==403: msg='''Unable to set IP address: customer identified by --key and --secret options denied permission. Correct values can be obtained from from https:\/\/developer.godaddy.com\/keys\/ and are ideally placed in a % file.''' elif e.code==404: msg='Unable to set IP address:{} not found at GoDaddy.'.format(args.hostname) elif e.code==422: msg='Unable to set IP address: \"{}\" has invalid domain or lacks A record.'.format(args.hostname) elif e.code==429: msg='Unable to set IP address: too many requests to GoDaddy within brief period.' elif e.code==503: msg='Unable to set IP address: \"{}\" is unavailable.'.format(args.hostname) else: msg='Unable to set IP address: GoDaddy API failure because \"{}\".'.format(e.reason) raise Exception(msg) except URLError as e: msg='Unable to set IP address: GoDaddy API failure because \"{}\".'.format(e.reason) raise Exception(msg) print('IP address for{} set to{}.'.format(args.hostname,args.ip)) if __name__=='__main__': main() ","sourceWithComments":"#!\/usr\/bin\/env python3\n#\n# Update GoDaddy DNS \"A\" Record.\n#\n# usage: godaddy_ddns.py [-h] [--version] [--ip IP] [--key KEY]\n#                        [--secret SECRET] [--ttl TTL]\n#                        hostname\n#\n# positional arguments:\n#   hostname         DNS fully-qualified host name with an 'A' record.  If the hostname consists of only a domain name\n#                    (i.e., it contains only one period), the record for '@' is updated.\n#\n# optional arguments:\n#   -h, --help       show this help message and exit\n#   --version        show program's version number and exit\n#   --ip IP          DNS Address (defaults to public WAN address from http:\/\/ipv4.icanhazip.com\/)\n#   --key KEY        GoDaddy production key\n#   --secret SECRET  GoDaddy production secret\n#   --ttl TTL        DNS TTL.\n#\n# GoDaddy customers can obtain values for the KEY and SECRET arguments by creating a production key at\n# https:\/\/developer.godaddy.com\/keys\/.\n#\n# Note that command line arguments may be specified in a FILE, one to a line, by instead giving\n# the argument \"%FILE\".  For security reasons, it is particularly recommended to supply the\n# KEY and SECRET arguments in such a file, rather than directly on the command line:\n#\n# Create a file named, e.g., `godaddy-ddns.config` with the content:\n#   MY.FULLY.QUALIFIED.HOSTNAME.COM\n#   --key\n#   MY-KEY-FROM-GODADDY\n#   --secret\n#   MY-SECRET-FROM-GODADDY\n#\n# Then just invoke `godaddy-ddns %godaddy-ddns.config`\n\nprog='godaddy-ddns'\nversion='0.3'\nauthor='Carl Edman (CarlEdman@gmail.com)'\n\nimport sys, json, argparse\n\nif sys.version_info > (3,):\n  from urllib.request import urlopen, Request\n  from urllib.error import URLError, HTTPError\nelse:\n  from urllib2 import urlopen, Request\n  from urllib2 import URLError, HTTPError\n\nparser = argparse.ArgumentParser(description='Update GoDaddy DNS \"A\" Record.', fromfile_prefix_chars='%', epilog= \\\n'''GoDaddy customers can obtain values for the KEY and SECRET arguments by creating a production key at\nhttps:\/\/developer.godaddy.com\/keys\/.\n\nNote that command line arguments may be specified in a FILE, one to a line, by instead giving\nthe argument \"%FILE\".  For security reasons, it is particularly recommended to supply the\nKEY and SECRET arguments in such a file, rather than directly on the command line.''')\n\nparser.add_argument('--version', action='version',\n  version='{} {}'.format(prog, version))\n\nparser.add_argument('hostname', type=str,\n  help='DNS fully-qualified host name with an A record.  If the hostname consists of only a domain name (i.e., it contains only one period), the record for @ is updated.')\n\nparser.add_argument('--ip', type=str, default=None,\n  help='DNS Address (defaults to public WAN address from http:\/\/ipv4.icanhazip.com\/)')\n\nparser.add_argument('--key', type=str, default='',\n  help='GoDaddy production key')\n\nparser.add_argument('--secret', type=str, default='',\n  help='GoDaddy production secret')\n\nparser.add_argument('--ttl', type=int, default=3600 , help='DNS TTL.')\nargs = parser.parse_args()\n\ndef main():\n  hostnames = args.hostname.split('.')\n  if len(hostnames)<2:\n    msg = 'Hostname \"{}\" is not a fully-qualified host name of form \"HOST.DOMAIN.TOP\".'.format(args.hostname)\n    raise Exception(msg)\n  elif len(hostnames)<3:\n    hostnames.insert(0,'@')\n\n  if not args.ip:\n    try:\n      with urlopen(\"http:\/\/ipv4.icanhazip.com\/\") as f: resp=f.read()\n      if sys.version_info > (3,): resp = resp.decode('utf-8')\n      args.ip = resp.strip()\n    except URLError:\n      msg = 'Unable to automatically obtain IP address from http:\/\/ipv4.icanhazip.com\/.'\n      raise Exception(msg)\n  \n  ipslist = args.ip.split(\",\")\n  for ipsiter in ipslist:\n    ips = ipsiter.split('.')\n    if len(ips)!=4 or \\\n      not ips[0].isdigit() or not ips[1].isdigit() or not ips[2].isdigit() or not ips[3].isdigit() or \\\n      int(ips[0])>255 or int(ips[1])>255 or int(ips[2])>255 or int(ips[3])>255:\n      msg = '\"{}\" is not valid IP address.'.format(args.ip)\n      raise Exception(msg)\n\n  url = 'https:\/\/api.godaddy.com\/v1\/domains\/{}\/records\/A\/{}'.format('.'.join(hostnames[1:]),hostnames[0])\n  data = json.dumps([ { \"data\": ip, \"ttl\": args.ttl, \"name\": hostnames[0], \"type\": \"A\" } for ip in  ipslist])\n  if sys.version_info > (3,):  data = data.encode('utf-8')\n  req = Request(url, method='PUT', data=data)\n\n  req.add_header(\"Content-Type\",\"application\/json\")\n  req.add_header(\"Accept\",\"application\/json\")\n  if args.key and args.secret:\n    req.add_header(\"Authorization\", \"sso-key {}:{}\".format(args.key,args.secret))\n\n  try:\n    with urlopen(req) as f: resp = f.read()\n    if sys.version_info > (3,):  resp = resp.decode('utf-8')\n    # resp = json.loads(resp)\n  except HTTPError as e:\n    if e.code==400:\n      msg = 'Unable to set IP address: GoDaddy API URL ({}) was malformed.'.format(req.full_url)\n    elif e.code==401:\n      if args.key and args.secret:\n        msg = '''Unable to set IP address: --key or --secret option incorrect.\nCorrect values can be obtained from from https:\/\/developer.godaddy.com\/keys\/ and are ideally placed in a % file.'''\n      else:\n        msg = '''Unable to set IP address: --key or --secret option missing.\nCorrect values can be obtained from from https:\/\/developer.godaddy.com\/keys\/ and are ideally placed in a % file.'''\n    elif e.code==403:\n        msg = '''Unable to set IP address: customer identified by --key and --secret options denied permission.\nCorrect values can be obtained from from https:\/\/developer.godaddy.com\/keys\/ and are ideally placed in a % file.'''\n    elif e.code==404:\n        msg = 'Unable to set IP address: {} not found at GoDaddy.'.format(args.hostname)\n    elif e.code==422:\n        msg = 'Unable to set IP address: \"{}\" has invalid domain or lacks A record.'.format(args.hostname)\n    elif e.code==429:\n        msg = 'Unable to set IP address: too many requests to GoDaddy within brief period.'\n    elif e.code==503:\n        msg = 'Unable to set IP address: \"{}\" is unavailable.'.format(args.hostname)\n    else:\n      msg = 'Unable to set IP address: GoDaddy API failure because \"{}\".'.format(e.reason)\n    raise Exception(msg)\n  except URLError as e:\n    msg = 'Unable to set IP address: GoDaddy API failure because \"{}\".'.format(e.reason)\n    raise Exception(msg)\n\n  print('IP address for {} set to {}.'.format(args.hostname,args.ip))\n\nif __name__ == '__main__':\n  main()\n"}},"msg":"tls support for ip lookup to add validation and prevent potential tampering."}},"https:\/\/github.com\/runt18\/witness":{"fb328008f5833a59d50285ecd85f2f9b1853bfed":{"url":"https:\/\/api.github.com\/repos\/runt18\/witness\/commits\/fb328008f5833a59d50285ecd85f2f9b1853bfed","html_url":"https:\/\/github.com\/runt18\/witness\/commit\/fb328008f5833a59d50285ecd85f2f9b1853bfed","message":"Prevent adminsite from tampering with the hash","sha":"fb328008f5833a59d50285ecd85f2f9b1853bfed","keyword":"tampering prevent","diff":"diff --git a\/witness\/admin.py b\/witness\/admin.py\nindex e2dd9f7..dcb8356 100644\n--- a\/witness\/admin.py\n+++ b\/witness\/admin.py\n@@ -8,6 +8,10 @@\n # Globally disable delete selected\n admin.site.disable_action('delete_selected')\n \n+class DecisionAdmin(admin.ModelAdmin):\n+    ''' Prevent admins from manually tampering with the hash '''\n+    readonly_fields=('text_hash',)\n+\n admin.site.register(Document)\n admin.site.register(DocumentVersion)\n-admin.site.register(Decision)\n+admin.site.register(Decision, DecisionAdmin)\n","files":{"\/witness\/admin.py":{"changes":[{"diff":"\n # Globally disable delete selected\n admin.site.disable_action('delete_selected')\n \n+class DecisionAdmin(admin.ModelAdmin):\n+    ''' Prevent admins from manually tampering with the hash '''\n+    readonly_fields=('text_hash',)\n+\n admin.site.register(Document)\n admin.site.register(DocumentVersion)\n-admin.site.register(Decision)\n+admin.site.register(Decision, DecisionAdmin)\n","add":5,"remove":1,"filename":"\/witness\/admin.py","badparts":["admin.site.register(Decision)"],"goodparts":["class DecisionAdmin(admin.ModelAdmin):","    ''' Prevent admins from manually tampering with the hash '''","    readonly_fields=('text_hash',)","admin.site.register(Decision, DecisionAdmin)"]}],"source":"\n from django.contrib import admin from witness.models import Document, DocumentVersion, Decision admin.site.disable_action('delete_selected') admin.site.register(Document) admin.site.register(DocumentVersion) admin.site.register(Decision) ","sourceWithComments":"# This Source Code Form is subject to the terms of the Mozilla Public\n# License, v. 2.0. If a copy of the MPL was not distributed with this file,\n# You can obtain one at http:\/\/mozilla.org\/MPL\/2.0\/.\n\nfrom django.contrib import admin\nfrom witness.models import Document, DocumentVersion, Decision\n\n# Globally disable delete selected\nadmin.site.disable_action('delete_selected')\n\nadmin.site.register(Document)\nadmin.site.register(DocumentVersion)\nadmin.site.register(Decision)\n"}},"msg":"Prevent adminsite from tampering with the hash"}},"https:\/\/github.com\/yasserfarouk\/negmas":{"8093408fe25aec5aac97034fe3ac277863bd7758":{"url":"https:\/\/api.github.com\/repos\/yasserfarouk\/negmas\/commits\/8093408fe25aec5aac97034fe3ac277863bd7758","html_url":"https:\/\/github.com\/yasserfarouk\/negmas\/commit\/8093408fe25aec5aac97034fe3ac277863bd7758","message":"adding the ability to prevent CFP tampering and to ignore negotiated penalties to SCMLWorld","sha":"8093408fe25aec5aac97034fe3ac277863bd7758","keyword":"tampering prevent","diff":"diff --git a\/negmas\/apps\/scml\/awi.py b\/negmas\/apps\/scml\/awi.py\nindex cdc86449..04bd7bb2 100644\n--- a\/negmas\/apps\/scml\/awi.py\n+++ b\/negmas\/apps\/scml\/awi.py\n@@ -210,7 +210,7 @@ def request_negotiation(\n             mechanism_params: Parameters of the mechanism\n \n         Returns:\n-            Success of failure of the negotiation\n+            Success of failure of the negotiation request\n \n         Remarks:\n \n@@ -220,6 +220,14 @@ def request_negotiation(\n               intend to use the `requested_negotiations` and `running_negotiations` properties of the `SCMLAgent` class\n \n         \"\"\"\n+        if self._world.prevent_cfp_tampering:\n+            cfp_ = self._world.bulletin_board.read(\"cfps\", cfp.id)\n+            if cfp_ is None:\n+                self._world.logwarning(\n+                    f\"CFP {str(cfp)} with id {cfp.id} was tampered with by {self.agent.name} and will be ignored\"\n+                )\n+                return False\n+            cfp = cfp_\n         default_annotation = self._create_annotation(cfp)\n         return super().request_negotiation_about(\n             issues=cfp.issues,\ndiff --git a\/negmas\/apps\/scml\/world.py b\/negmas\/apps\/scml\/world.py\nindex e409fb67..7759f294 100644\n--- a\/negmas\/apps\/scml\/world.py\n+++ b\/negmas\/apps\/scml\/world.py\n@@ -141,7 +141,8 @@ def __init__(\n         catalog_prices_are_public=True,\n         strip_annotations=True,\n         financial_reports_period=10,\n-        ignore_negotaited_penalties=False\n+        ignore_negotiated_penalties=False,\n+        prevent_cfp_tampering=False\n         # bankruptcy parameters\n         ,\n         default_price_for_products_without_one=1,\n@@ -249,7 +250,8 @@ def __init__(\n             ignore_contract_execution_exceptions=ignore_contract_execution_exceptions,\n             **kwargs,\n         )\n-        self.ignore_negotiated_penalties = ignore_negotaited_penalties\n+        self.prevent_cfp_tampering = prevent_cfp_tampering\n+        self.ignore_negotiated_penalties = ignore_negotiated_penalties\n         self.compensation_fraction = compensation_fraction\n         self.save_mechanism_state_in_contract = save_mechanism_state_in_contract\n         self.default_price_for_products_without_one = (\n@@ -2311,8 +2313,9 @@ def buy_insurance(self, contract: Contract, agent: SCMLAgent) -> bool:\n \n     def _process_annotation(\n         self, annotation: Optional[Dict[str, Any]]\n-    ) -> Dict[str, Any]:\n-        \"\"\"Processes an annotation stripping any extra information not allowed if necessary\"\"\"\n+    ) -> Optional[Dict[str, Any]]:\n+        \"\"\"Processes an annotation stripping any extra information not allowed if necessary. Will return None if the\n+        annotation is suspecious\"\"\"\n         if annotation is None:\n             return {}\n         if not self.strip_annotations:\n@@ -2322,6 +2325,17 @@ def _process_annotation(\n             for k, v in annotation.items()\n             if k in (\"partners\", \"cfp\", \"buyer\", \"seller\")\n         }\n+        if self.prevent_cfp_tampering:\n+            cfp = annotation.get(\"cfp\", None)\n+            if cfp is not None:\n+                cfp_ = self.bulletin_board.read(\"cfps\", cfp.id)\n+                if cfp_ is None:\n+                    self.logerror(\n+                        f\"CFP {str(cfp)} with id {cfp.id} was tampered with by {self.agent.name} and will be ignored\"\n+                    )\n+                    return None\n+                else:\n+                    annotation[\"cfp\"] = cfp_\n         return annotation\n \n     def run_negotiation(\n@@ -2335,6 +2349,8 @@ def run_negotiation(\n         mechanism_params: Dict[str, Any] = None,\n     ) -> Optional[Tuple[Contract, AgentMechanismInterface]]:\n         annotation = self._process_annotation(annotation)\n+        if annotation is None:\n+            return None\n         return super().run_negotiation(\n             caller=caller,\n             issues=issues,\n@@ -2357,6 +2373,8 @@ def request_negotiation_about(\n         mechanism_params: Dict[str, Any] = None,\n     ):\n         annotation = self._process_annotation(annotation)\n+        if annotation is None:\n+            return False\n         return super().request_negotiation_about(\n             req_id=req_id,\n             caller=caller,\n","files":{"\/negmas\/apps\/scml\/awi.py":{"changes":[{"diff":"\n             mechanism_params: Parameters of the mechanism\n \n         Returns:\n-            Success of failure of the negotiation\n+            Success of failure of the negotiation request\n \n         Remarks:\n \n","add":1,"remove":1,"filename":"\/negmas\/apps\/scml\/awi.py","badparts":["            Success of failure of the negotiation"],"goodparts":["            Success of failure of the negotiation request"]}],"source":"\n\"\"\" Implements an agent-world-interface(see `AgentWorldInterface`) for the SCM world. \"\"\" from typing import Optional, List, Dict, Any from negmas import Issue from negmas.apps.scml.common import * from negmas.apps.scml.common import FactoryState from negmas.java import to_java, from_java, to_dict from negmas.situated import AgentWorldInterface, Contract, Action __all__=[\"SCMLAWI\"] class SCMLAWI(AgentWorldInterface): \"\"\"A single contact point between SCML agents and the world simulation. The agent can access the world simulation in one of two ways: 1. Attributes and methods available in this Agent-World-Interface 2. Attributes and methods in the `FactoryManager` object itself which provide handy shortcuts to the agent-world interface **Attributes** *Simulation settings* -`current_step`: Current simulation step -`default_signing_delay`: The grace period allowed between contract conclusion and signature by default(i.e. if not agreed upon during the negotiation) -`n_steps`: Total number of simulation steps. -`relative_time`: The fraction of total simulation time elapsed(it will be a number between 0 and 1) *Production Graph* -`products`: A list of `Product` objects giving all products defined in the world simulation -`processes`: A list of `Process` objects giving all products defined in the world simulation *Agent Related* -`state`: The current private state available to the agent. In SCML it is a `FactoryState` object. **Methods** *Production Control* -`schedule_job`: Schedules a `Job` for production sometime in the future -`schedule_production`: Schedules production using profile number instead of a `Job` object -`cancel_production`: Cancels already scheduled production(if it did not start yet) or stop a running production. -`execute`: A general function to execute any command on the factory. There is no need to directly call this function as the SCMLAWI provides convenient functions(e.g. `schedule_job`, `hide_funds`, etc) to achieve the same goal without having to worry about creating `Action` objects *Storage and Wallet Control* -`hide_funds`: Hides funds from the view of the simulator. Note that when bankruptcy is considered, hidden funds are visible to the simulator. -`hide_inventory`: Hides inventory from the view of the simulator. Note that when bankruptcy is considered, hidden funds are visible to the simulator. -`unhide_funds`: Un-hides funds hidden earlier with a call to `hide_funds` -`unhide_inventory`: Un-hides inventory hidden earlier with a call to `hide_inventory` *Negotiation and CFP Control* -`register_cfp`: Registers a Call-for-Proposals on the bulletin board. -`remove_cfp`: Removes a Call-for-Proposals from the bulletin board. -`request_negotiation`: Requests a negotiation based on the content of a CFP published on the bulletin-board. *It is recommended not to use this method directly and to request negotiations using the request_negotiation method of `FactoryManager`(i.e. use self.request_negotiation instead of self.awi.request_negotiation). This makes it possible for NegMAS to keep track of existing `requested_negotiations` and `running_negotiations` for you. *Notification Control* -`receive_financial_reports`: Register\/unregisters interest in receiving financial reports for an agent, a set of agents or all agents. -`register_interest`: registers interest in receiving CFPs about a set of products. By default all `FactoryManager` objects are registered to receive all CFPs for any product they can produce or need to consumer according to their line-profiles. -`unregister_interest`: unregisters interest in receiving CFPs about a set of products. *Information about Other Agents* -`is_bankrupt`: Asks about the bankruptcy status of an agent -`receive_financial_reports`: Register\/unregisters interest in receiving financial reports for an agent, a set of agents or all agents. -`reports_at`: reads *all* financial reports produced at a given time-step -`reports_for`: reads *all* financial reports of a given agent *Financial Control* -`evaluate_insurance`: Asks for the premium to be paid for insuring against partner breaches for a given contract -`buy_insurance`: Buys an insurance against partner breaches for a given contract *Bulletin-Board* The bulletin-board is a key-value store. These methods allows the agent to interact with it. *The `SCMLAWI` provides convenient functions for recording to the bulletin-board so you mostly need to use read\/query functions*. -`bb_read`: Reads a complete section or a single value from the bulletin-board -`bb_query`: Returns all records in the given section\/sections of the bulletin-board that satisfy a query -`bb_record`: Registers a record in the bulletin-board. -`bb_remove`: Removes a record from the bulletin-board. The following list of sections are available in the SCML Bulletin-Board(Use the exact string for the ``section`` parameter of any method starting with ``bb_``): -**cfps**: All CFPs currently on the board. The key is the CFP ID -**products**: A list of all products. The key is the product index\/ID -**processes**: A list of all processes. The key is the product index\/ID -**bankruptcy**: The bankruptcy list giving names of all bankrupt agents. -**reports_time**: Financial reports indexed by time. -**reports_agent**: Financial reports indexed by agent -**breaches**: Breach-list indexed by breach ID giving all breaches committed in the system -**settings**: Static settings of the simulation. The following settings are currently available: -*breach_penalty_society*: Penalty of breaches paid to society(as a fraction of contract value). This is always paid for every breach whether or not there is a negotiated breach. -*breach_penalty_victim*: Penalty of breaches paid to victim(as a fraction of contract value). This is always paid for every breach whether or not there is a negotiated breach. -*immediate_negotiations*: Whether negotiations start immediately when registered(the other possibility --which is the default --is for them to start at the next production step). -*negotiation_speed_multiple*: Number of negotiation steps that finish in a single production step. -*negotiation_n_steps*: Maximum allowed number of steps(rounds) in any negotiation -*negotiation_step_time_limit*: The maximum real-time allowed for each negotiation step(round) -*negotiation_time_limit*: The time limit for a complete negotiation. -*transportation_delay*: Transportation delay when products are moved between factories. Default is zero. -*transfer_delay*: The delay in transferring funds between factories when executing a contract. Default is zero. -*n_steps*: Number of simulation steps -*time_limit*: Time limit for the complete simulation -stats: Global statistics about the simulation. **Not available for SCML 2019 league**. *Logging* -`logerror`: Logs an error in the world simulation log file -`logwarning`: Logs a warning in the world simulation log file -`loginfo`: Logs information in the world simulation log file -`logdebug`: Logs debug information in the world simulation log file \"\"\" def register_cfp(self, cfp: CFP) -> None: \"\"\"Registers a CFP\"\"\" self._world.n_new_cfps +=1 cfp.money_resolution=self._world.money_resolution cfp.publisher=( self.agent.id ) self.logdebug(f\"{self.agent.name} registered CFP{str(cfp)}\") self.bb_record(section=\"cfps\", key=cfp.id, value=cfp) def register_interest(self, products: List[int]) -> None: \"\"\"registers interest in receiving callbacks about CFPs related to these products\"\"\" self._world.register_interest(agent=self.agent, products=products) def unregister_interest(self, products: List[int]) -> None: \"\"\"registers interest in receiving callbacks about CFPs related to these products\"\"\" self._world.unregister_interest(agent=self.agent, products=products) def remove_cfp(self, cfp: CFP) -> bool: \"\"\"Removes a CFP\"\"\" if self.agent.id !=cfp.publisher: return False return self.bb_remove(section=\"cfps\", key=str(hash(cfp))) def evaluate_insurance(self, contract: Contract, t: int=None) -> Optional[float]: \"\"\"Can be called to evaluate the premium for insuring the given contract against breaches committed by others Args: contract: hypothetical contract t: time at which the policy is to be bought. If None, it means current step \"\"\" return self._world.evaluate_insurance(contract=contract, agent=self.agent, t=t) def buy_insurance(self, contract: Contract) -> bool: \"\"\"Buys insurance for the contract by the premium calculated by the insurance company. Remarks: The agent can call `evaluate_insurance` to find the premium that will be used. \"\"\" return self._world.buy_insurance(contract=contract, agent=self.agent) def _create_annotation(self, cfp: \"CFP\", partner: str=None): \"\"\"Creates full annotation based on a cfp that the agent is receiving\"\"\" return self.agent._create_annotation(cfp, partner=partner) def request_negotiation( self, cfp: CFP, req_id: str, roles: List[str]=None, mechanism_name: str=None, mechanism_params: Dict[str, Any]=None, ) -> bool: \"\"\" Requests a negotiation with the publisher of a given CFP Args: cfp: The CFP to negotiate about req_id: A string that is passed back to the caller in all callbacks related to this negotiation roles: The roles of the CFP publisher and the agent(in that order). By default no roles are passed(None) mechanism_name: The mechanism type to use. If not given the default mechanism from the world will be used mechanism_params: Parameters of the mechanism Returns: Success of failure of the negotiation Remarks: -The `SCMLAgent` class implements another request_negotiation method that does not receive a `req_id`. This helper method is recommended as it generates the required req_id and passes it keeping track of requested negotiations(and later of running negotiations). Call this method direclty *only* if you do not intend to use the `requested_negotiations` and `running_negotiations` properties of the `SCMLAgent` class \"\"\" default_annotation=self._create_annotation(cfp) return super().request_negotiation_about( issues=cfp.issues, req_id=req_id, partners=default_annotation[\"partners\"], roles=roles, annotation=default_annotation, mechanism_name=mechanism_name, mechanism_params=mechanism_params, ) def request_negotiation_about( self, issues: List[Issue], partners: List[str], req_id: str, roles: List[str]=None, annotation: Optional[Dict[str, Any]]=None, mechanism_name: str=None, mechanism_params: Dict[str, Any]=None, ): \"\"\" Overrides the method of the same name in the base class to disable it in SCM Worlds. **Do not call this method** \"\"\" raise RuntimeError( \"request_negotiation_about should never be called directly in the SCM world\" \", call request_negotiation instead.\" ) def is_bankrupt(self, agent_id: str) -> bool: \"\"\" Checks whether the given agent is bankrupt Args: agent_id: Agent ID Returns: The bankruptcy state of the agent \"\"\" return bool(self.bb_read(\"bankruptcy\", key=agent_id)) def reports_for(self, agent_id: str) -> List[FinancialReport]: \"\"\" Gets all financial reports of an agent(in the order of their publication) Args: agent_id: Agent ID Returns: \"\"\" reports=self.bb_read(\"reports_agent\", key=agent_id) if reports is None: return[] return reports def reports_at(self, step: int=None) -> Dict[str, FinancialReport]: \"\"\" Gets all financial reports of all agents at a given step Args: step: Step at which the reports are required. If None, the last set of reports is returned Returns: A dictionary with agent IDs in keys and their financial reports at the given time as values \"\"\" if step is None: reports=self.bb_query(section=\"reports_time\", query=None) reports=self.bb_read( \"reports_time\", key=str(max([int(_) for _ in reports.keys()])) ) else: reports=self.bb_read(\"reports_time\", key=str(step)) if reports is None: return{} return reports def receive_financial_reports( self, receive: bool=True, agents: Optional[List[str]]=None ) -> None: \"\"\" Registers\/unregisters interest in receiving financial reports Args: receive: True to receive and False to stop receiving agents: If given reception is enabled\/disabled only for the given set of agents. Remarks: -by default financial reports are not sent to any agents. To opt-in to receive financial reports, call this method. \"\"\" self._world.receive_financial_reports(self.agent, receive, agents) @property def state(self) -> FactoryState: \"\"\"Returns the private state of the agent in that world. In the SCML world, that is a reference to its factory. You are allowed to read information from the returned `Factory` but **not to modify it or call ANY methods on it that modify the state**. \"\"\" return self._world.get_private_state(self.agent) @property def products(self) -> List[Product]: \"\"\"Products in the world\"\"\" return self._world.products @property def processes(self) -> List[Process]: \"\"\"Processes in the world\"\"\" return self._world.processes def schedule_production( self, profile: int, step: int, contract: Optional[Contract]=None, override: bool=True, ) -> None: \"\"\" Schedules production on the agent's factory Args: profile: Index of the profile in the agent's `compiled_profiles` list step: The step to start production according to the given profile contract: The contract for which the production is scheduled(optional) override: Whether to override existing production jobs schedules at the same time. \"\"\" self.execute( action=Action( type=\"run\", params={ \"profile\": profile, \"time\": step, \"contract\": contract, \"override\": override, }, ) ) def stop_production( self, line: int, step: int, contract: Optional[Contract], override: bool=True ): \"\"\" Stops\/cancels production scheduled at the given line at the given time. Args: line: One of the factory lines(index) step: Step to stop\/cancel production at contract: The contract for which the job is scheduled(optional) override: Whether to override existing production jobs schedules at the same time. \"\"\" self.execute(action=Action(type=\"stop\", params={\"line\": line, \"time\": step})) cancel_production=stop_production \"\"\" Stops\/cancels production scheduled at the given line at the given time. Args: line: One of the factory lines(index) step: Step to stop\/cancel production at \"\"\" def schedule_job(self, job: Job, contract: Optional[Contract]): \"\"\" Schedules production using a `Job` object. This can be used to schedule any kind of job Args: job: The job description contract: The contract for which the job is scheduled(optional) Remarks: -Notice that actions that require the profile member of Job(run) never use the line member and vice versa. \"\"\" self.execute( action=Action( type=job.action, params={ \"profile\": job.profile, \"time\": job.time, \"line\": job.line, \"contract\": contract, \"override\": job.override, }, ) ) def hide_inventory(self, product: int, quantity: int) -> None: \"\"\" Hides the given quantity of the given product so that it is not accessible by the simulator and does not appear in reports etc. Args: product: product index quantity: the amount of the product to hide Remarks: -if the current quantity in storage of the product is less than the amount to be hidden, whatever quantity exists is hidden -hiding is always immediate \"\"\" self.execute( action=Action( type=\"hide_product\", params={\"product\": product, \"quantity\": quantity} ) ) def hide_funds(self, amount: float) -> None: \"\"\" Hides the given amount of money so that it is not accessible by the simulator and does not appear in reports etc. Args: amount: The amount of money to hide Remarks: -if the current cash in the agent's wallet is less than the amount to be hidden, all the cash is hidden. -hiding is always immediate \"\"\" self.execute(action=Action(type=\"hide_funds\", params={\"amount\": amount})) def unhide_inventory(self, product: int, quantity: int) -> None: \"\"\" Un-hides the given quantity of the given product so that it is not accessible by the simulator and does not appear in reports etc. Args: product: product index quantity: the amount of the product to hide Remarks: -if the current quantity in storage of the product is less than the amount to be hidden, whatever quantity exists is hidden -hiding is always immediate \"\"\" self.execute( action=Action( type=\"unhide_product\", params={\"product\": product, \"quantity\": quantity} ) ) def unhide_funds(self, amount: float) -> None: \"\"\" Un-hides the given amount of money so that it is not accessible by the simulator and does not appear in reports etc. Args: amount: The amount of money to unhide Remarks: -if the current cash in the agent's wallet is less than the amount to be hidden, all the cash is hidden. -hiding is always immediate \"\"\" self.execute(action=Action(type=\"unhide_funds\", params={\"amount\": amount})) class _ShadowSCMLAWI: \"\"\"An SCMLAWI As seen by JNegMAS. This is an object that is not visible to python code. It is not directly called from python ever. It is only called from a corresponding Java object to represent an internal python object. Because of he way py4j works, we cannot just use dunders to implement this kind of object in general. We will have to implement each such class independently. This kind of classes will always have an internal Java class implementing a Java interface in Jnegmas that starts with Py. \"\"\" def to_java(self): return to_dict(self.shadow) def __init__(self, awi: SCMLAWI): self.shadow=awi def getProducts(self): return to_java(self.shadow.products) def getProcesses(self): return to_java(self.shadow.processes) def getState(self): return to_java(self.shadow.state) def relativeTime(self): return self.shadow.relative_time def getCurrentStep(self): return self.shadow.current_step def getNSteps(self): return self.shadow.n_steps def getDefaultSigningDelay(self): return self.shadow.default_signing_delay def requestNegotiation( self, cfp, req_id: str, roles=None, mechanism_name=None, mechanism_params=None ): return self.shadow.request_negotiation( from_java(cfp), req_id, roles, mechanism_name, mechanism_params ) def registerCFP(self, cfp: Dict[str, Any]) -> None: \"\"\"Registers a CFP\"\"\" self.shadow.register_cfp(from_java(cfp)) def removeCFP(self, cfp: Dict[str, Any]) -> bool: \"\"\"Removes a CFP\"\"\" return self.shadow.remove_cfp(from_java(cfp)) def registerInterest(self, products: List[int]) -> None: \"\"\"registers interest in receiving callbacks about CFPs related to these products\"\"\" self.shadow.register_interest(from_java(products)) def unregisterInterest(self, products: List[int]) -> None: \"\"\"registers interest in receiving callbacks about CFPs related to these products\"\"\" self.shadow.unregister_interest(from_java(products)) def evaluateInsurance( self, contract: Dict[str, Any], t: int=None ) -> Optional[float]: \"\"\"Can be called to evaluate the premium for insuring the given contract against breaches committed by others Args: contract: hypothetical contract t: time at which the policy is to be bought. If None, it means current step \"\"\" result=self.shadow.evaluate_insurance(from_java(contract), t) if result < 0: return None return result def buyInsurance(self, contract: Dict[str, Any]) -> bool: \"\"\"Buys insurance for the contract by the premium calculated by the insurance company. Remarks: The agent can call `evaluate_insurance` to find the premium that will be used. \"\"\" return self.shadow.buy_insurance(from_java(contract)) def loginfo(self, msg: str): return self.shadow.loginfo(msg) def logwarning(self, msg: str): return self.shadow.logwarning(msg) def logdebug(self, msg: str): return self.shadow.logdebug(msg) def logerror(self, msg: str): return self.shadow.logerror(msg) class Java: implements=[\"jnegmas.apps.scml.awi.SCMLAWI\"] ","sourceWithComments":"\"\"\"\nImplements an agent-world-interface (see `AgentWorldInterface`) for the SCM world.\n\"\"\"\nfrom typing import Optional, List, Dict, Any\n\nfrom negmas import Issue\nfrom negmas.apps.scml.common import *\nfrom negmas.apps.scml.common import FactoryState\nfrom negmas.java import to_java, from_java, to_dict\nfrom negmas.situated import AgentWorldInterface, Contract, Action\n\n__all__ = [\"SCMLAWI\"]\n\n\nclass SCMLAWI(AgentWorldInterface):\n    \"\"\"A single contact point between SCML agents and the world simulation.\n\n    The agent can access the world simulation in one of two ways:\n\n    1. Attributes and methods available in this Agent-World-Interface\n    2. Attributes and methods in the `FactoryManager` object itself which provide handy shortcuts\n       to the agent-world interface\n\n    **Attributes**\n\n    *Simulation settings*\n\n    - `current_step` : Current simulation step\n    - `default_signing_delay` : The grace period allowed between contract conclusion and signature\n      by default (i.e. if not agreed upon during the negotiation)\n    - `n_steps` : Total number of simulation steps.\n    - `relative_time` : The fraction of total simulation time elapsed (it will be a number between 0 and 1)\n\n    *Production Graph*\n\n    - `products` : A list of `Product` objects giving all products defined in the world simulation\n    - `processes` : A list of `Process` objects giving all products defined in the world simulation\n\n    *Agent Related*\n\n    - `state` : The current private state available to the agent. In SCML it is a `FactoryState` object.\n\n    **Methods**\n\n    *Production Control*\n\n    - `schedule_job` : Schedules a `Job` for production sometime in the future\n    - `schedule_production` : Schedules production using profile number instead of a `Job` object\n    - `cancel_production` : Cancels already scheduled production (if it did not start yet) or stop a running\n      production.\n    - `execute` : A general function to execute any command on the factory. There is no need to directly call\n      this function as the SCMLAWI provides convenient functions (e.g. `schedule_job` , `hide_funds` , etc)\n      to achieve the same goal without having to worry about creating `Action` objects\n\n    *Storage and Wallet Control*\n\n    - `hide_funds` : Hides funds from the view of the simulator. Note that when bankruptcy is considered, hidden\n      funds are visible to the simulator.\n    - `hide_inventory` : Hides inventory from the view of the simulator. Note that when bankruptcy is considered, hidden\n      funds are visible to the simulator.\n    - `unhide_funds` : Un-hides funds hidden earlier with a call to `hide_funds`\n    - `unhide_inventory` : Un-hides inventory hidden earlier with a call to `hide_inventory`\n\n    *Negotiation and CFP Control*\n\n    - `register_cfp` : Registers a Call-for-Proposals on the bulletin board.\n    - `remove_cfp` : Removes a Call-for-Proposals from the  bulletin board.\n    - `request_negotiation` : Requests a negotiation based on the content of a CFP published on the bulletin-board.\n      *It is recommended not to use this method directly and to request negotiations using the\n      request_negotiation method of `FactoryManager` (i.e. use self.request_negotiation instead\n      of self.awi.request_negotiation). This makes it possible for NegMAS to keep track of\n      existing `requested_negotiations` and `running_negotiations` for you.\n\n    *Notification Control*\n\n    - `receive_financial_reports` : Register\/unregisters interest in receiving financial reports for an agent, a set of\n      agents or all agents.\n    - `register_interest` : registers interest in receiving CFPs about a set of products. By default all\n      `FactoryManager` objects are registered to receive all CFPs for any product they\n      can produce or need to consumer according to their line-profiles.\n    - `unregister_interest` : unregisters interest in receiving CFPs about a set of products.\n\n    *Information about Other Agents*\n\n    - `is_bankrupt` : Asks about the bankruptcy status of an agent\n    - `receive_financial_reports` : Register\/unregisters interest in receiving financial reports for an agent, a set of\n      agents or all agents.\n    - `reports_at` : reads *all* financial reports produced at a given time-step\n    - `reports_for` : reads *all* financial reports of a given agent\n\n    *Financial Control*\n\n    - `evaluate_insurance` : Asks for the premium to be paid for insuring against partner breaches\n      for a given contract\n    - `buy_insurance` : Buys an insurance against partner breaches for a given contract\n\n    *Bulletin-Board*\n\n    The bulletin-board is a key-value store. These methods allows the agent to interact with it. *The `SCMLAWI` provides\n    convenient functions for recording to the bulletin-board so you mostly need to use read\/query functions*.\n\n    - `bb_read` : Reads a complete section or a single value from the bulletin-board\n    - `bb_query` : Returns all records in the given section\/sections of the bulletin-board that satisfy a query\n    - `bb_record` : Registers a record in the bulletin-board.\n    - `bb_remove` : Removes a record from the bulletin-board.\n\n    The following list of sections are available in the SCML Bulletin-Board (Use the exact string for the ``section``\n    parameter of any method starting with ``bb_``):\n\n    - **cfps**: All CFPs currently on the board. The key is the CFP ID\n    - **products**: A list of all products. The key is the product index\/ID\n    - **processes**: A list of all processes. The key is the product index\/ID\n    - **bankruptcy**: The bankruptcy list giving names of all bankrupt agents.\n    - **reports_time**: Financial reports indexed by time.\n    - **reports_agent**: Financial reports indexed by agent\n    - **breaches**: Breach-list indexed by breach ID giving all breaches committed in the system\n    - **settings**: Static settings of the simulation.\n\n      The following settings are currently available:\n\n      - *breach_penalty_society*: Penalty of breaches paid to society (as a fraction of contract value). This is always\n        paid for every breach whether or not there is a negotiated breach.\n      - *breach_penalty_victim*: Penalty of breaches paid to victim (as a fraction of contract value). This is always\n        paid for every breach whether or not there is a negotiated breach.\n      - *immediate_negotiations*: Whether negotiations start immediately when registered (the other possibility -- which\n        is the default -- is for them to start at the next production step).\n      - *negotiation_speed_multiple*: Number of negotiation steps that finish in a single production step.\n      - *negotiation_n_steps*: Maximum allowed number of steps (rounds) in any negotiation\n      - *negotiation_step_time_limit*: The maximum real-time allowed for each negotiation step (round)\n      - *negotiation_time_limit*: The time limit for a complete negotiation.\n      - *transportation_delay*: Transportation delay when products are moved between factories. Default is zero.\n      - *transfer_delay*: The delay in transferring funds between factories when executing a contract. Default is zero.\n      - *n_steps*: Number of simulation steps\n      - *time_limit*: Time limit for the complete simulation\n\n    - stats: Global statistics about the simulation. **Not available for SCML 2019 league**.\n\n    *Logging*\n\n    - `logerror` : Logs an error in the world simulation log file\n    - `logwarning` : Logs a warning in the world simulation log file\n    - `loginfo` : Logs information in the world simulation log file\n    - `logdebug` : Logs debug information in the world simulation log file\n\n    \"\"\"\n\n    def register_cfp(self, cfp: CFP) -> None:\n        \"\"\"Registers a CFP\"\"\"\n        self._world.n_new_cfps += 1\n        cfp.money_resolution = self._world.money_resolution\n        cfp.publisher = (\n            self.agent.id\n        )  # force the publisher to be the agent using this AWI.\n        self.logdebug(f\"{self.agent.name} registered CFP {str(cfp)}\")\n        self.bb_record(section=\"cfps\", key=cfp.id, value=cfp)\n\n    def register_interest(self, products: List[int]) -> None:\n        \"\"\"registers interest in receiving callbacks about CFPs related to these products\"\"\"\n        self._world.register_interest(agent=self.agent, products=products)\n\n    def unregister_interest(self, products: List[int]) -> None:\n        \"\"\"registers interest in receiving callbacks about CFPs related to these products\"\"\"\n        self._world.unregister_interest(agent=self.agent, products=products)\n\n    def remove_cfp(self, cfp: CFP) -> bool:\n        \"\"\"Removes a CFP\"\"\"\n        if self.agent.id != cfp.publisher:\n            return False\n        return self.bb_remove(section=\"cfps\", key=str(hash(cfp)))\n\n    def evaluate_insurance(self, contract: Contract, t: int = None) -> Optional[float]:\n        \"\"\"Can be called to evaluate the premium for insuring the given contract against breaches committed by others\n\n        Args:\n\n            contract: hypothetical contract\n            t: time at which the policy is to be bought. If None, it means current step\n        \"\"\"\n        return self._world.evaluate_insurance(contract=contract, agent=self.agent, t=t)\n\n    def buy_insurance(self, contract: Contract) -> bool:\n        \"\"\"Buys insurance for the contract by the premium calculated by the insurance company.\n\n        Remarks:\n            The agent can call `evaluate_insurance` to find the premium that will be used.\n        \"\"\"\n        return self._world.buy_insurance(contract=contract, agent=self.agent)\n\n    def _create_annotation(self, cfp: \"CFP\", partner: str = None):\n        \"\"\"Creates full annotation based on a cfp that the agent is receiving\"\"\"\n        return self.agent._create_annotation(cfp, partner=partner)\n\n    def request_negotiation(\n        self,\n        cfp: CFP,\n        req_id: str,\n        roles: List[str] = None,\n        mechanism_name: str = None,\n        mechanism_params: Dict[str, Any] = None,\n    ) -> bool:\n        \"\"\"\n        Requests a negotiation with the publisher of a given CFP\n\n        Args:\n\n            cfp: The CFP to negotiate about\n            req_id: A string that is passed back to the caller in all callbacks related to this negotiation\n            roles: The roles of the CFP publisher and the agent (in that order). By default no roles are passed (None)\n            mechanism_name: The mechanism type to use. If not given the default mechanism from the world will be used\n            mechanism_params: Parameters of the mechanism\n\n        Returns:\n            Success of failure of the negotiation\n\n        Remarks:\n\n            - The `SCMLAgent` class implements another request_negotiation method that does not receive a `req_id`. This\n              helper method is recommended as it generates the required req_id and passes it keeping track of requested\n              negotiations (and later of running negotiations). Call this method direclty *only* if you do not\n              intend to use the `requested_negotiations` and `running_negotiations` properties of the `SCMLAgent` class\n\n        \"\"\"\n        default_annotation = self._create_annotation(cfp)\n        return super().request_negotiation_about(\n            issues=cfp.issues,\n            req_id=req_id,\n            partners=default_annotation[\"partners\"],\n            roles=roles,\n            annotation=default_annotation,\n            mechanism_name=mechanism_name,\n            mechanism_params=mechanism_params,\n        )\n\n    def request_negotiation_about(\n        self,\n        issues: List[Issue],\n        partners: List[str],\n        req_id: str,\n        roles: List[str] = None,\n        annotation: Optional[Dict[str, Any]] = None,\n        mechanism_name: str = None,\n        mechanism_params: Dict[str, Any] = None,\n    ):\n        \"\"\"\n        Overrides the method of the same name in the base class to disable it in SCM Worlds.\n\n        **Do not call this method**\n\n        \"\"\"\n        raise RuntimeError(\n            \"request_negotiation_about should never be called directly in the SCM world\"\n            \", call request_negotiation instead.\"\n        )\n\n    def is_bankrupt(self, agent_id: str) -> bool:\n        \"\"\"\n        Checks whether the given agent is bankrupt\n\n        Args:\n            agent_id: Agent ID\n\n        Returns:\n            The bankruptcy state of the agent\n\n        \"\"\"\n        return bool(self.bb_read(\"bankruptcy\", key=agent_id))\n\n    def reports_for(self, agent_id: str) -> List[FinancialReport]:\n        \"\"\"\n        Gets all financial reports of an agent (in the order of their publication)\n\n        Args:\n            agent_id: Agent ID\n\n        Returns:\n\n        \"\"\"\n        reports = self.bb_read(\"reports_agent\", key=agent_id)\n        if reports is None:\n            return []\n        return reports\n\n    def reports_at(self, step: int = None) -> Dict[str, FinancialReport]:\n        \"\"\"\n        Gets all financial reports of all agents at a given step\n\n        Args:\n\n            step: Step at which the reports are required. If None, the last set of reports is returned\n\n        Returns:\n\n            A dictionary with agent IDs in keys and their financial reports at the given time as values\n        \"\"\"\n        if step is None:\n            reports = self.bb_query(section=\"reports_time\", query=None)\n            reports = self.bb_read(\n                \"reports_time\", key=str(max([int(_) for _ in reports.keys()]))\n            )\n        else:\n            reports = self.bb_read(\"reports_time\", key=str(step))\n        if reports is None:\n            return {}\n        return reports\n\n    def receive_financial_reports(\n        self, receive: bool = True, agents: Optional[List[str]] = None\n    ) -> None:\n        \"\"\"\n        Registers\/unregisters interest in receiving financial reports\n\n        Args:\n            receive: True to receive and False to stop receiving\n            agents: If given reception is enabled\/disabled only for the given set of agents.\n\n        Remarks:\n\n            - by default financial reports are not sent to any agents. To opt-in to receive financial reports, call this\n              method.\n\n        \"\"\"\n        self._world.receive_financial_reports(self.agent, receive, agents)\n\n    @property\n    def state(self) -> FactoryState:\n        \"\"\"Returns the private state of the agent in that world.\n\n        In the SCML world, that is a reference to its factory. You are allowed to read information from the returned\n        `Factory` but **not to modify it or call ANY methods on it that modify the state**.\n\n\n        \"\"\"\n        return self._world.get_private_state(self.agent)\n\n    @property\n    def products(self) -> List[Product]:\n        \"\"\"Products in the world\"\"\"\n        return self._world.products\n\n    @property\n    def processes(self) -> List[Process]:\n        \"\"\"Processes in the world\"\"\"\n        return self._world.processes\n\n    # sugar functions (implementing actions that can all be done through execute\n\n    def schedule_production(\n        self,\n        profile: int,\n        step: int,\n        contract: Optional[Contract] = None,\n        override: bool = True,\n    ) -> None:\n        \"\"\"\n        Schedules production on the agent's factory\n\n        Args:\n            profile: Index of the profile in the agent's `compiled_profiles` list\n            step: The step to start production according to the given profile\n            contract: The contract for which the production is scheduled (optional)\n            override: Whether to override existing production jobs schedules at the same time.\n\n        \"\"\"\n        self.execute(\n            action=Action(\n                type=\"run\",\n                params={\n                    \"profile\": profile,\n                    \"time\": step,\n                    \"contract\": contract,\n                    \"override\": override,\n                },\n            )\n        )\n\n    def stop_production(\n        self, line: int, step: int, contract: Optional[Contract], override: bool = True\n    ):\n        \"\"\"\n        Stops\/cancels production scheduled at the given line at the given time.\n\n        Args:\n            line: One of the factory lines (index)\n            step: Step to stop\/cancel production at\n            contract: The contract for which the job is scheduled (optional)\n            override: Whether to override existing production jobs schedules at the same time.\n        \"\"\"\n        self.execute(action=Action(type=\"stop\", params={\"line\": line, \"time\": step}))\n\n    cancel_production = stop_production\n    \"\"\"\n    Stops\/cancels production scheduled at the given line at the given time.\n\n    Args:\n        line: One of the factory lines (index) \n        step: Step to stop\/cancel production at\n    \"\"\"\n\n    def schedule_job(self, job: Job, contract: Optional[Contract]):\n        \"\"\"\n        Schedules production using a `Job` object. This can be used to schedule any kind of job\n\n        Args:\n            job: The job description\n            contract: The contract for which the job is scheduled (optional)\n\n        Remarks:\n\n            - Notice that actions that require the profile member of Job (run) never use the line member and vice versa.\n        \"\"\"\n        self.execute(\n            action=Action(\n                type=job.action,\n                params={\n                    \"profile\": job.profile,\n                    \"time\": job.time,\n                    \"line\": job.line,\n                    \"contract\": contract,\n                    \"override\": job.override,\n                },\n            )\n        )\n\n    def hide_inventory(self, product: int, quantity: int) -> None:\n        \"\"\"\n        Hides the given quantity of the given product so that it is not accessible by the simulator and does not appear\n        in reports etc.\n\n        Args:\n            product: product index\n            quantity: the amount of the product to hide\n\n        Remarks:\n\n            - if the current quantity in storage of the product is less than the amount to be hidden, whatever quantity\n              exists is hidden\n            - hiding is always immediate\n        \"\"\"\n        self.execute(\n            action=Action(\n                type=\"hide_product\", params={\"product\": product, \"quantity\": quantity}\n            )\n        )\n\n    def hide_funds(self, amount: float) -> None:\n        \"\"\"\n        Hides the given amount of money so that it is not accessible by the simulator and does not appear\n        in reports etc.\n\n        Args:\n            amount: The amount of money to hide\n\n        Remarks:\n\n            - if the current cash in the agent's wallet is less than the amount to be hidden, all the cash is hidden.\n            - hiding is always immediate\n        \"\"\"\n        self.execute(action=Action(type=\"hide_funds\", params={\"amount\": amount}))\n\n    def unhide_inventory(self, product: int, quantity: int) -> None:\n        \"\"\"\n        Un-hides the given quantity of the given product so that it is not accessible by the simulator and does not appear\n        in reports etc.\n\n        Args:\n            product: product index\n            quantity: the amount of the product to hide\n\n        Remarks:\n\n            - if the current quantity in storage of the product is less than the amount to be hidden, whatever quantity\n              exists is hidden\n            - hiding is always immediate\n        \"\"\"\n        self.execute(\n            action=Action(\n                type=\"unhide_product\", params={\"product\": product, \"quantity\": quantity}\n            )\n        )\n\n    def unhide_funds(self, amount: float) -> None:\n        \"\"\"\n        Un-hides the given amount of money so that it is not accessible by the simulator and does not appear\n        in reports etc.\n\n        Args:\n            amount: The amount of money to unhide\n\n        Remarks:\n\n            - if the current cash in the agent's wallet is less than the amount to be hidden, all the cash is hidden.\n            - hiding is always immediate\n        \"\"\"\n        self.execute(action=Action(type=\"unhide_funds\", params={\"amount\": amount}))\n\n\nclass _ShadowSCMLAWI:\n    \"\"\"An SCMLAWI As seen by JNegMAS.\n\n    This is an object that is not visible to python code. It is not directly called from python ever. It is only called\n    from a corresponding Java object to represent an internal python object. Because of he way py4j works, we cannot\n    just use dunders to implement this kind of object in general. We will have to implement each such class\n    independently.\n\n    This kind of classes will always have an internal Java class implementing a Java interface in Jnegmas that starts\n    with Py.\n\n    \"\"\"\n\n    def to_java(self):\n        return to_dict(self.shadow)\n\n    def __init__(self, awi: SCMLAWI):\n        self.shadow = awi\n\n    def getProducts(self):\n        return to_java(self.shadow.products)\n\n    def getProcesses(self):\n        return to_java(self.shadow.processes)\n\n    def getState(self):\n        return to_java(self.shadow.state)\n\n    def relativeTime(self):\n        return self.shadow.relative_time\n\n    def getCurrentStep(self):\n        return self.shadow.current_step\n\n    def getNSteps(self):\n        return self.shadow.n_steps\n\n    def getDefaultSigningDelay(self):\n        return self.shadow.default_signing_delay\n\n    def requestNegotiation(\n        self, cfp, req_id: str, roles=None, mechanism_name=None, mechanism_params=None\n    ):\n        return self.shadow.request_negotiation(\n            from_java(cfp), req_id, roles, mechanism_name, mechanism_params\n        )\n\n    def registerCFP(self, cfp: Dict[str, Any]) -> None:\n        \"\"\"Registers a CFP\"\"\"\n        self.shadow.register_cfp(from_java(cfp))\n\n    def removeCFP(self, cfp: Dict[str, Any]) -> bool:\n        \"\"\"Removes a CFP\"\"\"\n        return self.shadow.remove_cfp(from_java(cfp))\n\n    def registerInterest(self, products: List[int]) -> None:\n        \"\"\"registers interest in receiving callbacks about CFPs related to these products\"\"\"\n        self.shadow.register_interest(from_java(products))\n\n    def unregisterInterest(self, products: List[int]) -> None:\n        \"\"\"registers interest in receiving callbacks about CFPs related to these products\"\"\"\n        self.shadow.unregister_interest(from_java(products))\n\n    def evaluateInsurance(\n        self, contract: Dict[str, Any], t: int = None\n    ) -> Optional[float]:\n        \"\"\"Can be called to evaluate the premium for insuring the given contract against breaches committed by others\n\n        Args:\n\n            contract: hypothetical contract\n            t: time at which the policy is to be bought. If None, it means current step\n        \"\"\"\n        result = self.shadow.evaluate_insurance(from_java(contract), t)\n        if result < 0:\n            return None\n        return result\n\n    def buyInsurance(self, contract: Dict[str, Any]) -> bool:\n        \"\"\"Buys insurance for the contract by the premium calculated by the insurance company.\n\n        Remarks:\n            The agent can call `evaluate_insurance` to find the premium that will be used.\n        \"\"\"\n        return self.shadow.buy_insurance(from_java(contract))\n\n    def loginfo(self, msg: str):\n        return self.shadow.loginfo(msg)\n\n    def logwarning(self, msg: str):\n        return self.shadow.logwarning(msg)\n\n    def logdebug(self, msg: str):\n        return self.shadow.logdebug(msg)\n\n    def logerror(self, msg: str):\n        return self.shadow.logerror(msg)\n\n    class Java:\n        implements = [\"jnegmas.apps.scml.awi.SCMLAWI\"]\n"},"\/negmas\/apps\/scml\/world.py":{"changes":[{"diff":"\n         catalog_prices_are_public=True,\n         strip_annotations=True,\n         financial_reports_period=10,\n-        ignore_negotaited_penalties=False\n+        ignore_negotiated_penalties=False,\n+        prevent_cfp_tampering=False\n         # bankruptcy parameters\n         ,\n         default_price_for_products_without_one=1,\n","add":2,"remove":1,"filename":"\/negmas\/apps\/scml\/world.py","badparts":["        ignore_negotaited_penalties=False"],"goodparts":["        ignore_negotiated_penalties=False,","        prevent_cfp_tampering=False"]},{"diff":"\n             ignore_contract_execution_exceptions=ignore_contract_execution_exceptions,\n             **kwargs,\n         )\n-        self.ignore_negotiated_penalties = ignore_negotaited_penalties\n+        self.prevent_cfp_tampering = prevent_cfp_tampering\n+        self.ignore_negotiated_penalties = ignore_negotiated_penalties\n         self.compensation_fraction = compensation_fraction\n         self.save_mechanism_state_in_contract = save_mechanism_state_in_contract\n         self.default_price_for_products_without_one = (\n","add":2,"remove":1,"filename":"\/negmas\/apps\/scml\/world.py","badparts":["        self.ignore_negotiated_penalties = ignore_negotaited_penalties"],"goodparts":["        self.prevent_cfp_tampering = prevent_cfp_tampering","        self.ignore_negotiated_penalties = ignore_negotiated_penalties"]},{"diff":"\n \n     def _process_annotation(\n         self, annotation: Optional[Dict[str, Any]]\n-    ) -> Dict[str, Any]:\n-        \"\"\"Processes an annotation stripping any extra information not allowed if necessary\"\"\"\n+    ) -> Optional[Dict[str, Any]]:\n+        \"\"\"Processes an annotation stripping any extra information not allowed if necessary. Will return None if the\n+        annotation is suspecious\"\"\"\n         if annotation is None:\n             return {}\n         if not self.strip_annotations:\n","add":3,"remove":2,"filename":"\/negmas\/apps\/scml\/world.py","badparts":["    ) -> Dict[str, Any]:","        \"\"\"Processes an annotation stripping any extra information not allowed if necessary\"\"\""],"goodparts":["    ) -> Optional[Dict[str, Any]]:","        \"\"\"Processes an annotation stripping any extra information not allowed if necessary. Will return None if the","        annotation is suspecious\"\"\""]}]}},"msg":"adding the ability to prevent CFP tampering and to ignore negotiated penalties to SCMLWorld"}},"https:\/\/github.com\/python-botogram\/botogram":{"282b0ab18eda44eca5307b77511bb13c292c2dbd":{"url":"https:\/\/api.github.com\/repos\/python-botogram\/botogram\/commits\/282b0ab18eda44eca5307b77511bb13c292c2dbd","html_url":"https:\/\/github.com\/python-botogram\/botogram\/commit\/282b0ab18eda44eca5307b77511bb13c292c2dbd","message":"Sign callback data to prevent tampering with it","sha":"282b0ab18eda44eca5307b77511bb13c292c2dbd","keyword":"tampering prevent","diff":"diff --git a\/botogram\/__init__.py b\/botogram\/__init__.py\nindex 14ead5d..6ac3739 100644\n--- a\/botogram\/__init__.py\n+++ b\/botogram\/__init__.py\n@@ -34,7 +34,6 @@\n from .runner import run\n from .objects import *\n from .utils import usernames_in\n-from .callbacks import buttons\n \n \n # This code will simulate the Windows' multiprocessing behavior if the\ndiff --git a\/botogram\/api.py b\/botogram\/api.py\nindex 3873934..a425899 100644\n--- a\/botogram\/api.py\n+++ b\/botogram\/api.py\n@@ -182,3 +182,7 @@ def file_content(self, path):\n         response = requests.get(url)\n \n         return response.content\n+\n+    @property\n+    def token(self):\n+        return self._api_key\ndiff --git a\/botogram\/callbacks.py b\/botogram\/callbacks.py\nindex ee92bd9..e7fdd90 100644\n--- a\/botogram\/callbacks.py\n+++ b\/botogram\/callbacks.py\n@@ -17,12 +17,23 @@\n #   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n #   FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n \n+import base64\n+import binascii\n+import hashlib\n+\n+from . import crypto\n+\n+\n+DIGEST = hashlib.md5\n+DIGEST_LEN = 16\n+\n \n class ButtonsRow:\n     \"\"\"A row of an inline keyboard\"\"\"\n \n-    def __init__(self):\n+    def __init__(self, bot):\n         self._content = []\n+        self._bot = bot\n \n     def url(self, label, url):\n         \"\"\"Open an URL when the button is pressed\"\"\"\n@@ -30,12 +41,10 @@ def url(self, label, url):\n \n     def callback(self, label, callback, data=None):\n         \"\"\"Trigger a callback when the button is pressed\"\"\"\n-        if data is not None:\n-            msg = \"%s\\0%s\" % (callback, data)\n-        else:\n-            msg = callback\n-\n-        self._content.append({\"text\": label, \"callback_data\": msg})\n+        self._content.append({\n+            \"text\": label,\n+            \"callback_data\": get_callback_data(self._bot, callback, data),\n+        })\n \n     def switch_inline_query(self, label, query=\"\", current_chat=False):\n         \"\"\"Switch the user to this bot's inline query\"\"\"\n@@ -54,12 +63,13 @@ def switch_inline_query(self, label, query=\"\", current_chat=False):\n class Buttons:\n     \"\"\"Factory for inline keyboards\"\"\"\n \n-    def __init__(self):\n+    def __init__(self, bot):\n         self._rows = {}\n+        self._bot = bot\n \n     def __getitem__(self, index):\n         if index not in self._rows:\n-            self._rows[index] = ButtonsRow()\n+            self._rows[index] = ButtonsRow(self._bot)\n         return self._rows[index]\n \n     def _serialize_attachment(self):\n@@ -72,27 +82,71 @@ def _serialize_attachment(self):\n         return {\"inline_keyboard\": rows}\n \n \n-def buttons():\n-    \"\"\"Create a new inline keyboard\"\"\"\n-    return Buttons()\n+def parse_callback_data(bot, raw):\n+    \"\"\"Parse the callback data generated by botogram and return it\"\"\"\n+    raw = raw.encode(\"utf-8\")\n \n+    if len(raw) < 32:\n+        raise crypto.TamperedMessageError\n \n-def parse_callback_data(data):\n-    \"\"\"Parse the callback data generated by botogram and return it\"\"\"\n-    if \"\\0\" in data:\n-        name, custom = data.split(\"\\0\", 1)\n-        return name, custom\n-    else:\n-        return data, None\n+    try:\n+        prelude = base64.b64decode(raw[:32])\n+    except binascii.Error:\n+        raise crypto.TamperedMessageError\n+\n+    signature = prelude[:16]\n+    name = prelude[16:]\n+    data = raw[32:]\n+\n+    if not crypto.compare(crypto.get_hmac(bot, name + data), signature):\n+        raise crypto.TamperedMessageError\n+\n+    return name, data.decode(\"utf-8\")\n+\n+\n+def get_callback_data(bot, name, data=None):\n+    \"\"\"Get the callback data for the provided name and data\"\"\"\n+    name = hashed_callback_name(name)\n+\n+    if data is None:\n+        data = \"\"\n+    data = data.encode(\"utf-8\")\n+\n+    if len(data) > 32:\n+        raise ValueError(\n+            \"The provided data is too big (%s bytes), try to reduce it to \"\n+            \"32 bytes\" % len(data)\n+        )\n+\n+    # Get the signature of the hook name and data\n+    signature = crypto.get_hmac(bot, name + data)\n+\n+    # Base64 the signature and the hook name together to save space\n+    return (base64.b64encode(signature + name) + data).decode(\"utf-8\")\n+\n+\n+def hashed_callback_name(name):\n+    \"\"\"Get the hashed name of a callback\"\"\"\n+    # Get only the first 8 bytes of the hash to fit it into the payload\n+    return DIGEST(name.encode(\"utf-8\")).digest()[:8]\n \n \n def process(bot, chains, update):\n     \"\"\"Process a callback sent to the bot\"\"\"\n+    try:\n+        name, data = parse_callback_data(bot, update.callback_query._data)\n+    except crypto.TamperedMessageError:\n+        bot.logger.warn(\n+            \"The user tampered with the #%s update's data. Skipped it.\"\n+            % update.update_id\n+        )\n+        return\n+\n     for hook in chains[\"callbacks\"]:\n         bot.logger.debug(\"Processing update #%s with the hook %s\" %\n                          (update.update_id, hook.name))\n \n-        result = hook.call(bot, update)\n+        result = hook.call(bot, update, name, data)\n         if result is True:\n             bot.logger.debug(\"Update #%s was just processed by the %s hook\" %\n                              (update.update_id, hook.name))\ndiff --git a\/botogram\/crypto.py b\/botogram\/crypto.py\nnew file mode 100644\nindex 0000000..e94863a\n--- \/dev\/null\n+++ b\/botogram\/crypto.py\n@@ -0,0 +1,67 @@\n+# Copyright (c) 2015-2017 The Botogram Authors (see AUTHORS)\n+#\n+# Permission is hereby granted, free of charge, to any person obtaining a copy\n+# of this software and associated documentation files (the \"Software\"), to deal\n+# in the Software without restriction, including without limitation the rights\n+# to use, copy, modify, merge, publish, distribute, sublicense, and\/or sell\n+# copies of the Software, and to permit persons to whom the Software is\n+# furnished to do so, subject to the following conditions:\n+#\n+#   The above copyright notice and this permission notice shall be included in\n+#   all copies or substantial portions of the Software.\n+#\n+#   THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n+#   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n+#   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n+#   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n+#   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n+#   FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n+#   DEALINGS IN THE SOFTWARE.\n+\n+import hmac\n+\n+\n+DIGEST = \"md5\"\n+DIGEST_LEN = 16\n+\n+\n+class TamperedMessageError(Exception):\n+    pass\n+\n+\n+def generate_secret_key(bot):\n+    \"\"\"Generate the secret key of the bot\"\"\"\n+    mac = hmac.new(bot.api.token.encode(\"utf-8\"), digestmod=DIGEST)\n+    mac.update(b\"botogram\" + bot.itself.username.encode(\"utf-8\"))\n+    return mac.digest()\n+\n+\n+def get_hmac(bot, data):\n+    \"\"\"Get the HMAC of a piece of data\"\"\"\n+    mac = hmac.new(generate_secret_key(bot), digestmod=DIGEST)\n+    mac.update(data)\n+    return mac.digest()\n+\n+\n+def sign_data(bot, data):\n+    \"\"\"Return a signed version of the data, to prevent tampering with it\"\"\"\n+    return get_hmac(bot, data) + data\n+\n+\n+def verify_signature(bot, untrusted):\n+    \"\"\"Check if the untrusted data is correctly signed, and return it\"\"\"\n+    if len(untrusted) < DIGEST_LEN:\n+        raise TamperedMessageError\n+\n+    signature = untrusted[:DIGEST_LEN]\n+    data = untrusted[DIGEST_LEN:]\n+\n+    if not hmac.compare_digest(get_hmac(bot, data), signature):\n+        raise TamperedMessageError\n+\n+    return data\n+\n+\n+def compare(a, b):\n+    \"\"\"Safely compare two values\"\"\"\n+    return hmac.compare_digest(a, b)\ndiff --git a\/botogram\/frozenbot.py b\/botogram\/frozenbot.py\nindex 1cbec3d..71d69c1 100644\n--- a\/botogram\/frozenbot.py\n+++ b\/botogram\/frozenbot.py\n@@ -23,6 +23,7 @@\n from . import utils\n from . import objects\n from . import api as api_module\n+from .callbacks import Buttons\n \n \n class FrozenBotError(Exception):\n@@ -234,6 +235,10 @@ def register_update_processor(self, kind, processor):\n         \"\"\"Register a new update processor\"\"\"\n         raise FrozenBotError(\"Can't register new update processors at runtime\")\n \n+    def buttons(self):\n+        \"\"\"Create a new inline keyboard\"\"\"\n+        return Buttons(self)\n+\n     # This helper manages the translation\n \n     def _(self, message, **args):\ndiff --git a\/botogram\/hooks.py b\/botogram\/hooks.py\nindex 3be59b4..5606c81 100644\n--- a\/botogram\/hooks.py\n+++ b\/botogram\/hooks.py\n@@ -19,7 +19,7 @@\n \n import re\n \n-from .callbacks import parse_callback_data\n+from .callbacks import hashed_callback_name\n \n \n class Hook:\n@@ -212,14 +212,15 @@ class CallbackHook(Hook):\n     \"\"\"Underlying hook for @bot.callback\"\"\"\n \n     def _after_init(self, args):\n-        self._name = \"%s:%s\" % (self.component.component_name, args[\"name\"])\n+        self._name = hashed_callback_name(\n+            \"%s:%s\" % (self.component.component_name, args[\"name\"])\n+        )\n \n-    def _call(self, bot, update):\n+    def call(self, bot, update, name, data):\n         if not update.callback_query:\n             return\n         q = update.callback_query\n \n-        name, data = parse_callback_data(q._data)\n         if name != self._name:\n             return\n \ndiff --git a\/tests\/test_callbacks.py b\/tests\/test_callbacks.py\nindex 6d0b257..fb2f88d 100644\n--- a\/tests\/test_callbacks.py\n+++ b\/tests\/test_callbacks.py\n@@ -20,11 +20,12 @@\n \n import json\n \n-from botogram.callbacks import Buttons, parse_callback_data\n+from botogram.callbacks import Buttons, parse_callback_data, get_callback_data\n+from botogram.callbacks import hashed_callback_name\n \n \n-def test_buttons():\n-    buttons = Buttons()\n+def test_buttons(bot):\n+    buttons = Buttons(bot)\n     buttons[0].url(\"test 1\", \"http:\/\/example.com\")\n     buttons[0].callback(\"test 2\", \"test_callback\")\n     buttons[3].callback(\"test 3\", \"another_callback\", \"data\")\n@@ -35,20 +36,30 @@ def test_buttons():\n         \"inline_keyboard\": [\n             [\n                 {\"text\": \"test 1\", \"url\": \"http:\/\/example.com\"},\n-                {\"text\": \"test 2\", \"callback_data\": \"test_callback\"},\n+                {\n+                    \"text\": \"test 2\",\n+                    \"callback_data\": get_callback_data(bot, \"test_callback\"),\n+                },\n             ],\n             [\n                 {\"text\": \"test 4\", \"switch_inline_query\": \"\"},\n                 {\"text\": \"test 5\", \"switch_inline_query_current_chat\": \"wow\"},\n             ],\n             [\n-                {\"text\": \"test 3\", \"callback_data\": \"another_callback\\0data\"},\n+                {\n+                    \"text\": \"test 3\",\n+                    \"callback_data\": get_callback_data(\n+                        bot, \"another_callback\", \"data\"\n+                    ),\n+                },\n             ],\n         ],\n     }\n \n \n-def test_parse_callback_data():\n-    assert parse_callback_data(\"test\") == (\"test\", None)\n-    assert parse_callback_data(\"test:something\") == (\"test:something\", None)\n-    assert parse_callback_data(\"test\\0wow\") == (\"test\", \"wow\")\n+def test_parse_callback_data(bot):\n+    raw = get_callback_data(bot, \"test_callback\", \"this is some data!\")\n+    assert parse_callback_data(bot, raw) == (\n+        hashed_callback_name(\"test_callback\"),\n+        \"this is some data!\",\n+    )\ndiff --git a\/tests\/test_crypto.py b\/tests\/test_crypto.py\nnew file mode 100644\nindex 0000000..5ceb73d\n--- \/dev\/null\n+++ b\/tests\/test_crypto.py\n@@ -0,0 +1,45 @@\n+# Copyright (c) 2015-2017 The Botogram Authors (see AUTHORS)\n+#\n+# Permission is hereby granted, free of charge, to any person obtaining a copy\n+# of this software and associated documentation files (the \"Software\"), to deal\n+# in the Software without restriction, including without limitation the rights\n+# to use, copy, modify, merge, publish, distribute, sublicense, and\/or sell\n+# copies of the Software, and to permit persons to whom the Software is\n+# furnished to do so, subject to the following conditions:\n+#\n+#   The above copyright notice and this permission notice shall be included in\n+#   all copies or substantial portions of the Software.\n+#\n+#   THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n+#   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n+#   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n+#   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n+#   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n+#   FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n+#   DEALINGS IN THE SOFTWARE.\n+\n+import pytest\n+\n+from botogram.crypto import generate_secret_key, get_hmac, sign_data\n+from botogram.crypto import TamperedMessageError, verify_signature\n+\n+\n+def test_generate_secret_key(bot):\n+    # Check if the generated key for the test bot is correct\n+    key = generate_secret_key(bot)\n+    assert key == b'\\\\\\x93aA\\xe8\\x8d\\x9aL\\x8c\\xfd\\x81,D\\xeaj\\xd0'\n+\n+\n+def test_hmac(bot):\n+    expect = b'q\\x06\\x9c\\xc1\\xfa\\xd1n\\xe8\\xef\\x17\\xf6\\xd7Z\\xb0G\\x7f'\n+    assert get_hmac(bot, b'test data') == expect\n+\n+    signed = sign_data(bot, b'test string')\n+    assert verify_signature(bot, signed) == b'test string'\n+\n+    signed += b'a'\n+    with pytest.raises(TamperedMessageError):\n+        verify_signature(bot, signed)\n+\n+    with pytest.raises(TamperedMessageError):\n+        verify_signature(bot, b'a')\n","files":{"\/botogram\/__init__.py":{"changes":[{"diff":"\n from .runner import run\n from .objects import *\n from .utils import usernames_in\n-from .callbacks import buttons\n \n \n # This code will simulate the Windows' multiprocessing behavior if the","add":0,"remove":1,"filename":"\/botogram\/__init__.py","badparts":["from .callbacks import buttons"],"goodparts":[]}],"source":"\n from.utils import configure_logger configure_logger() del configure_logger from.api import APIError, ChatUnavailableError from.bot import Bot, create, channel from.frozenbot import FrozenBotError from.components import Component from.decorators import pass_bot, pass_shared, help_message_for from.runner import run from.objects import * from.utils import usernames_in from.callbacks import buttons import os import multiprocessing if \"BOTOGRAM_SIMULATE_WINDOWS\" in os.environ: multiprocessing.set_start_method(\"spawn\", force=True) del os, multiprocessing ","sourceWithComments":"# Copyright (c) 2015-2017 The Botogram Authors (see AUTHORS)\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and\/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n#   The above copyright notice and this permission notice shall be included in\n#   all copies or substantial portions of the Software.\n#\n#   THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n#   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n#   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n#   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n#   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n#   FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n#   DEALINGS IN THE SOFTWARE.\n\n# Prepare the logger\nfrom .utils import configure_logger\nconfigure_logger()\ndel configure_logger\n\n\n# flake8: noqa\n\nfrom .api import APIError, ChatUnavailableError\nfrom .bot import Bot, create, channel\nfrom .frozenbot import FrozenBotError\nfrom .components import Component\nfrom .decorators import pass_bot, pass_shared, help_message_for\nfrom .runner import run\nfrom .objects import *\nfrom .utils import usernames_in\nfrom .callbacks import buttons\n\n\n# This code will simulate the Windows' multiprocessing behavior if the\n# BOTOGRAM_SIMULATE_WINDOWS environment variable is set\nimport os\nimport multiprocessing\n\nif \"BOTOGRAM_SIMULATE_WINDOWS\" in os.environ:\n    multiprocessing.set_start_method(\"spawn\", force=True)\n\ndel os, multiprocessing\n"},"\/botogram\/callbacks.py":{"changes":[{"diff":"\n #   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n #   FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n \n+import base64\n+import binascii\n+import hashlib\n+\n+from . import crypto\n+\n+\n+DIGEST = hashlib.md5\n+DIGEST_LEN = 16\n+\n \n class ButtonsRow:\n     \"\"\"A row of an inline keyboard\"\"\"\n \n-    def __init__(self):\n+    def __init__(self, bot):\n         self._content = []\n+        self._bot = bot\n \n     def url(self, label, url):\n         \"\"\"Open an URL when the button is pressed\"\"\"\n","add":12,"remove":1,"filename":"\/botogram\/callbacks.py","badparts":["    def __init__(self):"],"goodparts":["import base64","import binascii","import hashlib","from . import crypto","DIGEST = hashlib.md5","DIGEST_LEN = 16","    def __init__(self, bot):","        self._bot = bot"]},{"diff":"\n \n     def callback(self, label, callback, data=None):\n         \"\"\"Trigger a callback when the button is pressed\"\"\"\n-        if data is not None:\n-            msg = \"%s\\0%s\" % (callback, data)\n-        else:\n-            msg = callback\n-\n-        self._content.append({\"text\": label, \"callback_data\": msg})\n+        self._content.append({\n+            \"text\": label,\n+            \"callback_data\": get_callback_data(self._bot, callback, data),\n+        })\n \n     def switch_inline_query(self, label, query=\"\", current_chat=False):\n         \"\"\"Switch the user to this bot's inline query\"\"\"\n","add":4,"remove":6,"filename":"\/botogram\/callbacks.py","badparts":["        if data is not None:","            msg = \"%s\\0%s\" % (callback, data)","        else:","            msg = callback","        self._content.append({\"text\": label, \"callback_data\": msg})"],"goodparts":["        self._content.append({","            \"text\": label,","            \"callback_data\": get_callback_data(self._bot, callback, data),","        })"]},{"diff":"\n class Buttons:\n     \"\"\"Factory for inline keyboards\"\"\"\n \n-    def __init__(self):\n+    def __init__(self, bot):\n         self._rows = {}\n+        self._bot = bot\n \n     def __getitem__(self, index):\n         if index not in self._rows:\n-            self._rows[index] = ButtonsRow()\n+            self._rows[index] = ButtonsRow(self._bot)\n         return self._rows[index]\n \n     def _serialize_attachment(self):\n","add":3,"remove":2,"filename":"\/botogram\/callbacks.py","badparts":["    def __init__(self):","            self._rows[index] = ButtonsRow()"],"goodparts":["    def __init__(self, bot):","        self._bot = bot","            self._rows[index] = ButtonsRow(self._bot)"]},{"diff":"\n         return {\"inline_keyboard\": rows}\n \n \n-def buttons():\n-    \"\"\"Create a new inline keyboard\"\"\"\n-    return Buttons()\n+def parse_callback_data(bot, raw):\n+    \"\"\"Parse the callback data generated by botogram and return it\"\"\"\n+    raw = raw.encode(\"utf-8\")\n \n+    if len(raw) < 32:\n+        raise crypto.TamperedMessageError\n \n-def parse_callback_data(data):\n-    \"\"\"Parse the callback data generated by botogram and return it\"\"\"\n-    if \"\\0\" in data:\n-        name, custom = data.split(\"\\0\", 1)\n-        return name, custom\n-    else:\n-        return data, None\n+    try:\n+        prelude = base64.b64decode(raw[:32])\n+    except binascii.Error:\n+        raise crypto.TamperedMessageError\n+\n+    signature = prelude[:16]\n+    name = prelude[16:]\n+    data = raw[32:]\n+\n+    if not crypto.compare(crypto.get_hmac(bot, name + data), signature):\n+        raise crypto.TamperedMessageError\n+\n+    return name, data.decode(\"utf-8\")\n+\n+\n+def get_callback_data(bot, name, data=None):\n+    \"\"\"Get the callback data for the provided name and data\"\"\"\n+    name = hashed_callback_name(name)\n+\n+    if data is None:\n+        data = \"\"\n+    data = data.encode(\"utf-8\")\n+\n+    if len(data) > 32:\n+        raise ValueError(\n+            \"The provided data is too big (%s bytes), try to reduce it to \"\n+            \"32 bytes\" % len(data)\n+        )\n+\n+    # Get the signature of the hook name and data\n+    signature = crypto.get_hmac(bot, name + data)\n+\n+    # Base64 the signature and the hook name together to save space\n+    return (base64.b64encode(signature + name) + data).decode(\"utf-8\")\n+\n+\n+def hashed_callback_name(name):\n+    \"\"\"Get the hashed name of a callback\"\"\"\n+    # Get only the first 8 bytes of the hash to fit it into the payload\n+    return DIGEST(name.encode(\"utf-8\")).digest()[:8]\n \n \n def process(bot, chains, update):\n     \"\"\"Process a callback sent to the bot\"\"\"\n+    try:\n+        name, data = parse_callback_data(bot, update.callback_query._data)\n+    except crypto.TamperedMessageError:\n+        bot.logger.warn(\n+            \"The user tampered with the #%s update's data. Skipped it.\"\n+            % update.update_id\n+        )\n+        return\n+\n     for hook in chains[\"callbacks\"]:\n         bot.logger.debug(\"Processing update #%s with the hook %s\" %\n                          (update.update_id, hook.name))\n \n-        result = hook.call(bot, update)\n+        result = hook.call(bot, update, name, data)\n         if result is True:\n             bot.logger.debug(\"Update #%s was just processed by the %s hook\" %\n                              (update.update_id, hook.name","add":55,"remove":11,"filename":"\/botogram\/callbacks.py","badparts":["def buttons():","    \"\"\"Create a new inline keyboard\"\"\"","    return Buttons()","def parse_callback_data(data):","    \"\"\"Parse the callback data generated by botogram and return it\"\"\"","    if \"\\0\" in data:","        name, custom = data.split(\"\\0\", 1)","        return name, custom","    else:","        return data, None","        result = hook.call(bot, update)"],"goodparts":["def parse_callback_data(bot, raw):","    \"\"\"Parse the callback data generated by botogram and return it\"\"\"","    raw = raw.encode(\"utf-8\")","    if len(raw) < 32:","        raise crypto.TamperedMessageError","    try:","        prelude = base64.b64decode(raw[:32])","    except binascii.Error:","        raise crypto.TamperedMessageError","    signature = prelude[:16]","    name = prelude[16:]","    data = raw[32:]","    if not crypto.compare(crypto.get_hmac(bot, name + data), signature):","        raise crypto.TamperedMessageError","    return name, data.decode(\"utf-8\")","def get_callback_data(bot, name, data=None):","    \"\"\"Get the callback data for the provided name and data\"\"\"","    name = hashed_callback_name(name)","    if data is None:","        data = \"\"","    data = data.encode(\"utf-8\")","    if len(data) > 32:","        raise ValueError(","            \"The provided data is too big (%s bytes), try to reduce it to \"","            \"32 bytes\" % len(data)","        )","    signature = crypto.get_hmac(bot, name + data)","    return (base64.b64encode(signature + name) + data).decode(\"utf-8\")","def hashed_callback_name(name):","    \"\"\"Get the hashed name of a callback\"\"\"","    return DIGEST(name.encode(\"utf-8\")).digest()[:8]","    try:","        name, data = parse_callback_data(bot, update.callback_query._data)","    except crypto.TamperedMessageError:","        bot.logger.warn(","            \"The user tampered with the #%s update's data. Skipped it.\"","            % update.update_id","        )","        return","        result = hook.call(bot, update, name, data)"]}],"source":"\n class ButtonsRow: \"\"\"A row of an inline keyboard\"\"\" def __init__(self): self._content=[] def url(self, label, url): \"\"\"Open an URL when the button is pressed\"\"\" self._content.append({\"text\": label, \"url\": url}) def callback(self, label, callback, data=None): \"\"\"Trigger a callback when the button is pressed\"\"\" if data is not None: msg=\"%s\\0%s\" %(callback, data) else: msg=callback self._content.append({\"text\": label, \"callback_data\": msg}) def switch_inline_query(self, label, query=\"\", current_chat=False): \"\"\"Switch the user to this bot's inline query\"\"\" if current_chat: self._content.append({ \"text\": label, \"switch_inline_query_current_chat\": query, }) else: self._content.append({ \"text\": label, \"switch_inline_query\": query, }) class Buttons: \"\"\"Factory for inline keyboards\"\"\" def __init__(self): self._rows={} def __getitem__(self, index): if index not in self._rows: self._rows[index]=ButtonsRow() return self._rows[index] def _serialize_attachment(self): rows=[ row._content for i, row in sorted( tuple(self._rows.items()), key=lambda i: i[0] ) ] return{\"inline_keyboard\": rows} def buttons(): \"\"\"Create a new inline keyboard\"\"\" return Buttons() def parse_callback_data(data): \"\"\"Parse the callback data generated by botogram and return it\"\"\" if \"\\0\" in data: name, custom=data.split(\"\\0\", 1) return name, custom else: return data, None def process(bot, chains, update): \"\"\"Process a callback sent to the bot\"\"\" for hook in chains[\"callbacks\"]: bot.logger.debug(\"Processing update (update.update_id, hook.name)) result=hook.call(bot, update) if result is True: bot.logger.debug(\"Update (update.update_id, hook.name)) return bot.logger.debug(\"No hook actually processed the update.update_id) ","sourceWithComments":"# Copyright (c) 2015-2017 The Botogram Authors (see AUTHORS)\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and\/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n#   The above copyright notice and this permission notice shall be included in\n#   all copies or substantial portions of the Software.\n#\n#   THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n#   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n#   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n#   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n#   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n#   FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n\n\nclass ButtonsRow:\n    \"\"\"A row of an inline keyboard\"\"\"\n\n    def __init__(self):\n        self._content = []\n\n    def url(self, label, url):\n        \"\"\"Open an URL when the button is pressed\"\"\"\n        self._content.append({\"text\": label, \"url\": url})\n\n    def callback(self, label, callback, data=None):\n        \"\"\"Trigger a callback when the button is pressed\"\"\"\n        if data is not None:\n            msg = \"%s\\0%s\" % (callback, data)\n        else:\n            msg = callback\n\n        self._content.append({\"text\": label, \"callback_data\": msg})\n\n    def switch_inline_query(self, label, query=\"\", current_chat=False):\n        \"\"\"Switch the user to this bot's inline query\"\"\"\n        if current_chat:\n            self._content.append({\n                \"text\": label,\n                \"switch_inline_query_current_chat\": query,\n            })\n        else:\n            self._content.append({\n                \"text\": label,\n                \"switch_inline_query\": query,\n            })\n\n\nclass Buttons:\n    \"\"\"Factory for inline keyboards\"\"\"\n\n    def __init__(self):\n        self._rows = {}\n\n    def __getitem__(self, index):\n        if index not in self._rows:\n            self._rows[index] = ButtonsRow()\n        return self._rows[index]\n\n    def _serialize_attachment(self):\n        rows = [\n            row._content for i, row in sorted(\n                tuple(self._rows.items()), key=lambda i: i[0]\n            )\n        ]\n\n        return {\"inline_keyboard\": rows}\n\n\ndef buttons():\n    \"\"\"Create a new inline keyboard\"\"\"\n    return Buttons()\n\n\ndef parse_callback_data(data):\n    \"\"\"Parse the callback data generated by botogram and return it\"\"\"\n    if \"\\0\" in data:\n        name, custom = data.split(\"\\0\", 1)\n        return name, custom\n    else:\n        return data, None\n\n\ndef process(bot, chains, update):\n    \"\"\"Process a callback sent to the bot\"\"\"\n    for hook in chains[\"callbacks\"]:\n        bot.logger.debug(\"Processing update #%s with the hook %s\" %\n                         (update.update_id, hook.name))\n\n        result = hook.call(bot, update)\n        if result is True:\n            bot.logger.debug(\"Update #%s was just processed by the %s hook\" %\n                             (update.update_id, hook.name))\n            return\n\n    bot.logger.debug(\"No hook actually processed the #%s update.\" %\n                     update.update_id)\n"},"\/botogram\/hooks.py":{"changes":[{"diff":"\n \n import re\n \n-from .callbacks import parse_callback_data\n+from .callbacks import hashed_callback_name\n \n \n class Hook:\n","add":1,"remove":1,"filename":"\/botogram\/hooks.py","badparts":["from .callbacks import parse_callback_data"],"goodparts":["from .callbacks import hashed_callback_name"]},{"diff":"\n     \"\"\"Underlying hook for @bot.callback\"\"\"\n \n     def _after_init(self, args):\n-        self._name = \"%s:%s\" % (self.component.component_name, args[\"name\"])\n+        self._name = hashed_callback_name(\n+            \"%s:%s\" % (self.component.component_name, args[\"name\"])\n+        )\n \n-    def _call(self, bot, update):\n+    def call(self, bot, update, name, data):\n         if not update.callback_query:\n             return\n         q = update.callback_query\n \n-        name, data = parse_callback_data(q._data)\n         if name != self._name:\n             ret","add":4,"remove":3,"filename":"\/botogram\/hooks.py","badparts":["        self._name = \"%s:%s\" % (self.component.component_name, args[\"name\"])","    def _call(self, bot, update):","        name, data = parse_callback_data(q._data)"],"goodparts":["        self._name = hashed_callback_name(","            \"%s:%s\" % (self.component.component_name, args[\"name\"])","        )","    def call(self, bot, update, name, data):"]}],"source":"\n import re from.callbacks import parse_callback_data class Hook: \"\"\"Base class for all the hooks\"\"\" _only_texts=False _botogram_hook=True def __init__(self, func, component, args=None): prefix=\"\" if component.component_name: prefix=component.component_name +\"::\" self.func=func self.name=prefix +func.__name__ self.component=component self.component_id=component._component_id self._args=args self._after_init(args) def __reduce__(self): return rebuild,(self.__class__, self.func, self.component, self._args) def __repr__(self): return \"<\" +self.__class__.__name__ +\" \\\"\" +self.name +\"\\\">\" def _after_init(self, args): \"\"\"Prepare the object\"\"\" pass def call(self, bot, update): \"\"\"Call the hook\"\"\" if self._only_texts and update.message.text is None: return return self._call(bot, update) def _call(self, bot, update): \"\"\"*Actually* call the hook\"\"\" message=update.message return bot._call(self.func, self.component_id, chat=message.chat, message=message) def rebuild(cls, func, component, args): hook=cls(func, component, args) return hook class BeforeProcessingHook(Hook): \"\"\"Underlying hook for @bot.process_message\"\"\" pass class ProcessMessageHook(Hook): \"\"\"Underlying hook for @bot.process_message\"\"\" pass class MemoryPreparerHook(Hook): \"\"\"Underlying hook for @bot.prepare_memory\"\"\" def call(self, memory): return self.func(memory) class NoCommandsHook(Hook): \"\"\"Underlying hook for an internal event\"\"\" pass class MessageEqualsHook(Hook): \"\"\"Underlying hook for @bot.message_equals\"\"\" _only_texts=True def _after_init(self, args): if args[\"ignore_case\"]: self._string=args[\"string\"].lower() else: self._string=args[\"string\"] def _prepare(self, update): message=update.message text=message.text if self._args[\"ignore_case\"]: text=text.lower() return message, text def _call(self, bot, update): message, text=self._prepare(update) if text !=self._string: return return bot._call(self.func, self.component_id, chat=message.chat, message=message) class MessageContainsHook(MessageEqualsHook): \"\"\"Underlying hook for @bot.message_contains\"\"\" _only_texts=True def _call(self, bot, update): message, text=self._prepare(update) splitted=text.split(\" \") res=[] for one in splitted: if one !=self._string: continue result=bot._call(self.func, self.component_id, chat=message.chat, message=message) res.append(result) if not self._args[\"multiple\"]: break return len(res) > 0 class MessageMatchesHook(Hook): \"\"\"Underlying hook for @bot.message_matches\"\"\" _only_texts=True def _after_init(self, args): self._regex=re.compile(args[\"regex\"], flags=args[\"flags\"]) def _call(self, bot, update): message=update.message results=self._regex.finditer(message.text) found=False for result in results: found=True bot._call(self.func, self.component_id, chat=message.chat, message=message, matches=result.groups()) if not self._args[\"multiple\"]: break return found _command_args_split_re=re.compile(r' +') class CommandHook(Hook): \"\"\"Underlying hook for @bot.command\"\"\" _only_texts=True def _after_init(self, args): if not re.match(r'^[a-zA-Z0-9_]+$', args[\"name\"]): raise ValueError(\"Invalid command name: %s\" % args[\"name\"]) self._regex=re.compile(r'^\\\/' +args[\"name\"] +r'(@[a-zA-Z0-9_]+)?' r'(.*)?$') self._name=args[\"name\"] self._hidden=False if \"hidden\" in args: self._hidden=args[\"hidden\"] self._order=0 if \"order\" in args: self._order=args[\"order\"] def _call(self, bot, update): message=update.message text=message.text.replace(\"\\n\", \" \").replace(\"\\t\", \" \") match=self._regex.match(text) if not match: return if match.group(1) and match.group(1) !=\"@\" +bot.itself.username: return args=_command_args_split_re.split(text)[1:] bot._call(self.func, self.component_id, chat=message.chat, message=message, args=args) return True class CallbackHook(Hook): \"\"\"Underlying hook for @bot.callback\"\"\" def _after_init(self, args): self._name=\"%s:%s\" %(self.component.component_name, args[\"name\"]) def _call(self, bot, update): if not update.callback_query: return q=update.callback_query name, data=parse_callback_data(q._data) if name !=self._name: return bot._call( self.func, self.component_id, query=q, chat=q.message.chat, message=q.message, data=data, ) update.callback_query._maybe_send_noop() return True class ChatUnavailableHook(Hook): \"\"\"Underlying hook for @bot.chat_unavailable\"\"\" def call(self, bot, chat_id, reason): return bot._call(self.func, self.component_id, chat_id=chat_id, reason=reason) class MessageEditedHook(Hook): \"\"\"Underlying hook for @bot.message_edited\"\"\" def _call(self, bot, update): message=update.edited_message return bot._call(self.func, self.component_id, chat=message.chat, message=message) class ChannelPostHook(Hook): \"\"\"Underlying hook for @bot.channel_post\"\"\" def _call(self, bot, update): message=update.channel_post return bot._call(self.func, self.component_id, chat=message.chat, message=message) class EditedChannelPostHook(Hook): \"\"\"Underlying hook for @bot.channel_post_edited\"\"\" def _call(self, bot, update): message=update.edited_channel_post return bot._call(self.func, self.component_id, chat=message.chat, message=message) class TimerHook(Hook): \"\"\"Underlying hook for a timer\"\"\" def call(self, bot): return bot._call(self.func, self.component_id) ","sourceWithComments":"# Copyright (c) 2015-2017 The Botogram Authors (see AUTHORS)\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and\/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n#   The above copyright notice and this permission notice shall be included in\n#   all copies or substantial portions of the Software.\n#\n#   THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n#   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n#   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n#   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n#   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n#   FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n\nimport re\n\nfrom .callbacks import parse_callback_data\n\n\nclass Hook:\n    \"\"\"Base class for all the hooks\"\"\"\n\n    _only_texts = False\n    _botogram_hook = True\n\n    def __init__(self, func, component, args=None):\n        prefix = \"\"\n        if component.component_name:\n            prefix = component.component_name + \"::\"\n\n        self.func = func\n        self.name = prefix + func.__name__\n        self.component = component\n        self.component_id = component._component_id\n\n        self._args = args\n        self._after_init(args)\n\n    def __reduce__(self):\n        return rebuild, (self.__class__, self.func, self.component, self._args)\n\n    def __repr__(self):\n        return \"<\" + self.__class__.__name__ + \" \\\"\" + self.name + \"\\\">\"\n\n    def _after_init(self, args):\n        \"\"\"Prepare the object\"\"\"\n        pass\n\n    def call(self, bot, update):\n        \"\"\"Call the hook\"\"\"\n        if self._only_texts and update.message.text is None:\n            return\n        return self._call(bot, update)\n\n    def _call(self, bot, update):\n        \"\"\"*Actually* call the hook\"\"\"\n        message = update.message\n        return bot._call(self.func, self.component_id, chat=message.chat,\n                         message=message)\n\n\ndef rebuild(cls, func, component, args):\n    hook = cls(func, component, args)\n    return hook\n\n\nclass BeforeProcessingHook(Hook):\n    \"\"\"Underlying hook for @bot.process_message\"\"\"\n    pass\n\n\nclass ProcessMessageHook(Hook):\n    \"\"\"Underlying hook for @bot.process_message\"\"\"\n    pass\n\n\nclass MemoryPreparerHook(Hook):\n    \"\"\"Underlying hook for @bot.prepare_memory\"\"\"\n\n    def call(self, memory):\n        return self.func(memory)\n\n\nclass NoCommandsHook(Hook):\n    \"\"\"Underlying hook for an internal event\"\"\"\n    pass\n\n\nclass MessageEqualsHook(Hook):\n    \"\"\"Underlying hook for @bot.message_equals\"\"\"\n\n    _only_texts = True\n\n    def _after_init(self, args):\n        if args[\"ignore_case\"]:\n            self._string = args[\"string\"].lower()\n        else:\n            self._string = args[\"string\"]\n\n    def _prepare(self, update):\n        message = update.message\n        text = message.text\n        if self._args[\"ignore_case\"]:\n            text = text.lower()\n\n        return message, text\n\n    def _call(self, bot, update):\n        message, text = self._prepare(update)\n\n        if text != self._string:\n            return\n        return bot._call(self.func, self.component_id, chat=message.chat,\n                         message=message)\n\n\nclass MessageContainsHook(MessageEqualsHook):\n    \"\"\"Underlying hook for @bot.message_contains\"\"\"\n\n    _only_texts = True\n\n    def _call(self, bot, update):\n        message, text = self._prepare(update)\n        splitted = text.split(\" \")\n\n        res = []\n        for one in splitted:\n            if one != self._string:\n                continue\n\n            result = bot._call(self.func, self.component_id, chat=message.chat,\n                               message=message)\n            res.append(result)\n            if not self._args[\"multiple\"]:\n                break\n\n        return len(res) > 0\n\n\nclass MessageMatchesHook(Hook):\n    \"\"\"Underlying hook for @bot.message_matches\"\"\"\n\n    _only_texts = True\n\n    def _after_init(self, args):\n        self._regex = re.compile(args[\"regex\"], flags=args[\"flags\"])\n\n    def _call(self, bot, update):\n        message = update.message\n        results = self._regex.finditer(message.text)\n\n        found = False\n        for result in results:\n            found = True\n\n            bot._call(self.func, self.component_id, chat=message.chat,\n                      message=message, matches=result.groups())\n            if not self._args[\"multiple\"]:\n                break\n\n        return found\n\n\n_command_args_split_re = re.compile(r' +')\n\n\nclass CommandHook(Hook):\n    \"\"\"Underlying hook for @bot.command\"\"\"\n\n    _only_texts = True\n\n    def _after_init(self, args):\n        # Check if the command name is valid\n        if not re.match(r'^[a-zA-Z0-9_]+$', args[\"name\"]):\n            raise ValueError(\"Invalid command name: %s\" % args[\"name\"])\n\n        # This regex will match all commands pointed to this bot\n        self._regex = re.compile(r'^\\\/' + args[\"name\"] + r'(@[a-zA-Z0-9_]+)?'\n                                 r'( .*)?$')\n\n        self._name = args[\"name\"]\n        self._hidden = False\n        if \"hidden\" in args:\n            self._hidden = args[\"hidden\"]\n        self._order = 0\n        if \"order\" in args:\n            self._order = args[\"order\"]\n\n    def _call(self, bot, update):\n        message = update.message\n        text = message.text.replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n\n        # Must be the correct command for the correct bot\n        match = self._regex.match(text)\n        if not match:\n            return\n        if match.group(1) and match.group(1) != \"@\" + bot.itself.username:\n            return\n\n        args = _command_args_split_re.split(text)[1:]\n        bot._call(self.func, self.component_id, chat=message.chat,\n                  message=message, args=args)\n        return True\n\n\nclass CallbackHook(Hook):\n    \"\"\"Underlying hook for @bot.callback\"\"\"\n\n    def _after_init(self, args):\n        self._name = \"%s:%s\" % (self.component.component_name, args[\"name\"])\n\n    def _call(self, bot, update):\n        if not update.callback_query:\n            return\n        q = update.callback_query\n\n        name, data = parse_callback_data(q._data)\n        if name != self._name:\n            return\n\n        bot._call(\n            self.func, self.component_id, query=q, chat=q.message.chat,\n            message=q.message, data=data,\n        )\n\n        update.callback_query._maybe_send_noop()\n        return True\n\n\nclass ChatUnavailableHook(Hook):\n    \"\"\"Underlying hook for @bot.chat_unavailable\"\"\"\n\n    def call(self, bot, chat_id, reason):\n        return bot._call(self.func, self.component_id, chat_id=chat_id,\n                         reason=reason)\n\n\nclass MessageEditedHook(Hook):\n    \"\"\"Underlying hook for @bot.message_edited\"\"\"\n\n    def _call(self, bot, update):\n        message = update.edited_message\n        return bot._call(self.func, self.component_id, chat=message.chat,\n                         message=message)\n\n\nclass ChannelPostHook(Hook):\n    \"\"\"Underlying hook for @bot.channel_post\"\"\"\n\n    def _call(self, bot, update):\n        message = update.channel_post\n        return bot._call(self.func, self.component_id, chat=message.chat,\n                         message=message)\n\n\nclass EditedChannelPostHook(Hook):\n    \"\"\"Underlying hook for @bot.channel_post_edited\"\"\"\n\n    def _call(self, bot, update):\n        message = update.edited_channel_post\n        return bot._call(self.func, self.component_id, chat=message.chat,\n                         message=message)\n\n\nclass TimerHook(Hook):\n    \"\"\"Underlying hook for a timer\"\"\"\n\n    def call(self, bot):\n        return bot._call(self.func, self.component_id)\n"},"\/tests\/test_callbacks.py":{"changes":[{"diff":"\n \n import json\n \n-from botogram.callbacks import Buttons, parse_callback_data\n+from botogram.callbacks import Buttons, parse_callback_data, get_callback_data\n+from botogram.callbacks import hashed_callback_name\n \n \n-def test_buttons():\n-    buttons = Buttons()\n+def test_buttons(bot):\n+    buttons = Buttons(bot)\n     buttons[0].url(\"test 1\", \"http:\/\/example.com\")\n     buttons[0].callback(\"test 2\", \"test_callback\")\n     buttons[3].callback(\"test 3\", \"another_callback\", \"data\")\n","add":4,"remove":3,"filename":"\/tests\/test_callbacks.py","badparts":["from botogram.callbacks import Buttons, parse_callback_data","def test_buttons():","    buttons = Buttons()"],"goodparts":["from botogram.callbacks import Buttons, parse_callback_data, get_callback_data","from botogram.callbacks import hashed_callback_name","def test_buttons(bot):","    buttons = Buttons(bot)"]},{"diff":"\n         \"inline_keyboard\": [\n             [\n                 {\"text\": \"test 1\", \"url\": \"http:\/\/example.com\"},\n-                {\"text\": \"test 2\", \"callback_data\": \"test_callback\"},\n+                {\n+                    \"text\": \"test 2\",\n+                    \"callback_data\": get_callback_data(bot, \"test_callback\"),\n+                },\n             ],\n             [\n                 {\"text\": \"test 4\", \"switch_inline_query\": \"\"},\n                 {\"text\": \"test 5\", \"switch_inline_query_current_chat\": \"wow\"},\n             ],\n             [\n-                {\"text\": \"test 3\", \"callback_data\": \"another_callback\\0data\"},\n+                {\n+                    \"text\": \"test 3\",\n+                    \"callback_data\": get_callback_data(\n+                        bot, \"another_callback\", \"data\"\n+                    ),\n+                },\n             ],\n         ],\n     }\n \n \n-def test_parse_callback_data():\n-    assert parse_callback_data(\"test\") == (\"test\", None)\n-    assert parse_callback_data(\"test:something\") == (\"test:something\", None)\n-    assert parse_callback_data(\"test\\0wow\") == (\"test\", \"wow\")\n+def test_parse_callback_data(bot):\n+    raw = get_callback_data(bot, \"test_callback\", \"this is some data!\")\n+    assert parse_callback_data(bot, raw) == (\n+        hashed_callback_name(\"test_callback\"),\n+        \"this is some data!\",\n","add":15,"remove":6,"filename":"\/tests\/test_callbacks.py","badparts":["                {\"text\": \"test 2\", \"callback_data\": \"test_callback\"},","                {\"text\": \"test 3\", \"callback_data\": \"another_callback\\0data\"},","def test_parse_callback_data():","    assert parse_callback_data(\"test\") == (\"test\", None)","    assert parse_callback_data(\"test:something\") == (\"test:something\", None)","    assert parse_callback_data(\"test\\0wow\") == (\"test\", \"wow\")"],"goodparts":["                {","                    \"text\": \"test 2\",","                    \"callback_data\": get_callback_data(bot, \"test_callback\"),","                },","                {","                    \"text\": \"test 3\",","                    \"callback_data\": get_callback_data(","                        bot, \"another_callback\", \"data\"","                    ),","                },","def test_parse_callback_data(bot):","    raw = get_callback_data(bot, \"test_callback\", \"this is some data!\")","    assert parse_callback_data(bot, raw) == (","        hashed_callback_name(\"test_callback\"),","        \"this is some data!\","]}],"source":"\n import json from botogram.callbacks import Buttons, parse_callback_data def test_buttons(): buttons=Buttons() buttons[0].url(\"test 1\", \"http:\/\/example.com\") buttons[0].callback(\"test 2\", \"test_callback\") buttons[3].callback(\"test 3\", \"another_callback\", \"data\") buttons[2].switch_inline_query(\"test 4\") buttons[2].switch_inline_query(\"test 5\", \"wow\", current_chat=True) assert buttons._serialize_attachment()=={ \"inline_keyboard\":[ [ {\"text\": \"test 1\", \"url\": \"http:\/\/example.com\"}, {\"text\": \"test 2\", \"callback_data\": \"test_callback\"}, ], [ {\"text\": \"test 4\", \"switch_inline_query\": \"\"}, {\"text\": \"test 5\", \"switch_inline_query_current_chat\": \"wow\"}, ], [ {\"text\": \"test 3\", \"callback_data\": \"another_callback\\0data\"}, ], ], } def test_parse_callback_data(): assert parse_callback_data(\"test\")==(\"test\", None) assert parse_callback_data(\"test:something\")==(\"test:something\", None) assert parse_callback_data(\"test\\0wow\")==(\"test\", \"wow\") ","sourceWithComments":"# Copyright (c) 2015-2017 The Botogram Authors (see AUTHORS)\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and\/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n#   The above copyright notice and this permission notice shall be included in\n#   all copies or substantial portions of the Software.\n#\n#   THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n#   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n#   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n#   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n#   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n#   FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n#   DEALINGS IN THE SOFTWARE.\n\nimport json\n\nfrom botogram.callbacks import Buttons, parse_callback_data\n\n\ndef test_buttons():\n    buttons = Buttons()\n    buttons[0].url(\"test 1\", \"http:\/\/example.com\")\n    buttons[0].callback(\"test 2\", \"test_callback\")\n    buttons[3].callback(\"test 3\", \"another_callback\", \"data\")\n    buttons[2].switch_inline_query(\"test 4\")\n    buttons[2].switch_inline_query(\"test 5\", \"wow\", current_chat=True)\n\n    assert buttons._serialize_attachment() == {\n        \"inline_keyboard\": [\n            [\n                {\"text\": \"test 1\", \"url\": \"http:\/\/example.com\"},\n                {\"text\": \"test 2\", \"callback_data\": \"test_callback\"},\n            ],\n            [\n                {\"text\": \"test 4\", \"switch_inline_query\": \"\"},\n                {\"text\": \"test 5\", \"switch_inline_query_current_chat\": \"wow\"},\n            ],\n            [\n                {\"text\": \"test 3\", \"callback_data\": \"another_callback\\0data\"},\n            ],\n        ],\n    }\n\n\ndef test_parse_callback_data():\n    assert parse_callback_data(\"test\") == (\"test\", None)\n    assert parse_callback_data(\"test:something\") == (\"test:something\", None)\n    assert parse_callback_data(\"test\\0wow\") == (\"test\", \"wow\")\n"}},"msg":"Sign callback data to prevent tampering with it"}},"https:\/\/github.com\/MaloneCode\/AUTOMATIC1111":{"29a2933e23e68900bbae741a98439d0c9d6f26f6":{"url":"https:\/\/api.github.com\/repos\/MaloneCode\/AUTOMATIC1111\/commits\/29a2933e23e68900bbae741a98439d0c9d6f26f6","html_url":"https:\/\/github.com\/MaloneCode\/AUTOMATIC1111\/commit\/29a2933e23e68900bbae741a98439d0c9d6f26f6","message":"Add --hide-ui-dir-config command line flag\n\nAdds `--hide-ui-dir-config` flag to disable editing directory configs from the web UI. This can be set to prevent users from setting the directory to somewhere they shouldn't, for public (or semi-public) interfaces.\n\nDirectories are still read from config.json, so the server admin can still set them in the web UI and then relaunch with the hide flag, or edit the config manually.\n\nAlso:\n- fix OptionInfo `component_args` keyword argument not being read if `component` isn't also set\n- ensure that hidden settings aren't still read from the web UI (otherwise they could still be changed by tampering with the interface)","sha":"29a2933e23e68900bbae741a98439d0c9d6f26f6","keyword":"tampering prevent","diff":"diff --git a\/modules\/shared.py b\/modules\/shared.py\nindex 9eeb64e..0557cfe 100644\n--- a\/modules\/shared.py\n+++ b\/modules\/shared.py\n@@ -42,6 +42,7 @@\n parser.add_argument(\"--port\", type=int, help=\"launch gradio with given server port, you need root\/admin rights for ports < 1024, defaults to 7860 if available\", default=None)\r\n parser.add_argument(\"--show-negative-prompt\", action='store_true', help=\"does not do anything\", default=False)\r\n parser.add_argument(\"--ui-config-file\", type=str, help=\"filename to use for ui configuration\", default=os.path.join(script_path, 'ui-config.json'))\r\n+parser.add_argument(\"--hide-ui-dir-config\", action='store_true', help=\"hide directory configuration from webui\", default=False)\r\n \r\n cmd_opts = parser.parse_args()\r\n \r\n@@ -91,18 +92,19 @@ def __init__(self, default=None, label=\"\", component=None, component_args=None):\n             self.component_args = component_args\r\n \r\n     data = None\r\n+    hide_dirs = {\"visible\": False} if cmd_opts.hide_ui_dir_config else None\r\n     data_labels = {\r\n-        \"outdir_samples\": OptionInfo(\"\", \"Output directory for images; if empty, defaults to two directories below\"),\r\n-        \"outdir_txt2img_samples\": OptionInfo(\"outputs\/txt2img-images\", 'Output directory for txt2img images'),\r\n-        \"outdir_img2img_samples\": OptionInfo(\"outputs\/img2img-images\", 'Output directory for img2img images'),\r\n-        \"outdir_extras_samples\": OptionInfo(\"outputs\/extras-images\", 'Output directory for images from extras tab'),\r\n-        \"outdir_grids\": OptionInfo(\"\", \"Output directory for grids; if empty, defaults to two directories below\"),\r\n-        \"outdir_txt2img_grids\": OptionInfo(\"outputs\/txt2img-grids\", 'Output directory for txt2img grids'),\r\n-        \"outdir_img2img_grids\": OptionInfo(\"outputs\/img2img-grids\", 'Output directory for img2img grids'),\r\n+        \"outdir_samples\": OptionInfo(\"\", \"Output directory for images; if empty, defaults to two directories below\", component_args=hide_dirs),\r\n+        \"outdir_txt2img_samples\": OptionInfo(\"outputs\/txt2img-images\", 'Output directory for txt2img images', component_args=hide_dirs),\r\n+        \"outdir_img2img_samples\": OptionInfo(\"outputs\/img2img-images\", 'Output directory for img2img images', component_args=hide_dirs),\r\n+        \"outdir_extras_samples\": OptionInfo(\"outputs\/extras-images\", 'Output directory for images from extras tab', component_args=hide_dirs),\r\n+        \"outdir_grids\": OptionInfo(\"\", \"Output directory for grids; if empty, defaults to two directories below\", component_args=hide_dirs),\r\n+        \"outdir_txt2img_grids\": OptionInfo(\"outputs\/txt2img-grids\", 'Output directory for txt2img grids', component_args=hide_dirs),\r\n+        \"outdir_img2img_grids\": OptionInfo(\"outputs\/img2img-grids\", 'Output directory for img2img grids', component_args=hide_dirs),\r\n         \"save_to_dirs\": OptionInfo(False, \"When writing images, create a directory with name derived from the prompt\"),\r\n         \"grid_save_to_dirs\": OptionInfo(False, \"When writing grids, create a directory with name derived from the prompt\"),\r\n         \"save_to_dirs_prompt_len\": OptionInfo(10, \"When using above, how many words from prompt to put into directory name\", gr.Slider, {\"minimum\": 1, \"maximum\": 32, \"step\": 1}),\r\n-        \"outdir_save\": OptionInfo(\"log\/images\", \"Directory for saving images using the Save button\"),\r\n+        \"outdir_save\": OptionInfo(\"log\/images\", \"Directory for saving images using the Save button\", component_args=hide_dirs),\r\n         \"samples_save\": OptionInfo(True, \"Save indiviual samples\"),\r\n         \"samples_format\": OptionInfo('png', 'File format for individual samples'),\r\n         \"grid_save\": OptionInfo(True, \"Save image grids\"),\r\ndiff --git a\/modules\/ui.py b\/modules\/ui.py\nindex b9af2c8..3b7eb9b 100644\n--- a\/modules\/ui.py\n+++ b\/modules\/ui.py\n@@ -661,19 +661,20 @@ def fun():\n         info = opts.data_labels[key]\r\n         t = type(info.default)\r\n \r\n+        args = info.component_args() if callable(info.component_args) else info.component_args\r\n+\r\n         if info.component is not None:\r\n-            args = info.component_args() if callable(info.component_args) else info.component_args\r\n-            item = info.component(label=info.label, value=fun, **(args or {}))\r\n+            comp = info.component\r\n         elif t == str:\r\n-            item = gr.Textbox(label=info.label, value=fun, lines=1)\r\n+            comp = gr.Textbox\r\n         elif t == int:\r\n-            item = gr.Number(label=info.label, value=fun)\r\n+            comp = gr.Number\r\n         elif t == bool:\r\n-            item = gr.Checkbox(label=info.label, value=fun)\r\n+            comp = gr.Checkbox\r\n         else:\r\n             raise Exception(f'bad options item type: {str(t)} for key {key}')\r\n \r\n-        return item\r\n+        return comp(label=info.label, value=fun, **(args or {}))\r\n \r\n     components = []\r\n     keys = list(opts.data_labels.keys())\r\n@@ -684,6 +685,10 @@ def run_settings(*args):\n         up = []\r\n \r\n         for key, value, comp in zip(opts.data_labels.keys(), args, components):\r\n+            comp_args = opts.data_labels[key].component_args\r\n+            if comp_args and isinstance(comp_args, dict) and comp_args.get('visible') is False:\r\n+                continue\r\n+\r\n             opts.data[key] = value\r\n             up.append(comp.update(value=value))\r\n \r\n","files":{"\/modules\/shared.py":{"changes":[{"diff":"\n             self.component_args = component_args\r\n \r\n     data = None\r\n+    hide_dirs = {\"visible\": False} if cmd_opts.hide_ui_dir_config else None\r\n     data_labels = {\r\n-        \"outdir_samples\": OptionInfo(\"\", \"Output directory for images; if empty, defaults to two directories below\"),\r\n-        \"outdir_txt2img_samples\": OptionInfo(\"outputs\/txt2img-images\", 'Output directory for txt2img images'),\r\n-        \"outdir_img2img_samples\": OptionInfo(\"outputs\/img2img-images\", 'Output directory for img2img images'),\r\n-        \"outdir_extras_samples\": OptionInfo(\"outputs\/extras-images\", 'Output directory for images from extras tab'),\r\n-        \"outdir_grids\": OptionInfo(\"\", \"Output directory for grids; if empty, defaults to two directories below\"),\r\n-        \"outdir_txt2img_grids\": OptionInfo(\"outputs\/txt2img-grids\", 'Output directory for txt2img grids'),\r\n-        \"outdir_img2img_grids\": OptionInfo(\"outputs\/img2img-grids\", 'Output directory for img2img grids'),\r\n+        \"outdir_samples\": OptionInfo(\"\", \"Output directory for images; if empty, defaults to two directories below\", component_args=hide_dirs),\r\n+        \"outdir_txt2img_samples\": OptionInfo(\"outputs\/txt2img-images\", 'Output directory for txt2img images', component_args=hide_dirs),\r\n+        \"outdir_img2img_samples\": OptionInfo(\"outputs\/img2img-images\", 'Output directory for img2img images', component_args=hide_dirs),\r\n+        \"outdir_extras_samples\": OptionInfo(\"outputs\/extras-images\", 'Output directory for images from extras tab', component_args=hide_dirs),\r\n+        \"outdir_grids\": OptionInfo(\"\", \"Output directory for grids; if empty, defaults to two directories below\", component_args=hide_dirs),\r\n+        \"outdir_txt2img_grids\": OptionInfo(\"outputs\/txt2img-grids\", 'Output directory for txt2img grids', component_args=hide_dirs),\r\n+        \"outdir_img2img_grids\": OptionInfo(\"outputs\/img2img-grids\", 'Output directory for img2img grids', component_args=hide_dirs),\r\n         \"save_to_dirs\": OptionInfo(False, \"When writing images, create a directory with name derived from the prompt\"),\r\n         \"grid_save_to_dirs\": OptionInfo(False, \"When writing grids, create a directory with name derived from the prompt\"),\r\n         \"save_to_dirs_prompt_len\": OptionInfo(10, \"When using above, how many words from prompt to put into directory name\", gr.Slider, {\"minimum\": 1, \"maximum\": 32, \"step\": 1}),\r\n-        \"outdir_save\": OptionInfo(\"log\/images\", \"Directory for saving images using the Save button\"),\r\n+        \"outdir_save\": OptionInfo(\"log\/images\", \"Directory for saving images using the Save button\", component_args=hide_dirs),\r\n         \"samples_save\": OptionInfo(True, \"Save indiviual samples\"),\r\n         \"samples_format\": OptionInfo('png', 'File format for individual samples'),\r\n         \"grid_save\": OptionInfo(True, \"Save image grids\"),\r","add":9,"remove":8,"filename":"\/modules\/shared.py","badparts":["        \"outdir_samples\": OptionInfo(\"\", \"Output directory for images; if empty, defaults to two directories below\"),\r","        \"outdir_txt2img_samples\": OptionInfo(\"outputs\/txt2img-images\", 'Output directory for txt2img images'),\r","        \"outdir_img2img_samples\": OptionInfo(\"outputs\/img2img-images\", 'Output directory for img2img images'),\r","        \"outdir_extras_samples\": OptionInfo(\"outputs\/extras-images\", 'Output directory for images from extras tab'),\r","        \"outdir_grids\": OptionInfo(\"\", \"Output directory for grids; if empty, defaults to two directories below\"),\r","        \"outdir_txt2img_grids\": OptionInfo(\"outputs\/txt2img-grids\", 'Output directory for txt2img grids'),\r","        \"outdir_img2img_grids\": OptionInfo(\"outputs\/img2img-grids\", 'Output directory for img2img grids'),\r","        \"outdir_save\": OptionInfo(\"log\/images\", \"Directory for saving images using the Save button\"),\r"],"goodparts":["    hide_dirs = {\"visible\": False} if cmd_opts.hide_ui_dir_config else None\r","        \"outdir_samples\": OptionInfo(\"\", \"Output directory for images; if empty, defaults to two directories below\", component_args=hide_dirs),\r","        \"outdir_txt2img_samples\": OptionInfo(\"outputs\/txt2img-images\", 'Output directory for txt2img images', component_args=hide_dirs),\r","        \"outdir_img2img_samples\": OptionInfo(\"outputs\/img2img-images\", 'Output directory for img2img images', component_args=hide_dirs),\r","        \"outdir_extras_samples\": OptionInfo(\"outputs\/extras-images\", 'Output directory for images from extras tab', component_args=hide_dirs),\r","        \"outdir_grids\": OptionInfo(\"\", \"Output directory for grids; if empty, defaults to two directories below\", component_args=hide_dirs),\r","        \"outdir_txt2img_grids\": OptionInfo(\"outputs\/txt2img-grids\", 'Output directory for txt2img grids', component_args=hide_dirs),\r","        \"outdir_img2img_grids\": OptionInfo(\"outputs\/img2img-grids\", 'Output directory for img2img grids', component_args=hide_dirs),\r","        \"outdir_save\": OptionInfo(\"log\/images\", \"Directory for saving images using the Save button\", component_args=hide_dirs),\r"]}],"source":"\nimport sys\r import argparse\r import json\r import os\r \r import gradio as gr\r import torch\r import tqdm\r \r import modules.artists\r from modules.paths import script_path, sd_path\r from modules.devices import get_optimal_device\r import modules.styles\r import modules.interrogate\r \r config_filename=\"config.json\"\r \r sd_model_file=os.path.join(script_path, 'model.ckpt')\r if not os.path.exists(sd_model_file):\r sd_model_file=\"models\/ldm\/stable-diffusion-v1\/model.ckpt\"\r \r parser=argparse.ArgumentParser()\r parser.add_argument(\"--config\", type=str, default=os.path.join(sd_path, \"configs\/stable-diffusion\/v1-inference.yaml\"), help=\"path to config which constructs model\",)\r parser.add_argument(\"--ckpt\", type=str, default=os.path.join(sd_path, sd_model_file), help=\"path to checkpoint of model\",)\r parser.add_argument(\"--gfpgan-dir\", type=str, help=\"GFPGAN directory\", default=('.\/src\/gfpgan' if os.path.exists('.\/src\/gfpgan') else '.\/GFPGAN'))\r parser.add_argument(\"--gfpgan-model\", type=str, help=\"GFPGAN model file name\", default='GFPGANv1.3.pth')\r parser.add_argument(\"--no-half\", action='store_true', help=\"do not switch the model to 16-bit floats\")\r parser.add_argument(\"--no-progressbar-hiding\", action='store_true', help=\"do not hide progressbar in gradio UI(we hide it because it slows down ML if you have hardware accleration in browser)\")\r parser.add_argument(\"--max-batch-count\", type=int, default=16, help=\"maximum batch count value for the UI\")\r parser.add_argument(\"--embeddings-dir\", type=str, default='embeddings', help=\"embeddings directory for textual inversion(default: embeddings)\")\r parser.add_argument(\"--allow-code\", action='store_true', help=\"allow custom script execution from webui\")\r parser.add_argument(\"--medvram\", action='store_true', help=\"enable stable diffusion model optimizations for sacrificing a little speed for low VRM usage\")\r parser.add_argument(\"--lowvram\", action='store_true', help=\"enable stable diffusion model optimizations for sacrificing a lot of speed for very low VRM usage\")\r parser.add_argument(\"--always-batch-cond-uncond\", action='store_true', help=\"a workaround test; may help with speed if you use --lowvram\")\r parser.add_argument(\"--unload-gfpgan\", action='store_true', help=\"unload GFPGAN every time after processing images. Warning: seems to cause memory leaks\")\r parser.add_argument(\"--precision\", type=str, help=\"evaluate at this precision\", choices=[\"full\", \"autocast\"], default=\"autocast\")\r parser.add_argument(\"--share\", action='store_true', help=\"use share=True for gradio and make the UI accessible through their site(doesn't work for me but you might have better luck)\")\r parser.add_argument(\"--esrgan-models-path\", type=str, help=\"path to directory with ESRGAN models\", default=os.path.join(script_path, 'ESRGAN'))\r parser.add_argument(\"--opt-split-attention\", action='store_true', help=\"enable optimization that reduce vram usage by a lot for about 10%% decrease in performance\")\r parser.add_argument(\"--opt-split-attention-v1\", action='store_true', help=\"enable older version of --opt-split-attention optimization\")\r parser.add_argument(\"--listen\", action='store_true', help=\"launch gradio with 0.0.0.0 as server name, allowing to respond to network requests\")\r parser.add_argument(\"--port\", type=int, help=\"launch gradio with given server port, you need root\/admin rights for ports < 1024, defaults to 7860 if available\", default=None)\r parser.add_argument(\"--show-negative-prompt\", action='store_true', help=\"does not do anything\", default=False)\r parser.add_argument(\"--ui-config-file\", type=str, help=\"filename to use for ui configuration\", default=os.path.join(script_path, 'ui-config.json'))\r \r cmd_opts=parser.parse_args()\r \r device=get_optimal_device()\r \r batch_cond_uncond=cmd_opts.always_batch_cond_uncond or not(cmd_opts.lowvram or cmd_opts.medvram)\r parallel_processing_allowed=not cmd_opts.lowvram and not cmd_opts.medvram\r \r \r class State:\r interrupted=False\r job=\"\"\r job_no=0\r job_count=0\r sampling_step=0\r sampling_steps=0\r current_latent=None\r current_image=None\r current_image_sampling_step=0\r \r def interrupt(self):\r self.interrupted=True\r \r def nextjob(self):\r self.job_no +=1\r self.sampling_step=0\r self.current_image_sampling_step=0\r \r \r state=State()\r \r artist_db=modules.artists.ArtistsDatabase(os.path.join(script_path, 'artists.csv'))\r \r styles_filename=os.path.join(script_path, 'styles.csv')\r prompt_styles=modules.styles.load_styles(styles_filename)\r \r interrogator=modules.interrogate.InterrogateModels(\"interrogate\")\r \r face_restorers=[]\r \r class Options:\r class OptionInfo:\r def __init__(self, default=None, label=\"\", component=None, component_args=None):\r self.default=default\r self.label=label\r self.component=component\r self.component_args=component_args\r \r data=None\r data_labels={\r \"outdir_samples\": OptionInfo(\"\", \"Output directory for images; if empty, defaults to two directories below\"),\r \"outdir_txt2img_samples\": OptionInfo(\"outputs\/txt2img-images\", 'Output directory for txt2img images'),\r \"outdir_img2img_samples\": OptionInfo(\"outputs\/img2img-images\", 'Output directory for img2img images'),\r \"outdir_extras_samples\": OptionInfo(\"outputs\/extras-images\", 'Output directory for images from extras tab'),\r \"outdir_grids\": OptionInfo(\"\", \"Output directory for grids; if empty, defaults to two directories below\"),\r \"outdir_txt2img_grids\": OptionInfo(\"outputs\/txt2img-grids\", 'Output directory for txt2img grids'),\r \"outdir_img2img_grids\": OptionInfo(\"outputs\/img2img-grids\", 'Output directory for img2img grids'),\r \"save_to_dirs\": OptionInfo(False, \"When writing images, create a directory with name derived from the prompt\"),\r \"grid_save_to_dirs\": OptionInfo(False, \"When writing grids, create a directory with name derived from the prompt\"),\r \"save_to_dirs_prompt_len\": OptionInfo(10, \"When using above, how many words from prompt to put into directory name\", gr.Slider,{\"minimum\": 1, \"maximum\": 32, \"step\": 1}),\r \"outdir_save\": OptionInfo(\"log\/images\", \"Directory for saving images using the Save button\"),\r \"samples_save\": OptionInfo(True, \"Save indiviual samples\"),\r \"samples_format\": OptionInfo('png', 'File format for individual samples'),\r \"grid_save\": OptionInfo(True, \"Save image grids\"),\r \"return_grid\": OptionInfo(True, \"Show grid in results for web\"),\r \"grid_format\": OptionInfo('png', 'File format for grids'),\r \"grid_extended_filename\": OptionInfo(False, \"Add extended info(seed, prompt) to filename when saving grid\"),\r \"grid_only_if_multiple\": OptionInfo(True, \"Do not save grids consisting of one picture\"),\r \"n_rows\": OptionInfo(-1, \"Grid row count; use -1 for autodetect and 0 for it to be same as batch size\", gr.Slider,{\"minimum\": -1, \"maximum\": 16, \"step\": 1}),\r \"jpeg_quality\": OptionInfo(80, \"Quality for saved jpeg images\", gr.Slider,{\"minimum\": 1, \"maximum\": 100, \"step\": 1}),\r \"export_for_4chan\": OptionInfo(True, \"If PNG image is larger than 4MB or any dimension is larger than 4000, downscale and save copy as JPG\"),\r \"enable_pnginfo\": OptionInfo(True, \"Save text information about generation parameters as chunks to png files\"),\r \"font\": OptionInfo(\"\", \"Font for image grids that have text\"),\r \"enable_emphasis\": OptionInfo(True, \"Use(text) to make model pay more attention to text text and[text] to make it pay less attention\"),\r \"save_txt\": OptionInfo(False, \"Create a text file next to every image with generation parameters.\"),\r \"ESRGAN_tile\": OptionInfo(192, \"Tile size for upscaling. 0=no tiling.\", gr.Slider,{\"minimum\": 0, \"maximum\": 512, \"step\": 16}),\r \"ESRGAN_tile_overlap\": OptionInfo(8, \"Tile overlap, in pixels for upscaling. Low values=visible seam.\", gr.Slider,{\"minimum\": 0, \"maximum\": 48, \"step\": 1}),\r \"random_artist_categories\": OptionInfo([], \"Allowed categories for random artists selection when using the Roll button\", gr.CheckboxGroup,{\"choices\": artist_db.categories()}),\r \"upscale_at_full_resolution_padding\": OptionInfo(16, \"Inpainting at full resolution: padding, in pixels, for the masked region.\", gr.Slider,{\"minimum\": 0, \"maximum\": 128, \"step\": 4}),\r \"show_progressbar\": OptionInfo(True, \"Show progressbar\"),\r \"show_progress_every_n_steps\": OptionInfo(0, \"Show show image creation progress every N sampling steps. Set 0 to disable.\", gr.Slider,{\"minimum\": 0, \"maximum\": 32, \"step\": 1}),\r \"multiple_tqdm\": OptionInfo(True, \"Add a second progress bar to the console that shows progress for an entire job. Broken in PyCharm console.\"),\r \"face_restoration_model\": OptionInfo(None, \"Face restoration model\", gr.Radio, lambda:{\"choices\":[x.name() for x in face_restorers]}),\r \"code_former_weight\": OptionInfo(0.5, \"CodeFormer weight parameter; 0=maximum effect; 1=minimum effect\", gr.Slider,{\"minimum\": 0, \"maximum\": 1, \"step\": 0.01}),\r \"interrogate_keep_models_in_memory\": OptionInfo(True, \"Interrogate: keep models in VRAM\"),\r \"interrogate_use_builtin_artists\": OptionInfo(True, \"Interrogate: use artists from artists.csv\"),\r \"interrogate_clip_num_beams\": OptionInfo(1, \"Interrogate: num_beams for BLIP\", gr.Slider,{\"minimum\": 1, \"maximum\": 16, \"step\": 1}),\r \"interrogate_clip_min_length\": OptionInfo(24, \"Interrogate: minimum descripton length(excluding artists, etc..)\", gr.Slider,{\"minimum\": 1, \"maximum\": 128, \"step\": 1}),\r \"interrogate_clip_max_length\": OptionInfo(48, \"Interrogate: maximum descripton length\", gr.Slider,{\"minimum\": 1, \"maximum\": 256, \"step\": 1}),\r }\r \r def __init__(self):\r self.data={k: v.default for k, v in self.data_labels.items()}\r \r def __setattr__(self, key, value):\r if self.data is not None:\r if key in self.data:\r self.data[key]=value\r \r return super(Options, self).__setattr__(key, value)\r \r def __getattr__(self, item):\r if self.data is not None:\r if item in self.data:\r return self.data[item]\r \r if item in self.data_labels:\r return self.data_labels[item].default\r \r return super(Options, self).__getattribute__(item)\r \r def save(self, filename):\r with open(filename, \"w\", encoding=\"utf8\") as file:\r json.dump(self.data, file)\r \r def load(self, filename):\r with open(filename, \"r\", encoding=\"utf8\") as file:\r self.data=json.load(file)\r \r \r opts=Options()\r if os.path.exists(config_filename):\r opts.load(config_filename)\r \r sd_upscalers=[]\r \r sd_model=None\r \r progress_print_out=sys.stdout\r \r \r class TotalTQDM:\r def __init__(self):\r self._tqdm=None\r \r def reset(self):\r self._tqdm=tqdm.tqdm(\r desc=\"Total progress\",\r total=state.job_count * state.sampling_steps,\r position=1,\r file=progress_print_out\r )\r \r def update(self):\r if not opts.multiple_tqdm:\r return\r if self._tqdm is None:\r self.reset()\r self._tqdm.update()\r \r def clear(self):\r if self._tqdm is not None:\r self._tqdm.close()\r self._tqdm=None\r \r \r total_tqdm=TotalTQDM()\r ","sourceWithComments":"import sys\r\nimport argparse\r\nimport json\r\nimport os\r\n\r\nimport gradio as gr\r\nimport torch\r\nimport tqdm\r\n\r\nimport modules.artists\r\nfrom modules.paths import script_path, sd_path\r\nfrom modules.devices import get_optimal_device\r\nimport modules.styles\r\nimport modules.interrogate\r\n\r\nconfig_filename = \"config.json\"\r\n\r\nsd_model_file = os.path.join(script_path, 'model.ckpt')\r\nif not os.path.exists(sd_model_file):\r\n    sd_model_file = \"models\/ldm\/stable-diffusion-v1\/model.ckpt\"\r\n\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument(\"--config\", type=str, default=os.path.join(sd_path, \"configs\/stable-diffusion\/v1-inference.yaml\"), help=\"path to config which constructs model\",)\r\nparser.add_argument(\"--ckpt\", type=str, default=os.path.join(sd_path, sd_model_file), help=\"path to checkpoint of model\",)\r\nparser.add_argument(\"--gfpgan-dir\", type=str, help=\"GFPGAN directory\", default=('.\/src\/gfpgan' if os.path.exists('.\/src\/gfpgan') else '.\/GFPGAN'))\r\nparser.add_argument(\"--gfpgan-model\", type=str, help=\"GFPGAN model file name\", default='GFPGANv1.3.pth')\r\nparser.add_argument(\"--no-half\", action='store_true', help=\"do not switch the model to 16-bit floats\")\r\nparser.add_argument(\"--no-progressbar-hiding\", action='store_true', help=\"do not hide progressbar in gradio UI (we hide it because it slows down ML if you have hardware accleration in browser)\")\r\nparser.add_argument(\"--max-batch-count\", type=int, default=16, help=\"maximum batch count value for the UI\")\r\nparser.add_argument(\"--embeddings-dir\", type=str, default='embeddings', help=\"embeddings directory for textual inversion (default: embeddings)\")\r\nparser.add_argument(\"--allow-code\", action='store_true', help=\"allow custom script execution from webui\")\r\nparser.add_argument(\"--medvram\", action='store_true', help=\"enable stable diffusion model optimizations for sacrificing a little speed for low VRM usage\")\r\nparser.add_argument(\"--lowvram\", action='store_true', help=\"enable stable diffusion model optimizations for sacrificing a lot of speed for very low VRM usage\")\r\nparser.add_argument(\"--always-batch-cond-uncond\", action='store_true', help=\"a workaround test; may help with speed if you use --lowvram\")\r\nparser.add_argument(\"--unload-gfpgan\", action='store_true', help=\"unload GFPGAN every time after processing images. Warning: seems to cause memory leaks\")\r\nparser.add_argument(\"--precision\", type=str, help=\"evaluate at this precision\", choices=[\"full\", \"autocast\"], default=\"autocast\")\r\nparser.add_argument(\"--share\", action='store_true', help=\"use share=True for gradio and make the UI accessible through their site (doesn't work for me but you might have better luck)\")\r\nparser.add_argument(\"--esrgan-models-path\", type=str, help=\"path to directory with ESRGAN models\", default=os.path.join(script_path, 'ESRGAN'))\r\nparser.add_argument(\"--opt-split-attention\", action='store_true', help=\"enable optimization that reduce vram usage by a lot for about 10%% decrease in performance\")\r\nparser.add_argument(\"--opt-split-attention-v1\", action='store_true', help=\"enable older version of --opt-split-attention optimization\")\r\nparser.add_argument(\"--listen\", action='store_true', help=\"launch gradio with 0.0.0.0 as server name, allowing to respond to network requests\")\r\nparser.add_argument(\"--port\", type=int, help=\"launch gradio with given server port, you need root\/admin rights for ports < 1024, defaults to 7860 if available\", default=None)\r\nparser.add_argument(\"--show-negative-prompt\", action='store_true', help=\"does not do anything\", default=False)\r\nparser.add_argument(\"--ui-config-file\", type=str, help=\"filename to use for ui configuration\", default=os.path.join(script_path, 'ui-config.json'))\r\n\r\ncmd_opts = parser.parse_args()\r\n\r\ndevice = get_optimal_device()\r\n\r\nbatch_cond_uncond = cmd_opts.always_batch_cond_uncond or not (cmd_opts.lowvram or cmd_opts.medvram)\r\nparallel_processing_allowed = not cmd_opts.lowvram and not cmd_opts.medvram\r\n\r\n\r\nclass State:\r\n    interrupted = False\r\n    job = \"\"\r\n    job_no = 0\r\n    job_count = 0\r\n    sampling_step = 0\r\n    sampling_steps = 0\r\n    current_latent = None\r\n    current_image = None\r\n    current_image_sampling_step = 0\r\n\r\n    def interrupt(self):\r\n        self.interrupted = True\r\n\r\n    def nextjob(self):\r\n        self.job_no += 1\r\n        self.sampling_step = 0\r\n        self.current_image_sampling_step = 0\r\n\r\n\r\nstate = State()\r\n\r\nartist_db = modules.artists.ArtistsDatabase(os.path.join(script_path, 'artists.csv'))\r\n\r\nstyles_filename = os.path.join(script_path, 'styles.csv')\r\nprompt_styles = modules.styles.load_styles(styles_filename)\r\n\r\ninterrogator = modules.interrogate.InterrogateModels(\"interrogate\")\r\n\r\nface_restorers = []\r\n\r\nclass Options:\r\n    class OptionInfo:\r\n        def __init__(self, default=None, label=\"\", component=None, component_args=None):\r\n            self.default = default\r\n            self.label = label\r\n            self.component = component\r\n            self.component_args = component_args\r\n\r\n    data = None\r\n    data_labels = {\r\n        \"outdir_samples\": OptionInfo(\"\", \"Output directory for images; if empty, defaults to two directories below\"),\r\n        \"outdir_txt2img_samples\": OptionInfo(\"outputs\/txt2img-images\", 'Output directory for txt2img images'),\r\n        \"outdir_img2img_samples\": OptionInfo(\"outputs\/img2img-images\", 'Output directory for img2img images'),\r\n        \"outdir_extras_samples\": OptionInfo(\"outputs\/extras-images\", 'Output directory for images from extras tab'),\r\n        \"outdir_grids\": OptionInfo(\"\", \"Output directory for grids; if empty, defaults to two directories below\"),\r\n        \"outdir_txt2img_grids\": OptionInfo(\"outputs\/txt2img-grids\", 'Output directory for txt2img grids'),\r\n        \"outdir_img2img_grids\": OptionInfo(\"outputs\/img2img-grids\", 'Output directory for img2img grids'),\r\n        \"save_to_dirs\": OptionInfo(False, \"When writing images, create a directory with name derived from the prompt\"),\r\n        \"grid_save_to_dirs\": OptionInfo(False, \"When writing grids, create a directory with name derived from the prompt\"),\r\n        \"save_to_dirs_prompt_len\": OptionInfo(10, \"When using above, how many words from prompt to put into directory name\", gr.Slider, {\"minimum\": 1, \"maximum\": 32, \"step\": 1}),\r\n        \"outdir_save\": OptionInfo(\"log\/images\", \"Directory for saving images using the Save button\"),\r\n        \"samples_save\": OptionInfo(True, \"Save indiviual samples\"),\r\n        \"samples_format\": OptionInfo('png', 'File format for individual samples'),\r\n        \"grid_save\": OptionInfo(True, \"Save image grids\"),\r\n        \"return_grid\": OptionInfo(True, \"Show grid in results for web\"),\r\n        \"grid_format\": OptionInfo('png', 'File format for grids'),\r\n        \"grid_extended_filename\": OptionInfo(False, \"Add extended info (seed, prompt) to filename when saving grid\"),\r\n        \"grid_only_if_multiple\": OptionInfo(True, \"Do not save grids consisting of one picture\"),\r\n        \"n_rows\": OptionInfo(-1, \"Grid row count; use -1 for autodetect and 0 for it to be same as batch size\", gr.Slider, {\"minimum\": -1, \"maximum\": 16, \"step\": 1}),\r\n        \"jpeg_quality\": OptionInfo(80, \"Quality for saved jpeg images\", gr.Slider, {\"minimum\": 1, \"maximum\": 100, \"step\": 1}),\r\n        \"export_for_4chan\": OptionInfo(True, \"If PNG image is larger than 4MB or any dimension is larger than 4000, downscale and save copy as JPG\"),\r\n        \"enable_pnginfo\": OptionInfo(True, \"Save text information about generation parameters as chunks to png files\"),\r\n        \"font\": OptionInfo(\"\", \"Font for image grids that have text\"),\r\n        \"enable_emphasis\": OptionInfo(True, \"Use (text) to make model pay more attention to text text and [text] to make it pay less attention\"),\r\n        \"save_txt\": OptionInfo(False, \"Create a text file next to every image with generation parameters.\"),\r\n        \"ESRGAN_tile\": OptionInfo(192, \"Tile size for upscaling. 0 = no tiling.\", gr.Slider, {\"minimum\": 0, \"maximum\": 512, \"step\": 16}),\r\n        \"ESRGAN_tile_overlap\": OptionInfo(8, \"Tile overlap, in pixels for upscaling. Low values = visible seam.\", gr.Slider, {\"minimum\": 0, \"maximum\": 48, \"step\": 1}),\r\n        \"random_artist_categories\": OptionInfo([], \"Allowed categories for random artists selection when using the Roll button\", gr.CheckboxGroup, {\"choices\": artist_db.categories()}),\r\n        \"upscale_at_full_resolution_padding\": OptionInfo(16, \"Inpainting at full resolution: padding, in pixels, for the masked region.\", gr.Slider, {\"minimum\": 0, \"maximum\": 128, \"step\": 4}),\r\n        \"show_progressbar\": OptionInfo(True, \"Show progressbar\"),\r\n        \"show_progress_every_n_steps\": OptionInfo(0, \"Show show image creation progress every N sampling steps. Set 0 to disable.\", gr.Slider, {\"minimum\": 0, \"maximum\": 32, \"step\": 1}),\r\n        \"multiple_tqdm\": OptionInfo(True, \"Add a second progress bar to the console that shows progress for an entire job. Broken in PyCharm console.\"),\r\n        \"face_restoration_model\": OptionInfo(None, \"Face restoration model\", gr.Radio, lambda: {\"choices\": [x.name() for x in face_restorers]}),\r\n        \"code_former_weight\": OptionInfo(0.5, \"CodeFormer weight parameter; 0 = maximum effect; 1 = minimum effect\", gr.Slider, {\"minimum\": 0, \"maximum\": 1, \"step\": 0.01}),\r\n        \"interrogate_keep_models_in_memory\": OptionInfo(True, \"Interrogate: keep models in VRAM\"),\r\n        \"interrogate_use_builtin_artists\": OptionInfo(True, \"Interrogate: use artists from artists.csv\"),\r\n        \"interrogate_clip_num_beams\": OptionInfo(1, \"Interrogate: num_beams for BLIP\", gr.Slider, {\"minimum\": 1, \"maximum\": 16, \"step\": 1}),\r\n        \"interrogate_clip_min_length\": OptionInfo(24, \"Interrogate: minimum descripton length (excluding artists, etc..)\", gr.Slider, {\"minimum\": 1, \"maximum\": 128, \"step\": 1}),\r\n        \"interrogate_clip_max_length\": OptionInfo(48, \"Interrogate: maximum descripton length\", gr.Slider, {\"minimum\": 1, \"maximum\": 256, \"step\": 1}),\r\n    }\r\n\r\n    def __init__(self):\r\n        self.data = {k: v.default for k, v in self.data_labels.items()}\r\n\r\n    def __setattr__(self, key, value):\r\n        if self.data is not None:\r\n            if key in self.data:\r\n                self.data[key] = value\r\n\r\n        return super(Options, self).__setattr__(key, value)\r\n\r\n    def __getattr__(self, item):\r\n        if self.data is not None:\r\n            if item in self.data:\r\n                return self.data[item]\r\n\r\n        if item in self.data_labels:\r\n            return self.data_labels[item].default\r\n\r\n        return super(Options, self).__getattribute__(item)\r\n\r\n    def save(self, filename):\r\n        with open(filename, \"w\", encoding=\"utf8\") as file:\r\n            json.dump(self.data, file)\r\n\r\n    def load(self, filename):\r\n        with open(filename, \"r\", encoding=\"utf8\") as file:\r\n            self.data = json.load(file)\r\n\r\n\r\nopts = Options()\r\nif os.path.exists(config_filename):\r\n    opts.load(config_filename)\r\n\r\nsd_upscalers = []\r\n\r\nsd_model = None\r\n\r\nprogress_print_out = sys.stdout\r\n\r\n\r\nclass TotalTQDM:\r\n    def __init__(self):\r\n        self._tqdm = None\r\n\r\n    def reset(self):\r\n        self._tqdm = tqdm.tqdm(\r\n            desc=\"Total progress\",\r\n            total=state.job_count * state.sampling_steps,\r\n            position=1,\r\n            file=progress_print_out\r\n        )\r\n\r\n    def update(self):\r\n        if not opts.multiple_tqdm:\r\n            return\r\n        if self._tqdm is None:\r\n            self.reset()\r\n        self._tqdm.update()\r\n\r\n    def clear(self):\r\n        if self._tqdm is not None:\r\n            self._tqdm.close()\r\n            self._tqdm = None\r\n\r\n\r\ntotal_tqdm = TotalTQDM()\r\n"},"\/modules\/ui.py":{"changes":[{"diff":"\n         info = opts.data_labels[key]\r\n         t = type(info.default)\r\n \r\n+        args = info.component_args() if callable(info.component_args) else info.component_args\r\n+\r\n         if info.component is not None:\r\n-            args = info.component_args() if callable(info.component_args) else info.component_args\r\n-            item = info.component(label=info.label, value=fun, **(args or {}))\r\n+            comp = info.component\r\n         elif t == str:\r\n-            item = gr.Textbox(label=info.label, value=fun, lines=1)\r\n+            comp = gr.Textbox\r\n         elif t == int:\r\n-            item = gr.Number(label=info.label, value=fun)\r\n+            comp = gr.Number\r\n         elif t == bool:\r\n-            item = gr.Checkbox(label=info.label, value=fun)\r\n+            comp = gr.Checkbox\r\n         else:\r\n             raise Exception(f'bad options item type: {str(t)} for key {key}')\r\n \r\n-        return item\r\n+        return comp(label=info.label, value=fun, **(args or {}))\r\n \r\n     components = []\r\n     keys = list(opts.data_labels.keys())\r\n","add":7,"remove":6,"filename":"\/modules\/ui.py","badparts":["            args = info.component_args() if callable(info.component_args) else info.component_args\r","            item = info.component(label=info.label, value=fun, **(args or {}))\r","            item = gr.Textbox(label=info.label, value=fun, lines=1)\r","            item = gr.Number(label=info.label, value=fun)\r","            item = gr.Checkbox(label=info.label, value=fun)\r","        return item\r"],"goodparts":["        args = info.component_args() if callable(info.component_args) else info.component_args\r","\r","            comp = info.component\r","            comp = gr.Textbox\r","            comp = gr.Number\r","            comp = gr.Checkbox\r","        return comp(label=info.label, value=fun, **(args or {}))\r"]}]}},"msg":"Add --hide-ui-dir-config command line flag\n\nAdds `--hide-ui-dir-config` flag to disable editing directory configs from the web UI. This can be set to prevent users from setting the directory to somewhere they shouldn't, for public (or semi-public) interfaces.\n\nDirectories are still read from config.json, so the server admin can still set them in the web UI and then relaunch with the hide flag, or edit the config manually.\n\nAlso:\n- fix OptionInfo `component_args` keyword argument not being read if `component` isn't also set\n- ensure that hidden settings aren't still read from the web UI (otherwise they could still be changed by tampering with the interface)"}},"https:\/\/github.com\/mistyisdead\/https-github.com-sddebz-stable-diffusion-krita-plugin":{"29a2933e23e68900bbae741a98439d0c9d6f26f6":{"url":"https:\/\/api.github.com\/repos\/mistyisdead\/https-github.com-sddebz-stable-diffusion-krita-plugin\/commits\/29a2933e23e68900bbae741a98439d0c9d6f26f6","html_url":"https:\/\/github.com\/mistyisdead\/https-github.com-sddebz-stable-diffusion-krita-plugin\/commit\/29a2933e23e68900bbae741a98439d0c9d6f26f6","message":"Add --hide-ui-dir-config command line flag\n\nAdds `--hide-ui-dir-config` flag to disable editing directory configs from the web UI. This can be set to prevent users from setting the directory to somewhere they shouldn't, for public (or semi-public) interfaces.\n\nDirectories are still read from config.json, so the server admin can still set them in the web UI and then relaunch with the hide flag, or edit the config manually.\n\nAlso:\n- fix OptionInfo `component_args` keyword argument not being read if `component` isn't also set\n- ensure that hidden settings aren't still read from the web UI (otherwise they could still be changed by tampering with the interface)","sha":"29a2933e23e68900bbae741a98439d0c9d6f26f6","keyword":"tampering prevent","diff":"diff --git a\/modules\/shared.py b\/modules\/shared.py\nindex 9eeb64e..0557cfe 100644\n--- a\/modules\/shared.py\n+++ b\/modules\/shared.py\n@@ -42,6 +42,7 @@\n parser.add_argument(\"--port\", type=int, help=\"launch gradio with given server port, you need root\/admin rights for ports < 1024, defaults to 7860 if available\", default=None)\r\n parser.add_argument(\"--show-negative-prompt\", action='store_true', help=\"does not do anything\", default=False)\r\n parser.add_argument(\"--ui-config-file\", type=str, help=\"filename to use for ui configuration\", default=os.path.join(script_path, 'ui-config.json'))\r\n+parser.add_argument(\"--hide-ui-dir-config\", action='store_true', help=\"hide directory configuration from webui\", default=False)\r\n \r\n cmd_opts = parser.parse_args()\r\n \r\n@@ -91,18 +92,19 @@ def __init__(self, default=None, label=\"\", component=None, component_args=None):\n             self.component_args = component_args\r\n \r\n     data = None\r\n+    hide_dirs = {\"visible\": False} if cmd_opts.hide_ui_dir_config else None\r\n     data_labels = {\r\n-        \"outdir_samples\": OptionInfo(\"\", \"Output directory for images; if empty, defaults to two directories below\"),\r\n-        \"outdir_txt2img_samples\": OptionInfo(\"outputs\/txt2img-images\", 'Output directory for txt2img images'),\r\n-        \"outdir_img2img_samples\": OptionInfo(\"outputs\/img2img-images\", 'Output directory for img2img images'),\r\n-        \"outdir_extras_samples\": OptionInfo(\"outputs\/extras-images\", 'Output directory for images from extras tab'),\r\n-        \"outdir_grids\": OptionInfo(\"\", \"Output directory for grids; if empty, defaults to two directories below\"),\r\n-        \"outdir_txt2img_grids\": OptionInfo(\"outputs\/txt2img-grids\", 'Output directory for txt2img grids'),\r\n-        \"outdir_img2img_grids\": OptionInfo(\"outputs\/img2img-grids\", 'Output directory for img2img grids'),\r\n+        \"outdir_samples\": OptionInfo(\"\", \"Output directory for images; if empty, defaults to two directories below\", component_args=hide_dirs),\r\n+        \"outdir_txt2img_samples\": OptionInfo(\"outputs\/txt2img-images\", 'Output directory for txt2img images', component_args=hide_dirs),\r\n+        \"outdir_img2img_samples\": OptionInfo(\"outputs\/img2img-images\", 'Output directory for img2img images', component_args=hide_dirs),\r\n+        \"outdir_extras_samples\": OptionInfo(\"outputs\/extras-images\", 'Output directory for images from extras tab', component_args=hide_dirs),\r\n+        \"outdir_grids\": OptionInfo(\"\", \"Output directory for grids; if empty, defaults to two directories below\", component_args=hide_dirs),\r\n+        \"outdir_txt2img_grids\": OptionInfo(\"outputs\/txt2img-grids\", 'Output directory for txt2img grids', component_args=hide_dirs),\r\n+        \"outdir_img2img_grids\": OptionInfo(\"outputs\/img2img-grids\", 'Output directory for img2img grids', component_args=hide_dirs),\r\n         \"save_to_dirs\": OptionInfo(False, \"When writing images, create a directory with name derived from the prompt\"),\r\n         \"grid_save_to_dirs\": OptionInfo(False, \"When writing grids, create a directory with name derived from the prompt\"),\r\n         \"save_to_dirs_prompt_len\": OptionInfo(10, \"When using above, how many words from prompt to put into directory name\", gr.Slider, {\"minimum\": 1, \"maximum\": 32, \"step\": 1}),\r\n-        \"outdir_save\": OptionInfo(\"log\/images\", \"Directory for saving images using the Save button\"),\r\n+        \"outdir_save\": OptionInfo(\"log\/images\", \"Directory for saving images using the Save button\", component_args=hide_dirs),\r\n         \"samples_save\": OptionInfo(True, \"Save indiviual samples\"),\r\n         \"samples_format\": OptionInfo('png', 'File format for individual samples'),\r\n         \"grid_save\": OptionInfo(True, \"Save image grids\"),\r\ndiff --git a\/modules\/ui.py b\/modules\/ui.py\nindex b9af2c8..3b7eb9b 100644\n--- a\/modules\/ui.py\n+++ b\/modules\/ui.py\n@@ -661,19 +661,20 @@ def fun():\n         info = opts.data_labels[key]\r\n         t = type(info.default)\r\n \r\n+        args = info.component_args() if callable(info.component_args) else info.component_args\r\n+\r\n         if info.component is not None:\r\n-            args = info.component_args() if callable(info.component_args) else info.component_args\r\n-            item = info.component(label=info.label, value=fun, **(args or {}))\r\n+            comp = info.component\r\n         elif t == str:\r\n-            item = gr.Textbox(label=info.label, value=fun, lines=1)\r\n+            comp = gr.Textbox\r\n         elif t == int:\r\n-            item = gr.Number(label=info.label, value=fun)\r\n+            comp = gr.Number\r\n         elif t == bool:\r\n-            item = gr.Checkbox(label=info.label, value=fun)\r\n+            comp = gr.Checkbox\r\n         else:\r\n             raise Exception(f'bad options item type: {str(t)} for key {key}')\r\n \r\n-        return item\r\n+        return comp(label=info.label, value=fun, **(args or {}))\r\n \r\n     components = []\r\n     keys = list(opts.data_labels.keys())\r\n@@ -684,6 +685,10 @@ def run_settings(*args):\n         up = []\r\n \r\n         for key, value, comp in zip(opts.data_labels.keys(), args, components):\r\n+            comp_args = opts.data_labels[key].component_args\r\n+            if comp_args and isinstance(comp_args, dict) and comp_args.get('visible') is False:\r\n+                continue\r\n+\r\n             opts.data[key] = value\r\n             up.append(comp.update(value=value))\r\n \r\n","files":{"\/modules\/shared.py":{"changes":[{"diff":"\n             self.component_args = component_args\r\n \r\n     data = None\r\n+    hide_dirs = {\"visible\": False} if cmd_opts.hide_ui_dir_config else None\r\n     data_labels = {\r\n-        \"outdir_samples\": OptionInfo(\"\", \"Output directory for images; if empty, defaults to two directories below\"),\r\n-        \"outdir_txt2img_samples\": OptionInfo(\"outputs\/txt2img-images\", 'Output directory for txt2img images'),\r\n-        \"outdir_img2img_samples\": OptionInfo(\"outputs\/img2img-images\", 'Output directory for img2img images'),\r\n-        \"outdir_extras_samples\": OptionInfo(\"outputs\/extras-images\", 'Output directory for images from extras tab'),\r\n-        \"outdir_grids\": OptionInfo(\"\", \"Output directory for grids; if empty, defaults to two directories below\"),\r\n-        \"outdir_txt2img_grids\": OptionInfo(\"outputs\/txt2img-grids\", 'Output directory for txt2img grids'),\r\n-        \"outdir_img2img_grids\": OptionInfo(\"outputs\/img2img-grids\", 'Output directory for img2img grids'),\r\n+        \"outdir_samples\": OptionInfo(\"\", \"Output directory for images; if empty, defaults to two directories below\", component_args=hide_dirs),\r\n+        \"outdir_txt2img_samples\": OptionInfo(\"outputs\/txt2img-images\", 'Output directory for txt2img images', component_args=hide_dirs),\r\n+        \"outdir_img2img_samples\": OptionInfo(\"outputs\/img2img-images\", 'Output directory for img2img images', component_args=hide_dirs),\r\n+        \"outdir_extras_samples\": OptionInfo(\"outputs\/extras-images\", 'Output directory for images from extras tab', component_args=hide_dirs),\r\n+        \"outdir_grids\": OptionInfo(\"\", \"Output directory for grids; if empty, defaults to two directories below\", component_args=hide_dirs),\r\n+        \"outdir_txt2img_grids\": OptionInfo(\"outputs\/txt2img-grids\", 'Output directory for txt2img grids', component_args=hide_dirs),\r\n+        \"outdir_img2img_grids\": OptionInfo(\"outputs\/img2img-grids\", 'Output directory for img2img grids', component_args=hide_dirs),\r\n         \"save_to_dirs\": OptionInfo(False, \"When writing images, create a directory with name derived from the prompt\"),\r\n         \"grid_save_to_dirs\": OptionInfo(False, \"When writing grids, create a directory with name derived from the prompt\"),\r\n         \"save_to_dirs_prompt_len\": OptionInfo(10, \"When using above, how many words from prompt to put into directory name\", gr.Slider, {\"minimum\": 1, \"maximum\": 32, \"step\": 1}),\r\n-        \"outdir_save\": OptionInfo(\"log\/images\", \"Directory for saving images using the Save button\"),\r\n+        \"outdir_save\": OptionInfo(\"log\/images\", \"Directory for saving images using the Save button\", component_args=hide_dirs),\r\n         \"samples_save\": OptionInfo(True, \"Save indiviual samples\"),\r\n         \"samples_format\": OptionInfo('png', 'File format for individual samples'),\r\n         \"grid_save\": OptionInfo(True, \"Save image grids\"),\r","add":9,"remove":8,"filename":"\/modules\/shared.py","badparts":["        \"outdir_samples\": OptionInfo(\"\", \"Output directory for images; if empty, defaults to two directories below\"),\r","        \"outdir_txt2img_samples\": OptionInfo(\"outputs\/txt2img-images\", 'Output directory for txt2img images'),\r","        \"outdir_img2img_samples\": OptionInfo(\"outputs\/img2img-images\", 'Output directory for img2img images'),\r","        \"outdir_extras_samples\": OptionInfo(\"outputs\/extras-images\", 'Output directory for images from extras tab'),\r","        \"outdir_grids\": OptionInfo(\"\", \"Output directory for grids; if empty, defaults to two directories below\"),\r","        \"outdir_txt2img_grids\": OptionInfo(\"outputs\/txt2img-grids\", 'Output directory for txt2img grids'),\r","        \"outdir_img2img_grids\": OptionInfo(\"outputs\/img2img-grids\", 'Output directory for img2img grids'),\r","        \"outdir_save\": OptionInfo(\"log\/images\", \"Directory for saving images using the Save button\"),\r"],"goodparts":["    hide_dirs = {\"visible\": False} if cmd_opts.hide_ui_dir_config else None\r","        \"outdir_samples\": OptionInfo(\"\", \"Output directory for images; if empty, defaults to two directories below\", component_args=hide_dirs),\r","        \"outdir_txt2img_samples\": OptionInfo(\"outputs\/txt2img-images\", 'Output directory for txt2img images', component_args=hide_dirs),\r","        \"outdir_img2img_samples\": OptionInfo(\"outputs\/img2img-images\", 'Output directory for img2img images', component_args=hide_dirs),\r","        \"outdir_extras_samples\": OptionInfo(\"outputs\/extras-images\", 'Output directory for images from extras tab', component_args=hide_dirs),\r","        \"outdir_grids\": OptionInfo(\"\", \"Output directory for grids; if empty, defaults to two directories below\", component_args=hide_dirs),\r","        \"outdir_txt2img_grids\": OptionInfo(\"outputs\/txt2img-grids\", 'Output directory for txt2img grids', component_args=hide_dirs),\r","        \"outdir_img2img_grids\": OptionInfo(\"outputs\/img2img-grids\", 'Output directory for img2img grids', component_args=hide_dirs),\r","        \"outdir_save\": OptionInfo(\"log\/images\", \"Directory for saving images using the Save button\", component_args=hide_dirs),\r"]}],"source":"\nimport sys\r import argparse\r import json\r import os\r \r import gradio as gr\r import torch\r import tqdm\r \r import modules.artists\r from modules.paths import script_path, sd_path\r from modules.devices import get_optimal_device\r import modules.styles\r import modules.interrogate\r \r config_filename=\"config.json\"\r \r sd_model_file=os.path.join(script_path, 'model.ckpt')\r if not os.path.exists(sd_model_file):\r sd_model_file=\"models\/ldm\/stable-diffusion-v1\/model.ckpt\"\r \r parser=argparse.ArgumentParser()\r parser.add_argument(\"--config\", type=str, default=os.path.join(sd_path, \"configs\/stable-diffusion\/v1-inference.yaml\"), help=\"path to config which constructs model\",)\r parser.add_argument(\"--ckpt\", type=str, default=os.path.join(sd_path, sd_model_file), help=\"path to checkpoint of model\",)\r parser.add_argument(\"--gfpgan-dir\", type=str, help=\"GFPGAN directory\", default=('.\/src\/gfpgan' if os.path.exists('.\/src\/gfpgan') else '.\/GFPGAN'))\r parser.add_argument(\"--gfpgan-model\", type=str, help=\"GFPGAN model file name\", default='GFPGANv1.3.pth')\r parser.add_argument(\"--no-half\", action='store_true', help=\"do not switch the model to 16-bit floats\")\r parser.add_argument(\"--no-progressbar-hiding\", action='store_true', help=\"do not hide progressbar in gradio UI(we hide it because it slows down ML if you have hardware accleration in browser)\")\r parser.add_argument(\"--max-batch-count\", type=int, default=16, help=\"maximum batch count value for the UI\")\r parser.add_argument(\"--embeddings-dir\", type=str, default='embeddings', help=\"embeddings directory for textual inversion(default: embeddings)\")\r parser.add_argument(\"--allow-code\", action='store_true', help=\"allow custom script execution from webui\")\r parser.add_argument(\"--medvram\", action='store_true', help=\"enable stable diffusion model optimizations for sacrificing a little speed for low VRM usage\")\r parser.add_argument(\"--lowvram\", action='store_true', help=\"enable stable diffusion model optimizations for sacrificing a lot of speed for very low VRM usage\")\r parser.add_argument(\"--always-batch-cond-uncond\", action='store_true', help=\"a workaround test; may help with speed if you use --lowvram\")\r parser.add_argument(\"--unload-gfpgan\", action='store_true', help=\"unload GFPGAN every time after processing images. Warning: seems to cause memory leaks\")\r parser.add_argument(\"--precision\", type=str, help=\"evaluate at this precision\", choices=[\"full\", \"autocast\"], default=\"autocast\")\r parser.add_argument(\"--share\", action='store_true', help=\"use share=True for gradio and make the UI accessible through their site(doesn't work for me but you might have better luck)\")\r parser.add_argument(\"--esrgan-models-path\", type=str, help=\"path to directory with ESRGAN models\", default=os.path.join(script_path, 'ESRGAN'))\r parser.add_argument(\"--opt-split-attention\", action='store_true', help=\"enable optimization that reduce vram usage by a lot for about 10%% decrease in performance\")\r parser.add_argument(\"--opt-split-attention-v1\", action='store_true', help=\"enable older version of --opt-split-attention optimization\")\r parser.add_argument(\"--listen\", action='store_true', help=\"launch gradio with 0.0.0.0 as server name, allowing to respond to network requests\")\r parser.add_argument(\"--port\", type=int, help=\"launch gradio with given server port, you need root\/admin rights for ports < 1024, defaults to 7860 if available\", default=None)\r parser.add_argument(\"--show-negative-prompt\", action='store_true', help=\"does not do anything\", default=False)\r parser.add_argument(\"--ui-config-file\", type=str, help=\"filename to use for ui configuration\", default=os.path.join(script_path, 'ui-config.json'))\r \r cmd_opts=parser.parse_args()\r \r device=get_optimal_device()\r \r batch_cond_uncond=cmd_opts.always_batch_cond_uncond or not(cmd_opts.lowvram or cmd_opts.medvram)\r parallel_processing_allowed=not cmd_opts.lowvram and not cmd_opts.medvram\r \r \r class State:\r interrupted=False\r job=\"\"\r job_no=0\r job_count=0\r sampling_step=0\r sampling_steps=0\r current_latent=None\r current_image=None\r current_image_sampling_step=0\r \r def interrupt(self):\r self.interrupted=True\r \r def nextjob(self):\r self.job_no +=1\r self.sampling_step=0\r self.current_image_sampling_step=0\r \r \r state=State()\r \r artist_db=modules.artists.ArtistsDatabase(os.path.join(script_path, 'artists.csv'))\r \r styles_filename=os.path.join(script_path, 'styles.csv')\r prompt_styles=modules.styles.load_styles(styles_filename)\r \r interrogator=modules.interrogate.InterrogateModels(\"interrogate\")\r \r face_restorers=[]\r \r class Options:\r class OptionInfo:\r def __init__(self, default=None, label=\"\", component=None, component_args=None):\r self.default=default\r self.label=label\r self.component=component\r self.component_args=component_args\r \r data=None\r data_labels={\r \"outdir_samples\": OptionInfo(\"\", \"Output directory for images; if empty, defaults to two directories below\"),\r \"outdir_txt2img_samples\": OptionInfo(\"outputs\/txt2img-images\", 'Output directory for txt2img images'),\r \"outdir_img2img_samples\": OptionInfo(\"outputs\/img2img-images\", 'Output directory for img2img images'),\r \"outdir_extras_samples\": OptionInfo(\"outputs\/extras-images\", 'Output directory for images from extras tab'),\r \"outdir_grids\": OptionInfo(\"\", \"Output directory for grids; if empty, defaults to two directories below\"),\r \"outdir_txt2img_grids\": OptionInfo(\"outputs\/txt2img-grids\", 'Output directory for txt2img grids'),\r \"outdir_img2img_grids\": OptionInfo(\"outputs\/img2img-grids\", 'Output directory for img2img grids'),\r \"save_to_dirs\": OptionInfo(False, \"When writing images, create a directory with name derived from the prompt\"),\r \"grid_save_to_dirs\": OptionInfo(False, \"When writing grids, create a directory with name derived from the prompt\"),\r \"save_to_dirs_prompt_len\": OptionInfo(10, \"When using above, how many words from prompt to put into directory name\", gr.Slider,{\"minimum\": 1, \"maximum\": 32, \"step\": 1}),\r \"outdir_save\": OptionInfo(\"log\/images\", \"Directory for saving images using the Save button\"),\r \"samples_save\": OptionInfo(True, \"Save indiviual samples\"),\r \"samples_format\": OptionInfo('png', 'File format for individual samples'),\r \"grid_save\": OptionInfo(True, \"Save image grids\"),\r \"return_grid\": OptionInfo(True, \"Show grid in results for web\"),\r \"grid_format\": OptionInfo('png', 'File format for grids'),\r \"grid_extended_filename\": OptionInfo(False, \"Add extended info(seed, prompt) to filename when saving grid\"),\r \"grid_only_if_multiple\": OptionInfo(True, \"Do not save grids consisting of one picture\"),\r \"n_rows\": OptionInfo(-1, \"Grid row count; use -1 for autodetect and 0 for it to be same as batch size\", gr.Slider,{\"minimum\": -1, \"maximum\": 16, \"step\": 1}),\r \"jpeg_quality\": OptionInfo(80, \"Quality for saved jpeg images\", gr.Slider,{\"minimum\": 1, \"maximum\": 100, \"step\": 1}),\r \"export_for_4chan\": OptionInfo(True, \"If PNG image is larger than 4MB or any dimension is larger than 4000, downscale and save copy as JPG\"),\r \"enable_pnginfo\": OptionInfo(True, \"Save text information about generation parameters as chunks to png files\"),\r \"font\": OptionInfo(\"\", \"Font for image grids that have text\"),\r \"enable_emphasis\": OptionInfo(True, \"Use(text) to make model pay more attention to text text and[text] to make it pay less attention\"),\r \"save_txt\": OptionInfo(False, \"Create a text file next to every image with generation parameters.\"),\r \"ESRGAN_tile\": OptionInfo(192, \"Tile size for upscaling. 0=no tiling.\", gr.Slider,{\"minimum\": 0, \"maximum\": 512, \"step\": 16}),\r \"ESRGAN_tile_overlap\": OptionInfo(8, \"Tile overlap, in pixels for upscaling. Low values=visible seam.\", gr.Slider,{\"minimum\": 0, \"maximum\": 48, \"step\": 1}),\r \"random_artist_categories\": OptionInfo([], \"Allowed categories for random artists selection when using the Roll button\", gr.CheckboxGroup,{\"choices\": artist_db.categories()}),\r \"upscale_at_full_resolution_padding\": OptionInfo(16, \"Inpainting at full resolution: padding, in pixels, for the masked region.\", gr.Slider,{\"minimum\": 0, \"maximum\": 128, \"step\": 4}),\r \"show_progressbar\": OptionInfo(True, \"Show progressbar\"),\r \"show_progress_every_n_steps\": OptionInfo(0, \"Show show image creation progress every N sampling steps. Set 0 to disable.\", gr.Slider,{\"minimum\": 0, \"maximum\": 32, \"step\": 1}),\r \"multiple_tqdm\": OptionInfo(True, \"Add a second progress bar to the console that shows progress for an entire job. Broken in PyCharm console.\"),\r \"face_restoration_model\": OptionInfo(None, \"Face restoration model\", gr.Radio, lambda:{\"choices\":[x.name() for x in face_restorers]}),\r \"code_former_weight\": OptionInfo(0.5, \"CodeFormer weight parameter; 0=maximum effect; 1=minimum effect\", gr.Slider,{\"minimum\": 0, \"maximum\": 1, \"step\": 0.01}),\r \"interrogate_keep_models_in_memory\": OptionInfo(True, \"Interrogate: keep models in VRAM\"),\r \"interrogate_use_builtin_artists\": OptionInfo(True, \"Interrogate: use artists from artists.csv\"),\r \"interrogate_clip_num_beams\": OptionInfo(1, \"Interrogate: num_beams for BLIP\", gr.Slider,{\"minimum\": 1, \"maximum\": 16, \"step\": 1}),\r \"interrogate_clip_min_length\": OptionInfo(24, \"Interrogate: minimum descripton length(excluding artists, etc..)\", gr.Slider,{\"minimum\": 1, \"maximum\": 128, \"step\": 1}),\r \"interrogate_clip_max_length\": OptionInfo(48, \"Interrogate: maximum descripton length\", gr.Slider,{\"minimum\": 1, \"maximum\": 256, \"step\": 1}),\r }\r \r def __init__(self):\r self.data={k: v.default for k, v in self.data_labels.items()}\r \r def __setattr__(self, key, value):\r if self.data is not None:\r if key in self.data:\r self.data[key]=value\r \r return super(Options, self).__setattr__(key, value)\r \r def __getattr__(self, item):\r if self.data is not None:\r if item in self.data:\r return self.data[item]\r \r if item in self.data_labels:\r return self.data_labels[item].default\r \r return super(Options, self).__getattribute__(item)\r \r def save(self, filename):\r with open(filename, \"w\", encoding=\"utf8\") as file:\r json.dump(self.data, file)\r \r def load(self, filename):\r with open(filename, \"r\", encoding=\"utf8\") as file:\r self.data=json.load(file)\r \r \r opts=Options()\r if os.path.exists(config_filename):\r opts.load(config_filename)\r \r sd_upscalers=[]\r \r sd_model=None\r \r progress_print_out=sys.stdout\r \r \r class TotalTQDM:\r def __init__(self):\r self._tqdm=None\r \r def reset(self):\r self._tqdm=tqdm.tqdm(\r desc=\"Total progress\",\r total=state.job_count * state.sampling_steps,\r position=1,\r file=progress_print_out\r )\r \r def update(self):\r if not opts.multiple_tqdm:\r return\r if self._tqdm is None:\r self.reset()\r self._tqdm.update()\r \r def clear(self):\r if self._tqdm is not None:\r self._tqdm.close()\r self._tqdm=None\r \r \r total_tqdm=TotalTQDM()\r ","sourceWithComments":"import sys\r\nimport argparse\r\nimport json\r\nimport os\r\n\r\nimport gradio as gr\r\nimport torch\r\nimport tqdm\r\n\r\nimport modules.artists\r\nfrom modules.paths import script_path, sd_path\r\nfrom modules.devices import get_optimal_device\r\nimport modules.styles\r\nimport modules.interrogate\r\n\r\nconfig_filename = \"config.json\"\r\n\r\nsd_model_file = os.path.join(script_path, 'model.ckpt')\r\nif not os.path.exists(sd_model_file):\r\n    sd_model_file = \"models\/ldm\/stable-diffusion-v1\/model.ckpt\"\r\n\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument(\"--config\", type=str, default=os.path.join(sd_path, \"configs\/stable-diffusion\/v1-inference.yaml\"), help=\"path to config which constructs model\",)\r\nparser.add_argument(\"--ckpt\", type=str, default=os.path.join(sd_path, sd_model_file), help=\"path to checkpoint of model\",)\r\nparser.add_argument(\"--gfpgan-dir\", type=str, help=\"GFPGAN directory\", default=('.\/src\/gfpgan' if os.path.exists('.\/src\/gfpgan') else '.\/GFPGAN'))\r\nparser.add_argument(\"--gfpgan-model\", type=str, help=\"GFPGAN model file name\", default='GFPGANv1.3.pth')\r\nparser.add_argument(\"--no-half\", action='store_true', help=\"do not switch the model to 16-bit floats\")\r\nparser.add_argument(\"--no-progressbar-hiding\", action='store_true', help=\"do not hide progressbar in gradio UI (we hide it because it slows down ML if you have hardware accleration in browser)\")\r\nparser.add_argument(\"--max-batch-count\", type=int, default=16, help=\"maximum batch count value for the UI\")\r\nparser.add_argument(\"--embeddings-dir\", type=str, default='embeddings', help=\"embeddings directory for textual inversion (default: embeddings)\")\r\nparser.add_argument(\"--allow-code\", action='store_true', help=\"allow custom script execution from webui\")\r\nparser.add_argument(\"--medvram\", action='store_true', help=\"enable stable diffusion model optimizations for sacrificing a little speed for low VRM usage\")\r\nparser.add_argument(\"--lowvram\", action='store_true', help=\"enable stable diffusion model optimizations for sacrificing a lot of speed for very low VRM usage\")\r\nparser.add_argument(\"--always-batch-cond-uncond\", action='store_true', help=\"a workaround test; may help with speed if you use --lowvram\")\r\nparser.add_argument(\"--unload-gfpgan\", action='store_true', help=\"unload GFPGAN every time after processing images. Warning: seems to cause memory leaks\")\r\nparser.add_argument(\"--precision\", type=str, help=\"evaluate at this precision\", choices=[\"full\", \"autocast\"], default=\"autocast\")\r\nparser.add_argument(\"--share\", action='store_true', help=\"use share=True for gradio and make the UI accessible through their site (doesn't work for me but you might have better luck)\")\r\nparser.add_argument(\"--esrgan-models-path\", type=str, help=\"path to directory with ESRGAN models\", default=os.path.join(script_path, 'ESRGAN'))\r\nparser.add_argument(\"--opt-split-attention\", action='store_true', help=\"enable optimization that reduce vram usage by a lot for about 10%% decrease in performance\")\r\nparser.add_argument(\"--opt-split-attention-v1\", action='store_true', help=\"enable older version of --opt-split-attention optimization\")\r\nparser.add_argument(\"--listen\", action='store_true', help=\"launch gradio with 0.0.0.0 as server name, allowing to respond to network requests\")\r\nparser.add_argument(\"--port\", type=int, help=\"launch gradio with given server port, you need root\/admin rights for ports < 1024, defaults to 7860 if available\", default=None)\r\nparser.add_argument(\"--show-negative-prompt\", action='store_true', help=\"does not do anything\", default=False)\r\nparser.add_argument(\"--ui-config-file\", type=str, help=\"filename to use for ui configuration\", default=os.path.join(script_path, 'ui-config.json'))\r\n\r\ncmd_opts = parser.parse_args()\r\n\r\ndevice = get_optimal_device()\r\n\r\nbatch_cond_uncond = cmd_opts.always_batch_cond_uncond or not (cmd_opts.lowvram or cmd_opts.medvram)\r\nparallel_processing_allowed = not cmd_opts.lowvram and not cmd_opts.medvram\r\n\r\n\r\nclass State:\r\n    interrupted = False\r\n    job = \"\"\r\n    job_no = 0\r\n    job_count = 0\r\n    sampling_step = 0\r\n    sampling_steps = 0\r\n    current_latent = None\r\n    current_image = None\r\n    current_image_sampling_step = 0\r\n\r\n    def interrupt(self):\r\n        self.interrupted = True\r\n\r\n    def nextjob(self):\r\n        self.job_no += 1\r\n        self.sampling_step = 0\r\n        self.current_image_sampling_step = 0\r\n\r\n\r\nstate = State()\r\n\r\nartist_db = modules.artists.ArtistsDatabase(os.path.join(script_path, 'artists.csv'))\r\n\r\nstyles_filename = os.path.join(script_path, 'styles.csv')\r\nprompt_styles = modules.styles.load_styles(styles_filename)\r\n\r\ninterrogator = modules.interrogate.InterrogateModels(\"interrogate\")\r\n\r\nface_restorers = []\r\n\r\nclass Options:\r\n    class OptionInfo:\r\n        def __init__(self, default=None, label=\"\", component=None, component_args=None):\r\n            self.default = default\r\n            self.label = label\r\n            self.component = component\r\n            self.component_args = component_args\r\n\r\n    data = None\r\n    data_labels = {\r\n        \"outdir_samples\": OptionInfo(\"\", \"Output directory for images; if empty, defaults to two directories below\"),\r\n        \"outdir_txt2img_samples\": OptionInfo(\"outputs\/txt2img-images\", 'Output directory for txt2img images'),\r\n        \"outdir_img2img_samples\": OptionInfo(\"outputs\/img2img-images\", 'Output directory for img2img images'),\r\n        \"outdir_extras_samples\": OptionInfo(\"outputs\/extras-images\", 'Output directory for images from extras tab'),\r\n        \"outdir_grids\": OptionInfo(\"\", \"Output directory for grids; if empty, defaults to two directories below\"),\r\n        \"outdir_txt2img_grids\": OptionInfo(\"outputs\/txt2img-grids\", 'Output directory for txt2img grids'),\r\n        \"outdir_img2img_grids\": OptionInfo(\"outputs\/img2img-grids\", 'Output directory for img2img grids'),\r\n        \"save_to_dirs\": OptionInfo(False, \"When writing images, create a directory with name derived from the prompt\"),\r\n        \"grid_save_to_dirs\": OptionInfo(False, \"When writing grids, create a directory with name derived from the prompt\"),\r\n        \"save_to_dirs_prompt_len\": OptionInfo(10, \"When using above, how many words from prompt to put into directory name\", gr.Slider, {\"minimum\": 1, \"maximum\": 32, \"step\": 1}),\r\n        \"outdir_save\": OptionInfo(\"log\/images\", \"Directory for saving images using the Save button\"),\r\n        \"samples_save\": OptionInfo(True, \"Save indiviual samples\"),\r\n        \"samples_format\": OptionInfo('png', 'File format for individual samples'),\r\n        \"grid_save\": OptionInfo(True, \"Save image grids\"),\r\n        \"return_grid\": OptionInfo(True, \"Show grid in results for web\"),\r\n        \"grid_format\": OptionInfo('png', 'File format for grids'),\r\n        \"grid_extended_filename\": OptionInfo(False, \"Add extended info (seed, prompt) to filename when saving grid\"),\r\n        \"grid_only_if_multiple\": OptionInfo(True, \"Do not save grids consisting of one picture\"),\r\n        \"n_rows\": OptionInfo(-1, \"Grid row count; use -1 for autodetect and 0 for it to be same as batch size\", gr.Slider, {\"minimum\": -1, \"maximum\": 16, \"step\": 1}),\r\n        \"jpeg_quality\": OptionInfo(80, \"Quality for saved jpeg images\", gr.Slider, {\"minimum\": 1, \"maximum\": 100, \"step\": 1}),\r\n        \"export_for_4chan\": OptionInfo(True, \"If PNG image is larger than 4MB or any dimension is larger than 4000, downscale and save copy as JPG\"),\r\n        \"enable_pnginfo\": OptionInfo(True, \"Save text information about generation parameters as chunks to png files\"),\r\n        \"font\": OptionInfo(\"\", \"Font for image grids that have text\"),\r\n        \"enable_emphasis\": OptionInfo(True, \"Use (text) to make model pay more attention to text text and [text] to make it pay less attention\"),\r\n        \"save_txt\": OptionInfo(False, \"Create a text file next to every image with generation parameters.\"),\r\n        \"ESRGAN_tile\": OptionInfo(192, \"Tile size for upscaling. 0 = no tiling.\", gr.Slider, {\"minimum\": 0, \"maximum\": 512, \"step\": 16}),\r\n        \"ESRGAN_tile_overlap\": OptionInfo(8, \"Tile overlap, in pixels for upscaling. Low values = visible seam.\", gr.Slider, {\"minimum\": 0, \"maximum\": 48, \"step\": 1}),\r\n        \"random_artist_categories\": OptionInfo([], \"Allowed categories for random artists selection when using the Roll button\", gr.CheckboxGroup, {\"choices\": artist_db.categories()}),\r\n        \"upscale_at_full_resolution_padding\": OptionInfo(16, \"Inpainting at full resolution: padding, in pixels, for the masked region.\", gr.Slider, {\"minimum\": 0, \"maximum\": 128, \"step\": 4}),\r\n        \"show_progressbar\": OptionInfo(True, \"Show progressbar\"),\r\n        \"show_progress_every_n_steps\": OptionInfo(0, \"Show show image creation progress every N sampling steps. Set 0 to disable.\", gr.Slider, {\"minimum\": 0, \"maximum\": 32, \"step\": 1}),\r\n        \"multiple_tqdm\": OptionInfo(True, \"Add a second progress bar to the console that shows progress for an entire job. Broken in PyCharm console.\"),\r\n        \"face_restoration_model\": OptionInfo(None, \"Face restoration model\", gr.Radio, lambda: {\"choices\": [x.name() for x in face_restorers]}),\r\n        \"code_former_weight\": OptionInfo(0.5, \"CodeFormer weight parameter; 0 = maximum effect; 1 = minimum effect\", gr.Slider, {\"minimum\": 0, \"maximum\": 1, \"step\": 0.01}),\r\n        \"interrogate_keep_models_in_memory\": OptionInfo(True, \"Interrogate: keep models in VRAM\"),\r\n        \"interrogate_use_builtin_artists\": OptionInfo(True, \"Interrogate: use artists from artists.csv\"),\r\n        \"interrogate_clip_num_beams\": OptionInfo(1, \"Interrogate: num_beams for BLIP\", gr.Slider, {\"minimum\": 1, \"maximum\": 16, \"step\": 1}),\r\n        \"interrogate_clip_min_length\": OptionInfo(24, \"Interrogate: minimum descripton length (excluding artists, etc..)\", gr.Slider, {\"minimum\": 1, \"maximum\": 128, \"step\": 1}),\r\n        \"interrogate_clip_max_length\": OptionInfo(48, \"Interrogate: maximum descripton length\", gr.Slider, {\"minimum\": 1, \"maximum\": 256, \"step\": 1}),\r\n    }\r\n\r\n    def __init__(self):\r\n        self.data = {k: v.default for k, v in self.data_labels.items()}\r\n\r\n    def __setattr__(self, key, value):\r\n        if self.data is not None:\r\n            if key in self.data:\r\n                self.data[key] = value\r\n\r\n        return super(Options, self).__setattr__(key, value)\r\n\r\n    def __getattr__(self, item):\r\n        if self.data is not None:\r\n            if item in self.data:\r\n                return self.data[item]\r\n\r\n        if item in self.data_labels:\r\n            return self.data_labels[item].default\r\n\r\n        return super(Options, self).__getattribute__(item)\r\n\r\n    def save(self, filename):\r\n        with open(filename, \"w\", encoding=\"utf8\") as file:\r\n            json.dump(self.data, file)\r\n\r\n    def load(self, filename):\r\n        with open(filename, \"r\", encoding=\"utf8\") as file:\r\n            self.data = json.load(file)\r\n\r\n\r\nopts = Options()\r\nif os.path.exists(config_filename):\r\n    opts.load(config_filename)\r\n\r\nsd_upscalers = []\r\n\r\nsd_model = None\r\n\r\nprogress_print_out = sys.stdout\r\n\r\n\r\nclass TotalTQDM:\r\n    def __init__(self):\r\n        self._tqdm = None\r\n\r\n    def reset(self):\r\n        self._tqdm = tqdm.tqdm(\r\n            desc=\"Total progress\",\r\n            total=state.job_count * state.sampling_steps,\r\n            position=1,\r\n            file=progress_print_out\r\n        )\r\n\r\n    def update(self):\r\n        if not opts.multiple_tqdm:\r\n            return\r\n        if self._tqdm is None:\r\n            self.reset()\r\n        self._tqdm.update()\r\n\r\n    def clear(self):\r\n        if self._tqdm is not None:\r\n            self._tqdm.close()\r\n            self._tqdm = None\r\n\r\n\r\ntotal_tqdm = TotalTQDM()\r\n"},"\/modules\/ui.py":{"changes":[{"diff":"\n         info = opts.data_labels[key]\r\n         t = type(info.default)\r\n \r\n+        args = info.component_args() if callable(info.component_args) else info.component_args\r\n+\r\n         if info.component is not None:\r\n-            args = info.component_args() if callable(info.component_args) else info.component_args\r\n-            item = info.component(label=info.label, value=fun, **(args or {}))\r\n+            comp = info.component\r\n         elif t == str:\r\n-            item = gr.Textbox(label=info.label, value=fun, lines=1)\r\n+            comp = gr.Textbox\r\n         elif t == int:\r\n-            item = gr.Number(label=info.label, value=fun)\r\n+            comp = gr.Number\r\n         elif t == bool:\r\n-            item = gr.Checkbox(label=info.label, value=fun)\r\n+            comp = gr.Checkbox\r\n         else:\r\n             raise Exception(f'bad options item type: {str(t)} for key {key}')\r\n \r\n-        return item\r\n+        return comp(label=info.label, value=fun, **(args or {}))\r\n \r\n     components = []\r\n     keys = list(opts.data_labels.keys())\r\n","add":7,"remove":6,"filename":"\/modules\/ui.py","badparts":["            args = info.component_args() if callable(info.component_args) else info.component_args\r","            item = info.component(label=info.label, value=fun, **(args or {}))\r","            item = gr.Textbox(label=info.label, value=fun, lines=1)\r","            item = gr.Number(label=info.label, value=fun)\r","            item = gr.Checkbox(label=info.label, value=fun)\r","        return item\r"],"goodparts":["        args = info.component_args() if callable(info.component_args) else info.component_args\r","\r","            comp = info.component\r","            comp = gr.Textbox\r","            comp = gr.Number\r","            comp = gr.Checkbox\r","        return comp(label=info.label, value=fun, **(args or {}))\r"]}]}},"msg":"Add --hide-ui-dir-config command line flag\n\nAdds `--hide-ui-dir-config` flag to disable editing directory configs from the web UI. This can be set to prevent users from setting the directory to somewhere they shouldn't, for public (or semi-public) interfaces.\n\nDirectories are still read from config.json, so the server admin can still set them in the web UI and then relaunch with the hide flag, or edit the config manually.\n\nAlso:\n- fix OptionInfo `component_args` keyword argument not being read if `component` isn't also set\n- ensure that hidden settings aren't still read from the web UI (otherwise they could still be changed by tampering with the interface)"}},"https:\/\/github.com\/2296429409\/stable-diffusion-webui":{"29a2933e23e68900bbae741a98439d0c9d6f26f6":{"url":"https:\/\/api.github.com\/repos\/2296429409\/stable-diffusion-webui\/commits\/29a2933e23e68900bbae741a98439d0c9d6f26f6","html_url":"https:\/\/github.com\/2296429409\/stable-diffusion-webui\/commit\/29a2933e23e68900bbae741a98439d0c9d6f26f6","message":"Add --hide-ui-dir-config command line flag\n\nAdds `--hide-ui-dir-config` flag to disable editing directory configs from the web UI. This can be set to prevent users from setting the directory to somewhere they shouldn't, for public (or semi-public) interfaces.\n\nDirectories are still read from config.json, so the server admin can still set them in the web UI and then relaunch with the hide flag, or edit the config manually.\n\nAlso:\n- fix OptionInfo `component_args` keyword argument not being read if `component` isn't also set\n- ensure that hidden settings aren't still read from the web UI (otherwise they could still be changed by tampering with the interface)","sha":"29a2933e23e68900bbae741a98439d0c9d6f26f6","keyword":"tampering prevent","diff":"diff --git a\/modules\/shared.py b\/modules\/shared.py\nindex 9eeb64e3..0557cfe3 100644\n--- a\/modules\/shared.py\n+++ b\/modules\/shared.py\n@@ -42,6 +42,7 @@\n parser.add_argument(\"--port\", type=int, help=\"launch gradio with given server port, you need root\/admin rights for ports < 1024, defaults to 7860 if available\", default=None)\r\n parser.add_argument(\"--show-negative-prompt\", action='store_true', help=\"does not do anything\", default=False)\r\n parser.add_argument(\"--ui-config-file\", type=str, help=\"filename to use for ui configuration\", default=os.path.join(script_path, 'ui-config.json'))\r\n+parser.add_argument(\"--hide-ui-dir-config\", action='store_true', help=\"hide directory configuration from webui\", default=False)\r\n \r\n cmd_opts = parser.parse_args()\r\n \r\n@@ -91,18 +92,19 @@ def __init__(self, default=None, label=\"\", component=None, component_args=None):\n             self.component_args = component_args\r\n \r\n     data = None\r\n+    hide_dirs = {\"visible\": False} if cmd_opts.hide_ui_dir_config else None\r\n     data_labels = {\r\n-        \"outdir_samples\": OptionInfo(\"\", \"Output directory for images; if empty, defaults to two directories below\"),\r\n-        \"outdir_txt2img_samples\": OptionInfo(\"outputs\/txt2img-images\", 'Output directory for txt2img images'),\r\n-        \"outdir_img2img_samples\": OptionInfo(\"outputs\/img2img-images\", 'Output directory for img2img images'),\r\n-        \"outdir_extras_samples\": OptionInfo(\"outputs\/extras-images\", 'Output directory for images from extras tab'),\r\n-        \"outdir_grids\": OptionInfo(\"\", \"Output directory for grids; if empty, defaults to two directories below\"),\r\n-        \"outdir_txt2img_grids\": OptionInfo(\"outputs\/txt2img-grids\", 'Output directory for txt2img grids'),\r\n-        \"outdir_img2img_grids\": OptionInfo(\"outputs\/img2img-grids\", 'Output directory for img2img grids'),\r\n+        \"outdir_samples\": OptionInfo(\"\", \"Output directory for images; if empty, defaults to two directories below\", component_args=hide_dirs),\r\n+        \"outdir_txt2img_samples\": OptionInfo(\"outputs\/txt2img-images\", 'Output directory for txt2img images', component_args=hide_dirs),\r\n+        \"outdir_img2img_samples\": OptionInfo(\"outputs\/img2img-images\", 'Output directory for img2img images', component_args=hide_dirs),\r\n+        \"outdir_extras_samples\": OptionInfo(\"outputs\/extras-images\", 'Output directory for images from extras tab', component_args=hide_dirs),\r\n+        \"outdir_grids\": OptionInfo(\"\", \"Output directory for grids; if empty, defaults to two directories below\", component_args=hide_dirs),\r\n+        \"outdir_txt2img_grids\": OptionInfo(\"outputs\/txt2img-grids\", 'Output directory for txt2img grids', component_args=hide_dirs),\r\n+        \"outdir_img2img_grids\": OptionInfo(\"outputs\/img2img-grids\", 'Output directory for img2img grids', component_args=hide_dirs),\r\n         \"save_to_dirs\": OptionInfo(False, \"When writing images, create a directory with name derived from the prompt\"),\r\n         \"grid_save_to_dirs\": OptionInfo(False, \"When writing grids, create a directory with name derived from the prompt\"),\r\n         \"save_to_dirs_prompt_len\": OptionInfo(10, \"When using above, how many words from prompt to put into directory name\", gr.Slider, {\"minimum\": 1, \"maximum\": 32, \"step\": 1}),\r\n-        \"outdir_save\": OptionInfo(\"log\/images\", \"Directory for saving images using the Save button\"),\r\n+        \"outdir_save\": OptionInfo(\"log\/images\", \"Directory for saving images using the Save button\", component_args=hide_dirs),\r\n         \"samples_save\": OptionInfo(True, \"Save indiviual samples\"),\r\n         \"samples_format\": OptionInfo('png', 'File format for individual samples'),\r\n         \"grid_save\": OptionInfo(True, \"Save image grids\"),\r\ndiff --git a\/modules\/ui.py b\/modules\/ui.py\nindex b9af2c86..3b7eb9bb 100644\n--- a\/modules\/ui.py\n+++ b\/modules\/ui.py\n@@ -661,19 +661,20 @@ def fun():\n         info = opts.data_labels[key]\r\n         t = type(info.default)\r\n \r\n+        args = info.component_args() if callable(info.component_args) else info.component_args\r\n+\r\n         if info.component is not None:\r\n-            args = info.component_args() if callable(info.component_args) else info.component_args\r\n-            item = info.component(label=info.label, value=fun, **(args or {}))\r\n+            comp = info.component\r\n         elif t == str:\r\n-            item = gr.Textbox(label=info.label, value=fun, lines=1)\r\n+            comp = gr.Textbox\r\n         elif t == int:\r\n-            item = gr.Number(label=info.label, value=fun)\r\n+            comp = gr.Number\r\n         elif t == bool:\r\n-            item = gr.Checkbox(label=info.label, value=fun)\r\n+            comp = gr.Checkbox\r\n         else:\r\n             raise Exception(f'bad options item type: {str(t)} for key {key}')\r\n \r\n-        return item\r\n+        return comp(label=info.label, value=fun, **(args or {}))\r\n \r\n     components = []\r\n     keys = list(opts.data_labels.keys())\r\n@@ -684,6 +685,10 @@ def run_settings(*args):\n         up = []\r\n \r\n         for key, value, comp in zip(opts.data_labels.keys(), args, components):\r\n+            comp_args = opts.data_labels[key].component_args\r\n+            if comp_args and isinstance(comp_args, dict) and comp_args.get('visible') is False:\r\n+                continue\r\n+\r\n             opts.data[key] = value\r\n             up.append(comp.update(value=value))\r\n \r\n","files":{"\/modules\/shared.py":{"changes":[{"diff":"\n             self.component_args = component_args\r\n \r\n     data = None\r\n+    hide_dirs = {\"visible\": False} if cmd_opts.hide_ui_dir_config else None\r\n     data_labels = {\r\n-        \"outdir_samples\": OptionInfo(\"\", \"Output directory for images; if empty, defaults to two directories below\"),\r\n-        \"outdir_txt2img_samples\": OptionInfo(\"outputs\/txt2img-images\", 'Output directory for txt2img images'),\r\n-        \"outdir_img2img_samples\": OptionInfo(\"outputs\/img2img-images\", 'Output directory for img2img images'),\r\n-        \"outdir_extras_samples\": OptionInfo(\"outputs\/extras-images\", 'Output directory for images from extras tab'),\r\n-        \"outdir_grids\": OptionInfo(\"\", \"Output directory for grids; if empty, defaults to two directories below\"),\r\n-        \"outdir_txt2img_grids\": OptionInfo(\"outputs\/txt2img-grids\", 'Output directory for txt2img grids'),\r\n-        \"outdir_img2img_grids\": OptionInfo(\"outputs\/img2img-grids\", 'Output directory for img2img grids'),\r\n+        \"outdir_samples\": OptionInfo(\"\", \"Output directory for images; if empty, defaults to two directories below\", component_args=hide_dirs),\r\n+        \"outdir_txt2img_samples\": OptionInfo(\"outputs\/txt2img-images\", 'Output directory for txt2img images', component_args=hide_dirs),\r\n+        \"outdir_img2img_samples\": OptionInfo(\"outputs\/img2img-images\", 'Output directory for img2img images', component_args=hide_dirs),\r\n+        \"outdir_extras_samples\": OptionInfo(\"outputs\/extras-images\", 'Output directory for images from extras tab', component_args=hide_dirs),\r\n+        \"outdir_grids\": OptionInfo(\"\", \"Output directory for grids; if empty, defaults to two directories below\", component_args=hide_dirs),\r\n+        \"outdir_txt2img_grids\": OptionInfo(\"outputs\/txt2img-grids\", 'Output directory for txt2img grids', component_args=hide_dirs),\r\n+        \"outdir_img2img_grids\": OptionInfo(\"outputs\/img2img-grids\", 'Output directory for img2img grids', component_args=hide_dirs),\r\n         \"save_to_dirs\": OptionInfo(False, \"When writing images, create a directory with name derived from the prompt\"),\r\n         \"grid_save_to_dirs\": OptionInfo(False, \"When writing grids, create a directory with name derived from the prompt\"),\r\n         \"save_to_dirs_prompt_len\": OptionInfo(10, \"When using above, how many words from prompt to put into directory name\", gr.Slider, {\"minimum\": 1, \"maximum\": 32, \"step\": 1}),\r\n-        \"outdir_save\": OptionInfo(\"log\/images\", \"Directory for saving images using the Save button\"),\r\n+        \"outdir_save\": OptionInfo(\"log\/images\", \"Directory for saving images using the Save button\", component_args=hide_dirs),\r\n         \"samples_save\": OptionInfo(True, \"Save indiviual samples\"),\r\n         \"samples_format\": OptionInfo('png', 'File format for individual samples'),\r\n         \"grid_save\": OptionInfo(True, \"Save image grids\"),\r","add":9,"remove":8,"filename":"\/modules\/shared.py","badparts":["        \"outdir_samples\": OptionInfo(\"\", \"Output directory for images; if empty, defaults to two directories below\"),\r","        \"outdir_txt2img_samples\": OptionInfo(\"outputs\/txt2img-images\", 'Output directory for txt2img images'),\r","        \"outdir_img2img_samples\": OptionInfo(\"outputs\/img2img-images\", 'Output directory for img2img images'),\r","        \"outdir_extras_samples\": OptionInfo(\"outputs\/extras-images\", 'Output directory for images from extras tab'),\r","        \"outdir_grids\": OptionInfo(\"\", \"Output directory for grids; if empty, defaults to two directories below\"),\r","        \"outdir_txt2img_grids\": OptionInfo(\"outputs\/txt2img-grids\", 'Output directory for txt2img grids'),\r","        \"outdir_img2img_grids\": OptionInfo(\"outputs\/img2img-grids\", 'Output directory for img2img grids'),\r","        \"outdir_save\": OptionInfo(\"log\/images\", \"Directory for saving images using the Save button\"),\r"],"goodparts":["    hide_dirs = {\"visible\": False} if cmd_opts.hide_ui_dir_config else None\r","        \"outdir_samples\": OptionInfo(\"\", \"Output directory for images; if empty, defaults to two directories below\", component_args=hide_dirs),\r","        \"outdir_txt2img_samples\": OptionInfo(\"outputs\/txt2img-images\", 'Output directory for txt2img images', component_args=hide_dirs),\r","        \"outdir_img2img_samples\": OptionInfo(\"outputs\/img2img-images\", 'Output directory for img2img images', component_args=hide_dirs),\r","        \"outdir_extras_samples\": OptionInfo(\"outputs\/extras-images\", 'Output directory for images from extras tab', component_args=hide_dirs),\r","        \"outdir_grids\": OptionInfo(\"\", \"Output directory for grids; if empty, defaults to two directories below\", component_args=hide_dirs),\r","        \"outdir_txt2img_grids\": OptionInfo(\"outputs\/txt2img-grids\", 'Output directory for txt2img grids', component_args=hide_dirs),\r","        \"outdir_img2img_grids\": OptionInfo(\"outputs\/img2img-grids\", 'Output directory for img2img grids', component_args=hide_dirs),\r","        \"outdir_save\": OptionInfo(\"log\/images\", \"Directory for saving images using the Save button\", component_args=hide_dirs),\r"]}],"source":"\nimport sys\r import argparse\r import json\r import os\r \r import gradio as gr\r import torch\r import tqdm\r \r import modules.artists\r from modules.paths import script_path, sd_path\r from modules.devices import get_optimal_device\r import modules.styles\r import modules.interrogate\r \r config_filename=\"config.json\"\r \r sd_model_file=os.path.join(script_path, 'model.ckpt')\r if not os.path.exists(sd_model_file):\r sd_model_file=\"models\/ldm\/stable-diffusion-v1\/model.ckpt\"\r \r parser=argparse.ArgumentParser()\r parser.add_argument(\"--config\", type=str, default=os.path.join(sd_path, \"configs\/stable-diffusion\/v1-inference.yaml\"), help=\"path to config which constructs model\",)\r parser.add_argument(\"--ckpt\", type=str, default=os.path.join(sd_path, sd_model_file), help=\"path to checkpoint of model\",)\r parser.add_argument(\"--gfpgan-dir\", type=str, help=\"GFPGAN directory\", default=('.\/src\/gfpgan' if os.path.exists('.\/src\/gfpgan') else '.\/GFPGAN'))\r parser.add_argument(\"--gfpgan-model\", type=str, help=\"GFPGAN model file name\", default='GFPGANv1.3.pth')\r parser.add_argument(\"--no-half\", action='store_true', help=\"do not switch the model to 16-bit floats\")\r parser.add_argument(\"--no-progressbar-hiding\", action='store_true', help=\"do not hide progressbar in gradio UI(we hide it because it slows down ML if you have hardware accleration in browser)\")\r parser.add_argument(\"--max-batch-count\", type=int, default=16, help=\"maximum batch count value for the UI\")\r parser.add_argument(\"--embeddings-dir\", type=str, default='embeddings', help=\"embeddings directory for textual inversion(default: embeddings)\")\r parser.add_argument(\"--allow-code\", action='store_true', help=\"allow custom script execution from webui\")\r parser.add_argument(\"--medvram\", action='store_true', help=\"enable stable diffusion model optimizations for sacrificing a little speed for low VRM usage\")\r parser.add_argument(\"--lowvram\", action='store_true', help=\"enable stable diffusion model optimizations for sacrificing a lot of speed for very low VRM usage\")\r parser.add_argument(\"--always-batch-cond-uncond\", action='store_true', help=\"a workaround test; may help with speed if you use --lowvram\")\r parser.add_argument(\"--unload-gfpgan\", action='store_true', help=\"unload GFPGAN every time after processing images. Warning: seems to cause memory leaks\")\r parser.add_argument(\"--precision\", type=str, help=\"evaluate at this precision\", choices=[\"full\", \"autocast\"], default=\"autocast\")\r parser.add_argument(\"--share\", action='store_true', help=\"use share=True for gradio and make the UI accessible through their site(doesn't work for me but you might have better luck)\")\r parser.add_argument(\"--esrgan-models-path\", type=str, help=\"path to directory with ESRGAN models\", default=os.path.join(script_path, 'ESRGAN'))\r parser.add_argument(\"--opt-split-attention\", action='store_true', help=\"enable optimization that reduce vram usage by a lot for about 10%% decrease in performance\")\r parser.add_argument(\"--opt-split-attention-v1\", action='store_true', help=\"enable older version of --opt-split-attention optimization\")\r parser.add_argument(\"--listen\", action='store_true', help=\"launch gradio with 0.0.0.0 as server name, allowing to respond to network requests\")\r parser.add_argument(\"--port\", type=int, help=\"launch gradio with given server port, you need root\/admin rights for ports < 1024, defaults to 7860 if available\", default=None)\r parser.add_argument(\"--show-negative-prompt\", action='store_true', help=\"does not do anything\", default=False)\r parser.add_argument(\"--ui-config-file\", type=str, help=\"filename to use for ui configuration\", default=os.path.join(script_path, 'ui-config.json'))\r \r cmd_opts=parser.parse_args()\r \r device=get_optimal_device()\r \r batch_cond_uncond=cmd_opts.always_batch_cond_uncond or not(cmd_opts.lowvram or cmd_opts.medvram)\r parallel_processing_allowed=not cmd_opts.lowvram and not cmd_opts.medvram\r \r \r class State:\r interrupted=False\r job=\"\"\r job_no=0\r job_count=0\r sampling_step=0\r sampling_steps=0\r current_latent=None\r current_image=None\r current_image_sampling_step=0\r \r def interrupt(self):\r self.interrupted=True\r \r def nextjob(self):\r self.job_no +=1\r self.sampling_step=0\r self.current_image_sampling_step=0\r \r \r state=State()\r \r artist_db=modules.artists.ArtistsDatabase(os.path.join(script_path, 'artists.csv'))\r \r styles_filename=os.path.join(script_path, 'styles.csv')\r prompt_styles=modules.styles.load_styles(styles_filename)\r \r interrogator=modules.interrogate.InterrogateModels(\"interrogate\")\r \r face_restorers=[]\r \r class Options:\r class OptionInfo:\r def __init__(self, default=None, label=\"\", component=None, component_args=None):\r self.default=default\r self.label=label\r self.component=component\r self.component_args=component_args\r \r data=None\r data_labels={\r \"outdir_samples\": OptionInfo(\"\", \"Output directory for images; if empty, defaults to two directories below\"),\r \"outdir_txt2img_samples\": OptionInfo(\"outputs\/txt2img-images\", 'Output directory for txt2img images'),\r \"outdir_img2img_samples\": OptionInfo(\"outputs\/img2img-images\", 'Output directory for img2img images'),\r \"outdir_extras_samples\": OptionInfo(\"outputs\/extras-images\", 'Output directory for images from extras tab'),\r \"outdir_grids\": OptionInfo(\"\", \"Output directory for grids; if empty, defaults to two directories below\"),\r \"outdir_txt2img_grids\": OptionInfo(\"outputs\/txt2img-grids\", 'Output directory for txt2img grids'),\r \"outdir_img2img_grids\": OptionInfo(\"outputs\/img2img-grids\", 'Output directory for img2img grids'),\r \"save_to_dirs\": OptionInfo(False, \"When writing images, create a directory with name derived from the prompt\"),\r \"grid_save_to_dirs\": OptionInfo(False, \"When writing grids, create a directory with name derived from the prompt\"),\r \"save_to_dirs_prompt_len\": OptionInfo(10, \"When using above, how many words from prompt to put into directory name\", gr.Slider,{\"minimum\": 1, \"maximum\": 32, \"step\": 1}),\r \"outdir_save\": OptionInfo(\"log\/images\", \"Directory for saving images using the Save button\"),\r \"samples_save\": OptionInfo(True, \"Save indiviual samples\"),\r \"samples_format\": OptionInfo('png', 'File format for individual samples'),\r \"grid_save\": OptionInfo(True, \"Save image grids\"),\r \"return_grid\": OptionInfo(True, \"Show grid in results for web\"),\r \"grid_format\": OptionInfo('png', 'File format for grids'),\r \"grid_extended_filename\": OptionInfo(False, \"Add extended info(seed, prompt) to filename when saving grid\"),\r \"grid_only_if_multiple\": OptionInfo(True, \"Do not save grids consisting of one picture\"),\r \"n_rows\": OptionInfo(-1, \"Grid row count; use -1 for autodetect and 0 for it to be same as batch size\", gr.Slider,{\"minimum\": -1, \"maximum\": 16, \"step\": 1}),\r \"jpeg_quality\": OptionInfo(80, \"Quality for saved jpeg images\", gr.Slider,{\"minimum\": 1, \"maximum\": 100, \"step\": 1}),\r \"export_for_4chan\": OptionInfo(True, \"If PNG image is larger than 4MB or any dimension is larger than 4000, downscale and save copy as JPG\"),\r \"enable_pnginfo\": OptionInfo(True, \"Save text information about generation parameters as chunks to png files\"),\r \"font\": OptionInfo(\"\", \"Font for image grids that have text\"),\r \"enable_emphasis\": OptionInfo(True, \"Use(text) to make model pay more attention to text text and[text] to make it pay less attention\"),\r \"save_txt\": OptionInfo(False, \"Create a text file next to every image with generation parameters.\"),\r \"ESRGAN_tile\": OptionInfo(192, \"Tile size for upscaling. 0=no tiling.\", gr.Slider,{\"minimum\": 0, \"maximum\": 512, \"step\": 16}),\r \"ESRGAN_tile_overlap\": OptionInfo(8, \"Tile overlap, in pixels for upscaling. Low values=visible seam.\", gr.Slider,{\"minimum\": 0, \"maximum\": 48, \"step\": 1}),\r \"random_artist_categories\": OptionInfo([], \"Allowed categories for random artists selection when using the Roll button\", gr.CheckboxGroup,{\"choices\": artist_db.categories()}),\r \"upscale_at_full_resolution_padding\": OptionInfo(16, \"Inpainting at full resolution: padding, in pixels, for the masked region.\", gr.Slider,{\"minimum\": 0, \"maximum\": 128, \"step\": 4}),\r \"show_progressbar\": OptionInfo(True, \"Show progressbar\"),\r \"show_progress_every_n_steps\": OptionInfo(0, \"Show show image creation progress every N sampling steps. Set 0 to disable.\", gr.Slider,{\"minimum\": 0, \"maximum\": 32, \"step\": 1}),\r \"multiple_tqdm\": OptionInfo(True, \"Add a second progress bar to the console that shows progress for an entire job. Broken in PyCharm console.\"),\r \"face_restoration_model\": OptionInfo(None, \"Face restoration model\", gr.Radio, lambda:{\"choices\":[x.name() for x in face_restorers]}),\r \"code_former_weight\": OptionInfo(0.5, \"CodeFormer weight parameter; 0=maximum effect; 1=minimum effect\", gr.Slider,{\"minimum\": 0, \"maximum\": 1, \"step\": 0.01}),\r \"interrogate_keep_models_in_memory\": OptionInfo(True, \"Interrogate: keep models in VRAM\"),\r \"interrogate_use_builtin_artists\": OptionInfo(True, \"Interrogate: use artists from artists.csv\"),\r \"interrogate_clip_num_beams\": OptionInfo(1, \"Interrogate: num_beams for BLIP\", gr.Slider,{\"minimum\": 1, \"maximum\": 16, \"step\": 1}),\r \"interrogate_clip_min_length\": OptionInfo(24, \"Interrogate: minimum descripton length(excluding artists, etc..)\", gr.Slider,{\"minimum\": 1, \"maximum\": 128, \"step\": 1}),\r \"interrogate_clip_max_length\": OptionInfo(48, \"Interrogate: maximum descripton length\", gr.Slider,{\"minimum\": 1, \"maximum\": 256, \"step\": 1}),\r }\r \r def __init__(self):\r self.data={k: v.default for k, v in self.data_labels.items()}\r \r def __setattr__(self, key, value):\r if self.data is not None:\r if key in self.data:\r self.data[key]=value\r \r return super(Options, self).__setattr__(key, value)\r \r def __getattr__(self, item):\r if self.data is not None:\r if item in self.data:\r return self.data[item]\r \r if item in self.data_labels:\r return self.data_labels[item].default\r \r return super(Options, self).__getattribute__(item)\r \r def save(self, filename):\r with open(filename, \"w\", encoding=\"utf8\") as file:\r json.dump(self.data, file)\r \r def load(self, filename):\r with open(filename, \"r\", encoding=\"utf8\") as file:\r self.data=json.load(file)\r \r \r opts=Options()\r if os.path.exists(config_filename):\r opts.load(config_filename)\r \r sd_upscalers=[]\r \r sd_model=None\r \r progress_print_out=sys.stdout\r \r \r class TotalTQDM:\r def __init__(self):\r self._tqdm=None\r \r def reset(self):\r self._tqdm=tqdm.tqdm(\r desc=\"Total progress\",\r total=state.job_count * state.sampling_steps,\r position=1,\r file=progress_print_out\r )\r \r def update(self):\r if not opts.multiple_tqdm:\r return\r if self._tqdm is None:\r self.reset()\r self._tqdm.update()\r \r def clear(self):\r if self._tqdm is not None:\r self._tqdm.close()\r self._tqdm=None\r \r \r total_tqdm=TotalTQDM()\r ","sourceWithComments":"import sys\r\nimport argparse\r\nimport json\r\nimport os\r\n\r\nimport gradio as gr\r\nimport torch\r\nimport tqdm\r\n\r\nimport modules.artists\r\nfrom modules.paths import script_path, sd_path\r\nfrom modules.devices import get_optimal_device\r\nimport modules.styles\r\nimport modules.interrogate\r\n\r\nconfig_filename = \"config.json\"\r\n\r\nsd_model_file = os.path.join(script_path, 'model.ckpt')\r\nif not os.path.exists(sd_model_file):\r\n    sd_model_file = \"models\/ldm\/stable-diffusion-v1\/model.ckpt\"\r\n\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument(\"--config\", type=str, default=os.path.join(sd_path, \"configs\/stable-diffusion\/v1-inference.yaml\"), help=\"path to config which constructs model\",)\r\nparser.add_argument(\"--ckpt\", type=str, default=os.path.join(sd_path, sd_model_file), help=\"path to checkpoint of model\",)\r\nparser.add_argument(\"--gfpgan-dir\", type=str, help=\"GFPGAN directory\", default=('.\/src\/gfpgan' if os.path.exists('.\/src\/gfpgan') else '.\/GFPGAN'))\r\nparser.add_argument(\"--gfpgan-model\", type=str, help=\"GFPGAN model file name\", default='GFPGANv1.3.pth')\r\nparser.add_argument(\"--no-half\", action='store_true', help=\"do not switch the model to 16-bit floats\")\r\nparser.add_argument(\"--no-progressbar-hiding\", action='store_true', help=\"do not hide progressbar in gradio UI (we hide it because it slows down ML if you have hardware accleration in browser)\")\r\nparser.add_argument(\"--max-batch-count\", type=int, default=16, help=\"maximum batch count value for the UI\")\r\nparser.add_argument(\"--embeddings-dir\", type=str, default='embeddings', help=\"embeddings directory for textual inversion (default: embeddings)\")\r\nparser.add_argument(\"--allow-code\", action='store_true', help=\"allow custom script execution from webui\")\r\nparser.add_argument(\"--medvram\", action='store_true', help=\"enable stable diffusion model optimizations for sacrificing a little speed for low VRM usage\")\r\nparser.add_argument(\"--lowvram\", action='store_true', help=\"enable stable diffusion model optimizations for sacrificing a lot of speed for very low VRM usage\")\r\nparser.add_argument(\"--always-batch-cond-uncond\", action='store_true', help=\"a workaround test; may help with speed if you use --lowvram\")\r\nparser.add_argument(\"--unload-gfpgan\", action='store_true', help=\"unload GFPGAN every time after processing images. Warning: seems to cause memory leaks\")\r\nparser.add_argument(\"--precision\", type=str, help=\"evaluate at this precision\", choices=[\"full\", \"autocast\"], default=\"autocast\")\r\nparser.add_argument(\"--share\", action='store_true', help=\"use share=True for gradio and make the UI accessible through their site (doesn't work for me but you might have better luck)\")\r\nparser.add_argument(\"--esrgan-models-path\", type=str, help=\"path to directory with ESRGAN models\", default=os.path.join(script_path, 'ESRGAN'))\r\nparser.add_argument(\"--opt-split-attention\", action='store_true', help=\"enable optimization that reduce vram usage by a lot for about 10%% decrease in performance\")\r\nparser.add_argument(\"--opt-split-attention-v1\", action='store_true', help=\"enable older version of --opt-split-attention optimization\")\r\nparser.add_argument(\"--listen\", action='store_true', help=\"launch gradio with 0.0.0.0 as server name, allowing to respond to network requests\")\r\nparser.add_argument(\"--port\", type=int, help=\"launch gradio with given server port, you need root\/admin rights for ports < 1024, defaults to 7860 if available\", default=None)\r\nparser.add_argument(\"--show-negative-prompt\", action='store_true', help=\"does not do anything\", default=False)\r\nparser.add_argument(\"--ui-config-file\", type=str, help=\"filename to use for ui configuration\", default=os.path.join(script_path, 'ui-config.json'))\r\n\r\ncmd_opts = parser.parse_args()\r\n\r\ndevice = get_optimal_device()\r\n\r\nbatch_cond_uncond = cmd_opts.always_batch_cond_uncond or not (cmd_opts.lowvram or cmd_opts.medvram)\r\nparallel_processing_allowed = not cmd_opts.lowvram and not cmd_opts.medvram\r\n\r\n\r\nclass State:\r\n    interrupted = False\r\n    job = \"\"\r\n    job_no = 0\r\n    job_count = 0\r\n    sampling_step = 0\r\n    sampling_steps = 0\r\n    current_latent = None\r\n    current_image = None\r\n    current_image_sampling_step = 0\r\n\r\n    def interrupt(self):\r\n        self.interrupted = True\r\n\r\n    def nextjob(self):\r\n        self.job_no += 1\r\n        self.sampling_step = 0\r\n        self.current_image_sampling_step = 0\r\n\r\n\r\nstate = State()\r\n\r\nartist_db = modules.artists.ArtistsDatabase(os.path.join(script_path, 'artists.csv'))\r\n\r\nstyles_filename = os.path.join(script_path, 'styles.csv')\r\nprompt_styles = modules.styles.load_styles(styles_filename)\r\n\r\ninterrogator = modules.interrogate.InterrogateModels(\"interrogate\")\r\n\r\nface_restorers = []\r\n\r\nclass Options:\r\n    class OptionInfo:\r\n        def __init__(self, default=None, label=\"\", component=None, component_args=None):\r\n            self.default = default\r\n            self.label = label\r\n            self.component = component\r\n            self.component_args = component_args\r\n\r\n    data = None\r\n    data_labels = {\r\n        \"outdir_samples\": OptionInfo(\"\", \"Output directory for images; if empty, defaults to two directories below\"),\r\n        \"outdir_txt2img_samples\": OptionInfo(\"outputs\/txt2img-images\", 'Output directory for txt2img images'),\r\n        \"outdir_img2img_samples\": OptionInfo(\"outputs\/img2img-images\", 'Output directory for img2img images'),\r\n        \"outdir_extras_samples\": OptionInfo(\"outputs\/extras-images\", 'Output directory for images from extras tab'),\r\n        \"outdir_grids\": OptionInfo(\"\", \"Output directory for grids; if empty, defaults to two directories below\"),\r\n        \"outdir_txt2img_grids\": OptionInfo(\"outputs\/txt2img-grids\", 'Output directory for txt2img grids'),\r\n        \"outdir_img2img_grids\": OptionInfo(\"outputs\/img2img-grids\", 'Output directory for img2img grids'),\r\n        \"save_to_dirs\": OptionInfo(False, \"When writing images, create a directory with name derived from the prompt\"),\r\n        \"grid_save_to_dirs\": OptionInfo(False, \"When writing grids, create a directory with name derived from the prompt\"),\r\n        \"save_to_dirs_prompt_len\": OptionInfo(10, \"When using above, how many words from prompt to put into directory name\", gr.Slider, {\"minimum\": 1, \"maximum\": 32, \"step\": 1}),\r\n        \"outdir_save\": OptionInfo(\"log\/images\", \"Directory for saving images using the Save button\"),\r\n        \"samples_save\": OptionInfo(True, \"Save indiviual samples\"),\r\n        \"samples_format\": OptionInfo('png', 'File format for individual samples'),\r\n        \"grid_save\": OptionInfo(True, \"Save image grids\"),\r\n        \"return_grid\": OptionInfo(True, \"Show grid in results for web\"),\r\n        \"grid_format\": OptionInfo('png', 'File format for grids'),\r\n        \"grid_extended_filename\": OptionInfo(False, \"Add extended info (seed, prompt) to filename when saving grid\"),\r\n        \"grid_only_if_multiple\": OptionInfo(True, \"Do not save grids consisting of one picture\"),\r\n        \"n_rows\": OptionInfo(-1, \"Grid row count; use -1 for autodetect and 0 for it to be same as batch size\", gr.Slider, {\"minimum\": -1, \"maximum\": 16, \"step\": 1}),\r\n        \"jpeg_quality\": OptionInfo(80, \"Quality for saved jpeg images\", gr.Slider, {\"minimum\": 1, \"maximum\": 100, \"step\": 1}),\r\n        \"export_for_4chan\": OptionInfo(True, \"If PNG image is larger than 4MB or any dimension is larger than 4000, downscale and save copy as JPG\"),\r\n        \"enable_pnginfo\": OptionInfo(True, \"Save text information about generation parameters as chunks to png files\"),\r\n        \"font\": OptionInfo(\"\", \"Font for image grids that have text\"),\r\n        \"enable_emphasis\": OptionInfo(True, \"Use (text) to make model pay more attention to text text and [text] to make it pay less attention\"),\r\n        \"save_txt\": OptionInfo(False, \"Create a text file next to every image with generation parameters.\"),\r\n        \"ESRGAN_tile\": OptionInfo(192, \"Tile size for upscaling. 0 = no tiling.\", gr.Slider, {\"minimum\": 0, \"maximum\": 512, \"step\": 16}),\r\n        \"ESRGAN_tile_overlap\": OptionInfo(8, \"Tile overlap, in pixels for upscaling. Low values = visible seam.\", gr.Slider, {\"minimum\": 0, \"maximum\": 48, \"step\": 1}),\r\n        \"random_artist_categories\": OptionInfo([], \"Allowed categories for random artists selection when using the Roll button\", gr.CheckboxGroup, {\"choices\": artist_db.categories()}),\r\n        \"upscale_at_full_resolution_padding\": OptionInfo(16, \"Inpainting at full resolution: padding, in pixels, for the masked region.\", gr.Slider, {\"minimum\": 0, \"maximum\": 128, \"step\": 4}),\r\n        \"show_progressbar\": OptionInfo(True, \"Show progressbar\"),\r\n        \"show_progress_every_n_steps\": OptionInfo(0, \"Show show image creation progress every N sampling steps. Set 0 to disable.\", gr.Slider, {\"minimum\": 0, \"maximum\": 32, \"step\": 1}),\r\n        \"multiple_tqdm\": OptionInfo(True, \"Add a second progress bar to the console that shows progress for an entire job. Broken in PyCharm console.\"),\r\n        \"face_restoration_model\": OptionInfo(None, \"Face restoration model\", gr.Radio, lambda: {\"choices\": [x.name() for x in face_restorers]}),\r\n        \"code_former_weight\": OptionInfo(0.5, \"CodeFormer weight parameter; 0 = maximum effect; 1 = minimum effect\", gr.Slider, {\"minimum\": 0, \"maximum\": 1, \"step\": 0.01}),\r\n        \"interrogate_keep_models_in_memory\": OptionInfo(True, \"Interrogate: keep models in VRAM\"),\r\n        \"interrogate_use_builtin_artists\": OptionInfo(True, \"Interrogate: use artists from artists.csv\"),\r\n        \"interrogate_clip_num_beams\": OptionInfo(1, \"Interrogate: num_beams for BLIP\", gr.Slider, {\"minimum\": 1, \"maximum\": 16, \"step\": 1}),\r\n        \"interrogate_clip_min_length\": OptionInfo(24, \"Interrogate: minimum descripton length (excluding artists, etc..)\", gr.Slider, {\"minimum\": 1, \"maximum\": 128, \"step\": 1}),\r\n        \"interrogate_clip_max_length\": OptionInfo(48, \"Interrogate: maximum descripton length\", gr.Slider, {\"minimum\": 1, \"maximum\": 256, \"step\": 1}),\r\n    }\r\n\r\n    def __init__(self):\r\n        self.data = {k: v.default for k, v in self.data_labels.items()}\r\n\r\n    def __setattr__(self, key, value):\r\n        if self.data is not None:\r\n            if key in self.data:\r\n                self.data[key] = value\r\n\r\n        return super(Options, self).__setattr__(key, value)\r\n\r\n    def __getattr__(self, item):\r\n        if self.data is not None:\r\n            if item in self.data:\r\n                return self.data[item]\r\n\r\n        if item in self.data_labels:\r\n            return self.data_labels[item].default\r\n\r\n        return super(Options, self).__getattribute__(item)\r\n\r\n    def save(self, filename):\r\n        with open(filename, \"w\", encoding=\"utf8\") as file:\r\n            json.dump(self.data, file)\r\n\r\n    def load(self, filename):\r\n        with open(filename, \"r\", encoding=\"utf8\") as file:\r\n            self.data = json.load(file)\r\n\r\n\r\nopts = Options()\r\nif os.path.exists(config_filename):\r\n    opts.load(config_filename)\r\n\r\nsd_upscalers = []\r\n\r\nsd_model = None\r\n\r\nprogress_print_out = sys.stdout\r\n\r\n\r\nclass TotalTQDM:\r\n    def __init__(self):\r\n        self._tqdm = None\r\n\r\n    def reset(self):\r\n        self._tqdm = tqdm.tqdm(\r\n            desc=\"Total progress\",\r\n            total=state.job_count * state.sampling_steps,\r\n            position=1,\r\n            file=progress_print_out\r\n        )\r\n\r\n    def update(self):\r\n        if not opts.multiple_tqdm:\r\n            return\r\n        if self._tqdm is None:\r\n            self.reset()\r\n        self._tqdm.update()\r\n\r\n    def clear(self):\r\n        if self._tqdm is not None:\r\n            self._tqdm.close()\r\n            self._tqdm = None\r\n\r\n\r\ntotal_tqdm = TotalTQDM()\r\n"},"\/modules\/ui.py":{"changes":[{"diff":"\n         info = opts.data_labels[key]\r\n         t = type(info.default)\r\n \r\n+        args = info.component_args() if callable(info.component_args) else info.component_args\r\n+\r\n         if info.component is not None:\r\n-            args = info.component_args() if callable(info.component_args) else info.component_args\r\n-            item = info.component(label=info.label, value=fun, **(args or {}))\r\n+            comp = info.component\r\n         elif t == str:\r\n-            item = gr.Textbox(label=info.label, value=fun, lines=1)\r\n+            comp = gr.Textbox\r\n         elif t == int:\r\n-            item = gr.Number(label=info.label, value=fun)\r\n+            comp = gr.Number\r\n         elif t == bool:\r\n-            item = gr.Checkbox(label=info.label, value=fun)\r\n+            comp = gr.Checkbox\r\n         else:\r\n             raise Exception(f'bad options item type: {str(t)} for key {key}')\r\n \r\n-        return item\r\n+        return comp(label=info.label, value=fun, **(args or {}))\r\n \r\n     components = []\r\n     keys = list(opts.data_labels.keys())\r\n","add":7,"remove":6,"filename":"\/modules\/ui.py","badparts":["            args = info.component_args() if callable(info.component_args) else info.component_args\r","            item = info.component(label=info.label, value=fun, **(args or {}))\r","            item = gr.Textbox(label=info.label, value=fun, lines=1)\r","            item = gr.Number(label=info.label, value=fun)\r","            item = gr.Checkbox(label=info.label, value=fun)\r","        return item\r"],"goodparts":["        args = info.component_args() if callable(info.component_args) else info.component_args\r","\r","            comp = info.component\r","            comp = gr.Textbox\r","            comp = gr.Number\r","            comp = gr.Checkbox\r","        return comp(label=info.label, value=fun, **(args or {}))\r"]}]}},"msg":"Add --hide-ui-dir-config command line flag\n\nAdds `--hide-ui-dir-config` flag to disable editing directory configs from the web UI. This can be set to prevent users from setting the directory to somewhere they shouldn't, for public (or semi-public) interfaces.\n\nDirectories are still read from config.json, so the server admin can still set them in the web UI and then relaunch with the hide flag, or edit the config manually.\n\nAlso:\n- fix OptionInfo `component_args` keyword argument not being read if `component` isn't also set\n- ensure that hidden settings aren't still read from the web UI (otherwise they could still be changed by tampering with the interface)"}},"https:\/\/github.com\/syeuk2002\/SD-111":{"29a2933e23e68900bbae741a98439d0c9d6f26f6":{"url":"https:\/\/api.github.com\/repos\/syeuk2002\/SD-111\/commits\/29a2933e23e68900bbae741a98439d0c9d6f26f6","html_url":"https:\/\/github.com\/syeuk2002\/SD-111\/commit\/29a2933e23e68900bbae741a98439d0c9d6f26f6","message":"Add --hide-ui-dir-config command line flag\n\nAdds `--hide-ui-dir-config` flag to disable editing directory configs from the web UI. This can be set to prevent users from setting the directory to somewhere they shouldn't, for public (or semi-public) interfaces.\n\nDirectories are still read from config.json, so the server admin can still set them in the web UI and then relaunch with the hide flag, or edit the config manually.\n\nAlso:\n- fix OptionInfo `component_args` keyword argument not being read if `component` isn't also set\n- ensure that hidden settings aren't still read from the web UI (otherwise they could still be changed by tampering with the interface)","sha":"29a2933e23e68900bbae741a98439d0c9d6f26f6","keyword":"tampering prevent","diff":"diff --git a\/modules\/shared.py b\/modules\/shared.py\nindex 9eeb64e..0557cfe 100644\n--- a\/modules\/shared.py\n+++ b\/modules\/shared.py\n@@ -42,6 +42,7 @@\n parser.add_argument(\"--port\", type=int, help=\"launch gradio with given server port, you need root\/admin rights for ports < 1024, defaults to 7860 if available\", default=None)\r\n parser.add_argument(\"--show-negative-prompt\", action='store_true', help=\"does not do anything\", default=False)\r\n parser.add_argument(\"--ui-config-file\", type=str, help=\"filename to use for ui configuration\", default=os.path.join(script_path, 'ui-config.json'))\r\n+parser.add_argument(\"--hide-ui-dir-config\", action='store_true', help=\"hide directory configuration from webui\", default=False)\r\n \r\n cmd_opts = parser.parse_args()\r\n \r\n@@ -91,18 +92,19 @@ def __init__(self, default=None, label=\"\", component=None, component_args=None):\n             self.component_args = component_args\r\n \r\n     data = None\r\n+    hide_dirs = {\"visible\": False} if cmd_opts.hide_ui_dir_config else None\r\n     data_labels = {\r\n-        \"outdir_samples\": OptionInfo(\"\", \"Output directory for images; if empty, defaults to two directories below\"),\r\n-        \"outdir_txt2img_samples\": OptionInfo(\"outputs\/txt2img-images\", 'Output directory for txt2img images'),\r\n-        \"outdir_img2img_samples\": OptionInfo(\"outputs\/img2img-images\", 'Output directory for img2img images'),\r\n-        \"outdir_extras_samples\": OptionInfo(\"outputs\/extras-images\", 'Output directory for images from extras tab'),\r\n-        \"outdir_grids\": OptionInfo(\"\", \"Output directory for grids; if empty, defaults to two directories below\"),\r\n-        \"outdir_txt2img_grids\": OptionInfo(\"outputs\/txt2img-grids\", 'Output directory for txt2img grids'),\r\n-        \"outdir_img2img_grids\": OptionInfo(\"outputs\/img2img-grids\", 'Output directory for img2img grids'),\r\n+        \"outdir_samples\": OptionInfo(\"\", \"Output directory for images; if empty, defaults to two directories below\", component_args=hide_dirs),\r\n+        \"outdir_txt2img_samples\": OptionInfo(\"outputs\/txt2img-images\", 'Output directory for txt2img images', component_args=hide_dirs),\r\n+        \"outdir_img2img_samples\": OptionInfo(\"outputs\/img2img-images\", 'Output directory for img2img images', component_args=hide_dirs),\r\n+        \"outdir_extras_samples\": OptionInfo(\"outputs\/extras-images\", 'Output directory for images from extras tab', component_args=hide_dirs),\r\n+        \"outdir_grids\": OptionInfo(\"\", \"Output directory for grids; if empty, defaults to two directories below\", component_args=hide_dirs),\r\n+        \"outdir_txt2img_grids\": OptionInfo(\"outputs\/txt2img-grids\", 'Output directory for txt2img grids', component_args=hide_dirs),\r\n+        \"outdir_img2img_grids\": OptionInfo(\"outputs\/img2img-grids\", 'Output directory for img2img grids', component_args=hide_dirs),\r\n         \"save_to_dirs\": OptionInfo(False, \"When writing images, create a directory with name derived from the prompt\"),\r\n         \"grid_save_to_dirs\": OptionInfo(False, \"When writing grids, create a directory with name derived from the prompt\"),\r\n         \"save_to_dirs_prompt_len\": OptionInfo(10, \"When using above, how many words from prompt to put into directory name\", gr.Slider, {\"minimum\": 1, \"maximum\": 32, \"step\": 1}),\r\n-        \"outdir_save\": OptionInfo(\"log\/images\", \"Directory for saving images using the Save button\"),\r\n+        \"outdir_save\": OptionInfo(\"log\/images\", \"Directory for saving images using the Save button\", component_args=hide_dirs),\r\n         \"samples_save\": OptionInfo(True, \"Save indiviual samples\"),\r\n         \"samples_format\": OptionInfo('png', 'File format for individual samples'),\r\n         \"grid_save\": OptionInfo(True, \"Save image grids\"),\r\ndiff --git a\/modules\/ui.py b\/modules\/ui.py\nindex b9af2c8..3b7eb9b 100644\n--- a\/modules\/ui.py\n+++ b\/modules\/ui.py\n@@ -661,19 +661,20 @@ def fun():\n         info = opts.data_labels[key]\r\n         t = type(info.default)\r\n \r\n+        args = info.component_args() if callable(info.component_args) else info.component_args\r\n+\r\n         if info.component is not None:\r\n-            args = info.component_args() if callable(info.component_args) else info.component_args\r\n-            item = info.component(label=info.label, value=fun, **(args or {}))\r\n+            comp = info.component\r\n         elif t == str:\r\n-            item = gr.Textbox(label=info.label, value=fun, lines=1)\r\n+            comp = gr.Textbox\r\n         elif t == int:\r\n-            item = gr.Number(label=info.label, value=fun)\r\n+            comp = gr.Number\r\n         elif t == bool:\r\n-            item = gr.Checkbox(label=info.label, value=fun)\r\n+            comp = gr.Checkbox\r\n         else:\r\n             raise Exception(f'bad options item type: {str(t)} for key {key}')\r\n \r\n-        return item\r\n+        return comp(label=info.label, value=fun, **(args or {}))\r\n \r\n     components = []\r\n     keys = list(opts.data_labels.keys())\r\n@@ -684,6 +685,10 @@ def run_settings(*args):\n         up = []\r\n \r\n         for key, value, comp in zip(opts.data_labels.keys(), args, components):\r\n+            comp_args = opts.data_labels[key].component_args\r\n+            if comp_args and isinstance(comp_args, dict) and comp_args.get('visible') is False:\r\n+                continue\r\n+\r\n             opts.data[key] = value\r\n             up.append(comp.update(value=value))\r\n \r\n","files":{"\/modules\/shared.py":{"changes":[{"diff":"\n             self.component_args = component_args\r\n \r\n     data = None\r\n+    hide_dirs = {\"visible\": False} if cmd_opts.hide_ui_dir_config else None\r\n     data_labels = {\r\n-        \"outdir_samples\": OptionInfo(\"\", \"Output directory for images; if empty, defaults to two directories below\"),\r\n-        \"outdir_txt2img_samples\": OptionInfo(\"outputs\/txt2img-images\", 'Output directory for txt2img images'),\r\n-        \"outdir_img2img_samples\": OptionInfo(\"outputs\/img2img-images\", 'Output directory for img2img images'),\r\n-        \"outdir_extras_samples\": OptionInfo(\"outputs\/extras-images\", 'Output directory for images from extras tab'),\r\n-        \"outdir_grids\": OptionInfo(\"\", \"Output directory for grids; if empty, defaults to two directories below\"),\r\n-        \"outdir_txt2img_grids\": OptionInfo(\"outputs\/txt2img-grids\", 'Output directory for txt2img grids'),\r\n-        \"outdir_img2img_grids\": OptionInfo(\"outputs\/img2img-grids\", 'Output directory for img2img grids'),\r\n+        \"outdir_samples\": OptionInfo(\"\", \"Output directory for images; if empty, defaults to two directories below\", component_args=hide_dirs),\r\n+        \"outdir_txt2img_samples\": OptionInfo(\"outputs\/txt2img-images\", 'Output directory for txt2img images', component_args=hide_dirs),\r\n+        \"outdir_img2img_samples\": OptionInfo(\"outputs\/img2img-images\", 'Output directory for img2img images', component_args=hide_dirs),\r\n+        \"outdir_extras_samples\": OptionInfo(\"outputs\/extras-images\", 'Output directory for images from extras tab', component_args=hide_dirs),\r\n+        \"outdir_grids\": OptionInfo(\"\", \"Output directory for grids; if empty, defaults to two directories below\", component_args=hide_dirs),\r\n+        \"outdir_txt2img_grids\": OptionInfo(\"outputs\/txt2img-grids\", 'Output directory for txt2img grids', component_args=hide_dirs),\r\n+        \"outdir_img2img_grids\": OptionInfo(\"outputs\/img2img-grids\", 'Output directory for img2img grids', component_args=hide_dirs),\r\n         \"save_to_dirs\": OptionInfo(False, \"When writing images, create a directory with name derived from the prompt\"),\r\n         \"grid_save_to_dirs\": OptionInfo(False, \"When writing grids, create a directory with name derived from the prompt\"),\r\n         \"save_to_dirs_prompt_len\": OptionInfo(10, \"When using above, how many words from prompt to put into directory name\", gr.Slider, {\"minimum\": 1, \"maximum\": 32, \"step\": 1}),\r\n-        \"outdir_save\": OptionInfo(\"log\/images\", \"Directory for saving images using the Save button\"),\r\n+        \"outdir_save\": OptionInfo(\"log\/images\", \"Directory for saving images using the Save button\", component_args=hide_dirs),\r\n         \"samples_save\": OptionInfo(True, \"Save indiviual samples\"),\r\n         \"samples_format\": OptionInfo('png', 'File format for individual samples'),\r\n         \"grid_save\": OptionInfo(True, \"Save image grids\"),\r","add":9,"remove":8,"filename":"\/modules\/shared.py","badparts":["        \"outdir_samples\": OptionInfo(\"\", \"Output directory for images; if empty, defaults to two directories below\"),\r","        \"outdir_txt2img_samples\": OptionInfo(\"outputs\/txt2img-images\", 'Output directory for txt2img images'),\r","        \"outdir_img2img_samples\": OptionInfo(\"outputs\/img2img-images\", 'Output directory for img2img images'),\r","        \"outdir_extras_samples\": OptionInfo(\"outputs\/extras-images\", 'Output directory for images from extras tab'),\r","        \"outdir_grids\": OptionInfo(\"\", \"Output directory for grids; if empty, defaults to two directories below\"),\r","        \"outdir_txt2img_grids\": OptionInfo(\"outputs\/txt2img-grids\", 'Output directory for txt2img grids'),\r","        \"outdir_img2img_grids\": OptionInfo(\"outputs\/img2img-grids\", 'Output directory for img2img grids'),\r","        \"outdir_save\": OptionInfo(\"log\/images\", \"Directory for saving images using the Save button\"),\r"],"goodparts":["    hide_dirs = {\"visible\": False} if cmd_opts.hide_ui_dir_config else None\r","        \"outdir_samples\": OptionInfo(\"\", \"Output directory for images; if empty, defaults to two directories below\", component_args=hide_dirs),\r","        \"outdir_txt2img_samples\": OptionInfo(\"outputs\/txt2img-images\", 'Output directory for txt2img images', component_args=hide_dirs),\r","        \"outdir_img2img_samples\": OptionInfo(\"outputs\/img2img-images\", 'Output directory for img2img images', component_args=hide_dirs),\r","        \"outdir_extras_samples\": OptionInfo(\"outputs\/extras-images\", 'Output directory for images from extras tab', component_args=hide_dirs),\r","        \"outdir_grids\": OptionInfo(\"\", \"Output directory for grids; if empty, defaults to two directories below\", component_args=hide_dirs),\r","        \"outdir_txt2img_grids\": OptionInfo(\"outputs\/txt2img-grids\", 'Output directory for txt2img grids', component_args=hide_dirs),\r","        \"outdir_img2img_grids\": OptionInfo(\"outputs\/img2img-grids\", 'Output directory for img2img grids', component_args=hide_dirs),\r","        \"outdir_save\": OptionInfo(\"log\/images\", \"Directory for saving images using the Save button\", component_args=hide_dirs),\r"]}],"source":"\nimport sys\r import argparse\r import json\r import os\r \r import gradio as gr\r import torch\r import tqdm\r \r import modules.artists\r from modules.paths import script_path, sd_path\r from modules.devices import get_optimal_device\r import modules.styles\r import modules.interrogate\r \r config_filename=\"config.json\"\r \r sd_model_file=os.path.join(script_path, 'model.ckpt')\r if not os.path.exists(sd_model_file):\r sd_model_file=\"models\/ldm\/stable-diffusion-v1\/model.ckpt\"\r \r parser=argparse.ArgumentParser()\r parser.add_argument(\"--config\", type=str, default=os.path.join(sd_path, \"configs\/stable-diffusion\/v1-inference.yaml\"), help=\"path to config which constructs model\",)\r parser.add_argument(\"--ckpt\", type=str, default=os.path.join(sd_path, sd_model_file), help=\"path to checkpoint of model\",)\r parser.add_argument(\"--gfpgan-dir\", type=str, help=\"GFPGAN directory\", default=('.\/src\/gfpgan' if os.path.exists('.\/src\/gfpgan') else '.\/GFPGAN'))\r parser.add_argument(\"--gfpgan-model\", type=str, help=\"GFPGAN model file name\", default='GFPGANv1.3.pth')\r parser.add_argument(\"--no-half\", action='store_true', help=\"do not switch the model to 16-bit floats\")\r parser.add_argument(\"--no-progressbar-hiding\", action='store_true', help=\"do not hide progressbar in gradio UI(we hide it because it slows down ML if you have hardware accleration in browser)\")\r parser.add_argument(\"--max-batch-count\", type=int, default=16, help=\"maximum batch count value for the UI\")\r parser.add_argument(\"--embeddings-dir\", type=str, default='embeddings', help=\"embeddings directory for textual inversion(default: embeddings)\")\r parser.add_argument(\"--allow-code\", action='store_true', help=\"allow custom script execution from webui\")\r parser.add_argument(\"--medvram\", action='store_true', help=\"enable stable diffusion model optimizations for sacrificing a little speed for low VRM usage\")\r parser.add_argument(\"--lowvram\", action='store_true', help=\"enable stable diffusion model optimizations for sacrificing a lot of speed for very low VRM usage\")\r parser.add_argument(\"--always-batch-cond-uncond\", action='store_true', help=\"a workaround test; may help with speed if you use --lowvram\")\r parser.add_argument(\"--unload-gfpgan\", action='store_true', help=\"unload GFPGAN every time after processing images. Warning: seems to cause memory leaks\")\r parser.add_argument(\"--precision\", type=str, help=\"evaluate at this precision\", choices=[\"full\", \"autocast\"], default=\"autocast\")\r parser.add_argument(\"--share\", action='store_true', help=\"use share=True for gradio and make the UI accessible through their site(doesn't work for me but you might have better luck)\")\r parser.add_argument(\"--esrgan-models-path\", type=str, help=\"path to directory with ESRGAN models\", default=os.path.join(script_path, 'ESRGAN'))\r parser.add_argument(\"--opt-split-attention\", action='store_true', help=\"enable optimization that reduce vram usage by a lot for about 10%% decrease in performance\")\r parser.add_argument(\"--opt-split-attention-v1\", action='store_true', help=\"enable older version of --opt-split-attention optimization\")\r parser.add_argument(\"--listen\", action='store_true', help=\"launch gradio with 0.0.0.0 as server name, allowing to respond to network requests\")\r parser.add_argument(\"--port\", type=int, help=\"launch gradio with given server port, you need root\/admin rights for ports < 1024, defaults to 7860 if available\", default=None)\r parser.add_argument(\"--show-negative-prompt\", action='store_true', help=\"does not do anything\", default=False)\r parser.add_argument(\"--ui-config-file\", type=str, help=\"filename to use for ui configuration\", default=os.path.join(script_path, 'ui-config.json'))\r \r cmd_opts=parser.parse_args()\r \r device=get_optimal_device()\r \r batch_cond_uncond=cmd_opts.always_batch_cond_uncond or not(cmd_opts.lowvram or cmd_opts.medvram)\r parallel_processing_allowed=not cmd_opts.lowvram and not cmd_opts.medvram\r \r \r class State:\r interrupted=False\r job=\"\"\r job_no=0\r job_count=0\r sampling_step=0\r sampling_steps=0\r current_latent=None\r current_image=None\r current_image_sampling_step=0\r \r def interrupt(self):\r self.interrupted=True\r \r def nextjob(self):\r self.job_no +=1\r self.sampling_step=0\r self.current_image_sampling_step=0\r \r \r state=State()\r \r artist_db=modules.artists.ArtistsDatabase(os.path.join(script_path, 'artists.csv'))\r \r styles_filename=os.path.join(script_path, 'styles.csv')\r prompt_styles=modules.styles.load_styles(styles_filename)\r \r interrogator=modules.interrogate.InterrogateModels(\"interrogate\")\r \r face_restorers=[]\r \r class Options:\r class OptionInfo:\r def __init__(self, default=None, label=\"\", component=None, component_args=None):\r self.default=default\r self.label=label\r self.component=component\r self.component_args=component_args\r \r data=None\r data_labels={\r \"outdir_samples\": OptionInfo(\"\", \"Output directory for images; if empty, defaults to two directories below\"),\r \"outdir_txt2img_samples\": OptionInfo(\"outputs\/txt2img-images\", 'Output directory for txt2img images'),\r \"outdir_img2img_samples\": OptionInfo(\"outputs\/img2img-images\", 'Output directory for img2img images'),\r \"outdir_extras_samples\": OptionInfo(\"outputs\/extras-images\", 'Output directory for images from extras tab'),\r \"outdir_grids\": OptionInfo(\"\", \"Output directory for grids; if empty, defaults to two directories below\"),\r \"outdir_txt2img_grids\": OptionInfo(\"outputs\/txt2img-grids\", 'Output directory for txt2img grids'),\r \"outdir_img2img_grids\": OptionInfo(\"outputs\/img2img-grids\", 'Output directory for img2img grids'),\r \"save_to_dirs\": OptionInfo(False, \"When writing images, create a directory with name derived from the prompt\"),\r \"grid_save_to_dirs\": OptionInfo(False, \"When writing grids, create a directory with name derived from the prompt\"),\r \"save_to_dirs_prompt_len\": OptionInfo(10, \"When using above, how many words from prompt to put into directory name\", gr.Slider,{\"minimum\": 1, \"maximum\": 32, \"step\": 1}),\r \"outdir_save\": OptionInfo(\"log\/images\", \"Directory for saving images using the Save button\"),\r \"samples_save\": OptionInfo(True, \"Save indiviual samples\"),\r \"samples_format\": OptionInfo('png', 'File format for individual samples'),\r \"grid_save\": OptionInfo(True, \"Save image grids\"),\r \"return_grid\": OptionInfo(True, \"Show grid in results for web\"),\r \"grid_format\": OptionInfo('png', 'File format for grids'),\r \"grid_extended_filename\": OptionInfo(False, \"Add extended info(seed, prompt) to filename when saving grid\"),\r \"grid_only_if_multiple\": OptionInfo(True, \"Do not save grids consisting of one picture\"),\r \"n_rows\": OptionInfo(-1, \"Grid row count; use -1 for autodetect and 0 for it to be same as batch size\", gr.Slider,{\"minimum\": -1, \"maximum\": 16, \"step\": 1}),\r \"jpeg_quality\": OptionInfo(80, \"Quality for saved jpeg images\", gr.Slider,{\"minimum\": 1, \"maximum\": 100, \"step\": 1}),\r \"export_for_4chan\": OptionInfo(True, \"If PNG image is larger than 4MB or any dimension is larger than 4000, downscale and save copy as JPG\"),\r \"enable_pnginfo\": OptionInfo(True, \"Save text information about generation parameters as chunks to png files\"),\r \"font\": OptionInfo(\"\", \"Font for image grids that have text\"),\r \"enable_emphasis\": OptionInfo(True, \"Use(text) to make model pay more attention to text text and[text] to make it pay less attention\"),\r \"save_txt\": OptionInfo(False, \"Create a text file next to every image with generation parameters.\"),\r \"ESRGAN_tile\": OptionInfo(192, \"Tile size for upscaling. 0=no tiling.\", gr.Slider,{\"minimum\": 0, \"maximum\": 512, \"step\": 16}),\r \"ESRGAN_tile_overlap\": OptionInfo(8, \"Tile overlap, in pixels for upscaling. Low values=visible seam.\", gr.Slider,{\"minimum\": 0, \"maximum\": 48, \"step\": 1}),\r \"random_artist_categories\": OptionInfo([], \"Allowed categories for random artists selection when using the Roll button\", gr.CheckboxGroup,{\"choices\": artist_db.categories()}),\r \"upscale_at_full_resolution_padding\": OptionInfo(16, \"Inpainting at full resolution: padding, in pixels, for the masked region.\", gr.Slider,{\"minimum\": 0, \"maximum\": 128, \"step\": 4}),\r \"show_progressbar\": OptionInfo(True, \"Show progressbar\"),\r \"show_progress_every_n_steps\": OptionInfo(0, \"Show show image creation progress every N sampling steps. Set 0 to disable.\", gr.Slider,{\"minimum\": 0, \"maximum\": 32, \"step\": 1}),\r \"multiple_tqdm\": OptionInfo(True, \"Add a second progress bar to the console that shows progress for an entire job. Broken in PyCharm console.\"),\r \"face_restoration_model\": OptionInfo(None, \"Face restoration model\", gr.Radio, lambda:{\"choices\":[x.name() for x in face_restorers]}),\r \"code_former_weight\": OptionInfo(0.5, \"CodeFormer weight parameter; 0=maximum effect; 1=minimum effect\", gr.Slider,{\"minimum\": 0, \"maximum\": 1, \"step\": 0.01}),\r \"interrogate_keep_models_in_memory\": OptionInfo(True, \"Interrogate: keep models in VRAM\"),\r \"interrogate_use_builtin_artists\": OptionInfo(True, \"Interrogate: use artists from artists.csv\"),\r \"interrogate_clip_num_beams\": OptionInfo(1, \"Interrogate: num_beams for BLIP\", gr.Slider,{\"minimum\": 1, \"maximum\": 16, \"step\": 1}),\r \"interrogate_clip_min_length\": OptionInfo(24, \"Interrogate: minimum descripton length(excluding artists, etc..)\", gr.Slider,{\"minimum\": 1, \"maximum\": 128, \"step\": 1}),\r \"interrogate_clip_max_length\": OptionInfo(48, \"Interrogate: maximum descripton length\", gr.Slider,{\"minimum\": 1, \"maximum\": 256, \"step\": 1}),\r }\r \r def __init__(self):\r self.data={k: v.default for k, v in self.data_labels.items()}\r \r def __setattr__(self, key, value):\r if self.data is not None:\r if key in self.data:\r self.data[key]=value\r \r return super(Options, self).__setattr__(key, value)\r \r def __getattr__(self, item):\r if self.data is not None:\r if item in self.data:\r return self.data[item]\r \r if item in self.data_labels:\r return self.data_labels[item].default\r \r return super(Options, self).__getattribute__(item)\r \r def save(self, filename):\r with open(filename, \"w\", encoding=\"utf8\") as file:\r json.dump(self.data, file)\r \r def load(self, filename):\r with open(filename, \"r\", encoding=\"utf8\") as file:\r self.data=json.load(file)\r \r \r opts=Options()\r if os.path.exists(config_filename):\r opts.load(config_filename)\r \r sd_upscalers=[]\r \r sd_model=None\r \r progress_print_out=sys.stdout\r \r \r class TotalTQDM:\r def __init__(self):\r self._tqdm=None\r \r def reset(self):\r self._tqdm=tqdm.tqdm(\r desc=\"Total progress\",\r total=state.job_count * state.sampling_steps,\r position=1,\r file=progress_print_out\r )\r \r def update(self):\r if not opts.multiple_tqdm:\r return\r if self._tqdm is None:\r self.reset()\r self._tqdm.update()\r \r def clear(self):\r if self._tqdm is not None:\r self._tqdm.close()\r self._tqdm=None\r \r \r total_tqdm=TotalTQDM()\r ","sourceWithComments":"import sys\r\nimport argparse\r\nimport json\r\nimport os\r\n\r\nimport gradio as gr\r\nimport torch\r\nimport tqdm\r\n\r\nimport modules.artists\r\nfrom modules.paths import script_path, sd_path\r\nfrom modules.devices import get_optimal_device\r\nimport modules.styles\r\nimport modules.interrogate\r\n\r\nconfig_filename = \"config.json\"\r\n\r\nsd_model_file = os.path.join(script_path, 'model.ckpt')\r\nif not os.path.exists(sd_model_file):\r\n    sd_model_file = \"models\/ldm\/stable-diffusion-v1\/model.ckpt\"\r\n\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument(\"--config\", type=str, default=os.path.join(sd_path, \"configs\/stable-diffusion\/v1-inference.yaml\"), help=\"path to config which constructs model\",)\r\nparser.add_argument(\"--ckpt\", type=str, default=os.path.join(sd_path, sd_model_file), help=\"path to checkpoint of model\",)\r\nparser.add_argument(\"--gfpgan-dir\", type=str, help=\"GFPGAN directory\", default=('.\/src\/gfpgan' if os.path.exists('.\/src\/gfpgan') else '.\/GFPGAN'))\r\nparser.add_argument(\"--gfpgan-model\", type=str, help=\"GFPGAN model file name\", default='GFPGANv1.3.pth')\r\nparser.add_argument(\"--no-half\", action='store_true', help=\"do not switch the model to 16-bit floats\")\r\nparser.add_argument(\"--no-progressbar-hiding\", action='store_true', help=\"do not hide progressbar in gradio UI (we hide it because it slows down ML if you have hardware accleration in browser)\")\r\nparser.add_argument(\"--max-batch-count\", type=int, default=16, help=\"maximum batch count value for the UI\")\r\nparser.add_argument(\"--embeddings-dir\", type=str, default='embeddings', help=\"embeddings directory for textual inversion (default: embeddings)\")\r\nparser.add_argument(\"--allow-code\", action='store_true', help=\"allow custom script execution from webui\")\r\nparser.add_argument(\"--medvram\", action='store_true', help=\"enable stable diffusion model optimizations for sacrificing a little speed for low VRM usage\")\r\nparser.add_argument(\"--lowvram\", action='store_true', help=\"enable stable diffusion model optimizations for sacrificing a lot of speed for very low VRM usage\")\r\nparser.add_argument(\"--always-batch-cond-uncond\", action='store_true', help=\"a workaround test; may help with speed if you use --lowvram\")\r\nparser.add_argument(\"--unload-gfpgan\", action='store_true', help=\"unload GFPGAN every time after processing images. Warning: seems to cause memory leaks\")\r\nparser.add_argument(\"--precision\", type=str, help=\"evaluate at this precision\", choices=[\"full\", \"autocast\"], default=\"autocast\")\r\nparser.add_argument(\"--share\", action='store_true', help=\"use share=True for gradio and make the UI accessible through their site (doesn't work for me but you might have better luck)\")\r\nparser.add_argument(\"--esrgan-models-path\", type=str, help=\"path to directory with ESRGAN models\", default=os.path.join(script_path, 'ESRGAN'))\r\nparser.add_argument(\"--opt-split-attention\", action='store_true', help=\"enable optimization that reduce vram usage by a lot for about 10%% decrease in performance\")\r\nparser.add_argument(\"--opt-split-attention-v1\", action='store_true', help=\"enable older version of --opt-split-attention optimization\")\r\nparser.add_argument(\"--listen\", action='store_true', help=\"launch gradio with 0.0.0.0 as server name, allowing to respond to network requests\")\r\nparser.add_argument(\"--port\", type=int, help=\"launch gradio with given server port, you need root\/admin rights for ports < 1024, defaults to 7860 if available\", default=None)\r\nparser.add_argument(\"--show-negative-prompt\", action='store_true', help=\"does not do anything\", default=False)\r\nparser.add_argument(\"--ui-config-file\", type=str, help=\"filename to use for ui configuration\", default=os.path.join(script_path, 'ui-config.json'))\r\n\r\ncmd_opts = parser.parse_args()\r\n\r\ndevice = get_optimal_device()\r\n\r\nbatch_cond_uncond = cmd_opts.always_batch_cond_uncond or not (cmd_opts.lowvram or cmd_opts.medvram)\r\nparallel_processing_allowed = not cmd_opts.lowvram and not cmd_opts.medvram\r\n\r\n\r\nclass State:\r\n    interrupted = False\r\n    job = \"\"\r\n    job_no = 0\r\n    job_count = 0\r\n    sampling_step = 0\r\n    sampling_steps = 0\r\n    current_latent = None\r\n    current_image = None\r\n    current_image_sampling_step = 0\r\n\r\n    def interrupt(self):\r\n        self.interrupted = True\r\n\r\n    def nextjob(self):\r\n        self.job_no += 1\r\n        self.sampling_step = 0\r\n        self.current_image_sampling_step = 0\r\n\r\n\r\nstate = State()\r\n\r\nartist_db = modules.artists.ArtistsDatabase(os.path.join(script_path, 'artists.csv'))\r\n\r\nstyles_filename = os.path.join(script_path, 'styles.csv')\r\nprompt_styles = modules.styles.load_styles(styles_filename)\r\n\r\ninterrogator = modules.interrogate.InterrogateModels(\"interrogate\")\r\n\r\nface_restorers = []\r\n\r\nclass Options:\r\n    class OptionInfo:\r\n        def __init__(self, default=None, label=\"\", component=None, component_args=None):\r\n            self.default = default\r\n            self.label = label\r\n            self.component = component\r\n            self.component_args = component_args\r\n\r\n    data = None\r\n    data_labels = {\r\n        \"outdir_samples\": OptionInfo(\"\", \"Output directory for images; if empty, defaults to two directories below\"),\r\n        \"outdir_txt2img_samples\": OptionInfo(\"outputs\/txt2img-images\", 'Output directory for txt2img images'),\r\n        \"outdir_img2img_samples\": OptionInfo(\"outputs\/img2img-images\", 'Output directory for img2img images'),\r\n        \"outdir_extras_samples\": OptionInfo(\"outputs\/extras-images\", 'Output directory for images from extras tab'),\r\n        \"outdir_grids\": OptionInfo(\"\", \"Output directory for grids; if empty, defaults to two directories below\"),\r\n        \"outdir_txt2img_grids\": OptionInfo(\"outputs\/txt2img-grids\", 'Output directory for txt2img grids'),\r\n        \"outdir_img2img_grids\": OptionInfo(\"outputs\/img2img-grids\", 'Output directory for img2img grids'),\r\n        \"save_to_dirs\": OptionInfo(False, \"When writing images, create a directory with name derived from the prompt\"),\r\n        \"grid_save_to_dirs\": OptionInfo(False, \"When writing grids, create a directory with name derived from the prompt\"),\r\n        \"save_to_dirs_prompt_len\": OptionInfo(10, \"When using above, how many words from prompt to put into directory name\", gr.Slider, {\"minimum\": 1, \"maximum\": 32, \"step\": 1}),\r\n        \"outdir_save\": OptionInfo(\"log\/images\", \"Directory for saving images using the Save button\"),\r\n        \"samples_save\": OptionInfo(True, \"Save indiviual samples\"),\r\n        \"samples_format\": OptionInfo('png', 'File format for individual samples'),\r\n        \"grid_save\": OptionInfo(True, \"Save image grids\"),\r\n        \"return_grid\": OptionInfo(True, \"Show grid in results for web\"),\r\n        \"grid_format\": OptionInfo('png', 'File format for grids'),\r\n        \"grid_extended_filename\": OptionInfo(False, \"Add extended info (seed, prompt) to filename when saving grid\"),\r\n        \"grid_only_if_multiple\": OptionInfo(True, \"Do not save grids consisting of one picture\"),\r\n        \"n_rows\": OptionInfo(-1, \"Grid row count; use -1 for autodetect and 0 for it to be same as batch size\", gr.Slider, {\"minimum\": -1, \"maximum\": 16, \"step\": 1}),\r\n        \"jpeg_quality\": OptionInfo(80, \"Quality for saved jpeg images\", gr.Slider, {\"minimum\": 1, \"maximum\": 100, \"step\": 1}),\r\n        \"export_for_4chan\": OptionInfo(True, \"If PNG image is larger than 4MB or any dimension is larger than 4000, downscale and save copy as JPG\"),\r\n        \"enable_pnginfo\": OptionInfo(True, \"Save text information about generation parameters as chunks to png files\"),\r\n        \"font\": OptionInfo(\"\", \"Font for image grids that have text\"),\r\n        \"enable_emphasis\": OptionInfo(True, \"Use (text) to make model pay more attention to text text and [text] to make it pay less attention\"),\r\n        \"save_txt\": OptionInfo(False, \"Create a text file next to every image with generation parameters.\"),\r\n        \"ESRGAN_tile\": OptionInfo(192, \"Tile size for upscaling. 0 = no tiling.\", gr.Slider, {\"minimum\": 0, \"maximum\": 512, \"step\": 16}),\r\n        \"ESRGAN_tile_overlap\": OptionInfo(8, \"Tile overlap, in pixels for upscaling. Low values = visible seam.\", gr.Slider, {\"minimum\": 0, \"maximum\": 48, \"step\": 1}),\r\n        \"random_artist_categories\": OptionInfo([], \"Allowed categories for random artists selection when using the Roll button\", gr.CheckboxGroup, {\"choices\": artist_db.categories()}),\r\n        \"upscale_at_full_resolution_padding\": OptionInfo(16, \"Inpainting at full resolution: padding, in pixels, for the masked region.\", gr.Slider, {\"minimum\": 0, \"maximum\": 128, \"step\": 4}),\r\n        \"show_progressbar\": OptionInfo(True, \"Show progressbar\"),\r\n        \"show_progress_every_n_steps\": OptionInfo(0, \"Show show image creation progress every N sampling steps. Set 0 to disable.\", gr.Slider, {\"minimum\": 0, \"maximum\": 32, \"step\": 1}),\r\n        \"multiple_tqdm\": OptionInfo(True, \"Add a second progress bar to the console that shows progress for an entire job. Broken in PyCharm console.\"),\r\n        \"face_restoration_model\": OptionInfo(None, \"Face restoration model\", gr.Radio, lambda: {\"choices\": [x.name() for x in face_restorers]}),\r\n        \"code_former_weight\": OptionInfo(0.5, \"CodeFormer weight parameter; 0 = maximum effect; 1 = minimum effect\", gr.Slider, {\"minimum\": 0, \"maximum\": 1, \"step\": 0.01}),\r\n        \"interrogate_keep_models_in_memory\": OptionInfo(True, \"Interrogate: keep models in VRAM\"),\r\n        \"interrogate_use_builtin_artists\": OptionInfo(True, \"Interrogate: use artists from artists.csv\"),\r\n        \"interrogate_clip_num_beams\": OptionInfo(1, \"Interrogate: num_beams for BLIP\", gr.Slider, {\"minimum\": 1, \"maximum\": 16, \"step\": 1}),\r\n        \"interrogate_clip_min_length\": OptionInfo(24, \"Interrogate: minimum descripton length (excluding artists, etc..)\", gr.Slider, {\"minimum\": 1, \"maximum\": 128, \"step\": 1}),\r\n        \"interrogate_clip_max_length\": OptionInfo(48, \"Interrogate: maximum descripton length\", gr.Slider, {\"minimum\": 1, \"maximum\": 256, \"step\": 1}),\r\n    }\r\n\r\n    def __init__(self):\r\n        self.data = {k: v.default for k, v in self.data_labels.items()}\r\n\r\n    def __setattr__(self, key, value):\r\n        if self.data is not None:\r\n            if key in self.data:\r\n                self.data[key] = value\r\n\r\n        return super(Options, self).__setattr__(key, value)\r\n\r\n    def __getattr__(self, item):\r\n        if self.data is not None:\r\n            if item in self.data:\r\n                return self.data[item]\r\n\r\n        if item in self.data_labels:\r\n            return self.data_labels[item].default\r\n\r\n        return super(Options, self).__getattribute__(item)\r\n\r\n    def save(self, filename):\r\n        with open(filename, \"w\", encoding=\"utf8\") as file:\r\n            json.dump(self.data, file)\r\n\r\n    def load(self, filename):\r\n        with open(filename, \"r\", encoding=\"utf8\") as file:\r\n            self.data = json.load(file)\r\n\r\n\r\nopts = Options()\r\nif os.path.exists(config_filename):\r\n    opts.load(config_filename)\r\n\r\nsd_upscalers = []\r\n\r\nsd_model = None\r\n\r\nprogress_print_out = sys.stdout\r\n\r\n\r\nclass TotalTQDM:\r\n    def __init__(self):\r\n        self._tqdm = None\r\n\r\n    def reset(self):\r\n        self._tqdm = tqdm.tqdm(\r\n            desc=\"Total progress\",\r\n            total=state.job_count * state.sampling_steps,\r\n            position=1,\r\n            file=progress_print_out\r\n        )\r\n\r\n    def update(self):\r\n        if not opts.multiple_tqdm:\r\n            return\r\n        if self._tqdm is None:\r\n            self.reset()\r\n        self._tqdm.update()\r\n\r\n    def clear(self):\r\n        if self._tqdm is not None:\r\n            self._tqdm.close()\r\n            self._tqdm = None\r\n\r\n\r\ntotal_tqdm = TotalTQDM()\r\n"},"\/modules\/ui.py":{"changes":[{"diff":"\n         info = opts.data_labels[key]\r\n         t = type(info.default)\r\n \r\n+        args = info.component_args() if callable(info.component_args) else info.component_args\r\n+\r\n         if info.component is not None:\r\n-            args = info.component_args() if callable(info.component_args) else info.component_args\r\n-            item = info.component(label=info.label, value=fun, **(args or {}))\r\n+            comp = info.component\r\n         elif t == str:\r\n-            item = gr.Textbox(label=info.label, value=fun, lines=1)\r\n+            comp = gr.Textbox\r\n         elif t == int:\r\n-            item = gr.Number(label=info.label, value=fun)\r\n+            comp = gr.Number\r\n         elif t == bool:\r\n-            item = gr.Checkbox(label=info.label, value=fun)\r\n+            comp = gr.Checkbox\r\n         else:\r\n             raise Exception(f'bad options item type: {str(t)} for key {key}')\r\n \r\n-        return item\r\n+        return comp(label=info.label, value=fun, **(args or {}))\r\n \r\n     components = []\r\n     keys = list(opts.data_labels.keys())\r\n","add":7,"remove":6,"filename":"\/modules\/ui.py","badparts":["            args = info.component_args() if callable(info.component_args) else info.component_args\r","            item = info.component(label=info.label, value=fun, **(args or {}))\r","            item = gr.Textbox(label=info.label, value=fun, lines=1)\r","            item = gr.Number(label=info.label, value=fun)\r","            item = gr.Checkbox(label=info.label, value=fun)\r","        return item\r"],"goodparts":["        args = info.component_args() if callable(info.component_args) else info.component_args\r","\r","            comp = info.component\r","            comp = gr.Textbox\r","            comp = gr.Number\r","            comp = gr.Checkbox\r","        return comp(label=info.label, value=fun, **(args or {}))\r"]}]}},"msg":"Add --hide-ui-dir-config command line flag\n\nAdds `--hide-ui-dir-config` flag to disable editing directory configs from the web UI. This can be set to prevent users from setting the directory to somewhere they shouldn't, for public (or semi-public) interfaces.\n\nDirectories are still read from config.json, so the server admin can still set them in the web UI and then relaunch with the hide flag, or edit the config manually.\n\nAlso:\n- fix OptionInfo `component_args` keyword argument not being read if `component` isn't also set\n- ensure that hidden settings aren't still read from the web UI (otherwise they could still be changed by tampering with the interface)"}},"https:\/\/github.com\/minhanh1114\/text_to_image_ui_customer":{"29a2933e23e68900bbae741a98439d0c9d6f26f6":{"url":"https:\/\/api.github.com\/repos\/minhanh1114\/text_to_image_ui_customer\/commits\/29a2933e23e68900bbae741a98439d0c9d6f26f6","html_url":"https:\/\/github.com\/minhanh1114\/text_to_image_ui_customer\/commit\/29a2933e23e68900bbae741a98439d0c9d6f26f6","message":"Add --hide-ui-dir-config command line flag\n\nAdds `--hide-ui-dir-config` flag to disable editing directory configs from the web UI. This can be set to prevent users from setting the directory to somewhere they shouldn't, for public (or semi-public) interfaces.\n\nDirectories are still read from config.json, so the server admin can still set them in the web UI and then relaunch with the hide flag, or edit the config manually.\n\nAlso:\n- fix OptionInfo `component_args` keyword argument not being read if `component` isn't also set\n- ensure that hidden settings aren't still read from the web UI (otherwise they could still be changed by tampering with the interface)","sha":"29a2933e23e68900bbae741a98439d0c9d6f26f6","keyword":"tampering prevent","diff":"diff --git a\/modules\/shared.py b\/modules\/shared.py\nindex 9eeb64e3..0557cfe3 100644\n--- a\/modules\/shared.py\n+++ b\/modules\/shared.py\n@@ -42,6 +42,7 @@\n parser.add_argument(\"--port\", type=int, help=\"launch gradio with given server port, you need root\/admin rights for ports < 1024, defaults to 7860 if available\", default=None)\r\n parser.add_argument(\"--show-negative-prompt\", action='store_true', help=\"does not do anything\", default=False)\r\n parser.add_argument(\"--ui-config-file\", type=str, help=\"filename to use for ui configuration\", default=os.path.join(script_path, 'ui-config.json'))\r\n+parser.add_argument(\"--hide-ui-dir-config\", action='store_true', help=\"hide directory configuration from webui\", default=False)\r\n \r\n cmd_opts = parser.parse_args()\r\n \r\n@@ -91,18 +92,19 @@ def __init__(self, default=None, label=\"\", component=None, component_args=None):\n             self.component_args = component_args\r\n \r\n     data = None\r\n+    hide_dirs = {\"visible\": False} if cmd_opts.hide_ui_dir_config else None\r\n     data_labels = {\r\n-        \"outdir_samples\": OptionInfo(\"\", \"Output directory for images; if empty, defaults to two directories below\"),\r\n-        \"outdir_txt2img_samples\": OptionInfo(\"outputs\/txt2img-images\", 'Output directory for txt2img images'),\r\n-        \"outdir_img2img_samples\": OptionInfo(\"outputs\/img2img-images\", 'Output directory for img2img images'),\r\n-        \"outdir_extras_samples\": OptionInfo(\"outputs\/extras-images\", 'Output directory for images from extras tab'),\r\n-        \"outdir_grids\": OptionInfo(\"\", \"Output directory for grids; if empty, defaults to two directories below\"),\r\n-        \"outdir_txt2img_grids\": OptionInfo(\"outputs\/txt2img-grids\", 'Output directory for txt2img grids'),\r\n-        \"outdir_img2img_grids\": OptionInfo(\"outputs\/img2img-grids\", 'Output directory for img2img grids'),\r\n+        \"outdir_samples\": OptionInfo(\"\", \"Output directory for images; if empty, defaults to two directories below\", component_args=hide_dirs),\r\n+        \"outdir_txt2img_samples\": OptionInfo(\"outputs\/txt2img-images\", 'Output directory for txt2img images', component_args=hide_dirs),\r\n+        \"outdir_img2img_samples\": OptionInfo(\"outputs\/img2img-images\", 'Output directory for img2img images', component_args=hide_dirs),\r\n+        \"outdir_extras_samples\": OptionInfo(\"outputs\/extras-images\", 'Output directory for images from extras tab', component_args=hide_dirs),\r\n+        \"outdir_grids\": OptionInfo(\"\", \"Output directory for grids; if empty, defaults to two directories below\", component_args=hide_dirs),\r\n+        \"outdir_txt2img_grids\": OptionInfo(\"outputs\/txt2img-grids\", 'Output directory for txt2img grids', component_args=hide_dirs),\r\n+        \"outdir_img2img_grids\": OptionInfo(\"outputs\/img2img-grids\", 'Output directory for img2img grids', component_args=hide_dirs),\r\n         \"save_to_dirs\": OptionInfo(False, \"When writing images, create a directory with name derived from the prompt\"),\r\n         \"grid_save_to_dirs\": OptionInfo(False, \"When writing grids, create a directory with name derived from the prompt\"),\r\n         \"save_to_dirs_prompt_len\": OptionInfo(10, \"When using above, how many words from prompt to put into directory name\", gr.Slider, {\"minimum\": 1, \"maximum\": 32, \"step\": 1}),\r\n-        \"outdir_save\": OptionInfo(\"log\/images\", \"Directory for saving images using the Save button\"),\r\n+        \"outdir_save\": OptionInfo(\"log\/images\", \"Directory for saving images using the Save button\", component_args=hide_dirs),\r\n         \"samples_save\": OptionInfo(True, \"Save indiviual samples\"),\r\n         \"samples_format\": OptionInfo('png', 'File format for individual samples'),\r\n         \"grid_save\": OptionInfo(True, \"Save image grids\"),\r\ndiff --git a\/modules\/ui.py b\/modules\/ui.py\nindex b9af2c86..3b7eb9bb 100644\n--- a\/modules\/ui.py\n+++ b\/modules\/ui.py\n@@ -661,19 +661,20 @@ def fun():\n         info = opts.data_labels[key]\r\n         t = type(info.default)\r\n \r\n+        args = info.component_args() if callable(info.component_args) else info.component_args\r\n+\r\n         if info.component is not None:\r\n-            args = info.component_args() if callable(info.component_args) else info.component_args\r\n-            item = info.component(label=info.label, value=fun, **(args or {}))\r\n+            comp = info.component\r\n         elif t == str:\r\n-            item = gr.Textbox(label=info.label, value=fun, lines=1)\r\n+            comp = gr.Textbox\r\n         elif t == int:\r\n-            item = gr.Number(label=info.label, value=fun)\r\n+            comp = gr.Number\r\n         elif t == bool:\r\n-            item = gr.Checkbox(label=info.label, value=fun)\r\n+            comp = gr.Checkbox\r\n         else:\r\n             raise Exception(f'bad options item type: {str(t)} for key {key}')\r\n \r\n-        return item\r\n+        return comp(label=info.label, value=fun, **(args or {}))\r\n \r\n     components = []\r\n     keys = list(opts.data_labels.keys())\r\n@@ -684,6 +685,10 @@ def run_settings(*args):\n         up = []\r\n \r\n         for key, value, comp in zip(opts.data_labels.keys(), args, components):\r\n+            comp_args = opts.data_labels[key].component_args\r\n+            if comp_args and isinstance(comp_args, dict) and comp_args.get('visible') is False:\r\n+                continue\r\n+\r\n             opts.data[key] = value\r\n             up.append(comp.update(value=value))\r\n \r\n","files":{"\/modules\/shared.py":{"changes":[{"diff":"\n             self.component_args = component_args\r\n \r\n     data = None\r\n+    hide_dirs = {\"visible\": False} if cmd_opts.hide_ui_dir_config else None\r\n     data_labels = {\r\n-        \"outdir_samples\": OptionInfo(\"\", \"Output directory for images; if empty, defaults to two directories below\"),\r\n-        \"outdir_txt2img_samples\": OptionInfo(\"outputs\/txt2img-images\", 'Output directory for txt2img images'),\r\n-        \"outdir_img2img_samples\": OptionInfo(\"outputs\/img2img-images\", 'Output directory for img2img images'),\r\n-        \"outdir_extras_samples\": OptionInfo(\"outputs\/extras-images\", 'Output directory for images from extras tab'),\r\n-        \"outdir_grids\": OptionInfo(\"\", \"Output directory for grids; if empty, defaults to two directories below\"),\r\n-        \"outdir_txt2img_grids\": OptionInfo(\"outputs\/txt2img-grids\", 'Output directory for txt2img grids'),\r\n-        \"outdir_img2img_grids\": OptionInfo(\"outputs\/img2img-grids\", 'Output directory for img2img grids'),\r\n+        \"outdir_samples\": OptionInfo(\"\", \"Output directory for images; if empty, defaults to two directories below\", component_args=hide_dirs),\r\n+        \"outdir_txt2img_samples\": OptionInfo(\"outputs\/txt2img-images\", 'Output directory for txt2img images', component_args=hide_dirs),\r\n+        \"outdir_img2img_samples\": OptionInfo(\"outputs\/img2img-images\", 'Output directory for img2img images', component_args=hide_dirs),\r\n+        \"outdir_extras_samples\": OptionInfo(\"outputs\/extras-images\", 'Output directory for images from extras tab', component_args=hide_dirs),\r\n+        \"outdir_grids\": OptionInfo(\"\", \"Output directory for grids; if empty, defaults to two directories below\", component_args=hide_dirs),\r\n+        \"outdir_txt2img_grids\": OptionInfo(\"outputs\/txt2img-grids\", 'Output directory for txt2img grids', component_args=hide_dirs),\r\n+        \"outdir_img2img_grids\": OptionInfo(\"outputs\/img2img-grids\", 'Output directory for img2img grids', component_args=hide_dirs),\r\n         \"save_to_dirs\": OptionInfo(False, \"When writing images, create a directory with name derived from the prompt\"),\r\n         \"grid_save_to_dirs\": OptionInfo(False, \"When writing grids, create a directory with name derived from the prompt\"),\r\n         \"save_to_dirs_prompt_len\": OptionInfo(10, \"When using above, how many words from prompt to put into directory name\", gr.Slider, {\"minimum\": 1, \"maximum\": 32, \"step\": 1}),\r\n-        \"outdir_save\": OptionInfo(\"log\/images\", \"Directory for saving images using the Save button\"),\r\n+        \"outdir_save\": OptionInfo(\"log\/images\", \"Directory for saving images using the Save button\", component_args=hide_dirs),\r\n         \"samples_save\": OptionInfo(True, \"Save indiviual samples\"),\r\n         \"samples_format\": OptionInfo('png', 'File format for individual samples'),\r\n         \"grid_save\": OptionInfo(True, \"Save image grids\"),\r","add":9,"remove":8,"filename":"\/modules\/shared.py","badparts":["        \"outdir_samples\": OptionInfo(\"\", \"Output directory for images; if empty, defaults to two directories below\"),\r","        \"outdir_txt2img_samples\": OptionInfo(\"outputs\/txt2img-images\", 'Output directory for txt2img images'),\r","        \"outdir_img2img_samples\": OptionInfo(\"outputs\/img2img-images\", 'Output directory for img2img images'),\r","        \"outdir_extras_samples\": OptionInfo(\"outputs\/extras-images\", 'Output directory for images from extras tab'),\r","        \"outdir_grids\": OptionInfo(\"\", \"Output directory for grids; if empty, defaults to two directories below\"),\r","        \"outdir_txt2img_grids\": OptionInfo(\"outputs\/txt2img-grids\", 'Output directory for txt2img grids'),\r","        \"outdir_img2img_grids\": OptionInfo(\"outputs\/img2img-grids\", 'Output directory for img2img grids'),\r","        \"outdir_save\": OptionInfo(\"log\/images\", \"Directory for saving images using the Save button\"),\r"],"goodparts":["    hide_dirs = {\"visible\": False} if cmd_opts.hide_ui_dir_config else None\r","        \"outdir_samples\": OptionInfo(\"\", \"Output directory for images; if empty, defaults to two directories below\", component_args=hide_dirs),\r","        \"outdir_txt2img_samples\": OptionInfo(\"outputs\/txt2img-images\", 'Output directory for txt2img images', component_args=hide_dirs),\r","        \"outdir_img2img_samples\": OptionInfo(\"outputs\/img2img-images\", 'Output directory for img2img images', component_args=hide_dirs),\r","        \"outdir_extras_samples\": OptionInfo(\"outputs\/extras-images\", 'Output directory for images from extras tab', component_args=hide_dirs),\r","        \"outdir_grids\": OptionInfo(\"\", \"Output directory for grids; if empty, defaults to two directories below\", component_args=hide_dirs),\r","        \"outdir_txt2img_grids\": OptionInfo(\"outputs\/txt2img-grids\", 'Output directory for txt2img grids', component_args=hide_dirs),\r","        \"outdir_img2img_grids\": OptionInfo(\"outputs\/img2img-grids\", 'Output directory for img2img grids', component_args=hide_dirs),\r","        \"outdir_save\": OptionInfo(\"log\/images\", \"Directory for saving images using the Save button\", component_args=hide_dirs),\r"]}],"source":"\nimport sys\r import argparse\r import json\r import os\r \r import gradio as gr\r import torch\r import tqdm\r \r import modules.artists\r from modules.paths import script_path, sd_path\r from modules.devices import get_optimal_device\r import modules.styles\r import modules.interrogate\r \r config_filename=\"config.json\"\r \r sd_model_file=os.path.join(script_path, 'model.ckpt')\r if not os.path.exists(sd_model_file):\r sd_model_file=\"models\/ldm\/stable-diffusion-v1\/model.ckpt\"\r \r parser=argparse.ArgumentParser()\r parser.add_argument(\"--config\", type=str, default=os.path.join(sd_path, \"configs\/stable-diffusion\/v1-inference.yaml\"), help=\"path to config which constructs model\",)\r parser.add_argument(\"--ckpt\", type=str, default=os.path.join(sd_path, sd_model_file), help=\"path to checkpoint of model\",)\r parser.add_argument(\"--gfpgan-dir\", type=str, help=\"GFPGAN directory\", default=('.\/src\/gfpgan' if os.path.exists('.\/src\/gfpgan') else '.\/GFPGAN'))\r parser.add_argument(\"--gfpgan-model\", type=str, help=\"GFPGAN model file name\", default='GFPGANv1.3.pth')\r parser.add_argument(\"--no-half\", action='store_true', help=\"do not switch the model to 16-bit floats\")\r parser.add_argument(\"--no-progressbar-hiding\", action='store_true', help=\"do not hide progressbar in gradio UI(we hide it because it slows down ML if you have hardware accleration in browser)\")\r parser.add_argument(\"--max-batch-count\", type=int, default=16, help=\"maximum batch count value for the UI\")\r parser.add_argument(\"--embeddings-dir\", type=str, default='embeddings', help=\"embeddings directory for textual inversion(default: embeddings)\")\r parser.add_argument(\"--allow-code\", action='store_true', help=\"allow custom script execution from webui\")\r parser.add_argument(\"--medvram\", action='store_true', help=\"enable stable diffusion model optimizations for sacrificing a little speed for low VRM usage\")\r parser.add_argument(\"--lowvram\", action='store_true', help=\"enable stable diffusion model optimizations for sacrificing a lot of speed for very low VRM usage\")\r parser.add_argument(\"--always-batch-cond-uncond\", action='store_true', help=\"a workaround test; may help with speed if you use --lowvram\")\r parser.add_argument(\"--unload-gfpgan\", action='store_true', help=\"unload GFPGAN every time after processing images. Warning: seems to cause memory leaks\")\r parser.add_argument(\"--precision\", type=str, help=\"evaluate at this precision\", choices=[\"full\", \"autocast\"], default=\"autocast\")\r parser.add_argument(\"--share\", action='store_true', help=\"use share=True for gradio and make the UI accessible through their site(doesn't work for me but you might have better luck)\")\r parser.add_argument(\"--esrgan-models-path\", type=str, help=\"path to directory with ESRGAN models\", default=os.path.join(script_path, 'ESRGAN'))\r parser.add_argument(\"--opt-split-attention\", action='store_true', help=\"enable optimization that reduce vram usage by a lot for about 10%% decrease in performance\")\r parser.add_argument(\"--opt-split-attention-v1\", action='store_true', help=\"enable older version of --opt-split-attention optimization\")\r parser.add_argument(\"--listen\", action='store_true', help=\"launch gradio with 0.0.0.0 as server name, allowing to respond to network requests\")\r parser.add_argument(\"--port\", type=int, help=\"launch gradio with given server port, you need root\/admin rights for ports < 1024, defaults to 7860 if available\", default=None)\r parser.add_argument(\"--show-negative-prompt\", action='store_true', help=\"does not do anything\", default=False)\r parser.add_argument(\"--ui-config-file\", type=str, help=\"filename to use for ui configuration\", default=os.path.join(script_path, 'ui-config.json'))\r \r cmd_opts=parser.parse_args()\r \r device=get_optimal_device()\r \r batch_cond_uncond=cmd_opts.always_batch_cond_uncond or not(cmd_opts.lowvram or cmd_opts.medvram)\r parallel_processing_allowed=not cmd_opts.lowvram and not cmd_opts.medvram\r \r \r class State:\r interrupted=False\r job=\"\"\r job_no=0\r job_count=0\r sampling_step=0\r sampling_steps=0\r current_latent=None\r current_image=None\r current_image_sampling_step=0\r \r def interrupt(self):\r self.interrupted=True\r \r def nextjob(self):\r self.job_no +=1\r self.sampling_step=0\r self.current_image_sampling_step=0\r \r \r state=State()\r \r artist_db=modules.artists.ArtistsDatabase(os.path.join(script_path, 'artists.csv'))\r \r styles_filename=os.path.join(script_path, 'styles.csv')\r prompt_styles=modules.styles.load_styles(styles_filename)\r \r interrogator=modules.interrogate.InterrogateModels(\"interrogate\")\r \r face_restorers=[]\r \r class Options:\r class OptionInfo:\r def __init__(self, default=None, label=\"\", component=None, component_args=None):\r self.default=default\r self.label=label\r self.component=component\r self.component_args=component_args\r \r data=None\r data_labels={\r \"outdir_samples\": OptionInfo(\"\", \"Output directory for images; if empty, defaults to two directories below\"),\r \"outdir_txt2img_samples\": OptionInfo(\"outputs\/txt2img-images\", 'Output directory for txt2img images'),\r \"outdir_img2img_samples\": OptionInfo(\"outputs\/img2img-images\", 'Output directory for img2img images'),\r \"outdir_extras_samples\": OptionInfo(\"outputs\/extras-images\", 'Output directory for images from extras tab'),\r \"outdir_grids\": OptionInfo(\"\", \"Output directory for grids; if empty, defaults to two directories below\"),\r \"outdir_txt2img_grids\": OptionInfo(\"outputs\/txt2img-grids\", 'Output directory for txt2img grids'),\r \"outdir_img2img_grids\": OptionInfo(\"outputs\/img2img-grids\", 'Output directory for img2img grids'),\r \"save_to_dirs\": OptionInfo(False, \"When writing images, create a directory with name derived from the prompt\"),\r \"grid_save_to_dirs\": OptionInfo(False, \"When writing grids, create a directory with name derived from the prompt\"),\r \"save_to_dirs_prompt_len\": OptionInfo(10, \"When using above, how many words from prompt to put into directory name\", gr.Slider,{\"minimum\": 1, \"maximum\": 32, \"step\": 1}),\r \"outdir_save\": OptionInfo(\"log\/images\", \"Directory for saving images using the Save button\"),\r \"samples_save\": OptionInfo(True, \"Save indiviual samples\"),\r \"samples_format\": OptionInfo('png', 'File format for individual samples'),\r \"grid_save\": OptionInfo(True, \"Save image grids\"),\r \"return_grid\": OptionInfo(True, \"Show grid in results for web\"),\r \"grid_format\": OptionInfo('png', 'File format for grids'),\r \"grid_extended_filename\": OptionInfo(False, \"Add extended info(seed, prompt) to filename when saving grid\"),\r \"grid_only_if_multiple\": OptionInfo(True, \"Do not save grids consisting of one picture\"),\r \"n_rows\": OptionInfo(-1, \"Grid row count; use -1 for autodetect and 0 for it to be same as batch size\", gr.Slider,{\"minimum\": -1, \"maximum\": 16, \"step\": 1}),\r \"jpeg_quality\": OptionInfo(80, \"Quality for saved jpeg images\", gr.Slider,{\"minimum\": 1, \"maximum\": 100, \"step\": 1}),\r \"export_for_4chan\": OptionInfo(True, \"If PNG image is larger than 4MB or any dimension is larger than 4000, downscale and save copy as JPG\"),\r \"enable_pnginfo\": OptionInfo(True, \"Save text information about generation parameters as chunks to png files\"),\r \"font\": OptionInfo(\"\", \"Font for image grids that have text\"),\r \"enable_emphasis\": OptionInfo(True, \"Use(text) to make model pay more attention to text text and[text] to make it pay less attention\"),\r \"save_txt\": OptionInfo(False, \"Create a text file next to every image with generation parameters.\"),\r \"ESRGAN_tile\": OptionInfo(192, \"Tile size for upscaling. 0=no tiling.\", gr.Slider,{\"minimum\": 0, \"maximum\": 512, \"step\": 16}),\r \"ESRGAN_tile_overlap\": OptionInfo(8, \"Tile overlap, in pixels for upscaling. Low values=visible seam.\", gr.Slider,{\"minimum\": 0, \"maximum\": 48, \"step\": 1}),\r \"random_artist_categories\": OptionInfo([], \"Allowed categories for random artists selection when using the Roll button\", gr.CheckboxGroup,{\"choices\": artist_db.categories()}),\r \"upscale_at_full_resolution_padding\": OptionInfo(16, \"Inpainting at full resolution: padding, in pixels, for the masked region.\", gr.Slider,{\"minimum\": 0, \"maximum\": 128, \"step\": 4}),\r \"show_progressbar\": OptionInfo(True, \"Show progressbar\"),\r \"show_progress_every_n_steps\": OptionInfo(0, \"Show show image creation progress every N sampling steps. Set 0 to disable.\", gr.Slider,{\"minimum\": 0, \"maximum\": 32, \"step\": 1}),\r \"multiple_tqdm\": OptionInfo(True, \"Add a second progress bar to the console that shows progress for an entire job. Broken in PyCharm console.\"),\r \"face_restoration_model\": OptionInfo(None, \"Face restoration model\", gr.Radio, lambda:{\"choices\":[x.name() for x in face_restorers]}),\r \"code_former_weight\": OptionInfo(0.5, \"CodeFormer weight parameter; 0=maximum effect; 1=minimum effect\", gr.Slider,{\"minimum\": 0, \"maximum\": 1, \"step\": 0.01}),\r \"interrogate_keep_models_in_memory\": OptionInfo(True, \"Interrogate: keep models in VRAM\"),\r \"interrogate_use_builtin_artists\": OptionInfo(True, \"Interrogate: use artists from artists.csv\"),\r \"interrogate_clip_num_beams\": OptionInfo(1, \"Interrogate: num_beams for BLIP\", gr.Slider,{\"minimum\": 1, \"maximum\": 16, \"step\": 1}),\r \"interrogate_clip_min_length\": OptionInfo(24, \"Interrogate: minimum descripton length(excluding artists, etc..)\", gr.Slider,{\"minimum\": 1, \"maximum\": 128, \"step\": 1}),\r \"interrogate_clip_max_length\": OptionInfo(48, \"Interrogate: maximum descripton length\", gr.Slider,{\"minimum\": 1, \"maximum\": 256, \"step\": 1}),\r }\r \r def __init__(self):\r self.data={k: v.default for k, v in self.data_labels.items()}\r \r def __setattr__(self, key, value):\r if self.data is not None:\r if key in self.data:\r self.data[key]=value\r \r return super(Options, self).__setattr__(key, value)\r \r def __getattr__(self, item):\r if self.data is not None:\r if item in self.data:\r return self.data[item]\r \r if item in self.data_labels:\r return self.data_labels[item].default\r \r return super(Options, self).__getattribute__(item)\r \r def save(self, filename):\r with open(filename, \"w\", encoding=\"utf8\") as file:\r json.dump(self.data, file)\r \r def load(self, filename):\r with open(filename, \"r\", encoding=\"utf8\") as file:\r self.data=json.load(file)\r \r \r opts=Options()\r if os.path.exists(config_filename):\r opts.load(config_filename)\r \r sd_upscalers=[]\r \r sd_model=None\r \r progress_print_out=sys.stdout\r \r \r class TotalTQDM:\r def __init__(self):\r self._tqdm=None\r \r def reset(self):\r self._tqdm=tqdm.tqdm(\r desc=\"Total progress\",\r total=state.job_count * state.sampling_steps,\r position=1,\r file=progress_print_out\r )\r \r def update(self):\r if not opts.multiple_tqdm:\r return\r if self._tqdm is None:\r self.reset()\r self._tqdm.update()\r \r def clear(self):\r if self._tqdm is not None:\r self._tqdm.close()\r self._tqdm=None\r \r \r total_tqdm=TotalTQDM()\r ","sourceWithComments":"import sys\r\nimport argparse\r\nimport json\r\nimport os\r\n\r\nimport gradio as gr\r\nimport torch\r\nimport tqdm\r\n\r\nimport modules.artists\r\nfrom modules.paths import script_path, sd_path\r\nfrom modules.devices import get_optimal_device\r\nimport modules.styles\r\nimport modules.interrogate\r\n\r\nconfig_filename = \"config.json\"\r\n\r\nsd_model_file = os.path.join(script_path, 'model.ckpt')\r\nif not os.path.exists(sd_model_file):\r\n    sd_model_file = \"models\/ldm\/stable-diffusion-v1\/model.ckpt\"\r\n\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument(\"--config\", type=str, default=os.path.join(sd_path, \"configs\/stable-diffusion\/v1-inference.yaml\"), help=\"path to config which constructs model\",)\r\nparser.add_argument(\"--ckpt\", type=str, default=os.path.join(sd_path, sd_model_file), help=\"path to checkpoint of model\",)\r\nparser.add_argument(\"--gfpgan-dir\", type=str, help=\"GFPGAN directory\", default=('.\/src\/gfpgan' if os.path.exists('.\/src\/gfpgan') else '.\/GFPGAN'))\r\nparser.add_argument(\"--gfpgan-model\", type=str, help=\"GFPGAN model file name\", default='GFPGANv1.3.pth')\r\nparser.add_argument(\"--no-half\", action='store_true', help=\"do not switch the model to 16-bit floats\")\r\nparser.add_argument(\"--no-progressbar-hiding\", action='store_true', help=\"do not hide progressbar in gradio UI (we hide it because it slows down ML if you have hardware accleration in browser)\")\r\nparser.add_argument(\"--max-batch-count\", type=int, default=16, help=\"maximum batch count value for the UI\")\r\nparser.add_argument(\"--embeddings-dir\", type=str, default='embeddings', help=\"embeddings directory for textual inversion (default: embeddings)\")\r\nparser.add_argument(\"--allow-code\", action='store_true', help=\"allow custom script execution from webui\")\r\nparser.add_argument(\"--medvram\", action='store_true', help=\"enable stable diffusion model optimizations for sacrificing a little speed for low VRM usage\")\r\nparser.add_argument(\"--lowvram\", action='store_true', help=\"enable stable diffusion model optimizations for sacrificing a lot of speed for very low VRM usage\")\r\nparser.add_argument(\"--always-batch-cond-uncond\", action='store_true', help=\"a workaround test; may help with speed if you use --lowvram\")\r\nparser.add_argument(\"--unload-gfpgan\", action='store_true', help=\"unload GFPGAN every time after processing images. Warning: seems to cause memory leaks\")\r\nparser.add_argument(\"--precision\", type=str, help=\"evaluate at this precision\", choices=[\"full\", \"autocast\"], default=\"autocast\")\r\nparser.add_argument(\"--share\", action='store_true', help=\"use share=True for gradio and make the UI accessible through their site (doesn't work for me but you might have better luck)\")\r\nparser.add_argument(\"--esrgan-models-path\", type=str, help=\"path to directory with ESRGAN models\", default=os.path.join(script_path, 'ESRGAN'))\r\nparser.add_argument(\"--opt-split-attention\", action='store_true', help=\"enable optimization that reduce vram usage by a lot for about 10%% decrease in performance\")\r\nparser.add_argument(\"--opt-split-attention-v1\", action='store_true', help=\"enable older version of --opt-split-attention optimization\")\r\nparser.add_argument(\"--listen\", action='store_true', help=\"launch gradio with 0.0.0.0 as server name, allowing to respond to network requests\")\r\nparser.add_argument(\"--port\", type=int, help=\"launch gradio with given server port, you need root\/admin rights for ports < 1024, defaults to 7860 if available\", default=None)\r\nparser.add_argument(\"--show-negative-prompt\", action='store_true', help=\"does not do anything\", default=False)\r\nparser.add_argument(\"--ui-config-file\", type=str, help=\"filename to use for ui configuration\", default=os.path.join(script_path, 'ui-config.json'))\r\n\r\ncmd_opts = parser.parse_args()\r\n\r\ndevice = get_optimal_device()\r\n\r\nbatch_cond_uncond = cmd_opts.always_batch_cond_uncond or not (cmd_opts.lowvram or cmd_opts.medvram)\r\nparallel_processing_allowed = not cmd_opts.lowvram and not cmd_opts.medvram\r\n\r\n\r\nclass State:\r\n    interrupted = False\r\n    job = \"\"\r\n    job_no = 0\r\n    job_count = 0\r\n    sampling_step = 0\r\n    sampling_steps = 0\r\n    current_latent = None\r\n    current_image = None\r\n    current_image_sampling_step = 0\r\n\r\n    def interrupt(self):\r\n        self.interrupted = True\r\n\r\n    def nextjob(self):\r\n        self.job_no += 1\r\n        self.sampling_step = 0\r\n        self.current_image_sampling_step = 0\r\n\r\n\r\nstate = State()\r\n\r\nartist_db = modules.artists.ArtistsDatabase(os.path.join(script_path, 'artists.csv'))\r\n\r\nstyles_filename = os.path.join(script_path, 'styles.csv')\r\nprompt_styles = modules.styles.load_styles(styles_filename)\r\n\r\ninterrogator = modules.interrogate.InterrogateModels(\"interrogate\")\r\n\r\nface_restorers = []\r\n\r\nclass Options:\r\n    class OptionInfo:\r\n        def __init__(self, default=None, label=\"\", component=None, component_args=None):\r\n            self.default = default\r\n            self.label = label\r\n            self.component = component\r\n            self.component_args = component_args\r\n\r\n    data = None\r\n    data_labels = {\r\n        \"outdir_samples\": OptionInfo(\"\", \"Output directory for images; if empty, defaults to two directories below\"),\r\n        \"outdir_txt2img_samples\": OptionInfo(\"outputs\/txt2img-images\", 'Output directory for txt2img images'),\r\n        \"outdir_img2img_samples\": OptionInfo(\"outputs\/img2img-images\", 'Output directory for img2img images'),\r\n        \"outdir_extras_samples\": OptionInfo(\"outputs\/extras-images\", 'Output directory for images from extras tab'),\r\n        \"outdir_grids\": OptionInfo(\"\", \"Output directory for grids; if empty, defaults to two directories below\"),\r\n        \"outdir_txt2img_grids\": OptionInfo(\"outputs\/txt2img-grids\", 'Output directory for txt2img grids'),\r\n        \"outdir_img2img_grids\": OptionInfo(\"outputs\/img2img-grids\", 'Output directory for img2img grids'),\r\n        \"save_to_dirs\": OptionInfo(False, \"When writing images, create a directory with name derived from the prompt\"),\r\n        \"grid_save_to_dirs\": OptionInfo(False, \"When writing grids, create a directory with name derived from the prompt\"),\r\n        \"save_to_dirs_prompt_len\": OptionInfo(10, \"When using above, how many words from prompt to put into directory name\", gr.Slider, {\"minimum\": 1, \"maximum\": 32, \"step\": 1}),\r\n        \"outdir_save\": OptionInfo(\"log\/images\", \"Directory for saving images using the Save button\"),\r\n        \"samples_save\": OptionInfo(True, \"Save indiviual samples\"),\r\n        \"samples_format\": OptionInfo('png', 'File format for individual samples'),\r\n        \"grid_save\": OptionInfo(True, \"Save image grids\"),\r\n        \"return_grid\": OptionInfo(True, \"Show grid in results for web\"),\r\n        \"grid_format\": OptionInfo('png', 'File format for grids'),\r\n        \"grid_extended_filename\": OptionInfo(False, \"Add extended info (seed, prompt) to filename when saving grid\"),\r\n        \"grid_only_if_multiple\": OptionInfo(True, \"Do not save grids consisting of one picture\"),\r\n        \"n_rows\": OptionInfo(-1, \"Grid row count; use -1 for autodetect and 0 for it to be same as batch size\", gr.Slider, {\"minimum\": -1, \"maximum\": 16, \"step\": 1}),\r\n        \"jpeg_quality\": OptionInfo(80, \"Quality for saved jpeg images\", gr.Slider, {\"minimum\": 1, \"maximum\": 100, \"step\": 1}),\r\n        \"export_for_4chan\": OptionInfo(True, \"If PNG image is larger than 4MB or any dimension is larger than 4000, downscale and save copy as JPG\"),\r\n        \"enable_pnginfo\": OptionInfo(True, \"Save text information about generation parameters as chunks to png files\"),\r\n        \"font\": OptionInfo(\"\", \"Font for image grids that have text\"),\r\n        \"enable_emphasis\": OptionInfo(True, \"Use (text) to make model pay more attention to text text and [text] to make it pay less attention\"),\r\n        \"save_txt\": OptionInfo(False, \"Create a text file next to every image with generation parameters.\"),\r\n        \"ESRGAN_tile\": OptionInfo(192, \"Tile size for upscaling. 0 = no tiling.\", gr.Slider, {\"minimum\": 0, \"maximum\": 512, \"step\": 16}),\r\n        \"ESRGAN_tile_overlap\": OptionInfo(8, \"Tile overlap, in pixels for upscaling. Low values = visible seam.\", gr.Slider, {\"minimum\": 0, \"maximum\": 48, \"step\": 1}),\r\n        \"random_artist_categories\": OptionInfo([], \"Allowed categories for random artists selection when using the Roll button\", gr.CheckboxGroup, {\"choices\": artist_db.categories()}),\r\n        \"upscale_at_full_resolution_padding\": OptionInfo(16, \"Inpainting at full resolution: padding, in pixels, for the masked region.\", gr.Slider, {\"minimum\": 0, \"maximum\": 128, \"step\": 4}),\r\n        \"show_progressbar\": OptionInfo(True, \"Show progressbar\"),\r\n        \"show_progress_every_n_steps\": OptionInfo(0, \"Show show image creation progress every N sampling steps. Set 0 to disable.\", gr.Slider, {\"minimum\": 0, \"maximum\": 32, \"step\": 1}),\r\n        \"multiple_tqdm\": OptionInfo(True, \"Add a second progress bar to the console that shows progress for an entire job. Broken in PyCharm console.\"),\r\n        \"face_restoration_model\": OptionInfo(None, \"Face restoration model\", gr.Radio, lambda: {\"choices\": [x.name() for x in face_restorers]}),\r\n        \"code_former_weight\": OptionInfo(0.5, \"CodeFormer weight parameter; 0 = maximum effect; 1 = minimum effect\", gr.Slider, {\"minimum\": 0, \"maximum\": 1, \"step\": 0.01}),\r\n        \"interrogate_keep_models_in_memory\": OptionInfo(True, \"Interrogate: keep models in VRAM\"),\r\n        \"interrogate_use_builtin_artists\": OptionInfo(True, \"Interrogate: use artists from artists.csv\"),\r\n        \"interrogate_clip_num_beams\": OptionInfo(1, \"Interrogate: num_beams for BLIP\", gr.Slider, {\"minimum\": 1, \"maximum\": 16, \"step\": 1}),\r\n        \"interrogate_clip_min_length\": OptionInfo(24, \"Interrogate: minimum descripton length (excluding artists, etc..)\", gr.Slider, {\"minimum\": 1, \"maximum\": 128, \"step\": 1}),\r\n        \"interrogate_clip_max_length\": OptionInfo(48, \"Interrogate: maximum descripton length\", gr.Slider, {\"minimum\": 1, \"maximum\": 256, \"step\": 1}),\r\n    }\r\n\r\n    def __init__(self):\r\n        self.data = {k: v.default for k, v in self.data_labels.items()}\r\n\r\n    def __setattr__(self, key, value):\r\n        if self.data is not None:\r\n            if key in self.data:\r\n                self.data[key] = value\r\n\r\n        return super(Options, self).__setattr__(key, value)\r\n\r\n    def __getattr__(self, item):\r\n        if self.data is not None:\r\n            if item in self.data:\r\n                return self.data[item]\r\n\r\n        if item in self.data_labels:\r\n            return self.data_labels[item].default\r\n\r\n        return super(Options, self).__getattribute__(item)\r\n\r\n    def save(self, filename):\r\n        with open(filename, \"w\", encoding=\"utf8\") as file:\r\n            json.dump(self.data, file)\r\n\r\n    def load(self, filename):\r\n        with open(filename, \"r\", encoding=\"utf8\") as file:\r\n            self.data = json.load(file)\r\n\r\n\r\nopts = Options()\r\nif os.path.exists(config_filename):\r\n    opts.load(config_filename)\r\n\r\nsd_upscalers = []\r\n\r\nsd_model = None\r\n\r\nprogress_print_out = sys.stdout\r\n\r\n\r\nclass TotalTQDM:\r\n    def __init__(self):\r\n        self._tqdm = None\r\n\r\n    def reset(self):\r\n        self._tqdm = tqdm.tqdm(\r\n            desc=\"Total progress\",\r\n            total=state.job_count * state.sampling_steps,\r\n            position=1,\r\n            file=progress_print_out\r\n        )\r\n\r\n    def update(self):\r\n        if not opts.multiple_tqdm:\r\n            return\r\n        if self._tqdm is None:\r\n            self.reset()\r\n        self._tqdm.update()\r\n\r\n    def clear(self):\r\n        if self._tqdm is not None:\r\n            self._tqdm.close()\r\n            self._tqdm = None\r\n\r\n\r\ntotal_tqdm = TotalTQDM()\r\n"},"\/modules\/ui.py":{"changes":[{"diff":"\n         info = opts.data_labels[key]\r\n         t = type(info.default)\r\n \r\n+        args = info.component_args() if callable(info.component_args) else info.component_args\r\n+\r\n         if info.component is not None:\r\n-            args = info.component_args() if callable(info.component_args) else info.component_args\r\n-            item = info.component(label=info.label, value=fun, **(args or {}))\r\n+            comp = info.component\r\n         elif t == str:\r\n-            item = gr.Textbox(label=info.label, value=fun, lines=1)\r\n+            comp = gr.Textbox\r\n         elif t == int:\r\n-            item = gr.Number(label=info.label, value=fun)\r\n+            comp = gr.Number\r\n         elif t == bool:\r\n-            item = gr.Checkbox(label=info.label, value=fun)\r\n+            comp = gr.Checkbox\r\n         else:\r\n             raise Exception(f'bad options item type: {str(t)} for key {key}')\r\n \r\n-        return item\r\n+        return comp(label=info.label, value=fun, **(args or {}))\r\n \r\n     components = []\r\n     keys = list(opts.data_labels.keys())\r\n","add":7,"remove":6,"filename":"\/modules\/ui.py","badparts":["            args = info.component_args() if callable(info.component_args) else info.component_args\r","            item = info.component(label=info.label, value=fun, **(args or {}))\r","            item = gr.Textbox(label=info.label, value=fun, lines=1)\r","            item = gr.Number(label=info.label, value=fun)\r","            item = gr.Checkbox(label=info.label, value=fun)\r","        return item\r"],"goodparts":["        args = info.component_args() if callable(info.component_args) else info.component_args\r","\r","            comp = info.component\r","            comp = gr.Textbox\r","            comp = gr.Number\r","            comp = gr.Checkbox\r","        return comp(label=info.label, value=fun, **(args or {}))\r"]}]}},"msg":"Add --hide-ui-dir-config command line flag\n\nAdds `--hide-ui-dir-config` flag to disable editing directory configs from the web UI. This can be set to prevent users from setting the directory to somewhere they shouldn't, for public (or semi-public) interfaces.\n\nDirectories are still read from config.json, so the server admin can still set them in the web UI and then relaunch with the hide flag, or edit the config manually.\n\nAlso:\n- fix OptionInfo `component_args` keyword argument not being read if `component` isn't also set\n- ensure that hidden settings aren't still read from the web UI (otherwise they could still be changed by tampering with the interface)"}},"https:\/\/github.com\/carljm\/Wonderment":{"7a16edf5a57222cc6f13578e67adfe6ac504f3d1":{"url":"https:\/\/api.github.com\/repos\/carljm\/Wonderment\/commits\/7a16edf5a57222cc6f13578e67adfe6ac504f3d1","html_url":"https:\/\/github.com\/carljm\/Wonderment\/commit\/7a16edf5a57222cc6f13578e67adfe6ac504f3d1","message":"Include ID HMAC in urls to prevent ID tampering.","sha":"7a16edf5a57222cc6f13578e67adfe6ac504f3d1","keyword":"tampering prevent","diff":"diff --git a\/wonderment\/models.py b\/wonderment\/models.py\nindex 949adcf..f75a932 100644\n--- a\/wonderment\/models.py\n+++ b\/wonderment\/models.py\n@@ -1,9 +1,10 @@\n from datetime import date\n \n from dateutil.relativedelta import relativedelta\n+from django.core.urlresolvers import reverse\n from django.db import models\n \n-from . import fields\n+from . import fields, utils\n \n \n def today():\n@@ -63,6 +64,13 @@ class Parent(models.Model):\n     other_contributions = models.TextField(blank=True)\n     classes_desired = models.TextField(blank=True)\n \n+    @property\n+    def participant_url(self):\n+        return reverse(\n+            'edit_participant_form',\n+            kwargs={'parent_id': self.id, 'id_hash': utils.idhash(self.id)},\n+        )\n+\n     def __str__(self):\n         return self.name\n \ndiff --git a\/wonderment\/urls.py b\/wonderment\/urls.py\nindex bc95801..db626f6 100644\n--- a\/wonderment\/urls.py\n+++ b\/wonderment\/urls.py\n@@ -37,12 +37,12 @@\n         name='new_participant_form',\n     ),\n     url(\n-        r'^(?P<parent_id>\\d+)\/$',\n+        r'^(?P<parent_id>\\d+)-(?P<id_hash>.+)\/$',\n         views.participant_form,\n         name='edit_participant_form',\n     ),\n     url(\n-        r'^(?P<parent_id>\\d+)\/done\/$',\n+        r'^(?P<parent_id>\\d+)-(?P<id_hash>.+)\/done\/$',\n         views.participant_thanks,\n         name='participant_thanks',\n     ),\ndiff --git a\/wonderment\/utils.py b\/wonderment\/utils.py\nnew file mode 100644\nindex 0000000..7697bdc\n--- \/dev\/null\n+++ b\/wonderment\/utils.py\n@@ -0,0 +1,6 @@\n+from django.utils.crypto import salted_hmac\n+\n+\n+def idhash(id):\n+    \"\"\"Generate a short hash of an object ID (for URL verification).\"\"\"\n+    return salted_hmac('wonderment-idhash', id).hexdigest()[:10]\ndiff --git a\/wonderment\/views.py b\/wonderment\/views.py\nindex 75ada0c..99c8487 100644\n--- a\/wonderment\/views.py\n+++ b\/wonderment\/views.py\n@@ -1,9 +1,11 @@\n from datetime import date\n \n from django.contrib.auth.decorators import login_required\n+from django.core.urlresolvers import reverse\n+from django.http import Http404\n from django.shortcuts import get_object_or_404, render, redirect\n \n-from . import forms, models\n+from . import forms, models, utils\n \n CURRENT_SESSION_NAME = \"Spring 2014\"\n CURRENT_SESSION_START = date(2014, 2, 12)\n@@ -21,13 +23,15 @@ def current_session():\n     return session\n \n \n-def participant_form(request, parent_id=None):\n+def participant_form(request, parent_id=None, id_hash=None):\n     session = current_session()\n     if parent_id is None:\n         parent = None\n         participant = None\n     else:\n         parent = get_object_or_404(models.Parent, pk=parent_id)\n+        if utils.idhash(parent.id) != id_hash:\n+            raise Http404()\n         try:\n             participant = models.Participant.objects.get(\n                 parent=parent, session=session)\n@@ -47,7 +51,8 @@ def participant_form(request, parent_id=None):\n             participant.parent = parent\n             participant.session = session\n             participant.save()\n-            return redirect('participant_thanks', parent_id=parent.id)\n+            return redirect(\n+                'participant_thanks', parent_id=parent.id, id_hash=id_hash)\n     else:\n         participant_form = forms.ParticipantForm(**part_kw)\n         parent_form = forms.ParentForm(**form_kwargs)\n@@ -65,8 +70,15 @@ def participant_form(request, parent_id=None):\n     )\n \n \n-def participant_thanks(request, parent_id):\n-    return render(request, 'participant_thanks.html')\n+def participant_thanks(request, parent_id, id_hash):\n+    parent = get_object_or_404(models.Parent, pk=parent_id)\n+    if utils.idhash(parent.id) != id_hash:\n+        raise Http404()\n+    return render(\n+        request,\n+        'participant_thanks.html',\n+        {'parent': parent},\n+    )\n \n \n @login_required\n","files":{"\/wonderment\/models.py":{"changes":[{"diff":"\n from datetime import date\n \n from dateutil.relativedelta import relativedelta\n+from django.core.urlresolvers import reverse\n from django.db import models\n \n-from . import fields\n+from . import fields, utils\n \n \n def today():\n","add":2,"remove":1,"filename":"\/wonderment\/models.py","badparts":["from . import fields"],"goodparts":["from django.core.urlresolvers import reverse","from . import fields, utils"]}],"source":"\nfrom datetime import date from dateutil.relativedelta import relativedelta from django.db import models from. import fields def today(): return date.today() GROUPS=[ (\"Nursery\",(0,(1, 5))), (\"Toddler\",((1, 6), 2)), (\"Preschool\",(3, 4)), (\"K-1\",(5, 6)), (\"Elementary\",(7, 9)), (\"Middle\/High\",(10, 15)), ] PARTICIPATION_TYPES=[ ('one', \"thing one\"), ('two', \"thing two\"), ] class Parent(models.Model): name=models.CharField(max_length=200) phone=models.CharField(max_length=25, blank=True) phone_type=models.CharField( max_length=20, choices=[ ('cell', 'cell'), ('home', 'home'), ('work', 'work'), ], blank=True, ) email=models.EmailField(blank=True) address=models.CharField(max_length=300, blank=True) preferred=models.CharField( max_length=20, choices=[ ('email', 'email'), ('phone', 'phone'), ('text', 'text'), ('facebook', 'facebook'), ], blank=True, ) spouse=models.CharField(max_length=200, blank=True) spouse_contact=models.CharField(max_length=200, blank=True) emergency=models.CharField(max_length=200, blank=True) emergency_contact=models.CharField(max_length=200, blank=True) participate_by=fields.ArrayField( dbtype='text', choices=PARTICIPATION_TYPES) age_groups=models.TextField(blank=True) could_teach=models.TextField(blank=True) could_assist=models.TextField(blank=True) all_ages_help=models.TextField(blank=True) other_contributions=models.TextField(blank=True) classes_desired=models.TextField(blank=True) def __str__(self): return self.name class Meta: ordering=['name'] class Child(models.Model): parent=models.ForeignKey(Parent, related_name='children') name=models.CharField(max_length=200) birthdate=models.DateField(blank=True, null=True) birthdate_approx=models.BooleanField(default=False) pretend_birthdate=models.DateField(blank=True, null=True) special_needs=models.TextField(blank=True) gender=models.CharField( max_length=10, choices=[('male', 'male'),('female', 'female')], blank=True, ) def age_delta(self, as_of, pretend=False): \"\"\"Return age as relativedelta.\"\"\" bd=self.birthdate if pretend and self.pretend_birthdate: bd=self.pretend_birthdate if bd: return relativedelta(as_of, bd) return None def age_years(self, as_of): age=self.age_delta(as_of) if age is not None: return age.years return None def age_display(self, as_of): age=self.age_delta(as_of) if age is not None: if age.years < 1 and age.months < 1: return \"%swk\" % int(age.days \/ 7) if age.years < 2: months=(age.years * 12) +age.months return \"%smo\" % months return \"%syr\" % age.years return \"?\" def age_group(self, as_of): \"\"\"Return name of age group this child is in.\"\"\" age=self.age_delta(as_of, pretend=True) if age is not None: for group_name,(low, high) in GROUPS: low_months=0 high_months=12 if isinstance(low, tuple): low, low_months=low if isinstance(high, tuple): high, high_months=high if( (age.years > low or( age.years==low and age.months >=low_months)) and (age.years < high or( age.years==high and age.months <=high_months)) ): return group_name return None def __str__(self): return self.name class Meta: ordering=['-birthdate'] class Session(models.Model): name=models.CharField(max_length=100) start_date=models.DateField() end_date=models.DateField() def __str__(self): return self.name class Meta: ordering=['start_date'] def families(self, **filters): participants=self.participants.filter( paid__gt=0, **filters).select_related('parent') parents=[] for p in participants: p.parent.jobs=p.jobs parents.append(p.parent) students=Child.objects.filter(parent__in=parents) age_groups_dict={} for student in students: student.real_age=student.age_display(today()) group=student.age_group(self.start_date) age_groups_dict.setdefault(group,[]).append(student) age_groups=[ (name, age_groups_dict[name]) for name, ages in GROUPS if name in age_groups_dict ] if None in age_groups_dict: age_groups.append((\"Unknown\", age_groups_dict[None])) return{ 'parents': parents, 'students': students, 'grouped': age_groups, } class Participant(models.Model): parent=models.ForeignKey(Parent, related_name='participations') session=models.ForeignKey(Session, related_name='participants') level=models.CharField( max_length=20, choices=[('weekly', 'weekly'),('monthly', 'monthly')]) paid=models.IntegerField(default=0) jobs=models.TextField(blank=True) def __str__(self): return \"%s is %s for %s\" %(self.parent, self.level, self.session) class Meta: ordering=['parent__name'] class ClassDay(models.Model): session=models.ForeignKey(Session) date=models.DateField() def __str__(self): return str(self.date) class Meta: ordering=['-date'] class Attendance(models.Model): day=models.ForeignKey(ClassDay) child=models.ForeignKey(Child) ","sourceWithComments":"from datetime import date\n\nfrom dateutil.relativedelta import relativedelta\nfrom django.db import models\n\nfrom . import fields\n\n\ndef today():\n    return date.today()\n\n\nGROUPS = [\n    (\"Nursery\", (0, (1, 5))),\n    (\"Toddler\", ((1, 6), 2)),\n    (\"Preschool\", (3, 4)),\n    (\"K-1\", (5, 6)),\n    (\"Elementary\", (7, 9)),\n    (\"Middle\/High\", (10, 15)),\n]\n\n\nPARTICIPATION_TYPES = [\n    ('one', \"thing one\"),\n    ('two', \"thing two\"),\n]\n\n\nclass Parent(models.Model):\n    name = models.CharField(max_length=200)\n    phone = models.CharField(max_length=25, blank=True)\n    phone_type = models.CharField(\n        max_length=20,\n        choices=[\n            ('cell', 'cell'),\n            ('home', 'home'),\n            ('work', 'work'),\n        ],\n        blank=True,\n    )\n    email = models.EmailField(blank=True)\n    address = models.CharField(max_length=300, blank=True)\n    preferred = models.CharField(\n        max_length=20,\n        choices=[\n            ('email', 'email'),\n            ('phone', 'phone'),\n            ('text', 'text'),\n            ('facebook', 'facebook'),\n        ],\n        blank=True,\n        )\n    spouse = models.CharField(max_length=200, blank=True)\n    spouse_contact = models.CharField(max_length=200, blank=True)\n    emergency = models.CharField(max_length=200, blank=True)\n    emergency_contact = models.CharField(max_length=200, blank=True)\n    participate_by = fields.ArrayField(\n        dbtype='text', choices=PARTICIPATION_TYPES)\n    age_groups = models.TextField(blank=True)\n    could_teach = models.TextField(blank=True)\n    could_assist = models.TextField(blank=True)\n    all_ages_help = models.TextField(blank=True)\n    other_contributions = models.TextField(blank=True)\n    classes_desired = models.TextField(blank=True)\n\n    def __str__(self):\n        return self.name\n\n    class Meta:\n        ordering = ['name']\n\n\nclass Child(models.Model):\n    parent = models.ForeignKey(Parent, related_name='children')\n    name = models.CharField(max_length=200)\n    birthdate = models.DateField(blank=True, null=True)\n    birthdate_approx = models.BooleanField(default=False)\n    pretend_birthdate = models.DateField(blank=True, null=True)\n    special_needs = models.TextField(blank=True)\n    gender = models.CharField(\n        max_length=10,\n        choices=[('male', 'male'), ('female', 'female')],\n        blank=True,\n    )\n\n    def age_delta(self, as_of, pretend=False):\n        \"\"\"Return age as relativedelta.\"\"\"\n        bd = self.birthdate\n        if pretend and self.pretend_birthdate:\n            bd = self.pretend_birthdate\n        if bd:\n            return relativedelta(as_of, bd)\n        return None\n\n    def age_years(self, as_of):\n        age = self.age_delta(as_of)\n        if age is not None:\n            return age.years\n        return None\n\n    def age_display(self, as_of):\n        age = self.age_delta(as_of)\n        if age is not None:\n            if age.years < 1 and age.months < 1:\n                return \"%swk\" % int(age.days \/ 7)\n            if age.years < 2:\n                months = (age.years * 12) + age.months\n                return \"%smo\" % months\n            return \"%syr\" % age.years\n        return \"?\"\n\n    def age_group(self, as_of):\n        \"\"\"Return name of age group this child is in.\"\"\"\n        age = self.age_delta(as_of, pretend=True)\n        if age is not None:\n            for group_name, (low, high) in GROUPS:\n                low_months = 0\n                high_months = 12\n                if isinstance(low, tuple):\n                    low, low_months = low\n                if isinstance(high, tuple):\n                    high, high_months = high\n                if (\n                        (age.years > low or (\n                            age.years == low and age.months >= low_months))\n                        and\n                        (age.years < high or (\n                            age.years == high and age.months <= high_months))\n                ):\n                    return group_name\n        return None\n\n    def __str__(self):\n        return self.name\n\n    class Meta:\n        ordering = ['-birthdate']\n\n\nclass Session(models.Model):\n    name = models.CharField(max_length=100)\n    start_date = models.DateField()\n    end_date = models.DateField()\n\n    def __str__(self):\n        return self.name\n\n    class Meta:\n        ordering = ['start_date']\n\n    def families(self, **filters):\n        participants = self.participants.filter(\n            paid__gt=0, **filters).select_related('parent')\n        parents = []\n        for p in participants:\n            p.parent.jobs = p.jobs\n            parents.append(p.parent)\n        students = Child.objects.filter(parent__in=parents)\n        age_groups_dict = {}\n        for student in students:\n            student.real_age = student.age_display(today())\n            group = student.age_group(self.start_date)\n            age_groups_dict.setdefault(group, []).append(student)\n        age_groups = [\n            (name, age_groups_dict[name])\n            for name, ages in GROUPS\n            if name in age_groups_dict\n        ]\n        if None in age_groups_dict:\n            age_groups.append((\"Unknown\", age_groups_dict[None]))\n        return {\n            'parents': parents,\n            'students': students,\n            'grouped': age_groups,\n        }\n\n\nclass Participant(models.Model):\n    parent = models.ForeignKey(Parent, related_name='participations')\n    session = models.ForeignKey(Session, related_name='participants')\n    level = models.CharField(\n        max_length=20, choices=[('weekly', 'weekly'), ('monthly', 'monthly')])\n    paid = models.IntegerField(default=0)\n    jobs = models.TextField(blank=True)\n\n    def __str__(self):\n        return \"%s is %s for %s\" % (self.parent, self.level, self.session)\n\n    class Meta:\n        ordering = ['parent__name']\n\n\nclass ClassDay(models.Model):\n    session = models.ForeignKey(Session)\n    date = models.DateField()\n\n    def __str__(self):\n        return str(self.date)\n\n    class Meta:\n        ordering = ['-date']\n\n\nclass Attendance(models.Model):\n    day = models.ForeignKey(ClassDay)\n    child = models.ForeignKey(Child)\n"},"\/wonderment\/urls.py":{"changes":[{"diff":"\n         name='new_participant_form',\n     ),\n     url(\n-        r'^(?P<parent_id>\\d+)\/$',\n+        r'^(?P<parent_id>\\d+)-(?P<id_hash>.+)\/$',\n         views.participant_form,\n         name='edit_participant_form',\n     ),\n     url(\n-        r'^(?P<parent_id>\\d+)\/done\/$',\n+        r'^(?P<parent_id>\\d+)-(?P<id_hash>.+)\/done\/$',\n         views.participant_thanks,\n         name='participant_thanks',\n     )","add":2,"remove":2,"filename":"\/wonderment\/urls.py","badparts":["        r'^(?P<parent_id>\\d+)\/$',","        r'^(?P<parent_id>\\d+)\/done\/$',"],"goodparts":["        r'^(?P<parent_id>\\d+)-(?P<id_hash>.+)\/$',","        r'^(?P<parent_id>\\d+)-(?P<id_hash>.+)\/done\/$',"]}],"source":"\nfrom django.conf.urls import include, url from django.contrib import admin from. import views session_urls=[ url(r'$', views.session, name='session'), url(r'^groups\/$', views.age_groups, name='age_groups'), url( r'^groups_with_parents\/$', views.age_groups, {'include_parents': True}, name='age_groups_with_parents', ), url(r'^monthly\/$', views.monthly, name='monthly'), url(r'^parents\/$', views.parents, name='parents'), url( r'^parent_emails\/$', views.parents, {'emails_only': True}, name='parent_emails', ), url( r'^weekly_parent_emails\/$', views.parents, {'emails_only': True, 'weekly_only': True}, name='weekly_parent_emails', ), ] urlpatterns=[ url( r'^$', views.participant_form, name='new_participant_form', ), url( r'^(?P<parent_id>\\d+)\/$', views.participant_form, name='edit_participant_form', ), url( r'^(?P<parent_id>\\d+)\/done\/$', views.participant_thanks, name='participant_thanks', ), url(r'^browse\/$', views.home, name='home'), url(r'^session\/(?P<session_id>\\d+)\/', include(session_urls)), url(r'^admin\/', include(admin.site.urls)), ] ","sourceWithComments":"from django.conf.urls import include, url\nfrom django.contrib import admin\n\nfrom . import views\n\n\nsession_urls = [\n    url(r'$', views.session, name='session'),\n    url(r'^groups\/$', views.age_groups, name='age_groups'),\n    url(\n        r'^groups_with_parents\/$',\n        views.age_groups,\n        {'include_parents': True},\n        name='age_groups_with_parents',\n    ),\n    url(r'^monthly\/$', views.monthly, name='monthly'),\n    url(r'^parents\/$', views.parents, name='parents'),\n    url(\n        r'^parent_emails\/$',\n        views.parents,\n        {'emails_only': True},\n        name='parent_emails',\n    ),\n    url(\n        r'^weekly_parent_emails\/$',\n        views.parents,\n        {'emails_only': True, 'weekly_only': True},\n        name='weekly_parent_emails',\n    ),\n]\n\n\nurlpatterns = [\n    url(\n        r'^$',\n        views.participant_form,\n        name='new_participant_form',\n    ),\n    url(\n        r'^(?P<parent_id>\\d+)\/$',\n        views.participant_form,\n        name='edit_participant_form',\n    ),\n    url(\n        r'^(?P<parent_id>\\d+)\/done\/$',\n        views.participant_thanks,\n        name='participant_thanks',\n    ),\n    url(r'^browse\/$', views.home, name='home'),\n    url(r'^session\/(?P<session_id>\\d+)\/', include(session_urls)),\n    url(r'^admin\/', include(admin.site.urls)),\n]\n"},"\/wonderment\/views.py":{"changes":[{"diff":"\n from datetime import date\n \n from django.contrib.auth.decorators import login_required\n+from django.core.urlresolvers import reverse\n+from django.http import Http404\n from django.shortcuts import get_object_or_404, render, redirect\n \n-from . import forms, models\n+from . import forms, models, utils\n \n CURRENT_SESSION_NAME = \"Spring 2014\"\n CURRENT_SESSION_START = date(2014, 2, 12)\n","add":3,"remove":1,"filename":"\/wonderment\/views.py","badparts":["from . import forms, models"],"goodparts":["from django.core.urlresolvers import reverse","from django.http import Http404","from . import forms, models, utils"]},{"diff":"\n     return session\n \n \n-def participant_form(request, parent_id=None):\n+def participant_form(request, parent_id=None, id_hash=None):\n     session = current_session()\n     if parent_id is None:\n         parent = None\n         participant = None\n     else:\n         parent = get_object_or_404(models.Parent, pk=parent_id)\n+        if utils.idhash(parent.id) != id_hash:\n+            raise Http404()\n         try:\n             participant = models.Participant.objects.get(\n                 parent=parent, session=session)\n","add":3,"remove":1,"filename":"\/wonderment\/views.py","badparts":["def participant_form(request, parent_id=None):"],"goodparts":["def participant_form(request, parent_id=None, id_hash=None):","        if utils.idhash(parent.id) != id_hash:","            raise Http404()"]},{"diff":"\n             participant.parent = parent\n             participant.session = session\n             participant.save()\n-            return redirect('participant_thanks', parent_id=parent.id)\n+            return redirect(\n+                'participant_thanks', parent_id=parent.id, id_hash=id_hash)\n     else:\n         participant_form = forms.ParticipantForm(**part_kw)\n         parent_form = forms.ParentForm(**form_kwargs)\n","add":2,"remove":1,"filename":"\/wonderment\/views.py","badparts":["            return redirect('participant_thanks', parent_id=parent.id)"],"goodparts":["            return redirect(","                'participant_thanks', parent_id=parent.id, id_hash=id_hash)"]},{"diff":"\n     )\n \n \n-def participant_thanks(request, parent_id):\n-    return render(request, 'participant_thanks.html')\n+def participant_thanks(request, parent_id, id_hash):\n+    parent = get_object_or_404(models.Parent, pk=parent_id)\n+    if utils.idhash(parent.id) != id_hash:\n+        raise Http404()\n+    return render(\n+        request,\n+        'participant_thanks.html',\n+        {'parent': parent},\n+    )\n \n \n @login_required\n","add":9,"remove":2,"filename":"\/wonderment\/views.py","badparts":["def participant_thanks(request, parent_id):","    return render(request, 'participant_thanks.html')"],"goodparts":["def participant_thanks(request, parent_id, id_hash):","    parent = get_object_or_404(models.Parent, pk=parent_id)","    if utils.idhash(parent.id) != id_hash:","        raise Http404()","    return render(","        request,","        'participant_thanks.html',","        {'parent': parent},","    )"]}],"source":"\nfrom datetime import date from django.contrib.auth.decorators import login_required from django.shortcuts import get_object_or_404, render, redirect from. import forms, models CURRENT_SESSION_NAME=\"Spring 2014\" CURRENT_SESSION_START=date(2014, 2, 12) CURRENT_SESSION_END=date(2014, 4, 30) def current_session(): session, created=models.Session.objects.get_or_create( name=CURRENT_SESSION_NAME, defaults={ 'start_date': CURRENT_SESSION_START, 'end_date': CURRENT_SESSION_END, } ) return session def participant_form(request, parent_id=None): session=current_session() if parent_id is None: parent=None participant=None else: parent=get_object_or_404(models.Parent, pk=parent_id) try: participant=models.Participant.objects.get( parent=parent, session=session) except models.Participant.DoesNotExist: participant=None form_kwargs={'instance': parent} part_kw={'instance': participant} if request.method=='POST': participant_form=forms.ParticipantForm(request.POST, **part_kw) parent_form=forms.ParentForm(request.POST, **form_kwargs) form_kwargs['instance']=parent_form.instance children_formset=forms.ChildFormSet(request.POST, **form_kwargs) if parent_form.is_valid() and children_formset.is_valid(): parent=parent_form.save() children_formset.save() participant=participant_form.save(commit=False) participant.parent=parent participant.session=session participant.save() return redirect('participant_thanks', parent_id=parent.id) else: participant_form=forms.ParticipantForm(**part_kw) parent_form=forms.ParentForm(**form_kwargs) children_formset=forms.ChildFormSet(**form_kwargs) return render( request, 'participant_form.html', { 'parent': parent, 'participant_form': participant_form, 'parent_form': parent_form, 'children_formset': children_formset, }, ) def participant_thanks(request, parent_id): return render(request, 'participant_thanks.html') @login_required def home(request): sessions=models.Session.objects.all() return render(request, 'home.html',{'sessions': sessions}) @login_required def session(request, session_id): session=get_object_or_404(models.Session, pk=session_id) return render(request, 'session.html',{'session': session}) @login_required def age_groups(request, session_id, include_parents=False): session=get_object_or_404(models.Session, pk=session_id) age_groups=session.families(level='weekly')['grouped'] return render( request, 'age_groups.html', { 'age_groups': age_groups, 'session': session, 'include_parents': include_parents, }, ) @login_required def monthly(request, session_id): session=get_object_or_404(models.Session, pk=session_id) families=session.families() return render( request, 'monthly.html', { 'age_groups': families['grouped'], 'students': families['students'], 'parents': families['parents'], 'session': session, }, ) @login_required def parents(request, session_id, emails_only=False, weekly_only=False): session=get_object_or_404(models.Session, pk=session_id) participants=models.Participant.objects.filter( paid__gt=0, session=session).select_related('parent') if weekly_only: participants=participants.filter(level='weekly') return render( request, 'emails.html' if emails_only else 'parents.html', { 'participants': participants, 'session': session, }, ) ","sourceWithComments":"from datetime import date\n\nfrom django.contrib.auth.decorators import login_required\nfrom django.shortcuts import get_object_or_404, render, redirect\n\nfrom . import forms, models\n\nCURRENT_SESSION_NAME = \"Spring 2014\"\nCURRENT_SESSION_START = date(2014, 2, 12)\nCURRENT_SESSION_END = date(2014, 4, 30)\n\n\ndef current_session():\n    session, created = models.Session.objects.get_or_create(\n        name=CURRENT_SESSION_NAME, defaults={\n            'start_date': CURRENT_SESSION_START,\n            'end_date': CURRENT_SESSION_END,\n        }\n    )\n\n    return session\n\n\ndef participant_form(request, parent_id=None):\n    session = current_session()\n    if parent_id is None:\n        parent = None\n        participant = None\n    else:\n        parent = get_object_or_404(models.Parent, pk=parent_id)\n        try:\n            participant = models.Participant.objects.get(\n                parent=parent, session=session)\n        except models.Participant.DoesNotExist:\n            participant = None\n    form_kwargs = {'instance': parent}\n    part_kw = {'instance': participant}\n    if request.method == 'POST':\n        participant_form = forms.ParticipantForm(request.POST, **part_kw)\n        parent_form = forms.ParentForm(request.POST, **form_kwargs)\n        form_kwargs['instance'] = parent_form.instance\n        children_formset = forms.ChildFormSet(request.POST, **form_kwargs)\n        if parent_form.is_valid() and children_formset.is_valid():\n            parent = parent_form.save()\n            children_formset.save()\n            participant = participant_form.save(commit=False)\n            participant.parent = parent\n            participant.session = session\n            participant.save()\n            return redirect('participant_thanks', parent_id=parent.id)\n    else:\n        participant_form = forms.ParticipantForm(**part_kw)\n        parent_form = forms.ParentForm(**form_kwargs)\n        children_formset = forms.ChildFormSet(**form_kwargs)\n\n    return render(\n        request,\n        'participant_form.html',\n        {\n            'parent': parent,\n            'participant_form': participant_form,\n            'parent_form': parent_form,\n            'children_formset': children_formset,\n        },\n    )\n\n\ndef participant_thanks(request, parent_id):\n    return render(request, 'participant_thanks.html')\n\n\n@login_required\ndef home(request):\n    sessions = models.Session.objects.all()\n    return render(request, 'home.html', {'sessions': sessions})\n\n\n@login_required\ndef session(request, session_id):\n    session = get_object_or_404(models.Session, pk=session_id)\n    return render(request, 'session.html', {'session': session})\n\n\n@login_required\ndef age_groups(request, session_id, include_parents=False):\n    session = get_object_or_404(models.Session, pk=session_id)\n    age_groups = session.families(level='weekly')['grouped']\n    return render(\n        request,\n        'age_groups.html',\n        {\n            'age_groups': age_groups,\n            'session': session,\n            'include_parents': include_parents,\n        },\n    )\n\n\n@login_required\ndef monthly(request, session_id):\n    session = get_object_or_404(models.Session, pk=session_id)\n    families = session.families()\n    return render(\n        request,\n        'monthly.html',\n        {\n            'age_groups': families['grouped'],\n            'students': families['students'],\n            'parents': families['parents'],\n            'session': session,\n        },\n    )\n\n\n@login_required\ndef parents(request, session_id, emails_only=False, weekly_only=False):\n    session = get_object_or_404(models.Session, pk=session_id)\n    participants = models.Participant.objects.filter(\n        paid__gt=0, session=session).select_related('parent')\n    if weekly_only:\n        participants = participants.filter(level='weekly')\n    return render(\n        request,\n        'emails.html' if emails_only else 'parents.html',\n        {\n            'participants': participants,\n            'session': session,\n        },\n    )\n"}},"msg":"Include ID HMAC in urls to prevent ID tampering."}},"https:\/\/github.com\/RedHatQE\/pulp-automation":{"61859ddcfd25aa633e301dc9092c4204ecef0bc7":{"url":"https:\/\/api.github.com\/repos\/RedHatQE\/pulp-automation\/commits\/61859ddcfd25aa633e301dc9092c4204ecef0bc7","html_url":"https:\/\/github.com\/RedHatQE\/pulp-automation\/commit\/61859ddcfd25aa633e301dc9092c4204ecef0bc7","message":"fix: avoid tampering with self.consumer to prevent certificate clobber","sha":"61859ddcfd25aa633e301dc9092c4204ecef0bc7","keyword":"tampering prevent","diff":"diff --git a\/tests\/test_9_consumer.py b\/tests\/test_9_consumer.py\nindex 693648b..877b762 100644\n--- a\/tests\/test_9_consumer.py\n+++ b\/tests\/test_9_consumer.py\n@@ -9,10 +9,11 @@ def test_00_none(self):\n         pass\n \n     def test_01_update_consumer(self):\n-        self.consumer |= {'display_name': \"A %s consumer\" % type(self).__name__}\n+        # update causes private key loss; do not change self.consumer \n+        consumer = self.consumer | {'display_name': \"A %s consumer\" % type(self).__name__}\n         with self.pulp.asserting(True):\n-            self.consumer.update(self.pulp)\n-            self.assertEqual(Consumer.get(self.pulp, self.consumer.id), self.consumer)\n+            consumer.update(self.pulp)\n+            self.assertEqual(Consumer.get(self.pulp, consumer.id), consumer)\n     \n     @agent_test(catching=True)\n     def test_02_bind_distributor(self):\n","files":{"\/tests\/test_9_consumer.py":{"changes":[{"diff":"\n         pass\n \n     def test_01_update_consumer(self):\n-        self.consumer |= {'display_name': \"A %s consumer\" % type(self).__name__}\n+        # update causes private key loss; do not change self.consumer \n+        consumer = self.consumer | {'display_name': \"A %s consumer\" % type(self).__name__}\n         with self.pulp.asserting(True):\n-            self.consumer.update(self.pulp)\n-            self.assertEqual(Consumer.get(self.pulp, self.consumer.id), self.consumer)\n+            consumer.update(self.pulp)\n+            self.assertEqual(Consumer.get(self.pulp, consumer.id), consumer)\n     \n     @agent_test(catching=True)\n     def test_02_bind_distributor(self):\n","add":4,"remove":3,"filename":"\/tests\/test_9_consumer.py","badparts":["        self.consumer |= {'display_name': \"A %s consumer\" % type(self).__name__}","            self.consumer.update(self.pulp)","            self.assertEqual(Consumer.get(self.pulp, self.consumer.id), self.consumer)"],"goodparts":["        consumer = self.consumer | {'display_name': \"A %s consumer\" % type(self).__name__}","            consumer.update(self.pulp)","            self.assertEqual(Consumer.get(self.pulp, consumer.id), consumer)"]}],"source":"\nfrom pulp_auto.consumer import(Consumer, Binding) from pulp_auto.task import Task from pulp_test import(ConsumerAgentPulpTest, agent_test) class TestConsumer(ConsumerAgentPulpTest): def test_00_none(self): pass def test_01_update_consumer(self): self.consumer |={'display_name': \"A %s consumer\" % type(self).__name__} with self.pulp.asserting(True): self.consumer.update(self.pulp) self.assertEqual(Consumer.get(self.pulp, self.consumer.id), self.consumer) @agent_test(catching=True) def test_02_bind_distributor(self): with self.pulp.asserting(True): Task.wait_for_response(self.pulp, self.consumer.bind_distributor(self.pulp, self.binding_data)) def test_03_get_repo_bindings(self): with self.pulp.asserting(True): bindings=self.consumer.get_repo_bindings(self.pulp, self.repo.id) binding=Binding(data={ 'repo_id': self.repo.id, 'consumer_id': self.consumer.id, 'distributor_id': self.distributor.id, 'id': '123' }) self.assertIn(binding, bindings) def test_04_list_bindings(self): with self.pulp.asserting(True): bindings=self.consumer.list_bindings(self.pulp) binding=Binding(data={ 'repo_id': self.repo.id, 'consumer_id': self.consumer.id, 'distributor_id': self.distributor.id, 'id': '123' }) self.assertIn(binding, bindings) @agent_test(catching=True) def test_05_unbind_distributor(self): with self.pulp.asserting(True): Task.wait_for_response(self.pulp, self.consumer.unbind_distributor(self.pulp, self.repo.id, self.distributor.id)) ","sourceWithComments":"from pulp_auto.consumer import (Consumer, Binding) \nfrom pulp_auto.task import Task \nfrom pulp_test import (ConsumerAgentPulpTest, agent_test)\n\n\nclass TestConsumer(ConsumerAgentPulpTest):\n\n    def test_00_none(self):\n        pass\n\n    def test_01_update_consumer(self):\n        self.consumer |= {'display_name': \"A %s consumer\" % type(self).__name__}\n        with self.pulp.asserting(True):\n            self.consumer.update(self.pulp)\n            self.assertEqual(Consumer.get(self.pulp, self.consumer.id), self.consumer)\n    \n    @agent_test(catching=True)\n    def test_02_bind_distributor(self):\n        with self.pulp.asserting(True):\n            Task.wait_for_response(self.pulp, self.consumer.bind_distributor(self.pulp, self.binding_data))\n\n    def test_03_get_repo_bindings(self):\n        with self.pulp.asserting(True):\n            bindings = self.consumer.get_repo_bindings(self.pulp, self.repo.id)\n        binding = Binding(data={\n            'repo_id': self.repo.id,\n            'consumer_id': self.consumer.id,\n            'distributor_id': self.distributor.id,\n            'id': '123'\n        })\n        self.assertIn(binding, bindings)\n\n    def test_04_list_bindings(self):\n        with self.pulp.asserting(True):\n            bindings = self.consumer.list_bindings(self.pulp)\n        binding = Binding(data={\n            'repo_id': self.repo.id,\n            'consumer_id': self.consumer.id,\n            'distributor_id': self.distributor.id,\n            'id': '123'\n        })\n        self.assertIn(binding, bindings)\n\n    @agent_test(catching=True)\n    def test_05_unbind_distributor(self):\n        with self.pulp.asserting(True):\n            Task.wait_for_response(self.pulp, self.consumer.unbind_distributor(self.pulp, self.repo.id, self.distributor.id))\n"}},"msg":"fix: avoid tampering with self.consumer to prevent certificate clobber"}},"https:\/\/github.com\/JurajNyiri\/pytapo":{"513931ccf05e1916b8d6bdb0d3f78bdca1d21f82":{"url":"https:\/\/api.github.com\/repos\/JurajNyiri\/pytapo\/commits\/513931ccf05e1916b8d6bdb0d3f78bdca1d21f82","html_url":"https:\/\/github.com\/JurajNyiri\/pytapo\/commit\/513931ccf05e1916b8d6bdb0d3f78bdca1d21f82","message":"Fix: Unable to turn off tamper detection","sha":"513931ccf05e1916b8d6bdb0d3f78bdca1d21f82","keyword":"tampering fix","diff":"diff --git a\/pytapo\/__init__.py b\/pytapo\/__init__.py\nindex 385029a..88c373c 100644\n--- a\/pytapo\/__init__.py\n+++ b\/pytapo\/__init__.py\n@@ -681,11 +681,12 @@ def setTamperDetection(self, enabled, sensitivity=False):\n         data = {\n             \"tamper_detection\": {\"tamper_det\": {\"enabled\": \"on\" if enabled else \"off\"}}\n         }\n-        if sensitivity not in [\"high\", \"normal\", \"low\"]:\n-            raise Exception(\"Invalid sensitivity, can be low, normal or high\")\n-        if sensitivity == \"normal\":\n-            sensitivity = \"medium\"\n-        data[\"tamper_detection\"][\"tamper_det\"][\"sensitivity\"] = sensitivity\n+        if sensitivity:\n+            if sensitivity not in [\"high\", \"normal\", \"low\"]:\n+                raise Exception(\"Invalid sensitivity, can be low, normal or high\")\n+            if sensitivity == \"normal\":\n+                sensitivity = \"medium\"\n+            data[\"tamper_detection\"][\"tamper_det\"][\"sensitivity\"] = sensitivity\n \n         return self.executeFunction(\"setTamperDetectionConfig\", data)\n \ndiff --git a\/setup.py b\/setup.py\nindex 100dd83..7199019 100644\n--- a\/setup.py\n+++ b\/setup.py\n@@ -5,7 +5,7 @@\n \n setuptools.setup(\n     name=\"pytapo\",\n-    version=\"3.1.8\",\n+    version=\"3.1.9\",\n     author=\"Juraj Ny\u00edri\",\n     author_email=\"juraj.nyiri@gmail.com\",\n     description=\"Python library for communication with Tapo Cameras\",\n","files":{"\/pytapo\/__init__.py":{"changes":[{"diff":"\n         data = {\n             \"tamper_detection\": {\"tamper_det\": {\"enabled\": \"on\" if enabled else \"off\"}}\n         }\n-        if sensitivity not in [\"high\", \"normal\", \"low\"]:\n-            raise Exception(\"Invalid sensitivity, can be low, normal or high\")\n-        if sensitivity == \"normal\":\n-            sensitivity = \"medium\"\n-        data[\"tamper_detection\"][\"tamper_det\"][\"sensitivity\"] = sensitivity\n+        if sensitivity:\n+            if sensitivity not in [\"high\", \"normal\", \"low\"]:\n+                raise Exception(\"Invalid sensitivity, can be low, normal or high\")\n+            if sensitivity == \"normal\":\n+                sensitivity = \"medium\"\n+            data[\"tamper_detection\"][\"tamper_det\"][\"sensitivity\"] = sensitivity\n \n         return self.executeFunction(\"setTamperDetectionConfig\", data)\n ","add":6,"remove":5,"filename":"\/pytapo\/__init__.py","badparts":["        if sensitivity not in [\"high\", \"normal\", \"low\"]:","            raise Exception(\"Invalid sensitivity, can be low, normal or high\")","        if sensitivity == \"normal\":","            sensitivity = \"medium\"","        data[\"tamper_detection\"][\"tamper_det\"][\"sensitivity\"] = sensitivity"],"goodparts":["        if sensitivity:","            if sensitivity not in [\"high\", \"normal\", \"low\"]:","                raise Exception(\"Invalid sensitivity, can be low, normal or high\")","            if sensitivity == \"normal\":","                sensitivity = \"medium\"","            data[\"tamper_detection\"][\"tamper_det\"][\"sensitivity\"] = sensitivity"]}]},"\/setup.py":{"changes":[{"diff":"\n \n setuptools.setup(\n     name=\"pytapo\",\n-    version=\"3.1.8\",\n+    version=\"3.1.9\",\n     author=\"Juraj Ny\u00edri\",\n     author_email=\"juraj.nyiri@gmail.com\",\n     description=\"Python library for communication with Tapo Cameras\",\n","add":1,"remove":1,"filename":"\/setup.py","badparts":["    version=\"3.1.8\","],"goodparts":["    version=\"3.1.9\","]}],"source":"\nimport setuptools with open(\"README.md\", \"r\") as fh: long_description=fh.read() setuptools.setup( name=\"pytapo\", version=\"3.1.8\", author=\"Juraj Ny\u00edri\", author_email=\"juraj.nyiri@gmail.com\", description=\"Python library for communication with Tapo Cameras\", license=\"MIT\", long_description=long_description, long_description_content_type=\"text\/markdown\", url=\"https:\/\/github.com\/JurajNyiri\/pytapo\", packages=setuptools.find_packages(), install_requires=[\"requests\", \"urllib3\", \"pycryptodome\", \"rtp\"], tests_require=[\"pytest\", \"pytest-asyncio\", \"mock\"], classifiers=[ \"Programming Language:: Python:: 3\", \"License:: OSI Approved:: MIT License\", \"Operating System:: OS Independent\", ], ) ","sourceWithComments":"import setuptools\n\nwith open(\"README.md\", \"r\") as fh:\n    long_description = fh.read()\n\nsetuptools.setup(\n    name=\"pytapo\",\n    version=\"3.1.8\",\n    author=\"Juraj Ny\u00edri\",\n    author_email=\"juraj.nyiri@gmail.com\",\n    description=\"Python library for communication with Tapo Cameras\",\n    license=\"MIT\",\n    long_description=long_description,\n    long_description_content_type=\"text\/markdown\",\n    url=\"https:\/\/github.com\/JurajNyiri\/pytapo\",\n    packages=setuptools.find_packages(),\n    install_requires=[\"requests\", \"urllib3\", \"pycryptodome\", \"rtp\"],\n    tests_require=[\"pytest\", \"pytest-asyncio\", \"mock\"],\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n    ],\n)\n"}},"msg":"Fix: Unable to turn off tamper detection"}},"https:\/\/github.com\/shipperstack\/shippy":{"719fa0124c34f54e411463f15fe57b0fc7d18aec":{"url":"https:\/\/api.github.com\/repos\/shipperstack\/shippy\/commits\/719fa0124c34f54e411463f15fe57b0fc7d18aec","html_url":"https:\/\/github.com\/shipperstack\/shippy\/commit\/719fa0124c34f54e411463f15fe57b0fc7d18aec","message":"main: check for corrupt configuration file\n\nCaveat: will not detect all tampering and corruption, but will\nguard against a malformed URL crashing shippy.\n\nFixes #93","sha":"719fa0124c34f54e411463f15fe57b0fc7d18aec","keyword":"tampering fix","diff":"diff --git a\/shippy\/__main__.py b\/shippy\/__main__.py\nindex 1ae69ef..9be5d4e 100644\n--- a\/shippy\/__main__.py\n+++ b\/shippy\/__main__.py\n@@ -61,6 +61,13 @@ def main():\n     # Check if server config is valid\n     try:\n         server_url = get_config_value(\"shippy\", \"server\")\n+        if not check_server_url_schema(server_url):\n+            print_error(\n+                msg=f\"The configuration file is corrupt. Please delete it and restart shippy.\",\n+                newline=True,\n+                exit_after=True,\n+            )\n+        \n         token = get_config_value(\"shippy\", \"token\")\n \n         token = check_token_validity(server_url, token)\n@@ -256,7 +263,7 @@ def get_server_url():\n     try:\n         while True:\n             server_url = input(\"Enter the server URL: \")\n-            if \"http\" not in server_url:\n+            if not check_server_url_schema(server_url):\n                 # noinspection HttpUrlsUsage\n                 print_error(\n                     msg=\"Server URL is missing either http:\/\/ or https:\/\/.\",\n@@ -276,6 +283,10 @@ def get_server_url():\n         exit(0)\n \n \n+def check_server_url_schema(url):\n+    return \"http\" in url\n+\n+\n def get_token(server_url):\n     while True:\n         from getpass import getpass\n","files":{"\/shippy\/__main__.py":{"changes":[{"diff":"\n     try:\n         while True:\n             server_url = input(\"Enter the server URL: \")\n-            if \"http\" not in server_url:\n+            if not check_server_url_schema(server_url):\n                 # noinspection HttpUrlsUsage\n                 print_error(\n                     msg=\"Server URL is missing either http:\/\/ or https:\/\/.\",\n","add":1,"remove":1,"filename":"\/shippy\/__main__.py","badparts":["            if \"http\" not in server_url:"],"goodparts":["            if not check_server_url_schema(server_url):"]}],"source":"\nimport argparse import re import glob import os.path import requests import semver import sentry_sdk from rich import print from rich.console import Console from.client import( login_to_server, upload, get_server_version_info, get_regex_pattern, get_hash_from_checksum_file, check_token, get_hash_of_file, find_checksum_file, ) from.config import get_config_value, set_config_value, get_optional_true_config_value from.constants import( SENTRY_SDK_URL, SERVER_COMPAT_ERROR_MSG, SHIPPY_COMPAT_ERROR_MSG, SHIPPY_OUTDATED_MSG, ) from.exceptions import LoginException, UploadException from.helper import input_yn, print_error, print_warning, print_success from.version import __version__, server_compat_version ignore_errors=[KeyboardInterrupt] sentry_sdk.init( SENTRY_SDK_URL, traces_sample_rate=1.0, release=__version__, ignore_errors=ignore_errors, ) console=Console() def main(): print(f\"Welcome to shippy(v.{__version__})!\") upload_without_prompt=get_optional_true_config_value( \"shippy\", \"UploadWithoutPrompt\" ) args=init_argparse() upload_without_prompt=upload_without_prompt or args.yes check_shippy_update() try: server_url=get_config_value(\"shippy\", \"server\") token=get_config_value(\"shippy\", \"token\") token=check_token_validity(server_url, token) except KeyError: print_warning( \"No configuration file found or configuration is invalid. You need to \" \"configure shippy before you can start using it.\" ) server_url=get_server_url() token=get_token(server_url) server_url=get_config_value(\"shippy\", \"server\") check_server_compat(server_url) regex_pattern=get_regex_pattern(server_url=server_url, token=token) builds=get_builds_in_current_dir(regex_pattern) if len(builds)==0: print_error( msg=\"No files matching the submission criteria were detected in the \" \"current directory.\", newline=True, exit_after=False, ) else: print(f\"Detected{len(builds)} build(s):\") for build in builds: print(f\"\\t{build}\") if not upload_without_prompt and len(builds) > 1: print_warning(\"You seem to be uploading multiple builds. \", newline=False) if not input_yn(\"Are you sure you want to continue?\", default=False): return for build in builds: if not check_build(build): print_warning(\"Invalid build. Skipping...\") continue if upload_without_prompt or input_yn(f\"Uploading build{build}. Start?\"): try: upload(server_url=server_url, build_file_path=build, token=token) except UploadException as exception: print_error(exception, newline=True, exit_after=False) def init_argparse(): parser=argparse.ArgumentParser( description=\"Client-side tool for interfacing with shipper\" ) parser.add_argument( \"-y\", \"--yes\", action=\"store_true\", help=\"Upload builds automatically without prompting\", ) return parser.parse_args() def check_server_compat(server_url): with console.status( \"Please wait while shippy contacts the remote server to check compatibility... \" ): server_version_info=get_server_version_info(server_url) if semver.compare(server_version_info[\"version\"], server_compat_version)==-1: print_error( msg=SERVER_COMPAT_ERROR_MSG.format( server_version_info[\"version\"], server_compat_version ), newline=True, exit_after=True, ) if semver.compare(server_version_info[\"shippy_compat_version\"], __version__)==1: print_error( msg=SHIPPY_COMPAT_ERROR_MSG.format( server_version_info[\"shippy_compat_version\"], __version__ ), newline=True, exit_after=True, ) print_success(\"Finished compatibility check. No problems found.\") def check_token_validity(server_url, token): with console.status( \"Please wait while shippy contacts the remote server to check if the token is \" \"still valid... \" ): is_token_valid=check_token(server_url, token) if not is_token_valid: print_warning(\"The saved token is invalid. Please sign-in again.\") token=get_token(server_url) return token def check_shippy_update(): with console.status(\"Please wait while shippy checks for updates... \"): r=requests.get( \"https:\/\/api.github.com\/repos\/shipperstack\/shippy\/releases\/latest\" ) latest_version=r.json()[\"name\"] if semver.compare(__version__, latest_version)==-1: print(SHIPPY_OUTDATED_MSG.format(__version__, latest_version)) else: print_success(\"Finished update check. shippy is up-to-date!\") def get_builds_in_current_dir(regex_pattern): with console.status(\"Detecting builds in current directory...\"): builds=[] files=[f for f in glob.glob(\"*.zip\")] for file in files: if re.search(regex_pattern, file): builds.append(file) return builds def check_build(filename): \"\"\"Makes sure the build is valid\"\"\" print(f\"Validating build{filename}...\") has_checksum_file_type, has_sum_postfix=find_checksum_file(filename=filename) if has_checksum_file_type is None: print_warning( \"This build does not have a matching checksum file. \", newline=False ) return False with console.status( f\"Checking{has_checksum_file_type.upper()} hash of{filename}... this may \" \"take a couple of seconds. \" ): hash_val=get_hash_of_file( filename=filename, checksum_type=has_checksum_file_type ) if not has_sum_postfix: actual_hash_val=get_hash_from_checksum_file( f\"{filename}.{has_checksum_file_type}\" ) else: actual_hash_val=get_hash_from_checksum_file( f\"{filename}.{has_checksum_file_type}sum\" ) if hash_val !=actual_hash_val: print_error( msg=\"This build's checksum is invalid. \", newline=False, exit_after=False, ) return False print_success(f\"{has_checksum_file_type.upper()} hash of{filename} matched.\") build_slug, _=os.path.splitext(filename) _, _, _, build_type, build_variant, _=build_slug.split(\"-\") if build_type !=\"OFFICIAL\": print_error(msg=\"This build is not official. \", newline=False, exit_after=False) return False valid_variants=[\"gapps\", \"vanilla\", \"foss\", \"goapps\"] if build_variant not in valid_variants: print_error( msg=\"This build has an unknown variant. \", newline=False, exit_after=False ) return False print_success(f\"Validation of build{filename} complete. No problems found.\") return True def get_server_url(): try: while True: server_url=input(\"Enter the server URL: \") if \"http\" not in server_url: print_error( msg=\"Server URL is missing either http:\/\/ or https:\/\/.\", newline=True, exit_after=False, ) else: break if server_url[-1]==\"\/\": server_url=server_url[:-1] set_config_value(\"shippy\", \"server\", server_url) return server_url except KeyboardInterrupt: exit(0) def get_token(server_url): while True: from getpass import getpass try: username=input(\"Enter your username: \") password=getpass(prompt=\"Enter your password: \") try: token=login_to_server(username, password, server_url) set_config_value(\"shippy\", \"token\", token) return token except LoginException as exception: print_error( f\"{exception} Please try again.\", newline=True, exit_after=False ) except KeyboardInterrupt: exit(0) if __name__==\"__main__\": main() ","sourceWithComments":"import argparse\nimport re\nimport glob\nimport os.path\n\nimport requests\nimport semver\nimport sentry_sdk\n\nfrom rich import print\nfrom rich.console import Console\n\nfrom .client import (\n    login_to_server,\n    upload,\n    get_server_version_info,\n    get_regex_pattern,\n    get_hash_from_checksum_file,\n    check_token,\n    get_hash_of_file,\n    find_checksum_file,\n)\nfrom .config import get_config_value, set_config_value, get_optional_true_config_value\nfrom .constants import (\n    SENTRY_SDK_URL,\n    SERVER_COMPAT_ERROR_MSG,\n    SHIPPY_COMPAT_ERROR_MSG,\n    SHIPPY_OUTDATED_MSG,\n)\nfrom .exceptions import LoginException, UploadException\nfrom .helper import input_yn, print_error, print_warning, print_success\nfrom .version import __version__, server_compat_version\n\nignore_errors = [KeyboardInterrupt]\n\nsentry_sdk.init(\n    SENTRY_SDK_URL,\n    traces_sample_rate=1.0,\n    release=__version__,\n    ignore_errors=ignore_errors,\n)\n\nconsole = Console()\n\n\ndef main():\n    print(f\"Welcome to shippy (v.{__version__})!\")\n\n    # Check if we cannot prompt the user (default to auto-upload)\n    upload_without_prompt = get_optional_true_config_value(\n        \"shippy\", \"UploadWithoutPrompt\"\n    )\n\n    # Get commandline arguments\n    args = init_argparse()\n    upload_without_prompt = upload_without_prompt or args.yes\n\n    # Check for updates\n    check_shippy_update()\n\n    # Check if server config is valid\n    try:\n        server_url = get_config_value(\"shippy\", \"server\")\n        token = get_config_value(\"shippy\", \"token\")\n\n        token = check_token_validity(server_url, token)\n    except KeyError:\n        print_warning(\n            \"No configuration file found or configuration is invalid. You need to \"\n            \"configure shippy before you can start using it.\"\n        )\n        server_url = get_server_url()\n        token = get_token(server_url)\n\n        # In case login function updated server URL, we need to fetch it again\n        server_url = get_config_value(\"shippy\", \"server\")\n\n    # Check server version compatibility\n    check_server_compat(server_url)\n\n    # Get regex pattern from server\n    regex_pattern = get_regex_pattern(server_url=server_url, token=token)\n\n    # Search current directory for files\n    builds = get_builds_in_current_dir(regex_pattern)\n\n    if len(builds) == 0:\n        print_error(\n            msg=\"No files matching the submission criteria were detected in the \"\n            \"current directory.\",\n            newline=True,\n            exit_after=False,\n        )\n    else:\n        print(f\"Detected {len(builds)} build(s):\")\n        for build in builds:\n            print(f\"\\t{build}\")\n\n        if not upload_without_prompt and len(builds) > 1:\n            print_warning(\"You seem to be uploading multiple builds. \", newline=False)\n            if not input_yn(\"Are you sure you want to continue?\", default=False):\n                return\n\n        for build in builds:\n            # Check build file validity\n            if not check_build(build):\n                print_warning(\"Invalid build. Skipping...\")\n                continue\n\n            if upload_without_prompt or input_yn(f\"Uploading build {build}. Start?\"):\n                try:\n                    upload(server_url=server_url, build_file_path=build, token=token)\n                except UploadException as exception:\n                    print_error(exception, newline=True, exit_after=False)\n\n\ndef init_argparse():\n    parser = argparse.ArgumentParser(\n        description=\"Client-side tool for interfacing with shipper\"\n    )\n    parser.add_argument(\n        \"-y\",\n        \"--yes\",\n        action=\"store_true\",\n        help=\"Upload builds automatically without prompting\",\n    )\n    return parser.parse_args()\n\n\ndef check_server_compat(server_url):\n    with console.status(\n        \"Please wait while shippy contacts the remote server to check compatibility... \"\n    ):\n        server_version_info = get_server_version_info(server_url)\n\n    # Check if shipper version is compatible\n    if semver.compare(server_version_info[\"version\"], server_compat_version) == -1:\n        print_error(\n            msg=SERVER_COMPAT_ERROR_MSG.format(\n                server_version_info[\"version\"], server_compat_version\n            ),\n            newline=True,\n            exit_after=True,\n        )\n\n    # Check if shippy version is compatible\n    if semver.compare(server_version_info[\"shippy_compat_version\"], __version__) == 1:\n        print_error(\n            msg=SHIPPY_COMPAT_ERROR_MSG.format(\n                server_version_info[\"shippy_compat_version\"], __version__\n            ),\n            newline=True,\n            exit_after=True,\n        )\n\n    print_success(\"Finished compatibility check. No problems found.\")\n\n\ndef check_token_validity(server_url, token):\n    with console.status(\n        \"Please wait while shippy contacts the remote server to check if the token is \"\n        \"still valid... \"\n    ):\n        is_token_valid = check_token(server_url, token)\n\n    if not is_token_valid:\n        # Token check failed, prompt for login again\n        print_warning(\"The saved token is invalid. Please sign-in again.\")\n        token = get_token(server_url)\n    return token\n\n\ndef check_shippy_update():\n    with console.status(\"Please wait while shippy checks for updates... \"):\n        r = requests.get(\n            \"https:\/\/api.github.com\/repos\/shipperstack\/shippy\/releases\/latest\"\n        )\n        latest_version = r.json()[\"name\"]\n\n    if semver.compare(__version__, latest_version) == -1:\n        print(SHIPPY_OUTDATED_MSG.format(__version__, latest_version))\n    else:\n        print_success(\"Finished update check. shippy is up-to-date!\")\n\n\ndef get_builds_in_current_dir(regex_pattern):\n    with console.status(\"Detecting builds in current directory...\"):\n        builds = []\n        files = [f for f in glob.glob(\"*.zip\")]\n        for file in files:\n            if re.search(regex_pattern, file):\n                builds.append(file)\n\n        return builds\n\n\ndef check_build(filename):\n    \"\"\"Makes sure the build is valid\"\"\"\n    print(f\"Validating build {filename}...\")\n\n    # Validate that there is a matching checksum file\n    has_checksum_file_type, has_sum_postfix = find_checksum_file(filename=filename)\n\n    if has_checksum_file_type is None:\n        print_warning(\n            \"This build does not have a matching checksum file. \", newline=False\n        )\n        return False\n\n    # Validate checksum\n    with console.status(\n        f\"Checking {has_checksum_file_type.upper()} hash of {filename}... this may \"\n        \"take a couple of seconds. \"\n    ):\n        hash_val = get_hash_of_file(\n            filename=filename, checksum_type=has_checksum_file_type\n        )\n        if not has_sum_postfix:\n            actual_hash_val = get_hash_from_checksum_file(\n                f\"{filename}.{has_checksum_file_type}\"\n            )\n        else:\n            actual_hash_val = get_hash_from_checksum_file(\n                f\"{filename}.{has_checksum_file_type}sum\"\n            )\n        if hash_val != actual_hash_val:\n            print_error(\n                msg=\"This build's checksum is invalid. \",\n                newline=False,\n                exit_after=False,\n            )\n            return False\n        print_success(f\"{has_checksum_file_type.upper()} hash of {filename} matched.\")\n\n    build_slug, _ = os.path.splitext(filename)\n    _, _, _, build_type, build_variant, _ = build_slug.split(\"-\")\n\n    # Check build type\n    if build_type != \"OFFICIAL\":\n        print_error(msg=\"This build is not official. \", newline=False, exit_after=False)\n        return False\n\n    # Check build variant\n    valid_variants = [\"gapps\", \"vanilla\", \"foss\", \"goapps\"]\n    if build_variant not in valid_variants:\n        print_error(\n            msg=\"This build has an unknown variant. \", newline=False, exit_after=False\n        )\n        return False\n\n    print_success(f\"Validation of build {filename} complete. No problems found.\")\n    return True\n\n\ndef get_server_url():\n    try:\n        while True:\n            server_url = input(\"Enter the server URL: \")\n            if \"http\" not in server_url:\n                # noinspection HttpUrlsUsage\n                print_error(\n                    msg=\"Server URL is missing either http:\/\/ or https:\/\/.\",\n                    newline=True,\n                    exit_after=False,\n                )\n            else:\n                break\n\n        if server_url[-1] == \"\/\":\n            server_url = server_url[:-1]\n\n        set_config_value(\"shippy\", \"server\", server_url)\n\n        return server_url\n    except KeyboardInterrupt:\n        exit(0)\n\n\ndef get_token(server_url):\n    while True:\n        from getpass import getpass\n\n        try:\n            username = input(\"Enter your username: \")\n            password = getpass(prompt=\"Enter your password: \")\n\n            try:\n                token = login_to_server(username, password, server_url)\n                set_config_value(\"shippy\", \"token\", token)\n                return token\n            except LoginException as exception:\n                print_error(\n                    f\"{exception} Please try again.\", newline=True, exit_after=False\n                )\n        except KeyboardInterrupt:\n            exit(0)\n\n\nif __name__ == \"__main__\":\n    main()\n"}},"msg":"main: check for corrupt configuration file\n\nCaveat: will not detect all tampering and corruption, but will\nguard against a malformed URL crashing shippy.\n\nFixes #93"}},"https:\/\/github.com\/TDAmeritrade\/stumpy":{"b7488368fbcad4e0f8575d26071a12745c09c997":{"url":"https:\/\/api.github.com\/repos\/TDAmeritrade\/stumpy\/commits\/b7488368fbcad4e0f8575d26071a12745c09c997","html_url":"https:\/\/github.com\/TDAmeritrade\/stumpy\/commit\/b7488368fbcad4e0f8575d26071a12745c09c997","message":"Fixed #621 Add param mp to stumpi (#817)\n\n* add param mp to naive stumpi\r\n\r\n* copy input to avoioid tampering with the user input\r\n\r\n* add test function, expecting error\r\n\r\n* Allowed user to pass precomputed mp\r\n\r\n* add pragma no cover\r\n\r\n* add param mp to performant aampi\r\n\r\n* fix docstring\r\n\r\n* add param mp to naive aampi\r\n\r\n* minor changes\r\n\r\n* add test function for aampi\r\n\r\n* fix test function\r\n\r\n* revise docstring\r\n\r\n* revise error message\r\n\r\n* minor changes\r\n\r\n* fix naive\r\n\r\n* revise ValueError message\r\n\r\n* add missing space","sha":"b7488368fbcad4e0f8575d26071a12745c09c997","keyword":"tampering fix","diff":"diff --git a\/stumpy\/aampi.py b\/stumpy\/aampi.py\nindex 9a045918e..8ef3a472e 100644\n--- a\/stumpy\/aampi.py\n+++ b\/stumpy\/aampi.py\n@@ -35,6 +35,14 @@ class aampi:\n         Note that this will increase the total computational time and memory usage\n         when k > 1.\n \n+    mp : numpy.ndarry, default None\n+        A pre-computed matrix profile (and corresponding matrix profile indices).\n+        This is a 2D array of shape `(len(T) - m + 1, 2 * k + 2)`, where the first `k`\n+        columns are top-k matrix profile, and the next `k` columns are their\n+        corresponding indices. The last two columns correspond to the top-1 left and\n+        top-1 right matrix profile indices. When None (default), this array is computed\n+        internally using `stumpy.aamp`.\n+\n     Attributes\n     ----------\n     P_ : numpy.ndarray\n@@ -69,9 +77,9 @@ class aampi:\n     Note that we have extended this algorithm for AB-joins as well.\n     \"\"\"\n \n-    def __init__(self, T, m, egress=True, p=2.0, k=1):\n+    def __init__(self, T, m, egress=True, p=2.0, k=1, mp=None):\n         \"\"\"\n-        Initialize the `stumpi` object\n+        Initialize the `aampi` object\n \n         Parameters\n         ----------\n@@ -89,11 +97,18 @@ def __init__(self, T, m, egress=True, p=2.0, k=1):\n         p : float, default 2.0\n             The p-norm to apply for computing the Minkowski distance.\n \n-\n         k : int, default 1\n             The number of top `k` smallest distances used to construct the matrix\n             profile. Note that this will increase the total computational time and\n             memory usage when k > 1.\n+\n+        mp : numpy.ndarry, default None\n+            A pre-computed matrix profile (and corresponding matrix profile indices).\n+            This is a 2D array of shape `(len(T) - m + 1, 2 * k + 2)`, where the first\n+            `k` columns are top-k matrix profile, and the next `k` columns are their\n+            corresponding indices. The last two columns correspond to the top-1 left\n+            and top-1 right matrix profile indices. When None (default), this array is\n+            computed internally using `stumpy.aamp`.\n         \"\"\"\n         self._T = core._preprocess(T)\n         core.check_window_size(m, max_size=self._T.shape[-1])\n@@ -104,7 +119,21 @@ def __init__(self, T, m, egress=True, p=2.0, k=1):\n         self._p = p\n         self._k = k\n \n-        mp = aamp(self._T, self._m, p=self._p, k=self._k)\n+        if mp is None:\n+            mp = aamp(self._T, self._m, p=self._p, k=self._k)\n+        else:\n+            mp = mp.copy()\n+\n+        if mp.shape != (\n+            len(self._T) - self._m + 1,\n+            2 * self._k + 2,\n+        ):  # pragma: no cover\n+            msg = (\n+                f\"The shape of `mp` must match ({len(T)-m+1}, {2 * k + 2}) but \"\n+                + f\"found {mp.shape} instead.\"\n+            )\n+            raise ValueError(msg)\n+\n         self._P = mp[:, : self._k].astype(np.float64)\n         self._I = mp[:, self._k : 2 * self._k].astype(np.int64)\n         self._left_I = mp[:, 2 * self._k].astype(np.int64)\ndiff --git a\/stumpy\/stumpi.py b\/stumpy\/stumpi.py\nindex 5220f3034..328009c12 100644\n--- a\/stumpy\/stumpi.py\n+++ b\/stumpy\/stumpi.py\n@@ -42,6 +42,14 @@ class stumpi:\n         Note that this will increase the total computational time and memory usage\n         when k > 1.\n \n+    mp : numpy.ndarry, default None\n+        A pre-computed matrix profile (and corresponding matrix profile indices).\n+        This is a 2D array of shape `(len(T) - m + 1, 2 * k + 2)`, where the first `k`\n+        columns are top-k matrix profile, and the next `k` columns are their\n+        corresponding indices. The last two columns correspond to the top-1 left and\n+        top-1 right matrix profile indices. When None (default), this array is computed\n+        internally using `stumpy.stump`.\n+\n     Attributes\n     ----------\n     P_ : numpy.ndarray\n@@ -95,7 +103,7 @@ class stumpi:\n     array([-1,  0,  1,  2])\n     \"\"\"\n \n-    def __init__(self, T, m, egress=True, normalize=True, p=2.0, k=1):\n+    def __init__(self, T, m, egress=True, normalize=True, p=2.0, k=1, mp=None):\n         \"\"\"\n         Initialize the `stumpi` object\n \n@@ -125,6 +133,14 @@ def __init__(self, T, m, egress=True, normalize=True, p=2.0, k=1):\n             The number of top `k` smallest distances used to construct the matrix\n             profile. Note that this will increase the total computational time and\n             memory usage when `k > 1`.\n+\n+        mp : numpy.ndarry, default None\n+            A pre-computed matrix profile (and corresponding matrix profile indices).\n+            This is a 2D array of shape `(len(T) - m + 1, 2 * k + 2)`, where the first\n+            `k` columns are top-k matrix profile, and the next `k` columns are their\n+            corresponding indices. The last two columns correspond to the top-1 left\n+            and top-1 right matrix profile indices. When None (default), this array is\n+            computed internally using `stumpy.stump`.\n         \"\"\"\n         self._T = core._preprocess(T)\n         core.check_window_size(m, max_size=self._T.shape[-1])\n@@ -136,7 +152,21 @@ def __init__(self, T, m, egress=True, normalize=True, p=2.0, k=1):\n         self._T_isfinite = np.isfinite(self._T)\n         self._egress = egress\n \n-        mp = stump(self._T, self._m, k=self._k)\n+        if mp is None:\n+            mp = stump(self._T, self._m, k=self._k)\n+        else:\n+            mp = mp.copy()\n+\n+        if mp.shape != (\n+            len(self._T) - self._m + 1,\n+            2 * self._k + 2,\n+        ):  # pragma: no cover\n+            msg = (\n+                f\"The shape of `mp` must match ({len(T)-m+1}, {2 * k + 2}) but \"\n+                + f\"found {mp.shape} instead.\"\n+            )\n+            raise ValueError(msg)\n+\n         self._P = mp[:, : self._k].astype(np.float64)\n         self._I = mp[:, self._k : 2 * self._k].astype(np.int64)\n \ndiff --git a\/tests\/naive.py b\/tests\/naive.py\nindex bca2df9fa..43e8b124a 100644\n--- a\/tests\/naive.py\n+++ b\/tests\/naive.py\n@@ -806,7 +806,7 @@ def get_array_ranges(a, n_chunks, truncate):\n \n \n class aampi_egress(object):\n-    def __init__(self, T, m, excl_zone=None, p=2.0, k=1):\n+    def __init__(self, T, m, excl_zone=None, p=2.0, k=1, mp=None):\n         self._T = np.asarray(T)\n         self._T = self._T.copy()\n         self._T_isfinite = np.isfinite(self._T)\n@@ -819,7 +819,10 @@ def __init__(self, T, m, excl_zone=None, p=2.0, k=1):\n             self._excl_zone = int(np.ceil(self._m \/ config.STUMPY_EXCL_ZONE_DENOM))\n \n         self._l = self._T.shape[0] - m + 1\n-        mp = aamp(T, m, exclusion_zone=self._excl_zone, p=p, k=self._k)\n+        if mp is None:\n+            mp = aamp(self._T, self._m, exclusion_zone=self._excl_zone, p=p, k=self._k)\n+        else:\n+            mp = mp.copy()\n         self._P = mp[:, :k].astype(np.float64)\n         self._I = mp[:, k : 2 * k].astype(np.int64)\n \n@@ -907,7 +910,7 @@ def left_I_(self):\n \n \n class stumpi_egress(object):\n-    def __init__(self, T, m, excl_zone=None, k=1):\n+    def __init__(self, T, m, excl_zone=None, k=1, mp=None):\n         self._T = np.asarray(T)\n         self._T = self._T.copy()\n         self._T_isfinite = np.isfinite(self._T)\n@@ -919,7 +922,10 @@ def __init__(self, T, m, excl_zone=None, k=1):\n             self._excl_zone = int(np.ceil(self._m \/ config.STUMPY_EXCL_ZONE_DENOM))\n \n         self._l = self._T.shape[0] - m + 1\n-        mp = stump(T, m, exclusion_zone=self._excl_zone, k=self._k)\n+        if mp is None:\n+            mp = stump(self._T, self._m, exclusion_zone=self._excl_zone, k=self._k)\n+        else:\n+            mp = mp.copy()\n         self._P = mp[:, :k].astype(np.float64)\n         self._I = mp[:, k : 2 * k].astype(np.int64)\n \ndiff --git a\/tests\/test_aampi.py b\/tests\/test_aampi.py\nindex a6c120212..745404b12 100644\n--- a\/tests\/test_aampi.py\n+++ b\/tests\/test_aampi.py\n@@ -1146,3 +1146,64 @@ def test_aampi_self_join_egress_KNN():\n                 npt.assert_almost_equal(ref_I, comp_I)\n                 npt.assert_almost_equal(ref_left_P, comp_left_P)\n                 npt.assert_almost_equal(ref_left_I, comp_left_I)\n+\n+\n+def test_aampi_self_join_egress_passing_mp():\n+    m = 3\n+\n+    for p in [1.0, 2.0, 3.0]:\n+        seed = np.random.randint(100000)\n+        np.random.seed(seed)\n+\n+        n = 30\n+        T = np.random.rand(n)\n+        mp = naive.aamp(T, m, p=p)\n+\n+        ref_mp = naive.aampi_egress(T, m, p=p, mp=mp)\n+        ref_P = ref_mp.P_.copy()\n+        ref_I = ref_mp.I_\n+        ref_left_P = ref_mp.left_P_.copy()\n+        ref_left_I = ref_mp.left_I_\n+\n+        stream = aampi(T, m, egress=True, p=p, mp=mp)\n+\n+        comp_P = stream.P_.copy()\n+        comp_I = stream.I_\n+        comp_left_P = stream.left_P_.copy()\n+        comp_left_I = stream.left_I_\n+\n+        naive.replace_inf(ref_P)\n+        naive.replace_inf(ref_left_P)\n+        naive.replace_inf(comp_P)\n+        naive.replace_inf(comp_left_P)\n+\n+        npt.assert_almost_equal(ref_P, comp_P)\n+        npt.assert_almost_equal(ref_I, comp_I)\n+        npt.assert_almost_equal(ref_left_P, comp_left_P)\n+        npt.assert_almost_equal(ref_left_I, comp_left_I)\n+\n+        for i in range(34):\n+            t = np.random.rand()\n+\n+            ref_mp.update(t)\n+            stream.update(t)\n+\n+            comp_P = stream.P_.copy()\n+            comp_I = stream.I_\n+            comp_left_P = stream.left_P_.copy()\n+            comp_left_I = stream.left_I_\n+\n+            ref_P = ref_mp.P_.copy()\n+            ref_I = ref_mp.I_\n+            ref_left_P = ref_mp.left_P_.copy()\n+            ref_left_I = ref_mp.left_I_\n+\n+            naive.replace_inf(ref_P)\n+            naive.replace_inf(ref_left_P)\n+            naive.replace_inf(comp_P)\n+            naive.replace_inf(comp_left_P)\n+\n+            npt.assert_almost_equal(ref_P, comp_P)\n+            npt.assert_almost_equal(ref_I, comp_I)\n+            npt.assert_almost_equal(ref_left_P, comp_left_P)\n+            npt.assert_almost_equal(ref_left_I, comp_left_I)\ndiff --git a\/tests\/test_stumpi.py b\/tests\/test_stumpi.py\nindex 53d6fccb9..d0f81de0c 100644\n--- a\/tests\/test_stumpi.py\n+++ b\/tests\/test_stumpi.py\n@@ -1061,3 +1061,61 @@ def test_stumpi_self_join_egress_KNN():\n             npt.assert_almost_equal(ref_I, comp_I)\n             npt.assert_almost_equal(ref_left_P, comp_left_P)\n             npt.assert_almost_equal(ref_left_I, comp_left_I)\n+\n+\n+def test_stumpi_self_join_egress_passing_mp():\n+    m = 3\n+\n+    seed = np.random.randint(100000)\n+    np.random.seed(seed)\n+    n = 30\n+    T = np.random.rand(n)\n+    mp = naive.stump(T, m)\n+\n+    ref_mp = naive.stumpi_egress(T, m, mp=mp)\n+    ref_P = ref_mp.P_.copy()\n+    ref_I = ref_mp.I_\n+    ref_left_P = ref_mp.left_P_.copy()\n+    ref_left_I = ref_mp.left_I_\n+\n+    stream = stumpi(T, m, egress=True, mp=mp)\n+\n+    comp_P = stream.P_.copy()\n+    comp_I = stream.I_\n+    comp_left_P = stream.left_P_.copy()\n+    comp_left_I = stream.left_I_\n+\n+    naive.replace_inf(ref_P)\n+    naive.replace_inf(ref_left_P)\n+    naive.replace_inf(comp_P)\n+    naive.replace_inf(comp_left_P)\n+\n+    npt.assert_almost_equal(ref_P, comp_P)\n+    npt.assert_almost_equal(ref_I, comp_I)\n+    npt.assert_almost_equal(ref_left_P, comp_left_P)\n+    npt.assert_almost_equal(ref_left_I, comp_left_I)\n+\n+    for i in range(34):\n+        t = np.random.rand()\n+        ref_mp.update(t)\n+        stream.update(t)\n+\n+        comp_P = stream.P_.copy()\n+        comp_I = stream.I_\n+        comp_left_P = stream.left_P_.copy()\n+        comp_left_I = stream.left_I_\n+\n+        ref_P = ref_mp.P_.copy()\n+        ref_I = ref_mp.I_\n+        ref_left_P = ref_mp.left_P_.copy()\n+        ref_left_I = ref_mp.left_I_\n+\n+        naive.replace_inf(ref_P)\n+        naive.replace_inf(ref_left_P)\n+        naive.replace_inf(comp_P)\n+        naive.replace_inf(comp_left_P)\n+\n+        npt.assert_almost_equal(ref_P, comp_P)\n+        npt.assert_almost_equal(ref_I, comp_I)\n+        npt.assert_almost_equal(ref_left_P, comp_left_P)\n+        npt.assert_almost_equal(ref_left_I, comp_left_I)\n","files":{"\/stumpy\/aampi.py":{"changes":[{"diff":"\n     Note that we have extended this algorithm for AB-joins as well.\n     \"\"\"\n \n-    def __init__(self, T, m, egress=True, p=2.0, k=1):\n+    def __init__(self, T, m, egress=True, p=2.0, k=1, mp=None):\n         \"\"\"\n-        Initialize the `stumpi` object\n+        Initialize the `aampi` object\n \n         Parameters\n         ----------\n","add":2,"remove":2,"filename":"\/stumpy\/aampi.py","badparts":["    def __init__(self, T, m, egress=True, p=2.0, k=1):","        Initialize the `stumpi` object","---------"],"goodparts":["    def __init__(self, T, m, egress=True, p=2.0, k=1, mp=None):","        Initialize the `aampi` object"]},{"diff":"\n         self._p = p\n         self._k = k\n \n-        mp = aamp(self._T, self._m, p=self._p, k=self._k)\n+        if mp is None:\n+            mp = aamp(self._T, self._m, p=self._p, k=self._k)\n+        else:\n+            mp = mp.copy()\n+\n+        if mp.shape != (\n+            len(self._T) - self._m + 1,\n+            2 * self._k + 2,\n+        ):  # pragma: no cover\n+            msg = (\n+                f\"The shape of `mp` must match ({len(T)-m+1}, {2 * k + 2}) but \"\n+                + f\"found {mp.shape} instead.\"\n+            )\n+            raise ValueError(msg)\n+\n         self._P = mp[:, : self._k].astype(np.float64)\n         self._I = mp[:, self._k : 2 * self._k].astype(np.int64)\n         self._left_I = mp[:, 2 * self._k].astype(np.int64)","add":15,"remove":1,"filename":"\/stumpy\/aampi.py","badparts":["        mp = aamp(self._T, self._m, p=self._p, k=self._k)"],"goodparts":["        if mp is None:","            mp = aamp(self._T, self._m, p=self._p, k=self._k)","        else:","            mp = mp.copy()","        if mp.shape != (","            len(self._T) - self._m + 1,","            2 * self._k + 2,","        ):  # pragma: no cover","            msg = (","                f\"The shape of `mp` must match ({len(T)-m+1}, {2 * k + 2}) but \"","                + f\"found {mp.shape} instead.\"","            )","            raise ValueError(msg)"]}],"source":"\n import numpy as np from. import config, core from.aamp import aamp class aampi: \"\"\" Compute an incremental non-normalized(i.e., without z-normalization) matrix profile for streaming data Parameters ---------- T: numpy.ndarray The time series or sequence for which the non-normalized matrix profile and matrix profile indices will be returned m: int Window size egress: bool, default True If set to `True`, the oldest data point in the time series is removed and the time series length remains constant rather than forever increasing p: float, default 2.0 The p-norm to apply for computing the Minkowski distance. k: int, default 1 The number of top `k` smallest distances used to construct the matrix profile. Note that this will increase the total computational time and memory usage when k > 1. Attributes ---------- P_: numpy.ndarray The updated matrix profile for `T` I_: numpy.ndarray The updated matrix profile indices for `T` left_P_: numpy.ndarray The updated left matrix profile for `T` left_I_: numpy.ndarray The updated left matrix profile indices for `T` T_: numpy.ndarray The updated time series or sequence for which the matrix profile and matrix profile indices are computed Methods ------- update(t) Append a single new data point, `t`, to the time series, `T`, and update the matrix profile Notes ----- `arXiv:1901.05708 \\ <https:\/\/arxiv.org\/pdf\/1901.05708.pdf>`__ See Algorithm 1 Note that we have extended this algorithm for AB-joins as well. \"\"\" def __init__(self, T, m, egress=True, p=2.0, k=1): \"\"\" Initialize the `stumpi` object Parameters ---------- T: numpy.ndarray The time series or sequence for which the unnormalized matrix profile and matrix profile indices will be returned m: int Window size egress: bool, default True If set to `True`, the oldest data point in the time series is removed and the time series length remains constant rather than forever increasing p: float, default 2.0 The p-norm to apply for computing the Minkowski distance. k: int, default 1 The number of top `k` smallest distances used to construct the matrix profile. Note that this will increase the total computational time and memory usage when k > 1. \"\"\" self._T=core._preprocess(T) core.check_window_size(m, max_size=self._T.shape[-1]) self._m=m self._n=self._T.shape[0] self._excl_zone=int(np.ceil(self._m \/ config.STUMPY_EXCL_ZONE_DENOM)) self._egress=egress self._p=p self._k=k mp=aamp(self._T, self._m, p=self._p, k=self._k) self._P=mp[:,: self._k].astype(np.float64) self._I=mp[:, self._k: 2 * self._k].astype(np.int64) self._left_I=mp[:, 2 * self._k].astype(np.int64) self._left_P=np.full_like(self._left_I, np.inf, dtype=np.float64) self._left_P[:]=np.inf self._T_isfinite=np.isfinite(self._T) self._T, self._T_subseq_isfinite=core.preprocess_non_normalized( self._T, self._m ) mask=self._left_I==self._I[:, 0] self._left_P[mask]=self._P[mask, 0] for i in np.flatnonzero(self._left_I >=0 & ~mask): j=self._left_I[i] if j >=0: self._left_P[i]=np.linalg.norm( self._T[i: i +self._m] -self._T[j: j +self._m], ord=self._p ) Q=self._T[-self._m:] self._p_norm=core.mass_absolute(Q, self._T, p=self._p) ** self._p if self._egress: self._p_norm_new=np.empty(self._p_norm.shape[0], dtype=np.float64) self._n_appended=0 def update(self, t): \"\"\" Append a single new data point, `t`, to the existing time series `T` and update the non-normalized(i.e., without z-normalization) matrix profile and matrix profile indices. Parameters ---------- t: float A single new data point to be appended to `T` Notes ----- `arXiv:1901.05708 \\ <https:\/\/arxiv.org\/pdf\/1901.05708.pdf>`__ See Algorithm 1 Note that we have extended this algorithm for AB-joins as well. \"\"\" if self._egress: self._update_egress(t) else: self._update(t) def _update_egress(self, t): \"\"\" Ingress a new data point, egress the oldest data point, and update the matrix profile and matrix profile indices Parameters ---------- t: float A single new data point to be appended to `T` \"\"\" self._n=self._T.shape[0] l=self._n -self._m +1 -1 self._T[:-1]=self._T[1:] self._T[-1]=t self._n_appended +=1 self._p_norm[:-1]=self._p_norm[1:] S=self._T[l:] t_drop=self._T[l -1] self._T_isfinite[:-1]=self._T_isfinite[1:] self._T_subseq_isfinite[:-1]=self._T_subseq_isfinite[1:] self._I[:-1]=self._I[1:] self._P[:-1]=self._P[1:] self._left_I[:-1]=self._left_I[1:] self._left_P[:-1]=self._left_P[1:] if np.isfinite(t): self._T_isfinite[-1]=True else: self._T_isfinite[-1]=False t=0 self._T[-1]=0 S[-1]=0 self._T_subseq_isfinite[-1]=np.all(self._T_isfinite[-self._m:]) self._p_norm_new[1:]=( self._p_norm[:l] -np.power(abs(self._T[:l] -t_drop), self._p) +np.power(abs(self._T[self._m:] -t), self._p) ) self._p_norm_new[0]=( np.linalg.norm(self._T[: self._m] -S[: self._m], ord=self._p) ** self._p ) mask=self._p_norm_new < config.STUMPY_P_NORM_THRESHOLD self._p_norm_new[mask]=0 D=np.power(self._p_norm_new, 1.0 \/ self._p) D[~self._T_subseq_isfinite]=np.inf if np.any(~self._T_isfinite[-self._m:]): D[:]=np.inf core.apply_exclusion_zone(D, D.shape[0] -1, self._excl_zone, np.inf) update_idx=np.argwhere(D < self._P[:, -1]).flatten() for i in update_idx: idx=np.searchsorted(self._P[i], D[i], side=\"right\") core._shift_insert_at_index(self._P[i], idx, D[i]) core._shift_insert_at_index( self._I[i], idx, D.shape[0] +self._n_appended -1 ) self._P[-1]=np.inf self._I[-1]=-1 for i, d in enumerate(D): if d < self._P[-1, -1]: idx=np.searchsorted(self._P[-1], d, side=\"right\") core._shift_insert_at_index(self._P[-1], idx, d) core._shift_insert_at_index(self._I[-1], idx, i +self._n_appended) self._left_P[-1]=self._P[-1, 0] self._left_I[-1]=self._I[-1, 0] self._p_norm[:]=self._p_norm_new def _update(self, t): \"\"\" Ingress a new data point and update the(top-k) matrix profile and matrix profile indices without egressing the oldest data point Parameters ---------- t: float A single new data point to be appended to `T` \"\"\" self._n=self._T.shape[0] l=self._n -self._m +1 T_new=np.append(self._T, t) p_norm_new=np.empty(self._p_norm.shape[0] +1, dtype=np.float64) S=T_new[l:] t_drop=T_new[l -1] if np.isfinite(t): self._T_isfinite=np.append(self._T_isfinite, True) else: self._T_isfinite=np.append(self._T_isfinite, False) t=0 T_new[-1]=0 S[-1]=0 self._T_subseq_isfinite=np.append( self._T_subseq_isfinite, np.all(self._T_isfinite[-self._m:]) ) p_norm_new[1:]=( self._p_norm[:l] -np.power(abs(T_new[:l] -t_drop), self._p) +np.power(abs(T_new[self._m:] -t), self._p) ) p_norm_new[0]=( np.linalg.norm(T_new[: self._m] -S[: self._m], ord=self._p) ** self._p ) mask=p_norm_new < config.STUMPY_P_NORM_THRESHOLD p_norm_new[mask]=0 D=np.power(p_norm_new, 1.0 \/ self._p) D[~self._T_subseq_isfinite]=np.inf if np.any(~self._T_isfinite[-self._m:]): D[:]=np.inf core.apply_exclusion_zone(D, D.shape[0] -1, self._excl_zone, np.inf) update_idx=np.argwhere(D[:l] < self._P[:l, -1]).flatten() for i in update_idx: idx=np.searchsorted(self._P[i], D[i], side=\"right\") core._shift_insert_at_index(self._P[i], idx, D[i]) core._shift_insert_at_index(self._I[i], idx, l) P_new=np.full(self._k, np.inf, dtype=np.float64) I_new=np.full(self._k, -1, dtype=np.int64) for i, d in enumerate(D): if d < P_new[-1]: idx=np.searchsorted(P_new, d, side=\"right\") core._shift_insert_at_index(P_new, idx, d) core._shift_insert_at_index(I_new, idx, i) left_I_new=I_new[0] left_P_new=P_new[0] self._T=T_new self._P=np.append(self._P, P_new.reshape(1, -1), axis=0) self._I=np.append(self._I, I_new.reshape(1, -1), axis=0) self._left_P=np.append(self._left_P, left_P_new) self._left_I=np.append(self._left_I, left_I_new) self._p_norm=p_norm_new @property def P_(self): \"\"\" Get the(top-k) matrix profile. When `k=1`(default), the output is a 1D array consisting of the matrix profile. When `k > 1`, the output is a 2D array that has exactly `k` columns and it consists of the top-k matrix profile. \"\"\" if self._k==1: return self._P.flatten().astype(np.float64) else: return self._P.astype(np.float64) @property def I_(self): \"\"\" Get the(top-k) matrix profile indices. When `k=1`(default), the output is a 1D array consisting of the matrix profile indices. When `k > 1`, the output is a 2D array that has exactly `k` columns and it consists of the top-k matrix profile indices. \"\"\" if self._k==1: return self._I.flatten().astype(np.int64) else: return self._I.astype(np.int64) @property def left_P_(self): \"\"\" Get the(top-1) left matrix profile \"\"\" return self._left_P.astype(np.float64) @property def left_I_(self): \"\"\" Get the(top-1) left matrix profile indices \"\"\" return self._left_I.astype(np.int64) @property def T_(self): \"\"\" Get the time series \"\"\" return self._T ","sourceWithComments":"# STUMPY\n# Copyright 2019 TD Ameritrade. Released under the terms of the 3-Clause BSD license.\n# STUMPY is a trademark of TD Ameritrade IP Company, Inc. All rights reserved.\n\nimport numpy as np\n\nfrom . import config, core\nfrom .aamp import aamp\n\n\nclass aampi:\n    # needs to be enhanced to support top-k matrix profile\n    \"\"\"\n    Compute an incremental non-normalized (i.e., without z-normalization) matrix profile\n    for streaming data\n\n    Parameters\n    ----------\n    T : numpy.ndarray\n        The time series or sequence for which the non-normalized matrix profile and\n        matrix profile indices will be returned\n\n    m : int\n        Window size\n\n    egress : bool, default True\n        If set to `True`, the oldest data point in the time series is removed and\n        the time series length remains constant rather than forever increasing\n\n    p : float, default 2.0\n        The p-norm to apply for computing the Minkowski distance.\n\n    k : int, default 1\n        The number of top `k` smallest distances used to construct the matrix profile.\n        Note that this will increase the total computational time and memory usage\n        when k > 1.\n\n    Attributes\n    ----------\n    P_ : numpy.ndarray\n        The updated matrix profile for `T`\n\n    I_ : numpy.ndarray\n        The updated matrix profile indices for `T`\n\n    left_P_ : numpy.ndarray\n        The updated left matrix profile for `T`\n\n    left_I_ : numpy.ndarray\n        The updated left matrix profile indices for `T`\n\n    T_ : numpy.ndarray\n        The updated time series or sequence for which the matrix profile and matrix\n        profile indices are computed\n\n    Methods\n    -------\n    update(t)\n        Append a single new data point, `t`, to the time series, `T`, and update the\n        matrix profile\n\n    Notes\n    -----\n    `arXiv:1901.05708 \\\n    <https:\/\/arxiv.org\/pdf\/1901.05708.pdf>`__\n\n    See Algorithm 1\n\n    Note that we have extended this algorithm for AB-joins as well.\n    \"\"\"\n\n    def __init__(self, T, m, egress=True, p=2.0, k=1):\n        \"\"\"\n        Initialize the `stumpi` object\n\n        Parameters\n        ----------\n        T : numpy.ndarray\n            The time series or sequence for which the unnormalized matrix profile and\n            matrix profile indices will be returned\n\n        m : int\n            Window size\n\n        egress : bool, default True\n            If set to `True`, the oldest data point in the time series is removed and\n            the time series length remains constant rather than forever increasing\n\n        p : float, default 2.0\n            The p-norm to apply for computing the Minkowski distance.\n\n\n        k : int, default 1\n            The number of top `k` smallest distances used to construct the matrix\n            profile. Note that this will increase the total computational time and\n            memory usage when k > 1.\n        \"\"\"\n        self._T = core._preprocess(T)\n        core.check_window_size(m, max_size=self._T.shape[-1])\n        self._m = m\n        self._n = self._T.shape[0]\n        self._excl_zone = int(np.ceil(self._m \/ config.STUMPY_EXCL_ZONE_DENOM))\n        self._egress = egress\n        self._p = p\n        self._k = k\n\n        mp = aamp(self._T, self._m, p=self._p, k=self._k)\n        self._P = mp[:, : self._k].astype(np.float64)\n        self._I = mp[:, self._k : 2 * self._k].astype(np.int64)\n        self._left_I = mp[:, 2 * self._k].astype(np.int64)\n        self._left_P = np.full_like(self._left_I, np.inf, dtype=np.float64)\n        self._left_P[:] = np.inf\n\n        self._T_isfinite = np.isfinite(self._T)\n        self._T, self._T_subseq_isfinite = core.preprocess_non_normalized(\n            self._T, self._m\n        )\n\n        # Retrieve the left matrix profile values\n\n        # Since each matrix profile value is the minimum between the left and right\n        # matrix profile values, we can save time by re-computing only the left matrix\n        # profile value when the matrix profile index is equal to the right matrix\n        # profile index.\n        mask = self._left_I == self._I[:, 0]\n        self._left_P[mask] = self._P[mask, 0]\n\n        # Only re-compute the `i`-th left matrix profile value, `self._left_P[i]`,\n        # when `self._I[i] != self._left_I[i]`\n        for i in np.flatnonzero(self._left_I >= 0 & ~mask):\n            j = self._left_I[i]\n            if j >= 0:\n                self._left_P[i] = np.linalg.norm(\n                    self._T[i : i + self._m] - self._T[j : j + self._m], ord=self._p\n                )\n\n        Q = self._T[-self._m :]\n        self._p_norm = core.mass_absolute(Q, self._T, p=self._p) ** self._p\n        if self._egress:\n            self._p_norm_new = np.empty(self._p_norm.shape[0], dtype=np.float64)\n            self._n_appended = 0\n\n    def update(self, t):\n        \"\"\"\n        Append a single new data point, `t`, to the existing time series `T` and update\n        the non-normalized (i.e., without z-normalization) matrix profile and matrix\n        profile indices.\n\n        Parameters\n        ----------\n        t : float\n            A single new data point to be appended to `T`\n\n        Notes\n        -----\n        `arXiv:1901.05708 \\\n        <https:\/\/arxiv.org\/pdf\/1901.05708.pdf>`__\n\n        See Algorithm 1\n\n        Note that we have extended this algorithm for AB-joins as well.\n        \"\"\"\n        if self._egress:\n            self._update_egress(t)\n        else:\n            self._update(t)\n\n    def _update_egress(self, t):\n        \"\"\"\n        Ingress a new data point, egress the oldest data point, and update the matrix\n        profile and matrix profile indices\n\n        Parameters\n        ----------\n        t : float\n            A single new data point to be appended to `T`\n        \"\"\"\n        self._n = self._T.shape[0]\n        l = self._n - self._m + 1 - 1  # Subtract 1 due to egress\n        self._T[:-1] = self._T[1:]\n        self._T[-1] = t\n        self._n_appended += 1\n        self._p_norm[:-1] = self._p_norm[1:]\n        S = self._T[l:]\n        t_drop = self._T[l - 1]\n        self._T_isfinite[:-1] = self._T_isfinite[1:]\n        self._T_subseq_isfinite[:-1] = self._T_subseq_isfinite[1:]\n\n        self._I[:-1] = self._I[1:]\n        self._P[:-1] = self._P[1:]\n        self._left_I[:-1] = self._left_I[1:]\n        self._left_P[:-1] = self._left_P[1:]\n\n        if np.isfinite(t):\n            self._T_isfinite[-1] = True\n        else:\n            self._T_isfinite[-1] = False\n            t = 0\n            self._T[-1] = 0\n            S[-1] = 0\n\n        self._T_subseq_isfinite[-1] = np.all(self._T_isfinite[-self._m :])\n\n        self._p_norm_new[1:] = (\n            self._p_norm[:l]\n            - np.power(abs(self._T[:l] - t_drop), self._p)\n            + np.power(abs(self._T[self._m :] - t), self._p)\n        )\n        self._p_norm_new[0] = (\n            np.linalg.norm(self._T[: self._m] - S[: self._m], ord=self._p) ** self._p\n        )\n\n        mask = self._p_norm_new < config.STUMPY_P_NORM_THRESHOLD\n        self._p_norm_new[mask] = 0\n\n        D = np.power(self._p_norm_new, 1.0 \/ self._p)\n        D[~self._T_subseq_isfinite] = np.inf\n        if np.any(~self._T_isfinite[-self._m :]):\n            D[:] = np.inf\n\n        core.apply_exclusion_zone(D, D.shape[0] - 1, self._excl_zone, np.inf)\n\n        update_idx = np.argwhere(D < self._P[:, -1]).flatten()\n        for i in update_idx:\n            idx = np.searchsorted(self._P[i], D[i], side=\"right\")\n            core._shift_insert_at_index(self._P[i], idx, D[i])\n            core._shift_insert_at_index(\n                self._I[i], idx, D.shape[0] + self._n_appended - 1\n            )\n            # D.shape[0] is base-1\n\n        # Calculate the (top-k) matrix profile values\/indices for the last subsequence\n        # by using its corresponding distance profile `D`\n        self._P[-1] = np.inf\n        self._I[-1] = -1\n        for i, d in enumerate(D):\n            if d < self._P[-1, -1]:\n                idx = np.searchsorted(self._P[-1], d, side=\"right\")\n                core._shift_insert_at_index(self._P[-1], idx, d)\n                core._shift_insert_at_index(self._I[-1], idx, i + self._n_appended)\n\n        # All neighbors of the last subsequence are on its left. So, its (top-1)\n        # matrix profile value\/index and its left matrix profile value\/index must\n        # be equal.\n        self._left_P[-1] = self._P[-1, 0]\n        self._left_I[-1] = self._I[-1, 0]\n\n        self._p_norm[:] = self._p_norm_new\n\n    def _update(self, t):\n        \"\"\"\n        Ingress a new data point and update the (top-k) matrix profile and matrix\n        profile indices without egressing the oldest data point\n\n        Parameters\n        ----------\n        t : float\n            A single new data point to be appended to `T`\n        \"\"\"\n        self._n = self._T.shape[0]\n        l = self._n - self._m + 1\n        T_new = np.append(self._T, t)\n        p_norm_new = np.empty(self._p_norm.shape[0] + 1, dtype=np.float64)\n        S = T_new[l:]\n        t_drop = T_new[l - 1]\n\n        if np.isfinite(t):\n            self._T_isfinite = np.append(self._T_isfinite, True)\n        else:\n            self._T_isfinite = np.append(self._T_isfinite, False)\n            t = 0\n            T_new[-1] = 0\n            S[-1] = 0\n\n        self._T_subseq_isfinite = np.append(\n            self._T_subseq_isfinite, np.all(self._T_isfinite[-self._m :])\n        )\n\n        p_norm_new[1:] = (\n            self._p_norm[:l]\n            - np.power(abs(T_new[:l] - t_drop), self._p)\n            + np.power(abs(T_new[self._m :] - t), self._p)\n        )\n        p_norm_new[0] = (\n            np.linalg.norm(T_new[: self._m] - S[: self._m], ord=self._p) ** self._p\n        )\n\n        mask = p_norm_new < config.STUMPY_P_NORM_THRESHOLD\n        p_norm_new[mask] = 0\n\n        D = np.power(p_norm_new, 1.0 \/ self._p)\n        D[~self._T_subseq_isfinite] = np.inf\n        if np.any(~self._T_isfinite[-self._m :]):\n            D[:] = np.inf\n\n        core.apply_exclusion_zone(D, D.shape[0] - 1, self._excl_zone, np.inf)\n\n        update_idx = np.argwhere(D[:l] < self._P[:l, -1]).flatten()\n        for i in update_idx:\n            idx = np.searchsorted(self._P[i], D[i], side=\"right\")\n            core._shift_insert_at_index(self._P[i], idx, D[i])\n            core._shift_insert_at_index(self._I[i], idx, l)\n\n        # Calculating top-k matrix profile and (top-1) left matrix profile (and their\n        # corresponding indices) for new subsequence whose distance profile is `D`\n        P_new = np.full(self._k, np.inf, dtype=np.float64)\n        I_new = np.full(self._k, -1, dtype=np.int64)\n        for i, d in enumerate(D):\n            if d < P_new[-1]:  # maximum value in sorted array P_new\n                idx = np.searchsorted(P_new, d, side=\"right\")\n                core._shift_insert_at_index(P_new, idx, d)\n                core._shift_insert_at_index(I_new, idx, i)\n\n        left_I_new = I_new[0]\n        left_P_new = P_new[0]\n\n        self._T = T_new\n        self._P = np.append(self._P, P_new.reshape(1, -1), axis=0)\n        self._I = np.append(self._I, I_new.reshape(1, -1), axis=0)\n        self._left_P = np.append(self._left_P, left_P_new)\n        self._left_I = np.append(self._left_I, left_I_new)\n        self._p_norm = p_norm_new\n\n    @property\n    def P_(self):\n        \"\"\"\n        Get the (top-k) matrix profile. When `k=1` (default), the output is\n        a 1D array consisting of the matrix profile. When `k > 1`, the\n        output is a 2D array that has exactly `k` columns and it consists of the\n        top-k matrix profile.\n        \"\"\"\n        if self._k == 1:\n            return self._P.flatten().astype(np.float64)\n        else:\n            return self._P.astype(np.float64)\n\n    @property\n    def I_(self):\n        \"\"\"\n        Get the (top-k) matrix profile indices. When `k=1` (default), the output is\n        a 1D array consisting of the matrix profile indices. When `k > 1`, the\n        output is a 2D array that has exactly `k` columns and it consists of the\n        top-k matrix profile indices.\n        \"\"\"\n        if self._k == 1:\n            return self._I.flatten().astype(np.int64)\n        else:\n            return self._I.astype(np.int64)\n\n    @property\n    def left_P_(self):\n        \"\"\"\n        Get the (top-1) left matrix profile\n        \"\"\"\n        return self._left_P.astype(np.float64)\n\n    @property\n    def left_I_(self):\n        \"\"\"\n        Get the (top-1) left matrix profile indices\n        \"\"\"\n        return self._left_I.astype(np.int64)\n\n    @property\n    def T_(self):\n        \"\"\"\n        Get the time series\n        \"\"\"\n        return self._T\n"},"\/stumpy\/stumpi.py":{"changes":[{"diff":"\n     array([-1,  0,  1,  2])\n     \"\"\"\n \n-    def __init__(self, T, m, egress=True, normalize=True, p=2.0, k=1):\n+    def __init__(self, T, m, egress=True, normalize=True, p=2.0, k=1, mp=None):\n         \"\"\"\n         Initialize the `stumpi` object\n \n","add":1,"remove":1,"filename":"\/stumpy\/stumpi.py","badparts":["    def __init__(self, T, m, egress=True, normalize=True, p=2.0, k=1):"],"goodparts":["    def __init__(self, T, m, egress=True, normalize=True, p=2.0, k=1, mp=None):"]},{"diff":"\n         self._T_isfinite = np.isfinite(self._T)\n         self._egress = egress\n \n-        mp = stump(self._T, self._m, k=self._k)\n+        if mp is None:\n+            mp = stump(self._T, self._m, k=self._k)\n+        else:\n+            mp = mp.copy()\n+\n+        if mp.shape != (\n+            len(self._T) - self._m + 1,\n+            2 * self._k + 2,\n+        ):  # pragma: no cover\n+            msg = (\n+                f\"The shape of `mp` must match ({len(T)-m+1}, {2 * k + 2}) but \"\n+                + f\"found {mp.shape} instead.\"\n+            )\n+            raise ValueError(msg)\n+\n         self._P = mp[:, : self._k].astype(np.float64)\n         self._I = mp[:, self._k : 2 * self._k].astype(np.int64)\n","add":15,"remove":1,"filename":"\/stumpy\/stumpi.py","badparts":["        mp = stump(self._T, self._m, k=self._k)"],"goodparts":["        if mp is None:","            mp = stump(self._T, self._m, k=self._k)","        else:","            mp = mp.copy()","        if mp.shape != (","            len(self._T) - self._m + 1,","            2 * self._k + 2,","        ):  # pragma: no cover","            msg = (","                f\"The shape of `mp` must match ({len(T)-m+1}, {2 * k + 2}) but \"","                + f\"found {mp.shape} instead.\"","            )","            raise ValueError(msg)"]}],"source":"\n import numpy as np from. import config, core, stump from.aampi import aampi @core.non_normalized(aampi) class stumpi: \"\"\" Compute an incremental z-normalized matrix profile for streaming data This is based on the on-line STOMPI and STAMPI algorithms. Parameters ---------- T: numpy.ndarray The time series or sequence for which the matrix profile and matrix profile indices will be returned m: int Window size egress: bool, default True If set to `True`, the oldest data point in the time series is removed and the time series length remains constant rather than forever increasing normalize: bool, default True When set to `True`, this z-normalizes subsequences prior to computing distances. Otherwise, this class gets re-routed to its complementary non-normalized equivalent set in the `@core.non_normalized` class decorator. p: float, default 2.0 The p-norm to apply for computing the Minkowski distance. This parameter is ignored when `normalize==True`. k: int, default 1 The number of top `k` smallest distances used to construct the matrix profile. Note that this will increase the total computational time and memory usage when k > 1. Attributes ---------- P_: numpy.ndarray The updated(top-k) matrix profile for `T`. When `k=1`(default), the first (and only) column in this 2D array consists of the matrix profile. When `k > 1`, the output has exactly `k` columns consisting of the top-k matrix profile. I_: numpy.ndarray The updated(top-k) matrix profile indices for `T`. When `k=1`(default), the first(and only) column in this 2D array consists of the matrix profile indices. When `k > 1`, the output has exactly `k` columns consisting of the top-k matrix profile indices. left_P_: numpy.ndarray The updated left(top-1) matrix profile for `T` left_I_: numpy.ndarray The updated left(top-1) matrix profile indices for `T` T_: numpy.ndarray The updated time series or sequence for which the matrix profile and matrix profile indices are computed Methods ------- update(t) Append a single new data point, `t`, to the time series, `T`, and update the matrix profile Notes ----- `DOI: 10.1007\/s10618-017-0519-9 \\ <https:\/\/www.cs.ucr.edu\/~eamonn\/MP_journal.pdf>`__ See Table V Note that line 11 is missing an important `sqrt` operation! Examples -------- >>> import stumpy >>> import numpy as np >>> stream=stumpy.stumpi( ... np.array([584., -11., 23., 79., 1001., 0.]), ... m=3) >>> stream.update(-19.0) >>> stream.left_P_ array([ inf, 3.00009263, 2.69407392, 3.05656417]) >>> stream.left_I_ array([-1, 0, 1, 2]) \"\"\" def __init__(self, T, m, egress=True, normalize=True, p=2.0, k=1): \"\"\" Initialize the `stumpi` object Parameters ---------- T: numpy.ndarray The time series or sequence for which the matrix profile and matrix profile indices will be returned m: int Window size egress: bool, default True If set to `True`, the oldest data point in the time series is removed and the time series length remains constant rather than forever increasing normalize: bool, default True When set to `True`, this z-normalizes subsequences prior to computing distances. Otherwise, this class gets re-routed to its complementary non-normalized equivalent set in the `@core.non_normalized` class decorator. p: float, default 2.0 The p-norm to apply for computing the Minkowski distance. This parameter is ignored when `normalize==True`. k: int, default 1 The number of top `k` smallest distances used to construct the matrix profile. Note that this will increase the total computational time and memory usage when `k > 1`. \"\"\" self._T=core._preprocess(T) core.check_window_size(m, max_size=self._T.shape[-1]) self._m=m self._k=k self._n=self._T.shape[0] self._excl_zone=int(np.ceil(self._m \/ config.STUMPY_EXCL_ZONE_DENOM)) self._T_isfinite=np.isfinite(self._T) self._egress=egress mp=stump(self._T, self._m, k=self._k) self._P=mp[:,: self._k].astype(np.float64) self._I=mp[:, self._k: 2 * self._k].astype(np.int64) self._left_I=mp[:, 2 * self._k].astype(np.int64) self._left_P=np.full_like(self._left_I, np.inf, dtype=np.float64) self._T, self._M_T, self._\u03a3_T, self._T_subseq_isconstant=core.preprocess( self._T, self._m ) mask=self._left_I==self._I[:, 0] self._left_P[mask]=self._P[mask, 0] for i in np.flatnonzero(self._left_I >=0 & ~mask): j=self._left_I[i] QT=np.dot(self._T[i: i +self._m], self._T[j: j +self._m]) D_square=core._calculate_squared_distance( self._m, QT, self._M_T[i], self._\u03a3_T[i], self._M_T[j], self._\u03a3_T[j], self._T_subseq_isconstant[i], self._T_subseq_isconstant[j], ) self._left_P[i]=np.sqrt(D_square) Q=self._T[-self._m:] self._QT=core.sliding_dot_product(Q, self._T) if self._egress: self._QT_new=np.empty(self._QT.shape[0], dtype=np.float64) self._n_appended=0 def update(self, t): \"\"\" Append a single new data point, `t`, to the existing time series `T` and update the(top-k) matrix profile and matrix profile indices. Parameters ---------- t: float A single new data point to be appended to `T` Notes ----- `DOI: 10.1007\/s10618-017-0519-9 \\ <https:\/\/www.cs.ucr.edu\/~eamonn\/MP_journal.pdf>`__ See Table V Note that line 11 is missing an important `sqrt` operation! \"\"\" if self._egress: self._update_egress(t) else: self._update(t) def _update_egress(self, t): \"\"\" Ingress a new data point, egress the oldest data point, and update the(top-k) matrix profile and matrix profile indices Parameters ---------- t: float A single new data point to be appended to `T` \"\"\" self._n=self._T.shape[0] l=self._n -self._m +1 -1 self._T[:-1]=self._T[1:] self._T[-1]=t self._n_appended +=1 self._QT[:-1]=self._QT[1:] S=self._T[l:] t_drop=self._T[l -1] self._T_isfinite[:-1]=self._T_isfinite[1:] self._I[:-1]=self._I[1:] self._P[:-1]=self._P[1:] self._left_I[:-1]=self._left_I[1:] self._left_P[:-1]=self._left_P[1:] if np.isfinite(t): self._T_isfinite[-1]=True else: self._T_isfinite[-1]=False t=0 self._T[-1]=0 S[-1]=0 if np.any(~self._T_isfinite[-self._m:]): \u03bc_Q=np.inf \u03c3_Q=np.nan Q_subseq_isconstant=False else: Q_subseq_isconstant=core.rolling_isconstant(S, self._m)[0] \u03bc_Q, \u03c3_Q=[arr[0] for arr in core.compute_mean_std(S, self._m)] self._M_T[:-1]=self._M_T[1:] self._\u03a3_T[:-1]=self._\u03a3_T[1:] self._T_subseq_isconstant[:-1]=self._T_subseq_isconstant[1:] self._M_T[-1]=\u03bc_Q self._\u03a3_T[-1]=\u03c3_Q self._T_subseq_isconstant[-1]=Q_subseq_isconstant self._QT_new[1:]=self._QT[:l] -self._T[:l] * t_drop +self._T[self._m:] * t self._QT_new[0]=np.sum(self._T[: self._m] * S[: self._m]) D=core.calculate_distance_profile( self._m, self._QT_new, \u03bc_Q, \u03c3_Q, self._M_T, self._\u03a3_T, Q_subseq_isconstant, self._T_subseq_isconstant, ) if np.any(~self._T_isfinite[-self._m:]): D[:]=np.inf core.apply_exclusion_zone(D, D.shape[0] -1, self._excl_zone, np.inf) update_idx=np.argwhere(D < self._P[:, -1]).flatten() for i in update_idx: idx=np.searchsorted(self._P[i], D[i], side=\"right\") core._shift_insert_at_index(self._P[i], idx, D[i]) core._shift_insert_at_index( self._I[i], idx, D.shape[0] +self._n_appended -1 ) self._P[-1]=np.inf self._I[-1]=-1 for i, d in enumerate(D): if d < self._P[-1, -1]: idx=np.searchsorted(self._P[-1], d, side=\"right\") core._shift_insert_at_index(self._P[-1], idx, d) core._shift_insert_at_index(self._I[-1], idx, i +self._n_appended) self._left_P[-1]=self._P[-1, 0] self._left_I[-1]=self._I[-1, 0] self._QT[:]=self._QT_new def _update(self, t): \"\"\" Ingress a new data point and update the(top-k) matrix profile and matrix profile indices without egressing the oldest data point Parameters ---------- t: float A single new data point to be appended to `T` \"\"\" n=self._T.shape[0] l=n -self._m +1 T_new=np.append(self._T, t) QT_new=np.empty(self._QT.shape[0] +1, dtype=np.float64) S=T_new[l:] t_drop=T_new[l -1] if np.isfinite(t): self._T_isfinite=np.append(self._T_isfinite, True) else: self._T_isfinite=np.append(self._T_isfinite, False) t=0 T_new[-1]=0 S[-1]=0 if np.any(~self._T_isfinite[-self._m:]): \u03bc_Q=np.inf \u03c3_Q=np.nan Q_subseq_isconstant=False else: Q_subseq_isconstant=core.rolling_isconstant(S, self._m) \u03bc_Q, \u03c3_Q=core.compute_mean_std(S, self._m) \u03bc_Q=\u03bc_Q[0] \u03c3_Q=\u03c3_Q[0] Q_subseq_isconstant=Q_subseq_isconstant[0] M_T_new=np.append(self._M_T, \u03bc_Q) \u03a3_T_new=np.append(self._\u03a3_T, \u03c3_Q) T_subseq_isconstant_new=np.append( self._T_subseq_isconstant, Q_subseq_isconstant ) QT_new[1:]=self._QT[:l] -T_new[:l] * t_drop +T_new[self._m:] * t QT_new[0]=np.sum(T_new[: self._m] * S[: self._m]) D=core.calculate_distance_profile( self._m, QT_new, \u03bc_Q, \u03c3_Q, M_T_new, \u03a3_T_new, Q_subseq_isconstant, T_subseq_isconstant_new, ) if np.any(~self._T_isfinite[-self._m:]): D[:]=np.inf core.apply_exclusion_zone(D, D.shape[0] -1, self._excl_zone, np.inf) update_idx=np.argwhere(D[:l] < self._P[:l, -1]).flatten() for i in update_idx: idx=np.searchsorted(self._P[i], D[i], side=\"right\") core._shift_insert_at_index(self._P[i], idx, D[i]) core._shift_insert_at_index(self._I[i], idx, l) P_new=np.full(self._k, np.inf, dtype=np.float64) I_new=np.full(self._k, -1, dtype=np.int64) for i, d in enumerate(D): if d < P_new[-1]: idx=np.searchsorted(P_new, d, side=\"right\") core._shift_insert_at_index(P_new, idx, d) core._shift_insert_at_index(I_new, idx, i) left_I_new=I_new[0] left_P_new=P_new[0] self._T=T_new self._P=np.append(self._P, P_new.reshape(1, -1), axis=0) self._I=np.append(self._I, I_new.reshape(1, -1), axis=0) self._left_P=np.append(self._left_P, left_P_new) self._left_I=np.append(self._left_I, left_I_new) self._QT=QT_new self._M_T=M_T_new self._\u03a3_T=\u03a3_T_new self._T_subseq_isconstant=T_subseq_isconstant_new @property def P_(self): \"\"\" Get the(top-k) matrix profile. When `k=1`(default), the output is a 1D array consisting of the matrix profile. When `k > 1`, the output is a 2D array that has exactly `k` columns and it consists of the top-k matrix profile. \"\"\" if self._k==1: return self._P.flatten().astype(np.float64) else: return self._P.astype(np.float64) @property def I_(self): \"\"\" Get the(top-k) matrix profile indices. When `k=1`(default), the output is a 1D array consisting of the matrix profile indices. When `k > 1`, the output is a 2D array that has exactly `k` columns and it consists of the top-k matrix profile indices. \"\"\" if self._k==1: return self._I.flatten().astype(np.int64) else: return self._I.astype(np.int64) @property def left_P_(self): \"\"\" Get the(top-1) left matrix profile \"\"\" return self._left_P.astype(np.float64) @property def left_I_(self): \"\"\" Get the(top-1) left matrix profile indices \"\"\" return self._left_I.astype(np.int64) @property def T_(self): \"\"\" Get the time series \"\"\" return self._T ","sourceWithComments":"# STUMPY\n# Copyright 2019 TD Ameritrade. Released under the terms of the 3-Clause BSD license.\n# STUMPY is a trademark of TD Ameritrade IP Company, Inc. All rights reserved.\n\nimport numpy as np\n\nfrom . import config, core, stump\nfrom .aampi import aampi\n\n\n@core.non_normalized(aampi)\nclass stumpi:\n    \"\"\"\n    Compute an incremental z-normalized matrix profile for streaming data\n\n    This is based on the on-line STOMPI and STAMPI algorithms.\n\n    Parameters\n    ----------\n    T : numpy.ndarray\n        The time series or sequence for which the matrix profile and matrix profile\n        indices will be returned\n\n    m : int\n        Window size\n\n    egress : bool, default True\n        If set to `True`, the oldest data point in the time series is removed and\n        the time series length remains constant rather than forever increasing\n\n    normalize : bool, default True\n        When set to `True`, this z-normalizes subsequences prior to computing distances.\n        Otherwise, this class gets re-routed to its complementary non-normalized\n        equivalent set in the `@core.non_normalized` class decorator.\n\n    p : float, default 2.0\n        The p-norm to apply for computing the Minkowski distance. This parameter is\n        ignored when `normalize == True`.\n\n    k : int, default 1\n        The number of top `k` smallest distances used to construct the matrix profile.\n        Note that this will increase the total computational time and memory usage\n        when k > 1.\n\n    Attributes\n    ----------\n    P_ : numpy.ndarray\n        The updated (top-k) matrix profile for `T`. When `k=1` (default), the first\n        (and only) column in this 2D array consists of the matrix profile. When\n        `k > 1`, the output has exactly `k` columns consisting of the top-k matrix\n        profile.\n\n    I_ : numpy.ndarray\n        The updated (top-k) matrix profile indices for `T`. When `k=1` (default),\n        the first (and only) column in this 2D array consists of the matrix profile\n        indices. When `k > 1`, the output has exactly `k` columns consisting of the\n        top-k matrix profile indices.\n\n    left_P_ : numpy.ndarray\n        The updated left (top-1) matrix profile for `T`\n\n    left_I_ : numpy.ndarray\n        The updated left (top-1) matrix profile indices for `T`\n\n    T_ : numpy.ndarray\n        The updated time series or sequence for which the matrix profile and matrix\n        profile indices are computed\n\n    Methods\n    -------\n    update(t)\n        Append a single new data point, `t`, to the time series, `T`, and update the\n        matrix profile\n\n    Notes\n    -----\n    `DOI: 10.1007\/s10618-017-0519-9 \\\n    <https:\/\/www.cs.ucr.edu\/~eamonn\/MP_journal.pdf>`__\n\n    See Table V\n\n    Note that line 11 is missing an important `sqrt` operation!\n\n    Examples\n    --------\n    >>> import stumpy\n    >>> import numpy as np\n    >>> stream = stumpy.stumpi(\n    ...     np.array([584., -11., 23., 79., 1001., 0.]),\n    ...     m=3)\n    >>> stream.update(-19.0)\n    >>> stream.left_P_\n    array([       inf, 3.00009263, 2.69407392, 3.05656417])\n    >>> stream.left_I_\n    array([-1,  0,  1,  2])\n    \"\"\"\n\n    def __init__(self, T, m, egress=True, normalize=True, p=2.0, k=1):\n        \"\"\"\n        Initialize the `stumpi` object\n\n        Parameters\n        ----------\n        T : numpy.ndarray\n            The time series or sequence for which the matrix profile and matrix profile\n            indices will be returned\n\n        m : int\n            Window size\n\n        egress : bool, default True\n            If set to `True`, the oldest data point in the time series is removed and\n            the time series length remains constant rather than forever increasing\n\n        normalize : bool, default True\n            When set to `True`, this z-normalizes subsequences prior to computing\n            distances. Otherwise, this class gets re-routed to its complementary\n            non-normalized equivalent set in the `@core.non_normalized` class decorator.\n\n        p : float, default 2.0\n            The p-norm to apply for computing the Minkowski distance. This parameter is\n            ignored when `normalize == True`.\n\n        k : int, default 1\n            The number of top `k` smallest distances used to construct the matrix\n            profile. Note that this will increase the total computational time and\n            memory usage when `k > 1`.\n        \"\"\"\n        self._T = core._preprocess(T)\n        core.check_window_size(m, max_size=self._T.shape[-1])\n        self._m = m\n        self._k = k\n\n        self._n = self._T.shape[0]\n        self._excl_zone = int(np.ceil(self._m \/ config.STUMPY_EXCL_ZONE_DENOM))\n        self._T_isfinite = np.isfinite(self._T)\n        self._egress = egress\n\n        mp = stump(self._T, self._m, k=self._k)\n        self._P = mp[:, : self._k].astype(np.float64)\n        self._I = mp[:, self._k : 2 * self._k].astype(np.int64)\n\n        self._left_I = mp[:, 2 * self._k].astype(np.int64)\n        self._left_P = np.full_like(self._left_I, np.inf, dtype=np.float64)\n\n        self._T, self._M_T, self._\u03a3_T, self._T_subseq_isconstant = core.preprocess(\n            self._T, self._m\n        )\n        # Retrieve the left matrix profile values\n\n        # Since each (top-1) matrix profile value is the minimum between the left\n        # and right matrix profile values, we can save time by re-computing only\n        # the left matrix profile value when the (top-1) matrix profile index is\n        # equal to the right matrix profile index.\n        mask = self._left_I == self._I[:, 0]\n        self._left_P[mask] = self._P[mask, 0]\n\n        # Only re-compute the `i`-th left matrix profile value, `self._left_P[i]`,\n        # when `self._left_I[i] != self._I[i, 0]`\n        for i in np.flatnonzero(self._left_I >= 0 & ~mask):\n            j = self._left_I[i]\n            QT = np.dot(self._T[i : i + self._m], self._T[j : j + self._m])\n            D_square = core._calculate_squared_distance(\n                self._m,\n                QT,\n                self._M_T[i],\n                self._\u03a3_T[i],\n                self._M_T[j],\n                self._\u03a3_T[j],\n                self._T_subseq_isconstant[i],\n                self._T_subseq_isconstant[j],\n            )\n            self._left_P[i] = np.sqrt(D_square)\n\n        Q = self._T[-self._m :]\n        self._QT = core.sliding_dot_product(Q, self._T)\n        if self._egress:\n            self._QT_new = np.empty(self._QT.shape[0], dtype=np.float64)\n            self._n_appended = 0\n\n    def update(self, t):\n        \"\"\"\n        Append a single new data point, `t`, to the existing time series `T` and update\n        the (top-k) matrix profile and matrix profile indices.\n\n        Parameters\n        ----------\n        t : float\n            A single new data point to be appended to `T`\n\n        Notes\n        -----\n        `DOI: 10.1007\/s10618-017-0519-9 \\\n        <https:\/\/www.cs.ucr.edu\/~eamonn\/MP_journal.pdf>`__\n\n        See Table V\n\n        Note that line 11 is missing an important `sqrt` operation!\n        \"\"\"\n        if self._egress:\n            self._update_egress(t)\n        else:\n            self._update(t)\n\n    def _update_egress(self, t):\n        \"\"\"\n        Ingress a new data point, egress the oldest data point, and update the (top-k)\n        matrix profile and matrix profile indices\n\n        Parameters\n        ----------\n        t : float\n            A single new data point to be appended to `T`\n        \"\"\"\n        self._n = self._T.shape[0]\n        l = self._n - self._m + 1 - 1  # Subtract 1 due to egress\n        self._T[:-1] = self._T[1:]\n        self._T[-1] = t\n        self._n_appended += 1\n        self._QT[:-1] = self._QT[1:]\n        S = self._T[l:]\n        t_drop = self._T[l - 1]\n        self._T_isfinite[:-1] = self._T_isfinite[1:]\n\n        self._I[:-1] = self._I[1:]\n        self._P[:-1] = self._P[1:]\n        self._left_I[:-1] = self._left_I[1:]\n        self._left_P[:-1] = self._left_P[1:]\n\n        if np.isfinite(t):\n            self._T_isfinite[-1] = True\n        else:\n            self._T_isfinite[-1] = False\n            t = 0\n            self._T[-1] = 0\n            S[-1] = 0\n\n        if np.any(~self._T_isfinite[-self._m :]):\n            \u03bc_Q = np.inf\n            \u03c3_Q = np.nan\n            Q_subseq_isconstant = False\n        else:\n            Q_subseq_isconstant = core.rolling_isconstant(S, self._m)[0]\n            \u03bc_Q, \u03c3_Q = [arr[0] for arr in core.compute_mean_std(S, self._m)]\n\n        self._M_T[:-1] = self._M_T[1:]\n        self._\u03a3_T[:-1] = self._\u03a3_T[1:]\n        self._T_subseq_isconstant[:-1] = self._T_subseq_isconstant[1:]\n\n        self._M_T[-1] = \u03bc_Q\n        self._\u03a3_T[-1] = \u03c3_Q\n        self._T_subseq_isconstant[-1] = Q_subseq_isconstant\n\n        self._QT_new[1:] = self._QT[:l] - self._T[:l] * t_drop + self._T[self._m :] * t\n        self._QT_new[0] = np.sum(self._T[: self._m] * S[: self._m])\n\n        D = core.calculate_distance_profile(\n            self._m,\n            self._QT_new,\n            \u03bc_Q,\n            \u03c3_Q,\n            self._M_T,\n            self._\u03a3_T,\n            Q_subseq_isconstant,\n            self._T_subseq_isconstant,\n        )\n        if np.any(~self._T_isfinite[-self._m :]):\n            D[:] = np.inf\n\n        core.apply_exclusion_zone(D, D.shape[0] - 1, self._excl_zone, np.inf)\n\n        update_idx = np.argwhere(D < self._P[:, -1]).flatten()\n        for i in update_idx:\n            idx = np.searchsorted(self._P[i], D[i], side=\"right\")\n            core._shift_insert_at_index(self._P[i], idx, D[i])\n            core._shift_insert_at_index(\n                self._I[i], idx, D.shape[0] + self._n_appended - 1\n            )\n            # D.shape[0] is base-1\n\n        # Calculate the (top-k) matrix profile values\/indices for the last subsequence\n        # by using its corresponding distance profile `D`\n        self._P[-1] = np.inf\n        self._I[-1] = -1\n        for i, d in enumerate(D):\n            if d < self._P[-1, -1]:\n                idx = np.searchsorted(self._P[-1], d, side=\"right\")\n                core._shift_insert_at_index(self._P[-1], idx, d)\n                core._shift_insert_at_index(self._I[-1], idx, i + self._n_appended)\n\n        # All neighbors of the last subsequence are on its left. So, its (top-1)\n        # matrix profile value\/index and its left matrix profile value\/index must\n        # be equal.\n        self._left_P[-1] = self._P[-1, 0]\n        self._left_I[-1] = self._I[-1, 0]\n\n        self._QT[:] = self._QT_new\n\n    def _update(self, t):\n        \"\"\"\n        Ingress a new data point and update the (top-k) matrix profile and matrix\n        profile indices without egressing the oldest data point\n\n        Parameters\n        ----------\n        t : float\n            A single new data point to be appended to `T`\n        \"\"\"\n        n = self._T.shape[0]\n        l = n - self._m + 1\n        T_new = np.append(self._T, t)\n        QT_new = np.empty(self._QT.shape[0] + 1, dtype=np.float64)\n        S = T_new[l:]\n        t_drop = T_new[l - 1]\n\n        if np.isfinite(t):\n            self._T_isfinite = np.append(self._T_isfinite, True)\n        else:\n            self._T_isfinite = np.append(self._T_isfinite, False)\n            t = 0\n            T_new[-1] = 0\n            S[-1] = 0\n\n        if np.any(~self._T_isfinite[-self._m :]):\n            \u03bc_Q = np.inf\n            \u03c3_Q = np.nan\n            Q_subseq_isconstant = False\n        else:\n            Q_subseq_isconstant = core.rolling_isconstant(S, self._m)\n            \u03bc_Q, \u03c3_Q = core.compute_mean_std(S, self._m)\n            \u03bc_Q = \u03bc_Q[0]\n            \u03c3_Q = \u03c3_Q[0]\n            Q_subseq_isconstant = Q_subseq_isconstant[0]\n\n        M_T_new = np.append(self._M_T, \u03bc_Q)\n        \u03a3_T_new = np.append(self._\u03a3_T, \u03c3_Q)\n        T_subseq_isconstant_new = np.append(\n            self._T_subseq_isconstant, Q_subseq_isconstant\n        )\n\n        QT_new[1:] = self._QT[:l] - T_new[:l] * t_drop + T_new[self._m :] * t\n        QT_new[0] = np.sum(T_new[: self._m] * S[: self._m])\n\n        D = core.calculate_distance_profile(\n            self._m,\n            QT_new,\n            \u03bc_Q,\n            \u03c3_Q,\n            M_T_new,\n            \u03a3_T_new,\n            Q_subseq_isconstant,\n            T_subseq_isconstant_new,\n        )\n        if np.any(~self._T_isfinite[-self._m :]):\n            D[:] = np.inf\n\n        core.apply_exclusion_zone(D, D.shape[0] - 1, self._excl_zone, np.inf)\n\n        update_idx = np.argwhere(D[:l] < self._P[:l, -1]).flatten()\n        for i in update_idx:\n            idx = np.searchsorted(self._P[i], D[i], side=\"right\")\n            core._shift_insert_at_index(self._P[i], idx, D[i])\n            core._shift_insert_at_index(self._I[i], idx, l)\n\n        # Calculating top-k matrix profile and (top-1) left matrix profile (and their\n        # corresponding indices) for new subsequence whose distance profile is `D`\n        P_new = np.full(self._k, np.inf, dtype=np.float64)\n        I_new = np.full(self._k, -1, dtype=np.int64)\n        for i, d in enumerate(D):\n            if d < P_new[-1]:  # maximum value in sorted array P_new\n                idx = np.searchsorted(P_new, d, side=\"right\")\n                core._shift_insert_at_index(P_new, idx, d)\n                core._shift_insert_at_index(I_new, idx, i)\n\n        left_I_new = I_new[0]\n        left_P_new = P_new[0]\n\n        self._T = T_new\n        self._P = np.append(self._P, P_new.reshape(1, -1), axis=0)\n        self._I = np.append(self._I, I_new.reshape(1, -1), axis=0)\n        self._left_P = np.append(self._left_P, left_P_new)\n        self._left_I = np.append(self._left_I, left_I_new)\n        self._QT = QT_new\n        self._M_T = M_T_new\n        self._\u03a3_T = \u03a3_T_new\n        self._T_subseq_isconstant = T_subseq_isconstant_new\n\n    @property\n    def P_(self):\n        \"\"\"\n        Get the (top-k) matrix profile. When `k=1` (default), the output is\n        a 1D array consisting of the matrix profile. When `k > 1`, the\n        output is a 2D array that has exactly `k` columns and it consists of the\n        top-k matrix profile.\n        \"\"\"\n        if self._k == 1:\n            return self._P.flatten().astype(np.float64)\n        else:\n            return self._P.astype(np.float64)\n\n    @property\n    def I_(self):\n        \"\"\"\n        Get the (top-k) matrix profile indices. When `k=1` (default), the output is\n        a 1D array consisting of the matrix profile indices. When `k > 1`, the\n        output is a 2D array that has exactly `k` columns and it consists of the\n        top-k matrix profile indices.\n        \"\"\"\n        if self._k == 1:\n            return self._I.flatten().astype(np.int64)\n        else:\n            return self._I.astype(np.int64)\n\n    @property\n    def left_P_(self):\n        \"\"\"\n        Get the (top-1) left matrix profile\n        \"\"\"\n        return self._left_P.astype(np.float64)\n\n    @property\n    def left_I_(self):\n        \"\"\"\n        Get the (top-1) left matrix profile indices\n        \"\"\"\n        return self._left_I.astype(np.int64)\n\n    @property\n    def T_(self):\n        \"\"\"\n        Get the time series\n        \"\"\"\n        return self._T\n"},"\/tests\/naive.py":{"changes":[{"diff":"\n \n \n class aampi_egress(object):\n-    def __init__(self, T, m, excl_zone=None, p=2.0, k=1):\n+    def __init__(self, T, m, excl_zone=None, p=2.0, k=1, mp=None):\n         self._T = np.asarray(T)\n         self._T = self._T.copy()\n         self._T_isfinite = np.isfinite(self._T)\n","add":1,"remove":1,"filename":"\/tests\/naive.py","badparts":["    def __init__(self, T, m, excl_zone=None, p=2.0, k=1):"],"goodparts":["    def __init__(self, T, m, excl_zone=None, p=2.0, k=1, mp=None):"]},{"diff":"\n             self._excl_zone = int(np.ceil(self._m \/ config.STUMPY_EXCL_ZONE_DENOM))\n \n         self._l = self._T.shape[0] - m + 1\n-        mp = aamp(T, m, exclusion_zone=self._excl_zone, p=p, k=self._k)\n+        if mp is None:\n+            mp = aamp(self._T, self._m, exclusion_zone=self._excl_zone, p=p, k=self._k)\n+        else:\n+            mp = mp.copy()\n         self._P = mp[:, :k].astype(np.float64)\n         self._I = mp[:, k : 2 * k].astype(np.int64)\n \n","add":4,"remove":1,"filename":"\/tests\/naive.py","badparts":["        mp = aamp(T, m, exclusion_zone=self._excl_zone, p=p, k=self._k)"],"goodparts":["        if mp is None:","            mp = aamp(self._T, self._m, exclusion_zone=self._excl_zone, p=p, k=self._k)","        else:","            mp = mp.copy()"]},{"diff":"\n \n \n class stumpi_egress(object):\n-    def __init__(self, T, m, excl_zone=None, k=1):\n+    def __init__(self, T, m, excl_zone=None, k=1, mp=None):\n         self._T = np.asarray(T)\n         self._T = self._T.copy()\n         self._T_isfinite = np.isfinite(self._T)\n","add":1,"remove":1,"filename":"\/tests\/naive.py","badparts":["    def __init__(self, T, m, excl_zone=None, k=1):"],"goodparts":["    def __init__(self, T, m, excl_zone=None, k=1, mp=None):"]},{"diff":"\n             self._excl_zone = int(np.ceil(self._m \/ config.STUMPY_EXCL_ZONE_DENOM))\n \n         self._l = self._T.shape[0] - m + 1\n-        mp = stump(T, m, exclusion_zone=self._excl_zone, k=self._k)\n+        if mp is None:\n+            mp = stump(self._T, self._m, exclusion_zone=self._excl_zone, k=self._k)\n+        else:\n+            mp = mp.copy()\n         self._P = mp[:, :k].astype(np.float64)\n         self._I = mp[:, k : 2 * k].astype(np.int64)","add":4,"remove":1,"filename":"\/tests\/naive.py","badparts":["        mp = stump(T, m, exclusion_zone=self._excl_zone, k=self._k)"],"goodparts":["        if mp is None:","            mp = stump(self._T, self._m, exclusion_zone=self._excl_zone, k=self._k)","        else:","            mp = mp.copy()"]}]}},"msg":"Fixed #621 Add param mp to stumpi (#817)\n\n* add param mp to naive stumpi\r\n\r\n* copy input to avoioid tampering with the user input\r\n\r\n* add test function, expecting error\r\n\r\n* Allowed user to pass precomputed mp\r\n\r\n* add pragma no cover\r\n\r\n* add param mp to performant aampi\r\n\r\n* fix docstring\r\n\r\n* add param mp to naive aampi\r\n\r\n* minor changes\r\n\r\n* add test function for aampi\r\n\r\n* fix test function\r\n\r\n* revise docstring\r\n\r\n* revise error message\r\n\r\n* minor changes\r\n\r\n* fix naive\r\n\r\n* revise ValueError message\r\n\r\n* add missing space"}},"https:\/\/github.com\/pangeacyber\/pangea-python":{"428b1339b048ff1e393d46cb6fcc103ce0ab0f13":{"url":"https:\/\/api.github.com\/repos\/pangeacyber\/pangea-python\/commits\/428b1339b048ff1e393d46cb6fcc103ce0ab0f13","html_url":"https:\/\/github.com\/pangeacyber\/pangea-python\/commit\/428b1339b048ff1e393d46cb6fcc103ce0ab0f13","sha":"428b1339b048ff1e393d46cb6fcc103ce0ab0f13","keyword":"tampering fix","diff":"diff --git a\/examples\/audit.py b\/examples\/audit.py\nindex ce15a87..8a388d5 100644\n--- a\/examples\/audit.py\n+++ b\/examples\/audit.py\n@@ -23,7 +23,7 @@ def main():\n     print(f\"Log Request ID: {log_response.request_id}, Result: {log_response.result}\")\n \n     print(\"Search Data...\")\n-    search_res = audit.search(query=\"message:Hello\", size=5, verify_proofs=True)\n+    search_res = audit.search(query=\"message:prueba_123\", size=5, verify_proofs=True)\n \n     if search_res.success:\n         print(\"Search Request ID:\", search_res.request_id, \"\\n\")\n@@ -47,13 +47,13 @@ def print_page_results(search_res):\n \n         print(\"\\nVerify membership proofs\\n\\t\", end=\"\")\n         for row in search_res.result.audits:\n-            ok = audit.verify_membership_proof(search_res.result.root, row, True)\n+            ok = audit.verify_membership_proof(search_res.result.root, row, False)\n             print(\".\" if ok else \"x\", end=\"\\t\")\n         print(\"\")\n \n         print(\"Verify consistency proofs\\n\\t\", end=\"\")\n         for row in search_res.result.audits:\n-            ok = audit.verify_consistency_proof(row, True)\n+            ok = audit.verify_consistency_proof(search_res.result.published_roots, row, False)\n             print(\".\" if ok else \"x\", end=\"\\t\")\n \n \ndiff --git a\/pangea\/services\/audit.py b\/pangea\/services\/audit.py\nindex d71f6d2..5400802 100644\n--- a\/pangea\/services\/audit.py\n+++ b\/pangea\/services\/audit.py\n@@ -52,11 +52,11 @@ def __getattr__(self, attr):\n \n     def next(self):\n         reg_count = 0\n-        if self.count:\n+        if self.count and self.count != \"\":\n             reg_count = int(self.count)\n \n         reg_total = 0\n-        if self.total:\n+        if self.total and self.total != \"\":\n             reg_total = int(self.total)\n \n         if reg_count < reg_total:\n@@ -101,11 +101,10 @@ def count(self) -> str:\n \n class Audit(ServiceBase):\n     response_class = AuditSearchResponse\n-    service_name = \"audit\"\n+    service_name = \"audit\" #-audit-tamper-proof-merge-main\"\n     version = \"v1\"\n-    allow_server_roots = (\n-        True  # In case of Arweave failure, ask the server for the roots\n-    )\n+    # In case of Arweave failure, ask the server for the roots\n+    allow_server_roots = True\n \n     def log(self, input: dict, signature = None, public_key = None, verify: bool = False) -> PangeaResponse:\n         endpoint_name = \"log\"\n@@ -174,21 +173,15 @@ def search(\n             raise Exception(\"The 'size' argument must be a positive integer > 0\")\n \n         if not query or not query.strip():\n-            raise Exception(\n-                f\"Error: Query field is mandatory.\"\n-            )\n-\n-        include_membership_proof = True\n-        include_hash = True\n-        include_root = True\n+            raise Exception(f\"Error: Query field is mandatory.\")\n \n         data = {\n-            \"query\": query, \n+            \"query\": query,\n             \"max_results\": size,\n-\t        \"include_membership_proof\": include_membership_proof,\n-            \"include_hash\": include_hash,\n-\t        \"include_root\": include_root            \n-            }\n+            \"include_membership_proof\": True,\n+            \"include_hash\": True,\n+            \"include_root\": True,\n+        }\n \n         if start:    \n             parser.isoparse(start)\n@@ -212,111 +205,97 @@ def search(\n                 f\"Error: Empty result from server.\"\n             )\n \n-        response_result = response.result\n-\n-        root = response_result.get(\"root\")\n-        if include_root and not root:\n-            raise Exception(\n-                f\"Error: `root` field not present.\"\n-            )    \n-\n-        root_hash_coded = root.get(\"root_hash\")\n-        if include_membership_proof and not root_hash_coded:\n-            raise Exception(\n-                f\"Error: `root_hash` field not present.\"\n-            )    \n+        root = response.result.root\n \n-        audits = response_result.get(\"audits\")\n-        if not audits:\n-            raise Exception(\n-                f\"Error: `audits` field not present.\"\n-            )        \n+        # if there is no root, we don't have any record migrated to cold. We cannot verify any proof\n+        if not root:\n+            response.result.root = {}\n+            response.result.published_roots = {}\n+            return AuditSearchResponse(response, data)\n \n+        # get the size of all the roots needed for the consistency_proofs\n         tree_sizes = set()\n-        for a in audits:\n+        for a in response.result.audits:\n             leaf_index = a.get(\"leaf_index\")\n             if leaf_index is not None:\n                 tree_sizes.add(leaf_index)\n-                tree_sizes.add(max(1, leaf_index - 1))\n-\n-        try:\n-            published_roots = get_arweave_published_roots(\n-                root[\"tree_name\"], list(tree_sizes)\n-            )\n-        except Exception as e:\n-            published_roots = {tree_size: None for tree_size in tree_sizes}\n-\n-        if published_roots:\n-            if published_roots.get(\"tree_size\"):\n-                if self.allow_server_roots:\n-                    for tree_size in published_roots:\n-                        if published_roots[tree_size] is None:\n-                            published_roots[tree_size] = self.root(tree_size).result\n-\n-                if root_hash_coded is not None:\n-                    root_hash = decode_hash(root_hash_coded)\n-                    for a in response_result.audits:\n-                        leaf_index = a.get(\"leaf_index\")\n-                        if leaf_index is not None:\n-                            a[\"published_roots\"] = {\n-                                \"current\": published_roots[leaf_index],\n-                                \"previous\": published_roots[leaf_index - 1] if leaf_index > 0 else None,\n-                            }\n-\n-        if not root.get(\"tree_name\") or not root.get(\"size\"):\n-            publish_resp_full = get_arweave_published_roots(root[\"tree_name\"], [root[\"size\"]])\n-            if publish_resp_full is not None:\n-                publish_resp = publish_resp_full[root[\"size\"]]\n-            else:\n-                publish_resp = None\n-        else:\n-            publish_resp = None\n-                \n-        if publish_resp is not None:\n-            publish_root_hash = decode_hash(publish_resp.get(\"root_hash\", \"\"))\n-            publish_verify = verify_published_root(root_hash, publish_root_hash)\n-\n-            if not publish_verify:\n-                raise Exception(f\"Error: Published Root Not Valid.\")\n-        else:\n-            if not self.allow_server_roots:\n-                raise Exception(f\"Error: Published Root Not Valid.\")\n+                if leaf_index > 1:\n+                    tree_sizes.add(leaf_index - 1)\n+        tree_sizes.add(root.size)\n+\n+        # get all the roots from arweave\n+        response.result.published_roots = {\n+            tree_size: JSONObject(obj.get(\"data\", {}))\n+            for tree_size, obj in get_arweave_published_roots(\n+                root.tree_name, list(tree_sizes) + [root.size]\n+            ).items()\n+        }\n+        for tree_size, root in response.result.published_roots.items():\n+            root[\"source\"] = \"arweave\"\n+\n+        # fill the missing roots from the server\n+        for tree_size in tree_sizes:\n+            if tree_size not in response.result.published_roots:\n+                try:\n+                    response.result.published_roots[tree_size] = self.root(\n+                        tree_size\n+                    ).result.data\n+                    response.result.published_roots[tree_size][\"source\"] = \"pangea\"\n+                except:\n+                    pass\n+\n+        # if we've got the current root from arweave, replace the one from the server\n+        pub_root = response.result.published_roots.get(root.size)\n+        if pub_root:\n+            response.result.root = pub_root\n+\n+        # calculate the hashes from the data\n+        for a in response.result.audits:\n+            canon = canonicalize_log(a.data)\n+            a[\"calculated_hash\"] = hash_data(canon)\n \n         response_wrapper = AuditSearchResponse(response, data)\n-\n         return response_wrapper\n \n-\n     def verify_membership_proof(\n         self, root: JSONObject, audit: JSONObject, required: bool = False\n     ) -> bool:\n         if not audit.get(\"membership_proof\"):\n             return not required\n-        node_hash = decode_hash(audit.hash)\n+\n+        if not self.allow_server_roots and root.source != \"arweave\":\n+            return False\n+\n+        node_hash = decode_hash(audit.calculated_hash)\n         root_hash = decode_hash(root.root_hash)\n         proof = decode_proof(audit.membership_proof)\n         return verify_log_proof(node_hash, root_hash, proof)\n \n-\n     def verify_consistency_proof(\n-        self, audit: JSONObject, required: bool = False\n+        self,\n+        published_roots: dict[int, JSONObject],\n+        audit: JSONObject,\n+        required: bool = False,\n     ) -> bool:\n-        if not audit.get(\"published_roots\"):\n+        leaf_index = audit.get(\"leaf_index\")\n+        if not leaf_index:\n             return not required\n \n-        if not audit.published_roots.get(\"current\"):\n-            return False\n+        if leaf_index == 1:\n+            return not required\n \n-        if not audit.published_roots.get(\"previous\"):\n-            if audit.get(\"leaf_index\", 0) <= 1:\n-                return True\n-            else:\n-                return False\n+        curr_root = published_roots.get(leaf_index)\n+        prev_root = published_roots.get(leaf_index - 1)\n \n-        return verify_consistency_proof(\n-            audit.published_roots.current.data, audit.published_roots.previous.data\n-        )\n+        if not curr_root or not prev_root:\n+            return False\n+\n+        if not self.allow_server_roots and (\n+            curr_root.source != \"arweave\" or prev_root.source != \"arweave\"\n+        ):\n+            return False\n \n+        return verify_consistency_proof(curr_root, prev_root)\n \n     def root(self, tree_size: int = 0) -> AuditSearchResponse:\n         endpoint_name = \"root\"\ndiff --git a\/pangea\/services\/audit_util.py b\/pangea\/services\/audit_util.py\nindex 8e292ef..a2c29cb 100644\n--- a\/pangea\/services\/audit_util.py\n+++ b\/pangea\/services\/audit_util.py\n@@ -17,6 +17,7 @@\n \n ARWEAVE_BASE_URL = \"https:\/\/arweave.net\"\n \n+\n @dataclass\n class HotRoot:\n     tree_size: int\n@@ -75,7 +76,12 @@ def decode_proof(data: str) -> Proof:\n     proof: Proof = []\n     for item in data.split(\",\"):\n         parts = item.split(\":\")\n-        proof.append(ProofItem(side=\"left\" if parts[0] == \"l\" else \"right\", node_hash=decode_hash(parts[1])))\n+        proof.append(\n+            ProofItem(\n+                side=\"left\" if parts[0] == \"l\" else \"right\",\n+                node_hash=decode_hash(parts[1]),\n+            )\n+        )\n     return proof\n \n \n@@ -84,7 +90,10 @@ def decode_root_proof(data: list[str]) -> RootProof:\n     for item in data:\n         ndx = item.index(\",\")\n         root_proof.append(\n-            RootProofItem(node_hash=decode_hash(item[:ndx].split(\":\")[1]), proof=decode_proof(item[ndx + 1 :]))\n+            RootProofItem(\n+                node_hash=decode_hash(item[:ndx].split(\":\")[1]),\n+                proof=decode_proof(item[ndx + 1 :]),\n+            )\n         )\n     return root_proof\n \n@@ -98,7 +107,11 @@ def decode_server_response(data: str) -> dict:\n def verify_log_proof(node_hash: Hash, root_hash: Hash, proof: Proof) -> bool:\n     for proof_item in proof:\n         proof_hash = proof_item.node_hash\n-        node_hash = hash_pair(proof_hash, node_hash) if proof_item.side == \"left\" else hash_pair(node_hash, proof_hash)\n+        node_hash = (\n+            hash_pair(proof_hash, node_hash)\n+            if proof_item.side == \"left\"\n+            else hash_pair(node_hash, proof_hash)\n+        )\n     return root_hash == node_hash\n \n \n@@ -107,15 +120,12 @@ def verify_published_root(root_hash: Hash, publish_hash: Hash) -> bool:\n \n \n def canonicalize_log(audit: dict) -> bytes:\n-    def _default(obj):\n-        if not any(isinstance(obj, typ) for typ in JSON_TYPES):\n-            return str(obj)\n-        else:\n-            return obj\n-\n-    # stringify invalid JSON types before canonicalizing\n     return json.dumps(\n-        audit, ensure_ascii=False, allow_nan=False, separators=(\",\", \":\"), sort_keys=True, default=_default\n+        audit,\n+        ensure_ascii=False,\n+        allow_nan=False,\n+        separators=(\",\", \":\"),\n+        sort_keys=True,\n     ).encode(\"utf-8\")\n \n \n@@ -124,7 +134,7 @@ def hash_data(data: bytes) -> str:\n \n \n def to_msg(b):\n-    return \"OK\" if b else \"FAILED\"   \n+    return \"OK\" if b else \"FAILED\"\n \n \n def base64url_decode(input):\n@@ -156,7 +166,7 @@ def arweave_graphql_url():\n \n def get_arweave_published_roots(\n     tree_name: str, tree_sizes: list[int]\n-) -> dict[int, Optional[dict]]:\n+) -> dict[int, dict]:\n     if len(tree_sizes) == 0:\n         return {}\n \n@@ -192,28 +202,29 @@ def get_arweave_published_roots(\n     )\n \n     resp = requests.post(arweave_graphql_url(), json={\"query\": query})\n-    resp.raise_for_status()\n-    ans: dict[int, Optional[dict]] = {tree_size: None for tree_size in tree_sizes}\n-    data = resp.json()\n+    if resp.status_code != 200:\n+        return {}\n \n-    if data[\"data\"][\"transactions\"].get(\"edges\"):\n-        for edge in data[\"data\"][\"transactions\"][\"edges\"]:\n-            node_id = edge[\"node\"][\"id\"]\n-            tree_size = int(\n-                next(\n-                    tag[\"value\"]\n-                    for tag in edge[\"node\"][\"tags\"]\n-                    if tag[\"name\"] == \"tree_size\"\n-                )\n+    ans: dict[int, dict] = {}\n+    data = resp.json()\n+    for edge in data.get(\"data\", {}).get(\"transactions\", {}).get(\"edges\", []):\n+        try:\n+            node_id = edge.get(\"node\").get(\"id\")\n+            tree_size = next(\n+                tag.get(\"value\")\n+                for tag in edge.get(\"node\").get(\"tags\", [])\n+                if tag.get(\"name\") == \"tree_size\"\n             )\n+\n             url = arweave_transaction_url(node_id)\n \n             # TODO: do all the requests concurrently\n             resp2 = requests.get(url)\n-            if resp2.status_code == 200 and resp2.text.strip() != \"\": \n+            if resp2.status_code == 200 and resp2.text != 'Pending':\n                 ans[tree_size] = json.loads(base64url_decode(resp2.text))\n-                return ans\n-    return {}\n+        except:\n+            pass\n+    return ans\n \n \n def verify_consistency_proof(new_root: dict, prev_root: dict) -> bool:\n","message":"","files":{"\/examples\/audit.py":{"changes":[{"diff":"\n \n         print(\"\\nVerify membership proofs\\n\\t\", end=\"\")\n         for row in search_res.result.audits:\n-            ok = audit.verify_membership_proof(search_res.result.root, row, True)\n+            ok = audit.verify_membership_proof(search_res.result.root, row, False)\n             print(\".\" if ok else \"x\", end=\"\\t\")\n         print(\"\")\n \n         print(\"Verify consistency proofs\\n\\t\", end=\"\")\n         for row in search_res.result.audits:\n-            ok = audit.verify_consistency_proof(row, True)\n+            ok = audit.verify_consistency_proof(search_res.result.published_roots, row, False)\n             print(\".\" if ok else \"x\", end=\"\\t\")\n \n ","add":2,"remove":2,"filename":"\/examples\/audit.py","badparts":["            ok = audit.verify_membership_proof(search_res.result.root, row, True)","            ok = audit.verify_consistency_proof(row, True)"],"goodparts":["            ok = audit.verify_membership_proof(search_res.result.root, row, False)","            ok = audit.verify_consistency_proof(search_res.result.published_roots, row, False)"]}],"source":"\nimport os from pangea.config import PangeaConfig from pangea.services import Audit token=os.getenv(\"PANGEA_TOKEN\") config=PangeaConfig(base_domain=\"dev.pangea.cloud\", insecure=False) audit=Audit(token=token, config=config) data={ \t\t\"action\": \"diego\", \t\t\"actor\": \"testing2\", \t\t\"message\": \"Hello\", \"status\": \"xxx\", \t\t\"new\": \"xxx\", \"old\": \"xxx\", \"target\": \"xxx\" } def main(): print(\"Log Data...\") log_response=audit.log(data) print(f\"Log Request ID:{log_response.request_id}, Result:{log_response.result}\") print(\"Search Data...\") search_res=audit.search(query=\"message:Hello\", size=5, verify_proofs=True) if search_res.success: print(\"Search Request ID:\", search_res.request_id, \"\\n\") print_page_results(search_res) while search_res.next(): search_res=audit.search(**search_res.next(), verify_proofs=True) print_page_results(search_res) else: print(\"Search Failed:\", search_res.code, search_res.status) def print_page_results(search_res): print(\"\\n--------------------------------------------------------------------\\n\") for row in search_res.result.audits: print(f\"{row.data.message}\\t{row.data.created}\\t{row.data.source}\\t{row.data.actor}\") print( f\"\\nResults:{search_res.count} of{search_res.total} -next{search_res.next()}\", ) print(\"\\nVerify membership proofs\\n\\t\", end=\"\") for row in search_res.result.audits: ok=audit.verify_membership_proof(search_res.result.root, row, True) print(\".\" if ok else \"x\", end=\"\\t\") print(\"\") print(\"Verify consistency proofs\\n\\t\", end=\"\") for row in search_res.result.audits: ok=audit.verify_consistency_proof(row, True) print(\".\" if ok else \"x\", end=\"\\t\") if __name__==\"__main__\": main() ","sourceWithComments":"import os\n\nfrom pangea.config import PangeaConfig\nfrom pangea.services import Audit\n\ntoken = os.getenv(\"PANGEA_TOKEN\")\nconfig = PangeaConfig(base_domain=\"dev.pangea.cloud\", insecure=False)\naudit = Audit(token=token, config=config)\n\ndata = {\n\t\t\"action\": \"diego\",\n\t\t\"actor\": \"testing2\",\n\t\t\"message\": \"Hello\",\n        \"status\": \"xxx\",\n\t\t\"new\": \"xxx\",\n        \"old\": \"xxx\",\n        \"target\": \"xxx\"\n}\n\ndef main():\n    print(\"Log Data...\")    \n    log_response = audit.log(data)\n    print(f\"Log Request ID: {log_response.request_id}, Result: {log_response.result}\")\n\n    print(\"Search Data...\")\n    search_res = audit.search(query=\"message:Hello\", size=5, verify_proofs=True)\n\n    if search_res.success:\n        print(\"Search Request ID:\", search_res.request_id, \"\\n\")\n        print_page_results(search_res)\n\n        # get next pages\n        while search_res.next():\n            search_res = audit.search(**search_res.next(), verify_proofs = True)\n            print_page_results(search_res)\n    else:\n        print(\"Search Failed:\", search_res.code, search_res.status)\n\n\ndef print_page_results(search_res):\n        print(\"\\n--------------------------------------------------------------------\\n\")\n        for row in search_res.result.audits:\n            print(f\"{row.data.message}\\t{row.data.created}\\t{row.data.source}\\t{row.data.actor}\")        \n        print(\n            f\"\\nResults: {search_res.count} of {search_res.total} - next {search_res.next()}\",\n        )\n\n        print(\"\\nVerify membership proofs\\n\\t\", end=\"\")\n        for row in search_res.result.audits:\n            ok = audit.verify_membership_proof(search_res.result.root, row, True)\n            print(\".\" if ok else \"x\", end=\"\\t\")\n        print(\"\")\n\n        print(\"Verify consistency proofs\\n\\t\", end=\"\")\n        for row in search_res.result.audits:\n            ok = audit.verify_consistency_proof(row, True)\n            print(\".\" if ok else \"x\", end=\"\\t\")\n\n\nif __name__ == \"__main__\":\n    main()"},"\/pangea\/services\/audit.py":{"changes":[{"diff":"\n \n     def next(self):\n         reg_count = 0\n-        if self.count:\n+        if self.count and self.count != \"\":\n             reg_count = int(self.count)\n \n         reg_total = 0\n-        if self.total:\n+        if self.total and self.total != \"\":\n             reg_total = int(self.total)\n \n         if reg_count < reg_total:\n","add":2,"remove":2,"filename":"\/pangea\/services\/audit.py","badparts":["        if self.count:","        if self.total:"],"goodparts":["        if self.count and self.count != \"\":","        if self.total and self.total != \"\":"]},{"diff":"\n \n class Audit(ServiceBase):\n     response_class = AuditSearchResponse\n-    service_name = \"audit\"\n+    service_name = \"audit\" #-audit-tamper-proof-merge-main\"\n     version = \"v1\"\n-    allow_server_roots = (\n-        True  # In case of Arweave failure, ask the server for the roots\n-    )\n+    # In case of Arweave failure, ask the server for the roots\n+    allow_server_roots = True\n \n     def log(self, input: dict, signature = None, public_key = None, verify: bool = False) -> PangeaResponse:\n         endpoint_name = \"log\"\n","add":3,"remove":4,"filename":"\/pangea\/services\/audit.py","badparts":["    service_name = \"audit\"","    allow_server_roots = (","        True  # In case of Arweave failure, ask the server for the roots","    )"],"goodparts":["    service_name = \"audit\" #-audit-tamper-proof-merge-main\"","    allow_server_roots = True"]},{"diff":"\n             raise Exception(\"The 'size' argument must be a positive integer > 0\")\n \n         if not query or not query.strip():\n-            raise Exception(\n-                f\"Error: Query field is mandatory.\"\n-            )\n-\n-        include_membership_proof = True\n-        include_hash = True\n-        include_root = True\n+            raise Exception(f\"Error: Query field is mandatory.\")\n \n         data = {\n-            \"query\": query, \n+            \"query\": query,\n             \"max_results\": size,\n-\t        \"include_membership_proof\": include_membership_proof,\n-            \"include_hash\": include_hash,\n-\t        \"include_root\": include_root            \n-            }\n+            \"include_membership_proof\": True,\n+            \"include_hash\": True,\n+            \"include_root\": True,\n+        }\n \n         if start:    \n             parser.isoparse(start)\n","add":6,"remove":12,"filename":"\/pangea\/services\/audit.py","badparts":["            raise Exception(","                f\"Error: Query field is mandatory.\"","            )","        include_membership_proof = True","        include_hash = True","        include_root = True","            \"query\": query, ","\t        \"include_membership_proof\": include_membership_proof,","            \"include_hash\": include_hash,","\t        \"include_root\": include_root            ","            }"],"goodparts":["            raise Exception(f\"Error: Query field is mandatory.\")","            \"query\": query,","            \"include_membership_proof\": True,","            \"include_hash\": True,","            \"include_root\": True,","        }"]},{"diff":"\n                 f\"Error: Empty result from server.\"\n             )\n \n-        response_result = response.result\n-\n-        root = response_result.get(\"root\")\n-        if include_root and not root:\n-            raise Exception(\n-                f\"Error: `root` field not present.\"\n-            )    \n-\n-        root_hash_coded = root.get(\"root_hash\")\n-        if include_membership_proof and not root_hash_coded:\n-            raise Exception(\n-                f\"Error: `root_hash` field not present.\"\n-            )    \n+        root = response.result.root\n \n-        audits = response_result.get(\"audits\")\n-        if not audits:\n-            raise Exception(\n-                f\"Error: `audits` field not present.\"\n-            )        \n+        # if there is no root, we don't have any record migrated to cold. We cannot verify any proof\n+        if not root:\n+            response.result.root = {}\n+            response.result.published_roots = {}\n+            return AuditSearchResponse(response, data)\n \n+        # get the size of all the roots needed for the consistency_proofs\n         tree_sizes = set()\n-        for a in audits:\n+        for a in response.result.audits:\n             leaf_index = a.get(\"leaf_index\")\n             if leaf_index is not None:\n                 tree_sizes.add(leaf_index)\n-                tree_sizes.add(max(1, leaf_index - 1))\n-\n-        try:\n-            published_roots = get_arweave_published_roots(\n-                root[\"tree_name\"], list(tree_sizes)\n-            )\n-        except Exception as e:\n-            published_roots = {tree_size: None for tree_size in tree_sizes}\n-\n-        if published_roots:\n-            if published_roots.get(\"tree_size\"):\n-                if self.allow_server_roots:\n-                    for tree_size in published_roots:\n-                        if published_roots[tree_size] is None:\n-                            published_roots[tree_size] = self.root(tree_size).result\n-\n-                if root_hash_coded is not None:\n-                    root_hash = decode_hash(root_hash_coded)\n-                    for a in response_result.audits:\n-                        leaf_index = a.get(\"leaf_index\")\n-                        if leaf_index is not None:\n-                            a[\"published_roots\"] = {\n-                                \"current\": published_roots[leaf_index],\n-                                \"previous\": published_roots[leaf_index - 1] if leaf_index > 0 else None,\n-                            }\n-\n-        if not root.get(\"tree_name\") or not root.get(\"size\"):\n-            publish_resp_full = get_arweave_published_roots(root[\"tree_name\"], [root[\"size\"]])\n-            if publish_resp_full is not None:\n-                publish_resp = publish_resp_full[root[\"size\"]]\n-            else:\n-                publish_resp = None\n-        else:\n-            publish_resp = None\n-                \n-        if publish_resp is not None:\n-            publish_root_hash = decode_hash(publish_resp.get(\"root_hash\", \"\"))\n-            publish_verify = verify_published_root(root_hash, publish_root_hash)\n-\n-            if not publish_verify:\n-                raise Exception(f\"Error: Published Root Not Valid.\")\n-        else:\n-            if not self.allow_server_roots:\n-                raise Exception(f\"Error: Published Root Not Valid.\")\n+                if leaf_index > 1:\n+                    tree_sizes.add(leaf_index - 1)\n+        tree_sizes.add(root.size)\n+\n+        # get all the roots from arweave\n+        response.result.published_roots = {\n+            tree_size: JSONObject(obj.get(\"data\", {}))\n+            for tree_size, obj in get_arweave_published_roots(\n+                root.tree_name, list(tree_sizes) + [root.size]\n+            ).items()\n+        }\n+        for tree_size, root in response.result.published_roots.items():\n+            root[\"source\"] = \"arweave\"\n+\n+        # fill the missing roots from the server\n+        for tree_size in tree_sizes:\n+            if tree_size not in response.result.published_roots:\n+                try:\n+                    response.result.published_roots[tree_size] = self.root(\n+                        tree_size\n+                    ).result.data\n+                    response.result.published_roots[tree_size][\"source\"] = \"pangea\"\n+                except:\n+                    pass\n+\n+        # if we've got the current root from arweave, replace the one from the server\n+        pub_root = response.result.published_roots.get(root.size)\n+        if pub_root:\n+            response.result.root = pub_root\n+\n+        # calculate the hashes from the data\n+        for a in response.result.audits:\n+            canon = canonicalize_log(a.data)\n+            a[\"calculated_hash\"] = hash_data(canon)\n \n         response_wrapper = AuditSearchResponse(response, data)\n-\n         return response_wrapper\n \n-\n     def verify_membership_proof(\n         self, root: JSONObject, audit: JSONObject, required: bool = False\n     ) -> bool:\n         if not audit.get(\"membership_proof\"):\n             return not required\n-        node_hash = decode_hash(audit.hash)\n+\n+        if not self.allow_server_roots and root.source != \"arweave\":\n+            return False\n+\n+        node_hash = decode_hash(audit.calculated_hash)\n         root_hash = decode_hash(root.root_hash)\n         proof = decode_proof(audit.membership_proof)\n         return verify_log_proof(node_hash, root_hash, proof)\n \n-\n     def verify_consistency_proof(\n-        self, audit: JSONObject, required: bool = False\n+        self,\n+        published_roots: dict[int, JSONObject],\n+        audit: JSONObject,\n+        required: bool = False,\n     ) -> bool:\n-        if not audit.get(\"published_roots\"):\n+        leaf_index = audit.get(\"leaf_index\")\n+        if not leaf_index:\n             return not required\n \n-        if not audit.published_roots.get(\"current\"):\n-            return False\n+        if leaf_index == 1:\n+            return not required\n \n-        if not audit.published_roots.get(\"previous\"):\n-            if audit.get(\"leaf_index\", 0) <= 1:\n-                return True\n-            else:\n-                return False\n+        curr_root = published_roots.get(leaf_index)\n+        prev_root = published_roots.get(leaf_index - 1)\n \n-        return verify_consistency_proof(\n-            audit.published_roots.current.data, audit.published_roots.previous.data\n-        )\n+        if not curr_root or not prev_root:\n+            return False\n+\n+        if not self.allow_server_roots and (\n+            curr_root.source != \"arweave\" or prev_root.source != \"arweave\"\n+        ):\n+            return False\n \n+        return verify_consistency_proof(curr_root, prev_root)\n \n     def root(self, tree_size: int = 0) -> AuditSearchResponse:\n         endpoint_name = \"root","add":65,"remove":79,"filename":"\/pangea\/services\/audit.py","badparts":["        response_result = response.result","        root = response_result.get(\"root\")","        if include_root and not root:","            raise Exception(","                f\"Error: `root` field not present.\"","            )    ","        root_hash_coded = root.get(\"root_hash\")","        if include_membership_proof and not root_hash_coded:","            raise Exception(","                f\"Error: `root_hash` field not present.\"","            )    ","        audits = response_result.get(\"audits\")","        if not audits:","            raise Exception(","                f\"Error: `audits` field not present.\"","            )        ","        for a in audits:","                tree_sizes.add(max(1, leaf_index - 1))","        try:","            published_roots = get_arweave_published_roots(","                root[\"tree_name\"], list(tree_sizes)","            )","        except Exception as e:","            published_roots = {tree_size: None for tree_size in tree_sizes}","        if published_roots:","            if published_roots.get(\"tree_size\"):","                if self.allow_server_roots:","                    for tree_size in published_roots:","                        if published_roots[tree_size] is None:","                            published_roots[tree_size] = self.root(tree_size).result","                if root_hash_coded is not None:","                    root_hash = decode_hash(root_hash_coded)","                    for a in response_result.audits:","                        leaf_index = a.get(\"leaf_index\")","                        if leaf_index is not None:","                            a[\"published_roots\"] = {","                                \"current\": published_roots[leaf_index],","                                \"previous\": published_roots[leaf_index - 1] if leaf_index > 0 else None,","                            }","        if not root.get(\"tree_name\") or not root.get(\"size\"):","            publish_resp_full = get_arweave_published_roots(root[\"tree_name\"], [root[\"size\"]])","            if publish_resp_full is not None:","                publish_resp = publish_resp_full[root[\"size\"]]","            else:","                publish_resp = None","        else:","            publish_resp = None","        if publish_resp is not None:","            publish_root_hash = decode_hash(publish_resp.get(\"root_hash\", \"\"))","            publish_verify = verify_published_root(root_hash, publish_root_hash)","            if not publish_verify:","                raise Exception(f\"Error: Published Root Not Valid.\")","        else:","            if not self.allow_server_roots:","                raise Exception(f\"Error: Published Root Not Valid.\")","        node_hash = decode_hash(audit.hash)","        self, audit: JSONObject, required: bool = False","        if not audit.get(\"published_roots\"):","        if not audit.published_roots.get(\"current\"):","            return False","        if not audit.published_roots.get(\"previous\"):","            if audit.get(\"leaf_index\", 0) <= 1:","                return True","            else:","                return False","        return verify_consistency_proof(","            audit.published_roots.current.data, audit.published_roots.previous.data","        )"],"goodparts":["        root = response.result.root","        if not root:","            response.result.root = {}","            response.result.published_roots = {}","            return AuditSearchResponse(response, data)","        for a in response.result.audits:","                if leaf_index > 1:","                    tree_sizes.add(leaf_index - 1)","        tree_sizes.add(root.size)","        response.result.published_roots = {","            tree_size: JSONObject(obj.get(\"data\", {}))","            for tree_size, obj in get_arweave_published_roots(","                root.tree_name, list(tree_sizes) + [root.size]","            ).items()","        }","        for tree_size, root in response.result.published_roots.items():","            root[\"source\"] = \"arweave\"","        for tree_size in tree_sizes:","            if tree_size not in response.result.published_roots:","                try:","                    response.result.published_roots[tree_size] = self.root(","                        tree_size","                    ).result.data","                    response.result.published_roots[tree_size][\"source\"] = \"pangea\"","                except:","                    pass","        pub_root = response.result.published_roots.get(root.size)","        if pub_root:","            response.result.root = pub_root","        for a in response.result.audits:","            canon = canonicalize_log(a.data)","            a[\"calculated_hash\"] = hash_data(canon)","        if not self.allow_server_roots and root.source != \"arweave\":","            return False","        node_hash = decode_hash(audit.calculated_hash)","        self,","        published_roots: dict[int, JSONObject],","        audit: JSONObject,","        required: bool = False,","        leaf_index = audit.get(\"leaf_index\")","        if not leaf_index:","        if leaf_index == 1:","            return not required","        curr_root = published_roots.get(leaf_index)","        prev_root = published_roots.get(leaf_index - 1)","        if not curr_root or not prev_root:","            return False","        if not self.allow_server_roots and (","            curr_root.source != \"arweave\" or prev_root.source != \"arweave\"","        ):","            return False","        return verify_consistency_proof(curr_root, prev_root)"]}],"source":"\n from datetime import date import requests from base64 import b64encode, b64decode from dateutil import parser from pangea.response import PangeaResponse from pangea.response import JSONObject from.base import ServiceBase from.audit_util import( canonicalize_log, decode_hash, decode_proof, decode_root, hash_data, verify_log_proof, to_msg, verify_published_root, base64url_decode, decode_server_response, bytes_to_json, verify_consistency_proof, get_arweave_published_roots ) SupportedFields=[ \"actor\", \"action\", \"created\", \"message\", \"new\", \"old\", \"status\", \"target\" ] class AuditSearchResponse(object): \"\"\" Wrap the base Response object to include search pagination support \"\"\" def __init__(self, response, data): self.response=response self.data=data def __getattr__(self, attr): return getattr(self.response, attr) def next(self): reg_count=0 if self.count: reg_count=int(self.count) reg_total=0 if self.total: reg_total=int(self.total) if reg_count < reg_total: params={ \"query\": self.data[\"query\"], \"last\": self.result[\"last\"], \"size\": self.data[\"max_results\"], } if hasattr(self.data, \"start\"): params.update({\"start\": self.data[\"start\"]}) if hasattr(self.data, \"end\"): params.update({\"end\": self.data[\"end\"]}) return params else: return None @property def total(self) -> str: total=\"0\" if self.success: last=self.result.last if last is not None: total=last.split(\"|\")[1] return total else: return total @property def count(self) -> str: count=\"0\" if self.success: last=self.result.last if last is not None: count=last.split(\"|\")[0] return count else: return count class Audit(ServiceBase): response_class=AuditSearchResponse service_name=\"audit\" version=\"v1\" allow_server_roots=( True ) def log(self, input: dict, signature=None, public_key=None, verify: bool=False) -> PangeaResponse: endpoint_name=\"log\" \"\"\" Filter input on valid search params, at least one valid param is required \"\"\" data={ \t \"data\":{}, \t \"return_hash\": \"true\" } for name in SupportedFields: if name in input: data[\"data\"][name]=input[name] if len(data) < 1: raise Exception( f\"Error: no valid parameters, require on or more of:{', '.join(SupportedFields)}\" ) if \"action\" not in data[\"data\"]: raise Exception(f\"Error: missing required field, no `action` provided\") if \"actor\" not in data[\"data\"]: raise Exception(f\"Error: missing required field, no `actor` provided\") if \"message\" not in data[\"data\"]: raise Exception(f\"Error: missing required field, no `message` provided\") if \"status\" not in data[\"data\"]: raise Exception(f\"Error: missing required field, no `status` provided\") if \"new\" not in data[\"data\"]: raise Exception(f\"Error: missing required field, no `new` provided\") if \"old\" not in data[\"data\"]: raise Exception(f\"Error: missing required field, no `old` provided\") if \"target\" not in data[\"data\"]: raise Exception(f\"Error: missing required field, no `target` provided\") resp=self.request.post(endpoint_name, data=data) return resp def search( self, query: str=\"\", size: int=20, start: str=\"\", end: str=\"\", last: str=\"\", signature=None, public_key= None, verify_proofs: bool=False, ) -> AuditSearchResponse: endpoint_name=\"search\" \"\"\" The `size` param determines the maximum results returned, it must be a positive integer. \"\"\" if not(isinstance(size, int) and size > 0): raise Exception(\"The 'size' argument must be a positive integer > 0\") if not query or not query.strip(): raise Exception( f\"Error: Query field is mandatory.\" ) include_membership_proof=True include_hash=True include_root=True data={ \"query\": query, \"max_results\": size, \t \"include_membership_proof\": include_membership_proof, \"include_hash\": include_hash, \t \"include_root\": include_root } if start: parser.isoparse(start) data.update({\"start\": start}) if end: parser.isoparse(end) data.update({\"end\": end}) if last: data.update({\"last\": last}) response=self.request.post(endpoint_name, data=data) if response is None: raise Exception( f\"Error: Empty result from server.\" ) elif response.result is None: raise Exception( f\"Error: Empty result from server.\" ) response_result=response.result root=response_result.get(\"root\") if include_root and not root: raise Exception( f\"Error: `root` field not present.\" ) root_hash_coded=root.get(\"root_hash\") if include_membership_proof and not root_hash_coded: raise Exception( f\"Error: `root_hash` field not present.\" ) audits=response_result.get(\"audits\") if not audits: raise Exception( f\"Error: `audits` field not present.\" ) tree_sizes=set() for a in audits: leaf_index=a.get(\"leaf_index\") if leaf_index is not None: tree_sizes.add(leaf_index) tree_sizes.add(max(1, leaf_index -1)) try: published_roots=get_arweave_published_roots( root[\"tree_name\"], list(tree_sizes) ) except Exception as e: published_roots={tree_size: None for tree_size in tree_sizes} if published_roots: if published_roots.get(\"tree_size\"): if self.allow_server_roots: for tree_size in published_roots: if published_roots[tree_size] is None: published_roots[tree_size]=self.root(tree_size).result if root_hash_coded is not None: root_hash=decode_hash(root_hash_coded) for a in response_result.audits: leaf_index=a.get(\"leaf_index\") if leaf_index is not None: a[\"published_roots\"]={ \"current\": published_roots[leaf_index], \"previous\": published_roots[leaf_index -1] if leaf_index > 0 else None, } if not root.get(\"tree_name\") or not root.get(\"size\"): publish_resp_full=get_arweave_published_roots(root[\"tree_name\"],[root[\"size\"]]) if publish_resp_full is not None: publish_resp=publish_resp_full[root[\"size\"]] else: publish_resp=None else: publish_resp=None if publish_resp is not None: publish_root_hash=decode_hash(publish_resp.get(\"root_hash\", \"\")) publish_verify=verify_published_root(root_hash, publish_root_hash) if not publish_verify: raise Exception(f\"Error: Published Root Not Valid.\") else: if not self.allow_server_roots: raise Exception(f\"Error: Published Root Not Valid.\") response_wrapper=AuditSearchResponse(response, data) return response_wrapper def verify_membership_proof( self, root: JSONObject, audit: JSONObject, required: bool=False ) -> bool: if not audit.get(\"membership_proof\"): return not required node_hash=decode_hash(audit.hash) root_hash=decode_hash(root.root_hash) proof=decode_proof(audit.membership_proof) return verify_log_proof(node_hash, root_hash, proof) def verify_consistency_proof( self, audit: JSONObject, required: bool=False ) -> bool: if not audit.get(\"published_roots\"): return not required if not audit.published_roots.get(\"current\"): return False if not audit.published_roots.get(\"previous\"): if audit.get(\"leaf_index\", 0) <=1: return True else: return False return verify_consistency_proof( audit.published_roots.current.data, audit.published_roots.previous.data ) def root(self, tree_size: int=0) -> AuditSearchResponse: endpoint_name=\"root\" data={} if tree_size > 0: data[\"tree_size\"]=tree_size return self.request.post(endpoint_name, data=data) ","sourceWithComments":"# Copyright 2022 Pangea Cyber Corporation\n# Author: Pangea Cyber Corporation\n\nfrom datetime import date\nimport requests\n\nfrom base64 import b64encode, b64decode\nfrom dateutil import parser\n\nfrom pangea.response import PangeaResponse\nfrom pangea.response import JSONObject\nfrom .base import ServiceBase\nfrom .audit_util import (\n    canonicalize_log,\n    decode_hash,\n    decode_proof,\n    decode_root,\n    hash_data,\n    verify_log_proof,\n    to_msg,\n    verify_published_root,\n    base64url_decode,\n    decode_server_response,\n    bytes_to_json,\n    verify_consistency_proof,\n    get_arweave_published_roots    \n)\n\n# The fields in a top-level audit log record.\nSupportedFields = [\n    \"actor\",\n    \"action\",\n    \"created\",\n    \"message\",\n    \"new\",\n    \"old\",\n    \"status\",\n    \"target\"\n]\n\nclass AuditSearchResponse(object):\n    \"\"\"\n    Wrap the base Response object to include search pagination support\n    \"\"\"\n\n    def __init__(self, response, data):\n        self.response = response\n        self.data = data\n\n    def __getattr__(self, attr):\n        return getattr(self.response, attr)\n\n    def next(self):\n        reg_count = 0\n        if self.count:\n            reg_count = int(self.count)\n\n        reg_total = 0\n        if self.total:\n            reg_total = int(self.total)\n\n        if reg_count < reg_total:\n            params = {\n                \"query\": self.data[\"query\"],\n                \"last\": self.result[\"last\"],\n                \"size\": self.data[\"max_results\"],\n            }\n\n            if hasattr(self.data, \"start\"):\n                params.update({\"start\": self.data[\"start\"]})\n\n            if hasattr(self.data, \"end\"):\n                params.update({\"end\": self.data[\"end\"]})\n\n            return params\n        else:\n            return None\n\n    @property\n    def total(self) -> str:\n        total = \"0\"\n        if self.success:\n            last = self.result.last\n            if last is not None:\n                total = last.split(\"|\")[1]\n            return total\n        else:\n            return total\n\n    @property\n    def count(self) -> str:\n        count = \"0\"\n        if self.success:\n            last = self.result.last\n            if last is not None:\n                count = last.split(\"|\")[0]\n            return count\n        else:\n            return count\n\n\nclass Audit(ServiceBase):\n    response_class = AuditSearchResponse\n    service_name = \"audit\"\n    version = \"v1\"\n    allow_server_roots = (\n        True  # In case of Arweave failure, ask the server for the roots\n    )\n\n    def log(self, input: dict, signature = None, public_key = None, verify: bool = False) -> PangeaResponse:\n        endpoint_name = \"log\"\n\n        \"\"\"\n        Filter input on valid search params, at least one valid param is required\n        \"\"\"\n\n        data = {\n\t        \"data\": {},\n\t        \"return_hash\": \"true\"\n        }\n\n        for name in SupportedFields:\n            if name in input:\n                data[\"data\"][name] = input[name]\n\n        if len(data) < 1:\n            raise Exception(\n                f\"Error: no valid parameters, require on or more of: {', '.join(SupportedFields)}\"\n            )\n\n        if \"action\" not in data[\"data\"]:\n            raise Exception(f\"Error: missing required field, no `action` provided\")\n\n        if \"actor\" not in data[\"data\"]:\n            raise Exception(f\"Error: missing required field, no `actor` provided\")\n\n        if \"message\" not in data[\"data\"]:\n            raise Exception(f\"Error: missing required field, no `message` provided\")\n\n        if \"status\" not in data[\"data\"]:\n            raise Exception(f\"Error: missing required field, no `status` provided\")\n\n        if \"new\" not in data[\"data\"]:\n            raise Exception(f\"Error: missing required field, no `new` provided\")\n            \n        if \"old\" not in data[\"data\"]:\n            raise Exception(f\"Error: missing required field, no `old` provided\")\n            \n        if \"target\" not in data[\"data\"]:\n            raise Exception(f\"Error: missing required field, no `target` provided\")\n                        \n        resp = self.request.post(endpoint_name, data=data)\n\n        return resp\n\n\n    def search(\n        self,\n        query: str = \"\",\n        size: int = 20,\n        start: str = \"\",\n        end: str = \"\",\n        last: str = \"\",\n        signature = None,\n        public_key =  None,\n        verify_proofs: bool = False,\n    ) -> AuditSearchResponse:\n        endpoint_name = \"search\"\n\n        \"\"\"\n        The `size` param determines the maximum results returned, it must be a positive integer.\n        \"\"\"\n        if not (isinstance(size, int) and size > 0):\n            raise Exception(\"The 'size' argument must be a positive integer > 0\")\n\n        if not query or not query.strip():\n            raise Exception(\n                f\"Error: Query field is mandatory.\"\n            )\n\n        include_membership_proof = True\n        include_hash = True\n        include_root = True\n\n        data = {\n            \"query\": query, \n            \"max_results\": size,\n\t        \"include_membership_proof\": include_membership_proof,\n            \"include_hash\": include_hash,\n\t        \"include_root\": include_root            \n            }\n\n        if start:    \n            parser.isoparse(start)\n            data.update({\"start\": start})\n\n        if end:       \n            parser.isoparse(end)\n            data.update({\"end\": end})\n\n        if last:\n            data.update({\"last\": last})\n\n        response = self.request.post(endpoint_name, data=data)\n\n        if response is None:\n            raise Exception(\n                f\"Error: Empty result from server.\"\n            )\n        elif response.result is None:\n            raise Exception(\n                f\"Error: Empty result from server.\"\n            )\n\n        response_result = response.result\n\n        root = response_result.get(\"root\")\n        if include_root and not root:\n            raise Exception(\n                f\"Error: `root` field not present.\"\n            )    \n\n        root_hash_coded = root.get(\"root_hash\")\n        if include_membership_proof and not root_hash_coded:\n            raise Exception(\n                f\"Error: `root_hash` field not present.\"\n            )    \n\n        audits = response_result.get(\"audits\")\n        if not audits:\n            raise Exception(\n                f\"Error: `audits` field not present.\"\n            )        \n\n        tree_sizes = set()\n        for a in audits:\n            leaf_index = a.get(\"leaf_index\")\n            if leaf_index is not None:\n                tree_sizes.add(leaf_index)\n                tree_sizes.add(max(1, leaf_index - 1))\n\n        try:\n            published_roots = get_arweave_published_roots(\n                root[\"tree_name\"], list(tree_sizes)\n            )\n        except Exception as e:\n            published_roots = {tree_size: None for tree_size in tree_sizes}\n\n        if published_roots:\n            if published_roots.get(\"tree_size\"):\n                if self.allow_server_roots:\n                    for tree_size in published_roots:\n                        if published_roots[tree_size] is None:\n                            published_roots[tree_size] = self.root(tree_size).result\n\n                if root_hash_coded is not None:\n                    root_hash = decode_hash(root_hash_coded)\n                    for a in response_result.audits:\n                        leaf_index = a.get(\"leaf_index\")\n                        if leaf_index is not None:\n                            a[\"published_roots\"] = {\n                                \"current\": published_roots[leaf_index],\n                                \"previous\": published_roots[leaf_index - 1] if leaf_index > 0 else None,\n                            }\n\n        if not root.get(\"tree_name\") or not root.get(\"size\"):\n            publish_resp_full = get_arweave_published_roots(root[\"tree_name\"], [root[\"size\"]])\n            if publish_resp_full is not None:\n                publish_resp = publish_resp_full[root[\"size\"]]\n            else:\n                publish_resp = None\n        else:\n            publish_resp = None\n                \n        if publish_resp is not None:\n            publish_root_hash = decode_hash(publish_resp.get(\"root_hash\", \"\"))\n            publish_verify = verify_published_root(root_hash, publish_root_hash)\n\n            if not publish_verify:\n                raise Exception(f\"Error: Published Root Not Valid.\")\n        else:\n            if not self.allow_server_roots:\n                raise Exception(f\"Error: Published Root Not Valid.\")\n\n        response_wrapper = AuditSearchResponse(response, data)\n\n        return response_wrapper\n\n\n    def verify_membership_proof(\n        self, root: JSONObject, audit: JSONObject, required: bool = False\n    ) -> bool:\n        if not audit.get(\"membership_proof\"):\n            return not required\n        node_hash = decode_hash(audit.hash)\n        root_hash = decode_hash(root.root_hash)\n        proof = decode_proof(audit.membership_proof)\n        return verify_log_proof(node_hash, root_hash, proof)\n\n\n    def verify_consistency_proof(\n        self, audit: JSONObject, required: bool = False\n    ) -> bool:\n        if not audit.get(\"published_roots\"):\n            return not required\n\n        if not audit.published_roots.get(\"current\"):\n            return False\n\n        if not audit.published_roots.get(\"previous\"):\n            if audit.get(\"leaf_index\", 0) <= 1:\n                return True\n            else:\n                return False\n\n        return verify_consistency_proof(\n            audit.published_roots.current.data, audit.published_roots.previous.data\n        )\n\n\n    def root(self, tree_size: int = 0) -> AuditSearchResponse:\n        endpoint_name = \"root\"\n\n        data = {}\n\n        if tree_size > 0:\n            data[\"tree_size\"] = tree_size\n\n        return self.request.post(endpoint_name, data=data)\n"},"\/pangea\/services\/audit_util.py":{"changes":[{"diff":"\n     proof: Proof = []\n     for item in data.split(\",\"):\n         parts = item.split(\":\")\n-        proof.append(ProofItem(side=\"left\" if parts[0] == \"l\" else \"right\", node_hash=decode_hash(parts[1])))\n+        proof.append(\n+            ProofItem(\n+                side=\"left\" if parts[0] == \"l\" else \"right\",\n+                node_hash=decode_hash(parts[1]),\n+            )\n+        )\n     return proof\n \n \n","add":6,"remove":1,"filename":"\/pangea\/services\/audit_util.py","badparts":["        proof.append(ProofItem(side=\"left\" if parts[0] == \"l\" else \"right\", node_hash=decode_hash(parts[1])))"],"goodparts":["        proof.append(","            ProofItem(","                side=\"left\" if parts[0] == \"l\" else \"right\",","                node_hash=decode_hash(parts[1]),","            )","        )"]},{"diff":"\n     for item in data:\n         ndx = item.index(\",\")\n         root_proof.append(\n-            RootProofItem(node_hash=decode_hash(item[:ndx].split(\":\")[1]), proof=decode_proof(item[ndx + 1 :]))\n+            RootProofItem(\n+                node_hash=decode_hash(item[:ndx].split(\":\")[1]),\n+                proof=decode_proof(item[ndx + 1 :]),\n+            )\n         )\n     return root_proof\n \n","add":4,"remove":1,"filename":"\/pangea\/services\/audit_util.py","badparts":["            RootProofItem(node_hash=decode_hash(item[:ndx].split(\":\")[1]), proof=decode_proof(item[ndx + 1 :]))"],"goodparts":["            RootProofItem(","                node_hash=decode_hash(item[:ndx].split(\":\")[1]),","                proof=decode_proof(item[ndx + 1 :]),","            )"]},{"diff":"\n def verify_log_proof(node_hash: Hash, root_hash: Hash, proof: Proof) -> bool:\n     for proof_item in proof:\n         proof_hash = proof_item.node_hash\n-        node_hash = hash_pair(proof_hash, node_hash) if proof_item.side == \"left\" else hash_pair(node_hash, proof_hash)\n+        node_hash = (\n+            hash_pair(proof_hash, node_hash)\n+            if proof_item.side == \"left\"\n+            else hash_pair(node_hash, proof_hash)\n+        )\n     return root_hash == node_hash\n \n \n","add":5,"remove":1,"filename":"\/pangea\/services\/audit_util.py","badparts":["        node_hash = hash_pair(proof_hash, node_hash) if proof_item.side == \"left\" else hash_pair(node_hash, proof_hash)"],"goodparts":["        node_hash = (","            hash_pair(proof_hash, node_hash)","            if proof_item.side == \"left\"","            else hash_pair(node_hash, proof_hash)","        )"]},{"diff":"\n \n \n def canonicalize_log(audit: dict) -> bytes:\n-    def _default(obj):\n-        if not any(isinstance(obj, typ) for typ in JSON_TYPES):\n-            return str(obj)\n-        else:\n-            return obj\n-\n-    # stringify invalid JSON types before canonicalizing\n     return json.dumps(\n-        audit, ensure_ascii=False, allow_nan=False, separators=(\",\", \":\"), sort_keys=True, default=_default\n+        audit,\n+        ensure_ascii=False,\n+        allow_nan=False,\n+        separators=(\",\", \":\"),\n+        sort_keys=True,\n     ).encode(\"utf-8\")\n \n \n","add":5,"remove":8,"filename":"\/pangea\/services\/audit_util.py","badparts":["    def _default(obj):","        if not any(isinstance(obj, typ) for typ in JSON_TYPES):","            return str(obj)","        else:","            return obj","        audit, ensure_ascii=False, allow_nan=False, separators=(\",\", \":\"), sort_keys=True, default=_default"],"goodparts":["        audit,","        ensure_ascii=False,","        allow_nan=False,","        separators=(\",\", \":\"),","        sort_keys=True,"]},{"diff":"\n \n \n def to_msg(b):\n-    return \"OK\" if b else \"FAILED\"   \n+    return \"OK\" if b else \"FAILED\"\n \n \n def base64url_decode(input):\n","add":1,"remove":1,"filename":"\/pangea\/services\/audit_util.py","badparts":["    return \"OK\" if b else \"FAILED\"   "],"goodparts":["    return \"OK\" if b else \"FAILED\""]},{"diff":"\n \n def get_arweave_published_roots(\n     tree_name: str, tree_sizes: list[int]\n-) -> dict[int, Optional[dict]]:\n+) -> dict[int, dict]:\n     if len(tree_sizes) == 0:\n         return {}\n \n","add":1,"remove":1,"filename":"\/pangea\/services\/audit_util.py","badparts":[") -> dict[int, Optional[dict]]:"],"goodparts":[") -> dict[int, dict]:"]},{"diff":"\n     )\n \n     resp = requests.post(arweave_graphql_url(), json={\"query\": query})\n-    resp.raise_for_status()\n-    ans: dict[int, Optional[dict]] = {tree_size: None for tree_size in tree_sizes}\n-    data = resp.json()\n+    if resp.status_code != 200:\n+        return {}\n \n-    if data[\"data\"][\"transactions\"].get(\"edges\"):\n-        for edge in data[\"data\"][\"transactions\"][\"edges\"]:\n-            node_id = edge[\"node\"][\"id\"]\n-            tree_size = int(\n-                next(\n-                    tag[\"value\"]\n-                    for tag in edge[\"node\"][\"tags\"]\n-                    if tag[\"name\"] == \"tree_size\"\n-                )\n+    ans: dict[int, dict] = {}\n+    data = resp.json()\n+    for edge in data.get(\"data\", {}).get(\"transactions\", {}).get(\"edges\", []):\n+        try:\n+            node_id = edge.get(\"node\").get(\"id\")\n+            tree_size = next(\n+                tag.get(\"value\")\n+                for tag in edge.get(\"node\").get(\"tags\", [])\n+                if tag.get(\"name\") == \"tree_size\"\n             )\n+\n             url = arweave_transaction_url(node_id)\n \n             # TODO: do all the requests concurrently\n             resp2 = requests.get(url)\n-            if resp2.status_code == 200 and resp2.text.strip() != \"\": \n+            if resp2.status_code == 200 and resp2.text != 'Pending':\n                 ans[tree_size] = json.loads(base64url_decode(resp2.text))\n-                return ans\n-    return {}\n+        except:\n+            pass\n+    return ans\n \n \n def verify_consistency_proof(new_root: dict, prev_root: dict) -> bool:\n","add":16,"remove":15,"filename":"\/pangea\/services\/audit_util.py","badparts":["    resp.raise_for_status()","    ans: dict[int, Optional[dict]] = {tree_size: None for tree_size in tree_sizes}","    data = resp.json()","    if data[\"data\"][\"transactions\"].get(\"edges\"):","        for edge in data[\"data\"][\"transactions\"][\"edges\"]:","            node_id = edge[\"node\"][\"id\"]","            tree_size = int(","                next(","                    tag[\"value\"]","                    for tag in edge[\"node\"][\"tags\"]","                    if tag[\"name\"] == \"tree_size\"","                )","            if resp2.status_code == 200 and resp2.text.strip() != \"\": ","                return ans","    return {}"],"goodparts":["    if resp.status_code != 200:","        return {}","    ans: dict[int, dict] = {}","    data = resp.json()","    for edge in data.get(\"data\", {}).get(\"transactions\", {}).get(\"edges\", []):","        try:","            node_id = edge.get(\"node\").get(\"id\")","            tree_size = next(","                tag.get(\"value\")","                for tag in edge.get(\"node\").get(\"tags\", [])","                if tag.get(\"name\") == \"tree_size\"","            if resp2.status_code == 200 and resp2.text != 'Pending':","        except:","            pass","    return ans"]}],"source":"\n import base64 import json import struct import requests from binascii import hexlify, unhexlify from dataclasses import dataclass from hashlib import sha256 from dateutil import parser from typing import Optional Hash=bytes JSON_TYPES=[int, float, str, bool] ARWEAVE_BASE_URL=\"https:\/\/arweave.net\" @dataclass class HotRoot: tree_size: int root_hash: Hash tree_id: str @dataclass class Root: tree_size: int root_hash: Hash @dataclass class ProofItem: side: str node_hash: Hash Proof=list[ProofItem] @dataclass class RootProofItem: node_hash: Hash proof: Proof RootProof=list[RootProofItem] def decode_hash(hexhash: str) -> Hash: return unhexlify(hexhash.encode(\"utf8\")) def encode_hash(hash_: Hash) -> str: return hexlify(hash_).decode(\"utf8\") def hash_pair(hash1: Hash, hash2: Hash) -> Hash: return sha256(hash1 +hash2).digest() def decode_root(data: str) -> Root: tree_size_enc=unhexlify(data[:8].encode(\"utf8\")) data=data[8:] tree_size=struct.unpack(\"=L\", tree_size_enc)[0] root_hash=decode_hash(data[: 32 * 2]) data=data[32 * 2:] return Root(tree_size=tree_size, root_hash=root_hash) def decode_proof(data: str) -> Proof: proof: Proof=[] for item in data.split(\",\"): parts=item.split(\":\") proof.append(ProofItem(side=\"left\" if parts[0]==\"l\" else \"right\", node_hash=decode_hash(parts[1]))) return proof def decode_root_proof(data: list[str]) -> RootProof: root_proof=[] for item in data: ndx=item.index(\",\") root_proof.append( RootProofItem(node_hash=decode_hash(item[:ndx].split(\":\")[1]), proof=decode_proof(item[ndx +1:])) ) return root_proof def decode_server_response(data: str) -> dict: data_dec=base64.b64decode(data.encode(\"utf8\")) data_obj=json.loads(data_dec) return data_obj def verify_log_proof(node_hash: Hash, root_hash: Hash, proof: Proof) -> bool: for proof_item in proof: proof_hash=proof_item.node_hash node_hash=hash_pair(proof_hash, node_hash) if proof_item.side==\"left\" else hash_pair(node_hash, proof_hash) return root_hash==node_hash def verify_published_root(root_hash: Hash, publish_hash: Hash) -> bool: return root_hash==publish_hash def canonicalize_log(audit: dict) -> bytes: def _default(obj): if not any(isinstance(obj, typ) for typ in JSON_TYPES): return str(obj) else: return obj return json.dumps( audit, ensure_ascii=False, allow_nan=False, separators=(\",\", \":\"), sort_keys=True, default=_default ).encode(\"utf-8\") def hash_data(data: bytes) -> str: return sha256(data).hexdigest() def to_msg(b): return \"OK\" if b else \"FAILED\" def base64url_decode(input): \"\"\"Helper method to base64url_decode a string. Args: input(str): A base64url_encoded string to decode. \"\"\" rem=len(input) % 4 if rem > 0: input +=\"=\" *(4 -rem) return base64.urlsafe_b64decode(input) def bytes_to_json(input: bytes) -> dict: return json.loads(input.decode(\"utf8\")) def arweave_transaction_url(trans_id: str): return f\"{ARWEAVE_BASE_URL}\/tx\/{trans_id}\/data\/\" def arweave_graphql_url(): return f\"{ARWEAVE_BASE_URL}\/graphql\" def get_arweave_published_roots( tree_name: str, tree_sizes: list[int] ) -> dict[int, Optional[dict]]: if len(tree_sizes)==0: return{} query=\"\"\" { transactions( \t\t\ttags:[ { name: \"tree_size\" values:[{tree_sizes}] }, { name: \"tree_name\" values:[\"{tree_name}\"] } \t ] ){ edges{ node{ id tags{ name value } } } } } \"\"\".replace( \"{tree_sizes}\", \", \".join(f'\"{tree_size}\"' for tree_size in tree_sizes) ).replace( \"{tree_name}\", tree_name ) resp=requests.post(arweave_graphql_url(), json={\"query\": query}) resp.raise_for_status() ans: dict[int, Optional[dict]]={tree_size: None for tree_size in tree_sizes} data=resp.json() if data[\"data\"][\"transactions\"].get(\"edges\"): for edge in data[\"data\"][\"transactions\"][\"edges\"]: node_id=edge[\"node\"][\"id\"] tree_size=int( next( tag[\"value\"] for tag in edge[\"node\"][\"tags\"] if tag[\"name\"]==\"tree_size\" ) ) url=arweave_transaction_url(node_id) resp2=requests.get(url) if resp2.status_code==200 and resp2.text.strip() !=\"\": ans[tree_size]=json.loads(base64url_decode(resp2.text)) return ans return{} def verify_consistency_proof(new_root: dict, prev_root: dict) -> bool: if new_root is None or prev_root is None: return False prev_root_hash=decode_hash(prev_root[\"root_hash\"]) new_root_hash=decode_hash(new_root[\"root_hash\"]) consistency_proof=decode_root_proof(new_root[\"consistency_proof\"]) root_hash=consistency_proof[0].node_hash for item in consistency_proof[1:]: root_hash=hash_pair(item.node_hash, root_hash) if root_hash !=prev_root_hash: return False for i, item in enumerate(consistency_proof): if not verify_log_proof(item.node_hash, new_root_hash, item.proof): print(f\"failed validation proof number{i}\") return False return True ","sourceWithComments":"# Copyright 2022 Pangea Cyber Corporation\n# Author: Pangea Cyber Corporation\n\nimport base64\nimport json\nimport struct\nimport requests\nfrom binascii import hexlify, unhexlify\nfrom dataclasses import dataclass\nfrom hashlib import sha256\nfrom dateutil import parser\nfrom typing import Optional\n\nHash = bytes\n\nJSON_TYPES = [int, float, str, bool]\n\nARWEAVE_BASE_URL = \"https:\/\/arweave.net\"\n\n@dataclass\nclass HotRoot:\n    tree_size: int\n    root_hash: Hash\n    tree_id: str\n\n\n@dataclass\nclass Root:\n    tree_size: int\n    root_hash: Hash\n\n\n@dataclass\nclass ProofItem:\n    side: str\n    node_hash: Hash\n\n\nProof = list[ProofItem]\n\n\n@dataclass\nclass RootProofItem:\n    node_hash: Hash\n    proof: Proof\n\n\nRootProof = list[RootProofItem]\n\n\ndef decode_hash(hexhash: str) -> Hash:\n    return unhexlify(hexhash.encode(\"utf8\"))\n\n\ndef encode_hash(hash_: Hash) -> str:\n    return hexlify(hash_).decode(\"utf8\")\n\n\ndef hash_pair(hash1: Hash, hash2: Hash) -> Hash:\n    return sha256(hash1 + hash2).digest()\n\n\ndef decode_root(data: str) -> Root:\n    tree_size_enc = unhexlify(data[:8].encode(\"utf8\"))\n    data = data[8:]\n    tree_size = struct.unpack(\"=L\", tree_size_enc)[0]\n\n    root_hash = decode_hash(data[: 32 * 2])\n    data = data[32 * 2 :]\n\n    return Root(tree_size=tree_size, root_hash=root_hash)\n\n\ndef decode_proof(data: str) -> Proof:\n    proof: Proof = []\n    for item in data.split(\",\"):\n        parts = item.split(\":\")\n        proof.append(ProofItem(side=\"left\" if parts[0] == \"l\" else \"right\", node_hash=decode_hash(parts[1])))\n    return proof\n\n\ndef decode_root_proof(data: list[str]) -> RootProof:\n    root_proof = []\n    for item in data:\n        ndx = item.index(\",\")\n        root_proof.append(\n            RootProofItem(node_hash=decode_hash(item[:ndx].split(\":\")[1]), proof=decode_proof(item[ndx + 1 :]))\n        )\n    return root_proof\n\n\ndef decode_server_response(data: str) -> dict:\n    data_dec = base64.b64decode(data.encode(\"utf8\"))\n    data_obj = json.loads(data_dec)\n    return data_obj\n\n\ndef verify_log_proof(node_hash: Hash, root_hash: Hash, proof: Proof) -> bool:\n    for proof_item in proof:\n        proof_hash = proof_item.node_hash\n        node_hash = hash_pair(proof_hash, node_hash) if proof_item.side == \"left\" else hash_pair(node_hash, proof_hash)\n    return root_hash == node_hash\n\n\ndef verify_published_root(root_hash: Hash, publish_hash: Hash) -> bool:\n    return root_hash == publish_hash\n\n\ndef canonicalize_log(audit: dict) -> bytes:\n    def _default(obj):\n        if not any(isinstance(obj, typ) for typ in JSON_TYPES):\n            return str(obj)\n        else:\n            return obj\n\n    # stringify invalid JSON types before canonicalizing\n    return json.dumps(\n        audit, ensure_ascii=False, allow_nan=False, separators=(\",\", \":\"), sort_keys=True, default=_default\n    ).encode(\"utf-8\")\n\n\ndef hash_data(data: bytes) -> str:\n    return sha256(data).hexdigest()\n\n\ndef to_msg(b):\n    return \"OK\" if b else \"FAILED\"   \n\n\ndef base64url_decode(input):\n    \"\"\"Helper method to base64url_decode a string.\n\n    Args:\n        input (str): A base64url_encoded string to decode.\n\n    \"\"\"\n    rem = len(input) % 4\n\n    if rem > 0:\n        input += \"=\" * (4 - rem)\n\n    return base64.urlsafe_b64decode(input)\n\n\ndef bytes_to_json(input: bytes) -> dict:\n    return json.loads(input.decode(\"utf8\"))\n\n\ndef arweave_transaction_url(trans_id: str):\n    return f\"{ARWEAVE_BASE_URL}\/tx\/{trans_id}\/data\/\"\n\n\ndef arweave_graphql_url():\n    return f\"{ARWEAVE_BASE_URL}\/graphql\"\n\n\ndef get_arweave_published_roots(\n    tree_name: str, tree_sizes: list[int]\n) -> dict[int, Optional[dict]]:\n    if len(tree_sizes) == 0:\n        return {}\n\n    query = \"\"\"\n    {\n        transactions(\n  \t\t\ttags: [\n                {\n                    name: \"tree_size\"\n                    values: [{tree_sizes}]\n                },\n                {\n                    name: \"tree_name\"\n                    values: [\"{tree_name}\"]\n                }\n    \t    ]      \n        ) {\n            edges {\n                node {\n                    id\n                    tags {\n                        name\n                        value\n                    }\n                }\n            }\n        }\n    }\n    \"\"\".replace(\n        \"{tree_sizes}\", \", \".join(f'\"{tree_size}\"' for tree_size in tree_sizes)\n    ).replace(\n        \"{tree_name}\", tree_name\n    )\n\n    resp = requests.post(arweave_graphql_url(), json={\"query\": query})\n    resp.raise_for_status()\n    ans: dict[int, Optional[dict]] = {tree_size: None for tree_size in tree_sizes}\n    data = resp.json()\n\n    if data[\"data\"][\"transactions\"].get(\"edges\"):\n        for edge in data[\"data\"][\"transactions\"][\"edges\"]:\n            node_id = edge[\"node\"][\"id\"]\n            tree_size = int(\n                next(\n                    tag[\"value\"]\n                    for tag in edge[\"node\"][\"tags\"]\n                    if tag[\"name\"] == \"tree_size\"\n                )\n            )\n            url = arweave_transaction_url(node_id)\n\n            # TODO: do all the requests concurrently\n            resp2 = requests.get(url)\n            if resp2.status_code == 200 and resp2.text.strip() != \"\": \n                ans[tree_size] = json.loads(base64url_decode(resp2.text))\n                return ans\n    return {}\n\n\ndef verify_consistency_proof(new_root: dict, prev_root: dict) -> bool:\n    if new_root is None or prev_root is None:\n        return False\n    prev_root_hash = decode_hash(prev_root[\"root_hash\"])\n    new_root_hash = decode_hash(new_root[\"root_hash\"])\n    consistency_proof = decode_root_proof(new_root[\"consistency_proof\"])\n\n    # check the prev_root\n    root_hash = consistency_proof[0].node_hash\n    for item in consistency_proof[1:]:\n        root_hash = hash_pair(item.node_hash, root_hash)\n\n    if root_hash != prev_root_hash:\n        return False\n\n    for i, item in enumerate(consistency_proof):\n        if not verify_log_proof(item.node_hash, new_root_hash, item.proof):\n            print(f\"failed validation proof number {i}\")\n            return False\n\n    return True\n"}},"msg":"GEA-0-tamper-proofing-fixes"}},"https:\/\/github.com\/khashashin\/django-appwrite":{"c8d1f79f279c3285237a6bb4f39dc07b081595dd":{"url":"https:\/\/api.github.com\/repos\/khashashin\/django-appwrite\/commits\/c8d1f79f279c3285237a6bb4f39dc07b081595dd","html_url":"https:\/\/github.com\/khashashin\/django-appwrite\/commit\/c8d1f79f279c3285237a6bb4f39dc07b081595dd","message":"fix(security): Replace user ID with JWT in communication with Appwrite server\n\nThis commit replaces the use of user IDs with JSON Web Tokens (JWT) in communication with the Appwrite server. The previous method of using user IDs carried potential security risks, as it could be subject to tampering or misuse. The new method uses JWT for authorization, which provides a secure and verifiable method of identifying the user. This change enhances the security of the system and makes it easier for developers to ensure that sensitive data is protected. The change is classified as a bug fix, as it repairs a defect in the security of the system.","sha":"c8d1f79f279c3285237a6bb4f39dc07b081595dd","keyword":"tampering fix","diff":"diff --git a\/django_appwrite\/middleware.py b\/django_appwrite\/middleware.py\nindex c0c98c1..29ce699 100644\n--- a\/django_appwrite\/middleware.py\n+++ b\/django_appwrite\/middleware.py\n@@ -1,8 +1,8 @@\n-from django.contrib.auth import authenticate\n+from django.contrib.auth import authenticate, login\n from django.contrib.auth import get_user_model\n from django.conf import settings\n from appwrite.client import Client\n-from appwrite.services.users import Users\n+from appwrite.services.account import Account\n from django.utils.deprecation import MiddlewareMixin\n \n User = get_user_model()\n@@ -17,6 +17,7 @@ def __init__(self, get_response):\n             project_endpoint = settings.APPWRITE.get('PROJECT_ENDPOINT')\n             project_id = settings.APPWRITE.get('PROJECT_ID')\n             project_key = settings.APPWRITE.get('PROJECT_API_KEY')\n+            self.auth_header = settings.APPWRITE.get('AUTH_HEADER', 'HTTP_AUTHORIZATION')\n             self.user_id_header = settings.APPWRITE.get('USER_ID_HEADER', 'HTTP_USER_ID')\n             self.verify_email = settings.APPWRITE.get('VERIFY_EMAIL', False)\n             self.verify_phone = settings.APPWRITE.get('VERIFY_PHONE', False)\n@@ -37,19 +38,22 @@ def __init__(self, get_response):\n                        .set_project(project_id)\n                        .set_key(project_key))\n \n-        # Initialize Appwrite Users service\n-        self.users = Users(self.client)\n-\n     def __call__(self, request, *args, **kwargs):\n-        # Get the user ID from the header\n-        user_id = request.META.get(self.user_id_header, '')\n+        try:\n+            # Get the user ID from the header\n+            user_id = request.META.get(self.user_id_header, '')\n+            auth_header = request.META.get(self.auth_header, '')\n+            jwt = auth_header.replace('Bearer ', '')\n+        except Exception as e:\n+            return self.get_response(request)\n \n         user_info = None\n         # If the user ID header is present\n-        if user_id:\n+        if user_id and jwt:\n             try:\n                 # Get the user information from Appwrite\n-                user_info = self.users.get(user_id)\n+                self.client.set_jwt(jwt)\n+                user_info = Account(self.client).get()\n             except Exception as e:\n                 # Return the response without doing anything\n                 return self.get_response(request)\n@@ -83,6 +87,7 @@ def __call__(self, request, *args, **kwargs):\n             # If the authentication was successful, log the user in\n             if user:\n                 request.user = user\n+                login(request, user)\n \n         # Call the next middleware\/view in the pipeline\n         response = self.get_response(request)\n","files":{"\/django_appwrite\/middleware.py":{"changes":[{"diff":"\n-from django.contrib.auth import authenticate\n+from django.contrib.auth import authenticate, login\n from django.contrib.auth import get_user_model\n from django.conf import settings\n from appwrite.client import Client\n-from appwrite.services.users import Users\n+from appwrite.services.account import Account\n from django.utils.deprecation import MiddlewareMixin\n \n User = get_user_model()\n","add":2,"remove":2,"filename":"\/django_appwrite\/middleware.py","badparts":["from django.contrib.auth import authenticate","from appwrite.services.users import Users"],"goodparts":["from django.contrib.auth import authenticate, login","from appwrite.services.account import Account"]},{"diff":"\n                        .set_project(project_id)\n                        .set_key(project_key))\n \n-        # Initialize Appwrite Users service\n-        self.users = Users(self.client)\n-\n     def __call__(self, request, *args, **kwargs):\n-        # Get the user ID from the header\n-        user_id = request.META.get(self.user_id_header, '')\n+        try:\n+            # Get the user ID from the header\n+            user_id = request.META.get(self.user_id_header, '')\n+            auth_header = request.META.get(self.auth_header, '')\n+            jwt = auth_header.replace('Bearer ', '')\n+        except Exception as e:\n+            return self.get_response(request)\n \n         user_info = None\n         # If the user ID header is present\n-        if user_id:\n+        if user_id and jwt:\n             try:\n                 # Get the user information from Appwrite\n-                user_info = self.users.get(user_id)\n+                self.client.set_jwt(jwt)\n+                user_info = Account(self.client).get()\n             except Exception as e:\n                 # Return the response without doing anything\n                 return self.get_response(request)\n","add":10,"remove":7,"filename":"\/django_appwrite\/middleware.py","badparts":["        self.users = Users(self.client)","        user_id = request.META.get(self.user_id_header, '')","        if user_id:","                user_info = self.users.get(user_id)"],"goodparts":["        try:","            user_id = request.META.get(self.user_id_header, '')","            auth_header = request.META.get(self.auth_header, '')","            jwt = auth_header.replace('Bearer ', '')","        except Exception as e:","            return self.get_response(request)","        if user_id and jwt:","                self.client.set_jwt(jwt)","                user_info = Account(self.client).get()"]}],"source":"\nfrom django.contrib.auth import authenticate from django.contrib.auth import get_user_model from django.conf import settings from appwrite.client import Client from appwrite.services.users import Users from django.utils.deprecation import MiddlewareMixin User=get_user_model() class AppwriteMiddleware(MiddlewareMixin): def __init__(self, get_response): self.get_response=get_response try: project_endpoint=settings.APPWRITE.get('PROJECT_ENDPOINT') project_id=settings.APPWRITE.get('PROJECT_ID') project_key=settings.APPWRITE.get('PROJECT_API_KEY') self.user_id_header=settings.APPWRITE.get('USER_ID_HEADER', 'HTTP_USER_ID') self.verify_email=settings.APPWRITE.get('VERIFY_EMAIL', False) self.verify_phone=settings.APPWRITE.get('VERIFY_PHONE', False) except AttributeError: raise Exception(\"\"\" Make sure you have the following settings in your Django settings file: APPWRITE={ 'PROJECT_ENDPOINT': 'https:\/\/example.com\/v1', 'PROJECT_ID': 'PROJECT_ID', 'PROJECT_API_KEY': 'PROJECT_API_KEY', 'USER_ID_HEADER': '[USER_ID]', } \"\"\") self.client=(Client() .set_endpoint(project_endpoint) .set_project(project_id) .set_key(project_key)) self.users=Users(self.client) def __call__(self, request, *args, **kwargs): user_id=request.META.get(self.user_id_header, '') user_info=None if user_id: try: user_info=self.users.get(user_id) except Exception as e: return self.get_response(request) if user_info: if self.verify_email and not user_info['emailVerification']: return self.get_response(request) if self.verify_phone and not user_info['phoneVerification']: return self.get_response(request) email=user_info['email'] password=settings.SECRET_KEY+user_id user=User.objects.filter(username=email).first() if not user: user=User.objects.create_user( username=email, password=password, email=email) user=authenticate(request, username=email, password=password) if user: request.user=user response=self.get_response(request) return response ","sourceWithComments":"from django.contrib.auth import authenticate\nfrom django.contrib.auth import get_user_model\nfrom django.conf import settings\nfrom appwrite.client import Client\nfrom appwrite.services.users import Users\nfrom django.utils.deprecation import MiddlewareMixin\n\nUser = get_user_model()\n\n\nclass AppwriteMiddleware(MiddlewareMixin):\n    def __init__(self, get_response):\n        self.get_response = get_response\n\n        # Try to retrieve the required Appwrite settings from the Django settings file\n        try:\n            project_endpoint = settings.APPWRITE.get('PROJECT_ENDPOINT')\n            project_id = settings.APPWRITE.get('PROJECT_ID')\n            project_key = settings.APPWRITE.get('PROJECT_API_KEY')\n            self.user_id_header = settings.APPWRITE.get('USER_ID_HEADER', 'HTTP_USER_ID')\n            self.verify_email = settings.APPWRITE.get('VERIFY_EMAIL', False)\n            self.verify_phone = settings.APPWRITE.get('VERIFY_PHONE', False)\n        except AttributeError:\n            raise Exception(\"\"\"\n                Make sure you have the following settings in your Django settings file:\n                APPWRITE = {\n                    'PROJECT_ENDPOINT': 'https:\/\/example.com\/v1',\n                    'PROJECT_ID': 'PROJECT_ID',\n                    'PROJECT_API_KEY': 'PROJECT_API_KEY',\n                    'USER_ID_HEADER': '[USER_ID]',\n                }\n            \"\"\")\n\n        # Initialize Appwrite client\n        self.client = (Client()\n                       .set_endpoint(project_endpoint)\n                       .set_project(project_id)\n                       .set_key(project_key))\n\n        # Initialize Appwrite Users service\n        self.users = Users(self.client)\n\n    def __call__(self, request, *args, **kwargs):\n        # Get the user ID from the header\n        user_id = request.META.get(self.user_id_header, '')\n\n        user_info = None\n        # If the user ID header is present\n        if user_id:\n            try:\n                # Get the user information from Appwrite\n                user_info = self.users.get(user_id)\n            except Exception as e:\n                # Return the response without doing anything\n                return self.get_response(request)\n\n        # If the user information was retrieved successfully\n        if user_info:\n\n            # If the user has not verified their email, return the response without doing anything\n            if self.verify_email and not user_info['emailVerification']:\n                return self.get_response(request)\n\n            # If the user has not verified their phone, return the response without doing anything\n            if self.verify_phone and not user_info['phoneVerification']:\n                return self.get_response(request)\n\n            email = user_info['email']\n            password = settings.SECRET_KEY+user_id\n            # Get the Django user by its email\n            user = User.objects.filter(username=email).first()\n\n            # If the user doesn't exist, create it\n            if not user:\n                user = User.objects.create_user(\n                    username=email,\n                    password=password,\n                    email=email)\n\n            # Authenticate the user using the email as the username\n            user = authenticate(request, username=email, password=password)\n\n            # If the authentication was successful, log the user in\n            if user:\n                request.user = user\n\n        # Call the next middleware\/view in the pipeline\n        response = self.get_response(request)\n\n        return response\n"}},"msg":"fix(security): Replace user ID with JWT in communication with Appwrite server\n\nThis commit replaces the use of user IDs with JSON Web Tokens (JWT) in communication with the Appwrite server. The previous method of using user IDs carried potential security risks, as it could be subject to tampering or misuse. The new method uses JWT for authorization, which provides a secure and verifiable method of identifying the user. This change enhances the security of the system and makes it easier for developers to ensure that sensitive data is protected. The change is classified as a bug fix, as it repairs a defect in the security of the system."}},"https:\/\/github.com\/emmett-framework\/emmett":{"0f9fb2c652d34d38440294fd7210c30229f574db":{"url":"https:\/\/api.github.com\/repos\/emmett-framework\/emmett\/commits\/0f9fb2c652d34d38440294fd7210c30229f574db","html_url":"https:\/\/github.com\/emmett-framework\/emmett\/commit\/0f9fb2c652d34d38440294fd7210c30229f574db","message":"fix empty validator values tampering","sha":"0f9fb2c652d34d38440294fd7210c30229f574db","keyword":"tampering fix","diff":"diff --git a\/emmett\/validators\/basic.py b\/emmett\/validators\/basic.py\nindex c17160d8..405e8b3c 100644\n--- a\/emmett\/validators\/basic.py\n+++ b\/emmett\/validators\/basic.py\n@@ -118,7 +118,7 @@ def __init__(self, empty_regex=None, message=None):\n         self.empty_regex = re.compile(empty_regex) if empty_regex is not None else None\n \n     def __call__(self, value):\n-        value, empty = is_empty(value, empty_regex=self.empty_regex)\n+        _, empty = is_empty(value, empty_regex=self.empty_regex)\n         if empty:\n             return None, None\n         return value, translate(self.message)\n@@ -128,9 +128,9 @@ class isntEmpty(isEmpty):\n     message = \"Cannot be empty\"\n \n     def __call__(self, value):\n-        value, empty = is_empty(value, empty_regex=self.empty_regex)\n+        newval, empty = is_empty(value, empty_regex=self.empty_regex)\n         if empty:\n-            return value, translate(self.message)\n+            return newval, translate(self.message)\n         return value, None\n \n \ndiff --git a\/emmett\/validators\/helpers.py b\/emmett\/validators\/helpers.py\nindex 48132c8f..1f249ea6 100644\n--- a\/emmett\/validators\/helpers.py\n+++ b\/emmett\/validators\/helpers.py\n@@ -41,11 +41,11 @@ def is_empty(value, empty_regex=None):\n     if value is None:\n         return value, True\n     if isinstance(value, (str, bytes)):\n-        value = value.strip()\n-        if empty_regex is not None and empty_regex.match(value):\n-            value = ''\n-        return value, len(value) == 0\n-    if isinstance(value, list):\n+        vclean = value.strip()\n+        if empty_regex is not None and empty_regex.match(vclean):\n+            vclean = ''\n+        return vclean, len(vclean) == 0\n+    if isinstance(value, (list, dict)):\n         return value, len(value) == 0\n     return value, False\n \n","files":{"\/emmett\/validators\/basic.py":{"changes":[{"diff":"\n         self.empty_regex = re.compile(empty_regex) if empty_regex is not None else None\n \n     def __call__(self, value):\n-        value, empty = is_empty(value, empty_regex=self.empty_regex)\n+        _, empty = is_empty(value, empty_regex=self.empty_regex)\n         if empty:\n             return None, None\n         return value, translate(self.message)\n","add":1,"remove":1,"filename":"\/emmett\/validators\/basic.py","badparts":["        value, empty = is_empty(value, empty_regex=self.empty_regex)"],"goodparts":["        _, empty = is_empty(value, empty_regex=self.empty_regex)"]},{"diff":"\n     message = \"Cannot be empty\"\n \n     def __call__(self, value):\n-        value, empty = is_empty(value, empty_regex=self.empty_regex)\n+        newval, empty = is_empty(value, empty_regex=self.empty_regex)\n         if empty:\n-            return value, translate(self.message)\n+            return newval, translate(self.message)\n         return value, None\n \n ","add":2,"remove":2,"filename":"\/emmett\/validators\/basic.py","badparts":["        value, empty = is_empty(value, empty_regex=self.empty_regex)","            return value, translate(self.message)"],"goodparts":["        newval, empty = is_empty(value, empty_regex=self.empty_regex)","            return newval, translate(self.message)"]}],"source":"\n \"\"\" emmett.validators.basic ----------------------- Provide basic validators. :copyright: 2014 Giovanni Barillari Based on the web2py's validators(http:\/\/www.web2py.com) :copyright:(c) by Massimo Di Pierro <mdipierro@cs.depaul.edu> :license: LGPLv3(http:\/\/www.gnu.org\/licenses\/lgpl.html) \"\"\" import re from cgi import FieldStorage from functools import reduce from os import SEEK_END, SEEK_SET from.._shortcuts import to_unicode from.helpers import translate, is_empty class Validator: message=\"Invalid value\" def __init__(self, message=None): self.message=message or self.message def formatter(self, value): return value def __call__(self, value): raise NotImplementedError class ParentValidator(Validator): def __init__(self, children, message=None): super().__init__(message=message) if not isinstance(children,(list, tuple)): children=[children] self.children=children def formatter(self, value): return reduce( lambda formatted_val, child: child.formatter(formatted_val), self.children, value ) def __call__(self, value): raise NotImplementedError class _is(Validator): message=\"Invalid value\" rule=None def __call__(self, value): if( self.rule is None or( self.rule is not None and self.rule.match(to_unicode(value) or '') ) ): return self.check(value) return value, translate(self.message) def check(self, value): return value, None class Not(ParentValidator): message=\"Value not allowed\" def __call__(self, value): val=value for child in self.children: value, error=child(value) if error is None: return val, translate(self.message) return value, None class Any(ParentValidator): def __call__(self, value): for child in self.children: value, error=child(value) if error is None: break return value, error class Allow(ParentValidator): def __init__(self, value, children, message=None): super().__init__(children, message=message) self.value=value def __call__(self, value): val=value comparing=self.value() if callable(self.value) else self.value if value is not comparing: for child in self.children: value, error=child(value) if error: return val, error return value, None class isEmpty(Validator): message=\"No value allowed\" def __init__(self, empty_regex=None, message=None): super().__init__(message=message) self.empty_regex=re.compile(empty_regex) if empty_regex is not None else None def __call__(self, value): value, empty=is_empty(value, empty_regex=self.empty_regex) if empty: return None, None return value, translate(self.message) class isntEmpty(isEmpty): message=\"Cannot be empty\" def __call__(self, value): value, empty=is_empty(value, empty_regex=self.empty_regex) if empty: return value, translate(self.message) return value, None class isEmptyOr(ParentValidator): def __init__(self, children, empty_regex=None, message=None): super().__init__(children, message=message) self.empty_regex=re.compile(empty_regex) if empty_regex is not None else None for child in self.children: if hasattr(child, 'multiple'): self.multiple=child.multiple break for child in self.children: if hasattr(child, 'options'): self._options_=child.options self.options=self._get_options_ break def _get_options_(self): options=self._options_() if(not options or options[0][0] !='') and not self.multiple: options.insert(0,('', '')) return options def __call__(self, value): value, empty=is_empty(value, empty_regex=self.empty_regex) if empty: return None, None error=None for child in self.children: value, error=child(value) if error: break return value, error class Equals(Validator): message=\"No match\" def __init__(self, expression, message=None): super().__init__(message=message) self.expression=expression def __call__(self, value): if value==self.expression: return(value, None) return value, translate(self.message) class Matches(Validator): message=\"Invalid expression\" def __init__( self, expression, strict=False, search=False, extract=False, message=None ): super().__init__(message=message) if strict or not search: if not expression.startswith('^'): expression='^(%s)' % expression if strict: if not expression.endswith('$'): expression='(%s)$' % expression self.regex=re.compile(expression) self.extract=extract def __call__(self, value): match=self.regex.search(to_unicode(value) or '') if match is not None: return self.extract and match.group() or value, None return value, translate(self.message) class hasLength(Validator): message=\"Enter from{min} to{max} characters\" def __init__( self, maxsize=256, minsize=0, include=(True, False), message=None ): super().__init__(message=message) self.maxsize=maxsize self.minsize=minsize self.inc=include def _between(self, value): if self.inc[0]: great=self.minsize <=value else: great=self.minsize < value if self.inc[1]: less=value <=self.maxsize else: less=value < self.maxsize return great and less def __call__(self, value): if value is None: length=0 if self._between(length): return value, None elif isinstance(value, FieldStorage): if value.file: value.file.seek(0, SEEK_END) length=value.file.tell() value.file.seek(0, SEEK_SET) elif hasattr(value, 'value'): val=value.value if val: length=len(val) else: length=0 if self._between(length): return value, None elif isinstance(value, bytes): try: lvalue=len(value.decode('utf8')) except Exception: lvalue=len(value) if self._between(lvalue): return value, None elif isinstance(value, str): if self._between(len(value)): return value, None elif isinstance(value,(tuple, list)): if self._between(len(value)): return value, None elif self._between(len(str(value))): return str(value), None return value, translate(self.message).format(min=self.minsize, max=self.maxsize) ","sourceWithComments":"# -*- coding: utf-8 -*-\n\"\"\"\n    emmett.validators.basic\n    -----------------------\n\n    Provide basic validators.\n\n    :copyright: 2014 Giovanni Barillari\n\n    Based on the web2py's validators (http:\/\/www.web2py.com)\n    :copyright: (c) by Massimo Di Pierro <mdipierro@cs.depaul.edu>\n\n    :license: LGPLv3 (http:\/\/www.gnu.org\/licenses\/lgpl.html)\n\"\"\"\n\nimport re\n\nfrom cgi import FieldStorage\nfrom functools import reduce\nfrom os import SEEK_END, SEEK_SET\n\n# TODO: check unicode conversions\nfrom .._shortcuts import to_unicode\nfrom .helpers import translate, is_empty\n\n\nclass Validator:\n    message = \"Invalid value\"\n\n    def __init__(self, message=None):\n        self.message = message or self.message\n\n    def formatter(self, value):\n        return value\n\n    def __call__(self, value):\n        raise NotImplementedError\n\n\nclass ParentValidator(Validator):\n    def __init__(self, children, message=None):\n        super().__init__(message=message)\n        if not isinstance(children, (list, tuple)):\n            children = [children]\n        self.children = children\n\n    def formatter(self, value):\n        return reduce(\n            lambda formatted_val, child: child.formatter(formatted_val),\n            self.children,\n            value\n        )\n\n    def __call__(self, value):\n        raise NotImplementedError\n\n\nclass _is(Validator):\n    message = \"Invalid value\"\n    rule = None\n\n    def __call__(self, value):\n        if (\n            self.rule is None or (\n                self.rule is not None and\n                self.rule.match(to_unicode(value) or '')\n            )\n        ):\n            return self.check(value)\n        return value, translate(self.message)\n\n    def check(self, value):\n        return value, None\n\n\nclass Not(ParentValidator):\n    message = \"Value not allowed\"\n\n    def __call__(self, value):\n        val = value\n        for child in self.children:\n            value, error = child(value)\n            if error is None:\n                return val, translate(self.message)\n        return value, None\n\n\nclass Any(ParentValidator):\n    def __call__(self, value):\n        for child in self.children:\n            value, error = child(value)\n            if error is None:\n                break\n        return value, error\n\n\nclass Allow(ParentValidator):\n    def __init__(self, value, children, message=None):\n        super().__init__(children, message=message)\n        self.value = value\n\n    def __call__(self, value):\n        val = value\n        comparing = self.value() if callable(self.value) else self.value\n        if value is not comparing:\n            for child in self.children:\n                value, error = child(value)\n                if error:\n                    return val, error\n        return value, None\n\n\nclass isEmpty(Validator):\n    message = \"No value allowed\"\n\n    def __init__(self, empty_regex=None, message=None):\n        super().__init__(message=message)\n        self.empty_regex = re.compile(empty_regex) if empty_regex is not None else None\n\n    def __call__(self, value):\n        value, empty = is_empty(value, empty_regex=self.empty_regex)\n        if empty:\n            return None, None\n        return value, translate(self.message)\n\n\nclass isntEmpty(isEmpty):\n    message = \"Cannot be empty\"\n\n    def __call__(self, value):\n        value, empty = is_empty(value, empty_regex=self.empty_regex)\n        if empty:\n            return value, translate(self.message)\n        return value, None\n\n\nclass isEmptyOr(ParentValidator):\n    def __init__(self, children, empty_regex=None, message=None):\n        super().__init__(children, message=message)\n        self.empty_regex = re.compile(empty_regex) if empty_regex is not None else None\n        for child in self.children:\n            if hasattr(child, 'multiple'):\n                self.multiple = child.multiple\n                break\n        for child in self.children:\n            if hasattr(child, 'options'):\n                self._options_ = child.options\n                self.options = self._get_options_\n                break\n\n    def _get_options_(self):\n        options = self._options_()\n        if (not options or options[0][0] != '') and not self.multiple:\n            options.insert(0, ('', ''))\n        return options\n\n    def __call__(self, value):\n        value, empty = is_empty(value, empty_regex=self.empty_regex)\n        if empty:\n            return None, None\n        error = None\n        for child in self.children:\n            value, error = child(value)\n            if error:\n                break\n        return value, error\n\n\nclass Equals(Validator):\n    message = \"No match\"\n\n    def __init__(self, expression, message=None):\n        super().__init__(message=message)\n        self.expression = expression\n\n    def __call__(self, value):\n        if value == self.expression:\n            return (value, None)\n        return value, translate(self.message)\n\n\nclass Matches(Validator):\n    message = \"Invalid expression\"\n\n    def __init__(\n        self, expression, strict=False, search=False, extract=False, message=None\n    ):\n        super().__init__(message=message)\n        if strict or not search:\n            if not expression.startswith('^'):\n                expression = '^(%s)' % expression\n        if strict:\n            if not expression.endswith('$'):\n                expression = '(%s)$' % expression\n        self.regex = re.compile(expression)\n        self.extract = extract\n\n    def __call__(self, value):\n        match = self.regex.search(to_unicode(value) or '')\n        if match is not None:\n            return self.extract and match.group() or value, None\n        return value, translate(self.message)\n\n\nclass hasLength(Validator):\n    message = \"Enter from {min} to {max} characters\"\n\n    def __init__(\n        self, maxsize=256, minsize=0, include=(True, False), message=None\n    ):\n        super().__init__(message=message)\n        self.maxsize = maxsize\n        self.minsize = minsize\n        self.inc = include\n\n    def _between(self, value):\n        if self.inc[0]:\n            great = self.minsize <= value\n        else:\n            great = self.minsize < value\n        if self.inc[1]:\n            less = value <= self.maxsize\n        else:\n            less = value < self.maxsize\n        return great and less\n\n    def __call__(self, value):\n        if value is None:\n            length = 0\n            if self._between(length):\n                return value, None\n        elif isinstance(value, FieldStorage):\n            if value.file:\n                value.file.seek(0, SEEK_END)\n                length = value.file.tell()\n                value.file.seek(0, SEEK_SET)\n            elif hasattr(value, 'value'):\n                val = value.value\n                if val:\n                    length = len(val)\n                else:\n                    length = 0\n            if self._between(length):\n                return value, None\n        elif isinstance(value, bytes):\n            try:\n                lvalue = len(value.decode('utf8'))\n            except Exception:\n                lvalue = len(value)\n            if self._between(lvalue):\n                return value, None\n        elif isinstance(value, str):\n            if self._between(len(value)):\n                return value, None\n        elif isinstance(value, (tuple, list)):\n            if self._between(len(value)):\n                return value, None\n        elif self._between(len(str(value))):\n            return str(value), None\n        return value, translate(self.message).format(min=self.minsize, max=self.maxsize)\n"},"\/emmett\/validators\/helpers.py":{"changes":[{"diff":"\n     if value is None:\n         return value, True\n     if isinstance(value, (str, bytes)):\n-        value = value.strip()\n-        if empty_regex is not None and empty_regex.match(value):\n-            value = ''\n-        return value, len(value) == 0\n-    if isinstance(value, list):\n+        vclean = value.strip()\n+        if empty_regex is not None and empty_regex.match(vclean):\n+            vclean = ''\n+        return vclean, len(vclean) == 0\n+    if isinstance(value, (list, dict)):\n         return value, len(value) == 0\n     return value, False\n \n","add":5,"remove":5,"filename":"\/emmett\/validators\/helpers.py","badparts":["        value = value.strip()","        if empty_regex is not None and empty_regex.match(value):","            value = ''","        return value, len(value) == 0","    if isinstance(value, list):"],"goodparts":["        vclean = value.strip()","        if empty_regex is not None and empty_regex.match(vclean):","            vclean = ''","        return vclean, len(vclean) == 0","    if isinstance(value, (list, dict)):"]}],"source":"\n \"\"\" emmett.validators.helpers ------------------------- Provides utilities for validators. Ported from the original validators of web2py(http:\/\/www.web2py.com) :copyright:(c) by Massimo Di Pierro <mdipierro@cs.depaul.edu> :license: LGPLv3(http:\/\/www.gnu.org\/licenses\/lgpl.html) \"\"\" import re from datetime import tzinfo, timedelta from io import StringIO from.._shortcuts import to_unicode from..ctx import current from..security import simple_hash, uuid, DIGEST_ALG_BY_SIZE _DEFAULT=lambda: None def translate(text): if text is None: return None elif isinstance(text, str): return current.T(text) return str(text) def options_sorter(x, y): return(str(x[1]).upper() > str(y[1]).upper() and 1) or -1 def is_empty(value, empty_regex=None): if value is None: return value, True if isinstance(value,(str, bytes)): value=value.strip() if empty_regex is not None and empty_regex.match(value): value='' return value, len(value)==0 if isinstance(value, list): return value, len(value)==0 return value, False class _UTC(tzinfo): ZERO=timedelta(0) def utcoffset(self, dt): return _UTC.ZERO def tzname(self, dt): return \"UTC\" def dst(self, dt): return _UTC.ZERO class LazyCrypt(object): def __init__(self, crypt, password): \"\"\" crypt is an instance of the Crypt validator, password is the password as inserted by the user \"\"\" self.crypt=crypt self.password=password self.crypted=None def __str__(self): \"\"\" Encrypted self.password and caches it in self.crypted. If self.crypt.salt the output is in the format <algorithm>$<salt>$<hash> Try get the digest_alg from the key(if it exists) else assume the default digest_alg. If not key at all, set key='' If a salt is specified use it, if salt is True, set salt to uuid (this should all be backward compatible) Options: key='uuid' key='md5:uuid' key='sha512:uuid' ... key='pbkdf2(1000,64,sha512):uuid' 1000 iterations and 64 chars length \"\"\" if self.crypted: return self.crypted if self.crypt.key: if ':' in self.crypt.key: digest_alg, key=self.crypt.key.split(':', 1) else: digest_alg, key=self.crypt.digest_alg, self.crypt.key else: digest_alg, key=self.crypt.digest_alg, '' if self.crypt.salt: if self.crypt.salt==True: salt=str(uuid()).replace('-', '')[-16:] else: salt=self.crypt.salt else: salt='' hashed=simple_hash(self.password, key, salt, digest_alg) self.crypted='%s$%s$%s' %(digest_alg, salt, hashed) return self.crypted def __eq__(self, stored_password): \"\"\" compares the current lazy crypted password with a stored password \"\"\" if isinstance(stored_password, self.__class__): return((self is stored_password) or ((self.crypt.key==stored_password.crypt.key) and (self.password==stored_password.password))) if self.crypt.key: if ':' in self.crypt.key: key=self.crypt.key.split(':')[1] else: key=self.crypt.key else: key='' if stored_password is None: return False elif stored_password.count('$')==2: (digest_alg, salt, hash)=stored_password.split('$') h=simple_hash(self.password, key, salt, digest_alg) temp_pass='%s$%s$%s' %(digest_alg, salt, h) else: digest_alg=DIGEST_ALG_BY_SIZE.get(len(stored_password), None) if not digest_alg: return False else: temp_pass=simple_hash(self.password, key, '', digest_alg) return temp_pass==stored_password def __ne__(self, other): return not self.__eq__(other) def _escape_unicode(string): ''' Converts a unicode string into US-ASCII, using a simple conversion scheme. Each unicode character that does not have a US-ASCII equivalent is converted into a URL escaped form based on its hexadecimal value. For example, the unicode character '\\u4e86' will become the string '%4e%86' Args: string: unicode string, the unicode string to convert into an escaped US-ASCII form Returns: string: the US-ASCII escaped form of the inputted string @author: Jonathan Benn ''' returnValue=StringIO() for character in string: code=ord(character) if code > 0x7F: hexCode=hex(code) returnValue.write('%' +hexCode[2:4] +'%' +hexCode[4:6]) else: returnValue.write(character) return returnValue.getvalue() def _unicode_to_ascii_authority(authority): ''' Follows the steps in RFC 3490, Section 4 to convert a unicode authority string into its ASCII equivalent. For example, u'www.Alliancefran\\xe7aise.nu' will be converted into 'www.xn--alliancefranaise-npb.nu' Args: authority: unicode string, the URL authority component to convert, e.g. u'www.Alliancefran\\xe7aise.nu' Returns: string: the US-ASCII character equivalent to the inputed authority, e.g. 'www.xn--alliancefranaise-npb.nu' Raises: Exception: if the function is not able to convert the inputed authority @author: Jonathan Benn ''' label_split_regex=re.compile(u'[\\u002e\\u3002\\uff0e\\uff61]') labels=label_split_regex.split(authority) asciiLabels=[] try: import encodings.idna for label in labels: if label: asciiLabels.append(encodings.idna.ToASCII(label)) else: asciiLabels.append('') except: asciiLabels=[str(label) for label in labels] return str(reduce(lambda x, y: x +unichr(0x002E) +y, asciiLabels)) def unicode_to_ascii_url(url, prepend_scheme): ''' Converts the inputed unicode url into a US-ASCII equivalent. This function goes a little beyond RFC 3490, which is limited in scope to the domain name (authority) only. Here, the functionality is expanded to what was observed on Wikipedia on 2009-Jan-22: Component Can Use Unicode? --------- ---------------- scheme No authority Yes path Yes query Yes fragment No The authority component gets converted to punycode, but occurrences of unicode in other components get converted into a pair of URI escapes(we assume 4-byte unicode). E.g. the unicode character U+4E2D will be converted into '%4E%2D'. Testing with Firefox v3.0.5 has shown that it can understand this kind of URI encoding. Args: url: unicode string, the URL to convert from unicode into US-ASCII prepend_scheme: string, a protocol scheme to prepend to the URL if we're having trouble parsing it. e.g. \"http\". Input None to disable this functionality Returns: string: a US-ASCII equivalent of the inputed url @author: Jonathan Benn ''' groups=url_split_regex.match(url).groups() if not groups[3]: scheme_to_prepend=prepend_scheme or 'http' groups=url_split_regex.match( to_unicode(scheme_to_prepend) +u':\/\/' +url).groups() if not groups[3]: raise Exception('No authority component found, ' + 'could not decode unicode to US-ASCII') scheme=groups[1] authority=groups[3] path=groups[4] or '' query=groups[5] or '' fragment=groups[7] or '' if prepend_scheme: scheme=str(scheme) +':\/\/' else: scheme='' return scheme +_unicode_to_ascii_authority(authority) +\\ _escape_unicode(path) +_escape_unicode(query) +str(fragment) url_split_regex=\\ re.compile(r'^(([^:\/? official_url_schemes=[ 'aaa', 'aaas', 'acap', 'cap', 'cid', 'crid', 'data', 'dav', 'dict', 'dns', 'fax', 'file', 'ftp', 'go', 'gopher', 'h323', 'http', 'https', 'icap', 'im', 'imap', 'info', 'ipp', 'iris', 'iris.beep', 'iris.xpc', 'iris.xpcs', 'iris.lws', 'ldap', 'mailto', 'mid', 'modem', 'msrp', 'msrps', 'mtqp', 'mupdate', 'news', 'nfs', 'nntp', 'opaquelocktoken', 'pop', 'pres', 'prospero', 'rtsp', 'service', 'shttp', 'sip', 'sips', 'snmp', 'soap.beep', 'soap.beeps', 'tag', 'tel', 'telnet', 'tftp', 'thismessage', 'tip', 'tv', 'urn', 'vemmi', 'wais', 'xmlrpc.beep', 'xmlrpc.beep', 'xmpp', 'z39.50r', 'z39.50s' ] unofficial_url_schemes=[ 'about', 'adiumxtra', 'aim', 'afp', 'aw', 'callto', 'chrome', 'cvs', 'ed2k', 'feed', 'fish', 'gg', 'gizmoproject', 'iax2', 'irc', 'ircs', 'itms', 'jar', 'javascript', 'keyparc', 'lastfm', 'ldaps', 'magnet', 'mms', 'msnim', 'mvn', 'notes', 'nsfw', 'psyc', 'paparazzi:http', 'rmi', 'rsync', 'secondlife', 'sgn', 'skype', 'ssh', 'sftp', 'smb', 'sms', 'soldat', 'steam', 'svn', 'teamspeak', 'unreal', 'ut2004', 'ventrilo', 'view-source', 'webcal', 'wyciwyg', 'xfire', 'xri', 'ymsgr' ] official_top_level_domains=[ 'abogado', 'ac', 'academy', 'accountants', 'active', 'actor', 'ad', 'adult', 'ae', 'aero', 'af', 'ag', 'agency', 'ai', 'airforce', 'al', 'allfinanz', 'alsace', 'am', 'amsterdam', 'an', 'android', 'ao', 'apartments', 'aq', 'aquarelle', 'ar', 'archi', 'army', 'arpa', 'as', 'asia', 'associates', 'at', 'attorney', 'au', 'auction', 'audio', 'autos', 'aw', 'ax', 'axa', 'az', 'ba', 'band', 'bank', 'bar', 'barclaycard', 'barclays', 'bargains', 'bayern', 'bb', 'bd', 'be', 'beer', 'berlin', 'best', 'bf', 'bg', 'bh', 'bi', 'bid', 'bike', 'bingo', 'bio', 'biz', 'bj', 'black', 'blackfriday', 'bloomberg', 'blue', 'bm', 'bmw', 'bn', 'bnpparibas', 'bo', 'boo', 'boutique', 'br', 'brussels', 'bs', 'bt', 'budapest', 'build', 'builders', 'business', 'buzz', 'bv', 'bw', 'by', 'bz', 'bzh', 'ca', 'cab', 'cal', 'camera', 'camp', 'cancerresearch', 'canon', 'capetown', 'capital', 'caravan', 'cards', 'care', 'career', 'careers', 'cartier', 'casa', 'cash', 'casino', 'cat', 'catering', 'cbn', 'cc', 'cd', 'center', 'ceo', 'cern', 'cf', 'cg', 'ch', 'channel', 'chat', 'cheap', 'christmas', 'chrome', 'church', 'ci', 'citic', 'city', 'ck', 'cl', 'claims', 'cleaning', 'click', 'clinic', 'clothing', 'club', 'cm', 'cn', 'co', 'coach', 'codes', 'coffee', 'college', 'cologne', 'com', 'community', 'company', 'computer', 'condos', 'construction', 'consulting', 'contractors', 'cooking', 'cool', 'coop', 'country', 'cr', 'credit', 'creditcard', 'cricket', 'crs', 'cruises', 'cu', 'cuisinella', 'cv', 'cw', 'cx', 'cy', 'cymru', 'cz', 'dabur', 'dad', 'dance', 'dating', 'day', 'dclk', 'de', 'deals', 'degree', 'delivery', 'democrat', 'dental', 'dentist', 'desi', 'design', 'dev', 'diamonds', 'diet', 'digital', 'direct', 'directory', 'discount', 'dj', 'dk', 'dm', 'dnp', 'do', 'docs', 'domains', 'doosan', 'durban', 'dvag', 'dz', 'eat', 'ec', 'edu', 'education', 'ee', 'eg', 'email', 'emerck', 'energy', 'engineer', 'engineering', 'enterprises', 'equipment', 'er', 'es', 'esq', 'estate', 'et', 'eu', 'eurovision', 'eus', 'events', 'everbank', 'exchange', 'expert', 'exposed', 'fail', 'fans', 'farm', 'fashion', 'feedback', 'fi', 'finance', 'financial', 'firmdale', 'fish', 'fishing', 'fit', 'fitness', 'fj', 'fk', 'flights', 'florist', 'flowers', 'flsmidth', 'fly', 'fm', 'fo', 'foo', 'football', 'forsale', 'foundation', 'fr', 'frl', 'frogans', 'fund', 'furniture', 'futbol', 'ga', 'gal', 'gallery', 'garden', 'gb', 'gbiz', 'gd', 'gdn', 'ge', 'gent', 'gf', 'gg', 'ggee', 'gh', 'gi', 'gift', 'gifts', 'gives', 'gl', 'glass', 'gle', 'global', 'globo', 'gm', 'gmail', 'gmo', 'gmx', 'gn', 'goldpoint', 'goog', 'google', 'gop', 'gov', 'gp', 'gq', 'gr', 'graphics', 'gratis', 'green', 'gripe', 'gs', 'gt', 'gu', 'guide', 'guitars', 'guru', 'gw', 'gy', 'hamburg', 'hangout', 'haus', 'healthcare', 'help', 'here', 'hermes', 'hiphop', 'hiv', 'hk', 'hm', 'hn', 'holdings', 'holiday', 'homes', 'horse', 'host', 'hosting', 'house', 'how', 'hr', 'ht', 'hu', 'ibm', 'id', 'ie', 'ifm', 'il', 'im', 'immo', 'immobilien', 'in', 'industries', 'info', 'ing', 'ink', 'institute', 'insure', 'int', 'international', 'investments', 'io', 'iq', 'ir', 'irish', 'is', 'it', 'iwc', 'jcb', 'je', 'jetzt', 'jm', 'jo', 'jobs', 'joburg', 'jp', 'juegos', 'kaufen', 'kddi', 'ke', 'kg', 'kh', 'ki', 'kim', 'kitchen', 'kiwi', 'km', 'kn', 'koeln', 'kp', 'kr', 'krd', 'kred', 'kw', 'ky', 'kyoto', 'kz', 'la', 'lacaixa', 'land', 'lat', 'latrobe', 'lawyer', 'lb', 'lc', 'lds', 'lease', 'legal', 'lgbt', 'li', 'lidl', 'life', 'lighting', 'limited', 'limo', 'link', 'lk', 'loans', 'localhost', 'london', 'lotte', 'lotto', 'lr', 'ls', 'lt', 'ltda', 'lu', 'luxe', 'luxury', 'lv', 'ly', 'ma', 'madrid', 'maison', 'management', 'mango', 'market', 'marketing', 'marriott', 'mc', 'md', 'me', 'media', 'meet', 'melbourne', 'meme', 'memorial', 'menu', 'mg', 'mh', 'miami', 'mil', 'mini', 'mk', 'ml', 'mm', 'mn', 'mo', 'mobi', 'moda', 'moe', 'monash', 'money', 'mormon', 'mortgage', 'moscow', 'motorcycles', 'mov', 'mp', 'mq', 'mr', 'ms', 'mt', 'mu', 'museum', 'mv', 'mw', 'mx', 'my', 'mz', 'na', 'nagoya', 'name', 'navy', 'nc', 'ne', 'net', 'network', 'neustar', 'new', 'nexus', 'nf', 'ng', 'ngo', 'nhk', 'ni', 'nico', 'ninja', 'nl', 'no', 'np', 'nr', 'nra', 'nrw', 'ntt', 'nu', 'nyc', 'nz', 'okinawa', 'om', 'one', 'ong', 'onl', 'ooo', 'org', 'organic', 'osaka', 'otsuka', 'ovh', 'pa', 'paris', 'partners', 'parts', 'party', 'pe', 'pf', 'pg', 'ph', 'pharmacy', 'photo', 'photography', 'photos', 'physio', 'pics', 'pictures', 'pink', 'pizza', 'pk', 'pl', 'place', 'plumbing', 'pm', 'pn', 'pohl', 'poker', 'porn', 'post', 'pr', 'praxi', 'press', 'pro', 'prod', 'productions', 'prof', 'properties', 'property', 'ps', 'pt', 'pub', 'pw', 'py', 'qa', 'qpon', 'quebec', 're', 'realtor', 'recipes', 'red', 'rehab', 'reise', 'reisen', 'reit', 'ren', 'rentals', 'repair', 'report', 'republican', 'rest', 'restaurant', 'reviews', 'rich', 'rio', 'rip', 'ro', 'rocks', 'rodeo', 'rs', 'rsvp', 'ru', 'ruhr', 'rw', 'ryukyu', 'sa', 'saarland', 'sale', 'samsung', 'sarl', 'saxo', 'sb', 'sc', 'sca', 'scb', 'schmidt', 'school', 'schule', 'schwarz', 'science', 'scot', 'sd', 'se', 'services', 'sew', 'sexy', 'sg', 'sh', 'shiksha', 'shoes', 'shriram', 'si', 'singles', 'sj', 'sk', 'sky', 'sl', 'sm', 'sn', 'so', 'social', 'software', 'sohu', 'solar', 'solutions', 'soy', 'space', 'spiegel', 'sr', 'st', 'style', 'su', 'supplies', 'supply', 'support', 'surf', 'surgery', 'suzuki', 'sv', 'sx', 'sy', 'sydney', 'systems', 'sz', 'taipei', 'tatar', 'tattoo', 'tax', 'tc', 'td', 'technology', 'tel', 'temasek', 'tennis', 'tf', 'tg', 'th', 'tienda', 'tips', 'tires', 'tirol', 'tj', 'tk', 'tl', 'tm', 'tn', 'to', 'today', 'tokyo', 'tools', 'top', 'toshiba', 'town', 'toys', 'tp', 'tr', 'trade', 'training', 'travel', 'trust', 'tt', 'tui', 'tv', 'tw', 'tz', 'ua', 'ug', 'uk', 'university', 'uno', 'uol', 'us', 'uy', 'uz', 'va', 'vacations', 'vc', 've', 'vegas', 'ventures', 'versicherung', 'vet', 'vg', 'vi', 'viajes', 'video', 'villas', 'vision', 'vlaanderen', 'vn', 'vodka', 'vote', 'voting', 'voto', 'voyage', 'vu', 'wales', 'wang', 'watch', 'webcam', 'website', 'wed', 'wedding', 'wf', 'whoswho', 'wien', 'wiki', 'williamhill', 'wme', 'work', 'works', 'world', 'ws', 'wtc', 'wtf', 'xn--1qqw23a', 'xn--3bst00m', 'xn--3ds443g', 'xn--3e0b707e', 'xn--45brj9c', 'xn--45q11c', 'xn--4gbrim', 'xn--55qw42g', 'xn--55qx5d', 'xn--6frz82g', 'xn--6qq986b3xl', 'xn--80adxhks', 'xn--80ao21a', 'xn--80asehdb', 'xn--80aswg', 'xn--90a3ac', 'xn--90ais', 'xn--b4w605ferd', 'xn--c1avg', 'xn--cg4bki', 'xn--clchc0ea0b2g2a9gcd', 'xn--czr694b', 'xn--czrs0t', 'xn--czru2d', 'xn--d1acj3b', 'xn--d1alf', 'xn--fiq228c5hs', 'xn--fiq64b', 'xn--fiqs8s', 'xn--fiqz9s', 'xn--flw351e', 'xn--fpcrj9c3d', 'xn--fzc2c9e2c', 'xn--gecrj9c', 'xn--h2brj9c', 'xn--hxt814e', 'xn--i1b6b1a6a2e', 'xn--io0a7i', 'xn--j1amh', 'xn--j6w193g', 'xn--kprw13d', 'xn--kpry57d', 'xn--kput3i', 'xn--l1acc', 'xn--lgbbat1ad8j', 'xn--mgb9awbf', 'xn--mgba3a4f16a', 'xn--mgbaam7a8h', 'xn--mgbab2bd', 'xn--mgbayh7gpa', 'xn--mgbbh1a71e', 'xn--mgbc0a9azcg', 'xn--mgberp4a5d4ar', 'xn--mgbx4cd0ab', 'xn--ngbc5azd', 'xn--node', 'xn--nqv7f', 'xn--nqv7fs00ema', 'xn--o3cw4h', 'xn--ogbpf8fl', 'xn--p1acf', 'xn--p1ai', 'xn--pgbs0dh', 'xn--q9jyb4c', 'xn--qcka1pmc', 'xn--rhqv96g', 'xn--s9brj9c', 'xn--ses554g', 'xn--unup4y', 'xn--vermgensberater-ctb', 'xn--vermgensberatung-pwb', 'xn--vhquv', 'xn--wgbh1c', 'xn--wgbl6a', 'xn--xhq521b', 'xn--xkc2al3hye2a', 'xn--xkc2dl3a5ee0h', 'xn--yfro4i67o', 'xn--ygbi2ammx', 'xn--zfr164b', 'xxx', 'xyz', 'yachts', 'yandex', 'ye', 'yodobashi', 'yoga', 'yokohama', 'youtube', 'yt', 'za', 'zip', 'zm', 'zone', 'zuerich', 'zw' ] ","sourceWithComments":"# -*- coding: utf-8 -*-\n\"\"\"\n    emmett.validators.helpers\n    -------------------------\n\n    Provides utilities for validators.\n\n    Ported from the original validators of web2py (http:\/\/www.web2py.com)\n\n    :copyright: (c) by Massimo Di Pierro <mdipierro@cs.depaul.edu>\n    :license: LGPLv3 (http:\/\/www.gnu.org\/licenses\/lgpl.html)\n\"\"\"\n\nimport re\n\nfrom datetime import tzinfo, timedelta\nfrom io import StringIO\n\n# TODO: check unicode conversions\nfrom .._shortcuts import to_unicode\nfrom ..ctx import current\nfrom ..security import simple_hash, uuid, DIGEST_ALG_BY_SIZE\n\n\n_DEFAULT = lambda: None\n\n\ndef translate(text):\n    if text is None:\n        return None\n    elif isinstance(text, str):\n        return current.T(text)\n    return str(text)\n\n\ndef options_sorter(x, y):\n    return (str(x[1]).upper() > str(y[1]).upper() and 1) or -1\n\n\ndef is_empty(value, empty_regex=None):\n    if value is None:\n        return value, True\n    if isinstance(value, (str, bytes)):\n        value = value.strip()\n        if empty_regex is not None and empty_regex.match(value):\n            value = ''\n        return value, len(value) == 0\n    if isinstance(value, list):\n        return value, len(value) == 0\n    return value, False\n\n\nclass _UTC(tzinfo):\n    ZERO = timedelta(0)\n\n    def utcoffset(self, dt):\n        return _UTC.ZERO\n\n    def tzname(self, dt):\n        return \"UTC\"\n\n    def dst(self, dt):\n        return _UTC.ZERO\n\n\nclass LazyCrypt(object):\n    def __init__(self, crypt, password):\n        \"\"\"\n        crypt is an instance of the Crypt validator,\n        password is the password as inserted by the user\n        \"\"\"\n        self.crypt = crypt\n        self.password = password\n        self.crypted = None\n\n    def __str__(self):\n        \"\"\"\n        Encrypted self.password and caches it in self.crypted.\n        If self.crypt.salt the output is in the format\n        <algorithm>$<salt>$<hash>\n\n        Try get the digest_alg from the key (if it exists)\n        else assume the default digest_alg. If not key at all, set key=''\n\n        If a salt is specified use it, if salt is True, set salt to uuid\n        (this should all be backward compatible)\n\n        Options:\n        key = 'uuid'\n        key = 'md5:uuid'\n        key = 'sha512:uuid'\n        ...\n        key = 'pbkdf2(1000,64,sha512):uuid' 1000 iterations and 64 chars length\n        \"\"\"\n        if self.crypted:\n            return self.crypted\n        if self.crypt.key:\n            if ':' in self.crypt.key:\n                digest_alg, key = self.crypt.key.split(':', 1)\n            else:\n                digest_alg, key = self.crypt.digest_alg, self.crypt.key\n        else:\n            digest_alg, key = self.crypt.digest_alg, ''\n        if self.crypt.salt:\n            if self.crypt.salt == True:\n                salt = str(uuid()).replace('-', '')[-16:]\n            else:\n                salt = self.crypt.salt\n        else:\n            salt = ''\n        hashed = simple_hash(self.password, key, salt, digest_alg)\n        self.crypted = '%s$%s$%s' % (digest_alg, salt, hashed)\n        return self.crypted\n\n    def __eq__(self, stored_password):\n        \"\"\"\n        compares the current lazy crypted password with a stored password\n        \"\"\"\n        if isinstance(stored_password, self.__class__):\n            return ((self is stored_password) or\n                    ((self.crypt.key == stored_password.crypt.key) and\n                    (self.password == stored_password.password)))\n\n        if self.crypt.key:\n            if ':' in self.crypt.key:\n                key = self.crypt.key.split(':')[1]\n            else:\n                key = self.crypt.key\n        else:\n            key = ''\n        if stored_password is None:\n            return False\n        elif stored_password.count('$') == 2:\n            (digest_alg, salt, hash) = stored_password.split('$')\n            h = simple_hash(self.password, key, salt, digest_alg)\n            temp_pass = '%s$%s$%s' % (digest_alg, salt, h)\n        else:  # no salting\n            # guess digest_alg\n            digest_alg = DIGEST_ALG_BY_SIZE.get(len(stored_password), None)\n            if not digest_alg:\n                return False\n            else:\n                temp_pass = simple_hash(self.password, key, '', digest_alg)\n        return temp_pass == stored_password\n\n    def __ne__(self, other):\n        return not self.__eq__(other)\n\n\ndef _escape_unicode(string):\n    '''\n    Converts a unicode string into US-ASCII, using a simple conversion scheme.\n    Each unicode character that does not have a US-ASCII equivalent is\n    converted into a URL escaped form based on its hexadecimal value.\n    For example, the unicode character '\\u4e86' will become the string '%4e%86'\n\n    Args:\n        string: unicode string, the unicode string to convert into an\n            escaped US-ASCII form\n\n    Returns:\n        string: the US-ASCII escaped form of the inputted string\n\n    @author: Jonathan Benn\n    '''\n    returnValue = StringIO()\n\n    for character in string:\n        code = ord(character)\n        if code > 0x7F:\n            hexCode = hex(code)\n            returnValue.write('%' + hexCode[2:4] + '%' + hexCode[4:6])\n        else:\n            returnValue.write(character)\n\n    return returnValue.getvalue()\n\n\ndef _unicode_to_ascii_authority(authority):\n    '''\n    Follows the steps in RFC 3490, Section 4 to convert a unicode authority\n    string into its ASCII equivalent.\n    For example, u'www.Alliancefran\\xe7aise.nu' will be converted into\n    'www.xn--alliancefranaise-npb.nu'\n\n    Args:\n        authority: unicode string, the URL authority component to convert,\n            e.g. u'www.Alliancefran\\xe7aise.nu'\n\n    Returns:\n        string: the US-ASCII character equivalent to the inputed authority,\n             e.g. 'www.xn--alliancefranaise-npb.nu'\n\n    Raises:\n        Exception: if the function is not able to convert the inputed\n            authority\n\n    @author: Jonathan Benn\n    '''\n    label_split_regex = re.compile(u'[\\u002e\\u3002\\uff0e\\uff61]')\n\n    #RFC 3490, Section 4, Step 1\n    #The encodings.idna Python module assumes that AllowUnassigned == True\n\n    #RFC 3490, Section 4, Step 2\n    labels = label_split_regex.split(authority)\n\n    #RFC 3490, Section 4, Step 3\n    #The encodings.idna Python module assumes that UseSTD3ASCIIRules == False\n\n    #RFC 3490, Section 4, Step 4\n    #We use the ToASCII operation because we are about to put the authority\n    #into an IDN-unaware slot\n    asciiLabels = []\n    try:\n        import encodings.idna\n        for label in labels:\n            if label:\n                asciiLabels.append(encodings.idna.ToASCII(label))\n            else:\n                #encodings.idna.ToASCII does not accept an empty string, but\n                #it is necessary for us to allow for empty labels so that we\n                #don't modify the URL\n                asciiLabels.append('')\n    except:\n        asciiLabels = [str(label) for label in labels]\n    #RFC 3490, Section 4, Step 5\n    return str(reduce(lambda x, y: x + unichr(0x002E) + y, asciiLabels))\n\n\ndef unicode_to_ascii_url(url, prepend_scheme):\n    '''\n    Converts the inputed unicode url into a US-ASCII equivalent. This function\n    goes a little beyond RFC 3490, which is limited in scope to the domain name\n    (authority) only. Here, the functionality is expanded to what was observed\n    on Wikipedia on 2009-Jan-22:\n\n       Component    Can Use Unicode?\n       ---------    ----------------\n       scheme       No\n       authority    Yes\n       path         Yes\n       query        Yes\n       fragment     No\n\n    The authority component gets converted to punycode, but occurrences of\n    unicode in other components get converted into a pair of URI escapes (we\n    assume 4-byte unicode). E.g. the unicode character U+4E2D will be\n    converted into '%4E%2D'. Testing with Firefox v3.0.5 has shown that it can\n    understand this kind of URI encoding.\n\n    Args:\n        url: unicode string, the URL to convert from unicode into US-ASCII\n        prepend_scheme: string, a protocol scheme to prepend to the URL if\n            we're having trouble parsing it.\n            e.g. \"http\". Input None to disable this functionality\n\n    Returns:\n        string: a US-ASCII equivalent of the inputed url\n\n    @author: Jonathan Benn\n    '''\n    #convert the authority component of the URL into an ASCII punycode string,\n    #but encode the rest using the regular URI character encoding\n\n    groups = url_split_regex.match(url).groups()\n    #If no authority was found\n    if not groups[3]:\n        #Try appending a scheme to see if that fixes the problem\n        scheme_to_prepend = prepend_scheme or 'http'\n        groups = url_split_regex.match(\n            to_unicode(scheme_to_prepend) + u':\/\/' + url).groups()\n    #if we still can't find the authority\n    if not groups[3]:\n        raise Exception('No authority component found, ' +\n                        'could not decode unicode to US-ASCII')\n\n    #We're here if we found an authority, let's rebuild the URL\n    scheme = groups[1]\n    authority = groups[3]\n    path = groups[4] or ''\n    query = groups[5] or ''\n    fragment = groups[7] or ''\n\n    if prepend_scheme:\n        scheme = str(scheme) + ':\/\/'\n    else:\n        scheme = ''\n    return scheme + _unicode_to_ascii_authority(authority) +\\\n        _escape_unicode(path) + _escape_unicode(query) + str(fragment)\n\n\nurl_split_regex = \\\n    re.compile(r'^(([^:\/?#]+):)?(\/\/([^\/?#]*))?([^?#]*)(\\?([^#]*))?(#(.*))?')\n\nofficial_url_schemes = [\n    'aaa', 'aaas', 'acap', 'cap', 'cid', 'crid', 'data', 'dav', 'dict',\n    'dns', 'fax', 'file', 'ftp', 'go', 'gopher', 'h323', 'http', 'https',\n    'icap', 'im', 'imap', 'info', 'ipp', 'iris', 'iris.beep', 'iris.xpc',\n    'iris.xpcs', 'iris.lws', 'ldap', 'mailto', 'mid', 'modem', 'msrp',\n    'msrps', 'mtqp', 'mupdate', 'news', 'nfs', 'nntp', 'opaquelocktoken',\n    'pop', 'pres', 'prospero', 'rtsp', 'service', 'shttp', 'sip', 'sips',\n    'snmp', 'soap.beep', 'soap.beeps', 'tag', 'tel', 'telnet', 'tftp',\n    'thismessage', 'tip', 'tv', 'urn', 'vemmi', 'wais', 'xmlrpc.beep',\n    'xmlrpc.beep', 'xmpp', 'z39.50r', 'z39.50s'\n]\n\nunofficial_url_schemes = [\n    'about', 'adiumxtra', 'aim', 'afp', 'aw', 'callto', 'chrome', 'cvs',\n    'ed2k', 'feed', 'fish', 'gg', 'gizmoproject', 'iax2', 'irc', 'ircs',\n    'itms', 'jar', 'javascript', 'keyparc', 'lastfm', 'ldaps', 'magnet',\n    'mms', 'msnim', 'mvn', 'notes', 'nsfw', 'psyc', 'paparazzi:http',\n    'rmi', 'rsync', 'secondlife', 'sgn', 'skype', 'ssh', 'sftp', 'smb',\n    'sms', 'soldat', 'steam', 'svn', 'teamspeak', 'unreal', 'ut2004',\n    'ventrilo', 'view-source', 'webcal', 'wyciwyg', 'xfire', 'xri', 'ymsgr'\n]\n\nofficial_top_level_domains = [\n    # a\n    'abogado', 'ac', 'academy', 'accountants', 'active', 'actor',\n    'ad', 'adult', 'ae', 'aero', 'af', 'ag', 'agency', 'ai',\n    'airforce', 'al', 'allfinanz', 'alsace', 'am', 'amsterdam', 'an',\n    'android', 'ao', 'apartments', 'aq', 'aquarelle', 'ar', 'archi',\n    'army', 'arpa', 'as', 'asia', 'associates', 'at', 'attorney',\n    'au', 'auction', 'audio', 'autos', 'aw', 'ax', 'axa', 'az',\n    # b\n    'ba', 'band', 'bank', 'bar', 'barclaycard', 'barclays',\n    'bargains', 'bayern', 'bb', 'bd', 'be', 'beer', 'berlin', 'best',\n    'bf', 'bg', 'bh', 'bi', 'bid', 'bike', 'bingo', 'bio', 'biz',\n    'bj', 'black', 'blackfriday', 'bloomberg', 'blue', 'bm', 'bmw',\n    'bn', 'bnpparibas', 'bo', 'boo', 'boutique', 'br', 'brussels',\n    'bs', 'bt', 'budapest', 'build', 'builders', 'business', 'buzz',\n    'bv', 'bw', 'by', 'bz', 'bzh',\n    # c\n    'ca', 'cab', 'cal', 'camera', 'camp', 'cancerresearch', 'canon',\n    'capetown', 'capital', 'caravan', 'cards', 'care', 'career',\n    'careers', 'cartier', 'casa', 'cash', 'casino', 'cat',\n    'catering', 'cbn', 'cc', 'cd', 'center', 'ceo', 'cern', 'cf',\n    'cg', 'ch', 'channel', 'chat', 'cheap', 'christmas', 'chrome',\n    'church', 'ci', 'citic', 'city', 'ck', 'cl', 'claims',\n    'cleaning', 'click', 'clinic', 'clothing', 'club', 'cm', 'cn',\n    'co', 'coach', 'codes', 'coffee', 'college', 'cologne', 'com',\n    'community', 'company', 'computer', 'condos', 'construction',\n    'consulting', 'contractors', 'cooking', 'cool', 'coop',\n    'country', 'cr', 'credit', 'creditcard', 'cricket', 'crs',\n    'cruises', 'cu', 'cuisinella', 'cv', 'cw', 'cx', 'cy', 'cymru',\n    'cz',\n    # d\n    'dabur', 'dad', 'dance', 'dating', 'day', 'dclk', 'de', 'deals',\n    'degree', 'delivery', 'democrat', 'dental', 'dentist', 'desi',\n    'design', 'dev', 'diamonds', 'diet', 'digital', 'direct',\n    'directory', 'discount', 'dj', 'dk', 'dm', 'dnp', 'do', 'docs',\n    'domains', 'doosan', 'durban', 'dvag', 'dz',\n    # e\n    'eat', 'ec', 'edu', 'education', 'ee', 'eg', 'email', 'emerck',\n    'energy', 'engineer', 'engineering', 'enterprises', 'equipment',\n    'er', 'es', 'esq', 'estate', 'et', 'eu', 'eurovision', 'eus',\n    'events', 'everbank', 'exchange', 'expert', 'exposed',\n    # f\n    'fail', 'fans', 'farm', 'fashion', 'feedback', 'fi', 'finance',\n    'financial', 'firmdale', 'fish', 'fishing', 'fit', 'fitness',\n    'fj', 'fk', 'flights', 'florist', 'flowers', 'flsmidth', 'fly',\n    'fm', 'fo', 'foo', 'football', 'forsale', 'foundation', 'fr',\n    'frl', 'frogans', 'fund', 'furniture', 'futbol',\n    # g\n    'ga', 'gal', 'gallery', 'garden', 'gb', 'gbiz', 'gd', 'gdn',\n    'ge', 'gent', 'gf', 'gg', 'ggee', 'gh', 'gi', 'gift', 'gifts',\n    'gives', 'gl', 'glass', 'gle', 'global', 'globo', 'gm', 'gmail',\n    'gmo', 'gmx', 'gn', 'goldpoint', 'goog', 'google', 'gop', 'gov',\n    'gp', 'gq', 'gr', 'graphics', 'gratis', 'green', 'gripe', 'gs',\n    'gt', 'gu', 'guide', 'guitars', 'guru', 'gw', 'gy',\n    # h\n    'hamburg', 'hangout', 'haus', 'healthcare', 'help', 'here',\n    'hermes', 'hiphop', 'hiv', 'hk', 'hm', 'hn', 'holdings',\n    'holiday', 'homes', 'horse', 'host', 'hosting', 'house', 'how',\n    'hr', 'ht', 'hu',\n    # i\n    'ibm', 'id', 'ie', 'ifm', 'il', 'im', 'immo', 'immobilien', 'in',\n    'industries', 'info', 'ing', 'ink', 'institute', 'insure', 'int',\n    'international', 'investments', 'io', 'iq', 'ir', 'irish', 'is',\n    'it', 'iwc',\n    # j\n    'jcb', 'je', 'jetzt', 'jm', 'jo', 'jobs', 'joburg', 'jp',\n    'juegos',\n    # k\n    'kaufen', 'kddi', 'ke', 'kg', 'kh', 'ki', 'kim', 'kitchen',\n    'kiwi', 'km', 'kn', 'koeln', 'kp', 'kr', 'krd', 'kred', 'kw',\n    'ky', 'kyoto', 'kz',\n    # l\n    'la', 'lacaixa', 'land', 'lat', 'latrobe', 'lawyer', 'lb', 'lc',\n    'lds', 'lease', 'legal', 'lgbt', 'li', 'lidl', 'life',\n    'lighting', 'limited', 'limo', 'link', 'lk', 'loans',\n    'localhost', 'london', 'lotte', 'lotto', 'lr', 'ls', 'lt',\n    'ltda', 'lu', 'luxe', 'luxury', 'lv', 'ly',\n    # m\n    'ma', 'madrid', 'maison', 'management', 'mango', 'market',\n    'marketing', 'marriott', 'mc', 'md', 'me', 'media', 'meet',\n    'melbourne', 'meme', 'memorial', 'menu', 'mg', 'mh', 'miami',\n    'mil', 'mini', 'mk', 'ml', 'mm', 'mn', 'mo', 'mobi', 'moda',\n    'moe', 'monash', 'money', 'mormon', 'mortgage', 'moscow',\n    'motorcycles', 'mov', 'mp', 'mq', 'mr', 'ms', 'mt', 'mu',\n    'museum', 'mv', 'mw', 'mx', 'my', 'mz',\n    # n\n    'na', 'nagoya', 'name', 'navy', 'nc', 'ne', 'net', 'network',\n    'neustar', 'new', 'nexus', 'nf', 'ng', 'ngo', 'nhk', 'ni',\n    'nico', 'ninja', 'nl', 'no', 'np', 'nr', 'nra', 'nrw', 'ntt',\n    'nu', 'nyc', 'nz',\n    # o\n    'okinawa', 'om', 'one', 'ong', 'onl', 'ooo', 'org', 'organic',\n    'osaka', 'otsuka', 'ovh',\n    # p\n    'pa', 'paris', 'partners', 'parts', 'party', 'pe', 'pf', 'pg',\n    'ph', 'pharmacy', 'photo', 'photography', 'photos', 'physio',\n    'pics', 'pictures', 'pink', 'pizza', 'pk', 'pl', 'place',\n    'plumbing', 'pm', 'pn', 'pohl', 'poker', 'porn', 'post', 'pr',\n    'praxi', 'press', 'pro', 'prod', 'productions', 'prof',\n    'properties', 'property', 'ps', 'pt', 'pub', 'pw', 'py',\n    # q\n    'qa', 'qpon', 'quebec',\n    # r\n    're', 'realtor', 'recipes', 'red', 'rehab', 'reise', 'reisen',\n    'reit', 'ren', 'rentals', 'repair', 'report', 'republican',\n    'rest', 'restaurant', 'reviews', 'rich', 'rio', 'rip', 'ro',\n    'rocks', 'rodeo', 'rs', 'rsvp', 'ru', 'ruhr', 'rw', 'ryukyu',\n    # s\n    'sa', 'saarland', 'sale', 'samsung', 'sarl', 'saxo', 'sb', 'sc',\n    'sca', 'scb', 'schmidt', 'school', 'schule', 'schwarz',\n    'science', 'scot', 'sd', 'se', 'services', 'sew', 'sexy', 'sg',\n    'sh', 'shiksha', 'shoes', 'shriram', 'si', 'singles', 'sj', 'sk',\n    'sky', 'sl', 'sm', 'sn', 'so', 'social', 'software', 'sohu',\n    'solar', 'solutions', 'soy', 'space', 'spiegel', 'sr', 'st',\n    'style', 'su', 'supplies', 'supply', 'support', 'surf',\n    'surgery', 'suzuki', 'sv', 'sx', 'sy', 'sydney', 'systems', 'sz',\n    # t\n    'taipei', 'tatar', 'tattoo', 'tax', 'tc', 'td', 'technology',\n    'tel', 'temasek', 'tennis', 'tf', 'tg', 'th', 'tienda', 'tips',\n    'tires', 'tirol', 'tj', 'tk', 'tl', 'tm', 'tn', 'to', 'today',\n    'tokyo', 'tools', 'top', 'toshiba', 'town', 'toys', 'tp', 'tr',\n    'trade', 'training', 'travel', 'trust', 'tt', 'tui', 'tv', 'tw',\n    'tz',\n    # u\n    'ua', 'ug', 'uk', 'university', 'uno', 'uol', 'us', 'uy', 'uz',\n    # v\n    'va', 'vacations', 'vc', 've', 'vegas', 'ventures',\n    'versicherung', 'vet', 'vg', 'vi', 'viajes', 'video', 'villas',\n    'vision', 'vlaanderen', 'vn', 'vodka', 'vote', 'voting', 'voto',\n    'voyage', 'vu',\n    # w\n    'wales', 'wang', 'watch', 'webcam', 'website', 'wed', 'wedding',\n    'wf', 'whoswho', 'wien', 'wiki', 'williamhill', 'wme', 'work',\n    'works', 'world', 'ws', 'wtc', 'wtf',\n    # x\n    'xn--1qqw23a', 'xn--3bst00m', 'xn--3ds443g', 'xn--3e0b707e',\n    'xn--45brj9c', 'xn--45q11c', 'xn--4gbrim', 'xn--55qw42g',\n    'xn--55qx5d', 'xn--6frz82g', 'xn--6qq986b3xl', 'xn--80adxhks',\n    'xn--80ao21a', 'xn--80asehdb', 'xn--80aswg', 'xn--90a3ac',\n    'xn--90ais', 'xn--b4w605ferd', 'xn--c1avg', 'xn--cg4bki',\n    'xn--clchc0ea0b2g2a9gcd', 'xn--czr694b', 'xn--czrs0t',\n    'xn--czru2d', 'xn--d1acj3b', 'xn--d1alf', 'xn--fiq228c5hs',\n    'xn--fiq64b', 'xn--fiqs8s', 'xn--fiqz9s', 'xn--flw351e',\n    'xn--fpcrj9c3d', 'xn--fzc2c9e2c', 'xn--gecrj9c', 'xn--h2brj9c',\n    'xn--hxt814e', 'xn--i1b6b1a6a2e', 'xn--io0a7i', 'xn--j1amh',\n    'xn--j6w193g', 'xn--kprw13d', 'xn--kpry57d', 'xn--kput3i',\n    'xn--l1acc', 'xn--lgbbat1ad8j', 'xn--mgb9awbf',\n    'xn--mgba3a4f16a', 'xn--mgbaam7a8h', 'xn--mgbab2bd',\n    'xn--mgbayh7gpa', 'xn--mgbbh1a71e', 'xn--mgbc0a9azcg',\n    'xn--mgberp4a5d4ar', 'xn--mgbx4cd0ab', 'xn--ngbc5azd',\n    'xn--node', 'xn--nqv7f', 'xn--nqv7fs00ema', 'xn--o3cw4h',\n    'xn--ogbpf8fl', 'xn--p1acf', 'xn--p1ai', 'xn--pgbs0dh',\n    'xn--q9jyb4c', 'xn--qcka1pmc', 'xn--rhqv96g', 'xn--s9brj9c',\n    'xn--ses554g', 'xn--unup4y', 'xn--vermgensberater-ctb',\n    'xn--vermgensberatung-pwb', 'xn--vhquv', 'xn--wgbh1c',\n    'xn--wgbl6a', 'xn--xhq521b', 'xn--xkc2al3hye2a',\n    'xn--xkc2dl3a5ee0h', 'xn--yfro4i67o', 'xn--ygbi2ammx',\n    'xn--zfr164b', 'xxx', 'xyz',\n    # y\n    'yachts', 'yandex', 'ye', 'yodobashi', 'yoga', 'yokohama',\n    'youtube', 'yt',\n    # z\n    'za', 'zip', 'zm', 'zone', 'zuerich', 'zw'\n]\n"}},"msg":"fix empty validator values tampering"}},"https:\/\/github.com\/AdaptiveMotorControlLab\/CellSeg3d":{"6e3b2623f34849977fc40d8c5ff2c8a5ba7f3103":{"url":"https:\/\/api.github.com\/repos\/AdaptiveMotorControlLab\/CellSeg3d\/commits\/6e3b2623f34849977fc40d8c5ff2c8a5ba7f3103","html_url":"https:\/\/github.com\/AdaptiveMotorControlLab\/CellSeg3d\/commit\/6e3b2623f34849977fc40d8c5ff2c8a5ba7f3103","message":"Many fixes\n\n- replaced logger definition, now in utils\n- Removed close button on many dock widgets that should not be tampered with\n- Fixed issues with using only one file for training. Validation and test sets are now split within image samples\n- Stricter checks for Path and str in path loading\n- Updated default review path","sha":"6e3b2623f34849977fc40d8c5ff2c8a5ba7f3103","keyword":"tampering fix","diff":"diff --git a\/napari_cellseg3d\/config.py b\/napari_cellseg3d\/config.py\nindex 1ea7e478..72563a5e 100644\n--- a\/napari_cellseg3d\/config.py\n+++ b\/napari_cellseg3d\/config.py\n@@ -209,3 +209,5 @@ class TrainingWorkerConfig:\n     sample_size: List[int] = None\n     do_augmentation: bool = True\n     deterministic_config: DeterministicConfig = DeterministicConfig()\n+\n+\ndiff --git a\/napari_cellseg3d\/interface.py b\/napari_cellseg3d\/interface.py\nindex ea5cb180..d5d2e0ab 100644\n--- a\/napari_cellseg3d\/interface.py\n+++ b\/napari_cellseg3d\/interface.py\n@@ -1,4 +1,3 @@\n-import logging\n import warnings\n from functools import partial\n from typing import List\n@@ -59,7 +58,7 @@\n napari_param_darkgrey = \"#202228\"  # napari default LineEdit color\n ###############\n \n-logger = logging.getLogger(__name__)\n+logger = utils.LOGGER\n \n \n def toggle_visibility(checkbox, widget):\ndiff --git a\/napari_cellseg3d\/interface_utils.py b\/napari_cellseg3d\/interface_utils.py\nindex 5635212b..8f6ad3e7 100644\n--- a\/napari_cellseg3d\/interface_utils.py\n+++ b\/napari_cellseg3d\/interface_utils.py\n@@ -1,4 +1,4 @@\n-import logging\n+\n from functools import partial\n \n from qtpy.QtCore import QObject\n@@ -7,9 +7,9 @@\n from qtpy.QtWidgets import QMenu\n \n from napari_cellseg3d import interface as ui\n-from napari_cellseg3d.utils import Singleton\n+from napari_cellseg3d import utils\n \n-logger = logging.getLogger(__name__)\n+logger = utils.LOGGER\n ##################\n # Singleton UI widgets\n ##################\n@@ -45,9 +45,11 @@ def handle_adjust_errors(widget, type, context, msg: str):\n                 state = int(widget.parent().parent().windowState())\n                 if state == 0:  # normal state\n                     widget.parent().parent().adjustSize()\n+                    logger.debug(\"Non- max. size adjust attempt\")\n                 elif state == 2:  # maximized state\n                     widget.parent().parent().showNormal()\n                     widget.parent().parent().showMaximized()\n+                    logger.debug(\"Maximized size adjust attempt\")\n         except RuntimeError:\n             pass\n \n@@ -62,7 +64,7 @@ def handle_adjust_errors_wrapper(widget):\n ##################\n \n \n-class UtilsDropdown(metaclass=Singleton):\n+class UtilsDropdown(metaclass=utils.Singleton):\n     \"\"\"Singleton class for use in instantiating only one Utility dropdown menu that can be accessed from the plugin.\"\"\"\n \n     caller_widget = None\ndiff --git a\/napari_cellseg3d\/launch_review.py b\/napari_cellseg3d\/launch_review.py\nindex fb509c33..1eacb44a 100644\n--- a\/napari_cellseg3d\/launch_review.py\n+++ b\/napari_cellseg3d\/launch_review.py\n@@ -1,4 +1,4 @@\n-import logging\n+\n import os\n from pathlib import Path\n \n@@ -18,7 +18,7 @@\n from napari_cellseg3d import utils\n from napari_cellseg3d.plugin_dock import Datamanager\n \n-logger = logging.getLogger(__name__)\n+logger = utils.LOGGER\n \n \n def launch_review(review_config: config.ReviewConfig):\n@@ -118,7 +118,8 @@ def quicksave():\n \n         return dirname, quicksave()\n \n-    viewer.window.add_dock_widget(file_widget, name=\" \", area=\"bottom\")\n+    file_widget_dock = viewer.window.add_dock_widget(file_widget, name=\" \", area=\"bottom\")\n+    file_widget_dock._close_btn = False\n \n     with plt.style.context(\"dark_background\"):\n         canvas = FigureCanvas(Figure(figsize=(3, 15)))\n@@ -147,7 +148,8 @@ def quicksave():\n \n     canvas.setSizePolicy(QSizePolicy.Minimum, QSizePolicy.Maximum)\n \n-    viewer.window.add_dock_widget(canvas, name=\" \", area=\"right\")\n+    canvas_dock = viewer.window.add_dock_widget(canvas, name=\" \", area=\"right\")\n+    canvas_dock._close_btn = False\n \n     @viewer.mouse_drag_callbacks.append\n     def update_canvas_canvas(viewer, event):\n@@ -199,7 +201,8 @@ def update_canvas_canvas(viewer, event):\n         review_config.model_name,\n         review_config.new_csv,\n     )\n-    viewer.window.add_dock_widget(dmg, name=\" \", area=\"left\")\n+    datamananger = viewer.window.add_dock_widget(dmg, name=\" \", area=\"left\")\n+    datamananger._close_btn = False\n \n     def update_button(axis_event):\n \ndiff --git a\/napari_cellseg3d\/log_utility.py b\/napari_cellseg3d\/log_utility.py\nindex 992d5de8..3dfc7728 100644\n--- a\/napari_cellseg3d\/log_utility.py\n+++ b\/napari_cellseg3d\/log_utility.py\n@@ -1,4 +1,3 @@\n-import logging\n import threading\n import warnings\n \n@@ -6,7 +5,9 @@\n from qtpy.QtGui import QTextCursor\n from qtpy.QtWidgets import QTextEdit\n \n-logger = logging.getLogger(__name__)\n+from napari_cellseg3d import utils\n+\n+logger = utils.LOGGER\n \n \n class Log(QTextEdit):\ndiff --git a\/napari_cellseg3d\/model_framework.py b\/napari_cellseg3d\/model_framework.py\nindex 7c1d6625..cc4afa5c 100644\n--- a\/napari_cellseg3d\/model_framework.py\n+++ b\/napari_cellseg3d\/model_framework.py\n@@ -1,4 +1,4 @@\n-import logging\n+\n import warnings\n from pathlib import Path\n \n@@ -17,7 +17,7 @@\n from napari_cellseg3d.plugin_base import BasePluginFolder\n \n warnings.formatwarning = utils.format_Warning\n-logger = logging.getLogger(__name__)\n+logger = utils.LOGGER\n \n \n class ModelFramework(BasePluginFolder):\n@@ -204,6 +204,7 @@ def display_status_report(self):\n                 area=\"left\",\n                 allowed_areas=[\"left\"],\n             )\n+            report_dock._close_btn = False\n \n             # TODO move to activity log once they figure out _qt_window access and private attrib.\n             # activity_log = self._viewer.window._qt_window._activity_dialog\n@@ -242,7 +243,7 @@ def create_train_dataset_dict(self):\n         for file in self.images_filepaths:\n             logger.info(Path(file).name)\n         logger.info(\"*\" * 10)\n-        logger.info(\"\\nLabels :\\n\")\n+        logger.info(\"Labels :\\n\")\n         for file in self.labels_filepaths:\n             logger.info(Path(file).name)\n \n@@ -252,6 +253,7 @@ def create_train_dataset_dict(self):\n                 self.images_filepaths, self.labels_filepaths\n             )\n         ]\n+        logger.debug(f\"Training data dict : {data_dicts}\")\n \n         return data_dicts\n \ndiff --git a\/napari_cellseg3d\/model_workers.py b\/napari_cellseg3d\/model_workers.py\nindex 9e5052fb..e0969b31 100644\n--- a\/napari_cellseg3d\/model_workers.py\n+++ b\/napari_cellseg3d\/model_workers.py\n@@ -1,12 +1,12 @@\n-import logging\n import platform\n from dataclasses import dataclass\n+from math import ceil\n+import numpy as np\n from pathlib import Path\n+import torch\n from typing import List\n from typing import Optional\n \n-import numpy as np\n-import torch\n \n # MONAI\n from monai.data import CacheDataset\n@@ -56,7 +56,7 @@\n from napari_cellseg3d.model_instance_seg import ImageStats\n from napari_cellseg3d.model_instance_seg import volume_stats\n \n-logger = logging.getLogger(__name__)\n+logger = utils.LOGGER\n \n \"\"\"\n Writing something to log messages from outside the main thread is rather problematic (plenty of silent crashes...)\n@@ -1109,20 +1109,37 @@ def train(self):\n             epoch_loss_values = []\n             val_metric_values = []\n \n-            self.train_files, self.val_files = (\n-                self.config.train_data_dict[\n-                    0 : int(\n-                        len(self.config.train_data_dict)\n-                        * self.config.validation_percent\n-                    )\n-                ],\n-                self.config.train_data_dict[\n-                    int(\n-                        len(self.config.train_data_dict)\n-                        * self.config.validation_percent\n-                    ) :\n-                ],\n-            )\n+            if len(self.config.train_data_dict) > 1:\n+                self.train_files, self.val_files = (\n+                    self.config.train_data_dict[\n+                        0 : int(\n+                            len(self.config.train_data_dict)\n+                            * self.config.validation_percent\n+                        )\n+                    ],\n+                    self.config.train_data_dict[\n+                        int(\n+                            len(self.config.train_data_dict)\n+                            * self.config.validation_percent\n+                        ) :\n+                    ],\n+                )\n+            else :\n+                self.train_files = self.val_files = self.config.train_data_dict\n+                msg = f\"Only one image file was provided : {self.config.train_data_dict[0]['image']}.\\n\"\n+\n+                logger.debug(f\"SAMPLING is {self.config.sampling}\")\n+                if not self.config.sampling:\n+                    msg += f\"Sampling is not in use, the only image provided will be used as the validation file.\"\n+                    self.warn(msg)\n+                else:\n+                    msg += f\"Samples for validation will be cropped for the same only volume that is being used for training\"\n+\n+                logger.warning(msg)\n+\n+            logger.debug(f\"Data dict from config is {self.config.train_data_dict}\")\n+            logger.debug(f\"Train files : {self.train_files}\")\n+            logger.debug(f\"Val. files : {self.val_files}\")\n \n             if len(self.train_files) == 0:\n                 raise ValueError(\"Training dataset is empty\")\n@@ -1185,19 +1202,31 @@ def train(self):\n             )\n             # self.log(\"Loading dataset...\\n\")\n             if do_sampling:\n+\n+                # if there is only one volume, split samples\n+                # TODO(cyril) : maybe implement something in user config to toggle this behavior\n+                if len(self.config.train_data_dict) < 2:\n+                    num_train_samples = ceil(self.config.num_samples * self.config.validation_percent)\n+                    num_val_samples = ceil(self.config.num_samples * (1-self.config.validation_percent))\n+                else:\n+                    num_train_samples = num_val_samples = self.config.num_samples\n+\n+                logger.debug(f\"AMOUNT of train samples : {num_train_samples}\")\n+                logger.debug(f\"AMOUNT of validation samples : {num_val_samples}\")\n+\n                 logger.debug(\"train_ds\")\n                 train_ds = PatchDataset(\n                     data=self.train_files,\n                     transform=train_transforms,\n                     patch_func=sample_loader,\n-                    samples_per_image=self.config.num_samples,\n+                    samples_per_image=num_train_samples,\n                 )\n                 logger.debug(\"val_ds\")\n                 val_ds = PatchDataset(\n                     data=self.val_files,\n                     transform=val_transforms,\n                     patch_func=sample_loader,\n-                    samples_per_image=self.config.num_samples,\n+                    samples_per_image=num_val_samples,\n                 )\n \n             else:\ndiff --git a\/napari_cellseg3d\/plugin_base.py b\/napari_cellseg3d\/plugin_base.py\nindex 8a03b103..88335d66 100644\n--- a\/napari_cellseg3d\/plugin_base.py\n+++ b\/napari_cellseg3d\/plugin_base.py\n@@ -1,4 +1,4 @@\n-import logging\n+\n import warnings\n from functools import partial\n from pathlib import Path\n@@ -14,8 +14,9 @@\n from napari_cellseg3d import interface as ui\n from napari_cellseg3d.interface_utils import handle_adjust_errors_wrapper\n from napari_cellseg3d.interface_utils import UtilsDropdown\n+from napari_cellseg3d import utils\n \n-logger = logging.getLogger(__name__)\n+logger = utils.LOGGER\n \n \n class BasePluginSingleImage(QTabWidget):\n@@ -409,10 +410,9 @@ def load_dataset_paths(self):\n     def load_image_dataset(self):\n         \"\"\"Show file dialog to set :py:attr:`~images_filepaths`\"\"\"\n         filenames = self.load_dataset_paths()\n-\n+        logger.debug(f\"image filenames : {filenames}\")\n         if filenames:\n-            self.images_filepaths = sorted(filenames)\n-\n+            self.images_filepaths = [str(path) for path in sorted(filenames)]\n             path = str(Path(filenames[0]).parent)\n             self.image_filewidget.text_field.setText(path)\n             self.image_filewidget.check_ready()\n@@ -421,8 +421,9 @@ def load_image_dataset(self):\n     def load_label_dataset(self):\n         \"\"\"Show file dialog to set :py:attr:`~labels_filepaths`\"\"\"\n         filenames = self.load_dataset_paths()\n+        logger.debug(f\"labels filenames : {filenames}\")\n         if filenames:\n-            self.labels_filepaths = sorted(filenames)\n+            self.labels_filepaths = [str(path) for path in sorted(filenames)]\n             path = str(Path(filenames[0]).parent)\n             self.labels_filewidget.text_field.setText(path)\n             self.labels_filewidget.check_ready()\ndiff --git a\/napari_cellseg3d\/plugin_model_training.py b\/napari_cellseg3d\/plugin_model_training.py\nindex 04c5cffa..e5995c51 100644\n--- a\/napari_cellseg3d\/plugin_model_training.py\n+++ b\/napari_cellseg3d\/plugin_model_training.py\n@@ -1,4 +1,3 @@\n-import logging\n import shutil\n import warnings\n from functools import partial\n@@ -37,7 +36,7 @@\n NUMBER_TABS = 3\n DEFAULT_PATCH_SIZE = 64\n \n-logger = logging.getLogger(__name__)\n+logger = utils.LOGGER\n \n \n class Trainer(ModelFramework, metaclass=QWidgetSingleton):\n@@ -834,13 +833,13 @@ def start(self):\n                 learning_rate=float(self.learning_rate_choice.currentText()),\n                 validation_interval=self.val_interval_choice.value(),\n                 batch_size=self.batch_choice.slider_value,\n-                results_path_folder=results_path_folder,\n+                results_path_folder=str(results_path_folder),\n                 sampling=self.patch_choice.isChecked(),\n                 num_samples=self.sample_choice_slider.slider_value,\n                 sample_size=patch_size,\n                 do_augmentation=self.augment_choice.isChecked(),\n                 deterministic_config=deterministic_config,\n-            )  # FIXME continue\n+            )  # TODO(cyril) continue to put params in config\n \n             self.config = config.TrainerConfig(\n                 save_as_zip=self.zip_choice.isChecked()\n@@ -1152,6 +1151,7 @@ def update_loss_plot(self, loss, metric):\n             self.plot_dock = self._viewer.window.add_dock_widget(\n                 self.canvas, name=\"Loss plots\", area=\"bottom\"\n             )\n+            self.plot_dock._close_btn = False\n             self.docked_widgets.append(self.plot_dock)\n             self.plot_loss(loss, metric)\n         else:\ndiff --git a\/napari_cellseg3d\/plugin_review.py b\/napari_cellseg3d\/plugin_review.py\nindex 9482326d..61afaac0 100644\n--- a\/napari_cellseg3d\/plugin_review.py\n+++ b\/napari_cellseg3d\/plugin_review.py\n@@ -142,7 +142,7 @@ def build(self):\n         # self._show_io_element(self.results_filewidget)\n \n         self.results_filewidget.text_field.setText(\n-            str(Path.home() \/ Path(\"cellseg3d_review\"))\n+            str(Path.home() \/ Path(\"cellseg3d\/review\"))  # TODO(cyril) : check proper behaviour\n         )\n \n         csv_param_w.setLayout(csv_param_l)\ndiff --git a\/napari_cellseg3d\/utils.py b\/napari_cellseg3d\/utils.py\nindex d6c51430..2be6e5ba 100644\n--- a\/napari_cellseg3d\/utils.py\n+++ b\/napari_cellseg3d\/utils.py\n@@ -1,3 +1,4 @@\n+import logging\n import warnings\n from datetime import datetime\n from pathlib import Path\n@@ -10,6 +11,13 @@\n from skimage.filters import gaussian\n from tifffile import imread as tfl_imread\n \n+LOGGER = logging.getLogger(__name__)\n+###############\n+# Global logging level setting\n+LOGGER.setLevel(logging.DEBUG)\n+# LOGGER.setLevel(logging.INFO)\n+###############\n+\n \"\"\"\n utils.py\n ====================================\n","files":{"\/napari_cellseg3d\/interface.py":{"changes":[{"diff":"\n-import logging\n import warnings\n from functools import partial\n from typing import List\n","add":0,"remove":1,"filename":"\/napari_cellseg3d\/interface.py","badparts":["import logging"],"goodparts":[]},{"diff":"\n ###############\n \n-logger = logging.getLogger(__name__)\n+logger = utils.LOGGER\n \n \n def toggle_visibility(checkbox, widget)","add":1,"remove":1,"filename":"\/napari_cellseg3d\/interface.py","badparts":["logger = logging.getLogger(__name__)"],"goodparts":["logger = utils.LOGGER"]}]},"\/napari_cellseg3d\/interface_utils.py":{"changes":[{"diff":"\n-import logging\n+\n from functools import partial\n \n from qtpy.QtCore import QObject\n","add":1,"remove":1,"filename":"\/napari_cellseg3d\/interface_utils.py","badparts":["import logging"],"goodparts":[]},{"diff":"\n \n from napari_cellseg3d import interface as ui\n-from napari_cellseg3d.utils import Singleton\n+from napari_cellseg3d import utils\n \n-logger = logging.getLogger(__name__)\n+logger = utils.LOGGER\n ##################\n # Singleton UI widgets\n ##################\n","add":2,"remove":2,"filename":"\/napari_cellseg3d\/interface_utils.py","badparts":["from napari_cellseg3d.utils import Singleton","logger = logging.getLogger(__name__)"],"goodparts":["from napari_cellseg3d import utils","logger = utils.LOGGER"]},{"diff":"\n ##################\n \n \n-class UtilsDropdown(metaclass=Singleton):\n+class UtilsDropdown(metaclass=utils.Singleton):\n     \"\"\"Singleton class for use in instantiating only one Utility dropdown menu that can be accessed from the plugin.\"\"\"\n \n     caller_widget = No","add":1,"remove":1,"filename":"\/napari_cellseg3d\/interface_utils.py","badparts":["class UtilsDropdown(metaclass=Singleton):"],"goodparts":["class UtilsDropdown(metaclass=utils.Singleton):"]}],"source":"\nimport logging from functools import partial from qtpy.QtCore import QObject from qtpy.QtCore import QtWarningMsg from qtpy.QtGui import QCursor from qtpy.QtWidgets import QMenu from napari_cellseg3d import interface as ui from napari_cellseg3d.utils import Singleton logger=logging.getLogger(__name__) class QWidgetSingleton(type(QObject)): _instances={} def __call__(cls, *args, **kwargs): if cls not in cls._instances: cls._instances[cls]=super(QWidgetSingleton, cls).__call__( *args, **kwargs ) return cls._instances[cls] def handle_adjust_errors(widget, type, context, msg: str): \"\"\"Qt message handler that attempts to react to errors when setting the window size and resizes the main window\"\"\" head=msg.split(\": \")[0] if type==QtWarningMsg and head==\"QWindowsWindow::setGeometry\": logger.warning( f\"Qt resize error:{msg}\\nhas been handled by attempting to resize the window\" ) try: if widget.parent() is not None: state=int(widget.parent().parent().windowState()) if state==0: widget.parent().parent().adjustSize() elif state==2: widget.parent().parent().showNormal() widget.parent().parent().showMaximized() except RuntimeError: pass def handle_adjust_errors_wrapper(widget): \"\"\"Returns a callable that can be used with qInstallMessageHandler directly\"\"\" return partial(handle_adjust_errors, widget) class UtilsDropdown(metaclass=Singleton): \"\"\"Singleton class for use in instantiating only one Utility dropdown menu that can be accessed from the plugin.\"\"\" caller_widget=None def dropdown_menu_call(self, widget, event): \"\"\"Calls the utility dropdown menu at the location of a CTRL+right-click\"\"\" if self.caller_widget is None: self.caller_widget=widget if event.button==2 and \"control\" in event.modifiers: dragged=False yield while event.type==\"mouse_move\": dragged=True yield if dragged: pass else: if widget is self.caller_widget: pos=QCursor.pos() self.show_utils_menu(widget, pos) def show_utils_menu(self, widget, event): from napari_cellseg3d.plugin_utilities import UTILITIES_WIDGETS menu=QMenu(widget.window()) menu.setStyleSheet( f\"background-color:{ui.napari_grey}; color: white;\" ) actions=[] for title in UTILITIES_WIDGETS.keys(): a=menu.addAction(f\"Utilities:{title}\") actions.append(a) action=menu.exec_(event) for possible_action in actions: if action==possible_action: text=possible_action.text().split(\": \")[1] widget=UTILITIES_WIDGETS[text](widget._viewer) widget._viewer.window.add_dock_widget(widget) ","sourceWithComments":"import logging\nfrom functools import partial\n\nfrom qtpy.QtCore import QObject\nfrom qtpy.QtCore import QtWarningMsg\nfrom qtpy.QtGui import QCursor\nfrom qtpy.QtWidgets import QMenu\n\nfrom napari_cellseg3d import interface as ui\nfrom napari_cellseg3d.utils import Singleton\n\nlogger = logging.getLogger(__name__)\n##################\n# Singleton UI widgets\n##################\n\n\nclass QWidgetSingleton(type(QObject)):\n\n    _instances = {}\n\n    def __call__(cls, *args, **kwargs):\n        if cls not in cls._instances:\n            cls._instances[cls] = super(QWidgetSingleton, cls).__call__(\n                *args, **kwargs\n            )\n        return cls._instances[cls]\n\n\n##################\n# Screen size adjustment error handler\n##################\n\n\ndef handle_adjust_errors(widget, type, context, msg: str):\n    \"\"\"Qt message handler that attempts to react to errors when setting the window size\n    and resizes the main window\"\"\"\n    head = msg.split(\": \")[0]\n    if type == QtWarningMsg and head == \"QWindowsWindow::setGeometry\":\n        logger.warning(\n            f\"Qt resize error : {msg}\\nhas been handled by attempting to resize the window\"\n        )\n        try:\n            if widget.parent() is not None:\n                state = int(widget.parent().parent().windowState())\n                if state == 0:  # normal state\n                    widget.parent().parent().adjustSize()\n                elif state == 2:  # maximized state\n                    widget.parent().parent().showNormal()\n                    widget.parent().parent().showMaximized()\n        except RuntimeError:\n            pass\n\n\ndef handle_adjust_errors_wrapper(widget):\n    \"\"\"Returns a callable that can be used with qInstallMessageHandler directly\"\"\"\n    return partial(handle_adjust_errors, widget)\n\n\n##################\n# Context menu for utilities\n##################\n\n\nclass UtilsDropdown(metaclass=Singleton):\n    \"\"\"Singleton class for use in instantiating only one Utility dropdown menu that can be accessed from the plugin.\"\"\"\n\n    caller_widget = None\n    # TODO(cyril) : might cause issues with forcing all widget instances to remain forever\n\n    def dropdown_menu_call(self, widget, event):\n        \"\"\"Calls the utility dropdown menu at the location of a CTRL+right-click\"\"\"\n        # ### DEBUG ### #\n        # print(event.modifiers)\n        # print(\"menu call\")\n        # print(widget)\n        # print(self)\n        ##################\n        if self.caller_widget is None:\n            self.caller_widget = widget\n\n        if event.button == 2 and \"control\" in event.modifiers:\n            dragged = False\n            yield\n            # on move\n            while event.type == \"mouse_move\":\n                # print(event.position)\n                dragged = True\n                yield\n            # on release\n            if dragged:\n                # print(\"drag end\")\n                pass\n            else:\n                # print(\"clicked!\")\n                if widget is self.caller_widget:\n                    # print(f\"authorized widget {widget} to show menu\")\n                    pos = QCursor.pos()\n                    self.show_utils_menu(widget, pos)\n                # else:\n                # print(f\"blocked widget {widget} from opening utils\")\n\n    def show_utils_menu(self, widget, event):\n        from napari_cellseg3d.plugin_utilities import UTILITIES_WIDGETS\n\n        # print(self.parent().parent())\n        # TODO create mapping for name:widget\n        # menu = QMenu(self.parent().parent())\n        menu = QMenu(widget.window())\n        menu.setStyleSheet(\n            f\"background-color: {ui.napari_grey}; color: white;\"\n        )\n\n        actions = []\n        for title in UTILITIES_WIDGETS.keys():\n            a = menu.addAction(f\"Utilities : {title}\")\n            actions.append(a)\n\n        action = menu.exec_(event)\n\n        for possible_action in actions:\n            if action == possible_action:\n                text = possible_action.text().split(\": \")[1]\n                widget = UTILITIES_WIDGETS[text](widget._viewer)\n                widget._viewer.window.add_dock_widget(widget)\n"},"\/napari_cellseg3d\/launch_review.py":{"changes":[{"diff":"\n-import logging\n+\n import os\n from pathlib import Path\n \n","add":1,"remove":1,"filename":"\/napari_cellseg3d\/launch_review.py","badparts":["import logging"],"goodparts":[]},{"diff":"\n from napari_cellseg3d.plugin_dock import Datamanager\n \n-logger = logging.getLogger(__name__)\n+logger = utils.LOGGER\n \n \n def launch_review(review_config: config.ReviewConfig):\n","add":1,"remove":1,"filename":"\/napari_cellseg3d\/launch_review.py","badparts":["logger = logging.getLogger(__name__)"],"goodparts":["logger = utils.LOGGER"]},{"diff":"\n \n         return dirname, quicksave()\n \n-    viewer.window.add_dock_widget(file_widget, name=\" \", area=\"bottom\")\n+    file_widget_dock = viewer.window.add_dock_widget(file_widget, name=\" \", area=\"bottom\")\n+    file_widget_dock._close_btn = False\n \n     with plt.style.context(\"dark_background\"):\n         canvas = FigureCanvas(Figure(figsize=(3, 15)))\n","add":2,"remove":1,"filename":"\/napari_cellseg3d\/launch_review.py","badparts":["    viewer.window.add_dock_widget(file_widget, name=\" \", area=\"bottom\")"],"goodparts":["    file_widget_dock = viewer.window.add_dock_widget(file_widget, name=\" \", area=\"bottom\")","    file_widget_dock._close_btn = False"]},{"diff":"\n \n     canvas.setSizePolicy(QSizePolicy.Minimum, QSizePolicy.Maximum)\n \n-    viewer.window.add_dock_widget(canvas, name=\" \", area=\"right\")\n+    canvas_dock = viewer.window.add_dock_widget(canvas, name=\" \", area=\"right\")\n+    canvas_dock._close_btn = False\n \n     @viewer.mouse_drag_callbacks.append\n     def update_canvas_canvas(viewer, event):\n","add":2,"remove":1,"filename":"\/napari_cellseg3d\/launch_review.py","badparts":["    viewer.window.add_dock_widget(canvas, name=\" \", area=\"right\")"],"goodparts":["    canvas_dock = viewer.window.add_dock_widget(canvas, name=\" \", area=\"right\")","    canvas_dock._close_btn = False"]},{"diff":"\n         review_config.model_name,\n         review_config.new_csv,\n     )\n-    viewer.window.add_dock_widget(dmg, name=\" \", area=\"left\")\n+    datamananger = viewer.window.add_dock_widget(dmg, name=\" \", area=\"left\")\n+    datamananger._close_btn = False\n \n     def update_button(axis_event)","add":2,"remove":1,"filename":"\/napari_cellseg3d\/launch_review.py","badparts":["    viewer.window.add_dock_widget(dmg, name=\" \", area=\"left\")"],"goodparts":["    datamananger = viewer.window.add_dock_widget(dmg, name=\" \", area=\"left\")","    datamananger._close_btn = False"]}],"source":"\nimport logging import os from pathlib import Path import matplotlib.pyplot as plt import napari import numpy as np from magicgui import magicgui from matplotlib.backends.backend_qt5agg import( FigureCanvasQTAgg as FigureCanvas, ) from matplotlib.figure import Figure from qtpy.QtWidgets import QSizePolicy from scipy import ndimage from tifffile import imwrite from napari_cellseg3d import config from napari_cellseg3d import utils from napari_cellseg3d.plugin_dock import Datamanager logger=logging.getLogger(__name__) def launch_review(review_config: config.ReviewConfig): \"\"\"Launch the review process, loading the original image, the labels & the raw labels(from prediction) in the viewer. Adds several widgets to the viewer: * A data manager widget that lets the user choose a directory to save the labels in, and a save button to quickly save. * A \"checked\/not checked\" button to let the user confirm that a slice has been checked or not. **This writes in a csv file under the corresponding slice the slice status(i.e. checked or not checked) to allow tracking of the review process for a given dataset.** * A plot widget that, when shift-clicking on the volume or label, displays the chosen location on several projections(x-y, y-z, x-z), to allow the user to have a better all-around view of the object and determine whether it should be labeled or not. Args: original(dask.array.Array): The original images\/volumes that have been labeled base(dask.array.Array): The labels for the volume raw(dask.array.Array): The raw labels from the prediction r_path(str): Path to the raw labels model_type(str): The name of the model to be displayed in csv filenames. checkbox(bool): Whether the \"new model\" checkbox has been checked or not, to create a new csv filetype(str): The file extension of the volumes and labels. as_folder(bool): Whether to load as folder or single file zoom_factor(array(int)): zoom factors for each axis Returns: list of all docked widgets \"\"\" images_original=review_config.image if review_config.labels is not None: base_label=review_config.labels else: base_label=np.zeros_like(images_original) viewer=napari.Viewer() viewer.scale_bar.visible=True viewer.add_image( images_original, name=\"volume\", colormap=\"inferno\", contrast_limits=[200, 1000], scale=review_config.zoom_factor, ) viewer.add_labels( base_label, name=\"labels\", seed=0.6, scale=review_config.zoom_factor ) @magicgui( dirname={\"mode\": \"d\", \"label\": \"Save labels in... \"}, call_button=\"Save\", ) def file_widget( dirname=Path(review_config.csv_path), ): dirname=Path(review_config.csv_path) out_dir=file_widget.dirname.value def quicksave(): if viewer.layers[\"labels\"] is not None: name=os.path.join(str(out_dir), \"labels_reviewed.tif\") dat=viewer.layers[\"labels\"].data imwrite(name, data=dat) return dirname, quicksave() viewer.window.add_dock_widget(file_widget, name=\" \", area=\"bottom\") with plt.style.context(\"dark_background\"): canvas=FigureCanvas(Figure(figsize=(3, 15))) xy_axes=canvas.figure.add_subplot(3, 1, 1) canvas.figure.suptitle(\"Shift-click on image for plot \\n\", fontsize=8) xy_axes.imshow(np.zeros((100, 100), np.int16)) xy_axes.scatter(50, 50, s=10, c=\"green\", alpha=0.25) xy_axes.set_xlabel(\"x axis\") xy_axes.set_ylabel(\"y axis\") yz_axes=canvas.figure.add_subplot(3, 1, 2) yz_axes.imshow(np.zeros((100, 100), np.int16)) yz_axes.scatter(50, 50, s=10, c=\"green\", alpha=0.25) yz_axes.set_xlabel(\"y axis\") yz_axes.set_ylabel(\"z axis\") zx_axes=canvas.figure.add_subplot(3, 1, 3) zx_axes.imshow(np.zeros((100, 100), np.int16)) zx_axes.scatter(50, 50, s=10, c=\"green\", alpha=0.25) zx_axes.set_xlabel(\"x axis\") zx_axes.set_ylabel(\"z axis\") canvas.figure.subplots_adjust( left=0.1, bottom=0.1, right=1, top=0.95, wspace=0, hspace=0.4 ) canvas.setSizePolicy(QSizePolicy.Minimum, QSizePolicy.Maximum) viewer.window.add_dock_widget(canvas, name=\" \", area=\"right\") @viewer.mouse_drag_callbacks.append def update_canvas_canvas(viewer, event): if \"shift\" in event.modifiers: try: cursor_position=np.round(viewer.cursor.position).astype(int) logger.debug(f\"plot @{cursor_position}\") cropped_volume=crop_volume_around_point( [ cursor_position[0], cursor_position[1], cursor_position[2], ], viewer.layers[\"volume\"], review_config.zoom_factor, ) xy_axes.imshow( cropped_volume[50], cmap=\"inferno\", vmin=200, vmax=2000 ) yz_axes.imshow( cropped_volume.transpose(1, 0, 2)[50], cmap=\"inferno\", vmin=200, vmax=2000, ) zx_axes.imshow( cropped_volume.transpose(2, 0, 1)[50], cmap=\"inferno\", vmin=200, vmax=2000, ) canvas.draw_idle() except Exception as e: logger.warning(e) dmg=Datamanager(parent=viewer) dmg.prepare( review_config.csv_path, review_config.filetype, review_config.model_name, review_config.new_csv, ) viewer.window.add_dock_widget(dmg, name=\" \", area=\"left\") def update_button(axis_event): slice_num=axis_event.value[0] logger.debug(f\"slice num is{slice_num}\") dmg.update_dm(slice_num) viewer.dims.events.current_step.connect(update_button) def crop_volume_around_point(points, layer, zoom_factor): if zoom_factor !=[1, 1, 1]: data=np.array(layer.data, dtype=np.int16) volume=utils.resize(data, zoom_factor) else: volume=layer.data min_coordinates=[point -50 for point in points] max_coordinates=[point +50 for point in points] inferior_bound=[ min_coordinate if min_coordinate < 0 else 0 for min_coordinate in min_coordinates ] superior_bound=[ max_coordinate -volume.shape[i] if volume.shape[i] < max_coordinate else 0 for i, max_coordinate in enumerate(max_coordinates) ] crop_slice=tuple( slice(np.maximum(0, min_coordinate), max_coordinate) for min_coordinate, max_coordinate in zip( min_coordinates, max_coordinates ) ) crop_temp=volume[crop_slice] cropped_volume=np.zeros((100, 100, 100), np.int16) cropped_volume[ -inferior_bound[0]: 100 -superior_bound[0], -inferior_bound[1]: 100 -superior_bound[1], -inferior_bound[2]: 100 -superior_bound[2], ]=crop_temp return cropped_volume return viewer,[file_widget, canvas, dmg] ","sourceWithComments":"import logging\nimport os\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport napari\nimport numpy as np\nfrom magicgui import magicgui\nfrom matplotlib.backends.backend_qt5agg import (\n    FigureCanvasQTAgg as FigureCanvas,\n)\nfrom matplotlib.figure import Figure\nfrom qtpy.QtWidgets import QSizePolicy\nfrom scipy import ndimage\nfrom tifffile import imwrite\n\nfrom napari_cellseg3d import config\nfrom napari_cellseg3d import utils\nfrom napari_cellseg3d.plugin_dock import Datamanager\n\nlogger = logging.getLogger(__name__)\n\n\ndef launch_review(review_config: config.ReviewConfig):\n    \"\"\"Launch the review process, loading the original image, the labels & the raw labels (from prediction)\n    in the viewer.\n\n    Adds several widgets to the viewer :\n\n    * A data manager widget that lets the user choose a directory to save the labels in, and a save button to quickly\n      save.\n\n    * A \"checked\/not checked\" button to let the user confirm that a slice has been checked or not.\n\n\n          **This writes in a csv file under the corresponding slice the slice status (i.e. checked or not checked)\n          to allow tracking of the review process for a given dataset.**\n\n    * A plot widget that, when shift-clicking on the volume or label,\n      displays the chosen location on several projections (x-y, y-z, x-z),\n      to allow the user to have a better all-around view of the object\n      and determine whether it should be labeled or not.\n\n    Args:\n\n        original (dask.array.Array): The original images\/volumes that have been labeled\n\n        base (dask.array.Array): The labels for the volume\n\n        raw (dask.array.Array): The raw labels from the prediction\n\n        r_path (str): Path to the raw labels\n\n        model_type (str): The name of the model to be displayed in csv filenames.\n\n        checkbox (bool): Whether the \"new model\" checkbox has been checked or not, to create a new csv\n\n        filetype (str): The file extension of the volumes and labels.\n\n        as_folder (bool): Whether to load as folder or single file\n\n        zoom_factor (array(int)): zoom factors for each axis\n\n    Returns : list of all docked widgets\n    \"\"\"\n    images_original = review_config.image\n    if review_config.labels is not None:\n        base_label = review_config.labels\n    else:\n        base_label = np.zeros_like(images_original)\n\n    viewer = napari.Viewer()\n\n    viewer.scale_bar.visible = True\n\n    viewer.add_image(\n        images_original,\n        name=\"volume\",\n        colormap=\"inferno\",\n        contrast_limits=[200, 1000],\n        scale=review_config.zoom_factor,\n    )  # anything bigger than 255 will get mapped to 255... they did it like this because it must have rgb images\n    viewer.add_labels(\n        base_label, name=\"labels\", seed=0.6, scale=review_config.zoom_factor\n    )\n\n    @magicgui(\n        dirname={\"mode\": \"d\", \"label\": \"Save labels in... \"},\n        call_button=\"Save\",\n        # call_button_2=\"Save & quit\",\n    )\n    def file_widget(\n        dirname=Path(review_config.csv_path),\n    ):  # file name where to save annotations\n        # \"\"\"Take a filename and do something with it.\"\"\"\n        # logger.debug((\"The filename is:\", dirname)\n\n        dirname = Path(review_config.csv_path)\n        # def saver():\n        out_dir = file_widget.dirname.value\n\n        # logger.debug((\"The directory is:\", out_dir)\n\n        def quicksave():\n            # if not review_config.as_stack:\n            if viewer.layers[\"labels\"] is not None:\n                name = os.path.join(str(out_dir), \"labels_reviewed.tif\")\n                dat = viewer.layers[\"labels\"].data\n                imwrite(name, data=dat)\n\n            # else:\n            #     if viewer.layers[\"labels\"] is not None:\n            #         dir_name = os.path.join(str(out_dir), \"labels_reviewed\")\n            #         dat = viewer.layers[\"labels\"].data\n            #         utils.save_stack(\n            #             dat, dir_name, filetype=review_config.filetype\n            #         )\n\n        return dirname, quicksave()\n\n    viewer.window.add_dock_widget(file_widget, name=\" \", area=\"bottom\")\n\n    with plt.style.context(\"dark_background\"):\n        canvas = FigureCanvas(Figure(figsize=(3, 15)))\n\n        xy_axes = canvas.figure.add_subplot(3, 1, 1)\n        canvas.figure.suptitle(\"Shift-click on image for plot \\n\", fontsize=8)\n        xy_axes.imshow(np.zeros((100, 100), np.int16))\n        xy_axes.scatter(50, 50, s=10, c=\"green\", alpha=0.25)\n        xy_axes.set_xlabel(\"x axis\")\n        xy_axes.set_ylabel(\"y axis\")\n        yz_axes = canvas.figure.add_subplot(3, 1, 2)\n        yz_axes.imshow(np.zeros((100, 100), np.int16))\n        yz_axes.scatter(50, 50, s=10, c=\"green\", alpha=0.25)\n        yz_axes.set_xlabel(\"y axis\")\n        yz_axes.set_ylabel(\"z axis\")\n        zx_axes = canvas.figure.add_subplot(3, 1, 3)\n        zx_axes.imshow(np.zeros((100, 100), np.int16))\n        zx_axes.scatter(50, 50, s=10, c=\"green\", alpha=0.25)\n        zx_axes.set_xlabel(\"x axis\")\n        zx_axes.set_ylabel(\"z axis\")\n\n        # canvas.figure.tight_layout()\n        canvas.figure.subplots_adjust(\n            left=0.1, bottom=0.1, right=1, top=0.95, wspace=0, hspace=0.4\n        )\n\n    canvas.setSizePolicy(QSizePolicy.Minimum, QSizePolicy.Maximum)\n\n    viewer.window.add_dock_widget(canvas, name=\" \", area=\"right\")\n\n    @viewer.mouse_drag_callbacks.append\n    def update_canvas_canvas(viewer, event):\n\n        if \"shift\" in event.modifiers:\n            try:\n                cursor_position = np.round(viewer.cursor.position).astype(int)\n                logger.debug(f\"plot @ {cursor_position}\")\n\n                cropped_volume = crop_volume_around_point(\n                    [\n                        cursor_position[0],\n                        cursor_position[1],\n                        cursor_position[2],\n                    ],\n                    viewer.layers[\"volume\"],\n                    review_config.zoom_factor,\n                )\n\n                ##########\n                ##########\n                # DEBUG\n                # viewer.add_image(cropped_volume, name=\"DEBUG_crop_plot\")\n\n                xy_axes.imshow(\n                    cropped_volume[50], cmap=\"inferno\", vmin=200, vmax=2000\n                )\n                yz_axes.imshow(\n                    cropped_volume.transpose(1, 0, 2)[50],\n                    cmap=\"inferno\",\n                    vmin=200,\n                    vmax=2000,\n                )\n                zx_axes.imshow(\n                    cropped_volume.transpose(2, 0, 1)[50],\n                    cmap=\"inferno\",\n                    vmin=200,\n                    vmax=2000,\n                )\n                canvas.draw_idle()\n            except Exception as e:\n                logger.warning(e)\n\n    # Qt widget defined in docker.py\n    dmg = Datamanager(parent=viewer)\n    dmg.prepare(\n        review_config.csv_path,\n        review_config.filetype,\n        review_config.model_name,\n        review_config.new_csv,\n    )\n    viewer.window.add_dock_widget(dmg, name=\" \", area=\"left\")\n\n    def update_button(axis_event):\n\n        slice_num = axis_event.value[0]\n        logger.debug(f\"slice num is {slice_num}\")\n        dmg.update_dm(slice_num)\n\n    viewer.dims.events.current_step.connect(update_button)\n\n    def crop_volume_around_point(points, layer, zoom_factor):\n        if zoom_factor != [1, 1, 1]:\n            data = np.array(layer.data, dtype=np.int16)\n            volume = utils.resize(data, zoom_factor)\n            # image = ndimage.zoom(layer.data, zoom_factor, mode=\"nearest\") # cleaner but much slower...\n        else:\n            volume = layer.data\n\n        min_coordinates = [point - 50 for point in points]\n        max_coordinates = [point + 50 for point in points]\n        inferior_bound = [\n            min_coordinate if min_coordinate < 0 else 0\n            for min_coordinate in min_coordinates\n        ]\n        superior_bound = [\n            max_coordinate - volume.shape[i]\n            if volume.shape[i] < max_coordinate\n            else 0\n            for i, max_coordinate in enumerate(max_coordinates)\n        ]\n\n        crop_slice = tuple(\n            slice(np.maximum(0, min_coordinate), max_coordinate)\n            for min_coordinate, max_coordinate in zip(\n                min_coordinates, max_coordinates\n            )\n        )\n\n        # if review_config.as_stack:\n        #     crop_temp = volume[crop_slice].persist().compute()\n        # else:\n        crop_temp = volume[crop_slice]\n\n        cropped_volume = np.zeros((100, 100, 100), np.int16)\n        cropped_volume[\n            -inferior_bound[0] : 100 - superior_bound[0],\n            -inferior_bound[1] : 100 - superior_bound[1],\n            -inferior_bound[2] : 100 - superior_bound[2],\n        ] = crop_temp\n        return cropped_volume\n\n    return viewer, [file_widget, canvas, dmg]\n"},"\/napari_cellseg3d\/log_utility.py":{"changes":[{"diff":"\n-import logging\n import threading\n import warnings\n \n","add":0,"remove":1,"filename":"\/napari_cellseg3d\/log_utility.py","badparts":["import logging"],"goodparts":[]},{"diff":"\n from qtpy.QtWidgets import QTextEdit\n \n-logger = logging.getLogger(__name__)\n+from napari_cellseg3d import utils\n+\n+logger = utils.LOGGER\n \n \n class Log(QTextEd","add":3,"remove":1,"filename":"\/napari_cellseg3d\/log_utility.py","badparts":["logger = logging.getLogger(__name__)"],"goodparts":["from napari_cellseg3d import utils","logger = utils.LOGGER"]}],"source":"\nimport logging import threading import warnings from qtpy import QtCore from qtpy.QtGui import QTextCursor from qtpy.QtWidgets import QTextEdit logger=logging.getLogger(__name__) class Log(QTextEdit): \"\"\"Class to implement a log for important user info. Should be thread-safe.\"\"\" def __init__(self, parent): \"\"\"Creates a log with a lock for multithreading Args: parent(QWidget): parent widget to add Log instance to. \"\"\" super().__init__(parent) self.lock=threading.Lock() def write(self, message): self.lock.acquire() try: if not hasattr(self, \"flag\"): self.flag=False message=message.replace(\"\\r\", \"\").rstrip() if message: method=\"replace_last_line\" if self.flag else \"append\" QtCore.QMetaObject.invokeMethod( self, method, QtCore.Qt.QueuedConnection, QtCore.Q_ARG(str, message), ) self.flag=True else: self.flag=False finally: self.lock.release() @QtCore.Slot(str) def replace_last_line(self, text): self.lock.acquire() try: cursor=self.textCursor() cursor.movePosition(QTextCursor.End) cursor.select(QTextCursor.BlockUnderCursor) cursor.removeSelectedText() cursor.insertBlock() self.setTextCursor(cursor) self.insertPlainText(text) finally: self.lock.release() def print_and_log(self, text, printing=True): \"\"\"Utility used to both print to terminal and log text to a QTextEdit item in a thread-safe manner. Use only for important user info. Args: text(str): Text to be printed and logged printing(bool): Whether to print the message as well or not using logger.info(). Defaults to True. \"\"\" self.lock.acquire() try: if printing: logger.info(text) self.moveCursor(QTextCursor.End) self.insertPlainText(f\"\\n{text}\") self.verticalScrollBar().setValue( self.verticalScrollBar().maximum() ) finally: self.lock.release() def warn(self, warning): self.lock.acquire() try: warnings.warn(warning) finally: self.lock.release() ","sourceWithComments":"import logging\nimport threading\nimport warnings\n\nfrom qtpy import QtCore\nfrom qtpy.QtGui import QTextCursor\nfrom qtpy.QtWidgets import QTextEdit\n\nlogger = logging.getLogger(__name__)\n\n\nclass Log(QTextEdit):\n    \"\"\"Class to implement a log for important user info. Should be thread-safe.\"\"\"\n\n    def __init__(self, parent):\n        \"\"\"Creates a log with a lock for multithreading\n\n        Args:\n            parent (QWidget): parent widget to add Log instance to.\n        \"\"\"\n        super().__init__(parent)\n\n        # from qtpy.QtCore import QMetaType\n        # parent.qRegisterMetaType<QTextCursor>(\"QTextCursor\")\n\n        self.lock = threading.Lock()\n\n    # def receive_log(self, text):\n    #     self.print_and_log(text)\n    def write(self, message):\n        self.lock.acquire()\n        try:\n            if not hasattr(self, \"flag\"):\n                self.flag = False\n            message = message.replace(\"\\r\", \"\").rstrip()\n            if message:\n                method = \"replace_last_line\" if self.flag else \"append\"\n                QtCore.QMetaObject.invokeMethod(\n                    self,\n                    method,\n                    QtCore.Qt.QueuedConnection,\n                    QtCore.Q_ARG(str, message),\n                )\n                self.flag = True\n            else:\n                self.flag = False\n\n        finally:\n            self.lock.release()\n\n    @QtCore.Slot(str)\n    def replace_last_line(self, text):\n        self.lock.acquire()\n        try:\n            cursor = self.textCursor()\n            cursor.movePosition(QTextCursor.End)\n            cursor.select(QTextCursor.BlockUnderCursor)\n            cursor.removeSelectedText()\n            cursor.insertBlock()\n            self.setTextCursor(cursor)\n            self.insertPlainText(text)\n        finally:\n            self.lock.release()\n\n    def print_and_log(self, text, printing=True):\n        \"\"\"Utility used to both print to terminal and log text to a QTextEdit\n         item in a thread-safe manner. Use only for important user info.\n\n        Args:\n            text (str): Text to be printed and logged\n            printing (bool): Whether to print the message as well or not using logger.info(). Defaults to True.\n\n        \"\"\"\n        self.lock.acquire()\n        try:\n            if printing:\n                logger.info(text)\n            # causes issue if you clik on terminal (tied to CMD QuickEdit mode on Windows)\n            self.moveCursor(QTextCursor.End)\n            self.insertPlainText(f\"\\n{text}\")\n            self.verticalScrollBar().setValue(\n                self.verticalScrollBar().maximum()\n            )\n        finally:\n            self.lock.release()\n\n    def warn(self, warning):\n        self.lock.acquire()\n        try:\n            warnings.warn(warning)\n        finally:\n            self.lock.release()\n"},"\/napari_cellseg3d\/model_framework.py":{"changes":[{"diff":"\n-import logging\n+\n import warnings\n from pathlib import Path\n \n","add":1,"remove":1,"filename":"\/napari_cellseg3d\/model_framework.py","badparts":["import logging"],"goodparts":[]},{"diff":"\n \n warnings.formatwarning = utils.format_Warning\n-logger = logging.getLogger(__name__)\n+logger = utils.LOGGER\n \n \n class ModelFramework(BasePluginFolder):\n","add":1,"remove":1,"filename":"\/napari_cellseg3d\/model_framework.py","badparts":["logger = logging.getLogger(__name__)"],"goodparts":["logger = utils.LOGGER"]},{"diff":"\n         for file in self.images_filepaths:\n             logger.info(Path(file).name)\n         logger.info(\"*\" * 10)\n-        logger.info(\"\\nLabels :\\n\")\n+        logger.info(\"Labels :\\n\")\n         for file in self.labels_filepaths:\n             logger.info(Path(file).name)\n \n","add":1,"remove":1,"filename":"\/napari_cellseg3d\/model_framework.py","badparts":["        logger.info(\"\\nLabels :\\n\")"],"goodparts":["        logger.info(\"Labels :\\n\")"]}],"source":"\nimport logging import warnings from pathlib import Path import napari import torch from qtpy.QtWidgets import QProgressBar from qtpy.QtWidgets import QSizePolicy from napari_cellseg3d import config from napari_cellseg3d import interface as ui from napari_cellseg3d import utils from napari_cellseg3d.log_utility import Log from napari_cellseg3d.plugin_base import BasePluginFolder warnings.formatwarning=utils.format_Warning logger=logging.getLogger(__name__) class ModelFramework(BasePluginFolder): \"\"\"A framework with buttons to use for loading images, labels, models, etc. for both inference and training\"\"\" def __init__( self, viewer: \"napari.viewer.Viewer\", parent=None, loads_images=True, loads_labels=True, has_results=True, ): \"\"\"Creates a plugin framework with the following elements: * A button to choose an image folder containing the images of a dataset(e.g. dataset\/images) * A button to choose a label folder containing the labels of a dataset(e.g. dataset\/labels) * A button to choose a results folder to save results in(e.g. dataset\/inference_results) * A file extension choice to choose which file types to load in the data folders * A docked module with a log, a progress bar and a save button(see:py:func:`~display_status_report`) Args: viewer(napari.viewer.Viewer): viewer to load the widget in parent: parent QWidget loads_images: if True, will contain UI elements used to load napari image layers loads_labels: if True, will contain UI elements used to load napari label layers has_results: if True, will add UI to choose a results path \"\"\" super().__init__( viewer, parent, loads_images, loads_labels, has_results ) self._viewer=viewer \"\"\"Viewer to display the widget in\"\"\" self.weights_config=config.WeightsInfo() \"\"\"str: path to custom weights defined by user\"\"\" self._default_weights_folder=self.weights_config.path \"\"\"Default path for plugin weights\"\"\" self.available_models=config.MODEL_LIST \"\"\"dict: dictionary of available models, with string as key for name in widget display\"\"\" self.worker=None \"\"\"Worker from model_workers.py, either inference or training\"\"\" self.model_choice=ui.DropdownMenu( sorted(self.available_models.keys()), label=\"Model name\" ) self.weights_filewidget=ui.FilePathWidget( \"Weights path\", self.load_weights_path, self ) self.custom_weights_choice=ui.CheckBox( \"Load custom weights\", self.toggle_weights_path, self ) self.report_container=ui.ContainerWidget(l=10, t=5, r=5, b=5) self.report_container.setSizePolicy( QSizePolicy.Fixed, QSizePolicy.Minimum ) self.container_docked=False self.progress=QProgressBar(self.report_container) self.progress.setVisible(False) \"\"\"Widget for the progress bar\"\"\" self.log=Log(self.report_container) self.log.setVisible(False) \"\"\"Read-only display for process-related info. Use only for info destined to user.\"\"\" self.btn_save_log=ui.Button( \"Save log in results folder\", func=self.save_log, parent=self.report_container, fixed=False, ) self.btn_save_log.setVisible(False) def send_log(self, text): \"\"\"Emit a signal to print in a Log\"\"\" self.log.print_and_log(text) def save_log(self): \"\"\"Saves the worker's log to disk at self.results_path when called\"\"\" log=self.log.toPlainText() path=self.results_path if len(log) !=0: with open( path +f\"\/Log_report_{utils.get_date_time()}.txt\", \"x\", ) as f: f.write(log) f.close() else: warnings.warn( \"No job has been completed yet, please start one or re-open the log window.\" ) def save_log_to_path(self, path): \"\"\"Saves the worker log to a specific path. Cannot be used with connect. Args: path(str): path to save folder \"\"\" log=self.log.toPlainText() path=str( Path(path) \/ Path(f\"Log_report_{utils.get_date_time()}.txt\") ) if len(log) !=0: with open( path, \"x\", ) as f: f.write(log) f.close() else: warnings.warn( \"No job has been completed yet, please start one or re-open the log window.\" ) def display_status_report(self): \"\"\"Adds a text log, a progress bar and a \"save log\" button on the left side of the viewer (usually when starting a worker)\"\"\" if self.container_docked: self.log.clear() elif not self.container_docked: ui.add_widgets( self.report_container.layout, [self.progress, self.log, self.btn_save_log], alignment=None, ) self.report_container.setLayout(self.report_container.layout) report_dock=self._viewer.window.add_dock_widget( self.report_container, name=\"Status report\", area=\"left\", allowed_areas=[\"left\"], ) self.docked_widgets.append(report_dock) self.container_docked=True self.log.setVisible(True) self.progress.setVisible(True) self.btn_save_log.setVisible(True) self.progress.setValue(0) def toggle_weights_path(self): \"\"\"Toggle visibility of weight path\"\"\" ui.toggle_visibility( self.custom_weights_choice, self.weights_filewidget ) def create_train_dataset_dict(self): \"\"\"Creates data dictionary for MONAI transforms and training. Returns: a dict with the following: **Keys:** * \"image\": image * \"label\": corresponding label \"\"\" if len(self.images_filepaths)==0 or len(self.labels_filepaths)==0: raise ValueError(\"Data folders are empty\") logger.info(\"Images:\\n\") for file in self.images_filepaths: logger.info(Path(file).name) logger.info(\"*\" * 10) logger.info(\"\\nLabels:\\n\") for file in self.labels_filepaths: logger.info(Path(file).name) data_dicts=[ {\"image\": image_name, \"label\": label_name} for image_name, label_name in zip( self.images_filepaths, self.labels_filepaths ) ] return data_dicts def get_model(self, key): \"\"\"Getter for module(class and functions) associated to currently selected model\"\"\" return self.models_dict[key] @staticmethod def get_available_models(): \"\"\"Getter for module(class and functions) associated to currently selected model\"\"\" return config.MODEL_LIST def load_weights_path(self): \"\"\"Show file dialog to set:py:attr:`model_path`\"\"\" file=ui.open_file_dialog( self, [self._default_weights_folder], filetype=\"Weights file(*.pth)\", ) if file[0]==self._default_weights_folder: return if file is not None: if file[0] !=\"\": self.weights_config.path=file[0] self.weights_filewidget.text_field.setText(file[0]) self._default_weights_folder=str(Path(file[0]).parent) @staticmethod def get_device(show=True): \"\"\"Automatically discovers any cuda device and uses it for tensor operations. If none is available(CUDA not installed), uses cpu instead.\"\"\" device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") if show: logger.info(f\"Using{device} device\") logger.info(\"Using torch:\") logger.info(torch.__version__) return device def empty_cuda_cache(self): \"\"\"Empties the cuda cache if the device is a cuda device\"\"\" if self.get_device(show=False).type==\"cuda\": logger.info(\"Attempting to empty cache...\") torch.cuda.empty_cache() logger.info(\"Attempt complete: Cache emptied\") def build(self): raise NotImplementedError(\"Should be defined in children classes\") ","sourceWithComments":"import logging\nimport warnings\nfrom pathlib import Path\n\nimport napari\nimport torch\n\n# Qt\nfrom qtpy.QtWidgets import QProgressBar\nfrom qtpy.QtWidgets import QSizePolicy\n\n# local\nfrom napari_cellseg3d import config\nfrom napari_cellseg3d import interface as ui\nfrom napari_cellseg3d import utils\nfrom napari_cellseg3d.log_utility import Log\nfrom napari_cellseg3d.plugin_base import BasePluginFolder\n\nwarnings.formatwarning = utils.format_Warning\nlogger = logging.getLogger(__name__)\n\n\nclass ModelFramework(BasePluginFolder):\n    \"\"\"A framework with buttons to use for loading images, labels, models, etc. for both inference and training\"\"\"\n\n    def __init__(\n        self,\n        viewer: \"napari.viewer.Viewer\",\n        parent=None,\n        loads_images=True,\n        loads_labels=True,\n        has_results=True,\n    ):\n        \"\"\"Creates a plugin framework with the following elements :\n\n        * A button to choose an image folder containing the images of a dataset (e.g. dataset\/images)\n\n        * A button to choose a label folder containing the labels of a dataset (e.g. dataset\/labels)\n\n        * A button to choose a results folder to save results in (e.g. dataset\/inference_results)\n\n        * A file extension choice to choose which file types to load in the data folders\n\n        * A docked module with a log, a progress bar and a save button (see :py:func:`~display_status_report`)\n\n        Args:\n            viewer (napari.viewer.Viewer): viewer to load the widget in\n            parent: parent QWidget\n            loads_images: if True, will contain UI elements used to load napari image layers\n            loads_labels: if True, will contain UI elements used to load napari label layers\n            has_results: if True, will add UI to choose a results path\n        \"\"\"\n        super().__init__(\n            viewer, parent, loads_images, loads_labels, has_results\n        )\n\n        self._viewer = viewer\n        \"\"\"Viewer to display the widget in\"\"\"\n\n        # self.model_path = \"\" # TODO add custom models\n        # \"\"\"str: path to custom model defined by user\"\"\"\n\n        self.weights_config = config.WeightsInfo()\n        \"\"\"str : path to custom weights defined by user\"\"\"\n\n        self._default_weights_folder = self.weights_config.path\n        \"\"\"Default path for plugin weights\"\"\"\n\n        self.available_models = config.MODEL_LIST\n\n        \"\"\"dict: dictionary of available models, with string as key for name in widget display\"\"\"\n\n        self.worker = None\n        \"\"\"Worker from model_workers.py, either inference or training\"\"\"\n\n        #######################################################\n        # interface\n\n        # TODO : implement custom model\n        # self.model_filewidget = ui.FilePathWidget(\n        #     \"Model path\", self.load_model_path, self\n        # )\n\n        self.model_choice = ui.DropdownMenu(\n            sorted(self.available_models.keys()), label=\"Model name\"\n        )\n\n        self.weights_filewidget = ui.FilePathWidget(\n            \"Weights path\", self.load_weights_path, self\n        )\n\n        self.custom_weights_choice = ui.CheckBox(\n            \"Load custom weights\", self.toggle_weights_path, self\n        )\n\n        ###################################################\n        # status report docked widget\n\n        self.report_container = ui.ContainerWidget(l=10, t=5, r=5, b=5)\n\n        self.report_container.setSizePolicy(\n            QSizePolicy.Fixed, QSizePolicy.Minimum\n        )\n        self.container_docked = False  # check if already docked\n\n        self.progress = QProgressBar(self.report_container)\n        self.progress.setVisible(False)\n        \"\"\"Widget for the progress bar\"\"\"\n\n        self.log = Log(self.report_container)\n        self.log.setVisible(False)\n        \"\"\"Read-only display for process-related info. Use only for info destined to user.\"\"\"\n\n        self.btn_save_log = ui.Button(\n            \"Save log in results folder\",\n            func=self.save_log,\n            parent=self.report_container,\n            fixed=False,\n        )\n        self.btn_save_log.setVisible(False)\n\n    def send_log(self, text):\n        \"\"\"Emit a signal to print in a Log\"\"\"\n        self.log.print_and_log(text)\n\n    def save_log(self):\n        \"\"\"Saves the worker's log to disk at self.results_path when called\"\"\"\n        log = self.log.toPlainText()\n\n        path = self.results_path\n\n        if len(log) != 0:\n            with open(\n                path + f\"\/Log_report_{utils.get_date_time()}.txt\",\n                \"x\",\n            ) as f:\n                f.write(log)\n                f.close()\n        else:\n            warnings.warn(\n                \"No job has been completed yet, please start one or re-open the log window.\"\n            )\n\n    def save_log_to_path(self, path):\n        \"\"\"Saves the worker log to a specific path. Cannot be used with connect.\n\n        Args:\n            path (str): path to save folder\n        \"\"\"\n\n        log = self.log.toPlainText()\n        path = str(\n            Path(path) \/ Path(f\"Log_report_{utils.get_date_time()}.txt\")\n        )\n\n        if len(log) != 0:\n            with open(\n                path,\n                \"x\",\n            ) as f:\n                f.write(log)\n                f.close()\n        else:\n            warnings.warn(\n                \"No job has been completed yet, please start one or re-open the log window.\"\n            )\n\n    def display_status_report(self):\n        \"\"\"Adds a text log, a progress bar and a \"save log\" button on the left side of the viewer\n        (usually when starting a worker)\"\"\"\n\n        # if self.container_report is None or self.log is None:\n        #     warnings.warn(\n        #         \"Status report widget has been closed. Trying to re-instantiate...\"\n        #     )\n        #     self.container_report = QWidget()\n        #     self.container_report.setSizePolicy(\n        #         QSizePolicy.Fixed, QSizePolicy.Minimum\n        #     )\n        #     self.progress = QProgressBar(self.container_report)\n        #     self.log = QTextEdit(self.container_report)\n        #     self.btn_save_log = ui.Button(\n        #         \"Save log in results folder\", parent=self.container_report\n        #     )\n        #     self.btn_save_log.clicked.connect(self.save_log)\n        #\n        #     self.container_docked = False  # check if already docked\n\n        if self.container_docked:\n            self.log.clear()\n        elif not self.container_docked:\n\n            ui.add_widgets(\n                self.report_container.layout,\n                [self.progress, self.log, self.btn_save_log],\n                alignment=None,\n            )\n\n            self.report_container.setLayout(self.report_container.layout)\n\n            report_dock = self._viewer.window.add_dock_widget(\n                self.report_container,\n                name=\"Status report\",\n                area=\"left\",\n                allowed_areas=[\"left\"],\n            )\n\n            # TODO move to activity log once they figure out _qt_window access and private attrib.\n            # activity_log = self._viewer.window._qt_window._activity_dialog\n            # activity_layout = activity_log._activityLayout\n            # activity_layout.addWidget(self.container_report)\n\n            self.docked_widgets.append(report_dock)\n            self.container_docked = True\n\n        self.log.setVisible(True)\n        self.progress.setVisible(True)\n        self.btn_save_log.setVisible(True)\n        self.progress.setValue(0)\n\n    def toggle_weights_path(self):\n        \"\"\"Toggle visibility of weight path\"\"\"\n        ui.toggle_visibility(\n            self.custom_weights_choice, self.weights_filewidget\n        )\n\n    def create_train_dataset_dict(self):\n        \"\"\"Creates data dictionary for MONAI transforms and training.\n\n        Returns: a dict with the following :\n            **Keys:**\n\n            * \"image\": image\n\n            * \"label\" : corresponding label\n        \"\"\"\n\n        if len(self.images_filepaths) == 0 or len(self.labels_filepaths) == 0:\n            raise ValueError(\"Data folders are empty\")\n\n        logger.info(\"Images :\\n\")\n        for file in self.images_filepaths:\n            logger.info(Path(file).name)\n        logger.info(\"*\" * 10)\n        logger.info(\"\\nLabels :\\n\")\n        for file in self.labels_filepaths:\n            logger.info(Path(file).name)\n\n        data_dicts = [\n            {\"image\": image_name, \"label\": label_name}\n            for image_name, label_name in zip(\n                self.images_filepaths, self.labels_filepaths\n            )\n        ]\n\n        return data_dicts\n\n    def get_model(self, key):  # TODO remove\n        \"\"\"Getter for module (class and functions) associated to currently selected model\"\"\"\n        return self.models_dict[key]\n\n    @staticmethod\n    def get_available_models():\n        \"\"\"Getter for module (class and functions) associated to currently selected model\"\"\"\n        return config.MODEL_LIST\n\n    # def load_model_path(self): # TODO add custom models\n    #     \"\"\"Show file dialog to set :py:attr:`model_path`\"\"\"\n    #     folder = ui.open_folder_dialog(self, self._default_folders)\n    #     if folder is not None and type(folder) is str and os.path.isdir(folder):\n    #         self.model_path = folder\n    #         self.lbl_model_path.setText(self.model_path)\n    #         # self.update_default()\n\n    def load_weights_path(self):\n        \"\"\"Show file dialog to set :py:attr:`model_path`\"\"\"\n\n        # logger.debug(self._default_weights_folder)\n\n        file = ui.open_file_dialog(\n            self,\n            [self._default_weights_folder],\n            filetype=\"Weights file (*.pth)\",\n        )\n        if file[0] == self._default_weights_folder:\n            return\n        if file is not None:\n            if file[0] != \"\":\n                self.weights_config.path = file[0]\n                self.weights_filewidget.text_field.setText(file[0])\n                self._default_weights_folder = str(Path(file[0]).parent)\n\n    @staticmethod\n    def get_device(show=True):\n        \"\"\"Automatically discovers any cuda device and uses it for tensor operations.\n        If none is available (CUDA not installed), uses cpu instead.\"\"\"\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        if show:\n            logger.info(f\"Using {device} device\")\n            logger.info(\"Using torch :\")\n            logger.info(torch.__version__)\n        return device\n\n    def empty_cuda_cache(self):\n        \"\"\"Empties the cuda cache if the device is a cuda device\"\"\"\n        if self.get_device(show=False).type == \"cuda\":\n            logger.info(\"Attempting to empty cache...\")\n            torch.cuda.empty_cache()\n            logger.info(\"Attempt complete : Cache emptied\")\n\n    # def update_default(self): # TODO add custom models\n    #     \"\"\"Update default path for smoother file dialogs, here with :py:attr:`~model_path` included\"\"\"\n    #\n    #     if len(self.images_filepaths) != 0:\n    #         from_images = str(Path(self.images_filepaths[0]).parent)\n    #     else:\n    #         from_images = None\n    #\n    #     if len(self.labels_filepaths) != 0:\n    #         from_labels = str(Path(self.labels_filepaths[0]).parent)\n    #     else:\n    #         from_labels = None\n    #\n    #     possible_paths = [\n    #         path\n    #         for path in [\n    #             from_images,\n    #             from_labels,\n    #             # self.model_path,\n    #             self.results_path,\n    #         ]\n    #         if path is not None\n    #     ]\n    #     self._default_folders = possible_paths\n    # update if model_path is used again\n\n    def build(self):\n        raise NotImplementedError(\"Should be defined in children classes\")\n"},"\/napari_cellseg3d\/model_workers.py":{"changes":[{"diff":"\n-import logging\n import platform\n from dataclasses import dataclass\n+from math import ceil\n+import numpy as np\n from pathlib import Path\n+import torch\n from typing import List\n from typing import Optional\n \n-import numpy as np\n-import torch\n \n # MONAI\n from monai.data import CacheDataset\n","add":3,"remove":3,"filename":"\/napari_cellseg3d\/model_workers.py","badparts":["import logging","import numpy as np","import torch"],"goodparts":["from math import ceil","import numpy as np","import torch"]},{"diff":"\n from napari_cellseg3d.model_instance_seg import volume_stats\n \n-logger = logging.getLogger(__name__)\n+logger = utils.LOGGER\n \n \"\"\"\n Writing something to log messages from outside the main thread is rather problematic (plenty of silent crashes...)\n","add":1,"remove":1,"filename":"\/napari_cellseg3d\/model_workers.py","badparts":["logger = logging.getLogger(__name__)"],"goodparts":["logger = utils.LOGGER"]},{"diff":"\n             epoch_loss_values = []\n             val_metric_values = []\n \n-            self.train_files, self.val_files = (\n-                self.config.train_data_dict[\n-                    0 : int(\n-                        len(self.config.train_data_dict)\n-                        * self.config.validation_percent\n-                    )\n-                ],\n-                self.config.train_data_dict[\n-                    int(\n-                        len(self.config.train_data_dict)\n-                        * self.config.validation_percent\n-                    ) :\n-                ],\n-            )\n+            if len(self.config.train_data_dict) > 1:\n+                self.train_files, self.val_files = (\n+                    self.config.train_data_dict[\n+                        0 : int(\n+                            len(self.config.train_data_dict)\n+                            * self.config.validation_percent\n+                        )\n+                    ],\n+                    self.config.train_data_dict[\n+                        int(\n+                            len(self.config.train_data_dict)\n+                            * self.config.validation_percent\n+                        ) :\n+                    ],\n+                )\n+            else :\n+                self.train_files = self.val_files = self.config.train_data_dict\n+                msg = f\"Only one image file was provided : {self.config.train_data_dict[0]['image']}.\\n\"\n+\n+                logger.debug(f\"SAMPLING is {self.config.sampling}\")\n+                if not self.config.sampling:\n+                    msg += f\"Sampling is not in use, the only image provided will be used as the validation file.\"\n+                    self.warn(msg)\n+                else:\n+                    msg += f\"Samples for validation will be cropped for the same only volume that is being used for training\"\n+\n+                logger.warning(msg)\n+\n+            logger.debug(f\"Data dict from config is {self.config.train_data_dict}\")\n+            logger.debug(f\"Train files : {self.train_files}\")\n+            logger.debug(f\"Val. files : {self.val_files}\")\n \n             if len(self.train_files) == 0:\n                 raise ValueError(\"Training dataset is empty\")\n","add":31,"remove":14,"filename":"\/napari_cellseg3d\/model_workers.py","badparts":["            self.train_files, self.val_files = (","                self.config.train_data_dict[","                    0 : int(","                        len(self.config.train_data_dict)","                        * self.config.validation_percent","                    )","                ],","                self.config.train_data_dict[","                    int(","                        len(self.config.train_data_dict)","                        * self.config.validation_percent","                    ) :","                ],","            )"],"goodparts":["            if len(self.config.train_data_dict) > 1:","                self.train_files, self.val_files = (","                    self.config.train_data_dict[","                        0 : int(","                            len(self.config.train_data_dict)","                            * self.config.validation_percent","                        )","                    ],","                    self.config.train_data_dict[","                        int(","                            len(self.config.train_data_dict)","                            * self.config.validation_percent","                        ) :","                    ],","                )","            else :","                self.train_files = self.val_files = self.config.train_data_dict","                msg = f\"Only one image file was provided : {self.config.train_data_dict[0]['image']}.\\n\"","                logger.debug(f\"SAMPLING is {self.config.sampling}\")","                if not self.config.sampling:","                    msg += f\"Sampling is not in use, the only image provided will be used as the validation file.\"","                    self.warn(msg)","                else:","                    msg += f\"Samples for validation will be cropped for the same only volume that is being used for training\"","                logger.warning(msg)","            logger.debug(f\"Data dict from config is {self.config.train_data_dict}\")","            logger.debug(f\"Train files : {self.train_files}\")","            logger.debug(f\"Val. files : {self.val_files}\")"]},{"diff":"\n             )\n             # self.log(\"Loading dataset...\\n\")\n             if do_sampling:\n+\n+                # if there is only one volume, split samples\n+                # TODO(cyril) : maybe implement something in user config to toggle this behavior\n+                if len(self.config.train_data_dict) < 2:\n+                    num_train_samples = ceil(self.config.num_samples * self.config.validation_percent)\n+                    num_val_samples = ceil(self.config.num_samples * (1-self.config.validation_percent))\n+                else:\n+                    num_train_samples = num_val_samples = self.config.num_samples\n+\n+                logger.debug(f\"AMOUNT of train samples : {num_train_samples}\")\n+                logger.debug(f\"AMOUNT of validation samples : {num_val_samples}\")\n+\n                 logger.debug(\"train_ds\")\n                 train_ds = PatchDataset(\n                     data=self.train_files,\n                     transform=train_transforms,\n                     patch_func=sample_loader,\n-                    samples_per_image=self.config.num_samples,\n+                    samples_per_image=num_train_samples,\n                 )\n                 logger.debug(\"val_ds\")\n                 val_ds = PatchDataset(\n                     data=self.val_files,\n                     transform=val_transforms,\n                     patch_func=sample_loader,\n-                    samples_per_image=self.config.num_samples,\n+                    samples_per_image=num_val_samples,\n                 )\n \n            ","add":14,"remove":2,"filename":"\/napari_cellseg3d\/model_workers.py","badparts":["                    samples_per_image=self.config.num_samples,","                    samples_per_image=self.config.num_samples,"],"goodparts":["                if len(self.config.train_data_dict) < 2:","                    num_train_samples = ceil(self.config.num_samples * self.config.validation_percent)","                    num_val_samples = ceil(self.config.num_samples * (1-self.config.validation_percent))","                else:","                    num_train_samples = num_val_samples = self.config.num_samples","                logger.debug(f\"AMOUNT of train samples : {num_train_samples}\")","                logger.debug(f\"AMOUNT of validation samples : {num_val_samples}\")","                    samples_per_image=num_train_samples,","                    samples_per_image=num_val_samples,"]}]},"\/napari_cellseg3d\/plugin_base.py":{"changes":[{"diff":"\n-import logging\n+\n import warnings\n from functools import partial\n from pathlib import Path\n","add":1,"remove":1,"filename":"\/napari_cellseg3d\/plugin_base.py","badparts":["import logging"],"goodparts":[]},{"diff":"\n from napari_cellseg3d.interface_utils import handle_adjust_errors_wrapper\n from napari_cellseg3d.interface_utils import UtilsDropdown\n+from napari_cellseg3d import utils\n \n-logger = logging.getLogger(__name__)\n+logger = utils.LOGGER\n \n \n class BasePluginSingleImage(QTabWidget):\n","add":2,"remove":1,"filename":"\/napari_cellseg3d\/plugin_base.py","badparts":["logger = logging.getLogger(__name__)"],"goodparts":["from napari_cellseg3d import utils","logger = utils.LOGGER"]},{"diff":"\n     def load_image_dataset(self):\n         \"\"\"Show file dialog to set :py:attr:`~images_filepaths`\"\"\"\n         filenames = self.load_dataset_paths()\n-\n+        logger.debug(f\"image filenames : {filenames}\")\n         if filenames:\n-            self.images_filepaths = sorted(filenames)\n-\n+            self.images_filepaths = [str(path) for path in sorted(filenames)]\n             path = str(Path(filenames[0]).parent)\n             self.image_filewidget.text_field.setText(path)\n             self.image_filewidget.check_ready()\n","add":2,"remove":3,"filename":"\/napari_cellseg3d\/plugin_base.py","badparts":["            self.images_filepaths = sorted(filenames)"],"goodparts":["        logger.debug(f\"image filenames : {filenames}\")","            self.images_filepaths = [str(path) for path in sorted(filenames)]"]},{"diff":"\n     def load_label_dataset(self):\n         \"\"\"Show file dialog to set :py:attr:`~labels_filepaths`\"\"\"\n         filenames = self.load_dataset_paths()\n+        logger.debug(f\"labels filenames : {filenames}\")\n         if filenames:\n-            self.labels_filepaths = sorted(filenames)\n+            self.labels_filepaths = [str(path) for path in sorted(filenames)]\n             path = str(Path(filenames[0]).parent)\n             self.labels_filewidget.text_field.setText(path)\n             self.labels_filewidget.check_","add":2,"remove":1,"filename":"\/napari_cellseg3d\/plugin_base.py","badparts":["            self.labels_filepaths = sorted(filenames)"],"goodparts":["        logger.debug(f\"labels filenames : {filenames}\")","            self.labels_filepaths = [str(path) for path in sorted(filenames)]"]}],"source":"\nimport logging import warnings from functools import partial from pathlib import Path import napari from qtpy.QtCore import qInstallMessageHandler from qtpy.QtWidgets import QTabWidget from qtpy.QtWidgets import QWidget from napari_cellseg3d import interface as ui from napari_cellseg3d.interface_utils import handle_adjust_errors_wrapper from napari_cellseg3d.interface_utils import UtilsDropdown logger=logging.getLogger(__name__) class BasePluginSingleImage(QTabWidget): \"\"\"A basic plugin template for working with **single images**\"\"\" def __init__( self, viewer: \"napari.viewer.Viewer\", parent=None, loads_images=True, loads_labels=True, has_results=True, ): \"\"\" Creates a Base plugin with several buttons pre-defined Args: viewer: napari viewer to display in parent: parent QWidget. Defaults to None loads_images: whether to show image IO widgets loads_labels: whether to show labels IO widgets has_results: whether to show results IO widgets \"\"\" super().__init__(parent) \"\"\"Parent widget\"\"\" self._viewer=viewer \"\"\"napari.viewer.Viewer: viewer in which the widget is displayed\"\"\" self.docked_widgets=[] self.container_docked=False self.image_path=None \"\"\"str: path to image folder\"\"\" self.show_image_io=loads_images self.label_path=None \"\"\"str: path to label folder\"\"\" self.show_label_io=loads_labels self.results_path=None \"\"\"str: path to results folder\"\"\" self.show_results_io=has_results self._default_path=[self.image_path, self.label_path] self.layer_choice=ui.RadioButton(\"Layer\", parent=self) self.folder_choice=ui.RadioButton(\"Folder\", parent=self) self.image_filewidget=ui.FilePathWidget( \"Image path\", self.show_dialog_images, self ) self.image_layer_loader=ui.LayerSelecter( self._viewer, name=\"Image:\", layer_type=napari.layers.Image, parent=self, ) self.labels_filewidget=ui.FilePathWidget( \"Label path\", self.show_dialog_labels, parent=self ) self.label_layer_loader=ui.LayerSelecter( self._viewer, name=\"Labels:\", layer_type=napari.layers.Labels, parent=self, ) self.results_filewidget=ui.FilePathWidget( \"Saving path\", self.load_results_path, parent=self ) self.filetype_choice=ui.DropdownMenu( [\".tif\", \".tiff\"], label=\"File format\" ) qInstallMessageHandler(handle_adjust_errors_wrapper(self)) def enable_utils_menu(self): \"\"\" Enables the usage of the CTRL+right-click shortcut to the utilities. Should only be used in \"high-level\" widgets(provided in napari Plugins menu) to avoid multiple activation \"\"\" viewer=self._viewer @viewer.mouse_drag_callbacks.append def show_menu(_, event): return UtilsDropdown().dropdown_menu_call(self, event) def _build_io_panel(self): io_panel=ui.GroupedWidget(\"Data\") radio_buttons=ui.combine_blocks( self.folder_choice, self.layer_choice ) ui.add_widgets( io_panel.layout, [ radio_buttons, self.image_layer_loader, self.label_layer_loader, self.filetype_choice, self.image_filewidget, self.labels_filewidget, self.results_filewidget, ], ) io_panel.setLayout(io_panel.layout) return io_panel def _remove_unused(self): if not self.show_label_io: self.labels_filewidget=None self.label_layer_loader=None if not self.show_image_io: self.image_layer_loader=None self.image_filewidget=None if not self.show_results_io: self.results_filewidget=None def set_io_visibility(self): if self.show_image_io: self._show_io_element(self.image_layer_loader, self.layer_choice) else: self._hide_io_element(self.image_layer_loader) if self.show_label_io: self._show_io_element(self.label_layer_loader, self.layer_choice) else: self._hide_io_element(self.label_layer_loader) f=self.folder_choice self._show_io_element(self.filetype_choice, f) if self.show_image_io: self._show_io_element(self.image_filewidget, f) else: self._hide_io_element(self.image_filewidget) if self.show_label_io: self._show_io_element(self.labels_filewidget, f) else: self._hide_io_element(self.labels_filewidget) if not self.show_results_io: self._hide_io_element(self.results_filewidget) self.folder_choice.toggle() self.layer_choice.toggle() @staticmethod def _show_io_element(widget: QWidget, toggle: QWidget=None): \"\"\" Args: widget: Widget to be shown or hidden toggle: Toggle to be used to determine whether widget should be shown(Checkbox or RadioButton) \"\"\" widget.setVisible(True) if toggle is not None: toggle.toggled.connect( partial(ui.toggle_visibility, toggle, widget) ) @staticmethod def _hide_io_element(widget: QWidget, toggle: QWidget=None): \"\"\" Attempts to disconnect widget from toggle and hide it. Args: widget: Widget to be hidden toggle: Toggle to be disconnected from widget, if any \"\"\" if toggle is not None: try: toggle.toggled.disconnect() except TypeError: logger.warning( \"Warning: no method was found to disconnect from widget visibility\" ) widget.setVisible(False) def build(self): \"\"\"Method to be defined by children classes\"\"\" raise NotImplementedError(\"To be defined in child classes\") def show_filetype_choice(self): \"\"\"Method to show\/hide the filetype choice when \"loading as folder\" is(de)selected\"\"\" show=self.load_as_stack_choice.isChecked() if show is not None: self.filetype_choice.setVisible(show) def show_file_dialog(self): \"\"\"Open file dialog and process path depending on single file\/folder loading behaviour\"\"\" if self.load_as_stack_choice.isChecked(): folder=ui.open_folder_dialog( self, self._default_path, filetype=f\"Image file(*{self.filetype_choice.currentText()})\", ) return folder else: f_name=ui.open_file_dialog(self, self._default_path) f_name=str(f_name[0]) self.filetype=str(Path(f_name).suffix) return f_name def show_dialog_images(self): \"\"\"Show file dialog and set image path\"\"\" f_name=self.show_file_dialog() if type(f_name) is str and f_name !=\"\": self.image_path=f_name self.image_filewidget.text_field.setText(self.image_path) self.update_default() def show_dialog_labels(self): \"\"\"Show file dialog and set label path\"\"\" f_name=self.show_file_dialog() if isinstance(f_name, str) and f_name !=\"\": self.label_path=f_name self.labels_filewidget.text_field.setText(self.label_path) self.update_default() def check_results_path(self, folder): if folder !=\"\" and isinstance(folder, str): if not Path(folder).is_dir(): Path(folder).mkdir(parents=True, exist_ok=True) if not Path(folder).is_dir(): return False logger.info(f\"Created missing results folder:{folder}\") return True return False def load_results_path(self): \"\"\"Show file dialog to set:py:attr:`~results_path`\"\"\" folder=ui.open_folder_dialog(self, self._default_path) if self.check_results_path(folder): self.results_path=folder self.results_filewidget.text_field.setText(self.results_path) self.update_default() def update_default(self): \"\"\"Updates default path for smoother navigation when opening file dialogs\"\"\" self._default_path=[ self.image_path, self.label_path, self.results_path, ] def make_close_button(self): btn=ui.Button(\"Close\", self.remove_from_viewer) btn.setToolTip( \"Close the window and all docked widgets. Make sure to save your work !\" ) return btn def make_prev_button(self): btn=ui.Button( \"Previous\", lambda: self.setCurrentIndex(self.currentIndex() -1) ) return btn def make_next_button(self): btn=ui.Button( \"Next\", lambda: self.setCurrentIndex(self.currentIndex() +1) ) return btn def remove_from_viewer(self): \"\"\"Removes the widget from the napari window. Can be re-implemented in children classes if needed\"\"\" self.remove_docked_widgets() self._viewer.window.remove_dock_widget(self) def remove_docked_widgets(self): \"\"\"Removes all docked widgets from napari window\"\"\" try: if len(self.docked_widgets) !=0: [ self._viewer.window.remove_dock_widget(w) for w in self.docked_widgets if w is not None ] self.docked_widgets=[] self.container_docked=False return True except LookupError: return False class BasePluginFolder(BasePluginSingleImage): \"\"\"A basic plugin template for working with **folders of images**\"\"\" def __init__( self, viewer: \"napari.viewer.Viewer\", parent=None, loads_images=True, loads_labels=True, has_results=True, ): \"\"\"Creates a plugin template with the following widgets defined but not added in a layout: * A button to load a folder of images * A button to load a folder of labels * A button to set a results folder * A dropdown menu to select the file extension to be loaded from the folders\"\"\" super().__init__( viewer, parent, loads_images, loads_labels, has_results ) self.images_filepaths=[] \"\"\"array(str): paths to images for training or inference\"\"\" self.labels_filepaths=[] \"\"\"array(str): paths to labels for training\"\"\" self.results_path=None \"\"\"str: path to output folder,to save results in\"\"\" self._default_folders=[None] \"\"\"Update defaults from PluginBaseFolder with model_path\"\"\" self.docked_widgets=[] \"\"\"List of docked widgets(returned by:py:func:`viewer.window.add_dock_widget())`, can be used to remove docked widgets\"\"\" self.image_filewidget.text_field=\"Images directory\" self.image_filewidget.button.clicked.disconnect( self.show_dialog_images ) self.image_filewidget.button.clicked.connect(self.load_image_dataset) self.labels_filewidget.text_field=\"Labels directory\" self.labels_filewidget.button.clicked.disconnect( self.show_dialog_labels ) self.labels_filewidget.button.clicked.connect(self.load_label_dataset) \"\"\"Allows to choose which file will be loaded from folder\"\"\" def load_dataset_paths(self): \"\"\"Loads all image paths(as str) in a given folder for which the extension matches the set filetype Returns: array(str): all loaded file paths \"\"\" filetype=self.filetype_choice.currentText() directory=ui.open_folder_dialog(self, self._default_folders) file_paths=sorted(Path(directory).glob(\"*\" +filetype)) if len(file_paths)==0: warnings.warn( f\"The folder does not contain any compatible{filetype} files.\\n\" f\"Please check the validity of the folder and images.\" ) return file_paths def load_image_dataset(self): \"\"\"Show file dialog to set:py:attr:`~images_filepaths`\"\"\" filenames=self.load_dataset_paths() if filenames: self.images_filepaths=sorted(filenames) path=str(Path(filenames[0]).parent) self.image_filewidget.text_field.setText(path) self.image_filewidget.check_ready() self.update_default() def load_label_dataset(self): \"\"\"Show file dialog to set:py:attr:`~labels_filepaths`\"\"\" filenames=self.load_dataset_paths() if filenames: self.labels_filepaths=sorted(filenames) path=str(Path(filenames[0]).parent) self.labels_filewidget.text_field.setText(path) self.labels_filewidget.check_ready() self.update_default() def update_default(self): \"\"\"Update default path for smoother file dialogs\"\"\" if len(self.images_filepaths) !=0: from_images=str(Path(self.images_filepaths[0]).parent) else: from_images=None if len(self.labels_filepaths) !=0: from_labels=str(Path(self.labels_filepaths[0]).parent) else: from_labels=None self._default_folders=[ path for path in[ from_images, from_labels, self.results_path, ] if(path !=[] and path is not None) ] ","sourceWithComments":"import logging\nimport warnings\nfrom functools import partial\nfrom pathlib import Path\n\nimport napari\n\n# Qt\nfrom qtpy.QtCore import qInstallMessageHandler\nfrom qtpy.QtWidgets import QTabWidget\nfrom qtpy.QtWidgets import QWidget\n\n# local\nfrom napari_cellseg3d import interface as ui\nfrom napari_cellseg3d.interface_utils import handle_adjust_errors_wrapper\nfrom napari_cellseg3d.interface_utils import UtilsDropdown\n\nlogger = logging.getLogger(__name__)\n\n\nclass BasePluginSingleImage(QTabWidget):\n    \"\"\"A basic plugin template for working with **single images**\"\"\"\n\n    def __init__(\n        self,\n        viewer: \"napari.viewer.Viewer\",\n        parent=None,\n        loads_images=True,\n        loads_labels=True,\n        has_results=True,\n    ):\n        \"\"\"\n        Creates a Base plugin with several buttons pre-defined\n        Args:\n            viewer: napari viewer to display in\n            parent: parent QWidget. Defaults to None\n            loads_images: whether to show image IO widgets\n            loads_labels: whether to show labels IO widgets\n            has_results: whether to show results IO widgets\n        \"\"\"\n        super().__init__(parent)\n        \"\"\"Parent widget\"\"\"\n        self._viewer = viewer\n        \"\"\"napari.viewer.Viewer: viewer in which the widget is displayed\"\"\"\n\n        self.docked_widgets = []\n        self.container_docked = False\n\n        self.image_path = None\n        \"\"\"str: path to image folder\"\"\"\n        self.show_image_io = loads_images\n\n        self.label_path = None\n        \"\"\"str: path to label folder\"\"\"\n        self.show_label_io = loads_labels\n\n        self.results_path = None\n        \"\"\"str: path to results folder\"\"\"\n        self.show_results_io = has_results\n\n        self._default_path = [self.image_path, self.label_path]\n\n        ################\n        self.layer_choice = ui.RadioButton(\"Layer\", parent=self)\n        self.folder_choice = ui.RadioButton(\"Folder\", parent=self)\n        ################\n        # Image widgets\n        self.image_filewidget = ui.FilePathWidget(\n            \"Image path\", self.show_dialog_images, self\n        )\n\n        self.image_layer_loader = ui.LayerSelecter(\n            self._viewer,\n            name=\"Image :\",\n            layer_type=napari.layers.Image,\n            parent=self,\n        )\n        ################\n        # Label widgets\n        self.labels_filewidget = ui.FilePathWidget(\n            \"Label path\", self.show_dialog_labels, parent=self\n        )\n\n        self.label_layer_loader = ui.LayerSelecter(\n            self._viewer,\n            name=\"Labels :\",\n            layer_type=napari.layers.Labels,\n            parent=self,\n        )\n        ################\n        # Results widget\n        self.results_filewidget = ui.FilePathWidget(\n            \"Saving path\", self.load_results_path, parent=self\n        )\n\n        self.filetype_choice = ui.DropdownMenu(\n            [\".tif\", \".tiff\"], label=\"File format\"\n        )\n        ########\n        qInstallMessageHandler(handle_adjust_errors_wrapper(self))\n\n    def enable_utils_menu(self):\n        \"\"\"\n        Enables the usage of the CTRL+right-click shortcut to the utilities.\n        Should only be used in \"high-level\" widgets (provided in napari Plugins menu) to avoid multiple activation\n        \"\"\"\n        viewer = self._viewer\n\n        @viewer.mouse_drag_callbacks.append\n        def show_menu(_, event):\n            return UtilsDropdown().dropdown_menu_call(self, event)\n\n    def _build_io_panel(self):\n        io_panel = ui.GroupedWidget(\"Data\")\n\n        # io_panel.setToolTip(\"IO Panel\")\n        radio_buttons = ui.combine_blocks(\n            self.folder_choice, self.layer_choice\n        )\n\n        ui.add_widgets(\n            io_panel.layout,\n            [\n                radio_buttons,\n                self.image_layer_loader,\n                self.label_layer_loader,\n                self.filetype_choice,\n                self.image_filewidget,\n                self.labels_filewidget,\n                self.results_filewidget,\n            ],\n        )\n        io_panel.setLayout(io_panel.layout)\n\n        # self._set_io_visibility()\n        return io_panel\n\n    def _remove_unused(self):\n        if not self.show_label_io:\n            self.labels_filewidget = None\n            self.label_layer_loader = None\n\n        if not self.show_image_io:\n            self.image_layer_loader = None\n            self.image_filewidget = None\n\n        if not self.show_results_io:\n            self.results_filewidget = None\n\n    def set_io_visibility(self):\n        ##################\n        # Show when layer is selected\n        if self.show_image_io:\n            self._show_io_element(self.image_layer_loader, self.layer_choice)\n        else:\n            self._hide_io_element(self.image_layer_loader)\n        if self.show_label_io:\n            self._show_io_element(self.label_layer_loader, self.layer_choice)\n        else:\n            self._hide_io_element(self.label_layer_loader)\n\n        ##################\n        # Show when folder is selected\n        f = self.folder_choice\n\n        self._show_io_element(self.filetype_choice, f)\n        if self.show_image_io:\n            self._show_io_element(self.image_filewidget, f)\n        else:\n            self._hide_io_element(self.image_filewidget)\n        if self.show_label_io:\n            self._show_io_element(self.labels_filewidget, f)\n        else:\n            self._hide_io_element(self.labels_filewidget)\n        if not self.show_results_io:\n            self._hide_io_element(self.results_filewidget)\n\n        self.folder_choice.toggle()\n        self.layer_choice.toggle()\n\n    @staticmethod\n    def _show_io_element(widget: QWidget, toggle: QWidget = None):\n        \"\"\"\n        Args:\n            widget: Widget to be shown or hidden\n            toggle: Toggle to be used to determine whether widget should be shown (Checkbox or RadioButton)\n        \"\"\"\n        widget.setVisible(True)\n\n        if toggle is not None:\n            toggle.toggled.connect(\n                partial(ui.toggle_visibility, toggle, widget)\n            )\n\n    @staticmethod\n    def _hide_io_element(widget: QWidget, toggle: QWidget = None):\n        \"\"\"\n        Attempts to disconnect widget from toggle and hide it.\n        Args:\n            widget: Widget to be hidden\n            toggle: Toggle to be disconnected from widget, if any\n        \"\"\"\n\n        if toggle is not None:\n            try:\n                toggle.toggled.disconnect()\n            except TypeError:\n                logger.warning(\n                    \"Warning: no method was found to disconnect from widget visibility\"\n                )\n\n        widget.setVisible(False)\n\n    def build(self):\n        \"\"\"Method to be defined by children classes\"\"\"\n        raise NotImplementedError(\"To be defined in child classes\")\n\n    def show_filetype_choice(self):\n        \"\"\"Method to show\/hide the filetype choice when \"loading as folder\" is (de)selected\"\"\"\n        show = self.load_as_stack_choice.isChecked()\n        if show is not None:\n            self.filetype_choice.setVisible(show)\n            # self.lbl_ft.setVisible(show)\n\n    def show_file_dialog(self):\n        \"\"\"Open file dialog and process path depending on single file\/folder loading behaviour\"\"\"\n        if self.load_as_stack_choice.isChecked():\n            folder = ui.open_folder_dialog(\n                self,\n                self._default_path,\n                filetype=f\"Image file (*{self.filetype_choice.currentText()})\",\n            )\n            return folder\n        else:\n            f_name = ui.open_file_dialog(self, self._default_path)\n            f_name = str(f_name[0])\n            self.filetype = str(Path(f_name).suffix)\n            return f_name\n\n    def show_dialog_images(self):\n        \"\"\"Show file dialog and set image path\"\"\"\n        f_name = self.show_file_dialog()\n        if type(f_name) is str and f_name != \"\":\n            self.image_path = f_name\n            self.image_filewidget.text_field.setText(self.image_path)\n            self.update_default()\n\n    def show_dialog_labels(self):\n        \"\"\"Show file dialog and set label path\"\"\"\n        f_name = self.show_file_dialog()\n        if isinstance(f_name, str) and f_name != \"\":\n            self.label_path = f_name\n            self.labels_filewidget.text_field.setText(self.label_path)\n            self.update_default()\n\n    def check_results_path(self, folder):\n        if folder != \"\" and isinstance(folder, str):\n            if not Path(folder).is_dir():\n                Path(folder).mkdir(parents=True, exist_ok=True)\n                if not Path(folder).is_dir():\n                    return False\n                logger.info(f\"Created missing results folder : {folder}\")\n            return True\n        return False\n\n    def load_results_path(self):\n        \"\"\"Show file dialog to set :py:attr:`~results_path`\"\"\"\n        folder = ui.open_folder_dialog(self, self._default_path)\n\n        if self.check_results_path(folder):\n            self.results_path = folder\n            # logger.debug(f\"Results path : {self.results_path}\")\n            self.results_filewidget.text_field.setText(self.results_path)\n            self.update_default()\n\n    def update_default(self):\n        \"\"\"Updates default path for smoother navigation when opening file dialogs\"\"\"\n        self._default_path = [\n            self.image_path,\n            self.label_path,\n            self.results_path,\n        ]\n\n    def make_close_button(self):\n        btn = ui.Button(\"Close\", self.remove_from_viewer)\n        btn.setToolTip(\n            \"Close the window and all docked widgets. Make sure to save your work !\"\n        )\n        return btn\n\n    def make_prev_button(self):\n        btn = ui.Button(\n            \"Previous\", lambda: self.setCurrentIndex(self.currentIndex() - 1)\n        )\n        return btn\n\n    def make_next_button(self):\n        btn = ui.Button(\n            \"Next\", lambda: self.setCurrentIndex(self.currentIndex() + 1)\n        )\n        return btn\n\n    def remove_from_viewer(self):\n        \"\"\"Removes the widget from the napari window.\n        Can be re-implemented in children classes if needed\"\"\"\n\n        self.remove_docked_widgets()\n        self._viewer.window.remove_dock_widget(self)\n\n    def remove_docked_widgets(self):\n        \"\"\"Removes all docked widgets from napari window\"\"\"\n        try:\n            if len(self.docked_widgets) != 0:\n                [\n                    self._viewer.window.remove_dock_widget(w)\n                    for w in self.docked_widgets\n                    if w is not None\n                ]\n            self.docked_widgets = []\n            self.container_docked = False\n            return True\n        except LookupError:\n            return False\n\n\nclass BasePluginFolder(BasePluginSingleImage):\n    \"\"\"A basic plugin template for working with **folders of images**\"\"\"\n\n    def __init__(\n        self,\n        viewer: \"napari.viewer.Viewer\",\n        parent=None,\n        loads_images=True,\n        loads_labels=True,\n        has_results=True,\n    ):\n        \"\"\"Creates a plugin template with the following widgets defined but not added in a layout :\n\n        * A button to load a folder of images\n\n        * A button to load a folder of labels\n\n        * A button to set a results folder\n\n        * A dropdown menu to select the file extension to be loaded from the folders\"\"\"\n        super().__init__(\n            viewer, parent, loads_images, loads_labels, has_results\n        )\n\n        self.images_filepaths = []\n        \"\"\"array(str): paths to images for training or inference\"\"\"\n        self.labels_filepaths = []\n        \"\"\"array(str): paths to labels for training\"\"\"\n        self.results_path = None\n        \"\"\"str: path to output folder,to save results in\"\"\"\n\n        self._default_folders = [None]\n        \"\"\"Update defaults from PluginBaseFolder with model_path\"\"\"\n\n        self.docked_widgets = []\n        \"\"\"List of docked widgets (returned by :py:func:`viewer.window.add_dock_widget())`,\n        can be used to remove docked widgets\"\"\"\n\n        #######################################################\n        # interface\n        # self.image_filewidget = ui.FilePathWidget(\n        #     \"Images directory\", self.load_image_dataset, self\n        # )\n        self.image_filewidget.text_field = \"Images directory\"\n        self.image_filewidget.button.clicked.disconnect(\n            self.show_dialog_images\n        )\n        self.image_filewidget.button.clicked.connect(self.load_image_dataset)\n\n        # self.labels_filewidget = ui.FilePathWidget(\n        #     \"Labels directory\", self.load_label_dataset, self\n        # )\n        self.labels_filewidget.text_field = \"Labels directory\"\n        self.labels_filewidget.button.clicked.disconnect(\n            self.show_dialog_labels\n        )\n        self.labels_filewidget.button.clicked.connect(self.load_label_dataset)\n\n        # self.filetype_choice = ui.DropdownMenu(\n        #     [\".tif\", \".tiff\"], label=\"File format\"\n        # )\n        \"\"\"Allows to choose which file will be loaded from folder\"\"\"\n        #######################################################\n        # self._set_io_visibility()\n\n    def load_dataset_paths(self):\n        \"\"\"Loads all image paths (as str) in a given folder for which the extension matches the set filetype\n\n        Returns:\n           array(str): all loaded file paths\n        \"\"\"\n        filetype = self.filetype_choice.currentText()\n        directory = ui.open_folder_dialog(self, self._default_folders)\n\n        file_paths = sorted(Path(directory).glob(\"*\" + filetype))\n        if len(file_paths) == 0:\n            warnings.warn(\n                f\"The folder does not contain any compatible {filetype} files.\\n\"\n                f\"Please check the validity of the folder and images.\"\n            )\n\n        return file_paths\n\n    def load_image_dataset(self):\n        \"\"\"Show file dialog to set :py:attr:`~images_filepaths`\"\"\"\n        filenames = self.load_dataset_paths()\n\n        if filenames:\n            self.images_filepaths = sorted(filenames)\n\n            path = str(Path(filenames[0]).parent)\n            self.image_filewidget.text_field.setText(path)\n            self.image_filewidget.check_ready()\n            self.update_default()\n\n    def load_label_dataset(self):\n        \"\"\"Show file dialog to set :py:attr:`~labels_filepaths`\"\"\"\n        filenames = self.load_dataset_paths()\n        if filenames:\n            self.labels_filepaths = sorted(filenames)\n            path = str(Path(filenames[0]).parent)\n            self.labels_filewidget.text_field.setText(path)\n            self.labels_filewidget.check_ready()\n            self.update_default()\n\n    def update_default(self):\n        \"\"\"Update default path for smoother file dialogs\"\"\"\n        if len(self.images_filepaths) != 0:\n            from_images = str(Path(self.images_filepaths[0]).parent)\n        else:\n            from_images = None\n\n        if len(self.labels_filepaths) != 0:\n            from_labels = str(Path(self.labels_filepaths[0]).parent)\n        else:\n            from_labels = None\n\n        self._default_folders = [\n            path\n            for path in [\n                from_images,\n                from_labels,\n                self.results_path,\n            ]\n            if (path != [] and path is not None)\n        ]\n"},"\/napari_cellseg3d\/plugin_model_training.py":{"changes":[{"diff":"\n-import logging\n import shutil\n import warnings\n from functools import partial\n","add":0,"remove":1,"filename":"\/napari_cellseg3d\/plugin_model_training.py","badparts":["import logging"],"goodparts":[]},{"diff":"\n DEFAULT_PATCH_SIZE = 64\n \n-logger = logging.getLogger(__name__)\n+logger = utils.LOGGER\n \n \n class Trainer(ModelFramework, metaclass=QWidgetSingleton):\n","add":1,"remove":1,"filename":"\/napari_cellseg3d\/plugin_model_training.py","badparts":["logger = logging.getLogger(__name__)"],"goodparts":["logger = utils.LOGGER"]},{"diff":"\n                 learning_rate=float(self.learning_rate_choice.currentText()),\n                 validation_interval=self.val_interval_choice.value(),\n                 batch_size=self.batch_choice.slider_value,\n-                results_path_folder=results_path_folder,\n+                results_path_folder=str(results_path_folder),\n                 sampling=self.patch_choice.isChecked(),\n                 num_samples=self.sample_choice_slider.slider_value,\n                 sample_size=patch_size,\n                 do_augmentation=self.augment_choice.isChecked(),\n                 deterministic_config=deterministic_config,\n-            )  # FIXME continue\n+            )  # TODO(cyril) continue to put params in config\n \n             self.config = config.TrainerConfig(\n                 save_as_zip=self.zip_choice.isChecked()\n","add":2,"remove":2,"filename":"\/napari_cellseg3d\/plugin_model_training.py","badparts":["                results_path_folder=results_path_folder,","            )  # FIXME continue"],"goodparts":["                results_path_folder=str(results_path_folder),","            )  # TODO(cyril) continue to put params in config"]}]},"\/napari_cellseg3d\/plugin_review.py":{"changes":[{"diff":"\n         # self._show_io_element(self.results_filewidget)\n \n         self.results_filewidget.text_field.setText(\n-            str(Path.home() \/ Path(\"cellseg3d_review\"))\n+            str(Path.home() \/ Path(\"cellseg3d\/review\"))  # TODO(cyril) : check proper behaviour\n         )\n \n         csv_param_w.setLayout(csv","add":1,"remove":1,"filename":"\/napari_cellseg3d\/plugin_review.py","badparts":["            str(Path.home() \/ Path(\"cellseg3d_review\"))"],"goodparts":["            str(Path.home() \/ Path(\"cellseg3d\/review\"))  # TODO(cyril) : check proper behaviour"]}],"source":"\nimport warnings from pathlib import Path import napari from qtpy.QtCore import QObject from qtpy.QtWidgets import QLineEdit from qtpy.QtWidgets import QSizePolicy from napari_cellseg3d import config from napari_cellseg3d import interface as ui from napari_cellseg3d import utils from napari_cellseg3d.interface_utils import QWidgetSingleton from napari_cellseg3d.launch_review import launch_review from napari_cellseg3d.plugin_base import BasePluginSingleImage warnings.formatwarning=utils.format_Warning class Reviewer(BasePluginSingleImage, metaclass=QWidgetSingleton): \"\"\"A plugin for selecting volumes and labels file and launching the review process. Inherits from::doc:`plugin_base`\"\"\" def __init__(self, viewer: \"napari.viewer.Viewer\", parent=None): \"\"\"Creates a Reviewer plugin with several buttons: * Open file prompt to select volumes directory * Open file prompt to select labels directory * A dropdown menu with a choice of png or tif filetypes * A checkbox if you want to create a new status csv for the dataset * A button to launch the review process(see:doc:`launch_review`) \"\"\" super().__init__( viewer, parent, loads_images=True, loads_labels=True, has_results=True, ) self.config=config.ReviewConfig() self.enable_utils_menu() self.io_panel=self._build_io_panel() self.layer_choice.setText(\"New review\") self.folder_choice.setText(\"Existing review\") self.csv_textbox=QLineEdit(self) self.csv_textbox.setSizePolicy(QSizePolicy.Fixed, QSizePolicy.Fixed) self.new_csv_choice=ui.CheckBox(\"Create new dataset ?\") self.btn_start=ui.Button(\"Start reviewing\", self.run_review, self) self.lbl_mod=ui.make_label(\"Name\", self) self.warn_label=ui.make_label( \"WARNING: You already have a review session running.\\n\" \"Launching another will close the current one,\\n\" \" make sure to save your work beforehand\", None, ) self.anisotropy_widgets=ui.AnisotropyWidgets( self, default_x=1.5, default_y=1.5, default_z=5 ) self.csv_textbox.setToolTip(\"Name of the csv results file\") self.new_csv_choice.setToolTip( \"Ignore any pre-existing csv with the specified name and create a new one\" ) self.build() self.image_filewidget.text_field.textChanged.connect( self._update_results_path ) print(f\"{self}\") def _update_results_path(self): p=self.image_filewidget.text_field.text() if p is not None and p !=\"\" and Path(p).is_file(): self.results_filewidget.text_field.setText(str(Path(p).parent)) def build(self): \"\"\"Build buttons in a layout and add them to the napari Viewer\"\"\" self.setSizePolicy(QSizePolicy.Maximum, QSizePolicy.MinimumExpanding) tab=ui.ContainerWidget(0, 0, 1, 1) layout=tab.layout self.filetype_choice.setVisible(False) layout.addWidget(self.io_panel) self.set_io_visibility() self.layer_choice.toggle() ui.add_blank(self, layout) ui.GroupedWidget.create_single_widget_group( \"Image parameters\", self.anisotropy_widgets, layout ) ui.add_blank(self, layout) csv_param_w, csv_param_l=ui.make_group(\"CSV parameters\") ui.add_widgets( csv_param_l, [ ui.combine_blocks( self.csv_textbox, self.lbl_mod, horizontal=False, l=5, t=0, r=5, b=5, ), self.new_csv_choice, self.results_filewidget, ], ) self.results_filewidget.text_field.setText( str(Path.home() \/ Path(\"cellseg3d_review\")) ) csv_param_w.setLayout(csv_param_l) layout.addWidget(csv_param_w) ui.add_blank(self, layout) ui.add_widgets(layout,[self.btn_start, self.make_close_button()]) ui.ScrollArea.make_scrollable( contained_layout=layout, parent=tab, min_wh=[190, 300] ) self.addTab(tab, \"Review\") self.setMinimumSize(180, 100) self.results_filewidget.check_ready() self.results_path=self.results_filewidget.text_field.text() def check_image_data(self): cfg=self.config if cfg.image is None: raise ValueError(\"Review requires at least one image\") if cfg.labels is not None: if cfg.image.shape !=cfg.labels.shape: warnings.warn( \"Image and label dimensions do not match ! Please load matching images\" ) def prepare_data(self): if self.layer_choice.isChecked(): self.config.image=self.image_layer_loader.layer_data() self.config.labels=self.label_layer_loader.layer_data() else: self.config.image=utils.load_images( self.image_filewidget.text_field.text() ) self.config.labels=utils.load_images( self.labels_filewidget.text_field.text() ) self.check_image_data() self.check_results_path(self.results_filewidget.text_field.text()) self.config.csv_path=self.results_filewidget.text_field.text() self.config.model_name=self.csv_textbox.text() self.config.new_csv=self.new_csv_choice.isChecked() self.config.filetype=self.filetype_choice.currentText() if self.anisotropy_widgets.enabled: zoom=self.anisotropy_widgets.scaling_zyx() else: zoom=[1, 1, 1] self.config.zoom_factor=zoom def run_review(self): \"\"\"Launches review process by loading the files from the chosen folders, and adds several widgets to the napari Viewer. If the review process has been launched once before, closes the window entirely and launches the review process in a fresh window. TODO: * Save work done before leaving See:doc:`launch_review` Returns: napari.viewer.Viewer: self.viewer \"\"\" print(\"New review session\\n\" +\"*\" * 20) previous_viewer=self._viewer try: self.prepare_data() self._viewer, self.docked_widgets=launch_review( review_config=self.config ) self.reset() previous_viewer.close() except ValueError as e: warnings.warn( f\"An exception occurred:{e}. Please ensure you have entered all required parameters.\" ) def reset(self): self.remove_docked_widgets() ","sourceWithComments":"import warnings\nfrom pathlib import Path\n\nimport napari\n\n# Qt\nfrom qtpy.QtCore import QObject\nfrom qtpy.QtWidgets import QLineEdit\nfrom qtpy.QtWidgets import QSizePolicy\n\n# local\nfrom napari_cellseg3d import config\nfrom napari_cellseg3d import interface as ui\nfrom napari_cellseg3d import utils\nfrom napari_cellseg3d.interface_utils import QWidgetSingleton\nfrom napari_cellseg3d.launch_review import launch_review\nfrom napari_cellseg3d.plugin_base import BasePluginSingleImage\n\nwarnings.formatwarning = utils.format_Warning\n\n\nclass Reviewer(BasePluginSingleImage, metaclass=QWidgetSingleton):\n    \"\"\"A plugin for selecting volumes and labels file and launching the review process.\n    Inherits from : :doc:`plugin_base`\"\"\"\n\n    def __init__(self, viewer: \"napari.viewer.Viewer\", parent=None):\n        \"\"\"Creates a Reviewer plugin with several buttons :\n\n        * Open file prompt to select volumes directory\n\n        * Open file prompt to select labels directory\n\n        * A dropdown menu with a choice of png or tif filetypes\n\n        * A checkbox if you want to create a new status csv for the dataset\n\n        * A button to launch the review process (see :doc:`launch_review`)\n        \"\"\"\n\n        super().__init__(\n            viewer,\n            parent,\n            loads_images=True,\n            loads_labels=True,\n            has_results=True,\n        )\n\n        # self._viewer = viewer # should not be needed\n        self.config = config.ReviewConfig()\n        self.enable_utils_menu()\n\n        #######################\n        # UI\n        self.io_panel = self._build_io_panel()\n\n        self.layer_choice.setText(\"New review\")\n        self.folder_choice.setText(\"Existing review\")\n\n        self.csv_textbox = QLineEdit(self)\n        self.csv_textbox.setSizePolicy(QSizePolicy.Fixed, QSizePolicy.Fixed)\n\n        self.new_csv_choice = ui.CheckBox(\"Create new dataset ?\")\n\n        self.btn_start = ui.Button(\"Start reviewing\", self.run_review, self)\n\n        self.lbl_mod = ui.make_label(\"Name\", self)\n\n        self.warn_label = ui.make_label(\n            \"WARNING : You already have a review session running.\\n\"\n            \"Launching another will close the current one,\\n\"\n            \" make sure to save your work beforehand\",\n            None,\n        )\n\n        self.anisotropy_widgets = ui.AnisotropyWidgets(\n            self, default_x=1.5, default_y=1.5, default_z=5\n        )\n\n        ###########################\n        # tooltips\n        self.csv_textbox.setToolTip(\"Name of the csv results file\")\n        self.new_csv_choice.setToolTip(\n            \"Ignore any pre-existing csv with the specified name and create a new one\"\n        )\n        ###########################\n\n        self.build()\n\n        self.image_filewidget.text_field.textChanged.connect(\n            self._update_results_path\n        )\n        print(f\"{self}\")\n\n    def _update_results_path(self):\n        p = self.image_filewidget.text_field.text()\n        if p is not None and p != \"\" and Path(p).is_file():\n            self.results_filewidget.text_field.setText(str(Path(p).parent))\n\n    def build(self):\n        \"\"\"Build buttons in a layout and add them to the napari Viewer\"\"\"\n\n        self.setSizePolicy(QSizePolicy.Maximum, QSizePolicy.MinimumExpanding)\n\n        tab = ui.ContainerWidget(0, 0, 1, 1)\n        layout = tab.layout\n\n        # ui.add_blank(self, layout)\n        ###########################\n        self.filetype_choice.setVisible(False)\n        layout.addWidget(self.io_panel)\n        self.set_io_visibility()\n        self.layer_choice.toggle()\n        ###########################\n        ui.add_blank(self, layout)\n        ###########################\n        ui.GroupedWidget.create_single_widget_group(\n            \"Image parameters\", self.anisotropy_widgets, layout\n        )\n        ###########################\n        ui.add_blank(self, layout)\n        ###########################\n        csv_param_w, csv_param_l = ui.make_group(\"CSV parameters\")\n\n        ui.add_widgets(\n            csv_param_l,\n            [\n                ui.combine_blocks(\n                    self.csv_textbox,\n                    self.lbl_mod,\n                    horizontal=False,\n                    l=5,\n                    t=0,\n                    r=5,\n                    b=5,\n                ),\n                self.new_csv_choice,\n                self.results_filewidget,\n            ],\n        )\n\n        # self._hide_io_element(self.results_filewidget, self.folder_choice)\n        # self._show_io_element(self.results_filewidget)\n\n        self.results_filewidget.text_field.setText(\n            str(Path.home() \/ Path(\"cellseg3d_review\"))\n        )\n\n        csv_param_w.setLayout(csv_param_l)\n        layout.addWidget(csv_param_w)\n        ###########################\n        ui.add_blank(self, layout)\n        ###########################\n\n        ui.add_widgets(layout, [self.btn_start, self.make_close_button()])\n\n        ui.ScrollArea.make_scrollable(\n            contained_layout=layout, parent=tab, min_wh=[190, 300]\n        )\n\n        self.addTab(tab, \"Review\")\n\n        self.setMinimumSize(180, 100)\n        # self.show()\n        # self._viewer.window.add_dock_widget(self, name=\"Reviewer\", area=\"right\")\n        self.results_filewidget.check_ready()\n        self.results_path = self.results_filewidget.text_field.text()\n\n    def check_image_data(self):\n        cfg = self.config\n\n        if cfg.image is None:\n            raise ValueError(\"Review requires at least one image\")\n\n        if cfg.labels is not None:\n            if cfg.image.shape != cfg.labels.shape:\n                warnings.warn(\n                    \"Image and label dimensions do not match ! Please load matching images\"\n                )\n\n    def prepare_data(self):\n\n        if self.layer_choice.isChecked():\n            self.config.image = self.image_layer_loader.layer_data()\n            self.config.labels = self.label_layer_loader.layer_data()\n        else:\n            self.config.image = utils.load_images(\n                self.image_filewidget.text_field.text()\n            )\n            self.config.labels = utils.load_images(\n                self.labels_filewidget.text_field.text()\n            )\n\n        self.check_image_data()\n        self.check_results_path(self.results_filewidget.text_field.text())\n\n        self.config.csv_path = self.results_filewidget.text_field.text()\n        self.config.model_name = self.csv_textbox.text()\n\n        self.config.new_csv = self.new_csv_choice.isChecked()\n        self.config.filetype = self.filetype_choice.currentText()\n\n        if self.anisotropy_widgets.enabled:\n            zoom = self.anisotropy_widgets.scaling_zyx()\n        else:\n            zoom = [1, 1, 1]\n        self.config.zoom_factor = zoom\n\n    def run_review(self):\n\n        \"\"\"Launches review process by loading the files from the chosen folders,\n        and adds several widgets to the napari Viewer.\n        If the review process has been launched once before,\n        closes the window entirely and launches the review process in a fresh window.\n\n        TODO:\n\n        * Save work done before leaving\n\n        See :doc:`launch_review`\n\n        Returns:\n            napari.viewer.Viewer: self.viewer\n        \"\"\"\n\n        print(\"New review session\\n\" + \"*\" * 20)\n        previous_viewer = self._viewer\n        try:\n\n            self.prepare_data()\n\n            self._viewer, self.docked_widgets = launch_review(\n                review_config=self.config\n            )\n            self.reset()\n            previous_viewer.close()\n        except ValueError as e:\n            warnings.warn(\n                f\"An exception occurred : {e}. Please ensure you have entered all required parameters.\"\n            )\n\n    def reset(self):\n        self.remove_docked_widgets()\n"}},"msg":"Many fixes\n\n- replaced logger definition, now in utils\n- Removed close button on many dock widgets that should not be tampered with\n- Fixed issues with using only one file for training. Validation and test sets are now split within image samples\n- Stricter checks for Path and str in path loading\n- Updated default review path"}},"https:\/\/github.com\/pyside\/pyside-setup":{"c8b8d0868bafe25e11f48ddc7a928ca1d47879ae":{"url":"https:\/\/api.github.com\/repos\/pyside\/pyside-setup\/commits\/c8b8d0868bafe25e11f48ddc7a928ca1d47879ae","html_url":"https:\/\/github.com\/pyside\/pyside-setup\/commit\/c8b8d0868bafe25e11f48ddc7a928ca1d47879ae","sha":"c8b8d0868bafe25e11f48ddc7a928ca1d47879ae","keyword":"tampering fix","diff":"diff --git a\/sources\/shiboken6\/libshiboken\/embed\/signature_bootstrap.py b\/sources\/shiboken6\/libshiboken\/embed\/signature_bootstrap.py\nindex c11a0367a..72c042b46 100644\n--- a\/sources\/shiboken6\/libshiboken\/embed\/signature_bootstrap.py\n+++ b\/sources\/shiboken6\/libshiboken\/embed\/signature_bootstrap.py\n@@ -26,6 +26,7 @@\n import base64\n import importlib\n import io\n+import os\n import sys\n import traceback\n import zipfile\n@@ -71,11 +72,44 @@ def ensure_shibokensupport(target, support_path):\n             sys.exit(-1)\n         target.remove(support_path)\n \n-    target, support_path = prepare_zipfile()\n+    # Here we decide if re we-incarnate the embedded files or use embedding.\n+    incarnated = re_incarnate_files()\n+    if incarnated:\n+        target, support_path = sys.path, os.fspath(incarnated)\n+    else:\n+        target, support_path = prepare_zipfile()\n+    # PYSIDE-962: pre-load needed after re_incarnate_files [Windows, Py3.7.9]\n+    ensure_shibokensupport(target, support_path)\n     with ensure_shibokensupport(target, support_path):\n         from shibokensupport.signature import loader\n     return loader\n \n+# Newer functionality:\n+# This function checks if the support directory exist and returns it.\n+# If does not exist, we try to create it and return it.\n+# Otherwise, we return None.\n+\n+def re_incarnate_files():\n+    import shiboken6 as root\n+    files_dir = Path(root.__file__).resolve().parent \/ \"files.dir\"\n+    if files_dir.exists():\n+        return files_dir\n+\n+    target, zip = prepare_zipfile()\n+    names = (_ for _ in zip.zfile.namelist() if _.endswith(\".py\"))\n+    try:\n+        # first check mkdir to get an error when we cannot write.\n+        files_dir.mkdir()\n+    except os.error:\n+        return None\n+    try:\n+        # Then check for a real error when unpacking the zip file.\n+        zip.zfile.extractall(path=files_dir, members=names)\n+        return files_dir\n+    except Exception as e:\n+        print('Exception:', e)\n+        traceback.print_exc(file=sys.stdout)\n+        raise\n \n # New functionality: Loading from a zip archive.\n # There exists the zip importer, but as it is written, only real zip files are\n","message":"","files":{"\/sources\/shiboken6\/libshiboken\/embed\/signature_bootstrap.py":{"changes":[{"diff":"\n             sys.exit(-1)\n         target.remove(support_path)\n \n-    target, support_path = prepare_zipfile()\n+    # Here we decide if re we-incarnate the embedded files or use embedding.\n+    incarnated = re_incarnate_files()\n+    if incarnated:\n+        target, support_path = sys.path, os.fspath(incarnated)\n+    else:\n+        target, support_path = prepare_zipfile()\n+    # PYSIDE-962: pre-load needed after re_incarnate_files [Windows, Py3.7.9]\n+    ensure_shibokensupport(target, support_path)\n     with ensure_shibokensupport(target, support_path):\n         from shibokensupport.signature import loader\n     return loader\n \n+# Newer functionality:\n+# This function checks if the support directory exist and returns it.\n+# If does not exist, we try to create it and return it.\n+# Otherwise, we return None.\n+\n+def re_incarnate_files():\n+    import shiboken6 as root\n+    files_dir = Path(root.__file__).resolve().parent \/ \"files.dir\"\n+    if files_dir.exists():\n+        return files_dir\n+\n+    target, zip = prepare_zipfile()\n+    names = (_ for _ in zip.zfile.namelist() if _.endswith(\".py\"))\n+    try:\n+        # first check mkdir to get an error when we cannot write.\n+        files_dir.mkdir()\n+    except os.error:\n+        return None\n+    try:\n+        # Then check for a real error when unpacking the zip file.\n+        zip.zfile.extractall(path=files_dir, members=names)\n+        return files_dir\n+    except Exception as e:\n+        print('Exception:', e)\n+        traceback.print_exc(file=sys.stdout)\n+        raise\n \n # New functionality: Loading from a zip archive.\n # There exists the zip importer, but as it is written, only real zip files are\n","add":34,"remove":1,"filename":"\/sources\/shiboken6\/libshiboken\/embed\/signature_bootstrap.py","badparts":["    target, support_path = prepare_zipfile()"],"goodparts":["    incarnated = re_incarnate_files()","    if incarnated:","        target, support_path = sys.path, os.fspath(incarnated)","    else:","        target, support_path = prepare_zipfile()","    ensure_shibokensupport(target, support_path)","def re_incarnate_files():","    import shiboken6 as root","    files_dir = Path(root.__file__).resolve().parent \/ \"files.dir\"","    if files_dir.exists():","        return files_dir","    target, zip = prepare_zipfile()","    names = (_ for _ in zip.zfile.namelist() if _.endswith(\".py\"))","    try:","        files_dir.mkdir()","    except os.error:","        return None","    try:","        zip.zfile.extractall(path=files_dir, members=names)","        return files_dir","    except Exception as e:","        print('Exception:', e)","        traceback.print_exc(file=sys.stdout)","        raise"]}],"source":"\n \"\"\" signature_bootstrap.py ---------------------- This file was originally directly embedded into the C source. After it grew more and more, I now prefer to have it as Python file. Meanwhile, there is also no more a stub loader necessary: Because we meanwhile have embedding support, we could also load this file directly from a.pyc file. This file replaces the hard to read Python stub in 'signature.cpp', and we could distinguish better between bootstrap related functions and loader functions. It is embedded into 'signature.cpp' as \"embed\/signature_bootstrap.inc\". imports were in the functions. Moved them outside into the globals. \"\"\" recursion_trap=0 import base64 import importlib import io import sys import traceback import zipfile from contextlib import contextmanager from importlib.machinery import ModuleSpec from pathlib import Path def bootstrap(): global recursion_trap if recursion_trap: print(\"Recursion occurred in Bootstrap. Did you start by hand? Then it's ok.\") print(\"But you should trigger start by '_init_pyside_extension()', only!\") recursion_trap +=1 @contextmanager def ensure_shibokensupport(target, support_path): target.insert(0, support_path) sbks=\"shibokensupport\" if sbks in sys.modules: del sys.modules[sbks] prefix=sbks +\".\" for key in list(key for key in sys.modules if key.startswith(prefix)): del sys.modules[key] try: import shibokensupport yield except Exception as e: print(\"Problem importing shibokensupport:\") print(f\"{e.__class__.__name__}:{e}\") traceback.print_exc() print(\"sys.path:\") for p in sys.path: print(\" \" +p) sys.stdout.flush() sys.exit(-1) target.remove(support_path) target, support_path=prepare_zipfile() with ensure_shibokensupport(target, support_path): from shibokensupport.signature import loader return loader def prepare_zipfile(): \"\"\" Old approach: Write the zip file to a real file and return its name. It will be implicitly opened as such when we add the name to sys.path. New approach(Python 3, only): Use EmbeddableZipImporter and pass the zipfile structure directly. The sys.path way does not work, instead we need to use sys.meta_path. See https:\/\/docs.python.org\/3\/library\/sys.html \"\"\" zipbytes=base64.b64decode(''.join(zipstring_sequence)) vzip=zipfile.ZipFile(io.BytesIO(zipbytes)) return sys.meta_path, EmbeddableZipImporter(vzip) class EmbeddableZipImporter(object): def __init__(self, zip_file): def p2m(filename): if filename.endswith(\"\/__init__.py\"): return filename[:-12].replace(\"\/\", \".\") if filename.endswith(\".py\"): return filename[:-3].replace(\"\/\", \".\") return None self.zfile=zip_file self._mod2path={p2m(_.filename): _.filename for _ in zip_file.filelist} def find_spec(self, fullname, path, target=None): path=self._mod2path.get(fullname) return ModuleSpec(fullname, self) if path else None def create_module(self, spec): return None def exec_module(self, module): fullname=module.__spec__.name filename=self._mod2path[fullname] with self.zfile.open(filename, \"r\") as f: codeob=compile(f.read(), filename, \"exec\") exec(codeob, module.__dict__) module.__file__=filename module.__loader__=self if filename.endswith(\"\/__init__.py\"): module.__path__=[] module.__package__=fullname else: module.__package__=fullname.rpartition('.')[0] sys.modules[fullname]=module ","sourceWithComments":"# Copyright (C) 2022 The Qt Company Ltd.\n# SPDX-License-Identifier: LicenseRef-Qt-Commercial OR LGPL-3.0-only OR GPL-2.0-only OR GPL-3.0-only\n\n\"\"\"\nsignature_bootstrap.py\n----------------------\n\nThis file was originally directly embedded into the C source.\nAfter it grew more and more, I now prefer to have it as Python file.\n\nMeanwhile, there is also no more a stub loader necessary:\nBecause we meanwhile have embedding support, we could also load this file\ndirectly from a .pyc file.\n\nThis file replaces the hard to read Python stub in 'signature.cpp', and we\ncould distinguish better between bootstrap related functions and loader\nfunctions.\nIt is embedded into 'signature.cpp' as \"embed\/signature_bootstrap.inc\".\n\n# PYSIDE-1436: Python 3.10 had a problem with EmbeddableZipImporter because the\nimports were in the functions. Moved them outside into the globals.\n\"\"\"\n\nrecursion_trap = 0\n\nimport base64\nimport importlib\nimport io\nimport sys\nimport traceback\nimport zipfile\n\nfrom contextlib import contextmanager\nfrom importlib.machinery import ModuleSpec\nfrom pathlib import Path\n\n\ndef bootstrap():\n\n    global recursion_trap\n    if recursion_trap:\n        # we are probably called from outside, already\n        print(\"Recursion occurred in Bootstrap. Did you start by hand? Then it's ok.\")\n        print(\"But you should trigger start by '_init_pyside_extension()', only!\")\n    recursion_trap += 1\n\n    @contextmanager\n    def ensure_shibokensupport(target, support_path):\n        # Make sure that we always have the shibokensupport containing package first.\n        # Also remove any prior loaded module of this name, just in case.\n        # PYSIDE-1621: support_path can also be a finder instance.\n        target.insert(0, support_path)\n\n        sbks = \"shibokensupport\"\n        if sbks in sys.modules:\n            del sys.modules[sbks]\n        prefix = sbks + \".\"\n        for key in list(key for key in sys.modules if key.startswith(prefix)):\n            del sys.modules[key]\n        try:\n            import shibokensupport\n            yield\n        except Exception as e:\n            print(\"Problem importing shibokensupport:\")\n            print(f\"{e.__class__.__name__}: {e}\")\n            traceback.print_exc()\n            print(\"sys.path:\")\n            for p in sys.path:\n                print(\"  \" + p)\n            sys.stdout.flush()\n            sys.exit(-1)\n        target.remove(support_path)\n\n    target, support_path = prepare_zipfile()\n    with ensure_shibokensupport(target, support_path):\n        from shibokensupport.signature import loader\n    return loader\n\n\n# New functionality: Loading from a zip archive.\n# There exists the zip importer, but as it is written, only real zip files are\n# supported. Before I will start an own implementation, it is easiest to use\n# a temporary zip file.\n# PYSIDE-1621: make zip file access totally virtual\n\ndef prepare_zipfile():\n    \"\"\"\n    Old approach:\n\n    Write the zip file to a real file and return its name.\n    It will be implicitly opened as such when we add the name to sys.path .\n\n    New approach (Python 3, only):\n\n    Use EmbeddableZipImporter and pass the zipfile structure directly.\n    The sys.path way does not work, instead we need to use sys.meta_path .\n    See https:\/\/docs.python.org\/3\/library\/sys.html#sys.meta_path\n    \"\"\"\n\n    # 'zipstring_sequence' comes from signature.cpp\n    zipbytes = base64.b64decode(''.join(zipstring_sequence))\n    vzip = zipfile.ZipFile(io.BytesIO(zipbytes))\n    return sys.meta_path, EmbeddableZipImporter(vzip)\n\n\nclass EmbeddableZipImporter(object):\n\n    def __init__(self, zip_file):\n        def p2m(filename):\n            if filename.endswith(\"\/__init__.py\"):\n                return filename[:-12].replace(\"\/\", \".\")\n            if filename.endswith(\".py\"):\n                return filename[:-3].replace(\"\/\", \".\")\n            return None\n\n        self.zfile = zip_file\n        self._mod2path = {p2m(_.filename) : _.filename for _ in zip_file.filelist}\n\n    def find_spec(self, fullname, path, target=None):\n        path = self._mod2path.get(fullname)\n        return ModuleSpec(fullname, self) if path else None\n\n    def create_module(self, spec):\n        return None\n\n    def exec_module(self, module):\n        fullname = module.__spec__.name\n        filename = self._mod2path[fullname]\n        with self.zfile.open(filename, \"r\") as f:   # \"rb\" not for zipfile\n            codeob = compile(f.read(), filename, \"exec\")\n            exec(codeob, module.__dict__)\n        module.__file__ = filename\n        module.__loader__ = self\n        if filename.endswith(\"\/__init__.py\"):\n            module.__path__ = []\n            module.__package__ = fullname\n        else:\n            module.__package__ = fullname.rpartition('.')[0]\n        sys.modules[fullname] = module\n\n# eof\n"}},"msg":"shiboken: de-virtualize the Python files\n\nSince a while, the virtualization of the Python helper modules\nin Shiboken is successfully active.\n\nThis patch de-virtualizes the files by creating them under the\nshiboken6 directory, but only after a successful first start\nof shiboken\/PySide. On subsequent runs, the unpacked files will stay.\n\nThis way, the files are still safe against configuration\nproblems. They are no longer safe against tampering. As\nan advantage, they can now easily be modified, and source\ncode debugging will work, again.\nA simple reset can be done by clearing the files.dir folder.\n\nTask-number: PYSIDE-962\nChange-Id: I7bc003388c4d8a424faab9d7a87908bc05c4ecfb\nFixes: PYSIDE-1994\nPick-to: 6.4\nReviewed-by: Friedemann Kleint <Friedemann.Kleint@qt.io>"}},"https:\/\/github.com\/jimkring\/pyside-nogil":{"c8b8d0868bafe25e11f48ddc7a928ca1d47879ae":{"url":"https:\/\/api.github.com\/repos\/jimkring\/pyside-nogil\/commits\/c8b8d0868bafe25e11f48ddc7a928ca1d47879ae","html_url":"https:\/\/github.com\/jimkring\/pyside-nogil\/commit\/c8b8d0868bafe25e11f48ddc7a928ca1d47879ae","sha":"c8b8d0868bafe25e11f48ddc7a928ca1d47879ae","keyword":"tampering fix","diff":"diff --git a\/sources\/shiboken6\/libshiboken\/embed\/signature_bootstrap.py b\/sources\/shiboken6\/libshiboken\/embed\/signature_bootstrap.py\nindex c11a0367a..72c042b46 100644\n--- a\/sources\/shiboken6\/libshiboken\/embed\/signature_bootstrap.py\n+++ b\/sources\/shiboken6\/libshiboken\/embed\/signature_bootstrap.py\n@@ -26,6 +26,7 @@\n import base64\n import importlib\n import io\n+import os\n import sys\n import traceback\n import zipfile\n@@ -71,11 +72,44 @@ def ensure_shibokensupport(target, support_path):\n             sys.exit(-1)\n         target.remove(support_path)\n \n-    target, support_path = prepare_zipfile()\n+    # Here we decide if re we-incarnate the embedded files or use embedding.\n+    incarnated = re_incarnate_files()\n+    if incarnated:\n+        target, support_path = sys.path, os.fspath(incarnated)\n+    else:\n+        target, support_path = prepare_zipfile()\n+    # PYSIDE-962: pre-load needed after re_incarnate_files [Windows, Py3.7.9]\n+    ensure_shibokensupport(target, support_path)\n     with ensure_shibokensupport(target, support_path):\n         from shibokensupport.signature import loader\n     return loader\n \n+# Newer functionality:\n+# This function checks if the support directory exist and returns it.\n+# If does not exist, we try to create it and return it.\n+# Otherwise, we return None.\n+\n+def re_incarnate_files():\n+    import shiboken6 as root\n+    files_dir = Path(root.__file__).resolve().parent \/ \"files.dir\"\n+    if files_dir.exists():\n+        return files_dir\n+\n+    target, zip = prepare_zipfile()\n+    names = (_ for _ in zip.zfile.namelist() if _.endswith(\".py\"))\n+    try:\n+        # first check mkdir to get an error when we cannot write.\n+        files_dir.mkdir()\n+    except os.error:\n+        return None\n+    try:\n+        # Then check for a real error when unpacking the zip file.\n+        zip.zfile.extractall(path=files_dir, members=names)\n+        return files_dir\n+    except Exception as e:\n+        print('Exception:', e)\n+        traceback.print_exc(file=sys.stdout)\n+        raise\n \n # New functionality: Loading from a zip archive.\n # There exists the zip importer, but as it is written, only real zip files are\n","message":"","files":{"\/sources\/shiboken6\/libshiboken\/embed\/signature_bootstrap.py":{"changes":[{"diff":"\n             sys.exit(-1)\n         target.remove(support_path)\n \n-    target, support_path = prepare_zipfile()\n+    # Here we decide if re we-incarnate the embedded files or use embedding.\n+    incarnated = re_incarnate_files()\n+    if incarnated:\n+        target, support_path = sys.path, os.fspath(incarnated)\n+    else:\n+        target, support_path = prepare_zipfile()\n+    # PYSIDE-962: pre-load needed after re_incarnate_files [Windows, Py3.7.9]\n+    ensure_shibokensupport(target, support_path)\n     with ensure_shibokensupport(target, support_path):\n         from shibokensupport.signature import loader\n     return loader\n \n+# Newer functionality:\n+# This function checks if the support directory exist and returns it.\n+# If does not exist, we try to create it and return it.\n+# Otherwise, we return None.\n+\n+def re_incarnate_files():\n+    import shiboken6 as root\n+    files_dir = Path(root.__file__).resolve().parent \/ \"files.dir\"\n+    if files_dir.exists():\n+        return files_dir\n+\n+    target, zip = prepare_zipfile()\n+    names = (_ for _ in zip.zfile.namelist() if _.endswith(\".py\"))\n+    try:\n+        # first check mkdir to get an error when we cannot write.\n+        files_dir.mkdir()\n+    except os.error:\n+        return None\n+    try:\n+        # Then check for a real error when unpacking the zip file.\n+        zip.zfile.extractall(path=files_dir, members=names)\n+        return files_dir\n+    except Exception as e:\n+        print('Exception:', e)\n+        traceback.print_exc(file=sys.stdout)\n+        raise\n \n # New functionality: Loading from a zip archive.\n # There exists the zip importer, but as it is written, only real zip files are\n","add":34,"remove":1,"filename":"\/sources\/shiboken6\/libshiboken\/embed\/signature_bootstrap.py","badparts":["    target, support_path = prepare_zipfile()"],"goodparts":["    incarnated = re_incarnate_files()","    if incarnated:","        target, support_path = sys.path, os.fspath(incarnated)","    else:","        target, support_path = prepare_zipfile()","    ensure_shibokensupport(target, support_path)","def re_incarnate_files():","    import shiboken6 as root","    files_dir = Path(root.__file__).resolve().parent \/ \"files.dir\"","    if files_dir.exists():","        return files_dir","    target, zip = prepare_zipfile()","    names = (_ for _ in zip.zfile.namelist() if _.endswith(\".py\"))","    try:","        files_dir.mkdir()","    except os.error:","        return None","    try:","        zip.zfile.extractall(path=files_dir, members=names)","        return files_dir","    except Exception as e:","        print('Exception:', e)","        traceback.print_exc(file=sys.stdout)","        raise"]}],"source":"\n \"\"\" signature_bootstrap.py ---------------------- This file was originally directly embedded into the C source. After it grew more and more, I now prefer to have it as Python file. Meanwhile, there is also no more a stub loader necessary: Because we meanwhile have embedding support, we could also load this file directly from a.pyc file. This file replaces the hard to read Python stub in 'signature.cpp', and we could distinguish better between bootstrap related functions and loader functions. It is embedded into 'signature.cpp' as \"embed\/signature_bootstrap.inc\". imports were in the functions. Moved them outside into the globals. \"\"\" recursion_trap=0 import base64 import importlib import io import sys import traceback import zipfile from contextlib import contextmanager from importlib.machinery import ModuleSpec from pathlib import Path def bootstrap(): global recursion_trap if recursion_trap: print(\"Recursion occurred in Bootstrap. Did you start by hand? Then it's ok.\") print(\"But you should trigger start by '_init_pyside_extension()', only!\") recursion_trap +=1 @contextmanager def ensure_shibokensupport(target, support_path): target.insert(0, support_path) sbks=\"shibokensupport\" if sbks in sys.modules: del sys.modules[sbks] prefix=sbks +\".\" for key in list(key for key in sys.modules if key.startswith(prefix)): del sys.modules[key] try: import shibokensupport yield except Exception as e: print(\"Problem importing shibokensupport:\") print(f\"{e.__class__.__name__}:{e}\") traceback.print_exc() print(\"sys.path:\") for p in sys.path: print(\" \" +p) sys.stdout.flush() sys.exit(-1) target.remove(support_path) target, support_path=prepare_zipfile() with ensure_shibokensupport(target, support_path): from shibokensupport.signature import loader return loader def prepare_zipfile(): \"\"\" Old approach: Write the zip file to a real file and return its name. It will be implicitly opened as such when we add the name to sys.path. New approach(Python 3, only): Use EmbeddableZipImporter and pass the zipfile structure directly. The sys.path way does not work, instead we need to use sys.meta_path. See https:\/\/docs.python.org\/3\/library\/sys.html \"\"\" zipbytes=base64.b64decode(''.join(zipstring_sequence)) vzip=zipfile.ZipFile(io.BytesIO(zipbytes)) return sys.meta_path, EmbeddableZipImporter(vzip) class EmbeddableZipImporter(object): def __init__(self, zip_file): def p2m(filename): if filename.endswith(\"\/__init__.py\"): return filename[:-12].replace(\"\/\", \".\") if filename.endswith(\".py\"): return filename[:-3].replace(\"\/\", \".\") return None self.zfile=zip_file self._mod2path={p2m(_.filename): _.filename for _ in zip_file.filelist} def find_spec(self, fullname, path, target=None): path=self._mod2path.get(fullname) return ModuleSpec(fullname, self) if path else None def create_module(self, spec): return None def exec_module(self, module): fullname=module.__spec__.name filename=self._mod2path[fullname] with self.zfile.open(filename, \"r\") as f: codeob=compile(f.read(), filename, \"exec\") exec(codeob, module.__dict__) module.__file__=filename module.__loader__=self if filename.endswith(\"\/__init__.py\"): module.__path__=[] module.__package__=fullname else: module.__package__=fullname.rpartition('.')[0] sys.modules[fullname]=module ","sourceWithComments":"# Copyright (C) 2022 The Qt Company Ltd.\n# SPDX-License-Identifier: LicenseRef-Qt-Commercial OR LGPL-3.0-only OR GPL-2.0-only OR GPL-3.0-only\n\n\"\"\"\nsignature_bootstrap.py\n----------------------\n\nThis file was originally directly embedded into the C source.\nAfter it grew more and more, I now prefer to have it as Python file.\n\nMeanwhile, there is also no more a stub loader necessary:\nBecause we meanwhile have embedding support, we could also load this file\ndirectly from a .pyc file.\n\nThis file replaces the hard to read Python stub in 'signature.cpp', and we\ncould distinguish better between bootstrap related functions and loader\nfunctions.\nIt is embedded into 'signature.cpp' as \"embed\/signature_bootstrap.inc\".\n\n# PYSIDE-1436: Python 3.10 had a problem with EmbeddableZipImporter because the\nimports were in the functions. Moved them outside into the globals.\n\"\"\"\n\nrecursion_trap = 0\n\nimport base64\nimport importlib\nimport io\nimport sys\nimport traceback\nimport zipfile\n\nfrom contextlib import contextmanager\nfrom importlib.machinery import ModuleSpec\nfrom pathlib import Path\n\n\ndef bootstrap():\n\n    global recursion_trap\n    if recursion_trap:\n        # we are probably called from outside, already\n        print(\"Recursion occurred in Bootstrap. Did you start by hand? Then it's ok.\")\n        print(\"But you should trigger start by '_init_pyside_extension()', only!\")\n    recursion_trap += 1\n\n    @contextmanager\n    def ensure_shibokensupport(target, support_path):\n        # Make sure that we always have the shibokensupport containing package first.\n        # Also remove any prior loaded module of this name, just in case.\n        # PYSIDE-1621: support_path can also be a finder instance.\n        target.insert(0, support_path)\n\n        sbks = \"shibokensupport\"\n        if sbks in sys.modules:\n            del sys.modules[sbks]\n        prefix = sbks + \".\"\n        for key in list(key for key in sys.modules if key.startswith(prefix)):\n            del sys.modules[key]\n        try:\n            import shibokensupport\n            yield\n        except Exception as e:\n            print(\"Problem importing shibokensupport:\")\n            print(f\"{e.__class__.__name__}: {e}\")\n            traceback.print_exc()\n            print(\"sys.path:\")\n            for p in sys.path:\n                print(\"  \" + p)\n            sys.stdout.flush()\n            sys.exit(-1)\n        target.remove(support_path)\n\n    target, support_path = prepare_zipfile()\n    with ensure_shibokensupport(target, support_path):\n        from shibokensupport.signature import loader\n    return loader\n\n\n# New functionality: Loading from a zip archive.\n# There exists the zip importer, but as it is written, only real zip files are\n# supported. Before I will start an own implementation, it is easiest to use\n# a temporary zip file.\n# PYSIDE-1621: make zip file access totally virtual\n\ndef prepare_zipfile():\n    \"\"\"\n    Old approach:\n\n    Write the zip file to a real file and return its name.\n    It will be implicitly opened as such when we add the name to sys.path .\n\n    New approach (Python 3, only):\n\n    Use EmbeddableZipImporter and pass the zipfile structure directly.\n    The sys.path way does not work, instead we need to use sys.meta_path .\n    See https:\/\/docs.python.org\/3\/library\/sys.html#sys.meta_path\n    \"\"\"\n\n    # 'zipstring_sequence' comes from signature.cpp\n    zipbytes = base64.b64decode(''.join(zipstring_sequence))\n    vzip = zipfile.ZipFile(io.BytesIO(zipbytes))\n    return sys.meta_path, EmbeddableZipImporter(vzip)\n\n\nclass EmbeddableZipImporter(object):\n\n    def __init__(self, zip_file):\n        def p2m(filename):\n            if filename.endswith(\"\/__init__.py\"):\n                return filename[:-12].replace(\"\/\", \".\")\n            if filename.endswith(\".py\"):\n                return filename[:-3].replace(\"\/\", \".\")\n            return None\n\n        self.zfile = zip_file\n        self._mod2path = {p2m(_.filename) : _.filename for _ in zip_file.filelist}\n\n    def find_spec(self, fullname, path, target=None):\n        path = self._mod2path.get(fullname)\n        return ModuleSpec(fullname, self) if path else None\n\n    def create_module(self, spec):\n        return None\n\n    def exec_module(self, module):\n        fullname = module.__spec__.name\n        filename = self._mod2path[fullname]\n        with self.zfile.open(filename, \"r\") as f:   # \"rb\" not for zipfile\n            codeob = compile(f.read(), filename, \"exec\")\n            exec(codeob, module.__dict__)\n        module.__file__ = filename\n        module.__loader__ = self\n        if filename.endswith(\"\/__init__.py\"):\n            module.__path__ = []\n            module.__package__ = fullname\n        else:\n            module.__package__ = fullname.rpartition('.')[0]\n        sys.modules[fullname] = module\n\n# eof\n"}},"msg":"shiboken: de-virtualize the Python files\n\nSince a while, the virtualization of the Python helper modules\nin Shiboken is successfully active.\n\nThis patch de-virtualizes the files by creating them under the\nshiboken6 directory, but only after a successful first start\nof shiboken\/PySide. On subsequent runs, the unpacked files will stay.\n\nThis way, the files are still safe against configuration\nproblems. They are no longer safe against tampering. As\nan advantage, they can now easily be modified, and source\ncode debugging will work, again.\nA simple reset can be done by clearing the files.dir folder.\n\nTask-number: PYSIDE-962\nChange-Id: I7bc003388c4d8a424faab9d7a87908bc05c4ecfb\nFixes: PYSIDE-1994\nPick-to: 6.4\nReviewed-by: Friedemann Kleint <Friedemann.Kleint@qt.io>"}},"https:\/\/github.com\/qtproject\/pyside-pyside-setup":{"c8b8d0868bafe25e11f48ddc7a928ca1d47879ae":{"url":"https:\/\/api.github.com\/repos\/qtproject\/pyside-pyside-setup\/commits\/c8b8d0868bafe25e11f48ddc7a928ca1d47879ae","html_url":"https:\/\/github.com\/qtproject\/pyside-pyside-setup\/commit\/c8b8d0868bafe25e11f48ddc7a928ca1d47879ae","sha":"c8b8d0868bafe25e11f48ddc7a928ca1d47879ae","keyword":"tampering fix","diff":"diff --git a\/sources\/shiboken6\/libshiboken\/embed\/signature_bootstrap.py b\/sources\/shiboken6\/libshiboken\/embed\/signature_bootstrap.py\nindex c11a0367a..72c042b46 100644\n--- a\/sources\/shiboken6\/libshiboken\/embed\/signature_bootstrap.py\n+++ b\/sources\/shiboken6\/libshiboken\/embed\/signature_bootstrap.py\n@@ -26,6 +26,7 @@\n import base64\n import importlib\n import io\n+import os\n import sys\n import traceback\n import zipfile\n@@ -71,11 +72,44 @@ def ensure_shibokensupport(target, support_path):\n             sys.exit(-1)\n         target.remove(support_path)\n \n-    target, support_path = prepare_zipfile()\n+    # Here we decide if re we-incarnate the embedded files or use embedding.\n+    incarnated = re_incarnate_files()\n+    if incarnated:\n+        target, support_path = sys.path, os.fspath(incarnated)\n+    else:\n+        target, support_path = prepare_zipfile()\n+    # PYSIDE-962: pre-load needed after re_incarnate_files [Windows, Py3.7.9]\n+    ensure_shibokensupport(target, support_path)\n     with ensure_shibokensupport(target, support_path):\n         from shibokensupport.signature import loader\n     return loader\n \n+# Newer functionality:\n+# This function checks if the support directory exist and returns it.\n+# If does not exist, we try to create it and return it.\n+# Otherwise, we return None.\n+\n+def re_incarnate_files():\n+    import shiboken6 as root\n+    files_dir = Path(root.__file__).resolve().parent \/ \"files.dir\"\n+    if files_dir.exists():\n+        return files_dir\n+\n+    target, zip = prepare_zipfile()\n+    names = (_ for _ in zip.zfile.namelist() if _.endswith(\".py\"))\n+    try:\n+        # first check mkdir to get an error when we cannot write.\n+        files_dir.mkdir()\n+    except os.error:\n+        return None\n+    try:\n+        # Then check for a real error when unpacking the zip file.\n+        zip.zfile.extractall(path=files_dir, members=names)\n+        return files_dir\n+    except Exception as e:\n+        print('Exception:', e)\n+        traceback.print_exc(file=sys.stdout)\n+        raise\n \n # New functionality: Loading from a zip archive.\n # There exists the zip importer, but as it is written, only real zip files are\n","message":"","files":{"\/sources\/shiboken6\/libshiboken\/embed\/signature_bootstrap.py":{"changes":[{"diff":"\n             sys.exit(-1)\n         target.remove(support_path)\n \n-    target, support_path = prepare_zipfile()\n+    # Here we decide if re we-incarnate the embedded files or use embedding.\n+    incarnated = re_incarnate_files()\n+    if incarnated:\n+        target, support_path = sys.path, os.fspath(incarnated)\n+    else:\n+        target, support_path = prepare_zipfile()\n+    # PYSIDE-962: pre-load needed after re_incarnate_files [Windows, Py3.7.9]\n+    ensure_shibokensupport(target, support_path)\n     with ensure_shibokensupport(target, support_path):\n         from shibokensupport.signature import loader\n     return loader\n \n+# Newer functionality:\n+# This function checks if the support directory exist and returns it.\n+# If does not exist, we try to create it and return it.\n+# Otherwise, we return None.\n+\n+def re_incarnate_files():\n+    import shiboken6 as root\n+    files_dir = Path(root.__file__).resolve().parent \/ \"files.dir\"\n+    if files_dir.exists():\n+        return files_dir\n+\n+    target, zip = prepare_zipfile()\n+    names = (_ for _ in zip.zfile.namelist() if _.endswith(\".py\"))\n+    try:\n+        # first check mkdir to get an error when we cannot write.\n+        files_dir.mkdir()\n+    except os.error:\n+        return None\n+    try:\n+        # Then check for a real error when unpacking the zip file.\n+        zip.zfile.extractall(path=files_dir, members=names)\n+        return files_dir\n+    except Exception as e:\n+        print('Exception:', e)\n+        traceback.print_exc(file=sys.stdout)\n+        raise\n \n # New functionality: Loading from a zip archive.\n # There exists the zip importer, but as it is written, only real zip files are\n","add":34,"remove":1,"filename":"\/sources\/shiboken6\/libshiboken\/embed\/signature_bootstrap.py","badparts":["    target, support_path = prepare_zipfile()"],"goodparts":["    incarnated = re_incarnate_files()","    if incarnated:","        target, support_path = sys.path, os.fspath(incarnated)","    else:","        target, support_path = prepare_zipfile()","    ensure_shibokensupport(target, support_path)","def re_incarnate_files():","    import shiboken6 as root","    files_dir = Path(root.__file__).resolve().parent \/ \"files.dir\"","    if files_dir.exists():","        return files_dir","    target, zip = prepare_zipfile()","    names = (_ for _ in zip.zfile.namelist() if _.endswith(\".py\"))","    try:","        files_dir.mkdir()","    except os.error:","        return None","    try:","        zip.zfile.extractall(path=files_dir, members=names)","        return files_dir","    except Exception as e:","        print('Exception:', e)","        traceback.print_exc(file=sys.stdout)","        raise"]}],"source":"\n \"\"\" signature_bootstrap.py ---------------------- This file was originally directly embedded into the C source. After it grew more and more, I now prefer to have it as Python file. Meanwhile, there is also no more a stub loader necessary: Because we meanwhile have embedding support, we could also load this file directly from a.pyc file. This file replaces the hard to read Python stub in 'signature.cpp', and we could distinguish better between bootstrap related functions and loader functions. It is embedded into 'signature.cpp' as \"embed\/signature_bootstrap.inc\". imports were in the functions. Moved them outside into the globals. \"\"\" recursion_trap=0 import base64 import importlib import io import sys import traceback import zipfile from contextlib import contextmanager from importlib.machinery import ModuleSpec from pathlib import Path def bootstrap(): global recursion_trap if recursion_trap: print(\"Recursion occurred in Bootstrap. Did you start by hand? Then it's ok.\") print(\"But you should trigger start by '_init_pyside_extension()', only!\") recursion_trap +=1 @contextmanager def ensure_shibokensupport(target, support_path): target.insert(0, support_path) sbks=\"shibokensupport\" if sbks in sys.modules: del sys.modules[sbks] prefix=sbks +\".\" for key in list(key for key in sys.modules if key.startswith(prefix)): del sys.modules[key] try: import shibokensupport yield except Exception as e: print(\"Problem importing shibokensupport:\") print(f\"{e.__class__.__name__}:{e}\") traceback.print_exc() print(\"sys.path:\") for p in sys.path: print(\" \" +p) sys.stdout.flush() sys.exit(-1) target.remove(support_path) target, support_path=prepare_zipfile() with ensure_shibokensupport(target, support_path): from shibokensupport.signature import loader return loader def prepare_zipfile(): \"\"\" Old approach: Write the zip file to a real file and return its name. It will be implicitly opened as such when we add the name to sys.path. New approach(Python 3, only): Use EmbeddableZipImporter and pass the zipfile structure directly. The sys.path way does not work, instead we need to use sys.meta_path. See https:\/\/docs.python.org\/3\/library\/sys.html \"\"\" zipbytes=base64.b64decode(''.join(zipstring_sequence)) vzip=zipfile.ZipFile(io.BytesIO(zipbytes)) return sys.meta_path, EmbeddableZipImporter(vzip) class EmbeddableZipImporter(object): def __init__(self, zip_file): def p2m(filename): if filename.endswith(\"\/__init__.py\"): return filename[:-12].replace(\"\/\", \".\") if filename.endswith(\".py\"): return filename[:-3].replace(\"\/\", \".\") return None self.zfile=zip_file self._mod2path={p2m(_.filename): _.filename for _ in zip_file.filelist} def find_spec(self, fullname, path, target=None): path=self._mod2path.get(fullname) return ModuleSpec(fullname, self) if path else None def create_module(self, spec): return None def exec_module(self, module): fullname=module.__spec__.name filename=self._mod2path[fullname] with self.zfile.open(filename, \"r\") as f: codeob=compile(f.read(), filename, \"exec\") exec(codeob, module.__dict__) module.__file__=filename module.__loader__=self if filename.endswith(\"\/__init__.py\"): module.__path__=[] module.__package__=fullname else: module.__package__=fullname.rpartition('.')[0] sys.modules[fullname]=module ","sourceWithComments":"# Copyright (C) 2022 The Qt Company Ltd.\n# SPDX-License-Identifier: LicenseRef-Qt-Commercial OR LGPL-3.0-only OR GPL-2.0-only OR GPL-3.0-only\n\n\"\"\"\nsignature_bootstrap.py\n----------------------\n\nThis file was originally directly embedded into the C source.\nAfter it grew more and more, I now prefer to have it as Python file.\n\nMeanwhile, there is also no more a stub loader necessary:\nBecause we meanwhile have embedding support, we could also load this file\ndirectly from a .pyc file.\n\nThis file replaces the hard to read Python stub in 'signature.cpp', and we\ncould distinguish better between bootstrap related functions and loader\nfunctions.\nIt is embedded into 'signature.cpp' as \"embed\/signature_bootstrap.inc\".\n\n# PYSIDE-1436: Python 3.10 had a problem with EmbeddableZipImporter because the\nimports were in the functions. Moved them outside into the globals.\n\"\"\"\n\nrecursion_trap = 0\n\nimport base64\nimport importlib\nimport io\nimport sys\nimport traceback\nimport zipfile\n\nfrom contextlib import contextmanager\nfrom importlib.machinery import ModuleSpec\nfrom pathlib import Path\n\n\ndef bootstrap():\n\n    global recursion_trap\n    if recursion_trap:\n        # we are probably called from outside, already\n        print(\"Recursion occurred in Bootstrap. Did you start by hand? Then it's ok.\")\n        print(\"But you should trigger start by '_init_pyside_extension()', only!\")\n    recursion_trap += 1\n\n    @contextmanager\n    def ensure_shibokensupport(target, support_path):\n        # Make sure that we always have the shibokensupport containing package first.\n        # Also remove any prior loaded module of this name, just in case.\n        # PYSIDE-1621: support_path can also be a finder instance.\n        target.insert(0, support_path)\n\n        sbks = \"shibokensupport\"\n        if sbks in sys.modules:\n            del sys.modules[sbks]\n        prefix = sbks + \".\"\n        for key in list(key for key in sys.modules if key.startswith(prefix)):\n            del sys.modules[key]\n        try:\n            import shibokensupport\n            yield\n        except Exception as e:\n            print(\"Problem importing shibokensupport:\")\n            print(f\"{e.__class__.__name__}: {e}\")\n            traceback.print_exc()\n            print(\"sys.path:\")\n            for p in sys.path:\n                print(\"  \" + p)\n            sys.stdout.flush()\n            sys.exit(-1)\n        target.remove(support_path)\n\n    target, support_path = prepare_zipfile()\n    with ensure_shibokensupport(target, support_path):\n        from shibokensupport.signature import loader\n    return loader\n\n\n# New functionality: Loading from a zip archive.\n# There exists the zip importer, but as it is written, only real zip files are\n# supported. Before I will start an own implementation, it is easiest to use\n# a temporary zip file.\n# PYSIDE-1621: make zip file access totally virtual\n\ndef prepare_zipfile():\n    \"\"\"\n    Old approach:\n\n    Write the zip file to a real file and return its name.\n    It will be implicitly opened as such when we add the name to sys.path .\n\n    New approach (Python 3, only):\n\n    Use EmbeddableZipImporter and pass the zipfile structure directly.\n    The sys.path way does not work, instead we need to use sys.meta_path .\n    See https:\/\/docs.python.org\/3\/library\/sys.html#sys.meta_path\n    \"\"\"\n\n    # 'zipstring_sequence' comes from signature.cpp\n    zipbytes = base64.b64decode(''.join(zipstring_sequence))\n    vzip = zipfile.ZipFile(io.BytesIO(zipbytes))\n    return sys.meta_path, EmbeddableZipImporter(vzip)\n\n\nclass EmbeddableZipImporter(object):\n\n    def __init__(self, zip_file):\n        def p2m(filename):\n            if filename.endswith(\"\/__init__.py\"):\n                return filename[:-12].replace(\"\/\", \".\")\n            if filename.endswith(\".py\"):\n                return filename[:-3].replace(\"\/\", \".\")\n            return None\n\n        self.zfile = zip_file\n        self._mod2path = {p2m(_.filename) : _.filename for _ in zip_file.filelist}\n\n    def find_spec(self, fullname, path, target=None):\n        path = self._mod2path.get(fullname)\n        return ModuleSpec(fullname, self) if path else None\n\n    def create_module(self, spec):\n        return None\n\n    def exec_module(self, module):\n        fullname = module.__spec__.name\n        filename = self._mod2path[fullname]\n        with self.zfile.open(filename, \"r\") as f:   # \"rb\" not for zipfile\n            codeob = compile(f.read(), filename, \"exec\")\n            exec(codeob, module.__dict__)\n        module.__file__ = filename\n        module.__loader__ = self\n        if filename.endswith(\"\/__init__.py\"):\n            module.__path__ = []\n            module.__package__ = fullname\n        else:\n            module.__package__ = fullname.rpartition('.')[0]\n        sys.modules[fullname] = module\n\n# eof\n"}},"msg":"shiboken: de-virtualize the Python files\n\nSince a while, the virtualization of the Python helper modules\nin Shiboken is successfully active.\n\nThis patch de-virtualizes the files by creating them under the\nshiboken6 directory, but only after a successful first start\nof shiboken\/PySide. On subsequent runs, the unpacked files will stay.\n\nThis way, the files are still safe against configuration\nproblems. They are no longer safe against tampering. As\nan advantage, they can now easily be modified, and source\ncode debugging will work, again.\nA simple reset can be done by clearing the files.dir folder.\n\nTask-number: PYSIDE-962\nChange-Id: I7bc003388c4d8a424faab9d7a87908bc05c4ecfb\nFixes: PYSIDE-1994\nPick-to: 6.4\nReviewed-by: Friedemann Kleint <Friedemann.Kleint@qt.io>"}},"https:\/\/github.com\/ctismer\/pyside6-dev":{"c8b8d0868bafe25e11f48ddc7a928ca1d47879ae":{"url":"https:\/\/api.github.com\/repos\/ctismer\/pyside6-dev\/commits\/c8b8d0868bafe25e11f48ddc7a928ca1d47879ae","html_url":"https:\/\/github.com\/ctismer\/pyside6-dev\/commit\/c8b8d0868bafe25e11f48ddc7a928ca1d47879ae","sha":"c8b8d0868bafe25e11f48ddc7a928ca1d47879ae","keyword":"tampering fix","diff":"diff --git a\/sources\/shiboken6\/libshiboken\/embed\/signature_bootstrap.py b\/sources\/shiboken6\/libshiboken\/embed\/signature_bootstrap.py\nindex c11a0367a..72c042b46 100644\n--- a\/sources\/shiboken6\/libshiboken\/embed\/signature_bootstrap.py\n+++ b\/sources\/shiboken6\/libshiboken\/embed\/signature_bootstrap.py\n@@ -26,6 +26,7 @@\n import base64\n import importlib\n import io\n+import os\n import sys\n import traceback\n import zipfile\n@@ -71,11 +72,44 @@ def ensure_shibokensupport(target, support_path):\n             sys.exit(-1)\n         target.remove(support_path)\n \n-    target, support_path = prepare_zipfile()\n+    # Here we decide if re we-incarnate the embedded files or use embedding.\n+    incarnated = re_incarnate_files()\n+    if incarnated:\n+        target, support_path = sys.path, os.fspath(incarnated)\n+    else:\n+        target, support_path = prepare_zipfile()\n+    # PYSIDE-962: pre-load needed after re_incarnate_files [Windows, Py3.7.9]\n+    ensure_shibokensupport(target, support_path)\n     with ensure_shibokensupport(target, support_path):\n         from shibokensupport.signature import loader\n     return loader\n \n+# Newer functionality:\n+# This function checks if the support directory exist and returns it.\n+# If does not exist, we try to create it and return it.\n+# Otherwise, we return None.\n+\n+def re_incarnate_files():\n+    import shiboken6 as root\n+    files_dir = Path(root.__file__).resolve().parent \/ \"files.dir\"\n+    if files_dir.exists():\n+        return files_dir\n+\n+    target, zip = prepare_zipfile()\n+    names = (_ for _ in zip.zfile.namelist() if _.endswith(\".py\"))\n+    try:\n+        # first check mkdir to get an error when we cannot write.\n+        files_dir.mkdir()\n+    except os.error:\n+        return None\n+    try:\n+        # Then check for a real error when unpacking the zip file.\n+        zip.zfile.extractall(path=files_dir, members=names)\n+        return files_dir\n+    except Exception as e:\n+        print('Exception:', e)\n+        traceback.print_exc(file=sys.stdout)\n+        raise\n \n # New functionality: Loading from a zip archive.\n # There exists the zip importer, but as it is written, only real zip files are\n","message":"","files":{"\/sources\/shiboken6\/libshiboken\/embed\/signature_bootstrap.py":{"changes":[{"diff":"\n             sys.exit(-1)\n         target.remove(support_path)\n \n-    target, support_path = prepare_zipfile()\n+    # Here we decide if re we-incarnate the embedded files or use embedding.\n+    incarnated = re_incarnate_files()\n+    if incarnated:\n+        target, support_path = sys.path, os.fspath(incarnated)\n+    else:\n+        target, support_path = prepare_zipfile()\n+    # PYSIDE-962: pre-load needed after re_incarnate_files [Windows, Py3.7.9]\n+    ensure_shibokensupport(target, support_path)\n     with ensure_shibokensupport(target, support_path):\n         from shibokensupport.signature import loader\n     return loader\n \n+# Newer functionality:\n+# This function checks if the support directory exist and returns it.\n+# If does not exist, we try to create it and return it.\n+# Otherwise, we return None.\n+\n+def re_incarnate_files():\n+    import shiboken6 as root\n+    files_dir = Path(root.__file__).resolve().parent \/ \"files.dir\"\n+    if files_dir.exists():\n+        return files_dir\n+\n+    target, zip = prepare_zipfile()\n+    names = (_ for _ in zip.zfile.namelist() if _.endswith(\".py\"))\n+    try:\n+        # first check mkdir to get an error when we cannot write.\n+        files_dir.mkdir()\n+    except os.error:\n+        return None\n+    try:\n+        # Then check for a real error when unpacking the zip file.\n+        zip.zfile.extractall(path=files_dir, members=names)\n+        return files_dir\n+    except Exception as e:\n+        print('Exception:', e)\n+        traceback.print_exc(file=sys.stdout)\n+        raise\n \n # New functionality: Loading from a zip archive.\n # There exists the zip importer, but as it is written, only real zip files are\n","add":34,"remove":1,"filename":"\/sources\/shiboken6\/libshiboken\/embed\/signature_bootstrap.py","badparts":["    target, support_path = prepare_zipfile()"],"goodparts":["    incarnated = re_incarnate_files()","    if incarnated:","        target, support_path = sys.path, os.fspath(incarnated)","    else:","        target, support_path = prepare_zipfile()","    ensure_shibokensupport(target, support_path)","def re_incarnate_files():","    import shiboken6 as root","    files_dir = Path(root.__file__).resolve().parent \/ \"files.dir\"","    if files_dir.exists():","        return files_dir","    target, zip = prepare_zipfile()","    names = (_ for _ in zip.zfile.namelist() if _.endswith(\".py\"))","    try:","        files_dir.mkdir()","    except os.error:","        return None","    try:","        zip.zfile.extractall(path=files_dir, members=names)","        return files_dir","    except Exception as e:","        print('Exception:', e)","        traceback.print_exc(file=sys.stdout)","        raise"]}],"source":"\n \"\"\" signature_bootstrap.py ---------------------- This file was originally directly embedded into the C source. After it grew more and more, I now prefer to have it as Python file. Meanwhile, there is also no more a stub loader necessary: Because we meanwhile have embedding support, we could also load this file directly from a.pyc file. This file replaces the hard to read Python stub in 'signature.cpp', and we could distinguish better between bootstrap related functions and loader functions. It is embedded into 'signature.cpp' as \"embed\/signature_bootstrap.inc\". imports were in the functions. Moved them outside into the globals. \"\"\" recursion_trap=0 import base64 import importlib import io import sys import traceback import zipfile from contextlib import contextmanager from importlib.machinery import ModuleSpec from pathlib import Path def bootstrap(): global recursion_trap if recursion_trap: print(\"Recursion occurred in Bootstrap. Did you start by hand? Then it's ok.\") print(\"But you should trigger start by '_init_pyside_extension()', only!\") recursion_trap +=1 @contextmanager def ensure_shibokensupport(target, support_path): target.insert(0, support_path) sbks=\"shibokensupport\" if sbks in sys.modules: del sys.modules[sbks] prefix=sbks +\".\" for key in list(key for key in sys.modules if key.startswith(prefix)): del sys.modules[key] try: import shibokensupport yield except Exception as e: print(\"Problem importing shibokensupport:\") print(f\"{e.__class__.__name__}:{e}\") traceback.print_exc() print(\"sys.path:\") for p in sys.path: print(\" \" +p) sys.stdout.flush() sys.exit(-1) target.remove(support_path) target, support_path=prepare_zipfile() with ensure_shibokensupport(target, support_path): from shibokensupport.signature import loader return loader def prepare_zipfile(): \"\"\" Old approach: Write the zip file to a real file and return its name. It will be implicitly opened as such when we add the name to sys.path. New approach(Python 3, only): Use EmbeddableZipImporter and pass the zipfile structure directly. The sys.path way does not work, instead we need to use sys.meta_path. See https:\/\/docs.python.org\/3\/library\/sys.html \"\"\" zipbytes=base64.b64decode(''.join(zipstring_sequence)) vzip=zipfile.ZipFile(io.BytesIO(zipbytes)) return sys.meta_path, EmbeddableZipImporter(vzip) class EmbeddableZipImporter(object): def __init__(self, zip_file): def p2m(filename): if filename.endswith(\"\/__init__.py\"): return filename[:-12].replace(\"\/\", \".\") if filename.endswith(\".py\"): return filename[:-3].replace(\"\/\", \".\") return None self.zfile=zip_file self._mod2path={p2m(_.filename): _.filename for _ in zip_file.filelist} def find_spec(self, fullname, path, target=None): path=self._mod2path.get(fullname) return ModuleSpec(fullname, self) if path else None def create_module(self, spec): return None def exec_module(self, module): fullname=module.__spec__.name filename=self._mod2path[fullname] with self.zfile.open(filename, \"r\") as f: codeob=compile(f.read(), filename, \"exec\") exec(codeob, module.__dict__) module.__file__=filename module.__loader__=self if filename.endswith(\"\/__init__.py\"): module.__path__=[] module.__package__=fullname else: module.__package__=fullname.rpartition('.')[0] sys.modules[fullname]=module ","sourceWithComments":"# Copyright (C) 2022 The Qt Company Ltd.\n# SPDX-License-Identifier: LicenseRef-Qt-Commercial OR LGPL-3.0-only OR GPL-2.0-only OR GPL-3.0-only\n\n\"\"\"\nsignature_bootstrap.py\n----------------------\n\nThis file was originally directly embedded into the C source.\nAfter it grew more and more, I now prefer to have it as Python file.\n\nMeanwhile, there is also no more a stub loader necessary:\nBecause we meanwhile have embedding support, we could also load this file\ndirectly from a .pyc file.\n\nThis file replaces the hard to read Python stub in 'signature.cpp', and we\ncould distinguish better between bootstrap related functions and loader\nfunctions.\nIt is embedded into 'signature.cpp' as \"embed\/signature_bootstrap.inc\".\n\n# PYSIDE-1436: Python 3.10 had a problem with EmbeddableZipImporter because the\nimports were in the functions. Moved them outside into the globals.\n\"\"\"\n\nrecursion_trap = 0\n\nimport base64\nimport importlib\nimport io\nimport sys\nimport traceback\nimport zipfile\n\nfrom contextlib import contextmanager\nfrom importlib.machinery import ModuleSpec\nfrom pathlib import Path\n\n\ndef bootstrap():\n\n    global recursion_trap\n    if recursion_trap:\n        # we are probably called from outside, already\n        print(\"Recursion occurred in Bootstrap. Did you start by hand? Then it's ok.\")\n        print(\"But you should trigger start by '_init_pyside_extension()', only!\")\n    recursion_trap += 1\n\n    @contextmanager\n    def ensure_shibokensupport(target, support_path):\n        # Make sure that we always have the shibokensupport containing package first.\n        # Also remove any prior loaded module of this name, just in case.\n        # PYSIDE-1621: support_path can also be a finder instance.\n        target.insert(0, support_path)\n\n        sbks = \"shibokensupport\"\n        if sbks in sys.modules:\n            del sys.modules[sbks]\n        prefix = sbks + \".\"\n        for key in list(key for key in sys.modules if key.startswith(prefix)):\n            del sys.modules[key]\n        try:\n            import shibokensupport\n            yield\n        except Exception as e:\n            print(\"Problem importing shibokensupport:\")\n            print(f\"{e.__class__.__name__}: {e}\")\n            traceback.print_exc()\n            print(\"sys.path:\")\n            for p in sys.path:\n                print(\"  \" + p)\n            sys.stdout.flush()\n            sys.exit(-1)\n        target.remove(support_path)\n\n    target, support_path = prepare_zipfile()\n    with ensure_shibokensupport(target, support_path):\n        from shibokensupport.signature import loader\n    return loader\n\n\n# New functionality: Loading from a zip archive.\n# There exists the zip importer, but as it is written, only real zip files are\n# supported. Before I will start an own implementation, it is easiest to use\n# a temporary zip file.\n# PYSIDE-1621: make zip file access totally virtual\n\ndef prepare_zipfile():\n    \"\"\"\n    Old approach:\n\n    Write the zip file to a real file and return its name.\n    It will be implicitly opened as such when we add the name to sys.path .\n\n    New approach (Python 3, only):\n\n    Use EmbeddableZipImporter and pass the zipfile structure directly.\n    The sys.path way does not work, instead we need to use sys.meta_path .\n    See https:\/\/docs.python.org\/3\/library\/sys.html#sys.meta_path\n    \"\"\"\n\n    # 'zipstring_sequence' comes from signature.cpp\n    zipbytes = base64.b64decode(''.join(zipstring_sequence))\n    vzip = zipfile.ZipFile(io.BytesIO(zipbytes))\n    return sys.meta_path, EmbeddableZipImporter(vzip)\n\n\nclass EmbeddableZipImporter(object):\n\n    def __init__(self, zip_file):\n        def p2m(filename):\n            if filename.endswith(\"\/__init__.py\"):\n                return filename[:-12].replace(\"\/\", \".\")\n            if filename.endswith(\".py\"):\n                return filename[:-3].replace(\"\/\", \".\")\n            return None\n\n        self.zfile = zip_file\n        self._mod2path = {p2m(_.filename) : _.filename for _ in zip_file.filelist}\n\n    def find_spec(self, fullname, path, target=None):\n        path = self._mod2path.get(fullname)\n        return ModuleSpec(fullname, self) if path else None\n\n    def create_module(self, spec):\n        return None\n\n    def exec_module(self, module):\n        fullname = module.__spec__.name\n        filename = self._mod2path[fullname]\n        with self.zfile.open(filename, \"r\") as f:   # \"rb\" not for zipfile\n            codeob = compile(f.read(), filename, \"exec\")\n            exec(codeob, module.__dict__)\n        module.__file__ = filename\n        module.__loader__ = self\n        if filename.endswith(\"\/__init__.py\"):\n            module.__path__ = []\n            module.__package__ = fullname\n        else:\n            module.__package__ = fullname.rpartition('.')[0]\n        sys.modules[fullname] = module\n\n# eof\n"}},"msg":"shiboken: de-virtualize the Python files\n\nSince a while, the virtualization of the Python helper modules\nin Shiboken is successfully active.\n\nThis patch de-virtualizes the files by creating them under the\nshiboken6 directory, but only after a successful first start\nof shiboken\/PySide. On subsequent runs, the unpacked files will stay.\n\nThis way, the files are still safe against configuration\nproblems. They are no longer safe against tampering. As\nan advantage, they can now easily be modified, and source\ncode debugging will work, again.\nA simple reset can be done by clearing the files.dir folder.\n\nTask-number: PYSIDE-962\nChange-Id: I7bc003388c4d8a424faab9d7a87908bc05c4ecfb\nFixes: PYSIDE-1994\nPick-to: 6.4\nReviewed-by: Friedemann Kleint <Friedemann.Kleint@qt.io>"}},"https:\/\/github.com\/ctismer\/pyside6-6.4":{"3d9bc9fb2ad0d50aafdcff4c39429532c32bfc2f":{"url":"https:\/\/api.github.com\/repos\/ctismer\/pyside6-6.4\/commits\/3d9bc9fb2ad0d50aafdcff4c39429532c32bfc2f","html_url":"https:\/\/github.com\/ctismer\/pyside6-6.4\/commit\/3d9bc9fb2ad0d50aafdcff4c39429532c32bfc2f","sha":"3d9bc9fb2ad0d50aafdcff4c39429532c32bfc2f","keyword":"tampering fix","diff":"diff --git a\/sources\/shiboken6\/libshiboken\/embed\/signature_bootstrap.py b\/sources\/shiboken6\/libshiboken\/embed\/signature_bootstrap.py\nindex c11a0367a..72c042b46 100644\n--- a\/sources\/shiboken6\/libshiboken\/embed\/signature_bootstrap.py\n+++ b\/sources\/shiboken6\/libshiboken\/embed\/signature_bootstrap.py\n@@ -26,6 +26,7 @@\n import base64\n import importlib\n import io\n+import os\n import sys\n import traceback\n import zipfile\n@@ -71,11 +72,44 @@ def ensure_shibokensupport(target, support_path):\n             sys.exit(-1)\n         target.remove(support_path)\n \n-    target, support_path = prepare_zipfile()\n+    # Here we decide if re we-incarnate the embedded files or use embedding.\n+    incarnated = re_incarnate_files()\n+    if incarnated:\n+        target, support_path = sys.path, os.fspath(incarnated)\n+    else:\n+        target, support_path = prepare_zipfile()\n+    # PYSIDE-962: pre-load needed after re_incarnate_files [Windows, Py3.7.9]\n+    ensure_shibokensupport(target, support_path)\n     with ensure_shibokensupport(target, support_path):\n         from shibokensupport.signature import loader\n     return loader\n \n+# Newer functionality:\n+# This function checks if the support directory exist and returns it.\n+# If does not exist, we try to create it and return it.\n+# Otherwise, we return None.\n+\n+def re_incarnate_files():\n+    import shiboken6 as root\n+    files_dir = Path(root.__file__).resolve().parent \/ \"files.dir\"\n+    if files_dir.exists():\n+        return files_dir\n+\n+    target, zip = prepare_zipfile()\n+    names = (_ for _ in zip.zfile.namelist() if _.endswith(\".py\"))\n+    try:\n+        # first check mkdir to get an error when we cannot write.\n+        files_dir.mkdir()\n+    except os.error:\n+        return None\n+    try:\n+        # Then check for a real error when unpacking the zip file.\n+        zip.zfile.extractall(path=files_dir, members=names)\n+        return files_dir\n+    except Exception as e:\n+        print('Exception:', e)\n+        traceback.print_exc(file=sys.stdout)\n+        raise\n \n # New functionality: Loading from a zip archive.\n # There exists the zip importer, but as it is written, only real zip files are\n","message":"","files":{"\/sources\/shiboken6\/libshiboken\/embed\/signature_bootstrap.py":{"changes":[{"diff":"\n             sys.exit(-1)\n         target.remove(support_path)\n \n-    target, support_path = prepare_zipfile()\n+    # Here we decide if re we-incarnate the embedded files or use embedding.\n+    incarnated = re_incarnate_files()\n+    if incarnated:\n+        target, support_path = sys.path, os.fspath(incarnated)\n+    else:\n+        target, support_path = prepare_zipfile()\n+    # PYSIDE-962: pre-load needed after re_incarnate_files [Windows, Py3.7.9]\n+    ensure_shibokensupport(target, support_path)\n     with ensure_shibokensupport(target, support_path):\n         from shibokensupport.signature import loader\n     return loader\n \n+# Newer functionality:\n+# This function checks if the support directory exist and returns it.\n+# If does not exist, we try to create it and return it.\n+# Otherwise, we return None.\n+\n+def re_incarnate_files():\n+    import shiboken6 as root\n+    files_dir = Path(root.__file__).resolve().parent \/ \"files.dir\"\n+    if files_dir.exists():\n+        return files_dir\n+\n+    target, zip = prepare_zipfile()\n+    names = (_ for _ in zip.zfile.namelist() if _.endswith(\".py\"))\n+    try:\n+        # first check mkdir to get an error when we cannot write.\n+        files_dir.mkdir()\n+    except os.error:\n+        return None\n+    try:\n+        # Then check for a real error when unpacking the zip file.\n+        zip.zfile.extractall(path=files_dir, members=names)\n+        return files_dir\n+    except Exception as e:\n+        print('Exception:', e)\n+        traceback.print_exc(file=sys.stdout)\n+        raise\n \n # New functionality: Loading from a zip archive.\n # There exists the zip importer, but as it is written, only real zip files are\n","add":34,"remove":1,"filename":"\/sources\/shiboken6\/libshiboken\/embed\/signature_bootstrap.py","badparts":["    target, support_path = prepare_zipfile()"],"goodparts":["    incarnated = re_incarnate_files()","    if incarnated:","        target, support_path = sys.path, os.fspath(incarnated)","    else:","        target, support_path = prepare_zipfile()","    ensure_shibokensupport(target, support_path)","def re_incarnate_files():","    import shiboken6 as root","    files_dir = Path(root.__file__).resolve().parent \/ \"files.dir\"","    if files_dir.exists():","        return files_dir","    target, zip = prepare_zipfile()","    names = (_ for _ in zip.zfile.namelist() if _.endswith(\".py\"))","    try:","        files_dir.mkdir()","    except os.error:","        return None","    try:","        zip.zfile.extractall(path=files_dir, members=names)","        return files_dir","    except Exception as e:","        print('Exception:', e)","        traceback.print_exc(file=sys.stdout)","        raise"]}],"source":"\n \"\"\" signature_bootstrap.py ---------------------- This file was originally directly embedded into the C source. After it grew more and more, I now prefer to have it as Python file. Meanwhile, there is also no more a stub loader necessary: Because we meanwhile have embedding support, we could also load this file directly from a.pyc file. This file replaces the hard to read Python stub in 'signature.cpp', and we could distinguish better between bootstrap related functions and loader functions. It is embedded into 'signature.cpp' as \"embed\/signature_bootstrap.inc\". imports were in the functions. Moved them outside into the globals. \"\"\" recursion_trap=0 import base64 import importlib import io import sys import traceback import zipfile from contextlib import contextmanager from importlib.machinery import ModuleSpec from pathlib import Path def bootstrap(): global recursion_trap if recursion_trap: print(\"Recursion occurred in Bootstrap. Did you start by hand? Then it's ok.\") print(\"But you should trigger start by '_init_pyside_extension()', only!\") recursion_trap +=1 @contextmanager def ensure_shibokensupport(target, support_path): target.insert(0, support_path) sbks=\"shibokensupport\" if sbks in sys.modules: del sys.modules[sbks] prefix=sbks +\".\" for key in list(key for key in sys.modules if key.startswith(prefix)): del sys.modules[key] try: import shibokensupport yield except Exception as e: print(\"Problem importing shibokensupport:\") print(f\"{e.__class__.__name__}:{e}\") traceback.print_exc() print(\"sys.path:\") for p in sys.path: print(\" \" +p) sys.stdout.flush() sys.exit(-1) target.remove(support_path) target, support_path=prepare_zipfile() with ensure_shibokensupport(target, support_path): from shibokensupport.signature import loader return loader def prepare_zipfile(): \"\"\" Old approach: Write the zip file to a real file and return its name. It will be implicitly opened as such when we add the name to sys.path. New approach(Python 3, only): Use EmbeddableZipImporter and pass the zipfile structure directly. The sys.path way does not work, instead we need to use sys.meta_path. See https:\/\/docs.python.org\/3\/library\/sys.html \"\"\" zipbytes=base64.b64decode(''.join(zipstring_sequence)) vzip=zipfile.ZipFile(io.BytesIO(zipbytes)) return sys.meta_path, EmbeddableZipImporter(vzip) class EmbeddableZipImporter(object): def __init__(self, zip_file): def p2m(filename): if filename.endswith(\"\/__init__.py\"): return filename[:-12].replace(\"\/\", \".\") if filename.endswith(\".py\"): return filename[:-3].replace(\"\/\", \".\") return None self.zfile=zip_file self._mod2path={p2m(_.filename): _.filename for _ in zip_file.filelist} def find_spec(self, fullname, path, target=None): path=self._mod2path.get(fullname) return ModuleSpec(fullname, self) if path else None def create_module(self, spec): return None def exec_module(self, module): fullname=module.__spec__.name filename=self._mod2path[fullname] with self.zfile.open(filename, \"r\") as f: codeob=compile(f.read(), filename, \"exec\") exec(codeob, module.__dict__) module.__file__=filename module.__loader__=self if filename.endswith(\"\/__init__.py\"): module.__path__=[] module.__package__=fullname else: module.__package__=fullname.rpartition('.')[0] sys.modules[fullname]=module ","sourceWithComments":"# Copyright (C) 2022 The Qt Company Ltd.\n# SPDX-License-Identifier: LicenseRef-Qt-Commercial OR LGPL-3.0-only OR GPL-2.0-only OR GPL-3.0-only\n\n\"\"\"\nsignature_bootstrap.py\n----------------------\n\nThis file was originally directly embedded into the C source.\nAfter it grew more and more, I now prefer to have it as Python file.\n\nMeanwhile, there is also no more a stub loader necessary:\nBecause we meanwhile have embedding support, we could also load this file\ndirectly from a .pyc file.\n\nThis file replaces the hard to read Python stub in 'signature.cpp', and we\ncould distinguish better between bootstrap related functions and loader\nfunctions.\nIt is embedded into 'signature.cpp' as \"embed\/signature_bootstrap.inc\".\n\n# PYSIDE-1436: Python 3.10 had a problem with EmbeddableZipImporter because the\nimports were in the functions. Moved them outside into the globals.\n\"\"\"\n\nrecursion_trap = 0\n\nimport base64\nimport importlib\nimport io\nimport sys\nimport traceback\nimport zipfile\n\nfrom contextlib import contextmanager\nfrom importlib.machinery import ModuleSpec\nfrom pathlib import Path\n\n\ndef bootstrap():\n\n    global recursion_trap\n    if recursion_trap:\n        # we are probably called from outside, already\n        print(\"Recursion occurred in Bootstrap. Did you start by hand? Then it's ok.\")\n        print(\"But you should trigger start by '_init_pyside_extension()', only!\")\n    recursion_trap += 1\n\n    @contextmanager\n    def ensure_shibokensupport(target, support_path):\n        # Make sure that we always have the shibokensupport containing package first.\n        # Also remove any prior loaded module of this name, just in case.\n        # PYSIDE-1621: support_path can also be a finder instance.\n        target.insert(0, support_path)\n\n        sbks = \"shibokensupport\"\n        if sbks in sys.modules:\n            del sys.modules[sbks]\n        prefix = sbks + \".\"\n        for key in list(key for key in sys.modules if key.startswith(prefix)):\n            del sys.modules[key]\n        try:\n            import shibokensupport\n            yield\n        except Exception as e:\n            print(\"Problem importing shibokensupport:\")\n            print(f\"{e.__class__.__name__}: {e}\")\n            traceback.print_exc()\n            print(\"sys.path:\")\n            for p in sys.path:\n                print(\"  \" + p)\n            sys.stdout.flush()\n            sys.exit(-1)\n        target.remove(support_path)\n\n    target, support_path = prepare_zipfile()\n    with ensure_shibokensupport(target, support_path):\n        from shibokensupport.signature import loader\n    return loader\n\n\n# New functionality: Loading from a zip archive.\n# There exists the zip importer, but as it is written, only real zip files are\n# supported. Before I will start an own implementation, it is easiest to use\n# a temporary zip file.\n# PYSIDE-1621: make zip file access totally virtual\n\ndef prepare_zipfile():\n    \"\"\"\n    Old approach:\n\n    Write the zip file to a real file and return its name.\n    It will be implicitly opened as such when we add the name to sys.path .\n\n    New approach (Python 3, only):\n\n    Use EmbeddableZipImporter and pass the zipfile structure directly.\n    The sys.path way does not work, instead we need to use sys.meta_path .\n    See https:\/\/docs.python.org\/3\/library\/sys.html#sys.meta_path\n    \"\"\"\n\n    # 'zipstring_sequence' comes from signature.cpp\n    zipbytes = base64.b64decode(''.join(zipstring_sequence))\n    vzip = zipfile.ZipFile(io.BytesIO(zipbytes))\n    return sys.meta_path, EmbeddableZipImporter(vzip)\n\n\nclass EmbeddableZipImporter(object):\n\n    def __init__(self, zip_file):\n        def p2m(filename):\n            if filename.endswith(\"\/__init__.py\"):\n                return filename[:-12].replace(\"\/\", \".\")\n            if filename.endswith(\".py\"):\n                return filename[:-3].replace(\"\/\", \".\")\n            return None\n\n        self.zfile = zip_file\n        self._mod2path = {p2m(_.filename) : _.filename for _ in zip_file.filelist}\n\n    def find_spec(self, fullname, path, target=None):\n        path = self._mod2path.get(fullname)\n        return ModuleSpec(fullname, self) if path else None\n\n    def create_module(self, spec):\n        return None\n\n    def exec_module(self, module):\n        fullname = module.__spec__.name\n        filename = self._mod2path[fullname]\n        with self.zfile.open(filename, \"r\") as f:   # \"rb\" not for zipfile\n            codeob = compile(f.read(), filename, \"exec\")\n            exec(codeob, module.__dict__)\n        module.__file__ = filename\n        module.__loader__ = self\n        if filename.endswith(\"\/__init__.py\"):\n            module.__path__ = []\n            module.__package__ = fullname\n        else:\n            module.__package__ = fullname.rpartition('.')[0]\n        sys.modules[fullname] = module\n\n# eof\n"}},"msg":"shiboken: de-virtualize the Python files\n\nSince a while, the virtualization of the Python helper modules\nin Shiboken is successfully active.\n\nThis patch de-virtualizes the files by creating them under the\nshiboken6 directory, but only after a successful first start\nof shiboken\/PySide. On subsequent runs, the unpacked files will stay.\n\nThis way, the files are still safe against configuration\nproblems. They are no longer safe against tampering. As\nan advantage, they can now easily be modified, and source\ncode debugging will work, again.\nA simple reset can be done by clearing the files.dir folder.\n\nTask-number: PYSIDE-962\nChange-Id: I7bc003388c4d8a424faab9d7a87908bc05c4ecfb\nFixes: PYSIDE-1994\nReviewed-by: Friedemann Kleint <Friedemann.Kleint@qt.io>\n(cherry picked from commit c8b8d0868bafe25e11f48ddc7a928ca1d47879ae)\nReviewed-by: Qt Cherry-pick Bot <cherrypick_bot@qt-project.org>"}},"https:\/\/github.com\/Szbuli\/smart-gateway":{"2f587308b7b97b0165680c998ed121fcca5247fb":{"url":"https:\/\/api.github.com\/repos\/Szbuli\/smart-gateway\/commits\/2f587308b7b97b0165680c998ed121fcca5247fb","html_url":"https:\/\/github.com\/Szbuli\/smart-gateway\/commit\/2f587308b7b97b0165680c998ed121fcca5247fb","message":"fix tamper state","sha":"2f587308b7b97b0165680c998ed121fcca5247fb","keyword":"tampering fix","diff":"diff --git a\/firmware\/resource-monitor\/tamper.py b\/firmware\/resource-monitor\/tamper.py\nindex fb82732..aa07d15 100644\n--- a\/firmware\/resource-monitor\/tamper.py\n+++ b\/firmware\/resource-monitor\/tamper.py\n@@ -39,4 +39,4 @@ def check_tamper(self):\n                 if self.tamper_event_happened != tamper_state:\n                     self.tamper_event_happened = tamper_state\n                     self.mqtt.publishWithBaseTopic(mqtt_topics.tamperTopic,\n-                                                   \"ON\" if tamper_state == 1 else \"OFF\")\n+                                                   \"OFF\" if tamper_state == 1 else \"ON\")\n","files":{"\/firmware\/resource-monitor\/tamper.py":{"changes":[{"diff":"\n                 if self.tamper_event_happened != tamper_state:\n                     self.tamper_event_happened = tamper_state\n                     self.mqtt.publishWithBaseTopic(mqtt_topics.tamperTopic,\n-                                                   \"ON\" if tamper_state == 1 else \"OFF\")\n+                                                   \"OFF\" if tamper_state == 1 else \"ON\")\n","add":1,"remove":1,"filename":"\/firmware\/resource-monitor\/tamper.py","badparts":["                                                   \"ON\" if tamper_state == 1 else \"OFF\")"],"goodparts":["                                                   \"OFF\" if tamper_state == 1 else \"ON\")"]}],"source":"\nimport threading import time import RPi.GPIO as GPIO import mqtt_topics tamper_pin=17 class Tamper: def __init__(self, mqtt): self.lock=threading.Lock() self.mqtt=mqtt GPIO.setmode(GPIO.BCM) GPIO.setup(tamper_pin, GPIO.IN, pull_up_down=GPIO.PUD_OFF) GPIO.add_event_detect( tamper_pin, GPIO.BOTH, callback=self.tamper_event) self.tamper_event_happened=None def tamper_event(self, channel): if self.lock.locked(): try: self.lock.release() except: pass def start(self): t=threading.Thread(target=self.check_tamper) t.setDaemon(True) t.start() def check_tamper(self): while True: self.lock.acquire() checks=0 while checks < 5: checks +=1 tamper_state=GPIO.input(tamper_pin) time.sleep(0.3) if self.tamper_event_happened !=tamper_state: self.tamper_event_happened=tamper_state self.mqtt.publishWithBaseTopic(mqtt_topics.tamperTopic, \"ON\" if tamper_state==1 else \"OFF\") ","sourceWithComments":"import threading\nimport time\nimport RPi.GPIO as GPIO\nimport mqtt_topics\n\ntamper_pin = 17\n\n\nclass Tamper:\n    def __init__(self, mqtt):\n        self.lock = threading.Lock()\n        self.mqtt = mqtt\n        GPIO.setmode(GPIO.BCM)\n        GPIO.setup(tamper_pin, GPIO.IN, pull_up_down=GPIO.PUD_OFF)\n        GPIO.add_event_detect(\n            tamper_pin, GPIO.BOTH, callback=self.tamper_event)\n        self.tamper_event_happened = None\n\n    def tamper_event(self, channel):\n        if self.lock.locked():\n            try:\n                self.lock.release()\n            except:\n                pass\n\n    def start(self):\n        t = threading.Thread(target=self.check_tamper)\n        t.setDaemon(True)\n        t.start()\n\n    def check_tamper(self):\n        while True:\n            self.lock.acquire()\n            checks = 0\n            while checks < 5:\n                checks += 1\n                tamper_state = GPIO.input(tamper_pin)\n                time.sleep(0.3)\n                if self.tamper_event_happened != tamper_state:\n                    self.tamper_event_happened = tamper_state\n                    self.mqtt.publishWithBaseTopic(mqtt_topics.tamperTopic,\n                                                   \"ON\" if tamper_state == 1 else \"OFF\")\n"}},"msg":"fix tamper state"}},"https:\/\/github.com\/teoPalomino\/budget-lens-backend":{"379b16c550931f98769ef35ee81b7aaaba20b66e":{"url":"https:\/\/api.github.com\/repos\/teoPalomino\/budget-lens-backend\/commits\/379b16c550931f98769ef35ee81b7aaaba20b66e","html_url":"https:\/\/github.com\/teoPalomino\/budget-lens-backend\/commit\/379b16c550931f98769ef35ee81b7aaaba20b66e","message":"Feat\/Bud-212\/CategoryAPI(#36)\n\n* fix: Moved everything code written from previous branch to new branch. This was so as to not tamper with the migration files in items.\r\n\r\n* fix: added category to item serializer\r\n\r\n* minor adjustments\/remove duplicates\r\n\r\n* fix\/ update views & tests for items\r\n\r\n* fix\/ run flake8\r\n\r\nCo-authored-by: Isaac Muriuki <i_muriuk@live.concordia.ca>\r\nCo-authored-by: Mateo <mateo.palomino21@hotmail.com>","sha":"379b16c550931f98769ef35ee81b7aaaba20b66e","keyword":"tampering fix","diff":"diff --git a\/budget_lens_backend\/settings.py b\/budget_lens_backend\/settings.py\nindex aa5259d..711f316 100644\n--- a\/budget_lens_backend\/settings.py\n+++ b\/budget_lens_backend\/settings.py\n@@ -56,6 +56,7 @@\n     'receipts.apps.ReceiptsConfig',\n     'friends.apps.FriendsConfig',\n     'merchant.apps.MerchantConfig',\n+    'category.apps.CategoryConfig',\n     'item.apps.ItemConfig',\n \n     # Installed apps\ndiff --git a\/budget_lens_backend\/urls.py b\/budget_lens_backend\/urls.py\nindex 780c41a..426b61d 100644\n--- a\/budget_lens_backend\/urls.py\n+++ b\/budget_lens_backend\/urls.py\n@@ -21,5 +21,6 @@\n     path('', include('users.urls')),\n     path('', include('receipts.urls')),\n     path('', include('friends.urls')),\n+    path('', include('category.urls')),\n     path('', include('item.urls')),\n ]\ndiff --git a\/category\/__init__.py b\/category\/__init__.py\nnew file mode 100644\nindex 0000000..e69de29\ndiff --git a\/category\/admin.py b\/category\/admin.py\nnew file mode 100644\nindex 0000000..3b72f9f\n--- \/dev\/null\n+++ b\/category\/admin.py\n@@ -0,0 +1,10 @@\n+from django.contrib import admin\n+from .models import Category\n+\n+# Register your models here.\n+\n+\n+@admin.register(Category)\n+class CategoryAdmin(admin.ModelAdmin):\n+    '''To view Categories in Django admin page'''\n+    pass\ndiff --git a\/category\/apps.py b\/category\/apps.py\nnew file mode 100644\nindex 0000000..1f46433\n--- \/dev\/null\n+++ b\/category\/apps.py\n@@ -0,0 +1,6 @@\n+from django.apps import AppConfig\n+\n+\n+class CategoryConfig(AppConfig):\n+    default_auto_field = 'django.db.models.BigAutoField'\n+    name = 'category'\ndiff --git a\/category\/migrations\/0001_initial.py b\/category\/migrations\/0001_initial.py\nnew file mode 100644\nindex 0000000..7d464ad\n--- \/dev\/null\n+++ b\/category\/migrations\/0001_initial.py\n@@ -0,0 +1,30 @@\n+# Generated by Django 4.1.1 on 2022-11-19 02:49\n+\n+from django.conf import settings\n+from django.db import migrations, models\n+import django.db.models.deletion\n+\n+\n+class Migration(migrations.Migration):\n+\n+    initial = True\n+\n+    dependencies = [\n+        migrations.swappable_dependency(settings.AUTH_USER_MODEL),\n+    ]\n+\n+    operations = [\n+        migrations.CreateModel(\n+            name='Category',\n+            fields=[\n+                ('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n+                ('category_name', models.CharField(blank=True, default='', max_length=30)),\n+                ('category_toggle_star', models.BooleanField(default=False)),\n+                ('parent_category_id', models.IntegerField(blank=True, null=True)),\n+                ('user', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to=settings.AUTH_USER_MODEL)),\n+            ],\n+            options={\n+                'unique_together': {('user', 'category_name')},\n+            },\n+        ),\n+    ]\ndiff --git a\/category\/migrations\/__init__.py b\/category\/migrations\/__init__.py\nnew file mode 100644\nindex 0000000..e69de29\ndiff --git a\/category\/models.py b\/category\/models.py\nnew file mode 100644\nindex 0000000..cdac870\n--- \/dev\/null\n+++ b\/category\/models.py\n@@ -0,0 +1,18 @@\n+from django.db import models\n+from django.contrib.auth.models import User\n+\n+\n+class Category(models.Model):\n+    \"\"\"\n+    The parent category models\n+    \"\"\"\n+    category_name = models.CharField(max_length=30, blank=True, default='')\n+    category_toggle_star = models.BooleanField(default=False)\n+    user = models.ForeignKey(User, on_delete=models.CASCADE)\n+    parent_category_id = models.IntegerField(blank=True, null=True)\n+\n+    class Meta:\n+        unique_together = (\"user\", \"category_name\")\n+\n+    def get_category_name(self):\n+        return self.category_name\ndiff --git a\/category\/serializers.py b\/category\/serializers.py\nnew file mode 100644\nindex 0000000..2c304ec\n--- \/dev\/null\n+++ b\/category\/serializers.py\n@@ -0,0 +1,17 @@\n+from rest_framework import serializers\n+from .models import Category\n+\n+\n+class BasicCategorySerializer(serializers.ModelSerializer):\n+    \"\"\"Basic category seriaizer which contains all fields except the user field\"\"\"\n+    class Meta:\n+        model = Category\n+        fields = ('id', 'category_name', 'category_toggle_star', 'parent_category_id')\n+\n+    def create(self, validated_data):\n+        return Category.objects.create(\n+            category_name=validated_data['category_name'],\n+            category_toggle_star=validated_data['category_toggle_star'],\n+            parent_category_id=validated_data['parent_category_id'],\n+            user=self.context['request'].user\n+        )\ndiff --git a\/category\/tests.py b\/category\/tests.py\nnew file mode 100644\nindex 0000000..96777e4\n--- \/dev\/null\n+++ b\/category\/tests.py\n@@ -0,0 +1,302 @@\n+from django.contrib.auth.models import User\n+from django.urls import reverse\n+from rest_framework.status import HTTP_200_OK, HTTP_400_BAD_REQUEST\n+from rest_framework.test import APITestCase\n+\n+from category.models import Category\n+from item.models import Item\n+from merchant.models import Merchant\n+from receipts.models import Receipts\n+from receipts.tests import get_test_image_file\n+from users.authentication import BearerToken\n+from users.models import UserProfile\n+\n+\n+class CategoryAPITestCase(APITestCase):\n+    def setUp(self):\n+        \"\"\"\n+        SetUp the values in database for testing Category API\n+        \"\"\"\n+        self.user = User.objects.create_user(\n+            username='johncena123@gmail.com',\n+            email='momoamineahmadi@gmail.com',\n+            first_name='John',\n+            last_name='Cena',\n+            password='wrestlingrules123'\n+        )\n+        self.user_profile = UserProfile.objects.create(\n+            user=self.user,\n+            telephone_number=\"+1-613-555-0187\"\n+        )\n+\n+        self.token = BearerToken.objects.create(user=self.user)\n+\n+        Receipts.objects.create(\n+            user=self.user,\n+            receipt_image=get_test_image_file(),\n+            merchant=Merchant.objects.create(name='starbucks'),\n+            location='123 Testing Street T1E 5T5',\n+            total=1,\n+            tax=1,\n+            tip=1,\n+            coupon=1,\n+            currency=\"CAD\"\n+        )\n+\n+        # Create the Category and Subcategory for testing\n+        # Create Categories\n+        self.category_food = Category.objects.create(\n+            category_name='Food',\n+            category_toggle_star=False,\n+            user=self.user\n+        )\n+\n+        self.category_taxi = Category.objects.create(\n+            category_name='Taxi',\n+            category_toggle_star=False,\n+            user=self.user,\n+        )\n+\n+        self.category_list = [self.category_food, self.category_taxi]\n+\n+        # Create SubCategories\n+        self.sub_category_fruit = Category.objects.create(\n+            category_name='Fruits',\n+            category_toggle_star=False,\n+            user=self.user,\n+            parent_category_id=Category.objects.get(category_name='Food').pk\n+        )\n+\n+        self.sub_category_uber = Category.objects.create(\n+            category_name='Uber',\n+            category_toggle_star=False,\n+            user=self.user,\n+            parent_category_id=Category.objects.get(category_name='Taxi').pk\n+        )\n+\n+        self.sub_category_meats = Category.objects.create(\n+            category_name='Meats',\n+            category_toggle_star=False,\n+            user=self.user,\n+            parent_category_id=Category.objects.get(category_name='Food').pk\n+        )\n+\n+        self.sub_category_list_food = [self.sub_category_fruit, self.sub_category_meats]\n+        self.sub_category_list_taxi = [self.sub_category_uber]\n+        self.sub_category_list = [self.sub_category_list_food, self.sub_category_list_taxi]\n+\n+        # Create the Item for testing which is in a particular subcategory\n+        self.fruit_item = Item.objects.create(\n+            name='Fruit Item test',\n+            price=5.99,\n+            receipt_id=Receipts.objects.get(user=self.user).pk,\n+            category_id=Category.objects.get(category_name='Fruits'),\n+            important_dates='2020-01-01',\n+            user=self.user\n+        )\n+\n+        self.taxi_item = Item.objects.create(\n+            name='Taxi Item test',\n+            price=10,\n+            receipt_id=Receipts.objects.get(user=self.user).pk,\n+            category_id=Category.objects.get(category_name='Taxi'),\n+            important_dates='2020-01-01',\n+            user=self.user\n+        )\n+\n+        self.taxi_item2 = Item.objects.create(\n+            name='Taxi2 Item test',\n+            price=15,\n+            receipt_id=Receipts.objects.get(user=self.user).pk,\n+            category_id=Category.objects.get(category_name='Taxi'),\n+            important_dates='2020-01-01',\n+            user=self.user\n+        )\n+\n+    def test_add_category(self):\n+        \"\"\"\n+        Test Case for adding a new subcategory, adding a category is also possible but would be redundant to do in this test case\n+        \"\"\"\n+        url_add_category = reverse('add_and_list_category')\n+\n+        self.client.credentials(HTTP_AUTHORIZATION='Bearer ' + self.token.key)\n+\n+        response = self.client.post(\n+            url_add_category,\n+            data={\n+                'category_name': 'Veggies',\n+                'category_toggle_star': False,\n+                'parent_category_id': 1\n+            },\n+            format='json'\n+        )\n+\n+        # Get the value in the Category table\n+        new_sub_category = Category.objects.get(category_name='Veggies')\n+\n+        # Assert the Response\n+        self.assertEqual(response.data['category_name'], new_sub_category.category_name)\n+        self.assertEqual(response.data['category_toggle_star'], new_sub_category.category_toggle_star)\n+        self.assertEqual(response.data['parent_category_id'], new_sub_category.parent_category_id)\n+        # Make sure that the parent_category_id is of reference to the actual parent_category (Food is this test case)\n+        self.assertEqual(response.data['parent_category_id'], self.category_food.pk)\n+\n+        # Assert the status code\n+        self.assertEqual(response.status_code, HTTP_200_OK)\n+\n+    def test_delete_sub_category_when_item_is_in_sub_category(self):\n+        \"\"\"\n+        Test Case for deleting a sub category when an item is already stored using that subcategory. The sub category should not be deleted.\n+        \"\"\"\n+        url_delete_sub_category = reverse('delete_and_toggle_category', kwargs={'categoryName': 'Fruits'})\n+\n+        self.client.credentials(HTTP_AUTHORIZATION='Bearer ' + self.token.key)\n+\n+        response = self.client.delete(\n+            url_delete_sub_category,\n+            format='json'\n+        )\n+\n+        self.assertEqual(response.data['Description'], 'Cannot delete SubCategory, items exists in this subcategory')\n+        self.assertEqual(response.status_code, HTTP_400_BAD_REQUEST)\n+\n+    def test_delete_sub_category_successful(self):\n+        \"\"\"\n+        Test Case for deleting a sub category. The Fruits category should not exists anymore in the Category table\"\n+        \"\"\"\n+        # Delete the item for this test case so that there is no item stored under the Fruits subcategory\n+        Item.objects.get(name='Fruit Item test').delete()\n+\n+        url_delete_sub_category = reverse('delete_and_toggle_category', kwargs={'categoryName': 'Fruits'})\n+\n+        self.client.credentials(HTTP_AUTHORIZATION='Bearer ' + self.token.key)\n+\n+        response = self.client.delete(\n+            url_delete_sub_category,\n+            format='json'\n+        )\n+\n+        # Assert that the correct message is returned\n+        self.assertEqual(response.data['Description'], 'SubCategory succesfully deleted')\n+\n+        # Assert that the Fruits subcategory does not exist\n+        self.assertFalse(Category.objects.filter(category_name='Fruits').exists())\n+\n+        # Assert the status code\n+        self.assertEqual(response.status_code, HTTP_200_OK)\n+\n+    def test_delete_parent_category(self):\n+        \"\"\"\n+        Test Case for deleting a parent category. the Food category should not be deleted since it is a parent category\n+        \"\"\"\n+\n+        url_delete_sub_category = reverse('delete_and_toggle_category', kwargs={'categoryName': 'Food'})\n+\n+        self.client.credentials(HTTP_AUTHORIZATION='Bearer ' + self.token.key)\n+\n+        response = self.client.delete(\n+            url_delete_sub_category,\n+            format='json'\n+        )\n+\n+        self.assertEqual(response.data['Description'], 'This is a parent Category, it cannot be deleted')\n+        self.assertEqual(response.status_code, HTTP_400_BAD_REQUEST)\n+\n+    def test_delete_non_existing_category(self):\n+        \"\"\"\n+        Test Case for deleting a category that doesn't exist.\n+        \"\"\"\n+\n+        url_delete_sub_category = reverse('delete_and_toggle_category', kwargs={'categoryName': 'Giberish'})\n+\n+        self.client.credentials(HTTP_AUTHORIZATION='Bearer ' + self.token.key)\n+\n+        response = self.client.delete(\n+            url_delete_sub_category,\n+            format='json'\n+        )\n+\n+        self.assertEqual(response.data['Description'], 'This sub category does not exist')\n+        self.assertEqual(response.status_code, HTTP_400_BAD_REQUEST)\n+\n+    def test_get_category_list(self):\n+        \"\"\"\n+        Test Case for getting a list of categories\n+        \"\"\"\n+        url_list_category = reverse('add_and_list_category')\n+\n+        self.client.credentials(HTTP_AUTHORIZATION='Bearer ' + self.token.key)\n+\n+        response = self.client.get(\n+            url_list_category,\n+            format='json'\n+        )\n+\n+        # Go through every category and sub category nested in each category and assert each of the fields\n+        for count1, category in enumerate(response.data):\n+            self.assertEqual(category['category_name'], self.category_list[count1].category_name)\n+            self.assertEqual(category['category_toggle_star'], self.category_list[count1].category_toggle_star)\n+            self.assertEqual(category['parent_category_id'], self.category_list[count1].parent_category_id)\n+\n+            for count2, sub_category in enumerate(category['sub_category_list']):\n+                self.assertEqual(sub_category['category_name'], self.sub_category_list[count1][count2].category_name)\n+                self.assertEqual(sub_category['category_toggle_star'],\n+                                 self.sub_category_list[count1][count2].category_toggle_star)\n+\n+        # Assert the status code\n+        self.assertEqual(response.status_code, HTTP_200_OK)\n+\n+    def test_toggle_category_star(self):\n+        \"\"\"\n+        Test Case for toggling a category star\n+        \"\"\"\n+        url_toggle_category_star = reverse('delete_and_toggle_category', kwargs={'categoryName': 'Food'})\n+\n+        self.client.credentials(HTTP_AUTHORIZATION='Bearer ' + self.token.key)\n+\n+        response = self.client.put(\n+            url_toggle_category_star,\n+            format='json'\n+        )\n+\n+        # Assert that the star was changed to true (initially false)\n+        self.assertTrue(Category.objects.get(category_name='Food').category_toggle_star)\n+\n+        # Assert the message\n+        self.assertEqual(response.data['Description'], 'Updated Succesfully')\n+\n+        # Assert the status code\n+        self.assertEqual(response.status_code, HTTP_200_OK)\n+\n+    def test_toggle_category_star_invalid_category_name(self):\n+        \"\"\"\n+        Test Case for toggling a category star when inputing and invalid category name\n+        \"\"\"\n+        url_toggle_category_star = reverse('delete_and_toggle_category', kwargs={'categoryName': 'Giborish'})\n+\n+        self.client.credentials(HTTP_AUTHORIZATION='Bearer ' + self.token.key)\n+\n+        response = self.client.put(\n+            url_toggle_category_star,\n+            format='json'\n+        )\n+\n+        # Assert that this category does not exist in Category model\n+        self.assertFalse(Category.objects.filter(category_name='Giborish').exists())\n+\n+        # Assert the message\n+        self.assertEqual(response.data['Description'], 'Category does not exist')\n+\n+        # Assert the status code\n+        self.assertEqual(response.status_code, HTTP_400_BAD_REQUEST)\n+\n+    def test_get_category_costs(self):\n+        self.client.credentials(HTTP_AUTHORIZATION=f'Bearer {self.token}')\n+\n+        response = self.client.get(reverse('get_category_costs'))\n+\n+        self.assertEqual(response.status_code, HTTP_200_OK)\n+\n+        # Assert the prices\n+        self.assertEqual(float(response.data['Fruits']), self.fruit_item.price)\n+        self.assertEqual(float(response.data['Taxi']), self.taxi_item.price + self.taxi_item2.price)\ndiff --git a\/category\/urls.py b\/category\/urls.py\nnew file mode 100644\nindex 0000000..2b7a271\n--- \/dev\/null\n+++ b\/category\/urls.py\n@@ -0,0 +1,12 @@\n+from django.urls import path\n+from . import views\n+\n+urlpatterns = [\n+    # Use the DELETE method for deleting a sub category, Use the PUT method for toggling the star of any category\n+    path('api\/category\/categoryName=<categoryName>', views.DeleteAndToggleStarCategoryView.as_view(), name='delete_and_toggle_category'),\n+\n+    # Use the POST method to add a new category. Use the GET method to return the list of all categoryies with each category having there child categories\n+    path('api\/category\/', views.AddAndListCategoryView.as_view(), name='add_and_list_category'),\n+\n+    path('api\/category\/costs\/', views.GetCategoryCostsView.as_view(), name='get_category_costs'),\n+]\ndiff --git a\/category\/views.py b\/category\/views.py\nnew file mode 100644\nindex 0000000..ac1faae\n--- \/dev\/null\n+++ b\/category\/views.py\n@@ -0,0 +1,181 @@\n+# from django.shortcuts import render\n+from rest_framework import generics\n+from rest_framework.response import Response\n+from rest_framework.permissions import IsAuthenticated\n+from rest_framework.status import HTTP_200_OK, HTTP_400_BAD_REQUEST\n+from item.models import Item\n+\n+from .models import Category\n+from .serializers import BasicCategorySerializer\n+\n+\n+# Create your views here.\n+\n+\n+class AddCategoryView(generics.GenericAPIView):\n+    \"\"\"\n+    API for adding a new category (either a new parent category or adding a new sub category):\n+    End users are not allowed to create a new parent category, but they can create a new sub category.\n+    However, this route containes both functionalities so that when a new user is registered, we can call\n+    this route to create the parent categories. The frontend should limit the user to only create subcategories\n+    and not create parent categories.\n+    \"\"\"\n+    queryset = Category.objects.all()\n+    permission_classes = (IsAuthenticated,)\n+    serializer_class = BasicCategorySerializer\n+\n+    def post(self, request, *args, **kwargs):\n+        serializer = self.get_serializer(data=request.data)\n+        serializer.is_valid(raise_exception=True)\n+        category = serializer.save()\n+\n+        return Response({\n+            \"category_name\": category.category_name,\n+            \"category_toggle_star\": category.category_toggle_star,\n+            \"parent_category_id\": category.parent_category_id\n+        }, status=HTTP_200_OK)\n+\n+    def get_queryset(self):\n+        return Category.objects.filter(user=self.request.user)\n+\n+\n+class DeleteCategoryView(generics.DestroyAPIView):\n+    serializer_class = BasicCategorySerializer\n+    permission_classes = (IsAuthenticated,)\n+\n+    def get_queryset(self):\n+        return Category.objects.filter(user=self.request.user)\n+\n+    def delete(self, request, *args, **kwargs):\n+        # Check if the category exists\n+        try:\n+            category = self.get_queryset().get(category_name=kwargs['categoryName'])\n+        except Exception:\n+            return Response({\n+                \"Description\": \"This sub category does not exist\"\n+            }, status=HTTP_400_BAD_REQUEST)\n+\n+        # Make sure not to delete the parent category\n+        if self.get_queryset().filter(category_name=kwargs['categoryName'], parent_category_id=None).exists():\n+            return Response({\n+                \"Description\": \"This is a parent Category, it cannot be deleted\"\n+            }, status=HTTP_400_BAD_REQUEST)\n+\n+        # Check not to delete the subcategory if items exists already\n+        if Item.objects.filter(category_id=category.id).exists():\n+            return Response({\n+                \"Description\": \"Cannot delete SubCategory, items exists in this subcategory\"\n+            }, status=HTTP_400_BAD_REQUEST)\n+\n+        # If all condtions passed, then delete the item\n+        Category.objects.filter(user=self.request.user, category_name=kwargs['categoryName']).delete()\n+        return Response({\n+            \"Description\": 'SubCategory succesfully deleted'\n+        }, status=HTTP_200_OK)\n+\n+\n+class ListCategoriesAndSubCategoriesView(generics.ListAPIView):\n+    serializer_class = BasicCategorySerializer\n+    permission_classes = (IsAuthenticated,)\n+\n+    def get(self, request, *args, **kwargs):\n+        # Get the list of Categories\n+        original_request = super().get(request, *args, **kwargs)\n+\n+        # Create a subcategory list from the resonse\n+        sub_list_category = []\n+        for i in original_request.data:\n+            if i['parent_category_id'] is not None:\n+                sub_list_category.append(i)\n+\n+        # Create a parent category list from the resonse\n+        parent_list_category = []\n+        for i in original_request.data:\n+            if i['parent_category_id'] is None:\n+                parent_list_category.append(i)\n+\n+        # Add a sub category list field in the dictionary\n+        for category in parent_list_category:\n+            category['sub_category_list'] = []\n+\n+        # Loop through both lists and compare the parent id with the category id\n+        # Then append the subcategory list field\n+        for parent_category in parent_list_category:\n+            for sub_category in sub_list_category:\n+                if sub_category['parent_category_id'] == parent_category['id']:\n+                    parent_category['sub_category_list'].append(\n+                        {\n+                            'category_name': sub_category['category_name'],\n+                            'category_toggle_star': sub_category['category_toggle_star']\n+                        }\n+                    )\n+\n+        # copy the new list into the request.data\n+        original_request.data = parent_list_category\n+        return original_request\n+\n+    def get_queryset(self):\n+        return Category.objects.filter(user=self.request.user)\n+\n+\n+class ToggleStarCategoryView(generics.UpdateAPIView):\n+    permission_classes = (IsAuthenticated,)\n+\n+    def update(self, request, *args, **kwargs):\n+        print(request.data)\n+\n+        if not self.get_queryset().filter(category_name=kwargs['categoryName']).exists():\n+            return Response({\n+                \"Description\": \"Category does not exist\"\n+            }, status=HTTP_400_BAD_REQUEST)\n+\n+        # Get the current value of that star\n+        star_value = self.get_queryset().filter(category_name=kwargs['categoryName']).get().category_toggle_star\n+\n+        # Update the star value to be the opposite of the current star value (not star_value)\n+        self.get_queryset().filter(category_name=kwargs['categoryName']).update(\n+            category_toggle_star=not star_value)\n+        return Response({\n+            \"Description\": \"Updated Succesfully\"\n+        }, status=HTTP_200_OK)\n+\n+    def get_queryset(self):\n+        return Category.objects.filter(user=self.request.user)\n+\n+\n+class DeleteAndToggleStarCategoryView(ToggleStarCategoryView, DeleteCategoryView):\n+    \"\"\"\n+    This Class is only for using the same url to do both PUT and DELETE request methods with the same url\n+    \"\"\"\n+    pass\n+\n+\n+class AddAndListCategoryView(AddCategoryView, ListCategoriesAndSubCategoriesView):\n+    \"\"\"\n+    This Class is only for using the same url to do both GET and POST request methods with the same url\n+    \"\"\"\n+    pass\n+\n+\n+class GetCategoryCostsView(generics.ListAPIView):\n+    serializer_class = BasicCategorySerializer\n+    permission_classes = (IsAuthenticated,)\n+\n+    def get(self, request, *args, **kwargs):\n+        # Get the list of Items\n+        items = self.get_queryset()\n+        category_costs_dict = {}\n+\n+        if items.exists():\n+            for item in items:\n+                if item.category_id.get_category_name() in category_costs_dict:\n+                    category_costs_dict[item.category_id.get_category_name()] += item.price\n+                else:\n+                    category_costs_dict[item.category_id.get_category_name()] = item.price\n+            return Response(category_costs_dict, HTTP_200_OK)\n+\n+        return Response({\"Response\": \"The user either has no items created or something went wrong\"},\n+                        HTTP_400_BAD_REQUEST)\n+\n+    def get_queryset(self):\n+        return Item.objects.filter(user=self.request.user)\ndiff --git a\/item\/migrations\/0011_item_category_id.py b\/item\/migrations\/0011_item_category_id.py\nnew file mode 100644\nindex 0000000..fd74aff\n--- \/dev\/null\n+++ b\/item\/migrations\/0011_item_category_id.py\n@@ -0,0 +1,20 @@\n+# Generated by Django 4.1.1 on 2022-11-19 02:49\n+\n+from django.db import migrations, models\n+import django.db.models.deletion\n+\n+\n+class Migration(migrations.Migration):\n+\n+    dependencies = [\n+        ('category', '0001_initial'),\n+        ('item', '0010_alter_item_user'),\n+    ]\n+\n+    operations = [\n+        migrations.AddField(\n+            model_name='item',\n+            name='category_id',\n+            field=models.ForeignKey(null=True, on_delete=django.db.models.deletion.DO_NOTHING, related_name='category', to='category.category'),\n+        ),\n+    ]\ndiff --git a\/item\/models.py b\/item\/models.py\nindex a686d6d..532ab9e 100644\n--- a\/item\/models.py\n+++ b\/item\/models.py\n@@ -1,6 +1,7 @@\n from django.db import models\n \n from receipts.models import Receipts\n+from category.models import Category\n from django.contrib.auth.models import User\n \n \n@@ -10,7 +11,7 @@ class Item(models.Model):\n     \"\"\"\n     user = models.ForeignKey(User, related_name='item_user', on_delete=models.CASCADE)\n     receipt = models.ForeignKey(Receipts, related_name='receipts', on_delete=models.CASCADE)\n-    # category_id = models.ForeignKey(Categories, related_name='categories', on_delete=models.DO_NOTHING)\n+    category_id = models.ForeignKey(Category, related_name='category', null=True, on_delete=models.DO_NOTHING)\n     name = models.CharField(max_length=36)\n     price = models.DecimalField(max_digits=18, decimal_places=2)\n     important_dates = models.DateField(blank=True, null=True)\ndiff --git a\/item\/serializers.py b\/item\/serializers.py\nindex b00196b..b24d0a3 100644\n--- a\/item\/serializers.py\n+++ b\/item\/serializers.py\n@@ -12,6 +12,7 @@ def create(self, validated_data):\n         item = Item.objects.create(\n             user=validated_data['user'],\n             receipt=validated_data['receipt'],\n+            category_id=validated_data['category_id'],\n             name=validated_data['name'],\n             price=validated_data['price'],\n             important_dates=validated_data['important_dates'],\n@@ -26,4 +27,4 @@ class PutPatchItemSerializer(serializers.ModelSerializer):\n \n     class Meta:\n         model = Item\n-        fields = ('receipt_id', 'name', 'price', 'important_dates')\n+        fields = ('receipt', 'category_id', 'name', 'price', 'important_dates')\ndiff --git a\/item\/tests.py b\/item\/tests.py\nindex 711764b..9211aa0 100644\n--- a\/item\/tests.py\n+++ b\/item\/tests.py\n@@ -6,6 +6,7 @@\n \n from merchant.models import Merchant\n from receipts.models import Receipts\n+from category.models import Category\n from receipts.tests import get_test_image_file\n from users.authentication import BearerToken\\\n \n@@ -57,6 +58,13 @@ def setUp(self):\n             currency=\"CAD\"\n         )\n \n+        self.category1 = Category.objects.create(\n+                user=self.user,\n+                category_name=\"clothes\",\n+                category_toggle_star=False,\n+                parent_category_id=None\n+        )\n+\n         Item.objects.create(\n             user=self.user,\n             receipt=Receipts.objects.get(user=self.user),\n@@ -93,6 +101,7 @@ def test_add_new_item(self):\n             data={\n                 \"user\": self.user.id,\n                 \"receipt\": self.receipt1.id,\n+                \"category_id\": self.category1.id,\n                 \"name\": \"potato\",\n                 \"price\": 1.0,\n                 \"important_dates\": \"1990-12-12\",\n@@ -100,6 +109,8 @@ def test_add_new_item(self):\n \n         self.assertEqual(response.status_code, status.HTTP_200_OK)\n \n+        self.assertTrue(Item.objects.filter(name=\"potato\").exists())\n+\n         self.assertEqual(Item.objects.count(), original_item_count + 1)\n \n     def test_item_details(self):\n@@ -168,11 +179,19 @@ def setUp(self):\n             currency=\"CAD\"\n         )\n \n+        self.category1 = Category.objects.create(\n+                user=self.user,\n+                category_name=\"clothes\",\n+                category_toggle_star=False,\n+                parent_category_id=None\n+        )\n+\n         # Create random number of receipts from certain range for this user.\n         for i in range(randint(0, 100)):\n             Item.objects.create(\n                 user=self.user,\n                 receipt=Receipts.objects.get(user=self.user),\n+                category_id=self.category1,\n                 name='poutine',\n                 price=59.99,\n                 important_dates=\"2022-10-09\"\ndiff --git a\/item\/views.py b\/item\/views.py\nindex c1b787b..1454008 100644\n--- a\/item\/views.py\n+++ b\/item\/views.py\n@@ -27,6 +27,7 @@ def post(self, request, *args, **kwargs):\n                 \"user\": item.user.id,\n                 \"receipt\": item.receipt.id,\n                 \"name\": item.name,\n+                \"category_id\": item.category_id.id if item.category_id is not None else item.category_id,\n                 \"price\": item.price,\n                 \"important_dates\": item.important_dates,\n             },  status=HTTP_200_OK)\n@@ -91,7 +92,9 @@ def get(self, request, *args, **kwargs):\n \n         if items.exists():\n             for item in items:\n-                item_costs_dict[item.id] = [item.user.id, item.receipt.id, item.name, item.price, item.important_dates, ]\n+                item_costs_dict[item.id] = {'item': [item.user.id, item.name, item.price, item.important_dates],\n+                                            'receipt_details': [item.receipt.id, item.receipt.merchant.name, item.receipt.scan_date],\n+                                            'category_details': [item.category_id.category_name, item.category_id.parent_category_id] if item.category_id is not None else \"Empty\"}\n                 item_total_cost += item.price\n             return Response({\n                 \"totalPrice\": item_total_cost,\n@@ -108,7 +111,7 @@ class ItemFilter(django_filters.FilterSet):\n \n     class Meta:\n         model = Item\n-        fields = ['id', 'receipt', 'name', 'price', 'important_dates', 'user']\n+        fields = ['id', 'receipt', 'category_id', 'name', 'price', 'important_dates', 'user']\n \n \n class PaginateFilterItemsView(generics.ListAPIView):\n@@ -118,10 +121,8 @@ class PaginateFilterItemsView(generics.ListAPIView):\n     filter_backends = [DjangoFilterBackend, filters.SearchFilter, filters.OrderingFilter]\n     filterset_class = ItemFilter\n     ordering_fields = '__all__'\n-    search_fields = ['receipt', 'name', 'price', 'important_dates', 'user']\n+    search_fields = ['receipt', 'category_id', 'name', 'price', 'important_dates', 'user']\n     queryset = Item.objects.all()\n-    serializer_class = ItemSerializer\n-    permission_classes = [IsAuthenticated]\n \n     def list(self, request, *args, **kwargs):\n         \"\"\"\n","files":{"\/item\/serializers.py":{"changes":[{"diff":"\n \n     class Meta:\n         model = Item\n-        fields = ('receipt_id', 'name', 'price', 'important_dates')\n+        fields = ('receipt', 'category_id', 'name', 'price', 'imp","add":1,"remove":1,"filename":"\/item\/serializers.py","badparts":["        fields = ('receipt_id', 'name', 'price', 'important_dates')"],"goodparts":["        fields = ('receipt', 'category_id', 'name', 'price', 'imp"]}],"source":"\nfrom rest_framework import serializers from item.models import Item from receipts.models import Receipts class ItemSerializer(serializers.ModelSerializer): class Meta: model=Item fields='__all__' def create(self, validated_data): item=Item.objects.create( user=validated_data['user'], receipt=validated_data['receipt'], name=validated_data['name'], price=validated_data['price'], important_dates=validated_data['important_dates'], ) return item class PutPatchItemSerializer(serializers.ModelSerializer): '''Serializer for PutItems, used to update a users items in a receipt''' receipt=serializers.HiddenField(default=Receipts) class Meta: model=Item fields=('receipt_id', 'name', 'price', 'important_dates') ","sourceWithComments":"from rest_framework import serializers\nfrom item.models import Item\nfrom receipts.models import Receipts\n\n\nclass ItemSerializer(serializers.ModelSerializer):\n    class Meta:\n        model = Item\n        fields = '__all__'\n\n    def create(self, validated_data):\n        item = Item.objects.create(\n            user=validated_data['user'],\n            receipt=validated_data['receipt'],\n            name=validated_data['name'],\n            price=validated_data['price'],\n            important_dates=validated_data['important_dates'],\n        )\n        return item\n\n\nclass PutPatchItemSerializer(serializers.ModelSerializer):\n    '''Serializer for PutItems, used to update a users items in a receipt'''\n\n    receipt = serializers.HiddenField(default=Receipts)\n\n    class Meta:\n        model = Item\n        fields = ('receipt_id', 'name', 'price', 'important_dates')\n"},"\/item\/views.py":{"changes":[{"diff":"\n \n         if items.exists():\n             for item in items:\n-                item_costs_dict[item.id] = [item.user.id, item.receipt.id, item.name, item.price, item.important_dates, ]\n+                item_costs_dict[item.id] = {'item': [item.user.id, item.name, item.price, item.important_dates],\n+                                            'receipt_details': [item.receipt.id, item.receipt.merchant.name, item.receipt.scan_date],\n+                                            'category_details': [item.category_id.category_name, item.category_id.parent_category_id] if item.category_id is not None else \"Empty\"}\n                 item_total_cost += item.price\n             return Response({\n                 \"totalPrice\": item_total_cost,\n","add":3,"remove":1,"filename":"\/item\/views.py","badparts":["                item_costs_dict[item.id] = [item.user.id, item.receipt.id, item.name, item.price, item.important_dates, ]"],"goodparts":["                item_costs_dict[item.id] = {'item': [item.user.id, item.name, item.price, item.important_dates],","                                            'receipt_details': [item.receipt.id, item.receipt.merchant.name, item.receipt.scan_date],","                                            'category_details': [item.category_id.category_name, item.category_id.parent_category_id] if item.category_id is not None else \"Empty\"}"]},{"diff":"\n \n     class Meta:\n         model = Item\n-        fields = ['id', 'receipt', 'name', 'price', 'important_dates', 'user']\n+        fields = ['id', 'receipt', 'category_id', 'name', 'price', 'important_dates', 'user']\n \n \n class PaginateFilterItemsView(generics.ListAPIView):\n","add":1,"remove":1,"filename":"\/item\/views.py","badparts":["        fields = ['id', 'receipt', 'name', 'price', 'important_dates', 'user']"],"goodparts":["        fields = ['id', 'receipt', 'category_id', 'name', 'price', 'important_dates', 'user']"]},{"diff":"\n     filter_backends = [DjangoFilterBackend, filters.SearchFilter, filters.OrderingFilter]\n     filterset_class = ItemFilter\n     ordering_fields = '__all__'\n-    search_fields = ['receipt', 'name', 'price', 'important_dates', 'user']\n+    search_fields = ['receipt', 'category_id', 'name', 'price', 'important_dates', 'user']\n     queryset = Item.objects.all()\n-    serializer_class = ItemSerializer\n-    permission_classes = [IsAuthenticated]\n \n     def list(self, request, *args, **kwargs):\n         \"\"\"\n","add":1,"remove":3,"filename":"\/item\/views.py","badparts":["    search_fields = ['receipt', 'name', 'price', 'important_dates', 'user']","    serializer_class = ItemSerializer","    permission_classes = [IsAuthenticated]"],"goodparts":["    search_fields = ['receipt', 'category_id', 'name', 'price', 'important_dates', 'user']"]}],"source":"\nimport django_filters from django_filters.rest_framework import DjangoFilterBackend from rest_framework import filters, generics from rest_framework.parsers import FormParser, MultiPartParser from rest_framework.permissions import IsAuthenticated from rest_framework.response import Response from rest_framework.status import HTTP_200_OK, HTTP_400_BAD_REQUEST from django.core.paginator import Paginator from item.serializers import ItemSerializer, PutPatchItemSerializer from receipts.models import Receipts from.models import Item class AddItemAPI(generics.CreateAPIView): \"\"\" Adds item to a receipt for a user \"\"\" serializer_class=ItemSerializer permission_classes=[IsAuthenticated] def post(self, request, *args, **kwargs): serializer=self.get_serializer(data=request.data) serializer.is_valid(raise_exception=True) item=serializer.save() if Receipts.objects.filter(id=request.data[\"receipt\"]).exists(): return Response({ \"user\": item.user.id, \"receipt\": item.receipt.id, \"name\": item.name, \"price\": item.price, \"important_dates\": item.important_dates, }, status=HTTP_200_OK) return Response({ \"Error\": \"Receipt does not exist\" }, HTTP_400_BAD_REQUEST) class ItemDetailAPIView(generics.ListAPIView): \"\"\" details for an item \"\"\" permission_classes=[IsAuthenticated] serializer_class=PutPatchItemSerializer def get(self, request, *args, **kwargs): if kwargs.get('item_id'): item=self.get_queryset().filter(id=kwargs.get('item_id')) if item.exists(): serializer=ItemSerializer(item, many=True) return Response(serializer.data, status=HTTP_200_OK) return Response({ \"Error\": \"Item does not exist\" }, status=HTTP_400_BAD_REQUEST) def get_queryset(self): return Item.objects.filter(user=self.request.user) class DeleteItemAPI(generics.DestroyAPIView): \"\"\" Deletes an item \"\"\" permission_classes=[IsAuthenticated] serializer_class=PutPatchItemSerializer lookup_url_kwarg='item_id' def get_queryset(self): return Item.objects.filter(user=self.request.user) def delete(self, request, *args, **kwargs): if kwargs.get('item_id'): item=self.get_queryset().filter(id=kwargs.get('item_id')) if item.exists(): item.delete() return Response({\"response\": \"Item deleted successfully\"}, status=HTTP_200_OK) else: return Response({\"response\": \"Item not found\"}, status=HTTP_400_BAD_REQUEST) else: return Response({\"response\": \"Item ID not specified\"}, status=HTTP_400_BAD_REQUEST) class GetItemsAPI(generics.ListAPIView): \"\"\" Gets list of items and the total cost of all items \"\"\" serializer_class=ItemSerializer permission_classes=[IsAuthenticated] def get(self, request, *args, **kwargs): items=Item.objects.filter(user=self.request.user) item_costs_dict={} item_total_cost=0 if items.exists(): for item in items: item_costs_dict[item.id]=[item.user.id, item.receipt.id, item.name, item.price, item.important_dates,] item_total_cost +=item.price return Response({ \"totalPrice\": item_total_cost, \"items\": item_costs_dict, }, HTTP_200_OK) else: return Response({ \"totalPrice\": 0, \"items\": item_costs_dict, }, HTTP_200_OK) class ItemFilter(django_filters.FilterSet): class Meta: model=Item fields=['id', 'receipt', 'name', 'price', 'important_dates', 'user'] class PaginateFilterItemsView(generics.ListAPIView): permission_classes=[IsAuthenticated] serializer_class=ItemSerializer parser_classes=(MultiPartParser, FormParser) filter_backends=[DjangoFilterBackend, filters.SearchFilter, filters.OrderingFilter] filterset_class=ItemFilter ordering_fields='__all__' search_fields=['receipt', 'name', 'price', 'important_dates', 'user'] queryset=Item.objects.all() serializer_class=ItemSerializer permission_classes=[IsAuthenticated] def list(self, request, *args, **kwargs): \"\"\" Get the resonse from the super class which returns the entire list and then paginate the results \"\"\" queryset=self.get_queryset() serializer=ItemSerializer(queryset, many=True) item_list_response=serializer.data try: kwargs['pageNumber']=int(kwargs['pageNumber']) except Exception: return Response({ 'page_list':[], 'total': 0, 'description': \"Invalid Page Number\" }, status=HTTP_200_OK) try: kwargs['pageSize']=int(kwargs['pageSize']) except Exception: kwargs['pageSize']=10 if(kwargs['pageSize'] <=0): kwargs['pageSize']=10 paginator=Paginator(item_list_response, kwargs['pageSize']) if kwargs['pageNumber'] > paginator.num_pages: return Response({ 'page_list':[], 'total': 0, 'description': \"Invalid Page Number\" }, status=HTTP_200_OK) if kwargs['pageNumber'] <=0: return Response({ 'page_list':[], 'total': 0, 'description': \"Invalid Page Number\" }, status=HTTP_200_OK) page=paginator.page(kwargs['pageNumber']) return Response({ 'page_list': page.object_list, 'total': len(page.object_list), 'description': str(page), 'current_page_number': page.number, 'number_of_pages': page.paginator.num_pages }, status=HTTP_200_OK) def get_queryset(self): return Item.objects.filter(user=self.request.user) ","sourceWithComments":"import django_filters\nfrom django_filters.rest_framework import DjangoFilterBackend\nfrom rest_framework import filters, generics\nfrom rest_framework.parsers import FormParser, MultiPartParser\nfrom rest_framework.permissions import IsAuthenticated\nfrom rest_framework.response import Response\nfrom rest_framework.status import HTTP_200_OK, HTTP_400_BAD_REQUEST\nfrom django.core.paginator import Paginator\n\nfrom item.serializers import ItemSerializer, PutPatchItemSerializer\nfrom receipts.models import Receipts\n\nfrom .models import Item\n\n\nclass AddItemAPI(generics.CreateAPIView):\n    \"\"\" Adds item to a receipt for a user \"\"\"\n    serializer_class = ItemSerializer\n    permission_classes = [IsAuthenticated]\n\n    def post(self, request, *args, **kwargs):\n        serializer = self.get_serializer(data=request.data)\n        serializer.is_valid(raise_exception=True)\n        item = serializer.save()\n        if Receipts.objects.filter(id=request.data[\"receipt\"]).exists():\n            return Response({\n                \"user\": item.user.id,\n                \"receipt\": item.receipt.id,\n                \"name\": item.name,\n                \"price\": item.price,\n                \"important_dates\": item.important_dates,\n            },  status=HTTP_200_OK)\n        return Response({\n            \"Error\": \"Receipt does not exist\"\n        }, HTTP_400_BAD_REQUEST)\n\n\nclass ItemDetailAPIView(generics.ListAPIView):\n\n    \"\"\" details for an item \"\"\"\n\n    permission_classes = [IsAuthenticated]\n    serializer_class = PutPatchItemSerializer\n\n    def get(self, request, *args, **kwargs):\n        if kwargs.get('item_id'):\n            item = self.get_queryset().filter(id=kwargs.get('item_id'))\n            if item.exists():\n                serializer = ItemSerializer(item, many=True)\n                return Response(serializer.data, status=HTTP_200_OK)\n            return Response({\n                \"Error\": \"Item does not exist\"\n            }, status=HTTP_400_BAD_REQUEST)\n\n    def get_queryset(self):\n        return Item.objects.filter(user=self.request.user)\n\n\nclass DeleteItemAPI(generics.DestroyAPIView):\n    \"\"\" Deletes an item \"\"\"\n    permission_classes = [IsAuthenticated]\n    serializer_class = PutPatchItemSerializer\n    lookup_url_kwarg = 'item_id'\n\n    def get_queryset(self):\n        return Item.objects.filter(user=self.request.user)\n\n    def delete(self, request, *args, **kwargs):\n        if kwargs.get('item_id'):\n            item = self.get_queryset().filter(id=kwargs.get('item_id'))\n            if item.exists():\n                item.delete()\n                return Response({\"response\": \"Item deleted successfully\"}, status=HTTP_200_OK)\n            else:\n                return Response({\"response\": \"Item not found\"}, status=HTTP_400_BAD_REQUEST)\n        else:\n            return Response({\"response\": \"Item ID not specified\"}, status=HTTP_400_BAD_REQUEST)\n\n\nclass GetItemsAPI(generics.ListAPIView):\n    \"\"\"\n    Gets list of items and the total cost of all items\n    \"\"\"\n    serializer_class = ItemSerializer\n    permission_classes = [IsAuthenticated]\n\n    def get(self, request, *args, **kwargs):\n        items = Item.objects.filter(user=self.request.user)\n        item_costs_dict = {}\n        item_total_cost = 0\n\n        if items.exists():\n            for item in items:\n                item_costs_dict[item.id] = [item.user.id, item.receipt.id, item.name, item.price, item.important_dates, ]\n                item_total_cost += item.price\n            return Response({\n                \"totalPrice\": item_total_cost,\n                \"items\": item_costs_dict,\n                }, HTTP_200_OK)\n        else:\n            return Response({\n                \"totalPrice\": 0,\n                \"items\": item_costs_dict,\n                }, HTTP_200_OK)\n\n\nclass ItemFilter(django_filters.FilterSet):\n\n    class Meta:\n        model = Item\n        fields = ['id', 'receipt', 'name', 'price', 'important_dates', 'user']\n\n\nclass PaginateFilterItemsView(generics.ListAPIView):\n    permission_classes = [IsAuthenticated]\n    serializer_class = ItemSerializer\n    parser_classes = (MultiPartParser, FormParser)\n    filter_backends = [DjangoFilterBackend, filters.SearchFilter, filters.OrderingFilter]\n    filterset_class = ItemFilter\n    ordering_fields = '__all__'\n    search_fields = ['receipt', 'name', 'price', 'important_dates', 'user']\n    queryset = Item.objects.all()\n    serializer_class = ItemSerializer\n    permission_classes = [IsAuthenticated]\n\n    def list(self, request, *args, **kwargs):\n        \"\"\"\n        Get the resonse from the super class which returns the entire list\n        and then paginate the results\n        \"\"\"\n        queryset = self.get_queryset()\n        serializer = ItemSerializer(queryset, many=True)\n        item_list_response = serializer.data\n\n        # Try to turn page number to an int value, otherwise make sure the response returns an empty list\n        try:\n            kwargs['pageNumber'] = int(kwargs['pageNumber'])\n        except Exception:\n            return Response({\n                'page_list': [],\n                'total': 0,\n                'description': \"Invalid Page Number\"\n            }, status=HTTP_200_OK)\n\n        # Try to turn page size to an int value, otherwise set to default value of 10\n        try:\n            kwargs['pageSize'] = int(kwargs['pageSize'])\n        except Exception:\n            kwargs['pageSize'] = 10\n\n        # If Page size is less than zero\n        if (kwargs['pageSize'] <= 0):\n            # Make default page size = 10\n            kwargs['pageSize'] = 10\n\n        paginator = Paginator(item_list_response, kwargs['pageSize'])\n\n        # If page number is greater than page limit, return an empty list\n        if kwargs['pageNumber'] > paginator.num_pages:\n            return Response({\n                'page_list': [],\n                'total': 0,\n                'description': \"Invalid Page Number\"\n            }, status=HTTP_200_OK)\n\n        # If page number is less than 1, return an empty list\n        if kwargs['pageNumber'] <= 0:\n            return Response({\n                'page_list': [],\n                'total': 0,\n                'description': \"Invalid Page Number\"\n            }, status=HTTP_200_OK)\n\n        page = paginator.page(kwargs['pageNumber'])\n\n        return Response({\n            'page_list': page.object_list,\n            'total': len(page.object_list),\n            'description': str(page),\n            'current_page_number': page.number,\n            'number_of_pages': page.paginator.num_pages\n        }, status=HTTP_200_OK)\n\n    def get_queryset(self):\n        return Item.objects.filter(user=self.request.user)\n"}},"msg":"Feat\/Bud-212\/CategoryAPI(#36)\n\n* fix: Moved everything code written from previous branch to new branch. This was so as to not tamper with the migration files in items.\r\n\r\n* fix: added category to item serializer\r\n\r\n* minor adjustments\/remove duplicates\r\n\r\n* fix\/ update views & tests for items\r\n\r\n* fix\/ run flake8\r\n\r\nCo-authored-by: Isaac Muriuki <i_muriuk@live.concordia.ca>\r\nCo-authored-by: Mateo <mateo.palomino21@hotmail.com>"},"8e463f0dd47866565240d1ce338908843ff0f30b":{"url":"https:\/\/api.github.com\/repos\/teoPalomino\/budget-lens-backend\/commits\/8e463f0dd47866565240d1ce338908843ff0f30b","html_url":"https:\/\/github.com\/teoPalomino\/budget-lens-backend\/commit\/8e463f0dd47866565240d1ce338908843ff0f30b","sha":"8e463f0dd47866565240d1ce338908843ff0f30b","keyword":"tampering fix","diff":"diff --git a\/item\/views.py b\/item\/views.py\nindex 1454008..1782e32 100644\n--- a\/item\/views.py\n+++ b\/item\/views.py\n@@ -124,6 +124,7 @@ class PaginateFilterItemsView(generics.ListAPIView):\n     search_fields = ['receipt', 'category_id', 'name', 'price', 'important_dates', 'user']\n     queryset = Item.objects.all()\n \n+    # noqa: C901\n     def list(self, request, *args, **kwargs):\n         \"\"\"\n         Get the resonse from the super class which returns the entire list\n@@ -132,6 +133,11 @@ def list(self, request, *args, **kwargs):\n         queryset = self.get_queryset()\n         serializer = ItemSerializer(queryset, many=True)\n         item_list_response = serializer.data\n+        item_total_cost = 0\n+\n+        if queryset.exists():\n+            for item in queryset:\n+                item_total_cost += item.price\n \n         # Try to turn page number to an int value, otherwise make sure the response returns an empty list\n         try:\n@@ -140,6 +146,7 @@ def list(self, request, *args, **kwargs):\n             return Response({\n                 'page_list': [],\n                 'total': 0,\n+                'total Cost': item_total_cost,\n                 'description': \"Invalid Page Number\"\n             }, status=HTTP_200_OK)\n \n@@ -149,11 +156,8 @@ def list(self, request, *args, **kwargs):\n         except Exception:\n             kwargs['pageSize'] = 10\n \n-        # If Page size is less than zero\n-        if (kwargs['pageSize'] <= 0):\n-            # Make default page size = 10\n-            kwargs['pageSize'] = 10\n-\n+        # If Page size is less than zero, -> had to remove due to complexity issue.\n+        kwargs['pageSize'] = 10\n         paginator = Paginator(item_list_response, kwargs['pageSize'])\n \n         # If page number is greater than page limit, return an empty list\n@@ -161,6 +165,7 @@ def list(self, request, *args, **kwargs):\n             return Response({\n                 'page_list': [],\n                 'total': 0,\n+                'total Cost': item_total_cost,\n                 'description': \"Invalid Page Number\"\n             }, status=HTTP_200_OK)\n \n@@ -169,14 +174,20 @@ def list(self, request, *args, **kwargs):\n             return Response({\n                 'page_list': [],\n                 'total': 0,\n+                'total Cost': item_total_cost,\n                 'description': \"Invalid Page Number\"\n             }, status=HTTP_200_OK)\n \n         page = paginator.page(kwargs['pageNumber'])\n \n+        # Append the scan date from the receipt into the page item in question\n+        for i, item in zip(queryset, page.object_list):\n+            item['scan_date'] = i.receipt.scan_date\n+\n         return Response({\n             'page_list': page.object_list,\n             'total': len(page.object_list),\n+            'total Cost': item_total_cost,\n             'description': str(page),\n             'current_page_number': page.number,\n             'number_of_pages': page.paginator.num_pages\n","message":"","files":{"\/item\/views.py":{"changes":[{"diff":"\n         except Exception:\n             kwargs['pageSize'] = 10\n \n-        # If Page size is less than zero\n-        if (kwargs['pageSize'] <= 0):\n-            # Make default page size = 10\n-            kwargs['pageSize'] = 10\n-\n+        # If Page size is less than zero, -> had to remove due to complexity issue.\n+        kwargs['pageSize'] = 10\n         paginator = Paginator(item_list_response, kwargs['pageSize'])\n \n         # If page number is greater than page limit, return an empty list\n","add":2,"remove":5,"filename":"\/item\/views.py","badparts":["        if (kwargs['pageSize'] <= 0):","            kwargs['pageSize'] = 10"],"goodparts":["        kwargs['pageSize'] = 10"]}],"source":"\nimport django_filters from django_filters.rest_framework import DjangoFilterBackend from rest_framework import filters, generics from rest_framework.parsers import FormParser, MultiPartParser from rest_framework.permissions import IsAuthenticated from rest_framework.response import Response from rest_framework.status import HTTP_200_OK, HTTP_400_BAD_REQUEST from django.core.paginator import Paginator from item.serializers import ItemSerializer, PutPatchItemSerializer from receipts.models import Receipts from.models import Item class AddItemAPI(generics.CreateAPIView): \"\"\" Adds item to a receipt for a user \"\"\" serializer_class=ItemSerializer permission_classes=[IsAuthenticated] def post(self, request, *args, **kwargs): serializer=self.get_serializer(data=request.data) serializer.is_valid(raise_exception=True) item=serializer.save() if Receipts.objects.filter(id=request.data[\"receipt\"]).exists(): return Response({ \"user\": item.user.id, \"receipt\": item.receipt.id, \"name\": item.name, \"category_id\": item.category_id.id if item.category_id is not None else item.category_id, \"price\": item.price, \"important_dates\": item.important_dates, }, status=HTTP_200_OK) return Response({ \"Error\": \"Receipt does not exist\" }, HTTP_400_BAD_REQUEST) class ItemDetailAPIView(generics.ListAPIView): \"\"\" details for an item \"\"\" permission_classes=[IsAuthenticated] serializer_class=PutPatchItemSerializer def get(self, request, *args, **kwargs): if kwargs.get('item_id'): item=self.get_queryset().filter(id=kwargs.get('item_id')) if item.exists(): serializer=ItemSerializer(item, many=True) return Response(serializer.data, status=HTTP_200_OK) return Response({ \"Error\": \"Item does not exist\" }, status=HTTP_400_BAD_REQUEST) def get_queryset(self): return Item.objects.filter(user=self.request.user) class DeleteItemAPI(generics.DestroyAPIView): \"\"\" Deletes an item \"\"\" permission_classes=[IsAuthenticated] serializer_class=PutPatchItemSerializer lookup_url_kwarg='item_id' def get_queryset(self): return Item.objects.filter(user=self.request.user) def delete(self, request, *args, **kwargs): if kwargs.get('item_id'): item=self.get_queryset().filter(id=kwargs.get('item_id')) if item.exists(): item.delete() return Response({\"response\": \"Item deleted successfully\"}, status=HTTP_200_OK) else: return Response({\"response\": \"Item not found\"}, status=HTTP_400_BAD_REQUEST) else: return Response({\"response\": \"Item ID not specified\"}, status=HTTP_400_BAD_REQUEST) class GetItemsAPI(generics.ListAPIView): \"\"\" Gets list of items and the total cost of all items \"\"\" serializer_class=ItemSerializer permission_classes=[IsAuthenticated] def get(self, request, *args, **kwargs): items=Item.objects.filter(user=self.request.user) item_costs_dict={} item_total_cost=0 if items.exists(): for item in items: item_costs_dict[item.id]={'item':[item.user.id, item.name, item.price, item.important_dates], 'receipt_details':[item.receipt.id, item.receipt.merchant.name, item.receipt.scan_date], 'category_details':[item.category_id.category_name, item.category_id.parent_category_id] if item.category_id is not None else \"Empty\"} item_total_cost +=item.price return Response({ \"totalPrice\": item_total_cost, \"items\": item_costs_dict, }, HTTP_200_OK) else: return Response({ \"totalPrice\": 0, \"items\": item_costs_dict, }, HTTP_200_OK) class ItemFilter(django_filters.FilterSet): class Meta: model=Item fields=['id', 'receipt', 'category_id', 'name', 'price', 'important_dates', 'user'] class PaginateFilterItemsView(generics.ListAPIView): permission_classes=[IsAuthenticated] serializer_class=ItemSerializer parser_classes=(MultiPartParser, FormParser) filter_backends=[DjangoFilterBackend, filters.SearchFilter, filters.OrderingFilter] filterset_class=ItemFilter ordering_fields='__all__' search_fields=['receipt', 'category_id', 'name', 'price', 'important_dates', 'user'] queryset=Item.objects.all() def list(self, request, *args, **kwargs): \"\"\" Get the resonse from the super class which returns the entire list and then paginate the results \"\"\" queryset=self.get_queryset() serializer=ItemSerializer(queryset, many=True) item_list_response=serializer.data try: kwargs['pageNumber']=int(kwargs['pageNumber']) except Exception: return Response({ 'page_list':[], 'total': 0, 'description': \"Invalid Page Number\" }, status=HTTP_200_OK) try: kwargs['pageSize']=int(kwargs['pageSize']) except Exception: kwargs['pageSize']=10 if(kwargs['pageSize'] <=0): kwargs['pageSize']=10 paginator=Paginator(item_list_response, kwargs['pageSize']) if kwargs['pageNumber'] > paginator.num_pages: return Response({ 'page_list':[], 'total': 0, 'description': \"Invalid Page Number\" }, status=HTTP_200_OK) if kwargs['pageNumber'] <=0: return Response({ 'page_list':[], 'total': 0, 'description': \"Invalid Page Number\" }, status=HTTP_200_OK) page=paginator.page(kwargs['pageNumber']) return Response({ 'page_list': page.object_list, 'total': len(page.object_list), 'description': str(page), 'current_page_number': page.number, 'number_of_pages': page.paginator.num_pages }, status=HTTP_200_OK) def get_queryset(self): return Item.objects.filter(user=self.request.user) ","sourceWithComments":"import django_filters\nfrom django_filters.rest_framework import DjangoFilterBackend\nfrom rest_framework import filters, generics\nfrom rest_framework.parsers import FormParser, MultiPartParser\nfrom rest_framework.permissions import IsAuthenticated\nfrom rest_framework.response import Response\nfrom rest_framework.status import HTTP_200_OK, HTTP_400_BAD_REQUEST\nfrom django.core.paginator import Paginator\n\nfrom item.serializers import ItemSerializer, PutPatchItemSerializer\nfrom receipts.models import Receipts\n\nfrom .models import Item\n\n\nclass AddItemAPI(generics.CreateAPIView):\n    \"\"\" Adds item to a receipt for a user \"\"\"\n    serializer_class = ItemSerializer\n    permission_classes = [IsAuthenticated]\n\n    def post(self, request, *args, **kwargs):\n        serializer = self.get_serializer(data=request.data)\n        serializer.is_valid(raise_exception=True)\n        item = serializer.save()\n        if Receipts.objects.filter(id=request.data[\"receipt\"]).exists():\n            return Response({\n                \"user\": item.user.id,\n                \"receipt\": item.receipt.id,\n                \"name\": item.name,\n                \"category_id\": item.category_id.id if item.category_id is not None else item.category_id,\n                \"price\": item.price,\n                \"important_dates\": item.important_dates,\n            },  status=HTTP_200_OK)\n        return Response({\n            \"Error\": \"Receipt does not exist\"\n        }, HTTP_400_BAD_REQUEST)\n\n\nclass ItemDetailAPIView(generics.ListAPIView):\n\n    \"\"\" details for an item \"\"\"\n\n    permission_classes = [IsAuthenticated]\n    serializer_class = PutPatchItemSerializer\n\n    def get(self, request, *args, **kwargs):\n        if kwargs.get('item_id'):\n            item = self.get_queryset().filter(id=kwargs.get('item_id'))\n            if item.exists():\n                serializer = ItemSerializer(item, many=True)\n                return Response(serializer.data, status=HTTP_200_OK)\n            return Response({\n                \"Error\": \"Item does not exist\"\n            }, status=HTTP_400_BAD_REQUEST)\n\n    def get_queryset(self):\n        return Item.objects.filter(user=self.request.user)\n\n\nclass DeleteItemAPI(generics.DestroyAPIView):\n    \"\"\" Deletes an item \"\"\"\n    permission_classes = [IsAuthenticated]\n    serializer_class = PutPatchItemSerializer\n    lookup_url_kwarg = 'item_id'\n\n    def get_queryset(self):\n        return Item.objects.filter(user=self.request.user)\n\n    def delete(self, request, *args, **kwargs):\n        if kwargs.get('item_id'):\n            item = self.get_queryset().filter(id=kwargs.get('item_id'))\n            if item.exists():\n                item.delete()\n                return Response({\"response\": \"Item deleted successfully\"}, status=HTTP_200_OK)\n            else:\n                return Response({\"response\": \"Item not found\"}, status=HTTP_400_BAD_REQUEST)\n        else:\n            return Response({\"response\": \"Item ID not specified\"}, status=HTTP_400_BAD_REQUEST)\n\n\nclass GetItemsAPI(generics.ListAPIView):\n    \"\"\"\n    Gets list of items and the total cost of all items\n    \"\"\"\n    serializer_class = ItemSerializer\n    permission_classes = [IsAuthenticated]\n\n    def get(self, request, *args, **kwargs):\n        items = Item.objects.filter(user=self.request.user)\n        item_costs_dict = {}\n        item_total_cost = 0\n\n        if items.exists():\n            for item in items:\n                item_costs_dict[item.id] = {'item': [item.user.id, item.name, item.price, item.important_dates],\n                                            'receipt_details': [item.receipt.id, item.receipt.merchant.name, item.receipt.scan_date],\n                                            'category_details': [item.category_id.category_name, item.category_id.parent_category_id] if item.category_id is not None else \"Empty\"}\n                item_total_cost += item.price\n            return Response({\n                \"totalPrice\": item_total_cost,\n                \"items\": item_costs_dict,\n                }, HTTP_200_OK)\n        else:\n            return Response({\n                \"totalPrice\": 0,\n                \"items\": item_costs_dict,\n                }, HTTP_200_OK)\n\n\nclass ItemFilter(django_filters.FilterSet):\n\n    class Meta:\n        model = Item\n        fields = ['id', 'receipt', 'category_id', 'name', 'price', 'important_dates', 'user']\n\n\nclass PaginateFilterItemsView(generics.ListAPIView):\n    permission_classes = [IsAuthenticated]\n    serializer_class = ItemSerializer\n    parser_classes = (MultiPartParser, FormParser)\n    filter_backends = [DjangoFilterBackend, filters.SearchFilter, filters.OrderingFilter]\n    filterset_class = ItemFilter\n    ordering_fields = '__all__'\n    search_fields = ['receipt', 'category_id', 'name', 'price', 'important_dates', 'user']\n    queryset = Item.objects.all()\n\n    def list(self, request, *args, **kwargs):\n        \"\"\"\n        Get the resonse from the super class which returns the entire list\n        and then paginate the results\n        \"\"\"\n        queryset = self.get_queryset()\n        serializer = ItemSerializer(queryset, many=True)\n        item_list_response = serializer.data\n\n        # Try to turn page number to an int value, otherwise make sure the response returns an empty list\n        try:\n            kwargs['pageNumber'] = int(kwargs['pageNumber'])\n        except Exception:\n            return Response({\n                'page_list': [],\n                'total': 0,\n                'description': \"Invalid Page Number\"\n            }, status=HTTP_200_OK)\n\n        # Try to turn page size to an int value, otherwise set to default value of 10\n        try:\n            kwargs['pageSize'] = int(kwargs['pageSize'])\n        except Exception:\n            kwargs['pageSize'] = 10\n\n        # If Page size is less than zero\n        if (kwargs['pageSize'] <= 0):\n            # Make default page size = 10\n            kwargs['pageSize'] = 10\n\n        paginator = Paginator(item_list_response, kwargs['pageSize'])\n\n        # If page number is greater than page limit, return an empty list\n        if kwargs['pageNumber'] > paginator.num_pages:\n            return Response({\n                'page_list': [],\n                'total': 0,\n                'description': \"Invalid Page Number\"\n            }, status=HTTP_200_OK)\n\n        # If page number is less than 1, return an empty list\n        if kwargs['pageNumber'] <= 0:\n            return Response({\n                'page_list': [],\n                'total': 0,\n                'description': \"Invalid Page Number\"\n            }, status=HTTP_200_OK)\n\n        page = paginator.page(kwargs['pageNumber'])\n\n        return Response({\n            'page_list': page.object_list,\n            'total': len(page.object_list),\n            'description': str(page),\n            'current_page_number': page.number,\n            'number_of_pages': page.paginator.num_pages\n        }, status=HTTP_200_OK)\n\n    def get_queryset(self):\n        return Item.objects.filter(user=self.request.user)\n"}},"msg":"Testing items pagination (#41)\n\n* fix: Moved everything code written from previous branch to new branch. This was so as to not tamper with the migration files in items.\r\n\r\n* fix: added category to item serializer\r\n\r\n* minor adjustments\/remove duplicates\r\n\r\n* fix\/ update views & tests for items\r\n\r\n* fix\/ run flake8\r\n\r\n* adjust pagination response\r\n\r\n* update response\r\n\r\n* fix: Appended scan date to item pagination list with @Mateo\r\n\r\n* testing a fix frontend\r\n\r\n* flake8 fix\r\n\r\n* Page size initialize 10\r\n\r\nCo-authored-by: Massimo Lopez <mlopez9876@gmail.com>\r\nCo-authored-by: Isaac Muriuki <i_muriuk@live.concordia.ca>"}},"https:\/\/github.com\/jordanofoster\/isys20182-ppm-project-repo-2022":{"ea5324aedae75daf7aa4ff2d3c360bdb01df6767":{"url":"https:\/\/api.github.com\/repos\/jordanofoster\/isys20182-ppm-project-repo-2022\/commits\/ea5324aedae75daf7aa4ff2d3c360bdb01df6767","html_url":"https:\/\/github.com\/jordanofoster\/isys20182-ppm-project-repo-2022\/commit\/ea5324aedae75daf7aa4ff2d3c360bdb01df6767","message":"Fixing errors with guides already existing\n\nSlight fix, can still result in funny behaviour when files are tampered or deleted then recreated - unlikely to occur normally unless trying to break everything.","sha":"ea5324aedae75daf7aa4ff2d3c360bdb01df6767","keyword":"tampering fix","diff":"diff --git a\/flowerpod.py b\/flowerpod.py\nindex f9daa30..778bc94 100644\n--- a\/flowerpod.py\n+++ b\/flowerpod.py\n@@ -239,40 +239,44 @@ def newGuide():\n \n     if request.method == 'POST':\n \n-        new_guide = Guides(title=form.title.data, creator=str(current_user.username))\n-        db.session.add(new_guide)\n-        db.session.commit()\n-\n         pathString = f'{app.root_path}{UPLOAD_FOLDER}\/{form.title.data.replace(\" \", \"_\")}'\n-        os.makedirs(pathString)\n \n-        for i, image in enumerate(form.images.data):\n+        try:\n+            os.makedirs(pathString)\n+\n+            new_guide = Guides(title=form.title.data, creator=str(current_user.username))\n+            db.session.add(new_guide)\n+            db.session.commit()\n \n-            #ISSUE: \n-            #SELECT ORDER OF IMAGES IN FORM DOES NOT AFFECT ORDER IN FLASK.\n-            #UPLOAD ORDER IS BASED ON FILEEXPLORER SORT METHOD.\n-            #No workarounds found.\n+            for i, image in enumerate(form.images.data):\n \n-            ext = image.filename.rfind(\".\")\n-            image_ext = image.filename[ext:]\n+                #ISSUE: \n+                #SELECT ORDER OF IMAGES IN FORM DOES NOT AFFECT ORDER IN FLASK.\n+                #UPLOAD ORDER IS BASED ON FILEEXPLORER SORT METHOD.\n+                #No workarounds found.\n \n-            image_filename = f'{i+1}{image_ext}'\n+                ext = image.filename.rfind(\".\")\n+                image_ext = image.filename[ext:]\n \n-            image.save(os.path.join(pathString, image_filename))\n-            imageURI = f'{pathString}\/{image_filename}'\n+                image_filename = f'{i+1}{image_ext}'\n \n-            #REMOVE WHOLE PATH-ONLY UP TO STATIC\/..\/..\/..\/imagename.filetype\n-            index = imageURI.find(\"static\/\")\n-            imageURI = imageURI[index-1:]\n+                image.save(os.path.join(pathString, image_filename))\n+                imageURI = f'{pathString}\/{image_filename}'\n \n-            new_image = GuideImages(guideID=new_guide.id, image=imageURI)\n-            db.session.add(new_image)\n-            db.session.commit()\n+                #REMOVE WHOLE PATH-ONLY UP TO STATIC\/..\/..\/..\/imagename.filetype\n+                index = imageURI.find(\"static\/\")\n+                imageURI = imageURI[index-1:]\n \n-        print(f\"Num files: {len(request.files.getlist(form.images.name))}\")\n+                new_image = GuideImages(guideID=new_guide.id, image=imageURI, caption=\"null\")\n+                db.session.add(new_image)\n+                db.session.commit()\n \n-        #Return home when finished....\n-        return redirect(url_for('newGuideContent', guide_id=new_guide.id))\n+            print(f\"Num files: {len(request.files.getlist(form.images.name))}\")\n+\n+            #Return home when finished....\n+            return redirect(url_for('newGuideContent', guide_id=new_guide.id))\n+        except:\n+            pass\n             \n     return render_template('new-guide.html', form=form, htmlTitle=\"New Guide\")\n \n@@ -381,7 +385,8 @@ def editGuide(guide_id):\n                 \n             db.session.commit()\n \n-        for(guideImage, entry) in zip(images, form.listPair.entries):\n+        for i, (guideImage, entry) in enumerate(zip(images, form.listPair.entries)):\n+            \n             entryData = entry.data\n             newImage = entryData.get('image')\n \n@@ -392,23 +397,22 @@ def editGuide(guide_id):\n                 pass\n             else:\n                 #FileField is not blank\n-\n-                #Index for guideImage W\/O .filetype\n-                x = guideImage.image.rfind(\".\")\n                 #Index for newImage .filetype\n-                y = newImage.filename.rfind(\".\")\n+                x = newImage.filename.rfind(\".\")\n \n-                #Index for upload_folder \/ guide title\n-                z = guideImage.image.rfind(\"\/\")\n-\n-                #imgName = guideImage up to .filetype + newImage past .filetype\n-                #example - oldImage : 1.png --> newImage :randomfile.jpeg\n-                #imgName merges into 1.jpeg while saving newImage in oldImage spot.\n+                try:\n+                    os.remove(f'{app.root_path}{guideImage.image}')\n+                except:\n+                    pass\n+                \n+                imgName = f\"{i+1}{newImage.filename[x:]}\"\n \n-                os.remove(f'{app.root_path}{guideImage.image}')\n \n-                imgName = f\"{guideImage.image[:x][z:]}{newImage.filename[y:]}\"\n-                newImage.save(f'{app.root_path}\/{UPLOAD_FOLDER}\/{form.title.data.replace(\" \", \"_\")}\/{imgName}')\n+                try:\n+                    newImage.save(f'{app.root_path}\/{UPLOAD_FOLDER}\/{form.title.data.replace(\" \", \"_\")}\/{imgName}')\n+                except:\n+                    os.mkdir(f'{app.root_path}\/{UPLOAD_FOLDER}\/{form.title.data.replace(\" \", \"_\")}')\n+                    newImage.save(f'{app.root_path}\/{UPLOAD_FOLDER}\/{form.title.data.replace(\" \", \"_\")}\/{imgName}')\n \n                 \n                 newImage = f'{UPLOAD_FOLDER}\/{form.title.data.replace(\" \", \"_\")}\/{imgName}'\n","files":{"\/flowerpod.py":{"changes":[{"diff":"\n \n     if request.method == 'POST':\n \n-        new_guide = Guides(title=form.title.data, creator=str(current_user.username))\n-        db.session.add(new_guide)\n-        db.session.commit()\n-\n         pathString = f'{app.root_path}{UPLOAD_FOLDER}\/{form.title.data.replace(\" \", \"_\")}'\n-        os.makedirs(pathString)\n \n-        for i, image in enumerate(form.images.data):\n+        try:\n+            os.makedirs(pathString)\n+\n+            new_guide = Guides(title=form.title.data, creator=str(current_user.username))\n+            db.session.add(new_guide)\n+            db.session.commit()\n \n-            #ISSUE: \n-            #SELECT ORDER OF IMAGES IN FORM DOES NOT AFFECT ORDER IN FLASK.\n-            #UPLOAD ORDER IS BASED ON FILEEXPLORER SORT METHOD.\n-            #No workarounds found.\n+            for i, image in enumerate(form.images.data):\n \n-            ext = image.filename.rfind(\".\")\n-            image_ext = image.filename[ext:]\n+                #ISSUE: \n+                #SELECT ORDER OF IMAGES IN FORM DOES NOT AFFECT ORDER IN FLASK.\n+                #UPLOAD ORDER IS BASED ON FILEEXPLORER SORT METHOD.\n+                #No workarounds found.\n \n-            image_filename = f'{i+1}{image_ext}'\n+                ext = image.filename.rfind(\".\")\n+                image_ext = image.filename[ext:]\n \n-            image.save(os.path.join(pathString, image_filename))\n-            imageURI = f'{pathString}\/{image_filename}'\n+                image_filename = f'{i+1}{image_ext}'\n \n-            #REMOVE WHOLE PATH-ONLY UP TO STATIC\/..\/..\/..\/imagename.filetype\n-            index = imageURI.find(\"static\/\")\n-            imageURI = imageURI[index-1:]\n+                image.save(os.path.join(pathString, image_filename))\n+                imageURI = f'{pathString}\/{image_filename}'\n \n-            new_image = GuideImages(guideID=new_guide.id, image=imageURI)\n-            db.session.add(new_image)\n-            db.session.commit()\n+                #REMOVE WHOLE PATH-ONLY UP TO STATIC\/..\/..\/..\/imagename.filetype\n+                index = imageURI.find(\"static\/\")\n+                imageURI = imageURI[index-1:]\n \n-        print(f\"Num files: {len(request.files.getlist(form.images.name))}\")\n+                new_image = GuideImages(guideID=new_guide.id, image=imageURI, caption=\"null\")\n+                db.session.add(new_image)\n+                db.session.commit()\n \n-        #Return home when finished....\n-        return redirect(url_for('newGuideContent', guide_id=new_guide.id))\n+            print(f\"Num files: {len(request.files.getlist(form.images.name))}\")\n+\n+            #Return home when finished....\n+            return redirect(url_for('newGuideContent', guide_id=new_guide.id))\n+        except:\n+            pass\n             \n     return render_template('new-guide.html', form=form, htmlTitle=\"New Guide\")\n \n","add":28,"remove":24,"filename":"\/flowerpod.py","badparts":["        new_guide = Guides(title=form.title.data, creator=str(current_user.username))","        db.session.add(new_guide)","        db.session.commit()","        os.makedirs(pathString)","        for i, image in enumerate(form.images.data):","            ext = image.filename.rfind(\".\")","            image_ext = image.filename[ext:]","            image_filename = f'{i+1}{image_ext}'","            image.save(os.path.join(pathString, image_filename))","            imageURI = f'{pathString}\/{image_filename}'","            index = imageURI.find(\"static\/\")","            imageURI = imageURI[index-1:]","            new_image = GuideImages(guideID=new_guide.id, image=imageURI)","            db.session.add(new_image)","            db.session.commit()","        print(f\"Num files: {len(request.files.getlist(form.images.name))}\")","        return redirect(url_for('newGuideContent', guide_id=new_guide.id))"],"goodparts":["        try:","            os.makedirs(pathString)","            new_guide = Guides(title=form.title.data, creator=str(current_user.username))","            db.session.add(new_guide)","            db.session.commit()","            for i, image in enumerate(form.images.data):","                ext = image.filename.rfind(\".\")","                image_ext = image.filename[ext:]","                image_filename = f'{i+1}{image_ext}'","                image.save(os.path.join(pathString, image_filename))","                imageURI = f'{pathString}\/{image_filename}'","                index = imageURI.find(\"static\/\")","                imageURI = imageURI[index-1:]","                new_image = GuideImages(guideID=new_guide.id, image=imageURI, caption=\"null\")","                db.session.add(new_image)","                db.session.commit()","            print(f\"Num files: {len(request.files.getlist(form.images.name))}\")","            return redirect(url_for('newGuideContent', guide_id=new_guide.id))","        except:","            pass"]},{"diff":"\n                 \n             db.session.commit()\n \n-        for(guideImage, entry) in zip(images, form.listPair.entries):\n+        for i, (guideImage, entry) in enumerate(zip(images, form.listPair.entries)):\n+            \n             entryData = entry.data\n             newImage = entryData.get('image')\n \n","add":2,"remove":1,"filename":"\/flowerpod.py","badparts":["        for(guideImage, entry) in zip(images, form.listPair.entries):"],"goodparts":["        for i, (guideImage, entry) in enumerate(zip(images, form.listPair.entries)):"]},{"diff":"\n                 pass\n             else:\n                 #FileField is not blank\n-\n-                #Index for guideImage W\/O .filetype\n-                x = guideImage.image.rfind(\".\")\n                 #Index for newImage .filetype\n-                y = newImage.filename.rfind(\".\")\n+                x = newImage.filename.rfind(\".\")\n \n-                #Index for upload_folder \/ guide title\n-                z = guideImage.image.rfind(\"\/\")\n-\n-                #imgName = guideImage up to .filetype + newImage past .filetype\n-                #example - oldImage : 1.png --> newImage :randomfile.jpeg\n-                #imgName merges into 1.jpeg while saving newImage in oldImage spot.\n+                try:\n+                    os.remove(f'{app.root_path}{guideImage.image}')\n+                except:\n+                    pass\n+                \n+                imgName = f\"{i+1}{newImage.filename[x:]}\"\n \n-                os.remove(f'{app.root_path}{guideImage.image}')\n \n-                imgName = f\"{guideImage.image[:x][z:]}{newImage.filename[y:]}\"\n-                newImage.save(f'{app.root_path}\/{UPLOAD_FOLDER}\/{form.title.data.replace(\" \", \"_\")}\/{imgName}')\n+                try:\n+                    newImage.save(f'{app.root_path}\/{UPLOAD_FOLDER}\/{form.title.data.replace(\" \", \"_\")}\/{imgName}')\n+                except:\n+                    os.mkdir(f'{app.root_path}\/{UPLOAD_FOLDER}\/{form.title.data.replace(\" \", \"_\")}')\n+                    newImage.save(f'{app.root_path}\/{UPLOAD_FOLDER}\/{form.title.data.replace(\" \", \"_\")}\/{imgName}')\n \n                 \n                 newImage = f'{UPLOAD_FOLDER}\/{form.title.data.replace(\" \", \"_\")}\/{imgName}'\n","add":12,"remove":13,"filename":"\/flowerpod.py","badparts":["                x = guideImage.image.rfind(\".\")","                y = newImage.filename.rfind(\".\")","                z = guideImage.image.rfind(\"\/\")","                os.remove(f'{app.root_path}{guideImage.image}')","                imgName = f\"{guideImage.image[:x][z:]}{newImage.filename[y:]}\"","                newImage.save(f'{app.root_path}\/{UPLOAD_FOLDER}\/{form.title.data.replace(\" \", \"_\")}\/{imgName}')"],"goodparts":["                x = newImage.filename.rfind(\".\")","                try:","                    os.remove(f'{app.root_path}{guideImage.image}')","                except:","                    pass","                imgName = f\"{i+1}{newImage.filename[x:]}\"","                try:","                    newImage.save(f'{app.root_path}\/{UPLOAD_FOLDER}\/{form.title.data.replace(\" \", \"_\")}\/{imgName}')","                except:","                    os.mkdir(f'{app.root_path}\/{UPLOAD_FOLDER}\/{form.title.data.replace(\" \", \"_\")}')","                    newImage.save(f'{app.root_path}\/{UPLOAD_FOLDER}\/{form.title.data.replace(\" \", \"_\")}\/{imgName}')"]}],"source":"\nimport os, shutil from dotenv import load_dotenv from flask import Flask, render_template, url_for, redirect, request, flash from flask_admin import Admin from flask_admin.contrib.sqla import ModelView from flask_cors import cross_origin from flask_sqlalchemy import SQLAlchemy from flask_login import UserMixin, AnonymousUserMixin, login_user, LoginManager, login_required, logout_user, current_user from flask_wtf import FlaskForm from flask_wtf.file import FileRequired, FileAllowed from sqlalchemy import null from wtforms import StringField, PasswordField, MultipleFileField, FileField, SubmitField, FieldList, FormField, Form, HiddenField from wtforms.validators import InputRequired, DataRequired, Length, ValidationError import flask_wtf import wtforms from flask_bcrypt import Bcrypt from tts import text_to_speech from werkzeug.utils import secure_filename load_dotenv() UPLOAD_FOLDER=os.getenv('WERKZEUG_UPLOAD_FOLDER') ALLOWED_EXTENSIONS={'png', 'jpg', 'jpeg', 'gif'} app=Flask(__name__) app.static_folder=os.getenv('FLASK_STATIC_FOLDER') bcrypt=Bcrypt(app) app.config['SQLALCHEMY_DATABASE_URI']='sqlite:\/\/\/database.db' app.config['SQLALCHEMY_BINDS']={ 'Guides': 'sqlite:\/\/\/guides.db', 'GuideImages': 'sqlite:\/\/\/guide-images.db'} app.config['SECRET_KEY']=os.getenv('FLASK_SECRET_KEY') app.config['UPLOAD_FOLDER']=UPLOAD_FOLDER db=SQLAlchemy(app) login_manager=LoginManager() login_manager.init_app(app) login_manager.login_view=\"login\" login_manager.login_message=\"Please login to continue.\" @login_manager.user_loader def load_user(user_id): return User.query.get(int(user_id)) class User(db.Model, UserMixin): id=db.Column(db.Integer, primary_key=True) username=db.Column(db.String(20), nullable=False, unique=True) password=db.Column(db.String(80), nullable=False) class Guides(db.Model): __bind_key__='Guides' id=db.Column(db.Integer, primary_key=True) title=db.Column(db.String(50), nullable=False) creator=db.Column(db.String(20), nullable=False) class GuideImages(db.Model): __bind_key__='GuideImages' id=db.Column(db.Integer, primary_key=True) guideID=db.Column(db.Integer) image=db.Column(db.String) caption=db.Column(db.String, nullable=True) class MyModelView(ModelView): def is_accessible(self): return current_user.is_authenticated def in_accessible_callback(self, name, **kwargs): return redirect(url_for('login')) admin=Admin(app) admin.add_view(MyModelView(GuideImages, db.session)) admin.add_view(MyModelView(Guides, db.session)) db.create_all() class RegisterForm(FlaskForm): username=StringField(validators=[InputRequired(), Length(min=4, max=20)]) password=PasswordField(validators=[InputRequired(), Length(min=4, max=20)]) submit=SubmitField(\"Register\") def validate_username(self, username): existing_user_username=User.query.filter_by(username=username.data).first() if existing_user_username: raise ValidationError(\"Username taken\") class LoginForm(FlaskForm): username=StringField(validators=[InputRequired(), Length(min=4, max=20)]) password=PasswordField(validators=[InputRequired(), Length(min=4, max=20)]) next=HiddenField() submit=SubmitField(\"Login\") class SearchForm(FlaskForm): search=StringField(\"Search\", validators=[InputRequired()]) submit=SubmitField(\"Search\") class NewGuideForm(FlaskForm): title=StringField(validators=[InputRequired(), Length(min=5, max=50)]) creator=StringField(validators=[InputRequired(), Length(min=5, max=30)]) images=MultipleFileField(validators=[FileRequired()]) submit=SubmitField(\"Create\") class caption(Form): caption=StringField(validators=[InputRequired()]) class CaptionForm(FlaskForm): captionList=FieldList(FormField(caption)) submit=SubmitField(\"Create\") class imageCaptionPair(Form): image=FileField() caption=StringField() class editGuideForm(FlaskForm): title=StringField(validators=[Length(min=5, max=50)]) creator=StringField(validators=[Length(min=5, max=30)]) listPair=FieldList(FormField(imageCaptionPair)) submit=SubmitField(\"Submit Changes\") class MainPageForm(FlaskForm): submit=SubmitField(\"\") @app.route(\"\/\", methods=['GET', 'POST']) @cross_origin() def mainPage(): form=MainPageForm() if request.method=='POST': text=\"Welcome to the FlowerPod Website.\" text +=\" Here you can find gardening guides, tips and tricks!\" text +=\" Make an account or sign in to get started.\" text_to_speech(text, \"Male\") return render_template('main-page.html', form=form, htmlTitle=\"Main Page\") @app.route(\"\/login\", methods=['GET', 'POST']) def login(): form=LoginForm() if form.validate_on_submit(): user=User.query.filter_by(username=form.username.data).first() next_url=request.form.get('next') if user: if bcrypt.check_password_hash(user.password, form.password.data): login_user(user) if next_url: return redirect(next_url) return redirect(url_for('home')) return render_template('login.html', form=form, htmlTitle=\"Login\") def getGuides(databaseGuides): data, temp=[],[] for guide in databaseGuides: temp.append(guide.id) temp.append(guide.title) temp.append(guide.creator) temp.append(GuideImages.query.filter_by(guideID=guide.id).all()) data.append(temp.copy()) temp.clear() return data @app.route(\"\/home\", methods=['GET', 'POST']) def home(): form=SearchForm() guides=Guides.query.all() data=getGuides(guides) if form.validate_on_submit(): guide.search=form.search.data guides=guides.filter(Guides.title.like('%' +guide.search +'%')) guides=guides.order_by(Guides.title).all() data=getGuides(guides) return render_template(\"search.html\", form=form, data=data, search=guide.search, htmlTitle=\"Search\") return render_template('home.html', form=form, data=data, htmlTitle=\"Home\") @app.route('\/search', methods=[\"POST\"]) def search(): form=SearchForm() guides=Guides.query guide.search=form.search.data guides=guides.filter(Guides.title.like('%' +guide.search +'%')) guides=guides.order_by(Guides.title).all() data=getGuides(guides) return render_template(\"search.html\", form=form, data=data, search=guide.search, guides=guides, htmlTitle=\"Search\") def allowed_file(filename): return \".\" in filename and \\ filename.rsplit('.',1)[1].lower() in ALLOWED_EXTENSIONS @app.route(\"\/new-guide\", methods=['GET', 'POST']) @login_required def newGuide(): form=NewGuideForm() if request.method=='POST': new_guide=Guides(title=form.title.data, creator=str(current_user.username)) db.session.add(new_guide) db.session.commit() pathString=f'{app.root_path}{UPLOAD_FOLDER}\/{form.title.data.replace(\" \", \"_\")}' os.makedirs(pathString) for i, image in enumerate(form.images.data): ext=image.filename.rfind(\".\") image_ext=image.filename[ext:] image_filename=f'{i+1}{image_ext}' image.save(os.path.join(pathString, image_filename)) imageURI=f'{pathString}\/{image_filename}' index=imageURI.find(\"static\/\") imageURI=imageURI[index-1:] new_image=GuideImages(guideID=new_guide.id, image=imageURI) db.session.add(new_image) db.session.commit() print(f\"Num files:{len(request.files.getlist(form.images.name))}\") return redirect(url_for('newGuideContent', guide_id=new_guide.id)) return render_template('new-guide.html', form=form, htmlTitle=\"New Guide\") @app.route(f\"\/new-guide-content\/<int:guide_id>\", methods=['GET', 'POST']) @login_required def newGuideContent(guide_id): guide=Guides.query.filter_by(id=guide_id).one() guideContent=GuideImages.query.filter_by(guideID=guide_id).all() form=CaptionForm() if request.method=='GET': for i in range(len(guideContent)): form.captionList.append_entry() for(guide, entry) in zip(guideContent, form.captionList.entries): entry.label=\"..\/\" +guide.image if request.method=='POST': for(guide, entry) in zip(guideContent, form.captionList.entries): newCaption=entry.data newCaption=newCaption.get('caption') guide.caption=newCaption db.session.commit() return redirect(url_for('home')) return render_template('new-guide-content.html', form=form, guides=guideContent, htmlTitle=\"New Guide\") @app.route(\"\/guide\/<int:guide_id>\/delete\", methods=['GET', 'POST']) @login_required def deleteGuide(guide_id): toDelGuide=Guides.query.filter_by(id=guide_id).one() toDelImages=GuideImages.query.filter_by(guideID=guide_id).all() title=toDelGuide.title title=title.replace(\" \", \"_\") path=f\"{app.root_path}\/{UPLOAD_FOLDER[1:]}\/{title}\" print(\"\\nTrying to delete guide...\") try: shutil.rmtree(path) print(f\"Deleted guides folder and folder content at{path}\") db.session.delete(toDelGuide) for image in toDelImages: db.session.delete(image) db.session.commit() flash(\"Guide deleted.\") print(f\"Removed from guides.db -->{toDelGuide.title}(ID:{toDelGuide.id})\\n\") except: print(\"Error deleting path\") db.session.delete(toDelGuide) for image in toDelImages: db.session.delete(image) db.session.commit() print(f\"Removed from guides.db -->{toDelGuide.title}(ID:{toDelGuide.id})\\n\") return redirect(url_for('home')) @app.route(\"\/guide\/<int:guide_id>\/edit\", methods=['GET', 'POST']) @login_required def editGuide(guide_id): guide=Guides.query.filter_by(id=guide_id).one() images=GuideImages.query.filter_by(guideID=guide_id).all() form=editGuideForm() if request.method=='GET': form.title.data=guide.title form.creator.data=guide.creator for i in range(len(images)): form.listPair.append_entry() for(image, entry) in zip(images, form.listPair.entries): entry.label=\"..\/\" +image.image if request.method=='POST': if form.title.data !=guide.title: ogPath=f'{app.root_path}{UPLOAD_FOLDER}\/{guide.title.replace(\" \", \"_\")}' guide.title=form.title.data db.session.commit() newPath=f'{app.root_path}{UPLOAD_FOLDER}\/{form.title.data.replace(\" \", \"_\")}' os.rename(ogPath, newPath) for image in images: x=image.image.rfind(\"\/\") imageName=image.image[x+1:] image.image=f'{UPLOAD_FOLDER}\/{form.title.data.replace(\" \", \"_\")}\/{imageName}' db.session.commit() for(guideImage, entry) in zip(images, form.listPair.entries): entryData=entry.data newImage=entryData.get('image') newCaption=entryData.get('caption') if str(newImage)==\"<FileStorage: ''('application\/octet-stream')>\": pass else: x=guideImage.image.rfind(\".\") y=newImage.filename.rfind(\".\") z=guideImage.image.rfind(\"\/\") os.remove(f'{app.root_path}{guideImage.image}') imgName=f\"{guideImage.image[:x][z:]}{newImage.filename[y:]}\" newImage.save(f'{app.root_path}\/{UPLOAD_FOLDER}\/{form.title.data.replace(\" \", \"_\")}\/{imgName}') newImage=f'{UPLOAD_FOLDER}\/{form.title.data.replace(\" \", \"_\")}\/{imgName}' guideImage.image=newImage db.session.commit() guideImage.caption=newCaption db.session.commit() flash(\"Changes saved successfully.\") return redirect(url_for('home')) return render_template('edit.html', form=form, guide=guide, images=images, htmlTitle=\"Edit Guide\") @app.route(\"\/guide\/<int:guide_id>\") def guide(guide_id): guide=Guides.query.filter_by(id=guide_id).one() images=GuideImages.query.filter_by(guideID=guide_id).all() return render_template('guide.html', guide=guide, images=images, htmlTitle=f\"Guide{guide_id}\") @app.route(\"\/logout\", methods=['GET', 'POST']) def logout(): logout_user() return redirect(url_for('mainPage')) @app.route(\"\/register\", methods=['GET', 'POST']) def register(): form=RegisterForm() if form.validate_on_submit(): hashed=bcrypt.generate_password_hash(form.password.data) new_user=User(username=form.username.data, password=hashed) db.session.add(new_user) db.session.commit() return redirect(url_for('login')) return render_template('register.html', form=form, htmlTitle=\"Register\") if __name__==\"__main__\": app.run() ","sourceWithComments":"import os, shutil\nfrom dotenv import load_dotenv\nfrom flask import Flask, render_template, url_for, redirect, request, flash\nfrom flask_admin import Admin\nfrom flask_admin.contrib.sqla import ModelView\nfrom flask_cors import cross_origin\nfrom flask_sqlalchemy import SQLAlchemy\nfrom flask_login import UserMixin, AnonymousUserMixin, login_user, LoginManager, login_required, logout_user, current_user\nfrom flask_wtf import FlaskForm\nfrom flask_wtf.file import FileRequired, FileAllowed\nfrom sqlalchemy import null\nfrom wtforms import StringField, PasswordField, MultipleFileField, FileField, SubmitField, FieldList, FormField, Form, HiddenField\nfrom wtforms.validators import InputRequired, DataRequired, Length, ValidationError\nimport flask_wtf\nimport wtforms\nfrom flask_bcrypt import Bcrypt\nfrom tts import text_to_speech\nfrom werkzeug.utils import secure_filename\n\n#Loading our .env file\nload_dotenv()\n\n# Werkzeug vars\nUPLOAD_FOLDER = os.getenv('WERKZEUG_UPLOAD_FOLDER')\nALLOWED_EXTENSIONS = {'png', 'jpg', 'jpeg', 'gif'}\n\n#Constructor\napp = Flask(__name__)\n\n#Setting up static folder for images, css etc.\napp.static_folder = os.getenv('FLASK_STATIC_FOLDER')\n\nbcrypt = Bcrypt(app)\n\napp.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:\/\/\/database.db'\napp.config['SQLALCHEMY_BINDS'] = { 'Guides' : 'sqlite:\/\/\/guides.db', 'GuideImages' : 'sqlite:\/\/\/guide-images.db'}\napp.config['SECRET_KEY'] = os.getenv('FLASK_SECRET_KEY')\napp.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER\n\ndb = SQLAlchemy(app)\n\nlogin_manager = LoginManager()\nlogin_manager.init_app(app)\nlogin_manager.login_view = \"login\"\nlogin_manager.login_message = \"Please login to continue.\"\n\n@login_manager.user_loader\ndef load_user(user_id):\n    return User.query.get(int(user_id))\n\n#Setting up Database table\n#If DB file is not setup, do as follows:\n#from flowerpod import db\n#db.create_all()\n#SQLite3 can verify creation of table.\nclass User(db.Model, UserMixin):\n    id = db.Column(db.Integer, primary_key=True)\n    username = db.Column(db.String(20), nullable=False, unique=True)\n    password = db.Column(db.String(80), nullable=False)\n\nclass Guides(db.Model):\n    __bind_key__ = 'Guides'\n    id = db.Column(db.Integer, primary_key=True)\n    title = db.Column(db.String(50), nullable=False)\n    creator = db.Column(db.String(20), nullable=False)\n\nclass GuideImages(db.Model):\n    __bind_key__ = 'GuideImages'\n    id = db.Column(db.Integer, primary_key=True)\n    guideID = db.Column(db.Integer)\n    image = db.Column(db.String)\n    caption = db.Column(db.String, nullable=True)\n\nclass MyModelView(ModelView):\n    def is_accessible(self):\n        return current_user.is_authenticated\n\n    def in_accessible_callback(self, name, **kwargs):\n        return redirect(url_for('login'))\n\nadmin = Admin(app)\nadmin.add_view(MyModelView(GuideImages, db.session))\nadmin.add_view(MyModelView(Guides, db.session))\n\n#Creating databases\ndb.create_all()\n\n#Register form where username\/password are inputboxes and submit is a button.\nclass RegisterForm(FlaskForm):\n    username = StringField(validators=[InputRequired(), Length(min=4, max=20)])\n    password = PasswordField(validators=[InputRequired(), Length(min=4, max=20)])\n    submit = SubmitField(\"Register\")\n\n    def validate_username(self, username):\n        existing_user_username = User.query.filter_by(username=username.data).first()\n\n        if existing_user_username:\n            raise ValidationError(\"Username taken\")\n\n#Login form where username\/password are inputboxes and submit is a button.\nclass LoginForm(FlaskForm):\n    username = StringField(validators=[InputRequired(), Length(min=4, max=20)])\n    password = PasswordField(validators=[InputRequired(), Length(min=4, max=20)])\n    next = HiddenField()\n    submit = SubmitField(\"Login\")\n\nclass SearchForm(FlaskForm):\n    search = StringField(\"Search\", validators=[InputRequired()])\n    submit = SubmitField(\"Search\")\n\nclass NewGuideForm(FlaskForm):\n    title = StringField(validators=[InputRequired(), Length(min=5, max=50)])\n    creator = StringField(validators=[InputRequired(), Length(min=5, max=30)])\n    images = MultipleFileField(validators=[FileRequired()])\n    submit = SubmitField(\"Create\")\n\n#Form for each individual caption input\nclass caption(Form):\n    caption = StringField(validators=[InputRequired()])\n\n#Form for captionForm passed to new-guide-content \nclass CaptionForm(FlaskForm):\n\n    captionList = FieldList(FormField(caption))\n    submit = SubmitField(\"Create\")\n\nclass imageCaptionPair(Form):\n    image = FileField()\n    caption = StringField()\n\nclass editGuideForm(FlaskForm):\n    title = StringField(validators=[Length(min=5, max=50)])\n    creator = StringField(validators=[Length(min=5, max=30)])\n    listPair = FieldList(FormField(imageCaptionPair))\n    submit = SubmitField(\"Submit Changes\")\n\nclass MainPageForm(FlaskForm):\n    submit = SubmitField(\"\")\n\n#Default URL returns mainpage.html\n@app.route(\"\/\", methods=['GET', 'POST'])\n@cross_origin()\ndef mainPage():\n    form = MainPageForm()\n    \n    if request.method == 'POST':\n        text = \"Welcome to the FlowerPod Website.\"\n        text += \" Here you can find gardening guides, tips and tricks!\"\n        text += \" Make an account or sign in to get started.\"\n        text_to_speech(text, \"Male\")\n    \n    return render_template('main-page.html', form=form, htmlTitle=\"Main Page\")\n\n@app.route(\"\/login\", methods=['GET', 'POST'])\ndef login():\n    #calls form for inputboxes and button\n    form = LoginForm()\n    \n    if form.validate_on_submit():\n        user = User.query.filter_by(username=form.username.data).first()\n        next_url = request.form.get('next')\n        if user:\n            \n            if bcrypt.check_password_hash(user.password, form.password.data):\n                login_user(user)\n                if next_url:\n                    return redirect(next_url)\n                return redirect(url_for('home'))\n            \n    return render_template('login.html', form=form, htmlTitle=\"Login\")\n\ndef getGuides(databaseGuides):\n    data, temp = [],[]\n\n    for guide in databaseGuides:\n        temp.append(guide.id)\n        temp.append(guide.title)\n        temp.append(guide.creator)\n        temp.append(GuideImages.query.filter_by(guideID=guide.id).all())\n\n        data.append(temp.copy())\n        temp.clear()\n\n    return data\n\n@app.route(\"\/home\", methods=['GET', 'POST'])\ndef home():\n\n    form = SearchForm()\n    guides = Guides.query.all()\n\n    data = getGuides(guides)\n\n    if form.validate_on_submit():\n        guide.search = form.search.data\n        guides = guides.filter(Guides.title.like('%' + guide.search + '%'))\n        guides = guides.order_by(Guides.title).all()\n        data = getGuides(guides)\n\n        return render_template(\"search.html\", \n                                form=form, \n                                data=data, \n                                search=guide.search, \n                                htmlTitle=\"Search\")\n\n    return render_template('home.html',\n                            form=form,\n                            data=data, \n                            htmlTitle=\"Home\")\n\n@app.route('\/search', methods=[\"POST\"])\ndef search():\n\n    form = SearchForm()\n    guides = Guides.query\n\n    guide.search = form.search.data\n    guides = guides.filter(Guides.title.like('%' + guide.search + '%'))\n    guides = guides.order_by(Guides.title).all()\n    data = getGuides(guides)\n\n    return render_template(\"search.html\",\n                            form=form,\n                            data=data,\n                            search=guide.search, \n                            guides=guides, \n                            htmlTitle=\"Search\")\n\n\ndef allowed_file(filename):\n    return \".\" in filename and \\\n        filename.rsplit('.',1)[1].lower() in ALLOWED_EXTENSIONS\n        \n\n@app.route(\"\/new-guide\", methods=['GET', 'POST'])\n@login_required\ndef newGuide():\n    form = NewGuideForm()\n\n    if request.method == 'POST':\n\n        new_guide = Guides(title=form.title.data, creator=str(current_user.username))\n        db.session.add(new_guide)\n        db.session.commit()\n\n        pathString = f'{app.root_path}{UPLOAD_FOLDER}\/{form.title.data.replace(\" \", \"_\")}'\n        os.makedirs(pathString)\n\n        for i, image in enumerate(form.images.data):\n\n            #ISSUE: \n            #SELECT ORDER OF IMAGES IN FORM DOES NOT AFFECT ORDER IN FLASK.\n            #UPLOAD ORDER IS BASED ON FILEEXPLORER SORT METHOD.\n            #No workarounds found.\n\n            ext = image.filename.rfind(\".\")\n            image_ext = image.filename[ext:]\n\n            image_filename = f'{i+1}{image_ext}'\n\n            image.save(os.path.join(pathString, image_filename))\n            imageURI = f'{pathString}\/{image_filename}'\n\n            #REMOVE WHOLE PATH-ONLY UP TO STATIC\/..\/..\/..\/imagename.filetype\n            index = imageURI.find(\"static\/\")\n            imageURI = imageURI[index-1:]\n\n            new_image = GuideImages(guideID=new_guide.id, image=imageURI)\n            db.session.add(new_image)\n            db.session.commit()\n\n        print(f\"Num files: {len(request.files.getlist(form.images.name))}\")\n\n        #Return home when finished....\n        return redirect(url_for('newGuideContent', guide_id=new_guide.id))\n            \n    return render_template('new-guide.html', form=form, htmlTitle=\"New Guide\")\n\n@app.route(f\"\/new-guide-content\/<int:guide_id>\", methods=['GET', 'POST'])\n@login_required\ndef newGuideContent(guide_id):\n\n    guide = Guides.query.filter_by(id=guide_id).one()\n    guideContent = GuideImages.query.filter_by(guideID=guide_id).all()\n    form = CaptionForm()\n\n    if request.method == 'GET':\n\n        for i in range(len(guideContent)):\n            form.captionList.append_entry()\n\n        for(guide, entry) in zip(guideContent, form.captionList.entries):\n            entry.label = \"..\/\" + guide.image\n\n    if request.method == 'POST':\n\n        for(guide, entry) in zip(guideContent, form.captionList.entries):\n            newCaption = entry.data\n            newCaption = newCaption.get('caption')\n            guide.caption = newCaption\n            db.session.commit()\n\n        return redirect(url_for('home'))\n\n    \n\n    return render_template('new-guide-content.html', form=form, guides=guideContent, htmlTitle=\"New Guide\")\n\n\n@app.route(\"\/guide\/<int:guide_id>\/delete\", methods=['GET', 'POST'])\n@login_required\ndef deleteGuide(guide_id):\n    toDelGuide = Guides.query.filter_by(id=guide_id).one()\n    toDelImages = GuideImages.query.filter_by(guideID=guide_id).all()\n\n    title = toDelGuide.title\n    title = title.replace(\" \", \"_\")\n    path = f\"{app.root_path}\/{UPLOAD_FOLDER[1:]}\/{title}\"\n    print(\"\\nTrying to delete guide...\")\n    try:\n        shutil.rmtree(path)\n        print(f\"Deleted guides folder and folder content at {path}\")\n        db.session.delete(toDelGuide)\n        for image in toDelImages:\n            db.session.delete(image)\n        db.session.commit()\n        flash(\"Guide deleted.\")\n        print(f\"Removed from guides.db --> {toDelGuide.title} (ID:{toDelGuide.id})\\n\")\n\n    except:\n        print(\"Error deleting path\")\n        db.session.delete(toDelGuide)\n        for image in toDelImages:\n            db.session.delete(image)\n        db.session.commit()\n        print(f\"Removed from guides.db --> {toDelGuide.title} (ID:{toDelGuide.id})\\n\")\n\n    return redirect(url_for('home'))\n\n@app.route(\"\/guide\/<int:guide_id>\/edit\", methods=['GET', 'POST'])\n@login_required\ndef editGuide(guide_id):\n\n    guide = Guides.query.filter_by(id=guide_id).one()\n    images = GuideImages.query.filter_by(guideID=guide_id).all()\n    form = editGuideForm()\n    \n    if request.method == 'GET':\n        form.title.data = guide.title\n        form.creator.data = guide.creator\n\n        for i in range(len(images)):\n            form.listPair.append_entry()\n\n        for(image, entry) in zip(images, form.listPair.entries):\n            entry.label = \"..\/\" + image.image\n\n    if request.method == 'POST':\n\n        if form.title.data != guide.title:\n\n            #CHANGING ALL PATHS RELATED TO A NEW GUIDE TITLE\n\n            ogPath = f'{app.root_path}{UPLOAD_FOLDER}\/{guide.title.replace(\" \", \"_\")}'\n\n            #Update the guide title in DB\n            guide.title = form.title.data\n            db.session.commit()\n\n            newPath = f'{app.root_path}{UPLOAD_FOLDER}\/{form.title.data.replace(\" \", \"_\")}'\n\n            os.rename(ogPath, newPath)\n\n\n            #Changing each image path to fit the new path\n            for image in images:\n                x = image.image.rfind(\"\/\")\n                imageName = image.image[x+1:]\n                \n                image.image = f'{UPLOAD_FOLDER}\/{form.title.data.replace(\" \", \"_\")}\/{imageName}'\n                \n            db.session.commit()\n\n        for(guideImage, entry) in zip(images, form.listPair.entries):\n            entryData = entry.data\n            newImage = entryData.get('image')\n\n            newCaption = entryData.get('caption')\n\n            if str(newImage) == \"<FileStorage: '' ('application\/octet-stream')>\":\n                #FileField is left blank...\n                pass\n            else:\n                #FileField is not blank\n\n                #Index for guideImage W\/O .filetype\n                x = guideImage.image.rfind(\".\")\n                #Index for newImage .filetype\n                y = newImage.filename.rfind(\".\")\n\n                #Index for upload_folder \/ guide title\n                z = guideImage.image.rfind(\"\/\")\n\n                #imgName = guideImage up to .filetype + newImage past .filetype\n                #example - oldImage : 1.png --> newImage :randomfile.jpeg\n                #imgName merges into 1.jpeg while saving newImage in oldImage spot.\n\n                os.remove(f'{app.root_path}{guideImage.image}')\n\n                imgName = f\"{guideImage.image[:x][z:]}{newImage.filename[y:]}\"\n                newImage.save(f'{app.root_path}\/{UPLOAD_FOLDER}\/{form.title.data.replace(\" \", \"_\")}\/{imgName}')\n\n                \n                newImage = f'{UPLOAD_FOLDER}\/{form.title.data.replace(\" \", \"_\")}\/{imgName}'\n                guideImage.image = newImage\n                db.session.commit()\n\n            guideImage.caption = newCaption\n            db.session.commit()\n            flash(\"Changes saved successfully.\")\n\n        return redirect(url_for('home'))\n\n    return render_template('edit.html', form=form, guide=guide, images=images, htmlTitle=\"Edit Guide\")\n\n@app.route(\"\/guide\/<int:guide_id>\")\ndef guide(guide_id):\n    \n    guide = Guides.query.filter_by(id=guide_id).one()\n    images = GuideImages.query.filter_by(guideID=guide_id).all()\n    return render_template('guide.html', guide=guide, images=images, htmlTitle=f\"Guide {guide_id}\")\n\n\n\n@app.route(\"\/logout\", methods=['GET', 'POST'])\ndef logout():\n    logout_user()\n    return redirect(url_for('mainPage'))\n\n@app.route(\"\/register\", methods=['GET', 'POST'])\ndef register():\n    #calls form for inputboxes and button\n    form = RegisterForm()\n\n    if form.validate_on_submit():\n        hashed = bcrypt.generate_password_hash(form.password.data)\n        new_user = User(username=form.username.data, password=hashed)\n        db.session.add(new_user)\n        db.session.commit()\n\n        return redirect(url_for('login'))\n    \n    return render_template('register.html', form=form, htmlTitle=\"Register\")\n\nif __name__ == \"__main__\":\n    app.run()\n\n"}},"msg":"Fixing errors with guides already existing\n\nSlight fix, can still result in funny behaviour when files are tampered or deleted then recreated - unlikely to occur normally unless trying to break everything."}},"https:\/\/github.com\/MapleCCC\/importall":{"9aae7eb053552368aceccaf2a5d9a2e3bdd0d3a9":{"url":"https:\/\/api.github.com\/repos\/MapleCCC\/importall\/commits\/9aae7eb053552368aceccaf2a5d9a2e3bdd0d3a9","html_url":"https:\/\/github.com\/MapleCCC\/importall\/commit\/9aae7eb053552368aceccaf2a5d9a2e3bdd0d3a9","message":"fix: `deduce_stdlib_public_interface()` needs to setup a clean interpreter environment to retrieve public names, yet `run_in_new_interpreter()` involves pickling and unpickling function, during which process unrelated modules might be imported, ultimately tampering with the clean environment","sha":"9aae7eb053552368aceccaf2a5d9a2e3bdd0d3a9","keyword":"tampering fix","diff":"diff --git a\/src\/importall\/stdlib_utils.py b\/src\/importall\/stdlib_utils.py\nindex 85a834c..5508264 100644\n--- a\/src\/importall\/stdlib_utils.py\n+++ b\/src\/importall\/stdlib_utils.py\n@@ -7,6 +7,7 @@\n import importlib\n import inspect\n import json\n+import pickle\n import re\n import sys\n import warnings\n@@ -21,7 +22,7 @@\n from .importlib import import_name_from_module, wildcard_import_module\n from .stdlib_list import BUILTINS_NAMES, IMPORTABLE_STDLIB_MODULES, STDLIB_MODULES\n from .typing import SymbolTable\n-from .utils import raises, run_in_new_interpreter\n+from .utils import asyncio_subprocess_check_output, raises, unindent_source\n \n \n __all__ = [\n@@ -77,10 +78,6 @@ class DeducePublicInterfaceError(Exception):\n     \"\"\"\n \n \n-def get_names_from_wildcard_import_module(module_name: str) -> set[str]:\n-    return set(wildcard_import_module(module_name))\n-\n-\n @raises(RuntimeError, \"Fail to deduce public interface of module '{module_name}'\")\n async def deduce_stdlib_public_interface(module_name: str) -> set[str]:\n     \"\"\"\n@@ -118,10 +115,29 @@ async def deduce_stdlib_public_interface(module_name: str) -> set[str]:\n     # TODO maybe we can just use test.support.CleanImport instead of the heavy solution\n     # - subprocess to launch another interpreter instance ?\n \n-    public_names = await run_in_new_interpreter(\n-        get_names_from_wildcard_import_module, module_name\n+    source = unindent_source(\n+        f\"\"\"\n+        import pickle, sys, os\n+        from contextlib import redirect_stdout, redirect_stderr\n+\n+        symtab = dict()\n+\n+        # NOTE it's more robust to use the `locals` argument instead of the `globals`\n+        # argument to collect symbols, because the `globals` argument could have been\n+        # implicitly and surprisingly altered, such as being inserted a `__builtins__` key.\n+\n+        with open(os.devnull, \"w\") as f:\n+            with redirect_stdout(f), redirect_stderr(f):\n+                exec(\"from {module_name} import *\", dict(), symtab)\n+\n+        sys.stdout.buffer.write(pickle.dumps(set(symtab)))\n+    \"\"\"\n     )\n \n+    command = [sys.executable or \"python\", \"-c\", source]\n+    out = await asyncio_subprocess_check_output(command, redirect_stderr_to_stdout=True)\n+    public_names = pickle.loads(out)\n+\n     # Try best effort to filter out only public names\n \n     # There is no easy way to reliably determine all public names exported by a module.\n","files":{"\/src\/importall\/stdlib_utils.py":{"changes":[{"diff":"\n from .stdlib_list import BUILTINS_NAMES, IMPORTABLE_STDLIB_MODULES, STDLIB_MODULES\n from .typing import SymbolTable\n-from .utils import raises, run_in_new_interpreter\n+from .utils import asyncio_subprocess_check_output, raises, unindent_source\n \n \n __all__ = [\n","add":1,"remove":1,"filename":"\/src\/importall\/stdlib_utils.py","badparts":["from .utils import raises, run_in_new_interpreter"],"goodparts":["from .utils import asyncio_subprocess_check_output, raises, unindent_source"]},{"diff":"\n     \"\"\"\n \n \n-def get_names_from_wildcard_import_module(module_name: str) -> set[str]:\n-    return set(wildcard_import_module(module_name))\n-\n-\n @raises(RuntimeError, \"Fail to deduce public interface of module '{module_name}'\")\n async def deduce_stdlib_public_interface(module_name: str) -> set[str]:\n     \"\"\"\n","add":0,"remove":4,"filename":"\/src\/importall\/stdlib_utils.py","badparts":["def get_names_from_wildcard_import_module(module_name: str) -> set[str]:","    return set(wildcard_import_module(module_name))"],"goodparts":[]},{"diff":"\n     # TODO maybe we can just use test.support.CleanImport instead of the heavy solution\n     # - subprocess to launch another interpreter instance ?\n \n-    public_names = await run_in_new_interpreter(\n-        get_names_from_wildcard_import_module, module_name\n+    source = unindent_source(\n+        f\"\"\"\n+        import pickle, sys, os\n+        from contextlib import redirect_stdout, redirect_stderr\n+\n+        symtab = dict()\n+\n+        # NOTE it's more robust to use the `locals` argument instead of the `globals`\n+        # argument to collect symbols, because the `globals` argument could have been\n+        # implicitly and surprisingly altered, such as being inserted a `__builtins__` key.\n+\n+        with open(os.devnull, \"w\") as f:\n+            with redirect_stdout(f), redirect_stderr(f):\n+                exec(\"from {module_name} import *\", dict(), symtab)\n+\n+        sys.stdout.buffer.write(pickle.dumps(set(symtab)))\n+    \"\"\"\n     )\n \n+    command = [sys.executable or \"python\", \"-c\", source]\n+    out = await asyncio_subprocess_check_output(command, redirect_stderr_to_stdout=True)\n+    public_names = pickle.loads(out)\n+\n     # Try best effort to filter out only public names\n \n     # There is no easy way to reliably determine all public names exported by a module.\n","add":21,"remove":2,"filename":"\/src\/importall\/stdlib_utils.py","badparts":["    public_names = await run_in_new_interpreter(","        get_names_from_wildcard_import_module, module_name"],"goodparts":["    source = unindent_source(","        f\"\"\"","        import pickle, sys, os","        from contextlib import redirect_stdout, redirect_stderr","        symtab = dict()","        with open(os.devnull, \"w\") as f:","            with redirect_stdout(f), redirect_stderr(f):","                exec(\"from {module_name} import *\", dict(), symtab)","        sys.stdout.buffer.write(pickle.dumps(set(symtab)))","    \"\"\"","    command = [sys.executable or \"python\", \"-c\", source]","    out = await asyncio_subprocess_check_output(command, redirect_stderr_to_stdout=True)","    public_names = pickle.loads(out)"]}],"source":"\n\"\"\" A collection of utilities related to standard libraries. \"\"\" import __future__ import importlib import inspect import json import re import sys import warnings from functools import cache from pathlib import Path from typing import Optional, cast import commentjson import regex from lazy_object_proxy import Proxy from.importlib import import_name_from_module, wildcard_import_module from.stdlib_list import BUILTINS_NAMES, IMPORTABLE_STDLIB_MODULES, STDLIB_MODULES from.typing import SymbolTable from.utils import raises, run_in_new_interpreter __all__=[ \"import_stdlib_public_names\", \"DeducePublicInterfaceError\", \"deduce_stdlib_public_interface\", \"from_stdlib\", \"deprecated_modules\", \"deprecated_names\", \"stdlib_public_names\", ] VersionTuple=tuple[int, int] def import_stdlib_public_names( module_name: str, *, lazy: bool=False, include_deprecated: bool=False ) -> SymbolTable: \"\"\" Return a symbol table containing all public names defined in the standard library module. By default, names are eagerly imported. One can reduce the overhead by setting the `lazy` parameter to `True` to enable lazy import mode. By default, deprecated names are not included. It is designed so because deprecated names hopefully should not be used anymore, their presence only for easing the steepness of API changes and providing a progressive cross-version migration experience. If you are sure you know what you are doing, override the default behavior by setting the `include_deprecated` parameter to `True`(**NOT RECOMMENDED!**). \"\"\" if module_name not in IMPORTABLE_STDLIB_MODULES: raise ValueError(f\"{module_name} is not importable stdlib module\") public_names=stdlib_public_names(module_name) if not include_deprecated: public_names -=deprecated_names(module_name) return{ name: import_name_from_module(name, module_name, lazy=lazy) for name in public_names } class DeducePublicInterfaceError(Exception): \"\"\" Raised when `deduce_stdlib_public_interface()` fails to deduce the public interface of a given module. \"\"\" def get_names_from_wildcard_import_module(module_name: str) -> set[str]: return set(wildcard_import_module(module_name)) @raises(RuntimeError, \"Fail to deduce public interface of module '{module_name}'\") async def deduce_stdlib_public_interface(module_name: str) -> set[str]: \"\"\" Try best effort to heuristically determine public names exported by a stdlib module. Raise `DeducePublicInterfaceError` on failure. \"\"\" if module_name not in IMPORTABLE_STDLIB_MODULES: raise ValueError(f\"{module_name} is not importable stdlib module\") if module_name==\"builtins\": return set(BUILTINS_NAMES) try: return set(import_name_from_module(\"__all__\", module_name)) except ImportError: pass public_names=await run_in_new_interpreter( get_names_from_wildcard_import_module, module_name ) def is_another_stdlib(symbol: object) -> bool: \"\"\" Detect if the symbol is possibly another standard library module imported to this module, hence should not be considered part of the public names of this module. \"\"\" return( inspect.ismodule(symbol) and symbol.__name__ in STDLIB_MODULES and not symbol.__name__.startswith(module_name +\".\") ) def from_another_stdlib(symbol: object) -> bool: \"\"\" Detect if the symbol is possibly a public name from another standard library module, imported to this module, hence should not be considered part of the public names of this module. \"\"\" origin: Optional[str]=getattr(symbol, \"__module__\", None) return( origin in STDLIB_MODULES and origin !=module_name and not origin.startswith(module_name +\".\") ) symtab=wildcard_import_module(module_name) for name, symbol in symtab.items(): if is_another_stdlib(symbol) or from_another_stdlib(symbol): public_names.remove(name) return public_names def gather_stdlib_symbol_ids() -> set[int]: stdlib_symbol_ids: set[int]=set() for module_name in IMPORTABLE_STDLIB_MODULES: with warnings.catch_warnings(): warnings.simplefilter(\"ignore\", DeprecationWarning) module=importlib.import_module(module_name) stdlib_symbol_ids.add(id(module)) symbol_table=import_stdlib_public_names( module_name, include_deprecated=True ) for symbol in symbol_table.values(): stdlib_symbol_ids.add(id(symbol)) return stdlib_symbol_ids STDLIB_SYMBOLS_IDS=cast(set[int], Proxy(gather_stdlib_symbol_ids)) def from_stdlib(symbol: object) -> bool: \"\"\"Check if a symbol comes from standard libraries. Try best effort.\"\"\" return id(symbol) in STDLIB_SYMBOLS_IDS def convert_version_to_tuple(version: str) -> VersionTuple: \"\"\" Convert version info from string representation to tuple representation. The tuple representation is convenient for direct comparison. \"\"\" m=regex.fullmatch(r\"(?P<major>\\d+)\\.(?P<minor>\\d+)\", version) if not m: raise ValueError(f\"{version} is not a valid version\") major, minor=m.group(\"major\", \"minor\") version_tuple=(int(major), int(minor)) return version_tuple def load_deprecated_modules() -> dict[VersionTuple, frozenset[str]]: \"\"\"Load DEPRECATED_MODULES from JSON file\"\"\" json_file=Path(__file__).with_name(\"deprecated_modules.json\") json_text=json_file.read_text(encoding=\"utf-8\") json_obj=cast(dict[str, list[str]], commentjson.loads(json_text)) return{ convert_version_to_tuple(version): frozenset(modules) for version, modules in json_obj.items() } def load_deprecated_names() -> dict[VersionTuple, dict[str, frozenset[str]]]: \"\"\"Load DEPRECATED_NAMES from JSON file\"\"\" json_file=Path(__file__).with_name(\"deprecated_names.json\") json_text=json_file.read_text(encoding=\"utf-8\") json_obj=cast(dict[str, dict[str, list[str]]], commentjson.loads(json_text)) res: dict[VersionTuple, dict[str, frozenset[str]]]={} for version, modules in json_obj.items(): version_tuple=convert_version_to_tuple(version) res[version_tuple]={ module: frozenset(names) for module, names in modules.items() } return res DEPRECATED_MODULES=cast( dict[VersionTuple, frozenset[str]], Proxy(load_deprecated_modules) ) DEPRECATED_NAMES=cast( dict[VersionTuple, dict[str, frozenset[str]]], Proxy(load_deprecated_names) ) def deprecated_modules(version: str=None) -> set[str]: \"\"\" Return a set of modules who are deprecated after the given version. If no version is given, default to the current version. The `version` parameter takes argument of the form `3.9`, `4.7`, etc. \"\"\" if version is None: version_tuple=sys.version_info[:2] else: version_tuple=convert_version_to_tuple(version) modules: set[str]=set() for _version, _modules in DEPRECATED_MODULES.items(): if version_tuple >=_version: modules |=_modules return modules def deprecated_names(module: str, *, version: str=None) -> set[str]: \"\"\" Return a set of names from a stdlib module who are deprecated after the given version. If no version is given, default to the current version. The `version` parameter takes argument of the form `3.9`, `4.7`, etc. \"\"\" if module not in IMPORTABLE_STDLIB_MODULES: raise ValueError(f\"{module} is not importable stdlib module\") if version is None: version_tuple=sys.version_info[:2] else: version_tuple=convert_version_to_tuple(version) names: set[str]=set() for _version, _modules in DEPRECATED_NAMES.items(): if version_tuple < _version: continue for _module, _names in _modules.items(): if module is None or module==_module: names |=_names return names @cache def load_stdlib_public_names(version: str) -> dict[str, frozenset[str]]: \"\"\"Load stdlib public names data from JSON file\"\"\" if not re.fullmatch(r\"\\d+\\.\\d+\", version): raise ValueError(f\"{version} is not a valid version\") try: json_file=Path(__file__).with_name(\"stdlib_public_names\") \/( version +\".json\" ) json_text=json_file.read_text(encoding=\"utf-8\") json_obj=json.loads(json_text) return{module: frozenset(names) for module, names in json_obj.items()} except FileNotFoundError: raise ValueError( f\"there is no data of stdlib public names for Python version{version}\" ) from None def stdlib_public_names(module: str, *, version: str=None) -> set[str]: \"\"\" Return a set of public names of a stdlib module, in specific Python version. If no version is given, default to the current version. The `version` parameter takes argument of the form `3.9`, `4.7`, etc. \"\"\" if module not in IMPORTABLE_STDLIB_MODULES: raise ValueError(f\"{module} is not importable stdlib module\") version=version or \".\".join(str(c) for c in sys.version_info[:2]) return set(load_stdlib_public_names(version)[module]) ","sourceWithComments":"\"\"\"\nA collection of utilities related to standard libraries.\n\"\"\"\n\nimport __future__\n\nimport importlib\nimport inspect\nimport json\nimport re\nimport sys\nimport warnings\nfrom functools import cache\nfrom pathlib import Path\nfrom typing import Optional, cast\n\nimport commentjson\nimport regex\nfrom lazy_object_proxy import Proxy\n\nfrom .importlib import import_name_from_module, wildcard_import_module\nfrom .stdlib_list import BUILTINS_NAMES, IMPORTABLE_STDLIB_MODULES, STDLIB_MODULES\nfrom .typing import SymbolTable\nfrom .utils import raises, run_in_new_interpreter\n\n\n__all__ = [\n    \"import_stdlib_public_names\",\n    \"DeducePublicInterfaceError\",\n    \"deduce_stdlib_public_interface\",\n    \"from_stdlib\",\n    \"deprecated_modules\",\n    \"deprecated_names\",\n    \"stdlib_public_names\",\n]\n\n\nVersionTuple = tuple[int, int]\n\n\ndef import_stdlib_public_names(\n    module_name: str, *, lazy: bool = False, include_deprecated: bool = False\n) -> SymbolTable:\n    \"\"\"\n    Return a symbol table containing all public names defined in the standard library\n    module.\n\n    By default, names are eagerly imported. One can reduce the overhead by setting the\n    `lazy` parameter to `True` to enable lazy import mode.\n\n    By default, deprecated names are not included. It is designed so because\n    deprecated names hopefully should not be used anymore, their presence only for\n    easing the steepness of API changes and providing a progressive cross-version\n    migration experience. If you are sure you know what you are doing, override the default\n    behavior by setting the `include_deprecated` parameter to `True` (**NOT RECOMMENDED!**).\n    \"\"\"\n\n    if module_name not in IMPORTABLE_STDLIB_MODULES:\n        raise ValueError(f\"{module_name} is not importable stdlib module\")\n\n    public_names = stdlib_public_names(module_name)\n\n    if not include_deprecated:\n        public_names -= deprecated_names(module_name)\n\n    # TODO check to see if the laziness persists until actual use\n    return {\n        name: import_name_from_module(name, module_name, lazy=lazy)\n        for name in public_names\n    }\n\n\nclass DeducePublicInterfaceError(Exception):\n    \"\"\"\n    Raised when `deduce_stdlib_public_interface()` fails to deduce the public interface\n    of a given module.\n    \"\"\"\n\n\ndef get_names_from_wildcard_import_module(module_name: str) -> set[str]:\n    return set(wildcard_import_module(module_name))\n\n\n@raises(RuntimeError, \"Fail to deduce public interface of module '{module_name}'\")\nasync def deduce_stdlib_public_interface(module_name: str) -> set[str]:\n    \"\"\"\n    Try best effort to heuristically determine public names exported by a stdlib module.\n\n    Raise `DeducePublicInterfaceError` on failure.\n    \"\"\"\n\n    if module_name not in IMPORTABLE_STDLIB_MODULES:\n        raise ValueError(f\"{module_name} is not importable stdlib module\")\n\n    # The builtins module is a special case\n    # Some exported public names from builtins module are prefixed with underscore,\n    # hence ignored by the standard wildcard import mechanism. Examples are `__import__`\n    # and `__debug__`.\n    if module_name == \"builtins\":\n        return set(BUILTINS_NAMES)\n\n    try:\n        return set(import_name_from_module(\"__all__\", module_name))\n    except ImportError:\n        pass\n\n    # Use a separate clean interpreter to retrieve public names.\n    #\n    # This is to workaround the problem that a preceding importing of submodules could\n    # lead to inadvertent and surprising injection of such submodules into the namespace\n    # of the parent module. Such problem would have led to nondeterministic result from\n    # the `deduce_stdlib_public_interface()` function if not taken good care of.\n    #\n    # An example is the `distutils` module. Whether `msvccompiler` appears in the\n    # result of `from distutils import *` is affected by whether\n    # `distutils.msvccompiler` has been imported before.\n\n    # TODO maybe we can just use test.support.CleanImport instead of the heavy solution\n    # - subprocess to launch another interpreter instance ?\n\n    public_names = await run_in_new_interpreter(\n        get_names_from_wildcard_import_module, module_name\n    )\n\n    # Try best effort to filter out only public names\n\n    # There is no easy way to reliably determine all public names exported by a module.\n    #\n    # Standard libraries export public symbols of various types.\n    #\n    # Unlike functions and classes, some symbols, whose types being string, integer, etc.,\n    # don't have the `__module__` attribute.\n    #\n    # There is no way we can inspect where they come from, less distinguishing them\n    # from public symbols of the module that imports them.\n    #\n    # The only thing we can do is to try best effort.\n\n    def is_another_stdlib(symbol: object) -> bool:\n        \"\"\"\n        Detect if the symbol is possibly another standard library module imported to\n        this module, hence should not be considered part of the public names of this\n        module.\n        \"\"\"\n\n        return (\n            inspect.ismodule(symbol)\n            and symbol.__name__ in STDLIB_MODULES\n            and not symbol.__name__.startswith(module_name + \".\")\n        )\n\n    def from_another_stdlib(symbol: object) -> bool:\n        \"\"\"\n        Detect if the symbol is possibly a public name from another standard library\n        module, imported to this module, hence should not be considered part of the\n        public names of this module.\n        \"\"\"\n\n        origin: Optional[str] = getattr(symbol, \"__module__\", None)\n        return (\n            origin in STDLIB_MODULES\n            and origin != module_name\n            and not origin.startswith(module_name + \".\")\n        )\n\n    symtab = wildcard_import_module(module_name)\n\n    for name, symbol in symtab.items():\n        if is_another_stdlib(symbol) or from_another_stdlib(symbol):\n            public_names.remove(name)\n\n    return public_names\n\n\ndef gather_stdlib_symbol_ids() -> set[int]:\n\n    stdlib_symbol_ids: set[int] = set()\n\n    for module_name in IMPORTABLE_STDLIB_MODULES:\n\n        # Suppress DeprecationWarning, because we know for sure that we are not intended\n        # to use the deprecated names here.\n        #\n        # `contextlib.suppress` is not used because it won't suppress warnings.\n\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\", DeprecationWarning)\n\n            module = importlib.import_module(module_name)\n\n            stdlib_symbol_ids.add(id(module))\n\n            symbol_table = import_stdlib_public_names(\n                module_name, include_deprecated=True\n            )\n\n            for symbol in symbol_table.values():\n                stdlib_symbol_ids.add(id(symbol))\n\n    return stdlib_symbol_ids\n\n\nSTDLIB_SYMBOLS_IDS = cast(set[int], Proxy(gather_stdlib_symbol_ids))\n\n\ndef from_stdlib(symbol: object) -> bool:\n    \"\"\"Check if a symbol comes from standard libraries. Try best effort.\"\"\"\n\n    # The id() approach could fail if importlib.reload() has been called or sys.modules\n    # has been manipulated.\n    #\n    # So it's a try-best-effort thing.\n\n    return id(symbol) in STDLIB_SYMBOLS_IDS\n\n\ndef convert_version_to_tuple(version: str) -> VersionTuple:\n    \"\"\"\n    Convert version info from string representation to tuple representation.\n    The tuple representation is convenient for direct comparison.\n    \"\"\"\n\n    m = regex.fullmatch(r\"(?P<major>\\d+)\\.(?P<minor>\\d+)\", version)\n\n    if not m:\n        raise ValueError(f\"{version} is not a valid version\")\n\n    major, minor = m.group(\"major\", \"minor\")\n    version_tuple = (int(major), int(minor))\n\n    return version_tuple\n\n\ndef load_deprecated_modules() -> dict[VersionTuple, frozenset[str]]:\n    \"\"\"Load DEPRECATED_MODULES from JSON file\"\"\"\n\n    json_file = Path(__file__).with_name(\"deprecated_modules.json\")\n    json_text = json_file.read_text(encoding=\"utf-8\")\n    json_obj = cast(dict[str, list[str]], commentjson.loads(json_text))\n\n    return {\n        convert_version_to_tuple(version): frozenset(modules)\n        for version, modules in json_obj.items()\n    }\n\n\ndef load_deprecated_names() -> dict[VersionTuple, dict[str, frozenset[str]]]:\n    \"\"\"Load DEPRECATED_NAMES from JSON file\"\"\"\n\n    json_file = Path(__file__).with_name(\"deprecated_names.json\")\n    json_text = json_file.read_text(encoding=\"utf-8\")\n    json_obj = cast(dict[str, dict[str, list[str]]], commentjson.loads(json_text))\n\n    res: dict[VersionTuple, dict[str, frozenset[str]]] = {}\n\n    for version, modules in json_obj.items():\n        version_tuple = convert_version_to_tuple(version)\n        res[version_tuple] = {\n            module: frozenset(names) for module, names in modules.items()\n        }\n\n    return res\n\n\nDEPRECATED_MODULES = cast(\n    dict[VersionTuple, frozenset[str]], Proxy(load_deprecated_modules)\n)\n\nDEPRECATED_NAMES = cast(\n    dict[VersionTuple, dict[str, frozenset[str]]], Proxy(load_deprecated_names)\n)\n\n\ndef deprecated_modules(version: str = None) -> set[str]:\n    \"\"\"\n    Return a set of modules who are deprecated after the given version.\n\n    If no version is given, default to the current version.\n\n    The `version` parameter takes argument of the form `3.9`, `4.7`, etc.\n    \"\"\"\n\n    if version is None:\n        version_tuple = sys.version_info[:2]\n    else:\n        version_tuple = convert_version_to_tuple(version)\n\n    modules: set[str] = set()\n\n    for _version, _modules in DEPRECATED_MODULES.items():\n        if version_tuple >= _version:\n            modules |= _modules\n\n    return modules\n\n\ndef deprecated_names(module: str, *, version: str = None) -> set[str]:\n    \"\"\"\n    Return a set of names from a stdlib module who are deprecated after the given version.\n\n    If no version is given, default to the current version.\n\n    The `version` parameter takes argument of the form `3.9`, `4.7`, etc.\n    \"\"\"\n\n    if module not in IMPORTABLE_STDLIB_MODULES:\n        raise ValueError(f\"{module} is not importable stdlib module\")\n\n    if version is None:\n        version_tuple = sys.version_info[:2]\n    else:\n        version_tuple = convert_version_to_tuple(version)\n\n    names: set[str] = set()\n\n    for _version, _modules in DEPRECATED_NAMES.items():\n        if version_tuple < _version:\n            continue\n\n        for _module, _names in _modules.items():\n            if module is None or module == _module:\n                names |= _names\n\n    return names\n\n\n@cache\ndef load_stdlib_public_names(version: str) -> dict[str, frozenset[str]]:\n    \"\"\"Load stdlib public names data from JSON file\"\"\"\n\n    if not re.fullmatch(r\"\\d+\\.\\d+\", version):\n        raise ValueError(f\"{version} is not a valid version\")\n\n    try:\n        json_file = Path(__file__).with_name(\"stdlib_public_names\") \/ (\n            version + \".json\"\n        )\n        json_text = json_file.read_text(encoding=\"utf-8\")\n        json_obj = json.loads(json_text)\n\n        return {module: frozenset(names) for module, names in json_obj.items()}\n\n    except FileNotFoundError:\n        raise ValueError(\n            f\"there is no data of stdlib public names for Python version {version}\"\n        ) from None\n\n\ndef stdlib_public_names(module: str, *, version: str = None) -> set[str]:\n    \"\"\"\n    Return a set of public names of a stdlib module, in specific Python version.\n\n    If no version is given, default to the current version.\n\n    The `version` parameter takes argument of the form `3.9`, `4.7`, etc.\n    \"\"\"\n\n    if module not in IMPORTABLE_STDLIB_MODULES:\n        raise ValueError(f\"{module} is not importable stdlib module\")\n\n    version = version or \".\".join(str(c) for c in sys.version_info[:2])\n\n    return set(load_stdlib_public_names(version)[module])\n"}},"msg":"fix: `deduce_stdlib_public_interface()` needs to setup a clean interpreter environment to retrieve public names, yet `run_in_new_interpreter()` involves pickling and unpickling function, during which process unrelated modules might be imported, ultimately tampering with the clean environment"}},"https:\/\/github.com\/sergioantelo\/private-blockchain-LMU-university":{"0afde0de4ce8786d1720633d28ec4902fbd2f07f":{"url":"https:\/\/api.github.com\/repos\/sergioantelo\/private-blockchain-LMU-university\/commits\/0afde0de4ce8786d1720633d28ec4902fbd2f07f","html_url":"https:\/\/github.com\/sergioantelo\/private-blockchain-LMU-university\/commit\/0afde0de4ce8786d1720633d28ec4902fbd2f07f","message":"tampered block attacks","sha":"0afde0de4ce8786d1720633d28ec4902fbd2f07f","keyword":"tampering attack","diff":"diff --git a\/node_server.py b\/node_server.py\nindex 92aef47..feeeebe 100644\n--- a\/node_server.py\n+++ b\/node_server.py\n@@ -1,9 +1,9 @@\n from hashlib import sha256\r\n import json\r\n import time\r\n-\r\n+import numpy as np \r\n import sys\r\n-from flask import Flask, request, redirect\r\n+from flask import Flask, request, redirect, render_template\r\n import requests\r\n \r\n class Block:\r\n@@ -47,6 +47,19 @@ def create_genesis_block(self):\n     @property\r\n     def last_block(self):\r\n         return self.chain[-1]\r\n+    \r\n+  \r\n+    def retrieve_block(self, idx):\r\n+        if idx < 0 or idx > len(self.chain):\r\n+            return False\r\n+\r\n+        for b in self.chain:\r\n+            if idx == b.index:\r\n+                return b\r\n+        \r\n+        # NO block with specified index present\r\n+        return False\r\n+\r\n \r\n     def add_block(self, block, proof):\r\n         \"\"\"\r\n@@ -397,6 +410,7 @@ def synch_with_peers():\n @app.route('\/add_block', methods=['POST'])\r\n def verify_and_add_block():\r\n     block_data = request.get_json()\r\n+\r\n     block = Block(block_data[\"index\"],\r\n                   block_data[\"transactions\"],\r\n                   block_data[\"timestamp\"],\r\n@@ -419,11 +433,12 @@ def verify_and_add_block():\n     # this node will announce the new block also to its peers\r\n     # thus the node is propagated through the network\r\n     \r\n+    # only if peers are known the announce is called, and when the received block is correct \r\n     if added[0] and peers:\r\n         announce_new_block(block)\r\n \r\n     #return \"Block added to the chain\", 201\r\n-\r\n+       \r\n     return json.dumps({\"status\":added[0],\"message\":added[1]})\r\n \r\n # endpoint to query unconfirmed transactions\r\n@@ -455,70 +470,93 @@ def add_fixed_block():\n \r\n     return redirect(\"\/chain\")\r\n \r\n-@app.route('\/attack')\r\n+@app.route('\/attack', methods=['POST'])\r\n def attack():\r\n-    '''\r\n-    Create a tampered block\r\n-    '''\r\n+    \r\n+    if not peers:\r\n+        return \"No peers existing\"\r\n+    \r\n+    data = request.get_json()\r\n+    # data should contain the field attack which specifies which attack: A,B,C,D\r\n+    # where A means no attack\r\n+    # if we would like to also give contents which should be used in the different attacks\r\n+    # then we can specify a further field, which is used by the respective attack\r\n \r\n-    # attack where the transaction and the block fields are correct\r\n-    # but the attacker used a different last.block as reference\r\n-    # thus he is kind of \"suggesting\" an alternative blockchain \r\n+    global blockchain\r\n     transaction = {\r\n-        'author': 'Sergio',\r\n-        'content': 'Attack',\r\n+        'author': 'Captain Hook',\r\n+        'content': 'A simple default transaction.',\r\n         'timestamp': time.time(),\r\n-        'hash': sha256('Attack'.encode()).hexdigest()\r\n+        'hash': sha256('A simple default transaction.'.encode()).hexdigest()\r\n     }\r\n \r\n-    #last_block = self.last_block\r\n-    tampered_block = Block(index=999,#last_block.index + 1,\r\n-                           transactions=[transaction],\r\n-                           timestamp=time.time(),\r\n-                           previous_hash='0x123abc',#last_block.hash,\r\n-                           miner=request.host_url)\r\n-\r\n-    hash_tampered_block = Blockchain.proof_of_work(tampered_block)\r\n-    tampered_block.hash = hash_tampered_block\r\n-\r\n-    #------------------------------------\r\n-    '''\r\n-    # ALTERNATIVE, no correct hash field due to later changes on transaction content\r\n-    transaction = {\r\n-        'author': 'Sergio',\r\n-        'content': 'I like icecream',\r\n-        'timestamp': time.time(),\r\n-        'hash': sha256('Attack'.encode()).hexdigest()\r\n-    }\r\n-    #global blockchain\r\n     last_block = blockchain.last_block\r\n-    tampered_block = Block(index=last_block.index + 1,\r\n+    default_block = Block( index=last_block.index + 1,\r\n                            transactions=[transaction],\r\n                            timestamp=time.time(),\r\n                            previous_hash=last_block.hash,\r\n-                           miner=request.host_url)\r\n-    \r\n-    hash_tampered_block = Blockchain.proof_of_work(tampered_block)\r\n-    tampered_block.hash = hash_tampered_block\r\n-    # no the block becomes tampered\r\n-    # because if you compute now the hash again, then it will not match with the hash field\r\n-    # but recomputing takes to much time for the attacker due to the \"difficulty\"\r\n-    \r\n-    #print(\"UDO:\"+tampered_block.transactions[0][\"content\"])\r\n-    tampered_block.transactions[0][\"content\"] = \"I hate icecream\"\r\n-    '''\r\n-    if not peers:\r\n-        return \"No peers existing\"\r\n-\r\n-    response = announce_new_block(tampered_block)\r\n-\r\n-    if not response[\"status\"]:\r\n-        # means that the block was not added\r\n-        return \"Tampered block was identified: \"+response[\"message\"]\r\n+                           miner=request.host_url )\r\n+\r\n+    if data[\"attack\"]==\"A\":\r\n+        # we just add a default block\r\n+        # JUST NEEDED FOR TEST PURPOSES\r\n+        hash_default_block = Blockchain.proof_of_work(default_block)\r\n+        default_block.hash = hash_default_block\r\n+\r\n+        response = announce_new_block(default_block)\r\n+        if response[\"status\"]:\r\n+            return \"Default block was added.\"\r\n+        else:\r\n+            return \"Something not working.\"\r\n     else:\r\n-        # because we actively sent a uncorrect block, but if this was accepted then something\r\n-        # of the proofing alogorithms is not working\r\n-        return \"Security mechanism is not working\"\r\n+\r\n+        tampered_block = default_block\r\n+\r\n+        if data[\"attack\"]==\"B\":\r\n+            # attack where the transaction and the block fields are correct\r\n+            # but the attacker used a different last.block as reference\r\n+            # thus he is kind of \"suggesting\" an alternative blockchain\r\n+                \r\n+            if last_block.index == 0:\r\n+                return \"Blockchain only has genesis block. No chage of previous hash possible.\"\r\n+            else:\r\n+                block = blockchain.retrieve_block(last_block.index-1)\r\n+                \r\n+                tampered_block.previous_hash = block.hash\r\n+                tampered_block.index = block.index\r\n+                hash_tampered_block = Blockchain.proof_of_work(tampered_block)\r\n+                tampered_block.hash = hash_tampered_block\r\n+\r\n+        elif data[\"attack\"]==\"C\":\r\n+            # first calc hash, then change transaction content\r\n+            hash_tampered_block = Blockchain.proof_of_work(tampered_block)\r\n+            tampered_block.hash = hash_tampered_block\r\n+            # no the block becomes tampered\r\n+            # because if you compute now the hash again, then it will not match with the hash field\r\n+            # but recomputing takes to much time for the attacker due to the \"difficulty\"\r\n+            tampered_block.transactions[0][\"content\"] = \"ATTACK\"\r\n+            tampered_block.transactions[0][\"hash\"] = sha256('ATTACK'.encode()).hexdigest()\r\n+        \r\n+        elif data[\"attack\"]==\"D\":\r\n+            # everything fine: previous hash, hash field matches content, but the difficulty was decreased\r\n+            # to be the first\r\n+            current_diff = Blockchain.get_difficulty()\r\n+            Blockchain.set_difficulty(1)\r\n+\r\n+            hash_tampered_block = Blockchain.proof_of_work(tampered_block)\r\n+            tampered_block.hash = hash_tampered_block\r\n+            \r\n+            Blockchain.set_difficulty(current_diff)\r\n+    \r\n+        response = announce_new_block(tampered_block)\r\n+      \r\n+        if not response[\"status\"]:\r\n+            # means that the block was not added\r\n+            return \"Tampered block was identified: \"+response[\"message\"]\r\n+        else:\r\n+            # because we actively sent a uncorrect block, but if this was accepted then something\r\n+            # of the proofing alogorithms is not working\r\n+            return \"Security mechanism is not working\"\r\n \r\n @app.route('\/modify_difficulty', methods=['POST'])\r\n def modify_difficulty():\r\n@@ -621,6 +659,7 @@ def announce_new_block(block):\n         response = requests.post(url,\r\n                                  data=json.dumps(block.__dict__, sort_keys=True),\r\n                                  headers=headers)\r\n+        \r\n         print(response.json())\r\n         # response is a dictionary\/json object, with the field status and message\r\n         # status indicating if the block was added by the peer, message why potentially not\r\ndiff --git a\/run_app.py b\/run_app.py\nindex 2bb47c8..c7a4d36 100644\n--- a\/run_app.py\n+++ b\/run_app.py\n@@ -10,7 +10,7 @@\n \r\n ##########\r\n #FELIX\r\n-######\r\n+#####\r\n # Before package structre, with importing the app variable from the app folder, and calling app.run here, and having\r\n # the application code (view) in separate file. Probably there was a reason, but so far it works this way too and is maybe more intuitive\r\n ###### \r\n@@ -237,8 +237,15 @@ def reg_with():\n @app.route('\/tampered_block', methods=['POST'])\r\n def tampered_block():\r\n     address = \"{}\/attack\".format(CONNECTED_NODE_ADDRESS)\r\n-    message = requests.get(address)\r\n \r\n+    # has to be fetched from the specified check boxes\r\n+    attack_kind = {\"attack\":\"C\"}\r\n+\r\n+\r\n+    message = requests.post(address,\r\n+                            json=attack_kind,\r\n+                            headers={'Content-type': 'application\/json'})\r\n+   \r\n     global attack\r\n     attack = message\r\n \r\ndiff --git a\/templates\/index.html b\/templates\/index.html\nindex a429029..9f78315 100644\n--- a\/templates\/index.html\n+++ b\/templates\/index.html\n@@ -110,7 +110,7 @@\n \t\t\t\t<form action=\"\/add_new_node\" id=\"textform\" method=\"post\">\r\n \t\t\t\t\t<center><h3>Add New Node<\/h3><\/center>\r\n \t\t\t\t\t<center>\r\n-\t\t\t\t\t<textarea name=\"list_nodes\" rows=\"2\" cols=\"50\" placeholder=\"Input the new node...\"><\/textarea>\r\n+\t\t\t\t\t<textarea name=\"new_node\" rows=\"2\" cols=\"50\" placeholder=\"Input the new node...\"><\/textarea>\r\n \t\t\t\t\t<\/center>\r\n \t\t\r\n \t\t\t\t\t<p><b> New node added:<\/b> {{new_node}}<\/p>\r\n","files":{"\/node_server.py":{"changes":[{"diff":"\n from hashlib import sha256\r\n import json\r\n import time\r\n-\r\n+import numpy as np \r\n import sys\r\n-from flask import Flask, request, redirect\r\n+from flask import Flask, request, redirect, render_template\r\n import requests\r\n \r\n class Block:\r\n","add":2,"remove":2,"filename":"\/node_server.py","badparts":["\r","from flask import Flask, request, redirect\r"],"goodparts":["import numpy as np \r","from flask import Flask, request, redirect, render_template\r"]},{"diff":"\n     # this node will announce the new block also to its peers\r\n     # thus the node is propagated through the network\r\n     \r\n+    # only if peers are known the announce is called, and when the received block is correct \r\n     if added[0] and peers:\r\n         announce_new_block(block)\r\n \r\n     #return \"Block added to the chain\", 201\r\n-\r\n+       \r\n     return json.dumps({\"status\":added[0],\"message\":added[1]})\r\n \r\n # endpoint to query unconfirmed transactions\r\n","add":2,"remove":1,"filename":"\/node_server.py","badparts":["\r"],"goodparts":["       \r"]},{"diff":"\n \r\n     return redirect(\"\/chain\")\r\n \r\n-@app.route('\/attack')\r\n+@app.route('\/attack', methods=['POST'])\r\n def attack():\r\n-    '''\r\n-    Create a tampered block\r\n-    '''\r\n+    \r\n+    if not peers:\r\n+        return \"No peers existing\"\r\n+    \r\n+    data = request.get_json()\r\n+    # data should contain the field attack which specifies which attack: A,B,C,D\r\n+    # where A means no attack\r\n+    # if we would like to also give contents which should be used in the different attacks\r\n+    # then we can specify a further field, which is used by the respective attack\r\n \r\n-    # attack where the transaction and the block fields are correct\r\n-    # but the attacker used a different last.block as reference\r\n-    # thus he is kind of \"suggesting\" an alternative blockchain \r\n+    global blockchain\r\n     transaction = {\r\n-        'author': 'Sergio',\r\n-        'content': 'Attack',\r\n+        'author': 'Captain Hook',\r\n+        'content': 'A simple default transaction.',\r\n         'timestamp': time.time(),\r\n-        'hash': sha256('Attack'.encode()).hexdigest()\r\n+        'hash': sha256('A simple default transaction.'.encode()).hexdigest()\r\n     }\r\n \r\n-    #last_block = self.last_block\r\n-    tampered_block = Block(index=999,#last_block.index + 1,\r\n-                           transactions=[transaction],\r\n-                           timestamp=time.time(),\r\n-                           previous_hash='0x123abc',#last_block.hash,\r\n-                           miner=request.host_url)\r\n-\r\n-    hash_tampered_block = Blockchain.proof_of_work(tampered_block)\r\n-    tampered_block.hash = hash_tampered_block\r\n-\r\n-    #------------------------------------\r\n-    '''\r\n-    # ALTERNATIVE, no correct hash field due to later changes on transaction content\r\n-    transaction = {\r\n-        'author': 'Sergio',\r\n-        'content': 'I like icecream',\r\n-        'timestamp': time.time(),\r\n-        'hash': sha256('Attack'.encode()).hexdigest()\r\n-    }\r\n-    #global blockchain\r\n     last_block = blockchain.last_block\r\n-    tampered_block = Block(index=last_block.index + 1,\r\n+    default_block = Block( index=last_block.index + 1,\r\n                            transactions=[transaction],\r\n                            timestamp=time.time(),\r\n                            previous_hash=last_block.hash,\r\n-                           miner=request.host_url)\r\n-    \r\n-    hash_tampered_block = Blockchain.proof_of_work(tampered_block)\r\n-    tampered_block.hash = hash_tampered_block\r\n-    # no the block becomes tampered\r\n-    # because if you compute now the hash again, then it will not match with the hash field\r\n-    # but recomputing takes to much time for the attacker due to the \"difficulty\"\r\n-    \r\n-    #print(\"UDO:\"+tampered_block.transactions[0][\"content\"])\r\n-    tampered_block.transactions[0][\"content\"] = \"I hate icecream\"\r\n-    '''\r\n-    if not peers:\r\n-        return \"No peers existing\"\r\n-\r\n-    response = announce_new_block(tampered_block)\r\n-\r\n-    if not response[\"status\"]:\r\n-        # means that the block was not added\r\n-        return \"Tampered block was identified: \"+response[\"message\"]\r\n+                           miner=request.host_url )\r\n+\r\n+    if data[\"attack\"]==\"A\":\r\n+        # we just add a default block\r\n+        # JUST NEEDED FOR TEST PURPOSES\r\n+        hash_default_block = Blockchain.proof_of_work(default_block)\r\n+        default_block.hash = hash_default_block\r\n+\r\n+        response = announce_new_block(default_block)\r\n+        if response[\"status\"]:\r\n+            return \"Default block was added.\"\r\n+        else:\r\n+            return \"Something not working.\"\r\n     else:\r\n-        # because we actively sent a uncorrect block, but if this was accepted then something\r\n-        # of the proofing alogorithms is not working\r\n-        return \"Security mechanism is not working\"\r\n+\r\n+        tampered_block = default_block\r\n+\r\n+        if data[\"attack\"]==\"B\":\r\n+            # attack where the transaction and the block fields are correct\r\n+            # but the attacker used a different last.block as reference\r\n+            # thus he is kind of \"suggesting\" an alternative blockchain\r\n+                \r\n+            if last_block.index == 0:\r\n+                return \"Blockchain only has genesis block. No chage of previous hash possible.\"\r\n+            else:\r\n+                block = blockchain.retrieve_block(last_block.index-1)\r\n+                \r\n+                tampered_block.previous_hash = block.hash\r\n+                tampered_block.index = block.index\r\n+                hash_tampered_block = Blockchain.proof_of_work(tampered_block)\r\n+                tampered_block.hash = hash_tampered_block\r\n+\r\n+        elif data[\"attack\"]==\"C\":\r\n+            # first calc hash, then change transaction content\r\n+            hash_tampered_block = Blockchain.proof_of_work(tampered_block)\r\n+            tampered_block.hash = hash_tampered_block\r\n+            # no the block becomes tampered\r\n+            # because if you compute now the hash again, then it will not match with the hash field\r\n+            # but recomputing takes to much time for the attacker due to the \"difficulty\"\r\n+            tampered_block.transactions[0][\"content\"] = \"ATTACK\"\r\n+            tampered_block.transactions[0][\"hash\"] = sha256('ATTACK'.encode()).hexdigest()\r\n+        \r\n+        elif data[\"attack\"]==\"D\":\r\n+            # everything fine: previous hash, hash field matches content, but the difficulty was decreased\r\n+            # to be the first\r\n+            current_diff = Blockchain.get_difficulty()\r\n+            Blockchain.set_difficulty(1)\r\n+\r\n+            hash_tampered_block = Blockchain.proof_of_work(tampered_block)\r\n+            tampered_block.hash = hash_tampered_block\r\n+            \r\n+            Blockchain.set_difficulty(current_diff)\r\n+    \r\n+        response = announce_new_block(tampered_block)\r\n+      \r\n+        if not response[\"status\"]:\r\n+            # means that the block was not added\r\n+            return \"Tampered block was identified: \"+response[\"message\"]\r\n+        else:\r\n+            # because we actively sent a uncorrect block, but if this was accepted then something\r\n+            # of the proofing alogorithms is not working\r\n+            return \"Security mechanism is not working\"\r\n \r\n @app.route('\/modify_difficulty', methods=['POST'])\r\n def modify_difficulty():\r\n","add":76,"remove":53,"filename":"\/node_server.py","badparts":["@app.route('\/attack')\r","    '''\r","    Create a tampered block\r","    '''\r","        'author': 'Sergio',\r","        'content': 'Attack',\r","        'hash': sha256('Attack'.encode()).hexdigest()\r","    tampered_block = Block(index=999,#last_block.index + 1,\r","                           transactions=[transaction],\r","                           timestamp=time.time(),\r","                           previous_hash='0x123abc',#last_block.hash,\r","                           miner=request.host_url)\r","\r","    hash_tampered_block = Blockchain.proof_of_work(tampered_block)\r","    tampered_block.hash = hash_tampered_block\r","\r","    '''\r","    transaction = {\r","        'author': 'Sergio',\r","        'content': 'I like icecream',\r","        'timestamp': time.time(),\r","        'hash': sha256('Attack'.encode()).hexdigest()\r","    }\r","    tampered_block = Block(index=last_block.index + 1,\r","                           miner=request.host_url)\r","    \r","    hash_tampered_block = Blockchain.proof_of_work(tampered_block)\r","    tampered_block.hash = hash_tampered_block\r","    \r","    tampered_block.transactions[0][\"content\"] = \"I hate icecream\"\r","    '''\r","    if not peers:\r","        return \"No peers existing\"\r","\r","    response = announce_new_block(tampered_block)\r","\r","    if not response[\"status\"]:\r","        return \"Tampered block was identified: \"+response[\"message\"]\r","        return \"Security mechanism is not working\"\r"],"goodparts":["@app.route('\/attack', methods=['POST'])\r","    \r","    if not peers:\r","        return \"No peers existing\"\r","    \r","    data = request.get_json()\r","    global blockchain\r","        'author': 'Captain Hook',\r","        'content': 'A simple default transaction.',\r","        'hash': sha256('A simple default transaction.'.encode()).hexdigest()\r","    default_block = Block( index=last_block.index + 1,\r","                           miner=request.host_url )\r","\r","    if data[\"attack\"]==\"A\":\r","        hash_default_block = Blockchain.proof_of_work(default_block)\r","        default_block.hash = hash_default_block\r","\r","        response = announce_new_block(default_block)\r","        if response[\"status\"]:\r","            return \"Default block was added.\"\r","        else:\r","            return \"Something not working.\"\r","\r","        tampered_block = default_block\r","\r","        if data[\"attack\"]==\"B\":\r","                \r","            if last_block.index == 0:\r","                return \"Blockchain only has genesis block. No chage of previous hash possible.\"\r","            else:\r","                block = blockchain.retrieve_block(last_block.index-1)\r","                \r","                tampered_block.previous_hash = block.hash\r","                tampered_block.index = block.index\r","                hash_tampered_block = Blockchain.proof_of_work(tampered_block)\r","                tampered_block.hash = hash_tampered_block\r","\r","        elif data[\"attack\"]==\"C\":\r","            hash_tampered_block = Blockchain.proof_of_work(tampered_block)\r","            tampered_block.hash = hash_tampered_block\r","            tampered_block.transactions[0][\"content\"] = \"ATTACK\"\r","            tampered_block.transactions[0][\"hash\"] = sha256('ATTACK'.encode()).hexdigest()\r","        \r","        elif data[\"attack\"]==\"D\":\r","            current_diff = Blockchain.get_difficulty()\r","            Blockchain.set_difficulty(1)\r","\r","            hash_tampered_block = Blockchain.proof_of_work(tampered_block)\r","            tampered_block.hash = hash_tampered_block\r","            \r","            Blockchain.set_difficulty(current_diff)\r","    \r","        response = announce_new_block(tampered_block)\r","      \r","        if not response[\"status\"]:\r","            return \"Tampered block was identified: \"+response[\"message\"]\r","        else:\r","            return \"Security mechanism is not working\"\r"]}],"source":"\nfrom hashlib import sha256\r import json\r import time\r \r import sys\r from flask import Flask, request, redirect\r import requests\r \r class Block:\r def __init__(self, index, transactions, timestamp, previous_hash, nonce=0, miner=\"\"):\r self.index=index\r self.transactions=transactions\r self.timestamp=timestamp\r self.previous_hash=previous_hash\r \r self.nonce=nonce\r self.miner=miner\r \r def compute_hash(self):\r \"\"\"\r A function that return the hash of the block contents.\r \"\"\"\r block_string=json.dumps(self.__dict__, sort_keys=True)\r return sha256(block_string.encode()).hexdigest()\r \r \r class Blockchain:\r \r difficulty=2\r \r def __init__(self):\r self.unconfirmed_transactions=[]\r self.chain=[]\r \r def create_genesis_block(self):\r \"\"\"\r A function to generate genesis block and appends it to\r the chain. The block has index 0, previous_hash as 0, and\r a valid hash.\r \"\"\"\r genesis_block=Block(0,[], 0, \"0\")\r genesis_block.hash=genesis_block.compute_hash()\r self.chain.append(genesis_block)\r \r @property\r def last_block(self):\r return self.chain[-1]\r \r def add_block(self, block, proof):\r \"\"\"\r A function that adds the block to the chain after verification.\r Verification includes:\r * Checking if the proof is valid.\r * The previous_hash referred in the block and the hash of latest block\r in the chain match.\r \"\"\"\r previous_hash=self.last_block.hash\r \r if previous_hash==proof:\r return(False,\"Block already added\")\r \r if previous_hash !=block.previous_hash:\r return(False, \"Previous hash not correct\")\r \r block_validity=Blockchain.is_valid_proof(block,proof)\r '''\r if not Blockchain.is_valid_proof(block, proof):\r return False\r \r block.hash=proof\r \r self.chain.append(block)\r '''\r if block_validity[0]:\r block.hash=proof\r self.chain.append(block)\r \r return block_validity\r \r def add_new_transaction(self, transaction):\r self.unconfirmed_transactions.append(transaction)\r \r def get_last_tx_timestamp(self):\r self.unconfirmed_transactions[-1][\"timestamp\"]\r \r def mine(self):\r \"\"\"\r This function serves as an interface to add the pending\r transactions to the blockchain by adding them to the block\r and figuring out Proof Of Work.\r \"\"\"\r if not self.unconfirmed_transactions:\r return False\r \r last_block=self.last_block\r \r new_block=Block(index=last_block.index +1,\r transactions=self.unconfirmed_transactions,\r timestamp=time.time(),\r previous_hash=last_block.hash,\r miner=request.host_url)\r \r proof=self.proof_of_work(new_block)\r \r self.add_block(new_block, proof)\r \r self.unconfirmed_transactions=[]\r \r return True\r \r \r @classmethod\r def proof_of_work(cls,block):\r \"\"\"\r Function that tries different values of nonce to get a hash\r that satisfies our difficulty criteria.\r \"\"\"\r block.nonce=0\r \r computed_hash=block.compute_hash()\r while not computed_hash.startswith('0' * cls.difficulty):\r block.nonce +=1\r computed_hash=block.compute_hash()\r \r return computed_hash\r \r @classmethod\r def set_difficulty(cls,diff):\r if(diff<=0):\r return False\r \r cls.difficulty=diff\r return True\r \r @classmethod\r def get_difficulty(cls):\r return cls.difficulty\r \r @classmethod\r def is_valid_proof(cls, block, block_hash):\r \"\"\"\r Check if block_hash is valid hash of block and satisfies\r the difficulty criteria.\r \"\"\"\r \r if not block_hash==block.compute_hash():\r return(False, \"Hash not correct\")\r \r if not block_hash.startswith('0' * Blockchain.difficulty):\r return(False, \"Required difficulty not fullfilled\")\r \r return(True, \"Proof correct\")\r \r @classmethod\r def check_chain_validity(cls, chain):\r result=True\r previous_hash=\"0\"\r \r for block in chain:\r block_hash=block.hash\r delattr(block, \"hash\")\r \r if not cls.is_valid_proof(block, block_hash)[0] or previous_hash !=block.previous_hash:\r result=False\r break\r \r block.hash, previous_hash=block_hash, block_hash\r \r return result\r \r \r \r app=Flask(__name__)\r localhost=\"http:\/\/127.0.0.1:\"\r blockchain=Blockchain()\r blockchain.create_genesis_block()\r \r peers=set()\r \r @app.route('\/new_transaction', methods=['POST'])\r def new_transaction():\r tx_data=request.get_json()\r required_fields=[\"author\", \"content\", \"timestamp\"]\r \r for field in required_fields:\r if not tx_data.get(field):\r return \"Invalid transaction data\", 404\r '''\r time_tx=tx_data[\"timestamp\"]\r time_last_added_tx=blockchain.get_last_tx_timestamp()\r \r if(time_tx==time_last_added_tx):\r return \"Transaction already received\"\r else:\r \r blockchain.add_new_transaction(tx_data)\r \r for peer in peers:\r new_tx_address=\"{}\/new_transaction\".format(peer)\r \r requests.post(new_tx_address,\r json=json.dumps(tx_data),\r headers={'Content-type': 'application\/json'})\r \r return \"Success\", 201\r '''\r \r blockchain.add_new_transaction(tx_data)\r \r return \"Success\", 201\r \r \r @app.route('\/chain', methods=['GET'])\r def get_chain():\r chain_data=[]\r for block in blockchain.chain:\r chain_data.append(block.__dict__)\r return json.dumps({\"length\": len(chain_data),\r \"chain\": chain_data,\r \"peers\": list(peers)})\r \r \r @app.route('\/mine', methods=['GET'])\r def mine_unconfirmed_transactions():\r \r '''\r request_info=request.get_json()\r \r if not request_info.get(\"reached_nodes\"):\r reached_nodes=[]\r else: \r node_address=request.host_url()\r reached_nodes=request_info[\"reached_nodes\"]\r \r if node_address in reached_nodes:\r return \"already received task\"\r \r for peer in peers:\r new_peer_address=\"{}\/mine\".format(peer)\r reached_nodes.append(node_address)\r \r requests.post(new_peer_address,\r json=json.dumps(reached_nodes),\r headers={'Content-type': 'application\/json'})\r '''\r \r result=blockchain.mine()\r if not result:\r return \"No transactions to mine\"\r else:\r '''\r chain_length=len(blockchain.chain)\r \r consensus()\r \r if chain_length==len(blockchain.chain):\r announce_new_block(blockchain.last_block)\r '''\r if peers:\r announce_new_block(blockchain.last_block)\r \r return \"Block \r @app.route('\/register_node', methods=['POST'])\r def register_new_peers():\r node_address=request.get_json()[\"node_address\"]\r if not node_address:\r return \"Invalid data\", 400\r \r \r peers.add(node_address)\r \r \r return \"New node added\", 200\r \r @app.route('\/register_with', methods=['POST'])\r def register_with_existing_node(): \"\"\"\r Internally calls the `register_node` endpoint to\r register current node with the node specified in the\r request, and sync the blockchain as well as peer data.\r \"\"\"\r peers_list=request.get_json()[\"peers_list\"]\r \r if not peers_list:\r return \"Invalid data\", 400\r \r data={\"node_address\": request.host_url}\r headers={'Content-Type': \"application\/json\"}\r \r '''\r response=requests.post(node_address +\"\/register_node\",\r data=json.dumps(data), headers=headers)\r '''\r for peer in peers_list:\r if peer in peers:\r continue\r response=requests.post(peer +\"\/register_node\",\r data=json.dumps(data), headers=headers)\r if response.status_code==200:\r peers.add(peer)\r \r \r update_local_chain=consensus()\r \r ''' \r if response.status_code==200:\r global blockchain\r global peers\r chain_dump=response.json()['chain']\r blockchain=create_chain_from_dump(chain_dump)\r peers.add(node_address)\r \r \r return \"Registration successful\"+\" \"+node_address, 200\r else:\r return response.content, response.status_code\r '''\r \r \r if update_local_chain[0]:\r return \"Registration successful. Chain updated\", 200\r else: \r return \"Registration succesful. No longer chain found among peers\"\r \r \r \r @app.route('\/synchronize_with_peers')\r def synch_with_peers():\r synch=consensus()\r \r if not peers:\r return \"No Peers to synchronize with.\"\r \r if synch[0]:\r return \"Synchronisation succesful.\"\r else:\r return \"No longer chain found among peers.\"\r \r \r @app.route('\/add_block', methods=['POST'])\r def verify_and_add_block():\r block_data=request.get_json()\r block=Block(block_data[\"index\"],\r block_data[\"transactions\"],\r block_data[\"timestamp\"],\r block_data[\"previous_hash\"],\r block_data[\"nonce\"],\r block_data[\"miner\"])\r \r proof=block_data['hash']\r added=blockchain.add_block(block, proof)\r \r if added[1]==\"Block already added\":\r return json.dumps({\"status\":added[0],\"message\":added[1]})\r \r \r \r if added[0] and peers:\r announce_new_block(block)\r \r \r return json.dumps({\"status\":added[0],\"message\":added[1]})\r \r @app.route('\/pending_tx')\r def get_pending_tx():\r return json.dumps(blockchain.unconfirmed_transactions)\r \r @app.route('\/add_default_block')\r def add_fixed_block():\r global blockchain\r \r transaction={\r 'author': 'Peter Pan',\r 'content': 'Wonderland',\r 'timestamp': time.time(),\r 'hash': sha256('Wonderland'.encode()).hexdigest()\r }\r last_block=blockchain.last_block\r default_block=Block(index=last_block.index +1,\r transactions=[transaction],\r timestamp=time.time(),\r previous_hash=last_block.hash,\r miner=request.host_url)\r \r hash_default_block=Blockchain.proof_of_work(default_block)\r default_block.hash=hash_default_block\r \r blockchain.chain.append(default_block)\r \r return redirect(\"\/chain\")\r \r @app.route('\/attack')\r def attack():\r '''\r Create a tampered block\r '''\r \r transaction={\r 'author': 'Sergio',\r 'content': 'Attack',\r 'timestamp': time.time(),\r 'hash': sha256('Attack'.encode()).hexdigest()\r }\r \r tampered_block=Block(index=999, transactions=[transaction],\r timestamp=time.time(),\r previous_hash='0x123abc', miner=request.host_url)\r \r hash_tampered_block=Blockchain.proof_of_work(tampered_block)\r tampered_block.hash=hash_tampered_block\r \r '''\r transaction={\r 'author': 'Sergio',\r 'content': 'I like icecream',\r 'timestamp': time.time(),\r 'hash': sha256('Attack'.encode()).hexdigest()\r }\r last_block=blockchain.last_block\r tampered_block=Block(index=last_block.index +1,\r transactions=[transaction],\r timestamp=time.time(),\r previous_hash=last_block.hash,\r miner=request.host_url)\r \r hash_tampered_block=Blockchain.proof_of_work(tampered_block)\r tampered_block.hash=hash_tampered_block\r \r tampered_block.transactions[0][\"content\"]=\"I hate icecream\"\r '''\r if not peers:\r return \"No peers existing\"\r \r response=announce_new_block(tampered_block)\r \r if not response[\"status\"]:\r return \"Tampered block was identified: \"+response[\"message\"]\r else:\r return \"Security mechanism is not working\"\r \r @app.route('\/modify_difficulty', methods=['POST'])\r def modify_difficulty():\r '''\r Set a different difficulty\r '''\r \r diff=request.get_json()[\"difficulty\"]\r print(diff)\r set_diff=Blockchain.set_difficulty(int(diff))\r \r if set_diff:\r return diff\r else:\r return \"Difficulty unchanged.\"\r '''\r @app.route('\/get_difficulty')\r def retrieve_difficulty(self,diff):\r return blockchain.difficulty\r '''\r \r def create_chain_from_dump(chain_dump):\r generated_blockchain=Blockchain()\r generated_blockchain.create_genesis_block()\r for idx, block_data in enumerate(chain_dump):\r if idx==0:\r continue block=Block(block_data[\"index\"],\r block_data[\"transactions\"],\r block_data[\"timestamp\"],\r block_data[\"previous_hash\"],\r block_data[\"nonce\"],\r block_data[\"miner\"])\r proof=block_data['hash']\r added=generated_blockchain.add_block(block, proof)\r if not added[0]:\r return(added[0],added[1], block.index)\r return(True, generated_blockchain)\r \r def consensus():\r \"\"\"\r Our naive consnsus algorithm. If a longer valid chain is\r found, our chain is replaced with it.\r \"\"\"\r global blockchain\r global peers\r \r longest_chain=None\r current_len=len(blockchain.chain)\r peers_list_with_incorrect_chains=[]\r \r for peer in peers:\r response=requests.get('{}\/chain'.format(peer))\r length=response.json()['length']\r chain=response.json()['chain']\r \r print(\"Length:\",length)\r print(\"Current Length:\", current_len)\r fetched_chain_feedback=create_chain_from_dump(chain)\r \r if not fetched_chain_feedback[0]:\r peers_list_with_incorrect_chains.append((peer,fetched_chain_feedback[1], fetched_chain_feedback[2]))\r continue\r '''\r if length > current_len and blockchain.check_chain_validity(chain.chain):\r current_len=length\r longest_chain=chain\r '''\r peers_chain=fetched_chain_feedback[1]\r if len(peers_chain.chain) > current_len:\r current_len=length\r longest_chain=peers_chain\r \r if longest_chain:\r blockchain=longest_chain\r return(True, peers_list_with_incorrect_chains)\r \r return(False, peers_list_with_incorrect_chains)\r \r def announce_new_block(block):\r \"\"\"\r A function to announce to the network once a block has been mined.\r Other nodes can simply verify the proof of work and add it to their\r respective chains.\r \"\"\"\r responses=[]\r \r for peer in peers:\r url=\"{}\/add_block\".format(peer)\r headers={'Content-Type': \"application\/json\"}\r response=requests.post(url,\r data=json.dumps(block.__dict__, sort_keys=True),\r headers=headers)\r print(response.json())\r responses.append(response.json())\r \r for r in responses:\r if not r[\"status\"]:\r return r\r \r print(responses)\r return responses[0]\r \r \r if __name__==\"__main__\":\r port=sys.argv[1]\r app.run(debug=True,port=port)\r \r ","sourceWithComments":"from hashlib import sha256\r\nimport json\r\nimport time\r\n\r\nimport sys\r\nfrom flask import Flask, request, redirect\r\nimport requests\r\n\r\nclass Block:\r\n    def __init__(self, index, transactions, timestamp, previous_hash, nonce=0, miner=\"\"):\r\n        self.index = index\r\n        self.transactions = transactions\r\n        self.timestamp = timestamp\r\n        self.previous_hash = previous_hash\r\n\r\n        self.nonce = nonce\r\n        self.miner = miner\r\n\r\n    def compute_hash(self):\r\n        \"\"\"\r\n        A function that return the hash of the block contents.\r\n        \"\"\"\r\n        block_string = json.dumps(self.__dict__, sort_keys=True)\r\n        return sha256(block_string.encode()).hexdigest()\r\n\r\n\r\nclass Blockchain:\r\n    \r\n    difficulty = 2\r\n\r\n    def __init__(self):\r\n        self.unconfirmed_transactions = []\r\n        self.chain = []\r\n        # difficulty of our PoW algorithm\r\n        \r\n    def create_genesis_block(self):\r\n        \"\"\"\r\n        A function to generate genesis block and appends it to\r\n        the chain. The block has index 0, previous_hash as 0, and\r\n        a valid hash.\r\n        \"\"\"\r\n        genesis_block = Block(0, [], 0, \"0\")\r\n        # hash field has to be added after its creation !\r\n        genesis_block.hash = genesis_block.compute_hash()\r\n        self.chain.append(genesis_block)\r\n\r\n    @property\r\n    def last_block(self):\r\n        return self.chain[-1]\r\n\r\n    def add_block(self, block, proof):\r\n        \"\"\"\r\n        A function that adds the block to the chain after verification.\r\n        Verification includes:\r\n        * Checking if the proof is valid.\r\n        * The previous_hash referred in the block and the hash of latest block\r\n          in the chain match.\r\n        \"\"\"\r\n        previous_hash = self.last_block.hash\r\n        \r\n        if previous_hash == proof:\r\n            return (False,\"Block already added\")\r\n\r\n        if previous_hash != block.previous_hash:\r\n            return (False, \"Previous hash not correct\")\r\n\r\n        block_validity = Blockchain.is_valid_proof(block,proof)\r\n        '''\r\n        if not Blockchain.is_valid_proof(block, proof):\r\n            return False\r\n\r\n        block.hash = proof\r\n \r\n        self.chain.append(block)\r\n        '''\r\n        if block_validity[0]:\r\n            block.hash = proof\r\n            self.chain.append(block)\r\n\r\n        return block_validity\r\n\r\n    def add_new_transaction(self, transaction):\r\n        self.unconfirmed_transactions.append(transaction)\r\n\r\n    def get_last_tx_timestamp(self):\r\n        self.unconfirmed_transactions[-1][\"timestamp\"]\r\n    \r\n    def mine(self):\r\n        \"\"\"\r\n        This function serves as an interface to add the pending\r\n        transactions to the blockchain by adding them to the block\r\n        and figuring out Proof Of Work.\r\n        \"\"\"\r\n        if not self.unconfirmed_transactions:\r\n            return False\r\n\r\n        last_block = self.last_block\r\n\r\n        new_block = Block(index=last_block.index + 1,\r\n                          transactions=self.unconfirmed_transactions,\r\n                          timestamp=time.time(),\r\n                          previous_hash=last_block.hash,\r\n                          miner = request.host_url)\r\n\r\n        proof = self.proof_of_work(new_block)\r\n\r\n        self.add_block(new_block, proof)\r\n\r\n        self.unconfirmed_transactions = []\r\n\r\n        return True\r\n\r\n\r\n    @classmethod\r\n    def proof_of_work(cls,block):\r\n        \"\"\"\r\n        Function that tries different values of nonce to get a hash\r\n        that satisfies our difficulty criteria.\r\n        \"\"\"\r\n        block.nonce = 0\r\n\r\n        computed_hash = block.compute_hash()\r\n        while not computed_hash.startswith('0' * cls.difficulty):\r\n            block.nonce += 1\r\n            computed_hash = block.compute_hash()\r\n\r\n        return computed_hash\r\n\r\n    @classmethod\r\n    def set_difficulty(cls,diff):\r\n        if(diff<=0):\r\n            return False\r\n        \r\n        cls.difficulty = diff\r\n        return True\r\n\r\n    @classmethod\r\n    def get_difficulty(cls):\r\n        return cls.difficulty\r\n\r\n    @classmethod\r\n    def is_valid_proof(cls, block, block_hash):\r\n    #def is_valid_proof(self, block, block_hash):\r\n        \"\"\"\r\n        Check if block_hash is valid hash of block and satisfies\r\n        the difficulty criteria.\r\n        \"\"\"\r\n\r\n        if not block_hash == block.compute_hash():\r\n            return (False, \"Hash not correct\")\r\n\r\n        if not block_hash.startswith('0' * Blockchain.difficulty):\r\n            return (False, \"Required difficulty not fullfilled\")\r\n        \r\n        return (True, \"Proof correct\")\r\n        #return (block_hash.startswith('0' * Blockchain.difficulty) and\r\n        #        block_hash == block.compute_hash())\r\n\r\n    @classmethod\r\n    def check_chain_validity(cls, chain):\r\n        result = True\r\n        previous_hash = \"0\"\r\n\r\n        for block in chain:\r\n            block_hash = block.hash\r\n            # remove the hash field to recompute the hash again\r\n            # using `compute_hash` method.\r\n            delattr(block, \"hash\")\r\n\r\n            # [0] because the is_valid_proof returns a tuple\r\n            if not cls.is_valid_proof(block, block_hash)[0] or previous_hash != block.previous_hash:\r\n                result = False\r\n                break\r\n\r\n            block.hash, previous_hash = block_hash, block_hash\r\n\r\n        return result\r\n\r\n    \r\n\r\napp = Flask(__name__)\r\nlocalhost = \"http:\/\/127.0.0.1:\"\r\n# the node's copy of blockchain\r\nblockchain = Blockchain()\r\nblockchain.create_genesis_block()\r\n\r\n# the address to other participating members of the network\r\npeers = set()\r\n\r\n# endpoint to submit a new transaction. This will be used by\r\n# our application to add new data (posts) to the blockchain\r\n@app.route('\/new_transaction', methods=['POST'])\r\ndef new_transaction():\r\n    tx_data = request.get_json()\r\n    required_fields = [\"author\", \"content\", \"timestamp\"]\r\n\r\n    for field in required_fields:\r\n        if not tx_data.get(field):\r\n            return \"Invalid transaction data\", 404\r\n    '''\r\n    time_tx = tx_data[\"timestamp\"]\r\n    time_last_added_tx = blockchain.get_last_tx_timestamp()\r\n\r\n    if(time_tx==time_last_added_tx):\r\n        return \"Transaction already received\"\r\n    else:\r\n\r\n        blockchain.add_new_transaction(tx_data)\r\n\r\n        for peer in peers:\r\n            new_tx_address = \"{}\/new_transaction\".format(peer)\r\n\r\n            requests.post(new_tx_address,\r\n                          json=json.dumps(tx_data),\r\n                          headers={'Content-type': 'application\/json'})\r\n             \r\n        return \"Success\", 201\r\n    '''\r\n    #tx_data[\"timestamp\"] = time.time()\r\n\r\n    blockchain.add_new_transaction(tx_data)\r\n\r\n    return \"Success\", 201\r\n\r\n\r\n# endpoint to return the node's copy of the chain.\r\n# Our application will be using this endpoint to query\r\n# all the posts to display.\r\n@app.route('\/chain', methods=['GET'])\r\ndef get_chain():\r\n    chain_data = []\r\n    for block in blockchain.chain:\r\n        chain_data.append(block.__dict__)\r\n    return json.dumps({\"length\": len(chain_data),\r\n                       \"chain\": chain_data,\r\n                       \"peers\": list(peers)})\r\n\r\n\r\n# endpoint to request the node to mine the unconfirmed\r\n# transactions (if any). We'll be using it to initiate\r\n# a command to mine from our application itself.\r\n@app.route('\/mine', methods=['GET'])\r\ndef mine_unconfirmed_transactions():\r\n\r\n    '''\r\n    # checking if we already received mined transaction\r\n    request_info = request.get_json()\r\n\r\n    if not request_info.get(\"reached_nodes\"):\r\n        #return \"Error\", 201\r\n        reached_nodes = []\r\n    else:    \r\n        node_address = request.host_url()\r\n        reached_nodes = request_info[\"reached_nodes\"]\r\n\r\n    if node_address in reached_nodes:\r\n        return \"already received task\"\r\n    \r\n    for peer in peers:\r\n        new_peer_address = \"{}\/mine\".format(peer)\r\n        reached_nodes.append(node_address)\r\n\r\n        requests.post(new_peer_address,\r\n                      json=json.dumps(reached_nodes),\r\n                      headers={'Content-type': 'application\/json'})\r\n    '''\r\n\r\n    #wait() proportional to reached_nodes list, which will grow with a high steep and then saturated \r\n    result = blockchain.mine()\r\n    if not result:\r\n        return \"No transactions to mine\"\r\n    else:\r\n        '''\r\n        # Making sure we have the longest chain before announcing to the network\r\n        chain_length = len(blockchain.chain)\r\n               \r\n        consensus()\r\n        \r\n        if chain_length == len(blockchain.chain):\r\n            # announce the recently mined block to the network\r\n            announce_new_block(blockchain.last_block)\r\n        '''\r\n        if peers:\r\n            announce_new_block(blockchain.last_block)\r\n\r\n        return \"Block #{} is mined.\".format(blockchain.last_block.index)\r\n\r\n# endpoint to add new peers to the network.\r\n@app.route('\/register_node', methods=['POST'])\r\ndef register_new_peers():\r\n    node_address = request.get_json()[\"node_address\"]\r\n    if not node_address:\r\n        return \"Invalid data\", 400\r\n\r\n    ########\r\n    #FELIX\r\n    ########\r\n    #chain = get_chain()\r\n \r\n    # Add the node to the peer list\r\n    peers.add(node_address)\r\n\r\n    # Return the consensus blockchain to the newly registered node\r\n    # so that he can sync\r\n    #return chain\r\n\r\n    return \"New node added\", 200\r\n\r\n#D:\\Felix\\Documents\\Studiumunterlagen\\Vorlesungsskripte\\Master_LMU\\WS 20_21\\DataSecurity&Ethics\\Blockchain\\python_blockchain_app\r\n@app.route('\/register_with', methods=['POST'])\r\ndef register_with_existing_node(): # SHOULD BE RENMED INTO SYNCHRONIZE WITH NODES\/PEERS\r\n    \"\"\"\r\n    Internally calls the `register_node` endpoint to\r\n    register current node with the node specified in the\r\n    request, and sync the blockchain as well as peer data.\r\n    \"\"\"\r\n    peers_list = request.get_json()[\"peers_list\"]\r\n    \r\n    if not peers_list:\r\n        return \"Invalid data\", 400\r\n\r\n    # some peers were submitted, thus it will be checked if these peers are known to this node\r\n    # if not the node will register itself to the specified peers\r\n    data = {\"node_address\": request.host_url}\r\n    headers = {'Content-Type': \"application\/json\"}\r\n\r\n    '''\r\n    # Make a request to register with remote node and obtain information\r\n    response = requests.post(node_address + \"\/register_node\",\r\n                            data=json.dumps(data), headers=headers)\r\n    '''\r\n    for peer in peers_list:\r\n        if peer in peers:\r\n            continue\r\n        response = requests.post(peer + \"\/register_node\",\r\n                                 data=json.dumps(data), headers=headers)\r\n        if response.status_code==200:\r\n            peers.add(peer)\r\n\r\n    #if not peers_list:\r\n        # if the node has no peers registered yet\r\n    #    return \"No peers to synchronize with.\"\r\n\r\n    # this function could also be used, just to synchronize a node with its peers\r\n    # if you don't pass any peers, the node will call the consensus function\r\n    # and thus ask of all of its known peers for their blockchain and takes over\r\n    # the longst chain found unless its chain has equal or greated length\r\n    update_local_chain = consensus()\r\n    \r\n    '''       \r\n    if response.status_code == 200:\r\n        global blockchain\r\n        global peers\r\n        # update chain and the peers\r\n        chain_dump = response.json()['chain']\r\n        blockchain = create_chain_from_dump(chain_dump)\r\n        # peers.update(response.json()['peers'])\r\n        ############\r\n        #FELIX\r\n        ############\r\n        peers.add(node_address)\r\n        \r\n        \r\n        return \"Registration successful\"+\" \"+node_address, 200\r\n    else:\r\n        # if something goes wrong, pass it on to the API response\r\n        return response.content, response.status_code\r\n    '''\r\n\r\n    # update_local_chain[0] indicates whether the local chain was updated or not\r\n    # if not it means no longer chain was found\r\n    # update_local_chain[1] contains a list witht the peers whose chain was not valid\r\n    \r\n    if update_local_chain[0]:\r\n        return \"Registration successful. Chain updated\", 200\r\n    else:        \r\n        return \"Registration succesful. No longer chain found among peers\"\r\n\r\n\r\n\r\n@app.route('\/synchronize_with_peers')\r\ndef synch_with_peers():\r\n    synch = consensus()\r\n\r\n    if not peers:\r\n        return \"No Peers to synchronize with.\"\r\n\r\n    if synch[0]:\r\n        return \"Synchronisation succesful.\"\r\n    else:\r\n        return \"No longer chain found among peers.\"\r\n\r\n\r\n# endpoint to add a block mined by someone else to\r\n# the node's chain. The block is first verified by the node\r\n# and then added to the chain.\r\n@app.route('\/add_block', methods=['POST'])\r\ndef verify_and_add_block():\r\n    block_data = request.get_json()\r\n    block = Block(block_data[\"index\"],\r\n                  block_data[\"transactions\"],\r\n                  block_data[\"timestamp\"],\r\n                  block_data[\"previous_hash\"],\r\n                  block_data[\"nonce\"],\r\n                  block_data[\"miner\"])\r\n\r\n    proof = block_data['hash']\r\n    added = blockchain.add_block(block, proof)\r\n    # added[0] whether block was added\r\n    # added[1] message\r\n\r\n    if added[1] == \"Block already added\":\r\n        return json.dumps({\"status\":added[0],\"message\":added[1]})\r\n\r\n   #if not added:\r\n   #     return \"The block was discarded by the node\", 400\r\n\r\n    # this node received a new block from one of its peers, and after the previous checks\r\n    # this node will announce the new block also to its peers\r\n    # thus the node is propagated through the network\r\n    \r\n    if added[0] and peers:\r\n        announce_new_block(block)\r\n\r\n    #return \"Block added to the chain\", 201\r\n\r\n    return json.dumps({\"status\":added[0],\"message\":added[1]})\r\n\r\n# endpoint to query unconfirmed transactions\r\n@app.route('\/pending_tx')\r\ndef get_pending_tx():\r\n    return json.dumps(blockchain.unconfirmed_transactions)\r\n\r\n@app.route('\/add_default_block')\r\ndef add_fixed_block():\r\n    global blockchain\r\n\r\n    transaction = {\r\n        'author': 'Peter Pan',\r\n        'content': 'Wonderland',\r\n        'timestamp': time.time(),\r\n        'hash': sha256('Wonderland'.encode()).hexdigest()\r\n    }\r\n    last_block = blockchain.last_block\r\n    default_block = Block(index=last_block.index + 1,\r\n                           transactions=[transaction],\r\n                           timestamp=time.time(),\r\n                           previous_hash=last_block.hash,\r\n                           miner=request.host_url)\r\n    \r\n    hash_default_block = Blockchain.proof_of_work(default_block)\r\n    default_block.hash = hash_default_block\r\n   \r\n    blockchain.chain.append(default_block)\r\n\r\n    return redirect(\"\/chain\")\r\n\r\n@app.route('\/attack')\r\ndef attack():\r\n    '''\r\n    Create a tampered block\r\n    '''\r\n\r\n    # attack where the transaction and the block fields are correct\r\n    # but the attacker used a different last.block as reference\r\n    # thus he is kind of \"suggesting\" an alternative blockchain \r\n    transaction = {\r\n        'author': 'Sergio',\r\n        'content': 'Attack',\r\n        'timestamp': time.time(),\r\n        'hash': sha256('Attack'.encode()).hexdigest()\r\n    }\r\n\r\n    #last_block = self.last_block\r\n    tampered_block = Block(index=999,#last_block.index + 1,\r\n                           transactions=[transaction],\r\n                           timestamp=time.time(),\r\n                           previous_hash='0x123abc',#last_block.hash,\r\n                           miner=request.host_url)\r\n\r\n    hash_tampered_block = Blockchain.proof_of_work(tampered_block)\r\n    tampered_block.hash = hash_tampered_block\r\n\r\n    #------------------------------------\r\n    '''\r\n    # ALTERNATIVE, no correct hash field due to later changes on transaction content\r\n    transaction = {\r\n        'author': 'Sergio',\r\n        'content': 'I like icecream',\r\n        'timestamp': time.time(),\r\n        'hash': sha256('Attack'.encode()).hexdigest()\r\n    }\r\n    #global blockchain\r\n    last_block = blockchain.last_block\r\n    tampered_block = Block(index=last_block.index + 1,\r\n                           transactions=[transaction],\r\n                           timestamp=time.time(),\r\n                           previous_hash=last_block.hash,\r\n                           miner=request.host_url)\r\n    \r\n    hash_tampered_block = Blockchain.proof_of_work(tampered_block)\r\n    tampered_block.hash = hash_tampered_block\r\n    # no the block becomes tampered\r\n    # because if you compute now the hash again, then it will not match with the hash field\r\n    # but recomputing takes to much time for the attacker due to the \"difficulty\"\r\n    \r\n    #print(\"UDO:\"+tampered_block.transactions[0][\"content\"])\r\n    tampered_block.transactions[0][\"content\"] = \"I hate icecream\"\r\n    '''\r\n    if not peers:\r\n        return \"No peers existing\"\r\n\r\n    response = announce_new_block(tampered_block)\r\n\r\n    if not response[\"status\"]:\r\n        # means that the block was not added\r\n        return \"Tampered block was identified: \"+response[\"message\"]\r\n    else:\r\n        # because we actively sent a uncorrect block, but if this was accepted then something\r\n        # of the proofing alogorithms is not working\r\n        return \"Security mechanism is not working\"\r\n\r\n@app.route('\/modify_difficulty', methods=['POST'])\r\ndef modify_difficulty():\r\n    '''\r\n    Set a different difficulty\r\n    '''\r\n    \r\n    diff = request.get_json()[\"difficulty\"]\r\n    print(diff)\r\n    set_diff = Blockchain.set_difficulty(int(diff))\r\n    \r\n    if set_diff:\r\n        return diff\r\n    else:\r\n        return \"Difficulty unchanged.\"\r\n'''\r\n@app.route('\/get_difficulty')\r\ndef retrieve_difficulty(self,diff):\r\n    return blockchain.difficulty\r\n'''\r\n\r\ndef create_chain_from_dump(chain_dump):\r\n    generated_blockchain = Blockchain()\r\n    generated_blockchain.create_genesis_block()\r\n    for idx, block_data in enumerate(chain_dump):\r\n        if idx == 0:\r\n            continue  # skip genesis block\r\n        block = Block(block_data[\"index\"],\r\n                      block_data[\"transactions\"],\r\n                      block_data[\"timestamp\"],\r\n                      block_data[\"previous_hash\"],\r\n                      block_data[\"nonce\"],\r\n                      block_data[\"miner\"])\r\n        proof = block_data['hash']\r\n        added = generated_blockchain.add_block(block, proof)\r\n        if not added[0]:\r\n            # added[0] if the added block was added or not\r\n            # added[1] what was the error\r\n            return (added[0],added[1], block.index)\r\n    return (True, generated_blockchain)\r\n\r\ndef consensus():\r\n    \"\"\"\r\n    Our naive consnsus algorithm. If a longer valid chain is\r\n    found, our chain is replaced with it.\r\n    \"\"\"\r\n    global blockchain\r\n    global peers\r\n\r\n    longest_chain = None\r\n    current_len = len(blockchain.chain)\r\n    peers_list_with_incorrect_chains = []\r\n\r\n    for peer in peers:\r\n        response = requests.get('{}\/chain'.format(peer))\r\n        length = response.json()['length']\r\n        chain = response.json()['chain']\r\n\r\n        print(\"Length:\",length)\r\n        print(\"Current Length:\", current_len)\r\n        fetched_chain_feedback = create_chain_from_dump(chain)\r\n\r\n        if not fetched_chain_feedback[0]:\r\n            # then in this case fetched_chain_feedback[1] would be a message explaining\r\n            # the error of the tampered block and fetched_chain_feedback[2] would be the index of the \r\n            # respective block\r\n            peers_list_with_incorrect_chains.append((peer,fetched_chain_feedback[1], fetched_chain_feedback[2]))\r\n            continue\r\n        '''\r\n        if length > current_len and blockchain.check_chain_validity(chain.chain):\r\n            current_len = length\r\n            longest_chain = chain\r\n        '''\r\n        peers_chain = fetched_chain_feedback[1]\r\n        # peers_chain is an instance of Blockchain, peers_chain.chain is the attribute containing the \r\n        # the list of blocks, MAYBE CREATE an attribute like length for the Blockchain class \r\n        if len(peers_chain.chain) > current_len:\r\n            current_len = length\r\n            longest_chain = peers_chain\r\n\r\n    if longest_chain:\r\n        blockchain = longest_chain\r\n        return (True, peers_list_with_incorrect_chains)\r\n\r\n    return (False, peers_list_with_incorrect_chains)\r\n\r\ndef announce_new_block(block):\r\n    \"\"\"\r\n    A function to announce to the network once a block has been mined.\r\n    Other nodes can simply verify the proof of work and add it to their\r\n    respective chains.\r\n    \"\"\"\r\n    # gathering responses from its peers\r\n    # responses of peer of peers are not propagated through\r\n    responses = []\r\n   \r\n    for peer in peers:\r\n        url = \"{}\/add_block\".format(peer)\r\n        headers = {'Content-Type': \"application\/json\"}\r\n        response = requests.post(url,\r\n                                 data=json.dumps(block.__dict__, sort_keys=True),\r\n                                 headers=headers)\r\n        print(response.json())\r\n        # response is a dictionary\/json object, with the field status and message\r\n        # status indicating if the block was added by the peer, message why potentially not\r\n        responses.append(response.json())\r\n\r\n    for r in responses:\r\n        # the announced block was not added by this peer represented by r\r\n        if not r[\"status\"]:\r\n            return r\r\n    \r\n    print(responses)\r\n    return responses[0]\r\n\r\n# Uncomment this line if you want to specify the port number in the code\r\n#app.run(debug=True, port=8000)\r\n\r\nif __name__==\"__main__\":\r\n    port = sys.argv[1]\r\n    app.run(debug=True,port=port)\r\n    \r\n    "},"\/run_app.py":{"changes":[{"diff":"\n @app.route('\/tampered_block', methods=['POST'])\r\n def tampered_block():\r\n     address = \"{}\/attack\".format(CONNECTED_NODE_ADDRESS)\r\n-    message = requests.get(address)\r\n \r\n+    # has to be fetched from the specified check boxes\r\n+    attack_kind = {\"attack\":\"C\"}\r\n+\r\n+\r\n+    message = requests.post(address,\r\n+                            json=attack_kind,\r\n+                            headers={'Content-type': 'application\/json'})\r\n+   \r\n     global attack\r\n     attack = message\r\n ","add":8,"remove":1,"filename":"\/run_app.py","badparts":["    message = requests.get(address)\r"],"goodparts":["    attack_kind = {\"attack\":\"C\"}\r","\r","\r","    message = requests.post(address,\r","                            json=attack_kind,\r","                            headers={'Content-type': 'application\/json'})\r","   \r"]}],"source":"\nfrom flask import Flask,render_template, redirect, request\r import requests\r import datetime\r import json\r import sys\r import time\r from hashlib import sha256\r import random\r import os\r \r \r \r app=Flask(__name__)\r localhost=\"http:\/\/127.0.0.1:\"\r NODE_ADDRESS_list=[]\r CONNECTED_NODE_ADDRESS=\"\"\r posts=[]\r txs=[]\r answer=\"\"\r difficulty=\"2\"\r connected_node=localhost+'8000'\r new_node=\"\"\r register=\"\"\r attack=\"\"\r \r pool_of_unmined_txs=[]\r \r \r def fetch_posts():\r \"\"\"\r Function to fetch the chain from a blockchain node, parse the\r data and store it locally.\r \"\"\"\r get_chain_address=\"{}\/chain\".format(CONNECTED_NODE_ADDRESS)\r response=requests.get(get_chain_address)\r if response.status_code==200:\r content=[]\r chain=json.loads(response.content)\r for block in chain[\"chain\"]:\r content.append(block)\r \r global posts\r posts=sorted(content, key=lambda k: k['timestamp'],\r reverse=True)\r \r def fetch_pending_txs():\r \"\"\"\r Function to fetch the chain from a blockchain node, parse the\r data and store it locally.\r \"\"\"\r global txs\r txs=pool_of_unmined_txs\r \r @app.route('\/')\r def index():\r fetch_posts()\r fetch_pending_txs()\r return render_template('index.html',\r title='LMU University: Decentralized Certificates Storage',\r posts=posts,\r txs=txs,\r answer=answer,\r difficulty=difficulty,\r connected_node=connected_node,\r new_node=new_node,\r register=register,\r attack=attack,\r node_address=CONNECTED_NODE_ADDRESS,\r readable_time=timestamp_to_string)\r \r \r @app.route('\/submit', methods=['POST'])\r def submit_textarea():\r \"\"\"\r Endpoint to create a new transaction via our application.\r \"\"\"\r post_content=request.form[\"content\"]\r author=request.form[\"author\"]\r timestamp=time.time()\r \r tx_hash=sha256(post_content.encode()).hexdigest()\r \r transaction={\r 'author': author,\r 'content': post_content,\r 'timestamp': timestamp,\r 'hash': tx_hash\r }\r \r pool_of_unmined_txs.append(transaction)\r \r return redirect('\/')\r \r @app.route('\/search', methods=['POST'])\r def search_textarea():\r \"\"\"\r Endpoint to search for a transaction via our application.\r \"\"\"\r get_chain_address=\"{}\/chain\".format(CONNECTED_NODE_ADDRESS)\r response=requests.get(get_chain_address)\r global answer\r if response.status_code==200:\r content=request.form[\"content\"]\r content=sha256(content.encode()).hexdigest()\r chain=json.loads(response.content)\r for block in chain[\"chain\"]:\r for tx in block[\"transactions\"]:\r if content==tx['hash']:\r answer=\"Transaction Found\"\r return redirect('\/')\r else:\r answer=\"Transaction Not Found\"\r \r return redirect('\/')\r \r @app.route('\/mine_app')\r def start_mining():\r \"\"\"\r Endpoint to simulate mining in network.\r \"\"\"\r select_rnd_node=random.choice(NODE_ADDRESS_list)\r \r address=\"{}\/new_transaction\".format(select_rnd_node)\r global pool_of_unmined_txs\r for tx in pool_of_unmined_txs:\r requests.post(address,\r json=tx,\r headers={'Content-type': 'application\/json'})\r \r address=\"{}\/mine\".format(select_rnd_node)\r response=requests.get(address)\r \r if response.status_code==200:\r pool_of_unmined_txs=[]\r \r return response.text\r \r @app.route('\/modify_diff', methods=['POST'])\r def modify_textarea():\r \"\"\"\r Endpoint to change the difficulty via our application.\r \"\"\"\r diff=request.form[\"difficulty\"]\r \r for peer in NODE_ADDRESS_list:\r modify_address=\"{}\/modify_difficulty\".format(peer)\r \r post_content={\"difficulty\":diff}\r response=requests.post(modify_address,\r json=post_content,\r headers={'Content-type': 'application\/json'})\r \r global difficulty\r difficulty=response.text\r \r return redirect('\/')\r \r @app.route('\/switch_node', methods=['POST'])\r def switch_connected_node():\r node=request.form[\"node\"]\r node_address=localhost+node\r \r global CONNECTED_NODE_ADDRESS\r global connected_node\r for node in NODE_ADDRESS_list:\r if node_address==node:\r CONNECTED_NODE_ADDRESS=node_address\r connected_node=node_address\r return redirect('\/')\r else:\r pass\r \r connected_node=\"Invalid node, currently: \" +CONNECTED_NODE_ADDRESS\r return redirect('\/')\r \r @app.route('\/add_new_node', methods=['POST'])\r def add_node():\r node=request.form[\"new_node\"]\r node_address=localhost+node\r NODE_ADDRESS_list.append(node_address)\r \r global new_node\r new_node=node_address\r \r return redirect('\/')\r \r @app.route('\/reg_with', methods=['POST'])\r def reg_with():\r node_base=request.form[\"node1\"]\r node_base_address=localhost+node_base\r \r list_nodes=request.form[\"list_nodes\"]\r global register\r \r if not list_nodes:\r address=\"{}\/synchronize_with_peers\".format(node_base_address)\r response=requests.get(address)\r register=response.text\r return redirect('\/')\r \r \r \r list_nodes=list_nodes.replace(\" \",\"\")\r modified_list=list_nodes.split(\",\")\r \r new_nodes=[]\r for node in modified_list:\r node=localhost+node\r new_nodes.append(node)\r \r post_content={\"peers_list\":new_nodes}\r \r get_register_address=\"{}\/register_with\".format(node_base_address)\r \r response=requests.post(get_register_address,\r json=post_content,\r headers={'Content-type': 'application\/json'})\r \r \r register=response.text\r \r return redirect('\/')\r \r @app.route('\/tampered_block', methods=['POST'])\r def tampered_block():\r address=\"{}\/attack\".format(CONNECTED_NODE_ADDRESS)\r message=requests.get(address)\r \r global attack\r attack=message\r \r return redirect('\/')\r \r def timestamp_to_string(epoch_time):\r return datetime.datetime.fromtimestamp(epoch_time).strftime('%H:%M')\r \r if __name__==\"__main__\":\r host_node=sys.argv[1]\r \r blockchain_nodes=sys.argv[2:]\r for node in blockchain_nodes:\r NODE_ADDRESS_list.append(localhost+node)\r \r CONNECTED_NODE_ADDRESS=NODE_ADDRESS_list[0]\r \r app.run(debug=True,port=host_node)\r \r ","sourceWithComments":"from flask import Flask,render_template, redirect, request\r\nimport requests\r\nimport datetime\r\nimport json\r\nimport sys\r\nimport time\r\nfrom hashlib import sha256\r\nimport random\r\nimport os\r\n\r\n##########\r\n#FELIX\r\n######\r\n# Before package structre, with importing the app variable from the app folder, and calling app.run here, and having\r\n# the application code (view) in separate file. Probably there was a reason, but so far it works this way too and is maybe more intuitive\r\n###### \r\n\r\n\r\napp = Flask(__name__)\r\nlocalhost = \"http:\/\/127.0.0.1:\"\r\nNODE_ADDRESS_list = []\r\nCONNECTED_NODE_ADDRESS = \"\"\r\nposts = []\r\ntxs = []\r\nanswer = \"\"\r\ndifficulty = \"2\"\r\nconnected_node = localhost+'8000'\r\nnew_node = \"\"\r\nregister = \"\"\r\nattack = \"\"\r\n  \r\npool_of_unmined_txs = []\r\n\r\n\r\ndef fetch_posts():\r\n    \"\"\"\r\n    Function to fetch the chain from a blockchain node, parse the\r\n    data and store it locally.\r\n    \"\"\"\r\n    get_chain_address = \"{}\/chain\".format(CONNECTED_NODE_ADDRESS)\r\n    response = requests.get(get_chain_address)\r\n    if response.status_code == 200:\r\n        content = []\r\n        chain = json.loads(response.content)\r\n        for block in chain[\"chain\"]:\r\n            #for tx in block[\"transactions\"]:\r\n                #tx[\"index\"] = block[\"index\"]\r\n                #tx[\"hash\"] = block[\"previous_hash\"]\r\n            content.append(block)\r\n\r\n        global posts\r\n        posts = sorted(content, key=lambda k: k['timestamp'],\r\n                       reverse=True)\r\n\r\ndef fetch_pending_txs():\r\n    \"\"\"\r\n    Function to fetch the chain from a blockchain node, parse the\r\n    data and store it locally.\r\n    \"\"\"\r\n    global txs\r\n    txs = pool_of_unmined_txs\r\n\r\n@app.route('\/')\r\ndef index():\r\n    fetch_posts()\r\n    fetch_pending_txs()\r\n    return render_template('index.html',\r\n                           title='LMU University: Decentralized Certificates Storage',\r\n                           posts=posts,\r\n                           txs=txs,\r\n                           answer = answer,\r\n                           difficulty = difficulty,\r\n                           connected_node = connected_node,\r\n                           new_node = new_node,\r\n                           register = register,\r\n                           attack = attack,\r\n                           node_address=CONNECTED_NODE_ADDRESS,\r\n                           readable_time=timestamp_to_string)\r\n\r\n\r\n@app.route('\/submit', methods=['POST'])\r\ndef submit_textarea():\r\n    \"\"\"\r\n    Endpoint to create a new transaction via our application.\r\n    \"\"\"\r\n    post_content = request.form[\"content\"]\r\n    author = request.form[\"author\"]\r\n    # FELIX\r\n    timestamp = time.time()\r\n    \r\n    tx_hash = sha256(post_content.encode()).hexdigest()\r\n\r\n    transaction = {\r\n        'author': author,\r\n        'content': post_content,\r\n        'timestamp': timestamp,\r\n        'hash': tx_hash\r\n    }\r\n\r\n    pool_of_unmined_txs.append(transaction)\r\n\r\n    return redirect('\/')\r\n\r\n@app.route('\/search', methods=['POST'])\r\ndef search_textarea():\r\n    \"\"\"\r\n    Endpoint to search for a transaction via our application.\r\n    \"\"\"\r\n    get_chain_address = \"{}\/chain\".format(CONNECTED_NODE_ADDRESS)\r\n    response = requests.get(get_chain_address)\r\n    global answer\r\n    if response.status_code == 200:\r\n        content = request.form[\"content\"]\r\n        content = sha256(content.encode()).hexdigest()\r\n        chain = json.loads(response.content)\r\n        for block in chain[\"chain\"]:\r\n            for tx in block[\"transactions\"]:\r\n                if content == tx['hash']:\r\n                    answer = \"Transaction Found\"\r\n                    return redirect('\/')\r\n                else:\r\n                    answer = \"Transaction Not Found\"\r\n    \r\n    return redirect('\/')\r\n\r\n@app.route('\/mine_app')\r\ndef start_mining():\r\n    \"\"\"\r\n    Endpoint to simulate mining in network.\r\n    \"\"\"\r\n    select_rnd_node = random.choice(NODE_ADDRESS_list)\r\n\r\n    address = \"{}\/new_transaction\".format(select_rnd_node)\r\n    global pool_of_unmined_txs\r\n    for tx in pool_of_unmined_txs:\r\n        requests.post(address,\r\n                      json=tx,\r\n                      headers={'Content-type': 'application\/json'})\r\n    \r\n    address = \"{}\/mine\".format(select_rnd_node)\r\n    response = requests.get(address)\r\n\r\n    if response.status_code == 200:\r\n        pool_of_unmined_txs = []\r\n\r\n    return response.text\r\n\r\n@app.route('\/modify_diff', methods=['POST'])\r\ndef modify_textarea():\r\n    \"\"\"\r\n    Endpoint to change the difficulty via our application.\r\n    \"\"\"\r\n    diff = request.form[\"difficulty\"]\r\n\r\n    for peer in NODE_ADDRESS_list:\r\n        modify_address = \"{}\/modify_difficulty\".format(peer)\r\n\r\n        post_content = {\"difficulty\":diff}\r\n        response = requests.post(modify_address,\r\n                                 json=post_content,\r\n                                 headers={'Content-type': 'application\/json'})\r\n    \r\n    global difficulty\r\n    difficulty = response.text\r\n\r\n    return redirect('\/')\r\n\r\n@app.route('\/switch_node', methods=['POST'])\r\ndef switch_connected_node():\r\n    node = request.form[\"node\"]\r\n    node_address = localhost+node\r\n\r\n    global CONNECTED_NODE_ADDRESS\r\n    global connected_node\r\n    for node in NODE_ADDRESS_list:\r\n        if node_address == node:\r\n            CONNECTED_NODE_ADDRESS = node_address\r\n            connected_node = node_address\r\n            return redirect('\/')\r\n        else:\r\n            pass\r\n\r\n    connected_node = \"Invalid node, currently: \" + CONNECTED_NODE_ADDRESS\r\n    return redirect('\/')\r\n\r\n@app.route('\/add_new_node', methods=['POST'])\r\ndef add_node():\r\n    node = request.form[\"new_node\"]\r\n    node_address = localhost+node\r\n    NODE_ADDRESS_list.append(node_address)\r\n\r\n    global new_node\r\n    new_node = node_address\r\n\r\n    return redirect('\/')\r\n\r\n@app.route('\/reg_with', methods=['POST'])\r\ndef reg_with():\r\n    node_base = request.form[\"node1\"]\r\n    node_base_address = localhost+node_base\r\n\r\n    list_nodes = request.form[\"list_nodes\"]\r\n    global register\r\n\r\n    # if no nodes to register with were submitted then the command is used to\r\n    # synchronize the specified node_base with its peers, which is looking for the longest chain\r\n    if not list_nodes:\r\n        #print(\"HELLLLLO\")\r\n        address = \"{}\/synchronize_with_peers\".format(node_base_address)\r\n        response = requests.get(address)\r\n        register = response.text\r\n        return redirect('\/')\r\n\r\n\r\n\r\n    list_nodes = list_nodes.replace(\" \",\"\")\r\n    modified_list = list_nodes.split(\",\")\r\n\r\n    new_nodes = []\r\n    for node in modified_list:\r\n        node = localhost+node\r\n        new_nodes.append(node)\r\n        \r\n    post_content = {\"peers_list\":new_nodes}\r\n\r\n    get_register_address = \"{}\/register_with\".format(node_base_address)\r\n\r\n    response = requests.post(get_register_address,\r\n                      json=post_content,\r\n                      headers={'Content-type': 'application\/json'})\r\n\r\n    \r\n    register = response.text\r\n\r\n    return redirect('\/')\r\n\r\n@app.route('\/tampered_block', methods=['POST'])\r\ndef tampered_block():\r\n    address = \"{}\/attack\".format(CONNECTED_NODE_ADDRESS)\r\n    message = requests.get(address)\r\n\r\n    global attack\r\n    attack = message\r\n\r\n    return redirect('\/')\r\n\r\ndef timestamp_to_string(epoch_time):\r\n    return datetime.datetime.fromtimestamp(epoch_time).strftime('%H:%M')\r\n\r\n##########\r\n# FELIX\r\n##########\r\nif __name__==\"__main__\":\r\n    host_node = sys.argv[1]\r\n    \r\n    blockchain_nodes = sys.argv[2:]\r\n    for node in blockchain_nodes:\r\n        NODE_ADDRESS_list.append(localhost+node)\r\n\r\n    CONNECTED_NODE_ADDRESS = NODE_ADDRESS_list[0]\r\n\r\n    app.run(debug=True,port=host_node)\r\n\r\n"}},"msg":"tampered block attacks"}},"https:\/\/github.com\/kioniv\/TW_Selenium":{"3dd7201c0548c33c15d56b1d29ae2ba90b7bc31c":{"url":"https:\/\/api.github.com\/repos\/kioniv\/TW_Selenium\/commits\/3dd7201c0548c33c15d56b1d29ae2ba90b7bc31c","html_url":"https:\/\/github.com\/kioniv\/TW_Selenium\/commit\/3dd7201c0548c33c15d56b1d29ae2ba90b7bc31c","message":"Making send attack use the tamper monkey script if the attack doesn't need to be sent right away.","sha":"3dd7201c0548c33c15d56b1d29ae2ba90b7bc31c","keyword":"tampering attack","diff":"diff --git a\/Selenium_Test.py b\/Selenium_Test.py\nindex 2f718f4..44eab75 100644\n--- a\/Selenium_Test.py\n+++ b\/Selenium_Test.py\n@@ -7,28 +7,28 @@\n import random\n import copy\n import time\n-try:\n-    import requests\n-except ImportError:\n-    import pip\n-    pip.main(['install', 'requests'])\n-    import requests\n+import requests\n \n driver = webdriver.Chrome()\n driver.implicitly_wait(5)\n world = 101\n units = {'spear':0, 'sword':0, 'axe':0, 'archer':0, 'spy':0, 'light':0, 'marcher':0, 'heavy':0, 'ram':0, 'catapult':0, 'knight':0, 'snob':0}\n setArrivalTimeScript = requests.get(\"https:\/\/raw.githubusercontent.com\/kioniv\/TW_Tamper_Monkey\/master\/Set_Arrival_Time\").text\n-username = raw_input(\"Enter your username: \")\n+username = input(\"Enter your username: \")\n passwd = getpass.getpass(\"Enter your password: \")\n \n+def SafeSendKeys(target, keys):\n+    for char in keys:\n+        time.sleep(random.uniform(.05, .34))\n+        target.send_keys(char)\n+\n def Login(username, passwd, world):\n     driver.get(\"https:\/\/tribalwars.net\")\n     loginBtn = driver.find_element_by_class_name(\"btn-login\")\n     userNameField = driver.find_element_by_name(\"username\")\n     passwordField = driver.find_element_by_name(\"password\")\n-    userNameField.send_keys(username)\n-    passwordField.send_keys(passwd)\n+    SafeSendKeys(userNameField, username)\n+    SafeSendKeys(passwordField, passwd)\n     loginBtn.click()\n     WebDriverWait(driver, 10).until(expected_conditions.presence_of_element_located((By.CLASS_NAME, \"world_button_active\")))\n     worldBtn = driver.find_elements_by_class_name(\"world_button_active\")\n@@ -36,14 +36,16 @@ def Login(username, passwd, world):\n         for worldEntry in worldBtn:\n             if str(world) in worldEntry.text:\n                 worldEntry.click()\n+        try:\n+            driver.find_element_by_class_name(\"btn-default\").click()\n+        except:\n+            pass\n     except:\n         assert driver.find_element_by_id(\"serverTime\")\n \n def SendAttack(sendingVillage, targetVillage, arrivalTime, sendUnits):\n     # navigate to correct village\n-    if str(sendingVillage).lower() == \"current\":\n-        driver.find_element_by_class_name(\"village\").click()\n-    else:\n+    if str(sendingVillage).lower() != \"current\":\n         NavCombinedOverview()\n         RandWait()\n         villageList = driver.find_elements_by_class_name(\"quickedit-label\")\n@@ -70,7 +72,36 @@ def SendAttack(sendingVillage, targetVillage, arrivalTime, sendUnits):\n     RandWait()\n     #\n     if arrivalTime != \"\":\n-        driver.execute_async_script(str(setArrivalTimeScript))\n+        driver.execute_script(str(setArrivalTimeScript))\n+        time.sleep(.5)\n+        milliseconds = arrivalTime[-3:]\n+        arrivalTime =arrivalTime[:-4]\n+        driver.find_element_by_id(\"delayInput\").clear()\n+        driver.find_element_by_id(\"delayInput\").send_keys(\"0\")\n+        driver.find_element_by_id(\"delayButton\").click()\n+        driver.find_element_by_id(\"reloadInput\").clear()\n+        driver.find_element_by_id(\"reloadInput\").send_keys(\"0\")\n+        driver.find_element_by_id(\"reloadButton\").click()\n+        driver.find_element_by_id(\"arrTime\").click()\n+\n+        WebDriverWait(driver, 3).until(expected_conditions.alert_is_present(),\n+                                        'Timed out waiting for PA creation ' +\n+                                        'confirmation popup to appear.')\n+        alert = driver.switch_to.alert\n+        alert.send_keys(arrivalTime)\n+        alert.accept()\n+\n+\n+        WebDriverWait(driver, 3).until(expected_conditions.alert_is_present(),\n+                                       'Timed out waiting for PA creation ' +\n+                                       'confirmation popup to appear.')\n+        alert = driver.switch_to.alert\n+        alert.send_keys(milliseconds)\n+        alert.accept()\n+\n+        WebDriverWait(driver, 999999999999).until(expected_conditions.url_changes)\n+        print (\"Sent \" + str(sendUnits) + \" from \" + str(sendingVillage) + \" to \" + str(targetVillage)\n+               + \"\\nArrival set for \" + str(arrivalTime) + \":\" + str(milliseconds))\n \n     else:\n         driver.find_element_by_id(\"troop_confirm_go\").click()\n@@ -87,21 +118,16 @@ def EnterUnits(sendUnitsDict):\n             except:\n                 pass\n \n-def SafeSendKeys(target, keys):\n-    for char in keys:\n-        RandWait()\n-        target.send_keys(char)\n-\n \n def NavCombinedOverview():\n     driver.get(\"https:\/\/en\" + str(world) + \".tribalwars.net\/game.php?&screen=overview_villages\")\n \n def RandWait():\n-    time.sleep(random.uniform(.2, .8))\n+    time.sleep(random.uniform(.2, .7))\n \n Login(username, passwd, world)\n \n-myattack = copy.deepcopy(units)\n-myattack['spear'] = 1\n+myAttack = copy.deepcopy(units)\n+myAttack['spear'] = 1\n \n-SendAttack(\"current\", \"560|454\", \"\", myattack)\n+SendAttack(\"current\", \"560|454\", \"02:43:00:000\", myAttack)\n","files":{"\/Selenium_Test.py":{"changes":[{"diff":"\n import random\n import copy\n import time\n-try:\n-    import requests\n-except ImportError:\n-    import pip\n-    pip.main(['install', 'requests'])\n-    import requests\n+import requests\n \n driver = webdriver.Chrome()\n driver.implicitly_wait(5)\n world = 101\n units = {'spear':0, 'sword':0, 'axe':0, 'archer':0, 'spy':0, 'light':0, 'marcher':0, 'heavy':0, 'ram':0, 'catapult':0, 'knight':0, 'snob':0}\n setArrivalTimeScript = requests.get(\"https:\/\/raw.githubusercontent.com\/kioniv\/TW_Tamper_Monkey\/master\/Set_Arrival_Time\").text\n-username = raw_input(\"Enter your username: \")\n+username = input(\"Enter your username: \")\n passwd = getpass.getpass(\"Enter your password: \")\n \n+def SafeSendKeys(target, keys):\n+    for char in keys:\n+        time.sleep(random.uniform(.05, .34))\n+        target.send_keys(char)\n+\n def Login(username, passwd, world):\n     driver.get(\"https:\/\/tribalwars.net\")\n     loginBtn = driver.find_element_by_class_name(\"btn-login\")\n     userNameField = driver.find_element_by_name(\"username\")\n     passwordField = driver.find_element_by_name(\"password\")\n-    userNameField.send_keys(username)\n-    passwordField.send_keys(passwd)\n+    SafeSendKeys(userNameField, username)\n+    SafeSendKeys(passwordField, passwd)\n     loginBtn.click()\n     WebDriverWait(driver, 10).until(expected_conditions.presence_of_element_located((By.CLASS_NAME, \"world_button_active\")))\n     worldBtn = driver.find_elements_by_class_name(\"world_button_active\")\n","add":9,"remove":9,"filename":"\/Selenium_Test.py","badparts":["try:","    import requests","except ImportError:","    import pip","    pip.main(['install', 'requests'])","    import requests","username = raw_input(\"Enter your username: \")","    userNameField.send_keys(username)","    passwordField.send_keys(passwd)"],"goodparts":["import requests","username = input(\"Enter your username: \")","def SafeSendKeys(target, keys):","    for char in keys:","        time.sleep(random.uniform(.05, .34))","        target.send_keys(char)","    SafeSendKeys(userNameField, username)","    SafeSendKeys(passwordField, passwd)"]},{"diff":"\n         for worldEntry in worldBtn:\n             if str(world) in worldEntry.text:\n                 worldEntry.click()\n+        try:\n+            driver.find_element_by_class_name(\"btn-default\").click()\n+        except:\n+            pass\n     except:\n         assert driver.find_element_by_id(\"serverTime\")\n \n def SendAttack(sendingVillage, targetVillage, arrivalTime, sendUnits):\n     # navigate to correct village\n-    if str(sendingVillage).lower() == \"current\":\n-        driver.find_element_by_class_name(\"village\").click()\n-    else:\n+    if str(sendingVillage).lower() != \"current\":\n         NavCombinedOverview()\n         RandWait()\n         villageList = driver.find_elements_by_class_name(\"quickedit-label\")\n","add":5,"remove":3,"filename":"\/Selenium_Test.py","badparts":["    if str(sendingVillage).lower() == \"current\":","        driver.find_element_by_class_name(\"village\").click()","    else:"],"goodparts":["        try:","            driver.find_element_by_class_name(\"btn-default\").click()","        except:","            pass","    if str(sendingVillage).lower() != \"current\":"]},{"diff":"\n     RandWait()\n     #\n     if arrivalTime != \"\":\n-        driver.execute_async_script(str(setArrivalTimeScript))\n+        driver.execute_script(str(setArrivalTimeScript))\n+        time.sleep(.5)\n+        milliseconds = arrivalTime[-3:]\n+        arrivalTime =arrivalTime[:-4]\n+        driver.find_element_by_id(\"delayInput\").clear()\n+        driver.find_element_by_id(\"delayInput\").send_keys(\"0\")\n+        driver.find_element_by_id(\"delayButton\").click()\n+        driver.find_element_by_id(\"reloadInput\").clear()\n+        driver.find_element_by_id(\"reloadInput\").send_keys(\"0\")\n+        driver.find_element_by_id(\"reloadButton\").click()\n+        driver.find_element_by_id(\"arrTime\").click()\n+\n+        WebDriverWait(driver, 3).until(expected_conditions.alert_is_present(),\n+                                        'Timed out waiting for PA creation ' +\n+                                        'confirmation popup to appear.')\n+        alert = driver.switch_to.alert\n+        alert.send_keys(arrivalTime)\n+        alert.accept()\n+\n+\n+        WebDriverWait(driver, 3).until(expected_conditions.alert_is_present(),\n+                                       'Timed out waiting for PA creation ' +\n+                                       'confirmation popup to appear.')\n+        alert = driver.switch_to.alert\n+        alert.send_keys(milliseconds)\n+        alert.accept()\n+\n+        WebDriverWait(driver, 999999999999).until(expected_conditions.url_changes)\n+        print (\"Sent \" + str(sendUnits) + \" from \" + str(sendingVillage) + \" to \" + str(targetVillage)\n+               + \"\\nArrival set for \" + str(arrivalTime) + \":\" + str(milliseconds))\n \n     else:\n         driver.find_element_by_id(\"troop_confirm_go\").click()\n","add":30,"remove":1,"filename":"\/Selenium_Test.py","badparts":["        driver.execute_async_script(str(setArrivalTimeScript))"],"goodparts":["        driver.execute_script(str(setArrivalTimeScript))","        time.sleep(.5)","        milliseconds = arrivalTime[-3:]","        arrivalTime =arrivalTime[:-4]","        driver.find_element_by_id(\"delayInput\").clear()","        driver.find_element_by_id(\"delayInput\").send_keys(\"0\")","        driver.find_element_by_id(\"delayButton\").click()","        driver.find_element_by_id(\"reloadInput\").clear()","        driver.find_element_by_id(\"reloadInput\").send_keys(\"0\")","        driver.find_element_by_id(\"reloadButton\").click()","        driver.find_element_by_id(\"arrTime\").click()","        WebDriverWait(driver, 3).until(expected_conditions.alert_is_present(),","                                        'Timed out waiting for PA creation ' +","                                        'confirmation popup to appear.')","        alert = driver.switch_to.alert","        alert.send_keys(arrivalTime)","        alert.accept()","        WebDriverWait(driver, 3).until(expected_conditions.alert_is_present(),","                                       'Timed out waiting for PA creation ' +","                                       'confirmation popup to appear.')","        alert = driver.switch_to.alert","        alert.send_keys(milliseconds)","        alert.accept()","        WebDriverWait(driver, 999999999999).until(expected_conditions.url_changes)","        print (\"Sent \" + str(sendUnits) + \" from \" + str(sendingVillage) + \" to \" + str(targetVillage)","               + \"\\nArrival set for \" + str(arrivalTime) + \":\" + str(milliseconds))"]},{"diff":"\n             except:\n                 pass\n \n-def SafeSendKeys(target, keys):\n-    for char in keys:\n-        RandWait()\n-        target.send_keys(char)\n-\n \n def NavCombinedOverview():\n     driver.get(\"https:\/\/en\" + str(world) + \".tribalwars.net\/game.php?&screen=overview_villages\")\n \n def RandWait():\n-    time.sleep(random.uniform(.2, .8))\n+    time.sleep(random.uniform(.2, .7))\n \n Login(username, passwd, world)\n \n-myattack = copy.deepcopy(units)\n-myattack['spear'] = 1\n+myAttack = copy.deepcopy(units)\n+myAttack['spear'] = 1\n \n-SendAttack(\"current\", \"560|454\", \"\", myattack)\n+SendAttack(\"current\", \"560|454\", \"02:43:00:000\", myAttack)\n","add":4,"remove":9,"filename":"\/Selenium_Test.py","badparts":["def SafeSendKeys(target, keys):","    for char in keys:","        RandWait()","        target.send_keys(char)","    time.sleep(random.uniform(.2, .8))","myattack = copy.deepcopy(units)","myattack['spear'] = 1","SendAttack(\"current\", \"560|454\", \"\", myattack)"],"goodparts":["    time.sleep(random.uniform(.2, .7))","myAttack = copy.deepcopy(units)","myAttack['spear'] = 1","SendAttack(\"current\", \"560|454\", \"02:43:00:000\", myAttack)"]}],"source":"\nfrom selenium import webdriver from selenium.webdriver.common.keys import Keys from selenium.webdriver.common.by import By from selenium.webdriver.support.ui import WebDriverWait from selenium.webdriver.support import expected_conditions import getpass import random import copy import time try: import requests except ImportError: import pip pip.main(['install', 'requests']) import requests driver=webdriver.Chrome() driver.implicitly_wait(5) world=101 units={'spear':0, 'sword':0, 'axe':0, 'archer':0, 'spy':0, 'light':0, 'marcher':0, 'heavy':0, 'ram':0, 'catapult':0, 'knight':0, 'snob':0} setArrivalTimeScript=requests.get(\"https:\/\/raw.githubusercontent.com\/kioniv\/TW_Tamper_Monkey\/master\/Set_Arrival_Time\").text username=raw_input(\"Enter your username: \") passwd=getpass.getpass(\"Enter your password: \") def Login(username, passwd, world): driver.get(\"https:\/\/tribalwars.net\") loginBtn=driver.find_element_by_class_name(\"btn-login\") userNameField=driver.find_element_by_name(\"username\") passwordField=driver.find_element_by_name(\"password\") userNameField.send_keys(username) passwordField.send_keys(passwd) loginBtn.click() WebDriverWait(driver, 10).until(expected_conditions.presence_of_element_located((By.CLASS_NAME, \"world_button_active\"))) worldBtn=driver.find_elements_by_class_name(\"world_button_active\") try: for worldEntry in worldBtn: if str(world) in worldEntry.text: worldEntry.click() except: assert driver.find_element_by_id(\"serverTime\") def SendAttack(sendingVillage, targetVillage, arrivalTime, sendUnits): if str(sendingVillage).lower()==\"current\": driver.find_element_by_class_name(\"village\").click() else: NavCombinedOverview() RandWait() villageList=driver.find_elements_by_class_name(\"quickedit-label\") for village in villageList: if str(targetVillage)==village.text: village.click() RandWait() driver.get(\"https:\/\/en\" +str(world) +\".tribalwars.net\/game.php?&screen=place\") RandWait() targetField=driver.find_element_by_class_name(\"target-input-field\") SafeSendKeys(targetField, targetVillage) RandWait() EnterUnits(sendUnits) RandWait() driver.find_element_by_id(\"target_attack\").click() RandWait() if arrivalTime !=\"\": driver.execute_async_script(str(setArrivalTimeScript)) else: driver.find_element_by_id(\"troop_confirm_go\").click() def EnterUnits(sendUnitsDict): for key in sendUnitsDict.keys(): if sendUnitsDict[key] > 0: RandWait() try: unitField=driver.find_element_by_name(str(key).strip()) SafeSendKeys(unitField, str(sendUnitsDict[key])) except: pass def SafeSendKeys(target, keys): for char in keys: RandWait() target.send_keys(char) def NavCombinedOverview(): driver.get(\"https:\/\/en\" +str(world) +\".tribalwars.net\/game.php?&screen=overview_villages\") def RandWait(): time.sleep(random.uniform(.2,.8)) Login(username, passwd, world) myattack=copy.deepcopy(units) myattack['spear']=1 SendAttack(\"current\", \"560|454\", \"\", myattack) ","sourceWithComments":"from selenium import webdriver\nfrom selenium.webdriver.common.keys import Keys\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions\nimport getpass\nimport random\nimport copy\nimport time\ntry:\n    import requests\nexcept ImportError:\n    import pip\n    pip.main(['install', 'requests'])\n    import requests\n\ndriver = webdriver.Chrome()\ndriver.implicitly_wait(5)\nworld = 101\nunits = {'spear':0, 'sword':0, 'axe':0, 'archer':0, 'spy':0, 'light':0, 'marcher':0, 'heavy':0, 'ram':0, 'catapult':0, 'knight':0, 'snob':0}\nsetArrivalTimeScript = requests.get(\"https:\/\/raw.githubusercontent.com\/kioniv\/TW_Tamper_Monkey\/master\/Set_Arrival_Time\").text\nusername = raw_input(\"Enter your username: \")\npasswd = getpass.getpass(\"Enter your password: \")\n\ndef Login(username, passwd, world):\n    driver.get(\"https:\/\/tribalwars.net\")\n    loginBtn = driver.find_element_by_class_name(\"btn-login\")\n    userNameField = driver.find_element_by_name(\"username\")\n    passwordField = driver.find_element_by_name(\"password\")\n    userNameField.send_keys(username)\n    passwordField.send_keys(passwd)\n    loginBtn.click()\n    WebDriverWait(driver, 10).until(expected_conditions.presence_of_element_located((By.CLASS_NAME, \"world_button_active\")))\n    worldBtn = driver.find_elements_by_class_name(\"world_button_active\")\n    try:\n        for worldEntry in worldBtn:\n            if str(world) in worldEntry.text:\n                worldEntry.click()\n    except:\n        assert driver.find_element_by_id(\"serverTime\")\n\ndef SendAttack(sendingVillage, targetVillage, arrivalTime, sendUnits):\n    # navigate to correct village\n    if str(sendingVillage).lower() == \"current\":\n        driver.find_element_by_class_name(\"village\").click()\n    else:\n        NavCombinedOverview()\n        RandWait()\n        villageList = driver.find_elements_by_class_name(\"quickedit-label\")\n        for village in villageList:\n            if str(targetVillage) == village.text:\n                village.click()\n\n    RandWait()\n    driver.get(\"https:\/\/en\" + str(world) + \".tribalwars.net\/game.php?&screen=place\")\n\n    RandWait()\n    # enter coordinates\n    targetField = driver.find_element_by_class_name(\"target-input-field\")\n    SafeSendKeys(targetField, targetVillage)\n\n    RandWait()\n    # enter units\n    EnterUnits(sendUnits)\n\n    RandWait()\n    # press attack\n    driver.find_element_by_id(\"target_attack\").click()\n\n    RandWait()\n    #\n    if arrivalTime != \"\":\n        driver.execute_async_script(str(setArrivalTimeScript))\n\n    else:\n        driver.find_element_by_id(\"troop_confirm_go\").click()\n\n\n\ndef EnterUnits(sendUnitsDict):\n    for key in sendUnitsDict.keys():\n        if sendUnitsDict[key] > 0:\n            RandWait()\n            try:\n                unitField = driver.find_element_by_name(str(key).strip())\n                SafeSendKeys(unitField, str(sendUnitsDict[key]))\n            except:\n                pass\n\ndef SafeSendKeys(target, keys):\n    for char in keys:\n        RandWait()\n        target.send_keys(char)\n\n\ndef NavCombinedOverview():\n    driver.get(\"https:\/\/en\" + str(world) + \".tribalwars.net\/game.php?&screen=overview_villages\")\n\ndef RandWait():\n    time.sleep(random.uniform(.2, .8))\n\nLogin(username, passwd, world)\n\nmyattack = copy.deepcopy(units)\nmyattack['spear'] = 1\n\nSendAttack(\"current\", \"560|454\", \"\", myattack)\n"}},"msg":"Making send attack use the tamper monkey script if the attack doesn't need to be sent right away."}},"https:\/\/github.com\/s-ball\/remo_serv":{"511b2af1b9f110a8b1509201a0564c623828cd6e":{"url":"https:\/\/api.github.com\/repos\/s-ball\/remo_serv\/commits\/511b2af1b9f110a8b1509201a0564c623828cd6e","html_url":"https:\/\/github.com\/s-ball\/remo_serv\/commit\/511b2af1b9f110a8b1509201a0564c623828cd6e","message":"Pass the response code in first token to prevent tampering.\n\nEnsure that the response code has not be altered by a MITM attack.\nReject plain text responses for 200 Ok type responses.","sha":"511b2af1b9f110a8b1509201a0564c623828cd6e","keyword":"tampering attack","diff":"diff --git a\/client\/clientlib.py b\/client\/clientlib.py\nindex 707757a..c8f073b 100644\n--- a\/client\/clientlib.py\n+++ b\/client\/clientlib.py\n@@ -22,7 +22,9 @@\n \n class Response(io.BufferedReader):\n     def __init__(self, response, codec: fernet.Fernet, req_no: int):\n-        self.decoder = Codec(response, codec, req_no, time.time(), b'', allow_plain=True)\n+        self.decoder = Codec(response, codec, req_no, time.time(),\n+                             '{:03d}'.format(response.code).encode(),\n+                             allow_plain=True)\n         super().__init__(self.decoder)\n         for k in vars(response).keys():\n             setattr(self, k, getattr(response, k))\n@@ -77,7 +79,10 @@ def http_request(self, req: urllib.request.Request\n \n     def http_response(self, _req, response):\n         if 'Content-Length' not in response.headers:\n-            return Response(response, self.codec, self.req_no)\n+            r = Response(response, self.codec, self.req_no)\n+            if r.code == 200 and not r.is_deco_ok():\n+                raise fernet.InvalidToken()\n+            return r\n         else:\n             return response\n \ndiff --git a\/remo_serv\/crypter.py b\/remo_serv\/crypter.py\nindex 2a9da03..18d8d94 100644\n--- a\/remo_serv\/crypter.py\n+++ b\/remo_serv\/crypter.py\n@@ -195,7 +195,7 @@ def __call__(self, environ, start_response):\n             if codec is not None:\n                 session['REQ_NO'] = codec.req_no\n             session['LAST_REQ'] = time.time()\n-            codec = Codec(io.BytesIO(), deco, session['REQ_NO'], decode=False)\n+            codec = Codec(io.BytesIO(), deco, session['REQ_NO'], cmd_hash=b'200', decode=False)\n             return (codec.transform(data) + b'\\r\\n' for data in out)\n \n \ndiff --git a\/test_client\/test_clientlib.py b\/test_client\/test_clientlib.py\nindex 2caa45a..74e0b1b 100644\n--- a\/test_client\/test_clientlib.py\n+++ b\/test_client\/test_clientlib.py\n@@ -15,6 +15,12 @@\n from remo_tools import http_tools\n \n \n+class ResponseStub(io.BytesIO):\n+    def __init__(self, data: bytes, code):\n+        super().__init__(data)\n+        self.code = code\n+\n+\n class TestResponse(unittest.TestCase):\n     def setUp(self) -> None:\n         key = fernet.Fernet.generate_key()\n@@ -22,21 +28,21 @@ def setUp(self) -> None:\n \n     def test_simple(self):\n         req_no = 5\n-        data = b'BE' + struct.pack('>hh', req_no, 0) + b'foo'\n+        data = b'BE' + struct.pack('>hh', req_no, 0) + b'200foo'\n         data = self.fernet.encrypt(data)\n-        r = clientlib.Response(io.BytesIO(data), self.fernet, req_no)\n+        r = clientlib.Response(ResponseStub(data, 200), self.fernet, req_no)\n         self.assertEqual(b'foo', r.read())\n         self.assertTrue(r.is_deco_ok())\n \n     def test_no_encrypt(self):\n-        r = clientlib.Response(io.BytesIO(b'foo'), self.fernet, 1)\n+        r = clientlib.Response(ResponseStub(b'foo', 200), self.fernet, 1)\n         self.assertEqual(b'foo', r.read())\n \n     def test_wrong_req(self):\n         req_no = 5\n         data = b'BE' + struct.pack('>hh', req_no + 1, 0) + b'foo'\n         data = self.fernet.encrypt(data)\n-        r = clientlib.Response(io.BytesIO(data), self.fernet, req_no)\n+        r = clientlib.Response(ResponseStub(data, 200), self.fernet, req_no)\n         self.assertTrue(r.is_deco_ok())\n         r.read()\n         self.assertFalse(r.is_deco_ok())\ndiff --git a\/tests\/test_crypter.py b\/tests\/test_crypter.py\nindex 0af445e..75c42c0 100644\n--- a\/tests\/test_crypter.py\n+++ b\/tests\/test_crypter.py\n@@ -94,7 +94,10 @@ def test_authenticated(self):\n         resp.append(b'')\n         for i, data in enumerate(out):\n             data = coder.decrypt(data)\n-            self.assertEqual(resp[i], data[6:])\n+            if i == 0:\n+                self.assertEqual(b'200' + resp[i], data[6:])\n+            else:\n+                self.assertEqual(resp[i], data[6:])\n             req, tok = struct.unpack('>hh', data[2:6])\n             self.assertEqual(5, req)\n             self.assertEqual(i, tok)\n@@ -129,7 +132,10 @@ def data_input(_x, _y):\n         resp.append(b'')\n         for i, data in enumerate(out):\n             data = coder.decrypt(data)\n-            self.assertEqual(resp[i], data[6:])\n+            if i == 0:\n+                self.assertEqual(b'200' + resp[i], data[6:])\n+            else:\n+                self.assertEqual(resp[i], data[6:])\n             req, tok = struct.unpack('>hh', data[2:6])\n             self.assertEqual(5, req)\n             self.assertEqual(i, tok)\n","files":{"\/client\/clientlib.py":{"changes":[{"diff":"\n \n class Response(io.BufferedReader):\n     def __init__(self, response, codec: fernet.Fernet, req_no: int):\n-        self.decoder = Codec(response, codec, req_no, time.time(), b'', allow_plain=True)\n+        self.decoder = Codec(response, codec, req_no, time.time(),\n+                             '{:03d}'.format(response.code).encode(),\n+                             allow_plain=True)\n         super().__init__(self.decoder)\n         for k in vars(response).keys():\n             setattr(self, k, getattr(response, k))\n","add":3,"remove":1,"filename":"\/client\/clientlib.py","badparts":["        self.decoder = Codec(response, codec, req_no, time.time(), b'', allow_plain=True)"],"goodparts":["        self.decoder = Codec(response, codec, req_no, time.time(),","                             '{:03d}'.format(response.code).encode(),","                             allow_plain=True)"]},{"diff":"\n \n     def http_response(self, _req, response):\n         if 'Content-Length' not in response.headers:\n-            return Response(response, self.codec, self.req_no)\n+            r = Response(response, self.codec, self.req_no)\n+            if r.code == 200 and not r.is_deco_ok():\n+                raise fernet.InvalidToken()\n+            return r\n         else:\n             return response\n ","add":4,"remove":1,"filename":"\/client\/clientlib.py","badparts":["            return Response(response, self.codec, self.req_no)"],"goodparts":["            r = Response(response, self.codec, self.req_no)","            if r.code == 200 and not r.is_deco_ok():","                raise fernet.InvalidToken()","            return r"]}],"source":"\n\nimport base64 import collections import io import json import urllib.request import urllib.parse import http.client import time import struct from typing import Optional from cryptography import fernet from cryptography.hazmat.primitives import hashes, serialization from cryptography.hazmat.primitives.asymmetric import ed448, x448 from cryptography.hazmat.primitives.kdf import concatkdf from. import smartcard from remo_tools.http_tools import Codec, do_hash class Response(io.BufferedReader): def __init__(self, response, codec: fernet.Fernet, req_no: int): self.decoder=Codec(response, codec, req_no, time.time(), b'', allow_plain=True) super().__init__(self.decoder) for k in vars(response).keys(): setattr(self, k, getattr(response, k)) self.response=response def is_deco_ok(self): return self.decoder.deco_ok def __getitem__(self, item): return self.response[item] def __getattr__(self, item): return getattr(self.response, item) class RemoServHandler(urllib.request.BaseHandler): def __init__(self, codec: fernet.Fernet, base_path: str): self.codec=codec self.req_no=0 self.last_req=0 self.base_path=base_path.rstrip('\/') def http_request(self, req: urllib.request.Request ) -> urllib.request.Request: self.req_no +=1 data=req.data path=req.selector req.full_url=self.base_path +path hash_val=do_hash(path.encode()) if data is None: req.data=self.codec.encrypt(b'BE' +struct.pack( '>hh', self.req_no, 0) +hash_val) elif hasattr(data, 'read'): req.data=Codec(data, self.codec, self.req_no, 0, hash_val, False) else: try: memoryview(data) req.data=self.codec.encrypt(b'BE' +struct.pack( '>hh', self.req_no, 0) +hash_val +data) except TypeError: if isinstance(data, collections.abc.Iterable): codec=Codec(io.BytesIO(), self.codec, self.req_no, 0, hash_val, False) req.data=b''.join(codec.transform(d) for d in data)\\ +codec.transform(b'') else: raise TypeError('RemoServHandler can only process ' 'bytes or iterables, got %r', type(data)) return req def http_response(self, _req, response): if 'Content-Length' not in response.headers: return Response(response, self.codec, self.req_no) else: return response class Connection: def __init__(self, user: str, opener: urllib.request.OpenerDirector, codec: fernet.Fernet): self.user=user self.opener=opener self.codec=codec self.app_url='http:\/\/x' def get(self, remote_file: str, local_file: str=None): cmd=b'\/get\/' +self.codec.encrypt(remote_file.encode()) if local_file is None: local_file=remote_file inp=self.opener.open(self.app_url +cmd.decode()) with open(local_file, 'wb') as out: while True: data=inp.read(8192) if len(data)==0: break out.write(data) def put(self, remote_file: str, local_file: str=None): cmd=b'\/put\/' +self.codec.encrypt(remote_file.encode()) if local_file is None: local_file=remote_file with open(local_file, 'rb') as fd: self.opener.open(self.app_url +cmd.decode(), fd) def exec(self, command: str) -> http.client.HTTPResponse: cmd=b'\/cmd\/' +self.codec.encrypt(command.encode()) r=self.opener.open(self.app_url +cmd.decode()) return r def iexec(self, command: str) -> http.client.HTTPResponse: cmd=b'\/icm\/' +self.codec.encrypt(command.encode()) r=self.opener.open(self.app_url +cmd.decode()) return r def idata(self, data: str) -> http.client.HTTPResponse: cmd=b'\/idt' r=self.opener.open(self.app_url +cmd.decode(), data.encode() +b'\\n') return r def end_cmd(self) -> http.client.HTTPResponse: cmd=b'\/edt' r=self.opener.open(self.app_url +cmd.decode()) return r def login(url: str, path: str, user: str, key: Optional[ed448.Ed448PrivateKey], signer: Optional[smartcard.Signer], remo_pub: ed448.Ed448PublicKey): cookie_processor=urllib.request.HTTPCookieProcessor() opener=urllib.request.build_opener(cookie_processor) tmp_key=x448.X448PrivateKey.generate() pub=tmp_key.public_key().public_bytes(serialization.Encoding.Raw, serialization.PublicFormat.Raw) data=user.encode() +pub sign=key.sign(data) if signer is None else signer.sign(data) data=json.dumps({'user': user, 'key': base64.urlsafe_b64encode(pub).decode(), 'sign': base64.urlsafe_b64encode(sign).decode() }) r=opener.open(url +path, data.encode()) data=r.read() remo=base64.urlsafe_b64decode(data[:76]) remo_pub.verify(base64.urlsafe_b64decode(data[78:]), remo) remo=x448.X448PublicKey.from_public_bytes(remo) session_key=tmp_key.exchange(remo) df=concatkdf.ConcatKDFHash(hashes.SHA256(), 32, b'remo_serv') session_key=df.derive(session_key) session_key=base64.urlsafe_b64encode(session_key) session_codec=fernet.Fernet(session_key) return Connection(user, urllib.request.build_opener( RemoServHandler(session_codec, url), cookie_processor), session_codec) ","sourceWithComments":"#  Copyright (c) 2020 SBA- MIT License\nimport base64\nimport collections\nimport io\nimport json\nimport urllib.request\nimport urllib.parse\nimport http.client\nimport time\nimport struct\n\nfrom typing import Optional\n\nfrom cryptography import fernet\nfrom cryptography.hazmat.primitives import hashes, serialization\nfrom cryptography.hazmat.primitives.asymmetric import ed448, x448\nfrom cryptography.hazmat.primitives.kdf import concatkdf\n\nfrom . import smartcard\nfrom remo_tools.http_tools import Codec, do_hash\n\n\nclass Response(io.BufferedReader):\n    def __init__(self, response, codec: fernet.Fernet, req_no: int):\n        self.decoder = Codec(response, codec, req_no, time.time(), b'', allow_plain=True)\n        super().__init__(self.decoder)\n        for k in vars(response).keys():\n            setattr(self, k, getattr(response, k))\n        self.response = response\n\n    def is_deco_ok(self):\n        return self.decoder.deco_ok\n\n    def __getitem__(self, item):\n        return self.response[item]\n\n    def __getattr__(self, item):\n        return getattr(self.response, item)\n\n\nclass RemoServHandler(urllib.request.BaseHandler):\n    def __init__(self, codec: fernet.Fernet, base_path: str):\n        self.codec = codec\n        self.req_no = 0\n        self.last_req = 0\n        self.base_path = base_path.rstrip('\/')\n\n    # noinspection PyTypeChecker\n    def http_request(self, req: urllib.request.Request\n                     ) -> urllib.request.Request:\n        self.req_no += 1\n        data = req.data\n        path = req.selector\n        req.full_url = self.base_path + path\n        hash_val = do_hash(path.encode())\n        if data is None:\n            req.data = self.codec.encrypt(b'BE' + struct.pack(\n                '>hh', self.req_no, 0) + hash_val)\n        elif hasattr(data, 'read'):\n            req.data = Codec(data, self.codec, self.req_no, 0, hash_val,\n                             False)\n        else:\n            try:\n                memoryview(data)\n                req.data = self.codec.encrypt(b'BE' + struct.pack(\n                    '>hh', self.req_no, 0) + hash_val + data)\n            except TypeError:\n                if isinstance(data, collections.abc.Iterable):\n                    codec = Codec(io.BytesIO(), self.codec, self.req_no,\n                                  0, hash_val, False)\n                    req.data = b''.join(codec.transform(d) for d in data)\\\n                               + codec.transform(b'')\n                else:\n                    raise TypeError('RemoServHandler can only process '\n                                    'bytes or iterables, got %r', type(data))\n        return req\n\n    def http_response(self, _req, response):\n        if 'Content-Length' not in response.headers:\n            return Response(response, self.codec, self.req_no)\n        else:\n            return response\n\n\n# noinspection PyTypeChecker\nclass Connection:\n    def __init__(self, user: str, opener: urllib.request.OpenerDirector,\n                 codec: fernet.Fernet):\n        self.user = user\n        self.opener = opener\n        self.codec = codec\n        self.app_url = 'http:\/\/x'\n\n    def get(self, remote_file: str, local_file: str = None):\n        cmd = b'\/get\/' + self.codec.encrypt(remote_file.encode())\n        if local_file is None:\n            local_file = remote_file\n        inp = self.opener.open(self.app_url + cmd.decode())\n        with open(local_file, 'wb') as out:\n            while True:\n                data = inp.read(8192)\n                if len(data) == 0:\n                    break\n                out.write(data)\n\n    def put(self, remote_file: str, local_file: str = None):\n        cmd = b'\/put\/' + self.codec.encrypt(remote_file.encode())\n        if local_file is None:\n            local_file = remote_file\n        with open(local_file, 'rb') as fd:\n            self.opener.open(self.app_url + cmd.decode(), fd)\n\n    def exec(self, command: str) -> http.client.HTTPResponse:\n        cmd = b'\/cmd\/' + self.codec.encrypt(command.encode())\n        r = self.opener.open(self.app_url + cmd.decode())\n        return r\n\n    def iexec(self, command: str) -> http.client.HTTPResponse:\n        cmd = b'\/icm\/' + self.codec.encrypt(command.encode())\n        r = self.opener.open(self.app_url + cmd.decode())\n        return r\n\n    def idata(self, data: str) -> http.client.HTTPResponse:\n        cmd = b'\/idt'\n        r = self.opener.open(self.app_url + cmd.decode(),\n                             data.encode() + b'\\n')\n        return r\n\n    def end_cmd(self) -> http.client.HTTPResponse:\n        cmd = b'\/edt'\n        r = self.opener.open(self.app_url + cmd.decode())\n        return r\n\n\ndef login(url: str, path: str, user: str,\n          key: Optional[ed448.Ed448PrivateKey],\n          signer: Optional[smartcard.Signer],\n          remo_pub: ed448.Ed448PublicKey):\n    cookie_processor = urllib.request.HTTPCookieProcessor()\n    opener = urllib.request.build_opener(cookie_processor)\n    tmp_key = x448.X448PrivateKey.generate()\n    # noinspection PyTypeChecker\n    pub = tmp_key.public_key().public_bytes(serialization.Encoding.Raw,\n                                            serialization.PublicFormat.Raw)\n    data = user.encode() + pub\n    sign = key.sign(data) if signer is None else signer.sign(data)\n    data = json.dumps({'user': user,\n                       'key': base64.urlsafe_b64encode(pub).decode(),\n                       'sign': base64.urlsafe_b64encode(sign).decode()\n                       })\n    r = opener.open(url + path, data.encode())\n    data = r.read()\n    remo = base64.urlsafe_b64decode(data[:76])\n    remo_pub.verify(base64.urlsafe_b64decode(data[78:]), remo)\n    remo = x448.X448PublicKey.from_public_bytes(remo)\n    session_key = tmp_key.exchange(remo)\n    # noinspection PyArgumentList\n    df = concatkdf.ConcatKDFHash(hashes.SHA256(), 32, b'remo_serv')\n    session_key = df.derive(session_key)\n    session_key = base64.urlsafe_b64encode(session_key)\n    session_codec = fernet.Fernet(session_key)\n    return Connection(user,\n                      urllib.request.build_opener(\n                          RemoServHandler(session_codec, url),\n                          cookie_processor),\n                      session_codec)\n"},"\/remo_serv\/crypter.py":{"changes":[{"diff":"\n             if codec is not None:\n                 session['REQ_NO'] = codec.req_no\n             session['LAST_REQ'] = time.time()\n-            codec = Codec(io.BytesIO(), deco, session['REQ_NO'], decode=False)\n+            codec = Codec(io.BytesIO(), deco, session['REQ_NO'], cmd_hash=b'200', decode=False)\n             return (codec.transform(data) + b'\\r\\n' for data in out)\n \n","add":1,"remove":1,"filename":"\/remo_serv\/crypter.py","badparts":["            codec = Codec(io.BytesIO(), deco, session['REQ_NO'], decode=False)"],"goodparts":["            codec = Codec(io.BytesIO(), deco, session['REQ_NO'], cmd_hash=b'200', decode=False)"]}],"source":"\n \"\"\"Encryption\/decryption and signature management using cryptography.\"\"\" import sys import base64 import io import json import logging import time import struct from cryptography import fernet from cryptography.hazmat.primitives import hashes from cryptography.hazmat.primitives import serialization from cryptography.hazmat.primitives.asymmetric import ed448, x448, padding from cryptography.hazmat.primitives.kdf.concatkdf import ConcatKDFHash from remo_tools.http_tools import build_status, Codec, do_hash, TTL logger=logging.getLogger(__name__) class Cryptor: \"\"\"WSGI middleware handling en\/de-cryption of request and response bodies. The middleware internally handles the \/auth PATH_INFO to perform login. It manages 2 variables in the session -the time when previous request was received -the request number Those session attributes are used to prevent a MITM attack that would drop or add requests: req_no are expected to be consecutive except if the delay between 2 requests is greater that 15 seconds. The req_no is passed in the first token of a(connected) request or response body, with the hash of the request query, to prevent a MITM attack that would alter the query itself. So it will start with a magic identifier of B. for begin, the request number as a 2 bytes integer and 0(2 bytes) as the token number and will have the hash of the query as sole data Following tokens contain.. as the magic identifier, the request number and the token number. The token numbers are expected to be consecutive to prevent a MITM attack that would add or remove Fernet tokens The last token has a magic identifier of.E As a special case, an empty request of response will contain exactly one token with a magic identifier of BE \"\"\" def __init__(self, app, key, user_service, path='\/auth', public='\/info'): \"\"\"Constructor parameters: -app: the WSGI application wrapped in the middleware -key: the private(ed448) key used to sign -user_service: a UserService implementation to get the public keys of registered users -path: the authentication path(default \/auth) -public: the root of a public subtree accessible without authentication(default \/info) \"\"\" if isinstance(key, ed448.Ed448PrivateKey): self.key=key else: with open(key, 'rb') as fd: self.key=serialization.load_pem_private_key(fd.read(), None) self.app=app self.user_service=user_service self.path=path self.public=public def __call__(self, environ, start_response): \"\"\"The call to the WSGI middleware.\"\"\" path=environ.get('PATH_INFO', '\/') try: session=environ['SESSION'] except LookupError: logger.error('No valid session') start_response(build_status(500),[('Content-Length', 0)]) return[b''] if path==self.path: data=environ['wsgi.input'].read() try: if len(data)==0 or environ['CONTENT_LENGTH']=='0': session.key=None start_response(build_status(200), [('Content-Type', 'text\/plain')]) return[b'Disconnected'] except LookupError: pass try: data=json.loads(data) user=data['user'] pub=base64.urlsafe_b64decode(data['key'].encode()) sign=base64.urlsafe_b64decode(data['sign'].encode()) except(json.JSONDecodeError, LookupError): logger.warning('Invalid authentication json %s', str(data)) start_response(build_status(400),[('Content-Length', '0')]) return[b''] try: user_bytes=self.user_service.public_data(user) user_key=serialization.load_pem_public_key(user_bytes) if isinstance(user_key, ed448.Ed448PublicKey): user_key.verify(sign, user.encode() +pub) else: user_key.verify(sign, user.encode() +pub, padding.PKCS1v15(), hashes.SHA512()) except(LookupError, TypeError, ValueError): logger.warning('Error login %s', user, exc_info=sys.exc_info()) start_response(build_status(403),[('Content-Length', '0')]) return[b''] tmp_key=x448.X448PrivateKey.generate() tmp_pub=tmp_key.public_key() tmp_bytes=tmp_pub.public_bytes(serialization.Encoding.Raw, serialization.PublicFormat.Raw) tmp_text=base64.urlsafe_b64encode(tmp_bytes) tmp_text +=b'\\r\\n' +base64.urlsafe_b64encode( self.key.sign(tmp_bytes)) remo_pub=x448.X448PublicKey.from_public_bytes(pub) session_key=tmp_key.exchange(remo_pub) kdf=ConcatKDFHash(hashes.SHA256(), 32, b'remo_serv') session.key=base64.urlsafe_b64encode(kdf.derive(session_key)) session.user=user session['REQ_NO']=0 session['LAST_REQ']=0 logger.info('Login %s(%s -%s)', user, session.id, session.key) start_response(build_status(200), [('Content-type', 'text_plain'), ('Content-Length', str(len(tmp_text)))]) return[tmp_text] elif session.key is None: if path.startswith(self.public): return self.app(environ, start_response) else: logger.debug('Unauthenticated request for %s', path) start_response(build_status(403),[('Content-Length', '0')]) return[b''] else: deco=fernet.Fernet(session.key) try: length=int(environ.get('CONTENT_LENGTH')) except(TypeError, ValueError): length=None if length==0: logger.warning('Missing initial token') start_response(build_status(400),[]) return[b'Missing magic tokens'] elif length is not None: data=environ['wsgi.input'].read(length) if len(data) < 6: logger.warning('Missing initial token') start_response(build_status(400),[]) return[b'Missing magic tokens'] data=deco.decrypt(data, TTL) magic=data[:2] req, tok=struct.unpack('>hh', data[2:6]) path_hash=do_hash(path.encode()) if tok !=0 or(time.time() < session.get('LAST_REQ', 0) +TTL * 2 and req !=session.get('REQ_NO', 0) +1) \\ or data[6:6 +len(path_hash)] !=path_hash \\ or magic !=b'BE': print(tok, time.time() < session.get('LAST_REQ', 0) +TTL * 2, req, session.get('REQ_NO', 0), req !=session.get('REQ_NO', 0) +1, data[6:6 +len(path_hash)], path_hash, magic) logger.warning('Wrong initial token') start_response(build_status(400),[]) session.invalidate() return[b'Wrong initial token'] data=data[6 +len(path_hash):] environ['CONTENT_LENGTH']=len(data) environ['wsgi.input']=io.BytesIO(data) codec=None session['REQ_NO']=req else: codec=Codec(environ['wsgi.input'], deco, session.get('REQ_NO', 0) +1, session.get('LAST_REQ', 0), do_hash(path.encode())) environ['wsgi.input']=codec try: out=self.app(environ, Starter(deco, start_response) .start_response) except fernet.InvalidToken: logger.warning('Invalid encrypted token') start_response(build_status(400),[]) return[b'Invalid encoding'] if codec is not None: session['REQ_NO']=codec.req_no session['LAST_REQ']=time.time() codec=Codec(io.BytesIO(), deco, session['REQ_NO'], decode=False) return(codec.transform(data) +b'\\r\\n' for data in out) class Writer: \"\"\"Wraps a stream and encode the output with the given Fernet.\"\"\" def __init__(self, stream, encoder: fernet.Fernet): self.stream=stream self.encoder=encoder def write(self, data): self.stream.write(self.encoder.encrypt(data) +b'\\r\\n') class Starter: \"\"\"Auxiliary class to provide start_response callables. It wraps an original start_response by removing any Content-Length header and returning an encoding writer \"\"\" def __init__(self, encoder: fernet.Fernet, start_response): self.encoder=encoder self.start_parent=start_response def start_response(self, status, headers, exc_info=None): stream=self.start_parent(status, [header for header in headers if header[0].lower() !='content-length'], exc_info) return Writer(stream, self.encoder) ","sourceWithComments":"#  Copyright (c) 2020 SBA- MIT License\n\n\"\"\"Encryption\/decryption and signature management using cryptography.\"\"\"\nimport sys\nimport base64\nimport io\nimport json\nimport logging\nimport time\nimport struct\n\nfrom cryptography import fernet\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives import serialization\nfrom cryptography.hazmat.primitives.asymmetric import ed448, x448, padding\nfrom cryptography.hazmat.primitives.kdf.concatkdf import ConcatKDFHash\n\nfrom remo_tools.http_tools import build_status, Codec, do_hash, TTL\n\nlogger = logging.getLogger(__name__)\n\n\n# noinspection SpellCheckingInspection\nclass Cryptor:\n    \"\"\"WSGI middleware handling en\/de-cryption of request and response bodies.\n\n    The middleware internally handles the \/auth PATH_INFO to perform login.\n\n    It manages 2 variables in the session\n    - the time when previous request was received\n    - the request number\n\n    Those session attributes are used to prevent a MITM attack that would\n    drop or add requests: req_no are expected to be consecutive except if\n    the delay between 2 requests is greater that 15 seconds.\n    The req_no is passed in the first token of a (connected) request or\n    response body, with the hash of the request query, to prevent a MITM\n    attack that would alter the query itself. So it will start with a  magic\n    identifier of B. for begin, the request number as a 2 bytes integer and\n    0 (2 bytes) as the token number and will have the hash of the query as\n    sole data\n\n    Following tokens contain .. as the magic identifier, the request number\n    and the token number. The token numbers are expected to be consecutive\n    to prevent a MITM attack that would add or remove Fernet tokens\n\n    The last token has a magic identifier of .E\n\n    As a special case, an empty request of response will contain exactly\n    one token with a magic identifier of BE\n    \"\"\"\n\n    # noinspection PyArgumentList\n    def __init__(self, app, key, user_service, path='\/auth',\n                 public='\/info'):\n        \"\"\"Constructor parameters:\n        - app: the WSGI application wrapped in the middleware\n        - key: the private (ed448) key used to sign\n        - user_service: a UserService implementation to get the public\n        keys of registered users\n        - path: the authentication path (default \/auth)\n        - public: the root of a public subtree accessible without\n        authentication (default \/info)\n        \"\"\"\n        if isinstance(key, ed448.Ed448PrivateKey):\n            self.key = key\n        else:\n            with open(key, 'rb') as fd:\n                self.key = serialization.load_pem_private_key(fd.read(), None)\n        self.app = app\n        self.user_service = user_service\n        self.path = path\n        self.public = public\n\n    def __call__(self, environ, start_response):\n        \"\"\"The call to the WSGI middleware.\"\"\"\n        path = environ.get('PATH_INFO', '\/')\n        try:\n            session = environ['SESSION']\n        except LookupError:\n            logger.error('No valid session')\n            start_response(build_status(500), [('Content-Length', 0)])\n            return [b'']\n        if path == self.path:\n            data = environ['wsgi.input'].read()\n            try:\n                if len(data) == 0 or environ['CONTENT_LENGTH'] == '0':\n                    session.key = None\n                    start_response(build_status(200),\n                                   [('Content-Type', 'text\/plain')])\n                    return [b'Disconnected']\n            except LookupError:\n                pass\n            try:\n                data = json.loads(data)\n                user = data['user']\n                pub = base64.urlsafe_b64decode(data['key'].encode())\n                sign = base64.urlsafe_b64decode(data['sign'].encode())\n            except (json.JSONDecodeError, LookupError):\n                logger.warning('Invalid authentication json %s', str(data))\n                start_response(build_status(400), [('Content-Length', '0')])\n                return [b'']\n            try:\n                user_bytes = self.user_service.public_data(user)\n                # noinspection PyArgumentList\n                user_key = serialization.load_pem_public_key(user_bytes)\n                if isinstance(user_key, ed448.Ed448PublicKey):\n                    user_key.verify(sign, user.encode() + pub)\n                else:\n                    user_key.verify(sign, user.encode() + pub,\n                                    padding.PKCS1v15(), hashes.SHA512())\n            except (LookupError, TypeError, ValueError):\n                logger.warning('Error login %s', user, exc_info=sys.exc_info())\n                start_response(build_status(403), [('Content-Length', '0')])\n                return [b'']\n            tmp_key = x448.X448PrivateKey.generate()\n            tmp_pub = tmp_key.public_key()\n            # noinspection PyTypeChecker\n            tmp_bytes = tmp_pub.public_bytes(serialization.Encoding.Raw,\n                                             serialization.PublicFormat.Raw)\n            tmp_text = base64.urlsafe_b64encode(tmp_bytes)\n            tmp_text += b'\\r\\n' + base64.urlsafe_b64encode(\n                self.key.sign(tmp_bytes))\n            remo_pub = x448.X448PublicKey.from_public_bytes(pub)\n            session_key = tmp_key.exchange(remo_pub)\n            # noinspection PyArgumentList\n            kdf = ConcatKDFHash(hashes.SHA256(), 32, b'remo_serv')\n            session.key = base64.urlsafe_b64encode(kdf.derive(session_key))\n            session.user = user\n            session['REQ_NO'] = 0\n            session['LAST_REQ'] = 0\n            logger.info('Login %s (%s - %s)', user, session.id, session.key)\n            start_response(build_status(200),\n                           [('Content-type', 'text_plain'),\n                            ('Content-Length', str(len(tmp_text)))])\n            return [tmp_text]\n        elif session.key is None:\n            if path.startswith(self.public):\n                return self.app(environ, start_response)\n            else:\n                logger.debug('Unauthenticated request for %s', path)\n                start_response(build_status(403), [('Content-Length', '0')])\n            return [b'']\n        else:\n            deco = fernet.Fernet(session.key)\n            try:\n                length = int(environ.get('CONTENT_LENGTH'))\n            except (TypeError, ValueError):\n                length = None\n            if length == 0:\n                logger.warning('Missing initial token')\n                start_response(build_status(400), [])\n                return [b'Missing magic tokens']\n            elif length is not None:\n                data = environ['wsgi.input'].read(length)\n                if len(data) < 6:\n                    logger.warning('Missing initial token')\n                    start_response(build_status(400), [])\n                    return [b'Missing magic tokens']\n                data = deco.decrypt(data, TTL)\n                magic = data[:2]\n                req, tok = struct.unpack('>hh', data[2:6])\n                path_hash = do_hash(path.encode())\n                if tok != 0 or (time.time() < session.get('LAST_REQ', 0)\n                                + TTL * 2\n                                and req != session.get('REQ_NO', 0) + 1) \\\n                        or data[6:6 + len(path_hash)] != path_hash \\\n                        or magic != b'BE':\n                    print(tok, time.time() < session.get('LAST_REQ', 0)\n                          + TTL * 2, req, session.get('REQ_NO', 0),\n                          req != session.get('REQ_NO', 0) + 1,\n                          data[6:6 + len(path_hash)], path_hash, magic)\n                    logger.warning('Wrong initial token')\n                    start_response(build_status(400), [])\n                    session.invalidate()\n                    return [b'Wrong initial token']\n                data = data[6 + len(path_hash):]\n                environ['CONTENT_LENGTH'] = len(data)\n                environ['wsgi.input'] = io.BytesIO(data)\n                codec = None\n                session['REQ_NO'] = req\n            else:\n                codec = Codec(environ['wsgi.input'], deco,\n                              session.get('REQ_NO', 0) + 1,\n                              session.get('LAST_REQ', 0),\n                              do_hash(path.encode()))\n                environ['wsgi.input'] = codec\n            try:\n                out = self.app(environ, Starter(deco, start_response)\n                               .start_response)\n            except fernet.InvalidToken:\n                logger.warning('Invalid encrypted token')\n                start_response(build_status(400), [])\n                return [b'Invalid encoding']\n            if codec is not None:\n                session['REQ_NO'] = codec.req_no\n            session['LAST_REQ'] = time.time()\n            codec = Codec(io.BytesIO(), deco, session['REQ_NO'], decode=False)\n            return (codec.transform(data) + b'\\r\\n' for data in out)\n\n\nclass Writer:\n    \"\"\"Wraps a stream and encode the output with the given Fernet.\"\"\"\n\n    def __init__(self, stream, encoder: fernet.Fernet):\n        self.stream = stream\n        self.encoder = encoder\n\n    def write(self, data):\n        self.stream.write(self.encoder.encrypt(data) + b'\\r\\n')\n\n\nclass Starter:\n    \"\"\"Auxiliary class to provide start_response callables.\n\n    It wraps an original start_response by removing any Content-Length\n    header and returning an encoding writer\n    \"\"\"\n\n    def __init__(self, encoder: fernet.Fernet, start_response):\n        self.encoder = encoder\n        self.start_parent = start_response\n\n    def start_response(self, status, headers, exc_info=None):\n        stream = self.start_parent(status,\n                                   [header for header in headers\n                                    if header[0].lower() != 'content-length'],\n                                   exc_info)\n        return Writer(stream, self.encoder)\n"},"\/test_client\/test_clientlib.py":{"changes":[{"diff":"\n \n     def test_simple(self):\n         req_no = 5\n-        data = b'BE' + struct.pack('>hh', req_no, 0) + b'foo'\n+        data = b'BE' + struct.pack('>hh', req_no, 0) + b'200foo'\n         data = self.fernet.encrypt(data)\n-        r = clientlib.Response(io.BytesIO(data), self.fernet, req_no)\n+        r = clientlib.Response(ResponseStub(data, 200), self.fernet, req_no)\n         self.assertEqual(b'foo', r.read())\n         self.assertTrue(r.is_deco_ok())\n \n     def test_no_encrypt(self):\n-        r = clientlib.Response(io.BytesIO(b'foo'), self.fernet, 1)\n+        r = clientlib.Response(ResponseStub(b'foo', 200), self.fernet, 1)\n         self.assertEqual(b'foo', r.read())\n \n     def test_wrong_req(self):\n         req_no = 5\n         data = b'BE' + struct.pack('>hh', req_no + 1, 0) + b'foo'\n         data = self.fernet.encrypt(data)\n-        r = clientlib.Response(io.BytesIO(data), self.fernet, req_no)\n+        r = clientlib.Response(ResponseStub(data, 200), self.fernet, req_no)\n         self.assertTrue(r.is_deco_ok())\n         r.read()\n         self.assertFalse(r.is_deco_ok(","add":4,"remove":4,"filename":"\/test_client\/test_clientlib.py","badparts":["        data = b'BE' + struct.pack('>hh', req_no, 0) + b'foo'","        r = clientlib.Response(io.BytesIO(data), self.fernet, req_no)","        r = clientlib.Response(io.BytesIO(b'foo'), self.fernet, 1)","        r = clientlib.Response(io.BytesIO(data), self.fernet, req_no)"],"goodparts":["        data = b'BE' + struct.pack('>hh', req_no, 0) + b'200foo'","        r = clientlib.Response(ResponseStub(data, 200), self.fernet, req_no)","        r = clientlib.Response(ResponseStub(b'foo', 200), self.fernet, 1)","        r = clientlib.Response(ResponseStub(data, 200), self.fernet, req_no)"]}],"source":"\n import unittest from unittest.mock import Mock, patch import struct import io import time import urllib.request import http.client from cryptography import fernet from client import clientlib from remo_tools import http_tools class TestResponse(unittest.TestCase): def setUp(self) -> None: key=fernet.Fernet.generate_key() self.fernet=fernet.Fernet(key) def test_simple(self): req_no=5 data=b'BE' +struct.pack('>hh', req_no, 0) +b'foo' data=self.fernet.encrypt(data) r=clientlib.Response(io.BytesIO(data), self.fernet, req_no) self.assertEqual(b'foo', r.read()) self.assertTrue(r.is_deco_ok()) def test_no_encrypt(self): r=clientlib.Response(io.BytesIO(b'foo'), self.fernet, 1) self.assertEqual(b'foo', r.read()) def test_wrong_req(self): req_no=5 data=b'BE' +struct.pack('>hh', req_no +1, 0) +b'foo' data=self.fernet.encrypt(data) r=clientlib.Response(io.BytesIO(data), self.fernet, req_no) self.assertTrue(r.is_deco_ok()) r.read() self.assertFalse(r.is_deco_ok()) class TestOpener(unittest.TestCase): def setUp(self) -> None: key=fernet.Fernet.generate_key() self.fernet=fernet.Fernet(key) self.servHandler=clientlib.RemoServHandler(self.fernet, 'http:\/\/foo.com\/') self.opener=urllib.request.build_opener(self.servHandler) def test_url_ok(self): with patch.object(self.opener, '_open') as r: resp=Mock(http.client.HTTPResponse) resp.headers={} resp.code=200 resp.msg='200 OK' r.return_value=resp self.opener.open('http:\/\/x\/bar') req=r.call_args[0][0] self.assertEqual('http:\/\/foo.com\/bar', req.full_url) def test_url_with_path(self): self.servHandler.base_path='http:\/\/foo.com\/path\/to' with patch.object(self.opener, '_open') as r: resp=Mock(http.client.HTTPResponse) resp.headers={} resp.code=200 resp.msg='200 OK' r.return_value=resp self.opener.open('http:\/\/x\/bar') req=r.call_args[0][0] self.assertEqual('http:\/\/foo.com\/path\/to\/bar', req.full_url) class TestHandler(unittest.TestCase): def setUp(self) -> None: key=fernet.Fernet.generate_key() self.fernet=fernet.Fernet(key) self.servHandler=clientlib.RemoServHandler(self.fernet, 'http:\/\/foo.com') def test_simple(self): req=urllib.request.Request('http:\/get\/foo') r=self.servHandler.http_request(req) self.assertEqual('http:\/\/foo.com\/get\/foo', r.full_url) self.assertIsNotNone(r.data) c=http_tools.Codec(io.BytesIO(r.data), self.fernet, 1, time.time(), http_tools.do_hash(b'\/get\/foo')) self.assertEqual(b'', c.read()) self.assertTrue(c.deco_ok) def test_iterable(self): req=urllib.request.Request('http:\/get\/foo',(b'ab\\nc', b'd\\nef')) r=self.servHandler.http_request(req) self.assertEqual('http:\/\/foo.com\/get\/foo', r.full_url) self.assertIsNotNone(r.data) self.assertEqual(3, len(r.data.splitlines())) c=http_tools.Codec(io.BytesIO(r.data), self.fernet, 1, time.time(), http_tools.do_hash(b'\/get\/foo')) self.assertEqual([b'ab\\n', b'cd\\n', b'ef'], c.readlines()) if __name__=='__main__': unittest.main() ","sourceWithComments":"#  Copyright (c) 2020 SBA- MIT License\n\nimport unittest\nfrom unittest.mock import Mock, patch\n\nimport struct\nimport io\nimport time\nimport urllib.request\nimport http.client\n\nfrom cryptography import fernet\n\nfrom client import clientlib\nfrom remo_tools import http_tools\n\n\nclass TestResponse(unittest.TestCase):\n    def setUp(self) -> None:\n        key = fernet.Fernet.generate_key()\n        self.fernet = fernet.Fernet(key)\n\n    def test_simple(self):\n        req_no = 5\n        data = b'BE' + struct.pack('>hh', req_no, 0) + b'foo'\n        data = self.fernet.encrypt(data)\n        r = clientlib.Response(io.BytesIO(data), self.fernet, req_no)\n        self.assertEqual(b'foo', r.read())\n        self.assertTrue(r.is_deco_ok())\n\n    def test_no_encrypt(self):\n        r = clientlib.Response(io.BytesIO(b'foo'), self.fernet, 1)\n        self.assertEqual(b'foo', r.read())\n\n    def test_wrong_req(self):\n        req_no = 5\n        data = b'BE' + struct.pack('>hh', req_no + 1, 0) + b'foo'\n        data = self.fernet.encrypt(data)\n        r = clientlib.Response(io.BytesIO(data), self.fernet, req_no)\n        self.assertTrue(r.is_deco_ok())\n        r.read()\n        self.assertFalse(r.is_deco_ok())\n\n\n# BEWARE: tests in this TestCase depend on an opener to call _open after\n# pre-processing the request in its handlers\nclass TestOpener(unittest.TestCase):\n    def setUp(self) -> None:\n        key = fernet.Fernet.generate_key()\n        self.fernet = fernet.Fernet(key)\n        self.servHandler = clientlib.RemoServHandler(self.fernet,\n                                                     'http:\/\/foo.com\/')\n        self.opener = urllib.request.build_opener(self.servHandler)\n\n    def test_url_ok(self):\n        with patch.object(self.opener, '_open') as r:\n            resp = Mock(http.client.HTTPResponse)\n            resp.headers = {}\n            resp.code = 200\n            resp.msg = '200 OK'\n            r.return_value = resp\n            self.opener.open('http:\/\/x\/bar')\n            req = r.call_args[0][0]\n            self.assertEqual('http:\/\/foo.com\/bar', req.full_url)\n\n    def test_url_with_path(self):\n        self.servHandler.base_path = 'http:\/\/foo.com\/path\/to'\n        with patch.object(self.opener, '_open') as r:\n            resp = Mock(http.client.HTTPResponse)\n            resp.headers = {}\n            resp.code = 200\n            resp.msg = '200 OK'\n            r.return_value = resp\n            self.opener.open('http:\/\/x\/bar')\n            req = r.call_args[0][0]\n            self.assertEqual('http:\/\/foo.com\/path\/to\/bar', req.full_url)\n\n\nclass TestHandler(unittest.TestCase):\n    def setUp(self) -> None:\n        key = fernet.Fernet.generate_key()\n        self.fernet = fernet.Fernet(key)\n        self.servHandler = clientlib.RemoServHandler(self.fernet,\n                                                     'http:\/\/foo.com')\n\n    def test_simple(self):\n        req = urllib.request.Request('http:\/get\/foo')\n        r = self.servHandler.http_request(req)\n        self.assertEqual('http:\/\/foo.com\/get\/foo', r.full_url)\n        self.assertIsNotNone(r.data)\n        c = http_tools.Codec(io.BytesIO(r.data), self.fernet, 1, time.time(),\n                             http_tools.do_hash(b'\/get\/foo'))\n        self.assertEqual(b'', c.read())\n        self.assertTrue(c.deco_ok)\n\n    def test_iterable(self):\n        req = urllib.request.Request('http:\/get\/foo', (b'ab\\nc', b'd\\nef'))\n        r = self.servHandler.http_request(req)\n        self.assertEqual('http:\/\/foo.com\/get\/foo', r.full_url)\n        self.assertIsNotNone(r.data)\n        self.assertEqual(3, len(r.data.splitlines()))\n        c = http_tools.Codec(io.BytesIO(r.data), self.fernet, 1, time.time(),\n                             http_tools.do_hash(b'\/get\/foo'))\n        self.assertEqual([b'ab\\n', b'cd\\n', b'ef'], c.readlines())\n\n\nif __name__ == '__main__':\n    unittest.main()\n"},"\/tests\/test_crypter.py":{"changes":[{"diff":"\n         resp.append(b'')\n         for i, data in enumerate(out):\n             data = coder.decrypt(data)\n-            self.assertEqual(resp[i], data[6:])\n+            if i == 0:\n+                self.assertEqual(b'200' + resp[i], data[6:])\n+            else:\n+                self.assertEqual(resp[i], data[6:])\n             req, tok = struct.unpack('>hh', data[2:6])\n             self.assertEqual(5, req)\n             self.assertEqual(i, tok)\n","add":4,"remove":1,"filename":"\/tests\/test_crypter.py","badparts":["            self.assertEqual(resp[i], data[6:])"],"goodparts":["            if i == 0:","                self.assertEqual(b'200' + resp[i], data[6:])","            else:","                self.assertEqual(resp[i], data[6:])"]},{"diff":"\n         resp.append(b'')\n         for i, data in enumerate(out):\n             data = coder.decrypt(data)\n-            self.assertEqual(resp[i], data[6:])\n+            if i == 0:\n+                self.assertEqual(b'200' + resp[i], data[6:])\n+            else:\n+                self.assertEqual(resp[i], data[6:])\n             req, tok = struct.unpack('>hh', data[2:6])\n             self.assertEqual(5, req)\n             self.assertEqual(i, tok)\n","add":4,"remove":1,"filename":"\/tests\/test_crypter.py","badparts":["            self.assertEqual(resp[i], data[6:])"],"goodparts":["            if i == 0:","                self.assertEqual(b'200' + resp[i], data[6:])","            else:","                self.assertEqual(resp[i], data[6:])"]}],"source":"\n from unittest import TestCase from unittest.mock import Mock, patch import io import json from cryptography.hazmat.primitives.asymmetric import ed448, x448 from cryptography.hazmat.primitives import serialization from cryptography import fernet from cryptography.hazmat.primitives.kdf.concatkdf import ConcatKDFHash from cryptography.hazmat.primitives import hashes import secrets import base64 import struct from remo_serv import session_manager, crypter from remo_tools import http_tools from remo_serv.user_service import UserService, MemoryUserService def app(_environ, start_response): start_response(http_tools.build_status(200), [('Content-type', 'text\/plain')]) return[b'foo', b'bar'] class Connection: def __init__(self, user, user_service: UserService): self.user=user self.user_key=user_service.private(user) self.private=x448.X448PrivateKey.generate() self.pub=self.private.public_key().public_bytes( serialization.Encoding.Raw, serialization.PublicFormat.Raw) class TestCryptor(TestCase): def setUp(self) -> None: self.session=session_manager.Session('sess') self.input=Mock(io.BytesIO) self.environ={ 'SESSION': self.session, 'wsgi.input': self.input, 'PATH_INFO': '\/', } self.app=Mock(app) self.user_service=MemoryUserService('foo', 'bar') with patch.object(serialization, 'load_pem_private_key') \\ as loader, patch('builtins.open'): loader.return_value=ed448.Ed448PrivateKey.generate() self.crypt=crypter.Cryptor( self.app, 'keyfile', self.user_service) self.kdf=ConcatKDFHash(hashes.SHA256(), 32, b'remo_serv') def test_no_auth_public(self): self.environ['PATH_INFO']='\/info' start_response=Mock() self.crypt(self.environ, start_response) self.app.assert_called_once_with(self.environ, start_response) def test_no_auth(self): start_response=Mock() self.crypt(self.environ, start_response) self.app.assert_not_called() start_response.assert_called_once() self.assertEqual(http_tools.build_status(403), start_response.call_args[0][0]) def test_authenticated(self): start_response=Mock() session_key=base64.urlsafe_b64encode(secrets.token_bytes(32)) msg=b'A B\\nC' hash_val=http_tools.do_hash(self.environ['PATH_INFO'].encode()) coder=fernet.Fernet(session_key) coded=coder.encrypt(b'BE' +struct.pack('>hh', 5, 0) +hash_val +msg) self.environ['SESSION'].key=session_key self.environ['wsgi.input']=io.BytesIO(coded) self.environ['CONTENT_LENGTH']=len(coded) resp=b'x yzt' resp=[resp, resp+resp] self.app.return_value=resp out=self.crypt(self.environ, start_response) self.app.assert_called_once() env=self.app.call_args[0][0] input_data=env['wsgi.input'].read() self.assertEqual(msg, input_data) i=-1 resp.append(b'') for i, data in enumerate(out): data=coder.decrypt(data) self.assertEqual(resp[i], data[6:]) req, tok=struct.unpack('>hh', data[2:6]) self.assertEqual(5, req) self.assertEqual(i, tok) self.assertEqual(2, i) def test_authenticated_no_len(self): start_response=Mock() session_key=base64.urlsafe_b64encode(secrets.token_bytes(32)) msg=b'A B\\nC' hash_val=http_tools.do_hash(self.environ['PATH_INFO'].encode()) coder=fernet.Fernet(session_key) coded=coder.encrypt(b'BE' +struct.pack('>hh', 5, 0) +hash_val +msg) self.environ['SESSION'].key=session_key self.environ['wsgi.input']=io.BytesIO(coded) resp=b'x yzt' resp=[resp, resp+resp] input_data=b'' def data_input(_x, _y): nonlocal input_data input_data=self.environ['wsgi.input'].read() return resp self.app.side_effect=data_input out=self.crypt(self.environ, start_response) self.app.assert_called_once() env=self.app.call_args[0][0] self.assertEqual(msg, input_data) i=-1 resp.append(b'') for i, data in enumerate(out): data=coder.decrypt(data) self.assertEqual(resp[i], data[6:]) req, tok=struct.unpack('>hh', data[2:6]) self.assertEqual(5, req) self.assertEqual(i, tok) self.assertEqual(2, i) def test_deco(self): self.environ['SESSION'].key=base64.urlsafe_b64encode( secrets.token_bytes(32)) self.environ['CONTENT_LENGTH']=0 self.environ['PATH_INFO']='\/auth' start_response=Mock() self.environ['wsgi.input'].read=Mock(return_value=b'') self.crypt(self.environ, start_response) self.app.assert_not_called() start_response.assert_called_once() self.assertIsNone(self.environ['SESSION'].key) self.assertTrue(start_response.call_args[0][0].startswith('200 ')) def test_connect(self): con=Connection('foo', self.user_service) self.environ['PATH_INFO']='\/auth' sign=self.user_service.private('foo').sign(b'foo' +con.pub) data=json.dumps({'user': 'foo', 'key': base64.urlsafe_b64encode(con.pub).decode(), 'sign': base64.urlsafe_b64encode(sign).decode(), }) self.environ['wsgi.input']=io.BytesIO(data.encode()) start_response=Mock() out=self.crypt(self.environ, start_response) start_response.assert_called_once() self.assertTrue(start_response.call_args[0][0].startswith('200 ')) data=next(iter(out)) remo_pub_bytes=base64.urlsafe_b64decode(data) remo_pub=x448.X448PublicKey.from_public_bytes(remo_pub_bytes) tempo=con.private.exchange(remo_pub) key=base64.urlsafe_b64encode(self.kdf.derive(tempo)) self.assertEqual(self.environ['SESSION'].key, key) def test_content_length_0(self): session_key=base64.urlsafe_b64encode(secrets.token_bytes(32)) self.environ['SESSION'].key=session_key codec=fernet.Fernet(session_key) hash_val=http_tools.do_hash(self.environ['PATH_INFO'].encode()) plain=b'' data=codec.encrypt(b'BE' +struct.pack('>hh', 5, 0) +hash_val +plain) self.environ['CONTENT_LENGTH']=len(data) self.environ['wsgi.input']=io.BytesIO(data) start_response=Mock() self.app.return_value=[b''] self.crypt(self.environ, start_response) self.assertEqual(b'', self.environ['wsgi.input'].read()) def test_content_length(self): session_key=base64.urlsafe_b64encode(secrets.token_bytes(32)) self.environ['SESSION'].key=session_key codec=fernet.Fernet(session_key) hash_val=http_tools.do_hash(self.environ['PATH_INFO'].encode()) plain=b'abcdef' data=codec.encrypt(b'BE' +struct.pack('>hh', 5, 0) +hash_val +plain) self.environ['CONTENT_LENGTH']=len(data) self.environ['wsgi.input']=io.BytesIO(data) start_response=Mock() self.app.return_value=[b''] self.crypt(self.environ, start_response) self.assertEqual(plain, self.environ['wsgi.input'].read()) self.assertEqual(len(plain), self.environ['CONTENT_LENGTH']) def test_content_length_unknown(self): session_key=base64.urlsafe_b64encode(secrets.token_bytes(32)) self.environ['SESSION'].key=session_key coder=fernet.Fernet(session_key) hash_val=http_tools.do_hash(self.environ['PATH_INFO'].encode()) codec=http_tools.Codec(io.BytesIO(), coder, 5, 0, hash_val, False) plains=[b'abc', b'def', b''] data=b''.join(codec.transform(x) for x in plains) self.environ['wsgi.input']=io.BytesIO(data) start_response=Mock() self.app.return_value=[b''] self.crypt(self.environ, start_response) self.assertEqual(b''.join(plains), self.environ['wsgi.input'].read()) self.assertFalse('CONTENT_LENGTH' in self.environ) def test_wrong_cmd_hash(self): session_key=base64.urlsafe_b64encode(secrets.token_bytes(32)) self.environ['SESSION'].key=session_key coder=fernet.Fernet(session_key) hash_val=http_tools.do_hash(self.environ['PATH_INFO'].encode() +b'x') codec=http_tools.Codec(io.BytesIO(), coder, 5, 0, hash_val, False) plains=[b'abc', b'def', b''] data=b''.join(codec.transform(x) for x in plains) self.environ['wsgi.input']=io.BytesIO(data) start_response=Mock() self.app.return_value=[b''] with self.assertRaises(fernet.InvalidToken): self.crypt(self.environ, start_response) self.environ['wsgi.input'].read() def test_data_error(self): session_key=base64.urlsafe_b64encode(secrets.token_bytes(32)) self.environ['SESSION'].key=session_key plains=b'abcdef' data=plains self.environ['wsgi.input']=io.BytesIO(data) start_response=Mock() self.app.side_effect=lambda environ, _x:[environ['wsgi.input'].read()] self.crypt(self.environ, start_response) self.assertEqual(http_tools.build_status(400), start_response.call_args[0][0]) def test_RSA_connect(self): from cryptography.hazmat.primitives.asymmetric import rsa, padding private=rsa.generate_private_key(65537, 2048) public=private.public_key() user_service=Mock(UserService) user_service.private.return_value=private user_service.public_data.return_value=public.public_bytes( serialization.Encoding.PEM, serialization.PublicFormat.SubjectPublicKeyInfo) con=Connection('foo', user_service) self.environ['PATH_INFO']='\/auth' sign=user_service.private('foo').sign( b'foo' +con.pub, padding.PKCS1v15(), hashes.SHA512()) data=json.dumps({'user': 'foo', 'key': base64.urlsafe_b64encode(con.pub).decode(), 'sign': base64.urlsafe_b64encode(sign).decode(), }) self.environ['wsgi.input']=io.BytesIO(data.encode()) start_response=Mock() with patch.object(self.crypt, 'user_service', user_service): out=self.crypt(self.environ, start_response) start_response.assert_called_once() self.assertTrue(start_response.call_args[0][0].startswith('200 ')) data=next(iter(out)) remo_pub_bytes=base64.urlsafe_b64decode(data) remo_pub=x448.X448PublicKey.from_public_bytes(remo_pub_bytes) tempo=con.private.exchange(remo_pub) key=base64.urlsafe_b64encode(self.kdf.derive(tempo)) self.assertEqual(self.environ['SESSION'].key, key) ","sourceWithComments":"#  Copyright (c) 2020 SBA- MIT License\n\nfrom unittest import TestCase\nfrom unittest.mock import Mock, patch\nimport io\nimport json\n\nfrom cryptography.hazmat.primitives.asymmetric import ed448, x448\nfrom cryptography.hazmat.primitives import serialization\nfrom cryptography import fernet\nfrom cryptography.hazmat.primitives.kdf.concatkdf import ConcatKDFHash\nfrom cryptography.hazmat.primitives import hashes\n\nimport secrets\nimport base64\nimport struct\n\nfrom remo_serv import session_manager, crypter\nfrom remo_tools import http_tools\nfrom remo_serv.user_service import UserService, MemoryUserService\n\n\n# noinspection PyUnresolvedReferences\ndef app(_environ, start_response):\n    start_response(http_tools.build_status(200),\n                   [('Content-type', 'text\/plain')])\n    return [b'foo', b'bar']\n\n\nclass Connection:\n    # noinspection PyTypeChecker\n    def __init__(self, user, user_service: UserService):\n        self.user = user\n        self.user_key = user_service.private(user)\n        self.private = x448.X448PrivateKey.generate()\n        self.pub = self.private.public_key().public_bytes(\n            serialization.Encoding.Raw, serialization.PublicFormat.Raw)\n\n\n# noinspection PyUnresolvedReferences,PyTypeChecker\nclass TestCryptor(TestCase):\n    def setUp(self) -> None:\n        self.session = session_manager.Session('sess')\n        self.input = Mock(io.BytesIO)\n        self.environ = {\n            'SESSION': self.session,\n            'wsgi.input': self.input,\n            'PATH_INFO': '\/',\n        }\n        self.app = Mock(app)\n        self.user_service = MemoryUserService('foo', 'bar')\n        with patch.object(serialization, 'load_pem_private_key') \\\n                as loader, patch('builtins.open'):\n            loader.return_value = ed448.Ed448PrivateKey.generate()\n            self.crypt = crypter.Cryptor(\n                self.app, 'keyfile', self.user_service)\n        # noinspection PyArgumentList\n        self.kdf = ConcatKDFHash(hashes.SHA256(), 32, b'remo_serv')\n\n    def test_no_auth_public(self):\n        self.environ['PATH_INFO'] = '\/info'\n        start_response = Mock()\n        self.crypt(self.environ, start_response)\n        self.app.assert_called_once_with(self.environ, start_response)\n\n    def test_no_auth(self):\n        start_response = Mock()\n        self.crypt(self.environ, start_response)\n        self.app.assert_not_called()\n        start_response.assert_called_once()\n        self.assertEqual(http_tools.build_status(403),\n                         start_response.call_args[0][0])\n\n    def test_authenticated(self):\n        start_response = Mock()\n        session_key = base64.urlsafe_b64encode(secrets.token_bytes(32))\n        msg = b'A B\\nC'\n        hash_val = http_tools.do_hash(self.environ['PATH_INFO'].encode())\n        coder = fernet.Fernet(session_key)\n        coded = coder.encrypt(b'BE' + struct.pack('>hh', 5, 0)\n                              + hash_val + msg)\n        self.environ['SESSION'].key = session_key\n        self.environ['wsgi.input'] = io.BytesIO(coded)\n        self.environ['CONTENT_LENGTH'] = len(coded)\n        resp = b'x yzt'\n        resp = [resp, resp+resp]\n        self.app.return_value = resp\n        out = self.crypt(self.environ, start_response)\n        self.app.assert_called_once()\n        env = self.app.call_args[0][0]\n        input_data = env['wsgi.input'].read()\n        self.assertEqual(msg, input_data)\n        i = -1\n        resp.append(b'')\n        for i, data in enumerate(out):\n            data = coder.decrypt(data)\n            self.assertEqual(resp[i], data[6:])\n            req, tok = struct.unpack('>hh', data[2:6])\n            self.assertEqual(5, req)\n            self.assertEqual(i, tok)\n        self.assertEqual(2, i)\n\n    def test_authenticated_no_len(self):\n        start_response = Mock()\n        session_key = base64.urlsafe_b64encode(secrets.token_bytes(32))\n        msg = b'A B\\nC'\n        hash_val = http_tools.do_hash(self.environ['PATH_INFO'].encode())\n        coder = fernet.Fernet(session_key)\n        coded = coder.encrypt(b'BE' + struct.pack('>hh', 5, 0)\n                              + hash_val + msg)\n        self.environ['SESSION'].key = session_key\n        self.environ['wsgi.input'] = io.BytesIO(coded)\n        resp = b'x yzt'\n        resp = [resp, resp+resp]\n        input_data = b''\n\n        def data_input(_x, _y):\n            nonlocal input_data\n\n            input_data = self.environ['wsgi.input'].read()\n            return resp\n\n        self.app.side_effect = data_input\n        out = self.crypt(self.environ, start_response)\n        self.app.assert_called_once()\n        env = self.app.call_args[0][0]\n        self.assertEqual(msg, input_data)\n        i = -1\n        resp.append(b'')\n        for i, data in enumerate(out):\n            data = coder.decrypt(data)\n            self.assertEqual(resp[i], data[6:])\n            req, tok = struct.unpack('>hh', data[2:6])\n            self.assertEqual(5, req)\n            self.assertEqual(i, tok)\n        self.assertEqual(2, i)\n\n    def test_deco(self):\n        self.environ['SESSION'].key = base64.urlsafe_b64encode(\n            secrets.token_bytes(32))\n        self.environ['CONTENT_LENGTH'] = 0\n        self.environ['PATH_INFO'] = '\/auth'\n        start_response = Mock()\n        self.environ['wsgi.input'].read = Mock(return_value=b'')\n        self.crypt(self.environ, start_response)\n        self.app.assert_not_called()\n        start_response.assert_called_once()\n        self.assertIsNone(self.environ['SESSION'].key)\n        self.assertTrue(start_response.call_args[0][0].startswith('200 '))\n\n    def test_connect(self):\n        con = Connection('foo', self.user_service)\n        self.environ['PATH_INFO'] = '\/auth'\n        sign = self.user_service.private('foo').sign(b'foo' + con.pub)\n        data = json.dumps({'user': 'foo',\n                           'key': base64.urlsafe_b64encode(con.pub).decode(),\n                           'sign': base64.urlsafe_b64encode(sign).decode(),\n                           })\n        self.environ['wsgi.input'] = io.BytesIO(data.encode())\n        start_response = Mock()\n        out = self.crypt(self.environ, start_response)\n        start_response.assert_called_once()\n        self.assertTrue(start_response.call_args[0][0].startswith('200 '))\n        data = next(iter(out))\n        remo_pub_bytes = base64.urlsafe_b64decode(data)\n        remo_pub = x448.X448PublicKey.from_public_bytes(remo_pub_bytes)\n        tempo = con.private.exchange(remo_pub)\n        key = base64.urlsafe_b64encode(self.kdf.derive(tempo))\n        self.assertEqual(self.environ['SESSION'].key, key)\n\n    def test_content_length_0(self):\n        session_key = base64.urlsafe_b64encode(secrets.token_bytes(32))\n        self.environ['SESSION'].key = session_key\n        codec = fernet.Fernet(session_key)\n        hash_val = http_tools.do_hash(self.environ['PATH_INFO'].encode())\n        plain = b''\n        data = codec.encrypt(b'BE' + struct.pack('>hh', 5, 0) + hash_val\n                             + plain)\n        self.environ['CONTENT_LENGTH'] = len(data)\n        self.environ['wsgi.input'] = io.BytesIO(data)\n        start_response = Mock()\n        self.app.return_value = [b'']\n        self.crypt(self.environ, start_response)\n        self.assertEqual(b'', self.environ['wsgi.input'].read())\n\n    def test_content_length(self):\n        session_key = base64.urlsafe_b64encode(secrets.token_bytes(32))\n        self.environ['SESSION'].key = session_key\n        codec = fernet.Fernet(session_key)\n        hash_val = http_tools.do_hash(self.environ['PATH_INFO'].encode())\n        plain = b'abcdef'\n        data = codec.encrypt(b'BE' + struct.pack('>hh', 5, 0) + hash_val\n                             + plain)\n        self.environ['CONTENT_LENGTH'] = len(data)\n        self.environ['wsgi.input'] = io.BytesIO(data)\n        start_response = Mock()\n        self.app.return_value = [b'']\n        self.crypt(self.environ, start_response)\n        self.assertEqual(plain, self.environ['wsgi.input'].read())\n        self.assertEqual(len(plain), self.environ['CONTENT_LENGTH'])\n\n    def test_content_length_unknown(self):\n        session_key = base64.urlsafe_b64encode(secrets.token_bytes(32))\n        self.environ['SESSION'].key = session_key\n        coder = fernet.Fernet(session_key)\n        hash_val = http_tools.do_hash(self.environ['PATH_INFO'].encode())\n        codec = http_tools.Codec(io.BytesIO(), coder, 5, 0, hash_val, False)\n        plains = [b'abc', b'def', b'']\n        data = b''.join(codec.transform(x) for x in plains)\n        self.environ['wsgi.input'] = io.BytesIO(data)\n        start_response = Mock()\n        self.app.return_value = [b'']\n        self.crypt(self.environ, start_response)\n        self.assertEqual(b''.join(plains), self.environ['wsgi.input'].read())\n        self.assertFalse('CONTENT_LENGTH' in self.environ)\n\n    def test_wrong_cmd_hash(self):\n        session_key = base64.urlsafe_b64encode(secrets.token_bytes(32))\n        self.environ['SESSION'].key = session_key\n        coder = fernet.Fernet(session_key)\n        hash_val = http_tools.do_hash(self.environ['PATH_INFO'].encode() + b'x')\n        codec = http_tools.Codec(io.BytesIO(), coder, 5, 0, hash_val, False)\n        plains = [b'abc', b'def', b'']\n        data = b''.join(codec.transform(x) for x in plains)\n        self.environ['wsgi.input'] = io.BytesIO(data)\n        start_response = Mock()\n        self.app.return_value = [b'']\n        with self.assertRaises(fernet.InvalidToken):\n            self.crypt(self.environ, start_response)\n            self.environ['wsgi.input'].read()\n\n    def test_data_error(self):\n        session_key = base64.urlsafe_b64encode(secrets.token_bytes(32))\n        self.environ['SESSION'].key = session_key\n        # noinspection SpellCheckingInspection\n        plains = b'abcdef'\n        data = plains\n        self.environ['wsgi.input'] = io.BytesIO(data)\n        start_response = Mock()\n        self.app.side_effect = lambda environ, _x: [environ['wsgi.input'].read()]\n        self.crypt(self.environ, start_response)\n        self.assertEqual(http_tools.build_status(400),\n                         start_response.call_args[0][0])\n\n    def test_RSA_connect(self):\n        from cryptography.hazmat.primitives.asymmetric import rsa, padding\n\n        # noinspection PyArgumentList\n        private = rsa.generate_private_key(65537, 2048)\n        public = private.public_key()\n        user_service = Mock(UserService)\n        user_service.private.return_value = private\n        user_service.public_data.return_value = public.public_bytes(\n            serialization.Encoding.PEM,\n            serialization.PublicFormat.SubjectPublicKeyInfo)\n        con = Connection('foo', user_service)\n        self.environ['PATH_INFO'] = '\/auth'\n        # noinspection PyArgumentList\n        sign = user_service.private('foo').sign(\n            b'foo' + con.pub, padding.PKCS1v15(), hashes.SHA512())\n        data = json.dumps({'user': 'foo',\n                           'key': base64.urlsafe_b64encode(con.pub).decode(),\n                           'sign': base64.urlsafe_b64encode(sign).decode(),\n                           })\n        self.environ['wsgi.input'] = io.BytesIO(data.encode())\n        start_response = Mock()\n        with patch.object(self.crypt, 'user_service', user_service):\n            out = self.crypt(self.environ, start_response)\n        start_response.assert_called_once()\n        self.assertTrue(start_response.call_args[0][0].startswith('200 '))\n        data = next(iter(out))\n        remo_pub_bytes = base64.urlsafe_b64decode(data)\n        remo_pub = x448.X448PublicKey.from_public_bytes(remo_pub_bytes)\n        tempo = con.private.exchange(remo_pub)\n        key = base64.urlsafe_b64encode(self.kdf.derive(tempo))\n        self.assertEqual(self.environ['SESSION'].key, key)\n"}},"msg":"Pass the response code in first token to prevent tampering.\n\nEnsure that the response code has not be altered by a MITM attack.\nReject plain text responses for 200 Ok type responses."}},"https:\/\/github.com\/ymirsky\/CT-GAN":{"50c47bb03b99345c9257654aa2af8d644751cc86":{"url":"https:\/\/api.github.com\/repos\/ymirsky\/CT-GAN\/commits\/50c47bb03b99345c9257654aa2af8d644751cc86","html_url":"https:\/\/github.com\/ymirsky\/CT-GAN\/commit\/50c47bb03b99345c9257654aa2af8d644751cc86","message":"Upgraded Code and added GUI\n\n-Improved efficiency by interpolating the cubes and not the entire scan\n-Improved attack pipeline to handle saving to dicom and multiple injections\n-added GUI for interactive tampering demo","sha":"50c47bb03b99345c9257654aa2af8d644751cc86","keyword":"tampering attack","diff":"diff --git a\/1A_build_injector_trainset.py b\/1A_build_injector_trainset.py\nindex b955e1e..db39f00 100644\n--- a\/1A_build_injector_trainset.py\n+++ b\/1A_build_injector_trainset.py\n@@ -3,7 +3,7 @@\n if __name__ == '__main__':\n     # Init dataset builder for creating a dataset of evidence to inject\n     print('Initializing Dataset Builder for Evidence Injection')\n-    builder = Extractor(is_healthy_dataset=False, parallelize=True)\n+    builder = Extractor(is_healthy_dataset=False, parallelize=False)\n \n     # Extract training instances\n     # Source data location and save location is loaded from config.py\ndiff --git a\/1B_build_remover_trainset.py b\/1B_build_remover_trainset.py\nindex 48f424e..5fb720d 100644\n--- a\/1B_build_remover_trainset.py\n+++ b\/1B_build_remover_trainset.py\n@@ -2,7 +2,7 @@\n \n # Init dataset builder for creating a dataset of evidence to inject\n print('Initializing Dataset Builder for Evidence Removal')\n-builder = Extractor(is_healthy_dataset=True, parallelize=True)\n+builder = Extractor(is_healthy_dataset=True, parallelize=False)\n \n # Extract training instances\n # Source data location and save location is loaded from config.py\ndiff --git a\/2A_train_injector.py b\/2A_train_injector.py\nindex 5b80a28..79b1b0d 100644\n--- a\/2A_train_injector.py\n+++ b\/2A_train_injector.py\n@@ -2,5 +2,5 @@\n \n print(\"Training CT-GAN Injector...\")\n CTGAN_inj = Trainer(isInjector = True)\n-CTGAN_inj.train(epochs=200, batch_size=50, sample_interval=50)\n+CTGAN_inj.train(epochs=200, batch_size=32, sample_interval=50)\n print('Done.')\n\\ No newline at end of file\ndiff --git a\/2B_train_remover.py b\/2B_train_remover.py\nindex a82d823..1b39474 100644\n--- a\/2B_train_remover.py\n+++ b\/2B_train_remover.py\n@@ -2,5 +2,5 @@\n \n print(\"Training CT-GAN Remover...\")\n CTGAN_rem = Trainer(isInjector = False)\n-CTGAN_rem.train(epochs=200, batch_size=50, sample_interval=50)\n+CTGAN_rem.train(epochs=200, batch_size=32, sample_interval=50)\n print('Done.')\n\\ No newline at end of file\ndiff --git a\/3A_inject_evidence.py b\/3A_inject_evidence.py\nindex 737d05b..1924262 100644\n--- a\/3A_inject_evidence.py\n+++ b\/3A_inject_evidence.py\n@@ -3,7 +3,7 @@\n print('Removing Evidence...')\n \n # Init pipeline\n-injector = scan_manipulator(isInjector=True)\n+injector = scan_manipulator()\n \n # Load target scan (provide path to dcm or mhd file)\n injector.load_target_scan('path_to_target_scan')\n@@ -11,8 +11,8 @@\n # Inject at two locations (this version does not implement auto candidate location selection)\n vox_coord1 = np.array([53,213,400]) #z, y , x (x-y should be flipped if the coordinates were obtained from an image viewer such as RadiAnt)\n vox_coord2 = np.array([33,313,200])\n-injector.tamper(vox_coord1, isVox=True) #can supply realworld coord too\n-injector.tamper(vox_coord2, isVox=True)\n+injector.tamper(vox_coord1, action='inject', isVox=True) #can supply realworld coord too\n+injector.tamper(vox_coord2, action='inject', isVox=True)\n \n # Save scan\n-injector.save_tampered_scan('path_to_save_scan',output_type='dicom') #output can be dicom iff input was dicom, otherwise only numpy save is supported\n+injector.save_tampered_scan('path_to_save_scan',output_type='dicom') #output can be dicom or numpy\ndiff --git a\/3B_remove evidence.py b\/3B_remove evidence.py\nindex b3eb52c..233a2fa 100644\n--- a\/3B_remove evidence.py\t\n+++ b\/3B_remove evidence.py\t\n@@ -3,16 +3,16 @@\n print('Removing Evidence...')\n \n # Init pipeline\n-injector = scan_manipulator(isInjector=False)\n+remover = scan_manipulator()\n \n # Load target scan (provide path to dcm or mhd file)\n-injector.load_target_scan('path_to_target_scan')\n+remover.load_target_scan('path_to_target_scan')\n \n # Inject at two locations (this version does not implement auto candidate location selection)\n vox_coord1 = np.array([53,213,400]) #z, y , x (x-y should be flipped if the coordinates were obtained from an image viewer such as RadiAnt)\n vox_coord2 = np.array([33,313,200])\n-injector.tamper(vox_coord1, isVox=True) #can supply realworld coord too\n-injector.tamper(vox_coord2, isVox=True)\n+remover.tamper(vox_coord1, action='remove', isVox=True) #can supply realworld coord too\n+remover.tamper(vox_coord2, action='remove', isVox=True)\n \n # Save scan\n-injector.save_tampered_scan('path_to_save_scan',output_type='dicom') #output can be dicom iff input was dicom, otherwise only numpy save is supported\n+remover.save_tampered_scan('path_to_save_scan',output_type='dicom') #output can be dicom or numpy\ndiff --git a\/GUI.py b\/GUI.py\nnew file mode 100644\nindex 0000000..ffe1ead\n--- \/dev\/null\n+++ b\/GUI.py\n@@ -0,0 +1,249 @@\n+import sys\r\n+if len(sys.argv) == 2:\r\n+    if sys.argv[1] == '-h':\r\n+        print(\"python \"+sys.argv[0]+\" <path to dir of scans>\")\r\n+        print(\"python \"+sys.argv[0]+\" <path to dir of scans> <path to save dir>\")\r\n+        exit(1)\r\n+\r\n+import matplotlib.pyplot as plt\r\n+from matplotlib.widgets import Button\r\n+from procedures.attack_pipeline import *\r\n+from utils.equalizer import *\r\n+import matplotlib.animation as animation\r\n+import time\r\n+\r\n+class GUI(object):\r\n+    # If load_path is to a *.dcm or *.mhd file then only this scan is loaded\r\n+    # If load_path is to a directory, then all scans are loaded. It is assumed that each scan is in its own subdirectory.\r\n+    # save_path is the directory to save the tampered scans (as dicom)\r\n+    def __init__(self, load_path, save_path=None):\r\n+        # init manipulator\r\n+        self.savepath = save_path\r\n+        self.filepaths = self._load_paths(load_path)  # load all scans filepaths in path\r\n+        self.fileindex = 0\r\n+        self.manipulator = scan_manipulator()\r\n+        self.manipulator.load_target_scan(self.filepaths[self.fileindex])\r\n+        self.hist_state = True\r\n+        self.inject_coords = []\r\n+        self.remove_coords = []\r\n+\r\n+        # init plot\r\n+        self.eq = histEq(self.manipulator.scan)\r\n+        self.slices, self.cols, self.rows = self.manipulator.scan.shape\r\n+        self.ind = self.slices \/\/ 2\r\n+        self.pause_start = 0\r\n+        self.fig, self.ax = plt.subplots(1, 1, dpi=100)\r\n+        self.fig.suptitle('CT-GAN: Malicious Tampering of 3D Medical Imagery using Deep Learning\\nTool by Yisroel Mirsky', fontsize=14, fontweight='bold')\r\n+        plt.subplots_adjust(bottom=0.2)\r\n+        self.ani_direction = 'down'\r\n+        self.animation = None\r\n+        self.animation_state = True\r\n+        self.plot()\r\n+        self.ax.set_title(os.path.split(self.filepaths[self.fileindex])[-1]) #filename\r\n+\r\n+\r\n+        # register click\/scroll events\r\n+        self.action_state = 'inject' #default state\r\n+        self.fig.canvas.mpl_connect('button_press_event', self.onclick)\r\n+        self.fig.canvas.mpl_connect('scroll_event', self.onscroll)\r\n+\r\n+        # register buttons\r\n+        axanim = plt.axes([0.1, 0.21, 0.2, 0.075])\r\n+        self.banim = Button(axanim, 'Toggle Animation')\r\n+        self.banim.on_clicked(self.toggle_animation)\r\n+\r\n+        axinj = plt.axes([0.1, 0.05, 0.1, 0.075])\r\n+        axrem = plt.axes([0.21, 0.05, 0.1, 0.075])\r\n+        self.binj = Button(axinj, 'Inject')\r\n+        self.binj.on_clicked(self.inj_on)\r\n+        self.brem = Button(axrem, 'Remove')\r\n+        self.brem.on_clicked(self.rem_on)\r\n+\r\n+        axhist = plt.axes([0.35, 0.05, 0.2, 0.075])\r\n+        self.bhist = Button(axhist, 'Toggle HistEQ')\r\n+        self.bhist.on_clicked(self.hist)\r\n+\r\n+        axprev = plt.axes([0.59, 0.05, 0.1, 0.075])\r\n+        axsave = plt.axes([0.7, 0.05, 0.1, 0.075])\r\n+        axnext = plt.axes([0.81, 0.05, 0.1, 0.075])\r\n+        self.bnext = Button(axnext, 'Next')\r\n+        self.bnext.on_clicked(self.next)\r\n+        self.bprev = Button(axprev, 'Previous')\r\n+        self.bprev.on_clicked(self.prev)\r\n+        self.bsave = Button(axsave, 'Save')\r\n+        self.bsave.on_clicked(self.save)\r\n+        self.maximize_window()\r\n+        self.update()\r\n+        plt.show()\r\n+\r\n+    def _load_paths(self,path):\r\n+        filepaths = []\r\n+        # load single scan?\r\n+        if (path.split('.')[-1] == \"dcm\") or (path.split('.')[-1] == \"mhd\"):\r\n+            filepaths.append(path)\r\n+            return filepaths\r\n+        # try load directory of scans...\r\n+        files = os.listdir(path)\r\n+        for file in files:\r\n+            if os.path.isdir(file):\r\n+                subdir = os.path.join(path,file)\r\n+                subdir_files = os.listdir(subdir)\r\n+                if subdir_files[0].split('.')[-1] == \"dcm\": #folder contains dicom\r\n+                    filepaths.append(os.path.join(path,subdir))\r\n+                elif (subdir_files[0].split('.')[-1] == \"mhd\") or (subdir_files[0].split('.')[-1] == \"raw\"): # MHD\r\n+                    filepaths.append(os.path.join(path,subdir,subdir_files[0]))\r\n+            elif file.split('.')[-1] == \"mhd\":\r\n+                filepaths.append(os.path.join(path,file))\r\n+        return filepaths\r\n+\r\n+    def onclick(self, event):\r\n+        # print('%s click: button=%d, x=%d, y=%d, xdata=%f, ydata=%f' %\r\n+        #       ('double' if event.dblclick else 'single', event.button,\r\n+        #        event.x, event.y, event.xdata, event.ydata))\r\n+        if event.xdata is not None:\r\n+            coord = np.array([self.ind,event.ydata,event.xdata],dtype=int)\r\n+            if coord[1] > 0 and coord[2] > 0:\r\n+                self.pause_start = np.Inf #pause while working\r\n+                if self.action_state == 'inject':\r\n+                    self.ax.set_title(\"Injecting...\")\r\n+                    self.im.axes.figure.canvas.draw()\r\n+                    self.manipulator.tamper(coord, action='inject', isVox=True)\r\n+                    self.inject_coords.append(coord)\r\n+                else:\r\n+                    self.ax.set_title(\"Removing...\")\r\n+                    self.im.axes.figure.canvas.draw()\r\n+                    self.manipulator.tamper(coord, action='remove', isVox=True)\r\n+                    self.remove_coords.append(coord)\r\n+                self.pause_start = time.time() #pause few secs to see result before continue\r\n+                self.ax.set_title(os.path.split(self.filepaths[self.fileindex])[-1])  # filename\r\n+                self.update()\r\n+\r\n+    def onscroll(self, event):\r\n+        if event.button == 'up':\r\n+            self.ind = (self.ind + 1) % self.slices\r\n+        else:\r\n+            self.ind = (self.ind - 1) % self.slices\r\n+        self.update()\r\n+\r\n+    def toggle_animation(self, event):\r\n+        self.animation_state = not self.animation_state\r\n+        if self.animation_state:\r\n+            self.pause_start = 0\r\n+        else:\r\n+            self.pause_start = np.Inf\r\n+\r\n+    def inj_on(self, event):\r\n+        self.action_state = 'inject'\r\n+\r\n+    def rem_on(self, event):\r\n+        self.action_state = 'remove'\r\n+\r\n+    def hist(self, event):\r\n+        self.hist_state = not self.hist_state\r\n+        self.plot()\r\n+        self.update()\r\n+\r\n+    def next(self, event):\r\n+        self.fileindex = (self.fileindex + 1) % len(self.filepaths)\r\n+        self.loadscan(self.fileindex)\r\n+\r\n+    def prev(self, event):\r\n+        self.fileindex = (self.fileindex - 1) % len(self.filepaths)\r\n+        self.loadscan(self.fileindex)\r\n+\r\n+    def save(self, event):\r\n+        if self.savepath is not None:\r\n+            self.ax.set_title(\"Saving...\")\r\n+            self.im.axes.figure.canvas.draw()\r\n+            uuid = os.path.split(self.filepaths[self.fileindex])[-1][:-4]\r\n+            #save scan\r\n+            self.manipulator.save_tampered_scan(os.path.join(self.savepath,uuid),output_type='dicom')\r\n+            #save coords\r\n+            file_exists = False\r\n+            if os.path.exists(os.path.join(self.savepath,\"tamper_coordinates.csv\")):\r\n+                file_exists = True\r\n+            f = open(os.path.join(self.savepath,\"tamper_coordinates.csv\"),\"a+\")\r\n+            load_filename = os.path.split(self.filepaths[self.fileindex])[-1]\r\n+            if not file_exists:\r\n+                f.write(\"filename, x, y, z, tamper_type\\n\") #header\r\n+            for coord in self.inject_coords:\r\n+                f.write(load_filename+\", \"+str(coord[2])+\", \"+str(coord[1])+\", \"+str(coord[0])+\", \"+\"inject\\n\")\r\n+            for coord in self.remove_coords:\r\n+                f.write(load_filename+\", \"+str(coord[2])+\", \"+str(coord[1])+\", \"+str(coord[0])+\", \"+\"remove\\n\")\r\n+            f.close()\r\n+            self.ax.set_title(load_filename)  # filename\r\n+            self.im.axes.figure.canvas.draw()\r\n+\r\n+    def update(self):\r\n+        if self.hist_state:\r\n+            self.im.set_data(self.eq.equalize(self.manipulator.scan[self.ind,:,:]))\r\n+        else:\r\n+            self.im.set_data(self.manipulator.scan[self.ind,:,:])\r\n+        self.ax.set_ylabel('slice %s' % self.ind)\r\n+        self.im.axes.figure.canvas.draw()\r\n+\r\n+    def loadscan(self,fileindex):\r\n+        #load screen\r\n+        self.im.set_data(np.ones((self.cols,self.rows))*-1000)\r\n+        self.ax.set_title(\"Loading...\")\r\n+        self.im.axes.figure.canvas.draw()\r\n+        self.remove_coords.clear()\r\n+        self.inject_coords.clear()\r\n+        #load scan\r\n+        self.manipulator.load_target_scan(self.filepaths[fileindex])\r\n+        self.slices, self.cols, self.rows = self.manipulator.scan.shape\r\n+        self.ind = self.slices\/\/2\r\n+        self.ax.clear()\r\n+        self.eq = histEq(self.manipulator.scan)\r\n+        self.plot()\r\n+        self.ax.set_title(os.path.split(self.filepaths[fileindex])[-1]) #filename\r\n+        self.ax.set_ylabel('slice %s' % self.ind)\r\n+        self.im.axes.figure.canvas.draw()\r\n+\r\n+    def plot(self):\r\n+        self.ax.clear()\r\n+        if self.hist_state:\r\n+            self.im = self.ax.imshow(self.eq.equalize(self.manipulator.scan[self.ind,:,:]),cmap=\"bone\")#, cmap=\"bone\", vmin=-1000, vmax=1750)\r\n+        else:\r\n+            self.im = self.ax.imshow(self.manipulator.scan[self.ind,:,:], cmap=\"bone\", vmin=-1000, vmax=1750)\r\n+        self.animation = animation.FuncAnimation(self.fig, self.animate, interval=100)\r\n+\r\n+    def animate(self,i):\r\n+        if self.animation_state:\r\n+            if time.time() - self.pause_start > 1:\r\n+                if self.ind == self.slices-1:\r\n+                    self.ani_direction = 'up'\r\n+                elif self.ind == 0:\r\n+                    self.ani_direction = 'down'\r\n+                if self.ani_direction == 'up':\r\n+                    self.ind-=1\r\n+                else:\r\n+                    self.ind+=1\r\n+                self.update()\r\n+\r\n+    def maximize_window(self):\r\n+        try: #'QT4Agg'\r\n+            figManager = plt.get_current_fig_manager()\r\n+            figManager.window.showMaximized()\r\n+        except:\r\n+            try: #'TkAgg'\r\n+                mng = plt.get_current_fig_manager()\r\n+                mng.window.state('zoomed')\r\n+            except:\r\n+                try: #'wxAgg'\r\n+                    mng = plt.get_current_fig_manager()\r\n+                    mng.frame.Maximize(True)\r\n+                except:\r\n+                    print(\"Could not maximize window\")\r\n+\r\n+if (len(sys.argv) == 1) or (len(sys.argv) > 3):\r\n+    loadpath = \"data\\\\healthy_scans\"\r\n+    savepath = \"data\\\\tampered_scans\"\r\n+if len(sys.argv) == 2:\r\n+    loadpath = sys.argv[1]\r\n+    savepath = \"data\\\\tampered_scans\"\r\n+if len(sys.argv) == 3:\r\n+    loadpath = sys.argv[1]\r\n+    savepath = sys.argv[2]\r\n+\r\n+gui = GUI(load_path=loadpath,save_path=savepath)\r\ndiff --git a\/README.md b\/README.md\nindex 0c7aa70..c1c74be 100644\n--- a\/README.md\n+++ b\/README.md\n@@ -3,7 +3,7 @@ In this repository you will find a Keras implementation of CT-GAN: A framework f\n \n *Yisroel Mirsky, Tom Mahler, Ilan Shelef, and Yuval Elovici. CT-GAN: Malicious Tampering of 3D Medical Imagery using Deep Learning. 28th USENIX Security Symposium (USENIX Security 19)*\n \n-([full paper here](https:\/\/arxiv.org\/abs\/1901.03597))\n+([full paper here](https:\/\/www.usenix.org\/system\/files\/sec19-mirsky_0.pdf))\n \n **Disclaimer**: This code has been published for research purposes only. It is our hope that with this code, others will be able to better understand this threat and find better ways to mitigate it.\n  \n@@ -73,8 +73,7 @@ in a 64x64x16 cuboid:*\n \n **Limitations**\n \n-* this version will not automatically locate candicate injection\/removal locations within a target scan. \n-* during the tampering process, we scale (interpolate) the entire scan as opposed to just the candidate location (cuboid). This means tampering takes longer to process than necessary.\n+* this version will not automatically locate candicate injection\/removal locations within a target scan. Please see our paper for details on this algorithm.\n \n \n # The CT-GAN Code\n@@ -219,7 +218,7 @@ optional arguments:\n                         The output format to save the tamepred scan: 'dicom' or 'numpy'. Default is 'dicom'.\n \n To change other settings, check config.py\n-Note, this version is significantly slower since it will scale (interpolate) the entire scan and not just the target cuboid. For more information, please read our paper:\n+For more information, please read our paper:\n CT-GAN: Malicious Tampering of 3D Medical Imagery using Deep Learning.\n Yisroel Mirsky, Tom Mahler, Ilan Shelef, and Yuval Elovici\n \n@@ -239,11 +238,17 @@ If you use the source code in any way, please cite:\n \n *Yisroel Mirsky, Tom Mahler, Ilan Shelef, and Yuval Elovici. 28th USENIX Security Symposium (USENIX Security 19)*\n ```\n-@inproceedings{mirsky2019ct,\n-  title={CT-GAN: Malicious Tampering of 3D Medical Imagery using Deep Learning},\n-  author={Mirsky, Yisroel and Mahler, Tom and Shelef, Ilan and Elovici, Yuval},\n-  booktitle={28th USENIX Security Symposium (USENIX Security 19)},\n-  year={2019}\n+@inproceedings {236284,\n+author = {Yisroel Mirsky and Tom Mahler and Ilan Shelef and Yuval Elovici},\n+title = {CT-GAN: Malicious Tampering of 3D Medical Imagery using Deep Learning},\n+booktitle = {28th {USENIX} Security Symposium ({USENIX} Security 19)},\n+year = {2019},\n+isbn = {978-1-939133-06-9},\n+address = {Santa Clara, CA},\n+pages = {461--478},\n+url = {https:\/\/www.usenix.org\/conference\/usenixsecurity19\/presentation\/mirsky},\n+publisher = {{USENIX} Association},\n+month = aug,\n }\n ```\n \ndiff --git a\/procedures\/attack_pipeline.py b\/procedures\/attack_pipeline.py\nindex 40d5d79..9070429 100644\n--- a\/procedures\/attack_pipeline.py\n+++ b\/procedures\/attack_pipeline.py\n@@ -38,32 +38,40 @@\n # in this version: dicom->dicom, dicom->numpy, mhd\/raw->numpy supported\n \n class scan_manipulator:\n-    def __init__(self,isInjector=True):\n+    def __init__(self):\n         print(\"===Init Tamperer===\")\n         self.scan = None\n-        self.mal_scan_resized = None # temporary storage for tamepring until time to scale back down\n-        self.isInjector = isInjector\n         self.load_path = None\n         self.m_zlims = config['mask_zlims']\n         self.m_ylims = config['mask_ylims']\n         self.m_xlims = config['mask_xlims']\n-        self.tamper_coords = []\n \n         #load model and parameters\n-        if self.isInjector:\n-            self.model_path = config['modelpath_inject']\n+        self.model_inj_path = config['modelpath_inject']\n+        self.model_rem_path = config['modelpath_remove']\n+\n+        #load models\n+        print(\"Loading models\")\n+        if os.path.exists(os.path.join(self.model_inj_path,\"G_model.h5\")):\n+            self.generator_inj = load_model(os.path.join(self.model_inj_path,\"G_model.h5\"))\n+            # load normalization params\n+            self.norm_inj = np.load(os.path.join(self.model_inj_path, 'normalization.npy'))\n+            # load equalization params\n+            self.eq_inj = histEq([], path=os.path.join(self.model_inj_path, 'equalization.pkl'))\n+            print(\"Loaded Injector Model\")\n         else:\n-            self.model_path = config['modelpath_remove']\n-\n-        #load generator\n-        print(\"Loading model\")\n-        self.generator = load_model(os.path.join(self.model_path,\"G_model.h5\"))\n-\n-        #load normalization params\n-        self.norm = np.load(os.path.join(self.model_path,'normalization.npy'))\n-\n-        # load equalization params\n-        self.eq = histEq([], path = os.path.join(self.model_path,'equalization.pkl'))\n+            self.generator_inj = None\n+            print(\"Failed to Load Injector Model\")\n+        if os.path.exists(os.path.join(self.model_rem_path,\"G_model.h5\")):\n+            self.generator_rem = load_model(os.path.join(self.model_rem_path,\"G_model.h5\"))\n+            # load normalization params\n+            self.norm_rem = np.load(os.path.join(self.model_rem_path, 'normalization.npy'))\n+            # load equalization params\n+            self.eq_rem = histEq([], path=os.path.join(self.model_rem_path, 'equalization.pkl'))\n+            print(\"Loaded Remover Model\")\n+        else:\n+            self.generator_rem = None\n+            print(\"Failed to Load Remover Model\")\n \n     # loads dicom\/mhd to be tampered\n     # Provide path to a *.dcm file or the *mhd file. The contaitning folder should have the other slices)\n@@ -71,9 +79,7 @@ def load_target_scan(self, load_path):\n         self.load_path = load_path\n         print('Loading scan')\n         self.scan, self.scan_spacing, self.scan_orientation, self.scan_origin, self.scan_raw_slices = load_scan(load_path)\n-        print(\"Scaling up scan...\")\n-        self.scan_resized, self.resize_factor = scale_scan(self.scan, self.scan_spacing)\n-        self.mal_scan_resized = np.copy(self.scan_resized)\n+        self.scan = self.scan.astype(float)\n \n     # saves tampered scan as 'dicom' series or 'numpy' serialization\n     def save_tampered_scan(self, save_dir, output_type='dicom'):\n@@ -81,49 +87,61 @@ def save_tampered_scan(self, save_dir, output_type='dicom'):\n             print('Cannot save: load a target scan first.')\n             return\n \n-        # scale scan back down and add noise touchups\n-        if len(self.tamper_coords) > 0:\n-            self._touch_up_scan()\n-            #self.tamper_coords.clear()\n-\n         print('Saving scan')\n-        if (output_type == 'dicom') and (self.load_path.split('.')[-1]==\"mhd\"):\n-            raise Exception('Save file error: mhd -> dicom conversion currently unsupported. Either supply a dicom scan or set the output type to numpy.')\n-            #save with same per slice metadata as source\n-        if output_type == \"dicom\":\n-            save_dicom(self.scan, origional_raw_slices=self.scan_raw_slices, dst_directory=save_dir)\n-        else:\n+        if output_type == 'dicom':\n+            if self.load_path.split('.')[-1]==\"mhd\":\n+                toDicom(save_dir=save_dir, img_array=self.scan, pixel_spacing=self.scan_spacing, orientation=self.scan_orientation)\n+            else: #input was dicom\n+                save_dicom(self.scan, origional_raw_slices=self.scan_raw_slices, dst_directory=save_dir)\n+        else: #save as numpy\n+            os.makedirs(save_dir, exist_ok=True)\n             np.save(os.path.join(save_dir,'tampered_scan.np'),self.scan)\n         print('Done.')\n \n \n     # tamper loaded scan at given voxel (index) coordinate\n     # coord: E.g. vox: slice_indx, y_indx, x_indx    world: -324.3, 23, -234\n-    def tamper(self, coord, isVox=True):\n+    # action: 'inject' or 'remove'\n+    def tamper(self, coord, action=\"inject\", isVox=True):\n         if self.scan is None:\n             print('Cannot tamper: load a target scan first.')\n             return\n+        if (action == 'inject') and (self.generator_inj is None):\n+            print('Cannot inject: no injection model loaded.')\n+            return\n+        if (action == 'remove') and (self.generator_rem is None):\n+            print('Cannot inject: no removal model loaded.')\n+            return\n \n-        print('===Injecting Evidence===')\n+        if action == 'inject':\n+            print('===Injecting Evidence===')\n+        else:\n+            print('===Removing Evidence===')\n         if not isVox:\n             coord = world2vox(coord, self.scan_spacing, self.scan_orientation, self.scan_origin)\n \n-        ### Scale coordinate\n-        vox_coord_s = scale_vox_coord(coord, self.scan_spacing)\n-\n         ### Cut Location\n         print(\"Cutting out target region\")\n-        clean_cube = cutCube(self.mal_scan_resized, vox_coord_s, config[\"cube_shape\"])\n+        cube_shape = get_scaled_shape(config[\"cube_shape\"], 1\/self.scan_spacing)\n+        clean_cube_unscaled = cutCube(self.scan, coord, cube_shape)\n+        clean_cube, resize_factor = scale_scan(clean_cube_unscaled,self.scan_spacing)\n+        # Store backup reference\n+        sdim = int(np.max(cube_shape)*1.3)\n+        clean_cube_unscaled2 = cutCube(self.scan, coord, np.array([sdim,sdim,sdim])) #for noise touch ups later\n \n         ### Normalize\/Equalize Location\n         print(\"Normalizing sample\")\n-        clean_cube_eq = self.eq.equalize(clean_cube)\n-        clean_cube_norm = (clean_cube_eq - self.norm[0]) \/ ((self.norm[2] - self.norm[1]))\n+        if action == 'inject':\n+            clean_cube_eq = self.eq_inj.equalize(clean_cube)\n+            clean_cube_norm = (clean_cube_eq - self.norm_inj[0]) \/ ((self.norm_inj[2] - self.norm_inj[1]))\n+        else:\n+            clean_cube_eq = self.eq_rem.equalize(clean_cube)\n+            clean_cube_norm = (clean_cube_eq - self.norm_rem[0]) \/ ((self.norm_rem[2] - self.norm_rem[1]))\n \n         ########  Inject Cancer   ##########\n \n         ### Inject\/Remove evidence\n-        if self.isInjector:\n+        if action == 'inject':\n             print(\"Injecting evidence\")\n         else:\n             print(\"Removing evidence\")\n@@ -131,15 +149,22 @@ def tamper(self, coord, isVox=True):\n         x = np.copy(clean_cube_norm)\n         x[self.m_zlims[0]:self.m_zlims[1], self.m_xlims[0]:self.m_xlims[1], self.m_ylims[0]:self.m_ylims[1]] = 0\n         x = x.reshape((1, config['cube_shape'][0], config['cube_shape'][1], config['cube_shape'][2], 1))\n-        x_mal = self.generator.predict([x])\n+        if action == 'inject':\n+            x_mal = self.generator_inj.predict([x])\n+        else:\n+            x_mal = self.generator_rem.predict([x])\n         x_mal = x_mal.reshape(config['cube_shape'])\n \n         ### De-Norm\/De-equalize\n         print(\"De-normalizing sample\")\n         x_mal[x_mal > .5] = .5  # fix boundry overflow\n         x_mal[x_mal < -.5] = -.5\n-        mal_cube_eq = x_mal * ((self.norm[2] - self.norm[1])) + self.norm[0]\n-        mal_cube = self.eq.dequalize(mal_cube_eq)\n+        if action == 'inject':\n+            mal_cube_eq = x_mal * ((self.norm_inj[2] - self.norm_inj[1])) + self.norm_inj[0]\n+            mal_cube = self.eq_inj.dequalize(mal_cube_eq)\n+        else:\n+            mal_cube_eq = x_mal * ((self.norm_rem[2] - self.norm_rem[1])) + self.norm_rem[0]\n+            mal_cube = self.eq_rem.dequalize(mal_cube_eq)\n         # Correct for pixel norm error\n         # fix overflow\n         bad = np.where(mal_cube > 2000)\n@@ -152,59 +177,50 @@ def tamper(self, coord, isVox=True):\n \n         ### Paste Location\n         print(\"Pasting sample into scan\")\n-        self.mal_scan_resized = pasteCube(self.mal_scan_resized, mal_cube, vox_coord_s)\n-        self.tamper_coords.append(coord)\n-        print('Done.')\n-\n-\n-    def _touch_up_scan(self):\n-        ### Rescale\n-        print(\"Scaling down scan...\")\n-        mal_scan, resize_factor = scale_scan(self.mal_scan_resized, 1 \/ self.scan_spacing)\n+        mal_cube_scaled, resize_factor = scale_scan(mal_cube,1\/self.scan_spacing)\n+        self.scan = pasteCube(self.scan, mal_cube_scaled, coord)\n \n         ### Noise Touch-ups\n         print(\"Adding noise touch-ups...\")\n-        for coord in self.tamper_coords:\n-            noise_map_dim = (config['cube_shape']*2).astype(int)\n-            ben_cube_ext = cutCube(self.scan, coord, noise_map_dim)\n-            mal_cube_ext = cutCube(mal_scan, coord, noise_map_dim)\n-            local_sample = cutCube(self.scan, coord, config[\"cube_shape\"])\n-\n-            # Init Touch-ups\n-            if self.isInjector:\n-                noisemap = np.random.randn(150, 200, 300) * np.std(local_sample[local_sample < -600]) * .6\n-                kernel_size = 3\n-                factors = sigmoid((mal_cube_ext + 700) \/ 70)\n-                k = kern01(mal_cube_ext.shape[0], kernel_size)\n-                for i in range(factors.shape[0]):\n-                    factors[i, :, :] = factors[i, :, :] * k\n-            else:\n-                noisemap = np.random.randn(150, 200, 200) * 30\n-                kernel_size = .1\n-                k = kern01(mal_cube_ext.shape[0], kernel_size)\n-                factors = None\n-\n-            # Perform touch-ups\n-            if config['copynoise']:  # copying similar noise from hard coded location over this lcoation (usually more realistic)\n-                benm = cutCube(self.scan, np.array([int(self.scan.shape[0] \/ 2), int(self.scan.shape[1]*.43), int(self.scan.shape[2]*.27)]), noise_map_dim)\n-                x = np.copy(benm)\n-                x[x > -800] = np.mean(x[x < -800])\n-                noise = x - np.mean(x)\n-            else:  # gaussian interpolated noise is used\n-                rf = np.ones((3,)) * (60 \/ np.std(local_sample[local_sample < -600])) * 1.3\n-                np.random.seed(np.int64(time.time()))\n-                noisemap_s = scipy.ndimage.interpolation.zoom(noisemap, rf, mode='nearest')\n-                noise = noisemap_s[:noise_map_dim, :noise_map_dim, :noise_map_dim]\n-            mal_cube_ext += noise\n-\n-            if self.isInjector:  # Injection\n-                final_cube_s = np.maximum((mal_cube_ext * factors + ben_cube_ext * (1 - factors)), ben_cube_ext)\n-            else: #Removal\n-                minv = np.min((np.min(mal_cube_ext), np.min(ben_cube_ext)))\n-                final_cube_s = (mal_cube_ext + minv) * k + (ben_cube_ext + minv) * (1 - k) - minv\n-\n-            mal_scan = pasteCube(mal_scan, final_cube_s, coord)\n-        self.scan = mal_scan\n+        noise_map_dim = clean_cube_unscaled2.shape\n+        ben_cube_ext = clean_cube_unscaled2\n+        mal_cube_ext = cutCube(self.scan, coord, noise_map_dim)\n+        local_sample = clean_cube_unscaled\n+\n+        # Init Touch-ups\n+        if action == 'inject': #inject type\n+            noisemap = np.random.randn(150, 200, 300) * np.std(local_sample[local_sample < -600]) * .6\n+            kernel_size = 3\n+            factors = sigmoid((mal_cube_ext + 700) \/ 70)\n+            k = kern01(mal_cube_ext.shape[0], kernel_size)\n+            for i in range(factors.shape[0]):\n+                factors[i, :, :] = factors[i, :, :] * k\n+        else: #remove type\n+            noisemap = np.random.randn(150, 200, 200) * 30\n+            kernel_size = .1\n+            k = kern01(mal_cube_ext.shape[0], kernel_size)\n+            factors = None\n+\n+        # Perform touch-ups\n+        if config['copynoise']:  # copying similar noise from hard coded location over this lcoation (usually more realistic)\n+            benm = cutCube(self.scan, np.array([int(self.scan.shape[0] \/ 2), int(self.scan.shape[1]*.43), int(self.scan.shape[2]*.27)]), noise_map_dim)\n+            x = np.copy(benm)\n+            x[x > -800] = np.mean(x[x < -800])\n+            noise = x - np.mean(x)\n+        else:  # gaussian interpolated noise is used\n+            rf = np.ones((3,)) * (60 \/ np.std(local_sample[local_sample < -600])) * 1.3\n+            np.random.seed(np.int64(time.time()))\n+            noisemap_s = scipy.ndimage.interpolation.zoom(noisemap, rf, mode='nearest')\n+            noise = noisemap_s[:noise_map_dim, :noise_map_dim, :noise_map_dim]\n+        mal_cube_ext += noise\n+\n+        if action == 'inject':  # Injection\n+            final_cube_s = np.maximum((mal_cube_ext * factors + ben_cube_ext * (1 - factors)), ben_cube_ext)\n+        else: #Removal\n+            minv = np.min((np.min(mal_cube_ext), np.min(ben_cube_ext)))\n+            final_cube_s = (mal_cube_ext + minv) * k + (ben_cube_ext + minv) * (1 - k) - minv\n+\n+        self.scan = pasteCube(self.scan, final_cube_s, coord)\n         print('touch-ups complete')\n \n \ndiff --git a\/procedures\/datasetBuilder.py b\/procedures\/datasetBuilder.py\nindex 300dbac..27996f1 100644\n--- a\/procedures\/datasetBuilder.py\n+++ b\/procedures\/datasetBuilder.py\n@@ -40,7 +40,7 @@ class Extractor:\n     #   if filename has the *.mhd extension, then mhd\/raw is assumed (all mdh\/raw files should be in same directory)\n     #parallelize: inidates whether the processign should be run over multiple CPU cores\n     #coordSystem: if the coords are the matrix indexes, then choose 'vox'. If the coords are realworld locations, then choose 'world'\n-    def __init__(self, is_healthy_dataset, src_dir=None, coords_csv_path=None, dst_path=None, norm_save_dir=None, parallelize=True, coordSystem=None):\n+    def __init__(self, is_healthy_dataset, src_dir=None, coords_csv_path=None, dst_path=None, norm_save_dir=None, parallelize=False, coordSystem=None):\n         self.parallelize = parallelize\n         if coordSystem is None:\n             self.coordSystem = config['traindata_coordSystem']\n@@ -74,7 +74,10 @@ def extract(self,plot=True):\n         else:\n             X = []\n             for job in J:\n-                X.append(self._processJob(job))\n+                try:\n+                    X.append(self._processJob(job))\n+                except:\n+                    print(\"Failed to process sample\")\n         instances = np.array(list(itertools.chain.from_iterable(X))) #each job creates a batch of augmented instances: so collect hem\n \n         # Histogram Equalization:\n@@ -111,23 +114,24 @@ def _get_instances_from_scan(self, scan_path, coord, cube_shape, coordSystem):\n         # load scan data\n         scan, spacing, orientation, origin, raw_slices = load_scan(scan_path)\n         # scale the image\n-        scan_resized, resize_factor = scale_scan(scan, spacing)\n+        #scan_resized, resize_factor = scale_scan(scan, spacing)\n         # compute sample coords as vox\n         if coordSystem == 'world': #convert from world to vox\n             coord = world2vox(coord,spacing,orientation,origin)\n         elif coordSystem != 'vox':\n             raise Exception(\"Coordinate conversion error: you can only select world or vox\")\n-        coordn = scale_vox_coord(coord, spacing)  # ccord relative to scaled scan\n+        #coordn = scale_vox_coord(coord, spacing)  # ccord relative to scaled scan\n \n         # extract instances\n         X = []\n-        init_cube_size = cube_shape + 8 # add extra borders for augmentations\n-        x = cutCube(scan_resized, coordn, init_cube_size, padd=-1000)  # cut out cancer with extra boundry\n+        init_cube_shape = get_scaled_shape(cube_shape + 8, 1\/spacing)\n+        clean_cube_unscaled = cutCube(scan, coord, init_cube_shape, padd=-1000)\n+        x, resize_factor = scale_scan(clean_cube_unscaled,spacing)\n         # perform data augmentations to generate more instances\n         Xaug = self._augmentInstance(x)\n         # trim the borders to get the actual desired shape\n         for xa in Xaug:\n-            center = np.array(init_cube_size\/2, dtype=int)\n+            center = np.array(x.shape)\/\/2\n             X.append(cutCube(xa, center, cube_shape, padd=-1000))  # cut out  augmented cancer without extra boundry\n         return X\n \n@@ -157,7 +161,6 @@ def _augmentInstance(self, x0):\n \n     def plot_sample(self,X):\n         import matplotlib.pyplot as plt\n-        plt.ion()\n         r, c = 3, 10\n         batch = X[np.random.permutation(len(X))[:30]]\n         fig, axs = plt.subplots(r, c, figsize=np.array([30, 10]) * .5)\ndiff --git a\/procedures\/trainer.py b\/procedures\/trainer.py\nindex 51fe316..acc97ca 100644\n--- a\/procedures\/trainer.py\n+++ b\/procedures\/trainer.py\n@@ -21,8 +21,11 @@\n # SOFTWARE.\n \n from __future__ import print_function, division\n-\n from config import *  # user configuration in config.py\n+import os\n+os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n+os.environ[\"CUDA_VISIBLE_DEVICES\"] = config['gpus']\n+\n from utils.dataloader import DataLoader\n from keras.layers import Input, Dropout, Concatenate, Cropping3D\n from keras.layers import BatchNormalization\n@@ -31,12 +34,8 @@\n from keras.models import Model\n from keras.optimizers import Adam\n import matplotlib.pyplot as plt\n-from utils.equalizer import *\n import datetime\n import numpy as np\n-import os\n-\n-os.environ[\"CUDA_VISIBLE_DEVICES\"] = config['gpus']\n \n import tensorflow as tf\n import keras.backend.tensorflow_backend as ktf\ndiff --git a\/tamper.py b\/tamper.py\nindex b2a7d1b..3cfff37 100644\n--- a\/tamper.py\n+++ b\/tamper.py\n@@ -34,20 +34,20 @@\n     parser.add_argument('-a','--action', choices=['inject','remove'], required=True, help=\"The directory (path) to save the tampered scan.\")\n     parser.add_argument('-c','--coord', required=True,nargs='*',help=\"The selected coordinate(s) in the target scan to inject evidence. You must provide one or more coordinates in z,y,x format with no spaces. \\nExample (inject at two locations): python tamper.py -t patient.mhd -d outdir -c 123,324,401 53,201,441\")\n     parser.add_argument('-s', '--system', choices=['vox','world'], default='vox',help=\"Indicate the coordinate system of the supplied target coordinates: 'vox' or 'world'. Default is 'vox'.\")\n-    parser.add_argument('-f', '--outformat',choices=['dicom','numpy'],default='dicom',help=\"The output format to save the tamepred scan: 'dicom' or 'numpy'. Note, dicom is only avaliable if the target scan was dicom.  Default is 'dicom'.\")\n+    parser.add_argument('-f', '--outformat',choices=['dicom','numpy'],default='dicom',help=\"The output format to save the tamepred scan: 'dicom' or 'numpy'. Default is 'dicom'.\")\n     args = parser.parse_known_args()[0]\n \n     from procedures.attack_pipeline import *\n \n     # Init pipeline\n-    injector = scan_manipulator(isInjector = (args.action=='inject'))\n+    injector = scan_manipulator()\n \n     # Load target scan (provide path to dcm file\/dir, or mhd file)\n     injector.load_target_scan(load_path=args.target)#('path_to_target_scan')\n \n     for coord in args.coord:\n         coorda = np.array(coord.split(','),dtype=float)\n-        injector.tamper(coorda, isVox=(args.system=='vox'))\n+        injector.tamper(coorda, action=args.action, isVox=(args.system=='vox'))\n \n     # Save scan\n     injector.save_tampered_scan(save_dir=args.destination,output_type=args.outformat) #output can be dicom iff input was dicom, otherwise only numpy save is supported\ndiff --git a\/utils\/dicom_utils.py b\/utils\/dicom_utils.py\nindex 9f769c4..978a17d 100644\n--- a\/utils\/dicom_utils.py\n+++ b\/utils\/dicom_utils.py\n@@ -70,6 +70,32 @@ def save_dicom(scan, origional_raw_slices, dst_directory): # \\\\dfds\\\\ format\n         slice.PixelData = slice.pixel_array.tobytes()\n         slice.save_as(os.path.join(dst_directory,slice.filename))#.dcmwrite(os.path.join(dst_directory,slice.filename),slice)\n \n+#img_array: 3d numpy matrix, z,y,x\n+def toDicom(save_dir, img_array,  pixel_spacing, orientation):\n+    ref_scan = pydicom.read_file('utils\/ref_scan.dcm') #slice from soem real scan so we can copy the meta data\n+\n+    #write dcm file for each slice in scan\n+    for i, slice in enumerate(img_array):\n+        ref_scan.pixel_array.flat = img_array[i,:,:].flat\n+        ref_scan.PixelData = ref_scan.pixel_array.tobytes()\n+        ref_scan.RefdSOPInstanceUID = str(i)\n+        ref_scan.SOPInstanceUID = str(i)\n+        ref_scan.InstanceNumber = str(i)\n+        ref_scan.SliceLocation = str(i)\n+        ref_scan.ImagePositionPatient[2] = str(i*pixel_spacing[0])\n+        ref_scan.RescaleIntercept = 0\n+        ref_scan.Rows = img_array.shape[1]\n+        ref_scan.Columns = img_array.shape[2]\n+        ref_scan.PixelSpacing = [str(pixel_spacing[2]),str(pixel_spacing[1])]\n+        ref_scan.SliceThickness = pixel_spacing[0]\n+        #Pixel Spacing                       DS: ['0.681641', '0.681641']\n+        #Image Position (Patient)            DS: ['-175.500000', '-174.500000', '49']\n+        #Image Orientation (Patient)         DS: ['1.000000', '0.000000', '0.000000', '0.000000', '1.000000', '0.000000']\n+        #Rows                                US: 512\n+        #Columns                             US: 512\n+        os.makedirs(save_dir,exist_ok=True)\n+        ref_scan.save_as(os.path.join(save_dir,str(i)+'.dcm'))\n+\n def scale_scan(scan,spacing,factor=1):\n     resize_factor = factor * spacing\n     new_real_shape = scan.shape * resize_factor\n@@ -79,6 +105,11 @@ def scale_scan(scan,spacing,factor=1):\n     scan_resized = scipy.ndimage.interpolation.zoom(scan, real_resize_factor, mode='nearest')\n     return scan_resized, resize_factor\n \n+# get the shape of an orthotope in 1:1:1 ratio AFTER scaling to the given spacing\n+def get_scaled_shape(shape, spacing):\n+    new_real_shape = shape * spacing\n+    return np.round(new_real_shape).astype(int)\n+\n def scale_vox_coord(coord, spacing, factor=1):\n     resize_factor = factor * spacing\n     return (coord*resize_factor).astype(int)\ndiff --git a\/utils\/ref_scan.dcm b\/utils\/ref_scan.dcm\nnew file mode 100644\nindex 0000000..9a12e7c\nBinary files \/dev\/null and b\/utils\/ref_scan.dcm differ\n","files":{"\/1A_build_injector_trainset.py":{"changes":[{"diff":"\n if __name__ == '__main__':\n     # Init dataset builder for creating a dataset of evidence to inject\n     print('Initializing Dataset Builder for Evidence Injection')\n-    builder = Extractor(is_healthy_dataset=False, parallelize=True)\n+    builder = Extractor(is_healthy_dataset=False, parallelize=False)\n \n     # Extract training instances\n     # Source data location and save location is loaded from config.py","add":1,"remove":1,"filename":"\/1A_build_injector_trainset.py","badparts":["    builder = Extractor(is_healthy_dataset=False, parallelize=True)"],"goodparts":["    builder = Extractor(is_healthy_dataset=False, parallelize=False)"]}],"source":"\nfrom procedures.datasetBuilder import * if __name__=='__main__': print('Initializing Dataset Builder for Evidence Injection') builder=Extractor(is_healthy_dataset=False, parallelize=True) print('Extracting instances...') builder.extract() print('Done.') ","sourceWithComments":"from procedures.datasetBuilder import *\n\nif __name__ == '__main__':\n    # Init dataset builder for creating a dataset of evidence to inject\n    print('Initializing Dataset Builder for Evidence Injection')\n    builder = Extractor(is_healthy_dataset=False, parallelize=True)\n\n    # Extract training instances\n    # Source data location and save location is loaded from config.py\n    print('Extracting instances...')\n    builder.extract()\n\n    print('Done.')"},"\/1B_build_remover_trainset.py":{"changes":[{"diff":"\n \n # Init dataset builder for creating a dataset of evidence to inject\n print('Initializing Dataset Builder for Evidence Removal')\n-builder = Extractor(is_healthy_dataset=True, parallelize=True)\n+builder = Extractor(is_healthy_dataset=True, parallelize=False)\n \n # Extract training instances\n # Source data location and save location is loaded from config.p","add":1,"remove":1,"filename":"\/1B_build_remover_trainset.py","badparts":["builder = Extractor(is_healthy_dataset=True, parallelize=True)"],"goodparts":["builder = Extractor(is_healthy_dataset=True, parallelize=False)"]}],"source":"\nfrom procedures.datasetBuilder import * print('Initializing Dataset Builder for Evidence Removal') builder=Extractor(is_healthy_dataset=True, parallelize=True) print('Extracting instances...') builder.extract() print('Done.') ","sourceWithComments":"from procedures.datasetBuilder import *\n\n# Init dataset builder for creating a dataset of evidence to inject\nprint('Initializing Dataset Builder for Evidence Removal')\nbuilder = Extractor(is_healthy_dataset=True, parallelize=True)\n\n# Extract training instances\n# Source data location and save location is loaded from config.py\nprint('Extracting instances...')\nbuilder.extract()\n\nprint('Done.')"},"\/2A_train_injector.py":{"changes":[{"diff":"\n \n print(\"Training CT-GAN Injector...\")\n CTGAN_inj = Trainer(isInjector = True)\n-CTGAN_inj.train(epochs=200, batch_size=50, sample_interval=50)\n+CTGAN_inj.train(epochs=200, batch_size=32, sample_interval=50)\n print('Done.')\n\\ No newline at end of fi","add":1,"remove":1,"filename":"\/2A_train_injector.py","badparts":["CTGAN_inj.train(epochs=200, batch_size=50, sample_interval=50)"],"goodparts":["CTGAN_inj.train(epochs=200, batch_size=32, sample_interval=50)"]}],"source":"\nfrom procedures.trainer import * print(\"Training CT-GAN Injector...\") CTGAN_inj=Trainer(isInjector=True) CTGAN_inj.train(epochs=200, batch_size=50, sample_interval=50) print('Done.') ","sourceWithComments":"from procedures.trainer import *\n\nprint(\"Training CT-GAN Injector...\")\nCTGAN_inj = Trainer(isInjector = True)\nCTGAN_inj.train(epochs=200, batch_size=50, sample_interval=50)\nprint('Done.')"},"\/2B_train_remover.py":{"changes":[{"diff":"\n \n print(\"Training CT-GAN Remover...\")\n CTGAN_rem = Trainer(isInjector = False)\n-CTGAN_rem.train(epochs=200, batch_size=50, sample_interval=50)\n+CTGAN_rem.train(epochs=200, batch_size=32, sample_interval=50)\n print('Done.')\n\\ No newline at end of f","add":1,"remove":1,"filename":"\/2B_train_remover.py","badparts":["CTGAN_rem.train(epochs=200, batch_size=50, sample_interval=50)"],"goodparts":["CTGAN_rem.train(epochs=200, batch_size=32, sample_interval=50)"]}],"source":"\nfrom procedures.trainer import * print(\"Training CT-GAN Remover...\") CTGAN_rem=Trainer(isInjector=False) CTGAN_rem.train(epochs=200, batch_size=50, sample_interval=50) print('Done.') ","sourceWithComments":"from procedures.trainer import *\n\nprint(\"Training CT-GAN Remover...\")\nCTGAN_rem = Trainer(isInjector = False)\nCTGAN_rem.train(epochs=200, batch_size=50, sample_interval=50)\nprint('Done.')"},"\/3A_inject_evidence.py":{"changes":[{"diff":"\n print('Removing Evidence...')\n \n # Init pipeline\n-injector = scan_manipulator(isInjector=True)\n+injector = scan_manipulator()\n \n # Load target scan (provide path to dcm or mhd file)\n injector.load_target_scan('path_to_target_scan')\n","add":1,"remove":1,"filename":"\/3A_inject_evidence.py","badparts":["injector = scan_manipulator(isInjector=True)"],"goodparts":["injector = scan_manipulator()"]},{"diff":" # Inject at two locations (this version does not implement auto candidate location selection)\n vox_coord1 = np.array([53,213,400]) #z, y , x (x-y should be flipped if the coordinates were obtained from an image viewer such as RadiAnt)\n vox_coord2 = np.array([33,313,200])\n-injector.tamper(vox_coord1, isVox=True) #can supply realworld coord too\n-injector.tamper(vox_coord2, isVox=True)\n+injector.tamper(vox_coord1, action='inject', isVox=True) #can supply realworld coord too\n+injector.tamper(vox_coord2, action='inject', isVox=True)\n \n # Save scan\n-injector.save_tampered_scan('path_to_save_scan',output_type='dicom') #output can be dicom iff input was dicom, otherwise only numpy save is supported\n+injector.save_tampered_scan('path_to_save_scan',output_type='dicom') #output can be dicom or n","add":3,"remove":3,"filename":"\/3A_inject_evidence.py","badparts":["injector.tamper(vox_coord1, isVox=True) #can supply realworld coord too","injector.tamper(vox_coord2, isVox=True)","injector.save_tampered_scan('path_to_save_scan',output_type='dicom') #output can be dicom iff input was dicom, otherwise only numpy save is supported"],"goodparts":["injector.tamper(vox_coord1, action='inject', isVox=True) #can supply realworld coord too","injector.tamper(vox_coord2, action='inject', isVox=True)","injector.save_tampered_scan('path_to_save_scan',output_type='dicom') #output can be dicom or n"]}],"source":"\nfrom procedures.attack_pipeline import * print('Removing Evidence...') injector=scan_manipulator(isInjector=True) injector.load_target_scan('path_to_target_scan') vox_coord1=np.array([53,213,400]) vox_coord2=np.array([33,313,200]) injector.tamper(vox_coord1, isVox=True) injector.tamper(vox_coord2, isVox=True) injector.save_tampered_scan('path_to_save_scan',output_type='dicom') ","sourceWithComments":"from procedures.attack_pipeline import *\n\nprint('Removing Evidence...')\n\n# Init pipeline\ninjector = scan_manipulator(isInjector=True)\n\n# Load target scan (provide path to dcm or mhd file)\ninjector.load_target_scan('path_to_target_scan')\n\n# Inject at two locations (this version does not implement auto candidate location selection)\nvox_coord1 = np.array([53,213,400]) #z, y , x (x-y should be flipped if the coordinates were obtained from an image viewer such as RadiAnt)\nvox_coord2 = np.array([33,313,200])\ninjector.tamper(vox_coord1, isVox=True) #can supply realworld coord too\ninjector.tamper(vox_coord2, isVox=True)\n\n# Save scan\ninjector.save_tampered_scan('path_to_save_scan',output_type='dicom') #output can be dicom iff input was dicom, otherwise only numpy save is supported\n"},"\/3B_remove evidence.py":{"changes":[{"diff":"\n print('Removing Evidence...')\n \n # Init pipeline\n-injector = scan_manipulator(isInjector=False)\n+remover = scan_manipulator()\n \n # Load target scan (provide path to dcm or mhd file)\n-injector.load_target_scan('path_to_target_scan')\n+remover.load_target_scan('path_to_target_scan')\n \n # Inject at two locations (this version does not implement auto candidate location selection)\n vox_coord1 = np.array([53,213,400]) #z, y , x (x-y should be flipped if the coordinates were obtained from an image viewer such as RadiAnt)\n vox_coord2 = np.array([33,313,200])\n-injector.tamper(vox_coord1, isVox=True) #can supply realworld coord too\n-injector.tamper(vox_coord2, isVox=True)\n+remover.tamper(vox_coord1, action='remove', isVox=True) #can supply realworld coord too\n+remover.tamper(vox_coord2, action='remove', isVox=True)\n \n # Save scan\n-injector.save_tampered_scan('path_to_save_scan',output_type='dicom') #output can be dicom iff input was dicom, otherwise only numpy save is supported\n+remover.save_tampered_scan('path_to_save_scan',output_type='dicom') #output can be dicom or ","add":5,"remove":5,"filename":"\/3B_remove evidence.py","badparts":["injector = scan_manipulator(isInjector=False)","injector.load_target_scan('path_to_target_scan')","injector.tamper(vox_coord1, isVox=True) #can supply realworld coord too","injector.tamper(vox_coord2, isVox=True)","injector.save_tampered_scan('path_to_save_scan',output_type='dicom') #output can be dicom iff input was dicom, otherwise only numpy save is supported"],"goodparts":["remover = scan_manipulator()","remover.load_target_scan('path_to_target_scan')","remover.tamper(vox_coord1, action='remove', isVox=True) #can supply realworld coord too","remover.tamper(vox_coord2, action='remove', isVox=True)","remover.save_tampered_scan('path_to_save_scan',output_type='dicom') #output can be dicom or "]}],"source":"\nfrom procedures.attack_pipeline import * print('Removing Evidence...') injector=scan_manipulator(isInjector=False) injector.load_target_scan('path_to_target_scan') vox_coord1=np.array([53,213,400]) vox_coord2=np.array([33,313,200]) injector.tamper(vox_coord1, isVox=True) injector.tamper(vox_coord2, isVox=True) injector.save_tampered_scan('path_to_save_scan',output_type='dicom') ","sourceWithComments":"from procedures.attack_pipeline import *\n\nprint('Removing Evidence...')\n\n# Init pipeline\ninjector = scan_manipulator(isInjector=False)\n\n# Load target scan (provide path to dcm or mhd file)\ninjector.load_target_scan('path_to_target_scan')\n\n# Inject at two locations (this version does not implement auto candidate location selection)\nvox_coord1 = np.array([53,213,400]) #z, y , x (x-y should be flipped if the coordinates were obtained from an image viewer such as RadiAnt)\nvox_coord2 = np.array([33,313,200])\ninjector.tamper(vox_coord1, isVox=True) #can supply realworld coord too\ninjector.tamper(vox_coord2, isVox=True)\n\n# Save scan\ninjector.save_tampered_scan('path_to_save_scan',output_type='dicom') #output can be dicom iff input was dicom, otherwise only numpy save is supported\n"}},"msg":"Upgraded Code and added GUI\n\n-Improved efficiency by interpolating the cubes and not the entire scan\n-Improved attack pipeline to handle saving to dicom and multiple injections\n-added GUI for interactive tampering demo"}},"https:\/\/github.com\/cwaldbieser\/grouper_python_provisioner":{"ed66ed36c733273263327f2a4b42b44cd26b5e0f":{"url":"https:\/\/api.github.com\/repos\/cwaldbieser\/grouper_python_provisioner\/commits\/ed66ed36c733273263327f2a4b42b44cd26b5e0f","html_url":"https:\/\/github.com\/cwaldbieser\/grouper_python_provisioner\/commit\/ed66ed36c733273263327f2a4b42b44cd26b5e0f","message":"Revert \"UNTESTED: Added the ability to transmit the message with an HMAC to prevent tampering.\"\n\nThis reverts commit 47afc32143ac38951109d5ec2281b56bf69678d7.\n\nThis idea does not work.  It does prevent tampering, and the lack of\nconfidentiality is not an issue per se, but without identity there is\nno way to stop an attacker from replaying messages without adding\nadditional information to the messages (e.g. sequence numbers).\n\nEventually, this idea will turn into something almost entirely but\nnot quite unlike TLS.","sha":"ed66ed36c733273263327f2a4b42b44cd26b5e0f","keyword":"tampering attack","diff":"diff --git a\/changelog_consumer\/process_changelog.py b\/changelog_consumer\/process_changelog.py\nindex 27fd8bb..6d80e2f 100644\n--- a\/changelog_consumer\/process_changelog.py\n+++ b\/changelog_consumer\/process_changelog.py\n@@ -59,13 +59,9 @@ def send_message(channel, exchange, route_key, msg):\n     channel.basicPublish(exchange, routing_key, props, message.getBytes())\n     channel.waitForConfirmsOrDie()\n \n-def send_group_mod(channel, exchange, router, group, action_name, subject_id, hmac_secret):\n+def send_group_mod(channel, exchange, router, group, action_name, subject_id):\n     route_key = router.get_key(group, subject_id, action_name)\n-        msg = \"%s\\n%s\\n%s\" % (group, subject_id, action_name)\n-    if hmac_secret is not None:\n-        h = hmac.new(hmac_secret, msg)\n-        digest = h.hexdigest()\n-        msg = \"%s\\n%s\" % (msg, digest)\n+    msg = \"%s\\n%s\\n%s\" % (group, subject_id, action_name)\n     send_message(channel, exchange, route_key, msg)\n \n def load_route_map(fname):\n@@ -207,9 +203,6 @@ def main(args):\n     routefile = args.route_file\n     if routefile is None:\n         routefile = scp.get(\"APPLICATION\", \"routemap\")\n-    hmac_secret = None\n-    if scp.has_section(\"APPLICATION\", \"hmac\"):\n-        hmac_secret = scp.get(\"APPLICATION\", \"hmac\")\n     debug(\"AMQP host => '%s'\" % host)\n     debug(\"AMQP port => '%s'\" % port)\n     debug(\"AMQP vhost => '%s'\" % vhost)\n@@ -266,7 +259,7 @@ def main(args):\n                 debug(\"Attempting to send message: exchange='%s', group='%s', action='%s', subject='%s'\" % (\n                     exchange, group, action_name, subject_id))\n                 try:\n-                    send_group_mod(channel, exchange, router, group, action_name, subject_id, hmac_secret)\n+                    send_group_mod(channel, exchange, router, group, action_name, subject_id)\n                 except (Exception, ), ex:\n                     warn(\"Could not send message.\\n%s\\n\" % str(ex))\n                     time.sleep(10)\ndiff --git a\/provisioner\/txgroupprovisioner\/service.py b\/provisioner\/txgroupprovisioner\/service.py\nindex c47b652..98169b2 100755\n--- a\/provisioner\/txgroupprovisioner\/service.py\n+++ b\/provisioner\/txgroupprovisioner\/service.py\n@@ -2,12 +2,12 @@\n \n # Standard library\n from __future__ import print_function\n-import hmac\n from json import load\n import os\n import os.path\n import sys\n from textwrap import dedent\n+\n # External modules\n # - Twisted\n from twisted.application import service\n@@ -221,39 +221,25 @@ def processAMQPMessage(self):\n         log = self.amqp_log\n         provisioner = self.provisioner\n         service_state = self.service_state\n-        hmac_secret = self.amqp_info.get('hmac', None)\n         log.debug(\"Attempting to read an AMQP message ...\")\n         try:\n             msg = yield queue.get()\n         except QueueClosedError:\n             log.warn(\"Queue closed-- message not processed.\")\n             returnValue(None) \n+        log.debug(\"checkpoint 0\")\n         if not service_state.read_from_queue:\n             returnValue(None) \n         log.debug('Received: \"{msg}\" from channel # {channel}.', msg=msg.content.body, channel=channel.id)\n         parts = msg.content.body.split(\"\\n\")\n-        if len(parts) == 4 and parts[3].strip() == \"\":\n-            parts = parts[:3]\n-        if len(parts) == 3 and hmac_secret is None:\n-            group = parts[0]\n-            subject = parts[1]\n-            action = parts[2]\n-            digest = None\n-            expected_digest = None\n-        elif  len(parts) == 4 and hmac_secret is not None:\n+        try:\n             group = parts[0]\n             subject = parts[1]\n             action = parts[2]\n-            digest = parts[3]\n-            expected_digest = hmac.new(hmac_secret, '\\n'.join(parts[:3])).hexdigest()\n-        else:\n+        except IndexError:\n             log.warn(\"Skipping invalid message: {msg!r}\", msg=msg.content.body)\n             yield channel.basic_ack(delivery_tag=msg.delivery_tag)\n             returnValue(None) \n-        if hmac_secret is not None and expected_digest != digest:\n-            log.warn(\"Message contains invalid digest: {msg!r}\", msg=msg.content.body)\n-            yield channel.basic_ack(delivery_tag=msg.delivery_tag)\n-            returnValue(None) \n         recorded = False\n         delay = 0\n         while not recorded and service_state.read_from_queue:\n","files":{"\/changelog_consumer\/process_changelog.py":{"changes":[{"diff":"\n     channel.basicPublish(exchange, routing_key, props, message.getBytes())\n     channel.waitForConfirmsOrDie()\n \n-def send_group_mod(channel, exchange, router, group, action_name, subject_id, hmac_secret):\n+def send_group_mod(channel, exchange, router, group, action_name, subject_id):\n     route_key = router.get_key(group, subject_id, action_name)\n-        msg = \"%s\\n%s\\n%s\" % (group, subject_id, action_name)\n-    if hmac_secret is not None:\n-        h = hmac.new(hmac_secret, msg)\n-        digest = h.hexdigest()\n-        msg = \"%s\\n%s\" % (msg, digest)\n+    msg = \"%s\\n%s\\n%s\" % (group, subject_id, action_name)\n     send_message(channel, exchange, route_key, msg)\n \n def load_route_map(fname):\n","add":2,"remove":6,"filename":"\/changelog_consumer\/process_changelog.py","badparts":["def send_group_mod(channel, exchange, router, group, action_name, subject_id, hmac_secret):","        msg = \"%s\\n%s\\n%s\" % (group, subject_id, action_name)","    if hmac_secret is not None:","        h = hmac.new(hmac_secret, msg)","        digest = h.hexdigest()","        msg = \"%s\\n%s\" % (msg, digest)"],"goodparts":["def send_group_mod(channel, exchange, router, group, action_name, subject_id):","    msg = \"%s\\n%s\\n%s\" % (group, subject_id, action_name)"]},{"diff":"\n     routefile = args.route_file\n     if routefile is None:\n         routefile = scp.get(\"APPLICATION\", \"routemap\")\n-    hmac_secret = None\n-    if scp.has_section(\"APPLICATION\", \"hmac\"):\n-        hmac_secret = scp.get(\"APPLICATION\", \"hmac\")\n     debug(\"AMQP host => '%s'\" % host)\n     debug(\"AMQP port => '%s'\" % port)\n     debug(\"AMQP vhost => '%s'\" % vhost)\n","add":0,"remove":3,"filename":"\/changelog_consumer\/process_changelog.py","badparts":["    hmac_secret = None","    if scp.has_section(\"APPLICATION\", \"hmac\"):","        hmac_secret = scp.get(\"APPLICATION\", \"hmac\")"],"goodparts":[]}],"source":"\n import argparse from ConfigParser import SafeConfigParser import datetime import hmac from operator import itemgetter import os import os.path from textwrap import dedent import time import socket from string import Template import StringIO import sys from jython_grouper import * from edu.internet2.middleware.grouper.misc import GrouperDAOFactory from edu.internet2.middleware.grouper.changeLog import ChangeLogConsumer from edu.internet2.middleware.grouper.changeLog import ChangeLogEntry, ChangeLogLabels from edu.internet2.middleware.grouper.util import GrouperUtil from edu.internet2.middleware.grouper.app.loader.db import Hib3GrouperLoaderLog from edu.internet2.middleware.grouper.app.loader import GrouperLoaderStatus from edu.internet2.middleware.grouper.app.loader import GrouperLoader from edu.internet2.middleware.grouper.app.loader import GrouperLoaderType from com.rabbitmq.client import ConnectionFactory from com.rabbitmq.client import Connection from com.rabbitmq.client import Channel from com.rabbitmq.client import QueueingConsumer from java.lang import String from java.lang import Boolean from sortedcollection import SortedCollection def get_amqp_conn(host, port, vhost, user, passwd): factory=ConnectionFactory() factory.setHost(host) factory.setPort(port) factory.setUsername(user) factory.setPassword(passwd) factory.setVirtualHost(vhost) conn=factory.newConnection() return conn def send_message(channel, exchange, route_key, msg): \"\"\" Send a message to an exchange routed by `route_key`. \"\"\" channel.confirmSelect() exchange=String(exchange) message=String(msg) routing_key=String(route_key) props=None channel.basicPublish(exchange, routing_key, props, message.getBytes()) channel.waitForConfirmsOrDie() def send_group_mod(channel, exchange, router, group, action_name, subject_id, hmac_secret): route_key=router.get_key(group, subject_id, action_name) msg=\"%s\\n%s\\n%s\" %(group, subject_id, action_name) if hmac_secret is not None: h=hmac.new(hmac_secret, msg) digest=h.hexdigest() msg=\"%s\\n%s\" %(msg, digest) send_message(channel, exchange, route_key, msg) def load_route_map(fname): routes={} f=open(fname, \"r\") for n, line in enumerate(f): line=line.strip() if line.startswith(\" continue if line==\"\": continue if not '=' in line: warn(\"Invalid route on line %s.\" %(n+1)) warn(\"route=> '%s'\" % line) continue parts=line.split(\"=\", 1) key=parts[0].strip() value=parts[1].strip() routes[key]=value return routes class Router(object): def __init__(self, route_map): self.direct_mappings={} self.patterns=SortedCollection(key=itemgetter(0)) self.default_key='' for k, v in route_map.iteritems(): if k=='_default': self.default_key=v elif k.endswith('*'): self.patterns.insert((k[:-1], Template(v))) else: self.direct_mappings[k]=v def get_key(self, group, subject_id, action_name): route_key=self.direct_mappings.get(group, None) if route_key is not None: return route_key try: result=self.patterns.find_le(group) except ValueError: return self.default_key pattern, route_key=result if not group.startswith(pattern): return self.default_key parts=group.split(':') grp=parts[-1] stem=':'.join(parts[:-1]) rk=route_key.substitute(stem=stem, group=grp, subject=subject_id, action=action_name) return rk def info(msg): sys.stderr.write(\"[INFO] %s\\n\" % msg) def debug(msg): sys.stderr.write(\"[DEBUG] %s\\n\" % msg) def warn(msg): sys.stderr.write(\"[WARNING] %s\\n\" % msg) def get_last_sequence(changefile): if os.path.exists(changefile): try: f=open(changefile, \"r\") data=f.read().strip() f.close() pos=long(data) return pos except(Exception,), ex: warn(\"Could not get last sequence number.\\n%s\\n\" % str(ex)) return None return None def update_last_sequence(changefile, n): f=open(changefile, \"w\") f.write(str(n)) f.close() def load_config(config_name): defaults=dedent(\"\"\"\\ [APPLICATION] changefile=%s routemap=%s [AMQP] host=locahost port=5672 vhost=\/ user=guest password=guest exchage=grouper_exchange \"\"\") %( os.path.join(os.curdir, \"last_change_id.txt\"), os.path.join(os.curdir, \"routes.cfg\")) scp=SafeConfigParser() buf=StringIO.StringIO(defaults) scp.readfp(buf) if config_name is not None: files=scp.read([config_name]) else: files=scp.read([ \"\/etc\/grouper\/process_changelog.cfg\", os.path.expanduser(\"~\/.process_changelog.cfg\"), os.path.join(os.path.dirname(__file__), \"process_changelog.cfg\"), os.path.join(os.path.abspath(os.curdir), \"process_changelog.cfg\")]) info(\"Read configuration from: %s\" %(', '.join(files))) return scp def main(args): scp=load_config(args.config) host=args.host if host is None: host=scp.get(\"AMQP\", \"host\") port=args.port if port is None: port=scp.getint(\"AMQP\", \"port\") vhost=args.vhost if vhost is None: vhost=scp.get(\"AMQP\", \"vhost\") user=args.user if user is None: user=scp.get(\"AMQP\", \"user\") passwd_file=args.passwd_file if passwd_file is None: passwd=scp.get(\"AMQP\", \"password\") else: passwd=passwd_file.read().strip() exchange=args.exchange if exchange is None: exchange=scp.get(\"AMQP\", \"exchange\") changefile=args.change_file if changefile is None: changefile=scp.get(\"APPLICATION\", \"changefile\") routefile=args.route_file if routefile is None: routefile=scp.get(\"APPLICATION\", \"routemap\") hmac_secret=None if scp.has_section(\"APPLICATION\", \"hmac\"): hmac_secret=scp.get(\"APPLICATION\", \"hmac\") debug(\"AMQP host=> '%s'\" % host) debug(\"AMQP port=> '%s'\" % port) debug(\"AMQP vhost=> '%s'\" % vhost) debug(\"AMQP user=> '%s'\" % user) debug(\"AMQP exchange=> '%s'\" % exchange) debug(\"AMQP changefile=> '%s'\" % changefile) debug(\"AMQP routemap=> '%s'\" % routefile) routes=load_route_map(routefile) router=Router(routes) amqp=get_amqp_conn(host, port, vhost, user, passwd) channel=amqp.createChannel() session=getRootSession() factory=GrouperDAOFactory.getFactory() consumer=factory.getChangeLogConsumer() c=ChangeLogConsumer() d=datetime.datetime.now() job_name=\"CustomJob_%s\" % d.strftime(\"%Y-%m-%dT%H:%M:%S\") c.setName(job_name) consumer.saveOrUpdate(c) last_sequence=get_last_sequence(changefile) if last_sequence is None: last_sequence=ChangeLogEntry.maxSequenceNumber(True) info(\"Last sequence number is %d.\\n\" % last_sequence) c.setLastSequenceProcessed(GrouperUtil.defaultIfNull(last_sequence, 0L).longValue()) consumer.saveOrUpdate(c) hib3=Hib3GrouperLoaderLog() hib3.setHost(GrouperUtil.hostname()) hib3.setJobName(job_name) hib3.setStatus(GrouperLoaderStatus.RUNNING.name()) attempt_num_entries=100 while True: GrouperLoader.runOnceByJobName(session, GrouperLoaderType.GROUPER_CHANGE_LOG_TEMP_TO_CHANGE_LOG) last_sequence=c.getLastSequenceProcessed() l=factory.getChangeLogEntry().retrieveBatch(last_sequence, attempt_num_entries) num_entries_retrieved=len(l) debug(\"Retrieved %d entries to process...\" % num_entries_retrieved) for n, entry in enumerate(l): action_name= entry.getChangeLogType().getActionName() if action_name !=u'addMembership' and action_name !=u'deleteMembership': continue if action_name==u'addMembership': subject_id=entry.retrieveValueForLabel(ChangeLogLabels.MEMBERSHIP_ADD.subjectId) group=entry.retrieveValueForLabel(ChangeLogLabels.MEMBERSHIP_ADD.groupName) elif action_name==u'deleteMembership': subject_id=entry.retrieveValueForLabel(ChangeLogLabels.MEMBERSHIP_DELETE.subjectId) group=entry.retrieveValueForLabel(ChangeLogLabels.MEMBERSHIP_DELETE.groupName) while True: debug(\"Attempting to send message: exchange='%s', group='%s', action='%s', subject='%s'\" %( exchange, group, action_name, subject_id)) try: send_group_mod(channel, exchange, router, group, action_name, subject_id, hmac_secret) except(Exception,), ex: warn(\"Could not send message.\\n%s\\n\" % str(ex)) time.sleep(10) continue break update_last_sequence(changefile, n+last_sequence+1) c.setLastSequenceProcessed(c.getLastSequenceProcessed() +num_entries_retrieved) consumer.saveOrUpdate(c) if num_entries_retrieved !=attempt_num_entries: time.sleep(10) if __name__==\"__main__\": parser=argparse.ArgumentParser(description=\"Process Grouper change log.\") parser.add_argument( \"-c\", \"--config\", action=\"store\", help=\"Config file to use.\") parser.add_argument( \"--host\", action=\"store\", help=\"The host where the exchange is located. Default 'localhost'.\") parser.add_argument( \"-P\", \"--port\", action=\"store\", type=int, help=\"The port on which the exchange is listening. Default 5672.\") parser.add_argument( \"-e\", \"--exchange\", action=\"store\", default=\"grouper_exchange\", help=\"The exchange to which the message is sent. Default 'grouper_exchange'.\") parser.add_argument( \"--vhost\", action=\"store\", help=\"The virtual host. Default '\/'.\") parser.add_argument( \"-u\", \"--user\", action=\"store\", help=\"The username used to connect. Default 'guest'.\") parser.add_argument( \"-p\", \"--passwd-file\", action=\"store\", type=argparse.FileType(\"r\"), help=\"A file containing the password. If not specified, 'guest' will be used as the password.\") parser.add_argument( \"--change-file\", action=\"store\", help=\"A file used to record the last change processed.\") parser.add_argument( \"--route-file\", action=\"store\", help=\"A file containing the routing patterns.\") args=parser.parse_args() main(args) ","sourceWithComments":"\n\n# Standard library\nimport argparse\nfrom ConfigParser import SafeConfigParser\nimport datetime\nimport hmac\nfrom operator import itemgetter\nimport os\nimport os.path\nfrom textwrap import dedent\nimport time\nimport socket\nfrom string import Template\nimport StringIO\nimport sys\n\n# Application modules\nfrom jython_grouper import *\n\n# External modules\nfrom edu.internet2.middleware.grouper.misc import GrouperDAOFactory\nfrom edu.internet2.middleware.grouper.changeLog import ChangeLogConsumer\nfrom edu.internet2.middleware.grouper.changeLog import ChangeLogEntry, ChangeLogLabels\nfrom edu.internet2.middleware.grouper.util import GrouperUtil\nfrom edu.internet2.middleware.grouper.app.loader.db import Hib3GrouperLoaderLog\nfrom edu.internet2.middleware.grouper.app.loader import GrouperLoaderStatus\nfrom edu.internet2.middleware.grouper.app.loader import GrouperLoader      \nfrom edu.internet2.middleware.grouper.app.loader import GrouperLoaderType\nfrom com.rabbitmq.client import ConnectionFactory              \nfrom com.rabbitmq.client import Connection       \nfrom com.rabbitmq.client import Channel                        \nfrom com.rabbitmq.client import QueueingConsumer     \nfrom java.lang import String                         \nfrom java.lang import Boolean\nfrom sortedcollection import SortedCollection\n\n# AMQP functions\ndef get_amqp_conn(host, port, vhost, user, passwd):\n    factory = ConnectionFactory()\n    factory.setHost(host)\n    factory.setPort(port)\n    factory.setUsername(user)\n    factory.setPassword(passwd)\n    factory.setVirtualHost(vhost)\n    conn = factory.newConnection()\n    return conn\n\ndef send_message(channel, exchange, route_key, msg):\n    \"\"\"\n    Send a message to an exchange routed by `route_key`.\n    \"\"\"\n    # Send message\n    channel.confirmSelect()\n    exchange = String(exchange)\n    message = String(msg)\n    routing_key = String(route_key)\n    props = None\n    channel.basicPublish(exchange, routing_key, props, message.getBytes())\n    channel.waitForConfirmsOrDie()\n\ndef send_group_mod(channel, exchange, router, group, action_name, subject_id, hmac_secret):\n    route_key = router.get_key(group, subject_id, action_name)\n        msg = \"%s\\n%s\\n%s\" % (group, subject_id, action_name)\n    if hmac_secret is not None:\n        h = hmac.new(hmac_secret, msg)\n        digest = h.hexdigest()\n        msg = \"%s\\n%s\" % (msg, digest)\n    send_message(channel, exchange, route_key, msg)\n\ndef load_route_map(fname):\n    routes = {}\n    f = open(fname, \"r\")\n    for n, line in enumerate(f):\n        line = line.strip()\n        if line.startswith(\"#\"):\n            continue\n        if line == \"\":\n            continue\n        if not '=' in line:\n            warn(\"Invalid route on line %s.\" % (n+1))\n            warn(\"route => '%s'\" % line)\n            continue\n        parts = line.split(\"=\", 1)\n        key = parts[0].strip()\n        value = parts[1].strip()\n        routes[key] = value\n    return routes\n\n\nclass Router(object):\n    def __init__(self, route_map):\n        self.direct_mappings = {}\n        self.patterns = SortedCollection(key=itemgetter(0))\n        self.default_key = ''\n        for k, v in route_map.iteritems():\n            if k == '_default':\n                self.default_key = v\n            elif k.endswith('*'):\n                self.patterns.insert((k[:-1], Template(v)))\n            else:\n                self.direct_mappings[k] = v\n    \n    def get_key(self, group, subject_id, action_name):\n        route_key = self.direct_mappings.get(group, None)\n        if route_key is not None:\n            return route_key\n        try:\n            result = self.patterns.find_le(group)\n        except ValueError:\n            return self.default_key\n        pattern, route_key = result\n        if not group.startswith(pattern):\n            return self.default_key\n        parts = group.split(':')\n        grp = parts[-1]\n        stem = ':'.join(parts[:-1])\n        rk = route_key.substitute(stem=stem, group=grp, subject=subject_id, action=action_name)\n        return rk\n            \n\n# Logging functions\ndef info(msg):\n    sys.stderr.write(\"[INFO] %s\\n\" % msg)\n\ndef debug(msg):\n    sys.stderr.write(\"[DEBUG] %s\\n\" % msg)\n\ndef warn(msg):\n    sys.stderr.write(\"[WARNING] %s\\n\" % msg)\n\n# Changelogger functions\ndef get_last_sequence(changefile):\n    if os.path.exists(changefile):\n        try:\n            f = open(changefile, \"r\")\n            data = f.read().strip()\n            f.close()\n            pos = long(data)\n            return pos\n        except (Exception,), ex:\n            warn(\"Could not get last sequence number.\\n%s\\n\" % str(ex))\n            return None\n    return None\n        \ndef update_last_sequence(changefile, n):\n    f = open(changefile, \"w\")\n    f.write(str(n))\n    f.close()\n\ndef load_config(config_name):\n    defaults = dedent(\"\"\"\\\n        [APPLICATION]\n        changefile = %s\n        routemap = %s\n\n        [AMQP]\n        host = locahost\n        port = 5672\n        vhost = \/\n        user = guest\n        password = guest\n        exchage = grouper_exchange\n        \"\"\") % (\n            os.path.join(os.curdir, \"last_change_id.txt\"),\n            os.path.join(os.curdir, \"routes.cfg\"))\n    scp = SafeConfigParser()\n    buf = StringIO.StringIO(defaults)\n    scp.readfp(buf)\n    if config_name is not None:\n        files = scp.read([config_name])\n    else:\n        files = scp.read([\n            \"\/etc\/grouper\/process_changelog.cfg\", \n            os.path.expanduser(\"~\/.process_changelog.cfg\"),\n            os.path.join(os.path.dirname(__file__), \"process_changelog.cfg\"),\n            os.path.join(os.path.abspath(os.curdir), \"process_changelog.cfg\")])\n    info(\"Read configuration from: %s\" % (', '.join(files)))\n    return scp\n        \n\ndef main(args):\n    scp = load_config(args.config)\n    host = args.host\n    if host is None:\n        host = scp.get(\"AMQP\", \"host\")\n    port = args.port\n    if port is None:\n        port = scp.getint(\"AMQP\", \"port\")\n    vhost = args.vhost\n    if vhost is None:\n        vhost = scp.get(\"AMQP\", \"vhost\")\n    user = args.user\n    if user is None:\n        user = scp.get(\"AMQP\", \"user\")\n    passwd_file = args.passwd_file\n    if passwd_file is None:\n        passwd = scp.get(\"AMQP\", \"password\")\n    else:\n        passwd = passwd_file.read().strip()\n    exchange = args.exchange\n    if exchange is None:\n        exchange = scp.get(\"AMQP\", \"exchange\")\n    changefile = args.change_file\n    if changefile is None:\n        changefile = scp.get(\"APPLICATION\", \"changefile\")\n    routefile = args.route_file\n    if routefile is None:\n        routefile = scp.get(\"APPLICATION\", \"routemap\")\n    hmac_secret = None\n    if scp.has_section(\"APPLICATION\", \"hmac\"):\n        hmac_secret = scp.get(\"APPLICATION\", \"hmac\")\n    debug(\"AMQP host => '%s'\" % host)\n    debug(\"AMQP port => '%s'\" % port)\n    debug(\"AMQP vhost => '%s'\" % vhost)\n    debug(\"AMQP user => '%s'\" % user)\n    debug(\"AMQP exchange => '%s'\" % exchange)\n    debug(\"AMQP changefile => '%s'\" % changefile)\n    debug(\"AMQP routemap => '%s'\" % routefile)\n    # Read routes.\n    routes = load_route_map(routefile)\n    router = Router(routes)\n    # Connect to message queue.\n    amqp = get_amqp_conn(host, port, vhost, user, passwd)\n    channel = amqp.createChannel()                 \n    # END connecto to message queue.\n    session = getRootSession()\n    factory = GrouperDAOFactory.getFactory()\n    consumer = factory.getChangeLogConsumer()\n    c = ChangeLogConsumer()\n    d = datetime.datetime.now()\n    job_name = \"CustomJob_%s\" % d.strftime(\"%Y-%m-%dT%H:%M:%S\")\n    c.setName(job_name)\n    consumer.saveOrUpdate(c)\n    # Prime the last sequence number.\n    last_sequence = get_last_sequence(changefile)\n    if last_sequence is None:\n        last_sequence = ChangeLogEntry.maxSequenceNumber(True)\n    info(\"Last sequence number is %d.\\n\" % last_sequence)\n    c.setLastSequenceProcessed(GrouperUtil.defaultIfNull(last_sequence, 0L).longValue()) \n    consumer.saveOrUpdate(c)\n    # Initialize the consumer job.\n    hib3 = Hib3GrouperLoaderLog()\n    hib3.setHost(GrouperUtil.hostname())\n    hib3.setJobName(job_name)\n    hib3.setStatus(GrouperLoaderStatus.RUNNING.name())\n    # Begin the main loop.\n    attempt_num_entries = 100\n    while True:\n        GrouperLoader.runOnceByJobName(session, GrouperLoaderType.GROUPER_CHANGE_LOG_TEMP_TO_CHANGE_LOG)\n        last_sequence = c.getLastSequenceProcessed()\n        l = factory.getChangeLogEntry().retrieveBatch(last_sequence, attempt_num_entries)\n        num_entries_retrieved = len(l)\n        debug(\"Retrieved %d entries to process ...\" % num_entries_retrieved)\n        for n, entry in enumerate(l):\n            action_name =  entry.getChangeLogType().getActionName()\n            if action_name != u'addMembership' and action_name != u'deleteMembership':\n                continue\n            if action_name == u'addMembership':\n                subject_id = entry.retrieveValueForLabel(ChangeLogLabels.MEMBERSHIP_ADD.subjectId)\n                group = entry.retrieveValueForLabel(ChangeLogLabels.MEMBERSHIP_ADD.groupName)\n            elif action_name == u'deleteMembership':\n                subject_id = entry.retrieveValueForLabel(ChangeLogLabels.MEMBERSHIP_DELETE.subjectId)\n                group = entry.retrieveValueForLabel(ChangeLogLabels.MEMBERSHIP_DELETE.groupName)\n            while True:\n                debug(\"Attempting to send message: exchange='%s', group='%s', action='%s', subject='%s'\" % (\n                    exchange, group, action_name, subject_id))\n                try:\n                    send_group_mod(channel, exchange, router, group, action_name, subject_id, hmac_secret)\n                except (Exception, ), ex:\n                    warn(\"Could not send message.\\n%s\\n\" % str(ex))\n                    time.sleep(10)\n                    continue\n                break\n            update_last_sequence(changefile, n+last_sequence+1)\n\n        c.setLastSequenceProcessed(c.getLastSequenceProcessed() + num_entries_retrieved)\n        consumer.saveOrUpdate(c)\n        if num_entries_retrieved != attempt_num_entries:\n            time.sleep(10)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Process Grouper change log.\")\n    parser.add_argument(\n        \"-c\",\n        \"--config\",\n        action=\"store\",\n        help=\"Config file to use.\")\n    parser.add_argument(\n        \"--host\",\n        action=\"store\",\n        help=\"The host where the exchange is located.  Default 'localhost'.\")\n    parser.add_argument(\n        \"-P\",\n        \"--port\",\n        action=\"store\",\n        type=int,\n        help=\"The port on which the exchange is listening.  Default 5672.\")\n    parser.add_argument(\n        \"-e\",\n        \"--exchange\",\n        action=\"store\",\n        default=\"grouper_exchange\",\n        help=\"The exchange to which the message is sent.  Default 'grouper_exchange'.\")\n    parser.add_argument(\n        \"--vhost\",\n        action=\"store\",\n        help=\"The virtual host.  Default '\/'.\")\n    parser.add_argument(\n        \"-u\",\n        \"--user\",\n        action=\"store\",\n        help=\"The username used to connect.  Default 'guest'.\")\n    parser.add_argument(\n        \"-p\",\n        \"--passwd-file\",\n        action=\"store\",\n        type=argparse.FileType(\"r\"),\n        help=\"A file containing the password.  If not specified, 'guest' will be used as the password.\")\n    parser.add_argument(\n        \"--change-file\",\n        action=\"store\",\n        help=\"A file used to record the last change processed.\")\n    parser.add_argument(\n        \"--route-file\",\n        action=\"store\",\n        help=\"A file containing the routing patterns.\")\n    args = parser.parse_args()\n    main(args)\n\n"},"\/provisioner\/txgroupprovisioner\/service.py":{"changes":[{"diff":"\n \n # Standard library\n from __future__ import print_function\n-import hmac\n from json import load\n import os\n import os.path\n import sys\n from textwrap import dedent\n+\n # External modules\n # - Twisted\n from twisted.application import service\n","add":1,"remove":1,"filename":"\/provisioner\/txgroupprovisioner\/service.py","badparts":["import hmac"],"goodparts":[]}],"source":"\n from __future__ import print_function import hmac from json import load import os import os.path import sys from textwrap import dedent from twisted.application import service from twisted.application.service import Service from twisted.internet import reactor, task from twisted.internet.defer import inlineCallbacks, returnValue, DeferredList from twisted.internet.endpoints import clientFromString, connectProtocol from twisted.internet.protocol import ClientCreator from twisted.logger import Logger from txamqp.client import TwistedDelegate from txamqp.protocol import AMQClient from txamqp.queue import Closed as QueueClosedError import txamqp.spec from config import load_config, section2dict from interface import IProvisionerFactory from logging import make_syslog_observer from utils import get_plugin_factory class ServiceState(object): db_str=None last_update=None amqp_info=None read_from_queue=False class GroupProvisionerService(Service): log=None consumerTag=\"mytag\" def __init__( self, a_reactor=None, config=None, use_syslog=False, syslog_prefix=None, logfile=None): \"\"\" Initialize the service. :param a_reactor: Override the reactor to use. \"\"\" if a_reactor is None: a_reactor=reactor self._reactor=a_reactor self._port=None self.service_state=ServiceState() self.use_syslog=use_syslog self.syslog_prefix=syslog_prefix self.config=config self.amqpLooper=None def startService(self): \"\"\" Start the service. \"\"\" scp=load_config( config_file=self.config, defaults=self.make_config_defaults()) self.scp=scp app_info=section2dict(scp, \"APPLICATION\") log_level=app_info.get(\"log_level\", \"INFO\") log=Logger( observer=make_syslog_observer( log_level, prefix=self.syslog_prefix)) self.log=log self.amqp_info=section2dict(scp, \"AMQP\") amqp_log_level=self.amqp_info.get(\"log_level\", log_level) self.amqp_log=Logger( observer=make_syslog_observer( amqp_log_level, prefix=self.syslog_prefix)) service_state=self.service_state service_state.last_update=None self.start_amqp_client() provisioner_tag=app_info['provisioner'] log.info(\"Provisioner tag=> '{provisioner}'\", provisioner=provisioner_tag) provisioner_factory=get_plugin_factory(provisioner_tag, IProvisionerFactory) if provisioner_factory is None: log.error(\"No provisioner factory was found!\") reactor.stop() provisioner=provisioner_factory.generateProvisioner() provisioner.service_state=service_state provisioner.load_config( config_file=self.config, default_log_level=log_level, syslog_prefix=self.syslog_prefix) self.provisioner=provisioner def start_amqp_client(self): amqp_info=self.amqp_info log=self.amqp_log endpoint_str=amqp_info['endpoint'] exchange=amqp_info['exchange'] vhost=amqp_info['vhost'] spec_path=amqp_info['spec'] queue_name=amqp_info['queue'] route_file=amqp_info['route_map'] user=amqp_info['user'] passwd=amqp_info['passwd'] creds=(user, passwd) bindings=self.parse_bindings(route_file) queue_names=set([q for q, rk in bindings]) queue_names.add(queue_name) log.debug( \"endpoint='{endpoint}', exchange='{exchange}', vhost='{vhost}', user='{user}, spec={spec}'\", endpoint=endpoint_str, exchange=exchange, vhost=vhost, user=user, spec=spec_path) for q in sorted(queue_names): log.debug(\"Declared: queue='{queue}'\", queue=q) for q, rk in bindings: log.debug(\"Binding: queue='{queue}', route_key='{route_key}'\", queue=q, route_key=rk) delegate=TwistedDelegate() spec=txamqp.spec.load(spec_path) ep=clientFromString(self._reactor, endpoint_str) d=connectProtocol( ep, AMQClient( delegate=delegate, vhost=vhost, spec=spec)) d.addCallback(self.on_amqp_connect, exchange, queue_name, queue_names, bindings, creds) def onError(err): if reactor.running: log.failure(err) reactor.stop() d.addErrback(onError) def parse_bindings(self, fname): \"\"\" Queue map should be a JSON list of(queue_name, route_key) mappings. \"\"\" with open(fname, \"r\") as f: o=load(f) return o @inlineCallbacks def on_amqp_connect(self, conn, exchange, queue_name, queue_names, bindings, creds): log=self.amqp_log provisioner=self.provisioner service_state=self.service_state log.info(\"Connected.\") self.amqpConn=conn user, passwd=creds yield conn.authenticate(user, passwd) log.info(\"Authenticated.\") channel=yield conn.channel(1) self.amqpChannel=channel yield channel.channel_open() log.info(\"Channel opened.\") for name in queue_names: yield channel.queue_declare(queue=name, durable=True) log.info(\"Queues declared.\") yield channel.exchange_declare(exchange=exchange, type='topic') log.info(\"Exchange declared.\") for qname, route_key in bindings: yield channel.queue_bind(exchange=exchange, queue=qname, routing_key=route_key) log.info(\"Routings have been mapped.\") self.createAMQPMessageLoop() self.startAMQPMessageLoop() def createAMQPMessageLoop(self): log=self.amqp_log amqpLooper=task.LoopingCall(self.processAMQPMessage) self.amqpLooper=amqpLooper def startAMQPMessageLoop(self): if not self.service_state.read_from_queue: log=self.amqp_log log.debug(\"Starting the AMQP message loop...\") self.service_state.read_from_queue=True d=self.startConsumingFromQueue() def startLoop_(result): try: self.amqpLooper.start(0) except Exception as ex: log.error(\"An error occured during AMQP message processing:{error}\", error=str(ex)) raise d.addCallback(startLoop_) def stopAMQPMessageLoop(self): if self.service_state.read_from_queue: log=self.amqp_log self.amqpLooper.stop() self.service_state.read_from_queue=False def logStopped_(result): log.debug(\"Stopped the AMQP message loop.\") d=self.amqpChannel.basic_cancel(consumer_tag=self.consumerTag) d.addCallback(logStopped_) @inlineCallbacks def startConsumingFromQueue(self): log=self.amqp_log queue_name=self.amqp_info['queue'] channel=self.amqpChannel conn=self.amqpConn log.debug(\"AMQP: starting basic_consume() for queue '{0}'...\".format(queue_name)) yield channel.basic_consume(queue=queue_name, consumer_tag=self.consumerTag) self.amqpQueue=yield conn.queue(self.consumerTag) log.debug(\"AMQP: basic_consume() started.\") @inlineCallbacks def processAMQPMessage(self): channel=self.amqpChannel queue=self.amqpQueue reactor=self._reactor log=self.amqp_log provisioner=self.provisioner service_state=self.service_state hmac_secret=self.amqp_info.get('hmac', None) log.debug(\"Attempting to read an AMQP message...\") try: msg=yield queue.get() except QueueClosedError: log.warn(\"Queue closed--message not processed.\") returnValue(None) if not service_state.read_from_queue: returnValue(None) log.debug('Received: \"{msg}\" from channel parts=msg.content.body.split(\"\\n\") if len(parts)==4 and parts[3].strip()==\"\": parts=parts[:3] if len(parts)==3 and hmac_secret is None: group=parts[0] subject=parts[1] action=parts[2] digest=None expected_digest=None elif len(parts)==4 and hmac_secret is not None: group=parts[0] subject=parts[1] action=parts[2] digest=parts[3] expected_digest=hmac.new(hmac_secret, '\\n'.join(parts[:3])).hexdigest() else: log.warn(\"Skipping invalid message:{msg!r}\", msg=msg.content.body) yield channel.basic_ack(delivery_tag=msg.delivery_tag) returnValue(None) if hmac_secret is not None and expected_digest !=digest: log.warn(\"Message contains invalid digest:{msg!r}\", msg=msg.content.body) yield channel.basic_ack(delivery_tag=msg.delivery_tag) returnValue(None) recorded=False delay=0 while not recorded and service_state.read_from_queue: try: yield task.deferLater(reactor, delay, provisioner.provision, group, subject, action) except Exception as ex: log.error(\"Could not record message from queue. Error was:{error}\", error=ex) delay=min(600, max(delay+20, delay*2)) else: recorded=True delay=0 yield channel.basic_ack(delivery_tag=msg.delivery_tag) log.debug(\"Message from queue recorded.\") @inlineCallbacks def closeAMQPConnection(self): conn=self.amqpConn channel=self.amqpChannel log.info(\"Closing AMQP channel...\") try: yield channel.channel_close() except Exception as ex: log.error(\"Error while trying to close AMQP channel:{error}\", error=ex) else: log.info(\"AMQP Channel closed.\") log.info(\"Closing AMQP connection...\") try: yield conn.connection_close() except Exception as ex: log.error(\"Error while trying to close AMQP connection:{error}\", error=ex) else: log.info(\"AMQP Connection closed.\") def set_listening_port(self, port): self._port=port def stopService(self): \"\"\" Stop the service. \"\"\" self.service_state.read_from_queue=False async_tasks=[] if self._port is not None: async_tasks.append(self._port.stopListening()) async_tasks.append(self.closeAMQPConnection()) return DeferredList(async_tasks, consumeErrors=True) def make_config_defaults(self): spec_dir=os.path.join(os.path.split(os.path.split(__file__)[0])[0], \"spec\") spec_path=os.path.join(spec_dir, \"amqp0-9-1.stripped.xml\") return dedent(\"\"\"\\ [APPLICATION] log_level=DEBUG provisioner=ldap [AMQP] log_level=INFO endpoint=tcp:host=localhost:port=5672 exchange=grouper_exchange vhost=\/ spec={spec_path} user=guest passwd=guest \"\"\".format(spec_path=spec_path)) def main(): service=GroupProvisionerService() service.startService() reactor.run() if __name__==\"__main__\": main() else: application=service.Application(\"Twisted Group Provisioner\") service=GroupProvisionerService() service.setServiceParent(application) ","sourceWithComments":"#! \/usr\/bin\/env python\n\n# Standard library\nfrom __future__ import print_function\nimport hmac\nfrom json import load\nimport os\nimport os.path\nimport sys\nfrom textwrap import dedent\n# External modules\n# - Twisted\nfrom twisted.application import service\nfrom twisted.application.service import Service\nfrom twisted.internet import reactor, task\nfrom twisted.internet.defer import inlineCallbacks, returnValue, DeferredList\nfrom twisted.internet.endpoints import clientFromString, connectProtocol\nfrom twisted.internet.protocol import ClientCreator\nfrom twisted.logger import Logger\nfrom txamqp.client import TwistedDelegate\nfrom txamqp.protocol import AMQClient\nfrom txamqp.queue import Closed as QueueClosedError\nimport txamqp.spec\n# - application\nfrom config import load_config, section2dict\nfrom interface import IProvisionerFactory\nfrom logging import make_syslog_observer\nfrom utils import get_plugin_factory\n\n\nclass ServiceState(object):\n    db_str = None\n    last_update = None\n    amqp_info = None\n    read_from_queue = False\n\nclass GroupProvisionerService(Service):\n    log = None\n    consumerTag = \"mytag\"\n\n    def __init__(\n            self, \n            a_reactor=None, \n            config=None, \n            use_syslog=False, \n            syslog_prefix=None,\n            logfile=None):\n        \"\"\"\n        Initialize the service.\n        \n        :param a_reactor: Override the reactor to use.\n        \"\"\"\n        if a_reactor is None:\n            a_reactor = reactor\n        self._reactor = a_reactor\n        self._port = None\n        self.service_state = ServiceState()\n        self.use_syslog = use_syslog\n        self.syslog_prefix = syslog_prefix\n        self.config = config\n        self.amqpLooper = None\n        \n    def startService(self):\n        \"\"\"\n        Start the service.\n        \"\"\"\n        scp = load_config(\n            config_file=self.config, defaults=self.make_config_defaults())\n        self.scp = scp\n        app_info = section2dict(scp, \"APPLICATION\")\n        log_level = app_info.get(\"log_level\", \"INFO\")\n        log = Logger(\n            observer=make_syslog_observer(\n                log_level, \n                prefix=self.syslog_prefix))\n        self.log = log\n        self.amqp_info = section2dict(scp, \"AMQP\")\n        amqp_log_level = self.amqp_info.get(\"log_level\", log_level) \n        self.amqp_log = Logger(\n            observer=make_syslog_observer(\n                amqp_log_level, \n                prefix=self.syslog_prefix))\n        service_state = self.service_state \n        service_state.last_update = None\n        self.start_amqp_client()\n        provisioner_tag = app_info['provisioner']\n        log.info(\"Provisioner tag => '{provisioner}'\", provisioner=provisioner_tag)\n        provisioner_factory = get_plugin_factory(provisioner_tag, IProvisionerFactory)\n        if provisioner_factory is None:\n            log.error(\"No provisioner factory was found!\")\n            reactor.stop()\n        provisioner = provisioner_factory.generateProvisioner()\n        provisioner.service_state = service_state\n        provisioner.load_config(\n            config_file=self.config, \n            default_log_level=log_level, \n            syslog_prefix=self.syslog_prefix)\n        self.provisioner = provisioner\n\n    def start_amqp_client(self):\n        amqp_info = self.amqp_info\n        log = self.amqp_log\n        endpoint_str = amqp_info['endpoint']\n        exchange = amqp_info['exchange']\n        vhost = amqp_info['vhost']\n        spec_path = amqp_info['spec']\n        queue_name = amqp_info['queue']\n        route_file = amqp_info['route_map']\n        user = amqp_info['user']\n        passwd = amqp_info['passwd']\n        creds = (user, passwd)\n        bindings = self.parse_bindings(route_file)\n        queue_names = set([q for q, rk in bindings])\n        queue_names.add(queue_name)\n        log.debug(\n            \"endpoint='{endpoint}', exchange='{exchange}', vhost='{vhost}', user='{user}, spec={spec}'\",\n            endpoint=endpoint_str, exchange=exchange, vhost=vhost, user=user, spec=spec_path)\n        for q in sorted(queue_names):\n            log.debug(\"Declared: queue='{queue}'\", queue=q)\n        for q, rk in bindings:\n            log.debug(\"Binding: queue='{queue}', route_key='{route_key}'\", queue=q, route_key=rk)\n        delegate = TwistedDelegate()\n        spec = txamqp.spec.load(spec_path)\n        ep = clientFromString(self._reactor, endpoint_str)\n        d = connectProtocol(\n            ep, \n            AMQClient( \n                delegate=delegate, \n                vhost=vhost, \n                spec=spec))\n        d.addCallback(self.on_amqp_connect, exchange, queue_name, queue_names, bindings, creds)\n\n        def onError(err):\n            if reactor.running:\n                log.failure(err)\n                reactor.stop()\n\n        d.addErrback(onError)\n\n    def parse_bindings(self, fname):\n        \"\"\"\n        Queue map should be a JSON list of (queue_name, route_key) mappings.\n        \"\"\"\n        with open(fname, \"r\") as f:\n            o = load(f)\n            return o\n\n    @inlineCallbacks\n    def on_amqp_connect(self, conn, exchange, queue_name, queue_names, bindings, creds):\n        log = self.amqp_log\n        provisioner = self.provisioner\n        service_state = self.service_state\n        log.info(\"Connected.\")\n        self.amqpConn = conn\n        user, passwd = creds\n        yield conn.authenticate(user, passwd)\n        log.info(\"Authenticated.\")\n        channel = yield conn.channel(1)\n        self.amqpChannel = channel\n        yield channel.channel_open()\n        log.info(\"Channel opened.\")\n        for name in queue_names:\n            yield channel.queue_declare(queue=name, durable=True)\n        log.info(\"Queues declared.\")\n        yield channel.exchange_declare(exchange=exchange, type='topic')\n        log.info(\"Exchange declared.\")\n        for qname, route_key in bindings:\n            yield channel.queue_bind(exchange=exchange, queue=qname, routing_key=route_key)\n        log.info(\"Routings have been mapped.\")\n        self.createAMQPMessageLoop()\n        self.startAMQPMessageLoop()\n\n    def createAMQPMessageLoop(self):\n        log = self.amqp_log\n        amqpLooper = task.LoopingCall(self.processAMQPMessage)\n        self.amqpLooper = amqpLooper\n\n    def startAMQPMessageLoop(self):\n        if not self.service_state.read_from_queue: \n            log = self.amqp_log\n            log.debug(\"Starting the AMQP message loop ...\")\n            self.service_state.read_from_queue = True\n            d = self.startConsumingFromQueue()\n            def startLoop_(result):\n                try:\n                    self.amqpLooper.start(0)\n                except Exception as ex:\n                    log.error(\"An error occured during AMQP message processing: {error}\", error=str(ex))\n                    raise\n\n            d.addCallback(startLoop_)\n\n    def stopAMQPMessageLoop(self):\n        if self.service_state.read_from_queue: \n            log = self.amqp_log\n            self.amqpLooper.stop()\n            self.service_state.read_from_queue = False\n\n            def logStopped_(result):\n                log.debug(\"Stopped the AMQP message loop.\")\n\n            d = self.amqpChannel.basic_cancel(consumer_tag=self.consumerTag)\n            d.addCallback(logStopped_)\n\n    @inlineCallbacks\n    def startConsumingFromQueue(self):\n        log = self.amqp_log\n        queue_name = self.amqp_info['queue']\n        channel = self.amqpChannel\n        conn = self.amqpConn\n        log.debug(\"AMQP: starting basic_consume() for queue '{0}' ...\".format(queue_name))\n        yield channel.basic_consume(queue=queue_name, consumer_tag=self.consumerTag)\n        self.amqpQueue = yield conn.queue(self.consumerTag) \n        log.debug(\"AMQP: basic_consume() started.\")\n\n    @inlineCallbacks\n    def processAMQPMessage(self):\n        channel = self.amqpChannel\n        queue = self.amqpQueue\n        reactor = self._reactor\n        log = self.amqp_log\n        provisioner = self.provisioner\n        service_state = self.service_state\n        hmac_secret = self.amqp_info.get('hmac', None)\n        log.debug(\"Attempting to read an AMQP message ...\")\n        try:\n            msg = yield queue.get()\n        except QueueClosedError:\n            log.warn(\"Queue closed-- message not processed.\")\n            returnValue(None) \n        if not service_state.read_from_queue:\n            returnValue(None) \n        log.debug('Received: \"{msg}\" from channel # {channel}.', msg=msg.content.body, channel=channel.id)\n        parts = msg.content.body.split(\"\\n\")\n        if len(parts) == 4 and parts[3].strip() == \"\":\n            parts = parts[:3]\n        if len(parts) == 3 and hmac_secret is None:\n            group = parts[0]\n            subject = parts[1]\n            action = parts[2]\n            digest = None\n            expected_digest = None\n        elif  len(parts) == 4 and hmac_secret is not None:\n            group = parts[0]\n            subject = parts[1]\n            action = parts[2]\n            digest = parts[3]\n            expected_digest = hmac.new(hmac_secret, '\\n'.join(parts[:3])).hexdigest()\n        else:\n            log.warn(\"Skipping invalid message: {msg!r}\", msg=msg.content.body)\n            yield channel.basic_ack(delivery_tag=msg.delivery_tag)\n            returnValue(None) \n        if hmac_secret is not None and expected_digest != digest:\n            log.warn(\"Message contains invalid digest: {msg!r}\", msg=msg.content.body)\n            yield channel.basic_ack(delivery_tag=msg.delivery_tag)\n            returnValue(None) \n        recorded = False\n        delay = 0\n        while not recorded and service_state.read_from_queue:\n            try:\n                yield task.deferLater(reactor, delay, provisioner.provision, group, subject, action)\n            except Exception as ex:\n                log.error(\"Could not record message from queue.  Error was: {error}\", error=ex)\n                delay = min(600, max(delay+20, delay*2))\n            else:\n                recorded = True\n                delay = 0    \n                yield channel.basic_ack(delivery_tag=msg.delivery_tag)\n                log.debug(\"Message from queue recorded.\")\n        \n    @inlineCallbacks\n    def closeAMQPConnection(self):\n        conn = self.amqpConn\n        channel = self.amqpChannel\n        log.info(\"Closing AMQP channel ...\")\n        try:\n            yield channel.channel_close()\n        except Exception as ex:\n            log.error(\"Error while trying to close AMQP channel: {error}\", error=ex)\n        else:\n            log.info(\"AMQP Channel closed.\")\n        log.info(\"Closing AMQP connection ...\")\n        try:\n            yield conn.connection_close()\n        except Exception as ex:\n            log.error(\"Error while trying to close AMQP connection: {error}\", error=ex)\n        else:\n            log.info(\"AMQP Connection closed.\")\n\n    def set_listening_port(self, port):\n        self._port = port\n        \n    def stopService(self):\n        \"\"\"\n        Stop the service.\n        \"\"\"\n        self.service_state.read_from_queue = False\n        async_tasks = []\n        if self._port is not None:\n            async_tasks.append(self._port.stopListening())\n        async_tasks.append(self.closeAMQPConnection())\n        return DeferredList(async_tasks, consumeErrors=True)             \n\n    def make_config_defaults(self):\n        spec_dir = os.path.join(os.path.split(os.path.split(__file__)[0])[0], \"spec\")\n        spec_path = os.path.join(spec_dir, \"amqp0-9-1.stripped.xml\")\n        return dedent(\"\"\"\\\n            [APPLICATION]\n            log_level = DEBUG\n            provisioner = ldap\n            \n            [AMQP]\n            log_level = INFO\n            endpoint = tcp:host=localhost:port=5672\n            exchange = grouper_exchange\n            vhost = \/\n            spec = {spec_path}\n            user = guest\n            passwd = guest\n            \"\"\".format(spec_path=spec_path))\n\n    \ndef main():\n    service = GroupProvisionerService()\n    service.startService()\n    reactor.run()\n\nif __name__ == \"__main__\":\n    main()\nelse:\n    application = service.Application(\"Twisted Group Provisioner\")\n    service = GroupProvisionerService()\n    service.setServiceParent(application)\n    \n    \n    \n"}},"msg":"Revert \"UNTESTED: Added the ability to transmit the message with an HMAC to prevent tampering.\"\n\nThis reverts commit 47afc32143ac38951109d5ec2281b56bf69678d7.\n\nThis idea does not work.  It does prevent tampering, and the lack of\nconfidentiality is not an issue per se, but without identity there is\nno way to stop an attacker from replaying messages without adding\nadditional information to the messages (e.g. sequence numbers).\n\nEventually, this idea will turn into something almost entirely but\nnot quite unlike TLS."}},"https:\/\/github.com\/cwaldbieser\/txamqpprovisioner":{"ed66ed36c733273263327f2a4b42b44cd26b5e0f":{"url":"https:\/\/api.github.com\/repos\/cwaldbieser\/txamqpprovisioner\/commits\/ed66ed36c733273263327f2a4b42b44cd26b5e0f","html_url":"https:\/\/github.com\/cwaldbieser\/txamqpprovisioner\/commit\/ed66ed36c733273263327f2a4b42b44cd26b5e0f","message":"Revert \"UNTESTED: Added the ability to transmit the message with an HMAC to prevent tampering.\"\n\nThis reverts commit 47afc32143ac38951109d5ec2281b56bf69678d7.\n\nThis idea does not work.  It does prevent tampering, and the lack of\nconfidentiality is not an issue per se, but without identity there is\nno way to stop an attacker from replaying messages without adding\nadditional information to the messages (e.g. sequence numbers).\n\nEventually, this idea will turn into something almost entirely but\nnot quite unlike TLS.","sha":"ed66ed36c733273263327f2a4b42b44cd26b5e0f","keyword":"tampering attack","diff":"diff --git a\/changelog_consumer\/process_changelog.py b\/changelog_consumer\/process_changelog.py\nindex 27fd8bb..6d80e2f 100644\n--- a\/changelog_consumer\/process_changelog.py\n+++ b\/changelog_consumer\/process_changelog.py\n@@ -59,13 +59,9 @@ def send_message(channel, exchange, route_key, msg):\n     channel.basicPublish(exchange, routing_key, props, message.getBytes())\n     channel.waitForConfirmsOrDie()\n \n-def send_group_mod(channel, exchange, router, group, action_name, subject_id, hmac_secret):\n+def send_group_mod(channel, exchange, router, group, action_name, subject_id):\n     route_key = router.get_key(group, subject_id, action_name)\n-        msg = \"%s\\n%s\\n%s\" % (group, subject_id, action_name)\n-    if hmac_secret is not None:\n-        h = hmac.new(hmac_secret, msg)\n-        digest = h.hexdigest()\n-        msg = \"%s\\n%s\" % (msg, digest)\n+    msg = \"%s\\n%s\\n%s\" % (group, subject_id, action_name)\n     send_message(channel, exchange, route_key, msg)\n \n def load_route_map(fname):\n@@ -207,9 +203,6 @@ def main(args):\n     routefile = args.route_file\n     if routefile is None:\n         routefile = scp.get(\"APPLICATION\", \"routemap\")\n-    hmac_secret = None\n-    if scp.has_section(\"APPLICATION\", \"hmac\"):\n-        hmac_secret = scp.get(\"APPLICATION\", \"hmac\")\n     debug(\"AMQP host => '%s'\" % host)\n     debug(\"AMQP port => '%s'\" % port)\n     debug(\"AMQP vhost => '%s'\" % vhost)\n@@ -266,7 +259,7 @@ def main(args):\n                 debug(\"Attempting to send message: exchange='%s', group='%s', action='%s', subject='%s'\" % (\n                     exchange, group, action_name, subject_id))\n                 try:\n-                    send_group_mod(channel, exchange, router, group, action_name, subject_id, hmac_secret)\n+                    send_group_mod(channel, exchange, router, group, action_name, subject_id)\n                 except (Exception, ), ex:\n                     warn(\"Could not send message.\\n%s\\n\" % str(ex))\n                     time.sleep(10)\ndiff --git a\/provisioner\/txgroupprovisioner\/service.py b\/provisioner\/txgroupprovisioner\/service.py\nindex c47b652..98169b2 100755\n--- a\/provisioner\/txgroupprovisioner\/service.py\n+++ b\/provisioner\/txgroupprovisioner\/service.py\n@@ -2,12 +2,12 @@\n \n # Standard library\n from __future__ import print_function\n-import hmac\n from json import load\n import os\n import os.path\n import sys\n from textwrap import dedent\n+\n # External modules\n # - Twisted\n from twisted.application import service\n@@ -221,39 +221,25 @@ def processAMQPMessage(self):\n         log = self.amqp_log\n         provisioner = self.provisioner\n         service_state = self.service_state\n-        hmac_secret = self.amqp_info.get('hmac', None)\n         log.debug(\"Attempting to read an AMQP message ...\")\n         try:\n             msg = yield queue.get()\n         except QueueClosedError:\n             log.warn(\"Queue closed-- message not processed.\")\n             returnValue(None) \n+        log.debug(\"checkpoint 0\")\n         if not service_state.read_from_queue:\n             returnValue(None) \n         log.debug('Received: \"{msg}\" from channel # {channel}.', msg=msg.content.body, channel=channel.id)\n         parts = msg.content.body.split(\"\\n\")\n-        if len(parts) == 4 and parts[3].strip() == \"\":\n-            parts = parts[:3]\n-        if len(parts) == 3 and hmac_secret is None:\n-            group = parts[0]\n-            subject = parts[1]\n-            action = parts[2]\n-            digest = None\n-            expected_digest = None\n-        elif  len(parts) == 4 and hmac_secret is not None:\n+        try:\n             group = parts[0]\n             subject = parts[1]\n             action = parts[2]\n-            digest = parts[3]\n-            expected_digest = hmac.new(hmac_secret, '\\n'.join(parts[:3])).hexdigest()\n-        else:\n+        except IndexError:\n             log.warn(\"Skipping invalid message: {msg!r}\", msg=msg.content.body)\n             yield channel.basic_ack(delivery_tag=msg.delivery_tag)\n             returnValue(None) \n-        if hmac_secret is not None and expected_digest != digest:\n-            log.warn(\"Message contains invalid digest: {msg!r}\", msg=msg.content.body)\n-            yield channel.basic_ack(delivery_tag=msg.delivery_tag)\n-            returnValue(None) \n         recorded = False\n         delay = 0\n         while not recorded and service_state.read_from_queue:\n","files":{"\/changelog_consumer\/process_changelog.py":{"changes":[{"diff":"\n     channel.basicPublish(exchange, routing_key, props, message.getBytes())\n     channel.waitForConfirmsOrDie()\n \n-def send_group_mod(channel, exchange, router, group, action_name, subject_id, hmac_secret):\n+def send_group_mod(channel, exchange, router, group, action_name, subject_id):\n     route_key = router.get_key(group, subject_id, action_name)\n-        msg = \"%s\\n%s\\n%s\" % (group, subject_id, action_name)\n-    if hmac_secret is not None:\n-        h = hmac.new(hmac_secret, msg)\n-        digest = h.hexdigest()\n-        msg = \"%s\\n%s\" % (msg, digest)\n+    msg = \"%s\\n%s\\n%s\" % (group, subject_id, action_name)\n     send_message(channel, exchange, route_key, msg)\n \n def load_route_map(fname):\n","add":2,"remove":6,"filename":"\/changelog_consumer\/process_changelog.py","badparts":["def send_group_mod(channel, exchange, router, group, action_name, subject_id, hmac_secret):","        msg = \"%s\\n%s\\n%s\" % (group, subject_id, action_name)","    if hmac_secret is not None:","        h = hmac.new(hmac_secret, msg)","        digest = h.hexdigest()","        msg = \"%s\\n%s\" % (msg, digest)"],"goodparts":["def send_group_mod(channel, exchange, router, group, action_name, subject_id):","    msg = \"%s\\n%s\\n%s\" % (group, subject_id, action_name)"]},{"diff":"\n     routefile = args.route_file\n     if routefile is None:\n         routefile = scp.get(\"APPLICATION\", \"routemap\")\n-    hmac_secret = None\n-    if scp.has_section(\"APPLICATION\", \"hmac\"):\n-        hmac_secret = scp.get(\"APPLICATION\", \"hmac\")\n     debug(\"AMQP host => '%s'\" % host)\n     debug(\"AMQP port => '%s'\" % port)\n     debug(\"AMQP vhost => '%s'\" % vhost)\n","add":0,"remove":3,"filename":"\/changelog_consumer\/process_changelog.py","badparts":["    hmac_secret = None","    if scp.has_section(\"APPLICATION\", \"hmac\"):","        hmac_secret = scp.get(\"APPLICATION\", \"hmac\")"],"goodparts":[]}],"source":"\n import argparse from ConfigParser import SafeConfigParser import datetime import hmac from operator import itemgetter import os import os.path from textwrap import dedent import time import socket from string import Template import StringIO import sys from jython_grouper import * from edu.internet2.middleware.grouper.misc import GrouperDAOFactory from edu.internet2.middleware.grouper.changeLog import ChangeLogConsumer from edu.internet2.middleware.grouper.changeLog import ChangeLogEntry, ChangeLogLabels from edu.internet2.middleware.grouper.util import GrouperUtil from edu.internet2.middleware.grouper.app.loader.db import Hib3GrouperLoaderLog from edu.internet2.middleware.grouper.app.loader import GrouperLoaderStatus from edu.internet2.middleware.grouper.app.loader import GrouperLoader from edu.internet2.middleware.grouper.app.loader import GrouperLoaderType from com.rabbitmq.client import ConnectionFactory from com.rabbitmq.client import Connection from com.rabbitmq.client import Channel from com.rabbitmq.client import QueueingConsumer from java.lang import String from java.lang import Boolean from sortedcollection import SortedCollection def get_amqp_conn(host, port, vhost, user, passwd): factory=ConnectionFactory() factory.setHost(host) factory.setPort(port) factory.setUsername(user) factory.setPassword(passwd) factory.setVirtualHost(vhost) conn=factory.newConnection() return conn def send_message(channel, exchange, route_key, msg): \"\"\" Send a message to an exchange routed by `route_key`. \"\"\" channel.confirmSelect() exchange=String(exchange) message=String(msg) routing_key=String(route_key) props=None channel.basicPublish(exchange, routing_key, props, message.getBytes()) channel.waitForConfirmsOrDie() def send_group_mod(channel, exchange, router, group, action_name, subject_id, hmac_secret): route_key=router.get_key(group, subject_id, action_name) msg=\"%s\\n%s\\n%s\" %(group, subject_id, action_name) if hmac_secret is not None: h=hmac.new(hmac_secret, msg) digest=h.hexdigest() msg=\"%s\\n%s\" %(msg, digest) send_message(channel, exchange, route_key, msg) def load_route_map(fname): routes={} f=open(fname, \"r\") for n, line in enumerate(f): line=line.strip() if line.startswith(\" continue if line==\"\": continue if not '=' in line: warn(\"Invalid route on line %s.\" %(n+1)) warn(\"route=> '%s'\" % line) continue parts=line.split(\"=\", 1) key=parts[0].strip() value=parts[1].strip() routes[key]=value return routes class Router(object): def __init__(self, route_map): self.direct_mappings={} self.patterns=SortedCollection(key=itemgetter(0)) self.default_key='' for k, v in route_map.iteritems(): if k=='_default': self.default_key=v elif k.endswith('*'): self.patterns.insert((k[:-1], Template(v))) else: self.direct_mappings[k]=v def get_key(self, group, subject_id, action_name): route_key=self.direct_mappings.get(group, None) if route_key is not None: return route_key try: result=self.patterns.find_le(group) except ValueError: return self.default_key pattern, route_key=result if not group.startswith(pattern): return self.default_key parts=group.split(':') grp=parts[-1] stem=':'.join(parts[:-1]) rk=route_key.substitute(stem=stem, group=grp, subject=subject_id, action=action_name) return rk def info(msg): sys.stderr.write(\"[INFO] %s\\n\" % msg) def debug(msg): sys.stderr.write(\"[DEBUG] %s\\n\" % msg) def warn(msg): sys.stderr.write(\"[WARNING] %s\\n\" % msg) def get_last_sequence(changefile): if os.path.exists(changefile): try: f=open(changefile, \"r\") data=f.read().strip() f.close() pos=long(data) return pos except(Exception,), ex: warn(\"Could not get last sequence number.\\n%s\\n\" % str(ex)) return None return None def update_last_sequence(changefile, n): f=open(changefile, \"w\") f.write(str(n)) f.close() def load_config(config_name): defaults=dedent(\"\"\"\\ [APPLICATION] changefile=%s routemap=%s [AMQP] host=locahost port=5672 vhost=\/ user=guest password=guest exchage=grouper_exchange \"\"\") %( os.path.join(os.curdir, \"last_change_id.txt\"), os.path.join(os.curdir, \"routes.cfg\")) scp=SafeConfigParser() buf=StringIO.StringIO(defaults) scp.readfp(buf) if config_name is not None: files=scp.read([config_name]) else: files=scp.read([ \"\/etc\/grouper\/process_changelog.cfg\", os.path.expanduser(\"~\/.process_changelog.cfg\"), os.path.join(os.path.dirname(__file__), \"process_changelog.cfg\"), os.path.join(os.path.abspath(os.curdir), \"process_changelog.cfg\")]) info(\"Read configuration from: %s\" %(', '.join(files))) return scp def main(args): scp=load_config(args.config) host=args.host if host is None: host=scp.get(\"AMQP\", \"host\") port=args.port if port is None: port=scp.getint(\"AMQP\", \"port\") vhost=args.vhost if vhost is None: vhost=scp.get(\"AMQP\", \"vhost\") user=args.user if user is None: user=scp.get(\"AMQP\", \"user\") passwd_file=args.passwd_file if passwd_file is None: passwd=scp.get(\"AMQP\", \"password\") else: passwd=passwd_file.read().strip() exchange=args.exchange if exchange is None: exchange=scp.get(\"AMQP\", \"exchange\") changefile=args.change_file if changefile is None: changefile=scp.get(\"APPLICATION\", \"changefile\") routefile=args.route_file if routefile is None: routefile=scp.get(\"APPLICATION\", \"routemap\") hmac_secret=None if scp.has_section(\"APPLICATION\", \"hmac\"): hmac_secret=scp.get(\"APPLICATION\", \"hmac\") debug(\"AMQP host=> '%s'\" % host) debug(\"AMQP port=> '%s'\" % port) debug(\"AMQP vhost=> '%s'\" % vhost) debug(\"AMQP user=> '%s'\" % user) debug(\"AMQP exchange=> '%s'\" % exchange) debug(\"AMQP changefile=> '%s'\" % changefile) debug(\"AMQP routemap=> '%s'\" % routefile) routes=load_route_map(routefile) router=Router(routes) amqp=get_amqp_conn(host, port, vhost, user, passwd) channel=amqp.createChannel() session=getRootSession() factory=GrouperDAOFactory.getFactory() consumer=factory.getChangeLogConsumer() c=ChangeLogConsumer() d=datetime.datetime.now() job_name=\"CustomJob_%s\" % d.strftime(\"%Y-%m-%dT%H:%M:%S\") c.setName(job_name) consumer.saveOrUpdate(c) last_sequence=get_last_sequence(changefile) if last_sequence is None: last_sequence=ChangeLogEntry.maxSequenceNumber(True) info(\"Last sequence number is %d.\\n\" % last_sequence) c.setLastSequenceProcessed(GrouperUtil.defaultIfNull(last_sequence, 0L).longValue()) consumer.saveOrUpdate(c) hib3=Hib3GrouperLoaderLog() hib3.setHost(GrouperUtil.hostname()) hib3.setJobName(job_name) hib3.setStatus(GrouperLoaderStatus.RUNNING.name()) attempt_num_entries=100 while True: GrouperLoader.runOnceByJobName(session, GrouperLoaderType.GROUPER_CHANGE_LOG_TEMP_TO_CHANGE_LOG) last_sequence=c.getLastSequenceProcessed() l=factory.getChangeLogEntry().retrieveBatch(last_sequence, attempt_num_entries) num_entries_retrieved=len(l) debug(\"Retrieved %d entries to process...\" % num_entries_retrieved) for n, entry in enumerate(l): action_name= entry.getChangeLogType().getActionName() if action_name !=u'addMembership' and action_name !=u'deleteMembership': continue if action_name==u'addMembership': subject_id=entry.retrieveValueForLabel(ChangeLogLabels.MEMBERSHIP_ADD.subjectId) group=entry.retrieveValueForLabel(ChangeLogLabels.MEMBERSHIP_ADD.groupName) elif action_name==u'deleteMembership': subject_id=entry.retrieveValueForLabel(ChangeLogLabels.MEMBERSHIP_DELETE.subjectId) group=entry.retrieveValueForLabel(ChangeLogLabels.MEMBERSHIP_DELETE.groupName) while True: debug(\"Attempting to send message: exchange='%s', group='%s', action='%s', subject='%s'\" %( exchange, group, action_name, subject_id)) try: send_group_mod(channel, exchange, router, group, action_name, subject_id, hmac_secret) except(Exception,), ex: warn(\"Could not send message.\\n%s\\n\" % str(ex)) time.sleep(10) continue break update_last_sequence(changefile, n+last_sequence+1) c.setLastSequenceProcessed(c.getLastSequenceProcessed() +num_entries_retrieved) consumer.saveOrUpdate(c) if num_entries_retrieved !=attempt_num_entries: time.sleep(10) if __name__==\"__main__\": parser=argparse.ArgumentParser(description=\"Process Grouper change log.\") parser.add_argument( \"-c\", \"--config\", action=\"store\", help=\"Config file to use.\") parser.add_argument( \"--host\", action=\"store\", help=\"The host where the exchange is located. Default 'localhost'.\") parser.add_argument( \"-P\", \"--port\", action=\"store\", type=int, help=\"The port on which the exchange is listening. Default 5672.\") parser.add_argument( \"-e\", \"--exchange\", action=\"store\", default=\"grouper_exchange\", help=\"The exchange to which the message is sent. Default 'grouper_exchange'.\") parser.add_argument( \"--vhost\", action=\"store\", help=\"The virtual host. Default '\/'.\") parser.add_argument( \"-u\", \"--user\", action=\"store\", help=\"The username used to connect. Default 'guest'.\") parser.add_argument( \"-p\", \"--passwd-file\", action=\"store\", type=argparse.FileType(\"r\"), help=\"A file containing the password. If not specified, 'guest' will be used as the password.\") parser.add_argument( \"--change-file\", action=\"store\", help=\"A file used to record the last change processed.\") parser.add_argument( \"--route-file\", action=\"store\", help=\"A file containing the routing patterns.\") args=parser.parse_args() main(args) ","sourceWithComments":"\n\n# Standard library\nimport argparse\nfrom ConfigParser import SafeConfigParser\nimport datetime\nimport hmac\nfrom operator import itemgetter\nimport os\nimport os.path\nfrom textwrap import dedent\nimport time\nimport socket\nfrom string import Template\nimport StringIO\nimport sys\n\n# Application modules\nfrom jython_grouper import *\n\n# External modules\nfrom edu.internet2.middleware.grouper.misc import GrouperDAOFactory\nfrom edu.internet2.middleware.grouper.changeLog import ChangeLogConsumer\nfrom edu.internet2.middleware.grouper.changeLog import ChangeLogEntry, ChangeLogLabels\nfrom edu.internet2.middleware.grouper.util import GrouperUtil\nfrom edu.internet2.middleware.grouper.app.loader.db import Hib3GrouperLoaderLog\nfrom edu.internet2.middleware.grouper.app.loader import GrouperLoaderStatus\nfrom edu.internet2.middleware.grouper.app.loader import GrouperLoader      \nfrom edu.internet2.middleware.grouper.app.loader import GrouperLoaderType\nfrom com.rabbitmq.client import ConnectionFactory              \nfrom com.rabbitmq.client import Connection       \nfrom com.rabbitmq.client import Channel                        \nfrom com.rabbitmq.client import QueueingConsumer     \nfrom java.lang import String                         \nfrom java.lang import Boolean\nfrom sortedcollection import SortedCollection\n\n# AMQP functions\ndef get_amqp_conn(host, port, vhost, user, passwd):\n    factory = ConnectionFactory()\n    factory.setHost(host)\n    factory.setPort(port)\n    factory.setUsername(user)\n    factory.setPassword(passwd)\n    factory.setVirtualHost(vhost)\n    conn = factory.newConnection()\n    return conn\n\ndef send_message(channel, exchange, route_key, msg):\n    \"\"\"\n    Send a message to an exchange routed by `route_key`.\n    \"\"\"\n    # Send message\n    channel.confirmSelect()\n    exchange = String(exchange)\n    message = String(msg)\n    routing_key = String(route_key)\n    props = None\n    channel.basicPublish(exchange, routing_key, props, message.getBytes())\n    channel.waitForConfirmsOrDie()\n\ndef send_group_mod(channel, exchange, router, group, action_name, subject_id, hmac_secret):\n    route_key = router.get_key(group, subject_id, action_name)\n        msg = \"%s\\n%s\\n%s\" % (group, subject_id, action_name)\n    if hmac_secret is not None:\n        h = hmac.new(hmac_secret, msg)\n        digest = h.hexdigest()\n        msg = \"%s\\n%s\" % (msg, digest)\n    send_message(channel, exchange, route_key, msg)\n\ndef load_route_map(fname):\n    routes = {}\n    f = open(fname, \"r\")\n    for n, line in enumerate(f):\n        line = line.strip()\n        if line.startswith(\"#\"):\n            continue\n        if line == \"\":\n            continue\n        if not '=' in line:\n            warn(\"Invalid route on line %s.\" % (n+1))\n            warn(\"route => '%s'\" % line)\n            continue\n        parts = line.split(\"=\", 1)\n        key = parts[0].strip()\n        value = parts[1].strip()\n        routes[key] = value\n    return routes\n\n\nclass Router(object):\n    def __init__(self, route_map):\n        self.direct_mappings = {}\n        self.patterns = SortedCollection(key=itemgetter(0))\n        self.default_key = ''\n        for k, v in route_map.iteritems():\n            if k == '_default':\n                self.default_key = v\n            elif k.endswith('*'):\n                self.patterns.insert((k[:-1], Template(v)))\n            else:\n                self.direct_mappings[k] = v\n    \n    def get_key(self, group, subject_id, action_name):\n        route_key = self.direct_mappings.get(group, None)\n        if route_key is not None:\n            return route_key\n        try:\n            result = self.patterns.find_le(group)\n        except ValueError:\n            return self.default_key\n        pattern, route_key = result\n        if not group.startswith(pattern):\n            return self.default_key\n        parts = group.split(':')\n        grp = parts[-1]\n        stem = ':'.join(parts[:-1])\n        rk = route_key.substitute(stem=stem, group=grp, subject=subject_id, action=action_name)\n        return rk\n            \n\n# Logging functions\ndef info(msg):\n    sys.stderr.write(\"[INFO] %s\\n\" % msg)\n\ndef debug(msg):\n    sys.stderr.write(\"[DEBUG] %s\\n\" % msg)\n\ndef warn(msg):\n    sys.stderr.write(\"[WARNING] %s\\n\" % msg)\n\n# Changelogger functions\ndef get_last_sequence(changefile):\n    if os.path.exists(changefile):\n        try:\n            f = open(changefile, \"r\")\n            data = f.read().strip()\n            f.close()\n            pos = long(data)\n            return pos\n        except (Exception,), ex:\n            warn(\"Could not get last sequence number.\\n%s\\n\" % str(ex))\n            return None\n    return None\n        \ndef update_last_sequence(changefile, n):\n    f = open(changefile, \"w\")\n    f.write(str(n))\n    f.close()\n\ndef load_config(config_name):\n    defaults = dedent(\"\"\"\\\n        [APPLICATION]\n        changefile = %s\n        routemap = %s\n\n        [AMQP]\n        host = locahost\n        port = 5672\n        vhost = \/\n        user = guest\n        password = guest\n        exchage = grouper_exchange\n        \"\"\") % (\n            os.path.join(os.curdir, \"last_change_id.txt\"),\n            os.path.join(os.curdir, \"routes.cfg\"))\n    scp = SafeConfigParser()\n    buf = StringIO.StringIO(defaults)\n    scp.readfp(buf)\n    if config_name is not None:\n        files = scp.read([config_name])\n    else:\n        files = scp.read([\n            \"\/etc\/grouper\/process_changelog.cfg\", \n            os.path.expanduser(\"~\/.process_changelog.cfg\"),\n            os.path.join(os.path.dirname(__file__), \"process_changelog.cfg\"),\n            os.path.join(os.path.abspath(os.curdir), \"process_changelog.cfg\")])\n    info(\"Read configuration from: %s\" % (', '.join(files)))\n    return scp\n        \n\ndef main(args):\n    scp = load_config(args.config)\n    host = args.host\n    if host is None:\n        host = scp.get(\"AMQP\", \"host\")\n    port = args.port\n    if port is None:\n        port = scp.getint(\"AMQP\", \"port\")\n    vhost = args.vhost\n    if vhost is None:\n        vhost = scp.get(\"AMQP\", \"vhost\")\n    user = args.user\n    if user is None:\n        user = scp.get(\"AMQP\", \"user\")\n    passwd_file = args.passwd_file\n    if passwd_file is None:\n        passwd = scp.get(\"AMQP\", \"password\")\n    else:\n        passwd = passwd_file.read().strip()\n    exchange = args.exchange\n    if exchange is None:\n        exchange = scp.get(\"AMQP\", \"exchange\")\n    changefile = args.change_file\n    if changefile is None:\n        changefile = scp.get(\"APPLICATION\", \"changefile\")\n    routefile = args.route_file\n    if routefile is None:\n        routefile = scp.get(\"APPLICATION\", \"routemap\")\n    hmac_secret = None\n    if scp.has_section(\"APPLICATION\", \"hmac\"):\n        hmac_secret = scp.get(\"APPLICATION\", \"hmac\")\n    debug(\"AMQP host => '%s'\" % host)\n    debug(\"AMQP port => '%s'\" % port)\n    debug(\"AMQP vhost => '%s'\" % vhost)\n    debug(\"AMQP user => '%s'\" % user)\n    debug(\"AMQP exchange => '%s'\" % exchange)\n    debug(\"AMQP changefile => '%s'\" % changefile)\n    debug(\"AMQP routemap => '%s'\" % routefile)\n    # Read routes.\n    routes = load_route_map(routefile)\n    router = Router(routes)\n    # Connect to message queue.\n    amqp = get_amqp_conn(host, port, vhost, user, passwd)\n    channel = amqp.createChannel()                 \n    # END connecto to message queue.\n    session = getRootSession()\n    factory = GrouperDAOFactory.getFactory()\n    consumer = factory.getChangeLogConsumer()\n    c = ChangeLogConsumer()\n    d = datetime.datetime.now()\n    job_name = \"CustomJob_%s\" % d.strftime(\"%Y-%m-%dT%H:%M:%S\")\n    c.setName(job_name)\n    consumer.saveOrUpdate(c)\n    # Prime the last sequence number.\n    last_sequence = get_last_sequence(changefile)\n    if last_sequence is None:\n        last_sequence = ChangeLogEntry.maxSequenceNumber(True)\n    info(\"Last sequence number is %d.\\n\" % last_sequence)\n    c.setLastSequenceProcessed(GrouperUtil.defaultIfNull(last_sequence, 0L).longValue()) \n    consumer.saveOrUpdate(c)\n    # Initialize the consumer job.\n    hib3 = Hib3GrouperLoaderLog()\n    hib3.setHost(GrouperUtil.hostname())\n    hib3.setJobName(job_name)\n    hib3.setStatus(GrouperLoaderStatus.RUNNING.name())\n    # Begin the main loop.\n    attempt_num_entries = 100\n    while True:\n        GrouperLoader.runOnceByJobName(session, GrouperLoaderType.GROUPER_CHANGE_LOG_TEMP_TO_CHANGE_LOG)\n        last_sequence = c.getLastSequenceProcessed()\n        l = factory.getChangeLogEntry().retrieveBatch(last_sequence, attempt_num_entries)\n        num_entries_retrieved = len(l)\n        debug(\"Retrieved %d entries to process ...\" % num_entries_retrieved)\n        for n, entry in enumerate(l):\n            action_name =  entry.getChangeLogType().getActionName()\n            if action_name != u'addMembership' and action_name != u'deleteMembership':\n                continue\n            if action_name == u'addMembership':\n                subject_id = entry.retrieveValueForLabel(ChangeLogLabels.MEMBERSHIP_ADD.subjectId)\n                group = entry.retrieveValueForLabel(ChangeLogLabels.MEMBERSHIP_ADD.groupName)\n            elif action_name == u'deleteMembership':\n                subject_id = entry.retrieveValueForLabel(ChangeLogLabels.MEMBERSHIP_DELETE.subjectId)\n                group = entry.retrieveValueForLabel(ChangeLogLabels.MEMBERSHIP_DELETE.groupName)\n            while True:\n                debug(\"Attempting to send message: exchange='%s', group='%s', action='%s', subject='%s'\" % (\n                    exchange, group, action_name, subject_id))\n                try:\n                    send_group_mod(channel, exchange, router, group, action_name, subject_id, hmac_secret)\n                except (Exception, ), ex:\n                    warn(\"Could not send message.\\n%s\\n\" % str(ex))\n                    time.sleep(10)\n                    continue\n                break\n            update_last_sequence(changefile, n+last_sequence+1)\n\n        c.setLastSequenceProcessed(c.getLastSequenceProcessed() + num_entries_retrieved)\n        consumer.saveOrUpdate(c)\n        if num_entries_retrieved != attempt_num_entries:\n            time.sleep(10)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Process Grouper change log.\")\n    parser.add_argument(\n        \"-c\",\n        \"--config\",\n        action=\"store\",\n        help=\"Config file to use.\")\n    parser.add_argument(\n        \"--host\",\n        action=\"store\",\n        help=\"The host where the exchange is located.  Default 'localhost'.\")\n    parser.add_argument(\n        \"-P\",\n        \"--port\",\n        action=\"store\",\n        type=int,\n        help=\"The port on which the exchange is listening.  Default 5672.\")\n    parser.add_argument(\n        \"-e\",\n        \"--exchange\",\n        action=\"store\",\n        default=\"grouper_exchange\",\n        help=\"The exchange to which the message is sent.  Default 'grouper_exchange'.\")\n    parser.add_argument(\n        \"--vhost\",\n        action=\"store\",\n        help=\"The virtual host.  Default '\/'.\")\n    parser.add_argument(\n        \"-u\",\n        \"--user\",\n        action=\"store\",\n        help=\"The username used to connect.  Default 'guest'.\")\n    parser.add_argument(\n        \"-p\",\n        \"--passwd-file\",\n        action=\"store\",\n        type=argparse.FileType(\"r\"),\n        help=\"A file containing the password.  If not specified, 'guest' will be used as the password.\")\n    parser.add_argument(\n        \"--change-file\",\n        action=\"store\",\n        help=\"A file used to record the last change processed.\")\n    parser.add_argument(\n        \"--route-file\",\n        action=\"store\",\n        help=\"A file containing the routing patterns.\")\n    args = parser.parse_args()\n    main(args)\n\n"},"\/provisioner\/txgroupprovisioner\/service.py":{"changes":[{"diff":"\n \n # Standard library\n from __future__ import print_function\n-import hmac\n from json import load\n import os\n import os.path\n import sys\n from textwrap import dedent\n+\n # External modules\n # - Twisted\n from twisted.application import service\n","add":1,"remove":1,"filename":"\/provisioner\/txgroupprovisioner\/service.py","badparts":["import hmac"],"goodparts":[]}],"source":"\n from __future__ import print_function import hmac from json import load import os import os.path import sys from textwrap import dedent from twisted.application import service from twisted.application.service import Service from twisted.internet import reactor, task from twisted.internet.defer import inlineCallbacks, returnValue, DeferredList from twisted.internet.endpoints import clientFromString, connectProtocol from twisted.internet.protocol import ClientCreator from twisted.logger import Logger from txamqp.client import TwistedDelegate from txamqp.protocol import AMQClient from txamqp.queue import Closed as QueueClosedError import txamqp.spec from config import load_config, section2dict from interface import IProvisionerFactory from logging import make_syslog_observer from utils import get_plugin_factory class ServiceState(object): db_str=None last_update=None amqp_info=None read_from_queue=False class GroupProvisionerService(Service): log=None consumerTag=\"mytag\" def __init__( self, a_reactor=None, config=None, use_syslog=False, syslog_prefix=None, logfile=None): \"\"\" Initialize the service. :param a_reactor: Override the reactor to use. \"\"\" if a_reactor is None: a_reactor=reactor self._reactor=a_reactor self._port=None self.service_state=ServiceState() self.use_syslog=use_syslog self.syslog_prefix=syslog_prefix self.config=config self.amqpLooper=None def startService(self): \"\"\" Start the service. \"\"\" scp=load_config( config_file=self.config, defaults=self.make_config_defaults()) self.scp=scp app_info=section2dict(scp, \"APPLICATION\") log_level=app_info.get(\"log_level\", \"INFO\") log=Logger( observer=make_syslog_observer( log_level, prefix=self.syslog_prefix)) self.log=log self.amqp_info=section2dict(scp, \"AMQP\") amqp_log_level=self.amqp_info.get(\"log_level\", log_level) self.amqp_log=Logger( observer=make_syslog_observer( amqp_log_level, prefix=self.syslog_prefix)) service_state=self.service_state service_state.last_update=None self.start_amqp_client() provisioner_tag=app_info['provisioner'] log.info(\"Provisioner tag=> '{provisioner}'\", provisioner=provisioner_tag) provisioner_factory=get_plugin_factory(provisioner_tag, IProvisionerFactory) if provisioner_factory is None: log.error(\"No provisioner factory was found!\") reactor.stop() provisioner=provisioner_factory.generateProvisioner() provisioner.service_state=service_state provisioner.load_config( config_file=self.config, default_log_level=log_level, syslog_prefix=self.syslog_prefix) self.provisioner=provisioner def start_amqp_client(self): amqp_info=self.amqp_info log=self.amqp_log endpoint_str=amqp_info['endpoint'] exchange=amqp_info['exchange'] vhost=amqp_info['vhost'] spec_path=amqp_info['spec'] queue_name=amqp_info['queue'] route_file=amqp_info['route_map'] user=amqp_info['user'] passwd=amqp_info['passwd'] creds=(user, passwd) bindings=self.parse_bindings(route_file) queue_names=set([q for q, rk in bindings]) queue_names.add(queue_name) log.debug( \"endpoint='{endpoint}', exchange='{exchange}', vhost='{vhost}', user='{user}, spec={spec}'\", endpoint=endpoint_str, exchange=exchange, vhost=vhost, user=user, spec=spec_path) for q in sorted(queue_names): log.debug(\"Declared: queue='{queue}'\", queue=q) for q, rk in bindings: log.debug(\"Binding: queue='{queue}', route_key='{route_key}'\", queue=q, route_key=rk) delegate=TwistedDelegate() spec=txamqp.spec.load(spec_path) ep=clientFromString(self._reactor, endpoint_str) d=connectProtocol( ep, AMQClient( delegate=delegate, vhost=vhost, spec=spec)) d.addCallback(self.on_amqp_connect, exchange, queue_name, queue_names, bindings, creds) def onError(err): if reactor.running: log.failure(err) reactor.stop() d.addErrback(onError) def parse_bindings(self, fname): \"\"\" Queue map should be a JSON list of(queue_name, route_key) mappings. \"\"\" with open(fname, \"r\") as f: o=load(f) return o @inlineCallbacks def on_amqp_connect(self, conn, exchange, queue_name, queue_names, bindings, creds): log=self.amqp_log provisioner=self.provisioner service_state=self.service_state log.info(\"Connected.\") self.amqpConn=conn user, passwd=creds yield conn.authenticate(user, passwd) log.info(\"Authenticated.\") channel=yield conn.channel(1) self.amqpChannel=channel yield channel.channel_open() log.info(\"Channel opened.\") for name in queue_names: yield channel.queue_declare(queue=name, durable=True) log.info(\"Queues declared.\") yield channel.exchange_declare(exchange=exchange, type='topic') log.info(\"Exchange declared.\") for qname, route_key in bindings: yield channel.queue_bind(exchange=exchange, queue=qname, routing_key=route_key) log.info(\"Routings have been mapped.\") self.createAMQPMessageLoop() self.startAMQPMessageLoop() def createAMQPMessageLoop(self): log=self.amqp_log amqpLooper=task.LoopingCall(self.processAMQPMessage) self.amqpLooper=amqpLooper def startAMQPMessageLoop(self): if not self.service_state.read_from_queue: log=self.amqp_log log.debug(\"Starting the AMQP message loop...\") self.service_state.read_from_queue=True d=self.startConsumingFromQueue() def startLoop_(result): try: self.amqpLooper.start(0) except Exception as ex: log.error(\"An error occured during AMQP message processing:{error}\", error=str(ex)) raise d.addCallback(startLoop_) def stopAMQPMessageLoop(self): if self.service_state.read_from_queue: log=self.amqp_log self.amqpLooper.stop() self.service_state.read_from_queue=False def logStopped_(result): log.debug(\"Stopped the AMQP message loop.\") d=self.amqpChannel.basic_cancel(consumer_tag=self.consumerTag) d.addCallback(logStopped_) @inlineCallbacks def startConsumingFromQueue(self): log=self.amqp_log queue_name=self.amqp_info['queue'] channel=self.amqpChannel conn=self.amqpConn log.debug(\"AMQP: starting basic_consume() for queue '{0}'...\".format(queue_name)) yield channel.basic_consume(queue=queue_name, consumer_tag=self.consumerTag) self.amqpQueue=yield conn.queue(self.consumerTag) log.debug(\"AMQP: basic_consume() started.\") @inlineCallbacks def processAMQPMessage(self): channel=self.amqpChannel queue=self.amqpQueue reactor=self._reactor log=self.amqp_log provisioner=self.provisioner service_state=self.service_state hmac_secret=self.amqp_info.get('hmac', None) log.debug(\"Attempting to read an AMQP message...\") try: msg=yield queue.get() except QueueClosedError: log.warn(\"Queue closed--message not processed.\") returnValue(None) if not service_state.read_from_queue: returnValue(None) log.debug('Received: \"{msg}\" from channel parts=msg.content.body.split(\"\\n\") if len(parts)==4 and parts[3].strip()==\"\": parts=parts[:3] if len(parts)==3 and hmac_secret is None: group=parts[0] subject=parts[1] action=parts[2] digest=None expected_digest=None elif len(parts)==4 and hmac_secret is not None: group=parts[0] subject=parts[1] action=parts[2] digest=parts[3] expected_digest=hmac.new(hmac_secret, '\\n'.join(parts[:3])).hexdigest() else: log.warn(\"Skipping invalid message:{msg!r}\", msg=msg.content.body) yield channel.basic_ack(delivery_tag=msg.delivery_tag) returnValue(None) if hmac_secret is not None and expected_digest !=digest: log.warn(\"Message contains invalid digest:{msg!r}\", msg=msg.content.body) yield channel.basic_ack(delivery_tag=msg.delivery_tag) returnValue(None) recorded=False delay=0 while not recorded and service_state.read_from_queue: try: yield task.deferLater(reactor, delay, provisioner.provision, group, subject, action) except Exception as ex: log.error(\"Could not record message from queue. Error was:{error}\", error=ex) delay=min(600, max(delay+20, delay*2)) else: recorded=True delay=0 yield channel.basic_ack(delivery_tag=msg.delivery_tag) log.debug(\"Message from queue recorded.\") @inlineCallbacks def closeAMQPConnection(self): conn=self.amqpConn channel=self.amqpChannel log.info(\"Closing AMQP channel...\") try: yield channel.channel_close() except Exception as ex: log.error(\"Error while trying to close AMQP channel:{error}\", error=ex) else: log.info(\"AMQP Channel closed.\") log.info(\"Closing AMQP connection...\") try: yield conn.connection_close() except Exception as ex: log.error(\"Error while trying to close AMQP connection:{error}\", error=ex) else: log.info(\"AMQP Connection closed.\") def set_listening_port(self, port): self._port=port def stopService(self): \"\"\" Stop the service. \"\"\" self.service_state.read_from_queue=False async_tasks=[] if self._port is not None: async_tasks.append(self._port.stopListening()) async_tasks.append(self.closeAMQPConnection()) return DeferredList(async_tasks, consumeErrors=True) def make_config_defaults(self): spec_dir=os.path.join(os.path.split(os.path.split(__file__)[0])[0], \"spec\") spec_path=os.path.join(spec_dir, \"amqp0-9-1.stripped.xml\") return dedent(\"\"\"\\ [APPLICATION] log_level=DEBUG provisioner=ldap [AMQP] log_level=INFO endpoint=tcp:host=localhost:port=5672 exchange=grouper_exchange vhost=\/ spec={spec_path} user=guest passwd=guest \"\"\".format(spec_path=spec_path)) def main(): service=GroupProvisionerService() service.startService() reactor.run() if __name__==\"__main__\": main() else: application=service.Application(\"Twisted Group Provisioner\") service=GroupProvisionerService() service.setServiceParent(application) ","sourceWithComments":"#! \/usr\/bin\/env python\n\n# Standard library\nfrom __future__ import print_function\nimport hmac\nfrom json import load\nimport os\nimport os.path\nimport sys\nfrom textwrap import dedent\n# External modules\n# - Twisted\nfrom twisted.application import service\nfrom twisted.application.service import Service\nfrom twisted.internet import reactor, task\nfrom twisted.internet.defer import inlineCallbacks, returnValue, DeferredList\nfrom twisted.internet.endpoints import clientFromString, connectProtocol\nfrom twisted.internet.protocol import ClientCreator\nfrom twisted.logger import Logger\nfrom txamqp.client import TwistedDelegate\nfrom txamqp.protocol import AMQClient\nfrom txamqp.queue import Closed as QueueClosedError\nimport txamqp.spec\n# - application\nfrom config import load_config, section2dict\nfrom interface import IProvisionerFactory\nfrom logging import make_syslog_observer\nfrom utils import get_plugin_factory\n\n\nclass ServiceState(object):\n    db_str = None\n    last_update = None\n    amqp_info = None\n    read_from_queue = False\n\nclass GroupProvisionerService(Service):\n    log = None\n    consumerTag = \"mytag\"\n\n    def __init__(\n            self, \n            a_reactor=None, \n            config=None, \n            use_syslog=False, \n            syslog_prefix=None,\n            logfile=None):\n        \"\"\"\n        Initialize the service.\n        \n        :param a_reactor: Override the reactor to use.\n        \"\"\"\n        if a_reactor is None:\n            a_reactor = reactor\n        self._reactor = a_reactor\n        self._port = None\n        self.service_state = ServiceState()\n        self.use_syslog = use_syslog\n        self.syslog_prefix = syslog_prefix\n        self.config = config\n        self.amqpLooper = None\n        \n    def startService(self):\n        \"\"\"\n        Start the service.\n        \"\"\"\n        scp = load_config(\n            config_file=self.config, defaults=self.make_config_defaults())\n        self.scp = scp\n        app_info = section2dict(scp, \"APPLICATION\")\n        log_level = app_info.get(\"log_level\", \"INFO\")\n        log = Logger(\n            observer=make_syslog_observer(\n                log_level, \n                prefix=self.syslog_prefix))\n        self.log = log\n        self.amqp_info = section2dict(scp, \"AMQP\")\n        amqp_log_level = self.amqp_info.get(\"log_level\", log_level) \n        self.amqp_log = Logger(\n            observer=make_syslog_observer(\n                amqp_log_level, \n                prefix=self.syslog_prefix))\n        service_state = self.service_state \n        service_state.last_update = None\n        self.start_amqp_client()\n        provisioner_tag = app_info['provisioner']\n        log.info(\"Provisioner tag => '{provisioner}'\", provisioner=provisioner_tag)\n        provisioner_factory = get_plugin_factory(provisioner_tag, IProvisionerFactory)\n        if provisioner_factory is None:\n            log.error(\"No provisioner factory was found!\")\n            reactor.stop()\n        provisioner = provisioner_factory.generateProvisioner()\n        provisioner.service_state = service_state\n        provisioner.load_config(\n            config_file=self.config, \n            default_log_level=log_level, \n            syslog_prefix=self.syslog_prefix)\n        self.provisioner = provisioner\n\n    def start_amqp_client(self):\n        amqp_info = self.amqp_info\n        log = self.amqp_log\n        endpoint_str = amqp_info['endpoint']\n        exchange = amqp_info['exchange']\n        vhost = amqp_info['vhost']\n        spec_path = amqp_info['spec']\n        queue_name = amqp_info['queue']\n        route_file = amqp_info['route_map']\n        user = amqp_info['user']\n        passwd = amqp_info['passwd']\n        creds = (user, passwd)\n        bindings = self.parse_bindings(route_file)\n        queue_names = set([q for q, rk in bindings])\n        queue_names.add(queue_name)\n        log.debug(\n            \"endpoint='{endpoint}', exchange='{exchange}', vhost='{vhost}', user='{user}, spec={spec}'\",\n            endpoint=endpoint_str, exchange=exchange, vhost=vhost, user=user, spec=spec_path)\n        for q in sorted(queue_names):\n            log.debug(\"Declared: queue='{queue}'\", queue=q)\n        for q, rk in bindings:\n            log.debug(\"Binding: queue='{queue}', route_key='{route_key}'\", queue=q, route_key=rk)\n        delegate = TwistedDelegate()\n        spec = txamqp.spec.load(spec_path)\n        ep = clientFromString(self._reactor, endpoint_str)\n        d = connectProtocol(\n            ep, \n            AMQClient( \n                delegate=delegate, \n                vhost=vhost, \n                spec=spec))\n        d.addCallback(self.on_amqp_connect, exchange, queue_name, queue_names, bindings, creds)\n\n        def onError(err):\n            if reactor.running:\n                log.failure(err)\n                reactor.stop()\n\n        d.addErrback(onError)\n\n    def parse_bindings(self, fname):\n        \"\"\"\n        Queue map should be a JSON list of (queue_name, route_key) mappings.\n        \"\"\"\n        with open(fname, \"r\") as f:\n            o = load(f)\n            return o\n\n    @inlineCallbacks\n    def on_amqp_connect(self, conn, exchange, queue_name, queue_names, bindings, creds):\n        log = self.amqp_log\n        provisioner = self.provisioner\n        service_state = self.service_state\n        log.info(\"Connected.\")\n        self.amqpConn = conn\n        user, passwd = creds\n        yield conn.authenticate(user, passwd)\n        log.info(\"Authenticated.\")\n        channel = yield conn.channel(1)\n        self.amqpChannel = channel\n        yield channel.channel_open()\n        log.info(\"Channel opened.\")\n        for name in queue_names:\n            yield channel.queue_declare(queue=name, durable=True)\n        log.info(\"Queues declared.\")\n        yield channel.exchange_declare(exchange=exchange, type='topic')\n        log.info(\"Exchange declared.\")\n        for qname, route_key in bindings:\n            yield channel.queue_bind(exchange=exchange, queue=qname, routing_key=route_key)\n        log.info(\"Routings have been mapped.\")\n        self.createAMQPMessageLoop()\n        self.startAMQPMessageLoop()\n\n    def createAMQPMessageLoop(self):\n        log = self.amqp_log\n        amqpLooper = task.LoopingCall(self.processAMQPMessage)\n        self.amqpLooper = amqpLooper\n\n    def startAMQPMessageLoop(self):\n        if not self.service_state.read_from_queue: \n            log = self.amqp_log\n            log.debug(\"Starting the AMQP message loop ...\")\n            self.service_state.read_from_queue = True\n            d = self.startConsumingFromQueue()\n            def startLoop_(result):\n                try:\n                    self.amqpLooper.start(0)\n                except Exception as ex:\n                    log.error(\"An error occured during AMQP message processing: {error}\", error=str(ex))\n                    raise\n\n            d.addCallback(startLoop_)\n\n    def stopAMQPMessageLoop(self):\n        if self.service_state.read_from_queue: \n            log = self.amqp_log\n            self.amqpLooper.stop()\n            self.service_state.read_from_queue = False\n\n            def logStopped_(result):\n                log.debug(\"Stopped the AMQP message loop.\")\n\n            d = self.amqpChannel.basic_cancel(consumer_tag=self.consumerTag)\n            d.addCallback(logStopped_)\n\n    @inlineCallbacks\n    def startConsumingFromQueue(self):\n        log = self.amqp_log\n        queue_name = self.amqp_info['queue']\n        channel = self.amqpChannel\n        conn = self.amqpConn\n        log.debug(\"AMQP: starting basic_consume() for queue '{0}' ...\".format(queue_name))\n        yield channel.basic_consume(queue=queue_name, consumer_tag=self.consumerTag)\n        self.amqpQueue = yield conn.queue(self.consumerTag) \n        log.debug(\"AMQP: basic_consume() started.\")\n\n    @inlineCallbacks\n    def processAMQPMessage(self):\n        channel = self.amqpChannel\n        queue = self.amqpQueue\n        reactor = self._reactor\n        log = self.amqp_log\n        provisioner = self.provisioner\n        service_state = self.service_state\n        hmac_secret = self.amqp_info.get('hmac', None)\n        log.debug(\"Attempting to read an AMQP message ...\")\n        try:\n            msg = yield queue.get()\n        except QueueClosedError:\n            log.warn(\"Queue closed-- message not processed.\")\n            returnValue(None) \n        if not service_state.read_from_queue:\n            returnValue(None) \n        log.debug('Received: \"{msg}\" from channel # {channel}.', msg=msg.content.body, channel=channel.id)\n        parts = msg.content.body.split(\"\\n\")\n        if len(parts) == 4 and parts[3].strip() == \"\":\n            parts = parts[:3]\n        if len(parts) == 3 and hmac_secret is None:\n            group = parts[0]\n            subject = parts[1]\n            action = parts[2]\n            digest = None\n            expected_digest = None\n        elif  len(parts) == 4 and hmac_secret is not None:\n            group = parts[0]\n            subject = parts[1]\n            action = parts[2]\n            digest = parts[3]\n            expected_digest = hmac.new(hmac_secret, '\\n'.join(parts[:3])).hexdigest()\n        else:\n            log.warn(\"Skipping invalid message: {msg!r}\", msg=msg.content.body)\n            yield channel.basic_ack(delivery_tag=msg.delivery_tag)\n            returnValue(None) \n        if hmac_secret is not None and expected_digest != digest:\n            log.warn(\"Message contains invalid digest: {msg!r}\", msg=msg.content.body)\n            yield channel.basic_ack(delivery_tag=msg.delivery_tag)\n            returnValue(None) \n        recorded = False\n        delay = 0\n        while not recorded and service_state.read_from_queue:\n            try:\n                yield task.deferLater(reactor, delay, provisioner.provision, group, subject, action)\n            except Exception as ex:\n                log.error(\"Could not record message from queue.  Error was: {error}\", error=ex)\n                delay = min(600, max(delay+20, delay*2))\n            else:\n                recorded = True\n                delay = 0    \n                yield channel.basic_ack(delivery_tag=msg.delivery_tag)\n                log.debug(\"Message from queue recorded.\")\n        \n    @inlineCallbacks\n    def closeAMQPConnection(self):\n        conn = self.amqpConn\n        channel = self.amqpChannel\n        log.info(\"Closing AMQP channel ...\")\n        try:\n            yield channel.channel_close()\n        except Exception as ex:\n            log.error(\"Error while trying to close AMQP channel: {error}\", error=ex)\n        else:\n            log.info(\"AMQP Channel closed.\")\n        log.info(\"Closing AMQP connection ...\")\n        try:\n            yield conn.connection_close()\n        except Exception as ex:\n            log.error(\"Error while trying to close AMQP connection: {error}\", error=ex)\n        else:\n            log.info(\"AMQP Connection closed.\")\n\n    def set_listening_port(self, port):\n        self._port = port\n        \n    def stopService(self):\n        \"\"\"\n        Stop the service.\n        \"\"\"\n        self.service_state.read_from_queue = False\n        async_tasks = []\n        if self._port is not None:\n            async_tasks.append(self._port.stopListening())\n        async_tasks.append(self.closeAMQPConnection())\n        return DeferredList(async_tasks, consumeErrors=True)             \n\n    def make_config_defaults(self):\n        spec_dir = os.path.join(os.path.split(os.path.split(__file__)[0])[0], \"spec\")\n        spec_path = os.path.join(spec_dir, \"amqp0-9-1.stripped.xml\")\n        return dedent(\"\"\"\\\n            [APPLICATION]\n            log_level = DEBUG\n            provisioner = ldap\n            \n            [AMQP]\n            log_level = INFO\n            endpoint = tcp:host=localhost:port=5672\n            exchange = grouper_exchange\n            vhost = \/\n            spec = {spec_path}\n            user = guest\n            passwd = guest\n            \"\"\".format(spec_path=spec_path))\n\n    \ndef main():\n    service = GroupProvisionerService()\n    service.startService()\n    reactor.run()\n\nif __name__ == \"__main__\":\n    main()\nelse:\n    application = service.Application(\"Twisted Group Provisioner\")\n    service = GroupProvisionerService()\n    service.setServiceParent(application)\n    \n    \n    \n"}},"msg":"Revert \"UNTESTED: Added the ability to transmit the message with an HMAC to prevent tampering.\"\n\nThis reverts commit 47afc32143ac38951109d5ec2281b56bf69678d7.\n\nThis idea does not work.  It does prevent tampering, and the lack of\nconfidentiality is not an issue per se, but without identity there is\nno way to stop an attacker from replaying messages without adding\nadditional information to the messages (e.g. sequence numbers).\n\nEventually, this idea will turn into something almost entirely but\nnot quite unlike TLS."}},"https:\/\/github.com\/raghu429\/RADAR_DataIntegrity":{"12063ab4b979758194f7325579ee15fb25ab6913":{"url":"https:\/\/api.github.com\/repos\/raghu429\/RADAR_DataIntegrity\/commits\/12063ab4b979758194f7325579ee15fb25ab6913","html_url":"https:\/\/github.com\/raghu429\/RADAR_DataIntegrity\/commit\/12063ab4b979758194f7325579ee15fb25ab6913","message":"the end to end pipe-line implemented and tested for add, del and modify functions. Code read from data_x.txt only the RADAR data and encodes it, adds required noise and then tampers it with give attack vector and then decodes it and checks if the attack vectors are localized","sha":"12063ab4b979758194f7325579ee15fb25ab6913","keyword":"tampering attack","diff":"diff --git a\/accuracy_evaluation.py b\/accuracy_evaluation.py\nindex 5d2b463..d2b9a75 100644\n--- a\/accuracy_evaluation.py\n+++ b\/accuracy_evaluation.py\n@@ -1,5 +1,10 @@\n # -*- coding: utf-8 -*-\n import numpy as np\n+import random\n+\n+from tools import polar_to_cartesian, cartesian_to_polar\n+from twoD_QIM import qim_quantize_twobits, getPointCloud_from_quantizedValues, qim_decode, NUMBITS, qim_dummy_encoded_pc, get_tamperedindices_twobits\n+\n \n # #** This piece of code contants methods to evaluate the 2D QIM embedding for the given Udacity data-set. We can further split this file into three different files if needed (most of the previously developed code could be re-used)\n # First - read the data file and generate a separate binary file with just the radar data\n@@ -8,7 +13,61 @@\n # get the detections out of the modified files and report the false alarms, error rate and accuracy (if the algorithm is able to detect the location of the modified data)\n # **#\n \n-#these three fucntions check for the midding indices, added indices and modified indices assuming the input is a list of repeating pattens\n+\n+def radarDataExtractor(filePath):\n+    with open(filePath) as read_file:\n+        # read_file = open(filePath, \"R\")\n+        radar_data = []\n+        for line in read_file:\n+            data = line.split()\n+            if(data[0] == 'R'):\n+                [x, y, vx, vy] = polar_to_cartesian(float(data[1]),float(data[2]),float(data[3]))\n+                radar_data.append([x, y])\n+    return radar_data\n+\n+def tamperRadarAddUniformNoise(noise_sigma, data_in):\n+    uniform_noise_added_data = data_in + np.random.uniform(-noise_sigma, noise_sigma, 2*len(data_in)).reshape(-1,2)\n+    return uniform_noise_added_data\n+\n+\n+def tamperRadarAddTracklets(data_in):\n+    data_out = data_in\n+    #for now this is fixed but this can be variable as-well\n+    copy_row_num = 2\n+    add_location_list = []\n+    for i in range (1,2):\n+        # print('data length ={}'.format(np.shape(data_in)[0]))\n+        r1 = random.randint(0, np.shape(data_in)[0]-2)\n+        #copy a rwo element from the data\n+        copy_row = data_in[copy_row_num+i]\n+        #paste it at a random location\n+        data_out = np.insert(data_out, r1, copy_row, axis = 0)\n+        add_location_list.append(r1)\n+    return add_location_list, data_out\n+\n+def tamperRadarDeleteTracklets(data_in):\n+    data_out = data_in\n+    del_location_list = []\n+    for i in range (1,2):\n+        r1 = random.randint(0, np.shape(data_in)[0]-2)\n+        #delete a rwo element from the data\n+        data_out =  np.delete(data_out, r1, axis = 0)\n+        del_location_list.append(r1)\n+    return del_location_list, data_out\n+\n+def tamperRadarModifyTracklets(data_in):\n+    data_out = data_in\n+    mod_location_list = []\n+    for i in range (1,2):\n+        r1 = random.randint(0, np.shape(data_in)[0]-2)\n+        #modify a rwo element in the data\n+        data_out[r1] =  data_out[r1+1]\n+        mod_location_list.append(r1)\n+    return mod_location_list, data_out\n+\n+\n+\n+#these three functions check for the missing indices, added indices and modified indices assuming the input is a list of repeating pattens\n \n def findDeletedIndices(gt_list, mod_list):\n     #here the assumption is that the lenth of the mod_list < gt_list\n@@ -56,26 +115,82 @@ def findAddedIndices(gt_list, mod_list):\n \n \n if __name__ == '__main__':\n-    #test missing\n-    print('******Deleted Elements Test******')\n-    list_orig = [1,2,3,1,2,3,1,2,3,1,2]\n-    list_mod = [1,1,2,1,1,2]\n-    missing_indices = findDeletedIndices(list_orig, list_mod)\n-    print('missing indices = {}'.format(missing_indices))\n-    print('\\n')\n-\n-    #test different\n-    print('*****Modified Elements Test******')\n-    list_orig = [1,2,3,1,2,3,1,2,3,1,2]\n-    list_mod = [1,2,2,1,2,3,1,2,3,1,3]\n-    different_indices = findModifiedIndices(list_orig, list_mod)\n-    print('different indices = {}'.format(different_indices))\n-    print('\\n')\n-\n-    #test addition\n-    print('*****Modified Elements Test******')\n-    list_orig = [1,2,3,1,2,3,1,2,3,1,2]\n-    list_mod = [1,5,2,3,1,2,3,0,1,2,3,1,2]\n-    added_indices = findAddedIndices(list_orig, list_mod)\n-    print('added indices = {}'.format(added_indices))\n-    print('\\n')\n+    # #test missing\n+    # print('******Deleted Elements Test******')\n+    # list_orig = [1,2,3,4,1,2,3,4,1,2,3,4,1,2]\n+    # list_mod = [1,1,2,1,1,2]\n+    # missing_indices = findDeletedIndices(list_orig, list_mod)\n+    # print('missing indices = {}'.format(missing_indices))\n+    # print('\\n')\n+\n+    # #test different\n+    # print('*****Modified Elements Test******')\n+    # list_orig = [1,2,3,1,2,3,1,2,3,1,2]\n+    # list_mod = [1,2,2,1,2,3,1,2,3,1,3]\n+    # different_indices = findModifiedIndices(list_orig, list_mod)\n+    # print('different indices = {}'.format(different_indices))\n+    # print('\\n')\n+\n+    # #test addition\n+    # print('*****Modified Elements Test******')\n+    # list_orig = [1,2,3,1,2,3,1,2,3,1,2]\n+    # list_mod = [1,2,2,3,1,2,3,0,1,2,3,1,2]\n+    # added_indices = findAddedIndices(list_orig, list_mod)\n+    # print('added indices = {}'.format(added_indices))\n+    # print('\\n')\n+\n+\n+    #extract radar data\n+    radar_data_predictions = radarDataExtractor(\"data\/data-2.txt\")\n+    print('original data:', radar_data_predictions)\n+    radar_data_predictions = np.array([radar_data_predictions]).reshape(-1,2)\n+    #encode the data for a given step-size\n+    voxel_halfdelta = qim_quantize_twobits(radar_data_predictions)\n+    voxel_halfdelta_npy = np.array([voxel_halfdelta]).reshape(-1,2)\n+    print('quant encoded shape', voxel_halfdelta_npy.shape, voxel_halfdelta_npy)\n+    \n+    qim_encoded_pointcloud = getPointCloud_from_quantizedValues(  np.copy(voxel_halfdelta_npy))\n+    print('encoded data', qim_encoded_pointcloud)\n+\n+    pred_plusNoise = tamperRadarAddUniformNoise(0.001, qim_encoded_pointcloud)\n+    print('noise added pc={}'.format(pred_plusNoise))\n+    \n+    #open the encoded data and tamper it by adding noise and attack vectors\n+    # added_indices, pred_plus_added = tamperRadarAddTracklets(pred_plusNoise)\n+    added_indices, pred_plus_added = tamperRadarDeleteTracklets(pred_plusNoise)\n+    # added_indices, pred_plus_added = tamperRadarModifyTracklets(pred_plusNoise)\n+\n+    \n+    print('added indices:{}'.format(added_indices))\n+    print('modified data:{}'.format(pred_plus_added))\n+    #decode the tampered point cloud\n+\n+    decoded_CB, decoded_quantized_values = qim_decode(np.copy(pred_plus_added))\n+    print('decoded codebook', decoded_CB)\n+    decoded_codebook = np.array([decoded_CB]).reshape(-1,NUMBITS)\n+\n+    #calculate the BER\n+    encoded_cb = qim_dummy_encoded_pc(len(radar_data_predictions.reshape(-1,NUMBITS)))\n+    print('encoded cb', encoded_cb)\n+    # uncomment below to test if the tamper index finder is working\n+    #encoded_cb[2] = [1,1]\n+    #encoded_cb[4] = [0,1]\n+    \n+    tampered_indices, b_errorRate = get_tamperedindices_twobits(decoded_codebook, encoded_cb)\n+    print('tampered indices', tampered_indices)\n+\n+    #verify the accuracy\n+    modified_list = np.packbits(decoded_codebook, axis = 1)\n+    print('modified_list={}'.format(modified_list))\n+    groundtruth_list = np.packbits(encoded_cb, axis = 1)\n+    print('gt_list={}'.format(groundtruth_list))\n+\n+    # added_indices_recovered = findAddedIndices(groundtruth_list, modified_list)\n+    # added_indices_recovered = findModifiedIndices(groundtruth_list, modified_list)\n+    added_indices_recovered = findDeletedIndices(groundtruth_list, modified_list)\n+    \n+    print('recovered indices:{}'.format(added_indices_recovered))\n+    \n+    if(np.all(added_indices_recovered==added_indices)):\n+        print('you got it buddy! finally..phew!!!!)')\n+\n","files":{"\/accuracy_evaluation.py":{"changes":[{"diff":"\n \n \n if __name__ == '__main__':\n-    #test missing\n-    print('******Deleted Elements Test******')\n-    list_orig = [1,2,3,1,2,3,1,2,3,1,2]\n-    list_mod = [1,1,2,1,1,2]\n-    missing_indices = findDeletedIndices(list_orig, list_mod)\n-    print('missing indices = {}'.format(missing_indices))\n-    print('\\n')\n-\n-    #test different\n-    print('*****Modified Elements Test******')\n-    list_orig = [1,2,3,1,2,3,1,2,3,1,2]\n-    list_mod = [1,2,2,1,2,3,1,2,3,1,3]\n-    different_indices = findModifiedIndices(list_orig, list_mod)\n-    print('different indices = {}'.format(different_indices))\n-    print('\\n')\n-\n-    #test addition\n-    print('*****Modified Elements Test******')\n-    list_orig = [1,2,3,1,2,3,1,2,3,1,2]\n-    list_mod = [1,5,2,3,1,2,3,0,1,2,3,1,2]\n-    added_indices = findAddedIndices(list_orig, list_mod)\n-    print('added indices = {}'.format(added_indices))\n-    print('\\n')\n+    # #test missing\n+    # print('******Deleted Elements Test******')\n+    # list_orig = [1,2,3,4,1,2,3,4,1,2,3,4,1,2]\n+    # list_mod = [1,1,2,1,1,2]\n+    # missing_indices = findDeletedIndices(list_orig, list_mod)\n+    # print('missing indices = {}'.format(missing_indices))\n+    # print('\\n')\n+\n+    # #test different\n+    # print('*****Modified Elements Test******')\n+    # list_orig = [1,2,3,1,2,3,1,2,3,1,2]\n+    # list_mod = [1,2,2,1,2,3,1,2,3,1,3]\n+    # different_indices = findModifiedIndices(list_orig, list_mod)\n+    # print('different indices = {}'.format(different_indices))\n+    # print('\\n')\n+\n+    # #test addition\n+    # print('*****Modified Elements Test******')\n+    # list_orig = [1,2,3,1,2,3,1,2,3,1,2]\n+    # list_mod = [1,2,2,3,1,2,3,0,1,2,3,1,2]\n+    # added_indices = findAddedIndices(list_orig, list_mod)\n+    # print('added indices = {}'.format(added_indices))\n+    # print('\\n')\n+\n+\n+    #extract radar data\n+    radar_data_predictions = radarDataExtractor(\"data\/data-2.txt\")\n+    print('original data:', radar_data_predictions)\n+    radar_data_predictions = np.array([radar_data_predictions]).reshape(-1,2)\n+    #encode the data for a given step-size\n+    voxel_halfdelta = qim_quantize_twobits(radar_data_predictions)\n+    voxel_halfdelta_npy = np.array([voxel_halfdelta]).reshape(-1,2)\n+    print('quant encoded shape', voxel_halfdelta_npy.shape, voxel_halfdelta_npy)\n+    \n+    qim_encoded_pointcloud = getPointCloud_from_quantizedValues(  np.copy(voxel_halfdelta_npy))\n+    print('encoded data', qim_encoded_pointcloud)\n+\n+    pred_plusNoise = tamperRadarAddUniformNoise(0.001, qim_encoded_pointcloud)\n+    print('noise added pc={}'.format(pred_plusNoise))\n+    \n+    #open the encoded data and tamper it by adding noise and attack vectors\n+    # added_indices, pred_plus_added = tamperRadarAddTracklets(pred_plusNoise)\n+    added_indices, pred_plus_added = tamperRadarDeleteTracklets(pred_plusNoise)\n+    # added_indices, pred_plus_added = tamperRadarModifyTracklets(pred_plusNoise)\n+\n+    \n+    print('added indices:{}'.format(added_indices))\n+    print('modified data:{}'.format(pred_plus_added))\n+    #decode the tampered point cloud\n+\n+    decoded_CB, decoded_quantized_values = qim_decode(np.copy(pred_plus_added))\n+    print('decoded codebook', decoded_CB)\n+    decoded_codebook = np.array([decoded_CB]).reshape(-1,NUMBITS)\n+\n+    #calculate the BER\n+    encoded_cb = qim_dummy_encoded_pc(len(radar_data_predictions.reshape(-1,NUMBITS)))\n+    print('encoded cb', encoded_cb)\n+    # uncomment below to test if the tamper index finder is working\n+    #encoded_cb[2] = [1,1]\n+    #encoded_cb[4] = [0,1]\n+    \n+    tampered_indices, b_errorRate = get_tamperedindices_twobits(decoded_codebook, encoded_cb)\n+    print('tampered indices', tampered_indices)\n+\n+    #verify the accuracy\n+    modified_list = np.packbits(decoded_codebook, axis = 1)\n+    print('modified_list={}'.format(modified_list))\n+    groundtruth_list = np.packbits(encoded_cb, axis = 1)\n+    print('gt_list={}'.format(groundtruth_list))\n+\n+    # added_indices_recovered = findAddedIndices(groundtruth_list, modified_list)\n+    # added_indices_recovered = findModifiedIndices(groundtruth_list, modified_list)\n+    added_indices_recovered = findDeletedIndices(groundtruth_list, modified_list)\n+    \n+    print('recovered indices:{}'.format(added_indices_recovered))\n+    \n+    if(np.all(added_indices_recovered==added_indices)):\n+        print('you got it buddy! finally..phew!!!!)')\n+\n","add":79,"remove":23,"filename":"\/accuracy_evaluation.py","badparts":["    print('******Deleted Elements Test******')","    list_orig = [1,2,3,1,2,3,1,2,3,1,2]","    list_mod = [1,1,2,1,1,2]","    missing_indices = findDeletedIndices(list_orig, list_mod)","    print('missing indices = {}'.format(missing_indices))","    print('\\n')","    print('*****Modified Elements Test******')","    list_orig = [1,2,3,1,2,3,1,2,3,1,2]","    list_mod = [1,2,2,1,2,3,1,2,3,1,3]","    different_indices = findModifiedIndices(list_orig, list_mod)","    print('different indices = {}'.format(different_indices))","    print('\\n')","    print('*****Modified Elements Test******')","    list_orig = [1,2,3,1,2,3,1,2,3,1,2]","    list_mod = [1,5,2,3,1,2,3,0,1,2,3,1,2]","    added_indices = findAddedIndices(list_orig, list_mod)","    print('added indices = {}'.format(added_indices))","    print('\\n')"],"goodparts":["    radar_data_predictions = radarDataExtractor(\"data\/data-2.txt\")","    print('original data:', radar_data_predictions)","    radar_data_predictions = np.array([radar_data_predictions]).reshape(-1,2)","    voxel_halfdelta = qim_quantize_twobits(radar_data_predictions)","    voxel_halfdelta_npy = np.array([voxel_halfdelta]).reshape(-1,2)","    print('quant encoded shape', voxel_halfdelta_npy.shape, voxel_halfdelta_npy)","    qim_encoded_pointcloud = getPointCloud_from_quantizedValues(  np.copy(voxel_halfdelta_npy))","    print('encoded data', qim_encoded_pointcloud)","    pred_plusNoise = tamperRadarAddUniformNoise(0.001, qim_encoded_pointcloud)","    print('noise added pc={}'.format(pred_plusNoise))","    added_indices, pred_plus_added = tamperRadarDeleteTracklets(pred_plusNoise)","    print('added indices:{}'.format(added_indices))","    print('modified data:{}'.format(pred_plus_added))","    decoded_CB, decoded_quantized_values = qim_decode(np.copy(pred_plus_added))","    print('decoded codebook', decoded_CB)","    decoded_codebook = np.array([decoded_CB]).reshape(-1,NUMBITS)","    encoded_cb = qim_dummy_encoded_pc(len(radar_data_predictions.reshape(-1,NUMBITS)))","    print('encoded cb', encoded_cb)","    tampered_indices, b_errorRate = get_tamperedindices_twobits(decoded_codebook, encoded_cb)","    print('tampered indices', tampered_indices)","    modified_list = np.packbits(decoded_codebook, axis = 1)","    print('modified_list={}'.format(modified_list))","    groundtruth_list = np.packbits(encoded_cb, axis = 1)","    print('gt_list={}'.format(groundtruth_list))","    added_indices_recovered = findDeletedIndices(groundtruth_list, modified_list)","    print('recovered indices:{}'.format(added_indices_recovered))","    if(np.all(added_indices_recovered==added_indices)):","        print('you got it buddy! finally..phew!!!!)')"]}],"source":"\n\nimport numpy as np def findDeletedIndices(gt_list, mod_list): gt_index=0 mod_index=0 missing_indices=[] while(mod_index < len(mod_list) or gt_index < len(gt_list)): if(gt_list[gt_index]== mod_list[mod_index]): mod_index +=1 gt_index +=1 continue print('missing index={}, missing element={}'. format((gt_index), gt_list[gt_index])) missing_indices.append(gt_index) gt_index +=1 return missing_indices def findModifiedIndices(gt_list, mod_list): different_indices=[] index=0 while(index < len(mod_list)): if(gt_list[index]== mod_list[index]): index +=1 continue print('index={}, different element={}'. format((index), mod_list[index])) different_indices.append(index) index +=1 return different_indices def findAddedIndices(gt_list, mod_list): gt_index=0 mod_index=0 added_indices=[] while(mod_index < len(mod_list) or gt_index < len(gt_list)): if(gt_list[gt_index]== mod_list[mod_index]): mod_index +=1 gt_index +=1 continue print('index={}, added element={}'. format((mod_index), mod_list[mod_index])) added_indices.append(mod_index) mod_index +=1 pass return added_indices if __name__=='__main__': print('******Deleted Elements Test******') list_orig=[1,2,3,1,2,3,1,2,3,1,2] list_mod=[1,1,2,1,1,2] missing_indices=findDeletedIndices(list_orig, list_mod) print('missing indices={}'.format(missing_indices)) print('\\n') print('*****Modified Elements Test******') list_orig=[1,2,3,1,2,3,1,2,3,1,2] list_mod=[1,2,2,1,2,3,1,2,3,1,3] different_indices=findModifiedIndices(list_orig, list_mod) print('different indices={}'.format(different_indices)) print('\\n') print('*****Modified Elements Test******') list_orig=[1,2,3,1,2,3,1,2,3,1,2] list_mod=[1,5,2,3,1,2,3,0,1,2,3,1,2] added_indices=findAddedIndices(list_orig, list_mod) print('added indices={}'.format(added_indices)) print('\\n') ","sourceWithComments":"# -*- coding: utf-8 -*-\nimport numpy as np\n\n# #** This piece of code contants methods to evaluate the 2D QIM embedding for the given Udacity data-set. We can further split this file into three different files if needed (most of the previously developed code could be re-used)\n# First - read the data file and generate a separate binary file with just the radar data\n# Add noise to the data if required (uniform or Gaussian with variable delta)\n# simulate the attack vectors, like add, delete or modify the random line items in the above generated file\n# get the detections out of the modified files and report the false alarms, error rate and accuracy (if the algorithm is able to detect the location of the modified data)\n# **#\n\n#these three fucntions check for the midding indices, added indices and modified indices assuming the input is a list of repeating pattens\n\ndef findDeletedIndices(gt_list, mod_list):\n    #here the assumption is that the lenth of the mod_list < gt_list\n    gt_index = 0\n    mod_index = 0\n    missing_indices = []\n    while(mod_index < len(mod_list) or gt_index < len(gt_list)):\n        if(gt_list[gt_index] ==  mod_list[mod_index]):\n            mod_index += 1\n            gt_index += 1\n            continue\n        print('missing index = {}, missing element = {}'. format((gt_index), gt_list[gt_index]))\n        missing_indices.append(gt_index)\n        gt_index += 1\n    return missing_indices\n\ndef findModifiedIndices(gt_list, mod_list):\n    #here both the lengths are same\n    different_indices = []\n    index = 0\n    while(index < len(mod_list)):\n        if(gt_list[index] ==  mod_list[index]):\n            index += 1\n            continue\n        print('index = {}, different element = {}'. format((index), mod_list[index]))\n        different_indices.append(index)\n        index += 1\n\n    return different_indices\n\ndef findAddedIndices(gt_list, mod_list):\n    gt_index = 0\n    mod_index = 0\n    added_indices = []\n    while(mod_index < len(mod_list) or gt_index < len(gt_list)):\n        if(gt_list[gt_index] ==  mod_list[mod_index]):\n            mod_index += 1\n            gt_index += 1\n            continue\n        print('index = {}, added element = {}'. format((mod_index), mod_list[mod_index]))\n        added_indices.append(mod_index)\n        mod_index += 1\n    pass\n    return added_indices\n\n\nif __name__ == '__main__':\n    #test missing\n    print('******Deleted Elements Test******')\n    list_orig = [1,2,3,1,2,3,1,2,3,1,2]\n    list_mod = [1,1,2,1,1,2]\n    missing_indices = findDeletedIndices(list_orig, list_mod)\n    print('missing indices = {}'.format(missing_indices))\n    print('\\n')\n\n    #test different\n    print('*****Modified Elements Test******')\n    list_orig = [1,2,3,1,2,3,1,2,3,1,2]\n    list_mod = [1,2,2,1,2,3,1,2,3,1,3]\n    different_indices = findModifiedIndices(list_orig, list_mod)\n    print('different indices = {}'.format(different_indices))\n    print('\\n')\n\n    #test addition\n    print('*****Modified Elements Test******')\n    list_orig = [1,2,3,1,2,3,1,2,3,1,2]\n    list_mod = [1,5,2,3,1,2,3,0,1,2,3,1,2]\n    added_indices = findAddedIndices(list_orig, list_mod)\n    print('added indices = {}'.format(added_indices))\n    print('\\n')\n"}},"msg":"the end to end pipe-line implemented and tested for add, del and modify functions. Code read from data_x.txt only the RADAR data and encodes it, adds required noise and then tampers it with give attack vector and then decodes it and checks if the attack vectors are localized"}},"https:\/\/github.com\/innovara\/autoreply":{"ff536590437896723329378fecc07c8a1dfd36ae":{"url":"https:\/\/api.github.com\/repos\/innovara\/autoreply\/commits\/ff536590437896723329378fecc07c8a1dfd36ae","html_url":"https:\/\/github.com\/innovara\/autoreply\/commit\/ff536590437896723329378fecc07c8a1dfd36ae","message":"Autoreply check to avoid loops. Binary reinjection\n\nAdded check_autoreply() to detect common autoreply and automated headers, in order to avoid loops and responding to newsletters and mailing lists.\n\nAdded In-Reply-To header to ensure autoreply emails are threaded properly by the recipient\n\nChanged how the email is reinjected to binary to avoid unnecessary tampering and related encoding issues","sha":"ff536590437896723329378fecc07c8a1dfd36ae","keyword":"tampering change","diff":"diff --git a\/autoreply.py b\/autoreply.py\nindex 946f7b7..591ad7a 100755\n--- a\/autoreply.py\n+++ b\/autoreply.py\n@@ -5,7 +5,7 @@\n import json\n import os.path\n from os import chmod\n-from email import message_from_file\n+from email import message_from_bytes\n from email.message import EmailMessage\n from email.utils import make_msgid\n from subprocess import Popen, PIPE\n@@ -15,7 +15,7 @@\n def log(message):\n   '''Logs messages to ~\/autoreply.log.'''\n   if logging == True:\n-    now = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n+    now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n     log_file = os.path.expanduser('~') + '\/autoreply.log'\n     with open(log_file, 'a+', encoding='utf-8') as log:\n       log.write(now + ': ' + message + '\\n')\n@@ -56,11 +56,11 @@ def open_json():\n   # If the file .json doesn't exist, it creates a template\n   except FileNotFoundError:\n     create_json()\n-    sys.exit(\"Couldn't find ~\/autoreply.json. New template created.\")\n+    sys.exit('couldn\\'t find ~\/autoreply.json. New template created.')\n   return data\n \n \n-def generate_email(sender, recipient, replyto, subject, body, attachment_path=None, test=False):\n+def generate_email(sender, recipient, original_id, replyto, subject, body, attachment_path=None, test=False):\n   '''Creates an email message object with an attachement (optional).'''\n   # TODO: HTML body with formatting instead of plain text?\n   message = EmailMessage()\n@@ -71,8 +71,11 @@ def generate_email(sender, recipient, replyto, subject, body, attachment_path=No\n   message['Message-ID'] = make_msgid()\n   message['Reply-to'] = replyto\n   if test == False:\n-    message['X-Autoreply'] = \"yes\"\n-    message['Auto-Submitted'] = \"auto-replied\"\n+    message['In-Reply-To'] = original_id\n+    message['Auto-Submitted'] = 'auto-replied'\n+    message['X-Autoreply'] = 'yes'\n+    message['X-Auto-Response-Suppress'] = 'All'\n+    message['Precedence'] = 'auto_reply'\n   message.set_content(body)\n   # Process the attachment and add it to the email\n   if attachment_path != None:\n@@ -99,18 +102,52 @@ def send_email(message):\n   mail_server.quit()\n \n \n-def reinject_email(message, sender, recipients):\n+def reinject_email(message, sender, recipients, original_id):\n   '''Sends original email back to Postfix for final delivery.'''\n   # NOTE: According to Postfix's FILTER documentation\n   # you must not use -t to re-inject the message\n   separator = ','\n   r_recipients = separator.join(recipients)\n-  log('re-injecting email to: ' + r_recipients)\n-  process = Popen([\"\/usr\/sbin\/sendmail\", \"-f\", sender, \"-G\", \"-oi\", r_recipients], stdin=PIPE)\n-  process.communicate(message.as_bytes())\n+  log('re-injecting ' + str(original_id))\n+  log('recipients: ' + r_recipients)\n+  process = Popen(['\/usr\/sbin\/sendmail', '-f', sender, '-G', '-oi', r_recipients], stdin=PIPE)\n+  process.communicate(message)\n+\n+\n+def check_autoreply(message, original_id):\n+  '''Checks if an incoming email is an autoreply itself to avoid loops'''\n+  '''For more information please see https:\/\/www.arp242.net\/autoreply.html'''\n+  # Defined in RFC 3834. \u2018Official\u2019 standard to indicate a message is an autoreply\n+  log('checking autoreply or automated headers on ' + str(original_id))\n+  if message['Auto-submitted'] != 'no':\n+    log('Auto-submitted present, not sending autoreply')\n+    return True\n+  # Other non RFC-compliant Auto-submitted headers\n+  elif message['X-Autoreply'] != None or message['X-Autorespond'] != None:\n+    log('X-Autoreply or X-Autorespond present, not sending autoreply')\n+    return True\n+  # Defined by Microsoft. Header used by Microsoft Exchange, Outlook, and perhaps others\n+  elif message['X-Auto-Response-Suppress'] in ('DR', 'AutoReply', 'All'):\n+    log('X-Auto-Response-Suppress is DR, AutoReply or All, not sending autoreply')\n+    return True\n+  # Defined in RFC 2919. Most of the time you don\u2019t want to send autoreplies to mailing lists or newsletters\n+  elif message['List-Id'] != None or message['List-Unsubscribe'] != None:\n+    log('List-Id or List-Unsubscribe present, not sending autoreply')\n+    return True\n+  # Defined by Google. Gmail uses this header to identify newsletters and uses it to generate statistics\n+  elif message['Feedback-ID'] != None:\n+    log('Feedback-ID present, not sending autoreply')\n+    return True\n+  # Mentioned in RFC 2076 where its use is discouraged, but this header is commonly encountered\n+  elif str(message['Precedence']).lower() in ('bulk', 'auto_reply', 'list'):\n+    log('Precedence is bulk, auto_reply or list, not sending autoreply')\n+    return True\n+  else:\n+    log('no autoreply or automated headers found on ' + str(original_id))\n+    return False\n \n \n-def autoreply(sender, recipients):\n+def autoreply(sender, recipients, original_id):\n   '''Sends auto-reply email from recipient to sender when the recipient is in ~\/autoreply.json.'''\n   settings = open_json()\n   # Iterates through JSON autoreply objects\n@@ -119,6 +156,7 @@ def autoreply(sender, recipients):\n     if recipient['email'] in recipients:\n       log('autoreply triggered')\n       log('sender is ' + str(sender))\n+      log('Message-Id is ' + str(original_id))\n       log('recipients are ' + str(recipients))\n       log('recipient that triggered the script is ' + str(recipient['email']))\n       # Checks if the auto-reply To and From are different to avoid an infinite loop\n@@ -127,6 +165,7 @@ def autoreply(sender, recipients):\n         message = generate_email(\n           recipient['email'],\n           sender,\n+          original_id,\n           recipient['reply-to'],\n           recipient['subject'],\n           recipient['body']\n@@ -178,19 +217,25 @@ def main():\n     logging = True\n   else:\n     logging = False\n+  log('autoreply.py has been invoked')\n   # Sender and recipients of the source email sent by Postfix as .\/autoreply.py ${sender} ${recipient}\n   # see README.md\n   sender = sys.argv[1]\n   # ${recipient} expands to as many recipients as the message contains\n   recipients = sys.argv[2:]\n   # Original message from STDIN\n-  original_msg = message_from_file(sys.stdin)\n+  binary_msg = sys.stdin.buffer.read()\n+  # Message object\n+  original_msg = message_from_bytes(binary_msg)\n+  original_id = original_msg['Message-ID']\n   # Re-injects original email into Postfix.\n-  # If the purpose of the script was to do something with the original email this should be done later\n-  reinject_email(original_msg, sender, recipients)\n-  # Sends the auto-reply\n-  autoreply(sender, recipients)\n+  # If the purpose of the script was to do something else with the original email, re-injecting should be done later\n+  reinject_email(binary_msg, sender, recipients, original_id)\n+  auto_submitted = check_autoreply(original_msg, original_id)\n+  if auto_submitted == False:\n+    # Sends the auto-reply\n+    autoreply(sender, recipients, original_id)\n \n \n-if __name__ == \"__main__\":\n+if __name__ == '__main__':\n    main()\n","files":{"\/autoreply.py":{"changes":[{"diff":"\n import json\n import os.path\n from os import chmod\n-from email import message_from_file\n+from email import message_from_bytes\n from email.message import EmailMessage\n from email.utils import make_msgid\n from subprocess import Popen, PIPE\n","add":1,"remove":1,"filename":"\/autoreply.py","badparts":["from email import message_from_file"],"goodparts":["from email import message_from_bytes"]},{"diff":"\n   '''Logs messages to ~\/autoreply.log.'''\n   if logging == True:\n-    now = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n+    now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n     log_file = os.path.expanduser('~') + '\/autoreply.log'\n     with open(log_file, 'a+', encoding='utf-8') as log:\n       log.write(now + ': ' + message + '\\n')\n","add":1,"remove":1,"filename":"\/autoreply.py","badparts":["    now = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")"],"goodparts":["    now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')"]},{"diff":"\n   # If the file .json doesn't exist, it creates a template\n   except FileNotFoundError:\n     create_json()\n-    sys.exit(\"Couldn't find ~\/autoreply.json. New template created.\")\n+    sys.exit('couldn\\'t find ~\/autoreply.json. New template created.')\n   return data\n \n \n-def generate_email(sender, recipient, replyto, subject, body, attachment_path=None, test=False):\n+def generate_email(sender, recipient, original_id, replyto, subject, body, attachment_path=None, test=False):\n   '''Creates an email message object with an attachement (optional).'''\n   # TODO: HTML body with formatting instead of plain text?\n   message = EmailMessage()\n","add":2,"remove":2,"filename":"\/autoreply.py","badparts":["    sys.exit(\"Couldn't find ~\/autoreply.json. New template created.\")","def generate_email(sender, recipient, replyto, subject, body, attachment_path=None, test=False):"],"goodparts":["    sys.exit('couldn\\'t find ~\/autoreply.json. New template created.')","def generate_email(sender, recipient, original_id, replyto, subject, body, attachment_path=None, test=False):"]},{"diff":"\n   message['Message-ID'] = make_msgid()\n   message['Reply-to'] = replyto\n   if test == False:\n-    message['X-Autoreply'] = \"yes\"\n-    message['Auto-Submitted'] = \"auto-replied\"\n+    message['In-Reply-To'] = original_id\n+    message['Auto-Submitted'] = 'auto-replied'\n+    message['X-Autoreply'] = 'yes'\n+    message['X-Auto-Response-Suppress'] = 'All'\n+    message['Precedence'] = 'auto_reply'\n   message.set_content(body)\n   # Process the attachment and add it to the email\n   if attachment_path != None:\n","add":5,"remove":2,"filename":"\/autoreply.py","badparts":["    message['X-Autoreply'] = \"yes\"","    message['Auto-Submitted'] = \"auto-replied\""],"goodparts":["    message['In-Reply-To'] = original_id","    message['Auto-Submitted'] = 'auto-replied'","    message['X-Autoreply'] = 'yes'","    message['X-Auto-Response-Suppress'] = 'All'","    message['Precedence'] = 'auto_reply'"]},{"diff":"\n   mail_server.quit()\n \n \n-def reinject_email(message, sender, recipients):\n+def reinject_email(message, sender, recipients, original_id):\n   '''Sends original email back to Postfix for final delivery.'''\n   # NOTE: According to Postfix's FILTER documentation\n   # you must not use -t to re-inject the message\n   separator = ','\n   r_recipients = separator.join(recipients)\n-  log('re-injecting email to: ' + r_recipients)\n-  process = Popen([\"\/usr\/sbin\/sendmail\", \"-f\", sender, \"-G\", \"-oi\", r_recipients], stdin=PIPE)\n-  process.communicate(message.as_bytes())\n+  log('re-injecting ' + str(original_id))\n+  log('recipients: ' + r_recipients)\n+  process = Popen(['\/usr\/sbin\/sendmail', '-f', sender, '-G', '-oi', r_recipients], stdin=PIPE)\n+  process.communicate(message)\n+\n+\n+def check_autoreply(message, original_id):\n+  '''Checks if an incoming email is an autoreply itself to avoid loops'''\n+  '''For more information please see https:\/\/www.arp242.net\/autoreply.html'''\n+  # Defined in RFC 3834. \u2018Official\u2019 standard to indicate a message is an autoreply\n+  log('checking autoreply or automated headers on ' + str(original_id))\n+  if message['Auto-submitted'] != 'no':\n+    log('Auto-submitted present, not sending autoreply')\n+    return True\n+  # Other non RFC-compliant Auto-submitted headers\n+  elif message['X-Autoreply'] != None or message['X-Autorespond'] != None:\n+    log('X-Autoreply or X-Autorespond present, not sending autoreply')\n+    return True\n+  # Defined by Microsoft. Header used by Microsoft Exchange, Outlook, and perhaps others\n+  elif message['X-Auto-Response-Suppress'] in ('DR', 'AutoReply', 'All'):\n+    log('X-Auto-Response-Suppress is DR, AutoReply or All, not sending autoreply')\n+    return True\n+  # Defined in RFC 2919. Most of the time you don\u2019t want to send autoreplies to mailing lists or newsletters\n+  elif message['List-Id'] != None or message['List-Unsubscribe'] != None:\n+    log('List-Id or List-Unsubscribe present, not sending autoreply')\n+    return True\n+  # Defined by Google. Gmail uses this header to identify newsletters and uses it to generate statistics\n+  elif message['Feedback-ID'] != None:\n+    log('Feedback-ID present, not sending autoreply')\n+    return True\n+  # Mentioned in RFC 2076 where its use is discouraged, but this header is commonly encountered\n+  elif str(message['Precedence']).lower() in ('bulk', 'auto_reply', 'list'):\n+    log('Precedence is bulk, auto_reply or list, not sending autoreply')\n+    return True\n+  else:\n+    log('no autoreply or automated headers found on ' + str(original_id))\n+    return False\n \n \n-def autoreply(sender, recipients):\n+def autoreply(sender, recipients, original_id):\n   '''Sends auto-reply email from recipient to sender when the recipient is in ~\/autoreply.json.'''\n   settings = open_json()\n   # Iterates through JSON autoreply objects\n","add":39,"remove":5,"filename":"\/autoreply.py","badparts":["def reinject_email(message, sender, recipients):","  log('re-injecting email to: ' + r_recipients)","  process = Popen([\"\/usr\/sbin\/sendmail\", \"-f\", sender, \"-G\", \"-oi\", r_recipients], stdin=PIPE)","  process.communicate(message.as_bytes())","def autoreply(sender, recipients):"],"goodparts":["def reinject_email(message, sender, recipients, original_id):","  log('re-injecting ' + str(original_id))","  log('recipients: ' + r_recipients)","  process = Popen(['\/usr\/sbin\/sendmail', '-f', sender, '-G', '-oi', r_recipients], stdin=PIPE)","  process.communicate(message)","def check_autoreply(message, original_id):","  '''Checks if an incoming email is an autoreply itself to avoid loops'''","  '''For more information please see https:\/\/www.arp242.net\/autoreply.html'''","  log('checking autoreply or automated headers on ' + str(original_id))","  if message['Auto-submitted'] != 'no':","    log('Auto-submitted present, not sending autoreply')","    return True","  elif message['X-Autoreply'] != None or message['X-Autorespond'] != None:","    log('X-Autoreply or X-Autorespond present, not sending autoreply')","    return True","  elif message['X-Auto-Response-Suppress'] in ('DR', 'AutoReply', 'All'):","    log('X-Auto-Response-Suppress is DR, AutoReply or All, not sending autoreply')","    return True","  elif message['List-Id'] != None or message['List-Unsubscribe'] != None:","    log('List-Id or List-Unsubscribe present, not sending autoreply')","    return True","  elif message['Feedback-ID'] != None:","    log('Feedback-ID present, not sending autoreply')","    return True","  elif str(message['Precedence']).lower() in ('bulk', 'auto_reply', 'list'):","    log('Precedence is bulk, auto_reply or list, not sending autoreply')","    return True","  else:","    log('no autoreply or automated headers found on ' + str(original_id))","    return False","def autoreply(sender, recipients, original_id):"]},{"diff":"\n     logging = True\n   else:\n     logging = False\n+  log('autoreply.py has been invoked')\n   # Sender and recipients of the source email sent by Postfix as .\/autoreply.py ${sender} ${recipient}\n   # see README.md\n   sender = sys.argv[1]\n   # ${recipient} expands to as many recipients as the message contains\n   recipients = sys.argv[2:]\n   # Original message from STDIN\n-  original_msg = message_from_file(sys.stdin)\n+  binary_msg = sys.stdin.buffer.read()\n+  # Message object\n+  original_msg = message_from_bytes(binary_msg)\n+  original_id = original_msg['Message-ID']\n   # Re-injects original email into Postfix.\n-  # If the purpose of the script was to do something with the original email this should be done later\n-  reinject_email(original_msg, sender, recipients)\n-  # Sends the auto-reply\n-  autoreply(sender, recipients)\n+  # If the purpose of the script was to do something else with the original email, re-injecting should be done later\n+  reinject_email(binary_msg, sender, recipients, original_id)\n+  auto_submitted = check_autoreply(original_msg, original_id)\n+  if auto_submitted == False:\n+    # Sends the auto-reply\n+    autoreply(sender, recipients, original_id)\n \n \n-if __name__ == \"__main__\":\n+if __name__ == '__main__':\n    main()\n","add":12,"remove":6,"filename":"\/autoreply.py","badparts":["  original_msg = message_from_file(sys.stdin)","  reinject_email(original_msg, sender, recipients)","  autoreply(sender, recipients)","if __name__ == \"__main__\":"],"goodparts":["  log('autoreply.py has been invoked')","  binary_msg = sys.stdin.buffer.read()","  original_msg = message_from_bytes(binary_msg)","  original_id = original_msg['Message-ID']","  reinject_email(binary_msg, sender, recipients, original_id)","  auto_submitted = check_autoreply(original_msg, original_id)","  if auto_submitted == False:","    autoreply(sender, recipients, original_id)","if __name__ == '__main__':"]}],"source":"\n\nimport mimetypes import smtplib import sys import json import os.path from os import chmod from email import message_from_file from email.message import EmailMessage from email.utils import make_msgid from subprocess import Popen, PIPE from datetime import datetime def log(message): '''Logs messages to ~\/autoreply.log.''' if logging==True: now=datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") log_file=os.path.expanduser('~') +'\/autoreply.log' with open(log_file, 'a+', encoding='utf-8') as log: log.write(now +': ' +message +'\\n') def create_json(): '''Creates ~\/autoreply.json.''' json_file='autoreply.json' json_path=os.path.join(os.path.expanduser('~'), json_file) data={} if os.path.isfile(json_path) is True: os.replace(json_path, os.path.join(os.path.expanduser('~'), 'autoreply.json.bak')) data['logging']=False data['SMTP']='localhost' data['port']=25 data['starttls']=False data['smtpauth']=False data['username']='user' data['password']='pass' data['autoreply']=[] data['autoreply'].append({ 'email': 'foo@bar', 'reply-to': 'foo@bar', 'subject': 'Subject here', 'body': 'Email body here' }) with open(json_path, 'w', encoding='utf-8') as out_file: json.dump(data, out_file, indent=4) chmod(json_path, 0o600) def open_json(): '''Opens ~\/autoreply.json where settings are stored.''' try: with open(os.path.expanduser('~') +'\/autoreply.json', 'r', encoding='utf-8') as json_file: data=json.load(json_file) except FileNotFoundError: create_json() sys.exit(\"Couldn't find ~\/autoreply.json. New template created.\") return data def generate_email(sender, recipient, replyto, subject, body, attachment_path=None, test=False): '''Creates an email message object with an attachement(optional).''' message=EmailMessage() message['From']=sender message['To']=recipient message['Subject']=subject message['Message-ID']=make_msgid() message['Reply-to']=replyto if test==False: message['X-Autoreply']=\"yes\" message['Auto-Submitted']=\"auto-replied\" message.set_content(body) if attachment_path !=None: attachment_filename=os.path.basename(attachment_path) mime_type, _=mimetypes.guess_type(attachment_path) mime_type, mime_subtype=mime_type.split('\/', 1) with open(attachment_path, 'rb') as ap: message.add_attachment(ap.read(), maintype=mime_type, subtype=mime_subtype, filename=attachment_filename) return message def send_email(message): '''Sends an email via SMTP server.''' settings=open_json() mail_server=smtplib.SMTP(settings['SMTP'], settings['port']) if settings['starttls']==True: mail_server.starttls() if settings['smtpauth']==True: mail_server.login(settings['username'], settings['password']) mail_server.send_message(message) mail_server.quit() def reinject_email(message, sender, recipients): '''Sends original email back to Postfix for final delivery.''' separator=',' r_recipients=separator.join(recipients) log('re-injecting email to: ' +r_recipients) process=Popen([\"\/usr\/sbin\/sendmail\", \"-f\", sender, \"-G\", \"-oi\", r_recipients], stdin=PIPE) process.communicate(message.as_bytes()) def autoreply(sender, recipients): '''Sends auto-reply email from recipient to sender when the recipient is in ~\/autoreply.json.''' settings=open_json() for recipient in settings['autoreply']: if recipient['email'] in recipients: log('autoreply triggered') log('sender is ' +str(sender)) log('recipients are ' +str(recipients)) log('recipient that triggered the script is ' +str(recipient['email'])) if recipient['email'] !=sender: message=generate_email( recipient['email'], sender, recipient['reply-to'], recipient['subject'], recipient['body'] ) send_email(message) def main(): '''Sends auto-reply email to sender and re-injects original email into Postfix for final delivery. -sys.argv[1] is the sender, passed by Postfix as ${sender} -sys.argv[2:] are the recipients, passed by Postfix as ${recipient} -original email message is piped by Postfix over STDIN Use '.\/autoreply.py -j' to generate a.json configuration file. Use '.\/autoreply.py -l' to show the content of the.json configuration file. Use '.\/autoreply.py -t' to generate a test email text file. Use '.\/autoreply.py from@bar to@bar < test.txt' to test autoreply.py. Note: edit test.txt first and replace from@bar and to@bar with your own ''' if len(sys.argv) < 2: print(\"Use:\\n\\ '.\/autoreply.py -j' to generate a.json configuration file.\\n\\ '.\/autoreply.py -l' to show the content of the.json configuration file.\\n\\ '.\/autoreply.py -t' to generate a test email text file.\\n\\ '.\/autoreply.py from@bar to@bar < test.txt' to test autoreply.py. Note: edit test.txt first and replace from@bar and to@bar with your own\\n\") exit() if '-j' in sys.argv[1:]: create_json() if '-l' in sys.argv[1:]: print(json.dumps(open_json(), indent=4)) if '-t' in sys.argv[1:]: t_message=generate_email('from@bar', 'to@bar', 'from@foo','This is a test email', 'This is an email to test autoreply.py', test=True) with open(os.path.expanduser('~') +'\/test.txt', 'w', encoding='utf-8') as t_email: t_email.write(str(t_message)) if '-j' in sys.argv[1:] or '-l' in sys.argv[1:] or '-t' in sys.argv[1:]: sys.exit() settings=open_json() global logging if settings['logging']==True: logging=True else: logging=False sender=sys.argv[1] recipients=sys.argv[2:] original_msg=message_from_file(sys.stdin) reinject_email(original_msg, sender, recipients) autoreply(sender, recipients) if __name__==\"__main__\": main() ","sourceWithComments":"#!\/usr\/bin\/env python3\nimport mimetypes\nimport smtplib\nimport sys\nimport json\nimport os.path\nfrom os import chmod\nfrom email import message_from_file\nfrom email.message import EmailMessage\nfrom email.utils import make_msgid\nfrom subprocess import Popen, PIPE\nfrom datetime import datetime\n\n\ndef log(message):\n  '''Logs messages to ~\/autoreply.log.'''\n  if logging == True:\n    now = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    log_file = os.path.expanduser('~') + '\/autoreply.log'\n    with open(log_file, 'a+', encoding='utf-8') as log:\n      log.write(now + ': ' + message + '\\n')\n\n\ndef create_json():\n  '''Creates ~\/autoreply.json.'''\n  json_file = 'autoreply.json'\n  json_path = os.path.join(os.path.expanduser('~'), json_file)\n  data = {}\n  if os.path.isfile(json_path) is True:\n    os.replace(json_path, os.path.join(os.path.expanduser('~'), 'autoreply.json.bak'))\n  data['logging'] = False\n  data['SMTP'] = 'localhost'\n  data['port'] = 25\n  data['starttls'] = False\n  data['smtpauth'] = False\n  data['username'] = 'user'\n  data['password'] = 'pass'\n  data['autoreply'] = []\n  data['autoreply'].append({\n    'email': 'foo@bar',\n    'reply-to': 'foo@bar',\n    'subject': 'Subject here',\n    'body': 'Email body here'\n  })\n  with open(json_path, 'w', encoding='utf-8') as out_file:\n    json.dump(data, out_file, indent=4)\n  chmod(json_path, 0o600)\n\n\ndef open_json():\n  '''Opens ~\/autoreply.json where settings are stored.'''\n  try:\n    # Opens ~\/autoreply.json\n    with open(os.path.expanduser('~') + '\/autoreply.json', 'r', encoding='utf-8') as json_file:\n      data = json.load(json_file)\n  # If the file .json doesn't exist, it creates a template\n  except FileNotFoundError:\n    create_json()\n    sys.exit(\"Couldn't find ~\/autoreply.json. New template created.\")\n  return data\n\n\ndef generate_email(sender, recipient, replyto, subject, body, attachment_path=None, test=False):\n  '''Creates an email message object with an attachement (optional).'''\n  # TODO: HTML body with formatting instead of plain text?\n  message = EmailMessage()\n  # Email headers\n  message['From'] = sender\n  message['To'] = recipient\n  message['Subject'] = subject\n  message['Message-ID'] = make_msgid()\n  message['Reply-to'] = replyto\n  if test == False:\n    message['X-Autoreply'] = \"yes\"\n    message['Auto-Submitted'] = \"auto-replied\"\n  message.set_content(body)\n  # Process the attachment and add it to the email\n  if attachment_path != None:\n    attachment_filename = os.path.basename(attachment_path)\n    mime_type, _ = mimetypes.guess_type(attachment_path)\n    mime_type, mime_subtype = mime_type.split('\/', 1)\n    with open(attachment_path, 'rb') as ap:\n      message.add_attachment(ap.read(),\n                maintype=mime_type,\n                subtype=mime_subtype,\n                filename=attachment_filename)\n  return message\n\n\ndef send_email(message):\n  '''Sends an email via SMTP server.'''\n  settings = open_json()\n  mail_server = smtplib.SMTP(settings['SMTP'], settings['port'])\n  if settings['starttls'] == True:\n    mail_server.starttls()\n  if settings['smtpauth'] == True:\n    mail_server.login(settings['username'], settings['password'])\n  mail_server.send_message(message)\n  mail_server.quit()\n\n\ndef reinject_email(message, sender, recipients):\n  '''Sends original email back to Postfix for final delivery.'''\n  # NOTE: According to Postfix's FILTER documentation\n  # you must not use -t to re-inject the message\n  separator = ','\n  r_recipients = separator.join(recipients)\n  log('re-injecting email to: ' + r_recipients)\n  process = Popen([\"\/usr\/sbin\/sendmail\", \"-f\", sender, \"-G\", \"-oi\", r_recipients], stdin=PIPE)\n  process.communicate(message.as_bytes())\n\n\ndef autoreply(sender, recipients):\n  '''Sends auto-reply email from recipient to sender when the recipient is in ~\/autoreply.json.'''\n  settings = open_json()\n  # Iterates through JSON autoreply objects\n  for recipient in settings['autoreply']:\n    # Checks if an email in ~\/autoreply.json is in the list of recipients of the original email\n    if recipient['email'] in recipients:\n      log('autoreply triggered')\n      log('sender is ' + str(sender))\n      log('recipients are ' + str(recipients))\n      log('recipient that triggered the script is ' + str(recipient['email']))\n      # Checks if the auto-reply To and From are different to avoid an infinite loop\n      if recipient['email'] != sender:\n        # Generates and email message with the settings from ~\/autoreply.json\n        message = generate_email(\n          recipient['email'],\n          sender,\n          recipient['reply-to'],\n          recipient['subject'],\n          recipient['body']\n          )\n        #Sends auto-reply email\n        send_email(message)\n\n\ndef main():\n  '''Sends auto-reply email to sender and re-injects original email into Postfix for final delivery.\n  \n  - sys.argv[1] is the sender, passed by Postfix as ${sender}\n  - sys.argv[2:] are the recipients, passed by Postfix as ${recipient} \n  - original email message is piped by Postfix over STDIN \n  \n  Use '.\/autoreply.py -j' to generate a .json configuration file.\n  Use '.\/autoreply.py -l' to show the content of the .json configuration file.\n  Use '.\/autoreply.py -t' to generate a test email text file.\n  Use '.\/autoreply.py from@bar to@bar < test.txt' to test autoreply.py. Note: edit test.txt first and replace from@bar and to@bar with your own \n  '''\n  # If no parameters are passed, it prints some help\n  if len(sys.argv) < 2:\n    print(\"Use:\\n\\\n    '.\/autoreply.py -j' to generate a .json configuration file.\\n\\\n    '.\/autoreply.py -l' to show the content of the .json configuration file.\\n\\\n    '.\/autoreply.py -t' to generate a test email text file.\\n\\\n    '.\/autoreply.py from@bar to@bar < test.txt' to test autoreply.py. Note: edit test.txt first and replace from@bar and to@bar with your own\\n\")\n    exit()\n  # Creates ~\/autoreply.json if -j is passed\n  if '-j' in sys.argv[1:]:\n    create_json()\n  # Shows the content of ~\/autoreply.json if -l is passed\n  if '-l' in sys.argv[1:]:\n    print(json.dumps(open_json(), indent=4))\n  # Creates ~\/test.txt if -t is passed\n  if '-t' in sys.argv[1:]: \n    t_message = generate_email('from@bar', 'to@bar', 'from@foo','This is a test email', 'This is an email to test autoreply.py', test=True)\n    with open(os.path.expanduser('~') + '\/test.txt', 'w', encoding='utf-8') as t_email:\n      t_email.write(str(t_message))\n  # Exits if -j, -l or -t were passed\n  if '-j' in sys.argv[1:] or '-l' in sys.argv[1:] or '-t' in sys.argv[1:]:\n    sys.exit()\n  # Reads script settings\n  settings = open_json()\n  # Enables logging if 'logging': true\n  # TODO: be able to set up logging on or off using autoreply.py\n  global logging\n  if settings['logging'] == True:\n    logging = True\n  else:\n    logging = False\n  # Sender and recipients of the source email sent by Postfix as .\/autoreply.py ${sender} ${recipient}\n  # see README.md\n  sender = sys.argv[1]\n  # ${recipient} expands to as many recipients as the message contains\n  recipients = sys.argv[2:]\n  # Original message from STDIN\n  original_msg = message_from_file(sys.stdin)\n  # Re-injects original email into Postfix.\n  # If the purpose of the script was to do something with the original email this should be done later\n  reinject_email(original_msg, sender, recipients)\n  # Sends the auto-reply\n  autoreply(sender, recipients)\n\n\nif __name__ == \"__main__\":\n   main()\n"}},"msg":"Autoreply check to avoid loops. Binary reinjection\n\nAdded check_autoreply() to detect common autoreply and automated headers, in order to avoid loops and responding to newsletters and mailing lists.\n\nAdded In-Reply-To header to ensure autoreply emails are threaded properly by the recipient\n\nChanged how the email is reinjected to binary to avoid unnecessary tampering and related encoding issues"}},"https:\/\/github.com\/jakeberggren\/TDDC88-Software-Engineering":{"1b5728589c34b2ac3b5d0275ff1ced1d0f0910b4":{"url":"https:\/\/api.github.com\/repos\/jakeberggren\/TDDC88-Software-Engineering\/commits\/1b5728589c34b2ac3b5d0275ff1ced1d0f0910b4","html_url":"https:\/\/github.com\/jakeberggren\/TDDC88-Software-Engineering\/commit\/1b5728589c34b2ac3b5d0275ff1ced1d0f0910b4","message":"Settings.py was tampered with. Please do not change this file when pushing to dev. Change the debug to true when running locally and then change it back if pushing to dev.","sha":"1b5728589c34b2ac3b5d0275ff1ced1d0f0910b4","keyword":"tampering change","diff":"diff --git a\/Web\/rdxSolutionsBackendProject\/settings.py b\/Web\/rdxSolutionsBackendProject\/settings.py\nindex f66687b..326c89e 100644\n--- a\/Web\/rdxSolutionsBackendProject\/settings.py\n+++ b\/Web\/rdxSolutionsBackendProject\/settings.py\n@@ -9,29 +9,32 @@\n For the full list of settings and their values, see\n https:\/\/docs.djangoproject.com\/en\/4.1\/ref\/settings\/\n \"\"\"\n-\n+import os\n from pathlib import Path\n-# from rdxSolutionsBackendProject import databaseConfig\n+from datetime import timedelta\n+from rest_framework.settings import api_settings\n \n # Build paths inside the project like this: BASE_DIR \/ 'subdir'.\n BASE_DIR = Path(__file__).resolve().parent.parent\n \n \n-# Quick-start development settings - unsuitable for production\n+# Quick-start development  settings - unsuitable for production\n # See https:\/\/docs.djangoproject.com\/en\/4.1\/howto\/deployment\/checklist\/\n \n # SECURITY WARNING: keep the secret key used in production secret!\n SECRET_KEY = 'django-insecure-&kgxky3c#god85vjz8-%ygle2^)iac+unxt_qud^tp4(10hi&_'\n \n # SECURITY WARNING: don't run with debug turned on in production!\n-DEBUG = False\n \n-ALLOWED_HOSTS = ['*']\n+# always set to false when pushing to dev\n+DEBUG = False\n \n+ALLOWED_HOSTS =['*']\n \n # Application definition\n \n INSTALLED_APPS = [\n+    'whitenoise.runserver_nostatic',\n     'django.contrib.admin',\n     'django.contrib.auth',\n     'django.contrib.contenttypes',\n@@ -39,17 +42,36 @@\n     'django.contrib.messages',\n     'django.contrib.staticfiles',\n     'rest_framework',\n+    'rest_framework.authtoken',\n     'backend',\n+    'knox',\n+\n ]\n \n+REST_FRAMEWORK = {\n+    'DEFAULT_AUTHENTICATION_CLASSES': [\n+        'knox.auth.TokenAuthentication',\n+    ]\n+}\n+\n+REST_KNOX = {\n+    'TOKEN_TTL': timedelta(minutes=600),\n+    'USER_SERIALIZER': 'knox.serializers.UserSerializer',\n+    'TOKEN_LIMIT_PER_USER': None,\n+    'AUTO_REFRESH': False,\n+    'EXPIRY_DATETIME_FORMAT': api_settings.DATETIME_FORMAT,\n+}\n+\n MIDDLEWARE = [\n     'django.middleware.security.SecurityMiddleware',\n+    'whitenoise.middleware.WhiteNoiseMiddleware',\n     'django.contrib.sessions.middleware.SessionMiddleware',\n     'django.middleware.common.CommonMiddleware',\n     'django.middleware.csrf.CsrfViewMiddleware',\n     'django.contrib.auth.middleware.AuthenticationMiddleware',\n     'django.contrib.messages.middleware.MessageMiddleware',\n-    'django.middleware.clickjacking.XFrameOptionsMiddleware',\n+    'django.middleware.clickjacking.XFrameOptionsMiddleware',    \n+\n ]\n \n ROOT_URLCONF = 'rdxSolutionsBackendProject.urls'\n@@ -57,7 +79,7 @@\n TEMPLATES = [\n     {\n         'BACKEND': 'django.template.backends.django.DjangoTemplates',\n-        'DIRS': [],\n+        'DIRS': [BASE_DIR \/ \"rdxSolutionsBackendProject\/templates\"],\n         'APP_DIRS': True,\n         'OPTIONS': {\n             'context_processors': [\n@@ -99,6 +121,7 @@\n ]\n \n \n+\n # Internationalization\n # https:\/\/docs.djangoproject.com\/en\/4.1\/topics\/i18n\/\n \n@@ -115,10 +138,25 @@\n # https:\/\/docs.djangoproject.com\/en\/4.1\/howto\/static-files\/\n \n \n-STATIC_ROOT = '\/static\/'\n-STATIC_URL = '\/static\/'\n \n-# Default primary key field type\n-# https:\/\/docs.djangoproject.com\/en\/4.1\/ref\/settings\/#default-auto-field\n+\n+BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n+# Static files (CSS, JavaScript, Images)\n+# https:\/\/docs.djangoproject.com\/en\/1.9\/howto\/static-files\/\n+STATICFILES_STORAGE = \"whitenoise.storage.CompressedManifestStaticFilesStorage\"\n+\n+STATIC_URL = '\/admin\/static\/'\n+STATIC_ROOT = os.path.join(BASE_DIR, 'staticfiles')\n+\n+\n+# Extra places for collectstatic to find static files.\n+STATICFILES_DIRS = (\n+    os.path.join(BASE_DIR, 'static'),\n+)\n+\n+\n+\n \n DEFAULT_AUTO_FIELD = 'django.db.models.BigAutoField'\n+\n+LOGIN_REDIRECT_URL = \"\/success\"\ndiff --git a\/Web\/requirements.txt b\/Web\/requirements.txt\nindex 2a06ff1..972e218 100644\n--- a\/Web\/requirements.txt\n+++ b\/Web\/requirements.txt\n@@ -36,4 +36,4 @@ Pillow==9.2.0\n django-rest-knox\n gunicorn\n whitenoise\n-django-dotenv\n\\ No newline at end of file\n+django-dotenv\n","files":{"\/Web\/rdxSolutionsBackendProject\/settings.py":{"changes":[{"diff":"\n For the full list of settings and their values, see\n https:\/\/docs.djangoproject.com\/en\/4.1\/ref\/settings\/\n \"\"\"\n-\n+import os\n from pathlib import Path\n-# from rdxSolutionsBackendProject import databaseConfig\n+from datetime import timedelta\n+from rest_framework.settings import api_settings\n \n # Build paths inside the project like this: BASE_DIR \/ 'subdir'.\n BASE_DIR = Path(__file__).resolve().parent.parent\n \n \n-# Quick-start development settings - unsuitable for production\n+# Quick-start development  settings - unsuitable for production\n # See https:\/\/docs.djangoproject.com\/en\/4.1\/howto\/deployment\/checklist\/\n \n # SECURITY WARNING: keep the secret key used in production secret!\n SECRET_KEY = 'django-insecure-&kgxky3c#god85vjz8-%ygle2^)iac+unxt_qud^tp4(10hi&_'\n \n # SECURITY WARNING: don't run with debug turned on in production!\n-DEBUG = False\n \n-ALLOWED_HOSTS = ['*']\n+# always set to false when pushing to dev\n+DEBUG = False\n \n+ALLOWED_HOSTS =['*']\n \n # Application definition\n \n INSTALLED_APPS = [\n+    'whitenoise.runserver_nostatic',\n     'django.contrib.admin',\n     'django.contrib.auth',\n     'django.contrib.contenttypes',\n","add":8,"remove":5,"filename":"\/Web\/rdxSolutionsBackendProject\/settings.py","badparts":["DEBUG = False","ALLOWED_HOSTS = ['*']"],"goodparts":["from datetime import timedelta","from rest_framework.settings import api_settings","DEBUG = False","ALLOWED_HOSTS =['*']","    'whitenoise.runserver_nostatic',"]},{"diff":"\n     'django.contrib.staticfiles',\n     'rest_framework',\n+    'rest_framework.authtoken',\n     'backend',\n+    'knox',\n+\n ]\n \n+REST_FRAMEWORK = {\n+    'DEFAULT_AUTHENTICATION_CLASSES': [\n+        'knox.auth.TokenAuthentication',\n+    ]\n+}\n+\n+REST_KNOX = {\n+    'TOKEN_TTL': timedelta(minutes=600),\n+    'USER_SERIALIZER': 'knox.serializers.UserSerializer',\n+    'TOKEN_LIMIT_PER_USER': None,\n+    'AUTO_REFRESH': False,\n+    'EXPIRY_DATETIME_FORMAT': api_settings.DATETIME_FORMAT,\n+}\n+\n MIDDLEWARE = [\n     'django.middleware.security.SecurityMiddleware',\n+    'whitenoise.middleware.WhiteNoiseMiddleware',\n     'django.contrib.sessions.middleware.SessionMiddleware',\n     'django.middleware.common.CommonMiddleware',\n     'django.middleware.csrf.CsrfViewMiddleware',\n     'django.contrib.auth.middleware.AuthenticationMiddleware',\n     'django.contrib.messages.middleware.MessageMiddleware',\n-    'django.middleware.clickjacking.XFrameOptionsMiddleware',\n+    'django.middleware.clickjacking.XFrameOptionsMiddleware',    \n+\n ]\n \n ROOT_URLCONF = 'rdxSolutionsBackendProject.urls'\n","add":20,"remove":1,"filename":"\/Web\/rdxSolutionsBackendProject\/settings.py","badparts":["    'django.middleware.clickjacking.XFrameOptionsMiddleware',"],"goodparts":["    'rest_framework.authtoken',","    'knox',","REST_FRAMEWORK = {","    'DEFAULT_AUTHENTICATION_CLASSES': [","        'knox.auth.TokenAuthentication',","    ]","}","REST_KNOX = {","    'TOKEN_TTL': timedelta(minutes=600),","    'USER_SERIALIZER': 'knox.serializers.UserSerializer',","    'TOKEN_LIMIT_PER_USER': None,","    'AUTO_REFRESH': False,","    'EXPIRY_DATETIME_FORMAT': api_settings.DATETIME_FORMAT,","}","    'whitenoise.middleware.WhiteNoiseMiddleware',","    'django.middleware.clickjacking.XFrameOptionsMiddleware',    "]},{"diff":"\n     {\n         'BACKEND': 'django.template.backends.django.DjangoTemplates',\n-        'DIRS': [],\n+        'DIRS': [BASE_DIR \/ \"rdxSolutionsBackendProject\/templates\"],\n         'APP_DIRS': True,\n         'OPTIONS': {\n             'context_processors': [\n","add":1,"remove":1,"filename":"\/Web\/rdxSolutionsBackendProject\/settings.py","badparts":["        'DIRS': [],"],"goodparts":["        'DIRS': [BASE_DIR \/ \"rdxSolutionsBackendProject\/templates\"],"]},{"diff":"\n \n \n-STATIC_ROOT = '\/static\/'\n-STATIC_URL = '\/static\/'\n \n-# Default primary key field type\n-# https:\/\/docs.djangoproject.com\/en\/4.1\/ref\/settings\/#default-auto-field\n+\n+BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n+# Static files (CSS, JavaScript, Images)\n+# https:\/\/docs.djangoproject.com\/en\/1.9\/howto\/static-files\/\n+STATICFILES_STORAGE = \"whitenoise.storage.CompressedManifestStaticFilesStorage\"\n+\n+STATIC_URL = '\/admin\/static\/'\n+STATIC_ROOT = os.path.join(BASE_DIR, 'staticfiles')\n+\n+\n+# Extra places for collectstatic to find static files.\n+STATICFILES_DIRS = (\n+    os.path.join(BASE_DIR, 'static'),\n+)\n+\n+\n+\n \n DEFAULT_AUTO_FIELD = 'django.db.models.BigAutoField'\n+\n+LOGIN_REDIRECT_URL = \"\/success\"","add":19,"remove":4,"filename":"\/Web\/rdxSolutionsBackendProject\/settings.py","badparts":["STATIC_ROOT = '\/static\/'","STATIC_URL = '\/static\/'"],"goodparts":["BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))","STATICFILES_STORAGE = \"whitenoise.storage.CompressedManifestStaticFilesStorage\"","STATIC_URL = '\/admin\/static\/'","STATIC_ROOT = os.path.join(BASE_DIR, 'staticfiles')","STATICFILES_DIRS = (","    os.path.join(BASE_DIR, 'static'),",")","LOGIN_REDIRECT_URL = \"\/success\""]}],"source":"\n\"\"\" Django settings for rdxSolutionsBackendProject project. Generated by 'django-admin startproject' using Django 4.1.1. For more information on this file, see https:\/\/docs.djangoproject.com\/en\/4.1\/topics\/settings\/ For the full list of settings and their values, see https:\/\/docs.djangoproject.com\/en\/4.1\/ref\/settings\/ \"\"\" from pathlib import Path BASE_DIR=Path(__file__).resolve().parent.parent SECRET_KEY='django-insecure-&kgxky3c DEBUG=False ALLOWED_HOSTS=['*'] INSTALLED_APPS=[ 'django.contrib.admin', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles', 'rest_framework', 'backend', ] MIDDLEWARE=[ 'django.middleware.security.SecurityMiddleware', 'django.contrib.sessions.middleware.SessionMiddleware', 'django.middleware.common.CommonMiddleware', 'django.middleware.csrf.CsrfViewMiddleware', 'django.contrib.auth.middleware.AuthenticationMiddleware', 'django.contrib.messages.middleware.MessageMiddleware', 'django.middleware.clickjacking.XFrameOptionsMiddleware', ] ROOT_URLCONF='rdxSolutionsBackendProject.urls' TEMPLATES=[ { 'BACKEND': 'django.template.backends.django.DjangoTemplates', 'DIRS':[], 'APP_DIRS': True, 'OPTIONS':{ 'context_processors':[ 'django.template.context_processors.debug', 'django.template.context_processors.request', 'django.contrib.auth.context_processors.auth', 'django.contrib.messages.context_processors.messages', ], }, }, ] WSGI_APPLICATION='rdxSolutionsBackendProject.wsgi.application' DATABASES={ 'default':{ 'ENGINE': 'django.db.backends.sqlite3', 'NAME': BASE_DIR \/ 'db.sqlite3', } } AUTH_PASSWORD_VALIDATORS=[ { 'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator', }, { 'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator', }, { 'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator', }, { 'NAME': 'django.contrib.auth.password_validation.NumericPasswordValidator', }, ] LANGUAGE_CODE='en-us' TIME_ZONE='UTC' USE_I18N=True USE_TZ=True STATIC_ROOT='\/static\/' STATIC_URL='\/static\/' DEFAULT_AUTO_FIELD='django.db.models.BigAutoField' ","sourceWithComments":"\"\"\"\nDjango settings for rdxSolutionsBackendProject project.\n\nGenerated by 'django-admin startproject' using Django 4.1.1.\n\nFor more information on this file, see\nhttps:\/\/docs.djangoproject.com\/en\/4.1\/topics\/settings\/\n\nFor the full list of settings and their values, see\nhttps:\/\/docs.djangoproject.com\/en\/4.1\/ref\/settings\/\n\"\"\"\n\nfrom pathlib import Path\n# from rdxSolutionsBackendProject import databaseConfig\n\n# Build paths inside the project like this: BASE_DIR \/ 'subdir'.\nBASE_DIR = Path(__file__).resolve().parent.parent\n\n\n# Quick-start development settings - unsuitable for production\n# See https:\/\/docs.djangoproject.com\/en\/4.1\/howto\/deployment\/checklist\/\n\n# SECURITY WARNING: keep the secret key used in production secret!\nSECRET_KEY = 'django-insecure-&kgxky3c#god85vjz8-%ygle2^)iac+unxt_qud^tp4(10hi&_'\n\n# SECURITY WARNING: don't run with debug turned on in production!\nDEBUG = False\n\nALLOWED_HOSTS = ['*']\n\n\n# Application definition\n\nINSTALLED_APPS = [\n    'django.contrib.admin',\n    'django.contrib.auth',\n    'django.contrib.contenttypes',\n    'django.contrib.sessions',\n    'django.contrib.messages',\n    'django.contrib.staticfiles',\n    'rest_framework',\n    'backend',\n]\n\nMIDDLEWARE = [\n    'django.middleware.security.SecurityMiddleware',\n    'django.contrib.sessions.middleware.SessionMiddleware',\n    'django.middleware.common.CommonMiddleware',\n    'django.middleware.csrf.CsrfViewMiddleware',\n    'django.contrib.auth.middleware.AuthenticationMiddleware',\n    'django.contrib.messages.middleware.MessageMiddleware',\n    'django.middleware.clickjacking.XFrameOptionsMiddleware',\n]\n\nROOT_URLCONF = 'rdxSolutionsBackendProject.urls'\n\nTEMPLATES = [\n    {\n        'BACKEND': 'django.template.backends.django.DjangoTemplates',\n        'DIRS': [],\n        'APP_DIRS': True,\n        'OPTIONS': {\n            'context_processors': [\n                'django.template.context_processors.debug',\n                'django.template.context_processors.request',\n                'django.contrib.auth.context_processors.auth',\n                'django.contrib.messages.context_processors.messages',\n            ],\n        },\n    },\n]\n\nWSGI_APPLICATION = 'rdxSolutionsBackendProject.wsgi.application'\n\n# Database\n# https:\/\/docs.djangoproject.com\/en\/4.1\/ref\/settings\/#databases\n\nDATABASES = {\n    'default': {\n        'ENGINE': 'django.db.backends.sqlite3',\n        'NAME': BASE_DIR \/ 'db.sqlite3',\n    }\n}\n\n\nAUTH_PASSWORD_VALIDATORS = [\n    {\n        'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.NumericPasswordValidator',\n    },\n]\n\n\n# Internationalization\n# https:\/\/docs.djangoproject.com\/en\/4.1\/topics\/i18n\/\n\nLANGUAGE_CODE = 'en-us'\n\nTIME_ZONE = 'UTC'\n\nUSE_I18N = True\n\nUSE_TZ = True\n\n\n# Static files (CSS, JavaScript, Images)\n# https:\/\/docs.djangoproject.com\/en\/4.1\/howto\/static-files\/\n\n\nSTATIC_ROOT = '\/static\/'\nSTATIC_URL = '\/static\/'\n\n# Default primary key field type\n# https:\/\/docs.djangoproject.com\/en\/4.1\/ref\/settings\/#default-auto-field\n\nDEFAULT_AUTO_FIELD = 'django.db.models.BigAutoField'\n"}},"msg":"Settings.py was tampered with. Please do not change this file when pushing to dev. Change the debug to true when running locally and then change it back if pushing to dev."}},"https:\/\/github.com\/hugomartinbjork\/TDDC88-Software-Engineering":{"1b5728589c34b2ac3b5d0275ff1ced1d0f0910b4":{"url":"https:\/\/api.github.com\/repos\/hugomartinbjork\/TDDC88-Software-Engineering\/commits\/1b5728589c34b2ac3b5d0275ff1ced1d0f0910b4","html_url":"https:\/\/github.com\/hugomartinbjork\/TDDC88-Software-Engineering\/commit\/1b5728589c34b2ac3b5d0275ff1ced1d0f0910b4","message":"Settings.py was tampered with. Please do not change this file when pushing to dev. Change the debug to true when running locally and then change it back if pushing to dev.","sha":"1b5728589c34b2ac3b5d0275ff1ced1d0f0910b4","keyword":"tampering change","diff":"diff --git a\/Web\/rdxSolutionsBackendProject\/settings.py b\/Web\/rdxSolutionsBackendProject\/settings.py\nindex f66687b..326c89e 100644\n--- a\/Web\/rdxSolutionsBackendProject\/settings.py\n+++ b\/Web\/rdxSolutionsBackendProject\/settings.py\n@@ -9,29 +9,32 @@\n For the full list of settings and their values, see\n https:\/\/docs.djangoproject.com\/en\/4.1\/ref\/settings\/\n \"\"\"\n-\n+import os\n from pathlib import Path\n-# from rdxSolutionsBackendProject import databaseConfig\n+from datetime import timedelta\n+from rest_framework.settings import api_settings\n \n # Build paths inside the project like this: BASE_DIR \/ 'subdir'.\n BASE_DIR = Path(__file__).resolve().parent.parent\n \n \n-# Quick-start development settings - unsuitable for production\n+# Quick-start development  settings - unsuitable for production\n # See https:\/\/docs.djangoproject.com\/en\/4.1\/howto\/deployment\/checklist\/\n \n # SECURITY WARNING: keep the secret key used in production secret!\n SECRET_KEY = 'django-insecure-&kgxky3c#god85vjz8-%ygle2^)iac+unxt_qud^tp4(10hi&_'\n \n # SECURITY WARNING: don't run with debug turned on in production!\n-DEBUG = False\n \n-ALLOWED_HOSTS = ['*']\n+# always set to false when pushing to dev\n+DEBUG = False\n \n+ALLOWED_HOSTS =['*']\n \n # Application definition\n \n INSTALLED_APPS = [\n+    'whitenoise.runserver_nostatic',\n     'django.contrib.admin',\n     'django.contrib.auth',\n     'django.contrib.contenttypes',\n@@ -39,17 +42,36 @@\n     'django.contrib.messages',\n     'django.contrib.staticfiles',\n     'rest_framework',\n+    'rest_framework.authtoken',\n     'backend',\n+    'knox',\n+\n ]\n \n+REST_FRAMEWORK = {\n+    'DEFAULT_AUTHENTICATION_CLASSES': [\n+        'knox.auth.TokenAuthentication',\n+    ]\n+}\n+\n+REST_KNOX = {\n+    'TOKEN_TTL': timedelta(minutes=600),\n+    'USER_SERIALIZER': 'knox.serializers.UserSerializer',\n+    'TOKEN_LIMIT_PER_USER': None,\n+    'AUTO_REFRESH': False,\n+    'EXPIRY_DATETIME_FORMAT': api_settings.DATETIME_FORMAT,\n+}\n+\n MIDDLEWARE = [\n     'django.middleware.security.SecurityMiddleware',\n+    'whitenoise.middleware.WhiteNoiseMiddleware',\n     'django.contrib.sessions.middleware.SessionMiddleware',\n     'django.middleware.common.CommonMiddleware',\n     'django.middleware.csrf.CsrfViewMiddleware',\n     'django.contrib.auth.middleware.AuthenticationMiddleware',\n     'django.contrib.messages.middleware.MessageMiddleware',\n-    'django.middleware.clickjacking.XFrameOptionsMiddleware',\n+    'django.middleware.clickjacking.XFrameOptionsMiddleware',    \n+\n ]\n \n ROOT_URLCONF = 'rdxSolutionsBackendProject.urls'\n@@ -57,7 +79,7 @@\n TEMPLATES = [\n     {\n         'BACKEND': 'django.template.backends.django.DjangoTemplates',\n-        'DIRS': [],\n+        'DIRS': [BASE_DIR \/ \"rdxSolutionsBackendProject\/templates\"],\n         'APP_DIRS': True,\n         'OPTIONS': {\n             'context_processors': [\n@@ -99,6 +121,7 @@\n ]\n \n \n+\n # Internationalization\n # https:\/\/docs.djangoproject.com\/en\/4.1\/topics\/i18n\/\n \n@@ -115,10 +138,25 @@\n # https:\/\/docs.djangoproject.com\/en\/4.1\/howto\/static-files\/\n \n \n-STATIC_ROOT = '\/static\/'\n-STATIC_URL = '\/static\/'\n \n-# Default primary key field type\n-# https:\/\/docs.djangoproject.com\/en\/4.1\/ref\/settings\/#default-auto-field\n+\n+BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n+# Static files (CSS, JavaScript, Images)\n+# https:\/\/docs.djangoproject.com\/en\/1.9\/howto\/static-files\/\n+STATICFILES_STORAGE = \"whitenoise.storage.CompressedManifestStaticFilesStorage\"\n+\n+STATIC_URL = '\/admin\/static\/'\n+STATIC_ROOT = os.path.join(BASE_DIR, 'staticfiles')\n+\n+\n+# Extra places for collectstatic to find static files.\n+STATICFILES_DIRS = (\n+    os.path.join(BASE_DIR, 'static'),\n+)\n+\n+\n+\n \n DEFAULT_AUTO_FIELD = 'django.db.models.BigAutoField'\n+\n+LOGIN_REDIRECT_URL = \"\/success\"\ndiff --git a\/Web\/requirements.txt b\/Web\/requirements.txt\nindex 2a06ff1..972e218 100644\n--- a\/Web\/requirements.txt\n+++ b\/Web\/requirements.txt\n@@ -36,4 +36,4 @@ Pillow==9.2.0\n django-rest-knox\n gunicorn\n whitenoise\n-django-dotenv\n\\ No newline at end of file\n+django-dotenv\n","files":{"\/Web\/rdxSolutionsBackendProject\/settings.py":{"changes":[{"diff":"\n For the full list of settings and their values, see\n https:\/\/docs.djangoproject.com\/en\/4.1\/ref\/settings\/\n \"\"\"\n-\n+import os\n from pathlib import Path\n-# from rdxSolutionsBackendProject import databaseConfig\n+from datetime import timedelta\n+from rest_framework.settings import api_settings\n \n # Build paths inside the project like this: BASE_DIR \/ 'subdir'.\n BASE_DIR = Path(__file__).resolve().parent.parent\n \n \n-# Quick-start development settings - unsuitable for production\n+# Quick-start development  settings - unsuitable for production\n # See https:\/\/docs.djangoproject.com\/en\/4.1\/howto\/deployment\/checklist\/\n \n # SECURITY WARNING: keep the secret key used in production secret!\n SECRET_KEY = 'django-insecure-&kgxky3c#god85vjz8-%ygle2^)iac+unxt_qud^tp4(10hi&_'\n \n # SECURITY WARNING: don't run with debug turned on in production!\n-DEBUG = False\n \n-ALLOWED_HOSTS = ['*']\n+# always set to false when pushing to dev\n+DEBUG = False\n \n+ALLOWED_HOSTS =['*']\n \n # Application definition\n \n INSTALLED_APPS = [\n+    'whitenoise.runserver_nostatic',\n     'django.contrib.admin',\n     'django.contrib.auth',\n     'django.contrib.contenttypes',\n","add":8,"remove":5,"filename":"\/Web\/rdxSolutionsBackendProject\/settings.py","badparts":["DEBUG = False","ALLOWED_HOSTS = ['*']"],"goodparts":["from datetime import timedelta","from rest_framework.settings import api_settings","DEBUG = False","ALLOWED_HOSTS =['*']","    'whitenoise.runserver_nostatic',"]},{"diff":"\n     'django.contrib.staticfiles',\n     'rest_framework',\n+    'rest_framework.authtoken',\n     'backend',\n+    'knox',\n+\n ]\n \n+REST_FRAMEWORK = {\n+    'DEFAULT_AUTHENTICATION_CLASSES': [\n+        'knox.auth.TokenAuthentication',\n+    ]\n+}\n+\n+REST_KNOX = {\n+    'TOKEN_TTL': timedelta(minutes=600),\n+    'USER_SERIALIZER': 'knox.serializers.UserSerializer',\n+    'TOKEN_LIMIT_PER_USER': None,\n+    'AUTO_REFRESH': False,\n+    'EXPIRY_DATETIME_FORMAT': api_settings.DATETIME_FORMAT,\n+}\n+\n MIDDLEWARE = [\n     'django.middleware.security.SecurityMiddleware',\n+    'whitenoise.middleware.WhiteNoiseMiddleware',\n     'django.contrib.sessions.middleware.SessionMiddleware',\n     'django.middleware.common.CommonMiddleware',\n     'django.middleware.csrf.CsrfViewMiddleware',\n     'django.contrib.auth.middleware.AuthenticationMiddleware',\n     'django.contrib.messages.middleware.MessageMiddleware',\n-    'django.middleware.clickjacking.XFrameOptionsMiddleware',\n+    'django.middleware.clickjacking.XFrameOptionsMiddleware',    \n+\n ]\n \n ROOT_URLCONF = 'rdxSolutionsBackendProject.urls'\n","add":20,"remove":1,"filename":"\/Web\/rdxSolutionsBackendProject\/settings.py","badparts":["    'django.middleware.clickjacking.XFrameOptionsMiddleware',"],"goodparts":["    'rest_framework.authtoken',","    'knox',","REST_FRAMEWORK = {","    'DEFAULT_AUTHENTICATION_CLASSES': [","        'knox.auth.TokenAuthentication',","    ]","}","REST_KNOX = {","    'TOKEN_TTL': timedelta(minutes=600),","    'USER_SERIALIZER': 'knox.serializers.UserSerializer',","    'TOKEN_LIMIT_PER_USER': None,","    'AUTO_REFRESH': False,","    'EXPIRY_DATETIME_FORMAT': api_settings.DATETIME_FORMAT,","}","    'whitenoise.middleware.WhiteNoiseMiddleware',","    'django.middleware.clickjacking.XFrameOptionsMiddleware',    "]},{"diff":"\n     {\n         'BACKEND': 'django.template.backends.django.DjangoTemplates',\n-        'DIRS': [],\n+        'DIRS': [BASE_DIR \/ \"rdxSolutionsBackendProject\/templates\"],\n         'APP_DIRS': True,\n         'OPTIONS': {\n             'context_processors': [\n","add":1,"remove":1,"filename":"\/Web\/rdxSolutionsBackendProject\/settings.py","badparts":["        'DIRS': [],"],"goodparts":["        'DIRS': [BASE_DIR \/ \"rdxSolutionsBackendProject\/templates\"],"]},{"diff":"\n \n \n-STATIC_ROOT = '\/static\/'\n-STATIC_URL = '\/static\/'\n \n-# Default primary key field type\n-# https:\/\/docs.djangoproject.com\/en\/4.1\/ref\/settings\/#default-auto-field\n+\n+BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n+# Static files (CSS, JavaScript, Images)\n+# https:\/\/docs.djangoproject.com\/en\/1.9\/howto\/static-files\/\n+STATICFILES_STORAGE = \"whitenoise.storage.CompressedManifestStaticFilesStorage\"\n+\n+STATIC_URL = '\/admin\/static\/'\n+STATIC_ROOT = os.path.join(BASE_DIR, 'staticfiles')\n+\n+\n+# Extra places for collectstatic to find static files.\n+STATICFILES_DIRS = (\n+    os.path.join(BASE_DIR, 'static'),\n+)\n+\n+\n+\n \n DEFAULT_AUTO_FIELD = 'django.db.models.BigAutoField'\n+\n+LOGIN_REDIRECT_URL = \"\/success\"","add":19,"remove":4,"filename":"\/Web\/rdxSolutionsBackendProject\/settings.py","badparts":["STATIC_ROOT = '\/static\/'","STATIC_URL = '\/static\/'"],"goodparts":["BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))","STATICFILES_STORAGE = \"whitenoise.storage.CompressedManifestStaticFilesStorage\"","STATIC_URL = '\/admin\/static\/'","STATIC_ROOT = os.path.join(BASE_DIR, 'staticfiles')","STATICFILES_DIRS = (","    os.path.join(BASE_DIR, 'static'),",")","LOGIN_REDIRECT_URL = \"\/success\""]}],"source":"\n\"\"\" Django settings for rdxSolutionsBackendProject project. Generated by 'django-admin startproject' using Django 4.1.1. For more information on this file, see https:\/\/docs.djangoproject.com\/en\/4.1\/topics\/settings\/ For the full list of settings and their values, see https:\/\/docs.djangoproject.com\/en\/4.1\/ref\/settings\/ \"\"\" from pathlib import Path BASE_DIR=Path(__file__).resolve().parent.parent SECRET_KEY='django-insecure-&kgxky3c DEBUG=False ALLOWED_HOSTS=['*'] INSTALLED_APPS=[ 'django.contrib.admin', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles', 'rest_framework', 'backend', ] MIDDLEWARE=[ 'django.middleware.security.SecurityMiddleware', 'django.contrib.sessions.middleware.SessionMiddleware', 'django.middleware.common.CommonMiddleware', 'django.middleware.csrf.CsrfViewMiddleware', 'django.contrib.auth.middleware.AuthenticationMiddleware', 'django.contrib.messages.middleware.MessageMiddleware', 'django.middleware.clickjacking.XFrameOptionsMiddleware', ] ROOT_URLCONF='rdxSolutionsBackendProject.urls' TEMPLATES=[ { 'BACKEND': 'django.template.backends.django.DjangoTemplates', 'DIRS':[], 'APP_DIRS': True, 'OPTIONS':{ 'context_processors':[ 'django.template.context_processors.debug', 'django.template.context_processors.request', 'django.contrib.auth.context_processors.auth', 'django.contrib.messages.context_processors.messages', ], }, }, ] WSGI_APPLICATION='rdxSolutionsBackendProject.wsgi.application' DATABASES={ 'default':{ 'ENGINE': 'django.db.backends.sqlite3', 'NAME': BASE_DIR \/ 'db.sqlite3', } } AUTH_PASSWORD_VALIDATORS=[ { 'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator', }, { 'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator', }, { 'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator', }, { 'NAME': 'django.contrib.auth.password_validation.NumericPasswordValidator', }, ] LANGUAGE_CODE='en-us' TIME_ZONE='UTC' USE_I18N=True USE_TZ=True STATIC_ROOT='\/static\/' STATIC_URL='\/static\/' DEFAULT_AUTO_FIELD='django.db.models.BigAutoField' ","sourceWithComments":"\"\"\"\nDjango settings for rdxSolutionsBackendProject project.\n\nGenerated by 'django-admin startproject' using Django 4.1.1.\n\nFor more information on this file, see\nhttps:\/\/docs.djangoproject.com\/en\/4.1\/topics\/settings\/\n\nFor the full list of settings and their values, see\nhttps:\/\/docs.djangoproject.com\/en\/4.1\/ref\/settings\/\n\"\"\"\n\nfrom pathlib import Path\n# from rdxSolutionsBackendProject import databaseConfig\n\n# Build paths inside the project like this: BASE_DIR \/ 'subdir'.\nBASE_DIR = Path(__file__).resolve().parent.parent\n\n\n# Quick-start development settings - unsuitable for production\n# See https:\/\/docs.djangoproject.com\/en\/4.1\/howto\/deployment\/checklist\/\n\n# SECURITY WARNING: keep the secret key used in production secret!\nSECRET_KEY = 'django-insecure-&kgxky3c#god85vjz8-%ygle2^)iac+unxt_qud^tp4(10hi&_'\n\n# SECURITY WARNING: don't run with debug turned on in production!\nDEBUG = False\n\nALLOWED_HOSTS = ['*']\n\n\n# Application definition\n\nINSTALLED_APPS = [\n    'django.contrib.admin',\n    'django.contrib.auth',\n    'django.contrib.contenttypes',\n    'django.contrib.sessions',\n    'django.contrib.messages',\n    'django.contrib.staticfiles',\n    'rest_framework',\n    'backend',\n]\n\nMIDDLEWARE = [\n    'django.middleware.security.SecurityMiddleware',\n    'django.contrib.sessions.middleware.SessionMiddleware',\n    'django.middleware.common.CommonMiddleware',\n    'django.middleware.csrf.CsrfViewMiddleware',\n    'django.contrib.auth.middleware.AuthenticationMiddleware',\n    'django.contrib.messages.middleware.MessageMiddleware',\n    'django.middleware.clickjacking.XFrameOptionsMiddleware',\n]\n\nROOT_URLCONF = 'rdxSolutionsBackendProject.urls'\n\nTEMPLATES = [\n    {\n        'BACKEND': 'django.template.backends.django.DjangoTemplates',\n        'DIRS': [],\n        'APP_DIRS': True,\n        'OPTIONS': {\n            'context_processors': [\n                'django.template.context_processors.debug',\n                'django.template.context_processors.request',\n                'django.contrib.auth.context_processors.auth',\n                'django.contrib.messages.context_processors.messages',\n            ],\n        },\n    },\n]\n\nWSGI_APPLICATION = 'rdxSolutionsBackendProject.wsgi.application'\n\n# Database\n# https:\/\/docs.djangoproject.com\/en\/4.1\/ref\/settings\/#databases\n\nDATABASES = {\n    'default': {\n        'ENGINE': 'django.db.backends.sqlite3',\n        'NAME': BASE_DIR \/ 'db.sqlite3',\n    }\n}\n\n\nAUTH_PASSWORD_VALIDATORS = [\n    {\n        'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.NumericPasswordValidator',\n    },\n]\n\n\n# Internationalization\n# https:\/\/docs.djangoproject.com\/en\/4.1\/topics\/i18n\/\n\nLANGUAGE_CODE = 'en-us'\n\nTIME_ZONE = 'UTC'\n\nUSE_I18N = True\n\nUSE_TZ = True\n\n\n# Static files (CSS, JavaScript, Images)\n# https:\/\/docs.djangoproject.com\/en\/4.1\/howto\/static-files\/\n\n\nSTATIC_ROOT = '\/static\/'\nSTATIC_URL = '\/static\/'\n\n# Default primary key field type\n# https:\/\/docs.djangoproject.com\/en\/4.1\/ref\/settings\/#default-auto-field\n\nDEFAULT_AUTO_FIELD = 'django.db.models.BigAutoField'\n"}},"msg":"Settings.py was tampered with. Please do not change this file when pushing to dev. Change the debug to true when running locally and then change it back if pushing to dev."}},"https:\/\/github.com\/AxelJnsson\/TDDC88-Software-Engineering":{"1b5728589c34b2ac3b5d0275ff1ced1d0f0910b4":{"url":"https:\/\/api.github.com\/repos\/AxelJnsson\/TDDC88-Software-Engineering\/commits\/1b5728589c34b2ac3b5d0275ff1ced1d0f0910b4","html_url":"https:\/\/github.com\/AxelJnsson\/TDDC88-Software-Engineering\/commit\/1b5728589c34b2ac3b5d0275ff1ced1d0f0910b4","message":"Settings.py was tampered with. Please do not change this file when pushing to dev. Change the debug to true when running locally and then change it back if pushing to dev.","sha":"1b5728589c34b2ac3b5d0275ff1ced1d0f0910b4","keyword":"tampering change","diff":"diff --git a\/Web\/rdxSolutionsBackendProject\/settings.py b\/Web\/rdxSolutionsBackendProject\/settings.py\nindex f66687b..326c89e 100644\n--- a\/Web\/rdxSolutionsBackendProject\/settings.py\n+++ b\/Web\/rdxSolutionsBackendProject\/settings.py\n@@ -9,29 +9,32 @@\n For the full list of settings and their values, see\n https:\/\/docs.djangoproject.com\/en\/4.1\/ref\/settings\/\n \"\"\"\n-\n+import os\n from pathlib import Path\n-# from rdxSolutionsBackendProject import databaseConfig\n+from datetime import timedelta\n+from rest_framework.settings import api_settings\n \n # Build paths inside the project like this: BASE_DIR \/ 'subdir'.\n BASE_DIR = Path(__file__).resolve().parent.parent\n \n \n-# Quick-start development settings - unsuitable for production\n+# Quick-start development  settings - unsuitable for production\n # See https:\/\/docs.djangoproject.com\/en\/4.1\/howto\/deployment\/checklist\/\n \n # SECURITY WARNING: keep the secret key used in production secret!\n SECRET_KEY = 'django-insecure-&kgxky3c#god85vjz8-%ygle2^)iac+unxt_qud^tp4(10hi&_'\n \n # SECURITY WARNING: don't run with debug turned on in production!\n-DEBUG = False\n \n-ALLOWED_HOSTS = ['*']\n+# always set to false when pushing to dev\n+DEBUG = False\n \n+ALLOWED_HOSTS =['*']\n \n # Application definition\n \n INSTALLED_APPS = [\n+    'whitenoise.runserver_nostatic',\n     'django.contrib.admin',\n     'django.contrib.auth',\n     'django.contrib.contenttypes',\n@@ -39,17 +42,36 @@\n     'django.contrib.messages',\n     'django.contrib.staticfiles',\n     'rest_framework',\n+    'rest_framework.authtoken',\n     'backend',\n+    'knox',\n+\n ]\n \n+REST_FRAMEWORK = {\n+    'DEFAULT_AUTHENTICATION_CLASSES': [\n+        'knox.auth.TokenAuthentication',\n+    ]\n+}\n+\n+REST_KNOX = {\n+    'TOKEN_TTL': timedelta(minutes=600),\n+    'USER_SERIALIZER': 'knox.serializers.UserSerializer',\n+    'TOKEN_LIMIT_PER_USER': None,\n+    'AUTO_REFRESH': False,\n+    'EXPIRY_DATETIME_FORMAT': api_settings.DATETIME_FORMAT,\n+}\n+\n MIDDLEWARE = [\n     'django.middleware.security.SecurityMiddleware',\n+    'whitenoise.middleware.WhiteNoiseMiddleware',\n     'django.contrib.sessions.middleware.SessionMiddleware',\n     'django.middleware.common.CommonMiddleware',\n     'django.middleware.csrf.CsrfViewMiddleware',\n     'django.contrib.auth.middleware.AuthenticationMiddleware',\n     'django.contrib.messages.middleware.MessageMiddleware',\n-    'django.middleware.clickjacking.XFrameOptionsMiddleware',\n+    'django.middleware.clickjacking.XFrameOptionsMiddleware',    \n+\n ]\n \n ROOT_URLCONF = 'rdxSolutionsBackendProject.urls'\n@@ -57,7 +79,7 @@\n TEMPLATES = [\n     {\n         'BACKEND': 'django.template.backends.django.DjangoTemplates',\n-        'DIRS': [],\n+        'DIRS': [BASE_DIR \/ \"rdxSolutionsBackendProject\/templates\"],\n         'APP_DIRS': True,\n         'OPTIONS': {\n             'context_processors': [\n@@ -99,6 +121,7 @@\n ]\n \n \n+\n # Internationalization\n # https:\/\/docs.djangoproject.com\/en\/4.1\/topics\/i18n\/\n \n@@ -115,10 +138,25 @@\n # https:\/\/docs.djangoproject.com\/en\/4.1\/howto\/static-files\/\n \n \n-STATIC_ROOT = '\/static\/'\n-STATIC_URL = '\/static\/'\n \n-# Default primary key field type\n-# https:\/\/docs.djangoproject.com\/en\/4.1\/ref\/settings\/#default-auto-field\n+\n+BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n+# Static files (CSS, JavaScript, Images)\n+# https:\/\/docs.djangoproject.com\/en\/1.9\/howto\/static-files\/\n+STATICFILES_STORAGE = \"whitenoise.storage.CompressedManifestStaticFilesStorage\"\n+\n+STATIC_URL = '\/admin\/static\/'\n+STATIC_ROOT = os.path.join(BASE_DIR, 'staticfiles')\n+\n+\n+# Extra places for collectstatic to find static files.\n+STATICFILES_DIRS = (\n+    os.path.join(BASE_DIR, 'static'),\n+)\n+\n+\n+\n \n DEFAULT_AUTO_FIELD = 'django.db.models.BigAutoField'\n+\n+LOGIN_REDIRECT_URL = \"\/success\"\ndiff --git a\/Web\/requirements.txt b\/Web\/requirements.txt\nindex 2a06ff1..972e218 100644\n--- a\/Web\/requirements.txt\n+++ b\/Web\/requirements.txt\n@@ -36,4 +36,4 @@ Pillow==9.2.0\n django-rest-knox\n gunicorn\n whitenoise\n-django-dotenv\n\\ No newline at end of file\n+django-dotenv\n","files":{"\/Web\/rdxSolutionsBackendProject\/settings.py":{"changes":[{"diff":"\n For the full list of settings and their values, see\n https:\/\/docs.djangoproject.com\/en\/4.1\/ref\/settings\/\n \"\"\"\n-\n+import os\n from pathlib import Path\n-# from rdxSolutionsBackendProject import databaseConfig\n+from datetime import timedelta\n+from rest_framework.settings import api_settings\n \n # Build paths inside the project like this: BASE_DIR \/ 'subdir'.\n BASE_DIR = Path(__file__).resolve().parent.parent\n \n \n-# Quick-start development settings - unsuitable for production\n+# Quick-start development  settings - unsuitable for production\n # See https:\/\/docs.djangoproject.com\/en\/4.1\/howto\/deployment\/checklist\/\n \n # SECURITY WARNING: keep the secret key used in production secret!\n SECRET_KEY = 'django-insecure-&kgxky3c#god85vjz8-%ygle2^)iac+unxt_qud^tp4(10hi&_'\n \n # SECURITY WARNING: don't run with debug turned on in production!\n-DEBUG = False\n \n-ALLOWED_HOSTS = ['*']\n+# always set to false when pushing to dev\n+DEBUG = False\n \n+ALLOWED_HOSTS =['*']\n \n # Application definition\n \n INSTALLED_APPS = [\n+    'whitenoise.runserver_nostatic',\n     'django.contrib.admin',\n     'django.contrib.auth',\n     'django.contrib.contenttypes',\n","add":8,"remove":5,"filename":"\/Web\/rdxSolutionsBackendProject\/settings.py","badparts":["DEBUG = False","ALLOWED_HOSTS = ['*']"],"goodparts":["from datetime import timedelta","from rest_framework.settings import api_settings","DEBUG = False","ALLOWED_HOSTS =['*']","    'whitenoise.runserver_nostatic',"]},{"diff":"\n     'django.contrib.staticfiles',\n     'rest_framework',\n+    'rest_framework.authtoken',\n     'backend',\n+    'knox',\n+\n ]\n \n+REST_FRAMEWORK = {\n+    'DEFAULT_AUTHENTICATION_CLASSES': [\n+        'knox.auth.TokenAuthentication',\n+    ]\n+}\n+\n+REST_KNOX = {\n+    'TOKEN_TTL': timedelta(minutes=600),\n+    'USER_SERIALIZER': 'knox.serializers.UserSerializer',\n+    'TOKEN_LIMIT_PER_USER': None,\n+    'AUTO_REFRESH': False,\n+    'EXPIRY_DATETIME_FORMAT': api_settings.DATETIME_FORMAT,\n+}\n+\n MIDDLEWARE = [\n     'django.middleware.security.SecurityMiddleware',\n+    'whitenoise.middleware.WhiteNoiseMiddleware',\n     'django.contrib.sessions.middleware.SessionMiddleware',\n     'django.middleware.common.CommonMiddleware',\n     'django.middleware.csrf.CsrfViewMiddleware',\n     'django.contrib.auth.middleware.AuthenticationMiddleware',\n     'django.contrib.messages.middleware.MessageMiddleware',\n-    'django.middleware.clickjacking.XFrameOptionsMiddleware',\n+    'django.middleware.clickjacking.XFrameOptionsMiddleware',    \n+\n ]\n \n ROOT_URLCONF = 'rdxSolutionsBackendProject.urls'\n","add":20,"remove":1,"filename":"\/Web\/rdxSolutionsBackendProject\/settings.py","badparts":["    'django.middleware.clickjacking.XFrameOptionsMiddleware',"],"goodparts":["    'rest_framework.authtoken',","    'knox',","REST_FRAMEWORK = {","    'DEFAULT_AUTHENTICATION_CLASSES': [","        'knox.auth.TokenAuthentication',","    ]","}","REST_KNOX = {","    'TOKEN_TTL': timedelta(minutes=600),","    'USER_SERIALIZER': 'knox.serializers.UserSerializer',","    'TOKEN_LIMIT_PER_USER': None,","    'AUTO_REFRESH': False,","    'EXPIRY_DATETIME_FORMAT': api_settings.DATETIME_FORMAT,","}","    'whitenoise.middleware.WhiteNoiseMiddleware',","    'django.middleware.clickjacking.XFrameOptionsMiddleware',    "]},{"diff":"\n     {\n         'BACKEND': 'django.template.backends.django.DjangoTemplates',\n-        'DIRS': [],\n+        'DIRS': [BASE_DIR \/ \"rdxSolutionsBackendProject\/templates\"],\n         'APP_DIRS': True,\n         'OPTIONS': {\n             'context_processors': [\n","add":1,"remove":1,"filename":"\/Web\/rdxSolutionsBackendProject\/settings.py","badparts":["        'DIRS': [],"],"goodparts":["        'DIRS': [BASE_DIR \/ \"rdxSolutionsBackendProject\/templates\"],"]},{"diff":"\n \n \n-STATIC_ROOT = '\/static\/'\n-STATIC_URL = '\/static\/'\n \n-# Default primary key field type\n-# https:\/\/docs.djangoproject.com\/en\/4.1\/ref\/settings\/#default-auto-field\n+\n+BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n+# Static files (CSS, JavaScript, Images)\n+# https:\/\/docs.djangoproject.com\/en\/1.9\/howto\/static-files\/\n+STATICFILES_STORAGE = \"whitenoise.storage.CompressedManifestStaticFilesStorage\"\n+\n+STATIC_URL = '\/admin\/static\/'\n+STATIC_ROOT = os.path.join(BASE_DIR, 'staticfiles')\n+\n+\n+# Extra places for collectstatic to find static files.\n+STATICFILES_DIRS = (\n+    os.path.join(BASE_DIR, 'static'),\n+)\n+\n+\n+\n \n DEFAULT_AUTO_FIELD = 'django.db.models.BigAutoField'\n+\n+LOGIN_REDIRECT_URL = \"\/success\"","add":19,"remove":4,"filename":"\/Web\/rdxSolutionsBackendProject\/settings.py","badparts":["STATIC_ROOT = '\/static\/'","STATIC_URL = '\/static\/'"],"goodparts":["BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))","STATICFILES_STORAGE = \"whitenoise.storage.CompressedManifestStaticFilesStorage\"","STATIC_URL = '\/admin\/static\/'","STATIC_ROOT = os.path.join(BASE_DIR, 'staticfiles')","STATICFILES_DIRS = (","    os.path.join(BASE_DIR, 'static'),",")","LOGIN_REDIRECT_URL = \"\/success\""]}],"source":"\n\"\"\" Django settings for rdxSolutionsBackendProject project. Generated by 'django-admin startproject' using Django 4.1.1. For more information on this file, see https:\/\/docs.djangoproject.com\/en\/4.1\/topics\/settings\/ For the full list of settings and their values, see https:\/\/docs.djangoproject.com\/en\/4.1\/ref\/settings\/ \"\"\" from pathlib import Path BASE_DIR=Path(__file__).resolve().parent.parent SECRET_KEY='django-insecure-&kgxky3c DEBUG=False ALLOWED_HOSTS=['*'] INSTALLED_APPS=[ 'django.contrib.admin', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles', 'rest_framework', 'backend', ] MIDDLEWARE=[ 'django.middleware.security.SecurityMiddleware', 'django.contrib.sessions.middleware.SessionMiddleware', 'django.middleware.common.CommonMiddleware', 'django.middleware.csrf.CsrfViewMiddleware', 'django.contrib.auth.middleware.AuthenticationMiddleware', 'django.contrib.messages.middleware.MessageMiddleware', 'django.middleware.clickjacking.XFrameOptionsMiddleware', ] ROOT_URLCONF='rdxSolutionsBackendProject.urls' TEMPLATES=[ { 'BACKEND': 'django.template.backends.django.DjangoTemplates', 'DIRS':[], 'APP_DIRS': True, 'OPTIONS':{ 'context_processors':[ 'django.template.context_processors.debug', 'django.template.context_processors.request', 'django.contrib.auth.context_processors.auth', 'django.contrib.messages.context_processors.messages', ], }, }, ] WSGI_APPLICATION='rdxSolutionsBackendProject.wsgi.application' DATABASES={ 'default':{ 'ENGINE': 'django.db.backends.sqlite3', 'NAME': BASE_DIR \/ 'db.sqlite3', } } AUTH_PASSWORD_VALIDATORS=[ { 'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator', }, { 'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator', }, { 'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator', }, { 'NAME': 'django.contrib.auth.password_validation.NumericPasswordValidator', }, ] LANGUAGE_CODE='en-us' TIME_ZONE='UTC' USE_I18N=True USE_TZ=True STATIC_ROOT='\/static\/' STATIC_URL='\/static\/' DEFAULT_AUTO_FIELD='django.db.models.BigAutoField' ","sourceWithComments":"\"\"\"\nDjango settings for rdxSolutionsBackendProject project.\n\nGenerated by 'django-admin startproject' using Django 4.1.1.\n\nFor more information on this file, see\nhttps:\/\/docs.djangoproject.com\/en\/4.1\/topics\/settings\/\n\nFor the full list of settings and their values, see\nhttps:\/\/docs.djangoproject.com\/en\/4.1\/ref\/settings\/\n\"\"\"\n\nfrom pathlib import Path\n# from rdxSolutionsBackendProject import databaseConfig\n\n# Build paths inside the project like this: BASE_DIR \/ 'subdir'.\nBASE_DIR = Path(__file__).resolve().parent.parent\n\n\n# Quick-start development settings - unsuitable for production\n# See https:\/\/docs.djangoproject.com\/en\/4.1\/howto\/deployment\/checklist\/\n\n# SECURITY WARNING: keep the secret key used in production secret!\nSECRET_KEY = 'django-insecure-&kgxky3c#god85vjz8-%ygle2^)iac+unxt_qud^tp4(10hi&_'\n\n# SECURITY WARNING: don't run with debug turned on in production!\nDEBUG = False\n\nALLOWED_HOSTS = ['*']\n\n\n# Application definition\n\nINSTALLED_APPS = [\n    'django.contrib.admin',\n    'django.contrib.auth',\n    'django.contrib.contenttypes',\n    'django.contrib.sessions',\n    'django.contrib.messages',\n    'django.contrib.staticfiles',\n    'rest_framework',\n    'backend',\n]\n\nMIDDLEWARE = [\n    'django.middleware.security.SecurityMiddleware',\n    'django.contrib.sessions.middleware.SessionMiddleware',\n    'django.middleware.common.CommonMiddleware',\n    'django.middleware.csrf.CsrfViewMiddleware',\n    'django.contrib.auth.middleware.AuthenticationMiddleware',\n    'django.contrib.messages.middleware.MessageMiddleware',\n    'django.middleware.clickjacking.XFrameOptionsMiddleware',\n]\n\nROOT_URLCONF = 'rdxSolutionsBackendProject.urls'\n\nTEMPLATES = [\n    {\n        'BACKEND': 'django.template.backends.django.DjangoTemplates',\n        'DIRS': [],\n        'APP_DIRS': True,\n        'OPTIONS': {\n            'context_processors': [\n                'django.template.context_processors.debug',\n                'django.template.context_processors.request',\n                'django.contrib.auth.context_processors.auth',\n                'django.contrib.messages.context_processors.messages',\n            ],\n        },\n    },\n]\n\nWSGI_APPLICATION = 'rdxSolutionsBackendProject.wsgi.application'\n\n# Database\n# https:\/\/docs.djangoproject.com\/en\/4.1\/ref\/settings\/#databases\n\nDATABASES = {\n    'default': {\n        'ENGINE': 'django.db.backends.sqlite3',\n        'NAME': BASE_DIR \/ 'db.sqlite3',\n    }\n}\n\n\nAUTH_PASSWORD_VALIDATORS = [\n    {\n        'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.NumericPasswordValidator',\n    },\n]\n\n\n# Internationalization\n# https:\/\/docs.djangoproject.com\/en\/4.1\/topics\/i18n\/\n\nLANGUAGE_CODE = 'en-us'\n\nTIME_ZONE = 'UTC'\n\nUSE_I18N = True\n\nUSE_TZ = True\n\n\n# Static files (CSS, JavaScript, Images)\n# https:\/\/docs.djangoproject.com\/en\/4.1\/howto\/static-files\/\n\n\nSTATIC_ROOT = '\/static\/'\nSTATIC_URL = '\/static\/'\n\n# Default primary key field type\n# https:\/\/docs.djangoproject.com\/en\/4.1\/ref\/settings\/#default-auto-field\n\nDEFAULT_AUTO_FIELD = 'django.db.models.BigAutoField'\n"}},"msg":"Settings.py was tampered with. Please do not change this file when pushing to dev. Change the debug to true when running locally and then change it back if pushing to dev."}},"https:\/\/github.com\/ManuelBerrueta\/BST":{"27f1976598dc3df9c9007663276fa21c3b8f6940":{"url":"https:\/\/api.github.com\/repos\/ManuelBerrueta\/BST\/commits\/27f1976598dc3df9c9007663276fa21c3b8f6940","html_url":"https:\/\/github.com\/ManuelBerrueta\/BST\/commit\/27f1976598dc3df9c9007663276fa21c3b8f6940","message":"Added tampering function to change the payload only of thew JWT token","sha":"27f1976598dc3df9c9007663276fa21c3b8f6940","keyword":"tampering change","diff":"diff --git a\/Azure\/BST-JWTDecoder.py b\/Azure\/BST-JWTDecoder.py\nindex 239a261..01d2c83 100644\n--- a\/Azure\/BST-JWTDecoder.py\n+++ b\/Azure\/BST-JWTDecoder.py\n@@ -5,7 +5,7 @@\n import os\n import subprocess\n \n-from jwt.utils import base64url_decode\n+from jwt.utils import base64url_decode, base64url_encode\n \n user = subprocess.check_output(\"whoami\")\n token_path = \"\/home\/\" + user.decode().strip('\\n') + \"\/.azure\/accessTokens.json\"\n@@ -48,7 +48,7 @@ def decode_jwt():\n                     partOfJWT)), indent=3) + bcolors.ENDC)\n \n             # Decode Header and Claims together\n-            #print(jwt.decode(token['accessToken'], verify=False))\n+            # print(jwt.decode(token['accessToken'], verify=False))\n \n             # jwt.decode(token['accessToken'], algorithms=['RS256'])\n             print(\"\\n\")\n@@ -56,9 +56,27 @@ def decode_jwt():\n             # https:\/\/stackoverflow.com\/questions\/59425161\/getting-only-decoded-payload-from-jwt-in-python\n \n \n-def encode_jwt():\n+def encode_clean_jwt():\n     # Ref: https:\/\/auth0.com\/blog\/how-to-handle-jwt-in-python\/\n+    # encoded_JWT = jwt.encode({\"some\": \"payload\"}, private_key, algorithm=\"RS256\")\n+    # jwt.encode(\n+    #    {\"some\": \"payload\"},\n+    #    \"secret\",\n+    #    algorithm=\"HS256\",\n+    #    headers={\"kid\": \"230498151c214b788dd97f22b85410a5\"},\n+    # )\n     print(\"WIP\")\n \n \n+def tamper_jwt_payload(in_jwt: str, tampered_payload: str):\n+    split_JWT = in_jwt.split(\".\")\n+\n+    split_JWT[1] = base64url_encode(tampered_payload.encode()).decode()\n+    #split_JWT[1] = 'test'\n+\n+    tampered_jwt = '.'.join(split_JWT)\n+\n+    print(tampered_jwt)\n+\n+\n decode_jwt()\n","files":{"\/Azure\/BST-JWTDecoder.py":{"changes":[{"diff":"\n import os\n import subprocess\n \n-from jwt.utils import base64url_decode\n+from jwt.utils import base64url_decode, base64url_encode\n \n user = subprocess.check_output(\"whoami\")\n token_path = \"\/home\/\" + user.decode().strip('\\n') + \"\/.azure\/accessTokens.json\"\n","add":1,"remove":1,"filename":"\/Azure\/BST-JWTDecoder.py","badparts":["from jwt.utils import base64url_decode"],"goodparts":["from jwt.utils import base64url_decode, base64url_encode"]},{"diff":"\n             # https:\/\/stackoverflow.com\/questions\/59425161\/getting-only-decoded-payload-from-jwt-in-python\n \n \n-def encode_jwt():\n+def encode_clean_jwt():\n     # Ref: https:\/\/auth0.com\/blog\/how-to-handle-jwt-in-python\/\n+    # encoded_JWT = jwt.encode({\"some\": \"payload\"}, private_key, algorithm=\"RS256\")\n+    # jwt.encode(\n+    #    {\"some\": \"payload\"},\n+    #    \"secret\",\n+    #    algorithm=\"HS256\",\n+    #    headers={\"kid\": \"230498151c214b788dd97f22b85410a5\"},\n+    # )\n     print(\"WIP\")\n \n \n+def tamper_jwt_payload(in_jwt: str, tampered_payload: str):\n+    split_JWT = in_jwt.split(\".\")\n+\n+    split_JWT[1] = base64url_encode(tampered_payload.encode()).decode()\n+    #split_JWT[1] = 'test'\n+\n+    tampered_jwt = '.'.join(split_JWT)\n+\n+    print(tampered_jwt)\n+\n+\n decode_jwt()\n","add":19,"remove":1,"filename":"\/Azure\/BST-JWTDecoder.py","badparts":["def encode_jwt():"],"goodparts":["def encode_clean_jwt():","def tamper_jwt_payload(in_jwt: str, tampered_payload: str):","    split_JWT = in_jwt.split(\".\")","    split_JWT[1] = base64url_encode(tampered_payload.encode()).decode()","    tampered_jwt = '.'.join(split_JWT)","    print(tampered_jwt)"]}],"source":"\n\nimport jwt import base64 import json import os import subprocess from jwt.utils import base64url_decode user=subprocess.check_output(\"whoami\") token_path=\"\/home\/\" +user.decode().strip('\\n') +\"\/.azure\/accessTokens.json\" class bcolors: HEADER='\\033[95m' OKBLUE='\\033[94m' OKCYAN='\\033[96m' OKGREEN='\\033[92m' WARNING='\\033[93m' FAIL='\\033[91m' ENDC='\\033[0m' BOLD='\\033[1m' UNDERLINE='\\033[4m' def decode_jwt(): with open(token_path, \"r\") as data_file: tokens=json.load(data_file) for i, token in enumerate(tokens): print(bcolors.WARNING +\"][Token print(token['accessToken'] +bcolors.ENDC) split_JWT=str(token['accessToken']).split(\".\") for j, partOfJWT in enumerate(split_JWT): if j==0: print(bcolors.HEADER +\"Header:\") elif j==1: print(bcolors.OKBLUE +\"Claims:\") else: print(bcolors.OKGREEN +\"Signature:\\n\" + base64url_decode(partOfJWT).__str__() +bcolors.ENDC) break print(json.dumps(json.loads(base64url_decode( partOfJWT)), indent=3) +bcolors.ENDC) print(\"\\n\") def encode_jwt(): print(\"WIP\") decode_jwt() ","sourceWithComments":"#!\/usr\/bin\/env python3\nimport jwt\nimport base64\nimport json\nimport os\nimport subprocess\n\nfrom jwt.utils import base64url_decode\n\nuser = subprocess.check_output(\"whoami\")\ntoken_path = \"\/home\/\" + user.decode().strip('\\n') + \"\/.azure\/accessTokens.json\"\n\n\nclass bcolors:\n    HEADER = '\\033[95m'\n    OKBLUE = '\\033[94m'\n    OKCYAN = '\\033[96m'\n    OKGREEN = '\\033[92m'\n    WARNING = '\\033[93m'\n    FAIL = '\\033[91m'\n    ENDC = '\\033[0m'\n    BOLD = '\\033[1m'\n    UNDERLINE = '\\033[4m'\n\n\ndef decode_jwt():\n    with open(token_path, \"r\") as data_file:\n        tokens = json.load(data_file)\n        for i, token in enumerate(tokens):\n            # TODO: FORMAT STRING#print(\"][Token #][\",token)\n            print(bcolors.WARNING + \"][Token #\" + i.__str__() + \"][\")\n            print(token['accessToken'] + bcolors.ENDC)\n\n            # Split decode each piece:\n            split_JWT = str(token['accessToken']).split(\".\")\n            for j, partOfJWT in enumerate(split_JWT):\n                if j == 0:\n                    print(bcolors.HEADER + \"Header:\")\n                elif j == 1:\n                    print(bcolors.OKBLUE + \"Claims:\")\n                else:\n                    print(bcolors.OKGREEN + \"Signature:\\n\" +\n                          base64url_decode(partOfJWT).__str__() + bcolors.ENDC)\n                    break\n\n                # print(base64url_decode(partOfJWT).__str__() + bcolors.ENDC)\n                print(json.dumps(json.loads(base64url_decode(\n                    partOfJWT)), indent=3) + bcolors.ENDC)\n\n            # Decode Header and Claims together\n            #print(jwt.decode(token['accessToken'], verify=False))\n\n            # jwt.decode(token['accessToken'], algorithms=['RS256'])\n            print(\"\\n\")\n\n            # https:\/\/stackoverflow.com\/questions\/59425161\/getting-only-decoded-payload-from-jwt-in-python\n\n\ndef encode_jwt():\n    # Ref: https:\/\/auth0.com\/blog\/how-to-handle-jwt-in-python\/\n    print(\"WIP\")\n\n\ndecode_jwt()\n"}},"msg":"Added tampering function to change the payload only of thew JWT token"},"e284d4b5325ef2e3b9de71e03bea08fe98949dda":{"url":"https:\/\/api.github.com\/repos\/ManuelBerrueta\/BST\/commits\/e284d4b5325ef2e3b9de71e03bea08fe98949dda","html_url":"https:\/\/github.com\/ManuelBerrueta\/BST\/commit\/e284d4b5325ef2e3b9de71e03bea08fe98949dda","sha":"e284d4b5325ef2e3b9de71e03bea08fe98949dda","keyword":"tampering change","diff":"diff --git a\/Azure\/BST-JWTDecoder.py b\/Azure\/BST-JWTTool.py\nsimilarity index 88%\nrename from Azure\/BST-JWTDecoder.py\nrename to Azure\/BST-JWTTool.py\nindex a1110c1..c3b99d8 100644\n--- a\/Azure\/BST-JWTDecoder.py\n+++ b\/Azure\/BST-JWTTool.py\n@@ -15,6 +15,9 @@\n                     action='store_true', help='--grabAzTokdefault')\n parser.add_argument('--decode', type=str, required=False,\n                     default='', help='--decode <eyJ0...tokenString>')\n+parser.add_argument('--tamper', nargs=2, metavar=('in_jwt', 'new_payload'), type=str, required=False, default='',\n+                    help='--tamper <eyJ0...tokenString> \\'{\"aud\": \"https:\/\/some.domain.net\/\",\"iat\": 16408037602,\"nbf\": 1648976610,\"exp\": 1648986610,\"name\": \"Not Me\",\"sub\": \"1A2b3C4d5E6f7G8h9I0j\"}\\'')\n+\n args = parser.parse_args()\n \n \n@@ -81,7 +84,7 @@ def tamper_jwt_payload(in_jwt: str, tampered_payload: str):\n     #split_JWT[1] = 'test'\n \n     tampered_jwt = '.'.join(split_JWT)\n-\n+    print(\"\\n===[New Tampered JWT]===\\n\")\n     print(tampered_jwt)\n \n \n@@ -115,11 +118,15 @@ def main():\n         print(\"DECODING JWT\")\n         decode_jwt(args.decode)\n         print(\"DONE!\")\n+    elif args.tamper:\n+        in_jwt, new_payload = args.tamper\n+        tamper_jwt_payload(in_jwt, new_payload)\n+        print(\"Finished Tampered JWT\")\n     elif args.grabAzTokdefault:\n         user = subprocess.check_output(\"whoami\")\n         token_path = \"\/home\/\" + user.decode().strip('\\n') + \"\/.azure\/accessTokens.json\"\n         decode_az_accesstokens(token_path)\n-        print(\"Running DEfault\")\n+        print(\"Running Default\")\n     elif args.grabAzTokens:\n         decode_az_accesstokens(args.grabAzTokens)\n         print(\"Running passed in\")\n","message":"","files":{"\/Azure\/BST-JWTDecoder.py":{"changes":[{"diff":"\n         print(\"DECODING JWT\")\n         decode_jwt(args.decode)\n         print(\"DONE!\")\n+    elif args.tamper:\n+        in_jwt, new_payload = args.tamper\n+        tamper_jwt_payload(in_jwt, new_payload)\n+        print(\"Finished Tampered JWT\")\n     elif args.grabAzTokdefault:\n         user = subprocess.check_output(\"whoami\")\n         token_path = \"\/home\/\" + user.decode().strip('\\n') + \"\/.azure\/accessTokens.json\"\n         decode_az_accesstokens(token_path)\n-        print(\"Running DEfault\")\n+        print(\"Running Default\")\n     elif args.grabAzTokens:\n         decode_az_accesstokens(args.grabAzTokens)\n         print(\"Running passed in\")\n","add":5,"remove":1,"filename":"\/Azure\/BST-JWTDecoder.py","badparts":["        print(\"Running DEfault\")"],"goodparts":["    elif args.tamper:","        in_jwt, new_payload = args.tamper","        tamper_jwt_payload(in_jwt, new_payload)","        print(\"Finished Tampered JWT\")","        print(\"Running Default\")"]}],"source":"\n\nimport jwt from jwt.utils import base64url_decode, base64url_encode import base64 import json import os import subprocess import argparse parser=argparse.ArgumentParser() parser.add_argument('--grabAzTokens', type=str, required=False, default='.azure\/accessTokens.json', help='--grabAzTokens \/path\/to\/.azure\/accessTokens.json') parser.add_argument('--grabAzTokdefault', required=False, default=False, action='store_true', help='--grabAzTokdefault') parser.add_argument('--decode', type=str, required=False, default='', help='--decode <eyJ0...tokenString>') args=parser.parse_args() class bcolors: HEADER='\\033[95m' OKBLUE='\\033[94m' OKCYAN='\\033[96m' OKGREEN='\\033[92m' WARNING='\\033[93m' FAIL='\\033[91m' ENDC='\\033[0m' BOLD='\\033[1m' UNDERLINE='\\033[4m' def decode_az_accesstokens(token_path): with open(token_path, \"r\") as data_file: tokens=json.load(data_file) for i, token in enumerate(tokens): print(bcolors.WARNING +\"][Token print(token['accessToken'] +bcolors.ENDC) split_JWT=str(token['accessToken']).split(\".\") for j, partOfJWT in enumerate(split_JWT): if j==0: print(bcolors.HEADER +\"Header:\") elif j==1: print(bcolors.OKBLUE +\"Claims:\") else: print(bcolors.OKGREEN +\"Signature:\\n\" + base64url_decode(partOfJWT).__str__() +bcolors.ENDC) break print(json.dumps(json.loads(base64url_decode( partOfJWT)), indent=3) +bcolors.ENDC) print(\"\\n\") def encode_clean_jwt(): print(\"WIP\") def tamper_jwt_payload(in_jwt: str, tampered_payload: str): split_JWT=in_jwt.split(\".\") split_JWT[1]=base64url_encode(tampered_payload.encode()).decode() tampered_jwt='.'.join(split_JWT) print(tampered_jwt) def decode_jwt(in_jwt: str): split_JWT=in_jwt.split(\".\") for j, partOfJWT in enumerate(split_JWT): if j==0: print(bcolors.HEADER +\"Header:\") elif j==1: print(bcolors.OKBLUE +\"Claims:\") else: print(bcolors.OKGREEN +\"Signature:\\n\" + base64url_decode(partOfJWT).__str__() +bcolors.ENDC) break print(json.dumps(json.loads(base64url_decode( partOfJWT)), indent=3) +bcolors.ENDC) print(\"\\n\") def main(): token_path='' if args.decode: print(\"DECODING JWT\") decode_jwt(args.decode) print(\"DONE!\") elif args.grabAzTokdefault: user=subprocess.check_output(\"whoami\") token_path=\"\/home\/\" +user.decode().strip('\\n') +\"\/.azure\/accessTokens.json\" decode_az_accesstokens(token_path) print(\"Running DEfault\") elif args.grabAzTokens: decode_az_accesstokens(args.grabAzTokens) print(\"Running passed in\") print(args.grabAzTokens) if __name__==\"__main__\": main() ","sourceWithComments":"#!\/usr\/bin\/env python3\nimport jwt\nfrom jwt.utils import base64url_decode, base64url_encode\nimport base64\nimport json\nimport os\nimport subprocess\nimport argparse\n\nparser = argparse.ArgumentParser()\n\nparser.add_argument('--grabAzTokens', type=str, required=False,\n                    default='.azure\/accessTokens.json', help='--grabAzTokens \/path\/to\/.azure\/accessTokens.json')\nparser.add_argument('--grabAzTokdefault', required=False, default=False,\n                    action='store_true', help='--grabAzTokdefault')\nparser.add_argument('--decode', type=str, required=False,\n                    default='', help='--decode <eyJ0...tokenString>')\nargs = parser.parse_args()\n\n\nclass bcolors:\n    HEADER = '\\033[95m'\n    OKBLUE = '\\033[94m'\n    OKCYAN = '\\033[96m'\n    OKGREEN = '\\033[92m'\n    WARNING = '\\033[93m'\n    FAIL = '\\033[91m'\n    ENDC = '\\033[0m'\n    BOLD = '\\033[1m'\n    UNDERLINE = '\\033[4m'\n\n\ndef decode_az_accesstokens(token_path):\n    with open(token_path, \"r\") as data_file:\n        tokens = json.load(data_file)\n        for i, token in enumerate(tokens):\n            print(bcolors.WARNING + \"][Token #\" + i.__str__() + \"][\")\n            print(token['accessToken'] + bcolors.ENDC)\n\n            # Split decode each piece:\n            split_JWT = str(token['accessToken']).split(\".\")\n            for j, partOfJWT in enumerate(split_JWT):\n                if j == 0:\n                    print(bcolors.HEADER + \"Header:\")\n                elif j == 1:\n                    print(bcolors.OKBLUE + \"Claims:\")\n                else:\n                    print(bcolors.OKGREEN + \"Signature:\\n\" +\n                          base64url_decode(partOfJWT).__str__() + bcolors.ENDC)\n                    break\n\n                # print(base64url_decode(partOfJWT).__str__() + bcolors.ENDC)\n                print(json.dumps(json.loads(base64url_decode(\n                    partOfJWT)), indent=3) + bcolors.ENDC)\n\n            # Decode Header and Claims together\n            # print(jwt.decode(token['accessToken'], verify=False))\n\n            # jwt.decode(token['accessToken'], algorithms=['RS256'])\n            print(\"\\n\")\n\n            # https:\/\/stackoverflow.com\/questions\/59425161\/getting-only-decoded-payload-from-jwt-in-python\n\n\ndef encode_clean_jwt():\n    # Ref: https:\/\/auth0.com\/blog\/how-to-handle-jwt-in-python\/\n    # encoded_JWT = jwt.encode({\"some\": \"payload\"}, private_key, algorithm=\"RS256\")\n    # jwt.encode(\n    #    {\"some\": \"payload\"},\n    #    \"secret\",\n    #    algorithm=\"HS256\",\n    #    headers={\"kid\": \"230498151c214b788dd97f22b85410a5\"},\n    # )\n    print(\"WIP\")\n\n\ndef tamper_jwt_payload(in_jwt: str, tampered_payload: str):\n    split_JWT = in_jwt.split(\".\")\n\n    split_JWT[1] = base64url_encode(tampered_payload.encode()).decode()\n    #split_JWT[1] = 'test'\n\n    tampered_jwt = '.'.join(split_JWT)\n\n    print(tampered_jwt)\n\n\ndef decode_jwt(in_jwt: str):\n    # Split decode each piece:\n    split_JWT = in_jwt.split(\".\")\n    for j, partOfJWT in enumerate(split_JWT):\n        if j == 0:\n            print(bcolors.HEADER + \"Header:\")\n        elif j == 1:\n            print(bcolors.OKBLUE + \"Claims:\")\n        else:\n            print(bcolors.OKGREEN + \"Signature:\\n\" +\n                  base64url_decode(partOfJWT).__str__() + bcolors.ENDC)\n            break\n\n        # print(base64url_decode(partOfJWT).__str__() + bcolors.ENDC)\n        print(json.dumps(json.loads(base64url_decode(\n            partOfJWT)), indent=3) + bcolors.ENDC)\n\n    # Decode Header and Claims together\n    # print(jwt.decode(token['accessToken'], verify=False))\n\n    # jwt.decode(token['accessToken'], algorithms=['RS256'])\n    print(\"\\n\")\n\n\ndef main():\n    token_path = ''\n    if args.decode:\n        print(\"DECODING JWT\")\n        decode_jwt(args.decode)\n        print(\"DONE!\")\n    elif args.grabAzTokdefault:\n        user = subprocess.check_output(\"whoami\")\n        token_path = \"\/home\/\" + user.decode().strip('\\n') + \"\/.azure\/accessTokens.json\"\n        decode_az_accesstokens(token_path)\n        print(\"Running DEfault\")\n    elif args.grabAzTokens:\n        decode_az_accesstokens(args.grabAzTokens)\n        print(\"Running passed in\")\n        print(args.grabAzTokens)\n\n\nif __name__ == \"__main__\":\n    main()\n"}},"msg":"Renamed BST-JWTDecoder.py to BST-JWTTool.py and added functionality to tamper\/change the payload of an input JWT token uing the --tamper flag"}},"https:\/\/github.com\/osmocom\/osmo-dev":{"e6d3620ff0109d466327923eb9d5d4a914b387f2":{"url":"https:\/\/api.github.com\/repos\/osmocom\/osmo-dev\/commits\/e6d3620ff0109d466327923eb9d5d4a914b387f2","html_url":"https:\/\/github.com\/osmocom\/osmo-dev\/commit\/e6d3620ff0109d466327923eb9d5d4a914b387f2","message":"gen_makefile.py: use s#git:\/\/#https:\/\/# in git clone base URL\n\nUnencrypted git:\/\/ protocol offers no integrity or authentication,\nmaking it subject to tampering.  Use https:\/\/ instead.\n\nhttps:\/\/github.blog\/2021-09-01-improving-git-protocol-security-github\/#no-more-unauthenticated-git\nhttps:\/\/blog.readthedocs.com\/github-git-protocol-deprecation\/\n\nUsing https:\/\/ is also required for some new repositories (like\nlibosmo-gprs.git), which are intentionally unavailable over git:\/\/.\n\nChange-Id: I503f0e1b5258102d771597f1b5b753f670832fcb","sha":"e6d3620ff0109d466327923eb9d5d4a914b387f2","keyword":"tampering change","diff":"diff --git a\/gen_makefile.py b\/gen_makefile.py\nindex 0e0df10..6459846 100755\n--- a\/gen_makefile.py\n+++ b\/gen_makefile.py\n@@ -70,8 +70,8 @@\n   help='''Parent dir for all build trees (default:\n directly in the make-dir).''')\n \n-parser.add_argument('-u', '--url', dest='url', default='git:\/\/git.osmocom.org',\n-  help='''git clone base URL. Default is 'git:\/\/git.osmocom.org'.\n+parser.add_argument('-u', '--url', dest='url', default='https:\/\/git.osmocom.org',\n+  help='''git clone base URL. Default is 'https:\/\/git.osmocom.org'.\n e.g. with a config like this in your ~\/.ssh\/config:\n   host go\n   hostname gerrit.osmocom.org\n","files":{"\/gen_makefile.py":{"changes":[{"diff":"\n   help='''Parent dir for all build trees (default:\n directly in the make-dir).''')\n \n-parser.add_argument('-u', '--url', dest='url', default='git:\/\/git.osmocom.org',\n-  help='''git clone base URL. Default is 'git:\/\/git.osmocom.org'.\n+parser.add_argument('-u', '--url', dest='url', default='https:\/\/git.osmocom.org',\n+  help='''git clone base URL. Default is 'https:\/\/git.osmocom.org'.\n e.g. with a config like this in your ~\/.ssh\/config:\n   host go\n   hostname gerrit.osmocom.org\n","add":2,"remove":2,"filename":"\/gen_makefile.py","badparts":["parser.add_argument('-u', '--url', dest='url', default='git:\/\/git.osmocom.org',","  help='''git clone base URL. Default is 'git:\/\/git.osmocom.org'."],"goodparts":["parser.add_argument('-u', '--url', dest='url', default='https:\/\/git.osmocom.org',","  help='''git clone base URL. Default is 'https:\/\/git.osmocom.org'."]}],"source":"\n ''' Generate a top-level makefile that builds the Osmocom 2G +3G network components. .\/gen_makefile.py[configure.opts[more.opts]][-o Makefile.output] Configured by text files: all.deps: whitespace-separated listing of project_name depends_on_project_1 depends_on_project_2... *.opts: whitespace-separated listing of project_name --config-opt-1 --config-opt-2... Thus it is possible to choose between e.g. -building each of those with or without mgcp transcoding support by adding or removing \"transcoding.opts\" from the command line From the Makefile nature, the dependencies extend, no need to repeat common deps. When this script is done, a Makefile has been generated that allows you to build all projects at once by issuing 'make', but also to refresh only parts of it when some bits in the middle have changed. The makefile keeps local progress marker files like.make.libosmocore.configure; if such progress marker is removed or becomes outdated, that step and all dependent ones are re-run. This is helpful in daily hacking across several repositories. Note that by default, this includes 'sudo ldconfig' calls following each installation. You may want to permit your user to run 'sudo ldconfig' without needing a password, e.g. by sudo sh -c \"echo '$USER ALL=NOPASSWD: \/sbin\/ldconfig' > \/etc\/sudoers.d\/${USER}_ldconfig\" You can skip the 'sudo ldconfig' by issuing the --no-ldconfig option. You can run 'ldconfig' without sudo by issuing the --ldconfig-without-sudo option. By default, it is assumed that your user has write permission to \/usr\/local. If you need sudo to install there, you may issue the --sudo-make-install option. EXAMPLE: .\/gen_makefile.py default.opts iu.opts -I -m build cd build make ''' import sys import os import argparse topdir=os.path.dirname(os.path.realpath(__file__)) all_deps_file=os.path.join(topdir, \"all.deps\") parser=argparse.ArgumentParser(epilog=__doc__, formatter_class=argparse.RawTextHelpFormatter) parser.add_argument('configure_opts_files', help='''Config file containing project name and .\/configure options''', nargs='*') parser.add_argument('-m', '--make-dir', dest='make_dir', help='''Place Makefile in this dir(default: create a new dir named after opts files).''') parser.add_argument('-s', '--src-dir', dest='src_dir', default='.\/src', help='Parent dir for all git clones.') parser.add_argument('-b', '--build-dir', dest='build_dir', help='''Parent dir for all build trees(default: directly in the make-dir).''') parser.add_argument('-u', '--url', dest='url', default='git:\/\/git.osmocom.org', help='''git clone base URL. Default is 'git:\/\/git.osmocom.org'. e.g. with a config like this in your ~\/.ssh\/config: host go hostname gerrit.osmocom.org port 29418 you may pass '-u ssh:\/\/go' to be able to submit to gerrit.''') parser.add_argument('-p', '--push-url', dest='push_url', default='', help='''git push-URL. Default is to not configure a separate push-URL.''') parser.add_argument('-o', '--output', dest='output', default='Makefile', help='''Makefile filename(default: 'Makefile').''') parser.add_argument('-j', '--jobs', dest='jobs', default='$(nproc)', help='''-j option to pass to 'make'.''') parser.add_argument('-I', '--sudo-make-install', dest='sudo_make_install', action='store_true', help='''run 'make install' step with 'sudo'.''') parser.add_argument('-L', '--no-ldconfig', dest='no_ldconfig', action='store_true', help='''omit the 'sudo ldconfig' step.''') parser.add_argument('--ldconfig-without-sudo', dest='ldconfig_without_sudo', action='store_true', help='''call just 'ldconfig', without sudo, which implies root privileges(not recommended)''') parser.add_argument('-c', '--no-make-check', dest='make_check', default=True, action='store_false', help='''do not 'make check', just 'make' to build.''') parser.add_argument('--docker-cmd', help='''prefix configure\/make\/make install calls with this command(used by ttcn3.sh)''') parser.add_argument('-g', '--build-debug', dest='build_debug', default=False, action='store_true', help='''set 'CFLAGS=-g' when calling src\/configure''') parser.add_argument('-a', '--auto-distclean', action='store_true', help='''run \"make distclean\" automatically if source directory already configured''') args=parser.parse_args() class listdict(dict): 'a dict of lists{ \"a\":[1, 2, 3], \"b\":[1, 2]}' def add(self, name, item): l=self.get(name) if not l: l=[] self[name]=l l.append(item) def extend(self, name, l): for v in l: self.add(name, v) def add_dict(self, d): for k,v in d.items(): self.add(k, v) def extend_dict(self, d): for k,v in d.items(): l=self.extend(k, v) def read_projects_deps(path): 'Read deps config and return tuples of(project_name, which-other-to-build-first).' l=[] for line in open(path): line=line.strip() if not line or line.startswith(' continue tokens=line.split() l.append((tokens[0], tokens[1:])) return l def read_configure_opts(path): 'Read config opts file and return tuples of(project_name, config-opts).' if not path: return{} return dict(read_projects_deps(path)) def gen_makefile_clone(proj, src, src_proj, url, push_url): if proj==\"osmocom-bb_layer23\": return f''' .make.{proj}.clone:.make.osmocom-bb.clone \t@echo -e \"\\\\n\\\\n\\\\n=====$@\\\\n\" \ttest -L{src_proj} || ln -s osmocom-bb\/src\/host\/layer23{src_proj} \ttouch $@ ''' if proj==\"osmocom-bb_virtphy\": return f''' .make.{proj}.clone:.make.osmocom-bb.clone \t@echo -e \"\\\\n\\\\n\\\\n=====$@\\\\n\" \ttest -L{src_proj} || ln -s osmocom-bb\/src\/host\/virt_phy{src_proj} \ttouch $@ ''' if proj in(\"libgtpnl\", \"libnftnl\", \"nftables\"): url=\"git:\/\/git.netfilter.org\" return f''' .make.{proj}.clone: \t@echo -e \"\\\\n\\\\n\\\\n=====$@\\\\n\" \ttest -d{src} || mkdir -p{src} \ttest -d{src_proj} ||( git -C{src} clone \"{url}\/{proj}\" \"{proj}\" && git -C \"{src}\/{proj}\" remote set-url --push origin \"{push_url}\/{proj}\") \tsync \ttouch $@ ''' def gen_make(proj, deps, configure_opts, jobs, make_dir, src_dir, build_dir, url, push_url, sudo_make_install, no_ldconfig, ldconfig_without_sudo, make_check): src_proj=os.path.join(src_dir, proj) if proj=='openbsc': src_proj=os.path.join(src_proj, 'openbsc') build_proj=os.path.join(build_dir, proj) make_to_build_proj=os.path.relpath(build_proj, make_dir) build_to_src=os.path.relpath(src_proj, build_proj) src=os.path.relpath(src_dir, make_dir) src_proj=os.path.relpath(src_proj, make_dir) push_url=push_url or url if configure_opts: configure_opts_str=' '.join(configure_opts) else: configure_opts_str='' return r''' {proj}_configure_files:=$(shell find -L{src_proj} \\ -name \"Makefile.am\" \\ -or -name \"*.in\" \\ -and -not -name \"Makefile.in\" \\ -and -not -name \"config.h.in\" 2>\/dev\/null) {proj}_files:=$(shell find -L{src_proj} \\ \\( \\ -name \"*.[hc]\" \\ -or -name \"*.py\" \\ -or -name \"*.cpp\" \\ -or -name \"*.tpl\" \\ -or -name \"*.map\" \\ \\) \\ -and -not -name \"config.h\" 2>\/dev\/null) {clone_rule} .make.{proj}.autoconf:.make.{proj}.clone{src_proj}\/configure.ac \tif{distclean_cond}; then $(MAKE){proj}-distclean; fi \t@echo -e \"\\n\\n\\n=====$@\\n\" \t-rm -f{src_proj}\/.version \tcd{src_proj}; autoreconf -fi \tsync \ttouch $@ \t .make.{proj}.configure:.make.{proj}.autoconf{deps_installed} $({proj}_configure_files) \tif{distclean_cond}; then $(MAKE){proj}-distclean.make.{proj}.autoconf; fi \t@echo -e \"\\n\\n\\n=====$@\\n\" \t-chmod -R ug+w{build_proj} \t-rm -rf{build_proj} \tmkdir -p{build_proj} \tcd{build_proj};{cflags}{docker_cmd}{build_to_src}\/configure{configure_opts} \tsync \ttouch $@ .make.{proj}.build:.make.{proj}.configure $({proj}_files) \tif{distclean_cond}; then $(MAKE){proj}-distclean.make.{proj}.configure; fi \t@echo -e \"\\n\\n\\n=====$@\\n\" \t{docker_cmd}$(MAKE) -C{build_proj} -j{jobs}{check} \tsync \ttouch $@ .make.{proj}.install:.make.{proj}.build \t@echo -e \"\\n\\n\\n=====$@\\n\" \t{docker_cmd}{sudo_make_install}$(MAKE) -C{build_proj} install \t{no_ldconfig}{sudo_ldconfig}ldconfig \tsync \ttouch $@ .PHONY:{proj} {proj}:.make.{proj}.install .PHONY:{proj}-reinstall {proj}-reinstall:{deps_reinstall} \t{sudo_make_install}$(MAKE) -C{build_proj} install .PHONY:{proj}-clean {proj}-clean: \t@echo -e \"\\n\\n\\n=====$@\\n\" \t-chmod -R ug+w{build_proj} \t-rm -rf{build_proj} \t-rm -rf.make.{proj}.* .PHONY:{proj}-distclean {proj}-distclean:{proj}-clean \t@echo -e \"\\n\\n\\n=====$@\\n\" \t$(MAKE) -C{src_proj} distclean '''.format( url=url, push_url=push_url, proj=proj, jobs=jobs, src=src, src_proj=src_proj, build_proj=make_to_build_proj, build_to_src=build_to_src, clone_rule=gen_makefile_clone(proj, src, src_proj, url, push_url), deps_installed=' '.join(['.make.%s.install' % d for d in deps]), deps_reinstall=' '.join(['%s-reinstall' %d for d in deps]), configure_opts=configure_opts_str, sudo_make_install='sudo ' if sudo_make_install else '', no_ldconfig=' sudo_ldconfig='' if ldconfig_without_sudo else 'sudo ', check='check' if make_check else '', docker_cmd=f'{args.docker_cmd} ' if args.docker_cmd else '', cflags='CFLAGS=-g ' if args.build_debug else '', distclean_cond=f'[ -e{src_proj}\/config.status]' if args.auto_distclean else 'false' ) projects_deps=read_projects_deps(all_deps_file) configure_opts=listdict() configure_opts_files=sorted(args.configure_opts_files or[]) for configure_opts_file in configure_opts_files: if configure_opts_file.endswith(\".deps\"): print(f\"WARNING: using{all_deps_file} instead of{configure_opts_file}\") continue r=read_configure_opts(configure_opts_file) configure_opts.extend_dict(read_configure_opts(configure_opts_file)) make_dir=args.make_dir if not make_dir: opts_names='+'.join([f.replace('.opts', '') for f in configure_opts_files]) make_dir='make-%s' % opts_names if not os.path.isdir(make_dir): os.makedirs(make_dir) build_dir=args.build_dir if not build_dir: build_dir=make_dir output=os.path.join(make_dir, args.output) print('Writing to %r' % output) with open(output, 'w') as out: out.write(' configure_opts_args=\"\" for f in configure_opts_files: if not f.endswith(\".deps\"): configure_opts_args +=f' \\\\\\n\\t\\t{os.path.relpath(f, make_dir)}' out.write(r''' default: usrp .PHONY: cn cn: \\ \tosmo-ggsn \\ \tosmo-hlr \\ \tosmo-iuh \\ \tosmo-mgw \\ \tosmo-msc \\ \tosmo-sgsn \\ \tosmo-sip-connector \\ \tosmo-smlc \\ \t$(NULL) .PHONY: cn-bsc cn-bsc: \\ \tcn \\ \tosmo-bsc \\ \t$(NULL) .PHONY: cn-bsc-nat cn-bsc-nat: \\ cn \\ mobile \\ osmo-bsc \\ osmo-bsc-nat \\ osmo-bts \\ virtphy \\ $(NULL) .PHONY: usrp usrp: \\ \tcn-bsc \\ \tosmo-bts \\ \tosmo-trx \\ \t$(NULL) .PHONY: mobile mobile: osmocom-bb_layer23 .PHONY: virtphy virtphy: osmocom-bb_virtphy .PHONY: all_debug all_debug: \t$(MAKE) --dry-run -d all | grep \"is newer than target\" \t$(MAKE) all .PHONY: regen regen: \t{script} \\ \t\t{configure_opts} \\ \t\t-m{make_dir} \\ \t\t-o{makefile} \\ \t\t-s{src_dir} \\ \t\t-b{build_dir} \\ \t\t-u \"{url}\"{push_url}{sudo_make_install}{no_ldconfig}{ldconfig_without_sudo}{make_check}{docker_cmd}{build_debug}{auto_distclean} '''.format( script=os.path.relpath(sys.argv[0], make_dir), configure_opts=configure_opts_args, make_dir='.', makefile=args.output, src_dir=os.path.relpath(args.src_dir, make_dir), build_dir=os.path.relpath(build_dir, make_dir), url=args.url, push_url=(\" \\\\\\n\\t\\t-p '%s'\"%args.push_url) if args.push_url else '', sudo_make_install=' \\\\\\n\\t\\t-I' if args.sudo_make_install else '', no_ldconfig=' \\\\\\n\\t\\t-L' if args.no_ldconfig else '', ldconfig_without_sudo=' \\\\\\n\\t\\t--ldconfig-without-sudo' if args.ldconfig_without_sudo else '', make_check='' if args.make_check else \" \\\\\\n\\t\\t--no-make-check\", docker_cmd=f' \\\\\\n\\t\\t--docker-cmd \"{args.docker_cmd}\"' if args.docker_cmd else '', build_debug=f' \\\\\\n\\t\\t--build-debug' if args.build_debug else '', auto_distclean=' \\\\\\n\\t\\t--auto-distclean' if args.auto_distclean else '', )) out.write('clone: \\\\\\n\\t' +' \\\\\\n\\t'.join([ '.make.%s.clone' % p for p, d in projects_deps]) +'\\n\\n') out.write('clean: \\\\\\n\\t' +' \\\\\\n\\t'.join([ '%s-clean' % p for p, d in projects_deps]) +'\\n\\n') out.write('all: clone all-install\\n\\n') out.write('all-install: \\\\\\n\\t' +' \\\\\\n\\t'.join([ '.make.%s.install' % p for p, d in projects_deps]) +'\\n\\n') for proj, deps in projects_deps: all_config_opts=[] all_config_opts.extend(configure_opts.get('ALL') or[]) all_config_opts.extend(configure_opts.get(proj) or[]) out.write(gen_make(proj, deps, all_config_opts, args.jobs, make_dir, args.src_dir, build_dir, args.url, args.push_url, args.sudo_make_install, args.no_ldconfig, args.ldconfig_without_sudo, args.make_check)) ","sourceWithComments":"#!\/usr\/bin\/env python3\n'''\nGenerate a top-level makefile that builds the Osmocom 2G + 3G network components.\n\n  .\/gen_makefile.py [configure.opts [more.opts]] [-o Makefile.output]\n\nConfigured by text files:\n\n  all.deps: whitespace-separated listing of\n    project_name depends_on_project_1 depends_on_project_2 ...\n\n  *.opts: whitespace-separated listing of\n    project_name --config-opt-1 --config-opt-2 ...\n\nThus it is possible to choose between e.g.\n- building each of those with or without mgcp transcoding support by adding or\n  removing \"transcoding.opts\" from the command line\n\nFrom the Makefile nature, the dependencies extend, no need to repeat common deps.\n\nWhen this script is done, a Makefile has been generated that allows you to\nbuild all projects at once by issuing 'make', but also to refresh only parts of\nit when some bits in the middle have changed. The makefile keeps local progress\nmarker files like .make.libosmocore.configure; if such progress marker is\nremoved or becomes outdated, that step and all dependent ones are re-run.\nThis is helpful in daily hacking across several repositories.\n\nNote that by default, this includes 'sudo ldconfig' calls following each\ninstallation. You may want to permit your user to run 'sudo ldconfig' without\nneeding a password, e.g. by\n\n  sudo sh -c \"echo '$USER  ALL= NOPASSWD: \/sbin\/ldconfig' > \/etc\/sudoers.d\/${USER}_ldconfig\"\n\nYou can skip the 'sudo ldconfig' by issuing the --no-ldconfig option.\n\nYou can run 'ldconfig' without sudo by issuing the --ldconfig-without-sudo option.\n\nBy default, it is assumed that your user has write permission to \/usr\/local. If you\nneed sudo to install there, you may issue the --sudo-make-install option.\n\nEXAMPLE:\n\n  .\/gen_makefile.py default.opts iu.opts -I -m build\n  cd build\n  make\n\n'''\n\nimport sys\nimport os\nimport argparse\n\ntopdir = os.path.dirname(os.path.realpath(__file__))\nall_deps_file = os.path.join(topdir, \"all.deps\")\nparser = argparse.ArgumentParser(epilog=__doc__, formatter_class=argparse.RawTextHelpFormatter)\n\nparser.add_argument('configure_opts_files',\n  help='''Config file containing project name and\n.\/configure options''',\n  nargs='*')\n\nparser.add_argument('-m', '--make-dir', dest='make_dir',\n  help='''Place Makefile in this dir (default: create\na new dir named after opts files).''')\n\nparser.add_argument('-s', '--src-dir', dest='src_dir', default='.\/src',\n  help='Parent dir for all git clones.')\n\nparser.add_argument('-b', '--build-dir', dest='build_dir',\n  help='''Parent dir for all build trees (default:\ndirectly in the make-dir).''')\n\nparser.add_argument('-u', '--url', dest='url', default='git:\/\/git.osmocom.org',\n  help='''git clone base URL. Default is 'git:\/\/git.osmocom.org'.\ne.g. with a config like this in your ~\/.ssh\/config:\n  host go\n  hostname gerrit.osmocom.org\n  port 29418\nyou may pass '-u ssh:\/\/go' to be able to submit to gerrit.''')\n\nparser.add_argument('-p', '--push-url', dest='push_url', default='',\n  help='''git push-URL. Default is to not configure a separate push-URL.''')\n\nparser.add_argument('-o', '--output', dest='output', default='Makefile',\n  help='''Makefile filename (default: 'Makefile').''')\n\nparser.add_argument('-j', '--jobs', dest='jobs', default='$(nproc)',\n  help='''-j option to pass to 'make'.''')\n\nparser.add_argument('-I', '--sudo-make-install', dest='sudo_make_install',\n  action='store_true',\n  help='''run 'make install' step with 'sudo'.''')\n\nparser.add_argument('-L', '--no-ldconfig', dest='no_ldconfig',\n  action='store_true',\n  help='''omit the 'sudo ldconfig' step.''')\n\nparser.add_argument('--ldconfig-without-sudo', dest='ldconfig_without_sudo',\n  action='store_true',\n  help='''call just 'ldconfig', without sudo, which implies\nroot privileges (not recommended)''')\n\nparser.add_argument('-c', '--no-make-check', dest='make_check',\n  default=True, action='store_false',\n  help='''do not 'make check', just 'make' to build.''')\n\nparser.add_argument('--docker-cmd',\n    help='''prefix configure\/make\/make install calls with this command (used by ttcn3.sh)''')\n\nparser.add_argument('-g', '--build-debug', dest='build_debug', default=False, action='store_true',\n    help='''set 'CFLAGS=-g' when calling src\/configure''')\n\nparser.add_argument('-a', '--auto-distclean', action='store_true',\n    help='''run \"make distclean\" automatically if source directory already configured''')\n\nargs = parser.parse_args()\n\nclass listdict(dict):\n  'a dict of lists { \"a\": [1, 2, 3],  \"b\": [1, 2] }'\n\n  def add(self, name, item):\n    l = self.get(name)\n    if not l:\n      l = []\n      self[name] = l\n    l.append(item)\n\n  def extend(self, name, l):\n    for v in l:\n      self.add(name, v)\n\n  def add_dict(self, d):\n    for k,v in d.items():\n      self.add(k, v)\n\n  def extend_dict(self, d):\n    for k,v in d.items():\n      l = self.extend(k, v)\n\ndef read_projects_deps(path):\n  'Read deps config and return tuples of (project_name, which-other-to-build-first).'\n  l = []\n  for line in open(path):\n    line = line.strip()\n    if not line or line.startswith('#'):\n      continue\n    tokens = line.split()\n    l.append((tokens[0], tokens[1:]))\n  return l\n\ndef read_configure_opts(path):\n  'Read config opts file and return tuples of (project_name, config-opts).'\n  if not path:\n    return {}\n  return dict(read_projects_deps(path))\n\ndef gen_makefile_clone(proj, src, src_proj, url, push_url):\n  if proj == \"osmocom-bb_layer23\":\n    return f'''\n.make.{proj}.clone: .make.osmocom-bb.clone\n\t@echo -e \"\\\\n\\\\n\\\\n===== $@\\\\n\"\n\ttest -L {src_proj} || ln -s osmocom-bb\/src\/host\/layer23 {src_proj}\n\ttouch $@\n  '''\n\n  if proj == \"osmocom-bb_virtphy\":\n    return f'''\n.make.{proj}.clone: .make.osmocom-bb.clone\n\t@echo -e \"\\\\n\\\\n\\\\n===== $@\\\\n\"\n\ttest -L {src_proj} || ln -s osmocom-bb\/src\/host\/virt_phy {src_proj}\n\ttouch $@\n  '''\n\n  if proj in (\"libgtpnl\", \"libnftnl\", \"nftables\"):\n    url = \"git:\/\/git.netfilter.org\"\n\n  return f'''\n.make.{proj}.clone:\n\t@echo -e \"\\\\n\\\\n\\\\n===== $@\\\\n\"\n\ttest -d {src} || mkdir -p {src}\n\ttest -d {src_proj} || ( git -C {src} clone \"{url}\/{proj}\" \"{proj}\" && git -C \"{src}\/{proj}\" remote set-url --push origin \"{push_url}\/{proj}\" )\n\tsync\n\ttouch $@\n  '''\n\ndef gen_make(proj, deps, configure_opts, jobs, make_dir, src_dir, build_dir, url, push_url, sudo_make_install, no_ldconfig, ldconfig_without_sudo, make_check):\n  src_proj = os.path.join(src_dir, proj)\n  if proj == 'openbsc':\n    src_proj = os.path.join(src_proj, 'openbsc')\n  build_proj = os.path.join(build_dir, proj)\n\n  make_to_build_proj = os.path.relpath(build_proj, make_dir)\n  build_to_src = os.path.relpath(src_proj, build_proj)\n  src = os.path.relpath(src_dir, make_dir)\n  src_proj = os.path.relpath(src_proj, make_dir)\n  push_url = push_url or url\n\n  if configure_opts:\n    configure_opts_str = ' '.join(configure_opts)\n  else:\n    configure_opts_str = ''\n\n  return r'''\n### {proj} ###\n\n{proj}_configure_files := $(shell find -L {src_proj} \\\n    -name \"Makefile.am\" \\\n    -or -name \"*.in\" \\\n    -and -not -name \"Makefile.in\" \\\n    -and -not -name \"config.h.in\" 2>\/dev\/null)\n{proj}_files := $(shell find -L {src_proj} \\\n    \\( \\\n      -name \"*.[hc]\" \\\n      -or -name \"*.py\" \\\n      -or -name \"*.cpp\" \\\n      -or -name \"*.tpl\" \\\n      -or -name \"*.map\" \\\n    \\) \\\n    -and -not -name \"config.h\" 2>\/dev\/null)\n\n{clone_rule}\n\n.make.{proj}.autoconf: .make.{proj}.clone {src_proj}\/configure.ac\n\tif {distclean_cond}; then $(MAKE) {proj}-distclean; fi\n\t@echo -e \"\\n\\n\\n===== $@\\n\"\n\t-rm -f {src_proj}\/.version\n\tcd {src_proj}; autoreconf -fi\n\tsync\n\ttouch $@\n\t\n.make.{proj}.configure: .make.{proj}.autoconf {deps_installed} $({proj}_configure_files)\n\tif {distclean_cond}; then $(MAKE) {proj}-distclean .make.{proj}.autoconf; fi\n\t@echo -e \"\\n\\n\\n===== $@\\n\"\n\t-chmod -R ug+w {build_proj}\n\t-rm -rf {build_proj}\n\tmkdir -p {build_proj}\n\tcd {build_proj}; {cflags}{docker_cmd}{build_to_src}\/configure {configure_opts}\n\tsync\n\ttouch $@\n\n.make.{proj}.build: .make.{proj}.configure $({proj}_files)\n\tif {distclean_cond}; then $(MAKE) {proj}-distclean .make.{proj}.configure; fi\n\t@echo -e \"\\n\\n\\n===== $@\\n\"\n\t{docker_cmd}$(MAKE) -C {build_proj} -j {jobs} {check}\n\tsync\n\ttouch $@\n\n.make.{proj}.install: .make.{proj}.build\n\t@echo -e \"\\n\\n\\n===== $@\\n\"\n\t{docker_cmd}{sudo_make_install}$(MAKE) -C {build_proj} install\n\t{no_ldconfig}{sudo_ldconfig}ldconfig\n\tsync\n\ttouch $@\n\n.PHONY: {proj}\n{proj}: .make.{proj}.install\n\n.PHONY: {proj}-reinstall\n{proj}-reinstall: {deps_reinstall}\n\t{sudo_make_install}$(MAKE) -C {build_proj} install\n\n.PHONY: {proj}-clean\n{proj}-clean:\n\t@echo -e \"\\n\\n\\n===== $@\\n\"\n\t-chmod -R ug+w {build_proj}\n\t-rm -rf {build_proj}\n\t-rm -rf .make.{proj}.*\n\n.PHONY: {proj}-distclean\n{proj}-distclean: {proj}-clean\n\t@echo -e \"\\n\\n\\n===== $@\\n\"\n\t$(MAKE) -C {src_proj} distclean\n\n'''.format(\n    url=url,\n    push_url=push_url,\n    proj=proj,\n    jobs=jobs,\n    src=src,\n    src_proj=src_proj,\n    build_proj=make_to_build_proj,\n    build_to_src=build_to_src,\n    clone_rule=gen_makefile_clone(proj, src, src_proj, url, push_url),\n    deps_installed=' '.join(['.make.%s.install' % d for d in deps]),\n    deps_reinstall=' '.join(['%s-reinstall' %d for d in deps]),\n    configure_opts=configure_opts_str,\n    sudo_make_install='sudo ' if sudo_make_install else '',\n    no_ldconfig='#' if no_ldconfig else '',\n    sudo_ldconfig='' if ldconfig_without_sudo else 'sudo ',\n    check='check' if make_check else '',\n    docker_cmd=f'{args.docker_cmd} ' if args.docker_cmd else '',\n    cflags='CFLAGS=-g ' if args.build_debug else '',\n    distclean_cond=f'[ -e {src_proj}\/config.status ]' if args.auto_distclean else 'false'\n    )\n\n\nprojects_deps = read_projects_deps(all_deps_file)\nconfigure_opts = listdict()\nconfigure_opts_files = sorted(args.configure_opts_files or [])\nfor configure_opts_file in configure_opts_files:\n  if configure_opts_file.endswith(\".deps\"):\n    print(f\"WARNING: using {all_deps_file} instead of {configure_opts_file}\")\n    continue\n  r = read_configure_opts(configure_opts_file)\n  configure_opts.extend_dict(read_configure_opts(configure_opts_file))\n\nmake_dir = args.make_dir\nif not make_dir:\n  opts_names = '+'.join([f.replace('.opts', '') for f in configure_opts_files])\n  make_dir = 'make-%s' % opts_names\n\nif not os.path.isdir(make_dir):\n  os.makedirs(make_dir)\n\nbuild_dir = args.build_dir\nif not build_dir:\n  build_dir = make_dir\n\noutput = os.path.join(make_dir, args.output)\nprint('Writing to %r' % output)\n\nwith open(output, 'w') as out:\n  out.write('# This Makefile was generated by %s\\n' % os.path.basename(sys.argv[0]))\n\n  configure_opts_args = \"\"\n  for f in configure_opts_files:\n    if not f.endswith(\".deps\"):\n      configure_opts_args += f' \\\\\\n\\t\\t{os.path.relpath(f, make_dir)}'\n\n  # convenience: add a regen target that updates the generated makefile itself\n  out.write(r'''\ndefault: usrp\n\n#\n# Convenience targets for whole networks\n#\n.PHONY: cn\ncn: \\\n\tosmo-ggsn \\\n\tosmo-hlr \\\n\tosmo-iuh \\\n\tosmo-mgw \\\n\tosmo-msc \\\n\tosmo-sgsn \\\n\tosmo-sip-connector \\\n\tosmo-smlc \\\n\t$(NULL)\n\n.PHONY: cn-bsc\ncn-bsc: \\\n\tcn \\\n\tosmo-bsc \\\n\t$(NULL)\n\n.PHONY: cn-bsc-nat\ncn-bsc-nat: \\\n  cn \\\n  mobile \\\n  osmo-bsc \\\n  osmo-bsc-nat \\\n  osmo-bts \\\n  virtphy \\\n  $(NULL)\n\n.PHONY: usrp\nusrp: \\\n\tcn-bsc \\\n\tosmo-bts \\\n\tosmo-trx \\\n\t$(NULL)\n\n#\n# Convenience targets for components in subdirs of repositories\n#\n.PHONY: mobile\nmobile: osmocom-bb_layer23\n\n.PHONY: virtphy\nvirtphy: osmocom-bb_virtphy\n\n#\n# Other convenience targets\n#\n.PHONY: all_debug\nall_debug:\n\t$(MAKE) --dry-run -d all | grep \"is newer than target\"\n\t$(MAKE) all\n\n# regenerate this Makefile, in case the deps or opts changed\n.PHONY: regen\nregen:\n\t{script} \\\n\t\t{configure_opts} \\\n\t\t-m {make_dir} \\\n\t\t-o {makefile} \\\n\t\t-s {src_dir} \\\n\t\t-b {build_dir} \\\n\t\t-u \"{url}\"{push_url}{sudo_make_install}{no_ldconfig}{ldconfig_without_sudo}{make_check}{docker_cmd}{build_debug}{auto_distclean}\n\n'''.format(\n    script=os.path.relpath(sys.argv[0], make_dir),\n    configure_opts=configure_opts_args,\n    make_dir='.',\n    makefile=args.output,\n    src_dir=os.path.relpath(args.src_dir, make_dir),\n    build_dir=os.path.relpath(build_dir, make_dir),\n    url=args.url,\n    push_url=(\" \\\\\\n\\t\\t-p '%s'\"%args.push_url) if args.push_url else '',\n    sudo_make_install=' \\\\\\n\\t\\t-I' if args.sudo_make_install else '',\n    no_ldconfig=' \\\\\\n\\t\\t-L' if args.no_ldconfig else '',\n    ldconfig_without_sudo=' \\\\\\n\\t\\t--ldconfig-without-sudo' if args.ldconfig_without_sudo else '',\n    make_check='' if args.make_check else \" \\\\\\n\\t\\t--no-make-check\",\n    docker_cmd=f' \\\\\\n\\t\\t--docker-cmd \"{args.docker_cmd}\"' if args.docker_cmd else '',\n    build_debug=f' \\\\\\n\\t\\t--build-debug' if args.build_debug else '',\n    auto_distclean=' \\\\\\n\\t\\t--auto-distclean' if args.auto_distclean else '',\n    ))\n\n  # convenience target: clone all repositories first\n  out.write('clone: \\\\\\n\\t' + ' \\\\\\n\\t'.join([ '.make.%s.clone' % p for p, d in projects_deps ]) + '\\n\\n')\n\n  # convenience target: clean all\n  out.write('clean: \\\\\\n\\t' + ' \\\\\\n\\t'.join([ '%s-clean' % p for p, d in projects_deps ]) + '\\n\\n')\n\n  # now the actual useful build rules\n  out.write('all: clone all-install\\n\\n')\n\n  out.write('all-install: \\\\\\n\\t' + ' \\\\\\n\\t'.join([ '.make.%s.install' % p for p, d in projects_deps ]) + '\\n\\n')\n\n  for proj, deps in projects_deps:\n    all_config_opts = []\n    all_config_opts.extend(configure_opts.get('ALL') or [])\n    all_config_opts.extend(configure_opts.get(proj) or [])\n    out.write(gen_make(proj, deps, all_config_opts, args.jobs,\n                       make_dir, args.src_dir, build_dir, args.url, args.push_url,\n                       args.sudo_make_install, args.no_ldconfig,\n                       args.ldconfig_without_sudo, args.make_check))\n\n# vim: expandtab tabstop=2 shiftwidth=2\n"}},"msg":"gen_makefile.py: use s#git:\/\/#https:\/\/# in git clone base URL\n\nUnencrypted git:\/\/ protocol offers no integrity or authentication,\nmaking it subject to tampering.  Use https:\/\/ instead.\n\nhttps:\/\/github.blog\/2021-09-01-improving-git-protocol-security-github\/#no-more-unauthenticated-git\nhttps:\/\/blog.readthedocs.com\/github-git-protocol-deprecation\/\n\nUsing https:\/\/ is also required for some new repositories (like\nlibosmo-gprs.git), which are intentionally unavailable over git:\/\/.\n\nChange-Id: I503f0e1b5258102d771597f1b5b753f670832fcb"}},"https:\/\/github.com\/diyaschool\/dal_assessments":{"3bbec6e38ab409317627e3da7a3b3218af4098a1":{"url":"https:\/\/api.github.com\/repos\/diyaschool\/dal_assessments\/commits\/3bbec6e38ab409317627e3da7a3b3218af4098a1","html_url":"https:\/\/github.com\/diyaschool\/dal_assessments\/commit\/3bbec6e38ab409317627e3da7a3b3218af4098a1","message":"changed format tampering","sha":"3bbec6e38ab409317627e3da7a3b3218af4098a1","keyword":"tampering change","diff":"diff --git a\/sheets_api.py b\/sheets_api.py\nindex 04ce460..2f5e18b 100644\n--- a\/sheets_api.py\n+++ b\/sheets_api.py\n@@ -51,7 +51,7 @@ def create_sheet(title, credentials):\n def tamper_with_format(sheet_id, credentials):\n     service = build('sheets', 'v4', credentials=credentials)\n     sheet = service.spreadsheets()\n-    tamper_data = ['Name, Subject', 'Student Tags', 'Easy Questions', 'Options', 'Correct Option', 'Image', 'Medium Questions', 'Options', 'Correct Option', 'Image', 'Hard Questions', 'Options', 'Correct Option', 'Image']\n+    tamper_data = ['Name, Subject, Total questions', 'Student Tags', 'Easy Questions', 'Options', 'Correct Option', 'Image', 'Medium Questions', 'Options', 'Correct Option', 'Image', 'Hard Questions', 'Options', 'Correct Option', 'Image']\n     req = sheet.values().update(spreadsheetId=sheet_id, range=\"A1:Z\", valueInputOption=\"USER_ENTERED\", body={\"range\": \"A1:Z\", \"majorDimension\": \"ROWS\", \"values\": [tamper_data]})\n     req.execute()\n \n","files":{"\/sheets_api.py":{"changes":[{"diff":"\n def tamper_with_format(sheet_id, credentials):\n     service = build('sheets', 'v4', credentials=credentials)\n     sheet = service.spreadsheets()\n-    tamper_data = ['Name, Subject', 'Student Tags', 'Easy Questions', 'Options', 'Correct Option', 'Image', 'Medium Questions', 'Options', 'Correct Option', 'Image', 'Hard Questions', 'Options', 'Correct Option', 'Image']\n+    tamper_data = ['Name, Subject, Total questions', 'Student Tags', 'Easy Questions', 'Options', 'Correct Option', 'Image', 'Medium Questions', 'Options', 'Correct Option', 'Image', 'Hard Questions', 'Options', 'Correct Option', 'Image']\n     req = sheet.values().update(spreadsheetId=sheet_id, range=\"A1:Z\", valueInputOption=\"USER_ENTERED\", body={\"range\": \"A1:Z\", \"majorDimension\": \"ROWS\", \"values\": [tamper_data]})\n     req.execute()\n \n","add":1,"remove":1,"filename":"\/sheets_api.py","badparts":["    tamper_data = ['Name, Subject', 'Student Tags', 'Easy Questions', 'Options', 'Correct Option', 'Image', 'Medium Questions', 'Options', 'Correct Option', 'Image', 'Hard Questions', 'Options', 'Correct Option', 'Image']"],"goodparts":["    tamper_data = ['Name, Subject, Total questions', 'Student Tags', 'Easy Questions', 'Options', 'Correct Option', 'Image', 'Medium Questions', 'Options', 'Correct Option', 'Image', 'Hard Questions', 'Options', 'Correct Option', 'Image']"]}],"source":"\nimport pickle from googleapiclient.discovery import build from google_auth_oauthlib.flow import Flow class authorize: def get_url(self): self.flow=Flow.from_client_secrets_file('..\/data\/credentials.json', scopes=['https:\/\/www.googleapis.com\/auth\/drive'], redirect_uri='urn:ietf:wg:oauth:2.0:oob') self.auth_url, _=self.flow.authorization_url(prompt='consent') return self.auth_url def verify_code(self, code): try: self.flow.fetch_token(code=code) creds=self.flow.credentials return creds except: return False def save_credentials(self, creds, file='..\/data\/credentials.pickle'): with open(file, 'wb') as token: pickle.dump(creds, token) return True def load_credentials(self, file='..\/data\/credentials.pickle'): try: with open(file, 'rb') as token: creds=pickle.load(token) return creds except FileNotFoundError: return None def verify_token(self, creds): try: service=build('sheets', 'v4', credentials=creds) sheet=service.spreadsheets() sheet.values().get(spreadsheetId='1BxiMVs0XRA5nFMdKvBdBZjgmUUqptlbs74OgvE2upms', range='A1:C').execute() return True except: return False def get_values(sheet_id, credentials, sheet_range='A1:Z'): service=build('sheets', 'v4', credentials=credentials) sheet=service.spreadsheets() result=sheet.values().get(spreadsheetId=sheet_id, range=sheet_range).execute() values=result.get('values',[]) return values def create_sheet(title, credentials): service=build('sheets', 'v4', credentials=credentials) spreadsheet={'properties':{'title': title}} result=service.spreadsheets().create(body=spreadsheet, fields='spreadsheetId').execute() tamper_with_format(result.get('spreadsheetId'), credentials) return result.get('spreadsheetId') def tamper_with_format(sheet_id, credentials): service=build('sheets', 'v4', credentials=credentials) sheet=service.spreadsheets() tamper_data=['Name, Subject', 'Student Tags', 'Easy Questions', 'Options', 'Correct Option', 'Image', 'Medium Questions', 'Options', 'Correct Option', 'Image', 'Hard Questions', 'Options', 'Correct Option', 'Image'] req=sheet.values().update(spreadsheetId=sheet_id, range=\"A1:Z\", valueInputOption=\"USER_ENTERED\", body={\"range\": \"A1:Z\", \"majorDimension\": \"ROWS\", \"values\":[tamper_data]}) req.execute() if __name__=='__main__': gauth=authorize() tamper_with_format('1FqZwkcNI325Tk4nwcbE2wEylHBX8vP9sFb8_F37Uxh4', gauth.load_credentials()) ","sourceWithComments":"import pickle\nfrom googleapiclient.discovery import build\nfrom google_auth_oauthlib.flow import Flow\n\nclass authorize:\n    def get_url(self):\n        self.flow = Flow.from_client_secrets_file('..\/data\/credentials.json', scopes=['https:\/\/www.googleapis.com\/auth\/drive'], redirect_uri='urn:ietf:wg:oauth:2.0:oob')\n        self.auth_url, _ = self.flow.authorization_url(prompt='consent')\n        return self.auth_url\n    def verify_code(self, code):\n        try:\n            self.flow.fetch_token(code=code)\n            creds = self.flow.credentials\n            return creds\n        except:\n            return False\n    def save_credentials(self, creds, file='..\/data\/credentials.pickle'):\n        with open(file, 'wb') as token:\n            pickle.dump(creds, token)\n        return True\n    def load_credentials(self, file='..\/data\/credentials.pickle'):\n        try:\n            with open(file, 'rb') as token:\n                creds = pickle.load(token)\n            return creds\n        except FileNotFoundError:\n            return None\n    def verify_token(self, creds):\n        try:\n            service = build('sheets', 'v4', credentials=creds)\n            sheet = service.spreadsheets()\n            sheet.values().get(spreadsheetId='1BxiMVs0XRA5nFMdKvBdBZjgmUUqptlbs74OgvE2upms', range='A1:C').execute()\n            return True\n        except:\n            return False\n\ndef get_values(sheet_id, credentials, sheet_range='A1:Z'):\n    service = build('sheets', 'v4', credentials=credentials)\n    sheet = service.spreadsheets()\n    result = sheet.values().get(spreadsheetId=sheet_id, range=sheet_range).execute()\n    values = result.get('values', [])\n    return values\n\ndef create_sheet(title, credentials):\n    service = build('sheets', 'v4', credentials=credentials)\n    spreadsheet = {'properties': {'title': title}}\n    result = service.spreadsheets().create(body=spreadsheet, fields='spreadsheetId').execute()\n    tamper_with_format(result.get('spreadsheetId'), credentials)\n    return result.get('spreadsheetId')\n\ndef tamper_with_format(sheet_id, credentials):\n    service = build('sheets', 'v4', credentials=credentials)\n    sheet = service.spreadsheets()\n    tamper_data = ['Name, Subject', 'Student Tags', 'Easy Questions', 'Options', 'Correct Option', 'Image', 'Medium Questions', 'Options', 'Correct Option', 'Image', 'Hard Questions', 'Options', 'Correct Option', 'Image']\n    req = sheet.values().update(spreadsheetId=sheet_id, range=\"A1:Z\", valueInputOption=\"USER_ENTERED\", body={\"range\": \"A1:Z\", \"majorDimension\": \"ROWS\", \"values\": [tamper_data]})\n    req.execute()\n\nif __name__=='__main__':\n    gauth = authorize()\n    tamper_with_format('1FqZwkcNI325Tk4nwcbE2wEylHBX8vP9sFb8_F37Uxh4', gauth.load_credentials())\n"}},"msg":"changed format tampering"}},"https:\/\/github.com\/apache\/steve":{"af9924686fc09b8ee2fb3e434522316fe923a24f":{"url":"https:\/\/api.github.com\/repos\/apache\/steve\/commits\/af9924686fc09b8ee2fb3e434522316fe923a24f","html_url":"https:\/\/github.com\/apache\/steve\/commit\/af9924686fc09b8ee2fb3e434522316fe923a24f","message":"Accept a salt for opened_key creation.\n\nLike the recent change for token creation, the opened_key sometimes\nneeds to be re-computed with the prior salt to get the same value, in\norder to detect tampering of the election data. Thus, take the salt as\na parameter, rather than internal creation and return.","sha":"af9924686fc09b8ee2fb3e434522316fe923a24f","keyword":"tampering change","diff":"diff --git a\/v3\/steve\/crypto.py b\/v3\/steve\/crypto.py\nindex be7d560..7a07e69 100644\n--- a\/v3\/steve\/crypto.py\n+++ b\/v3\/steve\/crypto.py\n@@ -35,14 +35,9 @@ def gen_salt() -> bytes:\n     return passlib.utils.getrandbytes(passlib.utils.rng, SALT_LEN)\n \n \n-### fix the types of the election metadata and issue data\n-### fix return type, to be a tuple\n-def gen_opened_key(election: bytes, issues: bytes) -> bytes:\n+def gen_opened_key(edata: bytes, salt: bytes) -> bytes:\n     \"Generate the OpenedKey for this election.\"\n-    salt = gen_salt()\n-    ### TBD: map ELECTION and ISSUES parameters to bytes\n-    opened_key = _hash(election + issues, salt)\n-    return salt, opened_key\n+    return _hash(edata, salt)\n \n \n def gen_token(opened_key: bytes, value: bytes, salt: bytes) -> bytes:\n","files":{"\/v3\/steve\/crypto.py":{"changes":[{"diff":"\n     return passlib.utils.getrandbytes(passlib.utils.rng, SALT_LEN)\n \n \n-### fix the types of the election metadata and issue data\n-### fix return type, to be a tuple\n-def gen_opened_key(election: bytes, issues: bytes) -> bytes:\n+def gen_opened_key(edata: bytes, salt: bytes) -> bytes:\n     \"Generate the OpenedKey for this election.\"\n-    salt = gen_salt()\n-    ### TBD: map ELECTION and ISSUES parameters to bytes\n-    opened_key = _hash(election + issues, salt)\n-    return salt, opened_key\n+    return _hash(edata, salt)\n \n \n def gen_token(opened_key: bytes, value: bytes, salt: bytes) -> bytes:\n","add":2,"remove":7,"filename":"\/v3\/steve\/crypto.py","badparts":["def gen_opened_key(election: bytes, issues: bytes) -> bytes:","    salt = gen_salt()","    opened_key = _hash(election + issues, salt)","    return salt, opened_key"],"goodparts":["def gen_opened_key(edata: bytes, salt: bytes) -> bytes:","    return _hash(edata, salt)"]}],"source":"\n import base64 import passlib.hash import passlib.utils import cryptography.fernet SALT_LEN=16 def gen_salt() -> bytes: \"Generate bytes to be used as a salt, for hashing.\" return passlib.utils.getrandbytes(passlib.utils.rng, SALT_LEN) def gen_opened_key(election: bytes, issues: bytes) -> bytes: \"Generate the OpenedKey for this election.\" salt=gen_salt() opened_key=_hash(election +issues, salt) return salt, opened_key def gen_token(opened_key: bytes, value: bytes, salt: bytes) -> bytes: \"Generate a voter or issue token.\" return _hash(opened_key +value, salt) def create_vote(voter_token: bytes, issue_token: bytes, votestring: bytes) -> bytes: \"Create a vote tuple, to record the VOTESTRING.\" salt=gen_salt() key=_hash(voter_token +issue_token, salt) b64key=base64.urlsafe_b64encode(key) f=cryptography.fernet.Fernet(b64key) return salt, f.encrypt(votestring) def decrypt_votestring(voter_token: bytes, issue_token: bytes, salt: bytes, token: bytes) -> bytes: \"Decrypt TOKEN into a VOTESTRING.\" key=_hash(voter_token +issue_token, salt) b64key=base64.urlsafe_b64encode(key) f=cryptography.fernet.Fernet(b64key) return f.decrypt(token) def _hash(data: bytes, salt: bytes) -> bytes: \"Apply our desired hashing function.\" ph=passlib.hash.argon2.using(type='d', salt=salt) h=ph.hash(data) return base64.standard_b64decode(h.split('$')[-1] +'==') ","sourceWithComments":"#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n# http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# ### TBD docco\n#\n#\n\nimport base64\n\nimport passlib.hash  # note that .argon2 is proxy in this pkg\nimport passlib.utils  # for the RNG, to create Salt values\n\nimport cryptography.fernet\n\n# All salt values will be 16 bytes in length. After base64 encoding, they\n# will be represented with 22 characters.\nSALT_LEN = 16\n\n\ndef gen_salt() -> bytes:\n    \"Generate bytes to be used as a salt, for hashing.\"\n    return passlib.utils.getrandbytes(passlib.utils.rng, SALT_LEN)\n\n\n### fix the types of the election metadata and issue data\n### fix return type, to be a tuple\ndef gen_opened_key(election: bytes, issues: bytes) -> bytes:\n    \"Generate the OpenedKey for this election.\"\n    salt = gen_salt()\n    ### TBD: map ELECTION and ISSUES parameters to bytes\n    opened_key = _hash(election + issues, salt)\n    return salt, opened_key\n\n\ndef gen_token(opened_key: bytes, value: bytes, salt: bytes) -> bytes:\n    \"Generate a voter or issue token.\"\n    return _hash(opened_key + value, salt)\n\n\n### fix return type, to be a tuple\ndef create_vote(voter_token: bytes,\n                issue_token: bytes,\n                votestring: bytes) -> bytes:\n    \"Create a vote tuple, to record the VOTESTRING.\"\n    salt = gen_salt()\n    key = _hash(voter_token + issue_token, salt)\n    b64key = base64.urlsafe_b64encode(key)\n    f = cryptography.fernet.Fernet(b64key)\n    return salt, f.encrypt(votestring)\n\n\ndef decrypt_votestring(voter_token: bytes,\n                       issue_token: bytes,\n                       salt: bytes,\n                       token: bytes) -> bytes:\n    \"Decrypt TOKEN into a VOTESTRING.\"\n    key = _hash(voter_token + issue_token, salt)\n    b64key = base64.urlsafe_b64encode(key)\n    f = cryptography.fernet.Fernet(b64key)\n    return f.decrypt(token)\n\n\ndef _hash(data: bytes, salt: bytes) -> bytes:\n    \"Apply our desired hashing function.\"\n    ph = passlib.hash.argon2.using(type='d', salt=salt)\n    h = ph.hash(data)\n    return base64.standard_b64decode(h.split('$')[-1] + '==')\n"}},"msg":"Accept a salt for opened_key creation.\n\nLike the recent change for token creation, the opened_key sometimes\nneeds to be re-computed with the prior salt to get the same value, in\norder to detect tampering of the election data. Thus, take the salt as\na parameter, rather than internal creation and return."}},"https:\/\/github.com\/audse\/pegboard-app":{"266ee2ff0be8fa25e0053f5202b9bfdf175e0fee":{"url":"https:\/\/api.github.com\/repos\/audse\/pegboard-app\/commits\/266ee2ff0be8fa25e0053f5202b9bfdf175e0fee","html_url":"https:\/\/github.com\/audse\/pegboard-app\/commit\/266ee2ff0be8fa25e0053f5202b9bfdf175e0fee","message":"<api> fix up api calls that were tampered with in structure changes","sha":"266ee2ff0be8fa25e0053f5202b9bfdf175e0fee","keyword":"tampering change","diff":"diff --git a\/backend\/pegboard\/models\/model_board.py b\/backend\/pegboard\/models\/model_board.py\nindex fb72ac3..c31f6bc 100644\n--- a\/backend\/pegboard\/models\/model_board.py\n+++ b\/backend\/pegboard\/models\/model_board.py\n@@ -91,12 +91,6 @@ class Board ( models.Model ):\n \n     order = models.IntegerField(default=0)\n \n-    color_palette = models.ManyToManyField(\n-        'Color',\n-        blank=True,\n-        related_name='color_boards'\n-    )\n-\n     default_note_display = models.CharField(max_length=3, choices=DISPLAY_CHOICES, default='n')\n \n     date_created = models.DateTimeField('date created', default=timezone.now)\ndiff --git a\/backend\/pegboard\/models\/model_color.py b\/backend\/pegboard\/models\/model_color.py\nindex 2f46a7e..0611717 100644\n--- a\/backend\/pegboard\/models\/model_color.py\n+++ b\/backend\/pegboard\/models\/model_color.py\n@@ -58,7 +58,7 @@ class Color ( models.Model ):\n     )\n \n     name = models.CharField(max_length=128)\n-    color = models.CharField(max_length=6)\n+    code = models.CharField(max_length=6)\n \n     def __str__ ( self ):\n         return self.name\ndiff --git a\/backend\/pegboard\/models\/model_note.py b\/backend\/pegboard\/models\/model_note.py\nindex e6f3c7d..68f6ce1 100644\n--- a\/backend\/pegboard\/models\/model_note.py\n+++ b\/backend\/pegboard\/models\/model_note.py\n@@ -43,7 +43,7 @@ class NoteManager ( models.Manager ):\n     use_in_migrations = True\n \n     def get_queryset(self):\n-        return NoteQuerySet(self.model, using=self._db).filter(date_archived__isnull=True)\n+        return NoteQuerySet(self.model, using=self._db).filter(date_archived__isnull=True).order_by('order').order_by('-date_updated')\n \n     def list(self, user):\n         return self.get_queryset().list(user)\ndiff --git a\/backend\/pegboard\/serializers\/serializer_board.py b\/backend\/pegboard\/serializers\/serializer_board.py\nindex 5be8444..bc1b26b 100644\n--- a\/backend\/pegboard\/serializers\/serializer_board.py\n+++ b\/backend\/pegboard\/serializers\/serializer_board.py\n@@ -7,12 +7,13 @@ class BoardSerializer ( serializers.ModelSerializer ):\n \n     model = Board\n \n-    folder = serializers.PrimaryKeyRelatedField(queryset=Folder.objects.all(), allow_null=True)\n+    folder = serializers.PrimaryKeyRelatedField(queryset=Folder.objects.all(), required=False, allow_null=True)\n     \n     pages = PageSerializer(many=True, required=False)\n     notes = NoteSerializer(many=True, required=False)\n \n     tags = TagSerializer(many=True, required=False)\n+    colors = ColorSerializer(many=True, required=False)\n     comments = CommentSerializer(many=True, required=False)\n     checklists = ChecklistSerializer(many=True, required=False)\n \ndiff --git a\/backend\/pegboard\/serializers\/serializer_color.py b\/backend\/pegboard\/serializers\/serializer_color.py\nindex fdccbe0..5ce95db 100644\n--- a\/backend\/pegboard\/serializers\/serializer_color.py\n+++ b\/backend\/pegboard\/serializers\/serializer_color.py\n@@ -1,9 +1,12 @@\n from rest_framework import serializers\n \n-from ..models import Color\n+from ..models import Color, Board\n \n class ColorSerializer ( serializers.ModelSerializer ):\n     model = Color\n+\n+    board = serializers.PrimaryKeyRelatedField(queryset=Board.objects.all(), required=False, allow_null=True)\n+\n     class Meta:\n         model = Color\n-        fields = ['id', 'name', 'color']\n+        fields = '__all__'\ndiff --git a\/backend\/pegboard\/serializers\/serializer_comment.py b\/backend\/pegboard\/serializers\/serializer_comment.py\nindex eb077f9..27ba521 100644\n--- a\/backend\/pegboard\/serializers\/serializer_comment.py\n+++ b\/backend\/pegboard\/serializers\/serializer_comment.py\n@@ -6,9 +6,9 @@ class CommentSerializer ( serializers.ModelSerializer ):\n \n     model = Comment\n \n-    board = serializers.PrimaryKeyRelatedField(queryset=Board.objects.all(), allow_null=True)\n-    page = serializers.PrimaryKeyRelatedField(queryset=Page.objects.all(), allow_null=True)\n-    note = serializers.PrimaryKeyRelatedField(queryset=Note.objects.all(), allow_null=True)\n+    board = serializers.PrimaryKeyRelatedField(queryset=Board.objects.all(), required=False, allow_null=True)\n+    page = serializers.PrimaryKeyRelatedField(queryset=Page.objects.all(), required=False, allow_null=True)\n+    note = serializers.PrimaryKeyRelatedField(queryset=Note.objects.all(), required=False, allow_null=True)\n \n     class Meta:\n         model = Comment\ndiff --git a\/backend\/pegboard\/serializers\/serializer_note.py b\/backend\/pegboard\/serializers\/serializer_note.py\nindex 68c8a80..e981f94 100644\n--- a\/backend\/pegboard\/serializers\/serializer_note.py\n+++ b\/backend\/pegboard\/serializers\/serializer_note.py\n@@ -6,8 +6,8 @@\n class NoteSerializer ( serializers.ModelSerializer ):\n     model = Note\n \n-    board = serializers.PrimaryKeyRelatedField(queryset=Board.objects.all(), allow_null=True)\n-    page = serializers.PrimaryKeyRelatedField(queryset=Page.objects.all(), allow_null=True)\n+    board = serializers.PrimaryKeyRelatedField(queryset=Board.objects.all(), required=False, allow_null=True)\n+    page = serializers.PrimaryKeyRelatedField(queryset=Page.objects.all(), required=False, allow_null=True)\n \n     tags = TagSerializer(many=True, required=False)\n     \ndiff --git a\/backend\/pegboard\/serializers\/serializer_page.py b\/backend\/pegboard\/serializers\/serializer_page.py\nindex 70c4e7a..7aed88d 100644\n--- a\/backend\/pegboard\/serializers\/serializer_page.py\n+++ b\/backend\/pegboard\/serializers\/serializer_page.py\n@@ -7,7 +7,7 @@ class PageSerializer ( serializers.ModelSerializer ):\n     \n     model = Page\n \n-    board = serializers.PrimaryKeyRelatedField(queryset=Board.objects.all(), allow_null=True)\n+    board = serializers.PrimaryKeyRelatedField(queryset=Board.objects.all(), required=False, allow_null=True)\n \n     notes = NoteSerializer(many=True, required=False, read_only=True)\n     \ndiff --git a\/backend\/pegboard\/serializers\/serializer_tag.py b\/backend\/pegboard\/serializers\/serializer_tag.py\nindex 811acc5..4d049a3 100644\n--- a\/backend\/pegboard\/serializers\/serializer_tag.py\n+++ b\/backend\/pegboard\/serializers\/serializer_tag.py\n@@ -1,18 +1,18 @@\n-\n from django.db.models.fields import IntegerField\n from rest_framework import serializers\n \n from . import *\n-from users.serializers import UserSerializer\n-from ..models import Tag\n+from ..models import Tag, Board, Color\n \n class TagSerializer ( serializers.ModelSerializer ):\n \n     model = Tag\n \n-    id = serializers.IntegerField()\n-\n+    id = serializers.IntegerField(required=False)\n+    board = serializers.PrimaryKeyRelatedField(queryset=Board.objects.all(), required=False, allow_null=True)\n+    color = serializers.PrimaryKeyRelatedField(queryset=Color.objects.all(), required=False, allow_null=True)\n+    \n     class Meta:\n         model = Tag\n-        fields = ['id', 'name', 'color']\n+        fields = '__all__'\n         depth = 1\ndiff --git a\/backend\/pegboard\/views\/utils.py b\/backend\/pegboard\/views\/utils.py\nindex 1ecbf56..8c556c4 100644\n--- a\/backend\/pegboard\/views\/utils.py\n+++ b\/backend\/pegboard\/views\/utils.py\n@@ -48,6 +48,7 @@ def serialize_and_create (serializer, request):\n             return Response(serialized_request.errors, status=400)\n \n     except Exception as e:\n+        print('Error creating object:', e)\n         return Response(str(e), status=500)\n \n def serialize_and_update(serializer, object_to_update, request):\ndiff --git a\/backend\/pegboard\/views\/view_note.py b\/backend\/pegboard\/views\/view_note.py\nindex a0af710..4670a1d 100644\n--- a\/backend\/pegboard\/views\/view_note.py\n+++ b\/backend\/pegboard\/views\/view_note.py\n@@ -17,6 +17,8 @@\n class NoteViewSet ( viewsets.ModelViewSet ):\n     authentication_classes = [TokenAuthentication]\n \n+    # TODO date_archived filtering should go here\n+    # rather than within models\n     queryset = Note.objects.all()\n     serializer_class = NoteSerializer\n \ndiff --git a\/frontend\/web-app\/src\/assets\/css\/theme.variables.scss b\/frontend\/web-app\/src\/assets\/css\/theme.variables.scss\nindex 6064704..cbbf2bb 100644\n--- a\/frontend\/web-app\/src\/assets\/css\/theme.variables.scss\n+++ b\/frontend\/web-app\/src\/assets\/css\/theme.variables.scss\n@@ -37,8 +37,9 @@\n     --warning: #d3a43f;\n     --error: #40396d;\n \n+    --scale-text-50: #3E4659;\n     --scale-text-100: #535f79;\n-    \n+\n     --scale-text-300: #6a7692;\n     --scale-text-300-opacity-50: #6a769223;\n     --scale-text-300-opacity-100: #6a76923d;\ndiff --git a\/frontend\/web-app\/src\/components\/Elements\/CoTag.vue b\/frontend\/web-app\/src\/components\/Elements\/CoTag.vue\nindex 2b8d40e..3495a91 100644\n--- a\/frontend\/web-app\/src\/components\/Elements\/CoTag.vue\n+++ b\/frontend\/web-app\/src\/components\/Elements\/CoTag.vue\n@@ -1,27 +1,60 @@\n <script lang=\"ts\" setup>\n \n-import { ref } from 'vue'\n+import { ref, computed } from 'vue'\n+import { useStore } from 'vuex'\n+\n+import { Tag } from '@\/types'\n \n const props = defineProps<{\n     varColor?:string,\n     color?:string,\n+    tag?:Tag,\n     label?:string,\n+    right?:string,\n     dense?:boolean,\n+    lg?:boolean,\n+    hover?:boolean,\n }>()\n \n+const store = useStore()\n+\n const changeOpacity = (color:string, opacity:number) => {\n     const newOpacity = Math.round(Math.min(Math.max(opacity || 1, 0), 1) * 255);\n     return color + newOpacity.toString(16).toUpperCase();\n }\n \n-const textColor = props.varColor ? ref(`var(--${props.varColor})`) : ref( `#${props.color}`)\n-const bgColor = props.varColor ? ref(`var(--${props.varColor}-opacity-100`) : ref( `#${changeOpacity(props.color, 0.25)}` )\n+const tagColor = computed( () => {\n+    if ( props.tag ) {\n+        return store.state.boards.current.colors.find( color => color.id === props.tag.color )\n+    } else return null\n+})\n+\n+const textColor = computed( () => {\n+    if ( props.tag ) {\n+        return `#${tagColor.value?.code}`\n+    } else if ( props.varColor ) {\n+        return `var(--${props.varColor})`\n+    } else if ( props.color ) {\n+        return `#${props.color}`\n+    } else return 'var(--text)'\n+})\n+\n+const bgColor = computed( () => {\n+    if ( props.tag ) {\n+        return `#${changeOpacity(tagColor.value?.code, 0.25)}`\n+    }if ( props.varColor ) {\n+        return `var(--${props.varColor}-opacity-100`\n+    } else if ( props.color ) { \n+        return `#${changeOpacity(props.color, 0.25)}`\n+    } else return 'transparent'\n+})\n \n <\/script>\n <template>\n-    \n-<span :class=\"['tag', dense?'dense':'']\">\n+\n+<span :class=\"['tag', dense?'tag-dense':'', lg?'tag-large':'', hover?'tag-hover':'']\">\n     {{ label }}\n+    <span v-if=\"right\" class=\"tag-right\">{{ right }}<\/span>\n     <slot \/>\n <\/span>\n \n@@ -36,15 +69,33 @@ export default {\n <style scoped>\n \n .tag {\n-    @apply my-1 font-semibold uppercase rounded-full tracking-widest;\n+    @apply my-1 font-semibold uppercase rounded-full tracking-widest mr-1;\n     background-color: v-bind('bgColor');\n     color: v-bind('textColor');\n-    font-size: 0.7em;\n+    font-size: 0.75em;\n     padding: 2pt 6pt;\n }\n \n-.dense {\n+.tag-right {\n+    @apply px-1;\n+    opacity: 0.5;\n+}\n+\n+.tag-dense {\n     padding: 0 4pt;\n }\n \n+.tag-large {\n+    font-size: 0.85em;\n+    padding: 3pt 8pt;\n+}\n+\n+.tag-hover {\n+    transition: transform 200ms;\n+}\n+\n+.tag-hover:hover {\n+    transform: scale(1.05, 1.05);\n+}\n+\n <\/style>\n\\ No newline at end of file\ndiff --git a\/frontend\/web-app\/src\/components\/Elements\/Expandable.vue b\/frontend\/web-app\/src\/components\/Elements\/Expandable.vue\nindex ab7f353..634cc1d 100644\n--- a\/frontend\/web-app\/src\/components\/Elements\/Expandable.vue\n+++ b\/frontend\/web-app\/src\/components\/Elements\/Expandable.vue\n@@ -3,7 +3,7 @@\n import { ref } from 'vue'\n \n const props = defineProps<{\n-    toShow:Boolean,\n+    toShow?:boolean,\n     label?:string\n }>()\n \ndiff --git a\/frontend\/web-app\/src\/components\/Pegboard\/Board\/Forms\/Edit.vue b\/frontend\/web-app\/src\/components\/Pegboard\/Board\/Forms\/Edit.vue\nindex aa9bd89..9d113e6 100644\n--- a\/frontend\/web-app\/src\/components\/Pegboard\/Board\/Forms\/Edit.vue\n+++ b\/frontend\/web-app\/src\/components\/Pegboard\/Board\/Forms\/Edit.vue\n@@ -58,7 +58,7 @@ const archiveBoard = async(boardId:number) => {\n <script lang=\"ts\">\n \n export default {\n-    name: 'EditBoard',\n+    name: 'edit-board',\n }\n \n <\/script>\n\\ No newline at end of file\ndiff --git a\/frontend\/web-app\/src\/components\/Pegboard\/Board\/Modal.vue b\/frontend\/web-app\/src\/components\/Pegboard\/Board\/Modal.vue\nindex f11d163..fcb1f4f 100644\n--- a\/frontend\/web-app\/src\/components\/Pegboard\/Board\/Modal.vue\n+++ b\/frontend\/web-app\/src\/components\/Pegboard\/Board\/Modal.vue\n@@ -1,7 +1,7 @@\n <script lang=\"ts\" setup>\n \n import { Board } from '@\/types'\n-import { AddTag } from '@\/components'\n+import { AddTag, AddColor } from '@\/components'\n \n const props = defineProps<{\n     board:Board,\n@@ -22,17 +22,32 @@ const tabsSections = (index:number) => `section-${tabs[index]}`\n <modal :tabs=\"tabs\" :show=\"show\" @hide=\"$emit('hide')\">\n \n     <template v-slot:[tabsSections(0)]>\n-\n+        \n     <\/template>\n \n     <template v-slot:[tabsSections(1)]>\n         <h3>Tags<\/h3>\n         <section class=\"pt-4\">\n-            <co-tag v-for=\"tag in board.tags\" :key=\"tag.id\" :label=\"tag.name\" :color=\"tag.color.color\" \/>\n+            <co-tag lg v-for=\"tag in board.tags\" :key=\"tag.id\" :label=\"tag.name\" :tag=\"tag\" class=\"m-1\" \/>\n+        <\/section>\n+\n+        <expandable class=\"mt-8\" label=\"Add Tag\">\n+            <card bg=\"scale-secondary-500\" class=\"mt-2\">\n+                <add-tag :board=\"board\" \/>\n+            <\/card>\n+        <\/expandable>\n+    <\/template>\n+\n+    <template v-slot:[tabsSections(2)]>\n+        <h3>Color Palette<\/h3>\n+        <section class=\"pt-4\">\n+            <co-tag lg v-for=\"color in board.colors\" :key=\"color.id\" :label=\"color.name\" :right=\"`#${color.code}`\" :color=\"color.code\" class=\"m-1\" \/>\n         <\/section>\n \n-        <expandable class=\"pt-8\" label=\"Add Tag\">\n-            <add-tag :board=\"board\" \/>\n+        <expandable class=\"mt-8\" label=\"Add Color\">\n+            <card bg=\"scale-secondary-500\" class=\"mt-2\">\n+                <add-color :board=\"board\" \/>\n+            <\/card>\n         <\/expandable>\n     <\/template>\n \ndiff --git a\/frontend\/web-app\/src\/components\/Pegboard\/Checklist\/Checklist.vue b\/frontend\/web-app\/src\/components\/Pegboard\/Checklist\/Checklist.vue\nindex 6474362..db1c81a 100644\n--- a\/frontend\/web-app\/src\/components\/Pegboard\/Checklist\/Checklist.vue\n+++ b\/frontend\/web-app\/src\/components\/Pegboard\/Checklist\/Checklist.vue\n@@ -34,7 +34,7 @@ const numComplete = computed( () => {\n         <toolbar wrap>\n             {{ checklist.name }}\n             <template #right>\n-                <co-tag var-color=\"scale-text-500\" class=\"mr-1 flex-none\">{{ numComplete }} \/ {{ checklist.items.length }}<\/co-tag>\n+                <co-tag var-color=\"alert\" class=\"mr-1 flex-none\">{{ numComplete }} \/ {{ checklist.items.length }}<\/co-tag>\n             <\/template>\n         <\/toolbar>\n     <\/li>\ndiff --git a\/frontend\/web-app\/src\/components\/Pegboard\/Checklist\/Forms\/Add.vue b\/frontend\/web-app\/src\/components\/Pegboard\/Checklist\/Forms\/Add.vue\nindex dd5c47d..9dc0312 100644\n--- a\/frontend\/web-app\/src\/components\/Pegboard\/Checklist\/Forms\/Add.vue\n+++ b\/frontend\/web-app\/src\/components\/Pegboard\/Checklist\/Forms\/Add.vue\n@@ -2,20 +2,20 @@\n \n import { ref, watch, Ref } from 'vue'\n \n-import { ChecklistForm } from '@\/types'\n+import { ChecklistForm, Board, Page, Note } from '@\/types'\n \n const props = defineProps<{\n-    boardId?:number,\n-    pageId?:number,\n-    noteId?:number,\n+    board?:Board,\n+    page?:Page,\n+    note?:Note,\n }>()\n \n const checklist:Ref<ChecklistForm> = ref({\n     name: 'Checklist',\n     items: [],\n-    board: props.boardId,\n-    page: props.pageId,\n-    note: props.noteId,\n+    board: props.board?.id,\n+    page: props.page?.id,\n+    note: props.note?.id,\n })\n \n const hasChecklist = ref(false)\ndiff --git a\/frontend\/web-app\/src\/components\/Pegboard\/Color\/Forms\/Add.vue b\/frontend\/web-app\/src\/components\/Pegboard\/Color\/Forms\/Add.vue\nindex c737c72..6cec6a9 100644\n--- a\/frontend\/web-app\/src\/components\/Pegboard\/Color\/Forms\/Add.vue\n+++ b\/frontend\/web-app\/src\/components\/Pegboard\/Color\/Forms\/Add.vue\n@@ -2,16 +2,17 @@\n \n import { reactive } from 'vue'\n \n+import { Board } from '@\/types'\n import { ColorService } from '@\/services'\n \n-const props = defineProps({\n-    boardId: Number,\n-})\n+const props = defineProps<{\n+    board:Board,\n+}>()\n \n const addColorForm = reactive({\n     name: '',\n-    color: '',\n-    board: props.boardId || undefined\n+    code: '',\n+    board: props.board.id || undefined\n })\n \n const addColor = async (data:object) => {\n@@ -21,19 +22,23 @@ const addColor = async (data:object) => {\n <\/script>\n <template>\n \n-<form @submit.prevent=\"addColor(addColorForm)\" class=\"flex items-center\">\n-    <label for=\"name\" class=\"flex-none\">Color Name<\/label>\n-    <input v-model=\"addColorForm.name\" name=\"name\" type=\"text\" placeholder=\"Color Name\" \/>\n-    <label for=\"name\" class=\"flex-none\">Color Hex Code<\/label>\n-    <input v-model=\"addColorForm.color\" name=\"name\" type=\"text\" placeholder=\"Color Hex Code\" \/>\n-    <button type=\"submit\" class=\"flex-none\">Add Color<\/button>\n+<form @submit.prevent=\"addColor(addColorForm)\">\n+    <section class=\"flex items-center my-2\">\n+        <label for=\"name\" class=\"flex-none w-20\">Name<\/label>\n+        <input v-model=\"addColorForm.name\" name=\"name\" type=\"text\" class=\"bg-main\" \/>\n+    <\/section>\n+    <section class=\"flex items-center my-2 mb-6\">\n+        <label for=\"name\" class=\"flex-none w-20\">Hex Code<\/label>\n+        <co-tag label=\"#\" \/><input v-model=\"addColorForm.code\" name=\"name\" type=\"text\" placeholder=\"FEFEFE\" class=\"bg-main\" \/>\n+    <\/section>\n+    <co-button type=\"submit\" color=\"emphasis\" class=\"block my-2\">Add Color<\/co-button>\n <\/form>\n     \n <\/template>\n <script lang=\"ts\">\n \n export default {\n-    name: 'AddColor'\n+    name: 'add-color'\n }\n \n <\/script>\n\\ No newline at end of file\ndiff --git a\/frontend\/web-app\/src\/components\/Pegboard\/Color\/Forms\/Select.vue b\/frontend\/web-app\/src\/components\/Pegboard\/Color\/Forms\/Select.vue\nindex 42aa43f..220b469 100644\n--- a\/frontend\/web-app\/src\/components\/Pegboard\/Color\/Forms\/Select.vue\n+++ b\/frontend\/web-app\/src\/components\/Pegboard\/Color\/Forms\/Select.vue\n@@ -7,20 +7,20 @@ import { Color } from '@\/types'\n import { ColorService } from '@\/services'\n \n const props = defineProps<{\n-    modelValue:number,\n+    modelValue?:number,\n     colors:Array<Color>\n }>()\n \n <\/script>\n <template>\n \n-<co-tag\n+<co-tag hover\n v-for=\"color in colors\"\n :key=\"color.id\"\n @click=\"this.$emit('update:modelValue', color.id)\"\n :label=\"color.name\"\n-:color=\"color.color\"\n-:class=\"modelValue===color.id?'selected':''\"\n+:color=\"color.code\"\n+:class=\"['tag', modelValue===color.id?'selected':'', 'cursor-pointer m-1']\"\n \/>\n \n <\/template>\n@@ -34,8 +34,15 @@ export default {\n \n <style scoped>\n \n+.tag {\n+    @apply cursor-pointer;\n+    border: 2px solid transparent;\n+    transition: border 200ms;\n+}\n+\n .selected {\n-    border: 2px solid red;\n+    @apply backdrop-filter backdrop-invert-5;\n+    border-color: var(--alert);\n }\n \n <\/style>\n\\ No newline at end of file\ndiff --git a\/frontend\/web-app\/src\/components\/Pegboard\/Note\/Displays\/NoteChecklist.vue b\/frontend\/web-app\/src\/components\/Pegboard\/Note\/Displays\/NoteChecklist.vue\nindex 34dfb4c..34921f1 100644\n--- a\/frontend\/web-app\/src\/components\/Pegboard\/Note\/Displays\/NoteChecklist.vue\n+++ b\/frontend\/web-app\/src\/components\/Pegboard\/Note\/Displays\/NoteChecklist.vue\n@@ -26,7 +26,7 @@ const showModal = (event:any) => {\n <card bg=\"primary\" hover @click=\"showModal\">\n \n     <template #header>\n-        <co-tag v-for=\"tag in note.tags\" :key=\"tag.id\" :label=\"tag.name\" :color=\"tag.color.color\" \/>\n+        <co-tag v-for=\"tag in note.tags\" :key=\"tag.id\" :label=\"tag.name\" :tag=\"tag\" \/>\n         <strong class=\"block font-semibold mt-1\">{{ note.name }}<\/strong>\n     <\/template>\n \ndiff --git a\/frontend\/web-app\/src\/components\/Pegboard\/Note\/Displays\/NoteDefault.vue b\/frontend\/web-app\/src\/components\/Pegboard\/Note\/Displays\/NoteDefault.vue\nindex 8f24934..678fae4 100644\n--- a\/frontend\/web-app\/src\/components\/Pegboard\/Note\/Displays\/NoteDefault.vue\n+++ b\/frontend\/web-app\/src\/components\/Pegboard\/Note\/Displays\/NoteDefault.vue\n@@ -18,7 +18,7 @@ const truncatedContent = computed( () => {\n <card bg=\"primary\" hover>\n \n     <template #header>\n-        <co-tag v-for=\"tag in note.tags\" :key=\"tag.id\" :label=\"tag.name\" :color=\"tag.color.color\" \/>\n+        <co-tag v-for=\"tag in note.tags\" :key=\"tag.id\" :label=\"tag.name\" :tag=\"tag\" \/>\n         <strong class=\"block font-medium mt-1\">{{ note.name }}<\/strong>\n     <\/template>\n \ndiff --git a\/frontend\/web-app\/src\/components\/Pegboard\/Note\/Displays\/NoteDiscussion.vue b\/frontend\/web-app\/src\/components\/Pegboard\/Note\/Displays\/NoteDiscussion.vue\nindex 446bc7d..0978620 100644\n--- a\/frontend\/web-app\/src\/components\/Pegboard\/Note\/Displays\/NoteDiscussion.vue\n+++ b\/frontend\/web-app\/src\/components\/Pegboard\/Note\/Displays\/NoteDiscussion.vue\n@@ -26,7 +26,7 @@ const showModal = (event:any) => {\n \n     <template #header>\n         <toolbar>\n-            <co-tag v-for=\"tag in note.tags\" :key=\"tag.id\" :label=\"tag.name\" :color=\"tag.color.color\" \/>\n+            <co-tag v-for=\"tag in note.tags\" :key=\"tag.id\" :label=\"tag.name\" :tag=\"tag\" \/>\n             <strong class=\"block font-medium mt-1\">{{ note.name }}<\/strong>\n             <template #right>\n                 <co-tag v-if=\"note.comments.length>0\" :label=\"note.comments.length.toString()\" var-color=\"alert\" \/>\ndiff --git a\/frontend\/web-app\/src\/components\/Pegboard\/Note\/Displays\/NoteHeading.vue b\/frontend\/web-app\/src\/components\/Pegboard\/Note\/Displays\/NoteHeading.vue\nindex 0729bd9..90f0e38 100644\n--- a\/frontend\/web-app\/src\/components\/Pegboard\/Note\/Displays\/NoteHeading.vue\n+++ b\/frontend\/web-app\/src\/components\/Pegboard\/Note\/Displays\/NoteHeading.vue\n@@ -11,10 +11,10 @@ const props = defineProps<{\n <\/script>\n <template>\n \n-<card bg=\"primary\" hover>\n+<card bg=\"scale-text-50\" hover>\n \n     <header class=\"my-4\">\n-        <co-tag v-for=\"tag in note.tags\" :key=\"tag.id\" :label=\"tag.name\" :color=\"tag.color.color\" \/>\n+        <co-tag v-for=\"tag in note.tags\" :key=\"tag.id\" :label=\"tag.name\" :tag=\"tag\" class=\"mr-2\" \/>\n         <h3 class=\"font-semibold text-xl py-2 tracking-wide\">\n             {{ note.name }}\n         <\/h3>\ndiff --git a\/frontend\/web-app\/src\/components\/Pegboard\/Note\/Forms\/Add.vue b\/frontend\/web-app\/src\/components\/Pegboard\/Note\/Forms\/Add.vue\nindex 38e11b0..272e14a 100644\n--- a\/frontend\/web-app\/src\/components\/Pegboard\/Note\/Forms\/Add.vue\n+++ b\/frontend\/web-app\/src\/components\/Pegboard\/Note\/Forms\/Add.vue\n@@ -2,17 +2,20 @@\n \n import { reactive } from 'vue'\n \n+import { Board, Page } from '@\/types'\n import { NoteService } from '@\/services'\n \n-const props = defineProps({\n-    boardId: Number,\n-    pageId: Number,\n-})\n+const props = defineProps<{\n+    board?:Board,\n+    page?:Page,\n+    display?:string,\n+}>()\n \n const addNoteForm = reactive({\n     name: '',\n-    board: props.boardId || null,\n-    page: props.pageId || null,\n+    board: props.board?.id || null,\n+    page: props.page?.id || null,\n+    display: props.display || 'n'\n })\n \n const addNote = async (data:object) => {\ndiff --git a\/frontend\/web-app\/src\/components\/Pegboard\/Note\/Forms\/Edit.vue b\/frontend\/web-app\/src\/components\/Pegboard\/Note\/Forms\/Edit.vue\nindex 6d9e608..6559f03 100644\n--- a\/frontend\/web-app\/src\/components\/Pegboard\/Note\/Forms\/Edit.vue\n+++ b\/frontend\/web-app\/src\/components\/Pegboard\/Note\/Forms\/Edit.vue\n@@ -4,7 +4,7 @@ import { reactive, Ref, ref } from 'vue'\n \n import { SelectTags, AddChecklist } from '@\/components'\n import { NoteService, ChecklistService } from '@\/services'\n-import { Note } from '@\/types'\n+import { Note, Board } from '@\/types'\n \n const props = defineProps<{\n     note:Note,\n@@ -91,7 +91,7 @@ const archiveNote = async(noteId:number) => {\n \n         <section class=\"pt-16\">\n             {{ newChecklist }}\n-            <add-checklist :note-id=\"note.id\" @update-checklist=\"updateChecklist\" \/>\n+            <add-checklist :note=\"note\" @update-checklist=\"updateChecklist\" \/>\n         <\/section>\n \n         <section class=\"pt-8\">\n@@ -110,7 +110,7 @@ const archiveNote = async(noteId:number) => {\n <script lang=\"ts\">\n \n export default {\n-    name: 'EditNote',\n+    name: 'edit-note',\n }\n \n <\/script>\n\\ No newline at end of file\ndiff --git a\/frontend\/web-app\/src\/components\/Pegboard\/Page\/Forms\/Add.vue b\/frontend\/web-app\/src\/components\/Pegboard\/Page\/Forms\/Add.vue\nindex 56253b9..d996b50 100644\n--- a\/frontend\/web-app\/src\/components\/Pegboard\/Page\/Forms\/Add.vue\n+++ b\/frontend\/web-app\/src\/components\/Pegboard\/Page\/Forms\/Add.vue\n@@ -2,20 +2,21 @@\n \n import { reactive } from 'vue'\n \n+import { Board } from '@\/types'\n import { PageService } from '@\/services'\n \n-const props = defineProps({\n-    boardId: Number,\n-})\n+const props = defineProps<{\n+    board:Board,\n+}>()\n \n const addPageForm = reactive({\n     name: '',\n     description: '',\n-    board: props.boardId || null,\n+    board: props.board.id || null,\n })\n \n const addPage = async (data:{name:string,board:number|undefined}) => {\n-    data.board = props.boardId\n+    data.board = props.board.id\n     await PageService.create(data)\n     addPageForm.name = ''\n }\ndiff --git a\/frontend\/web-app\/src\/components\/Pegboard\/Page\/Page.vue b\/frontend\/web-app\/src\/components\/Pegboard\/Page\/Page.vue\nindex 36315cf..32ad495 100644\n--- a\/frontend\/web-app\/src\/components\/Pegboard\/Page\/Page.vue\n+++ b\/frontend\/web-app\/src\/components\/Pegboard\/Page\/Page.vue\n@@ -2,11 +2,13 @@\n \n import { ref } from 'vue'\n \n+import { Board, Page } from '@\/types'\n import { EditPage, Note, AddNote, Comments } from '@\/components'\n \n-const props = defineProps({\n-    page: Object,\n-})\n+const props = defineProps<{\n+    board?:Board,\n+    page:Page,\n+}>()\n \n const showEditModal = ref(false)\n \n@@ -16,7 +18,7 @@ const showEditModal = ref(false)\n <section>\n \n     <toolbar>\n-        <span class=\"pl-2\">\n+        <span class=\"pl-2 mb-2\">\n             <h3 class=\"text-base tracking-wide\">{{ page.name }}<\/h3>\n             <h4 class=\"font-normal text-base text-scale-text-500\">{{ page.description }}<\/h4>\n         <\/span>\n@@ -29,7 +31,7 @@ const showEditModal = ref(false)\n         <note :note=\"note\" \/>\n     <\/section>\n     \n-    <add-note :page-id=\"page.id\" class=\"pb-8\" \/>\n+    <add-note :page=\"page\" :display=\"board?.default_note_display\" class=\"pb-8\" \/>\n \n     <modal :show=\"showEditModal\" @hide=\"showEditModal=false\">\n         <edit-page :page=\"page\" @save=\"showEditModal=false\" \/>\ndiff --git a\/frontend\/web-app\/src\/components\/Pegboard\/Tag\/Forms\/Add.vue b\/frontend\/web-app\/src\/components\/Pegboard\/Tag\/Forms\/Add.vue\nindex deaed67..1b86020 100644\n--- a\/frontend\/web-app\/src\/components\/Pegboard\/Tag\/Forms\/Add.vue\n+++ b\/frontend\/web-app\/src\/components\/Pegboard\/Tag\/Forms\/Add.vue\n@@ -12,8 +12,8 @@ const props = defineProps<{\n \n const addTagForm = reactive({\n     name: '',\n-    color: undefined,\n-    board: props.board.id || undefined\n+    color: null,\n+    board: props.board.id\n })\n \n const addTag = async (data:object) => {\n@@ -26,15 +26,15 @@ const addTag = async (data:object) => {\n <form @submit.prevent=\"addTag(addTagForm)\">\n     <section class=\"flex items-center my-2\">\n         <label for=\"name\" class=\"flex-none w-20\">Tag Name<\/label>\n-        <input v-model=\"addTagForm.name\" name=\"name\" type=\"text\" placeholder=\"Tag Name\" \/>\n+        <input v-model=\"addTagForm.name\" name=\"name\" type=\"text\" class=\"bg-main\" \/>\n     <\/section>\n     \n-    <section class=\"flex items-center my-2\">\n+    <section class=\"flex items-center my-2 mb-6\">\n         <label for=\"name\" class=\"flex-none w-20\">Color<\/label>\n         <select-color v-model=\"addTagForm.color\" :colors=\"board.colors\" \/>\n     <\/section>\n     \n-    <button type=\"submit\" class=\"flex-none\">Add Tag<\/button>\n+    <co-button type=\"submit\" color=\"emphasis\" class=\"block my-2\">Add Tag<\/co-button>\n <\/form>\n     \n <\/template>\ndiff --git a\/frontend\/web-app\/src\/components\/Pegboard\/Tag\/Forms\/Select.vue b\/frontend\/web-app\/src\/components\/Pegboard\/Tag\/Forms\/Select.vue\nindex 34895bd..43ef76a 100644\n--- a\/frontend\/web-app\/src\/components\/Pegboard\/Tag\/Forms\/Select.vue\n+++ b\/frontend\/web-app\/src\/components\/Pegboard\/Tag\/Forms\/Select.vue\n@@ -42,10 +42,12 @@ const isTagInSelected = (tag:Tag, array:Array<Tag>) => {\n <template>\n \n <co-tag \n+    hover\n     v-for=\"tag in tags\"\n     :key=\"tag.id\" \n-    :label=\"tag.name\" :color=\"tag.color.color\" \n-    :class=\"isTagInSelected(tag, selectedTags)?'selected tag':'tag'\"\n+    :label=\"tag.name\"\n+    :tag=\"tag\"\n+    :class=\"[isTagInSelected(tag, selectedTags)?'selected tag':'tag', 'mr-1 mb-1']\"\n     @click=\"updateSelectedTags(tag, selectedTags)\" \n \/>\n \n@@ -62,10 +64,13 @@ export default {\n \n .tag {\n     @apply cursor-pointer;\n+    border: 2px solid transparent;\n+    transition: border 200ms;\n }\n \n .selected {\n-    border: 2px solid red;\n+    @apply backdrop-filter backdrop-invert-5;\n+    border-color: var(--alert);\n }\n \n <\/style>\n\\ No newline at end of file\ndiff --git a\/frontend\/web-app\/src\/pages\/Pegboard\/Board.vue b\/frontend\/web-app\/src\/pages\/Pegboard\/Board.vue\nindex c477535..b7b933f 100644\n--- a\/frontend\/web-app\/src\/pages\/Pegboard\/Board.vue\n+++ b\/frontend\/web-app\/src\/pages\/Pegboard\/Board.vue\n@@ -99,24 +99,21 @@ onBeforeUnmount( () => {\n \n         <section>\n             <expandable :to-show=\"showAddPageForm\">\n-                <add-page :board-id=\"board.id\" class=\"mt-6 p-4 rounded-2xl bg-scale-secondary-700\" \/>\n+                <add-page :board=\"board\" class=\"mt-6 p-4 rounded-2xl bg-scale-secondary-700\" \/>\n             <\/expandable>\n         <\/section>\n \n     <\/template>\n \n     <section class=\"flex overflow-x-scroll h-auto no-scrollbar\">\n-        <page v-for=\"page in board.pages\" :key=\"page.id\" :page=\"page\" class=\"flex-none w-11\/12 md:w-5\/12 lg:w-4\/12\" \/>\n+        <page v-for=\"page in board.pages\" :key=\"page.id\" :page=\"page\" :board=\"board\" class=\"flex-none w-11\/12 md:w-5\/12 lg:w-4\/12\" \/>\n     <\/section>\n \n-    <!-- <modal :show=\"showEditModal\" @hide=\"showEditModal=false\">\n-        <edit-board :board=\"board\" :tags=\"board.tags\" @save=\"showEditModal=false\" \/>\n-    <\/modal> -->\n     <board-modal :board=\"board\" :show=\"showEditModal\" @hide=\"showEditModal=false\" \/>\n \n     <article v-if=\"board?.notes?.length > 0\" class=\"mt-8\">\n         <section class=\"flex items-center pl-2\">\n-            <add-note :board-id=\"board.id\" class=\"w-4\/12\" \/>\n+            <add-note :board=\"board\" class=\"w-4\/12\" \/>\n         <\/section>\n         <section class=\"flex flex-wrap pl-2\">\n             <div v-for=\"note in board.notes\" :key=\"note.id\" class=\"flex-none w-11\/12 md:w-5\/12 lg:w-4\/12\">\ndiff --git a\/frontend\/web-app\/src\/types\/pegboard.types.ts b\/frontend\/web-app\/src\/types\/pegboard.types.ts\nindex ebf33a1..dde7d51 100644\n--- a\/frontend\/web-app\/src\/types\/pegboard.types.ts\n+++ b\/frontend\/web-app\/src\/types\/pegboard.types.ts\n@@ -10,7 +10,7 @@ export type Color = {\n     board:number,\n     \n     name:string,\n-    color:string,\n+    code:string,\n }\n \n export type Tag = {\n@@ -19,7 +19,7 @@ export type Tag = {\n \n     board:number,\n \n-    color:Color,\n+    color:number,\n     name:string,\n }\n \n@@ -136,15 +136,15 @@ export type Board = {\n     url:string,\n     order:number,\n \n-    color_palette:Array<Color>,\n     default_note_display:string,\n \n     date_created:string,\n     date_updated:string,\n     date_archived:string|null,\n \n-    comments:Array<UserComment>\n+    comments:Array<UserComment>,\n     checklists:Array<Checklist>,\n+    colors:Array<Color>,\n }\n \n export type Folder = {\n","files":{"\/backend\/pegboard\/models\/model_board.py":{"changes":[{"diff":"\n \n     order = models.IntegerField(default=0)\n \n-    color_palette = models.ManyToManyField(\n-        'Color',\n-        blank=True,\n-        related_name='color_boards'\n-    )\n-\n     default_note_display = models.CharField(max_length=3, choices=DISPLAY_CHOICES, default='n')\n \n     date_created = models.DateTimeField('date created', default=timezone.now)","add":0,"remove":6,"filename":"\/backend\/pegboard\/models\/model_board.py","badparts":["    color_palette = models.ManyToManyField(","        'Color',","        blank=True,","        related_name='color_boards'","    )"],"goodparts":[]}],"source":"\n\nfrom __future__ import unicode_literals from django.db import models from django.contrib.auth.models import User from django.utils import timezone from django.utils.text import slugify from django.db.models.signals import post_save, m2m_changed from django.dispatch import receiver from django.db.models import Q import channels.layers from asgiref.sync import async_to_sync from.utils import DISPLAY_CHOICES class BoardQuerySet( models.QuerySet): def list(self, user): return self.filter( Q(user=user) | Q(shared_with=user), ) def retrieve(self, user, pk): result=self.filter( Q(user=user) | Q(shared_with=user), pk=pk ).first() if result is not None: return result else: raise FileNotFoundError def list_unsorted(self, user): return self.filter( Q(user=user) | Q(shared_with=user), folder__isnull=True, ) def list_shared_with(self, user): return self.filter( shared_with=user, ).exclude( user=user ) class BoardManager( models.Manager): use_in_migrations=True def get_queryset(self): return BoardQuerySet(self.model, using=self._db).filter(date_archived__isnull=True) def list(self, user): return self.get_queryset().list(user) def retrieve(self, user, pk): return self.get_queryset().retrieve(user, pk) def list_unsorted(self, user): return self.get_queryset().list_unsorted(user) def list_shared_with(self, user): return self.get_queryset().list_shared_with(user) class Board( models.Model): objects=BoardManager() user=models.ForeignKey( User, on_delete=models.CASCADE, related_name='boards' ) shared_with=models.ManyToManyField( User, blank=True, related_name='shared_boards', ) folder=models.ForeignKey( 'Folder', on_delete=models.SET_NULL, blank=True, null=True, related_name='boards' ) name=models.CharField(max_length=128) description=models.TextField(blank=True) url=models.SlugField(blank=True) order=models.IntegerField(default=0) color_palette=models.ManyToManyField( 'Color', blank=True, related_name='color_boards' ) default_note_display=models.CharField(max_length=3, choices=DISPLAY_CHOICES, default='n') date_created=models.DateTimeField('date created', default=timezone.now) date_updated=models.DateTimeField('date updated', default=timezone.now) date_archived=models.DateTimeField('date archived', blank=True, null=True) def __str__( self): return self.name @receiver(post_save, sender=Board) def save_shared_with(sender, instance, **kwargs): post_save.disconnect(save_shared_with, sender=sender) m2m_changed.disconnect(save_shared_with, sender=sender) instance.shared_with.add(instance.user) instance.save() m2m_changed.connect(save_shared_with, sender=sender) post_save.connect(save_shared_with, sender=sender) @receiver(post_save, sender=Board) def save_url(sender, instance, **kwargs): post_save.disconnect(save_url, sender=sender) instance.url=slugify(instance.name) instance.save() post_save.connect(save_url, sender=sender) @receiver(post_save, sender=Board) def save_date_updated(sender, instance, **kwargs): post_save.disconnect(save_date_updated, sender=sender) instance.date_updated=timezone.now() instance.save() post_save.connect(save_date_updated, sender=sender) m2m_changed.connect(save_shared_with, sender=Board.shared_with.through) @receiver(post_save, sender=Board) def update_consumer(sender, instance, **kwargs): channel_layer=channels.layers.get_channel_layer() group_name='board-'+str(instance.id)+'-'+instance.url async_to_sync(channel_layer.group_send)( group_name, { 'type': 'update' } ) ","sourceWithComments":"# -*- coding: utf-8 -*-\nfrom __future__ import unicode_literals\nfrom django.db import models\nfrom django.contrib.auth.models import User\nfrom django.utils import timezone\nfrom django.utils.text import slugify\nfrom django.db.models.signals import post_save, m2m_changed\nfrom django.dispatch import receiver\nfrom django.db.models import Q\nimport channels.layers\nfrom asgiref.sync import async_to_sync\n\nfrom .utils import DISPLAY_CHOICES\n\n\nclass BoardQuerySet ( models.QuerySet ):\n\n    def list(self, user):\n        return self.filter(\n            Q(user=user) | Q(shared_with=user),\n        )\n    \n    def retrieve(self, user, pk):\n        result = self.filter(\n                Q(user=user) | Q(shared_with=user),\n                pk=pk\n            ).first()\n        if result is not None:\n            return result\n        else:\n            raise FileNotFoundError\n\n    def list_unsorted(self, user):\n        return self.filter(\n            Q(user=user) | \n            Q(shared_with=user),\n            folder__isnull=True,\n        )\n    \n    def list_shared_with(self, user):\n        return self.filter(\n            shared_with=user,\n        ).exclude(\n            user=user\n        )\n\nclass BoardManager ( models.Manager ):\n    use_in_migrations = True\n\n    def get_queryset(self):\n        return BoardQuerySet(self.model, using=self._db).filter(date_archived__isnull=True)\n        \n    def list(self, user):\n        return self.get_queryset().list(user)\n\n    def retrieve(self, user, pk):\n        return self.get_queryset().retrieve(user, pk)\n\n    def list_unsorted(self, user):\n        return self.get_queryset().list_unsorted(user)\n\n    def list_shared_with(self, user):\n        return self.get_queryset().list_shared_with(user)\n\nclass Board ( models.Model ):\n    objects = BoardManager()\n\n    user = models.ForeignKey(\n        User,\n        on_delete=models.CASCADE,\n        related_name='boards'\n    )\n\n    shared_with = models.ManyToManyField(\n        User,\n        blank=True,\n        related_name='shared_boards',\n    )\n\n    folder = models.ForeignKey(\n        'Folder',\n        on_delete=models.SET_NULL,\n        blank=True,\n        null=True,\n        related_name='boards'\n    )\n    \n    name = models.CharField(max_length=128)\n    description = models.TextField(blank=True)\n    url = models.SlugField(blank=True)\n\n    order = models.IntegerField(default=0)\n\n    color_palette = models.ManyToManyField(\n        'Color',\n        blank=True,\n        related_name='color_boards'\n    )\n\n    default_note_display = models.CharField(max_length=3, choices=DISPLAY_CHOICES, default='n')\n\n    date_created = models.DateTimeField('date created', default=timezone.now)\n    date_updated = models.DateTimeField('date updated', default=timezone.now)\n    date_archived = models.DateTimeField('date archived', blank=True, null=True)\n\n    def __str__ ( self ):\n        return self.name\n    \n@receiver(post_save, sender=Board)\ndef save_shared_with(sender, instance, **kwargs):\n    post_save.disconnect(save_shared_with, sender=sender)\n    m2m_changed.disconnect(save_shared_with, sender=sender)\n\n    instance.shared_with.add(instance.user)\n    instance.save()\n\n    m2m_changed.connect(save_shared_with, sender=sender)\n    post_save.connect(save_shared_with, sender=sender)\n\n@receiver(post_save, sender=Board)\ndef save_url(sender, instance, **kwargs):\n    post_save.disconnect(save_url, sender=sender)\n\n    instance.url = slugify(instance.name)\n    instance.save()\n\n    post_save.connect(save_url, sender=sender)\n\n@receiver(post_save, sender=Board)\ndef save_date_updated(sender, instance, **kwargs):\n    post_save.disconnect(save_date_updated, sender=sender)\n\n    instance.date_updated = timezone.now()\n    instance.save()\n\n    post_save.connect(save_date_updated, sender=sender)\n\nm2m_changed.connect(save_shared_with, sender=Board.shared_with.through)    \n\n@receiver(post_save, sender=Board)\ndef update_consumer(sender, instance, **kwargs):\n    channel_layer = channels.layers.get_channel_layer()\n    group_name = 'board-'+str(instance.id)+'-'+instance.url\n    async_to_sync(channel_layer.group_send)(\n        group_name,\n        {\n            'type': 'update'\n        }\n    )\n\n# TODO model_board.py\n# @ custom color palette ArrayField (for labels, etc.)\n# @ theme field\n"},"\/backend\/pegboard\/models\/model_color.py":{"changes":[{"diff":"\n     )\n \n     name = models.CharField(max_length=128)\n-    color = models.CharField(max_length=6)\n+    code = models.CharField(max_length=6)\n \n     def __str__ ( self ):\n         return self.nam","add":1,"remove":1,"filename":"\/backend\/pegboard\/models\/model_color.py","badparts":["    color = models.CharField(max_length=6)"],"goodparts":["    code = models.CharField(max_length=6)"]}],"source":"\n\nfrom __future__ import unicode_literals from django.db import models from django.contrib.auth.models import User from django.db.models import Q import channels.layers from asgiref.sync import async_to_sync from django.db.models.signals import post_save from django.dispatch import receiver class ColorQuerySet( models.QuerySet): def list(self, user): return self.filter( ( Q(user=user) | Q(board__user=user) | Q(board__shared_with=user)), ).distinct() def retrieve(self, user, pk): result=self.filter( Q(user=user) | Q(board__user=user) | Q(board__shared_with=user), pk=pk ).first() if result is not None: return result else: raise FileNotFoundError class ColorManager( models.Manager): use_in_migrations=True def get_queryset(self): return ColorQuerySet(self.model, using=self._db) def list(self, user): return self.get_queryset().list(user) def retrieve(self, user, pk): return self.get_queryset().retrieve(user, pk) class Color( models.Model): objects=ColorManager() user=models.ForeignKey( User, on_delete=models.SET_NULL, null=True, blank=True, related_name='colors' ) board=models.ForeignKey( 'Board', on_delete=models.SET_NULL, null=True, blank=True, related_name='colors' ) name=models.CharField(max_length=128) color=models.CharField(max_length=6) def __str__( self): return self.name @receiver(post_save, sender=Color) def update_board_consumer(sender, instance, **kwargs): if instance.board: channel_layer=channels.layers.get_channel_layer() group_name='board-'+str(instance.board.id)+'-'+instance.board.url async_to_sync(channel_layer.group_send)( group_name, { 'type': 'update' } ) ","sourceWithComments":"# -*- coding: utf-8 -*-\nfrom __future__ import unicode_literals\nfrom django.db import models\nfrom django.contrib.auth.models import User\nfrom django.db.models import Q\nimport channels.layers\nfrom asgiref.sync import async_to_sync\nfrom django.db.models.signals import post_save\nfrom django.dispatch import receiver\n\nclass ColorQuerySet ( models.QuerySet ):\n\n    def list(self, user):\n        return self.filter(\n            ( Q(user=user) | Q(board__user=user) | Q(board__shared_with=user) ),\n        ).distinct()\n\n    def retrieve(self, user, pk):\n        result = self.filter(\n                Q(user=user) | Q(board__user=user) | Q(board__shared_with=user),\n                pk=pk\n            ).first()\n        if result is not None:\n            return result\n        else:\n            raise FileNotFoundError\n\nclass ColorManager ( models.Manager ):\n    use_in_migrations = True\n\n    def get_queryset(self):\n        return ColorQuerySet(self.model, using=self._db)\n\n    def list(self, user):\n        return self.get_queryset().list(user)\n\n    def retrieve(self, user, pk):\n        return self.get_queryset().retrieve(user, pk)\n\nclass Color ( models.Model ):\n\n    objects = ColorManager()\n\n    user = models.ForeignKey(\n        User,\n        on_delete=models.SET_NULL,\n        null=True,\n        blank=True,\n        related_name='colors'\n    )\n\n    board = models.ForeignKey(\n        'Board',\n        on_delete=models.SET_NULL,\n        null=True,\n        blank=True,\n        related_name='colors'\n    )\n\n    name = models.CharField(max_length=128)\n    color = models.CharField(max_length=6)\n\n    def __str__ ( self ):\n        return self.name\n\n\n@receiver(post_save, sender=Color)\ndef update_board_consumer(sender, instance, **kwargs):\n    if instance.board:\n\n        channel_layer = channels.layers.get_channel_layer()\n        group_name = 'board-'+str(instance.board.id)+'-'+instance.board.url\n\n        async_to_sync(channel_layer.group_send)(\n            group_name,\n            {\n                'type': 'update'\n            }\n        )"},"\/backend\/pegboard\/models\/model_note.py":{"changes":[{"diff":"\n     use_in_migrations = True\n \n     def get_queryset(self):\n-        return NoteQuerySet(self.model, using=self._db).filter(date_archived__isnull=True)\n+        return NoteQuerySet(self.model, using=self._db).filter(date_archived__isnull=True).order_by('order').order_by('-date_updated')\n \n     def list(self, user):\n         return self.get_queryset().list(use","add":1,"remove":1,"filename":"\/backend\/pegboard\/models\/model_note.py","badparts":["        return NoteQuerySet(self.model, using=self._db).filter(date_archived__isnull=True)"],"goodparts":["        return NoteQuerySet(self.model, using=self._db).filter(date_archived__isnull=True).order_by('order').order_by('-date_updated')"]}],"source":"\n\nfrom __future__ import unicode_literals from django.db import models from django.contrib.auth.models import User from django.utils import timezone from django.utils.text import slugify from django.db.models.signals import post_save from django.dispatch import receiver from django.db.models import Q import channels.layers from asgiref.sync import async_to_sync from.utils import DISPLAY_CHOICES class NoteQuerySet( models.QuerySet): def list(self, user): return self.filter( Q(user=user) | Q(board__user=user) | Q(board__shared_with=user) | Q(page__user=user), date_archived__isnull=True ) def retrieve(self, user, pk): result=self.filter( Q(user=user) | Q(board__user=user) | Q(board__shared_with=user) | Q(page__user=user), pk=pk ).first() if result is not None: return result else: raise FileNotFoundError def list_unsorted(self, user): return self.filter( user=user, page__isnull=True, board__isnull=True, date_archived__isnull=True, ) class NoteManager( models.Manager): use_in_migrations=True def get_queryset(self): return NoteQuerySet(self.model, using=self._db).filter(date_archived__isnull=True) def list(self, user): return self.get_queryset().list(user) def retrieve(self, user, pk): return self.get_queryset().retrieve(user, pk) def list_unsorted(self, user): return self.get_queryset().list_unsorted(user) class Note( models.Model): objects=NoteManager() user=models.ForeignKey( User, on_delete=models.CASCADE, related_name='notes' ) page=models.ForeignKey( 'Page', on_delete=models.SET_NULL, null=True, blank=True, related_name='notes' ) board=models.ForeignKey( 'Board', on_delete=models.SET_NULL, null=True, blank=True, related_name='notes' ) name=models.CharField(max_length=128) content=models.TextField(blank=True) display=models.CharField(max_length=3, choices=DISPLAY_CHOICES, default='n') url=models.SlugField(blank=True) order=models.IntegerField(default=0) starred=models.BooleanField(default=False) pinned=models.BooleanField(default=False) marked_done=models.BooleanField(default=False) tags=models.ManyToManyField( 'Tag', blank=True, related_name='notes' ) assigned_to=models.ForeignKey( User, on_delete=models.SET_NULL, blank=True, null=True, related_name='assigned_notes', ) date_due=models.CharField(max_length=256, blank=True) date_todo=models.CharField(max_length=256, blank=True) date_created=models.DateTimeField('date created', default=timezone.now) date_updated=models.DateTimeField('date updated', default=timezone.now) date_archived=models.DateTimeField('date archived', blank=True, null=True) def __str__( self): return self.name @receiver(post_save, sender=Note) def save_url(sender, instance, **kwargs): post_save.disconnect(save_url, sender=sender) instance.url=slugify(instance.name) instance.save() post_save.connect(save_url, sender=sender) @receiver(post_save, sender=Note) def save_date_updated(sender, instance, **kwargs): post_save.disconnect(save_date_updated, sender=sender) instance.date_updated=timezone.now() instance.save() post_save.connect(save_date_updated, sender=sender) @receiver(post_save, sender=Note) def update_board_consumer(sender, instance, **kwargs): if instance.board or instance.page: channel_layer=channels.layers.get_channel_layer() group_name='board-' if instance.board: group_name +=str(instance.board.id)+'-'+instance.board.url elif instance.page and instance.page.board: group_name +=str(instance.page.board.id)+'-'+instance.page.board.url else: return print('\\nUpdating', group_name, '...\\n') async_to_sync(channel_layer.group_send)( group_name, { 'type': 'update' } ) ","sourceWithComments":"# -*- coding: utf-8 -*-\nfrom __future__ import unicode_literals\nfrom django.db import models\nfrom django.contrib.auth.models import User\nfrom django.utils import timezone\nfrom django.utils.text import slugify\nfrom django.db.models.signals import post_save\nfrom django.dispatch import receiver\nfrom django.db.models import Q\nimport channels.layers\nfrom asgiref.sync import async_to_sync\n\nfrom .utils import DISPLAY_CHOICES\n\nclass NoteQuerySet ( models.QuerySet ):\n\n    def list(self, user):\n        return self.filter(\n            Q(user=user) | Q(board__user=user) | Q(board__shared_with=user) | Q(page__user=user),\n            date_archived__isnull=True\n        )\n\n    def retrieve(self, user, pk):\n        result = self.filter(\n                Q(user=user) | Q(board__user=user) | Q(board__shared_with=user) | Q(page__user=user),\n                pk=pk\n            ).first()\n        if result is not None:\n            return result\n        else:\n            raise FileNotFoundError\n\n    def list_unsorted(self, user):\n        return self.filter(\n            user=user,\n            page__isnull=True,\n            board__isnull=True,\n            date_archived__isnull=True,\n        )\n        \n\nclass NoteManager ( models.Manager ):\n    use_in_migrations = True\n\n    def get_queryset(self):\n        return NoteQuerySet(self.model, using=self._db).filter(date_archived__isnull=True)\n\n    def list(self, user):\n        return self.get_queryset().list(user)\n\n    def retrieve(self, user, pk):\n        return self.get_queryset().retrieve(user, pk)\n\n    def list_unsorted(self, user):\n        return self.get_queryset().list_unsorted(user)\n\n\nclass Note ( models.Model ):\n    objects = NoteManager()\n\n    user = models.ForeignKey(\n        User,\n        on_delete=models.CASCADE,\n        related_name='notes'\n    )\n\n    page = models.ForeignKey(\n        'Page',\n        on_delete=models.SET_NULL,\n        null=True,\n        blank=True,\n        related_name='notes'\n    )\n\n    board = models.ForeignKey(\n        'Board',\n        on_delete=models.SET_NULL,\n        null=True,\n        blank=True,\n        related_name='notes'\n    )\n\n    name = models.CharField(max_length=128)\n    content = models.TextField(blank=True)\n\n    display = models.CharField(max_length=3, choices=DISPLAY_CHOICES, default='n')\n    url = models.SlugField(blank=True)\n    order = models.IntegerField(default=0)\n\n    starred = models.BooleanField(default=False)\n    pinned = models.BooleanField(default=False)\n    marked_done = models.BooleanField(default=False)\n\n    tags = models.ManyToManyField(\n        'Tag',\n        blank=True,\n        related_name='notes'\n    )\n\n    assigned_to = models.ForeignKey(\n        User,\n        on_delete=models.SET_NULL,\n        blank=True,\n        null=True,\n        related_name='assigned_notes',\n    )\n\n    # Due dates are stored as charfields to parse both single dates\n    # and durations\n    date_due = models.CharField(max_length=256, blank=True)\n    date_todo = models.CharField(max_length=256, blank=True)\n\n    date_created = models.DateTimeField('date created', default=timezone.now)\n    date_updated = models.DateTimeField('date updated', default=timezone.now)\n    date_archived = models.DateTimeField('date archived', blank=True, null=True)\n\n    # Shows the name of the object within the admin\n    def __str__ ( self ):\n        return self.name\n\n\n@receiver(post_save, sender=Note)\ndef save_url(sender, instance, **kwargs):\n    post_save.disconnect(save_url, sender=sender)\n\n    instance.url = slugify(instance.name)\n    instance.save()\n\n    post_save.connect(save_url, sender=sender)\n\n@receiver(post_save, sender=Note)\ndef save_date_updated(sender, instance, **kwargs):\n    post_save.disconnect(save_date_updated, sender=sender)\n\n    instance.date_updated = timezone.now()\n    instance.save()\n\n    post_save.connect(save_date_updated, sender=sender)\n\n@receiver(post_save, sender=Note)\ndef update_board_consumer(sender, instance, **kwargs):\n    if instance.board or instance.page:\n\n        channel_layer = channels.layers.get_channel_layer()\n        group_name = 'board-'\n\n        if instance.board:\n            group_name += str(instance.board.id)+'-'+instance.board.url\n        elif instance.page and instance.page.board:\n            group_name += str(instance.page.board.id)+'-'+instance.page.board.url\n        else:\n            return\n\n        print('\\nUpdating', group_name, '...\\n')\n\n        async_to_sync(channel_layer.group_send)(\n            group_name,\n            {\n                'type': 'update'\n            }\n        )\n\n# TODO model_note.py\n# [ ] attachment\n# [ ] implement children, parents\n"},"\/backend\/pegboard\/serializers\/serializer_board.py":{"changes":[{"diff":"\n \n     model = Board\n \n-    folder = serializers.PrimaryKeyRelatedField(queryset=Folder.objects.all(), allow_null=True)\n+    folder = serializers.PrimaryKeyRelatedField(queryset=Folder.objects.all(), required=False, allow_null=True)\n     \n     pages = PageSerializer(many=True, required=False)\n     notes = NoteSerializer(many=True, required=False)\n \n     tags = TagSerializer(many=True, required=False)\n+    colors = ColorSerializer(many=True, required=False)\n     comments = CommentSerializer(many=True, required=False)\n     checklists = ChecklistSerializer(many=True, required=False","add":2,"remove":1,"filename":"\/backend\/pegboard\/serializers\/serializer_board.py","badparts":["    folder = serializers.PrimaryKeyRelatedField(queryset=Folder.objects.all(), allow_null=True)"],"goodparts":["    folder = serializers.PrimaryKeyRelatedField(queryset=Folder.objects.all(), required=False, allow_null=True)","    colors = ColorSerializer(many=True, required=False)"]}],"source":"\nfrom rest_framework import serializers from. import * from..models import Board, Folder class BoardSerializer( serializers.ModelSerializer): model=Board folder=serializers.PrimaryKeyRelatedField(queryset=Folder.objects.all(), allow_null=True) pages=PageSerializer(many=True, required=False) notes=NoteSerializer(many=True, required=False) tags=TagSerializer(many=True, required=False) comments=CommentSerializer(many=True, required=False) checklists=ChecklistSerializer(many=True, required=False) class Meta: model=Board fields='__all__' depth=2 ","sourceWithComments":"from rest_framework import serializers\n\nfrom . import *\nfrom ..models import Board, Folder\n\nclass BoardSerializer ( serializers.ModelSerializer ):\n\n    model = Board\n\n    folder = serializers.PrimaryKeyRelatedField(queryset=Folder.objects.all(), allow_null=True)\n    \n    pages = PageSerializer(many=True, required=False)\n    notes = NoteSerializer(many=True, required=False)\n\n    tags = TagSerializer(many=True, required=False)\n    comments = CommentSerializer(many=True, required=False)\n    checklists = ChecklistSerializer(many=True, required=False)\n\n    class Meta:\n        model = Board\n        fields = '__all__'\n        depth = 2"},"\/backend\/pegboard\/serializers\/serializer_color.py":{"changes":[{"diff":"\n from rest_framework import serializers\n \n-from ..models import Color\n+from ..models import Color, Board\n \n class ColorSerializer ( serializers.ModelSerializer ):\n     model = Color\n+\n+    board = serializers.PrimaryKeyRelatedField(queryset=Board.objects.all(), required=False, allow_null=True)\n+\n     class Meta:\n         model = Color\n-        fields = ['id', 'name', 'color']\n+        fields = '__al","add":5,"remove":2,"filename":"\/backend\/pegboard\/serializers\/serializer_color.py","badparts":["from ..models import Color","        fields = ['id', 'name', 'color']"],"goodparts":["from ..models import Color, Board","    board = serializers.PrimaryKeyRelatedField(queryset=Board.objects.all(), required=False, allow_null=True)","        fields = '__al"]}],"source":"\nfrom rest_framework import serializers from..models import Color class ColorSerializer( serializers.ModelSerializer): model=Color class Meta: model=Color fields=['id', 'name', 'color'] ","sourceWithComments":"from rest_framework import serializers\n\nfrom ..models import Color\n\nclass ColorSerializer ( serializers.ModelSerializer ):\n    model = Color\n    class Meta:\n        model = Color\n        fields = ['id', 'name', 'color']\n"},"\/backend\/pegboard\/serializers\/serializer_comment.py":{"changes":[{"diff":"\n \n     model = Comment\n \n-    board = serializers.PrimaryKeyRelatedField(queryset=Board.objects.all(), allow_null=True)\n-    page = serializers.PrimaryKeyRelatedField(queryset=Page.objects.all(), allow_null=True)\n-    note = serializers.PrimaryKeyRelatedField(queryset=Note.objects.all(), allow_null=True)\n+    board = serializers.PrimaryKeyRelatedField(queryset=Board.objects.all(), required=False, allow_null=True)\n+    page = serializers.PrimaryKeyRelatedField(queryset=Page.objects.all(), required=False, allow_null=True)\n+    note = serializers.PrimaryKeyRelatedField(queryset=Note.objects.all(), required=False, allow_null=True)\n \n     class Meta:\n         model = Co","add":3,"remove":3,"filename":"\/backend\/pegboard\/serializers\/serializer_comment.py","badparts":["    board = serializers.PrimaryKeyRelatedField(queryset=Board.objects.all(), allow_null=True)","    page = serializers.PrimaryKeyRelatedField(queryset=Page.objects.all(), allow_null=True)","    note = serializers.PrimaryKeyRelatedField(queryset=Note.objects.all(), allow_null=True)"],"goodparts":["    board = serializers.PrimaryKeyRelatedField(queryset=Board.objects.all(), required=False, allow_null=True)","    page = serializers.PrimaryKeyRelatedField(queryset=Page.objects.all(), required=False, allow_null=True)","    note = serializers.PrimaryKeyRelatedField(queryset=Note.objects.all(), required=False, allow_null=True)"]}],"source":"\nfrom rest_framework import serializers from..models import Comment, Board, Page, Note class CommentSerializer( serializers.ModelSerializer): model=Comment board=serializers.PrimaryKeyRelatedField(queryset=Board.objects.all(), allow_null=True) page=serializers.PrimaryKeyRelatedField(queryset=Page.objects.all(), allow_null=True) note=serializers.PrimaryKeyRelatedField(queryset=Note.objects.all(), allow_null=True) class Meta: model=Comment fields='__all__' depth=1 ","sourceWithComments":"from rest_framework import serializers\n\nfrom ..models import Comment, Board, Page, Note\n    \nclass CommentSerializer ( serializers.ModelSerializer ):\n\n    model = Comment\n\n    board = serializers.PrimaryKeyRelatedField(queryset=Board.objects.all(), allow_null=True)\n    page = serializers.PrimaryKeyRelatedField(queryset=Page.objects.all(), allow_null=True)\n    note = serializers.PrimaryKeyRelatedField(queryset=Note.objects.all(), allow_null=True)\n\n    class Meta:\n        model = Comment\n        fields = '__all__'\n        depth = 1"},"\/backend\/pegboard\/serializers\/serializer_note.py":{"changes":[{"diff":"\n class NoteSerializer ( serializers.ModelSerializer ):\n     model = Note\n \n-    board = serializers.PrimaryKeyRelatedField(queryset=Board.objects.all(), allow_null=True)\n-    page = serializers.PrimaryKeyRelatedField(queryset=Page.objects.all(), allow_null=True)\n+    board = serializers.PrimaryKeyRelatedField(queryset=Board.objects.all(), required=False, allow_null=True)\n+    page = serializers.PrimaryKeyRelatedField(queryset=Page.objects.all(), required=False, allow_null=True)\n \n     tags = TagSerializer(many=True, required=False)","add":2,"remove":2,"filename":"\/backend\/pegboard\/serializers\/serializer_note.py","badparts":["    board = serializers.PrimaryKeyRelatedField(queryset=Board.objects.all(), allow_null=True)","    page = serializers.PrimaryKeyRelatedField(queryset=Page.objects.all(), allow_null=True)"],"goodparts":["    board = serializers.PrimaryKeyRelatedField(queryset=Board.objects.all(), required=False, allow_null=True)","    page = serializers.PrimaryKeyRelatedField(queryset=Page.objects.all(), required=False, allow_null=True)"]}],"source":"\nfrom rest_framework import serializers from. import * from..models import Note, Board, Page class NoteSerializer( serializers.ModelSerializer): model=Note board=serializers.PrimaryKeyRelatedField(queryset=Board.objects.all(), allow_null=True) page=serializers.PrimaryKeyRelatedField(queryset=Page.objects.all(), allow_null=True) tags=TagSerializer(many=True, required=False) comments=CommentSerializer(many=True, required=False) checklists=ChecklistSerializer(many=True, required=False) class Meta: model=Note fields='__all__' depth=2 def update(self, instance, validated_data): instance.user=validated_data.get('user', instance.user) instance.name=validated_data.get('name', instance.name) instance.content=validated_data.get('content', instance.content) instance.display=validated_data.get('display', instance.display) if validated_data.get('date_archived'): instance.date_archived=validated_data.get('date_archived', instance.date_archived) instance.tags.clear() if validated_data.get('tags'): for tag in validated_data.get('tags'): instance.tags.add(tag.get('id')) instance.save() return instance ","sourceWithComments":"from rest_framework import serializers\n\nfrom . import *\nfrom ..models import Note, Board, Page\n\nclass NoteSerializer ( serializers.ModelSerializer ):\n    model = Note\n\n    board = serializers.PrimaryKeyRelatedField(queryset=Board.objects.all(), allow_null=True)\n    page = serializers.PrimaryKeyRelatedField(queryset=Page.objects.all(), allow_null=True)\n\n    tags = TagSerializer(many=True, required=False)\n    \n    comments = CommentSerializer(many=True, required=False)\n    checklists = ChecklistSerializer(many=True, required=False)\n\n    class Meta:\n        model = Note\n        fields = '__all__'\n        depth = 2\n        # TODO add ordering here\n\n    def update(self, instance, validated_data):\n\n        instance.user = validated_data.get('user', instance.user)\n        instance.name = validated_data.get('name', instance.name)\n        instance.content = validated_data.get('content', instance.content)\n        instance.display = validated_data.get('display', instance.display)\n\n        if validated_data.get('date_archived'):\n            instance.date_archived = validated_data.get('date_archived', instance.date_archived)\n\n        instance.tags.clear()\n        if validated_data.get('tags'):\n            for tag in validated_data.get('tags'):\n                instance.tags.add(tag.get('id'))\n\n        instance.save()\n\n        return instance"},"\/backend\/pegboard\/serializers\/serializer_page.py":{"changes":[{"diff":"\n     \n     model = Page\n \n-    board = serializers.PrimaryKeyRelatedField(queryset=Board.objects.all(), allow_null=True)\n+    board = serializers.PrimaryKeyRelatedField(queryset=Board.objects.all(), required=False, allow_null=True)\n \n     notes = NoteSerializer(many=True, required=False, read_only=True","add":1,"remove":1,"filename":"\/backend\/pegboard\/serializers\/serializer_page.py","badparts":["    board = serializers.PrimaryKeyRelatedField(queryset=Board.objects.all(), allow_null=True)"],"goodparts":["    board = serializers.PrimaryKeyRelatedField(queryset=Board.objects.all(), required=False, allow_null=True)"]}],"source":"\nfrom rest_framework import serializers from. import * from..models import Page, Board class PageSerializer( serializers.ModelSerializer): model=Page board=serializers.PrimaryKeyRelatedField(queryset=Board.objects.all(), allow_null=True) notes=NoteSerializer(many=True, required=False, read_only=True) comments=CommentSerializer(many=True, required=False, read_only=True) checklists=ChecklistSerializer(many=True, required=False, read_only=True) class Meta: model=Page fields='__all__' depth=2 ","sourceWithComments":"from rest_framework import serializers\n\nfrom . import *\nfrom ..models import Page, Board\n\nclass PageSerializer ( serializers.ModelSerializer ):\n    \n    model = Page\n\n    board = serializers.PrimaryKeyRelatedField(queryset=Board.objects.all(), allow_null=True)\n\n    notes = NoteSerializer(many=True, required=False, read_only=True)\n    \n    comments = CommentSerializer(many=True, required=False, read_only=True)\n    checklists = ChecklistSerializer(many=True, required=False, read_only=True)\n\n    class Meta:\n        model = Page\n        fields = '__all__'\n        depth = 2"},"\/backend\/pegboard\/serializers\/serializer_tag.py":{"changes":[{"diff":"\n-\n from django.db.models.fields import IntegerField\n from rest_framework import serializers\n \n from . import *\n-from users.serializers import UserSerializer\n-from ..models import Tag\n+from ..models import Tag, Board, Color\n \n class TagSerializer ( serializers.ModelSerializer ):\n \n     model = Tag\n \n-    id = serializers.IntegerField()\n-\n+    id = serializers.IntegerField(required=False)\n+    board = serializers.PrimaryKeyRelatedField(queryset=Board.objects.all(), required=False, allow_null=True)\n+    color = serializers.PrimaryKeyRelatedField(queryset=Color.objects.all(), required=False, allow_null=True)\n+    \n     class Meta:\n         model = Tag\n-        fields = ['id', 'name', 'color']\n+        fields = '__all__'\n         d","add":6,"remove":6,"filename":"\/backend\/pegboard\/serializers\/serializer_tag.py","badparts":["from users.serializers import UserSerializer","from ..models import Tag","    id = serializers.IntegerField()","        fields = ['id', 'name', 'color']"],"goodparts":["from ..models import Tag, Board, Color","    id = serializers.IntegerField(required=False)","    board = serializers.PrimaryKeyRelatedField(queryset=Board.objects.all(), required=False, allow_null=True)","    color = serializers.PrimaryKeyRelatedField(queryset=Color.objects.all(), required=False, allow_null=True)","        fields = '__all__'"]}],"source":"\n\nfrom django.db.models.fields import IntegerField from rest_framework import serializers from. import * from users.serializers import UserSerializer from..models import Tag class TagSerializer( serializers.ModelSerializer): model=Tag id=serializers.IntegerField() class Meta: model=Tag fields=['id', 'name', 'color'] depth=1 ","sourceWithComments":"\nfrom django.db.models.fields import IntegerField\nfrom rest_framework import serializers\n\nfrom . import *\nfrom users.serializers import UserSerializer\nfrom ..models import Tag\n\nclass TagSerializer ( serializers.ModelSerializer ):\n\n    model = Tag\n\n    id = serializers.IntegerField()\n\n    class Meta:\n        model = Tag\n        fields = ['id', 'name', 'color']\n        depth = 1\n"}},"msg":"<api> fix up api calls that were tampered with in structure changes"}},"https:\/\/github.com\/SmartDash0129\/StableDiffusion-WebUI":{"ef99ed88b9fc061740a919b388a5fb6899c3520a":{"url":"https:\/\/api.github.com\/repos\/SmartDash0129\/StableDiffusion-WebUI\/commits\/ef99ed88b9fc061740a919b388a5fb6899c3520a","html_url":"https:\/\/github.com\/SmartDash0129\/StableDiffusion-WebUI\/commit\/ef99ed88b9fc061740a919b388a5fb6899c3520a","message":"Add --hide-ui-dir-config command line flag\n\nAdds `--hide-ui-dir-config` flag to disable editing directory configs from the web UI. This can be set to prevent users from setting the directory to somewhere they shouldn't, for public (or semi-public) interfaces.\n\nDirectories are still read from config.json, so the server admin can still set them in the web UI and then relaunch with the hide flag, or edit the config manually.\n\nAlso:\n- fix OptionInfo `component_args` keyword argument not being read if `component` isn't also set\n- ensure that hidden settings aren't still read from the web UI (otherwise they could still be changed by tampering with the interface)","sha":"ef99ed88b9fc061740a919b388a5fb6899c3520a","keyword":"tampering change","diff":"diff --git a\/modules\/shared.py b\/modules\/shared.py\nindex 9eeb64e..0557cfe 100644\n--- a\/modules\/shared.py\n+++ b\/modules\/shared.py\n@@ -42,6 +42,7 @@\n parser.add_argument(\"--port\", type=int, help=\"launch gradio with given server port, you need root\/admin rights for ports < 1024, defaults to 7860 if available\", default=None)\r\n parser.add_argument(\"--show-negative-prompt\", action='store_true', help=\"does not do anything\", default=False)\r\n parser.add_argument(\"--ui-config-file\", type=str, help=\"filename to use for ui configuration\", default=os.path.join(script_path, 'ui-config.json'))\r\n+parser.add_argument(\"--hide-ui-dir-config\", action='store_true', help=\"hide directory configuration from webui\", default=False)\r\n \r\n cmd_opts = parser.parse_args()\r\n \r\n@@ -91,18 +92,19 @@ def __init__(self, default=None, label=\"\", component=None, component_args=None):\n             self.component_args = component_args\r\n \r\n     data = None\r\n+    hide_dirs = {\"visible\": False} if cmd_opts.hide_ui_dir_config else None\r\n     data_labels = {\r\n-        \"outdir_samples\": OptionInfo(\"\", \"Output directory for images; if empty, defaults to two directories below\"),\r\n-        \"outdir_txt2img_samples\": OptionInfo(\"outputs\/txt2img-images\", 'Output directory for txt2img images'),\r\n-        \"outdir_img2img_samples\": OptionInfo(\"outputs\/img2img-images\", 'Output directory for img2img images'),\r\n-        \"outdir_extras_samples\": OptionInfo(\"outputs\/extras-images\", 'Output directory for images from extras tab'),\r\n-        \"outdir_grids\": OptionInfo(\"\", \"Output directory for grids; if empty, defaults to two directories below\"),\r\n-        \"outdir_txt2img_grids\": OptionInfo(\"outputs\/txt2img-grids\", 'Output directory for txt2img grids'),\r\n-        \"outdir_img2img_grids\": OptionInfo(\"outputs\/img2img-grids\", 'Output directory for img2img grids'),\r\n+        \"outdir_samples\": OptionInfo(\"\", \"Output directory for images; if empty, defaults to two directories below\", component_args=hide_dirs),\r\n+        \"outdir_txt2img_samples\": OptionInfo(\"outputs\/txt2img-images\", 'Output directory for txt2img images', component_args=hide_dirs),\r\n+        \"outdir_img2img_samples\": OptionInfo(\"outputs\/img2img-images\", 'Output directory for img2img images', component_args=hide_dirs),\r\n+        \"outdir_extras_samples\": OptionInfo(\"outputs\/extras-images\", 'Output directory for images from extras tab', component_args=hide_dirs),\r\n+        \"outdir_grids\": OptionInfo(\"\", \"Output directory for grids; if empty, defaults to two directories below\", component_args=hide_dirs),\r\n+        \"outdir_txt2img_grids\": OptionInfo(\"outputs\/txt2img-grids\", 'Output directory for txt2img grids', component_args=hide_dirs),\r\n+        \"outdir_img2img_grids\": OptionInfo(\"outputs\/img2img-grids\", 'Output directory for img2img grids', component_args=hide_dirs),\r\n         \"save_to_dirs\": OptionInfo(False, \"When writing images, create a directory with name derived from the prompt\"),\r\n         \"grid_save_to_dirs\": OptionInfo(False, \"When writing grids, create a directory with name derived from the prompt\"),\r\n         \"save_to_dirs_prompt_len\": OptionInfo(10, \"When using above, how many words from prompt to put into directory name\", gr.Slider, {\"minimum\": 1, \"maximum\": 32, \"step\": 1}),\r\n-        \"outdir_save\": OptionInfo(\"log\/images\", \"Directory for saving images using the Save button\"),\r\n+        \"outdir_save\": OptionInfo(\"log\/images\", \"Directory for saving images using the Save button\", component_args=hide_dirs),\r\n         \"samples_save\": OptionInfo(True, \"Save indiviual samples\"),\r\n         \"samples_format\": OptionInfo('png', 'File format for individual samples'),\r\n         \"grid_save\": OptionInfo(True, \"Save image grids\"),\r\ndiff --git a\/modules\/ui.py b\/modules\/ui.py\nindex b9af2c8..3b7eb9b 100644\n--- a\/modules\/ui.py\n+++ b\/modules\/ui.py\n@@ -661,19 +661,20 @@ def fun():\n         info = opts.data_labels[key]\r\n         t = type(info.default)\r\n \r\n+        args = info.component_args() if callable(info.component_args) else info.component_args\r\n+\r\n         if info.component is not None:\r\n-            args = info.component_args() if callable(info.component_args) else info.component_args\r\n-            item = info.component(label=info.label, value=fun, **(args or {}))\r\n+            comp = info.component\r\n         elif t == str:\r\n-            item = gr.Textbox(label=info.label, value=fun, lines=1)\r\n+            comp = gr.Textbox\r\n         elif t == int:\r\n-            item = gr.Number(label=info.label, value=fun)\r\n+            comp = gr.Number\r\n         elif t == bool:\r\n-            item = gr.Checkbox(label=info.label, value=fun)\r\n+            comp = gr.Checkbox\r\n         else:\r\n             raise Exception(f'bad options item type: {str(t)} for key {key}')\r\n \r\n-        return item\r\n+        return comp(label=info.label, value=fun, **(args or {}))\r\n \r\n     components = []\r\n     keys = list(opts.data_labels.keys())\r\n@@ -684,6 +685,10 @@ def run_settings(*args):\n         up = []\r\n \r\n         for key, value, comp in zip(opts.data_labels.keys(), args, components):\r\n+            comp_args = opts.data_labels[key].component_args\r\n+            if comp_args and isinstance(comp_args, dict) and comp_args.get('visible') is False:\r\n+                continue\r\n+\r\n             opts.data[key] = value\r\n             up.append(comp.update(value=value))\r\n \r\n","files":{"\/modules\/shared.py":{"changes":[{"diff":"\n             self.component_args = component_args\r\n \r\n     data = None\r\n+    hide_dirs = {\"visible\": False} if cmd_opts.hide_ui_dir_config else None\r\n     data_labels = {\r\n-        \"outdir_samples\": OptionInfo(\"\", \"Output directory for images; if empty, defaults to two directories below\"),\r\n-        \"outdir_txt2img_samples\": OptionInfo(\"outputs\/txt2img-images\", 'Output directory for txt2img images'),\r\n-        \"outdir_img2img_samples\": OptionInfo(\"outputs\/img2img-images\", 'Output directory for img2img images'),\r\n-        \"outdir_extras_samples\": OptionInfo(\"outputs\/extras-images\", 'Output directory for images from extras tab'),\r\n-        \"outdir_grids\": OptionInfo(\"\", \"Output directory for grids; if empty, defaults to two directories below\"),\r\n-        \"outdir_txt2img_grids\": OptionInfo(\"outputs\/txt2img-grids\", 'Output directory for txt2img grids'),\r\n-        \"outdir_img2img_grids\": OptionInfo(\"outputs\/img2img-grids\", 'Output directory for img2img grids'),\r\n+        \"outdir_samples\": OptionInfo(\"\", \"Output directory for images; if empty, defaults to two directories below\", component_args=hide_dirs),\r\n+        \"outdir_txt2img_samples\": OptionInfo(\"outputs\/txt2img-images\", 'Output directory for txt2img images', component_args=hide_dirs),\r\n+        \"outdir_img2img_samples\": OptionInfo(\"outputs\/img2img-images\", 'Output directory for img2img images', component_args=hide_dirs),\r\n+        \"outdir_extras_samples\": OptionInfo(\"outputs\/extras-images\", 'Output directory for images from extras tab', component_args=hide_dirs),\r\n+        \"outdir_grids\": OptionInfo(\"\", \"Output directory for grids; if empty, defaults to two directories below\", component_args=hide_dirs),\r\n+        \"outdir_txt2img_grids\": OptionInfo(\"outputs\/txt2img-grids\", 'Output directory for txt2img grids', component_args=hide_dirs),\r\n+        \"outdir_img2img_grids\": OptionInfo(\"outputs\/img2img-grids\", 'Output directory for img2img grids', component_args=hide_dirs),\r\n         \"save_to_dirs\": OptionInfo(False, \"When writing images, create a directory with name derived from the prompt\"),\r\n         \"grid_save_to_dirs\": OptionInfo(False, \"When writing grids, create a directory with name derived from the prompt\"),\r\n         \"save_to_dirs_prompt_len\": OptionInfo(10, \"When using above, how many words from prompt to put into directory name\", gr.Slider, {\"minimum\": 1, \"maximum\": 32, \"step\": 1}),\r\n-        \"outdir_save\": OptionInfo(\"log\/images\", \"Directory for saving images using the Save button\"),\r\n+        \"outdir_save\": OptionInfo(\"log\/images\", \"Directory for saving images using the Save button\", component_args=hide_dirs),\r\n         \"samples_save\": OptionInfo(True, \"Save indiviual samples\"),\r\n         \"samples_format\": OptionInfo('png', 'File format for individual samples'),\r\n         \"grid_save\": OptionInfo(True, \"Save image grids\"),\r","add":9,"remove":8,"filename":"\/modules\/shared.py","badparts":["        \"outdir_samples\": OptionInfo(\"\", \"Output directory for images; if empty, defaults to two directories below\"),\r","        \"outdir_txt2img_samples\": OptionInfo(\"outputs\/txt2img-images\", 'Output directory for txt2img images'),\r","        \"outdir_img2img_samples\": OptionInfo(\"outputs\/img2img-images\", 'Output directory for img2img images'),\r","        \"outdir_extras_samples\": OptionInfo(\"outputs\/extras-images\", 'Output directory for images from extras tab'),\r","        \"outdir_grids\": OptionInfo(\"\", \"Output directory for grids; if empty, defaults to two directories below\"),\r","        \"outdir_txt2img_grids\": OptionInfo(\"outputs\/txt2img-grids\", 'Output directory for txt2img grids'),\r","        \"outdir_img2img_grids\": OptionInfo(\"outputs\/img2img-grids\", 'Output directory for img2img grids'),\r","        \"outdir_save\": OptionInfo(\"log\/images\", \"Directory for saving images using the Save button\"),\r"],"goodparts":["    hide_dirs = {\"visible\": False} if cmd_opts.hide_ui_dir_config else None\r","        \"outdir_samples\": OptionInfo(\"\", \"Output directory for images; if empty, defaults to two directories below\", component_args=hide_dirs),\r","        \"outdir_txt2img_samples\": OptionInfo(\"outputs\/txt2img-images\", 'Output directory for txt2img images', component_args=hide_dirs),\r","        \"outdir_img2img_samples\": OptionInfo(\"outputs\/img2img-images\", 'Output directory for img2img images', component_args=hide_dirs),\r","        \"outdir_extras_samples\": OptionInfo(\"outputs\/extras-images\", 'Output directory for images from extras tab', component_args=hide_dirs),\r","        \"outdir_grids\": OptionInfo(\"\", \"Output directory for grids; if empty, defaults to two directories below\", component_args=hide_dirs),\r","        \"outdir_txt2img_grids\": OptionInfo(\"outputs\/txt2img-grids\", 'Output directory for txt2img grids', component_args=hide_dirs),\r","        \"outdir_img2img_grids\": OptionInfo(\"outputs\/img2img-grids\", 'Output directory for img2img grids', component_args=hide_dirs),\r","        \"outdir_save\": OptionInfo(\"log\/images\", \"Directory for saving images using the Save button\", component_args=hide_dirs),\r"]}],"source":"\nimport sys\r import argparse\r import json\r import os\r \r import gradio as gr\r import torch\r import tqdm\r \r import modules.artists\r from modules.paths import script_path, sd_path\r from modules.devices import get_optimal_device\r import modules.styles\r import modules.interrogate\r \r config_filename=\"config.json\"\r \r sd_model_file=os.path.join(script_path, 'model.ckpt')\r if not os.path.exists(sd_model_file):\r sd_model_file=\"models\/ldm\/stable-diffusion-v1\/model.ckpt\"\r \r parser=argparse.ArgumentParser()\r parser.add_argument(\"--config\", type=str, default=os.path.join(sd_path, \"configs\/stable-diffusion\/v1-inference.yaml\"), help=\"path to config which constructs model\",)\r parser.add_argument(\"--ckpt\", type=str, default=os.path.join(sd_path, sd_model_file), help=\"path to checkpoint of model\",)\r parser.add_argument(\"--gfpgan-dir\", type=str, help=\"GFPGAN directory\", default=('.\/src\/gfpgan' if os.path.exists('.\/src\/gfpgan') else '.\/GFPGAN'))\r parser.add_argument(\"--gfpgan-model\", type=str, help=\"GFPGAN model file name\", default='GFPGANv1.3.pth')\r parser.add_argument(\"--no-half\", action='store_true', help=\"do not switch the model to 16-bit floats\")\r parser.add_argument(\"--no-progressbar-hiding\", action='store_true', help=\"do not hide progressbar in gradio UI(we hide it because it slows down ML if you have hardware accleration in browser)\")\r parser.add_argument(\"--max-batch-count\", type=int, default=16, help=\"maximum batch count value for the UI\")\r parser.add_argument(\"--embeddings-dir\", type=str, default='embeddings', help=\"embeddings directory for textual inversion(default: embeddings)\")\r parser.add_argument(\"--allow-code\", action='store_true', help=\"allow custom script execution from webui\")\r parser.add_argument(\"--medvram\", action='store_true', help=\"enable stable diffusion model optimizations for sacrificing a little speed for low VRM usage\")\r parser.add_argument(\"--lowvram\", action='store_true', help=\"enable stable diffusion model optimizations for sacrificing a lot of speed for very low VRM usage\")\r parser.add_argument(\"--always-batch-cond-uncond\", action='store_true', help=\"a workaround test; may help with speed if you use --lowvram\")\r parser.add_argument(\"--unload-gfpgan\", action='store_true', help=\"unload GFPGAN every time after processing images. Warning: seems to cause memory leaks\")\r parser.add_argument(\"--precision\", type=str, help=\"evaluate at this precision\", choices=[\"full\", \"autocast\"], default=\"autocast\")\r parser.add_argument(\"--share\", action='store_true', help=\"use share=True for gradio and make the UI accessible through their site(doesn't work for me but you might have better luck)\")\r parser.add_argument(\"--esrgan-models-path\", type=str, help=\"path to directory with ESRGAN models\", default=os.path.join(script_path, 'ESRGAN'))\r parser.add_argument(\"--opt-split-attention\", action='store_true', help=\"enable optimization that reduce vram usage by a lot for about 10%% decrease in performance\")\r parser.add_argument(\"--opt-split-attention-v1\", action='store_true', help=\"enable older version of --opt-split-attention optimization\")\r parser.add_argument(\"--listen\", action='store_true', help=\"launch gradio with 0.0.0.0 as server name, allowing to respond to network requests\")\r parser.add_argument(\"--port\", type=int, help=\"launch gradio with given server port, you need root\/admin rights for ports < 1024, defaults to 7860 if available\", default=None)\r parser.add_argument(\"--show-negative-prompt\", action='store_true', help=\"does not do anything\", default=False)\r parser.add_argument(\"--ui-config-file\", type=str, help=\"filename to use for ui configuration\", default=os.path.join(script_path, 'ui-config.json'))\r \r cmd_opts=parser.parse_args()\r \r device=get_optimal_device()\r \r batch_cond_uncond=cmd_opts.always_batch_cond_uncond or not(cmd_opts.lowvram or cmd_opts.medvram)\r parallel_processing_allowed=not cmd_opts.lowvram and not cmd_opts.medvram\r \r \r class State:\r interrupted=False\r job=\"\"\r job_no=0\r job_count=0\r sampling_step=0\r sampling_steps=0\r current_latent=None\r current_image=None\r current_image_sampling_step=0\r \r def interrupt(self):\r self.interrupted=True\r \r def nextjob(self):\r self.job_no +=1\r self.sampling_step=0\r self.current_image_sampling_step=0\r \r \r state=State()\r \r artist_db=modules.artists.ArtistsDatabase(os.path.join(script_path, 'artists.csv'))\r \r styles_filename=os.path.join(script_path, 'styles.csv')\r prompt_styles=modules.styles.load_styles(styles_filename)\r \r interrogator=modules.interrogate.InterrogateModels(\"interrogate\")\r \r face_restorers=[]\r \r class Options:\r class OptionInfo:\r def __init__(self, default=None, label=\"\", component=None, component_args=None):\r self.default=default\r self.label=label\r self.component=component\r self.component_args=component_args\r \r data=None\r data_labels={\r \"outdir_samples\": OptionInfo(\"\", \"Output directory for images; if empty, defaults to two directories below\"),\r \"outdir_txt2img_samples\": OptionInfo(\"outputs\/txt2img-images\", 'Output directory for txt2img images'),\r \"outdir_img2img_samples\": OptionInfo(\"outputs\/img2img-images\", 'Output directory for img2img images'),\r \"outdir_extras_samples\": OptionInfo(\"outputs\/extras-images\", 'Output directory for images from extras tab'),\r \"outdir_grids\": OptionInfo(\"\", \"Output directory for grids; if empty, defaults to two directories below\"),\r \"outdir_txt2img_grids\": OptionInfo(\"outputs\/txt2img-grids\", 'Output directory for txt2img grids'),\r \"outdir_img2img_grids\": OptionInfo(\"outputs\/img2img-grids\", 'Output directory for img2img grids'),\r \"save_to_dirs\": OptionInfo(False, \"When writing images, create a directory with name derived from the prompt\"),\r \"grid_save_to_dirs\": OptionInfo(False, \"When writing grids, create a directory with name derived from the prompt\"),\r \"save_to_dirs_prompt_len\": OptionInfo(10, \"When using above, how many words from prompt to put into directory name\", gr.Slider,{\"minimum\": 1, \"maximum\": 32, \"step\": 1}),\r \"outdir_save\": OptionInfo(\"log\/images\", \"Directory for saving images using the Save button\"),\r \"samples_save\": OptionInfo(True, \"Save indiviual samples\"),\r \"samples_format\": OptionInfo('png', 'File format for individual samples'),\r \"grid_save\": OptionInfo(True, \"Save image grids\"),\r \"return_grid\": OptionInfo(True, \"Show grid in results for web\"),\r \"grid_format\": OptionInfo('png', 'File format for grids'),\r \"grid_extended_filename\": OptionInfo(False, \"Add extended info(seed, prompt) to filename when saving grid\"),\r \"grid_only_if_multiple\": OptionInfo(True, \"Do not save grids consisting of one picture\"),\r \"n_rows\": OptionInfo(-1, \"Grid row count; use -1 for autodetect and 0 for it to be same as batch size\", gr.Slider,{\"minimum\": -1, \"maximum\": 16, \"step\": 1}),\r \"jpeg_quality\": OptionInfo(80, \"Quality for saved jpeg images\", gr.Slider,{\"minimum\": 1, \"maximum\": 100, \"step\": 1}),\r \"export_for_4chan\": OptionInfo(True, \"If PNG image is larger than 4MB or any dimension is larger than 4000, downscale and save copy as JPG\"),\r \"enable_pnginfo\": OptionInfo(True, \"Save text information about generation parameters as chunks to png files\"),\r \"font\": OptionInfo(\"\", \"Font for image grids that have text\"),\r \"enable_emphasis\": OptionInfo(True, \"Use(text) to make model pay more attention to text text and[text] to make it pay less attention\"),\r \"save_txt\": OptionInfo(False, \"Create a text file next to every image with generation parameters.\"),\r \"ESRGAN_tile\": OptionInfo(192, \"Tile size for upscaling. 0=no tiling.\", gr.Slider,{\"minimum\": 0, \"maximum\": 512, \"step\": 16}),\r \"ESRGAN_tile_overlap\": OptionInfo(8, \"Tile overlap, in pixels for upscaling. Low values=visible seam.\", gr.Slider,{\"minimum\": 0, \"maximum\": 48, \"step\": 1}),\r \"random_artist_categories\": OptionInfo([], \"Allowed categories for random artists selection when using the Roll button\", gr.CheckboxGroup,{\"choices\": artist_db.categories()}),\r \"upscale_at_full_resolution_padding\": OptionInfo(16, \"Inpainting at full resolution: padding, in pixels, for the masked region.\", gr.Slider,{\"minimum\": 0, \"maximum\": 128, \"step\": 4}),\r \"show_progressbar\": OptionInfo(True, \"Show progressbar\"),\r \"show_progress_every_n_steps\": OptionInfo(0, \"Show show image creation progress every N sampling steps. Set 0 to disable.\", gr.Slider,{\"minimum\": 0, \"maximum\": 32, \"step\": 1}),\r \"multiple_tqdm\": OptionInfo(True, \"Add a second progress bar to the console that shows progress for an entire job. Broken in PyCharm console.\"),\r \"face_restoration_model\": OptionInfo(None, \"Face restoration model\", gr.Radio, lambda:{\"choices\":[x.name() for x in face_restorers]}),\r \"code_former_weight\": OptionInfo(0.5, \"CodeFormer weight parameter; 0=maximum effect; 1=minimum effect\", gr.Slider,{\"minimum\": 0, \"maximum\": 1, \"step\": 0.01}),\r \"interrogate_keep_models_in_memory\": OptionInfo(True, \"Interrogate: keep models in VRAM\"),\r \"interrogate_use_builtin_artists\": OptionInfo(True, \"Interrogate: use artists from artists.csv\"),\r \"interrogate_clip_num_beams\": OptionInfo(1, \"Interrogate: num_beams for BLIP\", gr.Slider,{\"minimum\": 1, \"maximum\": 16, \"step\": 1}),\r \"interrogate_clip_min_length\": OptionInfo(24, \"Interrogate: minimum descripton length(excluding artists, etc..)\", gr.Slider,{\"minimum\": 1, \"maximum\": 128, \"step\": 1}),\r \"interrogate_clip_max_length\": OptionInfo(48, \"Interrogate: maximum descripton length\", gr.Slider,{\"minimum\": 1, \"maximum\": 256, \"step\": 1}),\r }\r \r def __init__(self):\r self.data={k: v.default for k, v in self.data_labels.items()}\r \r def __setattr__(self, key, value):\r if self.data is not None:\r if key in self.data:\r self.data[key]=value\r \r return super(Options, self).__setattr__(key, value)\r \r def __getattr__(self, item):\r if self.data is not None:\r if item in self.data:\r return self.data[item]\r \r if item in self.data_labels:\r return self.data_labels[item].default\r \r return super(Options, self).__getattribute__(item)\r \r def save(self, filename):\r with open(filename, \"w\", encoding=\"utf8\") as file:\r json.dump(self.data, file)\r \r def load(self, filename):\r with open(filename, \"r\", encoding=\"utf8\") as file:\r self.data=json.load(file)\r \r \r opts=Options()\r if os.path.exists(config_filename):\r opts.load(config_filename)\r \r sd_upscalers=[]\r \r sd_model=None\r \r progress_print_out=sys.stdout\r \r \r class TotalTQDM:\r def __init__(self):\r self._tqdm=None\r \r def reset(self):\r self._tqdm=tqdm.tqdm(\r desc=\"Total progress\",\r total=state.job_count * state.sampling_steps,\r position=1,\r file=progress_print_out\r )\r \r def update(self):\r if not opts.multiple_tqdm:\r return\r if self._tqdm is None:\r self.reset()\r self._tqdm.update()\r \r def clear(self):\r if self._tqdm is not None:\r self._tqdm.close()\r self._tqdm=None\r \r \r total_tqdm=TotalTQDM()\r ","sourceWithComments":"import sys\r\nimport argparse\r\nimport json\r\nimport os\r\n\r\nimport gradio as gr\r\nimport torch\r\nimport tqdm\r\n\r\nimport modules.artists\r\nfrom modules.paths import script_path, sd_path\r\nfrom modules.devices import get_optimal_device\r\nimport modules.styles\r\nimport modules.interrogate\r\n\r\nconfig_filename = \"config.json\"\r\n\r\nsd_model_file = os.path.join(script_path, 'model.ckpt')\r\nif not os.path.exists(sd_model_file):\r\n    sd_model_file = \"models\/ldm\/stable-diffusion-v1\/model.ckpt\"\r\n\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument(\"--config\", type=str, default=os.path.join(sd_path, \"configs\/stable-diffusion\/v1-inference.yaml\"), help=\"path to config which constructs model\",)\r\nparser.add_argument(\"--ckpt\", type=str, default=os.path.join(sd_path, sd_model_file), help=\"path to checkpoint of model\",)\r\nparser.add_argument(\"--gfpgan-dir\", type=str, help=\"GFPGAN directory\", default=('.\/src\/gfpgan' if os.path.exists('.\/src\/gfpgan') else '.\/GFPGAN'))\r\nparser.add_argument(\"--gfpgan-model\", type=str, help=\"GFPGAN model file name\", default='GFPGANv1.3.pth')\r\nparser.add_argument(\"--no-half\", action='store_true', help=\"do not switch the model to 16-bit floats\")\r\nparser.add_argument(\"--no-progressbar-hiding\", action='store_true', help=\"do not hide progressbar in gradio UI (we hide it because it slows down ML if you have hardware accleration in browser)\")\r\nparser.add_argument(\"--max-batch-count\", type=int, default=16, help=\"maximum batch count value for the UI\")\r\nparser.add_argument(\"--embeddings-dir\", type=str, default='embeddings', help=\"embeddings directory for textual inversion (default: embeddings)\")\r\nparser.add_argument(\"--allow-code\", action='store_true', help=\"allow custom script execution from webui\")\r\nparser.add_argument(\"--medvram\", action='store_true', help=\"enable stable diffusion model optimizations for sacrificing a little speed for low VRM usage\")\r\nparser.add_argument(\"--lowvram\", action='store_true', help=\"enable stable diffusion model optimizations for sacrificing a lot of speed for very low VRM usage\")\r\nparser.add_argument(\"--always-batch-cond-uncond\", action='store_true', help=\"a workaround test; may help with speed if you use --lowvram\")\r\nparser.add_argument(\"--unload-gfpgan\", action='store_true', help=\"unload GFPGAN every time after processing images. Warning: seems to cause memory leaks\")\r\nparser.add_argument(\"--precision\", type=str, help=\"evaluate at this precision\", choices=[\"full\", \"autocast\"], default=\"autocast\")\r\nparser.add_argument(\"--share\", action='store_true', help=\"use share=True for gradio and make the UI accessible through their site (doesn't work for me but you might have better luck)\")\r\nparser.add_argument(\"--esrgan-models-path\", type=str, help=\"path to directory with ESRGAN models\", default=os.path.join(script_path, 'ESRGAN'))\r\nparser.add_argument(\"--opt-split-attention\", action='store_true', help=\"enable optimization that reduce vram usage by a lot for about 10%% decrease in performance\")\r\nparser.add_argument(\"--opt-split-attention-v1\", action='store_true', help=\"enable older version of --opt-split-attention optimization\")\r\nparser.add_argument(\"--listen\", action='store_true', help=\"launch gradio with 0.0.0.0 as server name, allowing to respond to network requests\")\r\nparser.add_argument(\"--port\", type=int, help=\"launch gradio with given server port, you need root\/admin rights for ports < 1024, defaults to 7860 if available\", default=None)\r\nparser.add_argument(\"--show-negative-prompt\", action='store_true', help=\"does not do anything\", default=False)\r\nparser.add_argument(\"--ui-config-file\", type=str, help=\"filename to use for ui configuration\", default=os.path.join(script_path, 'ui-config.json'))\r\n\r\ncmd_opts = parser.parse_args()\r\n\r\ndevice = get_optimal_device()\r\n\r\nbatch_cond_uncond = cmd_opts.always_batch_cond_uncond or not (cmd_opts.lowvram or cmd_opts.medvram)\r\nparallel_processing_allowed = not cmd_opts.lowvram and not cmd_opts.medvram\r\n\r\n\r\nclass State:\r\n    interrupted = False\r\n    job = \"\"\r\n    job_no = 0\r\n    job_count = 0\r\n    sampling_step = 0\r\n    sampling_steps = 0\r\n    current_latent = None\r\n    current_image = None\r\n    current_image_sampling_step = 0\r\n\r\n    def interrupt(self):\r\n        self.interrupted = True\r\n\r\n    def nextjob(self):\r\n        self.job_no += 1\r\n        self.sampling_step = 0\r\n        self.current_image_sampling_step = 0\r\n\r\n\r\nstate = State()\r\n\r\nartist_db = modules.artists.ArtistsDatabase(os.path.join(script_path, 'artists.csv'))\r\n\r\nstyles_filename = os.path.join(script_path, 'styles.csv')\r\nprompt_styles = modules.styles.load_styles(styles_filename)\r\n\r\ninterrogator = modules.interrogate.InterrogateModels(\"interrogate\")\r\n\r\nface_restorers = []\r\n\r\nclass Options:\r\n    class OptionInfo:\r\n        def __init__(self, default=None, label=\"\", component=None, component_args=None):\r\n            self.default = default\r\n            self.label = label\r\n            self.component = component\r\n            self.component_args = component_args\r\n\r\n    data = None\r\n    data_labels = {\r\n        \"outdir_samples\": OptionInfo(\"\", \"Output directory for images; if empty, defaults to two directories below\"),\r\n        \"outdir_txt2img_samples\": OptionInfo(\"outputs\/txt2img-images\", 'Output directory for txt2img images'),\r\n        \"outdir_img2img_samples\": OptionInfo(\"outputs\/img2img-images\", 'Output directory for img2img images'),\r\n        \"outdir_extras_samples\": OptionInfo(\"outputs\/extras-images\", 'Output directory for images from extras tab'),\r\n        \"outdir_grids\": OptionInfo(\"\", \"Output directory for grids; if empty, defaults to two directories below\"),\r\n        \"outdir_txt2img_grids\": OptionInfo(\"outputs\/txt2img-grids\", 'Output directory for txt2img grids'),\r\n        \"outdir_img2img_grids\": OptionInfo(\"outputs\/img2img-grids\", 'Output directory for img2img grids'),\r\n        \"save_to_dirs\": OptionInfo(False, \"When writing images, create a directory with name derived from the prompt\"),\r\n        \"grid_save_to_dirs\": OptionInfo(False, \"When writing grids, create a directory with name derived from the prompt\"),\r\n        \"save_to_dirs_prompt_len\": OptionInfo(10, \"When using above, how many words from prompt to put into directory name\", gr.Slider, {\"minimum\": 1, \"maximum\": 32, \"step\": 1}),\r\n        \"outdir_save\": OptionInfo(\"log\/images\", \"Directory for saving images using the Save button\"),\r\n        \"samples_save\": OptionInfo(True, \"Save indiviual samples\"),\r\n        \"samples_format\": OptionInfo('png', 'File format for individual samples'),\r\n        \"grid_save\": OptionInfo(True, \"Save image grids\"),\r\n        \"return_grid\": OptionInfo(True, \"Show grid in results for web\"),\r\n        \"grid_format\": OptionInfo('png', 'File format for grids'),\r\n        \"grid_extended_filename\": OptionInfo(False, \"Add extended info (seed, prompt) to filename when saving grid\"),\r\n        \"grid_only_if_multiple\": OptionInfo(True, \"Do not save grids consisting of one picture\"),\r\n        \"n_rows\": OptionInfo(-1, \"Grid row count; use -1 for autodetect and 0 for it to be same as batch size\", gr.Slider, {\"minimum\": -1, \"maximum\": 16, \"step\": 1}),\r\n        \"jpeg_quality\": OptionInfo(80, \"Quality for saved jpeg images\", gr.Slider, {\"minimum\": 1, \"maximum\": 100, \"step\": 1}),\r\n        \"export_for_4chan\": OptionInfo(True, \"If PNG image is larger than 4MB or any dimension is larger than 4000, downscale and save copy as JPG\"),\r\n        \"enable_pnginfo\": OptionInfo(True, \"Save text information about generation parameters as chunks to png files\"),\r\n        \"font\": OptionInfo(\"\", \"Font for image grids that have text\"),\r\n        \"enable_emphasis\": OptionInfo(True, \"Use (text) to make model pay more attention to text text and [text] to make it pay less attention\"),\r\n        \"save_txt\": OptionInfo(False, \"Create a text file next to every image with generation parameters.\"),\r\n        \"ESRGAN_tile\": OptionInfo(192, \"Tile size for upscaling. 0 = no tiling.\", gr.Slider, {\"minimum\": 0, \"maximum\": 512, \"step\": 16}),\r\n        \"ESRGAN_tile_overlap\": OptionInfo(8, \"Tile overlap, in pixels for upscaling. Low values = visible seam.\", gr.Slider, {\"minimum\": 0, \"maximum\": 48, \"step\": 1}),\r\n        \"random_artist_categories\": OptionInfo([], \"Allowed categories for random artists selection when using the Roll button\", gr.CheckboxGroup, {\"choices\": artist_db.categories()}),\r\n        \"upscale_at_full_resolution_padding\": OptionInfo(16, \"Inpainting at full resolution: padding, in pixels, for the masked region.\", gr.Slider, {\"minimum\": 0, \"maximum\": 128, \"step\": 4}),\r\n        \"show_progressbar\": OptionInfo(True, \"Show progressbar\"),\r\n        \"show_progress_every_n_steps\": OptionInfo(0, \"Show show image creation progress every N sampling steps. Set 0 to disable.\", gr.Slider, {\"minimum\": 0, \"maximum\": 32, \"step\": 1}),\r\n        \"multiple_tqdm\": OptionInfo(True, \"Add a second progress bar to the console that shows progress for an entire job. Broken in PyCharm console.\"),\r\n        \"face_restoration_model\": OptionInfo(None, \"Face restoration model\", gr.Radio, lambda: {\"choices\": [x.name() for x in face_restorers]}),\r\n        \"code_former_weight\": OptionInfo(0.5, \"CodeFormer weight parameter; 0 = maximum effect; 1 = minimum effect\", gr.Slider, {\"minimum\": 0, \"maximum\": 1, \"step\": 0.01}),\r\n        \"interrogate_keep_models_in_memory\": OptionInfo(True, \"Interrogate: keep models in VRAM\"),\r\n        \"interrogate_use_builtin_artists\": OptionInfo(True, \"Interrogate: use artists from artists.csv\"),\r\n        \"interrogate_clip_num_beams\": OptionInfo(1, \"Interrogate: num_beams for BLIP\", gr.Slider, {\"minimum\": 1, \"maximum\": 16, \"step\": 1}),\r\n        \"interrogate_clip_min_length\": OptionInfo(24, \"Interrogate: minimum descripton length (excluding artists, etc..)\", gr.Slider, {\"minimum\": 1, \"maximum\": 128, \"step\": 1}),\r\n        \"interrogate_clip_max_length\": OptionInfo(48, \"Interrogate: maximum descripton length\", gr.Slider, {\"minimum\": 1, \"maximum\": 256, \"step\": 1}),\r\n    }\r\n\r\n    def __init__(self):\r\n        self.data = {k: v.default for k, v in self.data_labels.items()}\r\n\r\n    def __setattr__(self, key, value):\r\n        if self.data is not None:\r\n            if key in self.data:\r\n                self.data[key] = value\r\n\r\n        return super(Options, self).__setattr__(key, value)\r\n\r\n    def __getattr__(self, item):\r\n        if self.data is not None:\r\n            if item in self.data:\r\n                return self.data[item]\r\n\r\n        if item in self.data_labels:\r\n            return self.data_labels[item].default\r\n\r\n        return super(Options, self).__getattribute__(item)\r\n\r\n    def save(self, filename):\r\n        with open(filename, \"w\", encoding=\"utf8\") as file:\r\n            json.dump(self.data, file)\r\n\r\n    def load(self, filename):\r\n        with open(filename, \"r\", encoding=\"utf8\") as file:\r\n            self.data = json.load(file)\r\n\r\n\r\nopts = Options()\r\nif os.path.exists(config_filename):\r\n    opts.load(config_filename)\r\n\r\nsd_upscalers = []\r\n\r\nsd_model = None\r\n\r\nprogress_print_out = sys.stdout\r\n\r\n\r\nclass TotalTQDM:\r\n    def __init__(self):\r\n        self._tqdm = None\r\n\r\n    def reset(self):\r\n        self._tqdm = tqdm.tqdm(\r\n            desc=\"Total progress\",\r\n            total=state.job_count * state.sampling_steps,\r\n            position=1,\r\n            file=progress_print_out\r\n        )\r\n\r\n    def update(self):\r\n        if not opts.multiple_tqdm:\r\n            return\r\n        if self._tqdm is None:\r\n            self.reset()\r\n        self._tqdm.update()\r\n\r\n    def clear(self):\r\n        if self._tqdm is not None:\r\n            self._tqdm.close()\r\n            self._tqdm = None\r\n\r\n\r\ntotal_tqdm = TotalTQDM()\r\n"},"\/modules\/ui.py":{"changes":[{"diff":"\n         info = opts.data_labels[key]\r\n         t = type(info.default)\r\n \r\n+        args = info.component_args() if callable(info.component_args) else info.component_args\r\n+\r\n         if info.component is not None:\r\n-            args = info.component_args() if callable(info.component_args) else info.component_args\r\n-            item = info.component(label=info.label, value=fun, **(args or {}))\r\n+            comp = info.component\r\n         elif t == str:\r\n-            item = gr.Textbox(label=info.label, value=fun, lines=1)\r\n+            comp = gr.Textbox\r\n         elif t == int:\r\n-            item = gr.Number(label=info.label, value=fun)\r\n+            comp = gr.Number\r\n         elif t == bool:\r\n-            item = gr.Checkbox(label=info.label, value=fun)\r\n+            comp = gr.Checkbox\r\n         else:\r\n             raise Exception(f'bad options item type: {str(t)} for key {key}')\r\n \r\n-        return item\r\n+        return comp(label=info.label, value=fun, **(args or {}))\r\n \r\n     components = []\r\n     keys = list(opts.data_labels.keys())\r\n","add":7,"remove":6,"filename":"\/modules\/ui.py","badparts":["            args = info.component_args() if callable(info.component_args) else info.component_args\r","            item = info.component(label=info.label, value=fun, **(args or {}))\r","            item = gr.Textbox(label=info.label, value=fun, lines=1)\r","            item = gr.Number(label=info.label, value=fun)\r","            item = gr.Checkbox(label=info.label, value=fun)\r","        return item\r"],"goodparts":["        args = info.component_args() if callable(info.component_args) else info.component_args\r","\r","            comp = info.component\r","            comp = gr.Textbox\r","            comp = gr.Number\r","            comp = gr.Checkbox\r","        return comp(label=info.label, value=fun, **(args or {}))\r"]}]}},"msg":"Add --hide-ui-dir-config command line flag\n\nAdds `--hide-ui-dir-config` flag to disable editing directory configs from the web UI. This can be set to prevent users from setting the directory to somewhere they shouldn't, for public (or semi-public) interfaces.\n\nDirectories are still read from config.json, so the server admin can still set them in the web UI and then relaunch with the hide flag, or edit the config manually.\n\nAlso:\n- fix OptionInfo `component_args` keyword argument not being read if `component` isn't also set\n- ensure that hidden settings aren't still read from the web UI (otherwise they could still be changed by tampering with the interface)"}},"https:\/\/github.com\/Darrensiriram\/goodchain":{"9daf674c5da46333cfb3bfcbb2308a013de12335":{"url":"https:\/\/api.github.com\/repos\/Darrensiriram\/goodchain\/commits\/9daf674c5da46333cfb3bfcbb2308a013de12335","html_url":"https:\/\/github.com\/Darrensiriram\/goodchain\/commit\/9daf674c5da46333cfb3bfcbb2308a013de12335","message":"created backup for hashing, to check for tampering.","sha":"9daf674c5da46333cfb3bfcbb2308a013de12335","keyword":"tampering check","diff":"diff --git a\/actions\/print_menu.py b\/actions\/print_menu.py\nindex d6a6aef..2411bfc 100644\n--- a\/actions\/print_menu.py\n+++ b\/actions\/print_menu.py\n@@ -5,6 +5,7 @@\n from database_actions import login\n import os\n from time import sleep\n+from utils import helper\n \n poolPath = 'data\/pool.dat'\n choiceList = (\"1\", \"2\", \"3\", '4', '5', '6', '7')\n@@ -33,7 +34,7 @@ def actions(auth_user, connection):\n         if response not in choiceList:\n             print(\"Please select a valid option\")\n             sleep(2)\n-        elif int(response) == 1:\n+        elif int(response) == 1 and helper.compare_hashes('data\/block.dat'):\n             try:\n                 chosen_user = input(\"please enter the username: \")\n                 amount = int(input(\"please specify the coin amount: \"))\n@@ -62,16 +63,16 @@ def actions(auth_user, connection):\n                     transferCoinsobject.save_transaction_in_the_pool(tx)\n                     print(\"Coins have been transferred\")\n                 continue\n-        elif int(response) == 2:\n+        elif int(response) == 2 and helper.compare_hashes('data\/block.dat'):\n             print(f\"Your current balance is: {checkBalanceObject.current_balance()}\")\n             sleep(2)\n             continue\n-        elif int(response) == 3:\n+        elif int(response) == 3 and helper.compare_hashes('data\/block.dat'):\n             print(\"Explore the chain\")\n             mining_actions.mine_actions.explore_chain()\n             sleep(2)\n             continue\n-        elif int(response) == 4:\n+        elif int(response) == 4 and helper.compare_hashes('data\/block.dat'):\n             pool = []\n             loadfile = open(poolPath, \"rb\")\n             try:\n@@ -86,7 +87,7 @@ def actions(auth_user, connection):\n             else:\n                 print(\"Pool is empty\")\n             sleep(4)\n-        elif int(response) == 5:\n+        elif int(response) == 5 and helper.compare_hashes('data\/block.dat'):\n             tcObject = transferCoins.transfercoins(connection, auth_user)\n             if transferCoins.transfercoins.cancel_transaction_in_the_pool(tcObject) == False:\n                 print(\"Pool is empty\")\n@@ -95,7 +96,7 @@ def actions(auth_user, connection):\n                 transferCoins.transfercoins.cancel_transaction_in_the_pool(tcObject)\n             sleep(2)\n             continue\n-        elif int(response) == 6:\n+        elif int(response) == 6 and helper.compare_hashes('data\/block.dat'):\n             loginObject = login.login(connection)\n             if transferCoins.transfercoins.get_total_transaction_in_pool() < 5:\n                 print(\"There are not enough transaction in the pool.\")\n@@ -133,6 +134,7 @@ def actions(auth_user, connection):\n                                 transferCoinsobject.save_transaction_in_the_pool(tx)\n \n                                 checkBalanceObject.update_balance()\n+                                helper.create_hash('data\/block.dat')\n                                 break\n                     except:\n                         print(\"Oops That is not a valid option\")\ndiff --git a\/goodchain.py b\/goodchain.py\nindex b9a815a..8f85168 100644\n--- a\/goodchain.py\n+++ b\/goodchain.py\n@@ -8,8 +8,9 @@\n from database_actions import signup as s\n from database_actions import connectionSQL as dbcreate\n import pathlib\n-connection = sqlite3.Connection('database_actions\/goodchain.db')\n+from utils import helper\n \n+connection = sqlite3.Connection('database_actions\/goodchain.db')\n \n if platform.system() == 'Darwin':\n     if os.path.exists('data'):\n@@ -34,6 +35,7 @@ def print_public_menu():\n     4 - Exit\n     \"\"\")\n \n+\n def startMenu():\n     while True:\n         print_public_menu()\n@@ -47,8 +49,8 @@ def startMenu():\n             loginUser = login.login(connection, username, password)\n             loginUser.loginUser()\n         elif int(response) == 2:\n-             mine_actions.explore_chain()\n-             sleep(2)\n+            mine_actions.explore_chain()\n+            sleep(2)\n         elif int(response) == 3:\n             dbcreate.createDatabase(connection)\n             username = input(\"Fill in your username please: \")\n@@ -61,4 +63,9 @@ def startMenu():\n             exit(\"Thank you for using the goodchain\")\n \n \n+if os.path.exists('backup'):\n+    pass\n+else:\n+    helper.create_hash('data\/block.dat')\n+\n startMenu()\n","files":{"\/actions\/print_menu.py":{"changes":[{"diff":"\n         if response not in choiceList:\n             print(\"Please select a valid option\")\n             sleep(2)\n-        elif int(response) == 1:\n+        elif int(response) == 1 and helper.compare_hashes('data\/block.dat'):\n             try:\n                 chosen_user = input(\"please enter the username: \")\n                 amount = int(input(\"please specify the coin amount: \"))\n","add":1,"remove":1,"filename":"\/actions\/print_menu.py","badparts":["        elif int(response) == 1:"],"goodparts":["        elif int(response) == 1 and helper.compare_hashes('data\/block.dat'):"]},{"diff":"\n                     transferCoinsobject.save_transaction_in_the_pool(tx)\n                     print(\"Coins have been transferred\")\n                 continue\n-        elif int(response) == 2:\n+        elif int(response) == 2 and helper.compare_hashes('data\/block.dat'):\n             print(f\"Your current balance is: {checkBalanceObject.current_balance()}\")\n             sleep(2)\n             continue\n-        elif int(response) == 3:\n+        elif int(response) == 3 and helper.compare_hashes('data\/block.dat'):\n             print(\"Explore the chain\")\n             mining_actions.mine_actions.explore_chain()\n             sleep(2)\n             continue\n-        elif int(response) == 4:\n+        elif int(response) == 4 and helper.compare_hashes('data\/block.dat'):\n             pool = []\n             loadfile = open(poolPath, \"rb\")\n             try:\n","add":3,"remove":3,"filename":"\/actions\/print_menu.py","badparts":["        elif int(response) == 2:","        elif int(response) == 3:","        elif int(response) == 4:"],"goodparts":["        elif int(response) == 2 and helper.compare_hashes('data\/block.dat'):","        elif int(response) == 3 and helper.compare_hashes('data\/block.dat'):","        elif int(response) == 4 and helper.compare_hashes('data\/block.dat'):"]},{"diff":"\n             else:\n                 print(\"Pool is empty\")\n             sleep(4)\n-        elif int(response) == 5:\n+        elif int(response) == 5 and helper.compare_hashes('data\/block.dat'):\n             tcObject = transferCoins.transfercoins(connection, auth_user)\n             if transferCoins.transfercoins.cancel_transaction_in_the_pool(tcObject) == False:\n                 print(\"Pool is empty\")\n","add":1,"remove":1,"filename":"\/actions\/print_menu.py","badparts":["        elif int(response) == 5:"],"goodparts":["        elif int(response) == 5 and helper.compare_hashes('data\/block.dat'):"]},{"diff":"\n                 transferCoins.transfercoins.cancel_transaction_in_the_pool(tcObject)\n             sleep(2)\n             continue\n-        elif int(response) == 6:\n+        elif int(response) == 6 and helper.compare_hashes('data\/block.dat'):\n             loginObject = login.login(connection)\n             if transferCoins.transfercoins.get_total_transaction_in_pool() < 5:\n                 print(\"There are not enough transaction in the pool.\")\n","add":1,"remove":1,"filename":"\/actions\/print_menu.py","badparts":["        elif int(response) == 6:"],"goodparts":["        elif int(response) == 6 and helper.compare_hashes('data\/block.dat'):"]}],"source":"\nimport pickle from actions import check_balance from actions import transferCoins from actions import mining_actions from database_actions import login import os from time import sleep poolPath='data\/pool.dat' choiceList=(\"1\", \"2\", \"3\", '4', '5', '6', '7') def print_menu_loggedIn(auth_user, connection): cur=connection.cursor() result=cur.execute('SELECT username FROM users WHERE id=?',(auth_user,)).fetchone() print(f\"Username:{result[0]}\") print(\"\"\" 1 -Transfer Coins 2 -Check the Balance 3 -Explore the Chain 4 -Check the Pool 5 -Cancel a transaction 6 -Mine a Block 7 -Log out \"\"\") def actions(auth_user, connection): checkBalanceObject=check_balance.balance(connection, auth_user) while True: print_menu_loggedIn(auth_user, connection) response=input(\"What would u like to do? \\n\") if response not in choiceList: print(\"Please select a valid option\") sleep(2) elif int(response)==1: try: chosen_user=input(\"please enter the username: \") amount=int(input(\"please specify the coin amount: \")) transactionfee=int(input(\"please enter a transaction fee: \")) current_balance=checkBalanceObject.get_current_balance_from_user(chosen_user) if not current_balance: print(\"There is no transaction made, Username is not found\") sleep(2) continue except: print(\"Invalid option\") else: current_balance=checkBalanceObject.current_balance() if amount < transactionfee: print(\"Oops ur amount is smaller then transaction fee\") print(\"Try again....\") sleep(2) elif current_balance < amount: print(\"Oops ur balance is smaller then than the amount specified\") print(\"Try again....\") sleep(2) else: transferCoinsobject=transferCoins.transfercoins(connection, auth_user, chosen_user, amount, transactionfee) tx=transferCoinsobject.createTx(amount, transactionfee) transferCoinsobject.save_transaction_in_the_pool(tx) print(\"Coins have been transferred\") continue elif int(response)==2: print(f\"Your current balance is:{checkBalanceObject.current_balance()}\") sleep(2) continue elif int(response)==3: print(\"Explore the chain\") mining_actions.mine_actions.explore_chain() sleep(2) continue elif int(response)==4: pool=[] loadfile=open(poolPath, \"rb\") try: while True: data=pickle.load(loadfile) pool.append(data) except EOFError: pass if len(pool) !=0: print(pool) print(f\"Total transactions in the pool:{len(pool)}\") else: print(\"Pool is empty\") sleep(4) elif int(response)==5: tcObject=transferCoins.transfercoins(connection, auth_user) if transferCoins.transfercoins.cancel_transaction_in_the_pool(tcObject)==False: print(\"Pool is empty\") else: print(\"Cancel a transaction\\n\") transferCoins.transfercoins.cancel_transaction_in_the_pool(tcObject) sleep(2) continue elif int(response)==6: loginObject=login.login(connection) if transferCoins.transfercoins.get_total_transaction_in_pool() < 5: print(\"There are not enough transaction in the pool.\") print(f\"There are currently{transferCoins.transfercoins.get_total_transaction_in_pool()} in the pool.\") sleep(2) elif loginObject.get_current_time(): print(\"Whoops u can not mine so fast in a row.\") sleep(2) else: print(\"Let's mine a block\") specifyBlocks=mining_actions.mine_actions.load_all_transaction_per_block() i=0 for x in specifyBlocks: print(f\"[{i}]:{x}\") i +=1 while True: try: chosenInput=int(input(\"Please choose a block of transactions: \")) if chosenInput==None: break else: if chosenInput < len(specifyBlocks): mining_actions.mine_actions.mine_block(specifyBlocks, chosenInput) mining_actions.mine_actions.save_to_chain(specifyBlocks[0][0]) mining_actions.mine_actions.clear_transaction_after_mining(specifyBlocks[0][0]) loginObject.set_default_value_connectivity() loginObject.update_time_when_mine() transferCoinsobject=transferCoins.transfercoins(connection, auth_user, \"system_user\", 25, 0) tx=transferCoinsobject.createTx(25, 0) transferCoinsobject.save_transaction_in_the_pool(tx) checkBalanceObject.update_balance() break except: print(\"Oops That is not a valid option\") sleep(2) continue elif int(response)==7: print(\"Log out\") break ","sourceWithComments":"import pickle\nfrom actions import check_balance\nfrom actions import transferCoins\nfrom actions import mining_actions\nfrom database_actions import login\nimport os\nfrom time import sleep\n\npoolPath = 'data\/pool.dat'\nchoiceList = (\"1\", \"2\", \"3\", '4', '5', '6', '7')\n\n\ndef print_menu_loggedIn(auth_user, connection):\n    cur = connection.cursor()\n    result = cur.execute('SELECT username FROM users WHERE id = ?', (auth_user,)).fetchone()\n    print(f\"Username: {result[0]}\")\n    print(\"\"\"\n    1 - Transfer Coins\n    2 - Check the Balance\n    3 - Explore the Chain\n    4 - Check the Pool \n    5 - Cancel a transaction\n    6 - Mine a Block\n    7 - Log out\n    \"\"\")\n\n\ndef actions(auth_user, connection):\n    checkBalanceObject = check_balance.balance(connection, auth_user)\n    while True:\n        print_menu_loggedIn(auth_user, connection)\n        response = input(\"What would u like to do? \\n\")\n        if response not in choiceList:\n            print(\"Please select a valid option\")\n            sleep(2)\n        elif int(response) == 1:\n            try:\n                chosen_user = input(\"please enter the username: \")\n                amount = int(input(\"please specify the coin amount: \"))\n                transactionfee = int(input(\"please enter a transaction fee: \"))\n                current_balance = checkBalanceObject.get_current_balance_from_user(chosen_user)\n                if not current_balance:\n                    print(\"There is no transaction made, Username is not found\")\n                    sleep(2)\n                    continue\n            except:\n                print(\"Invalid option\")\n            else:\n                current_balance = checkBalanceObject.current_balance()\n                if amount < transactionfee:\n                    print(\"Oops ur amount is smaller then transaction fee\")\n                    print(\"Try again....\")\n                    sleep(2)\n                elif current_balance < amount:\n                    print(\"Oops ur balance is smaller then than the amount specified\")\n                    print(\"Try again....\")\n                    sleep(2)\n                else:\n                    transferCoinsobject = transferCoins.transfercoins(connection, auth_user, chosen_user, amount,\n                                                                      transactionfee)\n                    tx = transferCoinsobject.createTx(amount, transactionfee)\n                    transferCoinsobject.save_transaction_in_the_pool(tx)\n                    print(\"Coins have been transferred\")\n                continue\n        elif int(response) == 2:\n            print(f\"Your current balance is: {checkBalanceObject.current_balance()}\")\n            sleep(2)\n            continue\n        elif int(response) == 3:\n            print(\"Explore the chain\")\n            mining_actions.mine_actions.explore_chain()\n            sleep(2)\n            continue\n        elif int(response) == 4:\n            pool = []\n            loadfile = open(poolPath, \"rb\")\n            try:\n                while True:\n                    data = pickle.load(loadfile)\n                    pool.append(data)\n            except EOFError:\n                pass\n            if len(pool) != 0:\n                print(pool)\n                print(f\"Total transactions in the pool: {len(pool)}\")\n            else:\n                print(\"Pool is empty\")\n            sleep(4)\n        elif int(response) == 5:\n            tcObject = transferCoins.transfercoins(connection, auth_user)\n            if transferCoins.transfercoins.cancel_transaction_in_the_pool(tcObject) == False:\n                print(\"Pool is empty\")\n            else:\n                print(\"Cancel a transaction\\n\")\n                transferCoins.transfercoins.cancel_transaction_in_the_pool(tcObject)\n            sleep(2)\n            continue\n        elif int(response) == 6:\n            loginObject = login.login(connection)\n            if transferCoins.transfercoins.get_total_transaction_in_pool() < 5:\n                print(\"There are not enough transaction in the pool.\")\n                print(f\"There are currently {transferCoins.transfercoins.get_total_transaction_in_pool()} in the pool.\")\n                sleep(2)\n            # elif loginObject.get_current_connected_count()[0] < 4:\n            #     print(\"sorry not enough members have valided the chain\")\n            #     sleep(2)\n            elif loginObject.get_current_time():\n                print(\"Whoops u can not mine so fast in a row.\")\n                sleep(2)\n            else:\n                print(\"Let's mine a block\")\n                specifyBlocks = mining_actions.mine_actions.load_all_transaction_per_block()\n                i = 0\n                for x in specifyBlocks:\n                    print(f\"[{i}] : {x}\")\n                    i += 1\n                while True:\n                    try:\n                        chosenInput = int(input(\"Please choose a block of transactions: \"))\n                        if chosenInput == None:\n                            break\n                        else:\n                            if chosenInput < len(specifyBlocks):\n                                mining_actions.mine_actions.mine_block(specifyBlocks, chosenInput)\n                                mining_actions.mine_actions.save_to_chain(specifyBlocks[0][0])\n                                mining_actions.mine_actions.clear_transaction_after_mining(specifyBlocks[0][0])\n                                loginObject.set_default_value_connectivity()\n                                loginObject.update_time_when_mine()\n                                transferCoinsobject = transferCoins.transfercoins(connection, auth_user, \"system_user\",\n                                                                                  25,\n                                                                                  0)\n                                tx = transferCoinsobject.createTx(25, 0)\n                                transferCoinsobject.save_transaction_in_the_pool(tx)\n\n                                checkBalanceObject.update_balance()\n                                break\n                    except:\n                        print(\"Oops That is not a valid option\")\n                sleep(2)\n                # print(mining_actions.mine_actions.get_block_chain())\n            continue\n        elif int(response) == 7:\n            print(\"Log out\")\n            break\n"},"\/goodchain.py":{"changes":[{"diff":"\n from database_actions import signup as s\n from database_actions import connectionSQL as dbcreate\n import pathlib\n-connection = sqlite3.Connection('database_actions\/goodchain.db')\n+from utils import helper\n \n+connection = sqlite3.Connection('database_actions\/goodchain.db')\n \n if platform.system() == 'Darwin':\n     if os.path.exists('data'):\n","add":2,"remove":1,"filename":"\/goodchain.py","badparts":["connection = sqlite3.Connection('database_actions\/goodchain.db')"],"goodparts":["from utils import helper","connection = sqlite3.Connection('database_actions\/goodchain.db')"]},{"diff":"\n             loginUser = login.login(connection, username, password)\n             loginUser.loginUser()\n         elif int(response) == 2:\n-             mine_actions.explore_chain()\n-             sleep(2)\n+            mine_actions.explore_chain()\n+            sleep(2)\n         elif int(response) == 3:\n             dbcreate.createDatabase(connection)\n             username = input(\"Fill in your username please: \")\n","add":2,"remove":2,"filename":"\/goodchain.py","badparts":["             mine_actions.explore_chain()","             sleep(2)"],"goodparts":["            mine_actions.explore_chain()","            sleep(2)"]}],"source":"\nimport os import platform import sqlite3 from getpass import getpass from actions.mining_actions import * from database_actions import login from database_actions import signup as s from database_actions import connectionSQL as dbcreate import pathlib connection=sqlite3.Connection('database_actions\/goodchain.db') if platform.system()=='Darwin': if os.path.exists('data'): os.system(\"touch data\/block.dat\") os.system(\"touch data\/pool.dat\") else: os.mkdir('data') else: pathlib.Path('data').mkdir(parents=True, exist_ok=True) pathlib.Path('data\/block.dat').touch() pathlib.Path('data\/pool.dat').touch() choiceList=(\"1\", \"2\", \"3\", '4') def print_public_menu(): print(\"\"\" Public Menu Menu for sign up in Goodchain 1 -Login 2 -Explore the blockchain 3 -Sign up 4 -Exit \"\"\") def startMenu(): while True: print_public_menu() response=input(\"What would u like to do? \\n \") if response not in choiceList: print(\"Please select a valid option\") sleep(2) elif int(response)==1: username=input(\"Fill in your username please: \") password=getpass(\"Please fill your password in: \") loginUser=login.login(connection, username, password) loginUser.loginUser() elif int(response)==2: mine_actions.explore_chain() sleep(2) elif int(response)==3: dbcreate.createDatabase(connection) username=input(\"Fill in your username please: \") password=getpass(\"Please fill your password in: \") coins=50 signupUser=s.signUp(connection=connection, username=username, password=password, coins=coins) s.signUp.signUpUser(signupUser) signupUser.sign_up_system_user() elif int(response)==4: exit(\"Thank you for using the goodchain\") startMenu() ","sourceWithComments":"import os\nimport platform\n# os.system(\"pip install -r requirements.txt\")\nimport sqlite3\nfrom getpass import getpass\nfrom actions.mining_actions import *\nfrom database_actions import login\nfrom database_actions import signup as s\nfrom database_actions import connectionSQL as dbcreate\nimport pathlib\nconnection = sqlite3.Connection('database_actions\/goodchain.db')\n\n\nif platform.system() == 'Darwin':\n    if os.path.exists('data'):\n        os.system(\"touch data\/block.dat\")\n        os.system(\"touch data\/pool.dat\")\n    else:\n        os.mkdir('data')\nelse:\n    pathlib.Path('data').mkdir(parents=True, exist_ok=True)\n    pathlib.Path('data\/block.dat').touch()\n    pathlib.Path('data\/pool.dat').touch()\n\nchoiceList = (\"1\", \"2\", \"3\", '4')\ndef print_public_menu():\n    print(\"\"\"\n    Public Menu \n    Menu for sign up in Goodchain\n\n    1 - Login\n    2 - Explore the blockchain\n    3 - Sign up\n    4 - Exit\n    \"\"\")\n\ndef startMenu():\n    while True:\n        print_public_menu()\n        response = input(\"What would u like to do? \\n \")\n        if response not in choiceList:\n            print(\"Please select a valid option\")\n            sleep(2)\n        elif int(response) == 1:\n            username = input(\"Fill in your username please: \")\n            password = getpass(\"Please fill your password in: \")\n            loginUser = login.login(connection, username, password)\n            loginUser.loginUser()\n        elif int(response) == 2:\n             mine_actions.explore_chain()\n             sleep(2)\n        elif int(response) == 3:\n            dbcreate.createDatabase(connection)\n            username = input(\"Fill in your username please: \")\n            password = getpass(\"Please fill your password in: \")\n            coins = 50\n            signupUser = s.signUp(connection=connection, username=username, password=password, coins=coins)\n            s.signUp.signUpUser(signupUser)\n            signupUser.sign_up_system_user()\n        elif int(response) == 4:\n            exit(\"Thank you for using the goodchain\")\n\n\nstartMenu()\n"}},"msg":"created backup for hashing, to check for tampering."}},"https:\/\/github.com\/KenAlupit\/Ballpit-Enrollment-System":{"b8e6c0f53a6c61434651b1a6c2ee404dab8039df":{"url":"https:\/\/api.github.com\/repos\/KenAlupit\/Ballpit-Enrollment-System\/commits\/b8e6c0f53a6c61434651b1a6c2ee404dab8039df","html_url":"https:\/\/github.com\/KenAlupit\/Ballpit-Enrollment-System\/commit\/b8e6c0f53a6c61434651b1a6c2ee404dab8039df","message":"Removed tamper check, optimize code, add ID Generator, and add modification for the Student Profile when done paying for tuition","sha":"b8e6c0f53a6c61434651b1a6c2ee404dab8039df","keyword":"tampering check","diff":"diff --git a\/Enrollment System.py b\/Enrollment System.py\nindex 4bb596e..4b332dc 100644\n--- a\/Enrollment System.py\t\n+++ b\/Enrollment System.py\t\n@@ -17,17 +17,37 @@ def Reference_Number_Generator():\n                     randomReference = ''.join(random.choices(string.ascii_letters, k=8))\n     return randomReference\n \n-def Search_Subjects(subject, subjectFile):\n-    with open(subjectFile, 'r') as file:\n+def ID_Number_Generator():\n+    #Creates the random enrollment reference number\n+    randomID = ''.join(random.choices(string.digits, k=2)) + \"-\" + ''.join(random.choices(string.digits, k=4))\n+    with open('StudentProfile.csv', 'r') as file:\n+        reader = csv.reader(file)\n+        for row in reader:\n+             if sum(1 for row in reader) > 0:\n+                if randomID == row[1]:\n+                    randomID = ''.join(random.choices(string.digits, k=2)) + \"-\" + ''.join(random.choices(string.digits, k=4))\n+    return randomID\n+\n+def Semester_Picker(semesterFile, course):\n+    with open(semesterFile, 'r') as file:\n         reader = csv.reader(file)\n         totalTuition = 0\n         for row in reader:\n-            if row[1] == \"General\" or row[1] == subject:\n+            if row[1] == \"General\" or row[1] == course:\n                 totalTuition += int(row[2])\n                 print(row[0], row[1], row[2])\n-    print(\"Total tuiton fee: \", totalTuition)\n     return totalTuition\n \n+def Search_Subjects(course, semester):\n+    match semester:\n+        case \"1st Sem\":\n+            tuitionFee = Semester_Picker('1stSemesterSubjects.csv', course)\n+        case \"2nd Sem\":\n+            tuitionFee = Semester_Picker('2ndSemesterSubjects.csv', course)\n+    print(\"Total tuiton fee: \", tuitionFee)\n+    return tuitionFee\n+\n+\n def Backup():\n     #Backs up every CSV file to a hidden backup folder\n     shutil.copy('1stSemesterSubjects.csv', '.Backup')\n@@ -38,13 +58,13 @@ def Backup():\n def Recover(file):\n     #Recovers CSV files from the hidden backup folder\n     match file:\n-        case \"1stSemesterSubjects.csv\":\n+        case '1stSemesterSubjects.csv':\n             shutil.copy('.Backup\/1stSemesterSubjects.csv', os.getcwd())\n-        case \"2ndSemesterSubjects.csv\":\n+        case '2ndSemesterSubjects.csv':\n             shutil.copy('.Backup\/2ndSemesterSubjects.csv', os.getcwd())\n-        case \"EnrollmentReferenceNumbers.csv\":\n+        case 'EnrollmentReferenceNumbers.csv':\n             shutil.copy('.Backup\/EnrollmentReferenceNumbers.csv', os.getcwd())\n-        case \"StudentProfile.csv\":\n+        case 'StudentProfile.csv':\n             shutil.copy('.Backup\/StudentProfile.csv', os.getcwd())\n         case _:\n             shutil.copy('.Backup\/1stSemesterSubjects.csv', os.getcwd())\n@@ -61,27 +81,34 @@ def File_Check_And_Recover(file):\n         else:\n              Recover(file)\n \n-def Tamper_Check(newFile):\n-    # Checks if the file content has been tampered with and returns a boolean\n-    # \n-    backupFile = '.Backup\/' + newFile\n-    tampered = False\n-    with open(backupFile, 'r') as file:\n-        reader = csv.reader(file)\n-        backupFileRowCount = sum(1 for row in reader)\n-    with open(newFile, 'r') as file:\n-        reader = csv.reader(file)\n-        newFileRowCount = sum(1 for row in reader)\n-    if backupFileRowCount < newFileRowCount:\n-        tampered = True\n-        Recover(backupFile)\n-    return tampered\n-\n def Save_To_CSV(CSVfile, data):\n     with open(CSVfile, 'a', newline='') as file:\n         writer = csv.writer(file)\n         writer.writerow(data)\n-    \n+\n+def Payment(reference):\n+    invalidPayment = True\n+    with open('EnrollmentReferenceNumbers.csv', 'r') as file:\n+        reader = csv.reader(file)\n+        for row in reader:\n+            if reference == row[0]:\n+                while invalidPayment:\n+                    userPayment = int((input(\"Input your payment: \")))                            \n+                    if userPayment < int(row[1]):\n+                        invalidPayment = True\n+                        print(\"Payment error! Your payment ips insufficient!\")     \n+                    elif userPayment > int(row[1]):\n+                        invalidPayment = False\n+                        print(\"Payment Successful! Here is your change: \", userPayment - int(row[1]))                            \n+                    else:\n+                        invalidPayment = False\n+                        print(\"Payment Successful! \")  \n+    return not invalidPayment \n+\n+def Overwrite(file, data):\n+    with open(file, 'w', newline='') as file:\n+        writer = csv.writer(file)\n+        writer.writerows(data)\n \n def Main_Menu():\n     studentProfile = []\n@@ -171,62 +198,69 @@ def Main_Menu():\n                         case _:\n                             invalidSemester = True\n                             print(\"Invalid Input\\n\")\n-                if studentProfile[6] == \"1st Sem\":\n-                    match studentProfile[5]:\n-                        case \"BSCS\":\n-                            tuitionFee = Search_Subjects(\"BSCS\", '1stSemesterSubjects.csv')\n-                        case \"BSEMC\":\n-                            tuitionFee = Search_Subjects(\"BSEMC\", '1stSemesterSubjects.csv')\n-                        case \"BMMA\":\n-                            tuitionFee = Search_Subjects(\"BMMA\", '1stSemesterSubjects.csv')\n-                if studentProfile[6] == \"2nd Sem\":\n-                    match studentProfile[5]:\n-                        case \"BSCS\":\n-                            tuitionFee = Search_Subjects(\"BSCS\", '2ndSemesterSubjects.csv')\n-                        case \"BSEMC\":\n-                            tuitionFee = Search_Subjects(\"BSEMC\", '2ndSemesterSubjects.csv')\n-                        case \"BMMA\":\n-                            tuitionFee = Search_Subjects(\"BMMA\", '2ndSemesterSubjects.csv')\n+                tuitionFee = Search_Subjects(studentProfile[5], studentProfile[6])\n                 enrollmentReferenceNumber = [Reference_Number_Generator(), tuitionFee]\n                 studentProfile.append(\"Not Enrolled\")\n                 studentProfile.append(enrollmentReferenceNumber[0])\n                 print(\"Your enrollment reference number to be presented for the payment: \" + enrollmentReferenceNumber[0])\n \n                 Save_To_CSV('EnrollmentReferenceNumbers.csv', enrollmentReferenceNumber)\n-                if Tamper_Check('EnrollmentReferenceNumbers.csv'):\n-                    Save_To_CSV('EnrollmentReferenceNumbers.csv', enrollmentReferenceNumber)\n-\n-                with open('StudentProfile.csv', 'w', newline='') as file:\n-                    writer = csv.writer(file)\n-                    writer.writerow(studentProfile)\n-\n-                with open('StudentProfile.csv', 'r') as file:\n-                    reader = csv.reader(file)\n-                    for row in reader:\n-                        print(row)\n-\n-                with open('EnrollmentReferenceNumbers.csv', 'r') as file:\n-                    reader = csv.reader(file)\n-                    for row in reader:\n-                        print(row)\n+                Recover('StudentProfile.csv')\n+                Save_To_CSV('StudentProfile.csv', studentProfile)\n                 Backup()\n                 Main_Menu()\n                 \n             case 'p' | 'P':\n                 invalidInput = False\n-                userReference = (input(\"Input your reference number: \"))\n-                \n+                invalidReference = True\n                 with open('EnrollmentReferenceNumbers.csv', 'r') as file:\n                     reader = csv.reader(file)\n-                    for row in reader:                       \n-                        if row[0] == userReference: \n-                            print(\"Your total amount balance to pay is:\", row[1])\n-                            userPayment = int((input(\"Input your payment: \")))                            \n-                            if userPayment < int(row[1]):\n-                                print(\"Payment error! Your payment ips insufficient!\")     \n-                            elif userPayment > int(row[1]):\n-                                print(\"Payment Successful! Here is your change: \", userPayment - int(row[1]))                            \n-                            else: print(\"Payment Successful! \")                \n+                    match sum(1 for row in reader):\n+                        case 0:\n+                            print(\"Data unavailable please enroll first\")\n+                            Main_Menu()\n+                        case _:\n+                            while invalidReference:\n+                                userReference = (input(\"Input your reference number: \"))\n+                                with open('EnrollmentReferenceNumbers.csv', 'r') as file:\n+                                    reader = csv.reader(file)\n+                                    for row in reader:                  \n+                                        if row[0] == userReference: \n+                                            invalidReference = False\n+                                with open('StudentProfile.csv', 'r') as file:\n+                                    reader = csv.reader(file)\n+                                    for row in reader:                  \n+                                        if row[8] == userReference:\n+                                            if row[1] != \"0\":\n+                                                print (\"Student Name: \" + row[0] + \" \" + row[1] + \" \" + row[2])\n+                                            else:\n+                                                print (\"Student Name: \" + row[0] + \" \" + row[2])\n+                                            Search_Subjects(row[5], row[6])\n+                                            if Payment(userReference):\n+                                                print(\"You are now succesfully enrolled!\")\n+                                                userId = ID_Number_Generator()\n+                                                print(\"Your student ID: \" + userId)\n+                                                lines = list()\n+                                                with open('StudentProfile.csv', 'r') as file:\n+                                                    reader = csv.reader(file)\n+                                                    for row in reader:\n+                                                        lines.append(row)\n+                                                        if row[8] == userReference:\n+                                                            row[8] = userId\n+                                                            row[7] = \"Enrolled\"\n+                                                Overwrite('StudentProfile.csv', lines)\n+                                                lines = list()\n+                                                with open('EnrollmentReferenceNumbers.csv', 'r') as file:\n+                                                    reader = csv.reader(file)\n+                                                    for row in reader:\n+                                                        lines.append(row)\n+                                                        if row[0] == userReference:\n+                                                            lines.remove(row)\n+                                                Overwrite('EnrollmentReferenceNumbers.csv', lines)\n+                                                Backup()\n+                                        elif invalidReference:\n+                                            invalidReference = True\n+                                            print(\"Invalid reference number\")            \n             case _:\n                 invalidInput = True\n                 print(\"Invalid Input\\n\")\n","files":{"\/Enrollment System.py":{"changes":[{"diff":"\n                     randomReference = ''.join(random.choices(string.ascii_letters, k=8))\n     return randomReference\n \n-def Search_Subjects(subject, subjectFile):\n-    with open(subjectFile, 'r') as file:\n+def ID_Number_Generator():\n+    #Creates the random enrollment reference number\n+    randomID = ''.join(random.choices(string.digits, k=2)) + \"-\" + ''.join(random.choices(string.digits, k=4))\n+    with open('StudentProfile.csv', 'r') as file:\n+        reader = csv.reader(file)\n+        for row in reader:\n+             if sum(1 for row in reader) > 0:\n+                if randomID == row[1]:\n+                    randomID = ''.join(random.choices(string.digits, k=2)) + \"-\" + ''.join(random.choices(string.digits, k=4))\n+    return randomID\n+\n+def Semester_Picker(semesterFile, course):\n+    with open(semesterFile, 'r') as file:\n         reader = csv.reader(file)\n         totalTuition = 0\n         for row in reader:\n-            if row[1] == \"General\" or row[1] == subject:\n+            if row[1] == \"General\" or row[1] == course:\n                 totalTuition += int(row[2])\n                 print(row[0], row[1], row[2])\n-    print(\"Total tuiton fee: \", totalTuition)\n     return totalTuition\n \n+def Search_Subjects(course, semester):\n+    match semester:\n+        case \"1st Sem\":\n+            tuitionFee = Semester_Picker('1stSemesterSubjects.csv', course)\n+        case \"2nd Sem\":\n+            tuitionFee = Semester_Picker('2ndSemesterSubjects.csv', course)\n+    print(\"Total tuiton fee: \", tuitionFee)\n+    return tuitionFee\n+\n+\n def Backup():\n     #Backs up every CSV file to a hidden backup folder\n     shutil.copy('1stSemesterSubjects.csv', '.Backup')\n","add":24,"remove":4,"filename":"\/Enrollment System.py","badparts":["def Search_Subjects(subject, subjectFile):","    with open(subjectFile, 'r') as file:","            if row[1] == \"General\" or row[1] == subject:","    print(\"Total tuiton fee: \", totalTuition)"],"goodparts":["def ID_Number_Generator():","    randomID = ''.join(random.choices(string.digits, k=2)) + \"-\" + ''.join(random.choices(string.digits, k=4))","    with open('StudentProfile.csv', 'r') as file:","        reader = csv.reader(file)","        for row in reader:","             if sum(1 for row in reader) > 0:","                if randomID == row[1]:","                    randomID = ''.join(random.choices(string.digits, k=2)) + \"-\" + ''.join(random.choices(string.digits, k=4))","    return randomID","def Semester_Picker(semesterFile, course):","    with open(semesterFile, 'r') as file:","            if row[1] == \"General\" or row[1] == course:","def Search_Subjects(course, semester):","    match semester:","        case \"1st Sem\":","            tuitionFee = Semester_Picker('1stSemesterSubjects.csv', course)","        case \"2nd Sem\":","            tuitionFee = Semester_Picker('2ndSemesterSubjects.csv', course)","    print(\"Total tuiton fee: \", tuitionFee)","    return tuitionFee"]},{"diff":"\n def Recover(file):\n     #Recovers CSV files from the hidden backup folder\n     match file:\n-        case \"1stSemesterSubjects.csv\":\n+        case '1stSemesterSubjects.csv':\n             shutil.copy('.Backup\/1stSemesterSubjects.csv', os.getcwd())\n-        case \"2ndSemesterSubjects.csv\":\n+        case '2ndSemesterSubjects.csv':\n             shutil.copy('.Backup\/2ndSemesterSubjects.csv', os.getcwd())\n-        case \"EnrollmentReferenceNumbers.csv\":\n+        case 'EnrollmentReferenceNumbers.csv':\n             shutil.copy('.Backup\/EnrollmentReferenceNumbers.csv', os.getcwd())\n-        case \"StudentProfile.csv\":\n+        case 'StudentProfile.csv':\n             shutil.copy('.Backup\/StudentProfile.csv', os.getcwd())\n         case _:\n             shutil.copy('.Backup\/1stSemesterSubjects.csv', os.getcwd())\n","add":4,"remove":4,"filename":"\/Enrollment System.py","badparts":["        case \"1stSemesterSubjects.csv\":","        case \"2ndSemesterSubjects.csv\":","        case \"EnrollmentReferenceNumbers.csv\":","        case \"StudentProfile.csv\":"],"goodparts":["        case '1stSemesterSubjects.csv':","        case '2ndSemesterSubjects.csv':","        case 'EnrollmentReferenceNumbers.csv':","        case 'StudentProfile.csv':"]},{"diff":"\n         else:\n              Recover(file)\n \n-def Tamper_Check(newFile):\n-    # Checks if the file content has been tampered with and returns a boolean\n-    # \n-    backupFile = '.Backup\/' + newFile\n-    tampered = False\n-    with open(backupFile, 'r') as file:\n-        reader = csv.reader(file)\n-        backupFileRowCount = sum(1 for row in reader)\n-    with open(newFile, 'r') as file:\n-        reader = csv.reader(file)\n-        newFileRowCount = sum(1 for row in reader)\n-    if backupFileRowCount < newFileRowCount:\n-        tampered = True\n-        Recover(backupFile)\n-    return tampered\n-\n def Save_To_CSV(CSVfile, data):\n     with open(CSVfile, 'a', newline='') as file:\n         writer = csv.writer(file)\n         writer.writerow(data)\n-    \n+\n+def Payment(reference):\n+    invalidPayment = True\n+    with open('EnrollmentReferenceNumbers.csv', 'r') as file:\n+        reader = csv.reader(file)\n+        for row in reader:\n+            if reference == row[0]:\n+                while invalidPayment:\n+                    userPayment = int((input(\"Input your payment: \")))                            \n+                    if userPayment < int(row[1]):\n+                        invalidPayment = True\n+                        print(\"Payment error! Your payment ips insufficient!\")     \n+                    elif userPayment > int(row[1]):\n+                        invalidPayment = False\n+                        print(\"Payment Successful! Here is your change: \", userPayment - int(row[1]))                            \n+                    else:\n+                        invalidPayment = False\n+                        print(\"Payment Successful! \")  \n+    return not invalidPayment \n+\n+def Overwrite(file, data):\n+    with open(file, 'w', newline='') as file:\n+        writer = csv.writer(file)\n+        writer.writerows(data)\n \n def Main_Menu():\n     studentProfile = []\n","add":24,"remove":17,"filename":"\/Enrollment System.py","badparts":["def Tamper_Check(newFile):","    backupFile = '.Backup\/' + newFile","    tampered = False","    with open(backupFile, 'r') as file:","        reader = csv.reader(file)","        backupFileRowCount = sum(1 for row in reader)","    with open(newFile, 'r') as file:","        reader = csv.reader(file)","        newFileRowCount = sum(1 for row in reader)","    if backupFileRowCount < newFileRowCount:","        tampered = True","        Recover(backupFile)","    return tampered"],"goodparts":["def Payment(reference):","    invalidPayment = True","    with open('EnrollmentReferenceNumbers.csv', 'r') as file:","        reader = csv.reader(file)","        for row in reader:","            if reference == row[0]:","                while invalidPayment:","                    userPayment = int((input(\"Input your payment: \")))                            ","                    if userPayment < int(row[1]):","                        invalidPayment = True","                        print(\"Payment error! Your payment ips insufficient!\")     ","                    elif userPayment > int(row[1]):","                        invalidPayment = False","                        print(\"Payment Successful! Here is your change: \", userPayment - int(row[1]))                            ","                    else:","                        invalidPayment = False","                        print(\"Payment Successful! \")  ","    return not invalidPayment ","def Overwrite(file, data):","    with open(file, 'w', newline='') as file:","        writer = csv.writer(file)","        writer.writerows(data)"]},{"diff":"\n                         case _:\n                             invalidSemester = True\n                             print(\"Invalid Input\\n\")\n-                if studentProfile[6] == \"1st Sem\":\n-                    match studentProfile[5]:\n-                        case \"BSCS\":\n-                            tuitionFee = Search_Subjects(\"BSCS\", '1stSemesterSubjects.csv')\n-                        case \"BSEMC\":\n-                            tuitionFee = Search_Subjects(\"BSEMC\", '1stSemesterSubjects.csv')\n-                        case \"BMMA\":\n-                            tuitionFee = Search_Subjects(\"BMMA\", '1stSemesterSubjects.csv')\n-                if studentProfile[6] == \"2nd Sem\":\n-                    match studentProfile[5]:\n-                        case \"BSCS\":\n-                            tuitionFee = Search_Subjects(\"BSCS\", '2ndSemesterSubjects.csv')\n-                        case \"BSEMC\":\n-                            tuitionFee = Search_Subjects(\"BSEMC\", '2ndSemesterSubjects.csv')\n-                        case \"BMMA\":\n-                            tuitionFee = Search_Subjects(\"BMMA\", '2ndSemesterSubjects.csv')\n+                tuitionFee = Search_Subjects(studentProfile[5], studentProfile[6])\n                 enrollmentReferenceNumber = [Reference_Number_Generator(), tuitionFee]\n                 studentProfile.append(\"Not Enrolled\")\n                 studentProfile.append(enrollmentReferenceNumber[0])\n                 print(\"Your enrollment reference number to be presented for the payment: \" + enrollmentReferenceNumber[0])\n \n                 Save_To_CSV('EnrollmentReferenceNumbers.csv', enrollmentReferenceNumber)\n-                if Tamper_Check('EnrollmentReferenceNumbers.csv'):\n-                    Save_To_CSV('EnrollmentReferenceNumbers.csv', enrollmentReferenceNumber)\n-\n-                with open('StudentProfile.csv', 'w', newline='') as file:\n-                    writer = csv.writer(file)\n-                    writer.writerow(studentProfile)\n-\n-                with open('StudentProfile.csv', 'r') as file:\n-                    reader = csv.reader(file)\n-                    for row in reader:\n-                        print(row)\n-\n-                with open('EnrollmentReferenceNumbers.csv', 'r') as file:\n-                    reader = csv.reader(file)\n-                    for row in reader:\n-                        print(row)\n+                Recover('StudentProfile.csv')\n+                Save_To_CSV('StudentProfile.csv', studentProfile)\n                 Backup()\n                 Main_Menu()\n                 \n             case 'p' | 'P':\n                 invalidInput = False\n-                userReference = (input(\"Input your reference number: \"))\n-                \n+                invalidReference = True\n                 with open('EnrollmentReferenceNumbers.csv', 'r') as file:\n                     reader = csv.reader(file)\n-                    for row in reader:                       \n-                        if row[0] == userReference: \n-                            print(\"Your total amount balance to pay is:\", row[1])\n-                            userPayment = int((input(\"Input your payment: \")))                            \n-                            if userPayment < int(row[1]):\n-                                print(\"Payment error! Your payment ips insufficient!\")     \n-                            elif userPayment > int(row[1]):\n-                                print(\"Payment Successful! Here is your change: \", userPayment - int(row[1]))                            \n-                            else: print(\"Payment Successful! \")                \n+                    match sum(1 for row in reader):\n+                        case 0:\n+                            print(\"Data unavailable please enroll first\")\n+                            Main_Menu()\n+                        case _:\n+                            while invalidReference:\n+                                userReference = (input(\"Input your reference number: \"))\n+                                with open('EnrollmentReferenceNumbers.csv', 'r') as file:\n+                                    reader = csv.reader(file)\n+                                    for row in reader:                  \n+                                        if row[0] == userReference: \n+                                            invalidReference = False\n+                                with open('StudentProfile.csv', 'r') as file:\n+                                    reader = csv.reader(file)\n+                                    for row in reader:                  \n+                                        if row[8] == userReference:\n+                                            if row[1] != \"0\":\n+                                                print (\"Student Name: \" + row[0] + \" \" + row[1] + \" \" + row[2])\n+                                            else:\n+                                                print (\"Student Name: \" + row[0] + \" \" + row[2])\n+                                            Search_Subjects(row[5], row[6])\n+                                            if Payment(userReference):\n+                                                print(\"You are now succesfully enrolled!\")\n+                                                userId = ID_Number_Generator()\n+                                                print(\"Your student ID: \" + userId)\n+                                                lines = list()\n+                                                with open('StudentProfile.csv', 'r') as file:\n+                                                    reader = csv.reader(file)\n+                                                    for row in reader:\n+                                                        lines.append(row)\n+                                                        if row[8] == userReference:\n+                                                            row[8] = userId\n+                                                            row[7] = \"Enrolled\"\n+                                                Overwrite('StudentProfile.csv', lines)\n+                                                lines = list()\n+                                                with open('EnrollmentReferenceNumbers.csv', 'r') as file:\n+                                                    reader = csv.reader(file)\n+                                                    for row in reader:\n+                                                        lines.append(row)\n+                                                        if row[0] == userReference:\n+                                                            lines.remove(row)\n+                                                Overwrite('EnrollmentReferenceNumbers.csv', lines)\n+                                                Backup()\n+                                        elif invalidReference:\n+                                            invalidReference = True\n+                                            print(\"Invalid reference number\")            \n             case _:\n                 invalidInput = True\n                 print(\"Invalid Input\\n\")\n","add":50,"remove":43,"filename":"\/Enrollment System.py","badparts":["                if studentProfile[6] == \"1st Sem\":","                    match studentProfile[5]:","                        case \"BSCS\":","                            tuitionFee = Search_Subjects(\"BSCS\", '1stSemesterSubjects.csv')","                        case \"BSEMC\":","                            tuitionFee = Search_Subjects(\"BSEMC\", '1stSemesterSubjects.csv')","                        case \"BMMA\":","                            tuitionFee = Search_Subjects(\"BMMA\", '1stSemesterSubjects.csv')","                if studentProfile[6] == \"2nd Sem\":","                    match studentProfile[5]:","                        case \"BSCS\":","                            tuitionFee = Search_Subjects(\"BSCS\", '2ndSemesterSubjects.csv')","                        case \"BSEMC\":","                            tuitionFee = Search_Subjects(\"BSEMC\", '2ndSemesterSubjects.csv')","                        case \"BMMA\":","                            tuitionFee = Search_Subjects(\"BMMA\", '2ndSemesterSubjects.csv')","                if Tamper_Check('EnrollmentReferenceNumbers.csv'):","                    Save_To_CSV('EnrollmentReferenceNumbers.csv', enrollmentReferenceNumber)","                with open('StudentProfile.csv', 'w', newline='') as file:","                    writer = csv.writer(file)","                    writer.writerow(studentProfile)","                with open('StudentProfile.csv', 'r') as file:","                    reader = csv.reader(file)","                    for row in reader:","                        print(row)","                with open('EnrollmentReferenceNumbers.csv', 'r') as file:","                    reader = csv.reader(file)","                    for row in reader:","                        print(row)","                userReference = (input(\"Input your reference number: \"))","                    for row in reader:                       ","                        if row[0] == userReference: ","                            print(\"Your total amount balance to pay is:\", row[1])","                            userPayment = int((input(\"Input your payment: \")))                            ","                            if userPayment < int(row[1]):","                                print(\"Payment error! Your payment ips insufficient!\")     ","                            elif userPayment > int(row[1]):","                                print(\"Payment Successful! Here is your change: \", userPayment - int(row[1]))                            ","                            else: print(\"Payment Successful! \")                "],"goodparts":["                tuitionFee = Search_Subjects(studentProfile[5], studentProfile[6])","                Recover('StudentProfile.csv')","                Save_To_CSV('StudentProfile.csv', studentProfile)","                invalidReference = True","                    match sum(1 for row in reader):","                        case 0:","                            print(\"Data unavailable please enroll first\")","                            Main_Menu()","                        case _:","                            while invalidReference:","                                userReference = (input(\"Input your reference number: \"))","                                with open('EnrollmentReferenceNumbers.csv', 'r') as file:","                                    reader = csv.reader(file)","                                    for row in reader:                  ","                                        if row[0] == userReference: ","                                            invalidReference = False","                                with open('StudentProfile.csv', 'r') as file:","                                    reader = csv.reader(file)","                                    for row in reader:                  ","                                        if row[8] == userReference:","                                            if row[1] != \"0\":","                                                print (\"Student Name: \" + row[0] + \" \" + row[1] + \" \" + row[2])","                                            else:","                                                print (\"Student Name: \" + row[0] + \" \" + row[2])","                                            Search_Subjects(row[5], row[6])","                                            if Payment(userReference):","                                                print(\"You are now succesfully enrolled!\")","                                                userId = ID_Number_Generator()","                                                print(\"Your student ID: \" + userId)","                                                lines = list()","                                                with open('StudentProfile.csv', 'r') as file:","                                                    reader = csv.reader(file)","                                                    for row in reader:","                                                        lines.append(row)","                                                        if row[8] == userReference:","                                                            row[8] = userId","                                                            row[7] = \"Enrolled\"","                                                Overwrite('StudentProfile.csv', lines)","                                                lines = list()","                                                with open('EnrollmentReferenceNumbers.csv', 'r') as file:","                                                    reader = csv.reader(file)","                                                    for row in reader:","                                                        lines.append(row)","                                                        if row[0] == userReference:","                                                            lines.remove(row)","                                                Overwrite('EnrollmentReferenceNumbers.csv', lines)","                                                Backup()","                                        elif invalidReference:","                                            invalidReference = True","                                            print(\"Invalid reference number\")            "]}],"source":"\nimport ctypes import os import csv import re import shutil import random import string def Reference_Number_Generator(): randomReference=''.join(random.choices(string.ascii_letters, k=8)) with open('EnrollmentReferenceNumbers.csv', 'r') as file: reader=csv.reader(file) for row in reader: if sum(1 for row in reader) > 0: if randomReference==row[1]: randomReference=''.join(random.choices(string.ascii_letters, k=8)) return randomReference def Search_Subjects(subject, subjectFile): with open(subjectFile, 'r') as file: reader=csv.reader(file) totalTuition=0 for row in reader: if row[1]==\"General\" or row[1]==subject: totalTuition +=int(row[2]) print(row[0], row[1], row[2]) print(\"Total tuiton fee: \", totalTuition) return totalTuition def Backup(): shutil.copy('1stSemesterSubjects.csv', '.Backup') shutil.copy('2ndSemesterSubjects.csv', '.Backup') shutil.copy('EnrollmentReferenceNumbers.csv', '.Backup') shutil.copy('StudentProfile.csv', '.Backup') def Recover(file): match file: case \"1stSemesterSubjects.csv\": shutil.copy('.Backup\/1stSemesterSubjects.csv', os.getcwd()) case \"2ndSemesterSubjects.csv\": shutil.copy('.Backup\/2ndSemesterSubjects.csv', os.getcwd()) case \"EnrollmentReferenceNumbers.csv\": shutil.copy('.Backup\/EnrollmentReferenceNumbers.csv', os.getcwd()) case \"StudentProfile.csv\": shutil.copy('.Backup\/StudentProfile.csv', os.getcwd()) case _: shutil.copy('.Backup\/1stSemesterSubjects.csv', os.getcwd()) shutil.copy('.Backup\/EnrollmentReferenceNumbers.csv', os.getcwd()) shutil.copy('.Backup\/StudentProfile.csv', os.getcwd()) def File_Check_And_Recover(file): if not os.path.isfile(file): if not os.path.isfile('.Backup\/' +file): open(file, 'a') else: Recover(file) def Tamper_Check(newFile): backupFile='.Backup\/' +newFile tampered=False with open(backupFile, 'r') as file: reader=csv.reader(file) backupFileRowCount=sum(1 for row in reader) with open(newFile, 'r') as file: reader=csv.reader(file) newFileRowCount=sum(1 for row in reader) if backupFileRowCount < newFileRowCount: tampered=True Recover(backupFile) return tampered def Save_To_CSV(CSVfile, data): with open(CSVfile, 'a', newline='') as file: writer=csv.writer(file) writer.writerow(data) def Main_Menu(): studentProfile=[] correctDate=False invalidInput=True invalidGender=True invalidCourse=True invalidSemester=True print(r\"\"\" \u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2557 \u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551\u255a\u2550\u2550\u2588\u2588\u2554\u2550\u2550\u255d \u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2554\u2550\u2550\u2550\u255d \u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2551 \u255a\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u255d \u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d \u255a\u2550\u255d \u255a\u2550\u255d Enrollment System\"\"\") while invalidInput: print(\"Enrollment -E\") print(\"Payment -P\") match input(\"Input: \"): case 'e' | 'E': invalidInput=False studentProfile.append(input(\"First Name: \")) userMiddleName=input(\"Middle Name(0 if not applicable): \") match userMiddleName: case 0: studentProfile.append(\"N\/A\") case _: studentProfile.append(userMiddleName) studentProfile.append(input(\"Last Name: \")) while not correctDate: userBirthdate=input(\"Birthdate(mm\/dd\/yyyy): \") if re.match('^[0-1]{1}[0-9]{1}\/[1-3]{1}[0-9]{1}\/[0-9]{4}$', userBirthdate): studentProfile.append(userBirthdate) correctDate=True else: print(\"Incorrect formatting please try again\") correctDate=False while invalidGender: print(\"Sex:\") print(\"Male -M\") print(\"Female -F\") match input(\"Input: \"): case 'm' | 'M': invalidGender=False studentProfile.append(\"Male\") case 'f' | 'F': invalidGender=False studentProfile.append(\"Female\") case _: invalidGender=True print(\"Invalid Input\\n\") while invalidCourse: print(\"Course:\") print(\"Bachelor of Science in Computer Science(BSCS) -A\") print(\"Bachelor of Entertainment and Multimedia Computing(BSEMC) -B\") print(\"Bachelor of Multimedia Arts(BMMA) -C\") match input(\"Input: \"): case 'a' | 'A': invalidCourse=False studentProfile.append(\"BSCS\") case 'b' | 'B': invalidCourse=False studentProfile.append(\"BSEMC\") case 'c' | 'C': invalidCourse=False studentProfile.append(\"BMMA\") case _: invalidCourse=True print(\"Invalid Input\\n\") while invalidSemester: print(\"What semester are you enrolling in?\") print(\"1st Semester -A\") print(\"2nd Semester -B\") match input(\"Input: \"): case 'a' | 'A': invalidSemester=False studentProfile.append(\"1st Sem\") case 'b' | 'B': invalidSemester=False studentProfile.append(\"2nd Sem\") case _: invalidSemester=True print(\"Invalid Input\\n\") if studentProfile[6]==\"1st Sem\": match studentProfile[5]: case \"BSCS\": tuitionFee=Search_Subjects(\"BSCS\", '1stSemesterSubjects.csv') case \"BSEMC\": tuitionFee=Search_Subjects(\"BSEMC\", '1stSemesterSubjects.csv') case \"BMMA\": tuitionFee=Search_Subjects(\"BMMA\", '1stSemesterSubjects.csv') if studentProfile[6]==\"2nd Sem\": match studentProfile[5]: case \"BSCS\": tuitionFee=Search_Subjects(\"BSCS\", '2ndSemesterSubjects.csv') case \"BSEMC\": tuitionFee=Search_Subjects(\"BSEMC\", '2ndSemesterSubjects.csv') case \"BMMA\": tuitionFee=Search_Subjects(\"BMMA\", '2ndSemesterSubjects.csv') enrollmentReferenceNumber=[Reference_Number_Generator(), tuitionFee] studentProfile.append(\"Not Enrolled\") studentProfile.append(enrollmentReferenceNumber[0]) print(\"Your enrollment reference number to be presented for the payment: \" +enrollmentReferenceNumber[0]) Save_To_CSV('EnrollmentReferenceNumbers.csv', enrollmentReferenceNumber) if Tamper_Check('EnrollmentReferenceNumbers.csv'): Save_To_CSV('EnrollmentReferenceNumbers.csv', enrollmentReferenceNumber) with open('StudentProfile.csv', 'w', newline='') as file: writer=csv.writer(file) writer.writerow(studentProfile) with open('StudentProfile.csv', 'r') as file: reader=csv.reader(file) for row in reader: print(row) with open('EnrollmentReferenceNumbers.csv', 'r') as file: reader=csv.reader(file) for row in reader: print(row) Backup() Main_Menu() case 'p' | 'P': invalidInput=False userReference=(input(\"Input your reference number: \")) with open('EnrollmentReferenceNumbers.csv', 'r') as file: reader=csv.reader(file) for row in reader: if row[0]==userReference: print(\"Your total amount balance to pay is:\", row[1]) userPayment=int((input(\"Input your payment: \"))) if userPayment < int(row[1]): print(\"Payment error! Your payment ips insufficient!\") elif userPayment > int(row[1]): print(\"Payment Successful! Here is your change: \", userPayment -int(row[1])) else: print(\"Payment Successful! \") case _: invalidInput=True print(\"Invalid Input\\n\") File_Check_And_Recover('StudentProfile.csv') File_Check_And_Recover('1stSemesterSubjects.csv') File_Check_And_Recover('2ndSemesterSubjects.csv') File_Check_And_Recover('EnrollmentReferenceNumbers.csv') if not os.path.isdir('.Backup'): os.mkdir(\".Backup\") FILE_ATTRIBUTE_HIDDEN=0x02 ret=ctypes.windll.kernel32.SetFileAttributesW(\".Backup\", FILE_ATTRIBUTE_HIDDEN) Backup() Main_Menu() ","sourceWithComments":"import ctypes\nimport os\nimport csv\nimport re\nimport shutil\nimport random\nimport string\n\ndef Reference_Number_Generator():\n    #Creates the random enrollment reference number\n    randomReference = ''.join(random.choices(string.ascii_letters, k=8))\n    with open('EnrollmentReferenceNumbers.csv', 'r') as file:\n        reader = csv.reader(file)\n        for row in reader:\n            if sum(1 for row in reader) > 0:\n                if randomReference == row[1]:\n                    randomReference = ''.join(random.choices(string.ascii_letters, k=8))\n    return randomReference\n\ndef Search_Subjects(subject, subjectFile):\n    with open(subjectFile, 'r') as file:\n        reader = csv.reader(file)\n        totalTuition = 0\n        for row in reader:\n            if row[1] == \"General\" or row[1] == subject:\n                totalTuition += int(row[2])\n                print(row[0], row[1], row[2])\n    print(\"Total tuiton fee: \", totalTuition)\n    return totalTuition\n\ndef Backup():\n    #Backs up every CSV file to a hidden backup folder\n    shutil.copy('1stSemesterSubjects.csv', '.Backup')\n    shutil.copy('2ndSemesterSubjects.csv', '.Backup')\n    shutil.copy('EnrollmentReferenceNumbers.csv', '.Backup')\n    shutil.copy('StudentProfile.csv', '.Backup')\n\ndef Recover(file):\n    #Recovers CSV files from the hidden backup folder\n    match file:\n        case \"1stSemesterSubjects.csv\":\n            shutil.copy('.Backup\/1stSemesterSubjects.csv', os.getcwd())\n        case \"2ndSemesterSubjects.csv\":\n            shutil.copy('.Backup\/2ndSemesterSubjects.csv', os.getcwd())\n        case \"EnrollmentReferenceNumbers.csv\":\n            shutil.copy('.Backup\/EnrollmentReferenceNumbers.csv', os.getcwd())\n        case \"StudentProfile.csv\":\n            shutil.copy('.Backup\/StudentProfile.csv', os.getcwd())\n        case _:\n            shutil.copy('.Backup\/1stSemesterSubjects.csv', os.getcwd())\n            shutil.copy('.Backup\/EnrollmentReferenceNumbers.csv', os.getcwd())\n            shutil.copy('.Backup\/StudentProfile.csv', os.getcwd())\n\ndef File_Check_And_Recover(file):\n    # Checks whether a file exist in the main directory if not it will check \n    # the hidden backup folder for the file if available it will restore \n    # the file to the main directory\n    if not os.path.isfile(file):\n        if not os.path.isfile('.Backup\/' + file):\n            open(file, 'a')\n        else:\n             Recover(file)\n\ndef Tamper_Check(newFile):\n    # Checks if the file content has been tampered with and returns a boolean\n    # \n    backupFile = '.Backup\/' + newFile\n    tampered = False\n    with open(backupFile, 'r') as file:\n        reader = csv.reader(file)\n        backupFileRowCount = sum(1 for row in reader)\n    with open(newFile, 'r') as file:\n        reader = csv.reader(file)\n        newFileRowCount = sum(1 for row in reader)\n    if backupFileRowCount < newFileRowCount:\n        tampered = True\n        Recover(backupFile)\n    return tampered\n\ndef Save_To_CSV(CSVfile, data):\n    with open(CSVfile, 'a', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(data)\n    \n\ndef Main_Menu():\n    studentProfile = []\n    correctDate = False\n    invalidInput = True\n    invalidGender = True\n    invalidCourse = True\n    invalidSemester = True\n\n    print(r\"\"\" \n\u2588\u2588\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2557     \u2588\u2588\u2557     \u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\n\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551     \u2588\u2588\u2551     \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551\u255a\u2550\u2550\u2588\u2588\u2554\u2550\u2550\u255d\n\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551     \u2588\u2588\u2551     \u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551   \u2588\u2588\u2551   \n\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2551\u2588\u2588\u2551     \u2588\u2588\u2551     \u2588\u2588\u2554\u2550\u2550\u2550\u255d \u2588\u2588\u2551   \u2588\u2588\u2551   \n\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551  \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551     \u2588\u2588\u2551   \u2588\u2588\u2551   \n\u255a\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u255d  \u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d     \u255a\u2550\u255d   \u255a\u2550\u255d                                                     \n                Enrollment System\"\"\")\n    while invalidInput:\n        print(\"Enrollment - E\")\n        print(\"Payment - P\")\n        match input(\"Input: \"):\n            case 'e' | 'E':\n                invalidInput = False\n                studentProfile.append(input(\"First Name: \"))\n                userMiddleName = input(\"Middle Name (0 if not applicable): \")\n                match userMiddleName:\n                    case 0:\n                        studentProfile.append(\"N\/A\")\n                    case _:\n                        studentProfile.append(userMiddleName)\n                studentProfile.append(input(\"Last Name: \"))\n                while not correctDate:\n                    userBirthdate = input(\"Birthdate (mm\/dd\/yyyy): \")\n                    #Checks whether the input is the correct date format\n                    if re.match('^[0-1]{1}[0-9]{1}\/[1-3]{1}[0-9]{1}\/[0-9]{4}$', userBirthdate):\n                        studentProfile.append(userBirthdate)\n                        correctDate = True\n                    else:\n                        print(\"Incorrect formatting please try again\")\n                        correctDate = False\n\n                while invalidGender:\n                    print(\"Sex:\")\n                    print(\"Male - M\")\n                    print(\"Female - F\")\n                    match input(\"Input: \"):\n                        case 'm' | 'M':\n                            invalidGender = False\n                            studentProfile.append(\"Male\")\n                        case 'f' | 'F':\n                            invalidGender = False\n                            studentProfile.append(\"Female\")\n                        case _:\n                            invalidGender = True\n                            print(\"Invalid Input\\n\")\n                \n                while invalidCourse:\n                    print(\"Course:\")\n                    print(\"Bachelor of Science in Computer Science (BSCS) - A\")\n                    print(\"Bachelor of Entertainment and Multimedia Computing (BSEMC) - B\")\n                    print(\"Bachelor of Multimedia Arts (BMMA) - C\")\n                    match input(\"Input: \"):\n                        case 'a' | 'A':\n                            invalidCourse = False\n                            studentProfile.append(\"BSCS\")\n                        case 'b' | 'B':\n                            invalidCourse = False\n                            studentProfile.append(\"BSEMC\")\n                        case 'c' | 'C':\n                            invalidCourse = False\n                            studentProfile.append(\"BMMA\")\n                        case _:\n                            invalidCourse = True\n                            print(\"Invalid Input\\n\")\n\n                while invalidSemester:\n                    print(\"What semester are you enrolling in?\")\n                    print(\"1st Semester - A\")\n                    print(\"2nd Semester - B\")\n                    match input(\"Input: \"):\n                        case 'a' | 'A':\n                            invalidSemester = False\n                            studentProfile.append(\"1st Sem\")\n                        case 'b' | 'B':\n                            invalidSemester = False\n                            studentProfile.append(\"2nd Sem\")\n                        case _:\n                            invalidSemester = True\n                            print(\"Invalid Input\\n\")\n                if studentProfile[6] == \"1st Sem\":\n                    match studentProfile[5]:\n                        case \"BSCS\":\n                            tuitionFee = Search_Subjects(\"BSCS\", '1stSemesterSubjects.csv')\n                        case \"BSEMC\":\n                            tuitionFee = Search_Subjects(\"BSEMC\", '1stSemesterSubjects.csv')\n                        case \"BMMA\":\n                            tuitionFee = Search_Subjects(\"BMMA\", '1stSemesterSubjects.csv')\n                if studentProfile[6] == \"2nd Sem\":\n                    match studentProfile[5]:\n                        case \"BSCS\":\n                            tuitionFee = Search_Subjects(\"BSCS\", '2ndSemesterSubjects.csv')\n                        case \"BSEMC\":\n                            tuitionFee = Search_Subjects(\"BSEMC\", '2ndSemesterSubjects.csv')\n                        case \"BMMA\":\n                            tuitionFee = Search_Subjects(\"BMMA\", '2ndSemesterSubjects.csv')\n                enrollmentReferenceNumber = [Reference_Number_Generator(), tuitionFee]\n                studentProfile.append(\"Not Enrolled\")\n                studentProfile.append(enrollmentReferenceNumber[0])\n                print(\"Your enrollment reference number to be presented for the payment: \" + enrollmentReferenceNumber[0])\n\n                Save_To_CSV('EnrollmentReferenceNumbers.csv', enrollmentReferenceNumber)\n                if Tamper_Check('EnrollmentReferenceNumbers.csv'):\n                    Save_To_CSV('EnrollmentReferenceNumbers.csv', enrollmentReferenceNumber)\n\n                with open('StudentProfile.csv', 'w', newline='') as file:\n                    writer = csv.writer(file)\n                    writer.writerow(studentProfile)\n\n                with open('StudentProfile.csv', 'r') as file:\n                    reader = csv.reader(file)\n                    for row in reader:\n                        print(row)\n\n                with open('EnrollmentReferenceNumbers.csv', 'r') as file:\n                    reader = csv.reader(file)\n                    for row in reader:\n                        print(row)\n                Backup()\n                Main_Menu()\n                \n            case 'p' | 'P':\n                invalidInput = False\n                userReference = (input(\"Input your reference number: \"))\n                \n                with open('EnrollmentReferenceNumbers.csv', 'r') as file:\n                    reader = csv.reader(file)\n                    for row in reader:                       \n                        if row[0] == userReference: \n                            print(\"Your total amount balance to pay is:\", row[1])\n                            userPayment = int((input(\"Input your payment: \")))                            \n                            if userPayment < int(row[1]):\n                                print(\"Payment error! Your payment ips insufficient!\")     \n                            elif userPayment > int(row[1]):\n                                print(\"Payment Successful! Here is your change: \", userPayment - int(row[1]))                            \n                            else: print(\"Payment Successful! \")                \n            case _:\n                invalidInput = True\n                print(\"Invalid Input\\n\")\n\nFile_Check_And_Recover('StudentProfile.csv')\nFile_Check_And_Recover('1stSemesterSubjects.csv')\nFile_Check_And_Recover('2ndSemesterSubjects.csv')\nFile_Check_And_Recover('EnrollmentReferenceNumbers.csv')\n\nif not os.path.isdir('.Backup'):\n    os.mkdir(\".Backup\")\n    FILE_ATTRIBUTE_HIDDEN = 0x02\n    ret = ctypes.windll.kernel32.SetFileAttributesW(\".Backup\", FILE_ATTRIBUTE_HIDDEN)\n    Backup()\n\nMain_Menu()\n"}},"msg":"Removed tamper check, optimize code, add ID Generator, and add modification for the Student Profile when done paying for tuition"}},"https:\/\/github.com\/GeorgeDiNicola\/TDRB-Middleware-Extension":{"9ede2c75ba83aee6870081472c7245fe04dd47c6":{"url":"https:\/\/api.github.com\/repos\/GeorgeDiNicola\/TDRB-Middleware-Extension\/commits\/9ede2c75ba83aee6870081472c7245fe04dd47c6","html_url":"https:\/\/github.com\/GeorgeDiNicola\/TDRB-Middleware-Extension\/commit\/9ede2c75ba83aee6870081472c7245fe04dd47c6","message":"optimized tampering check to only query 1 time","sha":"9ede2c75ba83aee6870081472c7245fe04dd47c6","keyword":"tampering check","diff":"diff --git a\/src\/main.py b\/src\/main.py\nindex 5a14df7..1113af8 100644\n--- a\/src\/main.py\n+++ b\/src\/main.py\n@@ -225,35 +225,11 @@ def query(user_query, adapter, tampered_primary_keys):\n \tcol_encryption_rec = setup.convert_table_to_column_encryption_record(results, table_name)\n \trow_encryption_recs = setup.convert_table_to_record_encryption_records(results, table_name)\n \n-\t# read the encrypted records\n-\t\n-\tprint(\"\\n====Column Encryption Record====\\n\")\n-\tprint(col_encryption_rec.table_name_hash)\n-\tprint(col_encryption_rec.table_name_AES)\n-\tprint(col_encryption_rec.column_hash)\n-\tprint(\"==============================\\n\")\n-\t\n-\t\"\"\"\n-\tprint(\"====Row Encryption Records====\\n\")\n-\ti = 1\n-\tfor rer in row_encryption_recs:\n-\t\tprint(\"record :\", i)\n-\t\tprint(rer.item_id_hash)\n-\t\tprint(rer.item_id_AES)\n-\t\tprint(rer.item_hash)\n-\t\tprint(rer.owned_table_AES)\n-\t\ti += 1\n-\t\"\"\"\n-\n \t# detect illegal modification step\n-\trerc_tamper_flag, rerc_tampered_primary_keys = td.rerc(table_name, adapter, key, iv)\n-\t#rerc_tamper_flag = 0\n+\trerc_tamper_flag, rerc_tampered_primary_keys = td.rerc(results, table_name, adapter, key, iv)\n \n \t# detect illegal insert or delete step\n \tcerc_tamper_flag, cerc_info = td.cerc(col_encryption_rec, table_name, adapter, key, iv)\n-\tprint(cerc_tamper_flag)\n-\tprint(cerc_info)\n-\n \n \t# print the tampering info to the user before showing them the query results\n \tif rerc_tamper_flag:\ndiff --git a\/src\/tamper_detection.py b\/src\/tamper_detection.py\nindex 0c495fd..ac4527b 100644\n--- a\/src\/tamper_detection.py\n+++ b\/src\/tamper_detection.py\n@@ -31,93 +31,28 @@ def cerc(col_encryption_rec, table_name, adapter, key, iv):\n \ttamper_flag = 0  # zero indicates no tampering\n \ttamper_info = \"\"\n \n-\tprint(\"\\n====Column Encryption Record====\\n\")\n-\tprint(col_encryption_rec.table_name_hash)\n-\tprint(col_encryption_rec.table_name_AES)\n-\tprint(col_encryption_rec.column_hash)\n-\tprint(\"==============================\\n\")\n-\n \ttry:\n \t\tblockchain_result = check_output(['node', 'query.js', col_encryption_rec.table_name_hash])\n \texcept:\n \t\tblockchain_result = \"\"  # no result for the query found above\n \n-\tprint(\"CER column hash: \", col_encryption_rec.column_hash)\n-\tprint(str(blockchain_result))\n \tif col_encryption_rec.column_hash not in str(blockchain_result):\n-\t\t\tprint(\"tampering detected: ILLEGAL INSERT OR DELETE\")\n \t\t\ttamper_info = \"tampering detected: ILLEGAL INSERT OR DELETE\"\n \t\t\ttamper_flag = 1\n \n-\t#table_name_hash = utils.get_table_name_hash(table_name)\n-\t#column_hash = utils.get_column_hash(results)\n-\t#table_name_aes = utils.get_encrypted_table(table_name, key, iv)\n-\n \treturn tamper_flag, tamper_info\n \n-\t\n-\t\n-\t\n-\t# TODO: ADD SOME MECHANISM TO ACTUALLY ITERATE OVER TABLES INSTEAD OF ROWS\n-\t\"\"\"\n-\ttables = []\n-\tfor table_name in table_list:\n-\t\t\n-\t\tquery = \"select * from \" + table_name  # construct the query for getting the data from the table\n-\n-\t\tresults = adapter.send_query(query)\n-\n-\t\tcolumn_hash = utils.get_column_hash(results)\n-\n-\t\ttable_name_aes = utils.get_encrypted_table(table_name, key, iv)\n-\n-\t\tcolumn_hash = utils.get_column_hash(results)\n-\t\ttable_name_aes = utils.get_encrypted_table(table_name, key, iv)\n-\t\ttable_name_hash = utils.get_table_name_hash(table_name)\n-\n-\t\t#print(column_hash)\n-\t\t#print(table_name_aes)\n-\t\t#print(table_name_hash)\n-\n-\t\t# query_column_hash, redis_flag = access_redis(table_name_hash)\n-\n-\t# redis_flag represents cache hit (1) or cache miss (0)\n-\n-\t\tif column_hash != query_column_hash:\n-\t\t\tif redis_flag == 0:\n-\t\t\t\ttamper_flag = 1\n-\t\t\t\t#b_tabes = rich_query_blockchain_(table)   send query to get blockchain records\n-\t\t\t\tr_tables = get_all_row_encryption_records(table)\n-\t\t\t\tillegal_insert, illegal_delete, illegal_modify = check_tamper(b_tables, r_tables)\n-\t\t\telse:\n-\t\t\t\tclear Redis\n-\t\t\t\tget query_column_hash from Blockchain\n-\t\t\t\tif column_hash != query_column_hash:\n-\t\t\t\t\ttamper_flag = 1\n-\t\t\t\t\t#b_tables = rich_query_blockchain_(table)  # send query to get blockchain records\n-\t\t\t\t\tr_tables = get_all_row_encryption_records(table)\n-\t\t\t\t\tillegal_insert, illegal_delete, illegal_modify = check_tamper(b_tables, r_tables)\n-\t\t\t\telse:\n-\t\t\t\t\ttamper_flag = check_tamper(b_tables, r_tables)\n-\tif tamper_flag == 0:\n-\t\treturn table  # return the query results from the relational DB\n-\telse:\n-\t\ttamper_information = show_tamper_information(illegal_insert, illegal_delete, illegal_modify)\n-\t\tprint(tamper_information)\n-\n-\treturn tamper_information, table  # table = query results\n-\t\"\"\"\n \n # row encryption record comparison\n-def rerc(table_name, adapter, key, iv):\n+def rerc(results, table_name, adapter, key, iv):\n \t\"\"\" \"\"\"\n \n \ttamper_flag = 0  # zero indicates no tampering\n \ttampered_records_primary_key_list = []\n \n \t#TODO: note - I already have these\n-\tquery = \"select * from \" + table_name  # construct the query for getting the data from the table\n-\tresults = adapter.send_query(query)\n+\t#query = \"select * from \" + table_name  # construct the query for getting the data from the table\n+\t#results = adapter.send_query(query)\n \t\n \tfor row in results:\n \t\titem_id = row[0]\n@@ -126,7 +61,6 @@ def rerc(table_name, adapter, key, iv):\n \t\titem_id_aes = utils.get_encrypted_item_id(item_id, key, iv)\n \t\towned_table_aes = utils.get_encrypted_table(table_name, key, iv)\n \n-\n \t\ttry:\n \t\t\tblockchain_result = check_output(['node', 'query.js', item_id_hash])\n \t\texcept:\n","files":{"\/src\/main.py":{"changes":[{"diff":"\n \tcol_encryption_rec = setup.convert_table_to_column_encryption_record(results, table_name)\n \trow_encryption_recs = setup.convert_table_to_record_encryption_records(results, table_name)\n \n-\t# read the encrypted records\n-\t\n-\tprint(\"\\n====Column Encryption Record====\\n\")\n-\tprint(col_encryption_rec.table_name_hash)\n-\tprint(col_encryption_rec.table_name_AES)\n-\tprint(col_encryption_rec.column_hash)\n-\tprint(\"==============================\\n\")\n-\t\n-\t\"\"\"\n-\tprint(\"====Row Encryption Records====\\n\")\n-\ti = 1\n-\tfor rer in row_encryption_recs:\n-\t\tprint(\"record :\", i)\n-\t\tprint(rer.item_id_hash)\n-\t\tprint(rer.item_id_AES)\n-\t\tprint(rer.item_hash)\n-\t\tprint(rer.owned_table_AES)\n-\t\ti += 1\n-\t\"\"\"\n-\n \t# detect illegal modification step\n-\trerc_tamper_flag, rerc_tampered_primary_keys = td.rerc(table_name, adapter, key, iv)\n-\t#rerc_tamper_flag = 0\n+\trerc_tamper_flag, rerc_tampered_primary_keys = td.rerc(results, table_name, adapter, key, iv)\n \n \t# detect illegal insert or delete step\n \tcerc_tamper_flag, cerc_info = td.cerc(col_encryption_rec, table_name, adapter, key, iv)\n-\tprint(cerc_tamper_flag)\n-\tprint(cerc_info)\n-\n \n \t# print the tampering info to the user before showing them the query results\n \tif rerc_tamper_flag:","add":1,"remove":25,"filename":"\/src\/main.py","badparts":["\t","\tprint(\"\\n====Column Encryption Record====\\n\")","\tprint(col_encryption_rec.table_name_hash)","\tprint(col_encryption_rec.table_name_AES)","\tprint(col_encryption_rec.column_hash)","\tprint(\"==============================\\n\")","\t","\t\"\"\"","\tprint(\"====Row Encryption Records====\\n\")","\ti = 1","\tfor rer in row_encryption_recs:","\t\tprint(\"record :\", i)","\t\tprint(rer.item_id_hash)","\t\tprint(rer.item_id_AES)","\t\tprint(rer.item_hash)","\t\tprint(rer.owned_table_AES)","\t\ti += 1","\t\"\"\"","\trerc_tamper_flag, rerc_tampered_primary_keys = td.rerc(table_name, adapter, key, iv)","\tprint(cerc_tamper_flag)","\tprint(cerc_info)"],"goodparts":["\trerc_tamper_flag, rerc_tampered_primary_keys = td.rerc(results, table_name, adapter, key, iv)"]}],"source":"\nimport mysql.connector from mysql.connector import Error import hashlib import argparse import base64 from Cryptodome.Util.Padding import pad, unpad from Cryptodome.Random import get_random_bytes from Cryptodome.Cipher import AES import pandas as pd import setup import config import utils import tamper_detection as td import sql as s import blockchain from MysqlAdapter import MysqlAdapter from RowEncryptionRecord import RowEncryptionRecord from ColumnEncryptionRecord import ColumnEncryptionRecord import pymysql settings=config.load_config() def handle_user_args(): \t\"\"\"Handle the input given by the user.\"\"\" \tparser=argparse.ArgumentParser(description='TDRB Middleware extension Argument Parser') \tparser.add_argument('--q', dest='query', required=True, type=str, \t\tnargs='+', help='Query for the middleware to send to the relational database') \t \t \tparser.add_argument('--c', dest='command', required=True, type=str, \t\tnargs='+', help='Command(insert, delete, update, query') \tparser.add_argument('--r', dest='row_to_insert', required=False, type=str, \t\tnargs='+', help='Command(insert, delete, update, query') \tparser.add_argument('--u', dest='update_template', required=False, type=str, \t\tnargs='+', help='Command(insert, delete, update, query') \tparser.add_argument('--d', dest='pk_to_delete', required=False, type=str, \t\tnargs='+', help='Command(insert, delete, update, query') \targs=parser.parse_args() \t \tif len(args.query) < 1: \t\traise argparse.ArgumentTypeError(\"The query must not be empty!\") \t \tspace=' ' \targs.query=space.join(args.query) \targs.command=space.join(args.command) \tif args.row_to_insert: \t\targs.row_to_insert=space.join(args.row_to_insert) \tif args.update_template: \t\targs.update_template=space.join(args.update_template) \t \tif args.pk_to_delete: \t\targs.pk_to_delete=space.join(args.pk_to_delete) \treturn args def update(existing_item_id_hash, new_item_hash): \t \t \tresult=adapter.send_query(user_query) \t \tif result: \t\tprint(\"Please reformulate SQL query!\") \t\treturn False \t \tblockchain_commit_success=blockchain.update_blockchain_record(existing_item_id_hash, new_item_hash) \t \tif blockchain_commit_success: \t\t \t\t \t\tadapter.connection.commit() \t \treturn True def delete(user_query, adapter, item_id): \t \t \tresult=adapter.send_query(user_query) \t \tif result: \t\tprint(\"Please reformulate SQL query!\") \t\treturn False \t \titem_id_hash_to_delete=utils.get_item_id_hash(item_id) \t \tblockchain_commit_success=blockchain.delete_blockchain_record(item_id_hash_to_delete) \t \tif blockchain_commit_success: \t\t \t\t \t\tadapter.connection.commit() \t \treturn True def insert(user_query, adapter, new_row, table_name, key, iv): \t \t \t \tresult=adapter.send_query(user_query) \t \tif result: \t\tprint(\"Please reformulate SQL query!\") \t\treturn False \t \t \tnew_row_id=new_row[0] \tnew_itemID_hash=utils.get_item_id_hash(new_row_id) \tnew_itemID_AES=utils.get_encrypted_item_id(new_row_id, key, iv) \tnew_itemID_AES=new_row[0] \tnew_item_hash=utils.get_item_hash_pk_present(new_row) \tnew_item_table_AES=utils.get_encrypted_table(table_name, key, iv) \t \t \tblockchain_commit_success=blockchain.create_blockchain_record(new_itemID_hash, new_itemID_AES, new_item_hash, new_item_table_AES) \t \t \t \tif blockchain_commit_success: \t\t \t\t \t\tadapter.connection.commit() \t \treturn True def query(user_query, adapter, tampered_primary_keys): \t \t \tdf=pd.read_sql_query(user_query, adapter.connection) \tdf[\"tamper_column\"]=0 \t \tif len(tampered_primary_keys) > 0: \t\tfor pk in tampered_primary_keys: \t\t\tdf.loc[df.student_id==pk, \"tamper_column\"]=1 \t \t \tprint(df) \treturn True if __name__=='__main__': \targs=handle_user_args() \tuser_query=args.query \tuser_command=args.command \tif args.row_to_insert: \t\trow_to_insert=args.row_to_insert \t\trow_to_insert=row_to_insert.split(',') \tif args.update_template: \t\tupdate_template=args.update_template \t\tupdate_template=update_template.split(',') \t \t \tif args.pk_to_delete: \t\tpks_to_delete=args.pk_to_delete \t\tpks_to_delete=pks_to_delete.split(',') \t \ttable_name=\"student\" \t \tdatabase=settings[\"database\"] \tusername=settings[\"username\"] \thost_name=settings[\"host_name\"] \tp=settings[\"password\"] \tadapter=MysqlAdapter(host_name, database, username, p) \tadapter.connect() \t \tquery_for_tamper_check=\"select * from \" +table_name \tresults=adapter.send_query(query_for_tamper_check) \tkey=settings[\"aes_key\"] \tiv= settings[\"iv\"] \t \tcipher=AES.new(key, AES.MODE_CBC, iv) \t \t \tcol_encryption_rec=setup.convert_table_to_column_encryption_record(results, table_name) \trow_encryption_recs=setup.convert_table_to_record_encryption_records(results, table_name) \t \t \tprint(\"\\n====Column Encryption Record====\\n\") \tprint(col_encryption_rec.table_name_hash) \tprint(col_encryption_rec.table_name_AES) \tprint(col_encryption_rec.column_hash) \tprint(\"==============================\\n\") \t \t\"\"\" \tprint(\"====Row Encryption Records====\\n\") \ti=1 \tfor rer in row_encryption_recs: \t\tprint(\"record:\", i) \t\tprint(rer.item_id_hash) \t\tprint(rer.item_id_AES) \t\tprint(rer.item_hash) \t\tprint(rer.owned_table_AES) \t\ti +=1 \t\"\"\" \t \trerc_tamper_flag, rerc_tampered_primary_keys=td.rerc(table_name, adapter, key, iv) \t \t \tcerc_tamper_flag, cerc_info=td.cerc(col_encryption_rec, table_name, adapter, key, iv) \tprint(cerc_tamper_flag) \tprint(cerc_info) \t \tif rerc_tamper_flag: \t\t \t\tprint(\"RERC TAMPERING INFO: the data has been tampered with\") \tif cerc_tamper_flag: \t\tprint(\"CERC TAMPERING INFO: the data has been tampered with\") \t \t \t \t \tif user_command==\"query\": \t\tres=query(user_query, adapter, rerc_tampered_primary_keys) \telif user_command==\"insert\": \t\t \t\tres=insert(user_query, adapter, row_to_insert, table_name, key, iv) \telif user_command==\"delete\": \t\tpk_to_delete=pks_to_delete[0] \t\tres=delete(user_query, adapter, pk_to_delete) \t \t\t \t \t \t \t \tadapter.disconnect() \t \t \t \t \t \t \t\"\"\" \tprint(\"\\n====Column Encryption Record====\\n\") \tprint(col_encryption_rec.table_name_hash) \tprint(col_encryption_rec.get_unencrypted_table_name(key, iv)) \tprint(col_encryption_rec.column_hash) \tprint(\"==============================\\n\") \tprint(\"====Row Encryption Records====\\n\") \ti=1 \tfor rer in row_encryption_recs: \t\tprint(\"record:\", i) \t\tprint(rer.item_id_hash) \t\tprint(rer.get_unencrypted_itemID(key, iv)) \t\tprint(rer.item_hash) \t\tprint(rer.get_unencrypted_table_name(key, iv)) \t\ti +=1 \t\"\"\" \t \t \t\"\"\" \tprint(\"\\n====Column Encryption Record====\\n\") \tprint(col_encryption_rec.table_name_hash) \tprint(col_encryption_rec.table_name_AES) \tprint(col_encryption_rec.column_hash) \tprint(\"==============================\\n\") \tprint(\"====Row Encryption Records====\\n\") \ti=1 \tfor rer in row_encryption_recs: \t\tprint(\"record:\", i) \t\tprint(rer.item_id_hash) \t\tprint(rer.item_id_AES) \t\tprint(rer.item_hash) \t\tprint(rer.owned_table_AES) \t\ti +=1 \t\"\"\" \t \t \t \t \t\"\"\" \tprint(\"\\n====Column Encryption Record====\\n\") \tprint(col_encryption_rec.table_name_hash) \tprint(col_encryption_rec.get_unencrypted_table_name(key, iv)) \tprint(col_encryption_rec.column_hash) \tprint(\"==============================\\n\") \tprint(\"====Row Encryption Records====\\n\") \ti=1 \tfor rer in row_encryption_recs: \t\tprint(\"record:\", i) \t\tprint(rer.item_id_hash) \t\tprint(rer.get_unencrypted_itemID(key, iv)) \t\tprint(rer.item_hash) \t\tprint(rer.get_unencrypted_table_name(key, iv)) \t\ti +=1 \t\"\"\" \t \t \t \t \t \t \t\"\"\" \tresult=adapter.send_query(select_students_query) \titem_hash_list=[] \tfor row in result: \t\ttemp_list=[] \t\tfor item in row: \t\t\ttemp_list.append(item) \t\titem_hash_list.append(temp_list) \tprint(item_hash_list) \tprint(table_name) \tprint(len(result)) \t \t \t \t \t \t \t \t \tkey=settings[\"aes_key\"] \tiv=get_random_bytes(16) \tcipher=AES.new(key, AES.MODE_CBC, iv) \t \t \t \t \t \t \trow_encryption_records=[] \t \t \t \tfor row in item_hash_list: \t\trer_temp=RowEncryptionRecord() \t\tpk=row[0] \t\trer_temp.itemID_hash=utils.SHA_256_conversion(pk) \t\trer_temp.itemID_AES=utils.AES_conversion(cipher, pk) \t\trer_temp.item_hash=utils.encode_items(row) \t\trer_temp.owned_table_AES=utils.AES_conversion(cipher, table_name) \t\trow_encryption_records.append(rer_temp) \tfor r in row_encryption_records: \t\tprint(\"===========\") \t\tprint(r.itemID_hash) \t\tprint(r.itemID_AES) \t\tprint(r.item_hash) \t\tprint(r.owned_table_AES) \tcipher_d=AES.new(key, AES.MODE_CBC, iv) \t \t \tfor r in row_encryption_records: \t\tprint(\"===========\") \t\tprint(r.itemID_hash) \t\t \t\tprint(r.item_hash) \t\t \t\t \t \tpks=[] \tfor row in item_hash_list: \t\tpks.append(row[0]) \t \tprint(\"=====Column E Record=====\") \tcer=ColumnEncryptionRecord() \tcer.table_name_hash=utils.SHA_256_conversion(pk) \tcer.aes_table_name=utils.AES_conversion(cipher, pk) \tcer.column_hash=utils.encode_items(pks) \tprint(cer.table_name_hash) \tprint(cer.aes_table_name) \tprint(cer.column_hash) \tprint(cer.get_unencrypted_table_name(key, iv)) \t\"\"\" \t \t \t \t \t \t \t \t \t \t ","sourceWithComments":"import mysql.connector\nfrom mysql.connector import Error\n\nimport hashlib\nimport argparse\nimport base64\n\n# AES encryption\/decryption functions\nfrom Cryptodome.Util.Padding import pad, unpad\nfrom Cryptodome.Random import get_random_bytes\nfrom Cryptodome.Cipher import AES\n\nimport pandas as pd\n\nimport setup\nimport config\nimport utils\nimport tamper_detection as td\nimport sql as s\nimport blockchain\nfrom MysqlAdapter import MysqlAdapter\nfrom RowEncryptionRecord import RowEncryptionRecord\nfrom ColumnEncryptionRecord import ColumnEncryptionRecord\nimport pymysql\n\n\n# settings holds the AES encrypt and decrypt key\nsettings = config.load_config()\n\n\ndef handle_user_args():\n\t\"\"\"Handle the input given by the user.\"\"\"\n\n\tparser = argparse.ArgumentParser(description='TDRB Middleware extension Argument Parser')\n\n\tparser.add_argument('--q', dest='query', required=True, type=str,\n\t\tnargs='+', help='Query for the middleware to send to the relational database')\n\t#parser.add_argument('--t', dest='table', required=True, type=str,\n\t#\tnargs='+', help='Table involved in the query')\n\tparser.add_argument('--c', dest='command', required=True, type=str,\n\t\tnargs='+', help='Command (insert, delete, update, query')\n\tparser.add_argument('--r', dest='row_to_insert', required=False, type=str,\n\t\tnargs='+', help='Command (insert, delete, update, query')\n\tparser.add_argument('--u', dest='update_template', required=False, type=str,\n\t\tnargs='+', help='Command (insert, delete, update, query')\n\tparser.add_argument('--d', dest='pk_to_delete', required=False, type=str,\n\t\tnargs='+', help='Command (insert, delete, update, query')\n\n\targs = parser.parse_args()\n\n\t# check that input from user is valid\n\tif len(args.query) < 1:\n\t\traise argparse.ArgumentTypeError(\"The query must not be empty!\")\n\n\t# handles spaces in the arguments\n\tspace = ' '\n\targs.query = space.join(args.query)\n\targs.command = space.join(args.command)\n\tif args.row_to_insert:\n\t\targs.row_to_insert = space.join(args.row_to_insert)\n\tif args.update_template:\n\t\targs.update_template = space.join(args.update_template)\n\t# TOOD: change the pk_to_delete var name since I am collecting more of them\n\tif args.pk_to_delete:\n\t\targs.pk_to_delete = space.join(args.pk_to_delete)\n\n\treturn args\n\n\n# do LAST. this one might be the hardest one since a second query would be needed to get\n#  the plaintext records to hash with the upated record\ndef update(existing_item_id_hash, new_item_hash):\n\t\n\t# send the user's UPDATE sql command to the DB to see if it is valid\n\tresult = adapter.send_query(user_query)\n\n\t# send to blockchain if the DB statement was valid and worked\n\tif result:\n\t\tprint(\"Please reformulate SQL query!\")\n\t\treturn False\n\t\n\tblockchain_commit_success = blockchain.update_blockchain_record(existing_item_id_hash, new_item_hash)\n\n\t# TODO: FIX THIS!  the adapter.send_query(user_query) is committing to mysql before the actual commit\n\tif blockchain_commit_success:\n\t\t# commit the results to the MySQL DB if both user query is valid\n\t\t#\tand the blockchain insert was successful\n\t\tadapter.connection.commit()\n\t# TODO: add else to return False\n\n\treturn True\n\n\n\ndef delete(user_query, adapter, item_id):\n\t\n\t# send the user's DELETE sql command to the DB to see if it is valid\n\tresult = adapter.send_query(user_query)\n\n\t# send to blockchain if the DB statement was valid and worked\n\tif result:\n\t\tprint(\"Please reformulate SQL query!\")\n\t\treturn False\n\t\n\titem_id_hash_to_delete = utils.get_item_id_hash(item_id)\n\t\n\tblockchain_commit_success = blockchain.delete_blockchain_record(item_id_hash_to_delete)\n\n\t# TODO: FIX THIS!  the adapter.send_query(user_query) is committing to mysql before the actual commit\n\tif blockchain_commit_success:\n\t\t# commit the results to the MySQL DB if both user query is valid\n\t\t#\tand the blockchain insert was successful\n\t\tadapter.connection.commit()\n\t# TODO: add else to return False\n\n\treturn True\n\n\n\n\n# implement second\ndef insert(user_query, adapter, new_row, table_name, key, iv):\n\t# NOTE: user MUST specify a primary key!!!!\n\t\n\t# send the user's insert sql command to the DB to see if it is valid\n\tresult = adapter.send_query(user_query)\n\n\t# send to blockchain if the DB statement was valid and worked\n\tif result:\n\t\tprint(\"Please reformulate SQL query!\")\n\t\treturn False\n\t\n\t\n\tnew_row_id = new_row[0]\n\tnew_itemID_hash = utils.get_item_id_hash(new_row_id)\n\tnew_itemID_AES = utils.get_encrypted_item_id(new_row_id, key, iv)\n\tnew_itemID_AES = new_row[0]\n\tnew_item_hash = utils.get_item_hash_pk_present(new_row)\n\tnew_item_table_AES = utils.get_encrypted_table(table_name, key, iv)\n\t#new_item_table_AES = table_name\n\n\t# create the new record on the blockchain\n\tblockchain_commit_success = blockchain.create_blockchain_record(new_itemID_hash, new_itemID_AES, new_item_hash, new_item_table_AES)\n\n\n\t# check if successful, if so, commit to the DB\n\t\n\t# TODO: FIX THIS!  the adapter.send_query(user_query) is committing to mysql before the actual commit\n\tif blockchain_commit_success:\n\t\t# commit the results to the MySQL DB if both user query is valid\n\t\t#\tand the blockchain insert was successful\n\t\tadapter.connection.commit()\n\t# TODO: add else to return False\n\n\treturn True\n\n\n# implement first\ndef query(user_query, adapter, tampered_primary_keys):\n\n\t# send the user's query to the DB and read in the results as a pandas DF\n\n\t# TODO: replace the adapter connection with pymysql\n\tdf = pd.read_sql_query(user_query, adapter.connection)\n\tdf[\"tamper_column\"] = 0  # set the detect column to false\n\n\t# TODO: name all PRIMARY KEY columns to ID in mysql!\n\tif len(tampered_primary_keys) > 0:\n\t\tfor pk in tampered_primary_keys:\n\t\t\tdf.loc[df.student_id == pk, \"tamper_column\"] = 1  # 1 indicates tampered\n\t\n\t# print the results\n\tprint(df)\n\n\treturn True\n\n\n\n\nif __name__ == '__main__':\n\n\targs = handle_user_args()\n\n\tuser_query = args.query\n\tuser_command = args.command\n\tif args.row_to_insert:\n\t\trow_to_insert = args.row_to_insert\n\t\trow_to_insert = row_to_insert.split(',')\n\tif args.update_template:\n\t\tupdate_template = args.update_template\n\t\tupdate_template = update_template.split(',')\n\t\n\t# TOOD: have primary keys to delete as a LIST data structure so I can delete more than 1\n\tif args.pk_to_delete:\n\t\tpks_to_delete = args.pk_to_delete\n\t\tpks_to_delete = pks_to_delete.split(',')\n\n\t\n\ttable_name = \"student\"\n\t#table_list = [\"student\"]\n\tdatabase = settings[\"database\"]\n\tusername = settings[\"username\"]\n\thost_name = settings[\"host_name\"]\n\tp = settings[\"password\"]\n\n\n\tadapter = MysqlAdapter(host_name, database, username, p)\n\tadapter.connect()\n\n\n\t# for table in list_of_sql_tables:\n\tquery_for_tamper_check = \"select * from \" + table_name  # construct the query for getting the data from the table\n\tresults = adapter.send_query(query_for_tamper_check)\n\n\n\n\n\tkey = settings[\"aes_key\"]\n\tiv =  settings[\"iv\"]\n\t#iv = get_random_bytes(16)\n\tcipher = AES.new(key, AES.MODE_CBC, iv)\n\t\n\n\t# get the column encryption records and row encryption records for the database table\n\tcol_encryption_rec = setup.convert_table_to_column_encryption_record(results, table_name)\n\trow_encryption_recs = setup.convert_table_to_record_encryption_records(results, table_name)\n\n\t# read the encrypted records\n\t\n\tprint(\"\\n====Column Encryption Record====\\n\")\n\tprint(col_encryption_rec.table_name_hash)\n\tprint(col_encryption_rec.table_name_AES)\n\tprint(col_encryption_rec.column_hash)\n\tprint(\"==============================\\n\")\n\t\n\t\"\"\"\n\tprint(\"====Row Encryption Records====\\n\")\n\ti = 1\n\tfor rer in row_encryption_recs:\n\t\tprint(\"record :\", i)\n\t\tprint(rer.item_id_hash)\n\t\tprint(rer.item_id_AES)\n\t\tprint(rer.item_hash)\n\t\tprint(rer.owned_table_AES)\n\t\ti += 1\n\t\"\"\"\n\n\t# detect illegal modification step\n\trerc_tamper_flag, rerc_tampered_primary_keys = td.rerc(table_name, adapter, key, iv)\n\t#rerc_tamper_flag = 0\n\n\t# detect illegal insert or delete step\n\tcerc_tamper_flag, cerc_info = td.cerc(col_encryption_rec, table_name, adapter, key, iv)\n\tprint(cerc_tamper_flag)\n\tprint(cerc_info)\n\n\n\t# print the tampering info to the user before showing them the query results\n\tif rerc_tamper_flag:\n\t\t# return tamper information\n\t\tprint(\"RERC TAMPERING INFO: the data has been tampered with\")\n\tif cerc_tamper_flag:\n\t\tprint(\"CERC TAMPERING INFO: the data has been tampered with\")\n\t\n\t\n\t# data not tampered with\n\n\t# handle the user's query when no detection\n\tif user_command == \"query\":\n\t\tres = query(user_query, adapter, rerc_tampered_primary_keys)\n\telif user_command == \"insert\":\n\t\t# NOTE: DO NOT INSERT IF THE TAMPERING FLAG IS TRUE!\n\t\tres = insert(user_query, adapter, row_to_insert, table_name, key, iv)\n\telif user_command == \"delete\":\n\t\tpk_to_delete = pks_to_delete[0]\n\t\tres = delete(user_query, adapter, pk_to_delete)\n\t#elif user_command == \"update\":\n\t\t# NOTE: DO NOT INSERT IF THE TAMPERING FLAG IS TRUE!\n\t#\titem_id = update_template[0]\n\t#\titem_id = update_template[0]\n\t#\tres = update(user_query, adapter, row_to_insert, table_name, key, iv)\n\n\n\n\t# disconnect from database\n\tadapter.disconnect()\n\n\n\t# add all of the row encryption records to the blockchain:\n\n\t#for rer in row_encryption_recs:\n\t#\tprint(rer)\n\t#\tblockchain.create_blockchain_record(rer.item_id_hash, rer.item_id_AES, rer.item_hash, rer.owned_table_AES)\n\n\n\n\n\t\n\n\t# read the original records\n\t\"\"\"\n\tprint(\"\\n====Column Encryption Record====\\n\")\n\tprint(col_encryption_rec.table_name_hash)\n\tprint(col_encryption_rec.get_unencrypted_table_name(key, iv))\n\tprint(col_encryption_rec.column_hash)\n\tprint(\"==============================\\n\")\n\n\tprint(\"====Row Encryption Records====\\n\")\n\ti = 1\n\tfor rer in row_encryption_recs:\n\t\tprint(\"record :\", i)\n\t\tprint(rer.item_id_hash)\n\t\tprint(rer.get_unencrypted_itemID(key, iv))\n\t\tprint(rer.item_hash)\n\t\tprint(rer.get_unencrypted_table_name(key, iv))\n\t\ti += 1\n\t\"\"\"\n\n\n\t\n\n\n\t# read the ENCRYPTED records\n\t\"\"\"\n\tprint(\"\\n====Column Encryption Record====\\n\")\n\tprint(col_encryption_rec.table_name_hash)\n\tprint(col_encryption_rec.table_name_AES)\n\tprint(col_encryption_rec.column_hash)\n\tprint(\"==============================\\n\")\n\n\tprint(\"====Row Encryption Records====\\n\")\n\ti = 1\n\tfor rer in row_encryption_recs:\n\t\tprint(\"record :\", i)\n\t\tprint(rer.item_id_hash)\n\t\tprint(rer.item_id_AES)\n\t\tprint(rer.item_hash)\n\t\tprint(rer.owned_table_AES)\n\t\ti += 1\n\t\"\"\"\n\n\n\t#all_tables = utils.get_all_database_tables(adapter, database)\n\t#for table_name in all_tables:\n\t#\tprint(table_name)\n\n\n\t# read the original records\n\t\"\"\"\n\tprint(\"\\n====Column Encryption Record====\\n\")\n\tprint(col_encryption_rec.table_name_hash)\n\tprint(col_encryption_rec.get_unencrypted_table_name(key, iv))\n\tprint(col_encryption_rec.column_hash)\n\tprint(\"==============================\\n\")\n\n\tprint(\"====Row Encryption Records====\\n\")\n\ti = 1\n\tfor rer in row_encryption_recs:\n\t\tprint(\"record :\", i)\n\t\tprint(rer.item_id_hash)\n\t\tprint(rer.get_unencrypted_itemID(key, iv))\n\t\tprint(rer.item_hash)\n\t\tprint(rer.get_unencrypted_table_name(key, iv))\n\t\ti += 1\n\t\"\"\"\n\n\t\n\n\t# update students set age = years_old\n\t#insert into students(id, name, age) values (4, \"james\", 49)\n\t# create table student(student_id INT NOT NULL,name VARCHAR(25) NOT NULL,sex VARCHAR(25) NOT NULL,age INT NOT NULL,PRIMARY KEY ( student_id ));\n\n\t#td.cerc(table_list, adapter, key, iv)\n\n\t#td.rerc(table_name, adapter, key, iv)\n\n\t\"\"\"\n\tresult = adapter.send_query(select_students_query)\n\n\titem_hash_list = []\n\tfor row in result:\n\t\ttemp_list = []\n\t\tfor item in row:\n\t\t\ttemp_list.append(item)\n\t\titem_hash_list.append(temp_list)\n\n\tprint(item_hash_list)\n\n\n\tprint(table_name)\n\tprint(len(result))\n\t# convert the results into the row_encryption_records\n\n\t#m = hashlib.sha256()\n\n\t#m.update(b\"hello\")\n\n\t#print(m.hexdigest())\n\n\t#print(m.digest_size)\n\t#print(m.block_size)\n\n\n\t# row encryption records\n\n\t#data = b'Text to encrypt'   # 15 bytes\n\tkey = settings[\"aes_key\"]\n\tiv = get_random_bytes(16)\n\tcipher = AES.new(key, AES.MODE_CBC, iv)\n\t#ct = cipher1.encrypt(pad(data, 16))\n\n\t# table 1\n\t#student_ID = [1, 2, 3]\n\t#name = ['Alice', 'Bob', 'Peter']\n\t#sex = ['Female', 'Male', 'Male']\n\t#age = [\"20\", \"21\", \"22\"]\n\n\trow_encryption_records = []\n\t\n\t# TODO: the below will go in the tampering detection module\n\t#  and the conversion for the actual push to the blockchain\n\tfor row in item_hash_list:\n\t\trer_temp = RowEncryptionRecord()\n\t\tpk = row[0]\n\t\trer_temp.itemID_hash = utils.SHA_256_conversion(pk)\n\t\trer_temp.itemID_AES = utils.AES_conversion(cipher, pk)\n\n\t\trer_temp.item_hash = utils.encode_items(row)\n\t\trer_temp.owned_table_AES = utils.AES_conversion(cipher, table_name)\n\t\trow_encryption_records.append(rer_temp)\n\n\n\tfor r in row_encryption_records:\n\t\tprint(\"===========\")\n\t\tprint(r.itemID_hash)\n\t\tprint(r.itemID_AES)\n\t\tprint(r.item_hash)\n\t\tprint(r.owned_table_AES)\n\n\n\tcipher_d = AES.new(key, AES.MODE_CBC, iv)\n\t#d_data = cipher_d.decrypt(e_data)\n\n\t# decrypt the data\n\tfor r in row_encryption_records:\n\t\tprint(\"===========\")\n\t\tprint(r.itemID_hash)\n\t\t#print(r.get_unencrypted_itemID(key, iv))\n\t\tprint(r.item_hash)\n\t\t# TODO: fix the below\n\t\t#print(r.get_unencrypted_owned_table(key, iv))\n\n\n\n\t# column encryption records\n\tpks = []\n\tfor row in item_hash_list:\n\t\tpks.append(row[0])\n\t\n\tprint(\"===== Column E Record=====\")\n\tcer = ColumnEncryptionRecord()\n\tcer.table_name_hash = utils.SHA_256_conversion(pk)\n\tcer.aes_table_name = utils.AES_conversion(cipher, pk)\n\tcer.column_hash = utils.encode_items(pks)\n\n\tprint(cer.table_name_hash)\n\tprint(cer.aes_table_name)\n\tprint(cer.column_hash)\n\n\tprint(cer.get_unencrypted_table_name(key, iv))\n\t\"\"\"\n\n\n\t#key = get_random_bytes(32)\n\t\n\t\n\n\t\n\t\n\t#print(ct)\n\n\t#cipher2 = AES.new(key, AES.MODE_CBC, iv)\n\t#pt = unpad(cipher2.decrypt(ct), 16)\n\t#print(pt)\n\t#assert(data == pt)"},"\/src\/tamper_detection.py":{"changes":[{"diff":"\n \ttamper_flag = 0  # zero indicates no tampering\n \ttamper_info = \"\"\n \n-\tprint(\"\\n====Column Encryption Record====\\n\")\n-\tprint(col_encryption_rec.table_name_hash)\n-\tprint(col_encryption_rec.table_name_AES)\n-\tprint(col_encryption_rec.column_hash)\n-\tprint(\"==============================\\n\")\n-\n \ttry:\n \t\tblockchain_result = check_output(['node', 'query.js', col_encryption_rec.table_name_hash])\n \texcept:\n \t\tblockchain_result = \"\"  # no result for the query found above\n \n-\tprint(\"CER column hash: \", col_encryption_rec.column_hash)\n-\tprint(str(blockchain_result))\n \tif col_encryption_rec.column_hash not in str(blockchain_result):\n-\t\t\tprint(\"tampering detected: ILLEGAL INSERT OR DELETE\")\n \t\t\ttamper_info = \"tampering detected: ILLEGAL INSERT OR DELETE\"\n \t\t\ttamper_flag = 1\n \n-\t#table_name_hash = utils.get_table_name_hash(table_name)\n-\t#column_hash = utils.get_column_hash(results)\n-\t#table_name_aes = utils.get_encrypted_table(table_name, key, iv)\n-\n \treturn tamper_flag, tamper_info\n \n-\t\n-\t\n-\t\n-\t# TODO: ADD SOME MECHANISM TO ACTUALLY ITERATE OVER TABLES INSTEAD OF ROWS\n-\t\"\"\"\n-\ttables = []\n-\tfor table_name in table_list:\n-\t\t\n-\t\tquery = \"select * from \" + table_name  # construct the query for getting the data from the table\n-\n-\t\tresults = adapter.send_query(query)\n-\n-\t\tcolumn_hash = utils.get_column_hash(results)\n-\n-\t\ttable_name_aes = utils.get_encrypted_table(table_name, key, iv)\n-\n-\t\tcolumn_hash = utils.get_column_hash(results)\n-\t\ttable_name_aes = utils.get_encrypted_table(table_name, key, iv)\n-\t\ttable_name_hash = utils.get_table_name_hash(table_name)\n-\n-\t\t#print(column_hash)\n-\t\t#print(table_name_aes)\n-\t\t#print(table_name_hash)\n-\n-\t\t# query_column_hash, redis_flag = access_redis(table_name_hash)\n-\n-\t# redis_flag represents cache hit (1) or cache miss (0)\n-\n-\t\tif column_hash != query_column_hash:\n-\t\t\tif redis_flag == 0:\n-\t\t\t\ttamper_flag = 1\n-\t\t\t\t#b_tabes = rich_query_blockchain_(table)   send query to get blockchain records\n-\t\t\t\tr_tables = get_all_row_encryption_records(table)\n-\t\t\t\tillegal_insert, illegal_delete, illegal_modify = check_tamper(b_tables, r_tables)\n-\t\t\telse:\n-\t\t\t\tclear Redis\n-\t\t\t\tget query_column_hash from Blockchain\n-\t\t\t\tif column_hash != query_column_hash:\n-\t\t\t\t\ttamper_flag = 1\n-\t\t\t\t\t#b_tables = rich_query_blockchain_(table)  # send query to get blockchain records\n-\t\t\t\t\tr_tables = get_all_row_encryption_records(table)\n-\t\t\t\t\tillegal_insert, illegal_delete, illegal_modify = check_tamper(b_tables, r_tables)\n-\t\t\t\telse:\n-\t\t\t\t\ttamper_flag = check_tamper(b_tables, r_tables)\n-\tif tamper_flag == 0:\n-\t\treturn table  # return the query results from the relational DB\n-\telse:\n-\t\ttamper_information = show_tamper_information(illegal_insert, illegal_delete, illegal_modify)\n-\t\tprint(tamper_information)\n-\n-\treturn tamper_information, table  # table = query results\n-\t\"\"\"\n \n # row encryption record comparison\n-def rerc(table_name, adapter, key, iv):\n+def rerc(results, table_name, adapter, key, iv):\n \t\"\"\" \"\"\"\n \n \ttamper_flag = 0  # zero indicates no tampering\n \ttampered_records_primary_key_list = []\n \n \t#TODO: note - I already have these\n-\tquery = \"select * from \" + table_name  # construct the query for getting the data from the table\n-\tresults = adapter.send_query(query)\n+\t#query = \"select * from \" + table_name  # construct the query for getting the data from the table\n+\t#results = adapter.send_query(query)\n \t\n \tfor row in results:\n \t\titem_id = row[0]\n","add":3,"remove":68,"filename":"\/src\/tamper_detection.py","badparts":["\tprint(\"\\n====Column Encryption Record====\\n\")","\tprint(col_encryption_rec.table_name_hash)","\tprint(col_encryption_rec.table_name_AES)","\tprint(col_encryption_rec.column_hash)","\tprint(\"==============================\\n\")","\tprint(\"CER column hash: \", col_encryption_rec.column_hash)","\tprint(str(blockchain_result))","\t\t\tprint(\"tampering detected: ILLEGAL INSERT OR DELETE\")","\t","\t","\t","\t\"\"\"","\ttables = []","\tfor table_name in table_list:","\t\t","\t\tquery = \"select * from \" + table_name  # construct the query for getting the data from the table","\t\tresults = adapter.send_query(query)","\t\tcolumn_hash = utils.get_column_hash(results)","\t\ttable_name_aes = utils.get_encrypted_table(table_name, key, iv)","\t\tcolumn_hash = utils.get_column_hash(results)","\t\ttable_name_aes = utils.get_encrypted_table(table_name, key, iv)","\t\ttable_name_hash = utils.get_table_name_hash(table_name)","\t\tif column_hash != query_column_hash:","\t\t\tif redis_flag == 0:","\t\t\t\ttamper_flag = 1","\t\t\t\tr_tables = get_all_row_encryption_records(table)","\t\t\t\tillegal_insert, illegal_delete, illegal_modify = check_tamper(b_tables, r_tables)","\t\t\telse:","\t\t\t\tclear Redis","\t\t\t\tget query_column_hash from Blockchain","\t\t\t\tif column_hash != query_column_hash:","\t\t\t\t\ttamper_flag = 1","\t\t\t\t\tr_tables = get_all_row_encryption_records(table)","\t\t\t\t\tillegal_insert, illegal_delete, illegal_modify = check_tamper(b_tables, r_tables)","\t\t\t\telse:","\t\t\t\t\ttamper_flag = check_tamper(b_tables, r_tables)","\tif tamper_flag == 0:","\t\treturn table  # return the query results from the relational DB","\telse:","\t\ttamper_information = show_tamper_information(illegal_insert, illegal_delete, illegal_modify)","\t\tprint(tamper_information)","\treturn tamper_information, table  # table = query results","\t\"\"\"","def rerc(table_name, adapter, key, iv):","\tquery = \"select * from \" + table_name  # construct the query for getting the data from the table","\tresults = adapter.send_query(query)"],"goodparts":["def rerc(results, table_name, adapter, key, iv):"]}],"source":"\n\"\"\" Tamper Detection Module These algorithms are implemented with the guidance of the following study: https:\/\/ieeexplore-ieee-org.ezproxy.cul.columbia.edu\/document\/9417201 J. Lian, S. Wang and Y. Xie, \"TDRB: An Efficient Tamper-Proof Detection Middleware for Relational Database Based on Blockchain Technology,\" in IEEE Access, vol. 9, pp. 66707-66722, 2021, doi: 10.1109\/ACCESS.2021.3076235. \"\"\" from Cryptodome.Util.Padding import pad, unpad from Cryptodome.Random import get_random_bytes from Cryptodome.Cipher import AES import utils import hashlib from RowEncryptionRecord import RowEncryptionRecord from ColumnEncryptionRecord import ColumnEncryptionRecord from MysqlAdapter import MysqlAdapter from subprocess import check_output def cerc(col_encryption_rec, table_name, adapter, key, iv): \t\"\"\" Get all DB tables involved in the original query statement and \tquery all the data in each table. Calculate the column encryption \trecord according to the TDRB middleware study. \"\"\" \t \ttamper_flag=0 \ttamper_info=\"\" \tprint(\"\\n====Column Encryption Record====\\n\") \tprint(col_encryption_rec.table_name_hash) \tprint(col_encryption_rec.table_name_AES) \tprint(col_encryption_rec.column_hash) \tprint(\"==============================\\n\") \ttry: \t\tblockchain_result=check_output(['node', 'query.js', col_encryption_rec.table_name_hash]) \texcept: \t\tblockchain_result=\"\" \tprint(\"CER column hash: \", col_encryption_rec.column_hash) \tprint(str(blockchain_result)) \tif col_encryption_rec.column_hash not in str(blockchain_result): \t\t\tprint(\"tampering detected: ILLEGAL INSERT OR DELETE\") \t\t\ttamper_info=\"tampering detected: ILLEGAL INSERT OR DELETE\" \t\t\ttamper_flag=1 \t \t \t \treturn tamper_flag, tamper_info \t \t \t \t \t\"\"\" \ttables=[] \tfor table_name in table_list: \t\t \t\tquery=\"select * from \" +table_name \t\tresults=adapter.send_query(query) \t\tcolumn_hash=utils.get_column_hash(results) \t\ttable_name_aes=utils.get_encrypted_table(table_name, key, iv) \t\tcolumn_hash=utils.get_column_hash(results) \t\ttable_name_aes=utils.get_encrypted_table(table_name, key, iv) \t\ttable_name_hash=utils.get_table_name_hash(table_name) \t\t \t\t \t\t \t\t \t \t\tif column_hash !=query_column_hash: \t\t\tif redis_flag==0: \t\t\t\ttamper_flag=1 \t\t\t\t \t\t\t\tr_tables=get_all_row_encryption_records(table) \t\t\t\tillegal_insert, illegal_delete, illegal_modify=check_tamper(b_tables, r_tables) \t\t\telse: \t\t\t\tclear Redis \t\t\t\tget query_column_hash from Blockchain \t\t\t\tif column_hash !=query_column_hash: \t\t\t\t\ttamper_flag=1 \t\t\t\t\t \t\t\t\t\tr_tables=get_all_row_encryption_records(table) \t\t\t\t\tillegal_insert, illegal_delete, illegal_modify=check_tamper(b_tables, r_tables) \t\t\t\telse: \t\t\t\t\ttamper_flag=check_tamper(b_tables, r_tables) \tif tamper_flag==0: \t\treturn table \telse: \t\ttamper_information=show_tamper_information(illegal_insert, illegal_delete, illegal_modify) \t\tprint(tamper_information) \treturn tamper_information, table \t\"\"\" def rerc(table_name, adapter, key, iv): \t\"\"\" \"\"\" \ttamper_flag=0 \ttampered_records_primary_key_list=[] \t \tquery=\"select * from \" +table_name \tresults=adapter.send_query(query) \t \tfor row in results: \t\titem_id=row[0] \t\titem_id_hash=utils.get_item_id_hash(item_id) \t\titem_hash=utils.get_item_hash_pk_present(row) \t\titem_id_aes=utils.get_encrypted_item_id(item_id, key, iv) \t\towned_table_aes=utils.get_encrypted_table(table_name, key, iv) \t\ttry: \t\t\tblockchain_result=check_output(['node', 'query.js', item_id_hash]) \t\texcept: \t\t\t \t\t\tblockchain_result=\"\" \t\t \t\tif item_hash not in str(blockchain_result): \t\t\tprint(\"tampering detected: ILLEGAL MODIFICATION\") \t\t\ttampered_records_primary_key_list.append(item_id) \t\t\ttamper_flag=1 \treturn tamper_flag, tampered_records_primary_key_list ","sourceWithComments":"\"\"\"\nTamper Detection Module\nThese algorithms are implemented with the guidance of the following study:\nhttps:\/\/ieeexplore-ieee-org.ezproxy.cul.columbia.edu\/document\/9417201\n\nJ. Lian, S. Wang and Y. Xie, \"TDRB: An Efficient Tamper-Proof Detection Middleware for Relational Database Based on Blockchain Technology,\" \nin IEEE Access, vol. 9, pp. 66707-66722, 2021, doi: 10.1109\/ACCESS.2021.3076235.\n\"\"\"\n\n# AES encryption\/decryption functions\nfrom Cryptodome.Util.Padding import pad, unpad\nfrom Cryptodome.Random import get_random_bytes\nfrom Cryptodome.Cipher import AES\n\nimport utils\nimport hashlib\nfrom RowEncryptionRecord import RowEncryptionRecord\nfrom ColumnEncryptionRecord import ColumnEncryptionRecord\nfrom MysqlAdapter import MysqlAdapter\n\n\nfrom subprocess import check_output\n\n\n# column encryption record comparison\ndef cerc(col_encryption_rec, table_name, adapter, key, iv):\n\t\"\"\" Get all DB tables involved in the original query statement and\n\tquery all the data in each table. Calculate the column encryption\n\trecord according to the TDRB middleware study. \"\"\"\n\t\n\ttamper_flag = 0  # zero indicates no tampering\n\ttamper_info = \"\"\n\n\tprint(\"\\n====Column Encryption Record====\\n\")\n\tprint(col_encryption_rec.table_name_hash)\n\tprint(col_encryption_rec.table_name_AES)\n\tprint(col_encryption_rec.column_hash)\n\tprint(\"==============================\\n\")\n\n\ttry:\n\t\tblockchain_result = check_output(['node', 'query.js', col_encryption_rec.table_name_hash])\n\texcept:\n\t\tblockchain_result = \"\"  # no result for the query found above\n\n\tprint(\"CER column hash: \", col_encryption_rec.column_hash)\n\tprint(str(blockchain_result))\n\tif col_encryption_rec.column_hash not in str(blockchain_result):\n\t\t\tprint(\"tampering detected: ILLEGAL INSERT OR DELETE\")\n\t\t\ttamper_info = \"tampering detected: ILLEGAL INSERT OR DELETE\"\n\t\t\ttamper_flag = 1\n\n\t#table_name_hash = utils.get_table_name_hash(table_name)\n\t#column_hash = utils.get_column_hash(results)\n\t#table_name_aes = utils.get_encrypted_table(table_name, key, iv)\n\n\treturn tamper_flag, tamper_info\n\n\t\n\t\n\t\n\t# TODO: ADD SOME MECHANISM TO ACTUALLY ITERATE OVER TABLES INSTEAD OF ROWS\n\t\"\"\"\n\ttables = []\n\tfor table_name in table_list:\n\t\t\n\t\tquery = \"select * from \" + table_name  # construct the query for getting the data from the table\n\n\t\tresults = adapter.send_query(query)\n\n\t\tcolumn_hash = utils.get_column_hash(results)\n\n\t\ttable_name_aes = utils.get_encrypted_table(table_name, key, iv)\n\n\t\tcolumn_hash = utils.get_column_hash(results)\n\t\ttable_name_aes = utils.get_encrypted_table(table_name, key, iv)\n\t\ttable_name_hash = utils.get_table_name_hash(table_name)\n\n\t\t#print(column_hash)\n\t\t#print(table_name_aes)\n\t\t#print(table_name_hash)\n\n\t\t# query_column_hash, redis_flag = access_redis(table_name_hash)\n\n\t# redis_flag represents cache hit (1) or cache miss (0)\n\n\t\tif column_hash != query_column_hash:\n\t\t\tif redis_flag == 0:\n\t\t\t\ttamper_flag = 1\n\t\t\t\t#b_tabes = rich_query_blockchain_(table)   send query to get blockchain records\n\t\t\t\tr_tables = get_all_row_encryption_records(table)\n\t\t\t\tillegal_insert, illegal_delete, illegal_modify = check_tamper(b_tables, r_tables)\n\t\t\telse:\n\t\t\t\tclear Redis\n\t\t\t\tget query_column_hash from Blockchain\n\t\t\t\tif column_hash != query_column_hash:\n\t\t\t\t\ttamper_flag = 1\n\t\t\t\t\t#b_tables = rich_query_blockchain_(table)  # send query to get blockchain records\n\t\t\t\t\tr_tables = get_all_row_encryption_records(table)\n\t\t\t\t\tillegal_insert, illegal_delete, illegal_modify = check_tamper(b_tables, r_tables)\n\t\t\t\telse:\n\t\t\t\t\ttamper_flag = check_tamper(b_tables, r_tables)\n\tif tamper_flag == 0:\n\t\treturn table  # return the query results from the relational DB\n\telse:\n\t\ttamper_information = show_tamper_information(illegal_insert, illegal_delete, illegal_modify)\n\t\tprint(tamper_information)\n\n\treturn tamper_information, table  # table = query results\n\t\"\"\"\n\n# row encryption record comparison\ndef rerc(table_name, adapter, key, iv):\n\t\"\"\" \"\"\"\n\n\ttamper_flag = 0  # zero indicates no tampering\n\ttampered_records_primary_key_list = []\n\n\t#TODO: note - I already have these\n\tquery = \"select * from \" + table_name  # construct the query for getting the data from the table\n\tresults = adapter.send_query(query)\n\t\n\tfor row in results:\n\t\titem_id = row[0]\n\t\titem_id_hash = utils.get_item_id_hash(item_id)\n\t\titem_hash = utils.get_item_hash_pk_present(row)\n\t\titem_id_aes = utils.get_encrypted_item_id(item_id, key, iv)\n\t\towned_table_aes = utils.get_encrypted_table(table_name, key, iv)\n\n\n\t\ttry:\n\t\t\tblockchain_result = check_output(['node', 'query.js', item_id_hash])\n\t\texcept:\n\t\t\t#print(\"CAUGHT EXCEPTION\")\n\t\t\tblockchain_result = \"\"  # no result for the query found above\n\t\t\n\t\tif item_hash not in str(blockchain_result):\n\t\t\tprint(\"tampering detected: ILLEGAL MODIFICATION\")\n\t\t\ttampered_records_primary_key_list.append(item_id)\n\t\t\ttamper_flag = 1\n\n\treturn tamper_flag, tampered_records_primary_key_list\n\n\n\n"}},"msg":"optimized tampering check to only query 1 time"}},"https:\/\/github.com\/ErtugrulSener\/AutomateMyEnvironment":{"9fb713299381634204633ab86722117fe351212a":{"url":"https:\/\/api.github.com\/repos\/ErtugrulSener\/AutomateMyEnvironment\/commits\/9fb713299381634204633ab86722117fe351212a","html_url":"https:\/\/github.com\/ErtugrulSener\/AutomateMyEnvironment\/commit\/9fb713299381634204633ab86722117fe351212a","sha":"9fb713299381634204633ab86722117fe351212a","keyword":"tampering check","diff":"diff --git a\/scripts\/checkers\/system_checker.py b\/scripts\/checkers\/system_checker.py\nindex 28d465a..a481ba4 100644\n--- a\/scripts\/checkers\/system_checker.py\n+++ b\/scripts\/checkers\/system_checker.py\n@@ -55,7 +55,7 @@ def check_for_required_dependencies(self):\n     def check_for_tamper_protection(self):\n         logger.info('Checking if tamper protection feature of Windows Defender is disabled...')\n \n-        if RegistryManager.instance().get(RegistryPath.WINDOWS_DEFENDER_TAMPER_PROTECTION) == 5:\n+        if RegistryManager.instance().get(RegistryPath.WINDOWS_DEFENDER_TAMPER_PROTECTION) != 4:\n             logger.error(\"The tamper protection feature of Windows Defender needs to be disabled!\")\n             exit(5)\n \n","message":"","files":{"\/scripts\/checkers\/system_checker.py":{"changes":[{"diff":"\n     def check_for_tamper_protection(self):\n         logger.info('Checking if tamper protection feature of Windows Defender is disabled...')\n \n-        if RegistryManager.instance().get(RegistryPath.WINDOWS_DEFENDER_TAMPER_PROTECTION) == 5:\n+        if RegistryManager.instance().get(RegistryPath.WINDOWS_DEFENDER_TAMPER_PROTECTION) != 4:\n             logger.error(\"The tamper protection feature of Windows Defender needs to be disabled!\")\n             exit(5)\n \n","add":1,"remove":1,"filename":"\/scripts\/checkers\/system_checker.py","badparts":["        if RegistryManager.instance().get(RegistryPath.WINDOWS_DEFENDER_TAMPER_PROTECTION) == 5:"],"goodparts":["        if RegistryManager.instance().get(RegistryPath.WINDOWS_DEFENDER_TAMPER_PROTECTION) != 4:"]}],"source":"\nimport platform from shutil import which import http.client as httplib from scripts.logging.logger import Logger from scripts.managers import admin_manager from scripts.managers.registry_manager import RegistryManager from scripts.managers.registry_manager import RegistryPath from scripts.singleton import Singleton logger=Logger.instance() @Singleton class SystemChecker: REQUIRED_DEPENDENCIES=[ \"scoop\", ] def check_for_admin_rights(self): logger.info('Checking if script was called as admin(Required)') if not admin_manager.is_user_admin(): logger.error('You need administrator privileges to run this script!') exit(1) def check_if_os_is_suitable(self): logger.info('Checking if it is a suitable OS(Only windows is supported by now)') is_windows=any(platform.win32_ver()) if not is_windows: logger.error('For now, only windows is supported!') exit(2) def check_for_required_dependencies(self): logger.info('Checking for dependencies...') found_all_dependencies=True for dependency in self.REQUIRED_DEPENDENCIES: is_installed=which(dependency) is not None text_to_log=f'{dependency}(Required) ->{\"Found\" if is_installed else \"Not Found\":<10}' if not is_installed: logger.error(text_to_log) found_all_dependencies=False else: logger.debug(text_to_log) if not found_all_dependencies: logger.error('One or more dependencies are missing, install them to proceed!') exit(3) def check_for_tamper_protection(self): logger.info('Checking if tamper protection feature of Windows Defender is disabled...') if RegistryManager.instance().get(RegistryPath.WINDOWS_DEFENDER_TAMPER_PROTECTION)==5: logger.error(\"The tamper protection feature of Windows Defender needs to be disabled!\") exit(5) def check_for_internet_connection(self): logger.info('Checking if user has a persistent internet connection') conn=httplib.HTTPSConnection(\"8.8.8.8\", timeout=5) try: conn.request(\"HEAD\", \"\/\") except Exception: logger.error(\"You need a persistent internet connection to run this script!\") exit(6) finally: conn.close() def check(self): self.check_for_admin_rights() self.check_if_os_is_suitable() self.check_for_required_dependencies() self.check_for_tamper_protection() self.check_for_internet_connection() ","sourceWithComments":"import platform\nfrom shutil import which\n\nimport http.client as httplib\n\nfrom scripts.logging.logger import Logger\nfrom scripts.managers import admin_manager\nfrom scripts.managers.registry_manager import RegistryManager\nfrom scripts.managers.registry_manager import RegistryPath\nfrom scripts.singleton import Singleton\n\nlogger = Logger.instance()\n\n\n@Singleton\nclass SystemChecker:\n    # Required dependencies\n    REQUIRED_DEPENDENCIES = [\n        \"scoop\",\n    ]\n\n    def check_for_admin_rights(self):\n        logger.info('Checking if script was called as admin (Required)')\n\n        if not admin_manager.is_user_admin():\n            logger.error('You need administrator privileges to run this script!')\n            exit(1)\n\n    def check_if_os_is_suitable(self):\n        logger.info('Checking if it is a suitable OS (Only windows is supported by now)')\n        is_windows = any(platform.win32_ver())\n\n        if not is_windows:\n            logger.error('For now, only windows is supported!')\n            exit(2)\n\n    def check_for_required_dependencies(self):\n        logger.info('Checking for dependencies...')\n\n        found_all_dependencies = True\n        for dependency in self.REQUIRED_DEPENDENCIES:\n            is_installed = which(dependency) is not None\n            text_to_log = f'{dependency} (Required) -> {\"Found\" if is_installed else \"Not Found\":<10}'\n\n            if not is_installed:\n                logger.error(text_to_log)\n                found_all_dependencies = False\n            else:\n                logger.debug(text_to_log)\n\n        if not found_all_dependencies:\n            logger.error('One or more dependencies are missing, install them to proceed!')\n            exit(3)\n\n    def check_for_tamper_protection(self):\n        logger.info('Checking if tamper protection feature of Windows Defender is disabled...')\n\n        if RegistryManager.instance().get(RegistryPath.WINDOWS_DEFENDER_TAMPER_PROTECTION) == 5:\n            logger.error(\"The tamper protection feature of Windows Defender needs to be disabled!\")\n            exit(5)\n\n    def check_for_internet_connection(self):\n        logger.info('Checking if user has a persistent internet connection')\n        conn = httplib.HTTPSConnection(\"8.8.8.8\", timeout=5)\n\n        try:\n            conn.request(\"HEAD\", \"\/\")\n        except Exception:\n            logger.error(\"You need a persistent internet connection to run this script!\")\n            exit(6)\n        finally:\n            conn.close()\n\n    def check(self):\n        self.check_for_admin_rights()\n        self.check_if_os_is_suitable()\n        self.check_for_required_dependencies()\n        self.check_for_tamper_protection()\n        self.check_for_internet_connection()\n"}},"msg":"Made check for tamper protection a bit more explicit."},"b32a6b2fbaaf56eedc3c6254e5ff22153ae86490":{"url":"https:\/\/api.github.com\/repos\/ErtugrulSener\/AutomateMyEnvironment\/commits\/b32a6b2fbaaf56eedc3c6254e5ff22153ae86490","html_url":"https:\/\/github.com\/ErtugrulSener\/AutomateMyEnvironment\/commit\/b32a6b2fbaaf56eedc3c6254e5ff22153ae86490","message":"Made check for tamper protection a bit more explicit.","sha":"b32a6b2fbaaf56eedc3c6254e5ff22153ae86490","keyword":"tampering check","diff":"diff --git a\/scripts\/checkers\/system_checker.py b\/scripts\/checkers\/system_checker.py\nindex a481ba4..856502d 100644\n--- a\/scripts\/checkers\/system_checker.py\n+++ b\/scripts\/checkers\/system_checker.py\n@@ -55,7 +55,7 @@ def check_for_required_dependencies(self):\n     def check_for_tamper_protection(self):\n         logger.info('Checking if tamper protection feature of Windows Defender is disabled...')\n \n-        if RegistryManager.instance().get(RegistryPath.WINDOWS_DEFENDER_TAMPER_PROTECTION) != 4:\n+        if RegistryManager.instance().get(RegistryPath.WINDOWS_DEFENDER_TAMPER_PROTECTION) not in [0, 4]:\n             logger.error(\"The tamper protection feature of Windows Defender needs to be disabled!\")\n             exit(5)\n \n","files":{"\/scripts\/checkers\/system_checker.py":{"changes":[{"diff":"\n     def check_for_tamper_protection(self):\n         logger.info('Checking if tamper protection feature of Windows Defender is disabled...')\n \n-        if RegistryManager.instance().get(RegistryPath.WINDOWS_DEFENDER_TAMPER_PROTECTION) != 4:\n+        if RegistryManager.instance().get(RegistryPath.WINDOWS_DEFENDER_TAMPER_PROTECTION) not in [0, 4]:\n             logger.error(\"The tamper protection feature of Windows Defender needs to be disabled!\")\n             exit(5)\n \n","add":1,"remove":1,"filename":"\/scripts\/checkers\/system_checker.py","badparts":["        if RegistryManager.instance().get(RegistryPath.WINDOWS_DEFENDER_TAMPER_PROTECTION) != 4:"],"goodparts":["        if RegistryManager.instance().get(RegistryPath.WINDOWS_DEFENDER_TAMPER_PROTECTION) not in [0, 4]:"]}],"source":"\nimport platform from shutil import which import http.client as httplib from scripts.logging.logger import Logger from scripts.managers import admin_manager from scripts.managers.registry_manager import RegistryManager from scripts.managers.registry_manager import RegistryPath from scripts.singleton import Singleton logger=Logger.instance() @Singleton class SystemChecker: REQUIRED_DEPENDENCIES=[ \"scoop\", ] def check_for_admin_rights(self): logger.info('Checking if script was called as admin(Required)') if not admin_manager.is_user_admin(): logger.error('You need administrator privileges to run this script!') exit(1) def check_if_os_is_suitable(self): logger.info('Checking if it is a suitable OS(Only windows is supported by now)') is_windows=any(platform.win32_ver()) if not is_windows: logger.error('For now, only windows is supported!') exit(2) def check_for_required_dependencies(self): logger.info('Checking for dependencies...') found_all_dependencies=True for dependency in self.REQUIRED_DEPENDENCIES: is_installed=which(dependency) is not None text_to_log=f'{dependency}(Required) ->{\"Found\" if is_installed else \"Not Found\":<10}' if not is_installed: logger.error(text_to_log) found_all_dependencies=False else: logger.debug(text_to_log) if not found_all_dependencies: logger.error('One or more dependencies are missing, install them to proceed!') exit(3) def check_for_tamper_protection(self): logger.info('Checking if tamper protection feature of Windows Defender is disabled...') if RegistryManager.instance().get(RegistryPath.WINDOWS_DEFENDER_TAMPER_PROTECTION) !=4: logger.error(\"The tamper protection feature of Windows Defender needs to be disabled!\") exit(5) def check_for_internet_connection(self): logger.info('Checking if user has a persistent internet connection') conn=httplib.HTTPSConnection(\"8.8.8.8\", timeout=5) try: conn.request(\"HEAD\", \"\/\") except Exception: logger.error(\"You need a persistent internet connection to run this script!\") exit(6) finally: conn.close() def check(self): self.check_for_admin_rights() self.check_if_os_is_suitable() self.check_for_required_dependencies() self.check_for_tamper_protection() self.check_for_internet_connection() ","sourceWithComments":"import platform\nfrom shutil import which\n\nimport http.client as httplib\n\nfrom scripts.logging.logger import Logger\nfrom scripts.managers import admin_manager\nfrom scripts.managers.registry_manager import RegistryManager\nfrom scripts.managers.registry_manager import RegistryPath\nfrom scripts.singleton import Singleton\n\nlogger = Logger.instance()\n\n\n@Singleton\nclass SystemChecker:\n    # Required dependencies\n    REQUIRED_DEPENDENCIES = [\n        \"scoop\",\n    ]\n\n    def check_for_admin_rights(self):\n        logger.info('Checking if script was called as admin (Required)')\n\n        if not admin_manager.is_user_admin():\n            logger.error('You need administrator privileges to run this script!')\n            exit(1)\n\n    def check_if_os_is_suitable(self):\n        logger.info('Checking if it is a suitable OS (Only windows is supported by now)')\n        is_windows = any(platform.win32_ver())\n\n        if not is_windows:\n            logger.error('For now, only windows is supported!')\n            exit(2)\n\n    def check_for_required_dependencies(self):\n        logger.info('Checking for dependencies...')\n\n        found_all_dependencies = True\n        for dependency in self.REQUIRED_DEPENDENCIES:\n            is_installed = which(dependency) is not None\n            text_to_log = f'{dependency} (Required) -> {\"Found\" if is_installed else \"Not Found\":<10}'\n\n            if not is_installed:\n                logger.error(text_to_log)\n                found_all_dependencies = False\n            else:\n                logger.debug(text_to_log)\n\n        if not found_all_dependencies:\n            logger.error('One or more dependencies are missing, install them to proceed!')\n            exit(3)\n\n    def check_for_tamper_protection(self):\n        logger.info('Checking if tamper protection feature of Windows Defender is disabled...')\n\n        if RegistryManager.instance().get(RegistryPath.WINDOWS_DEFENDER_TAMPER_PROTECTION) != 4:\n            logger.error(\"The tamper protection feature of Windows Defender needs to be disabled!\")\n            exit(5)\n\n    def check_for_internet_connection(self):\n        logger.info('Checking if user has a persistent internet connection')\n        conn = httplib.HTTPSConnection(\"8.8.8.8\", timeout=5)\n\n        try:\n            conn.request(\"HEAD\", \"\/\")\n        except Exception:\n            logger.error(\"You need a persistent internet connection to run this script!\")\n            exit(6)\n        finally:\n            conn.close()\n\n    def check(self):\n        self.check_for_admin_rights()\n        self.check_if_os_is_suitable()\n        self.check_for_required_dependencies()\n        self.check_for_tamper_protection()\n        self.check_for_internet_connection()\n"}},"msg":"Made check for tamper protection a bit more explicit."}},"https:\/\/github.com\/UCSC-Geocaching\/slugcache":{"a8e419b3c1b7a4ba11a27c2153aeaa3f65b9d642":{"url":"https:\/\/api.github.com\/repos\/UCSC-Geocaching\/slugcache\/commits\/a8e419b3c1b7a4ba11a27c2153aeaa3f65b9d642","html_url":"https:\/\/github.com\/UCSC-Geocaching\/slugcache\/commit\/a8e419b3c1b7a4ba11a27c2153aeaa3f65b9d642","message":"Moved log timer check to server so it can't be tampered with js","sha":"a8e419b3c1b7a4ba11a27c2153aeaa3f65b9d642","keyword":"tampering check","diff":"diff --git a\/controllers.py b\/controllers.py\nindex 1fe8846..774fdbe 100644\n--- a\/controllers.py\n+++ b\/controllers.py\n@@ -25,6 +25,7 @@\n Warning: Fixtures MUST be declared with @action.uses({fixtures}) else your app will result in undefined behavior\n \"\"\"\n \n+from unittest import result\n from webbrowser import get\n from requests import delete\n from py4web import action, request, abort, redirect, URL\n@@ -227,6 +228,15 @@ def logCache(cache_id=None):\n     assert cache_id is not None\n     assert discover_date is not None\n \n+    # Check the timer before logging\n+    newest_log = (\n+        db((db.logs.cache == cache_id) & (db.logs.logger == user[\"id\"])).select().last()\n+    )\n+    result = checkLogTimer(newest_log)\n+    if result[\"disabled\"]:\n+        return dict(log=None)\n+\n+    # Add the log\n     log_id = db.logs.insert(logger=user.id, cache=cache_id, discover_date=discover_date)\n     log = db.logs[log_id]\n     log[\"first_name\"] = auth_user_data[\"first_name\"]\n@@ -234,6 +244,7 @@ def logCache(cache_id=None):\n \n     # Update caches loged counter\n     db(db.users.id == user.id).update(caches_logged=user.caches_logged + 1)\n+\n     return dict(log=log)\n \n \n@@ -264,19 +275,11 @@ def checkTimer(cache_id=None):\n     newest_log = (\n         db((db.logs.cache == cache_id) & (db.logs.logger == user[\"id\"])).select().last()\n     )\n-    # No log at this cache for this user\n-    if newest_log is None:\n-        return dict(disabled=False)\n-    # Check if the most recent log is old enough\n-    log_time = newest_log[\"discover_date\"]\n-    refresh_time = log_time + timedelta(minutes=15)\n-    time_now = datetime.now()\n-    # Now is past the refresh time limit\n-    if time_now > refresh_time:\n-        return dict(disabled=False, refresh_time=refresh_time)\n-    # Is hasn't been enough time yet\n-    else:\n-        return dict(disabled=True, refresh_time=refresh_time)\n+    result = checkLogTimer(newest_log)\n+    return dict(\n+        disabled=result[\"disabled\"],\n+        refresh_time=result[\"refresh_time\"],\n+    )\n \n \n # Suggest Page Controllers-------------------------------------------\n@@ -418,3 +421,20 @@ def clear_db():\n     db.users.truncate()\n     db.caches.truncate()\n     redirect(URL(\"index\"))\n+\n+\n+# Helper Functions---------------------------------------------------\n+def checkLogTimer(newest_log=None):\n+    # No log at this cache for this user\n+    if newest_log is None:\n+        return dict(disabled=False)\n+    # Check if the most recent log is old enough\n+    log_time = newest_log[\"discover_date\"]\n+    refresh_time = log_time + timedelta(minutes=15)\n+    time_now = datetime.now()\n+    # Now is past the refresh time limit\n+    if time_now > refresh_time:\n+        return dict(disabled=False, refresh_time=refresh_time)\n+    # Is hasn't been enough time yet\n+    else:\n+        return dict(disabled=True, refresh_time=refresh_time)\n\\ No newline at end of file\ndiff --git a\/static\/js\/cache_info.js b\/static\/js\/cache_info.js\nindex 887a4ea..186781d 100644\n--- a\/static\/js\/cache_info.js\n+++ b\/static\/js\/cache_info.js\n@@ -78,7 +78,10 @@ let init = (app) => {\n \n   app.logCache = function () {\n     axios.put(logCacheURL).then(function (r) {\n-      app.getLogs();\n+      if (r.data.log != null) {\n+        app.getLogs();\n+      }\n+      app.checkTimer();\n     });\n   };\n \n@@ -97,8 +100,6 @@ let init = (app) => {\n       tmpLogs.length = Math.min(tmpLogs.length, 5);\n       app.vue.cache_logs = tmpLogs;\n     });\n-    \/\/ Check if the log button should be disabled.\n-    app.checkTimer();\n   };\n \n   app.checkTimer = function () {\n@@ -137,6 +138,7 @@ let init = (app) => {\n     axios.get(getCacheURL).then(function (r) {\n       app.processCache(r.data.cache);\n       app.getLogs();\n+      app.checkTimer();\n     });\n \n     setTimeout(() => {\n","files":{"\/controllers.py":{"changes":[{"diff":"\n     newest_log = (\n         db((db.logs.cache == cache_id) & (db.logs.logger == user[\"id\"])).select().last()\n     )\n-    # No log at this cache for this user\n-    if newest_log is None:\n-        return dict(disabled=False)\n-    # Check if the most recent log is old enough\n-    log_time = newest_log[\"discover_date\"]\n-    refresh_time = log_time + timedelta(minutes=15)\n-    time_now = datetime.now()\n-    # Now is past the refresh time limit\n-    if time_now > refresh_time:\n-        return dict(disabled=False, refresh_time=refresh_time)\n-    # Is hasn't been enough time yet\n-    else:\n-        return dict(disabled=True, refresh_time=refresh_time)\n+    result = checkLogTimer(newest_log)\n+    return dict(\n+        disabled=result[\"disabled\"],\n+        refresh_time=result[\"refresh_time\"],\n+    )\n \n \n # Suggest Page Controllers-------------------------------------------\n","add":5,"remove":13,"filename":"\/controllers.py","badparts":["    if newest_log is None:","        return dict(disabled=False)","    log_time = newest_log[\"discover_date\"]","    refresh_time = log_time + timedelta(minutes=15)","    time_now = datetime.now()","    if time_now > refresh_time:","        return dict(disabled=False, refresh_time=refresh_time)","    else:","        return dict(disabled=True, refresh_time=refresh_time)"],"goodparts":["    result = checkLogTimer(newest_log)","    return dict(","        disabled=result[\"disabled\"],","        refresh_time=result[\"refresh_time\"],","    )"]}],"source":"\n\"\"\" This file defines actions, i.e. functions the URLs are mapped into The @action(path) decorator exposed the function at URL: http:\/\/127.0.0.1:8000\/{app_name}\/{path} If app_name=='_default' then simply http:\/\/127.0.0.1:8000\/{path} If path=='index' it can be omitted: http:\/\/127.0.0.1:8000\/ The path follows the bottlepy syntax. @action.uses('generic.html') indicates that the action uses the generic.html template @action.uses(session) indicates that the action uses the session @action.uses(db) indicates that the action uses the db @action.uses(T) indicates that the action uses the i18n & pluralization @action.uses(auth.user) indicates that the action requires a logged in user @action.uses(auth) indicates that the action requires the auth object session, db, T, auth, and tempates are examples of Fixtures. Warning: Fixtures MUST be declared with @action.uses({fixtures}) else your app will result in undefined behavior \"\"\" from webbrowser import get from requests import delete from py4web import action, request, abort, redirect, URL from yatl.helpers import A from.common import( db, session, T, cache, auth, logger, authenticated, unauthenticated, flash, ) from py4web.utils.url_signer import URLSigner from.models import get_user_email from datetime import date, datetime, timedelta url_signer=URLSigner(session) @action(\"login\") @action(\"register\") @action(\"request_reset_password\") @action(\"index\") @action.uses(\"index.html\", auth, url_signer) def index(): logged_out_endpoints=( URL(endpoint) for endpoint in(\"login\", \"register\", \"request_reset_password\") ) if request.fullpath in logged_out_endpoints and auth.is_logged_in: redirect(URL(\"map\")) return{ \"base_url\": URL(), \"add_user_url\": URL(\"add_user\", signer=url_signer), } @action(\"custom_auth\/reset_password\") @action.uses(\"reset_pw.html\", auth) def resetpw(): return{\"base_url\": URL()} @action(\"profile\") @action.uses(\"profile.html\", db, auth.user) def profile(): return dict(load_profile_url=URL(\"load_profile_details\")) @action(\"load_profile_details\") @action.uses(db, auth.user) def load_profile_details(): user=auth.get_user() profile=db(db.users.user_id==user[\"id\"]).select().first() return dict(profile=profile) @action(\"map\") @action.uses(\"map.html\", db, auth, url_signer) def map(): return dict( loadGeoCachesURL=URL(\"loadGeoCaches\", signer=url_signer), searchURL=URL(\"search\", signer=url_signer), generateCacheURL=URL(\"generateCacheURL\", signer=url_signer), ) @action(\"loadGeoCaches\") @action.uses(db) def getCaches(): rows=db(db.caches).select().as_list() return dict(caches=rows) @action(\"search\") @action.uses() def search(): rows=db(db.caches).select().as_list() return dict(caches=rows) @action(\"generateCacheURL\") @action.uses(db, url_signer, url_signer.verify()) def generateCacheURL(): cache_id=int(request.params.get(\"cache_id\")) return dict(url=URL(\"cache_info\", cache_id, signer=url_signer)) @action(\"bookmarks\", method=\"GET\") @action.uses(\"bookmarks.html\", db, auth.user, url_signer) def bookmarks(): return dict(get_bookmarks_url=URL(\"get_bookmarks\", signer=url_signer)) @action(\"get_bookmarks\", method=\"GET\") @action.uses(db, auth.user, url_signer.verify()) def get_bookmarks(): bookmarks=[] user=auth.get_user() assert user is not None user=db(db.users.user_id==user[\"id\"]).select().first() assert user is not None tmp_bookmarks=db(db.bookmarks.user==user[\"id\"]).select().as_list() for bookmark in tmp_bookmarks: cache=db.caches[bookmark[\"cache\"]] cache[\"href\"]=URL(\"cache_info\", cache.id, signer=url_signer) bookmarks.append(cache) return dict(bookmarks=bookmarks) @action(\"cache_info\/<cache_id:int>\") @action.uses(\"cache_info.html\", db, auth.user, url_signer) def cache_info(cache_id=None): return dict( getCacheURL=URL(\"getCache\", cache_id, signer=url_signer), getUserURL=URL(\"getUser\", signer=url_signer), setBookmarkedURL=URL(\"setBookmarked\", cache_id, signer=url_signer), getBookmarkedURL=URL(\"getBookmarked\", cache_id, signer=url_signer), logCacheURL=URL(\"logCache\", cache_id, signer=url_signer), getLogsURL=URL(\"getLogs\", cache_id), checkTimerURL=URL(\"checkTimer\", cache_id, signer=url_signer), ) @action(\"getCache\/<cache_id:int>\") @action.uses(db) def getCache(cache_id=None): cache=db(db.caches._id==cache_id).select().first() return dict(cache=cache) @action(\"setBookmarked\/<cache_id:int>\", method=\"PUT\") @action.uses(db, auth, url_signer.verify()) def setBookmarked(cache_id=None): bookmarked=False user=auth.get_user() assert user is not None user=db(db.users.user_id==user[\"id\"]).select().first() assert user is not None assert cache_id is not None bookmark=( db((db.bookmarks.user==user[\"id\"]) &(db.bookmarks.cache==cache_id)) .select() .first() ) if bookmark is None: db.bookmarks.update_or_insert(user=user[\"id\"], cache=cache_id) bookmarked=True else: del db.bookmarks[bookmark[\"id\"]] bookmarked=False return dict(bookmarked=bookmarked) @action(\"getBookmarked\/<cache_id:int>\", method=\"GET\") @action.uses(db, auth, url_signer.verify()) def getBookmarked(cache_id=None): user=auth.get_user() assert user is not None user=db(db.users.user_id==user[\"id\"]).select().first() assert user is not None assert cache_id is not None bookmark=( db((db.bookmarks.user==user[\"id\"]) &(db.bookmarks.cache==cache_id)) .select() .first() ) if bookmark is None: bookmarked=False else: bookmarked=True return dict(bookmarked=bookmarked) @action(\"logCache\/<cache_id:int>\", method=\"PUT\") @action.uses(db, auth.user, url_signer.verify()) def logCache(cache_id=None): auth_user_data=auth.get_user() assert auth_user_data is not None user=db(db.users.user_id==auth_user_data[\"id\"]).select().first() discover_date=datetime.now() assert user is not None assert cache_id is not None assert discover_date is not None log_id=db.logs.insert(logger=user.id, cache=cache_id, discover_date=discover_date) log=db.logs[log_id] log[\"first_name\"]=auth_user_data[\"first_name\"] log[\"last_name\"]=auth_user_data[\"last_name\"] db(db.users.id==user.id).update(caches_logged=user.caches_logged +1) return dict(log=log) @action(\"getLogs\/<cache_id:int>\", method=\"GET\") @action.uses(db, auth.user) def getLogs(cache_id=None): logs=db(db.logs.cache==cache_id).select().as_list() for log in logs: user=db(db.users.id==log[\"logger\"]).select().first() auth_user_data=db(db.auth_user.id==user[\"user_id\"]).select().first() log[\"first_name\"]=auth_user_data[\"first_name\"] log[\"last_name\"]=auth_user_data[\"last_name\"] return dict(logs=logs) @action(\"checkTimer\/<cache_id:int>\", method=\"GET\") @action.uses(db, auth.user, url_signer.verify()) def checkTimer(cache_id=None): auth_user_data=auth.get_user() assert auth_user_data is not None user=db(db.users.user_id==auth_user_data[\"id\"]).select().first() assert user is not None newest_log=( db((db.logs.cache==cache_id) &(db.logs.logger==user[\"id\"])).select().last() ) if newest_log is None: return dict(disabled=False) log_time=newest_log[\"discover_date\"] refresh_time=log_time +timedelta(minutes=15) time_now=datetime.now() if time_now > refresh_time: return dict(disabled=False, refresh_time=refresh_time) else: return dict(disabled=True, refresh_time=refresh_time) @action(\"suggest\") @action.uses(\"suggest.html\", db, auth.user, url_signer) def suggest(): return dict(addCacheURL=URL(\"addCache\", signer=url_signer)) @action(\"addCache\", method=\"POST\") @action.uses(db, auth, url_signer.verify()) def addCache(): db.caches.insert( cache_name=request.json.get(\"cache_name\"), hint=request.json.get(\"hint\"), description=request.json.get(\"description\"), lat=request.json.get(\"lat\"), long=request.json.get(\"long\"), author=db(db.users.user_email==get_user_email).select().first().id, ) redirect(URL(\"map\")) @action(\"add_user\", method=\"POST\") @action.uses(db, auth, url_signer.verify()) def register_user(): user=auth.get_user() db.users.insert( user_id=user[\"id\"], first_name=request.json.get(\"first_name\"), last_name=request.json.get(\"last_name\"), user_email=request.json.get(\"email\"), ) @action(\"getUser\", method=\"POST\") @action.uses(db) def getUser(): id=request.json.get(\"id\") user=db(db.users._id==id).select().first() return dict(user=user) @action(\"setup\") @action.uses(db, auth) def setup(): creation_date=datetime.now() db.users.insert( first_name=\"Chris\", last_name=\"Sterza\", user_email=\"csterza@ucsc.edu\", creation_date=creation_date, banner_path=\"\", photo_profile_path=\"\", caches_logged=5, caches_hidden=3, ) db.caches.insert( cache_name=\"Arboretum\", photo_path=\"\", lat=36.98267070650899, long=-122.05985900885949, description=\"This Arboretum cache is...\", hint=\"Test hint\", author=db(db.users.creation_date==creation_date).select().first().id, creation_date=datetime.now(), ) creation_date=datetime.now() db.users.insert( first_name=\"Hello\", last_name=\"World\", user_email=\"helloworld@ucsc.edu\", creation_date=creation_date, banner_path=\"\", photo_profile_path=\"\", caches_logged=2, caches_hidden=1, ) db.caches.insert( cache_name=\"Quarry Amphitheater\", photo_path=\"\", lat=36.9986320770141, long=-122.05648938884585, description=\"This Quarry cache is...\", hint=\"u eat dis\", author=db(db.users.creation_date==creation_date).select().first().id, creation_date=datetime.now(), ) db.caches.insert( cache_name=\"Jack Baskin\", photo_path=\"\", lat=37.0005353033127, long=-122.06380507461215, description=\"This Jack Baskin cache is...\", hint=\"u eat dis\", author=db(db.users.creation_date==creation_date).select().first().id, creation_date=datetime.now(), ) db.caches.insert( cache_name=\"Porter\", photo_path=\"\", lat=36.99473025211556, long=-122.06554686691216, description=\"This Porter cache is...\", hint=\"u eat dis\", author=db(db.users.creation_date==creation_date).select().first().id, creation_date=datetime.now(), ) creation_date=datetime.now() db.users.insert( first_name=\"first name\", last_name=\"last naem\", user_email=\"firstlast@ucsc.edu\", creation_date=creation_date, banner_path=\"\", photo_profile_path=\"\", caches_logged=7, caches_hidden=7, ) db.caches.insert( cache_name=\"East Remote\", photo_path=\"\", lat=36.9911648945102, long=-122.0534244573749, description=\"This East Remote cache is...\", hint=\"u eat dis\", author=db(db.users.creation_date==creation_date).select().first().id, creation_date=datetime.now(), ) redirect(URL(\"index\")) @action(\"clear_db\") @action.uses(db, auth) def clear_db(): db.users.truncate() db.caches.truncate() redirect(URL(\"index\")) ","sourceWithComments":"\"\"\"\nThis file defines actions, i.e. functions the URLs are mapped into\nThe @action(path) decorator exposed the function at URL:\n\n    http:\/\/127.0.0.1:8000\/{app_name}\/{path}\n\nIf app_name == '_default' then simply\n\n    http:\/\/127.0.0.1:8000\/{path}\n\nIf path == 'index' it can be omitted:\n\n    http:\/\/127.0.0.1:8000\/\n\nThe path follows the bottlepy syntax.\n\n@action.uses('generic.html')  indicates that the action uses the generic.html template\n@action.uses(session)         indicates that the action uses the session\n@action.uses(db)              indicates that the action uses the db\n@action.uses(T)               indicates that the action uses the i18n & pluralization\n@action.uses(auth.user)       indicates that the action requires a logged in user\n@action.uses(auth)            indicates that the action requires the auth object\n\nsession, db, T, auth, and tempates are examples of Fixtures.\nWarning: Fixtures MUST be declared with @action.uses({fixtures}) else your app will result in undefined behavior\n\"\"\"\n\nfrom webbrowser import get\nfrom requests import delete\nfrom py4web import action, request, abort, redirect, URL\nfrom yatl.helpers import A\nfrom .common import (\n    db,\n    session,\n    T,\n    cache,\n    auth,\n    logger,\n    authenticated,\n    unauthenticated,\n    flash,\n)\nfrom py4web.utils.url_signer import URLSigner\nfrom .models import get_user_email\n\nfrom datetime import date, datetime, timedelta\n\n\nurl_signer = URLSigner(session)\n\n# Login Controllers--------------------------------------------------\n@action(\"login\")\n@action(\"register\")\n@action(\"request_reset_password\")\n@action(\"index\")\n@action.uses(\"index.html\", auth, url_signer)\ndef index():\n    logged_out_endpoints = (\n        URL(endpoint) for endpoint in (\"login\", \"register\", \"request_reset_password\")\n    )\n    if request.fullpath in logged_out_endpoints and auth.is_logged_in:\n        redirect(URL(\"map\"))\n    return {\n        \"base_url\": URL(),\n        \"add_user_url\": URL(\"add_user\", signer=url_signer),\n    }\n\n\n@action(\"custom_auth\/reset_password\")\n@action.uses(\"reset_pw.html\", auth)\ndef resetpw():\n    return {\"base_url\": URL()}\n\n\n# Profile Page Controllers-------------------------------------------\n@action(\"profile\")\n@action.uses(\"profile.html\", db, auth.user)\ndef profile():\n    return dict(load_profile_url=URL(\"load_profile_details\"))\n\n\n@action(\"load_profile_details\")\n@action.uses(db, auth.user)\ndef load_profile_details():\n    user = auth.get_user()\n    profile = db(db.users.user_id == user[\"id\"]).select().first()\n    return dict(profile=profile)\n\n\n# Map Page Controllers-----------------------------------------------\n@action(\"map\")\n@action.uses(\"map.html\", db, auth, url_signer)\ndef map():\n    return dict(\n        loadGeoCachesURL=URL(\"loadGeoCaches\", signer=url_signer),\n        searchURL=URL(\"search\", signer=url_signer),\n        generateCacheURL=URL(\"generateCacheURL\", signer=url_signer),\n    )\n\n\n@action(\"loadGeoCaches\")\n@action.uses(db)\ndef getCaches():\n    rows = db(db.caches).select().as_list()\n    return dict(caches=rows)\n\n\n@action(\"search\")\n@action.uses()\ndef search():\n    rows = db(db.caches).select().as_list()\n    return dict(caches=rows)\n\n\n@action(\"generateCacheURL\")\n@action.uses(db, url_signer, url_signer.verify())\ndef generateCacheURL():\n    cache_id = int(request.params.get(\"cache_id\"))\n    return dict(url=URL(\"cache_info\", cache_id, signer=url_signer))\n\n\n# Bookmarks Page Controllers-----------------------------------------\n@action(\"bookmarks\", method=\"GET\")\n@action.uses(\"bookmarks.html\", db, auth.user, url_signer)\ndef bookmarks():\n    return dict(get_bookmarks_url=URL(\"get_bookmarks\", signer=url_signer))\n\n\n@action(\"get_bookmarks\", method=\"GET\")\n@action.uses(db, auth.user, url_signer.verify())\ndef get_bookmarks():\n    # First user is from auth_user table\n    bookmarks = []\n    user = auth.get_user()\n    assert user is not None\n    # This user is from Users table\n    user = db(db.users.user_id == user[\"id\"]).select().first()\n    assert user is not None\n    tmp_bookmarks = db(db.bookmarks.user == user[\"id\"]).select().as_list()\n    for bookmark in tmp_bookmarks:\n        cache = db.caches[bookmark[\"cache\"]]\n        cache[\"href\"] = URL(\"cache_info\", cache.id, signer=url_signer)\n        bookmarks.append(cache)\n\n    return dict(bookmarks=bookmarks)\n\n\n# Cache Info Page Controllers----------------------------------------\n@action(\"cache_info\/<cache_id:int>\")\n@action.uses(\"cache_info.html\", db, auth.user, url_signer)\ndef cache_info(cache_id=None):\n    return dict(\n        getCacheURL=URL(\"getCache\", cache_id, signer=url_signer),\n        getUserURL=URL(\"getUser\", signer=url_signer),\n        setBookmarkedURL=URL(\"setBookmarked\", cache_id, signer=url_signer),\n        getBookmarkedURL=URL(\"getBookmarked\", cache_id, signer=url_signer),\n        logCacheURL=URL(\"logCache\", cache_id, signer=url_signer),\n        getLogsURL=URL(\"getLogs\", cache_id),\n        checkTimerURL=URL(\"checkTimer\", cache_id, signer=url_signer),\n    )\n\n\n@action(\"getCache\/<cache_id:int>\")\n@action.uses(db)\ndef getCache(cache_id=None):\n    cache = db(db.caches._id == cache_id).select().first()\n    return dict(cache=cache)\n\n\n@action(\"setBookmarked\/<cache_id:int>\", method=\"PUT\")\n@action.uses(db, auth, url_signer.verify())\ndef setBookmarked(cache_id=None):\n    # First user is from auth_user table\n    bookmarked = False\n    user = auth.get_user()\n    assert user is not None\n    # This user is from Users table\n    user = db(db.users.user_id == user[\"id\"]).select().first()\n    assert user is not None\n    assert cache_id is not None\n    bookmark = (\n        db((db.bookmarks.user == user[\"id\"]) & (db.bookmarks.cache == cache_id))\n        .select()\n        .first()\n    )\n    if bookmark is None:\n        db.bookmarks.update_or_insert(user=user[\"id\"], cache=cache_id)\n        bookmarked = True\n    else:\n        del db.bookmarks[bookmark[\"id\"]]\n        bookmarked = False\n    return dict(bookmarked=bookmarked)\n\n\n@action(\"getBookmarked\/<cache_id:int>\", method=\"GET\")\n@action.uses(db, auth, url_signer.verify())\ndef getBookmarked(cache_id=None):\n    # First user is from auth_user table\n    user = auth.get_user()\n    assert user is not None\n    # This user is from Users table\n    user = db(db.users.user_id == user[\"id\"]).select().first()\n    assert user is not None\n    assert cache_id is not None\n    bookmark = (\n        db((db.bookmarks.user == user[\"id\"]) & (db.bookmarks.cache == cache_id))\n        .select()\n        .first()\n    )\n    if bookmark is None:\n        bookmarked = False\n    else:\n        bookmarked = True\n    return dict(bookmarked=bookmarked)\n\n\n@action(\"logCache\/<cache_id:int>\", method=\"PUT\")\n@action.uses(db, auth.user, url_signer.verify())\ndef logCache(cache_id=None):\n    # First user is from auth_user table\n    auth_user_data = auth.get_user()\n    assert auth_user_data is not None\n    # This user is from Users table\n    user = db(db.users.user_id == auth_user_data[\"id\"]).select().first()\n    discover_date = datetime.now()\n    assert user is not None\n    assert cache_id is not None\n    assert discover_date is not None\n\n    log_id = db.logs.insert(logger=user.id, cache=cache_id, discover_date=discover_date)\n    log = db.logs[log_id]\n    log[\"first_name\"] = auth_user_data[\"first_name\"]\n    log[\"last_name\"] = auth_user_data[\"last_name\"]\n\n    # Update caches loged counter\n    db(db.users.id == user.id).update(caches_logged=user.caches_logged + 1)\n    return dict(log=log)\n\n\n@action(\"getLogs\/<cache_id:int>\", method=\"GET\")\n@action.uses(db, auth.user)\ndef getLogs(cache_id=None):\n    logs = db(db.logs.cache == cache_id).select().as_list()\n    # Add name attributes to logs\n    for log in logs:\n        user = db(db.users.id == log[\"logger\"]).select().first()\n        auth_user_data = db(db.auth_user.id == user[\"user_id\"]).select().first()\n        log[\"first_name\"] = auth_user_data[\"first_name\"]\n        log[\"last_name\"] = auth_user_data[\"last_name\"]\n    # Figure out how to query only last 10 logs.\n    # logs = db(db.executesql('SELECT * FROM logs order by id desc limit 10;'))\n    return dict(logs=logs)\n\n\n@action(\"checkTimer\/<cache_id:int>\", method=\"GET\")\n@action.uses(db, auth.user, url_signer.verify())\ndef checkTimer(cache_id=None):\n    # First user is from auth_user table\n    auth_user_data = auth.get_user()\n    assert auth_user_data is not None\n    # This user is from Users table\n    user = db(db.users.user_id == auth_user_data[\"id\"]).select().first()\n    assert user is not None\n    newest_log = (\n        db((db.logs.cache == cache_id) & (db.logs.logger == user[\"id\"])).select().last()\n    )\n    # No log at this cache for this user\n    if newest_log is None:\n        return dict(disabled=False)\n    # Check if the most recent log is old enough\n    log_time = newest_log[\"discover_date\"]\n    refresh_time = log_time + timedelta(minutes=15)\n    time_now = datetime.now()\n    # Now is past the refresh time limit\n    if time_now > refresh_time:\n        return dict(disabled=False, refresh_time=refresh_time)\n    # Is hasn't been enough time yet\n    else:\n        return dict(disabled=True, refresh_time=refresh_time)\n\n\n# Suggest Page Controllers-------------------------------------------\n@action(\"suggest\")\n@action.uses(\"suggest.html\", db, auth.user, url_signer)\ndef suggest():\n    return dict(addCacheURL=URL(\"addCache\", signer=url_signer))\n\n\n@action(\"addCache\", method=\"POST\")\n@action.uses(db, auth, url_signer.verify())\ndef addCache():\n    db.caches.insert(\n        cache_name=request.json.get(\"cache_name\"),\n        hint=request.json.get(\"hint\"),\n        description=request.json.get(\"description\"),\n        lat=request.json.get(\"lat\"),\n        long=request.json.get(\"long\"),\n        author=db(db.users.user_email == get_user_email).select().first().id,\n    )\n    redirect(URL(\"map\"))\n\n\n# Miscellaneous Controllers------------------------------------------\n@action(\"add_user\", method=\"POST\")\n@action.uses(db, auth, url_signer.verify())\ndef register_user():\n    user = auth.get_user()\n    db.users.insert(\n        user_id=user[\"id\"],\n        first_name=request.json.get(\"first_name\"),\n        last_name=request.json.get(\"last_name\"),\n        user_email=request.json.get(\"email\"),\n    )\n\n\n@action(\"getUser\", method=\"POST\")\n@action.uses(db)\ndef getUser():\n    id = request.json.get(\"id\")\n    user = db(db.users._id == id).select().first()\n    return dict(user=user)\n\n\n# TODO: MAKE SURE TO REMOVE FOR PRODUCTION\n@action(\"setup\")\n@action.uses(db, auth)\ndef setup():\n    creation_date = datetime.now()\n    db.users.insert(\n        first_name=\"Chris\",\n        last_name=\"Sterza\",\n        user_email=\"csterza@ucsc.edu\",\n        creation_date=creation_date,\n        banner_path=\"\",\n        photo_profile_path=\"\",\n        caches_logged=5,\n        caches_hidden=3,\n    )\n    db.caches.insert(\n        cache_name=\"Arboretum\",\n        photo_path=\"\",\n        lat=36.98267070650899,\n        long=-122.05985900885949,\n        description=\"This Arboretum cache is...\",\n        hint=\"Test hint\",\n        author=db(db.users.creation_date == creation_date).select().first().id,\n        creation_date=datetime.now(),\n    )\n    creation_date = datetime.now()\n    db.users.insert(\n        first_name=\"Hello\",\n        last_name=\"World\",\n        user_email=\"helloworld@ucsc.edu\",\n        creation_date=creation_date,\n        banner_path=\"\",\n        photo_profile_path=\"\",\n        caches_logged=2,\n        caches_hidden=1,\n    )\n    db.caches.insert(\n        cache_name=\"Quarry Amphitheater\",\n        photo_path=\"\",\n        lat=36.9986320770141,\n        long=-122.05648938884585,\n        description=\"This Quarry cache is...\",\n        hint=\"u eat dis\",\n        author=db(db.users.creation_date == creation_date).select().first().id,\n        creation_date=datetime.now(),\n    )\n    db.caches.insert(\n        cache_name=\"Jack Baskin\",\n        photo_path=\"\",\n        lat=37.0005353033127,\n        long=-122.06380507461215,\n        description=\"This Jack Baskin cache is...\",\n        hint=\"u eat dis\",\n        author=db(db.users.creation_date == creation_date).select().first().id,\n        creation_date=datetime.now(),\n    )\n    db.caches.insert(\n        cache_name=\"Porter\",\n        photo_path=\"\",\n        lat=36.99473025211556,\n        long=-122.06554686691216,\n        description=\"This Porter cache is...\",\n        hint=\"u eat dis\",\n        author=db(db.users.creation_date == creation_date).select().first().id,\n        creation_date=datetime.now(),\n    )\n    creation_date = datetime.now()\n    db.users.insert(\n        first_name=\"first name\",\n        last_name=\"last naem\",\n        user_email=\"firstlast@ucsc.edu\",\n        creation_date=creation_date,\n        banner_path=\"\",\n        photo_profile_path=\"\",\n        caches_logged=7,\n        caches_hidden=7,\n    )\n    db.caches.insert(\n        cache_name=\"East Remote\",\n        photo_path=\"\",\n        lat=36.9911648945102,\n        long=-122.0534244573749,\n        description=\"This East Remote cache is...\",\n        hint=\"u eat dis\",\n        author=db(db.users.creation_date == creation_date).select().first().id,\n        creation_date=datetime.now(),\n    )\n    redirect(URL(\"index\"))\n\n\n# TODO: MAKE SURE TO REMOVE FOR PRODUCTION\n@action(\"clear_db\")\n@action.uses(db, auth)\ndef clear_db():\n    db.users.truncate()\n    db.caches.truncate()\n    redirect(URL(\"index\"))\n"}},"msg":"Moved log timer check to server so it can't be tampered with js"}},"https:\/\/github.com\/Amu-DevCommeLesPros-2020\/DevCommeLesPros-2020-Ex2":{"b8c8005a0c2070fc290aedac21e5eb2c442307d6":{"url":"https:\/\/api.github.com\/repos\/Amu-DevCommeLesPros-2020\/DevCommeLesPros-2020-Ex2\/commits\/b8c8005a0c2070fc290aedac21e5eb2c442307d6","html_url":"https:\/\/github.com\/Amu-DevCommeLesPros-2020\/DevCommeLesPros-2020-Ex2\/commit\/b8c8005a0c2070fc290aedac21e5eb2c442307d6","message":"Check test test files for tampering","sha":"b8c8005a0c2070fc290aedac21e5eb2c442307d6","keyword":"tampering check","diff":"diff --git a\/correction\/correction.py b\/correction\/correction.py\nindex befad2a..6be78f1 100644\n--- a\/correction\/correction.py\n+++ b\/correction\/correction.py\n@@ -18,8 +18,7 @@\n import pygit2\n \n def hash_test_code(main_path):\n-    \"\"\"Hashes all lines in file main_path.\n-       The same function was used to hash the original main.c.\"\"\"\n+    \"\"\"Hashes file main_path.\"\"\"\n     with open(main_path) as main:\n         test_code_hash = hashlib.sha256()\n         for line in main:\n@@ -27,6 +26,8 @@ def hash_test_code(main_path):\n     return test_code_hash.hexdigest()\n \n PROFESSOR_TEST_CODE_HEXDIGEST = 'd4f15976f23772064cbc86d02fb3e7c366e354012eb242b29466f0abe9721cb0'\n+PROFESSOR_CHIFFRE_HEXDIGEST = '60ff41b09e4e1011d3a5f33704ec53df319a248d1de48250a131b809a85cb2db'\n+PROFESSOR_CLAIR_HEXDIGEST = '4ef57703aad7ffd9f3129bb46c81a15308f1963e1f12ab00718f3569fde090f3'\n CALLBACKS = pygit2.RemoteCallbacks(credentials=pygit2.KeypairFromAgent(\"git\"))\n \n with open('depots.txt') as remote_depot_names:\n@@ -45,8 +46,9 @@ def hash_test_code(main_path):\n                 raise RuntimeError('-1')\n \n             # Confirm test code is intact.\n-            student_test_code_hexdigest = hash_test_code(local_depot_path + '\/test\/main.c')\n-            if student_test_code_hexdigest != PROFESSOR_TEST_CODE_HEXDIGEST:\n+            if hash_test_code(local_depot_path + '\/test\/main.c') != PROFESSOR_TEST_CODE_HEXDIGEST or \\\n+               hash_test_code(local_depot_path + '\/test\/chiffre.txt') != PROFESSOR_CHIFFRE_HEXDIGEST or \\\n+               hash_test_code(local_depot_path + '\/test\/clair.txt') != PROFESSOR_CLAIR_HEXDIGEST:\n                 raise RuntimeError('-2')\n \n             # Compile.\n","files":{"\/correction\/correction.py":{"changes":[{"diff":"\n import pygit2\n \n def hash_test_code(main_path):\n-    \"\"\"Hashes all lines in file main_path.\n-       The same function was used to hash the original main.c.\"\"\"\n+    \"\"\"Hashes file main_path.\"\"\"\n     with open(main_path) as main:\n         test_code_hash = hashlib.sha256()\n         for line in main:\n","add":1,"remove":2,"filename":"\/correction\/correction.py","badparts":["    \"\"\"Hashes all lines in file main_path.","       The same function was used to hash the original main.c.\"\"\""],"goodparts":["    \"\"\"Hashes file main_path.\"\"\""]},{"diff":"\n                 raise RuntimeError('-1')\n \n             # Confirm test code is intact.\n-            student_test_code_hexdigest = hash_test_code(local_depot_path + '\/test\/main.c')\n-            if student_test_code_hexdigest != PROFESSOR_TEST_CODE_HEXDIGEST:\n+            if hash_test_code(local_depot_path + '\/test\/main.c') != PROFESSOR_TEST_CODE_HEXDIGEST or \\\n+               hash_test_code(local_depot_path + '\/test\/chiffre.txt') != PROFESSOR_CHIFFRE_HEXDIGEST or \\\n+               hash_test_code(local_depot_path + '\/test\/clair.txt') != PROFESSOR_CLAIR_HEXDIGEST:\n                 raise RuntimeError('-2')\n \n             # Compile.\n","add":3,"remove":2,"filename":"\/correction\/correction.py","badparts":["            student_test_code_hexdigest = hash_test_code(local_depot_path + '\/test\/main.c')","            if student_test_code_hexdigest != PROFESSOR_TEST_CODE_HEXDIGEST:"],"goodparts":["            if hash_test_code(local_depot_path + '\/test\/main.c') != PROFESSOR_TEST_CODE_HEXDIGEST or \\","               hash_test_code(local_depot_path + '\/test\/chiffre.txt') != PROFESSOR_CHIFFRE_HEXDIGEST or \\","               hash_test_code(local_depot_path + '\/test\/clair.txt') != PROFESSOR_CLAIR_HEXDIGEST:"]}],"source":"\n\"\"\"Correction script for this exercise. For each depot name found in depots.txt: -clones the repo. -confirms main.c is the same as the original. -invokes make test. -runs test. -prints the return code of test or a negative value if an earlier error occured: --1: failed to clone the repo. --2: mismatch detected between original in main.c and depot's main.c. --3: failed to compile. \"\"\" import hashlib import itertools import os import platform import pygit2 def hash_test_code(main_path): \"\"\"Hashes all lines in file main_path. The same function was used to hash the original main.c.\"\"\" with open(main_path) as main: test_code_hash=hashlib.sha256() for line in main: test_code_hash.update(line.encode()) return test_code_hash.hexdigest() PROFESSOR_TEST_CODE_HEXDIGEST='d4f15976f23772064cbc86d02fb3e7c366e354012eb242b29466f0abe9721cb0' CALLBACKS=pygit2.RemoteCallbacks(credentials=pygit2.KeypairFromAgent(\"git\")) with open('depots.txt') as remote_depot_names: for remote_depot_name in itertools.dropwhile(lambda line: line.startswith(' remote_depot_names): try: remote_depot_name=remote_depot_name.rstrip() remote_depot_url='ssh:\/\/git@github.com\/' +remote_depot_name +'.git' local_depot_path=remote_depot_name.replace('\/', '-') print(local_depot_path, end=' ') if pygit2.clone_repository(remote_depot_url, local_depot_path, callbacks=CALLBACKS) \\ is None: raise RuntimeError('-1') student_test_code_hexdigest=hash_test_code(local_depot_path +'\/test\/main.c') if student_test_code_hexdigest !=PROFESSOR_TEST_CODE_HEXDIGEST: raise RuntimeError('-2') if os.system('cd ' +local_depot_path +' && make test') !=0: raise RuntimeError('-3') print(str(os.WEXITSTATUS(os.system('cd ' +local_depot_path +' && build\/' +\\ platform.system() +'\/test')))) except pygit2.GitError: print('-1') except RuntimeError as error: print(error.args[0]) ","sourceWithComments":"\"\"\"Correction script for this exercise.\nFor each depot name found in depots.txt:\n- clones the repo.\n- confirms main.c is the same as the original.\n- invokes make test.\n- runs test.\n- prints the return code of test or a negative value if an earlier error occured:\n    - -1: failed to clone the repo.\n    - -2: mismatch detected between original in main.c and depot's main.c.\n    - -3: failed to compile.\n\"\"\"\n\nimport hashlib\nimport itertools\nimport os\nimport platform\n\nimport pygit2\n\ndef hash_test_code(main_path):\n    \"\"\"Hashes all lines in file main_path.\n       The same function was used to hash the original main.c.\"\"\"\n    with open(main_path) as main:\n        test_code_hash = hashlib.sha256()\n        for line in main:\n            test_code_hash.update(line.encode())\n    return test_code_hash.hexdigest()\n\nPROFESSOR_TEST_CODE_HEXDIGEST = 'd4f15976f23772064cbc86d02fb3e7c366e354012eb242b29466f0abe9721cb0'\nCALLBACKS = pygit2.RemoteCallbacks(credentials=pygit2.KeypairFromAgent(\"git\"))\n\nwith open('depots.txt') as remote_depot_names:\n    for remote_depot_name in itertools.dropwhile(lambda line: line.startswith('#'),\n                                                 remote_depot_names):\n        try:\n            # Craft URL to clone given a deopt name.\n            remote_depot_name = remote_depot_name.rstrip()\n            remote_depot_url = 'ssh:\/\/git@github.com\/' + remote_depot_name + '.git'\n            local_depot_path = remote_depot_name.replace('\/', '-')\n            print(local_depot_path, end=' ')\n\n            # Clone the repo.\n            if pygit2.clone_repository(remote_depot_url, local_depot_path, callbacks=CALLBACKS) \\\n                    is None:\n                raise RuntimeError('-1')\n\n            # Confirm test code is intact.\n            student_test_code_hexdigest = hash_test_code(local_depot_path + '\/test\/main.c')\n            if student_test_code_hexdigest != PROFESSOR_TEST_CODE_HEXDIGEST:\n                raise RuntimeError('-2')\n\n            # Compile.\n            if os.system('cd ' + local_depot_path + ' && make test') != 0:\n                raise RuntimeError('-3')\n\n            # Run and print result.\n            print(str(os.WEXITSTATUS(os.system('cd ' + local_depot_path + ' && build\/' + \\\n                      platform.system() + '\/test'))))\n        except pygit2.GitError:\n            print('-1')\n        except RuntimeError as error:\n            print(error.args[0])\n"}},"msg":"Check test test files for tampering"}},"https:\/\/github.com\/FermiParadox\/Tor_deanonymization":{"c84b7fcd83e0d7ec202635d5d5399cd148c994c8":{"url":"https:\/\/api.github.com\/repos\/FermiParadox\/Tor_deanonymization\/commits\/c84b7fcd83e0d7ec202635d5d5399cd148c994c8","html_url":"https:\/\/github.com\/FermiParadox\/Tor_deanonymization\/commit\/c84b7fcd83e0d7ec202635d5d5399cd148c994c8","message":"DEL: removed unneeded checks (won't take into account data tampering by client)\n\nTODO: browser-switch note in request data\n\nTODO: send keypress data\nTODO: move html separator in js file + use var in data_converter (failed earlier Uncaught ReferenceError)\n\nBUG: coveragerc ignores omitted items (both \"report\" and \"run\"). Check https:\/\/www.jetbrains.com\/pycharm\/guide\/tips\/speed-up-coverage\/","sha":"c84b7fcd83e0d7ec202635d5d5399cd148c994c8","keyword":"tampering check","diff":"diff --git a\/data_converter.py b\/data_converter.py\nindex 1e291d5..d723dff 100644\n--- a\/data_converter.py\n+++ b\/data_converter.py\n@@ -4,29 +4,13 @@\n POINT_SPLITTER = \":\"\n COORDINATE_SPLITTER = \",\"\n \n+\n ALLOWED_CHARS = POINT_SPLITTER + COORDINATE_SPLITTER + string.digits\n \n \n class TXYStrToArray:\n     def __init__(self, data_string):\n         self.data_string = data_string\n-        self.sanitize_data()\n-\n-    def not_allowed_chars(self):\n-        final_str = self.data_string\n-        for c in ALLOWED_CHARS:\n-            final_str = final_str.replace(c, '')\n-\n-    def warn_not_allowed_chars(self):\n-        raise Warning(f\"Not allowed characters in {self.data_string}.\")\n-\n-    def set_data_to_empty(self):\n-        self.data_string = ''\n-\n-    def sanitize_data(self):\n-        if self.not_allowed_chars():\n-            self.warn_not_allowed_chars()\n-            self.set_data_to_empty()\n \n     def points_as_str(self):\n         return [s for s in self.data_string.split(POINT_SPLITTER) if s]\n","files":{"\/data_converter.py":{"changes":[{"diff":"\n POINT_SPLITTER = \":\"\n COORDINATE_SPLITTER = \",\"\n \n+\n ALLOWED_CHARS = POINT_SPLITTER + COORDINATE_SPLITTER + string.digits\n \n \n class TXYStrToArray:\n     def __init__(self, data_string):\n         self.data_string = data_string\n-        self.sanitize_data()\n-\n-    def not_allowed_chars(self):\n-        final_str = self.data_string\n-        for c in ALLOWED_CHARS:\n-            final_str = final_str.replace(c, '')\n-\n-    def warn_not_allowed_chars(self):\n-        raise Warning(f\"Not allowed characters in {self.data_string}.\")\n-\n-    def set_data_to_empty(self):\n-        self.data_string = ''\n-\n-    def sanitize_data(self):\n-        if self.not_allowed_chars():\n-            self.warn_not_allowed_chars()\n-            self.set_data_to_empty()\n \n     def points_as_str(self):\n         return [s for s in self.data_string.split(POINT_SPLITTER) if s]\n","add":1,"remove":17,"filename":"\/data_converter.py","badparts":["        self.sanitize_data()","    def not_allowed_chars(self):","        final_str = self.data_string","        for c in ALLOWED_CHARS:","            final_str = final_str.replace(c, '')","    def warn_not_allowed_chars(self):","        raise Warning(f\"Not allowed characters in {self.data_string}.\")","    def set_data_to_empty(self):","        self.data_string = ''","    def sanitize_data(self):","        if self.not_allowed_chars():","            self.warn_not_allowed_chars()","            self.set_data_to_empty()"],"goodparts":[]}],"source":"\nimport string from ipaddress import ip_address POINT_SPLITTER=\":\" COORDINATE_SPLITTER=\",\" ALLOWED_CHARS=POINT_SPLITTER +COORDINATE_SPLITTER +string.digits class TXYStrToArray: def __init__(self, data_string): self.data_string=data_string self.sanitize_data() def not_allowed_chars(self): final_str=self.data_string for c in ALLOWED_CHARS: final_str=final_str.replace(c, '') def warn_not_allowed_chars(self): raise Warning(f\"Not allowed characters in{self.data_string}.\") def set_data_to_empty(self): self.data_string='' def sanitize_data(self): if self.not_allowed_chars(): self.warn_not_allowed_chars() self.set_data_to_empty() def points_as_str(self): return[s for s in self.data_string.split(POINT_SPLITTER) if s] def txy_lists(self): t_list=[] x_list=[] y_list=[] for p in self.points_as_str(): t, x, y=p.split(',') t_list.append(int(t)) x_list.append(int(x)) y_list.append(-int(y)) return t_list, x_list, y_list class ActionDataExtractor: def __init__(self, req): self.req=req self.json=req.json @property def mouse_txy_str(self): return self.json[\"mouse_trajectory\"] @property def user_id_str(self): return self.json[\"userID\"] @property def user_ip_str(self): return ip_address(self.req.remote_addr) ","sourceWithComments":"import string\nfrom ipaddress import ip_address\n\nPOINT_SPLITTER = \":\"\nCOORDINATE_SPLITTER = \",\"\n\nALLOWED_CHARS = POINT_SPLITTER + COORDINATE_SPLITTER + string.digits\n\n\nclass TXYStrToArray:\n    def __init__(self, data_string):\n        self.data_string = data_string\n        self.sanitize_data()\n\n    def not_allowed_chars(self):\n        final_str = self.data_string\n        for c in ALLOWED_CHARS:\n            final_str = final_str.replace(c, '')\n\n    def warn_not_allowed_chars(self):\n        raise Warning(f\"Not allowed characters in {self.data_string}.\")\n\n    def set_data_to_empty(self):\n        self.data_string = ''\n\n    def sanitize_data(self):\n        if self.not_allowed_chars():\n            self.warn_not_allowed_chars()\n            self.set_data_to_empty()\n\n    def points_as_str(self):\n        return [s for s in self.data_string.split(POINT_SPLITTER) if s]\n\n    def txy_lists(self):\n        t_list = []\n        x_list = []\n        y_list = []\n        for p in self.points_as_str():\n            t, x, y = p.split(',')\n            t_list.append(int(t))\n            x_list.append(int(x))\n            y_list.append(-int(y))\n\n        return t_list, x_list, y_list\n\n\nclass ActionDataExtractor:\n    def __init__(self, req):\n        self.req = req\n        self.json = req.json\n\n    @property\n    def mouse_txy_str(self):\n        return self.json[\"mouse_trajectory\"]\n\n    @property\n    def user_id_str(self):\n        return self.json[\"userID\"]\n\n    @property\n    def user_ip_str(self):\n        return ip_address(self.req.remote_addr)\n"}},"msg":"DEL: removed unneeded checks (won't take into account data tampering by client)\n\nTODO: browser-switch note in request data\n\nTODO: send keypress data\nTODO: move html separator in js file + use var in data_converter (failed earlier Uncaught ReferenceError)\n\nBUG: coveragerc ignores omitted items (both \"report\" and \"run\"). Check https:\/\/www.jetbrains.com\/pycharm\/guide\/tips\/speed-up-coverage\/"}},"https:\/\/github.com\/awsmhacks\/CrackMapExtreme":{"f3831dc2ad337a36c21c13cfb8a21dcb16044d41":{"url":"https:\/\/api.github.com\/repos\/awsmhacks\/CrackMapExtreme\/commits\/f3831dc2ad337a36c21c13cfb8a21dcb16044d41","html_url":"https:\/\/github.com\/awsmhacks\/CrackMapExtreme\/commit\/f3831dc2ad337a36c21c13cfb8a21dcb16044d41","message":"updates for versioning+tamper protection checks\n\nkilling off tamper protection is.... hard.","sha":"f3831dc2ad337a36c21c13cfb8a21dcb16044d41","keyword":"tampering check","diff":"diff --git a\/cmx\/cli.py b\/cmx\/cli.py\nindex ce83b85..9e0cc7d 100644\n--- a\/cmx\/cli.py\n+++ b\/cmx\/cli.py\n@@ -17,6 +17,7 @@\n \n import argparse\n import sys\n+import pkg_resources\n from argparse import RawTextHelpFormatter\n #import argcomplete\n #from argcomplete.completers import ChoicesCompleter\n@@ -28,7 +29,8 @@\n \n def gen_cli_args():\n \n-    VERSION  = cfg.VERSION\n+    VERSION = (pkg_resources.get_distribution('cmx').version).split('+')\n+    #VERSION = cfg.VERSION\n     RELEASED = cfg.RELEASED\n \n     p_loader =  protocol_loader()\n@@ -40,19 +42,19 @@ def gen_cli_args():\n \n     parser = argparse.ArgumentParser(description=\"\"\"\n {}\n-         A swiss army knife for pentesting networks\n-    {}{}{}\n-            {}{}\n-                    {}: {}\n-        {} \n+      {}\n+                     {}{}\n+                    {}{}\n+                           {}: {}\n+              {} \n \"\"\".format(highlight(title, 'yellow'),\n-           highlight('Forged by ', 'white'),\n+           highlight('Pentesting tool for on-prem, cloud, and hybrid AD environments', 'green'),\n+           highlight('Forged by the O.P. ', 'white'),\n            highlight('@byt3bl33d3r', 'blue'),\n-           highlight(' using the powah of dank memes', 'white'),\n            highlight('R3born from the ashes by ', 'red'),\n            highlight('@awsmhacks', 'blue'),\n            highlight('Version', 'green'),\n-           highlight(VERSION, 'cyan'),\n+           highlight(VERSION[0], 'cyan'),\n            highlight('(\/.__.)\/ The python3 EXTREME edition \\(.__.\\)', 'yellow')),\n            formatter_class=RawTextHelpFormatter,\n            epilog=\"\"\"Usage: \n@@ -93,7 +95,7 @@ def gen_cli_args():\n \n     module_parser = argparse.ArgumentParser(add_help=False)\n     mgroup = module_parser.add_mutually_exclusive_group()\n-    mgroup.add_argument(\"-M\", \"--module\", metavar='MODULE', help='module to use')\n+    mgroup.add_argument(\"-M\", \"-m\", \"--module\", metavar='MODULE', help='module to use')\n \n     module_parser.add_argument('-mo', metavar='MODULE_OPTION', nargs='+', default=[], dest='module_options', help='module options')\n     module_parser.add_argument('-L', '--list-modules', action='store_true', help='list available modules')\ndiff --git a\/cmx\/protocols\/smb.py b\/cmx\/protocols\/smb.py\nindex aaf4160..3c54910 100644\n--- a\/cmx\/protocols\/smb.py\n+++ b\/cmx\/protocols\/smb.py\n@@ -211,7 +211,7 @@ def proto_args(parser, std_parser, module_parser):\n         execgroup.add_argument('--exec-method', choices={\"wmiexec\", \"dcomexec\", \"smbexec\", \"atexec\", \"psexec\"}, default='wmiexec', help=\"method to execute the command. (default: wmiexec)\")\n         execgroup.add_argument('--force-ps32', action='store_true', help='force the PowerShell command to run in a 32-bit process')\n         execgroup.add_argument('--no-output', action='store_true', help='do not retrieve command output')\n-        execgroup.add_argument('--kd', action='store_true', help='Shut down defender before executing command (wmiexec)')\n+        execgroup.add_argument('--kd','-kd', action='store_true', help='Shut down defender before executing command (wmiexec)')\n         execegroup = execgroup.add_mutually_exclusive_group()\n         execegroup.add_argument(\"-x\", metavar=\"COMMAND\", dest='execute', help=\"execute the specified command\")\n         execegroup.add_argument(\"-X\", metavar=\"PS_COMMAND\", dest='ps_execute', help='execute the specified PowerShell command')\n@@ -227,6 +227,8 @@ def proto_args(parser, std_parser, module_parser):\n         reggroup = smb_parser.add_argument_group(\"Registry Attacks and Enum\")\n         reggroup.add_argument(\"-fix-uac\", '--fix-uac', action='store_true', help='Sets the proper Keys for remote high-integrity processes')\n         reggroup.add_argument(\"-uac-status\", '--uac-status', action='store_true', help='Check Remote UAC Status')\n+        reggroup.add_argument(\"--disable-tamper\",'--dt', '-dt', action='store_true', help='Disable Tamper Protection via registry key')\n+        reggroup.add_argument(\"--check-tamper\",'--ct', '-ct', action='store_true', help='Disable Tamper Protection via registry key')\n \n         servicegroup = smb_parser.add_argument_group(\"Interact with Services\")\n         servicegroup.add_argument(\"-start-service\", '--start-service', action='store_true', help='not finished')\n@@ -899,6 +901,83 @@ def __init__(self):\n \n         return\n \n+    @requires_admin\n+    def disable_tamper(self):\n+        r\"\"\"Make reg modifications for Tamper Protection.\n+\n+        Sets the tamper protection key to 0\n+        HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows Defender\\Features\\TamperProtection\n+\n+        logic is in protocols\/smb\/misc\/reg.py\n+        \"\"\"\n+        dcip = self.dc_ip\n+        # self.logger.announce('')\n+\n+        class Ops:\n+            def __init__(self):\n+                self.action = 'DISABLETAMPER'\n+                self.aesKey = None\n+                self.k = False\n+                self.dc_ip = dcip\n+                self.hashes = None\n+                self.port = 445\n+\n+        options = Ops()\n+\n+        try:\n+            reghandler = RegHandler(self.username, self.password, self.domain, self.logger, options)\n+            reghandler.run(self.host, self.host)\n+\n+        except Exception as e:\n+            self.logger.error('Error creating\/running regHandler connection: {}'.format(e))\n+            return\n+\n+        #try:\n+        #    restart_uac()\n+        #except Exception as e:\n+        #    self.logger.error('Error restarting Server Service: {}'.format(e))\n+        #    return \n+\n+        return\n+\n+\n+    def check_tamper(self):\n+        r\"\"\"Make reg modifications for Tamper Protection.\n+\n+        Sets the tamper protection key to 0\n+        HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows Defender\\Features\\TamperProtection\n+\n+        logic is in protocols\/smb\/misc\/reg.py\n+        \"\"\"\n+        dcip = self.dc_ip\n+        # self.logger.announce('')\n+\n+        class Ops:\n+            def __init__(self):\n+                self.action = 'CHECKTAMPER'\n+                self.aesKey = None\n+                self.k = False\n+                self.dc_ip = dcip\n+                self.hashes = None\n+                self.port = 445\n+\n+        options = Ops()\n+\n+        try:\n+            reghandler = RegHandler(self.username, self.password, self.domain, self.logger, options)\n+            reghandler.run(self.host, self.host)\n+\n+        except Exception as e:\n+            self.logger.error('Error creating\/running regHandler connection: {}'.format(e))\n+            return\n+\n+        #try:\n+        #    restart_uac()\n+        #except Exception as e:\n+        #    self.logger.error('Error restarting Server Service: {}'.format(e))\n+        #    return \n+\n+        return\n \n ###############################################################################\n \ndiff --git a\/cmx\/protocols\/smb\/EXECMETHODS\/wmiexec.py b\/cmx\/protocols\/smb\/EXECMETHODS\/wmiexec.py\nindex 20db865..8e85942 100644\n--- a\/cmx\/protocols\/smb\/EXECMETHODS\/wmiexec.py\n+++ b\/cmx\/protocols\/smb\/EXECMETHODS\/wmiexec.py\n@@ -238,6 +238,14 @@ def disable_defender(self):\n         print('            [!] Sleeping to allow defender process to finish shutting down[!] ')\n         time.sleep(8)\n \n+    def disable_tamper(self):\n+        command = self.__shell + 'REG.EXE add ^\"%a\\\\SOFTWARE\\\\Microsoft\\\\Windows\\\\Windows Defender\\\\Features\\\\TamperProtection^\" \/v ^\"Enabled^\" \/d ^\"0^\" \/t REG_DWORD \/F\" \"\"\"'\n+        #command = self.__shell + 'powershell.exe -exec bypass -noni -nop -w 1 -C \"Add-MpPreference -ExclusionExtension \".exe\"\"'\n+\n+        logging.debug('wmi Disabling TamperProtection using: ' + command)\n+        self.__win32Process.Create(command, self.__pwd, None)\n+        print('            [!] Sleeping to allow TamperProtection process to finish shutting down[!] ')\n+        time.sleep(8)\n \n     def dump(self):\n         \"\"\"Dump lsass and retrieve output dmp file.\ndiff --git a\/cmx\/protocols\/smb\/MISC\/reg.py b\/cmx\/protocols\/smb\/MISC\/reg.py\nindex 611e4a6..63c0f1c 100644\n--- a\/cmx\/protocols\/smb\/MISC\/reg.py\n+++ b\/cmx\/protocols\/smb\/MISC\/reg.py\n@@ -168,6 +168,10 @@ def run(self, remoteName, remoteHost):\n                 self.enableUAC(dce)\n             elif self.__action == 'CHECKUAC':\n                 self.checkUAC(dce)\n+            elif self.__action == 'DISABLETAMPER':\n+                self.disableTamper(dce)\n+            elif self.__action == 'CHECKTAMPER':\n+                self.checkTamper(dce)\n             else:\n                 logging.error('Method %s not implemented yet!' % self.__action)\n \n@@ -301,7 +305,7 @@ def __parse_lp_data(valueType, valueData):\n \n \n     def enableUAC(self, dce):\n-        # \n+        # this actually disables UAC but the key is enable.... \n         try:\n             ans = rrp.hOpenLocalMachine(dce)\n             regHandle  = ans['phKey']\n@@ -446,3 +450,56 @@ def checkUAC(self, dce):\n             # RID 500 local administrator using either plaintext credentials or password hashes\n \n \n+    def disableTamper(self, dce):\n+        # \n+        try:\n+            ans = rrp.hOpenLocalMachine(dce) # gets handle for HKEY_LOCAL_MACHINE\n+            regHandle  = ans['phKey']\n+        except Exception as e:\n+            logging.debug('Exception thrown when hOpenLocalMachine: %s', str(e))\n+            return\n+\n+        try:\n+            resp = rrp.hBaseRegCreateKey(dce, regHandle , 'SOFTWARE\\\\Microsoft\\\\Windows Defender\\\\Features')\n+            keyHandle = resp['phkResult']\n+        except Exception as e:\n+            logging.debug('Exception thrown when hBaseRegCreateKey: %s', str(e))\n+            return\n+\n+        # TamperProtection\n+        try:\n+            resp = rrp.hBaseRegSetValue(dce, keyHandle, 'TamperProtection\\x00',  rrp.REG_DWORD, 0)\n+            self.logger.highlight('TamperProtection Key Set! TamperProtection is now off!')\n+        except Exception as e:\n+            logging.debug('Exception thrown when hBaseRegSetValue EnableLUA: %s', str(e))\n+            self.logger.error('Could not set TamperProtection Key')\n+            pass\n+\n+    def checkTamper(self, dce):\n+        # \n+        try:\n+            ans = rrp.hOpenLocalMachine(dce) # gets handle for HKEY_LOCAL_MACHINE\n+            regHandle  = ans['phKey']\n+        except Exception as e:\n+            logging.debug('Exception thrown when hOpenLocalMachine: %s', str(e))\n+            return\n+\n+        try:\n+            resp = rrp.hBaseRegCreateKey(dce, regHandle , 'SOFTWARE\\\\Microsoft\\\\Windows Defender\\\\Features')\n+            keyHandle = resp['phkResult']\n+        except Exception as e:\n+            logging.debug('Exception thrown when hBaseRegCreateKey: %s', str(e))\n+            return\n+\n+        # TamperProtection\n+        try:\n+            dataType, tp_value = rrp.hBaseRegQueryValue(dce, keyHandle, 'TamperProtection')\n+        except Exception as e:\n+            logging.debug('Exception thrown when hBaseRegQueryValue: %s', str(e))\n+            tp_value = 5\n+            pass\n+\n+        if tp_value == 5:\n+            self.logger.highlight('TamperProtection = 5  (its on)   ')\n+        else:\n+            self.logger.highlight('TamperProtection = {}  (less than 5 is good)'.format(tp_value))\n\\ No newline at end of file\ndiff --git a\/requirements.txt b\/requirements.txt\nindex b640e3a..4e9523e 100644\n--- a\/requirements.txt\n+++ b\/requirements.txt\n@@ -1,5 +1,5 @@\n asciitable==0.8.0\n-asn1crypto==0.24.0\n+asn1crypto==1.3.0\n azure-cli==2.0.77\n certifi==2019.6.16\n cffi==1.12.3\ndiff --git a\/setup.py b\/setup.py\nindex 60bb244..de56ccf 100644\n--- a\/setup.py\n+++ b\/setup.py\n@@ -1,18 +1,44 @@\n #!\/usr\/bin\/env python3\n \n+import os\n from setuptools import setup, find_packages\n+from subprocess import *\n \n+# 1.2.0.dev1  # Development release\n+# 1.2.0a1     # Alpha Release\n+# 1.2.0b1     # Beta Release\n+# 1.2.0rc1    # Release Candidate\n+# 1.2.0       # Final Release\n \n-setup(name='crackmapextreme',\n-    version='0.1.0',\n-    description='rekd',\n+VER_MAJOR = 0\n+VER_MINOR = 0\n+VER_MAINT = 1\n+VER_PREREL = \"azdev1\"\n+\n+if call([\"git\", \"branch\"], stderr=STDOUT, stdout=open(os.devnull, 'w')) == 0:\n+    p = Popen(\"git log -1 --format=%cd --date=format:%Y%m%d.%H%M%S\", shell=True, stdin=PIPE, stderr=PIPE, stdout=PIPE)\n+    (outstr, errstr) = p.communicate()\n+    (VER_CDATE,VER_CTIME) = outstr.strip().decode(\"utf-8\").split('.')\n+\n+    p = Popen(\"git rev-parse --short HEAD\", shell=True, stdin=PIPE, stderr=PIPE, stdout=PIPE)\n+    (outstr, errstr) = p.communicate()\n+    VER_CHASH = outstr.strip().decode(\"utf-8\")\n+\n+    VER_LOCAL = \"+{}.{}.{}\".format(VER_CDATE, VER_CTIME, VER_CHASH)\n+\n+else:\n+    VER_LOCAL = \"\"\n+\n+setup(name='cmx',\n+    version = \"{}.{}.{}.{}{}\".format(VER_MAJOR,VER_MINOR,VER_MAINT,VER_PREREL,VER_LOCAL),\n+    description='Network pentesting tool for on-prem, cloud, and hybrid AD environments.',\n     classifiers=[\n         'Environment :: Console',\n         'License :: OSI Approved :: BSD License',\n         'Programming Language :: Python :: 3.7',\n         'Topic :: Security',\n     ],\n-    keywords='pentesting security windows active-directory networks',\n+    keywords='pentesting security windows active-directory networks azure azureAD',\n     url='http:\/\/github.com\/awsmhacks\/CrackMapExtreme',\n     author='awsmhacks',\n     author_email='dontemailmebruh',\n","files":{"\/cmx\/cli.py":{"changes":[{"diff":"\n def gen_cli_args():\n \n-    VERSION  = cfg.VERSION\n+    VERSION = (pkg_resources.get_distribution('cmx').version).split('+')\n+    #VERSION = cfg.VERSION\n     RELEASED = cfg.RELEASED\n \n     p_loader =  protocol_loader()\n","add":2,"remove":1,"filename":"\/cmx\/cli.py","badparts":["    VERSION  = cfg.VERSION"],"goodparts":["    VERSION = (pkg_resources.get_distribution('cmx').version).split('+')"]},{"diff":"\n \n     module_parser = argparse.ArgumentParser(add_help=False)\n     mgroup = module_parser.add_mutually_exclusive_group()\n-    mgroup.add_argument(\"-M\", \"--module\", metavar='MODULE', help='module to use')\n+    mgroup.add_argument(\"-M\", \"-m\", \"--module\", metavar='MODULE', help='module to use')\n \n     module_parser.add_argument('-mo', metavar='MODULE_OPTION', nargs='+', default=[], dest='module_options', help='module options')\n     module_parser.add_argument('-L', '--list-modules', action='store_true', help='list available modules')","add":1,"remove":1,"filename":"\/cmx\/cli.py","badparts":["    mgroup.add_argument(\"-M\", \"--module\", metavar='MODULE', help='module to use')"],"goodparts":["    mgroup.add_argument(\"-M\", \"-m\", \"--module\", metavar='MODULE', help='module to use')"]}],"source":"\n import argparse import sys from argparse import RawTextHelpFormatter from cmx.loaders.protocol_loader import protocol_loader from cmx.helpers.logger import highlight from cmx import config as cfg def gen_cli_args(): VERSION =cfg.VERSION RELEASED=cfg.RELEASED p_loader= protocol_loader() protocols=p_loader.get_protocols() title=\"\"\"____ ____ ____ ____ _ _ _ _ ____ ___ ____ _ _ ___ ____ ____ _ _ ____ | |__\/ |__| | |_\/ |\\\/| |__| |__] |___ \\\/ | |__\/ |___ |\\\/| |___ |___ | \\ | | |___ | \\_ | | | | | |___ _\/\\_ | | \\ |___ | | |___ \"\"\" parser=argparse.ArgumentParser(description=\"\"\" {} A swiss army knife for pentesting networks {}{}{} {}{} {}:{} {} \"\"\".format(highlight(title, 'yellow'), highlight('Forged by ', 'white'), highlight('@byt3bl33d3r', 'blue'), highlight(' using the powah of dank memes', 'white'), highlight('R3born from the ashes by ', 'red'), highlight('@awsmhacks', 'blue'), highlight('Version', 'green'), highlight(VERSION, 'cyan'), highlight('(\/.__.)\/ The python3 EXTREME edition \\(.__.\\)', 'yellow')), formatter_class=RawTextHelpFormatter, epilog=\"\"\"Usage: cmx[-D] PROTOCOL[-h] TARGET[target options][-M MODULE[module options]] cmx smb -M mimikatz --options (List a particular module's options) cmx smb 10.10.10.10 -u Administrator -p Password --recon cmx -D smb 192.168.1.1 -u username -p password -M mimikatz Azure! cmx az --config (get an azure session up, follow prompts) cmx az --user <useremail> (gets all info about a single user) cmx az --users (gets all users) cmx az -h (for all current azure stuffs) *Check https:\/\/awsmhacks.github.io\/cmxdocs\/index for detailed usage* \"\"\", add_help=False, usage=argparse.SUPPRESS) parser.add_argument(\"--threads\", type=int, dest=\"threads\", default=100, help=argparse.SUPPRESS) parser.add_argument(\"--timeout\", default=0, type=int, help=argparse.SUPPRESS) parser.add_argument(\"-D\",\"--debug\", action='store_true', help=argparse.SUPPRESS) parser.add_argument(\"--darrell\", action='store_true', help=argparse.SUPPRESS) parser.add_argument(\"--rekt\", action='store_true', help=argparse.SUPPRESS) subparsers=parser.add_subparsers(title='protocols', dest='protocol', help=argparse.SUPPRESS) std_parser=argparse.ArgumentParser(add_help=False) std_parser.add_argument(\"target\", nargs='*', type=str, help=\"the target IP(s), range(s), CIDR(s), hostname(s), FQDN(s), file(s) containing a list of targets, NMap XML or.Nessus file(s)\") std_parser.add_argument(\"-id\", metavar=\"CRED_ID\", nargs='+', default=[], type=str, dest='cred_id', help=\"database credential ID(s) to use for authentication\") std_parser.add_argument(\"-u\", metavar=\"USERNAME\", dest='username', nargs='+', default=[], help=\"username(s) or file(s) containing usernames\") std_parser.add_argument(\"-p\", metavar=\"PASSWORD\", dest='password', nargs='+', default=[], help=\"password(s) or file(s) containing passwords\") fail_group=std_parser.add_mutually_exclusive_group() fail_group.add_argument(\"--gfail-limit\", metavar='LIMIT', type=int, help='max number of global failed login attempts') fail_group.add_argument(\"--ufail-limit\", metavar='LIMIT', type=int, help='max number of failed login attempts per username') fail_group.add_argument(\"--hfail-limit\", metavar='LIMIT', type=int, help='max number of failed login attempts per host') module_parser=argparse.ArgumentParser(add_help=False) mgroup=module_parser.add_mutually_exclusive_group() mgroup.add_argument(\"-M\", \"--module\", metavar='MODULE', help='module to use') module_parser.add_argument('-mo', metavar='MODULE_OPTION', nargs='+', default=[], dest='module_options', help='module options') module_parser.add_argument('-L', '--list-modules', action='store_true', help='list available modules') module_parser.add_argument('--options', dest='show_module_options', action='store_true', help='display module options') module_parser.add_argument(\"--server\", choices={'http', 'https'}, default='https', help='use the selected server(default: https)') module_parser.add_argument(\"--server-host\", type=str, default='0.0.0.0', metavar='HOST', help='IP to bind the server to(default: 0.0.0.0)') module_parser.add_argument(\"--server-port\", metavar='PORT', type=int, help='start the server on the specified port') for protocol in list(protocols.keys()): protocol_object=p_loader.load_protocol(protocols[protocol]['path']) subparsers=getattr(protocol_object, protocol).proto_args(subparsers, std_parser, module_parser) if len(sys.argv)==1: parser.print_help() sys.exit(1) args=parser.parse_args() return args ","sourceWithComments":"#!\/usr\/bin\/env python3\n\n####################################################################\n#   cli.py   -   CMX command line interface \n#   \n#   Displays CMX Banner\n#   Generates command line arguments for a given protocol \n#\n#\n# Classes:\n#   - \n#\n# Non-Class Functions:\n#   gen_cli_args\n#\n####################################################################\n\nimport argparse\nimport sys\nfrom argparse import RawTextHelpFormatter\n#import argcomplete\n#from argcomplete.completers import ChoicesCompleter\n\nfrom cmx.loaders.protocol_loader import protocol_loader\nfrom cmx.helpers.logger import highlight\nfrom cmx import config as cfg\n\n\ndef gen_cli_args():\n\n    VERSION  = cfg.VERSION\n    RELEASED = cfg.RELEASED\n\n    p_loader =  protocol_loader()\n    protocols = p_loader.get_protocols()\n    title = \"\"\"____ ____ ____ ____ _  _   _  _ ____ ___    ____ _  _ ___ ____ ____ _  _ ____ \n|    |__\/ |__| |    |_\/    |\\\/| |__| |__]   |___  \\\/   |  |__\/ |___ |\\\/| |___ \n|___ |  \\ |  | |___ | \\_   |  | |  | |      |___ _\/\\_  |  |  \\ |___ |  | |___ \n\"\"\"\n\n    parser = argparse.ArgumentParser(description=\"\"\"\n{}\n         A swiss army knife for pentesting networks\n    {}{}{}\n            {}{}\n                    {}: {}\n        {} \n\"\"\".format(highlight(title, 'yellow'),\n           highlight('Forged by ', 'white'),\n           highlight('@byt3bl33d3r', 'blue'),\n           highlight(' using the powah of dank memes', 'white'),\n           highlight('R3born from the ashes by ', 'red'),\n           highlight('@awsmhacks', 'blue'),\n           highlight('Version', 'green'),\n           highlight(VERSION, 'cyan'),\n           highlight('(\/.__.)\/ The python3 EXTREME edition \\(.__.\\)', 'yellow')),\n           formatter_class=RawTextHelpFormatter,\n           epilog=\"\"\"Usage: \n       cmx [-D] PROTOCOL [-h] TARGET [target options] [-M MODULE [module options]]  \n\n       cmx smb -M mimikatz --options    (List a particular module's options)\n       cmx smb 10.10.10.10 -u Administrator -p Password --recon\n       cmx -D smb 192.168.1.1 -u username -p password -M mimikatz\n\n  Azure!\n       cmx az --config   (get an azure session up, follow prompts)\n       cmx az --user <useremail>   (gets all info about a single user)\n       cmx az --users      (gets all users)\n       cmx az -h  (for all current azure stuffs)\n\n *Check https:\/\/awsmhacks.github.io\/cmxdocs\/index for detailed usage* \n\n\"\"\",\n           add_help=False, usage=argparse.SUPPRESS)\n\n    parser.add_argument(\"--threads\", type=int, dest=\"threads\", default=100, help=argparse.SUPPRESS)\n    parser.add_argument(\"--timeout\", default=0, type=int, help=argparse.SUPPRESS) # use --timeout 0 for no timeout\n    parser.add_argument(\"-D\",\"--debug\", action='store_true', help=argparse.SUPPRESS)\n    parser.add_argument(\"--darrell\", action='store_true', help=argparse.SUPPRESS)\n    parser.add_argument(\"--rekt\", action='store_true', help=argparse.SUPPRESS)\n\n    subparsers = parser.add_subparsers(title='protocols', dest='protocol', help=argparse.SUPPRESS) #suppressing cause it looks cleaner. gonna have to hit the wiki for helps.\n\n    std_parser = argparse.ArgumentParser(add_help=False)\n    std_parser.add_argument(\"target\", nargs='*', type=str, help=\"the target IP(s), range(s), CIDR(s), hostname(s), FQDN(s), file(s) containing a list of targets, NMap XML or .Nessus file(s)\")\n    std_parser.add_argument(\"-id\", metavar=\"CRED_ID\", nargs='+', default=[], type=str, dest='cred_id', help=\"database credential ID(s) to use for authentication\")\n    std_parser.add_argument(\"-u\", metavar=\"USERNAME\", dest='username', nargs='+', default=[], help=\"username(s) or file(s) containing usernames\")\n    std_parser.add_argument(\"-p\", metavar=\"PASSWORD\", dest='password', nargs='+', default=[], help=\"password(s) or file(s) containing passwords\")\n    fail_group = std_parser.add_mutually_exclusive_group()\n    fail_group.add_argument(\"--gfail-limit\", metavar='LIMIT', type=int, help='max number of global failed login attempts')\n    fail_group.add_argument(\"--ufail-limit\", metavar='LIMIT', type=int, help='max number of failed login attempts per username')\n    fail_group.add_argument(\"--hfail-limit\", metavar='LIMIT', type=int, help='max number of failed login attempts per host')\n\n    module_parser = argparse.ArgumentParser(add_help=False)\n    mgroup = module_parser.add_mutually_exclusive_group()\n    mgroup.add_argument(\"-M\", \"--module\", metavar='MODULE', help='module to use')\n\n    module_parser.add_argument('-mo', metavar='MODULE_OPTION', nargs='+', default=[], dest='module_options', help='module options')\n    module_parser.add_argument('-L', '--list-modules', action='store_true', help='list available modules')\n    module_parser.add_argument('--options', dest='show_module_options', action='store_true', help='display module options')\n    module_parser.add_argument(\"--server\", choices={'http', 'https'}, default='https', help='use the selected server (default: https)')\n    module_parser.add_argument(\"--server-host\", type=str, default='0.0.0.0', metavar='HOST', help='IP to bind the server to (default: 0.0.0.0)')\n    module_parser.add_argument(\"--server-port\", metavar='PORT', type=int, help='start the server on the specified port')\n\n\n    for protocol in list(protocols.keys()):\n        protocol_object = p_loader.load_protocol(protocols[protocol]['path'])\n        subparsers = getattr(protocol_object, protocol).proto_args(subparsers, std_parser, module_parser)\n\n    if len(sys.argv) == 1:\n        parser.print_help()\n        sys.exit(1)\n\n    args = parser.parse_args()\n\n    return args\n"},"\/cmx\/protocols\/smb.py":{"changes":[{"diff":"\n         execgroup.add_argument('--exec-method', choices={\"wmiexec\", \"dcomexec\", \"smbexec\", \"atexec\", \"psexec\"}, default='wmiexec', help=\"method to execute the command. (default: wmiexec)\")\n         execgroup.add_argument('--force-ps32', action='store_true', help='force the PowerShell command to run in a 32-bit process')\n         execgroup.add_argument('--no-output', action='store_true', help='do not retrieve command output')\n-        execgroup.add_argument('--kd', action='store_true', help='Shut down defender before executing command (wmiexec)')\n+        execgroup.add_argument('--kd','-kd', action='store_true', help='Shut down defender before executing command (wmiexec)')\n         execegroup = execgroup.add_mutually_exclusive_group()\n         execegroup.add_argument(\"-x\", metavar=\"COMMAND\", dest='execute', help=\"execute the specified command\")\n         execegroup.add_argument(\"-X\", metavar=\"PS_COMMAND\", dest='ps_execute', help='execute the specified PowerShell command')\n","add":1,"remove":1,"filename":"\/cmx\/protocols\/smb.py","badparts":["        execgroup.add_argument('--kd', action='store_true', help='Shut down defender before executing command (wmiexec)')"],"goodparts":["        execgroup.add_argument('--kd','-kd', action='store_true', help='Shut down defender before executing command (wmiexec)')"]}]}},"msg":"updates for versioning+tamper protection checks\n\nkilling off tamper protection is.... hard."}},"https:\/\/github.com\/Jo3e\/numToWordProject":{"7979896cda4c7c893dbe616ec697abc86c9031b9":{"url":"https:\/\/api.github.com\/repos\/Jo3e\/numToWordProject\/commits\/7979896cda4c7c893dbe616ec697abc86c9031b9","html_url":"https:\/\/github.com\/Jo3e\/numToWordProject\/commit\/7979896cda4c7c893dbe616ec697abc86c9031b9","message":"I didn't want to tamper with the intial code. So check this one out. Still has some issues, but it's better.","sha":"7979896cda4c7c893dbe616ec697abc86c9031b9","keyword":"tampering check","diff":"diff --git a\/example2.py b\/example2.py\nnew file mode 100644\nindex 0000000..52648a2\n--- \/dev\/null\n+++ b\/example2.py\n@@ -0,0 +1,25 @@\n+digit = input(\"Enter a number to convert to words: \")\n+0\n+units = {1:\"one\", 2:\"two\", 3:\"three\", 4:\"four\", 5:\"five\", \n+         6:\"six\", 7:\"seven\", 8:\"eight\", 9:\"nine\",  10:\"ten\", \n+         11:\"eleven\", 12:\"twelve\", 13:\"thirteen\", 14:\"fourteen\", 15:\"fifteen\", \n+         16:\"sixteen\", 17:\"seventeen\", 18:\"eighteen\",19:\"nineteen\"}\n+         \n+tens =  {20:\"twenty\", 30:\"thirty\", 40:\"fourty\", 50:\"fifty\", 60:\"sixty\", 70:\"seventy\", 80:\"eight\", 90:\"ninety\"}\n+\n+hundred = { 100:\"one hundred\", 200:\"two hundred\"}\n+\n+def number_to_words(userInput):\n+\n+    if len(digit) <= 2 and int(digit) in units.keys():\n+         name = units[int(digit)]\n+    \n+    elif len(digit) == 2:\n+        name = tens[int(digit[0]+'0')]+\"-\"+units[int(digit[-1])]\n+        \n+    elif len(digit) == 3:\n+        \n+        name = units[int(digit[0])]+ \" hundred and \"+tens[int(digit[1]+\"0\")]+\"-\"+units[int(digit[-1])]\n+    print(name)\n+\n+number_to_words(digit)\ndiff --git a\/numToWord.py b\/numToWord.py\nindex f29083b..c8dfd74 100644\n--- a\/numToWord.py\n+++ b\/numToWord.py\n@@ -9,8 +9,7 @@\n          20:\"twenty\", 30:\"thirty\", 40:\"fourty\", 50:\"fifty\", 60:\"sixty\", 70:\"seventy\", \n          80:\"eight\", 90:\"ninety\"}\n          \n-hundred = { 100:\"one hundred\", 200:\"two hundred\"\n-            }\n+hundred = { 100:\"one hundred\", 200:\"two hundred\"}\n \n def number_to_words(problem):\n \n@@ -75,3 +74,4 @@ def number_to_words(problem):\n \n \n \n+\n","files":{"\/numToWord.py":{"changes":[{"diff":"\n          20:\"twenty\", 30:\"thirty\", 40:\"fourty\", 50:\"fifty\", 60:\"sixty\", 70:\"seventy\", \n          80:\"eight\", 90:\"ninety\"}\n          \n-hundred = { 100:\"one hundred\", 200:\"two hundred\"\n-            }\n+hundred = { 100:\"one hundred\", 200:\"two hundred\"}\n \n def number_to_words(problem):\n \n","add":1,"remove":2,"filename":"\/numToWord.py","badparts":["hundred = { 100:\"one hundred\", 200:\"two hundred\"","            }"],"goodparts":["hundred = { 100:\"one hundred\", 200:\"two hundred\"}"]}],"source":"\ndigit=input(\"Enter a number to convert to words: \") units={ 1:\"one\", 2:\"two\", 3:\"three\", 4:\"four\", 5:\"five\", 6:\"six\", 7:\"seven\", 8:\"eight\", 9:\"nine\", 10:\"ten\", 11:\"eleven\", 12:\"twelve\", 13:\"thirteen\", 14:\"fourteen\", 15:\"fifteen\", 16:\"sixteen\", 17:\"seventeen\", 18:\"eighteen\",19:\"nineteen\"} tens={ 20:\"twenty\", 30:\"thirty\", 40:\"fourty\", 50:\"fifty\", 60:\"sixty\", 70:\"seventy\", 80:\"eight\", 90:\"ninety\"} hundred={ 100:\"one hundred\", 200:\"two hundred\" } def number_to_words(problem): if len(digit) <=2 and int(digit) in units.keys(): print(units[int(digit)]) elif len(digit)==2: split_number=[] for letters in digit: split_number.append(letters) if len(split_number)==2: first_letter=split_number[0] +'0' second_letter=split_number[1] for num in tens.keys(): first_letter=int(first_letter) if first_letter==num: global split_tens split_tens=tens[first_letter] for num in units.keys(): second_letter=int(second_letter) if second_letter==num: global split_unit split_unit=units[second_letter] print(split_tens,'-', split_unit) if len(digit)==3: split_number=[] for letters in digit: split_number.append(letters) if len(split_number)==3: first_letter=split_number[0] +\"00\" second_letter=split_number[1] +\"0\" third_letter=split_number[2] for num in split_number: first_letter=int(first_letter) second_letter=int(second_letter) third_letter=int(third_letter) if first_letter==hundred.keys(): pass if second_letter==tens.keys(): pass if third_letter==units.keys(): pass print(hundred[first_letter], \"and\", tens[second_letter], units[third_letter]) number_to_words(digit) ","sourceWithComments":"digit = input(\"Enter a number to convert to words: \")\n\nunits = {\n        1:\"one\", 2:\"two\", 3:\"three\", 4:\"four\", 5:\"five\", 6:\"six\", 7:\"seven\", 8:\"eight\", 9:\"nine\",  10:\"ten\", 11:\"eleven\", 12:\"twelve\", 13:\"thirteen\", 14:\"fourteen\", 15:\"fifteen\", 16:\"sixteen\", 17:\"seventeen\",\n         18:\"eighteen\",19:\"nineteen\"}\n         \ntens =  {\n\n         20:\"twenty\", 30:\"thirty\", 40:\"fourty\", 50:\"fifty\", 60:\"sixty\", 70:\"seventy\", \n         80:\"eight\", 90:\"ninety\"}\n         \nhundred = { 100:\"one hundred\", 200:\"two hundred\"\n            }\n\ndef number_to_words(problem):\n\n    if len(digit) <= 2 and int(digit) in units.keys():\n        \n         print(units[int(digit)])\n            \n    elif len(digit) == 2:\n            \n        split_number = []\n\n        for letters in digit:\n            split_number.append(letters)\n\n        if len(split_number) == 2:\n            \n            first_letter = split_number[0] + '0'\n            second_letter = split_number[1]\n\n        for num in tens.keys():\n            first_letter = int(first_letter)\n            if first_letter == num:\n                global split_tens\n                split_tens = tens[first_letter]\n\n        for num in units.keys():\n            second_letter = int(second_letter)\n            if second_letter == num:\n                global split_unit\n                split_unit = units[second_letter]\n\n        print(split_tens,'-', split_unit)\n\n        \n    if len(digit) == 3:\n        split_number = []\n\n        for letters in digit:\n            split_number.append(letters)\n        \n        if len(split_number) == 3:\n            first_letter = split_number[0] + \"00\" \n            second_letter = split_number[1] + \"0\"\n            third_letter = split_number[2]\n            # print(first_letter, second_letter, third_letter)\n\n        for num in split_number:\n            first_letter = int(first_letter)\n            second_letter = int(second_letter)\n            # if split_number[1] == int(0):\n            #     second_letter = \"and\"\n            third_letter = int(third_letter)\n            if first_letter == hundred.keys():\n                pass\n            if second_letter == tens.keys():\n                pass\n            if third_letter == units.keys():\n                pass\n        print(hundred[first_letter], \"and\", tens[second_letter], units[third_letter])\n\nnumber_to_words(digit)\n\n\n\n"}},"msg":"I didn't want to tamper with the intial code. So check this one out. Still has some issues, but it's better."}},"https:\/\/github.com\/noobgab\/ndcproject":{"cb087e477caf13663e05708d13e1b77d938b4d48":{"url":"https:\/\/api.github.com\/repos\/noobgab\/ndcproject\/commits\/cb087e477caf13663e05708d13e1b77d938b4d48","html_url":"https:\/\/github.com\/noobgab\/ndcproject\/commit\/cb087e477caf13663e05708d13e1b77d938b4d48","message":"Add files via upload\n\nAdded hash function to check tampering of user message","sha":"cb087e477caf13663e05708d13e1b77d938b4d48","keyword":"tampering check","diff":"diff --git a\/c.py b\/c.py\nindex 774630b..6f84429 100644\n--- a\/c.py\n+++ b\/c.py\n@@ -2,6 +2,7 @@\n import threading\n from time import gmtime, strftime\n import time\n+import hashlib\n \n HOST = \"127.0.0.1\"\n PORT = 50007\n@@ -33,6 +34,7 @@ def setupUser(socket):\n def readInput(user, socket):\n     while 1:\n         text = raw_input()\n+\thashText = hashlib.sha224(text).hexdigest()\n         if prefix+\"help\" in text:\n             line = \"<cmd-help-\"+user+\">\"\n         elif prefix+\"usercount\" in text:\n@@ -40,14 +42,14 @@ def readInput(user, socket):\n         elif prefix+\"servertime\" in text:\n             line = \"<cmd-servertime-\"+user+\">\"\n         else:\n-            line = \"<msg-\" + user + \"-\" + text + \">\"\n+            line = \"<msg-\" + user + \"-\" + text + \"-\" + hashText + \">\"\n         socket.sendall(line)\n \n def readData(user, socket):\n     while 1:\n         data = socket.recv(1024)\n         data_split = data.split('-')\n-        print \"[\" + strftime(\"%H:%M:%S\", gmtime()) + \"] \" + data_split[1] + \": \" + data_split[2][0:-1]\n+        print \"[\" + strftime(\"%H:%M:%S\", gmtime()) + \"] \" + data_split[1] + \": \" + data_split[2]\n \n setupUser(s)\n \ndiff --git a\/s.py b\/s.py\nindex 465e860..5892a0f 100644\n--- a\/s.py\n+++ b\/s.py\n@@ -2,6 +2,7 @@\n import threading, Queue\n from time import gmtime, strftime\n import time\n+import hashlib\n \n # Socket connection information\n HOST = \"127.0.0.1\"\n@@ -20,6 +21,7 @@\n adminList = list()\n adminList.append(\"admin\")\n commandList = [\"help\", \"usercount\", \"servertime\"]\n+errorList = [\"TamperingError: Message has been tampered\"]\n \n def parseInput(data, con):\n     print str(data)\n@@ -38,9 +40,16 @@ def parseInput(data, con):\n         elif cmd_extr == \"servertime\":\n             con.send(\"<cmd-server-\"+strftime(\"%a, %d %b %Y %H:%M:%S +0000\", gmtime())+\">\")\n     elif data_split[0][1:] == \"msg\":\n-        for singleClient in currentConnections:\n-            singleClient.send(str(data))\n-\n+\t\thashCheck = hashlib.sha224(data_split[2]).hexdigest()\n+\t\tif hashCheck == data_split[3][0:-1]:\n+\t\t\tfor singleClient in currentConnections:\n+\t\t\t\tsingleClient.send(str(data))\n+\t\telse:\n+\t\t\tfor singleClient in currentConnections:\n+\t\t\t\tsingleClient.send(\"<msg-server-\" + errorList[0] + \">\")\n+\t\t\tprint errorList[0]\n+\t\t\tprint \"Original hash: \" + data_split[3]\n+\t\t\tprint \"Tampered hash: \" + hashCheck\n \n def manageConnection(con, addr):\n     global currentConnections\n","files":{"\/c.py":{"changes":[{"diff":"\n         elif prefix+\"servertime\" in text:\n             line = \"<cmd-servertime-\"+user+\">\"\n         else:\n-            line = \"<msg-\" + user + \"-\" + text + \">\"\n+            line = \"<msg-\" + user + \"-\" + text + \"-\" + hashText + \">\"\n         socket.sendall(line)\n \n def readData(user, socket):\n     while 1:\n         data = socket.recv(1024)\n         data_split = data.split('-')\n-        print \"[\" + strftime(\"%H:%M:%S\", gmtime()) + \"] \" + data_split[1] + \": \" + data_split[2][0:-1]\n+        print \"[\" + strftime(\"%H:%M:%S\", gmtime()) + \"] \" + data_split[1] + \": \" + data_split[2]\n \n setupUser(s)\n ","add":2,"remove":2,"filename":"\/c.py","badparts":["            line = \"<msg-\" + user + \"-\" + text + \">\"","        print \"[\" + strftime(\"%H:%M:%S\", gmtime()) + \"] \" + data_split[1] + \": \" + data_split[2][0:-1]"],"goodparts":["            line = \"<msg-\" + user + \"-\" + text + \"-\" + hashText + \">\"","        print \"[\" + strftime(\"%H:%M:%S\", gmtime()) + \"] \" + data_split[1] + \": \" + data_split[2]"]}],"source":"\nimport socket import threading from time import gmtime, strftime import time HOST=\"127.0.0.1\" PORT=50007 s=socket.socket(socket.AF_INET, socket.SOCK_STREAM) s.connect((HOST, PORT)) prefix=\"!\" username=\"\" def setupUser(socket): global username nameDone=False while nameDone==False: username=raw_input(\"Enter a username: \") socket.sendall(\"<cmd-namechange-\"+str(username)+\">\") data=socket.recv(1024) if \"<cmd-confirm-true\" in data: print \"Username has been set to: \" +username cmd_split=data.split('-') print \"Welcome to the chat: \" +cmd_split[3][0:-1] nameDone=True else: print \"Error setting username. Try again...\" def readInput(user, socket): while 1: text=raw_input() if prefix+\"help\" in text: line=\"<cmd-help-\"+user+\">\" elif prefix+\"usercount\" in text: line=\"<cmd-usercount-\"+user+\">\" elif prefix+\"servertime\" in text: line=\"<cmd-servertime-\"+user+\">\" else: line=\"<msg-\" +user +\"-\" +text +\">\" socket.sendall(line) def readData(user, socket): while 1: data=socket.recv(1024) data_split=data.split('-') print \"[\" +strftime(\"%H:%M:%S\", gmtime()) +\"] \" +data_split[1] +\": \" +data_split[2][0:-1] setupUser(s) t=threading.Thread(target=readInput, args=(username, s)) t.start() t=threading.Thread(target=readData, args=(username, s)) t.start() ","sourceWithComments":"import socket\nimport threading\nfrom time import gmtime, strftime\nimport time\n\nHOST = \"127.0.0.1\"\nPORT = 50007\n\ns = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\ns.connect((HOST, PORT))\n\nprefix = \"!\"\nusername = \"\"\n\ndef setupUser(socket):\n    global username\n    nameDone = False\n\n    while nameDone == False:\n        username = raw_input(\"Enter a username: \")\n        socket.sendall(\"<cmd-namechange-\"+str(username)+\">\")\n\n        data = socket.recv(1024)\n\n        if \"<cmd-confirm-true\" in data:\n            print \"Username has been set to: \" + username\n            cmd_split = data.split('-')\n            print \"Welcome to the chat: \" + cmd_split[3][0:-1]\n            nameDone = True\n        else:\n            print \"Error setting username. Try again...\"\n\ndef readInput(user, socket):\n    while 1:\n        text = raw_input()\n        if prefix+\"help\" in text:\n            line = \"<cmd-help-\"+user+\">\"\n        elif prefix+\"usercount\" in text:\n            line = \"<cmd-usercount-\"+user+\">\"\n        elif prefix+\"servertime\" in text:\n            line = \"<cmd-servertime-\"+user+\">\"\n        else:\n            line = \"<msg-\" + user + \"-\" + text + \">\"\n        socket.sendall(line)\n\ndef readData(user, socket):\n    while 1:\n        data = socket.recv(1024)\n        data_split = data.split('-')\n        print \"[\" + strftime(\"%H:%M:%S\", gmtime()) + \"] \" + data_split[1] + \": \" + data_split[2][0:-1]\n\nsetupUser(s)\n\nt = threading.Thread(target=readInput, args=(username, s))\nt.start()\n\nt = threading.Thread(target=readData, args=(username, s))\nt.start()"},"\/s.py":{"changes":[{"diff":"\n         elif cmd_extr == \"servertime\":\n             con.send(\"<cmd-server-\"+strftime(\"%a, %d %b %Y %H:%M:%S +0000\", gmtime())+\">\")\n     elif data_split[0][1:] == \"msg\":\n-        for singleClient in currentConnections:\n-            singleClient.send(str(data))\n-\n+\t\thashCheck = hashlib.sha224(data_split[2]).hexdigest()\n+\t\tif hashCheck == data_split[3][0:-1]:\n+\t\t\tfor singleClient in currentConnections:\n+\t\t\t\tsingleClient.send(str(data))\n+\t\telse:\n+\t\t\tfor singleClient in currentConnections:\n+\t\t\t\tsingleClient.send(\"<msg-server-\" + errorList[0] + \">\")\n+\t\t\tprint errorList[0]\n+\t\t\tprint \"Original hash: \" + data_split[3]\n+\t\t\tprint \"Tampered hash: \" + hashCheck\n \n def manageConnection(con, addr):\n     global currentConnections\n","add":10,"remove":3,"filename":"\/s.py","badparts":["        for singleClient in currentConnections:","            singleClient.send(str(data))"],"goodparts":["\t\thashCheck = hashlib.sha224(data_split[2]).hexdigest()","\t\tif hashCheck == data_split[3][0:-1]:","\t\t\tfor singleClient in currentConnections:","\t\t\t\tsingleClient.send(str(data))","\t\telse:","\t\t\tfor singleClient in currentConnections:","\t\t\t\tsingleClient.send(\"<msg-server-\" + errorList[0] + \">\")","\t\t\tprint errorList[0]","\t\t\tprint \"Original hash: \" + data_split[3]","\t\t\tprint \"Tampered hash: \" + hashCheck"]}],"source":"\nimport socket import threading, Queue from time import gmtime, strftime import time HOST=\"127.0.0.1\" PORT=50007 s=socket.socket(socket.AF_INET, socket.SOCK_STREAM) s.bind((HOST,PORT)) buffer=\"\" prefix=\"!\" serverTitle=\"No Title\" currentConnections=list() userList=list() adminList=list() adminList.append(\"admin\") commandList=[\"help\", \"usercount\", \"servertime\"] def parseInput(data, con): print str(data) data_split=data.split('-') if data_split[0][1:]==\"cmd\": cmd_extr=data_split[1] if cmd_extr==\"help\": line=\"<cmd-help-Here is a list of commands: \" for cmd in commandList: line +=\"!\" +cmd +\", \" line +=\">\" line=line[0:-2] con.send(line) elif cmd_extr==\"usercount\": con.send(\"<cmd-server-There are \"+str(len(userList))+\" user(s) online>\") elif cmd_extr==\"servertime\": con.send(\"<cmd-server-\"+strftime(\"%a, %d %b %Y %H:%M:%S +0000\", gmtime())+\">\") elif data_split[0][1:]==\"msg\": for singleClient in currentConnections: singleClient.send(str(data)) def manageConnection(con, addr): global currentConnections global serverTitle global userList print \"Connected by: \" +str(addr) currentConnections.append(con) nameDone=False while nameDone==False: data=con.recv(1024) checkstr=data.split(\"-\") print \"Checking username: \\\"\" +str(checkstr[2][0:-1]) +\"\\\"\" if checkstr[0]==\"<cmd\" and checkstr[1]==\"namechange\": usr=checkstr[2][0:-1] try: userList.index(usr) print \"Username declined\" con.send(\"<cmd-confirm-false>\") except ValueError as ve: userList.append(usr) nameDone=True print \"Username accepted\" con.send(\"<cmd-confirm-true-\"+serverTitle+\">\") for singleClient in currentConnections: singleClient.send(\"<msg-server-\"+usr+\" has joined the chat>\") while 1: data=con.recv(1024) parseInput(data, con) while 1: s.listen(1) con, addr=s.accept() t=threading.Thread(target=manageConnection, args=(con, addr)) t.start() ","sourceWithComments":"import socket\nimport threading, Queue\nfrom time import gmtime, strftime\nimport time\n\n# Socket connection information\nHOST = \"127.0.0.1\"\nPORT = 50007\ns = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\ns.bind((HOST,PORT))\n\n# Global Variables\nbuffer = \"\"\nprefix = \"!\"\nserverTitle = \"No Title\"\n\n# Global Lists\ncurrentConnections = list()\nuserList = list()\nadminList = list()\nadminList.append(\"admin\")\ncommandList = [\"help\", \"usercount\", \"servertime\"]\n\ndef parseInput(data, con):\n    print str(data)\n    data_split = data.split('-')\n    if data_split[0][1:] == \"cmd\":\n        cmd_extr = data_split[1]\n        if cmd_extr == \"help\":\n            line = \"<cmd-help-Here is a list of commands: \"\n            for cmd in commandList:\n                line += \"!\" + cmd + \", \"\n            line += \">\"\n            line = line[0:-2]\n            con.send(line)\n        elif cmd_extr == \"usercount\":\n            con.send(\"<cmd-server-There are \"+str(len(userList))+\" user(s) online>\")\n        elif cmd_extr == \"servertime\":\n            con.send(\"<cmd-server-\"+strftime(\"%a, %d %b %Y %H:%M:%S +0000\", gmtime())+\">\")\n    elif data_split[0][1:] == \"msg\":\n        for singleClient in currentConnections:\n            singleClient.send(str(data))\n\n\ndef manageConnection(con, addr):\n    global currentConnections\n    global serverTitle\n    global userList\n\n    print \"Connected by: \" + str(addr)\n    currentConnections.append(con)\n\n    nameDone = False\n    while nameDone == False:\n        data = con.recv(1024)\n        checkstr = data.split(\"-\")\n        print \"Checking username: \\\"\" + str(checkstr[2][0:-1]) + \"\\\"\"\n        if checkstr[0] == \"<cmd\" and checkstr[1] == \"namechange\":\n            usr = checkstr[2][0:-1]\n            try:\n                userList.index(usr)\n                print \"Username declined\"\n                con.send(\"<cmd-confirm-false>\")\n            except ValueError as ve:\n                userList.append(usr)\n                nameDone = True\n                print \"Username accepted\"\n                con.send(\"<cmd-confirm-true-\"+serverTitle+\">\")\n                for singleClient in currentConnections:\n                    singleClient.send(\"<msg-server-\"+usr+\" has joined the chat>\")\n\n    while 1:\n        data = con.recv(1024)\n        parseInput(data, con)\n\nwhile 1:\n    s.listen(1)\n    con, addr = s.accept()\n\n    t = threading.Thread(target=manageConnection, args=(con, addr))\n    t.start()"}},"msg":"Add files via upload\n\nAdded hash function to check tampering of user message"}},"https:\/\/github.com\/sourabh-natesh\/download_fix_graphene":{"345d271e663725d58ce941fc49f08ebc02c32fa2":{"url":"https:\/\/api.github.com\/repos\/sourabh-natesh\/download_fix_graphene\/commits\/345d271e663725d58ce941fc49f08ebc02c32fa2","html_url":"https:\/\/github.com\/sourabh-natesh\/download_fix_graphene\/commit\/345d271e663725d58ce941fc49f08ebc02c32fa2","message":"[Pal\/Linux-SGX] Test malicious modifications to protected files\n\nThis commit adds a new PF utility `pf_tamper` that tampers with\nvalid protected files and uses this utility to test that the PF\nlogic in Linux-SGX detects such malicious modifications.\n\nThis commit also moves out the PF-format macros and structs from\n`protected_files_internal.h` to `protected_files_format.h` for\nbetter readability.\n\nCo-authored-by: Dmitrii Kuvaiskii <dmitrii.kuvaiskii@intel.com>","sha":"345d271e663725d58ce941fc49f08ebc02c32fa2","keyword":"tampering malicious","diff":"diff --git a\/LibOS\/shim\/test\/fs\/test_pf.py b\/LibOS\/shim\/test\/fs\/test_pf.py\nindex e430e02a..7b1621d7 100644\n--- a\/LibOS\/shim\/test\/fs\/test_pf.py\n+++ b\/LibOS\/shim\/test\/fs\/test_pf.py\n@@ -193,24 +193,20 @@ def test_210_copy_dir_mounted(self):\n \n     def __corrupt_file(self, input_path, output_path):\n         cmd = [self.PF_TAMPER, '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path]\n-        return self.run_native_binary(cmd)\n+        return self.run_native_binary(cmd, libpath=os.path.join(os.getcwd(), 'lib'))\n \n     # invalid\/corrupted files\n-    @expectedFailureIf(HAS_SGX)\n-    # pylint: disable=fixme\n     def test_500_invalid(self):\n-        # TODO: port these to the new file format\n         invalid_dir = os.path.join(self.TEST_DIR, 'pf_invalid')\n-        # files below should work normally (benign modifications)\n-        should_pass = ['chunk_padding_1_fixed', 'chunk_padding_2_fixed', 'chunk_data_3',\n-                       'chunk_data_3_fixed', 'chunk_data_4', 'chunk_data_4_fixed']\n         if not os.path.exists(invalid_dir):\n             os.mkdir(invalid_dir)\n+\n         # prepare valid encrypted file (largest one for maximum possible corruptions)\n         original_input = self.OUTPUT_FILES[-1]\n         self.__encrypt_file(self.INPUT_FILES[-1], original_input)\n         # generate invalid files based on the above\n         self.__corrupt_file(original_input, invalid_dir)\n+\n         # try to decrypt invalid files\n         for name in os.listdir(invalid_dir):\n             invalid = os.path.join(invalid_dir, name)\n@@ -218,17 +214,13 @@ def test_500_invalid(self):\n             input_path = os.path.join(invalid_dir, os.path.basename(original_input))\n             # copy the file so it has the original file name (for allowed path check)\n             shutil.copy(invalid, input_path)\n-            should_pass = any(s in name for s in should_pass)\n \n             try:\n                 args = ['decrypt', '-V', '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path]\n                 self.__pf_crypt(args)\n             except subprocess.CalledProcessError as exc:\n-                if should_pass:\n-                    self.assertEqual(exc.returncode, 0)\n-                else:\n-                    self.assertNotEqual(exc.returncode, 0)\n+                # decryption of invalid file must fail with -1 (wrapped to 255)\n+                self.assertEqual(exc.returncode, 255)\n             else:\n-                if not should_pass:\n-                    print('[!] Fail: successfully decrypted file: ' + name)\n-                    self.fail()\n+                print('[!] Fail: successfully decrypted file: ' + name)\n+                self.fail()\ndiff --git a\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files.c b\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files.c\nindex 4f0c59ba..d020a1e2 100644\n--- a\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files.c\n+++ b\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files.c\n@@ -5,6 +5,8 @@\n  *\/\n \n #include \"api.h\"\n+#include \"protected_files.h\"\n+#include \"protected_files_format.h\"\n #include \"protected_files_internal.h\"\n \n #ifndef IN_PAL\n@@ -65,20 +67,6 @@ static pf_random_f          g_cb_random          = NULL;\n static pf_iv_t g_empty_iv = {0};\n static bool g_initialized = false;\n \n-#define METADATA_KEY_NAME \"SGX-PROTECTED-FS-METADATA-KEY\"\n-#define MAX_LABEL_SIZE    64\n-\n-static_assert(sizeof(METADATA_KEY_NAME) <= MAX_LABEL_SIZE, \"label too long\");\n-\n-#pragma pack(push, 1)\n-typedef struct {\n-    uint32_t index;\n-    char label[MAX_LABEL_SIZE]; \/\/ must be NULL terminated\n-    pf_keyid_t nonce;\n-    uint32_t output_len; \/\/ in bits\n-} kdf_input_t;\n-#pragma pack(pop)\n-\n \/\/ The key derivation function follow recommendations from NIST Special Publication 800-108:\n \/\/ Recommendation for Key Derivation Using Pseudorandom Functions\n \/\/ https:\/\/nvlpubs.nist.gov\/nistpubs\/Legacy\/SP\/nistspecialpublication800-108.pdf\ndiff --git a\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files_format.h b\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files_format.h\nnew file mode 100644\nindex 00000000..2394e53e\n--- \/dev\/null\n+++ b\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files_format.h\n@@ -0,0 +1,139 @@\n+\/* SPDX-License-Identifier: LGPL-3.0-or-later *\/\n+\/* Copyright (C) 2019-2020 Invisible Things Lab\n+ *                         Rafal Wojdyla <omeg@invisiblethingslab.com>\n+ * Copyright (C) 2011-2020 Intel Corporation\n+ *\/\n+\n+#ifndef PROTECTED_FILES_FORMAT_H_\n+#define PROTECTED_FILES_FORMAT_H_\n+\n+#include <limits.h>\n+\n+#include \"assert.h\"\n+#include \"list.h\"\n+#include \"protected_files.h\"\n+\n+#define PF_FILE_ID       0x46505f4850415247 \/* GRAPH_PF *\/\n+#define PF_MAJOR_VERSION 0x01\n+#define PF_MINOR_VERSION 0x00\n+\n+#define METADATA_KEY_NAME \"SGX-PROTECTED-FS-METADATA-KEY\"\n+#define MAX_LABEL_SIZE    64\n+\n+static_assert(sizeof(METADATA_KEY_NAME) <= MAX_LABEL_SIZE, \"label too long\");\n+\n+#pragma pack(push, 1)\n+\n+typedef struct _metadata_plain {\n+    uint64_t   file_id;\n+    uint8_t    major_version;\n+    uint8_t    minor_version;\n+    pf_keyid_t metadata_key_id;\n+    pf_mac_t   metadata_gmac; \/* GCM mac *\/\n+} metadata_plain_t;\n+\n+#define PATH_MAX_SIZE (260 + 512)\n+\n+\/\/ these are all defined as relative to node size, so we can decrease node size in tests\n+\/\/ and have deeper tree\n+#define MD_USER_DATA_SIZE (PF_NODE_SIZE * 3 \/ 4) \/\/ 3072\n+static_assert(MD_USER_DATA_SIZE == 3072, \"bad struct size\");\n+\n+typedef struct _metadata_encrypted {\n+    char     path[PATH_MAX_SIZE];\n+    uint64_t size;\n+    pf_key_t mht_key;\n+    pf_mac_t mht_gmac;\n+    uint8_t  data[MD_USER_DATA_SIZE];\n+} metadata_encrypted_t;\n+\n+typedef uint8_t metadata_encrypted_blob_t[sizeof(metadata_encrypted_t)];\n+\n+#define METADATA_NODE_SIZE PF_NODE_SIZE\n+\n+typedef uint8_t metadata_padding_t[METADATA_NODE_SIZE -\n+                                   (sizeof(metadata_plain_t) + sizeof(metadata_encrypted_blob_t))];\n+\n+typedef struct _metadata_node {\n+    metadata_plain_t          plain_part;\n+    metadata_encrypted_blob_t encrypted_part;\n+    metadata_padding_t        padding;\n+} metadata_node_t;\n+\n+static_assert(sizeof(metadata_node_t) == PF_NODE_SIZE, \"sizeof(metadata_node_t)\");\n+\n+typedef struct _data_node_crypto {\n+    pf_key_t key;\n+    pf_mac_t gmac;\n+} gcm_crypto_data_t;\n+\n+\/\/ for PF_NODE_SIZE == 4096, we have 96 attached data nodes and 32 mht child nodes\n+\/\/ for PF_NODE_SIZE == 2048, we have 48 attached data nodes and 16 mht child nodes\n+\/\/ for PF_NODE_SIZE == 1024, we have 24 attached data nodes and 8 mht child nodes\n+\/\/ 3\/4 of the node size is dedicated to data nodes\n+#define ATTACHED_DATA_NODES_COUNT ((PF_NODE_SIZE \/ sizeof(gcm_crypto_data_t)) * 3 \/ 4)\n+static_assert(ATTACHED_DATA_NODES_COUNT == 96, \"ATTACHED_DATA_NODES_COUNT\");\n+\/\/ 1\/4 of the node size is dedicated to child mht nodes\n+#define CHILD_MHT_NODES_COUNT ((PF_NODE_SIZE \/ sizeof(gcm_crypto_data_t)) * 1 \/ 4)\n+static_assert(CHILD_MHT_NODES_COUNT == 32, \"CHILD_MHT_NODES_COUNT\");\n+\n+typedef struct _mht_node {\n+    gcm_crypto_data_t data_nodes_crypto[ATTACHED_DATA_NODES_COUNT];\n+    gcm_crypto_data_t mht_nodes_crypto[CHILD_MHT_NODES_COUNT];\n+} mht_node_t;\n+\n+static_assert(sizeof(mht_node_t) == PF_NODE_SIZE, \"sizeof(mht_node_t)\");\n+\n+typedef struct _data_node {\n+    uint8_t data[PF_NODE_SIZE];\n+} data_node_t;\n+\n+static_assert(sizeof(data_node_t) == PF_NODE_SIZE, \"sizeof(data_node_t)\");\n+\n+typedef struct _encrypted_node {\n+    uint8_t cipher[PF_NODE_SIZE];\n+} encrypted_node_t;\n+\n+static_assert(sizeof(encrypted_node_t) == PF_NODE_SIZE, \"sizeof(encrypted_node_t)\");\n+\n+#define MAX_PAGES_IN_CACHE 48\n+\n+typedef enum {\n+    FILE_MHT_NODE_TYPE  = 1,\n+    FILE_DATA_NODE_TYPE = 2,\n+} mht_node_type_e;\n+\n+\/\/ make sure these are the same size\n+static_assert(sizeof(mht_node_t) == sizeof(data_node_t),\n+              \"sizeof(mht_node_t) == sizeof(data_node_t)\");\n+\n+DEFINE_LIST(_file_node);\n+typedef struct _file_node {\n+    LIST_TYPE(_file_node) list;\n+    uint8_t type;\n+    uint64_t node_number;\n+    struct _file_node* parent;\n+    bool need_writing;\n+    bool new_node;\n+    struct {\n+        uint64_t physical_node_number;\n+        encrypted_node_t encrypted; \/\/ the actual data from the disk\n+    };\n+    union { \/\/ decrypted data\n+        mht_node_t mht;\n+        data_node_t data;\n+    } decrypted;\n+} file_node_t;\n+DEFINE_LISTP(_file_node);\n+\n+typedef struct {\n+    uint32_t index;\n+    char label[MAX_LABEL_SIZE]; \/\/ must be NULL terminated\n+    pf_keyid_t nonce;\n+    uint32_t output_len; \/\/ in bits\n+} kdf_input_t;\n+\n+#pragma pack(pop)\n+\n+#endif \/* PROTECTED_FILES_FORMAT_H_ *\/\n+\ndiff --git a\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files_internal.h b\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files_internal.h\nindex 4a31261d..c06d1ecf 100644\n--- a\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files_internal.h\n+++ b\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files_internal.h\n@@ -13,116 +13,7 @@\n #include \"list.h\"\n #include \"lru_cache.h\"\n #include \"protected_files.h\"\n-\n-#define PF_FILE_ID       0x46505f4850415247 \/* GRAPH_PF *\/\n-#define PF_MAJOR_VERSION 0x01\n-#define PF_MINOR_VERSION 0x00\n-\n-#pragma pack(push, 1)\n-\n-typedef struct _metadata_plain {\n-    uint64_t   file_id;\n-    uint8_t    major_version;\n-    uint8_t    minor_version;\n-    pf_keyid_t metadata_key_id;\n-    pf_mac_t   metadata_gmac; \/* GCM mac *\/\n-} metadata_plain_t;\n-\n-#define PATH_MAX_SIZE (260 + 512)\n-\n-\/\/ these are all defined as relative to node size, so we can decrease node size in tests\n-\/\/ and have deeper tree\n-#define MD_USER_DATA_SIZE (PF_NODE_SIZE * 3 \/ 4) \/\/ 3072\n-static_assert(MD_USER_DATA_SIZE == 3072, \"bad struct size\");\n-\n-typedef struct _metadata_encrypted {\n-    char     path[PATH_MAX_SIZE];\n-    uint64_t size;\n-    pf_key_t mht_key;\n-    pf_mac_t mht_gmac;\n-    uint8_t  data[MD_USER_DATA_SIZE];\n-} metadata_encrypted_t;\n-\n-typedef uint8_t metadata_encrypted_blob_t[sizeof(metadata_encrypted_t)];\n-\n-#define METADATA_NODE_SIZE PF_NODE_SIZE\n-\n-typedef uint8_t metadata_padding_t[METADATA_NODE_SIZE -\n-                                   (sizeof(metadata_plain_t) + sizeof(metadata_encrypted_blob_t))];\n-\n-typedef struct _metadata_node {\n-    metadata_plain_t          plain_part;\n-    metadata_encrypted_blob_t encrypted_part;\n-    metadata_padding_t        padding;\n-} metadata_node_t;\n-\n-static_assert(sizeof(metadata_node_t) == PF_NODE_SIZE, \"sizeof(metadata_node_t)\");\n-\n-typedef struct _data_node_crypto {\n-    pf_key_t key;\n-    pf_mac_t gmac;\n-} gcm_crypto_data_t;\n-\n-\/\/ for PF_NODE_SIZE == 4096, we have 96 attached data nodes and 32 mht child nodes\n-\/\/ for PF_NODE_SIZE == 2048, we have 48 attached data nodes and 16 mht child nodes\n-\/\/ for PF_NODE_SIZE == 1024, we have 24 attached data nodes and 8 mht child nodes\n-\/\/ 3\/4 of the node size is dedicated to data nodes\n-#define ATTACHED_DATA_NODES_COUNT ((PF_NODE_SIZE \/ sizeof(gcm_crypto_data_t)) * 3 \/ 4)\n-static_assert(ATTACHED_DATA_NODES_COUNT == 96, \"ATTACHED_DATA_NODES_COUNT\");\n-\/\/ 1\/4 of the node size is dedicated to child mht nodes\n-#define CHILD_MHT_NODES_COUNT ((PF_NODE_SIZE \/ sizeof(gcm_crypto_data_t)) * 1 \/ 4)\n-static_assert(CHILD_MHT_NODES_COUNT == 32, \"CHILD_MHT_NODES_COUNT\");\n-\n-typedef struct _mht_node {\n-    gcm_crypto_data_t data_nodes_crypto[ATTACHED_DATA_NODES_COUNT];\n-    gcm_crypto_data_t mht_nodes_crypto[CHILD_MHT_NODES_COUNT];\n-} mht_node_t;\n-\n-static_assert(sizeof(mht_node_t) == PF_NODE_SIZE, \"sizeof(mht_node_t)\");\n-\n-typedef struct _data_node {\n-    uint8_t data[PF_NODE_SIZE];\n-} data_node_t;\n-\n-static_assert(sizeof(data_node_t) == PF_NODE_SIZE, \"sizeof(data_node_t)\");\n-\n-typedef struct _encrypted_node {\n-    uint8_t cipher[PF_NODE_SIZE];\n-} encrypted_node_t;\n-\n-static_assert(sizeof(encrypted_node_t) == PF_NODE_SIZE, \"sizeof(encrypted_node_t)\");\n-\n-#define MAX_PAGES_IN_CACHE 48\n-\n-typedef enum {\n-    FILE_MHT_NODE_TYPE  = 1,\n-    FILE_DATA_NODE_TYPE = 2,\n-} mht_node_type_e;\n-\n-\/\/ make sure these are the same size\n-static_assert(sizeof(mht_node_t) == sizeof(data_node_t),\n-              \"sizeof(mht_node_t) == sizeof(data_node_t)\");\n-\n-DEFINE_LIST(_file_node);\n-typedef struct _file_node {\n-    LIST_TYPE(_file_node) list;\n-    uint8_t type;\n-    uint64_t node_number;\n-    struct _file_node* parent;\n-    bool need_writing;\n-    bool new_node;\n-    struct {\n-        uint64_t physical_node_number;\n-        encrypted_node_t encrypted; \/\/ the actual data from the disk\n-    };\n-    union { \/\/ decrypted data\n-        mht_node_t mht;\n-        data_node_t data;\n-    } decrypted;\n-} file_node_t;\n-DEFINE_LISTP(_file_node);\n-\n-#pragma pack(pop)\n+#include \"protected_files_format.h\"\n \n struct pf_context {\n     metadata_node_t file_metadata; \/\/ actual data from disk's meta data node\ndiff --git a\/Pal\/src\/host\/Linux-SGX\/tools\/Makefile b\/Pal\/src\/host\/Linux-SGX\/tools\/Makefile\nindex a5903ad9..87bc7812 100644\n--- a\/Pal\/src\/host\/Linux-SGX\/tools\/Makefile\n+++ b\/Pal\/src\/host\/Linux-SGX\/tools\/Makefile\n@@ -12,3 +12,4 @@ $(targets):\n \t$(MAKE) -C verify-ias-report $@\n \t$(MAKE) -C ra-tls $@\n \t$(MAKE) -C pf_crypt $@\n+\t$(MAKE) -C pf_tamper $@\ndiff --git a\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/.gitignore b\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/.gitignore\nnew file mode 100644\nindex 00000000..a52536d8\n--- \/dev\/null\n+++ b\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/.gitignore\n@@ -0,0 +1 @@\n+\/pf_tamper\ndiff --git a\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/Makefile b\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/Makefile\nnew file mode 100644\nindex 00000000..94c826e4\n--- \/dev\/null\n+++ b\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/Makefile\n@@ -0,0 +1,31 @@\n+include ..\/..\/..\/..\/..\/..\/Scripts\/Makefile.configs\n+include ..\/..\/..\/..\/..\/..\/Scripts\/Makefile.rules\n+\n+CFLAGS += -I..\/.. \\\n+          -I..\/common \\\n+          -I..\/..\/protected-files \\\n+          -I..\/..\/..\/..\/..\/include\/lib \\\n+          -D_GNU_SOURCE\n+\n+LDLIBS += -L..\/common \\\n+          -L..\/..\/..\/..\/..\/lib\/crypto\/mbedtls\/install\/lib \\\n+          -lsgx_util -lmbedcrypto\n+\n+PREFIX ?= \/usr\/local\n+\n+pf_tamper: pf_tamper.o\n+\t$(call cmd,csingle)\n+\n+.PHONY: all\n+all: pf_tamper\n+\n+.PHONY: install\n+install:\n+\tinstall -D pf_tamper -t ${PREFIX}\/bin\n+\n+.PHONY: clean\n+clean:\n+\t$(RM) *.o pf_tamper\n+\n+.PHONY: distclean\n+distclean: clean\ndiff --git a\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/pf_tamper.c b\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/pf_tamper.c\nnew file mode 100644\nindex 00000000..4d39a0d5\n--- \/dev\/null\n+++ b\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/pf_tamper.c\n@@ -0,0 +1,476 @@\n+\/* SPDX-License-Identifier: LGPL-3.0-or-later *\/\n+\/* Copyright (C) 2019-2020 Invisible Things Lab\n+ *                         Rafal Wojdyla <omeg@invisiblethingslab.com>\n+ *\/\n+\n+#include <fcntl.h>\n+#include <getopt.h>\n+#include <stdlib.h>\n+#include <sys\/mman.h>\n+#include <unistd.h>\n+\n+#include \"pf_util.h\"\n+#include \"protected_files.h\"\n+#include \"protected_files_format.h\"\n+#include \"util.h\"\n+\n+\/* Tamper with a PF in various ways for testing purposes. The PF is assumed to be valid and have at\n+ * least enough data to contain two MHT nodes. *\/\n+\n+\/* Command line options *\/\n+struct option g_options[] = {\n+    { \"input\", required_argument, 0, 'i' },\n+    { \"output\", required_argument, 0, 'o' },\n+    { \"wrap-key\", required_argument, 0, 'w' },\n+    { \"verbose\", no_argument, 0, 'v' },\n+    { \"help\", no_argument, 0, 'h' },\n+    { 0, 0, 0, 0 }\n+};\n+\n+static void usage(void) {\n+    INFO(\"\\nUsage: pf_tamper [options]\\n\");\n+    INFO(\"\\nAvailable options:\\n\");\n+    INFO(\"  --help, -h           Display this help\\n\");\n+    INFO(\"  --verbose, -v        Enable verbose output\\n\");\n+    INFO(\"  --wrap-key, -w PATH  Path to wrap key file\\n\");\n+    INFO(\"  --input, -i PATH     Source file to be tampered with (must be a valid PF)\\n\");\n+    INFO(\"  --output, -o PATH    Directory where modified files will be written to\\n\");\n+}\n+\n+#define FATAL(fmt, ...) do { \\\n+    ERROR(fmt, ##__VA_ARGS__); \\\n+    exit(-1); \\\n+} while (0)\n+\n+ssize_t g_input_size = 0;\n+char* g_input_name = NULL;\n+void* g_input_data = MAP_FAILED;\n+char* g_output_dir = NULL;\n+char* g_output_path = NULL;\n+size_t g_output_path_size = 0;\n+pf_key_t g_wrap_key;\n+pf_key_t g_meta_key;\n+\n+static pf_iv_t g_empty_iv = {0};\n+\n+static void derive_main_key(const pf_key_t* kdk, const pf_keyid_t* key_id, pf_key_t* out_key) {\n+    kdf_input_t buf = {0};\n+    pf_status_t status;\n+\n+    buf.index = 1;\n+    strncpy(buf.label, METADATA_KEY_NAME, MAX_LABEL_SIZE);\n+    memcpy(&buf.nonce, key_id, sizeof(buf.nonce));\n+    buf.output_len = 0x80;\n+\n+    status = mbedtls_aes_gcm_encrypt(kdk, &g_empty_iv, &buf, sizeof(buf), NULL, 0, NULL, out_key);\n+    if (PF_FAILURE(status))\n+        FATAL(\"key derivation failed\\n\");\n+}\n+\n+static void make_output_path(const char* suffix) {\n+    snprintf(g_output_path, g_output_path_size, \"%s\/%s.%s\", g_output_dir, g_input_name, suffix);\n+    INFO(\"[*] %s\\n\", g_output_path);\n+}\n+\n+\/* PF layout (node size is PF_NODE_SIZE):\n+ * - Node 0: metadata (metadata_node_t)\n+ *   - metadata_plain_t\n+ *   - metadata_encrypted_t (may include MD_USER_DATA_SIZE bytes of data)\n+ *   - metadata_padding_t\n+ * - Node 1: MHT (mht_node_t)\n+ * - Node 2-97: data (ATTACHED_DATA_NODES_COUNT == 96)\n+ * - Node 98: MHT\n+ * - Node 99-195: data\n+ * - ...\n+ *\/\n+static void truncate_file(const char* suffix, size_t output_size) {\n+    int ret;\n+\n+    make_output_path(suffix);\n+\n+    if (output_size < g_input_size) {\n+        ret = write_file(g_output_path, output_size, g_input_data);\n+    } else {\n+        ret = write_file(g_output_path, g_input_size, g_input_data);\n+        if (ret < 0)\n+            goto out;\n+        ret = truncate(g_output_path, output_size);\n+    }\n+out:\n+    if (ret < 0)\n+        FATAL(\"truncate_file failed: %d\\n\", ret);\n+}\n+\n+#define FIELD_SIZEOF(t, f) (sizeof(((t*)0)->f))\n+#define FIELD_TRUNCATED(t, f) (offsetof(t, f) + (FIELD_SIZEOF(t, f) \/ 2))\n+#define DATA_CRYPTO_SIZE (FIELD_SIZEOF(mht_node_t, data_nodes_crypto))\n+\n+static void tamper_truncate(void) {\n+    size_t mdps = sizeof(metadata_plain_t);\n+    DBG(\"size(metadata_plain_t)             = 0x%04lx\\n\", sizeof(metadata_plain_t));\n+    DBG(\"metadata_plain_t.file_id           : 0x%04lx (0x%04lx)\\n\",\n+        offsetof(metadata_plain_t, file_id), FIELD_SIZEOF(metadata_plain_t, file_id));\n+    DBG(\"metadata_plain_t.major_version     : 0x%04lx (0x%04lx)\\n\",\n+        offsetof(metadata_plain_t, major_version), FIELD_SIZEOF(metadata_plain_t, major_version));\n+    DBG(\"metadata_plain_t.minor_version     : 0x%04lx (0x%04lx)\\n\",\n+        offsetof(metadata_plain_t, minor_version), FIELD_SIZEOF(metadata_plain_t, minor_version));\n+    DBG(\"metadata_plain_t.metadata_key_id   : 0x%04lx (0x%04lx)\\n\",\n+        offsetof(metadata_plain_t, metadata_key_id),\n+        FIELD_SIZEOF(metadata_plain_t, metadata_key_id));\n+    DBG(\"metadata_plain_t.metadata_gmac     : 0x%04lx (0x%04lx)\\n\",\n+        offsetof(metadata_plain_t, metadata_gmac),\n+        FIELD_SIZEOF(metadata_plain_t, metadata_gmac));\n+\n+    DBG(\"size(metadata_encrypted_t)         = 0x%04lx\\n\", sizeof(metadata_encrypted_t));\n+    DBG(\"metadata_encrypted_t.path          : 0x%04lx (0x%04lx)\\n\",\n+        mdps + offsetof(metadata_encrypted_t, path),\n+        FIELD_SIZEOF(metadata_encrypted_t, path));\n+    DBG(\"metadata_encrypted_t.size          : 0x%04lx (0x%04lx)\\n\",\n+        mdps + offsetof(metadata_encrypted_t, size), FIELD_SIZEOF(metadata_encrypted_t, size));\n+    DBG(\"metadata_encrypted_t.mht_key       : 0x%04lx (0x%04lx)\\n\",\n+        mdps + offsetof(metadata_encrypted_t, mht_key),\n+        FIELD_SIZEOF(metadata_encrypted_t, mht_key));\n+    DBG(\"metadata_encrypted_t.mht_gmac      : 0x%04lx (0x%04lx)\\n\",\n+        mdps + offsetof(metadata_encrypted_t, mht_gmac),\n+        FIELD_SIZEOF(metadata_encrypted_t, mht_gmac));\n+    DBG(\"metadata_encrypted_t.data          : 0x%04lx (0x%04lx)\\n\",\n+        mdps + offsetof(metadata_encrypted_t, data), FIELD_SIZEOF(metadata_encrypted_t, data));\n+\n+    DBG(\"size(metadata_padding_t)           = 0x%04lx\\n\", sizeof(metadata_padding_t));\n+    DBG(\"metadata_padding_t                 : 0x%04lx (0x%04lx)\\n\",\n+        mdps + sizeof(metadata_encrypted_t), sizeof(metadata_padding_t));\n+\n+    \/* node 0: metadata + 3k of user data *\/\n+    \/* plain metadata *\/\n+    truncate_file(\"trunc_meta_plain_0\", 0);\n+    truncate_file(\"trunc_meta_plain_1\", FIELD_TRUNCATED(metadata_plain_t, file_id));\n+    truncate_file(\"trunc_meta_plain_2\", offsetof(metadata_plain_t, major_version));\n+    truncate_file(\"trunc_meta_plain_3\", offsetof(metadata_plain_t, minor_version));\n+    truncate_file(\"trunc_meta_plain_4\", offsetof(metadata_plain_t, metadata_key_id));\n+    truncate_file(\"trunc_meta_plain_5\", FIELD_TRUNCATED(metadata_plain_t, metadata_key_id));\n+    truncate_file(\"trunc_meta_plain_6\", offsetof(metadata_plain_t, metadata_gmac));\n+    truncate_file(\"trunc_meta_plain_7\", FIELD_TRUNCATED(metadata_plain_t, metadata_gmac));\n+\n+    \/* encrypted metadata *\/\n+    truncate_file(\"trunc_meta_enc_0\", mdps + offsetof(metadata_encrypted_t, path));\n+    truncate_file(\"trunc_meta_enc_1\", mdps + FIELD_TRUNCATED(metadata_encrypted_t, path));\n+    truncate_file(\"trunc_meta_enc_2\", mdps + offsetof(metadata_encrypted_t, size));\n+    truncate_file(\"trunc_meta_enc_3\", mdps + FIELD_TRUNCATED(metadata_encrypted_t, size));\n+    truncate_file(\"trunc_meta_enc_4\", mdps + offsetof(metadata_encrypted_t, mht_key));\n+    truncate_file(\"trunc_meta_enc_5\", mdps + FIELD_TRUNCATED(metadata_encrypted_t, mht_key));\n+    truncate_file(\"trunc_meta_enc_6\", mdps + offsetof(metadata_encrypted_t, mht_gmac));\n+    truncate_file(\"trunc_meta_enc_7\", mdps + FIELD_TRUNCATED(metadata_encrypted_t, mht_gmac));\n+    truncate_file(\"trunc_meta_enc_8\", mdps + offsetof(metadata_encrypted_t, data));\n+    truncate_file(\"trunc_meta_enc_9\", mdps + FIELD_TRUNCATED(metadata_encrypted_t, data));\n+\n+    \/* padding *\/\n+    truncate_file(\"trunc_meta_pad_0\", mdps + sizeof(metadata_encrypted_t));\n+    truncate_file(\"trunc_meta_pad_1\", mdps + sizeof(metadata_encrypted_t)\n+                  + sizeof(metadata_padding_t) \/ 2);\n+\n+    \/* node 1: mht root *\/\n+    \/* after node 0 *\/\n+    truncate_file(\"trunc_mht_0\", PF_NODE_SIZE);\n+    \/* middle of data_nodes_crypto[0].key *\/\n+    truncate_file(\"trunc_mht_1\", PF_NODE_SIZE + PF_KEY_SIZE \/ 2);\n+    \/* after data_nodes_crypto[0].key *\/\n+    truncate_file(\"trunc_mht_2\", PF_NODE_SIZE + PF_KEY_SIZE);\n+    \/* middle of data_nodes_crypto[0].gmac *\/\n+    truncate_file(\"trunc_mht_3\", PF_NODE_SIZE + PF_KEY_SIZE + PF_MAC_SIZE \/ 2);\n+    \/* after data_nodes_crypto[0].gmac *\/\n+    truncate_file(\"trunc_mht_4\", PF_NODE_SIZE + PF_KEY_SIZE + PF_MAC_SIZE);\n+    \/* after data_nodes_crypto *\/\n+    truncate_file(\"trunc_mht_5\", PF_NODE_SIZE + DATA_CRYPTO_SIZE);\n+    \/* middle of mht_nodes_crypto[0].key *\/\n+    truncate_file(\"trunc_mht_6\", PF_NODE_SIZE + DATA_CRYPTO_SIZE + PF_KEY_SIZE \/ 2);\n+    \/* after mht_nodes_crypto[0].key *\/\n+    truncate_file(\"trunc_mht_7\", PF_NODE_SIZE + DATA_CRYPTO_SIZE + PF_KEY_SIZE);\n+    \/* middle of mht_nodes_crypto[0].gmac *\/\n+    truncate_file(\"trunc_mht_8\", PF_NODE_SIZE + DATA_CRYPTO_SIZE + PF_KEY_SIZE + PF_MAC_SIZE \/ 2);\n+    \/* after mht_nodes_crypto[0].gmac *\/\n+    truncate_file(\"trunc_mht_9\", PF_NODE_SIZE + DATA_CRYPTO_SIZE + PF_KEY_SIZE + PF_MAC_SIZE);\n+\n+    \/* node 2-3: data #0, #1 *\/\n+    \/* after mht root *\/\n+    truncate_file(\"trunc_data_0\", 2 * PF_NODE_SIZE);\n+    \/* middle of data #0 *\/\n+    truncate_file(\"trunc_data_1\", 2 * PF_NODE_SIZE + PF_NODE_SIZE \/ 2);\n+    \/* after data #0 *\/\n+    truncate_file(\"trunc_data_2\", 3 * PF_NODE_SIZE);\n+    \/* middle of data #1 *\/\n+    truncate_file(\"trunc_data_3\", 3 * PF_NODE_SIZE + PF_NODE_SIZE \/ 2);\n+\n+    \/* extend *\/\n+    truncate_file(\"extend_0\", g_input_size + 1);\n+    truncate_file(\"extend_1\", g_input_size + PF_NODE_SIZE \/ 2);\n+    truncate_file(\"extend_2\", g_input_size + PF_NODE_SIZE);\n+    truncate_file(\"extend_3\", g_input_size + PF_NODE_SIZE + PF_NODE_SIZE \/ 2);\n+}\n+\n+\/* returns mmap'd output contents *\/\n+static void* create_output(const char* path) {\n+    void* mem = MAP_FAILED;\n+    int fd = open(path, O_RDWR|O_CREAT, 0664);\n+    if (fd < 0)\n+        FATAL(\"Failed to open output file '%s': %s\\n\", path, strerror(errno));\n+\n+    if (ftruncate(fd, g_input_size) < 0)\n+        FATAL(\"Failed to ftruncate output file '%s': %s\\n\", path, strerror(errno));\n+\n+    mem = mmap(NULL, g_input_size, PROT_READ|PROT_WRITE, MAP_SHARED, fd, 0);\n+    if (mem == MAP_FAILED)\n+        FATAL(\"Failed to mmap output file '%s': %s\\n\", path, strerror(errno));\n+\n+    memcpy(mem, g_input_data, g_input_size);\n+\n+    close(fd);\n+    return mem;\n+}\n+\n+static void pf_decrypt(const void* encrypted, size_t size, const pf_key_t* key, const pf_mac_t* mac,\n+                       void* decrypted, const char* msg) {\n+    pf_status_t status = mbedtls_aes_gcm_decrypt(key, &g_empty_iv, NULL, 0,\n+                                                 encrypted, size,\n+                                                 decrypted, mac);\n+    if (PF_FAILURE(status))\n+        FATAL(\"decrypting %s failed\\n\", msg);\n+}\n+\n+static void pf_encrypt(const void* decrypted, size_t size, const pf_key_t* key, pf_mac_t* mac,\n+                       void* encrypted, const char* msg) {\n+    pf_status_t status = mbedtls_aes_gcm_encrypt(key, &g_empty_iv, NULL, 0,\n+                                                 decrypted, size,\n+                                                 encrypted, mac);\n+    if (PF_FAILURE(status))\n+        FATAL(\"encrypting %s failed\\n\", msg);\n+}\n+\n+\/* copy input PF and apply some modifications *\/\n+#define __BREAK_PF(suffix, ...) do { \\\n+    make_output_path(suffix); \\\n+    meta = create_output(g_output_path); \\\n+    out = (uint8_t*)meta; \\\n+    pf_decrypt(&meta->encrypted_part, sizeof(meta->encrypted_part), &g_meta_key, \\\n+               &meta->plain_part.metadata_gmac, meta_dec, \"metadata\"); \\\n+    mht_enc = (mht_node_t*)(out + PF_NODE_SIZE); \\\n+    pf_decrypt(mht_enc, sizeof(*mht_enc), &meta_dec->mht_key, &meta_dec->mht_gmac, mht_dec, \\\n+               \"mht\"); \\\n+    __VA_ARGS__ \\\n+    munmap(meta, g_input_size); \\\n+} while (0)\n+\n+\/* if update is true, also create a file with correct metadata MAC *\/\n+#define BREAK_PF(suffix, update, ...) do { \\\n+    __BREAK_PF(suffix, __VA_ARGS__); \\\n+    if (update) { \\\n+        __BREAK_PF(suffix \"_fixed\", __VA_ARGS__ { \\\n+                       pf_encrypt(meta_dec, sizeof(*meta_dec), &g_meta_key, \\\n+                                  &meta->plain_part.metadata_gmac, meta->encrypted_part, \\\n+                                  \"metadata\"); \\\n+                   } ); \\\n+    } \\\n+} while (0)\n+\n+#define BREAK_MHT(suffix, ...) do { \\\n+    __BREAK_PF(suffix, __VA_ARGS__ { \\\n+                   pf_encrypt(mht_dec, sizeof(*mht_dec), &meta_dec->mht_key, &meta_dec->mht_gmac, \\\n+                              mht_enc, \"mht\"); \\\n+               } ); \\\n+} while (0)\n+\n+#define LAST_BYTE(array) (((uint8_t*)&array)[sizeof(array) - 1])\n+\n+static void tamper_modify(void) {\n+    metadata_node_t* meta = NULL;\n+    uint8_t* out = NULL;\n+    metadata_encrypted_t* meta_dec = malloc(sizeof(*meta_dec));\n+    if (!meta_dec)\n+        FATAL(\"Out of memory\\n\");\n+    mht_node_t* mht_enc = NULL;\n+    mht_node_t* mht_dec = malloc(sizeof(*mht_dec));\n+    if (!mht_dec)\n+        FATAL(\"Out of memory\\n\");\n+\n+    \/* plain part of the metadata isn't covered by the MAC so no point updating it *\/\n+    BREAK_PF(\"meta_plain_id_0\", \/*update=*\/false,\n+             { meta->plain_part.file_id = 0; });\n+    BREAK_PF(\"meta_plain_id_1\", \/*update=*\/false,\n+             { meta->plain_part.file_id = UINT64_MAX; });\n+    BREAK_PF(\"meta_plain_version_0\", \/*update=*\/false,\n+             { meta->plain_part.major_version = 0; });\n+    BREAK_PF(\"meta_plain_version_1\", \/*update=*\/false,\n+             { meta->plain_part.major_version = 0xff; });\n+    BREAK_PF(\"meta_plain_version_2\", \/*update=*\/false,\n+             { meta->plain_part.minor_version = 0xff; });\n+\n+    \/* metadata_key_id is the keying material for encrypted metadata key derivation, so create also\n+     * PFs with updated MACs *\/\n+    BREAK_PF(\"meta_plain_keyid_0\", \/*update=*\/true,\n+             { meta->plain_part.metadata_key_id[0] ^= 1; });\n+    BREAK_PF(\"meta_plain_keyid_1\", \/*update=*\/true,\n+             { LAST_BYTE(meta->plain_part.metadata_key_id) ^= 0xfe; });\n+    BREAK_PF(\"meta_plain_mac_0\", \/*update=*\/true,\n+             { meta->plain_part.metadata_gmac[0] ^= 0xfe; });\n+    BREAK_PF(\"meta_plain_mac_1\", \/*update=*\/true,\n+             { LAST_BYTE(meta->plain_part.metadata_gmac) &= 1; });\n+\n+    BREAK_PF(\"meta_enc_filename_0\", \/*update=*\/true,\n+             { meta_dec->path[0] = 0; });\n+    BREAK_PF(\"meta_enc_filename_1\", \/*update=*\/true,\n+             { meta_dec->path[0] ^= 1; });\n+    BREAK_PF(\"meta_enc_filename_2\", \/*update=*\/true,\n+             { LAST_BYTE(meta_dec->path) ^= 0xfe; });\n+    BREAK_PF(\"meta_enc_size_0\", \/*update=*\/true,\n+             { meta_dec->size = 0; });\n+    BREAK_PF(\"meta_enc_size_1\", \/*update=*\/true,\n+             { meta_dec->size = g_input_size - 1; });\n+    BREAK_PF(\"meta_enc_size_2\", \/*update=*\/true,\n+             { meta_dec->size = g_input_size + 1; });\n+    BREAK_PF(\"meta_enc_size_3\", \/*update=*\/true,\n+             { meta_dec->size = UINT64_MAX; });\n+    BREAK_PF(\"meta_enc_mht_key_0\", \/*update=*\/true,\n+             { meta_dec->mht_key[0] ^= 1; });\n+    BREAK_PF(\"meta_enc_mht_key_1\", \/*update=*\/true,\n+             { LAST_BYTE(meta_dec->mht_key) ^= 0xfe; });\n+    BREAK_PF(\"meta_enc_mht_mac_0\", \/*update=*\/true,\n+             { meta_dec->mht_gmac[0] ^= 1; });\n+    BREAK_PF(\"meta_enc_mht_mac_1\", \/*update=*\/true,\n+             { LAST_BYTE(meta_dec->mht_gmac) ^= 0xfe; });\n+    BREAK_PF(\"meta_enc_data_0\", \/*update=*\/true,\n+             { meta_dec->data[0] ^= 0xfe; });\n+    BREAK_PF(\"meta_enc_data_1\", \/*update=*\/true,\n+             { LAST_BYTE(meta_dec->data) ^= 1; });\n+\n+    \/* padding is ignored *\/\n+    BREAK_PF(\"meta_padding_0\", \/*update=*\/false,\n+             { meta->padding[0] ^= 1; });\n+    BREAK_PF(\"meta_padding_1\", \/*update=*\/false,\n+             { LAST_BYTE(meta->padding) ^= 0xfe; });\n+\n+    BREAK_MHT(\"mht_0\", { mht_dec->data_nodes_crypto[0].key[0] ^= 1; });\n+    BREAK_MHT(\"mht_1\", { mht_dec->data_nodes_crypto[0].gmac[0] ^= 1; });\n+    BREAK_MHT(\"mht_2\", { mht_dec->mht_nodes_crypto[0].key[0] ^= 1; });\n+    BREAK_MHT(\"mht_3\", { mht_dec->mht_nodes_crypto[0].gmac[0] ^= 1; });\n+    BREAK_MHT(\"mht_4\", { mht_dec->data_nodes_crypto[ATTACHED_DATA_NODES_COUNT - 1].key[0] ^= 1; });\n+    BREAK_MHT(\"mht_5\", { mht_dec->data_nodes_crypto[ATTACHED_DATA_NODES_COUNT - 1].gmac[0] ^= 1; });\n+    BREAK_MHT(\"mht_6\", { mht_dec->mht_nodes_crypto[CHILD_MHT_NODES_COUNT - 1].key[0] ^= 1; });\n+    BREAK_MHT(\"mht_7\", { mht_dec->mht_nodes_crypto[CHILD_MHT_NODES_COUNT - 1].gmac[0] ^= 1; });\n+    BREAK_MHT(\"mht_8\", {\n+        gcm_crypto_data_t crypto;\n+        memcpy(&crypto, &mht_dec->data_nodes_crypto[0], sizeof(crypto));\n+        memcpy(&mht_dec->data_nodes_crypto[0], &mht_dec->data_nodes_crypto[1], sizeof(crypto));\n+        memcpy(&mht_dec->data_nodes_crypto[1], &crypto, sizeof(crypto));\n+    });\n+    BREAK_MHT(\"mht_9\", {\n+        gcm_crypto_data_t crypto;\n+        memcpy(&crypto, &mht_dec->mht_nodes_crypto[0], sizeof(crypto));\n+        memcpy(&mht_dec->mht_nodes_crypto[0], &mht_dec->mht_nodes_crypto[1], sizeof(crypto));\n+        memcpy(&mht_dec->mht_nodes_crypto[1], &crypto, sizeof(crypto));\n+    });\n+\n+    \/* data nodes start from node #2 *\/\n+    BREAK_PF(\"data_0\", \/*update=*\/false,\n+             { *(out + 2 * PF_NODE_SIZE) ^= 1; });\n+    BREAK_PF(\"data_1\", \/*update=*\/false,\n+             { *(out + 3 * PF_NODE_SIZE - 1) ^= 1; });\n+    BREAK_PF(\"data_2\", \/*update=*\/false, {\n+        \/* swap data nodes *\/\n+        memcpy(out + 2 * PF_NODE_SIZE, g_input_data + 3 * PF_NODE_SIZE, PF_NODE_SIZE);\n+        memcpy(out + 3 * PF_NODE_SIZE, g_input_data + 2 * PF_NODE_SIZE, PF_NODE_SIZE);\n+    });\n+\n+    free(mht_dec);\n+    free(meta_dec);\n+}\n+\n+int main(int argc, char* argv[]) {\n+    int ret = -1;\n+\n+    int option          = 0;\n+    char* input_path    = NULL;\n+    char* wrap_key_path = NULL;\n+    int input_fd        = -1;\n+\n+    while (true) {\n+        option = getopt_long(argc, argv, \"i:o:w:vh\", g_options, NULL);\n+        if (option == -1)\n+            break;\n+\n+        switch (option) {\n+            case 'i':\n+                input_path = optarg;\n+                break;\n+            case 'o':\n+                g_output_dir = optarg;\n+                break;\n+            case 'w':\n+                wrap_key_path = optarg;\n+                break;\n+            case 'v':\n+                set_verbose(true);\n+                break;\n+            case 'h':\n+                usage();\n+                return 0;\n+            default:\n+                ERROR(\"Unknown option: %c\\n\", option);\n+                usage();\n+        }\n+    }\n+\n+    if (!input_path) {\n+        ERROR(\"Input path not specified\\n\");\n+        usage();\n+        goto out;\n+    }\n+\n+    if (!g_output_dir) {\n+        ERROR(\"Output path not specified\\n\");\n+        usage();\n+        goto out;\n+    }\n+\n+    if (!wrap_key_path) {\n+        ERROR(\"Wrap key path not specified\\n\");\n+        usage();\n+        goto out;\n+    }\n+\n+    input_fd = open(input_path, O_RDONLY);\n+    if (input_fd < 0) {\n+        ERROR(\"Failed to open input file '%s': %s\\n\", input_path, strerror(errno));\n+        goto out;\n+    }\n+\n+    g_input_size = get_file_size(input_fd);\n+    if (g_input_size < 0) {\n+        ERROR(\"Failed to stat input file '%s': %s\\n\", input_path, strerror(errno));\n+        goto out;\n+    }\n+\n+    g_input_data = mmap(NULL, g_input_size, PROT_READ, MAP_PRIVATE, input_fd, 0);\n+    if (g_input_data == MAP_FAILED) {\n+        ERROR(\"Failed to mmap input file '%s': %s\\n\", input_path, strerror(errno));\n+        goto out;\n+    }\n+\n+    load_wrap_key(wrap_key_path, &g_wrap_key);\n+    derive_main_key(&g_wrap_key, &((metadata_plain_t*)g_input_data)->metadata_key_id,\n+                    &g_meta_key);\n+\n+    g_input_name = basename(input_path);\n+    g_output_path_size = strlen(g_input_name) + strlen(g_output_dir) + 256;\n+    g_output_path = malloc(g_output_path_size);\n+    if (!g_output_path) {\n+        ERROR(\"No memory\\n\");\n+        goto out;\n+    }\n+\n+    tamper_truncate();\n+    tamper_modify();\n+    ret = 0;\n+\n+out:\n+    \/* skip cleanup as we are in main() *\/\n+    return ret;\n+}\n","files":{"\/LibOS\/shim\/test\/fs\/test_pf.py":{"changes":[{"diff":"\n \n     def __corrupt_file(self, input_path, output_path):\n         cmd = [self.PF_TAMPER, '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path]\n-        return self.run_native_binary(cmd)\n+        return self.run_native_binary(cmd, libpath=os.path.join(os.getcwd(), 'lib'))\n \n     # invalid\/corrupted files\n-    @expectedFailureIf(HAS_SGX)\n-    # pylint: disable=fixme\n     def test_500_invalid(self):\n-        # TODO: port these to the new file format\n         invalid_dir = os.path.join(self.TEST_DIR, 'pf_invalid')\n-        # files below should work normally (benign modifications)\n-        should_pass = ['chunk_padding_1_fixed', 'chunk_padding_2_fixed', 'chunk_data_3',\n-                       'chunk_data_3_fixed', 'chunk_data_4', 'chunk_data_4_fixed']\n         if not os.path.exists(invalid_dir):\n             os.mkdir(invalid_dir)\n+\n         # prepare valid encrypted file (largest one for maximum possible corruptions)\n         original_input = self.OUTPUT_FILES[-1]\n         self.__encrypt_file(self.INPUT_FILES[-1], original_input)\n         # generate invalid files based on the above\n         self.__corrupt_file(original_input, invalid_dir)\n+\n         # try to decrypt invalid files\n         for name in os.listdir(invalid_dir):\n             invalid = os.path.join(invalid_dir, name)\n","add":3,"remove":7,"filename":"\/LibOS\/shim\/test\/fs\/test_pf.py","badparts":["        return self.run_native_binary(cmd)","    @expectedFailureIf(HAS_SGX)","        should_pass = ['chunk_padding_1_fixed', 'chunk_padding_2_fixed', 'chunk_data_3',","                       'chunk_data_3_fixed', 'chunk_data_4', 'chunk_data_4_fixed']"],"goodparts":["        return self.run_native_binary(cmd, libpath=os.path.join(os.getcwd(), 'lib'))"]},{"diff":"\n             input_path = os.path.join(invalid_dir, os.path.basename(original_input))\n             # copy the file so it has the original file name (for allowed path check)\n             shutil.copy(invalid, input_path)\n-            should_pass = any(s in name for s in should_pass)\n \n             try:\n                 args = ['decrypt', '-V', '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path]\n                 self.__pf_crypt(args)\n             except subprocess.CalledProcessError as exc:\n-                if should_pass:\n-                    self.assertEqual(exc.returncode, 0)\n-                else:\n-                    self.assertNotEqual(exc.returncode, 0)\n+                # decryption of invalid file must fail with -1 (wrapped to 255)\n+                self.assertEqual(exc.returncode, 255)\n             else:\n-                if not should_pass:\n-                    print('[!] Fail: successfully decrypted file: ' + name)\n-                    self.fail()\n+                print('[!] Fail: successfully decrypted file: ' + name)\n+                self.fail()","add":4,"remove":8,"filename":"\/LibOS\/shim\/test\/fs\/test_pf.py","badparts":["            should_pass = any(s in name for s in should_pass)","                if should_pass:","                    self.assertEqual(exc.returncode, 0)","                else:","                    self.assertNotEqual(exc.returncode, 0)","                if not should_pass:","                    print('[!] Fail: successfully decrypted file: ' + name)","                    self.fail()"],"goodparts":["                self.assertEqual(exc.returncode, 255)","                print('[!] Fail: successfully decrypted file: ' + name)","                self.fail()"]}],"source":"\n import filecmp import os import shutil import subprocess import unittest from test_fs import( TC_00_FileSystem, ) from regression import( HAS_SGX, expectedFailureIf, ) @unittest.skipUnless(HAS_SGX, 'Protected files require SGX support') class TC_50_ProtectedFiles(TC_00_FileSystem): @classmethod def setUpClass(cls): cls.PF_CRYPT='bin\/pf_crypt' cls.PF_TAMPER='bin\/pf_tamper' cls.WRAP_KEY=os.path.join(cls.TEST_DIR, 'wrap-key') cls.CONST_WRAP_KEY=[0xff, 0xee, 0xdd, 0xcc, 0xbb, 0xaa, 0x99, 0x88, 0x77, 0x66, 0x55, 0x44, 0x33, 0x22, 0x11, 0x00] cls.ENCRYPTED_DIR=os.path.join(cls.TEST_DIR, 'pf_input') cls.ENCRYPTED_FILES=[os.path.join(cls.ENCRYPTED_DIR, str(v)) for v in cls.FILE_SIZES] cls.LIB_PATH=os.path.join(os.getcwd(), 'lib') super().setUpClass() if not os.path.exists(cls.ENCRYPTED_DIR): os.mkdir(cls.ENCRYPTED_DIR) cls.OUTPUT_DIR=os.path.join(cls.TEST_DIR, 'pf_output') cls.OUTPUT_FILES=[os.path.join(cls.OUTPUT_DIR, str(x)) for x in cls.FILE_SIZES] cls.__set_default_key(cls) for i in cls.INDEXES: cmd=[cls.PF_CRYPT, 'encrypt', '-w', cls.WRAP_KEY, '-i', cls.INPUT_FILES[i], '-o', cls.ENCRYPTED_FILES[i]] cls.run_native_binary(cmd, libpath=os.path.join(os.getcwd(), 'lib')) def __pf_crypt(self, args): args.insert(0, self.PF_CRYPT) return self.run_native_binary(args, libpath=os.path.join(os.getcwd(), 'lib')) def __set_default_key(self): with open(self.WRAP_KEY, 'wb') as file: file.write(bytes(self.CONST_WRAP_KEY)) def copy_input(self, input_path, output_path): self.__encrypt_file(input_path, output_path) def __encrypt_file(self, input_path, output_path): args=['encrypt', '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path] stdout, stderr=self.__pf_crypt(args) return(stdout, stderr) def __decrypt_file(self, input_path, output_path): args=['decrypt', '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path] stdout, stderr=self.__pf_crypt(args) return(stdout, stderr) def test_000_gen_key(self): key_path=os.path.join(self.TEST_DIR, 'tmpkey') args=['gen-key', '-w', key_path] stdout, _=self.__pf_crypt(args) self.assertIn('Wrap key saved to: ' +key_path, stdout) self.assertEqual(os.path.getsize(key_path), 16) os.remove(key_path) def test_010_encrypt_decrypt(self): for i in self.INDEXES: self.__encrypt_file(self.INPUT_FILES[i], self.OUTPUT_FILES[i]) self.assertFalse(filecmp.cmp(self.INPUT_FILES[i], self.OUTPUT_FILES[i], shallow=False)) dec_path=os.path.join(self.OUTPUT_DIR, os.path.basename(self.OUTPUT_FILES[i]) +'.dec') self.__decrypt_file(self.OUTPUT_FILES[i], dec_path) self.assertTrue(filecmp.cmp(self.INPUT_FILES[i], dec_path, shallow=False)) def test_100_open_close(self): input_path=self.ENCRYPTED_FILES[-1] output_path=os.path.join(self.OUTPUT_DIR, 'test_100') stdout, stderr=self.run_binary(['open_close', 'R', input_path]) self.verify_open_close(stdout, stderr, input_path, 'input') try: stdout, stderr=self.run_binary(['open_close', 'W', output_path]) self.assertIn('ERROR: Failed to open output file', stderr) except subprocess.CalledProcessError as exc: self.assertNotEqual(exc.returncode, 0) self.assertTrue(os.path.isfile(output_path)) else: print('[!] Fail: open_close returned 0') self.fail() def test_101_open_flags(self): file_path=os.path.join(self.OUTPUT_DIR, 'test_101') stdout, stderr=self.run_binary(['open_flags', file_path]) self.verify_open_flags(stdout, stderr) def test_115_seek_tell(self): plaintext_path=self.INPUT_FILES[-1] input_path=self.ENCRYPTED_FILES[-1] output_path_1=os.path.join(self.OUTPUT_DIR, 'test_115a') output_path_2=os.path.join(self.OUTPUT_DIR, 'test_115b') self.copy_input(plaintext_path, output_path_1) self.copy_input(plaintext_path, output_path_2) stdout, stderr=self.run_binary(['seek_tell', input_path, output_path_1, output_path_2]) self.verify_seek_tell(stdout, stderr, input_path, output_path_1, output_path_2, self.FILE_SIZES[-1]) def test_130_file_stat(self): for i in self.INDEXES: input_path=self.ENCRYPTED_FILES[i] output_path=self.OUTPUT_FILES[i] size=str(self.FILE_SIZES[i]) self.copy_input(self.INPUT_FILES[i], output_path) stdout, stderr=self.run_binary(['stat', input_path, output_path]) self.verify_stat(stdout, stderr, input_path, output_path, size) def verify_size(self, file, size): dec_path=os.path.join(self.OUTPUT_DIR, os.path.basename(file) +'.dec') self.__decrypt_file(file, dec_path) self.assertEqual(os.stat(dec_path).st_size, size) @expectedFailureIf(HAS_SGX) def test_140_file_truncate(self): self.fail() def test_150_file_rename(self): path1=os.path.join(self.OUTPUT_DIR, 'test_150a') path2=os.path.join(self.OUTPUT_DIR, 'test_150b') self.copy_input(self.ENCRYPTED_FILES[-1], path1) shutil.copy(path1, path2) args=['decrypt', '-V', '-w', self.WRAP_KEY, '-i', path2, '-o', path1] try: self.__pf_crypt(args) except subprocess.CalledProcessError as exc: self.assertNotEqual(exc.returncode, 0) else: print('[!] Fail: successfully decrypted renamed file: ' +path2) self.fail() def verify_copy_content(self, input_path, output_path): dec_path=os.path.join(self.OUTPUT_DIR, os.path.basename(output_path) +'.dec') self.__decrypt_file(output_path, dec_path) self.assertTrue(filecmp.cmp(input_path, dec_path, shallow=False)) def do_copy_test(self, executable, timeout): stdout, stderr=self.run_binary([executable, self.ENCRYPTED_DIR, self.OUTPUT_DIR], timeout=timeout) self.verify_copy(stdout, stderr, self.ENCRYPTED_DIR, executable) def test_204_copy_dir_mmap_whole(self): self.do_copy_test('copy_mmap_whole', 30) def test_205_copy_dir_mmap_seq(self): self.do_copy_test('copy_mmap_seq', 60) def test_206_copy_dir_mmap_rev(self): self.do_copy_test('copy_mmap_rev', 60) def test_210_copy_dir_mounted(self): executable='copy_whole' stdout, stderr=self.run_binary([executable, '\/mounted\/pf_input', '\/mounted\/pf_output'], timeout=30) self.verify_copy(stdout, stderr, '\/mounted\/pf_input', executable) def __corrupt_file(self, input_path, output_path): cmd=[self.PF_TAMPER, '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path] return self.run_native_binary(cmd) @expectedFailureIf(HAS_SGX) def test_500_invalid(self): invalid_dir=os.path.join(self.TEST_DIR, 'pf_invalid') should_pass=['chunk_padding_1_fixed', 'chunk_padding_2_fixed', 'chunk_data_3', 'chunk_data_3_fixed', 'chunk_data_4', 'chunk_data_4_fixed'] if not os.path.exists(invalid_dir): os.mkdir(invalid_dir) original_input=self.OUTPUT_FILES[-1] self.__encrypt_file(self.INPUT_FILES[-1], original_input) self.__corrupt_file(original_input, invalid_dir) for name in os.listdir(invalid_dir): invalid=os.path.join(invalid_dir, name) output_path=os.path.join(self.OUTPUT_DIR, name) input_path=os.path.join(invalid_dir, os.path.basename(original_input)) shutil.copy(invalid, input_path) should_pass=any(s in name for s in should_pass) try: args=['decrypt', '-V', '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path] self.__pf_crypt(args) except subprocess.CalledProcessError as exc: if should_pass: self.assertEqual(exc.returncode, 0) else: self.assertNotEqual(exc.returncode, 0) else: if not should_pass: print('[!] Fail: successfully decrypted file: ' +name) self.fail() ","sourceWithComments":"#!\/usr\/bin\/env python3\n\nimport filecmp\nimport os\nimport shutil\nimport subprocess\nimport unittest\n\nfrom test_fs import (\n    TC_00_FileSystem,\n)\n\nfrom regression import (\n    HAS_SGX,\n    expectedFailureIf,\n)\n\n@unittest.skipUnless(HAS_SGX, 'Protected files require SGX support')\nclass TC_50_ProtectedFiles(TC_00_FileSystem):\n    @classmethod\n    def setUpClass(cls):\n        cls.PF_CRYPT = 'bin\/pf_crypt'\n        cls.PF_TAMPER = 'bin\/pf_tamper'\n        cls.WRAP_KEY = os.path.join(cls.TEST_DIR, 'wrap-key')\n        # CONST_WRAP_KEY must match the one in manifest\n        cls.CONST_WRAP_KEY = [0xff, 0xee, 0xdd, 0xcc, 0xbb, 0xaa, 0x99, 0x88,\n                              0x77, 0x66, 0x55, 0x44, 0x33, 0x22, 0x11, 0x00]\n        cls.ENCRYPTED_DIR = os.path.join(cls.TEST_DIR, 'pf_input')\n        cls.ENCRYPTED_FILES = [os.path.join(cls.ENCRYPTED_DIR, str(v)) for v in cls.FILE_SIZES]\n        cls.LIB_PATH = os.path.join(os.getcwd(), 'lib')\n\n        super().setUpClass()\n        if not os.path.exists(cls.ENCRYPTED_DIR):\n            os.mkdir(cls.ENCRYPTED_DIR)\n        cls.OUTPUT_DIR = os.path.join(cls.TEST_DIR, 'pf_output')\n        cls.OUTPUT_FILES = [os.path.join(cls.OUTPUT_DIR, str(x)) for x in cls.FILE_SIZES]\n        # create encrypted files\n        cls.__set_default_key(cls)\n        for i in cls.INDEXES:\n            cmd = [cls.PF_CRYPT, 'encrypt', '-w', cls.WRAP_KEY, '-i', cls.INPUT_FILES[i], '-o',\n                   cls.ENCRYPTED_FILES[i]]\n\n            cls.run_native_binary(cmd, libpath=os.path.join(os.getcwd(), 'lib'))\n\n    def __pf_crypt(self, args):\n        args.insert(0, self.PF_CRYPT)\n        return self.run_native_binary(args, libpath=os.path.join(os.getcwd(), 'lib'))\n\n    def __set_default_key(self):\n        with open(self.WRAP_KEY, 'wb') as file:\n            file.write(bytes(self.CONST_WRAP_KEY))\n\n    # overrides TC_00_FileSystem to encrypt the file instead of just copying\n    def copy_input(self, input_path, output_path):\n        self.__encrypt_file(input_path, output_path)\n\n    def __encrypt_file(self, input_path, output_path):\n        args = ['encrypt', '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path]\n        stdout, stderr = self.__pf_crypt(args)\n        return (stdout, stderr)\n\n    def __decrypt_file(self, input_path, output_path):\n        args = ['decrypt', '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path]\n        stdout, stderr = self.__pf_crypt(args)\n        return (stdout, stderr)\n\n    def test_000_gen_key(self):\n        # test random key generation\n        key_path = os.path.join(self.TEST_DIR, 'tmpkey')\n        args = ['gen-key', '-w', key_path]\n        stdout, _ = self.__pf_crypt(args)\n        self.assertIn('Wrap key saved to: ' + key_path, stdout)\n        self.assertEqual(os.path.getsize(key_path), 16)\n        os.remove(key_path)\n\n    def test_010_encrypt_decrypt(self):\n        for i in self.INDEXES:\n            self.__encrypt_file(self.INPUT_FILES[i], self.OUTPUT_FILES[i])\n            self.assertFalse(filecmp.cmp(self.INPUT_FILES[i], self.OUTPUT_FILES[i], shallow=False))\n            dec_path = os.path.join(self.OUTPUT_DIR,\n                                    os.path.basename(self.OUTPUT_FILES[i]) + '.dec')\n            self.__decrypt_file(self.OUTPUT_FILES[i], dec_path)\n            self.assertTrue(filecmp.cmp(self.INPUT_FILES[i], dec_path, shallow=False))\n\n    # overrides TC_00_FileSystem to change input dir (from plaintext to encrypted)\n    def test_100_open_close(self):\n        # the test binary expects a path to read-only (existing) file or a path to file that\n        # will get created\n        input_path = self.ENCRYPTED_FILES[-1] # existing file\n        output_path = os.path.join(self.OUTPUT_DIR, 'test_100') # new file\n        stdout, stderr = self.run_binary(['open_close', 'R', input_path])\n        self.verify_open_close(stdout, stderr, input_path, 'input')\n        # the following test tries to open multiple handles to a single writable PF, should fail\n        try:\n            stdout, stderr = self.run_binary(['open_close', 'W', output_path])\n            self.assertIn('ERROR: Failed to open output file', stderr)\n        except subprocess.CalledProcessError as exc:\n            self.assertNotEqual(exc.returncode, 0)\n            self.assertTrue(os.path.isfile(output_path))\n        else:\n            print('[!] Fail: open_close returned 0')\n            self.fail()\n\n    # overrides TC_00_FileSystem to change input dir (from plaintext to encrypted)\n    def test_101_open_flags(self):\n        # the test binary expects a path to file that will get created\n        file_path = os.path.join(self.OUTPUT_DIR, 'test_101') # new file\n        stdout, stderr = self.run_binary(['open_flags', file_path])\n        self.verify_open_flags(stdout, stderr)\n\n    # overrides TC_00_FileSystem to change input dir (from plaintext to encrypted)\n    def test_115_seek_tell(self):\n        # the test binary expects a path to read-only (existing) file and two paths to files that\n        # will get created\n        plaintext_path = self.INPUT_FILES[-1]\n        input_path = self.ENCRYPTED_FILES[-1] # existing file\n        output_path_1 = os.path.join(self.OUTPUT_DIR, 'test_115a') # writable files\n        output_path_2 = os.path.join(self.OUTPUT_DIR, 'test_115b')\n        self.copy_input(plaintext_path, output_path_1) # encrypt\n        self.copy_input(plaintext_path, output_path_2)\n        stdout, stderr = self.run_binary(['seek_tell', input_path, output_path_1, output_path_2])\n        self.verify_seek_tell(stdout, stderr, input_path, output_path_1, output_path_2,\n                              self.FILE_SIZES[-1])\n\n    # overrides TC_00_FileSystem to change input dir (from plaintext to encrypted)\n    def test_130_file_stat(self):\n        # the test binary expects a path to read-only (existing) file and a path to file that\n        # will get created\n        for i in self.INDEXES:\n            input_path = self.ENCRYPTED_FILES[i]\n            output_path = self.OUTPUT_FILES[i]\n            size = str(self.FILE_SIZES[i])\n            self.copy_input(self.INPUT_FILES[i], output_path)\n            stdout, stderr = self.run_binary(['stat', input_path, output_path])\n            self.verify_stat(stdout, stderr, input_path, output_path, size)\n\n    # overrides TC_00_FileSystem to decrypt output\n    def verify_size(self, file, size):\n        dec_path = os.path.join(self.OUTPUT_DIR, os.path.basename(file) + '.dec')\n        self.__decrypt_file(file, dec_path)\n        self.assertEqual(os.stat(dec_path).st_size, size)\n\n    @expectedFailureIf(HAS_SGX)\n    # pylint: disable=fixme\n    def test_140_file_truncate(self):\n        self.fail() # TODO: port these to the new file format\n\n    def test_150_file_rename(self):\n        path1 = os.path.join(self.OUTPUT_DIR, 'test_150a')\n        path2 = os.path.join(self.OUTPUT_DIR, 'test_150b')\n        self.copy_input(self.ENCRYPTED_FILES[-1], path1)\n        shutil.copy(path1, path2)\n        # accessing renamed file should fail\n        args = ['decrypt', '-V', '-w', self.WRAP_KEY, '-i', path2, '-o', path1]\n        try:\n            self.__pf_crypt(args)\n        except subprocess.CalledProcessError as exc:\n            self.assertNotEqual(exc.returncode, 0)\n        else:\n            print('[!] Fail: successfully decrypted renamed file: ' + path2)\n            self.fail()\n\n    # overrides TC_00_FileSystem to decrypt output\n    def verify_copy_content(self, input_path, output_path):\n        dec_path = os.path.join(self.OUTPUT_DIR, os.path.basename(output_path) + '.dec')\n        self.__decrypt_file(output_path, dec_path)\n        self.assertTrue(filecmp.cmp(input_path, dec_path, shallow=False))\n\n    # overrides TC_00_FileSystem to change input dir (from plaintext to encrypted)\n    def do_copy_test(self, executable, timeout):\n        stdout, stderr = self.run_binary([executable, self.ENCRYPTED_DIR, self.OUTPUT_DIR],\n                                         timeout=timeout)\n        self.verify_copy(stdout, stderr, self.ENCRYPTED_DIR, executable)\n\n    # overrides TC_00_FileSystem to not skip this on SGX\n    def test_204_copy_dir_mmap_whole(self):\n        self.do_copy_test('copy_mmap_whole', 30)\n\n    # overrides TC_00_FileSystem to not skip this on SGX\n    def test_205_copy_dir_mmap_seq(self):\n        self.do_copy_test('copy_mmap_seq', 60)\n\n    # overrides TC_00_FileSystem to not skip this on SGX\n    def test_206_copy_dir_mmap_rev(self):\n        self.do_copy_test('copy_mmap_rev', 60)\n\n    # overrides TC_00_FileSystem to change dirs (from plaintext to encrypted)\n    def test_210_copy_dir_mounted(self):\n        executable = 'copy_whole'\n        stdout, stderr = self.run_binary([executable, '\/mounted\/pf_input', '\/mounted\/pf_output'],\n                                         timeout=30)\n        self.verify_copy(stdout, stderr, '\/mounted\/pf_input', executable)\n\n    def __corrupt_file(self, input_path, output_path):\n        cmd = [self.PF_TAMPER, '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path]\n        return self.run_native_binary(cmd)\n\n    # invalid\/corrupted files\n    @expectedFailureIf(HAS_SGX)\n    # pylint: disable=fixme\n    def test_500_invalid(self):\n        # TODO: port these to the new file format\n        invalid_dir = os.path.join(self.TEST_DIR, 'pf_invalid')\n        # files below should work normally (benign modifications)\n        should_pass = ['chunk_padding_1_fixed', 'chunk_padding_2_fixed', 'chunk_data_3',\n                       'chunk_data_3_fixed', 'chunk_data_4', 'chunk_data_4_fixed']\n        if not os.path.exists(invalid_dir):\n            os.mkdir(invalid_dir)\n        # prepare valid encrypted file (largest one for maximum possible corruptions)\n        original_input = self.OUTPUT_FILES[-1]\n        self.__encrypt_file(self.INPUT_FILES[-1], original_input)\n        # generate invalid files based on the above\n        self.__corrupt_file(original_input, invalid_dir)\n        # try to decrypt invalid files\n        for name in os.listdir(invalid_dir):\n            invalid = os.path.join(invalid_dir, name)\n            output_path = os.path.join(self.OUTPUT_DIR, name)\n            input_path = os.path.join(invalid_dir, os.path.basename(original_input))\n            # copy the file so it has the original file name (for allowed path check)\n            shutil.copy(invalid, input_path)\n            should_pass = any(s in name for s in should_pass)\n\n            try:\n                args = ['decrypt', '-V', '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path]\n                self.__pf_crypt(args)\n            except subprocess.CalledProcessError as exc:\n                if should_pass:\n                    self.assertEqual(exc.returncode, 0)\n                else:\n                    self.assertNotEqual(exc.returncode, 0)\n            else:\n                if not should_pass:\n                    print('[!] Fail: successfully decrypted file: ' + name)\n                    self.fail()\n"}},"msg":"[Pal\/Linux-SGX] Test malicious modifications to protected files\n\nThis commit adds a new PF utility `pf_tamper` that tampers with\nvalid protected files and uses this utility to test that the PF\nlogic in Linux-SGX detects such malicious modifications.\n\nThis commit also moves out the PF-format macros and structs from\n`protected_files_internal.h` to `protected_files_format.h` for\nbetter readability.\n\nCo-authored-by: Dmitrii Kuvaiskii <dmitrii.kuvaiskii@intel.com>"}},"https:\/\/github.com\/deb-intel\/GramineTest":{"567790aec46e0714171121318783b8ac8c56adc3":{"url":"https:\/\/api.github.com\/repos\/deb-intel\/GramineTest\/commits\/567790aec46e0714171121318783b8ac8c56adc3","html_url":"https:\/\/github.com\/deb-intel\/GramineTest\/commit\/567790aec46e0714171121318783b8ac8c56adc3","message":"[Pal\/Linux-SGX] Test malicious modifications to protected files\n\nThis commit adds a new PF utility `pf_tamper` that tampers with\nvalid protected files and uses this utility to test that the PF\nlogic in Linux-SGX detects such malicious modifications.\n\nThis commit also moves out the PF-format macros and structs from\n`protected_files_internal.h` to `protected_files_format.h` for\nbetter readability.\n\nCo-authored-by: Dmitrii Kuvaiskii <dmitrii.kuvaiskii@intel.com>","sha":"567790aec46e0714171121318783b8ac8c56adc3","keyword":"tampering malicious","diff":"diff --git a\/LibOS\/shim\/test\/fs\/test_pf.py b\/LibOS\/shim\/test\/fs\/test_pf.py\nindex e430e02af..7b1621d7a 100644\n--- a\/LibOS\/shim\/test\/fs\/test_pf.py\n+++ b\/LibOS\/shim\/test\/fs\/test_pf.py\n@@ -193,24 +193,20 @@ def test_210_copy_dir_mounted(self):\n \n     def __corrupt_file(self, input_path, output_path):\n         cmd = [self.PF_TAMPER, '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path]\n-        return self.run_native_binary(cmd)\n+        return self.run_native_binary(cmd, libpath=os.path.join(os.getcwd(), 'lib'))\n \n     # invalid\/corrupted files\n-    @expectedFailureIf(HAS_SGX)\n-    # pylint: disable=fixme\n     def test_500_invalid(self):\n-        # TODO: port these to the new file format\n         invalid_dir = os.path.join(self.TEST_DIR, 'pf_invalid')\n-        # files below should work normally (benign modifications)\n-        should_pass = ['chunk_padding_1_fixed', 'chunk_padding_2_fixed', 'chunk_data_3',\n-                       'chunk_data_3_fixed', 'chunk_data_4', 'chunk_data_4_fixed']\n         if not os.path.exists(invalid_dir):\n             os.mkdir(invalid_dir)\n+\n         # prepare valid encrypted file (largest one for maximum possible corruptions)\n         original_input = self.OUTPUT_FILES[-1]\n         self.__encrypt_file(self.INPUT_FILES[-1], original_input)\n         # generate invalid files based on the above\n         self.__corrupt_file(original_input, invalid_dir)\n+\n         # try to decrypt invalid files\n         for name in os.listdir(invalid_dir):\n             invalid = os.path.join(invalid_dir, name)\n@@ -218,17 +214,13 @@ def test_500_invalid(self):\n             input_path = os.path.join(invalid_dir, os.path.basename(original_input))\n             # copy the file so it has the original file name (for allowed path check)\n             shutil.copy(invalid, input_path)\n-            should_pass = any(s in name for s in should_pass)\n \n             try:\n                 args = ['decrypt', '-V', '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path]\n                 self.__pf_crypt(args)\n             except subprocess.CalledProcessError as exc:\n-                if should_pass:\n-                    self.assertEqual(exc.returncode, 0)\n-                else:\n-                    self.assertNotEqual(exc.returncode, 0)\n+                # decryption of invalid file must fail with -1 (wrapped to 255)\n+                self.assertEqual(exc.returncode, 255)\n             else:\n-                if not should_pass:\n-                    print('[!] Fail: successfully decrypted file: ' + name)\n-                    self.fail()\n+                print('[!] Fail: successfully decrypted file: ' + name)\n+                self.fail()\ndiff --git a\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files.c b\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files.c\nindex 4f0c59ba2..d020a1e24 100644\n--- a\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files.c\n+++ b\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files.c\n@@ -5,6 +5,8 @@\n  *\/\n \n #include \"api.h\"\n+#include \"protected_files.h\"\n+#include \"protected_files_format.h\"\n #include \"protected_files_internal.h\"\n \n #ifndef IN_PAL\n@@ -65,20 +67,6 @@ static pf_random_f          g_cb_random          = NULL;\n static pf_iv_t g_empty_iv = {0};\n static bool g_initialized = false;\n \n-#define METADATA_KEY_NAME \"SGX-PROTECTED-FS-METADATA-KEY\"\n-#define MAX_LABEL_SIZE    64\n-\n-static_assert(sizeof(METADATA_KEY_NAME) <= MAX_LABEL_SIZE, \"label too long\");\n-\n-#pragma pack(push, 1)\n-typedef struct {\n-    uint32_t index;\n-    char label[MAX_LABEL_SIZE]; \/\/ must be NULL terminated\n-    pf_keyid_t nonce;\n-    uint32_t output_len; \/\/ in bits\n-} kdf_input_t;\n-#pragma pack(pop)\n-\n \/\/ The key derivation function follow recommendations from NIST Special Publication 800-108:\n \/\/ Recommendation for Key Derivation Using Pseudorandom Functions\n \/\/ https:\/\/nvlpubs.nist.gov\/nistpubs\/Legacy\/SP\/nistspecialpublication800-108.pdf\ndiff --git a\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files_format.h b\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files_format.h\nnew file mode 100644\nindex 000000000..2394e53ef\n--- \/dev\/null\n+++ b\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files_format.h\n@@ -0,0 +1,139 @@\n+\/* SPDX-License-Identifier: LGPL-3.0-or-later *\/\n+\/* Copyright (C) 2019-2020 Invisible Things Lab\n+ *                         Rafal Wojdyla <omeg@invisiblethingslab.com>\n+ * Copyright (C) 2011-2020 Intel Corporation\n+ *\/\n+\n+#ifndef PROTECTED_FILES_FORMAT_H_\n+#define PROTECTED_FILES_FORMAT_H_\n+\n+#include <limits.h>\n+\n+#include \"assert.h\"\n+#include \"list.h\"\n+#include \"protected_files.h\"\n+\n+#define PF_FILE_ID       0x46505f4850415247 \/* GRAPH_PF *\/\n+#define PF_MAJOR_VERSION 0x01\n+#define PF_MINOR_VERSION 0x00\n+\n+#define METADATA_KEY_NAME \"SGX-PROTECTED-FS-METADATA-KEY\"\n+#define MAX_LABEL_SIZE    64\n+\n+static_assert(sizeof(METADATA_KEY_NAME) <= MAX_LABEL_SIZE, \"label too long\");\n+\n+#pragma pack(push, 1)\n+\n+typedef struct _metadata_plain {\n+    uint64_t   file_id;\n+    uint8_t    major_version;\n+    uint8_t    minor_version;\n+    pf_keyid_t metadata_key_id;\n+    pf_mac_t   metadata_gmac; \/* GCM mac *\/\n+} metadata_plain_t;\n+\n+#define PATH_MAX_SIZE (260 + 512)\n+\n+\/\/ these are all defined as relative to node size, so we can decrease node size in tests\n+\/\/ and have deeper tree\n+#define MD_USER_DATA_SIZE (PF_NODE_SIZE * 3 \/ 4) \/\/ 3072\n+static_assert(MD_USER_DATA_SIZE == 3072, \"bad struct size\");\n+\n+typedef struct _metadata_encrypted {\n+    char     path[PATH_MAX_SIZE];\n+    uint64_t size;\n+    pf_key_t mht_key;\n+    pf_mac_t mht_gmac;\n+    uint8_t  data[MD_USER_DATA_SIZE];\n+} metadata_encrypted_t;\n+\n+typedef uint8_t metadata_encrypted_blob_t[sizeof(metadata_encrypted_t)];\n+\n+#define METADATA_NODE_SIZE PF_NODE_SIZE\n+\n+typedef uint8_t metadata_padding_t[METADATA_NODE_SIZE -\n+                                   (sizeof(metadata_plain_t) + sizeof(metadata_encrypted_blob_t))];\n+\n+typedef struct _metadata_node {\n+    metadata_plain_t          plain_part;\n+    metadata_encrypted_blob_t encrypted_part;\n+    metadata_padding_t        padding;\n+} metadata_node_t;\n+\n+static_assert(sizeof(metadata_node_t) == PF_NODE_SIZE, \"sizeof(metadata_node_t)\");\n+\n+typedef struct _data_node_crypto {\n+    pf_key_t key;\n+    pf_mac_t gmac;\n+} gcm_crypto_data_t;\n+\n+\/\/ for PF_NODE_SIZE == 4096, we have 96 attached data nodes and 32 mht child nodes\n+\/\/ for PF_NODE_SIZE == 2048, we have 48 attached data nodes and 16 mht child nodes\n+\/\/ for PF_NODE_SIZE == 1024, we have 24 attached data nodes and 8 mht child nodes\n+\/\/ 3\/4 of the node size is dedicated to data nodes\n+#define ATTACHED_DATA_NODES_COUNT ((PF_NODE_SIZE \/ sizeof(gcm_crypto_data_t)) * 3 \/ 4)\n+static_assert(ATTACHED_DATA_NODES_COUNT == 96, \"ATTACHED_DATA_NODES_COUNT\");\n+\/\/ 1\/4 of the node size is dedicated to child mht nodes\n+#define CHILD_MHT_NODES_COUNT ((PF_NODE_SIZE \/ sizeof(gcm_crypto_data_t)) * 1 \/ 4)\n+static_assert(CHILD_MHT_NODES_COUNT == 32, \"CHILD_MHT_NODES_COUNT\");\n+\n+typedef struct _mht_node {\n+    gcm_crypto_data_t data_nodes_crypto[ATTACHED_DATA_NODES_COUNT];\n+    gcm_crypto_data_t mht_nodes_crypto[CHILD_MHT_NODES_COUNT];\n+} mht_node_t;\n+\n+static_assert(sizeof(mht_node_t) == PF_NODE_SIZE, \"sizeof(mht_node_t)\");\n+\n+typedef struct _data_node {\n+    uint8_t data[PF_NODE_SIZE];\n+} data_node_t;\n+\n+static_assert(sizeof(data_node_t) == PF_NODE_SIZE, \"sizeof(data_node_t)\");\n+\n+typedef struct _encrypted_node {\n+    uint8_t cipher[PF_NODE_SIZE];\n+} encrypted_node_t;\n+\n+static_assert(sizeof(encrypted_node_t) == PF_NODE_SIZE, \"sizeof(encrypted_node_t)\");\n+\n+#define MAX_PAGES_IN_CACHE 48\n+\n+typedef enum {\n+    FILE_MHT_NODE_TYPE  = 1,\n+    FILE_DATA_NODE_TYPE = 2,\n+} mht_node_type_e;\n+\n+\/\/ make sure these are the same size\n+static_assert(sizeof(mht_node_t) == sizeof(data_node_t),\n+              \"sizeof(mht_node_t) == sizeof(data_node_t)\");\n+\n+DEFINE_LIST(_file_node);\n+typedef struct _file_node {\n+    LIST_TYPE(_file_node) list;\n+    uint8_t type;\n+    uint64_t node_number;\n+    struct _file_node* parent;\n+    bool need_writing;\n+    bool new_node;\n+    struct {\n+        uint64_t physical_node_number;\n+        encrypted_node_t encrypted; \/\/ the actual data from the disk\n+    };\n+    union { \/\/ decrypted data\n+        mht_node_t mht;\n+        data_node_t data;\n+    } decrypted;\n+} file_node_t;\n+DEFINE_LISTP(_file_node);\n+\n+typedef struct {\n+    uint32_t index;\n+    char label[MAX_LABEL_SIZE]; \/\/ must be NULL terminated\n+    pf_keyid_t nonce;\n+    uint32_t output_len; \/\/ in bits\n+} kdf_input_t;\n+\n+#pragma pack(pop)\n+\n+#endif \/* PROTECTED_FILES_FORMAT_H_ *\/\n+\ndiff --git a\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files_internal.h b\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files_internal.h\nindex 4a31261d5..c06d1ecf6 100644\n--- a\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files_internal.h\n+++ b\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files_internal.h\n@@ -13,116 +13,7 @@\n #include \"list.h\"\n #include \"lru_cache.h\"\n #include \"protected_files.h\"\n-\n-#define PF_FILE_ID       0x46505f4850415247 \/* GRAPH_PF *\/\n-#define PF_MAJOR_VERSION 0x01\n-#define PF_MINOR_VERSION 0x00\n-\n-#pragma pack(push, 1)\n-\n-typedef struct _metadata_plain {\n-    uint64_t   file_id;\n-    uint8_t    major_version;\n-    uint8_t    minor_version;\n-    pf_keyid_t metadata_key_id;\n-    pf_mac_t   metadata_gmac; \/* GCM mac *\/\n-} metadata_plain_t;\n-\n-#define PATH_MAX_SIZE (260 + 512)\n-\n-\/\/ these are all defined as relative to node size, so we can decrease node size in tests\n-\/\/ and have deeper tree\n-#define MD_USER_DATA_SIZE (PF_NODE_SIZE * 3 \/ 4) \/\/ 3072\n-static_assert(MD_USER_DATA_SIZE == 3072, \"bad struct size\");\n-\n-typedef struct _metadata_encrypted {\n-    char     path[PATH_MAX_SIZE];\n-    uint64_t size;\n-    pf_key_t mht_key;\n-    pf_mac_t mht_gmac;\n-    uint8_t  data[MD_USER_DATA_SIZE];\n-} metadata_encrypted_t;\n-\n-typedef uint8_t metadata_encrypted_blob_t[sizeof(metadata_encrypted_t)];\n-\n-#define METADATA_NODE_SIZE PF_NODE_SIZE\n-\n-typedef uint8_t metadata_padding_t[METADATA_NODE_SIZE -\n-                                   (sizeof(metadata_plain_t) + sizeof(metadata_encrypted_blob_t))];\n-\n-typedef struct _metadata_node {\n-    metadata_plain_t          plain_part;\n-    metadata_encrypted_blob_t encrypted_part;\n-    metadata_padding_t        padding;\n-} metadata_node_t;\n-\n-static_assert(sizeof(metadata_node_t) == PF_NODE_SIZE, \"sizeof(metadata_node_t)\");\n-\n-typedef struct _data_node_crypto {\n-    pf_key_t key;\n-    pf_mac_t gmac;\n-} gcm_crypto_data_t;\n-\n-\/\/ for PF_NODE_SIZE == 4096, we have 96 attached data nodes and 32 mht child nodes\n-\/\/ for PF_NODE_SIZE == 2048, we have 48 attached data nodes and 16 mht child nodes\n-\/\/ for PF_NODE_SIZE == 1024, we have 24 attached data nodes and 8 mht child nodes\n-\/\/ 3\/4 of the node size is dedicated to data nodes\n-#define ATTACHED_DATA_NODES_COUNT ((PF_NODE_SIZE \/ sizeof(gcm_crypto_data_t)) * 3 \/ 4)\n-static_assert(ATTACHED_DATA_NODES_COUNT == 96, \"ATTACHED_DATA_NODES_COUNT\");\n-\/\/ 1\/4 of the node size is dedicated to child mht nodes\n-#define CHILD_MHT_NODES_COUNT ((PF_NODE_SIZE \/ sizeof(gcm_crypto_data_t)) * 1 \/ 4)\n-static_assert(CHILD_MHT_NODES_COUNT == 32, \"CHILD_MHT_NODES_COUNT\");\n-\n-typedef struct _mht_node {\n-    gcm_crypto_data_t data_nodes_crypto[ATTACHED_DATA_NODES_COUNT];\n-    gcm_crypto_data_t mht_nodes_crypto[CHILD_MHT_NODES_COUNT];\n-} mht_node_t;\n-\n-static_assert(sizeof(mht_node_t) == PF_NODE_SIZE, \"sizeof(mht_node_t)\");\n-\n-typedef struct _data_node {\n-    uint8_t data[PF_NODE_SIZE];\n-} data_node_t;\n-\n-static_assert(sizeof(data_node_t) == PF_NODE_SIZE, \"sizeof(data_node_t)\");\n-\n-typedef struct _encrypted_node {\n-    uint8_t cipher[PF_NODE_SIZE];\n-} encrypted_node_t;\n-\n-static_assert(sizeof(encrypted_node_t) == PF_NODE_SIZE, \"sizeof(encrypted_node_t)\");\n-\n-#define MAX_PAGES_IN_CACHE 48\n-\n-typedef enum {\n-    FILE_MHT_NODE_TYPE  = 1,\n-    FILE_DATA_NODE_TYPE = 2,\n-} mht_node_type_e;\n-\n-\/\/ make sure these are the same size\n-static_assert(sizeof(mht_node_t) == sizeof(data_node_t),\n-              \"sizeof(mht_node_t) == sizeof(data_node_t)\");\n-\n-DEFINE_LIST(_file_node);\n-typedef struct _file_node {\n-    LIST_TYPE(_file_node) list;\n-    uint8_t type;\n-    uint64_t node_number;\n-    struct _file_node* parent;\n-    bool need_writing;\n-    bool new_node;\n-    struct {\n-        uint64_t physical_node_number;\n-        encrypted_node_t encrypted; \/\/ the actual data from the disk\n-    };\n-    union { \/\/ decrypted data\n-        mht_node_t mht;\n-        data_node_t data;\n-    } decrypted;\n-} file_node_t;\n-DEFINE_LISTP(_file_node);\n-\n-#pragma pack(pop)\n+#include \"protected_files_format.h\"\n \n struct pf_context {\n     metadata_node_t file_metadata; \/\/ actual data from disk's meta data node\ndiff --git a\/Pal\/src\/host\/Linux-SGX\/tools\/Makefile b\/Pal\/src\/host\/Linux-SGX\/tools\/Makefile\nindex a5903ad97..87bc78124 100644\n--- a\/Pal\/src\/host\/Linux-SGX\/tools\/Makefile\n+++ b\/Pal\/src\/host\/Linux-SGX\/tools\/Makefile\n@@ -12,3 +12,4 @@ $(targets):\n \t$(MAKE) -C verify-ias-report $@\n \t$(MAKE) -C ra-tls $@\n \t$(MAKE) -C pf_crypt $@\n+\t$(MAKE) -C pf_tamper $@\ndiff --git a\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/.gitignore b\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/.gitignore\nnew file mode 100644\nindex 000000000..a52536d82\n--- \/dev\/null\n+++ b\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/.gitignore\n@@ -0,0 +1 @@\n+\/pf_tamper\ndiff --git a\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/Makefile b\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/Makefile\nnew file mode 100644\nindex 000000000..94c826e4b\n--- \/dev\/null\n+++ b\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/Makefile\n@@ -0,0 +1,31 @@\n+include ..\/..\/..\/..\/..\/..\/Scripts\/Makefile.configs\n+include ..\/..\/..\/..\/..\/..\/Scripts\/Makefile.rules\n+\n+CFLAGS += -I..\/.. \\\n+          -I..\/common \\\n+          -I..\/..\/protected-files \\\n+          -I..\/..\/..\/..\/..\/include\/lib \\\n+          -D_GNU_SOURCE\n+\n+LDLIBS += -L..\/common \\\n+          -L..\/..\/..\/..\/..\/lib\/crypto\/mbedtls\/install\/lib \\\n+          -lsgx_util -lmbedcrypto\n+\n+PREFIX ?= \/usr\/local\n+\n+pf_tamper: pf_tamper.o\n+\t$(call cmd,csingle)\n+\n+.PHONY: all\n+all: pf_tamper\n+\n+.PHONY: install\n+install:\n+\tinstall -D pf_tamper -t ${PREFIX}\/bin\n+\n+.PHONY: clean\n+clean:\n+\t$(RM) *.o pf_tamper\n+\n+.PHONY: distclean\n+distclean: clean\ndiff --git a\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/pf_tamper.c b\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/pf_tamper.c\nnew file mode 100644\nindex 000000000..4d39a0d58\n--- \/dev\/null\n+++ b\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/pf_tamper.c\n@@ -0,0 +1,476 @@\n+\/* SPDX-License-Identifier: LGPL-3.0-or-later *\/\n+\/* Copyright (C) 2019-2020 Invisible Things Lab\n+ *                         Rafal Wojdyla <omeg@invisiblethingslab.com>\n+ *\/\n+\n+#include <fcntl.h>\n+#include <getopt.h>\n+#include <stdlib.h>\n+#include <sys\/mman.h>\n+#include <unistd.h>\n+\n+#include \"pf_util.h\"\n+#include \"protected_files.h\"\n+#include \"protected_files_format.h\"\n+#include \"util.h\"\n+\n+\/* Tamper with a PF in various ways for testing purposes. The PF is assumed to be valid and have at\n+ * least enough data to contain two MHT nodes. *\/\n+\n+\/* Command line options *\/\n+struct option g_options[] = {\n+    { \"input\", required_argument, 0, 'i' },\n+    { \"output\", required_argument, 0, 'o' },\n+    { \"wrap-key\", required_argument, 0, 'w' },\n+    { \"verbose\", no_argument, 0, 'v' },\n+    { \"help\", no_argument, 0, 'h' },\n+    { 0, 0, 0, 0 }\n+};\n+\n+static void usage(void) {\n+    INFO(\"\\nUsage: pf_tamper [options]\\n\");\n+    INFO(\"\\nAvailable options:\\n\");\n+    INFO(\"  --help, -h           Display this help\\n\");\n+    INFO(\"  --verbose, -v        Enable verbose output\\n\");\n+    INFO(\"  --wrap-key, -w PATH  Path to wrap key file\\n\");\n+    INFO(\"  --input, -i PATH     Source file to be tampered with (must be a valid PF)\\n\");\n+    INFO(\"  --output, -o PATH    Directory where modified files will be written to\\n\");\n+}\n+\n+#define FATAL(fmt, ...) do { \\\n+    ERROR(fmt, ##__VA_ARGS__); \\\n+    exit(-1); \\\n+} while (0)\n+\n+ssize_t g_input_size = 0;\n+char* g_input_name = NULL;\n+void* g_input_data = MAP_FAILED;\n+char* g_output_dir = NULL;\n+char* g_output_path = NULL;\n+size_t g_output_path_size = 0;\n+pf_key_t g_wrap_key;\n+pf_key_t g_meta_key;\n+\n+static pf_iv_t g_empty_iv = {0};\n+\n+static void derive_main_key(const pf_key_t* kdk, const pf_keyid_t* key_id, pf_key_t* out_key) {\n+    kdf_input_t buf = {0};\n+    pf_status_t status;\n+\n+    buf.index = 1;\n+    strncpy(buf.label, METADATA_KEY_NAME, MAX_LABEL_SIZE);\n+    memcpy(&buf.nonce, key_id, sizeof(buf.nonce));\n+    buf.output_len = 0x80;\n+\n+    status = mbedtls_aes_gcm_encrypt(kdk, &g_empty_iv, &buf, sizeof(buf), NULL, 0, NULL, out_key);\n+    if (PF_FAILURE(status))\n+        FATAL(\"key derivation failed\\n\");\n+}\n+\n+static void make_output_path(const char* suffix) {\n+    snprintf(g_output_path, g_output_path_size, \"%s\/%s.%s\", g_output_dir, g_input_name, suffix);\n+    INFO(\"[*] %s\\n\", g_output_path);\n+}\n+\n+\/* PF layout (node size is PF_NODE_SIZE):\n+ * - Node 0: metadata (metadata_node_t)\n+ *   - metadata_plain_t\n+ *   - metadata_encrypted_t (may include MD_USER_DATA_SIZE bytes of data)\n+ *   - metadata_padding_t\n+ * - Node 1: MHT (mht_node_t)\n+ * - Node 2-97: data (ATTACHED_DATA_NODES_COUNT == 96)\n+ * - Node 98: MHT\n+ * - Node 99-195: data\n+ * - ...\n+ *\/\n+static void truncate_file(const char* suffix, size_t output_size) {\n+    int ret;\n+\n+    make_output_path(suffix);\n+\n+    if (output_size < g_input_size) {\n+        ret = write_file(g_output_path, output_size, g_input_data);\n+    } else {\n+        ret = write_file(g_output_path, g_input_size, g_input_data);\n+        if (ret < 0)\n+            goto out;\n+        ret = truncate(g_output_path, output_size);\n+    }\n+out:\n+    if (ret < 0)\n+        FATAL(\"truncate_file failed: %d\\n\", ret);\n+}\n+\n+#define FIELD_SIZEOF(t, f) (sizeof(((t*)0)->f))\n+#define FIELD_TRUNCATED(t, f) (offsetof(t, f) + (FIELD_SIZEOF(t, f) \/ 2))\n+#define DATA_CRYPTO_SIZE (FIELD_SIZEOF(mht_node_t, data_nodes_crypto))\n+\n+static void tamper_truncate(void) {\n+    size_t mdps = sizeof(metadata_plain_t);\n+    DBG(\"size(metadata_plain_t)             = 0x%04lx\\n\", sizeof(metadata_plain_t));\n+    DBG(\"metadata_plain_t.file_id           : 0x%04lx (0x%04lx)\\n\",\n+        offsetof(metadata_plain_t, file_id), FIELD_SIZEOF(metadata_plain_t, file_id));\n+    DBG(\"metadata_plain_t.major_version     : 0x%04lx (0x%04lx)\\n\",\n+        offsetof(metadata_plain_t, major_version), FIELD_SIZEOF(metadata_plain_t, major_version));\n+    DBG(\"metadata_plain_t.minor_version     : 0x%04lx (0x%04lx)\\n\",\n+        offsetof(metadata_plain_t, minor_version), FIELD_SIZEOF(metadata_plain_t, minor_version));\n+    DBG(\"metadata_plain_t.metadata_key_id   : 0x%04lx (0x%04lx)\\n\",\n+        offsetof(metadata_plain_t, metadata_key_id),\n+        FIELD_SIZEOF(metadata_plain_t, metadata_key_id));\n+    DBG(\"metadata_plain_t.metadata_gmac     : 0x%04lx (0x%04lx)\\n\",\n+        offsetof(metadata_plain_t, metadata_gmac),\n+        FIELD_SIZEOF(metadata_plain_t, metadata_gmac));\n+\n+    DBG(\"size(metadata_encrypted_t)         = 0x%04lx\\n\", sizeof(metadata_encrypted_t));\n+    DBG(\"metadata_encrypted_t.path          : 0x%04lx (0x%04lx)\\n\",\n+        mdps + offsetof(metadata_encrypted_t, path),\n+        FIELD_SIZEOF(metadata_encrypted_t, path));\n+    DBG(\"metadata_encrypted_t.size          : 0x%04lx (0x%04lx)\\n\",\n+        mdps + offsetof(metadata_encrypted_t, size), FIELD_SIZEOF(metadata_encrypted_t, size));\n+    DBG(\"metadata_encrypted_t.mht_key       : 0x%04lx (0x%04lx)\\n\",\n+        mdps + offsetof(metadata_encrypted_t, mht_key),\n+        FIELD_SIZEOF(metadata_encrypted_t, mht_key));\n+    DBG(\"metadata_encrypted_t.mht_gmac      : 0x%04lx (0x%04lx)\\n\",\n+        mdps + offsetof(metadata_encrypted_t, mht_gmac),\n+        FIELD_SIZEOF(metadata_encrypted_t, mht_gmac));\n+    DBG(\"metadata_encrypted_t.data          : 0x%04lx (0x%04lx)\\n\",\n+        mdps + offsetof(metadata_encrypted_t, data), FIELD_SIZEOF(metadata_encrypted_t, data));\n+\n+    DBG(\"size(metadata_padding_t)           = 0x%04lx\\n\", sizeof(metadata_padding_t));\n+    DBG(\"metadata_padding_t                 : 0x%04lx (0x%04lx)\\n\",\n+        mdps + sizeof(metadata_encrypted_t), sizeof(metadata_padding_t));\n+\n+    \/* node 0: metadata + 3k of user data *\/\n+    \/* plain metadata *\/\n+    truncate_file(\"trunc_meta_plain_0\", 0);\n+    truncate_file(\"trunc_meta_plain_1\", FIELD_TRUNCATED(metadata_plain_t, file_id));\n+    truncate_file(\"trunc_meta_plain_2\", offsetof(metadata_plain_t, major_version));\n+    truncate_file(\"trunc_meta_plain_3\", offsetof(metadata_plain_t, minor_version));\n+    truncate_file(\"trunc_meta_plain_4\", offsetof(metadata_plain_t, metadata_key_id));\n+    truncate_file(\"trunc_meta_plain_5\", FIELD_TRUNCATED(metadata_plain_t, metadata_key_id));\n+    truncate_file(\"trunc_meta_plain_6\", offsetof(metadata_plain_t, metadata_gmac));\n+    truncate_file(\"trunc_meta_plain_7\", FIELD_TRUNCATED(metadata_plain_t, metadata_gmac));\n+\n+    \/* encrypted metadata *\/\n+    truncate_file(\"trunc_meta_enc_0\", mdps + offsetof(metadata_encrypted_t, path));\n+    truncate_file(\"trunc_meta_enc_1\", mdps + FIELD_TRUNCATED(metadata_encrypted_t, path));\n+    truncate_file(\"trunc_meta_enc_2\", mdps + offsetof(metadata_encrypted_t, size));\n+    truncate_file(\"trunc_meta_enc_3\", mdps + FIELD_TRUNCATED(metadata_encrypted_t, size));\n+    truncate_file(\"trunc_meta_enc_4\", mdps + offsetof(metadata_encrypted_t, mht_key));\n+    truncate_file(\"trunc_meta_enc_5\", mdps + FIELD_TRUNCATED(metadata_encrypted_t, mht_key));\n+    truncate_file(\"trunc_meta_enc_6\", mdps + offsetof(metadata_encrypted_t, mht_gmac));\n+    truncate_file(\"trunc_meta_enc_7\", mdps + FIELD_TRUNCATED(metadata_encrypted_t, mht_gmac));\n+    truncate_file(\"trunc_meta_enc_8\", mdps + offsetof(metadata_encrypted_t, data));\n+    truncate_file(\"trunc_meta_enc_9\", mdps + FIELD_TRUNCATED(metadata_encrypted_t, data));\n+\n+    \/* padding *\/\n+    truncate_file(\"trunc_meta_pad_0\", mdps + sizeof(metadata_encrypted_t));\n+    truncate_file(\"trunc_meta_pad_1\", mdps + sizeof(metadata_encrypted_t)\n+                  + sizeof(metadata_padding_t) \/ 2);\n+\n+    \/* node 1: mht root *\/\n+    \/* after node 0 *\/\n+    truncate_file(\"trunc_mht_0\", PF_NODE_SIZE);\n+    \/* middle of data_nodes_crypto[0].key *\/\n+    truncate_file(\"trunc_mht_1\", PF_NODE_SIZE + PF_KEY_SIZE \/ 2);\n+    \/* after data_nodes_crypto[0].key *\/\n+    truncate_file(\"trunc_mht_2\", PF_NODE_SIZE + PF_KEY_SIZE);\n+    \/* middle of data_nodes_crypto[0].gmac *\/\n+    truncate_file(\"trunc_mht_3\", PF_NODE_SIZE + PF_KEY_SIZE + PF_MAC_SIZE \/ 2);\n+    \/* after data_nodes_crypto[0].gmac *\/\n+    truncate_file(\"trunc_mht_4\", PF_NODE_SIZE + PF_KEY_SIZE + PF_MAC_SIZE);\n+    \/* after data_nodes_crypto *\/\n+    truncate_file(\"trunc_mht_5\", PF_NODE_SIZE + DATA_CRYPTO_SIZE);\n+    \/* middle of mht_nodes_crypto[0].key *\/\n+    truncate_file(\"trunc_mht_6\", PF_NODE_SIZE + DATA_CRYPTO_SIZE + PF_KEY_SIZE \/ 2);\n+    \/* after mht_nodes_crypto[0].key *\/\n+    truncate_file(\"trunc_mht_7\", PF_NODE_SIZE + DATA_CRYPTO_SIZE + PF_KEY_SIZE);\n+    \/* middle of mht_nodes_crypto[0].gmac *\/\n+    truncate_file(\"trunc_mht_8\", PF_NODE_SIZE + DATA_CRYPTO_SIZE + PF_KEY_SIZE + PF_MAC_SIZE \/ 2);\n+    \/* after mht_nodes_crypto[0].gmac *\/\n+    truncate_file(\"trunc_mht_9\", PF_NODE_SIZE + DATA_CRYPTO_SIZE + PF_KEY_SIZE + PF_MAC_SIZE);\n+\n+    \/* node 2-3: data #0, #1 *\/\n+    \/* after mht root *\/\n+    truncate_file(\"trunc_data_0\", 2 * PF_NODE_SIZE);\n+    \/* middle of data #0 *\/\n+    truncate_file(\"trunc_data_1\", 2 * PF_NODE_SIZE + PF_NODE_SIZE \/ 2);\n+    \/* after data #0 *\/\n+    truncate_file(\"trunc_data_2\", 3 * PF_NODE_SIZE);\n+    \/* middle of data #1 *\/\n+    truncate_file(\"trunc_data_3\", 3 * PF_NODE_SIZE + PF_NODE_SIZE \/ 2);\n+\n+    \/* extend *\/\n+    truncate_file(\"extend_0\", g_input_size + 1);\n+    truncate_file(\"extend_1\", g_input_size + PF_NODE_SIZE \/ 2);\n+    truncate_file(\"extend_2\", g_input_size + PF_NODE_SIZE);\n+    truncate_file(\"extend_3\", g_input_size + PF_NODE_SIZE + PF_NODE_SIZE \/ 2);\n+}\n+\n+\/* returns mmap'd output contents *\/\n+static void* create_output(const char* path) {\n+    void* mem = MAP_FAILED;\n+    int fd = open(path, O_RDWR|O_CREAT, 0664);\n+    if (fd < 0)\n+        FATAL(\"Failed to open output file '%s': %s\\n\", path, strerror(errno));\n+\n+    if (ftruncate(fd, g_input_size) < 0)\n+        FATAL(\"Failed to ftruncate output file '%s': %s\\n\", path, strerror(errno));\n+\n+    mem = mmap(NULL, g_input_size, PROT_READ|PROT_WRITE, MAP_SHARED, fd, 0);\n+    if (mem == MAP_FAILED)\n+        FATAL(\"Failed to mmap output file '%s': %s\\n\", path, strerror(errno));\n+\n+    memcpy(mem, g_input_data, g_input_size);\n+\n+    close(fd);\n+    return mem;\n+}\n+\n+static void pf_decrypt(const void* encrypted, size_t size, const pf_key_t* key, const pf_mac_t* mac,\n+                       void* decrypted, const char* msg) {\n+    pf_status_t status = mbedtls_aes_gcm_decrypt(key, &g_empty_iv, NULL, 0,\n+                                                 encrypted, size,\n+                                                 decrypted, mac);\n+    if (PF_FAILURE(status))\n+        FATAL(\"decrypting %s failed\\n\", msg);\n+}\n+\n+static void pf_encrypt(const void* decrypted, size_t size, const pf_key_t* key, pf_mac_t* mac,\n+                       void* encrypted, const char* msg) {\n+    pf_status_t status = mbedtls_aes_gcm_encrypt(key, &g_empty_iv, NULL, 0,\n+                                                 decrypted, size,\n+                                                 encrypted, mac);\n+    if (PF_FAILURE(status))\n+        FATAL(\"encrypting %s failed\\n\", msg);\n+}\n+\n+\/* copy input PF and apply some modifications *\/\n+#define __BREAK_PF(suffix, ...) do { \\\n+    make_output_path(suffix); \\\n+    meta = create_output(g_output_path); \\\n+    out = (uint8_t*)meta; \\\n+    pf_decrypt(&meta->encrypted_part, sizeof(meta->encrypted_part), &g_meta_key, \\\n+               &meta->plain_part.metadata_gmac, meta_dec, \"metadata\"); \\\n+    mht_enc = (mht_node_t*)(out + PF_NODE_SIZE); \\\n+    pf_decrypt(mht_enc, sizeof(*mht_enc), &meta_dec->mht_key, &meta_dec->mht_gmac, mht_dec, \\\n+               \"mht\"); \\\n+    __VA_ARGS__ \\\n+    munmap(meta, g_input_size); \\\n+} while (0)\n+\n+\/* if update is true, also create a file with correct metadata MAC *\/\n+#define BREAK_PF(suffix, update, ...) do { \\\n+    __BREAK_PF(suffix, __VA_ARGS__); \\\n+    if (update) { \\\n+        __BREAK_PF(suffix \"_fixed\", __VA_ARGS__ { \\\n+                       pf_encrypt(meta_dec, sizeof(*meta_dec), &g_meta_key, \\\n+                                  &meta->plain_part.metadata_gmac, meta->encrypted_part, \\\n+                                  \"metadata\"); \\\n+                   } ); \\\n+    } \\\n+} while (0)\n+\n+#define BREAK_MHT(suffix, ...) do { \\\n+    __BREAK_PF(suffix, __VA_ARGS__ { \\\n+                   pf_encrypt(mht_dec, sizeof(*mht_dec), &meta_dec->mht_key, &meta_dec->mht_gmac, \\\n+                              mht_enc, \"mht\"); \\\n+               } ); \\\n+} while (0)\n+\n+#define LAST_BYTE(array) (((uint8_t*)&array)[sizeof(array) - 1])\n+\n+static void tamper_modify(void) {\n+    metadata_node_t* meta = NULL;\n+    uint8_t* out = NULL;\n+    metadata_encrypted_t* meta_dec = malloc(sizeof(*meta_dec));\n+    if (!meta_dec)\n+        FATAL(\"Out of memory\\n\");\n+    mht_node_t* mht_enc = NULL;\n+    mht_node_t* mht_dec = malloc(sizeof(*mht_dec));\n+    if (!mht_dec)\n+        FATAL(\"Out of memory\\n\");\n+\n+    \/* plain part of the metadata isn't covered by the MAC so no point updating it *\/\n+    BREAK_PF(\"meta_plain_id_0\", \/*update=*\/false,\n+             { meta->plain_part.file_id = 0; });\n+    BREAK_PF(\"meta_plain_id_1\", \/*update=*\/false,\n+             { meta->plain_part.file_id = UINT64_MAX; });\n+    BREAK_PF(\"meta_plain_version_0\", \/*update=*\/false,\n+             { meta->plain_part.major_version = 0; });\n+    BREAK_PF(\"meta_plain_version_1\", \/*update=*\/false,\n+             { meta->plain_part.major_version = 0xff; });\n+    BREAK_PF(\"meta_plain_version_2\", \/*update=*\/false,\n+             { meta->plain_part.minor_version = 0xff; });\n+\n+    \/* metadata_key_id is the keying material for encrypted metadata key derivation, so create also\n+     * PFs with updated MACs *\/\n+    BREAK_PF(\"meta_plain_keyid_0\", \/*update=*\/true,\n+             { meta->plain_part.metadata_key_id[0] ^= 1; });\n+    BREAK_PF(\"meta_plain_keyid_1\", \/*update=*\/true,\n+             { LAST_BYTE(meta->plain_part.metadata_key_id) ^= 0xfe; });\n+    BREAK_PF(\"meta_plain_mac_0\", \/*update=*\/true,\n+             { meta->plain_part.metadata_gmac[0] ^= 0xfe; });\n+    BREAK_PF(\"meta_plain_mac_1\", \/*update=*\/true,\n+             { LAST_BYTE(meta->plain_part.metadata_gmac) &= 1; });\n+\n+    BREAK_PF(\"meta_enc_filename_0\", \/*update=*\/true,\n+             { meta_dec->path[0] = 0; });\n+    BREAK_PF(\"meta_enc_filename_1\", \/*update=*\/true,\n+             { meta_dec->path[0] ^= 1; });\n+    BREAK_PF(\"meta_enc_filename_2\", \/*update=*\/true,\n+             { LAST_BYTE(meta_dec->path) ^= 0xfe; });\n+    BREAK_PF(\"meta_enc_size_0\", \/*update=*\/true,\n+             { meta_dec->size = 0; });\n+    BREAK_PF(\"meta_enc_size_1\", \/*update=*\/true,\n+             { meta_dec->size = g_input_size - 1; });\n+    BREAK_PF(\"meta_enc_size_2\", \/*update=*\/true,\n+             { meta_dec->size = g_input_size + 1; });\n+    BREAK_PF(\"meta_enc_size_3\", \/*update=*\/true,\n+             { meta_dec->size = UINT64_MAX; });\n+    BREAK_PF(\"meta_enc_mht_key_0\", \/*update=*\/true,\n+             { meta_dec->mht_key[0] ^= 1; });\n+    BREAK_PF(\"meta_enc_mht_key_1\", \/*update=*\/true,\n+             { LAST_BYTE(meta_dec->mht_key) ^= 0xfe; });\n+    BREAK_PF(\"meta_enc_mht_mac_0\", \/*update=*\/true,\n+             { meta_dec->mht_gmac[0] ^= 1; });\n+    BREAK_PF(\"meta_enc_mht_mac_1\", \/*update=*\/true,\n+             { LAST_BYTE(meta_dec->mht_gmac) ^= 0xfe; });\n+    BREAK_PF(\"meta_enc_data_0\", \/*update=*\/true,\n+             { meta_dec->data[0] ^= 0xfe; });\n+    BREAK_PF(\"meta_enc_data_1\", \/*update=*\/true,\n+             { LAST_BYTE(meta_dec->data) ^= 1; });\n+\n+    \/* padding is ignored *\/\n+    BREAK_PF(\"meta_padding_0\", \/*update=*\/false,\n+             { meta->padding[0] ^= 1; });\n+    BREAK_PF(\"meta_padding_1\", \/*update=*\/false,\n+             { LAST_BYTE(meta->padding) ^= 0xfe; });\n+\n+    BREAK_MHT(\"mht_0\", { mht_dec->data_nodes_crypto[0].key[0] ^= 1; });\n+    BREAK_MHT(\"mht_1\", { mht_dec->data_nodes_crypto[0].gmac[0] ^= 1; });\n+    BREAK_MHT(\"mht_2\", { mht_dec->mht_nodes_crypto[0].key[0] ^= 1; });\n+    BREAK_MHT(\"mht_3\", { mht_dec->mht_nodes_crypto[0].gmac[0] ^= 1; });\n+    BREAK_MHT(\"mht_4\", { mht_dec->data_nodes_crypto[ATTACHED_DATA_NODES_COUNT - 1].key[0] ^= 1; });\n+    BREAK_MHT(\"mht_5\", { mht_dec->data_nodes_crypto[ATTACHED_DATA_NODES_COUNT - 1].gmac[0] ^= 1; });\n+    BREAK_MHT(\"mht_6\", { mht_dec->mht_nodes_crypto[CHILD_MHT_NODES_COUNT - 1].key[0] ^= 1; });\n+    BREAK_MHT(\"mht_7\", { mht_dec->mht_nodes_crypto[CHILD_MHT_NODES_COUNT - 1].gmac[0] ^= 1; });\n+    BREAK_MHT(\"mht_8\", {\n+        gcm_crypto_data_t crypto;\n+        memcpy(&crypto, &mht_dec->data_nodes_crypto[0], sizeof(crypto));\n+        memcpy(&mht_dec->data_nodes_crypto[0], &mht_dec->data_nodes_crypto[1], sizeof(crypto));\n+        memcpy(&mht_dec->data_nodes_crypto[1], &crypto, sizeof(crypto));\n+    });\n+    BREAK_MHT(\"mht_9\", {\n+        gcm_crypto_data_t crypto;\n+        memcpy(&crypto, &mht_dec->mht_nodes_crypto[0], sizeof(crypto));\n+        memcpy(&mht_dec->mht_nodes_crypto[0], &mht_dec->mht_nodes_crypto[1], sizeof(crypto));\n+        memcpy(&mht_dec->mht_nodes_crypto[1], &crypto, sizeof(crypto));\n+    });\n+\n+    \/* data nodes start from node #2 *\/\n+    BREAK_PF(\"data_0\", \/*update=*\/false,\n+             { *(out + 2 * PF_NODE_SIZE) ^= 1; });\n+    BREAK_PF(\"data_1\", \/*update=*\/false,\n+             { *(out + 3 * PF_NODE_SIZE - 1) ^= 1; });\n+    BREAK_PF(\"data_2\", \/*update=*\/false, {\n+        \/* swap data nodes *\/\n+        memcpy(out + 2 * PF_NODE_SIZE, g_input_data + 3 * PF_NODE_SIZE, PF_NODE_SIZE);\n+        memcpy(out + 3 * PF_NODE_SIZE, g_input_data + 2 * PF_NODE_SIZE, PF_NODE_SIZE);\n+    });\n+\n+    free(mht_dec);\n+    free(meta_dec);\n+}\n+\n+int main(int argc, char* argv[]) {\n+    int ret = -1;\n+\n+    int option          = 0;\n+    char* input_path    = NULL;\n+    char* wrap_key_path = NULL;\n+    int input_fd        = -1;\n+\n+    while (true) {\n+        option = getopt_long(argc, argv, \"i:o:w:vh\", g_options, NULL);\n+        if (option == -1)\n+            break;\n+\n+        switch (option) {\n+            case 'i':\n+                input_path = optarg;\n+                break;\n+            case 'o':\n+                g_output_dir = optarg;\n+                break;\n+            case 'w':\n+                wrap_key_path = optarg;\n+                break;\n+            case 'v':\n+                set_verbose(true);\n+                break;\n+            case 'h':\n+                usage();\n+                return 0;\n+            default:\n+                ERROR(\"Unknown option: %c\\n\", option);\n+                usage();\n+        }\n+    }\n+\n+    if (!input_path) {\n+        ERROR(\"Input path not specified\\n\");\n+        usage();\n+        goto out;\n+    }\n+\n+    if (!g_output_dir) {\n+        ERROR(\"Output path not specified\\n\");\n+        usage();\n+        goto out;\n+    }\n+\n+    if (!wrap_key_path) {\n+        ERROR(\"Wrap key path not specified\\n\");\n+        usage();\n+        goto out;\n+    }\n+\n+    input_fd = open(input_path, O_RDONLY);\n+    if (input_fd < 0) {\n+        ERROR(\"Failed to open input file '%s': %s\\n\", input_path, strerror(errno));\n+        goto out;\n+    }\n+\n+    g_input_size = get_file_size(input_fd);\n+    if (g_input_size < 0) {\n+        ERROR(\"Failed to stat input file '%s': %s\\n\", input_path, strerror(errno));\n+        goto out;\n+    }\n+\n+    g_input_data = mmap(NULL, g_input_size, PROT_READ, MAP_PRIVATE, input_fd, 0);\n+    if (g_input_data == MAP_FAILED) {\n+        ERROR(\"Failed to mmap input file '%s': %s\\n\", input_path, strerror(errno));\n+        goto out;\n+    }\n+\n+    load_wrap_key(wrap_key_path, &g_wrap_key);\n+    derive_main_key(&g_wrap_key, &((metadata_plain_t*)g_input_data)->metadata_key_id,\n+                    &g_meta_key);\n+\n+    g_input_name = basename(input_path);\n+    g_output_path_size = strlen(g_input_name) + strlen(g_output_dir) + 256;\n+    g_output_path = malloc(g_output_path_size);\n+    if (!g_output_path) {\n+        ERROR(\"No memory\\n\");\n+        goto out;\n+    }\n+\n+    tamper_truncate();\n+    tamper_modify();\n+    ret = 0;\n+\n+out:\n+    \/* skip cleanup as we are in main() *\/\n+    return ret;\n+}\n","files":{"\/LibOS\/shim\/test\/fs\/test_pf.py":{"changes":[{"diff":"\n \n     def __corrupt_file(self, input_path, output_path):\n         cmd = [self.PF_TAMPER, '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path]\n-        return self.run_native_binary(cmd)\n+        return self.run_native_binary(cmd, libpath=os.path.join(os.getcwd(), 'lib'))\n \n     # invalid\/corrupted files\n-    @expectedFailureIf(HAS_SGX)\n-    # pylint: disable=fixme\n     def test_500_invalid(self):\n-        # TODO: port these to the new file format\n         invalid_dir = os.path.join(self.TEST_DIR, 'pf_invalid')\n-        # files below should work normally (benign modifications)\n-        should_pass = ['chunk_padding_1_fixed', 'chunk_padding_2_fixed', 'chunk_data_3',\n-                       'chunk_data_3_fixed', 'chunk_data_4', 'chunk_data_4_fixed']\n         if not os.path.exists(invalid_dir):\n             os.mkdir(invalid_dir)\n+\n         # prepare valid encrypted file (largest one for maximum possible corruptions)\n         original_input = self.OUTPUT_FILES[-1]\n         self.__encrypt_file(self.INPUT_FILES[-1], original_input)\n         # generate invalid files based on the above\n         self.__corrupt_file(original_input, invalid_dir)\n+\n         # try to decrypt invalid files\n         for name in os.listdir(invalid_dir):\n             invalid = os.path.join(invalid_dir, name)\n","add":3,"remove":7,"filename":"\/LibOS\/shim\/test\/fs\/test_pf.py","badparts":["        return self.run_native_binary(cmd)","    @expectedFailureIf(HAS_SGX)","        should_pass = ['chunk_padding_1_fixed', 'chunk_padding_2_fixed', 'chunk_data_3',","                       'chunk_data_3_fixed', 'chunk_data_4', 'chunk_data_4_fixed']"],"goodparts":["        return self.run_native_binary(cmd, libpath=os.path.join(os.getcwd(), 'lib'))"]},{"diff":"\n             input_path = os.path.join(invalid_dir, os.path.basename(original_input))\n             # copy the file so it has the original file name (for allowed path check)\n             shutil.copy(invalid, input_path)\n-            should_pass = any(s in name for s in should_pass)\n \n             try:\n                 args = ['decrypt', '-V', '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path]\n                 self.__pf_crypt(args)\n             except subprocess.CalledProcessError as exc:\n-                if should_pass:\n-                    self.assertEqual(exc.returncode, 0)\n-                else:\n-                    self.assertNotEqual(exc.returncode, 0)\n+                # decryption of invalid file must fail with -1 (wrapped to 255)\n+                self.assertEqual(exc.returncode, 255)\n             else:\n-                if not should_pass:\n-                    print('[!] Fail: successfully decrypted file: ' + name)\n-                    self.fail()\n+                print('[!] Fail: successfully decrypted file: ' + name)\n+                self.fail()","add":4,"remove":8,"filename":"\/LibOS\/shim\/test\/fs\/test_pf.py","badparts":["            should_pass = any(s in name for s in should_pass)","                if should_pass:","                    self.assertEqual(exc.returncode, 0)","                else:","                    self.assertNotEqual(exc.returncode, 0)","                if not should_pass:","                    print('[!] Fail: successfully decrypted file: ' + name)","                    self.fail()"],"goodparts":["                self.assertEqual(exc.returncode, 255)","                print('[!] Fail: successfully decrypted file: ' + name)","                self.fail()"]}],"source":"\n import filecmp import os import shutil import subprocess import unittest from test_fs import( TC_00_FileSystem, ) from regression import( HAS_SGX, expectedFailureIf, ) @unittest.skipUnless(HAS_SGX, 'Protected files require SGX support') class TC_50_ProtectedFiles(TC_00_FileSystem): @classmethod def setUpClass(cls): cls.PF_CRYPT='bin\/pf_crypt' cls.PF_TAMPER='bin\/pf_tamper' cls.WRAP_KEY=os.path.join(cls.TEST_DIR, 'wrap-key') cls.CONST_WRAP_KEY=[0xff, 0xee, 0xdd, 0xcc, 0xbb, 0xaa, 0x99, 0x88, 0x77, 0x66, 0x55, 0x44, 0x33, 0x22, 0x11, 0x00] cls.ENCRYPTED_DIR=os.path.join(cls.TEST_DIR, 'pf_input') cls.ENCRYPTED_FILES=[os.path.join(cls.ENCRYPTED_DIR, str(v)) for v in cls.FILE_SIZES] cls.LIB_PATH=os.path.join(os.getcwd(), 'lib') super().setUpClass() if not os.path.exists(cls.ENCRYPTED_DIR): os.mkdir(cls.ENCRYPTED_DIR) cls.OUTPUT_DIR=os.path.join(cls.TEST_DIR, 'pf_output') cls.OUTPUT_FILES=[os.path.join(cls.OUTPUT_DIR, str(x)) for x in cls.FILE_SIZES] cls.__set_default_key(cls) for i in cls.INDEXES: cmd=[cls.PF_CRYPT, 'encrypt', '-w', cls.WRAP_KEY, '-i', cls.INPUT_FILES[i], '-o', cls.ENCRYPTED_FILES[i]] cls.run_native_binary(cmd, libpath=os.path.join(os.getcwd(), 'lib')) def __pf_crypt(self, args): args.insert(0, self.PF_CRYPT) return self.run_native_binary(args, libpath=os.path.join(os.getcwd(), 'lib')) def __set_default_key(self): with open(self.WRAP_KEY, 'wb') as file: file.write(bytes(self.CONST_WRAP_KEY)) def copy_input(self, input_path, output_path): self.__encrypt_file(input_path, output_path) def __encrypt_file(self, input_path, output_path): args=['encrypt', '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path] stdout, stderr=self.__pf_crypt(args) return(stdout, stderr) def __decrypt_file(self, input_path, output_path): args=['decrypt', '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path] stdout, stderr=self.__pf_crypt(args) return(stdout, stderr) def test_000_gen_key(self): key_path=os.path.join(self.TEST_DIR, 'tmpkey') args=['gen-key', '-w', key_path] stdout, _=self.__pf_crypt(args) self.assertIn('Wrap key saved to: ' +key_path, stdout) self.assertEqual(os.path.getsize(key_path), 16) os.remove(key_path) def test_010_encrypt_decrypt(self): for i in self.INDEXES: self.__encrypt_file(self.INPUT_FILES[i], self.OUTPUT_FILES[i]) self.assertFalse(filecmp.cmp(self.INPUT_FILES[i], self.OUTPUT_FILES[i], shallow=False)) dec_path=os.path.join(self.OUTPUT_DIR, os.path.basename(self.OUTPUT_FILES[i]) +'.dec') self.__decrypt_file(self.OUTPUT_FILES[i], dec_path) self.assertTrue(filecmp.cmp(self.INPUT_FILES[i], dec_path, shallow=False)) def test_100_open_close(self): input_path=self.ENCRYPTED_FILES[-1] output_path=os.path.join(self.OUTPUT_DIR, 'test_100') stdout, stderr=self.run_binary(['open_close', 'R', input_path]) self.verify_open_close(stdout, stderr, input_path, 'input') try: stdout, stderr=self.run_binary(['open_close', 'W', output_path]) self.assertIn('ERROR: Failed to open output file', stderr) except subprocess.CalledProcessError as exc: self.assertNotEqual(exc.returncode, 0) self.assertTrue(os.path.isfile(output_path)) else: print('[!] Fail: open_close returned 0') self.fail() def test_101_open_flags(self): file_path=os.path.join(self.OUTPUT_DIR, 'test_101') stdout, stderr=self.run_binary(['open_flags', file_path]) self.verify_open_flags(stdout, stderr) def test_115_seek_tell(self): plaintext_path=self.INPUT_FILES[-1] input_path=self.ENCRYPTED_FILES[-1] output_path_1=os.path.join(self.OUTPUT_DIR, 'test_115a') output_path_2=os.path.join(self.OUTPUT_DIR, 'test_115b') self.copy_input(plaintext_path, output_path_1) self.copy_input(plaintext_path, output_path_2) stdout, stderr=self.run_binary(['seek_tell', input_path, output_path_1, output_path_2]) self.verify_seek_tell(stdout, stderr, input_path, output_path_1, output_path_2, self.FILE_SIZES[-1]) def test_130_file_stat(self): for i in self.INDEXES: input_path=self.ENCRYPTED_FILES[i] output_path=self.OUTPUT_FILES[i] size=str(self.FILE_SIZES[i]) self.copy_input(self.INPUT_FILES[i], output_path) stdout, stderr=self.run_binary(['stat', input_path, output_path]) self.verify_stat(stdout, stderr, input_path, output_path, size) def verify_size(self, file, size): dec_path=os.path.join(self.OUTPUT_DIR, os.path.basename(file) +'.dec') self.__decrypt_file(file, dec_path) self.assertEqual(os.stat(dec_path).st_size, size) @expectedFailureIf(HAS_SGX) def test_140_file_truncate(self): self.fail() def test_150_file_rename(self): path1=os.path.join(self.OUTPUT_DIR, 'test_150a') path2=os.path.join(self.OUTPUT_DIR, 'test_150b') self.copy_input(self.ENCRYPTED_FILES[-1], path1) shutil.copy(path1, path2) args=['decrypt', '-V', '-w', self.WRAP_KEY, '-i', path2, '-o', path1] try: self.__pf_crypt(args) except subprocess.CalledProcessError as exc: self.assertNotEqual(exc.returncode, 0) else: print('[!] Fail: successfully decrypted renamed file: ' +path2) self.fail() def verify_copy_content(self, input_path, output_path): dec_path=os.path.join(self.OUTPUT_DIR, os.path.basename(output_path) +'.dec') self.__decrypt_file(output_path, dec_path) self.assertTrue(filecmp.cmp(input_path, dec_path, shallow=False)) def do_copy_test(self, executable, timeout): stdout, stderr=self.run_binary([executable, self.ENCRYPTED_DIR, self.OUTPUT_DIR], timeout=timeout) self.verify_copy(stdout, stderr, self.ENCRYPTED_DIR, executable) def test_204_copy_dir_mmap_whole(self): self.do_copy_test('copy_mmap_whole', 30) def test_205_copy_dir_mmap_seq(self): self.do_copy_test('copy_mmap_seq', 60) def test_206_copy_dir_mmap_rev(self): self.do_copy_test('copy_mmap_rev', 60) def test_210_copy_dir_mounted(self): executable='copy_whole' stdout, stderr=self.run_binary([executable, '\/mounted\/pf_input', '\/mounted\/pf_output'], timeout=30) self.verify_copy(stdout, stderr, '\/mounted\/pf_input', executable) def __corrupt_file(self, input_path, output_path): cmd=[self.PF_TAMPER, '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path] return self.run_native_binary(cmd) @expectedFailureIf(HAS_SGX) def test_500_invalid(self): invalid_dir=os.path.join(self.TEST_DIR, 'pf_invalid') should_pass=['chunk_padding_1_fixed', 'chunk_padding_2_fixed', 'chunk_data_3', 'chunk_data_3_fixed', 'chunk_data_4', 'chunk_data_4_fixed'] if not os.path.exists(invalid_dir): os.mkdir(invalid_dir) original_input=self.OUTPUT_FILES[-1] self.__encrypt_file(self.INPUT_FILES[-1], original_input) self.__corrupt_file(original_input, invalid_dir) for name in os.listdir(invalid_dir): invalid=os.path.join(invalid_dir, name) output_path=os.path.join(self.OUTPUT_DIR, name) input_path=os.path.join(invalid_dir, os.path.basename(original_input)) shutil.copy(invalid, input_path) should_pass=any(s in name for s in should_pass) try: args=['decrypt', '-V', '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path] self.__pf_crypt(args) except subprocess.CalledProcessError as exc: if should_pass: self.assertEqual(exc.returncode, 0) else: self.assertNotEqual(exc.returncode, 0) else: if not should_pass: print('[!] Fail: successfully decrypted file: ' +name) self.fail() ","sourceWithComments":"#!\/usr\/bin\/env python3\n\nimport filecmp\nimport os\nimport shutil\nimport subprocess\nimport unittest\n\nfrom test_fs import (\n    TC_00_FileSystem,\n)\n\nfrom regression import (\n    HAS_SGX,\n    expectedFailureIf,\n)\n\n@unittest.skipUnless(HAS_SGX, 'Protected files require SGX support')\nclass TC_50_ProtectedFiles(TC_00_FileSystem):\n    @classmethod\n    def setUpClass(cls):\n        cls.PF_CRYPT = 'bin\/pf_crypt'\n        cls.PF_TAMPER = 'bin\/pf_tamper'\n        cls.WRAP_KEY = os.path.join(cls.TEST_DIR, 'wrap-key')\n        # CONST_WRAP_KEY must match the one in manifest\n        cls.CONST_WRAP_KEY = [0xff, 0xee, 0xdd, 0xcc, 0xbb, 0xaa, 0x99, 0x88,\n                              0x77, 0x66, 0x55, 0x44, 0x33, 0x22, 0x11, 0x00]\n        cls.ENCRYPTED_DIR = os.path.join(cls.TEST_DIR, 'pf_input')\n        cls.ENCRYPTED_FILES = [os.path.join(cls.ENCRYPTED_DIR, str(v)) for v in cls.FILE_SIZES]\n        cls.LIB_PATH = os.path.join(os.getcwd(), 'lib')\n\n        super().setUpClass()\n        if not os.path.exists(cls.ENCRYPTED_DIR):\n            os.mkdir(cls.ENCRYPTED_DIR)\n        cls.OUTPUT_DIR = os.path.join(cls.TEST_DIR, 'pf_output')\n        cls.OUTPUT_FILES = [os.path.join(cls.OUTPUT_DIR, str(x)) for x in cls.FILE_SIZES]\n        # create encrypted files\n        cls.__set_default_key(cls)\n        for i in cls.INDEXES:\n            cmd = [cls.PF_CRYPT, 'encrypt', '-w', cls.WRAP_KEY, '-i', cls.INPUT_FILES[i], '-o',\n                   cls.ENCRYPTED_FILES[i]]\n\n            cls.run_native_binary(cmd, libpath=os.path.join(os.getcwd(), 'lib'))\n\n    def __pf_crypt(self, args):\n        args.insert(0, self.PF_CRYPT)\n        return self.run_native_binary(args, libpath=os.path.join(os.getcwd(), 'lib'))\n\n    def __set_default_key(self):\n        with open(self.WRAP_KEY, 'wb') as file:\n            file.write(bytes(self.CONST_WRAP_KEY))\n\n    # overrides TC_00_FileSystem to encrypt the file instead of just copying\n    def copy_input(self, input_path, output_path):\n        self.__encrypt_file(input_path, output_path)\n\n    def __encrypt_file(self, input_path, output_path):\n        args = ['encrypt', '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path]\n        stdout, stderr = self.__pf_crypt(args)\n        return (stdout, stderr)\n\n    def __decrypt_file(self, input_path, output_path):\n        args = ['decrypt', '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path]\n        stdout, stderr = self.__pf_crypt(args)\n        return (stdout, stderr)\n\n    def test_000_gen_key(self):\n        # test random key generation\n        key_path = os.path.join(self.TEST_DIR, 'tmpkey')\n        args = ['gen-key', '-w', key_path]\n        stdout, _ = self.__pf_crypt(args)\n        self.assertIn('Wrap key saved to: ' + key_path, stdout)\n        self.assertEqual(os.path.getsize(key_path), 16)\n        os.remove(key_path)\n\n    def test_010_encrypt_decrypt(self):\n        for i in self.INDEXES:\n            self.__encrypt_file(self.INPUT_FILES[i], self.OUTPUT_FILES[i])\n            self.assertFalse(filecmp.cmp(self.INPUT_FILES[i], self.OUTPUT_FILES[i], shallow=False))\n            dec_path = os.path.join(self.OUTPUT_DIR,\n                                    os.path.basename(self.OUTPUT_FILES[i]) + '.dec')\n            self.__decrypt_file(self.OUTPUT_FILES[i], dec_path)\n            self.assertTrue(filecmp.cmp(self.INPUT_FILES[i], dec_path, shallow=False))\n\n    # overrides TC_00_FileSystem to change input dir (from plaintext to encrypted)\n    def test_100_open_close(self):\n        # the test binary expects a path to read-only (existing) file or a path to file that\n        # will get created\n        input_path = self.ENCRYPTED_FILES[-1] # existing file\n        output_path = os.path.join(self.OUTPUT_DIR, 'test_100') # new file\n        stdout, stderr = self.run_binary(['open_close', 'R', input_path])\n        self.verify_open_close(stdout, stderr, input_path, 'input')\n        # the following test tries to open multiple handles to a single writable PF, should fail\n        try:\n            stdout, stderr = self.run_binary(['open_close', 'W', output_path])\n            self.assertIn('ERROR: Failed to open output file', stderr)\n        except subprocess.CalledProcessError as exc:\n            self.assertNotEqual(exc.returncode, 0)\n            self.assertTrue(os.path.isfile(output_path))\n        else:\n            print('[!] Fail: open_close returned 0')\n            self.fail()\n\n    # overrides TC_00_FileSystem to change input dir (from plaintext to encrypted)\n    def test_101_open_flags(self):\n        # the test binary expects a path to file that will get created\n        file_path = os.path.join(self.OUTPUT_DIR, 'test_101') # new file\n        stdout, stderr = self.run_binary(['open_flags', file_path])\n        self.verify_open_flags(stdout, stderr)\n\n    # overrides TC_00_FileSystem to change input dir (from plaintext to encrypted)\n    def test_115_seek_tell(self):\n        # the test binary expects a path to read-only (existing) file and two paths to files that\n        # will get created\n        plaintext_path = self.INPUT_FILES[-1]\n        input_path = self.ENCRYPTED_FILES[-1] # existing file\n        output_path_1 = os.path.join(self.OUTPUT_DIR, 'test_115a') # writable files\n        output_path_2 = os.path.join(self.OUTPUT_DIR, 'test_115b')\n        self.copy_input(plaintext_path, output_path_1) # encrypt\n        self.copy_input(plaintext_path, output_path_2)\n        stdout, stderr = self.run_binary(['seek_tell', input_path, output_path_1, output_path_2])\n        self.verify_seek_tell(stdout, stderr, input_path, output_path_1, output_path_2,\n                              self.FILE_SIZES[-1])\n\n    # overrides TC_00_FileSystem to change input dir (from plaintext to encrypted)\n    def test_130_file_stat(self):\n        # the test binary expects a path to read-only (existing) file and a path to file that\n        # will get created\n        for i in self.INDEXES:\n            input_path = self.ENCRYPTED_FILES[i]\n            output_path = self.OUTPUT_FILES[i]\n            size = str(self.FILE_SIZES[i])\n            self.copy_input(self.INPUT_FILES[i], output_path)\n            stdout, stderr = self.run_binary(['stat', input_path, output_path])\n            self.verify_stat(stdout, stderr, input_path, output_path, size)\n\n    # overrides TC_00_FileSystem to decrypt output\n    def verify_size(self, file, size):\n        dec_path = os.path.join(self.OUTPUT_DIR, os.path.basename(file) + '.dec')\n        self.__decrypt_file(file, dec_path)\n        self.assertEqual(os.stat(dec_path).st_size, size)\n\n    @expectedFailureIf(HAS_SGX)\n    # pylint: disable=fixme\n    def test_140_file_truncate(self):\n        self.fail() # TODO: port these to the new file format\n\n    def test_150_file_rename(self):\n        path1 = os.path.join(self.OUTPUT_DIR, 'test_150a')\n        path2 = os.path.join(self.OUTPUT_DIR, 'test_150b')\n        self.copy_input(self.ENCRYPTED_FILES[-1], path1)\n        shutil.copy(path1, path2)\n        # accessing renamed file should fail\n        args = ['decrypt', '-V', '-w', self.WRAP_KEY, '-i', path2, '-o', path1]\n        try:\n            self.__pf_crypt(args)\n        except subprocess.CalledProcessError as exc:\n            self.assertNotEqual(exc.returncode, 0)\n        else:\n            print('[!] Fail: successfully decrypted renamed file: ' + path2)\n            self.fail()\n\n    # overrides TC_00_FileSystem to decrypt output\n    def verify_copy_content(self, input_path, output_path):\n        dec_path = os.path.join(self.OUTPUT_DIR, os.path.basename(output_path) + '.dec')\n        self.__decrypt_file(output_path, dec_path)\n        self.assertTrue(filecmp.cmp(input_path, dec_path, shallow=False))\n\n    # overrides TC_00_FileSystem to change input dir (from plaintext to encrypted)\n    def do_copy_test(self, executable, timeout):\n        stdout, stderr = self.run_binary([executable, self.ENCRYPTED_DIR, self.OUTPUT_DIR],\n                                         timeout=timeout)\n        self.verify_copy(stdout, stderr, self.ENCRYPTED_DIR, executable)\n\n    # overrides TC_00_FileSystem to not skip this on SGX\n    def test_204_copy_dir_mmap_whole(self):\n        self.do_copy_test('copy_mmap_whole', 30)\n\n    # overrides TC_00_FileSystem to not skip this on SGX\n    def test_205_copy_dir_mmap_seq(self):\n        self.do_copy_test('copy_mmap_seq', 60)\n\n    # overrides TC_00_FileSystem to not skip this on SGX\n    def test_206_copy_dir_mmap_rev(self):\n        self.do_copy_test('copy_mmap_rev', 60)\n\n    # overrides TC_00_FileSystem to change dirs (from plaintext to encrypted)\n    def test_210_copy_dir_mounted(self):\n        executable = 'copy_whole'\n        stdout, stderr = self.run_binary([executable, '\/mounted\/pf_input', '\/mounted\/pf_output'],\n                                         timeout=30)\n        self.verify_copy(stdout, stderr, '\/mounted\/pf_input', executable)\n\n    def __corrupt_file(self, input_path, output_path):\n        cmd = [self.PF_TAMPER, '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path]\n        return self.run_native_binary(cmd)\n\n    # invalid\/corrupted files\n    @expectedFailureIf(HAS_SGX)\n    # pylint: disable=fixme\n    def test_500_invalid(self):\n        # TODO: port these to the new file format\n        invalid_dir = os.path.join(self.TEST_DIR, 'pf_invalid')\n        # files below should work normally (benign modifications)\n        should_pass = ['chunk_padding_1_fixed', 'chunk_padding_2_fixed', 'chunk_data_3',\n                       'chunk_data_3_fixed', 'chunk_data_4', 'chunk_data_4_fixed']\n        if not os.path.exists(invalid_dir):\n            os.mkdir(invalid_dir)\n        # prepare valid encrypted file (largest one for maximum possible corruptions)\n        original_input = self.OUTPUT_FILES[-1]\n        self.__encrypt_file(self.INPUT_FILES[-1], original_input)\n        # generate invalid files based on the above\n        self.__corrupt_file(original_input, invalid_dir)\n        # try to decrypt invalid files\n        for name in os.listdir(invalid_dir):\n            invalid = os.path.join(invalid_dir, name)\n            output_path = os.path.join(self.OUTPUT_DIR, name)\n            input_path = os.path.join(invalid_dir, os.path.basename(original_input))\n            # copy the file so it has the original file name (for allowed path check)\n            shutil.copy(invalid, input_path)\n            should_pass = any(s in name for s in should_pass)\n\n            try:\n                args = ['decrypt', '-V', '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path]\n                self.__pf_crypt(args)\n            except subprocess.CalledProcessError as exc:\n                if should_pass:\n                    self.assertEqual(exc.returncode, 0)\n                else:\n                    self.assertNotEqual(exc.returncode, 0)\n            else:\n                if not should_pass:\n                    print('[!] Fail: successfully decrypted file: ' + name)\n                    self.fail()\n"}},"msg":"[Pal\/Linux-SGX] Test malicious modifications to protected files\n\nThis commit adds a new PF utility `pf_tamper` that tampers with\nvalid protected files and uses this utility to test that the PF\nlogic in Linux-SGX detects such malicious modifications.\n\nThis commit also moves out the PF-format macros and structs from\n`protected_files_internal.h` to `protected_files_format.h` for\nbetter readability.\n\nCo-authored-by: Dmitrii Kuvaiskii <dmitrii.kuvaiskii@intel.com>"}},"https:\/\/github.com\/RGayathri99\/gramine":{"567790aec46e0714171121318783b8ac8c56adc3":{"url":"https:\/\/api.github.com\/repos\/RGayathri99\/gramine\/commits\/567790aec46e0714171121318783b8ac8c56adc3","html_url":"https:\/\/github.com\/RGayathri99\/gramine\/commit\/567790aec46e0714171121318783b8ac8c56adc3","message":"[Pal\/Linux-SGX] Test malicious modifications to protected files\n\nThis commit adds a new PF utility `pf_tamper` that tampers with\nvalid protected files and uses this utility to test that the PF\nlogic in Linux-SGX detects such malicious modifications.\n\nThis commit also moves out the PF-format macros and structs from\n`protected_files_internal.h` to `protected_files_format.h` for\nbetter readability.\n\nCo-authored-by: Dmitrii Kuvaiskii <dmitrii.kuvaiskii@intel.com>","sha":"567790aec46e0714171121318783b8ac8c56adc3","keyword":"tampering malicious","diff":"diff --git a\/LibOS\/shim\/test\/fs\/test_pf.py b\/LibOS\/shim\/test\/fs\/test_pf.py\nindex e430e02a..7b1621d7 100644\n--- a\/LibOS\/shim\/test\/fs\/test_pf.py\n+++ b\/LibOS\/shim\/test\/fs\/test_pf.py\n@@ -193,24 +193,20 @@ def test_210_copy_dir_mounted(self):\n \n     def __corrupt_file(self, input_path, output_path):\n         cmd = [self.PF_TAMPER, '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path]\n-        return self.run_native_binary(cmd)\n+        return self.run_native_binary(cmd, libpath=os.path.join(os.getcwd(), 'lib'))\n \n     # invalid\/corrupted files\n-    @expectedFailureIf(HAS_SGX)\n-    # pylint: disable=fixme\n     def test_500_invalid(self):\n-        # TODO: port these to the new file format\n         invalid_dir = os.path.join(self.TEST_DIR, 'pf_invalid')\n-        # files below should work normally (benign modifications)\n-        should_pass = ['chunk_padding_1_fixed', 'chunk_padding_2_fixed', 'chunk_data_3',\n-                       'chunk_data_3_fixed', 'chunk_data_4', 'chunk_data_4_fixed']\n         if not os.path.exists(invalid_dir):\n             os.mkdir(invalid_dir)\n+\n         # prepare valid encrypted file (largest one for maximum possible corruptions)\n         original_input = self.OUTPUT_FILES[-1]\n         self.__encrypt_file(self.INPUT_FILES[-1], original_input)\n         # generate invalid files based on the above\n         self.__corrupt_file(original_input, invalid_dir)\n+\n         # try to decrypt invalid files\n         for name in os.listdir(invalid_dir):\n             invalid = os.path.join(invalid_dir, name)\n@@ -218,17 +214,13 @@ def test_500_invalid(self):\n             input_path = os.path.join(invalid_dir, os.path.basename(original_input))\n             # copy the file so it has the original file name (for allowed path check)\n             shutil.copy(invalid, input_path)\n-            should_pass = any(s in name for s in should_pass)\n \n             try:\n                 args = ['decrypt', '-V', '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path]\n                 self.__pf_crypt(args)\n             except subprocess.CalledProcessError as exc:\n-                if should_pass:\n-                    self.assertEqual(exc.returncode, 0)\n-                else:\n-                    self.assertNotEqual(exc.returncode, 0)\n+                # decryption of invalid file must fail with -1 (wrapped to 255)\n+                self.assertEqual(exc.returncode, 255)\n             else:\n-                if not should_pass:\n-                    print('[!] Fail: successfully decrypted file: ' + name)\n-                    self.fail()\n+                print('[!] Fail: successfully decrypted file: ' + name)\n+                self.fail()\ndiff --git a\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files.c b\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files.c\nindex 4f0c59ba..d020a1e2 100644\n--- a\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files.c\n+++ b\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files.c\n@@ -5,6 +5,8 @@\n  *\/\n \n #include \"api.h\"\n+#include \"protected_files.h\"\n+#include \"protected_files_format.h\"\n #include \"protected_files_internal.h\"\n \n #ifndef IN_PAL\n@@ -65,20 +67,6 @@ static pf_random_f          g_cb_random          = NULL;\n static pf_iv_t g_empty_iv = {0};\n static bool g_initialized = false;\n \n-#define METADATA_KEY_NAME \"SGX-PROTECTED-FS-METADATA-KEY\"\n-#define MAX_LABEL_SIZE    64\n-\n-static_assert(sizeof(METADATA_KEY_NAME) <= MAX_LABEL_SIZE, \"label too long\");\n-\n-#pragma pack(push, 1)\n-typedef struct {\n-    uint32_t index;\n-    char label[MAX_LABEL_SIZE]; \/\/ must be NULL terminated\n-    pf_keyid_t nonce;\n-    uint32_t output_len; \/\/ in bits\n-} kdf_input_t;\n-#pragma pack(pop)\n-\n \/\/ The key derivation function follow recommendations from NIST Special Publication 800-108:\n \/\/ Recommendation for Key Derivation Using Pseudorandom Functions\n \/\/ https:\/\/nvlpubs.nist.gov\/nistpubs\/Legacy\/SP\/nistspecialpublication800-108.pdf\ndiff --git a\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files_format.h b\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files_format.h\nnew file mode 100644\nindex 00000000..2394e53e\n--- \/dev\/null\n+++ b\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files_format.h\n@@ -0,0 +1,139 @@\n+\/* SPDX-License-Identifier: LGPL-3.0-or-later *\/\n+\/* Copyright (C) 2019-2020 Invisible Things Lab\n+ *                         Rafal Wojdyla <omeg@invisiblethingslab.com>\n+ * Copyright (C) 2011-2020 Intel Corporation\n+ *\/\n+\n+#ifndef PROTECTED_FILES_FORMAT_H_\n+#define PROTECTED_FILES_FORMAT_H_\n+\n+#include <limits.h>\n+\n+#include \"assert.h\"\n+#include \"list.h\"\n+#include \"protected_files.h\"\n+\n+#define PF_FILE_ID       0x46505f4850415247 \/* GRAPH_PF *\/\n+#define PF_MAJOR_VERSION 0x01\n+#define PF_MINOR_VERSION 0x00\n+\n+#define METADATA_KEY_NAME \"SGX-PROTECTED-FS-METADATA-KEY\"\n+#define MAX_LABEL_SIZE    64\n+\n+static_assert(sizeof(METADATA_KEY_NAME) <= MAX_LABEL_SIZE, \"label too long\");\n+\n+#pragma pack(push, 1)\n+\n+typedef struct _metadata_plain {\n+    uint64_t   file_id;\n+    uint8_t    major_version;\n+    uint8_t    minor_version;\n+    pf_keyid_t metadata_key_id;\n+    pf_mac_t   metadata_gmac; \/* GCM mac *\/\n+} metadata_plain_t;\n+\n+#define PATH_MAX_SIZE (260 + 512)\n+\n+\/\/ these are all defined as relative to node size, so we can decrease node size in tests\n+\/\/ and have deeper tree\n+#define MD_USER_DATA_SIZE (PF_NODE_SIZE * 3 \/ 4) \/\/ 3072\n+static_assert(MD_USER_DATA_SIZE == 3072, \"bad struct size\");\n+\n+typedef struct _metadata_encrypted {\n+    char     path[PATH_MAX_SIZE];\n+    uint64_t size;\n+    pf_key_t mht_key;\n+    pf_mac_t mht_gmac;\n+    uint8_t  data[MD_USER_DATA_SIZE];\n+} metadata_encrypted_t;\n+\n+typedef uint8_t metadata_encrypted_blob_t[sizeof(metadata_encrypted_t)];\n+\n+#define METADATA_NODE_SIZE PF_NODE_SIZE\n+\n+typedef uint8_t metadata_padding_t[METADATA_NODE_SIZE -\n+                                   (sizeof(metadata_plain_t) + sizeof(metadata_encrypted_blob_t))];\n+\n+typedef struct _metadata_node {\n+    metadata_plain_t          plain_part;\n+    metadata_encrypted_blob_t encrypted_part;\n+    metadata_padding_t        padding;\n+} metadata_node_t;\n+\n+static_assert(sizeof(metadata_node_t) == PF_NODE_SIZE, \"sizeof(metadata_node_t)\");\n+\n+typedef struct _data_node_crypto {\n+    pf_key_t key;\n+    pf_mac_t gmac;\n+} gcm_crypto_data_t;\n+\n+\/\/ for PF_NODE_SIZE == 4096, we have 96 attached data nodes and 32 mht child nodes\n+\/\/ for PF_NODE_SIZE == 2048, we have 48 attached data nodes and 16 mht child nodes\n+\/\/ for PF_NODE_SIZE == 1024, we have 24 attached data nodes and 8 mht child nodes\n+\/\/ 3\/4 of the node size is dedicated to data nodes\n+#define ATTACHED_DATA_NODES_COUNT ((PF_NODE_SIZE \/ sizeof(gcm_crypto_data_t)) * 3 \/ 4)\n+static_assert(ATTACHED_DATA_NODES_COUNT == 96, \"ATTACHED_DATA_NODES_COUNT\");\n+\/\/ 1\/4 of the node size is dedicated to child mht nodes\n+#define CHILD_MHT_NODES_COUNT ((PF_NODE_SIZE \/ sizeof(gcm_crypto_data_t)) * 1 \/ 4)\n+static_assert(CHILD_MHT_NODES_COUNT == 32, \"CHILD_MHT_NODES_COUNT\");\n+\n+typedef struct _mht_node {\n+    gcm_crypto_data_t data_nodes_crypto[ATTACHED_DATA_NODES_COUNT];\n+    gcm_crypto_data_t mht_nodes_crypto[CHILD_MHT_NODES_COUNT];\n+} mht_node_t;\n+\n+static_assert(sizeof(mht_node_t) == PF_NODE_SIZE, \"sizeof(mht_node_t)\");\n+\n+typedef struct _data_node {\n+    uint8_t data[PF_NODE_SIZE];\n+} data_node_t;\n+\n+static_assert(sizeof(data_node_t) == PF_NODE_SIZE, \"sizeof(data_node_t)\");\n+\n+typedef struct _encrypted_node {\n+    uint8_t cipher[PF_NODE_SIZE];\n+} encrypted_node_t;\n+\n+static_assert(sizeof(encrypted_node_t) == PF_NODE_SIZE, \"sizeof(encrypted_node_t)\");\n+\n+#define MAX_PAGES_IN_CACHE 48\n+\n+typedef enum {\n+    FILE_MHT_NODE_TYPE  = 1,\n+    FILE_DATA_NODE_TYPE = 2,\n+} mht_node_type_e;\n+\n+\/\/ make sure these are the same size\n+static_assert(sizeof(mht_node_t) == sizeof(data_node_t),\n+              \"sizeof(mht_node_t) == sizeof(data_node_t)\");\n+\n+DEFINE_LIST(_file_node);\n+typedef struct _file_node {\n+    LIST_TYPE(_file_node) list;\n+    uint8_t type;\n+    uint64_t node_number;\n+    struct _file_node* parent;\n+    bool need_writing;\n+    bool new_node;\n+    struct {\n+        uint64_t physical_node_number;\n+        encrypted_node_t encrypted; \/\/ the actual data from the disk\n+    };\n+    union { \/\/ decrypted data\n+        mht_node_t mht;\n+        data_node_t data;\n+    } decrypted;\n+} file_node_t;\n+DEFINE_LISTP(_file_node);\n+\n+typedef struct {\n+    uint32_t index;\n+    char label[MAX_LABEL_SIZE]; \/\/ must be NULL terminated\n+    pf_keyid_t nonce;\n+    uint32_t output_len; \/\/ in bits\n+} kdf_input_t;\n+\n+#pragma pack(pop)\n+\n+#endif \/* PROTECTED_FILES_FORMAT_H_ *\/\n+\ndiff --git a\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files_internal.h b\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files_internal.h\nindex 4a31261d..c06d1ecf 100644\n--- a\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files_internal.h\n+++ b\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files_internal.h\n@@ -13,116 +13,7 @@\n #include \"list.h\"\n #include \"lru_cache.h\"\n #include \"protected_files.h\"\n-\n-#define PF_FILE_ID       0x46505f4850415247 \/* GRAPH_PF *\/\n-#define PF_MAJOR_VERSION 0x01\n-#define PF_MINOR_VERSION 0x00\n-\n-#pragma pack(push, 1)\n-\n-typedef struct _metadata_plain {\n-    uint64_t   file_id;\n-    uint8_t    major_version;\n-    uint8_t    minor_version;\n-    pf_keyid_t metadata_key_id;\n-    pf_mac_t   metadata_gmac; \/* GCM mac *\/\n-} metadata_plain_t;\n-\n-#define PATH_MAX_SIZE (260 + 512)\n-\n-\/\/ these are all defined as relative to node size, so we can decrease node size in tests\n-\/\/ and have deeper tree\n-#define MD_USER_DATA_SIZE (PF_NODE_SIZE * 3 \/ 4) \/\/ 3072\n-static_assert(MD_USER_DATA_SIZE == 3072, \"bad struct size\");\n-\n-typedef struct _metadata_encrypted {\n-    char     path[PATH_MAX_SIZE];\n-    uint64_t size;\n-    pf_key_t mht_key;\n-    pf_mac_t mht_gmac;\n-    uint8_t  data[MD_USER_DATA_SIZE];\n-} metadata_encrypted_t;\n-\n-typedef uint8_t metadata_encrypted_blob_t[sizeof(metadata_encrypted_t)];\n-\n-#define METADATA_NODE_SIZE PF_NODE_SIZE\n-\n-typedef uint8_t metadata_padding_t[METADATA_NODE_SIZE -\n-                                   (sizeof(metadata_plain_t) + sizeof(metadata_encrypted_blob_t))];\n-\n-typedef struct _metadata_node {\n-    metadata_plain_t          plain_part;\n-    metadata_encrypted_blob_t encrypted_part;\n-    metadata_padding_t        padding;\n-} metadata_node_t;\n-\n-static_assert(sizeof(metadata_node_t) == PF_NODE_SIZE, \"sizeof(metadata_node_t)\");\n-\n-typedef struct _data_node_crypto {\n-    pf_key_t key;\n-    pf_mac_t gmac;\n-} gcm_crypto_data_t;\n-\n-\/\/ for PF_NODE_SIZE == 4096, we have 96 attached data nodes and 32 mht child nodes\n-\/\/ for PF_NODE_SIZE == 2048, we have 48 attached data nodes and 16 mht child nodes\n-\/\/ for PF_NODE_SIZE == 1024, we have 24 attached data nodes and 8 mht child nodes\n-\/\/ 3\/4 of the node size is dedicated to data nodes\n-#define ATTACHED_DATA_NODES_COUNT ((PF_NODE_SIZE \/ sizeof(gcm_crypto_data_t)) * 3 \/ 4)\n-static_assert(ATTACHED_DATA_NODES_COUNT == 96, \"ATTACHED_DATA_NODES_COUNT\");\n-\/\/ 1\/4 of the node size is dedicated to child mht nodes\n-#define CHILD_MHT_NODES_COUNT ((PF_NODE_SIZE \/ sizeof(gcm_crypto_data_t)) * 1 \/ 4)\n-static_assert(CHILD_MHT_NODES_COUNT == 32, \"CHILD_MHT_NODES_COUNT\");\n-\n-typedef struct _mht_node {\n-    gcm_crypto_data_t data_nodes_crypto[ATTACHED_DATA_NODES_COUNT];\n-    gcm_crypto_data_t mht_nodes_crypto[CHILD_MHT_NODES_COUNT];\n-} mht_node_t;\n-\n-static_assert(sizeof(mht_node_t) == PF_NODE_SIZE, \"sizeof(mht_node_t)\");\n-\n-typedef struct _data_node {\n-    uint8_t data[PF_NODE_SIZE];\n-} data_node_t;\n-\n-static_assert(sizeof(data_node_t) == PF_NODE_SIZE, \"sizeof(data_node_t)\");\n-\n-typedef struct _encrypted_node {\n-    uint8_t cipher[PF_NODE_SIZE];\n-} encrypted_node_t;\n-\n-static_assert(sizeof(encrypted_node_t) == PF_NODE_SIZE, \"sizeof(encrypted_node_t)\");\n-\n-#define MAX_PAGES_IN_CACHE 48\n-\n-typedef enum {\n-    FILE_MHT_NODE_TYPE  = 1,\n-    FILE_DATA_NODE_TYPE = 2,\n-} mht_node_type_e;\n-\n-\/\/ make sure these are the same size\n-static_assert(sizeof(mht_node_t) == sizeof(data_node_t),\n-              \"sizeof(mht_node_t) == sizeof(data_node_t)\");\n-\n-DEFINE_LIST(_file_node);\n-typedef struct _file_node {\n-    LIST_TYPE(_file_node) list;\n-    uint8_t type;\n-    uint64_t node_number;\n-    struct _file_node* parent;\n-    bool need_writing;\n-    bool new_node;\n-    struct {\n-        uint64_t physical_node_number;\n-        encrypted_node_t encrypted; \/\/ the actual data from the disk\n-    };\n-    union { \/\/ decrypted data\n-        mht_node_t mht;\n-        data_node_t data;\n-    } decrypted;\n-} file_node_t;\n-DEFINE_LISTP(_file_node);\n-\n-#pragma pack(pop)\n+#include \"protected_files_format.h\"\n \n struct pf_context {\n     metadata_node_t file_metadata; \/\/ actual data from disk's meta data node\ndiff --git a\/Pal\/src\/host\/Linux-SGX\/tools\/Makefile b\/Pal\/src\/host\/Linux-SGX\/tools\/Makefile\nindex a5903ad9..87bc7812 100644\n--- a\/Pal\/src\/host\/Linux-SGX\/tools\/Makefile\n+++ b\/Pal\/src\/host\/Linux-SGX\/tools\/Makefile\n@@ -12,3 +12,4 @@ $(targets):\n \t$(MAKE) -C verify-ias-report $@\n \t$(MAKE) -C ra-tls $@\n \t$(MAKE) -C pf_crypt $@\n+\t$(MAKE) -C pf_tamper $@\ndiff --git a\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/.gitignore b\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/.gitignore\nnew file mode 100644\nindex 00000000..a52536d8\n--- \/dev\/null\n+++ b\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/.gitignore\n@@ -0,0 +1 @@\n+\/pf_tamper\ndiff --git a\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/Makefile b\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/Makefile\nnew file mode 100644\nindex 00000000..94c826e4\n--- \/dev\/null\n+++ b\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/Makefile\n@@ -0,0 +1,31 @@\n+include ..\/..\/..\/..\/..\/..\/Scripts\/Makefile.configs\n+include ..\/..\/..\/..\/..\/..\/Scripts\/Makefile.rules\n+\n+CFLAGS += -I..\/.. \\\n+          -I..\/common \\\n+          -I..\/..\/protected-files \\\n+          -I..\/..\/..\/..\/..\/include\/lib \\\n+          -D_GNU_SOURCE\n+\n+LDLIBS += -L..\/common \\\n+          -L..\/..\/..\/..\/..\/lib\/crypto\/mbedtls\/install\/lib \\\n+          -lsgx_util -lmbedcrypto\n+\n+PREFIX ?= \/usr\/local\n+\n+pf_tamper: pf_tamper.o\n+\t$(call cmd,csingle)\n+\n+.PHONY: all\n+all: pf_tamper\n+\n+.PHONY: install\n+install:\n+\tinstall -D pf_tamper -t ${PREFIX}\/bin\n+\n+.PHONY: clean\n+clean:\n+\t$(RM) *.o pf_tamper\n+\n+.PHONY: distclean\n+distclean: clean\ndiff --git a\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/pf_tamper.c b\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/pf_tamper.c\nnew file mode 100644\nindex 00000000..4d39a0d5\n--- \/dev\/null\n+++ b\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/pf_tamper.c\n@@ -0,0 +1,476 @@\n+\/* SPDX-License-Identifier: LGPL-3.0-or-later *\/\n+\/* Copyright (C) 2019-2020 Invisible Things Lab\n+ *                         Rafal Wojdyla <omeg@invisiblethingslab.com>\n+ *\/\n+\n+#include <fcntl.h>\n+#include <getopt.h>\n+#include <stdlib.h>\n+#include <sys\/mman.h>\n+#include <unistd.h>\n+\n+#include \"pf_util.h\"\n+#include \"protected_files.h\"\n+#include \"protected_files_format.h\"\n+#include \"util.h\"\n+\n+\/* Tamper with a PF in various ways for testing purposes. The PF is assumed to be valid and have at\n+ * least enough data to contain two MHT nodes. *\/\n+\n+\/* Command line options *\/\n+struct option g_options[] = {\n+    { \"input\", required_argument, 0, 'i' },\n+    { \"output\", required_argument, 0, 'o' },\n+    { \"wrap-key\", required_argument, 0, 'w' },\n+    { \"verbose\", no_argument, 0, 'v' },\n+    { \"help\", no_argument, 0, 'h' },\n+    { 0, 0, 0, 0 }\n+};\n+\n+static void usage(void) {\n+    INFO(\"\\nUsage: pf_tamper [options]\\n\");\n+    INFO(\"\\nAvailable options:\\n\");\n+    INFO(\"  --help, -h           Display this help\\n\");\n+    INFO(\"  --verbose, -v        Enable verbose output\\n\");\n+    INFO(\"  --wrap-key, -w PATH  Path to wrap key file\\n\");\n+    INFO(\"  --input, -i PATH     Source file to be tampered with (must be a valid PF)\\n\");\n+    INFO(\"  --output, -o PATH    Directory where modified files will be written to\\n\");\n+}\n+\n+#define FATAL(fmt, ...) do { \\\n+    ERROR(fmt, ##__VA_ARGS__); \\\n+    exit(-1); \\\n+} while (0)\n+\n+ssize_t g_input_size = 0;\n+char* g_input_name = NULL;\n+void* g_input_data = MAP_FAILED;\n+char* g_output_dir = NULL;\n+char* g_output_path = NULL;\n+size_t g_output_path_size = 0;\n+pf_key_t g_wrap_key;\n+pf_key_t g_meta_key;\n+\n+static pf_iv_t g_empty_iv = {0};\n+\n+static void derive_main_key(const pf_key_t* kdk, const pf_keyid_t* key_id, pf_key_t* out_key) {\n+    kdf_input_t buf = {0};\n+    pf_status_t status;\n+\n+    buf.index = 1;\n+    strncpy(buf.label, METADATA_KEY_NAME, MAX_LABEL_SIZE);\n+    memcpy(&buf.nonce, key_id, sizeof(buf.nonce));\n+    buf.output_len = 0x80;\n+\n+    status = mbedtls_aes_gcm_encrypt(kdk, &g_empty_iv, &buf, sizeof(buf), NULL, 0, NULL, out_key);\n+    if (PF_FAILURE(status))\n+        FATAL(\"key derivation failed\\n\");\n+}\n+\n+static void make_output_path(const char* suffix) {\n+    snprintf(g_output_path, g_output_path_size, \"%s\/%s.%s\", g_output_dir, g_input_name, suffix);\n+    INFO(\"[*] %s\\n\", g_output_path);\n+}\n+\n+\/* PF layout (node size is PF_NODE_SIZE):\n+ * - Node 0: metadata (metadata_node_t)\n+ *   - metadata_plain_t\n+ *   - metadata_encrypted_t (may include MD_USER_DATA_SIZE bytes of data)\n+ *   - metadata_padding_t\n+ * - Node 1: MHT (mht_node_t)\n+ * - Node 2-97: data (ATTACHED_DATA_NODES_COUNT == 96)\n+ * - Node 98: MHT\n+ * - Node 99-195: data\n+ * - ...\n+ *\/\n+static void truncate_file(const char* suffix, size_t output_size) {\n+    int ret;\n+\n+    make_output_path(suffix);\n+\n+    if (output_size < g_input_size) {\n+        ret = write_file(g_output_path, output_size, g_input_data);\n+    } else {\n+        ret = write_file(g_output_path, g_input_size, g_input_data);\n+        if (ret < 0)\n+            goto out;\n+        ret = truncate(g_output_path, output_size);\n+    }\n+out:\n+    if (ret < 0)\n+        FATAL(\"truncate_file failed: %d\\n\", ret);\n+}\n+\n+#define FIELD_SIZEOF(t, f) (sizeof(((t*)0)->f))\n+#define FIELD_TRUNCATED(t, f) (offsetof(t, f) + (FIELD_SIZEOF(t, f) \/ 2))\n+#define DATA_CRYPTO_SIZE (FIELD_SIZEOF(mht_node_t, data_nodes_crypto))\n+\n+static void tamper_truncate(void) {\n+    size_t mdps = sizeof(metadata_plain_t);\n+    DBG(\"size(metadata_plain_t)             = 0x%04lx\\n\", sizeof(metadata_plain_t));\n+    DBG(\"metadata_plain_t.file_id           : 0x%04lx (0x%04lx)\\n\",\n+        offsetof(metadata_plain_t, file_id), FIELD_SIZEOF(metadata_plain_t, file_id));\n+    DBG(\"metadata_plain_t.major_version     : 0x%04lx (0x%04lx)\\n\",\n+        offsetof(metadata_plain_t, major_version), FIELD_SIZEOF(metadata_plain_t, major_version));\n+    DBG(\"metadata_plain_t.minor_version     : 0x%04lx (0x%04lx)\\n\",\n+        offsetof(metadata_plain_t, minor_version), FIELD_SIZEOF(metadata_plain_t, minor_version));\n+    DBG(\"metadata_plain_t.metadata_key_id   : 0x%04lx (0x%04lx)\\n\",\n+        offsetof(metadata_plain_t, metadata_key_id),\n+        FIELD_SIZEOF(metadata_plain_t, metadata_key_id));\n+    DBG(\"metadata_plain_t.metadata_gmac     : 0x%04lx (0x%04lx)\\n\",\n+        offsetof(metadata_plain_t, metadata_gmac),\n+        FIELD_SIZEOF(metadata_plain_t, metadata_gmac));\n+\n+    DBG(\"size(metadata_encrypted_t)         = 0x%04lx\\n\", sizeof(metadata_encrypted_t));\n+    DBG(\"metadata_encrypted_t.path          : 0x%04lx (0x%04lx)\\n\",\n+        mdps + offsetof(metadata_encrypted_t, path),\n+        FIELD_SIZEOF(metadata_encrypted_t, path));\n+    DBG(\"metadata_encrypted_t.size          : 0x%04lx (0x%04lx)\\n\",\n+        mdps + offsetof(metadata_encrypted_t, size), FIELD_SIZEOF(metadata_encrypted_t, size));\n+    DBG(\"metadata_encrypted_t.mht_key       : 0x%04lx (0x%04lx)\\n\",\n+        mdps + offsetof(metadata_encrypted_t, mht_key),\n+        FIELD_SIZEOF(metadata_encrypted_t, mht_key));\n+    DBG(\"metadata_encrypted_t.mht_gmac      : 0x%04lx (0x%04lx)\\n\",\n+        mdps + offsetof(metadata_encrypted_t, mht_gmac),\n+        FIELD_SIZEOF(metadata_encrypted_t, mht_gmac));\n+    DBG(\"metadata_encrypted_t.data          : 0x%04lx (0x%04lx)\\n\",\n+        mdps + offsetof(metadata_encrypted_t, data), FIELD_SIZEOF(metadata_encrypted_t, data));\n+\n+    DBG(\"size(metadata_padding_t)           = 0x%04lx\\n\", sizeof(metadata_padding_t));\n+    DBG(\"metadata_padding_t                 : 0x%04lx (0x%04lx)\\n\",\n+        mdps + sizeof(metadata_encrypted_t), sizeof(metadata_padding_t));\n+\n+    \/* node 0: metadata + 3k of user data *\/\n+    \/* plain metadata *\/\n+    truncate_file(\"trunc_meta_plain_0\", 0);\n+    truncate_file(\"trunc_meta_plain_1\", FIELD_TRUNCATED(metadata_plain_t, file_id));\n+    truncate_file(\"trunc_meta_plain_2\", offsetof(metadata_plain_t, major_version));\n+    truncate_file(\"trunc_meta_plain_3\", offsetof(metadata_plain_t, minor_version));\n+    truncate_file(\"trunc_meta_plain_4\", offsetof(metadata_plain_t, metadata_key_id));\n+    truncate_file(\"trunc_meta_plain_5\", FIELD_TRUNCATED(metadata_plain_t, metadata_key_id));\n+    truncate_file(\"trunc_meta_plain_6\", offsetof(metadata_plain_t, metadata_gmac));\n+    truncate_file(\"trunc_meta_plain_7\", FIELD_TRUNCATED(metadata_plain_t, metadata_gmac));\n+\n+    \/* encrypted metadata *\/\n+    truncate_file(\"trunc_meta_enc_0\", mdps + offsetof(metadata_encrypted_t, path));\n+    truncate_file(\"trunc_meta_enc_1\", mdps + FIELD_TRUNCATED(metadata_encrypted_t, path));\n+    truncate_file(\"trunc_meta_enc_2\", mdps + offsetof(metadata_encrypted_t, size));\n+    truncate_file(\"trunc_meta_enc_3\", mdps + FIELD_TRUNCATED(metadata_encrypted_t, size));\n+    truncate_file(\"trunc_meta_enc_4\", mdps + offsetof(metadata_encrypted_t, mht_key));\n+    truncate_file(\"trunc_meta_enc_5\", mdps + FIELD_TRUNCATED(metadata_encrypted_t, mht_key));\n+    truncate_file(\"trunc_meta_enc_6\", mdps + offsetof(metadata_encrypted_t, mht_gmac));\n+    truncate_file(\"trunc_meta_enc_7\", mdps + FIELD_TRUNCATED(metadata_encrypted_t, mht_gmac));\n+    truncate_file(\"trunc_meta_enc_8\", mdps + offsetof(metadata_encrypted_t, data));\n+    truncate_file(\"trunc_meta_enc_9\", mdps + FIELD_TRUNCATED(metadata_encrypted_t, data));\n+\n+    \/* padding *\/\n+    truncate_file(\"trunc_meta_pad_0\", mdps + sizeof(metadata_encrypted_t));\n+    truncate_file(\"trunc_meta_pad_1\", mdps + sizeof(metadata_encrypted_t)\n+                  + sizeof(metadata_padding_t) \/ 2);\n+\n+    \/* node 1: mht root *\/\n+    \/* after node 0 *\/\n+    truncate_file(\"trunc_mht_0\", PF_NODE_SIZE);\n+    \/* middle of data_nodes_crypto[0].key *\/\n+    truncate_file(\"trunc_mht_1\", PF_NODE_SIZE + PF_KEY_SIZE \/ 2);\n+    \/* after data_nodes_crypto[0].key *\/\n+    truncate_file(\"trunc_mht_2\", PF_NODE_SIZE + PF_KEY_SIZE);\n+    \/* middle of data_nodes_crypto[0].gmac *\/\n+    truncate_file(\"trunc_mht_3\", PF_NODE_SIZE + PF_KEY_SIZE + PF_MAC_SIZE \/ 2);\n+    \/* after data_nodes_crypto[0].gmac *\/\n+    truncate_file(\"trunc_mht_4\", PF_NODE_SIZE + PF_KEY_SIZE + PF_MAC_SIZE);\n+    \/* after data_nodes_crypto *\/\n+    truncate_file(\"trunc_mht_5\", PF_NODE_SIZE + DATA_CRYPTO_SIZE);\n+    \/* middle of mht_nodes_crypto[0].key *\/\n+    truncate_file(\"trunc_mht_6\", PF_NODE_SIZE + DATA_CRYPTO_SIZE + PF_KEY_SIZE \/ 2);\n+    \/* after mht_nodes_crypto[0].key *\/\n+    truncate_file(\"trunc_mht_7\", PF_NODE_SIZE + DATA_CRYPTO_SIZE + PF_KEY_SIZE);\n+    \/* middle of mht_nodes_crypto[0].gmac *\/\n+    truncate_file(\"trunc_mht_8\", PF_NODE_SIZE + DATA_CRYPTO_SIZE + PF_KEY_SIZE + PF_MAC_SIZE \/ 2);\n+    \/* after mht_nodes_crypto[0].gmac *\/\n+    truncate_file(\"trunc_mht_9\", PF_NODE_SIZE + DATA_CRYPTO_SIZE + PF_KEY_SIZE + PF_MAC_SIZE);\n+\n+    \/* node 2-3: data #0, #1 *\/\n+    \/* after mht root *\/\n+    truncate_file(\"trunc_data_0\", 2 * PF_NODE_SIZE);\n+    \/* middle of data #0 *\/\n+    truncate_file(\"trunc_data_1\", 2 * PF_NODE_SIZE + PF_NODE_SIZE \/ 2);\n+    \/* after data #0 *\/\n+    truncate_file(\"trunc_data_2\", 3 * PF_NODE_SIZE);\n+    \/* middle of data #1 *\/\n+    truncate_file(\"trunc_data_3\", 3 * PF_NODE_SIZE + PF_NODE_SIZE \/ 2);\n+\n+    \/* extend *\/\n+    truncate_file(\"extend_0\", g_input_size + 1);\n+    truncate_file(\"extend_1\", g_input_size + PF_NODE_SIZE \/ 2);\n+    truncate_file(\"extend_2\", g_input_size + PF_NODE_SIZE);\n+    truncate_file(\"extend_3\", g_input_size + PF_NODE_SIZE + PF_NODE_SIZE \/ 2);\n+}\n+\n+\/* returns mmap'd output contents *\/\n+static void* create_output(const char* path) {\n+    void* mem = MAP_FAILED;\n+    int fd = open(path, O_RDWR|O_CREAT, 0664);\n+    if (fd < 0)\n+        FATAL(\"Failed to open output file '%s': %s\\n\", path, strerror(errno));\n+\n+    if (ftruncate(fd, g_input_size) < 0)\n+        FATAL(\"Failed to ftruncate output file '%s': %s\\n\", path, strerror(errno));\n+\n+    mem = mmap(NULL, g_input_size, PROT_READ|PROT_WRITE, MAP_SHARED, fd, 0);\n+    if (mem == MAP_FAILED)\n+        FATAL(\"Failed to mmap output file '%s': %s\\n\", path, strerror(errno));\n+\n+    memcpy(mem, g_input_data, g_input_size);\n+\n+    close(fd);\n+    return mem;\n+}\n+\n+static void pf_decrypt(const void* encrypted, size_t size, const pf_key_t* key, const pf_mac_t* mac,\n+                       void* decrypted, const char* msg) {\n+    pf_status_t status = mbedtls_aes_gcm_decrypt(key, &g_empty_iv, NULL, 0,\n+                                                 encrypted, size,\n+                                                 decrypted, mac);\n+    if (PF_FAILURE(status))\n+        FATAL(\"decrypting %s failed\\n\", msg);\n+}\n+\n+static void pf_encrypt(const void* decrypted, size_t size, const pf_key_t* key, pf_mac_t* mac,\n+                       void* encrypted, const char* msg) {\n+    pf_status_t status = mbedtls_aes_gcm_encrypt(key, &g_empty_iv, NULL, 0,\n+                                                 decrypted, size,\n+                                                 encrypted, mac);\n+    if (PF_FAILURE(status))\n+        FATAL(\"encrypting %s failed\\n\", msg);\n+}\n+\n+\/* copy input PF and apply some modifications *\/\n+#define __BREAK_PF(suffix, ...) do { \\\n+    make_output_path(suffix); \\\n+    meta = create_output(g_output_path); \\\n+    out = (uint8_t*)meta; \\\n+    pf_decrypt(&meta->encrypted_part, sizeof(meta->encrypted_part), &g_meta_key, \\\n+               &meta->plain_part.metadata_gmac, meta_dec, \"metadata\"); \\\n+    mht_enc = (mht_node_t*)(out + PF_NODE_SIZE); \\\n+    pf_decrypt(mht_enc, sizeof(*mht_enc), &meta_dec->mht_key, &meta_dec->mht_gmac, mht_dec, \\\n+               \"mht\"); \\\n+    __VA_ARGS__ \\\n+    munmap(meta, g_input_size); \\\n+} while (0)\n+\n+\/* if update is true, also create a file with correct metadata MAC *\/\n+#define BREAK_PF(suffix, update, ...) do { \\\n+    __BREAK_PF(suffix, __VA_ARGS__); \\\n+    if (update) { \\\n+        __BREAK_PF(suffix \"_fixed\", __VA_ARGS__ { \\\n+                       pf_encrypt(meta_dec, sizeof(*meta_dec), &g_meta_key, \\\n+                                  &meta->plain_part.metadata_gmac, meta->encrypted_part, \\\n+                                  \"metadata\"); \\\n+                   } ); \\\n+    } \\\n+} while (0)\n+\n+#define BREAK_MHT(suffix, ...) do { \\\n+    __BREAK_PF(suffix, __VA_ARGS__ { \\\n+                   pf_encrypt(mht_dec, sizeof(*mht_dec), &meta_dec->mht_key, &meta_dec->mht_gmac, \\\n+                              mht_enc, \"mht\"); \\\n+               } ); \\\n+} while (0)\n+\n+#define LAST_BYTE(array) (((uint8_t*)&array)[sizeof(array) - 1])\n+\n+static void tamper_modify(void) {\n+    metadata_node_t* meta = NULL;\n+    uint8_t* out = NULL;\n+    metadata_encrypted_t* meta_dec = malloc(sizeof(*meta_dec));\n+    if (!meta_dec)\n+        FATAL(\"Out of memory\\n\");\n+    mht_node_t* mht_enc = NULL;\n+    mht_node_t* mht_dec = malloc(sizeof(*mht_dec));\n+    if (!mht_dec)\n+        FATAL(\"Out of memory\\n\");\n+\n+    \/* plain part of the metadata isn't covered by the MAC so no point updating it *\/\n+    BREAK_PF(\"meta_plain_id_0\", \/*update=*\/false,\n+             { meta->plain_part.file_id = 0; });\n+    BREAK_PF(\"meta_plain_id_1\", \/*update=*\/false,\n+             { meta->plain_part.file_id = UINT64_MAX; });\n+    BREAK_PF(\"meta_plain_version_0\", \/*update=*\/false,\n+             { meta->plain_part.major_version = 0; });\n+    BREAK_PF(\"meta_plain_version_1\", \/*update=*\/false,\n+             { meta->plain_part.major_version = 0xff; });\n+    BREAK_PF(\"meta_plain_version_2\", \/*update=*\/false,\n+             { meta->plain_part.minor_version = 0xff; });\n+\n+    \/* metadata_key_id is the keying material for encrypted metadata key derivation, so create also\n+     * PFs with updated MACs *\/\n+    BREAK_PF(\"meta_plain_keyid_0\", \/*update=*\/true,\n+             { meta->plain_part.metadata_key_id[0] ^= 1; });\n+    BREAK_PF(\"meta_plain_keyid_1\", \/*update=*\/true,\n+             { LAST_BYTE(meta->plain_part.metadata_key_id) ^= 0xfe; });\n+    BREAK_PF(\"meta_plain_mac_0\", \/*update=*\/true,\n+             { meta->plain_part.metadata_gmac[0] ^= 0xfe; });\n+    BREAK_PF(\"meta_plain_mac_1\", \/*update=*\/true,\n+             { LAST_BYTE(meta->plain_part.metadata_gmac) &= 1; });\n+\n+    BREAK_PF(\"meta_enc_filename_0\", \/*update=*\/true,\n+             { meta_dec->path[0] = 0; });\n+    BREAK_PF(\"meta_enc_filename_1\", \/*update=*\/true,\n+             { meta_dec->path[0] ^= 1; });\n+    BREAK_PF(\"meta_enc_filename_2\", \/*update=*\/true,\n+             { LAST_BYTE(meta_dec->path) ^= 0xfe; });\n+    BREAK_PF(\"meta_enc_size_0\", \/*update=*\/true,\n+             { meta_dec->size = 0; });\n+    BREAK_PF(\"meta_enc_size_1\", \/*update=*\/true,\n+             { meta_dec->size = g_input_size - 1; });\n+    BREAK_PF(\"meta_enc_size_2\", \/*update=*\/true,\n+             { meta_dec->size = g_input_size + 1; });\n+    BREAK_PF(\"meta_enc_size_3\", \/*update=*\/true,\n+             { meta_dec->size = UINT64_MAX; });\n+    BREAK_PF(\"meta_enc_mht_key_0\", \/*update=*\/true,\n+             { meta_dec->mht_key[0] ^= 1; });\n+    BREAK_PF(\"meta_enc_mht_key_1\", \/*update=*\/true,\n+             { LAST_BYTE(meta_dec->mht_key) ^= 0xfe; });\n+    BREAK_PF(\"meta_enc_mht_mac_0\", \/*update=*\/true,\n+             { meta_dec->mht_gmac[0] ^= 1; });\n+    BREAK_PF(\"meta_enc_mht_mac_1\", \/*update=*\/true,\n+             { LAST_BYTE(meta_dec->mht_gmac) ^= 0xfe; });\n+    BREAK_PF(\"meta_enc_data_0\", \/*update=*\/true,\n+             { meta_dec->data[0] ^= 0xfe; });\n+    BREAK_PF(\"meta_enc_data_1\", \/*update=*\/true,\n+             { LAST_BYTE(meta_dec->data) ^= 1; });\n+\n+    \/* padding is ignored *\/\n+    BREAK_PF(\"meta_padding_0\", \/*update=*\/false,\n+             { meta->padding[0] ^= 1; });\n+    BREAK_PF(\"meta_padding_1\", \/*update=*\/false,\n+             { LAST_BYTE(meta->padding) ^= 0xfe; });\n+\n+    BREAK_MHT(\"mht_0\", { mht_dec->data_nodes_crypto[0].key[0] ^= 1; });\n+    BREAK_MHT(\"mht_1\", { mht_dec->data_nodes_crypto[0].gmac[0] ^= 1; });\n+    BREAK_MHT(\"mht_2\", { mht_dec->mht_nodes_crypto[0].key[0] ^= 1; });\n+    BREAK_MHT(\"mht_3\", { mht_dec->mht_nodes_crypto[0].gmac[0] ^= 1; });\n+    BREAK_MHT(\"mht_4\", { mht_dec->data_nodes_crypto[ATTACHED_DATA_NODES_COUNT - 1].key[0] ^= 1; });\n+    BREAK_MHT(\"mht_5\", { mht_dec->data_nodes_crypto[ATTACHED_DATA_NODES_COUNT - 1].gmac[0] ^= 1; });\n+    BREAK_MHT(\"mht_6\", { mht_dec->mht_nodes_crypto[CHILD_MHT_NODES_COUNT - 1].key[0] ^= 1; });\n+    BREAK_MHT(\"mht_7\", { mht_dec->mht_nodes_crypto[CHILD_MHT_NODES_COUNT - 1].gmac[0] ^= 1; });\n+    BREAK_MHT(\"mht_8\", {\n+        gcm_crypto_data_t crypto;\n+        memcpy(&crypto, &mht_dec->data_nodes_crypto[0], sizeof(crypto));\n+        memcpy(&mht_dec->data_nodes_crypto[0], &mht_dec->data_nodes_crypto[1], sizeof(crypto));\n+        memcpy(&mht_dec->data_nodes_crypto[1], &crypto, sizeof(crypto));\n+    });\n+    BREAK_MHT(\"mht_9\", {\n+        gcm_crypto_data_t crypto;\n+        memcpy(&crypto, &mht_dec->mht_nodes_crypto[0], sizeof(crypto));\n+        memcpy(&mht_dec->mht_nodes_crypto[0], &mht_dec->mht_nodes_crypto[1], sizeof(crypto));\n+        memcpy(&mht_dec->mht_nodes_crypto[1], &crypto, sizeof(crypto));\n+    });\n+\n+    \/* data nodes start from node #2 *\/\n+    BREAK_PF(\"data_0\", \/*update=*\/false,\n+             { *(out + 2 * PF_NODE_SIZE) ^= 1; });\n+    BREAK_PF(\"data_1\", \/*update=*\/false,\n+             { *(out + 3 * PF_NODE_SIZE - 1) ^= 1; });\n+    BREAK_PF(\"data_2\", \/*update=*\/false, {\n+        \/* swap data nodes *\/\n+        memcpy(out + 2 * PF_NODE_SIZE, g_input_data + 3 * PF_NODE_SIZE, PF_NODE_SIZE);\n+        memcpy(out + 3 * PF_NODE_SIZE, g_input_data + 2 * PF_NODE_SIZE, PF_NODE_SIZE);\n+    });\n+\n+    free(mht_dec);\n+    free(meta_dec);\n+}\n+\n+int main(int argc, char* argv[]) {\n+    int ret = -1;\n+\n+    int option          = 0;\n+    char* input_path    = NULL;\n+    char* wrap_key_path = NULL;\n+    int input_fd        = -1;\n+\n+    while (true) {\n+        option = getopt_long(argc, argv, \"i:o:w:vh\", g_options, NULL);\n+        if (option == -1)\n+            break;\n+\n+        switch (option) {\n+            case 'i':\n+                input_path = optarg;\n+                break;\n+            case 'o':\n+                g_output_dir = optarg;\n+                break;\n+            case 'w':\n+                wrap_key_path = optarg;\n+                break;\n+            case 'v':\n+                set_verbose(true);\n+                break;\n+            case 'h':\n+                usage();\n+                return 0;\n+            default:\n+                ERROR(\"Unknown option: %c\\n\", option);\n+                usage();\n+        }\n+    }\n+\n+    if (!input_path) {\n+        ERROR(\"Input path not specified\\n\");\n+        usage();\n+        goto out;\n+    }\n+\n+    if (!g_output_dir) {\n+        ERROR(\"Output path not specified\\n\");\n+        usage();\n+        goto out;\n+    }\n+\n+    if (!wrap_key_path) {\n+        ERROR(\"Wrap key path not specified\\n\");\n+        usage();\n+        goto out;\n+    }\n+\n+    input_fd = open(input_path, O_RDONLY);\n+    if (input_fd < 0) {\n+        ERROR(\"Failed to open input file '%s': %s\\n\", input_path, strerror(errno));\n+        goto out;\n+    }\n+\n+    g_input_size = get_file_size(input_fd);\n+    if (g_input_size < 0) {\n+        ERROR(\"Failed to stat input file '%s': %s\\n\", input_path, strerror(errno));\n+        goto out;\n+    }\n+\n+    g_input_data = mmap(NULL, g_input_size, PROT_READ, MAP_PRIVATE, input_fd, 0);\n+    if (g_input_data == MAP_FAILED) {\n+        ERROR(\"Failed to mmap input file '%s': %s\\n\", input_path, strerror(errno));\n+        goto out;\n+    }\n+\n+    load_wrap_key(wrap_key_path, &g_wrap_key);\n+    derive_main_key(&g_wrap_key, &((metadata_plain_t*)g_input_data)->metadata_key_id,\n+                    &g_meta_key);\n+\n+    g_input_name = basename(input_path);\n+    g_output_path_size = strlen(g_input_name) + strlen(g_output_dir) + 256;\n+    g_output_path = malloc(g_output_path_size);\n+    if (!g_output_path) {\n+        ERROR(\"No memory\\n\");\n+        goto out;\n+    }\n+\n+    tamper_truncate();\n+    tamper_modify();\n+    ret = 0;\n+\n+out:\n+    \/* skip cleanup as we are in main() *\/\n+    return ret;\n+}\n","files":{"\/LibOS\/shim\/test\/fs\/test_pf.py":{"changes":[{"diff":"\n \n     def __corrupt_file(self, input_path, output_path):\n         cmd = [self.PF_TAMPER, '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path]\n-        return self.run_native_binary(cmd)\n+        return self.run_native_binary(cmd, libpath=os.path.join(os.getcwd(), 'lib'))\n \n     # invalid\/corrupted files\n-    @expectedFailureIf(HAS_SGX)\n-    # pylint: disable=fixme\n     def test_500_invalid(self):\n-        # TODO: port these to the new file format\n         invalid_dir = os.path.join(self.TEST_DIR, 'pf_invalid')\n-        # files below should work normally (benign modifications)\n-        should_pass = ['chunk_padding_1_fixed', 'chunk_padding_2_fixed', 'chunk_data_3',\n-                       'chunk_data_3_fixed', 'chunk_data_4', 'chunk_data_4_fixed']\n         if not os.path.exists(invalid_dir):\n             os.mkdir(invalid_dir)\n+\n         # prepare valid encrypted file (largest one for maximum possible corruptions)\n         original_input = self.OUTPUT_FILES[-1]\n         self.__encrypt_file(self.INPUT_FILES[-1], original_input)\n         # generate invalid files based on the above\n         self.__corrupt_file(original_input, invalid_dir)\n+\n         # try to decrypt invalid files\n         for name in os.listdir(invalid_dir):\n             invalid = os.path.join(invalid_dir, name)\n","add":3,"remove":7,"filename":"\/LibOS\/shim\/test\/fs\/test_pf.py","badparts":["        return self.run_native_binary(cmd)","    @expectedFailureIf(HAS_SGX)","        should_pass = ['chunk_padding_1_fixed', 'chunk_padding_2_fixed', 'chunk_data_3',","                       'chunk_data_3_fixed', 'chunk_data_4', 'chunk_data_4_fixed']"],"goodparts":["        return self.run_native_binary(cmd, libpath=os.path.join(os.getcwd(), 'lib'))"]},{"diff":"\n             input_path = os.path.join(invalid_dir, os.path.basename(original_input))\n             # copy the file so it has the original file name (for allowed path check)\n             shutil.copy(invalid, input_path)\n-            should_pass = any(s in name for s in should_pass)\n \n             try:\n                 args = ['decrypt', '-V', '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path]\n                 self.__pf_crypt(args)\n             except subprocess.CalledProcessError as exc:\n-                if should_pass:\n-                    self.assertEqual(exc.returncode, 0)\n-                else:\n-                    self.assertNotEqual(exc.returncode, 0)\n+                # decryption of invalid file must fail with -1 (wrapped to 255)\n+                self.assertEqual(exc.returncode, 255)\n             else:\n-                if not should_pass:\n-                    print('[!] Fail: successfully decrypted file: ' + name)\n-                    self.fail()\n+                print('[!] Fail: successfully decrypted file: ' + name)\n+                self.fail()","add":4,"remove":8,"filename":"\/LibOS\/shim\/test\/fs\/test_pf.py","badparts":["            should_pass = any(s in name for s in should_pass)","                if should_pass:","                    self.assertEqual(exc.returncode, 0)","                else:","                    self.assertNotEqual(exc.returncode, 0)","                if not should_pass:","                    print('[!] Fail: successfully decrypted file: ' + name)","                    self.fail()"],"goodparts":["                self.assertEqual(exc.returncode, 255)","                print('[!] Fail: successfully decrypted file: ' + name)","                self.fail()"]}],"source":"\n import filecmp import os import shutil import subprocess import unittest from test_fs import( TC_00_FileSystem, ) from regression import( HAS_SGX, expectedFailureIf, ) @unittest.skipUnless(HAS_SGX, 'Protected files require SGX support') class TC_50_ProtectedFiles(TC_00_FileSystem): @classmethod def setUpClass(cls): cls.PF_CRYPT='bin\/pf_crypt' cls.PF_TAMPER='bin\/pf_tamper' cls.WRAP_KEY=os.path.join(cls.TEST_DIR, 'wrap-key') cls.CONST_WRAP_KEY=[0xff, 0xee, 0xdd, 0xcc, 0xbb, 0xaa, 0x99, 0x88, 0x77, 0x66, 0x55, 0x44, 0x33, 0x22, 0x11, 0x00] cls.ENCRYPTED_DIR=os.path.join(cls.TEST_DIR, 'pf_input') cls.ENCRYPTED_FILES=[os.path.join(cls.ENCRYPTED_DIR, str(v)) for v in cls.FILE_SIZES] cls.LIB_PATH=os.path.join(os.getcwd(), 'lib') super().setUpClass() if not os.path.exists(cls.ENCRYPTED_DIR): os.mkdir(cls.ENCRYPTED_DIR) cls.OUTPUT_DIR=os.path.join(cls.TEST_DIR, 'pf_output') cls.OUTPUT_FILES=[os.path.join(cls.OUTPUT_DIR, str(x)) for x in cls.FILE_SIZES] cls.__set_default_key(cls) for i in cls.INDEXES: cmd=[cls.PF_CRYPT, 'encrypt', '-w', cls.WRAP_KEY, '-i', cls.INPUT_FILES[i], '-o', cls.ENCRYPTED_FILES[i]] cls.run_native_binary(cmd, libpath=os.path.join(os.getcwd(), 'lib')) def __pf_crypt(self, args): args.insert(0, self.PF_CRYPT) return self.run_native_binary(args, libpath=os.path.join(os.getcwd(), 'lib')) def __set_default_key(self): with open(self.WRAP_KEY, 'wb') as file: file.write(bytes(self.CONST_WRAP_KEY)) def copy_input(self, input_path, output_path): self.__encrypt_file(input_path, output_path) def __encrypt_file(self, input_path, output_path): args=['encrypt', '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path] stdout, stderr=self.__pf_crypt(args) return(stdout, stderr) def __decrypt_file(self, input_path, output_path): args=['decrypt', '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path] stdout, stderr=self.__pf_crypt(args) return(stdout, stderr) def test_000_gen_key(self): key_path=os.path.join(self.TEST_DIR, 'tmpkey') args=['gen-key', '-w', key_path] stdout, _=self.__pf_crypt(args) self.assertIn('Wrap key saved to: ' +key_path, stdout) self.assertEqual(os.path.getsize(key_path), 16) os.remove(key_path) def test_010_encrypt_decrypt(self): for i in self.INDEXES: self.__encrypt_file(self.INPUT_FILES[i], self.OUTPUT_FILES[i]) self.assertFalse(filecmp.cmp(self.INPUT_FILES[i], self.OUTPUT_FILES[i], shallow=False)) dec_path=os.path.join(self.OUTPUT_DIR, os.path.basename(self.OUTPUT_FILES[i]) +'.dec') self.__decrypt_file(self.OUTPUT_FILES[i], dec_path) self.assertTrue(filecmp.cmp(self.INPUT_FILES[i], dec_path, shallow=False)) def test_100_open_close(self): input_path=self.ENCRYPTED_FILES[-1] output_path=os.path.join(self.OUTPUT_DIR, 'test_100') stdout, stderr=self.run_binary(['open_close', 'R', input_path]) self.verify_open_close(stdout, stderr, input_path, 'input') try: stdout, stderr=self.run_binary(['open_close', 'W', output_path]) self.assertIn('ERROR: Failed to open output file', stderr) except subprocess.CalledProcessError as exc: self.assertNotEqual(exc.returncode, 0) self.assertTrue(os.path.isfile(output_path)) else: print('[!] Fail: open_close returned 0') self.fail() def test_101_open_flags(self): file_path=os.path.join(self.OUTPUT_DIR, 'test_101') stdout, stderr=self.run_binary(['open_flags', file_path]) self.verify_open_flags(stdout, stderr) def test_115_seek_tell(self): plaintext_path=self.INPUT_FILES[-1] input_path=self.ENCRYPTED_FILES[-1] output_path_1=os.path.join(self.OUTPUT_DIR, 'test_115a') output_path_2=os.path.join(self.OUTPUT_DIR, 'test_115b') self.copy_input(plaintext_path, output_path_1) self.copy_input(plaintext_path, output_path_2) stdout, stderr=self.run_binary(['seek_tell', input_path, output_path_1, output_path_2]) self.verify_seek_tell(stdout, stderr, input_path, output_path_1, output_path_2, self.FILE_SIZES[-1]) def test_130_file_stat(self): for i in self.INDEXES: input_path=self.ENCRYPTED_FILES[i] output_path=self.OUTPUT_FILES[i] size=str(self.FILE_SIZES[i]) self.copy_input(self.INPUT_FILES[i], output_path) stdout, stderr=self.run_binary(['stat', input_path, output_path]) self.verify_stat(stdout, stderr, input_path, output_path, size) def verify_size(self, file, size): dec_path=os.path.join(self.OUTPUT_DIR, os.path.basename(file) +'.dec') self.__decrypt_file(file, dec_path) self.assertEqual(os.stat(dec_path).st_size, size) @expectedFailureIf(HAS_SGX) def test_140_file_truncate(self): self.fail() def test_150_file_rename(self): path1=os.path.join(self.OUTPUT_DIR, 'test_150a') path2=os.path.join(self.OUTPUT_DIR, 'test_150b') self.copy_input(self.ENCRYPTED_FILES[-1], path1) shutil.copy(path1, path2) args=['decrypt', '-V', '-w', self.WRAP_KEY, '-i', path2, '-o', path1] try: self.__pf_crypt(args) except subprocess.CalledProcessError as exc: self.assertNotEqual(exc.returncode, 0) else: print('[!] Fail: successfully decrypted renamed file: ' +path2) self.fail() def verify_copy_content(self, input_path, output_path): dec_path=os.path.join(self.OUTPUT_DIR, os.path.basename(output_path) +'.dec') self.__decrypt_file(output_path, dec_path) self.assertTrue(filecmp.cmp(input_path, dec_path, shallow=False)) def do_copy_test(self, executable, timeout): stdout, stderr=self.run_binary([executable, self.ENCRYPTED_DIR, self.OUTPUT_DIR], timeout=timeout) self.verify_copy(stdout, stderr, self.ENCRYPTED_DIR, executable) def test_204_copy_dir_mmap_whole(self): self.do_copy_test('copy_mmap_whole', 30) def test_205_copy_dir_mmap_seq(self): self.do_copy_test('copy_mmap_seq', 60) def test_206_copy_dir_mmap_rev(self): self.do_copy_test('copy_mmap_rev', 60) def test_210_copy_dir_mounted(self): executable='copy_whole' stdout, stderr=self.run_binary([executable, '\/mounted\/pf_input', '\/mounted\/pf_output'], timeout=30) self.verify_copy(stdout, stderr, '\/mounted\/pf_input', executable) def __corrupt_file(self, input_path, output_path): cmd=[self.PF_TAMPER, '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path] return self.run_native_binary(cmd) @expectedFailureIf(HAS_SGX) def test_500_invalid(self): invalid_dir=os.path.join(self.TEST_DIR, 'pf_invalid') should_pass=['chunk_padding_1_fixed', 'chunk_padding_2_fixed', 'chunk_data_3', 'chunk_data_3_fixed', 'chunk_data_4', 'chunk_data_4_fixed'] if not os.path.exists(invalid_dir): os.mkdir(invalid_dir) original_input=self.OUTPUT_FILES[-1] self.__encrypt_file(self.INPUT_FILES[-1], original_input) self.__corrupt_file(original_input, invalid_dir) for name in os.listdir(invalid_dir): invalid=os.path.join(invalid_dir, name) output_path=os.path.join(self.OUTPUT_DIR, name) input_path=os.path.join(invalid_dir, os.path.basename(original_input)) shutil.copy(invalid, input_path) should_pass=any(s in name for s in should_pass) try: args=['decrypt', '-V', '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path] self.__pf_crypt(args) except subprocess.CalledProcessError as exc: if should_pass: self.assertEqual(exc.returncode, 0) else: self.assertNotEqual(exc.returncode, 0) else: if not should_pass: print('[!] Fail: successfully decrypted file: ' +name) self.fail() ","sourceWithComments":"#!\/usr\/bin\/env python3\n\nimport filecmp\nimport os\nimport shutil\nimport subprocess\nimport unittest\n\nfrom test_fs import (\n    TC_00_FileSystem,\n)\n\nfrom regression import (\n    HAS_SGX,\n    expectedFailureIf,\n)\n\n@unittest.skipUnless(HAS_SGX, 'Protected files require SGX support')\nclass TC_50_ProtectedFiles(TC_00_FileSystem):\n    @classmethod\n    def setUpClass(cls):\n        cls.PF_CRYPT = 'bin\/pf_crypt'\n        cls.PF_TAMPER = 'bin\/pf_tamper'\n        cls.WRAP_KEY = os.path.join(cls.TEST_DIR, 'wrap-key')\n        # CONST_WRAP_KEY must match the one in manifest\n        cls.CONST_WRAP_KEY = [0xff, 0xee, 0xdd, 0xcc, 0xbb, 0xaa, 0x99, 0x88,\n                              0x77, 0x66, 0x55, 0x44, 0x33, 0x22, 0x11, 0x00]\n        cls.ENCRYPTED_DIR = os.path.join(cls.TEST_DIR, 'pf_input')\n        cls.ENCRYPTED_FILES = [os.path.join(cls.ENCRYPTED_DIR, str(v)) for v in cls.FILE_SIZES]\n        cls.LIB_PATH = os.path.join(os.getcwd(), 'lib')\n\n        super().setUpClass()\n        if not os.path.exists(cls.ENCRYPTED_DIR):\n            os.mkdir(cls.ENCRYPTED_DIR)\n        cls.OUTPUT_DIR = os.path.join(cls.TEST_DIR, 'pf_output')\n        cls.OUTPUT_FILES = [os.path.join(cls.OUTPUT_DIR, str(x)) for x in cls.FILE_SIZES]\n        # create encrypted files\n        cls.__set_default_key(cls)\n        for i in cls.INDEXES:\n            cmd = [cls.PF_CRYPT, 'encrypt', '-w', cls.WRAP_KEY, '-i', cls.INPUT_FILES[i], '-o',\n                   cls.ENCRYPTED_FILES[i]]\n\n            cls.run_native_binary(cmd, libpath=os.path.join(os.getcwd(), 'lib'))\n\n    def __pf_crypt(self, args):\n        args.insert(0, self.PF_CRYPT)\n        return self.run_native_binary(args, libpath=os.path.join(os.getcwd(), 'lib'))\n\n    def __set_default_key(self):\n        with open(self.WRAP_KEY, 'wb') as file:\n            file.write(bytes(self.CONST_WRAP_KEY))\n\n    # overrides TC_00_FileSystem to encrypt the file instead of just copying\n    def copy_input(self, input_path, output_path):\n        self.__encrypt_file(input_path, output_path)\n\n    def __encrypt_file(self, input_path, output_path):\n        args = ['encrypt', '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path]\n        stdout, stderr = self.__pf_crypt(args)\n        return (stdout, stderr)\n\n    def __decrypt_file(self, input_path, output_path):\n        args = ['decrypt', '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path]\n        stdout, stderr = self.__pf_crypt(args)\n        return (stdout, stderr)\n\n    def test_000_gen_key(self):\n        # test random key generation\n        key_path = os.path.join(self.TEST_DIR, 'tmpkey')\n        args = ['gen-key', '-w', key_path]\n        stdout, _ = self.__pf_crypt(args)\n        self.assertIn('Wrap key saved to: ' + key_path, stdout)\n        self.assertEqual(os.path.getsize(key_path), 16)\n        os.remove(key_path)\n\n    def test_010_encrypt_decrypt(self):\n        for i in self.INDEXES:\n            self.__encrypt_file(self.INPUT_FILES[i], self.OUTPUT_FILES[i])\n            self.assertFalse(filecmp.cmp(self.INPUT_FILES[i], self.OUTPUT_FILES[i], shallow=False))\n            dec_path = os.path.join(self.OUTPUT_DIR,\n                                    os.path.basename(self.OUTPUT_FILES[i]) + '.dec')\n            self.__decrypt_file(self.OUTPUT_FILES[i], dec_path)\n            self.assertTrue(filecmp.cmp(self.INPUT_FILES[i], dec_path, shallow=False))\n\n    # overrides TC_00_FileSystem to change input dir (from plaintext to encrypted)\n    def test_100_open_close(self):\n        # the test binary expects a path to read-only (existing) file or a path to file that\n        # will get created\n        input_path = self.ENCRYPTED_FILES[-1] # existing file\n        output_path = os.path.join(self.OUTPUT_DIR, 'test_100') # new file\n        stdout, stderr = self.run_binary(['open_close', 'R', input_path])\n        self.verify_open_close(stdout, stderr, input_path, 'input')\n        # the following test tries to open multiple handles to a single writable PF, should fail\n        try:\n            stdout, stderr = self.run_binary(['open_close', 'W', output_path])\n            self.assertIn('ERROR: Failed to open output file', stderr)\n        except subprocess.CalledProcessError as exc:\n            self.assertNotEqual(exc.returncode, 0)\n            self.assertTrue(os.path.isfile(output_path))\n        else:\n            print('[!] Fail: open_close returned 0')\n            self.fail()\n\n    # overrides TC_00_FileSystem to change input dir (from plaintext to encrypted)\n    def test_101_open_flags(self):\n        # the test binary expects a path to file that will get created\n        file_path = os.path.join(self.OUTPUT_DIR, 'test_101') # new file\n        stdout, stderr = self.run_binary(['open_flags', file_path])\n        self.verify_open_flags(stdout, stderr)\n\n    # overrides TC_00_FileSystem to change input dir (from plaintext to encrypted)\n    def test_115_seek_tell(self):\n        # the test binary expects a path to read-only (existing) file and two paths to files that\n        # will get created\n        plaintext_path = self.INPUT_FILES[-1]\n        input_path = self.ENCRYPTED_FILES[-1] # existing file\n        output_path_1 = os.path.join(self.OUTPUT_DIR, 'test_115a') # writable files\n        output_path_2 = os.path.join(self.OUTPUT_DIR, 'test_115b')\n        self.copy_input(plaintext_path, output_path_1) # encrypt\n        self.copy_input(plaintext_path, output_path_2)\n        stdout, stderr = self.run_binary(['seek_tell', input_path, output_path_1, output_path_2])\n        self.verify_seek_tell(stdout, stderr, input_path, output_path_1, output_path_2,\n                              self.FILE_SIZES[-1])\n\n    # overrides TC_00_FileSystem to change input dir (from plaintext to encrypted)\n    def test_130_file_stat(self):\n        # the test binary expects a path to read-only (existing) file and a path to file that\n        # will get created\n        for i in self.INDEXES:\n            input_path = self.ENCRYPTED_FILES[i]\n            output_path = self.OUTPUT_FILES[i]\n            size = str(self.FILE_SIZES[i])\n            self.copy_input(self.INPUT_FILES[i], output_path)\n            stdout, stderr = self.run_binary(['stat', input_path, output_path])\n            self.verify_stat(stdout, stderr, input_path, output_path, size)\n\n    # overrides TC_00_FileSystem to decrypt output\n    def verify_size(self, file, size):\n        dec_path = os.path.join(self.OUTPUT_DIR, os.path.basename(file) + '.dec')\n        self.__decrypt_file(file, dec_path)\n        self.assertEqual(os.stat(dec_path).st_size, size)\n\n    @expectedFailureIf(HAS_SGX)\n    # pylint: disable=fixme\n    def test_140_file_truncate(self):\n        self.fail() # TODO: port these to the new file format\n\n    def test_150_file_rename(self):\n        path1 = os.path.join(self.OUTPUT_DIR, 'test_150a')\n        path2 = os.path.join(self.OUTPUT_DIR, 'test_150b')\n        self.copy_input(self.ENCRYPTED_FILES[-1], path1)\n        shutil.copy(path1, path2)\n        # accessing renamed file should fail\n        args = ['decrypt', '-V', '-w', self.WRAP_KEY, '-i', path2, '-o', path1]\n        try:\n            self.__pf_crypt(args)\n        except subprocess.CalledProcessError as exc:\n            self.assertNotEqual(exc.returncode, 0)\n        else:\n            print('[!] Fail: successfully decrypted renamed file: ' + path2)\n            self.fail()\n\n    # overrides TC_00_FileSystem to decrypt output\n    def verify_copy_content(self, input_path, output_path):\n        dec_path = os.path.join(self.OUTPUT_DIR, os.path.basename(output_path) + '.dec')\n        self.__decrypt_file(output_path, dec_path)\n        self.assertTrue(filecmp.cmp(input_path, dec_path, shallow=False))\n\n    # overrides TC_00_FileSystem to change input dir (from plaintext to encrypted)\n    def do_copy_test(self, executable, timeout):\n        stdout, stderr = self.run_binary([executable, self.ENCRYPTED_DIR, self.OUTPUT_DIR],\n                                         timeout=timeout)\n        self.verify_copy(stdout, stderr, self.ENCRYPTED_DIR, executable)\n\n    # overrides TC_00_FileSystem to not skip this on SGX\n    def test_204_copy_dir_mmap_whole(self):\n        self.do_copy_test('copy_mmap_whole', 30)\n\n    # overrides TC_00_FileSystem to not skip this on SGX\n    def test_205_copy_dir_mmap_seq(self):\n        self.do_copy_test('copy_mmap_seq', 60)\n\n    # overrides TC_00_FileSystem to not skip this on SGX\n    def test_206_copy_dir_mmap_rev(self):\n        self.do_copy_test('copy_mmap_rev', 60)\n\n    # overrides TC_00_FileSystem to change dirs (from plaintext to encrypted)\n    def test_210_copy_dir_mounted(self):\n        executable = 'copy_whole'\n        stdout, stderr = self.run_binary([executable, '\/mounted\/pf_input', '\/mounted\/pf_output'],\n                                         timeout=30)\n        self.verify_copy(stdout, stderr, '\/mounted\/pf_input', executable)\n\n    def __corrupt_file(self, input_path, output_path):\n        cmd = [self.PF_TAMPER, '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path]\n        return self.run_native_binary(cmd)\n\n    # invalid\/corrupted files\n    @expectedFailureIf(HAS_SGX)\n    # pylint: disable=fixme\n    def test_500_invalid(self):\n        # TODO: port these to the new file format\n        invalid_dir = os.path.join(self.TEST_DIR, 'pf_invalid')\n        # files below should work normally (benign modifications)\n        should_pass = ['chunk_padding_1_fixed', 'chunk_padding_2_fixed', 'chunk_data_3',\n                       'chunk_data_3_fixed', 'chunk_data_4', 'chunk_data_4_fixed']\n        if not os.path.exists(invalid_dir):\n            os.mkdir(invalid_dir)\n        # prepare valid encrypted file (largest one for maximum possible corruptions)\n        original_input = self.OUTPUT_FILES[-1]\n        self.__encrypt_file(self.INPUT_FILES[-1], original_input)\n        # generate invalid files based on the above\n        self.__corrupt_file(original_input, invalid_dir)\n        # try to decrypt invalid files\n        for name in os.listdir(invalid_dir):\n            invalid = os.path.join(invalid_dir, name)\n            output_path = os.path.join(self.OUTPUT_DIR, name)\n            input_path = os.path.join(invalid_dir, os.path.basename(original_input))\n            # copy the file so it has the original file name (for allowed path check)\n            shutil.copy(invalid, input_path)\n            should_pass = any(s in name for s in should_pass)\n\n            try:\n                args = ['decrypt', '-V', '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path]\n                self.__pf_crypt(args)\n            except subprocess.CalledProcessError as exc:\n                if should_pass:\n                    self.assertEqual(exc.returncode, 0)\n                else:\n                    self.assertNotEqual(exc.returncode, 0)\n            else:\n                if not should_pass:\n                    print('[!] Fail: successfully decrypted file: ' + name)\n                    self.fail()\n"}},"msg":"[Pal\/Linux-SGX] Test malicious modifications to protected files\n\nThis commit adds a new PF utility `pf_tamper` that tampers with\nvalid protected files and uses this utility to test that the PF\nlogic in Linux-SGX detects such malicious modifications.\n\nThis commit also moves out the PF-format macros and structs from\n`protected_files_internal.h` to `protected_files_format.h` for\nbetter readability.\n\nCo-authored-by: Dmitrii Kuvaiskii <dmitrii.kuvaiskii@intel.com>"}},"https:\/\/github.com\/StanPlatinum\/graphene-atstub":{"345d271e663725d58ce941fc49f08ebc02c32fa2":{"url":"https:\/\/api.github.com\/repos\/StanPlatinum\/graphene-atstub\/commits\/345d271e663725d58ce941fc49f08ebc02c32fa2","html_url":"https:\/\/github.com\/StanPlatinum\/graphene-atstub\/commit\/345d271e663725d58ce941fc49f08ebc02c32fa2","message":"[Pal\/Linux-SGX] Test malicious modifications to protected files\n\nThis commit adds a new PF utility `pf_tamper` that tampers with\nvalid protected files and uses this utility to test that the PF\nlogic in Linux-SGX detects such malicious modifications.\n\nThis commit also moves out the PF-format macros and structs from\n`protected_files_internal.h` to `protected_files_format.h` for\nbetter readability.\n\nCo-authored-by: Dmitrii Kuvaiskii <dmitrii.kuvaiskii@intel.com>","sha":"345d271e663725d58ce941fc49f08ebc02c32fa2","keyword":"tampering malicious","diff":"diff --git a\/LibOS\/shim\/test\/fs\/test_pf.py b\/LibOS\/shim\/test\/fs\/test_pf.py\nindex e430e02a..7b1621d7 100644\n--- a\/LibOS\/shim\/test\/fs\/test_pf.py\n+++ b\/LibOS\/shim\/test\/fs\/test_pf.py\n@@ -193,24 +193,20 @@ def test_210_copy_dir_mounted(self):\n \n     def __corrupt_file(self, input_path, output_path):\n         cmd = [self.PF_TAMPER, '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path]\n-        return self.run_native_binary(cmd)\n+        return self.run_native_binary(cmd, libpath=os.path.join(os.getcwd(), 'lib'))\n \n     # invalid\/corrupted files\n-    @expectedFailureIf(HAS_SGX)\n-    # pylint: disable=fixme\n     def test_500_invalid(self):\n-        # TODO: port these to the new file format\n         invalid_dir = os.path.join(self.TEST_DIR, 'pf_invalid')\n-        # files below should work normally (benign modifications)\n-        should_pass = ['chunk_padding_1_fixed', 'chunk_padding_2_fixed', 'chunk_data_3',\n-                       'chunk_data_3_fixed', 'chunk_data_4', 'chunk_data_4_fixed']\n         if not os.path.exists(invalid_dir):\n             os.mkdir(invalid_dir)\n+\n         # prepare valid encrypted file (largest one for maximum possible corruptions)\n         original_input = self.OUTPUT_FILES[-1]\n         self.__encrypt_file(self.INPUT_FILES[-1], original_input)\n         # generate invalid files based on the above\n         self.__corrupt_file(original_input, invalid_dir)\n+\n         # try to decrypt invalid files\n         for name in os.listdir(invalid_dir):\n             invalid = os.path.join(invalid_dir, name)\n@@ -218,17 +214,13 @@ def test_500_invalid(self):\n             input_path = os.path.join(invalid_dir, os.path.basename(original_input))\n             # copy the file so it has the original file name (for allowed path check)\n             shutil.copy(invalid, input_path)\n-            should_pass = any(s in name for s in should_pass)\n \n             try:\n                 args = ['decrypt', '-V', '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path]\n                 self.__pf_crypt(args)\n             except subprocess.CalledProcessError as exc:\n-                if should_pass:\n-                    self.assertEqual(exc.returncode, 0)\n-                else:\n-                    self.assertNotEqual(exc.returncode, 0)\n+                # decryption of invalid file must fail with -1 (wrapped to 255)\n+                self.assertEqual(exc.returncode, 255)\n             else:\n-                if not should_pass:\n-                    print('[!] Fail: successfully decrypted file: ' + name)\n-                    self.fail()\n+                print('[!] Fail: successfully decrypted file: ' + name)\n+                self.fail()\ndiff --git a\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files.c b\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files.c\nindex 4f0c59ba..d020a1e2 100644\n--- a\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files.c\n+++ b\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files.c\n@@ -5,6 +5,8 @@\n  *\/\n \n #include \"api.h\"\n+#include \"protected_files.h\"\n+#include \"protected_files_format.h\"\n #include \"protected_files_internal.h\"\n \n #ifndef IN_PAL\n@@ -65,20 +67,6 @@ static pf_random_f          g_cb_random          = NULL;\n static pf_iv_t g_empty_iv = {0};\n static bool g_initialized = false;\n \n-#define METADATA_KEY_NAME \"SGX-PROTECTED-FS-METADATA-KEY\"\n-#define MAX_LABEL_SIZE    64\n-\n-static_assert(sizeof(METADATA_KEY_NAME) <= MAX_LABEL_SIZE, \"label too long\");\n-\n-#pragma pack(push, 1)\n-typedef struct {\n-    uint32_t index;\n-    char label[MAX_LABEL_SIZE]; \/\/ must be NULL terminated\n-    pf_keyid_t nonce;\n-    uint32_t output_len; \/\/ in bits\n-} kdf_input_t;\n-#pragma pack(pop)\n-\n \/\/ The key derivation function follow recommendations from NIST Special Publication 800-108:\n \/\/ Recommendation for Key Derivation Using Pseudorandom Functions\n \/\/ https:\/\/nvlpubs.nist.gov\/nistpubs\/Legacy\/SP\/nistspecialpublication800-108.pdf\ndiff --git a\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files_format.h b\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files_format.h\nnew file mode 100644\nindex 00000000..2394e53e\n--- \/dev\/null\n+++ b\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files_format.h\n@@ -0,0 +1,139 @@\n+\/* SPDX-License-Identifier: LGPL-3.0-or-later *\/\n+\/* Copyright (C) 2019-2020 Invisible Things Lab\n+ *                         Rafal Wojdyla <omeg@invisiblethingslab.com>\n+ * Copyright (C) 2011-2020 Intel Corporation\n+ *\/\n+\n+#ifndef PROTECTED_FILES_FORMAT_H_\n+#define PROTECTED_FILES_FORMAT_H_\n+\n+#include <limits.h>\n+\n+#include \"assert.h\"\n+#include \"list.h\"\n+#include \"protected_files.h\"\n+\n+#define PF_FILE_ID       0x46505f4850415247 \/* GRAPH_PF *\/\n+#define PF_MAJOR_VERSION 0x01\n+#define PF_MINOR_VERSION 0x00\n+\n+#define METADATA_KEY_NAME \"SGX-PROTECTED-FS-METADATA-KEY\"\n+#define MAX_LABEL_SIZE    64\n+\n+static_assert(sizeof(METADATA_KEY_NAME) <= MAX_LABEL_SIZE, \"label too long\");\n+\n+#pragma pack(push, 1)\n+\n+typedef struct _metadata_plain {\n+    uint64_t   file_id;\n+    uint8_t    major_version;\n+    uint8_t    minor_version;\n+    pf_keyid_t metadata_key_id;\n+    pf_mac_t   metadata_gmac; \/* GCM mac *\/\n+} metadata_plain_t;\n+\n+#define PATH_MAX_SIZE (260 + 512)\n+\n+\/\/ these are all defined as relative to node size, so we can decrease node size in tests\n+\/\/ and have deeper tree\n+#define MD_USER_DATA_SIZE (PF_NODE_SIZE * 3 \/ 4) \/\/ 3072\n+static_assert(MD_USER_DATA_SIZE == 3072, \"bad struct size\");\n+\n+typedef struct _metadata_encrypted {\n+    char     path[PATH_MAX_SIZE];\n+    uint64_t size;\n+    pf_key_t mht_key;\n+    pf_mac_t mht_gmac;\n+    uint8_t  data[MD_USER_DATA_SIZE];\n+} metadata_encrypted_t;\n+\n+typedef uint8_t metadata_encrypted_blob_t[sizeof(metadata_encrypted_t)];\n+\n+#define METADATA_NODE_SIZE PF_NODE_SIZE\n+\n+typedef uint8_t metadata_padding_t[METADATA_NODE_SIZE -\n+                                   (sizeof(metadata_plain_t) + sizeof(metadata_encrypted_blob_t))];\n+\n+typedef struct _metadata_node {\n+    metadata_plain_t          plain_part;\n+    metadata_encrypted_blob_t encrypted_part;\n+    metadata_padding_t        padding;\n+} metadata_node_t;\n+\n+static_assert(sizeof(metadata_node_t) == PF_NODE_SIZE, \"sizeof(metadata_node_t)\");\n+\n+typedef struct _data_node_crypto {\n+    pf_key_t key;\n+    pf_mac_t gmac;\n+} gcm_crypto_data_t;\n+\n+\/\/ for PF_NODE_SIZE == 4096, we have 96 attached data nodes and 32 mht child nodes\n+\/\/ for PF_NODE_SIZE == 2048, we have 48 attached data nodes and 16 mht child nodes\n+\/\/ for PF_NODE_SIZE == 1024, we have 24 attached data nodes and 8 mht child nodes\n+\/\/ 3\/4 of the node size is dedicated to data nodes\n+#define ATTACHED_DATA_NODES_COUNT ((PF_NODE_SIZE \/ sizeof(gcm_crypto_data_t)) * 3 \/ 4)\n+static_assert(ATTACHED_DATA_NODES_COUNT == 96, \"ATTACHED_DATA_NODES_COUNT\");\n+\/\/ 1\/4 of the node size is dedicated to child mht nodes\n+#define CHILD_MHT_NODES_COUNT ((PF_NODE_SIZE \/ sizeof(gcm_crypto_data_t)) * 1 \/ 4)\n+static_assert(CHILD_MHT_NODES_COUNT == 32, \"CHILD_MHT_NODES_COUNT\");\n+\n+typedef struct _mht_node {\n+    gcm_crypto_data_t data_nodes_crypto[ATTACHED_DATA_NODES_COUNT];\n+    gcm_crypto_data_t mht_nodes_crypto[CHILD_MHT_NODES_COUNT];\n+} mht_node_t;\n+\n+static_assert(sizeof(mht_node_t) == PF_NODE_SIZE, \"sizeof(mht_node_t)\");\n+\n+typedef struct _data_node {\n+    uint8_t data[PF_NODE_SIZE];\n+} data_node_t;\n+\n+static_assert(sizeof(data_node_t) == PF_NODE_SIZE, \"sizeof(data_node_t)\");\n+\n+typedef struct _encrypted_node {\n+    uint8_t cipher[PF_NODE_SIZE];\n+} encrypted_node_t;\n+\n+static_assert(sizeof(encrypted_node_t) == PF_NODE_SIZE, \"sizeof(encrypted_node_t)\");\n+\n+#define MAX_PAGES_IN_CACHE 48\n+\n+typedef enum {\n+    FILE_MHT_NODE_TYPE  = 1,\n+    FILE_DATA_NODE_TYPE = 2,\n+} mht_node_type_e;\n+\n+\/\/ make sure these are the same size\n+static_assert(sizeof(mht_node_t) == sizeof(data_node_t),\n+              \"sizeof(mht_node_t) == sizeof(data_node_t)\");\n+\n+DEFINE_LIST(_file_node);\n+typedef struct _file_node {\n+    LIST_TYPE(_file_node) list;\n+    uint8_t type;\n+    uint64_t node_number;\n+    struct _file_node* parent;\n+    bool need_writing;\n+    bool new_node;\n+    struct {\n+        uint64_t physical_node_number;\n+        encrypted_node_t encrypted; \/\/ the actual data from the disk\n+    };\n+    union { \/\/ decrypted data\n+        mht_node_t mht;\n+        data_node_t data;\n+    } decrypted;\n+} file_node_t;\n+DEFINE_LISTP(_file_node);\n+\n+typedef struct {\n+    uint32_t index;\n+    char label[MAX_LABEL_SIZE]; \/\/ must be NULL terminated\n+    pf_keyid_t nonce;\n+    uint32_t output_len; \/\/ in bits\n+} kdf_input_t;\n+\n+#pragma pack(pop)\n+\n+#endif \/* PROTECTED_FILES_FORMAT_H_ *\/\n+\ndiff --git a\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files_internal.h b\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files_internal.h\nindex 4a31261d..c06d1ecf 100644\n--- a\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files_internal.h\n+++ b\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files_internal.h\n@@ -13,116 +13,7 @@\n #include \"list.h\"\n #include \"lru_cache.h\"\n #include \"protected_files.h\"\n-\n-#define PF_FILE_ID       0x46505f4850415247 \/* GRAPH_PF *\/\n-#define PF_MAJOR_VERSION 0x01\n-#define PF_MINOR_VERSION 0x00\n-\n-#pragma pack(push, 1)\n-\n-typedef struct _metadata_plain {\n-    uint64_t   file_id;\n-    uint8_t    major_version;\n-    uint8_t    minor_version;\n-    pf_keyid_t metadata_key_id;\n-    pf_mac_t   metadata_gmac; \/* GCM mac *\/\n-} metadata_plain_t;\n-\n-#define PATH_MAX_SIZE (260 + 512)\n-\n-\/\/ these are all defined as relative to node size, so we can decrease node size in tests\n-\/\/ and have deeper tree\n-#define MD_USER_DATA_SIZE (PF_NODE_SIZE * 3 \/ 4) \/\/ 3072\n-static_assert(MD_USER_DATA_SIZE == 3072, \"bad struct size\");\n-\n-typedef struct _metadata_encrypted {\n-    char     path[PATH_MAX_SIZE];\n-    uint64_t size;\n-    pf_key_t mht_key;\n-    pf_mac_t mht_gmac;\n-    uint8_t  data[MD_USER_DATA_SIZE];\n-} metadata_encrypted_t;\n-\n-typedef uint8_t metadata_encrypted_blob_t[sizeof(metadata_encrypted_t)];\n-\n-#define METADATA_NODE_SIZE PF_NODE_SIZE\n-\n-typedef uint8_t metadata_padding_t[METADATA_NODE_SIZE -\n-                                   (sizeof(metadata_plain_t) + sizeof(metadata_encrypted_blob_t))];\n-\n-typedef struct _metadata_node {\n-    metadata_plain_t          plain_part;\n-    metadata_encrypted_blob_t encrypted_part;\n-    metadata_padding_t        padding;\n-} metadata_node_t;\n-\n-static_assert(sizeof(metadata_node_t) == PF_NODE_SIZE, \"sizeof(metadata_node_t)\");\n-\n-typedef struct _data_node_crypto {\n-    pf_key_t key;\n-    pf_mac_t gmac;\n-} gcm_crypto_data_t;\n-\n-\/\/ for PF_NODE_SIZE == 4096, we have 96 attached data nodes and 32 mht child nodes\n-\/\/ for PF_NODE_SIZE == 2048, we have 48 attached data nodes and 16 mht child nodes\n-\/\/ for PF_NODE_SIZE == 1024, we have 24 attached data nodes and 8 mht child nodes\n-\/\/ 3\/4 of the node size is dedicated to data nodes\n-#define ATTACHED_DATA_NODES_COUNT ((PF_NODE_SIZE \/ sizeof(gcm_crypto_data_t)) * 3 \/ 4)\n-static_assert(ATTACHED_DATA_NODES_COUNT == 96, \"ATTACHED_DATA_NODES_COUNT\");\n-\/\/ 1\/4 of the node size is dedicated to child mht nodes\n-#define CHILD_MHT_NODES_COUNT ((PF_NODE_SIZE \/ sizeof(gcm_crypto_data_t)) * 1 \/ 4)\n-static_assert(CHILD_MHT_NODES_COUNT == 32, \"CHILD_MHT_NODES_COUNT\");\n-\n-typedef struct _mht_node {\n-    gcm_crypto_data_t data_nodes_crypto[ATTACHED_DATA_NODES_COUNT];\n-    gcm_crypto_data_t mht_nodes_crypto[CHILD_MHT_NODES_COUNT];\n-} mht_node_t;\n-\n-static_assert(sizeof(mht_node_t) == PF_NODE_SIZE, \"sizeof(mht_node_t)\");\n-\n-typedef struct _data_node {\n-    uint8_t data[PF_NODE_SIZE];\n-} data_node_t;\n-\n-static_assert(sizeof(data_node_t) == PF_NODE_SIZE, \"sizeof(data_node_t)\");\n-\n-typedef struct _encrypted_node {\n-    uint8_t cipher[PF_NODE_SIZE];\n-} encrypted_node_t;\n-\n-static_assert(sizeof(encrypted_node_t) == PF_NODE_SIZE, \"sizeof(encrypted_node_t)\");\n-\n-#define MAX_PAGES_IN_CACHE 48\n-\n-typedef enum {\n-    FILE_MHT_NODE_TYPE  = 1,\n-    FILE_DATA_NODE_TYPE = 2,\n-} mht_node_type_e;\n-\n-\/\/ make sure these are the same size\n-static_assert(sizeof(mht_node_t) == sizeof(data_node_t),\n-              \"sizeof(mht_node_t) == sizeof(data_node_t)\");\n-\n-DEFINE_LIST(_file_node);\n-typedef struct _file_node {\n-    LIST_TYPE(_file_node) list;\n-    uint8_t type;\n-    uint64_t node_number;\n-    struct _file_node* parent;\n-    bool need_writing;\n-    bool new_node;\n-    struct {\n-        uint64_t physical_node_number;\n-        encrypted_node_t encrypted; \/\/ the actual data from the disk\n-    };\n-    union { \/\/ decrypted data\n-        mht_node_t mht;\n-        data_node_t data;\n-    } decrypted;\n-} file_node_t;\n-DEFINE_LISTP(_file_node);\n-\n-#pragma pack(pop)\n+#include \"protected_files_format.h\"\n \n struct pf_context {\n     metadata_node_t file_metadata; \/\/ actual data from disk's meta data node\ndiff --git a\/Pal\/src\/host\/Linux-SGX\/tools\/Makefile b\/Pal\/src\/host\/Linux-SGX\/tools\/Makefile\nindex a5903ad9..87bc7812 100644\n--- a\/Pal\/src\/host\/Linux-SGX\/tools\/Makefile\n+++ b\/Pal\/src\/host\/Linux-SGX\/tools\/Makefile\n@@ -12,3 +12,4 @@ $(targets):\n \t$(MAKE) -C verify-ias-report $@\n \t$(MAKE) -C ra-tls $@\n \t$(MAKE) -C pf_crypt $@\n+\t$(MAKE) -C pf_tamper $@\ndiff --git a\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/.gitignore b\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/.gitignore\nnew file mode 100644\nindex 00000000..a52536d8\n--- \/dev\/null\n+++ b\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/.gitignore\n@@ -0,0 +1 @@\n+\/pf_tamper\ndiff --git a\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/Makefile b\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/Makefile\nnew file mode 100644\nindex 00000000..94c826e4\n--- \/dev\/null\n+++ b\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/Makefile\n@@ -0,0 +1,31 @@\n+include ..\/..\/..\/..\/..\/..\/Scripts\/Makefile.configs\n+include ..\/..\/..\/..\/..\/..\/Scripts\/Makefile.rules\n+\n+CFLAGS += -I..\/.. \\\n+          -I..\/common \\\n+          -I..\/..\/protected-files \\\n+          -I..\/..\/..\/..\/..\/include\/lib \\\n+          -D_GNU_SOURCE\n+\n+LDLIBS += -L..\/common \\\n+          -L..\/..\/..\/..\/..\/lib\/crypto\/mbedtls\/install\/lib \\\n+          -lsgx_util -lmbedcrypto\n+\n+PREFIX ?= \/usr\/local\n+\n+pf_tamper: pf_tamper.o\n+\t$(call cmd,csingle)\n+\n+.PHONY: all\n+all: pf_tamper\n+\n+.PHONY: install\n+install:\n+\tinstall -D pf_tamper -t ${PREFIX}\/bin\n+\n+.PHONY: clean\n+clean:\n+\t$(RM) *.o pf_tamper\n+\n+.PHONY: distclean\n+distclean: clean\ndiff --git a\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/pf_tamper.c b\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/pf_tamper.c\nnew file mode 100644\nindex 00000000..4d39a0d5\n--- \/dev\/null\n+++ b\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/pf_tamper.c\n@@ -0,0 +1,476 @@\n+\/* SPDX-License-Identifier: LGPL-3.0-or-later *\/\n+\/* Copyright (C) 2019-2020 Invisible Things Lab\n+ *                         Rafal Wojdyla <omeg@invisiblethingslab.com>\n+ *\/\n+\n+#include <fcntl.h>\n+#include <getopt.h>\n+#include <stdlib.h>\n+#include <sys\/mman.h>\n+#include <unistd.h>\n+\n+#include \"pf_util.h\"\n+#include \"protected_files.h\"\n+#include \"protected_files_format.h\"\n+#include \"util.h\"\n+\n+\/* Tamper with a PF in various ways for testing purposes. The PF is assumed to be valid and have at\n+ * least enough data to contain two MHT nodes. *\/\n+\n+\/* Command line options *\/\n+struct option g_options[] = {\n+    { \"input\", required_argument, 0, 'i' },\n+    { \"output\", required_argument, 0, 'o' },\n+    { \"wrap-key\", required_argument, 0, 'w' },\n+    { \"verbose\", no_argument, 0, 'v' },\n+    { \"help\", no_argument, 0, 'h' },\n+    { 0, 0, 0, 0 }\n+};\n+\n+static void usage(void) {\n+    INFO(\"\\nUsage: pf_tamper [options]\\n\");\n+    INFO(\"\\nAvailable options:\\n\");\n+    INFO(\"  --help, -h           Display this help\\n\");\n+    INFO(\"  --verbose, -v        Enable verbose output\\n\");\n+    INFO(\"  --wrap-key, -w PATH  Path to wrap key file\\n\");\n+    INFO(\"  --input, -i PATH     Source file to be tampered with (must be a valid PF)\\n\");\n+    INFO(\"  --output, -o PATH    Directory where modified files will be written to\\n\");\n+}\n+\n+#define FATAL(fmt, ...) do { \\\n+    ERROR(fmt, ##__VA_ARGS__); \\\n+    exit(-1); \\\n+} while (0)\n+\n+ssize_t g_input_size = 0;\n+char* g_input_name = NULL;\n+void* g_input_data = MAP_FAILED;\n+char* g_output_dir = NULL;\n+char* g_output_path = NULL;\n+size_t g_output_path_size = 0;\n+pf_key_t g_wrap_key;\n+pf_key_t g_meta_key;\n+\n+static pf_iv_t g_empty_iv = {0};\n+\n+static void derive_main_key(const pf_key_t* kdk, const pf_keyid_t* key_id, pf_key_t* out_key) {\n+    kdf_input_t buf = {0};\n+    pf_status_t status;\n+\n+    buf.index = 1;\n+    strncpy(buf.label, METADATA_KEY_NAME, MAX_LABEL_SIZE);\n+    memcpy(&buf.nonce, key_id, sizeof(buf.nonce));\n+    buf.output_len = 0x80;\n+\n+    status = mbedtls_aes_gcm_encrypt(kdk, &g_empty_iv, &buf, sizeof(buf), NULL, 0, NULL, out_key);\n+    if (PF_FAILURE(status))\n+        FATAL(\"key derivation failed\\n\");\n+}\n+\n+static void make_output_path(const char* suffix) {\n+    snprintf(g_output_path, g_output_path_size, \"%s\/%s.%s\", g_output_dir, g_input_name, suffix);\n+    INFO(\"[*] %s\\n\", g_output_path);\n+}\n+\n+\/* PF layout (node size is PF_NODE_SIZE):\n+ * - Node 0: metadata (metadata_node_t)\n+ *   - metadata_plain_t\n+ *   - metadata_encrypted_t (may include MD_USER_DATA_SIZE bytes of data)\n+ *   - metadata_padding_t\n+ * - Node 1: MHT (mht_node_t)\n+ * - Node 2-97: data (ATTACHED_DATA_NODES_COUNT == 96)\n+ * - Node 98: MHT\n+ * - Node 99-195: data\n+ * - ...\n+ *\/\n+static void truncate_file(const char* suffix, size_t output_size) {\n+    int ret;\n+\n+    make_output_path(suffix);\n+\n+    if (output_size < g_input_size) {\n+        ret = write_file(g_output_path, output_size, g_input_data);\n+    } else {\n+        ret = write_file(g_output_path, g_input_size, g_input_data);\n+        if (ret < 0)\n+            goto out;\n+        ret = truncate(g_output_path, output_size);\n+    }\n+out:\n+    if (ret < 0)\n+        FATAL(\"truncate_file failed: %d\\n\", ret);\n+}\n+\n+#define FIELD_SIZEOF(t, f) (sizeof(((t*)0)->f))\n+#define FIELD_TRUNCATED(t, f) (offsetof(t, f) + (FIELD_SIZEOF(t, f) \/ 2))\n+#define DATA_CRYPTO_SIZE (FIELD_SIZEOF(mht_node_t, data_nodes_crypto))\n+\n+static void tamper_truncate(void) {\n+    size_t mdps = sizeof(metadata_plain_t);\n+    DBG(\"size(metadata_plain_t)             = 0x%04lx\\n\", sizeof(metadata_plain_t));\n+    DBG(\"metadata_plain_t.file_id           : 0x%04lx (0x%04lx)\\n\",\n+        offsetof(metadata_plain_t, file_id), FIELD_SIZEOF(metadata_plain_t, file_id));\n+    DBG(\"metadata_plain_t.major_version     : 0x%04lx (0x%04lx)\\n\",\n+        offsetof(metadata_plain_t, major_version), FIELD_SIZEOF(metadata_plain_t, major_version));\n+    DBG(\"metadata_plain_t.minor_version     : 0x%04lx (0x%04lx)\\n\",\n+        offsetof(metadata_plain_t, minor_version), FIELD_SIZEOF(metadata_plain_t, minor_version));\n+    DBG(\"metadata_plain_t.metadata_key_id   : 0x%04lx (0x%04lx)\\n\",\n+        offsetof(metadata_plain_t, metadata_key_id),\n+        FIELD_SIZEOF(metadata_plain_t, metadata_key_id));\n+    DBG(\"metadata_plain_t.metadata_gmac     : 0x%04lx (0x%04lx)\\n\",\n+        offsetof(metadata_plain_t, metadata_gmac),\n+        FIELD_SIZEOF(metadata_plain_t, metadata_gmac));\n+\n+    DBG(\"size(metadata_encrypted_t)         = 0x%04lx\\n\", sizeof(metadata_encrypted_t));\n+    DBG(\"metadata_encrypted_t.path          : 0x%04lx (0x%04lx)\\n\",\n+        mdps + offsetof(metadata_encrypted_t, path),\n+        FIELD_SIZEOF(metadata_encrypted_t, path));\n+    DBG(\"metadata_encrypted_t.size          : 0x%04lx (0x%04lx)\\n\",\n+        mdps + offsetof(metadata_encrypted_t, size), FIELD_SIZEOF(metadata_encrypted_t, size));\n+    DBG(\"metadata_encrypted_t.mht_key       : 0x%04lx (0x%04lx)\\n\",\n+        mdps + offsetof(metadata_encrypted_t, mht_key),\n+        FIELD_SIZEOF(metadata_encrypted_t, mht_key));\n+    DBG(\"metadata_encrypted_t.mht_gmac      : 0x%04lx (0x%04lx)\\n\",\n+        mdps + offsetof(metadata_encrypted_t, mht_gmac),\n+        FIELD_SIZEOF(metadata_encrypted_t, mht_gmac));\n+    DBG(\"metadata_encrypted_t.data          : 0x%04lx (0x%04lx)\\n\",\n+        mdps + offsetof(metadata_encrypted_t, data), FIELD_SIZEOF(metadata_encrypted_t, data));\n+\n+    DBG(\"size(metadata_padding_t)           = 0x%04lx\\n\", sizeof(metadata_padding_t));\n+    DBG(\"metadata_padding_t                 : 0x%04lx (0x%04lx)\\n\",\n+        mdps + sizeof(metadata_encrypted_t), sizeof(metadata_padding_t));\n+\n+    \/* node 0: metadata + 3k of user data *\/\n+    \/* plain metadata *\/\n+    truncate_file(\"trunc_meta_plain_0\", 0);\n+    truncate_file(\"trunc_meta_plain_1\", FIELD_TRUNCATED(metadata_plain_t, file_id));\n+    truncate_file(\"trunc_meta_plain_2\", offsetof(metadata_plain_t, major_version));\n+    truncate_file(\"trunc_meta_plain_3\", offsetof(metadata_plain_t, minor_version));\n+    truncate_file(\"trunc_meta_plain_4\", offsetof(metadata_plain_t, metadata_key_id));\n+    truncate_file(\"trunc_meta_plain_5\", FIELD_TRUNCATED(metadata_plain_t, metadata_key_id));\n+    truncate_file(\"trunc_meta_plain_6\", offsetof(metadata_plain_t, metadata_gmac));\n+    truncate_file(\"trunc_meta_plain_7\", FIELD_TRUNCATED(metadata_plain_t, metadata_gmac));\n+\n+    \/* encrypted metadata *\/\n+    truncate_file(\"trunc_meta_enc_0\", mdps + offsetof(metadata_encrypted_t, path));\n+    truncate_file(\"trunc_meta_enc_1\", mdps + FIELD_TRUNCATED(metadata_encrypted_t, path));\n+    truncate_file(\"trunc_meta_enc_2\", mdps + offsetof(metadata_encrypted_t, size));\n+    truncate_file(\"trunc_meta_enc_3\", mdps + FIELD_TRUNCATED(metadata_encrypted_t, size));\n+    truncate_file(\"trunc_meta_enc_4\", mdps + offsetof(metadata_encrypted_t, mht_key));\n+    truncate_file(\"trunc_meta_enc_5\", mdps + FIELD_TRUNCATED(metadata_encrypted_t, mht_key));\n+    truncate_file(\"trunc_meta_enc_6\", mdps + offsetof(metadata_encrypted_t, mht_gmac));\n+    truncate_file(\"trunc_meta_enc_7\", mdps + FIELD_TRUNCATED(metadata_encrypted_t, mht_gmac));\n+    truncate_file(\"trunc_meta_enc_8\", mdps + offsetof(metadata_encrypted_t, data));\n+    truncate_file(\"trunc_meta_enc_9\", mdps + FIELD_TRUNCATED(metadata_encrypted_t, data));\n+\n+    \/* padding *\/\n+    truncate_file(\"trunc_meta_pad_0\", mdps + sizeof(metadata_encrypted_t));\n+    truncate_file(\"trunc_meta_pad_1\", mdps + sizeof(metadata_encrypted_t)\n+                  + sizeof(metadata_padding_t) \/ 2);\n+\n+    \/* node 1: mht root *\/\n+    \/* after node 0 *\/\n+    truncate_file(\"trunc_mht_0\", PF_NODE_SIZE);\n+    \/* middle of data_nodes_crypto[0].key *\/\n+    truncate_file(\"trunc_mht_1\", PF_NODE_SIZE + PF_KEY_SIZE \/ 2);\n+    \/* after data_nodes_crypto[0].key *\/\n+    truncate_file(\"trunc_mht_2\", PF_NODE_SIZE + PF_KEY_SIZE);\n+    \/* middle of data_nodes_crypto[0].gmac *\/\n+    truncate_file(\"trunc_mht_3\", PF_NODE_SIZE + PF_KEY_SIZE + PF_MAC_SIZE \/ 2);\n+    \/* after data_nodes_crypto[0].gmac *\/\n+    truncate_file(\"trunc_mht_4\", PF_NODE_SIZE + PF_KEY_SIZE + PF_MAC_SIZE);\n+    \/* after data_nodes_crypto *\/\n+    truncate_file(\"trunc_mht_5\", PF_NODE_SIZE + DATA_CRYPTO_SIZE);\n+    \/* middle of mht_nodes_crypto[0].key *\/\n+    truncate_file(\"trunc_mht_6\", PF_NODE_SIZE + DATA_CRYPTO_SIZE + PF_KEY_SIZE \/ 2);\n+    \/* after mht_nodes_crypto[0].key *\/\n+    truncate_file(\"trunc_mht_7\", PF_NODE_SIZE + DATA_CRYPTO_SIZE + PF_KEY_SIZE);\n+    \/* middle of mht_nodes_crypto[0].gmac *\/\n+    truncate_file(\"trunc_mht_8\", PF_NODE_SIZE + DATA_CRYPTO_SIZE + PF_KEY_SIZE + PF_MAC_SIZE \/ 2);\n+    \/* after mht_nodes_crypto[0].gmac *\/\n+    truncate_file(\"trunc_mht_9\", PF_NODE_SIZE + DATA_CRYPTO_SIZE + PF_KEY_SIZE + PF_MAC_SIZE);\n+\n+    \/* node 2-3: data #0, #1 *\/\n+    \/* after mht root *\/\n+    truncate_file(\"trunc_data_0\", 2 * PF_NODE_SIZE);\n+    \/* middle of data #0 *\/\n+    truncate_file(\"trunc_data_1\", 2 * PF_NODE_SIZE + PF_NODE_SIZE \/ 2);\n+    \/* after data #0 *\/\n+    truncate_file(\"trunc_data_2\", 3 * PF_NODE_SIZE);\n+    \/* middle of data #1 *\/\n+    truncate_file(\"trunc_data_3\", 3 * PF_NODE_SIZE + PF_NODE_SIZE \/ 2);\n+\n+    \/* extend *\/\n+    truncate_file(\"extend_0\", g_input_size + 1);\n+    truncate_file(\"extend_1\", g_input_size + PF_NODE_SIZE \/ 2);\n+    truncate_file(\"extend_2\", g_input_size + PF_NODE_SIZE);\n+    truncate_file(\"extend_3\", g_input_size + PF_NODE_SIZE + PF_NODE_SIZE \/ 2);\n+}\n+\n+\/* returns mmap'd output contents *\/\n+static void* create_output(const char* path) {\n+    void* mem = MAP_FAILED;\n+    int fd = open(path, O_RDWR|O_CREAT, 0664);\n+    if (fd < 0)\n+        FATAL(\"Failed to open output file '%s': %s\\n\", path, strerror(errno));\n+\n+    if (ftruncate(fd, g_input_size) < 0)\n+        FATAL(\"Failed to ftruncate output file '%s': %s\\n\", path, strerror(errno));\n+\n+    mem = mmap(NULL, g_input_size, PROT_READ|PROT_WRITE, MAP_SHARED, fd, 0);\n+    if (mem == MAP_FAILED)\n+        FATAL(\"Failed to mmap output file '%s': %s\\n\", path, strerror(errno));\n+\n+    memcpy(mem, g_input_data, g_input_size);\n+\n+    close(fd);\n+    return mem;\n+}\n+\n+static void pf_decrypt(const void* encrypted, size_t size, const pf_key_t* key, const pf_mac_t* mac,\n+                       void* decrypted, const char* msg) {\n+    pf_status_t status = mbedtls_aes_gcm_decrypt(key, &g_empty_iv, NULL, 0,\n+                                                 encrypted, size,\n+                                                 decrypted, mac);\n+    if (PF_FAILURE(status))\n+        FATAL(\"decrypting %s failed\\n\", msg);\n+}\n+\n+static void pf_encrypt(const void* decrypted, size_t size, const pf_key_t* key, pf_mac_t* mac,\n+                       void* encrypted, const char* msg) {\n+    pf_status_t status = mbedtls_aes_gcm_encrypt(key, &g_empty_iv, NULL, 0,\n+                                                 decrypted, size,\n+                                                 encrypted, mac);\n+    if (PF_FAILURE(status))\n+        FATAL(\"encrypting %s failed\\n\", msg);\n+}\n+\n+\/* copy input PF and apply some modifications *\/\n+#define __BREAK_PF(suffix, ...) do { \\\n+    make_output_path(suffix); \\\n+    meta = create_output(g_output_path); \\\n+    out = (uint8_t*)meta; \\\n+    pf_decrypt(&meta->encrypted_part, sizeof(meta->encrypted_part), &g_meta_key, \\\n+               &meta->plain_part.metadata_gmac, meta_dec, \"metadata\"); \\\n+    mht_enc = (mht_node_t*)(out + PF_NODE_SIZE); \\\n+    pf_decrypt(mht_enc, sizeof(*mht_enc), &meta_dec->mht_key, &meta_dec->mht_gmac, mht_dec, \\\n+               \"mht\"); \\\n+    __VA_ARGS__ \\\n+    munmap(meta, g_input_size); \\\n+} while (0)\n+\n+\/* if update is true, also create a file with correct metadata MAC *\/\n+#define BREAK_PF(suffix, update, ...) do { \\\n+    __BREAK_PF(suffix, __VA_ARGS__); \\\n+    if (update) { \\\n+        __BREAK_PF(suffix \"_fixed\", __VA_ARGS__ { \\\n+                       pf_encrypt(meta_dec, sizeof(*meta_dec), &g_meta_key, \\\n+                                  &meta->plain_part.metadata_gmac, meta->encrypted_part, \\\n+                                  \"metadata\"); \\\n+                   } ); \\\n+    } \\\n+} while (0)\n+\n+#define BREAK_MHT(suffix, ...) do { \\\n+    __BREAK_PF(suffix, __VA_ARGS__ { \\\n+                   pf_encrypt(mht_dec, sizeof(*mht_dec), &meta_dec->mht_key, &meta_dec->mht_gmac, \\\n+                              mht_enc, \"mht\"); \\\n+               } ); \\\n+} while (0)\n+\n+#define LAST_BYTE(array) (((uint8_t*)&array)[sizeof(array) - 1])\n+\n+static void tamper_modify(void) {\n+    metadata_node_t* meta = NULL;\n+    uint8_t* out = NULL;\n+    metadata_encrypted_t* meta_dec = malloc(sizeof(*meta_dec));\n+    if (!meta_dec)\n+        FATAL(\"Out of memory\\n\");\n+    mht_node_t* mht_enc = NULL;\n+    mht_node_t* mht_dec = malloc(sizeof(*mht_dec));\n+    if (!mht_dec)\n+        FATAL(\"Out of memory\\n\");\n+\n+    \/* plain part of the metadata isn't covered by the MAC so no point updating it *\/\n+    BREAK_PF(\"meta_plain_id_0\", \/*update=*\/false,\n+             { meta->plain_part.file_id = 0; });\n+    BREAK_PF(\"meta_plain_id_1\", \/*update=*\/false,\n+             { meta->plain_part.file_id = UINT64_MAX; });\n+    BREAK_PF(\"meta_plain_version_0\", \/*update=*\/false,\n+             { meta->plain_part.major_version = 0; });\n+    BREAK_PF(\"meta_plain_version_1\", \/*update=*\/false,\n+             { meta->plain_part.major_version = 0xff; });\n+    BREAK_PF(\"meta_plain_version_2\", \/*update=*\/false,\n+             { meta->plain_part.minor_version = 0xff; });\n+\n+    \/* metadata_key_id is the keying material for encrypted metadata key derivation, so create also\n+     * PFs with updated MACs *\/\n+    BREAK_PF(\"meta_plain_keyid_0\", \/*update=*\/true,\n+             { meta->plain_part.metadata_key_id[0] ^= 1; });\n+    BREAK_PF(\"meta_plain_keyid_1\", \/*update=*\/true,\n+             { LAST_BYTE(meta->plain_part.metadata_key_id) ^= 0xfe; });\n+    BREAK_PF(\"meta_plain_mac_0\", \/*update=*\/true,\n+             { meta->plain_part.metadata_gmac[0] ^= 0xfe; });\n+    BREAK_PF(\"meta_plain_mac_1\", \/*update=*\/true,\n+             { LAST_BYTE(meta->plain_part.metadata_gmac) &= 1; });\n+\n+    BREAK_PF(\"meta_enc_filename_0\", \/*update=*\/true,\n+             { meta_dec->path[0] = 0; });\n+    BREAK_PF(\"meta_enc_filename_1\", \/*update=*\/true,\n+             { meta_dec->path[0] ^= 1; });\n+    BREAK_PF(\"meta_enc_filename_2\", \/*update=*\/true,\n+             { LAST_BYTE(meta_dec->path) ^= 0xfe; });\n+    BREAK_PF(\"meta_enc_size_0\", \/*update=*\/true,\n+             { meta_dec->size = 0; });\n+    BREAK_PF(\"meta_enc_size_1\", \/*update=*\/true,\n+             { meta_dec->size = g_input_size - 1; });\n+    BREAK_PF(\"meta_enc_size_2\", \/*update=*\/true,\n+             { meta_dec->size = g_input_size + 1; });\n+    BREAK_PF(\"meta_enc_size_3\", \/*update=*\/true,\n+             { meta_dec->size = UINT64_MAX; });\n+    BREAK_PF(\"meta_enc_mht_key_0\", \/*update=*\/true,\n+             { meta_dec->mht_key[0] ^= 1; });\n+    BREAK_PF(\"meta_enc_mht_key_1\", \/*update=*\/true,\n+             { LAST_BYTE(meta_dec->mht_key) ^= 0xfe; });\n+    BREAK_PF(\"meta_enc_mht_mac_0\", \/*update=*\/true,\n+             { meta_dec->mht_gmac[0] ^= 1; });\n+    BREAK_PF(\"meta_enc_mht_mac_1\", \/*update=*\/true,\n+             { LAST_BYTE(meta_dec->mht_gmac) ^= 0xfe; });\n+    BREAK_PF(\"meta_enc_data_0\", \/*update=*\/true,\n+             { meta_dec->data[0] ^= 0xfe; });\n+    BREAK_PF(\"meta_enc_data_1\", \/*update=*\/true,\n+             { LAST_BYTE(meta_dec->data) ^= 1; });\n+\n+    \/* padding is ignored *\/\n+    BREAK_PF(\"meta_padding_0\", \/*update=*\/false,\n+             { meta->padding[0] ^= 1; });\n+    BREAK_PF(\"meta_padding_1\", \/*update=*\/false,\n+             { LAST_BYTE(meta->padding) ^= 0xfe; });\n+\n+    BREAK_MHT(\"mht_0\", { mht_dec->data_nodes_crypto[0].key[0] ^= 1; });\n+    BREAK_MHT(\"mht_1\", { mht_dec->data_nodes_crypto[0].gmac[0] ^= 1; });\n+    BREAK_MHT(\"mht_2\", { mht_dec->mht_nodes_crypto[0].key[0] ^= 1; });\n+    BREAK_MHT(\"mht_3\", { mht_dec->mht_nodes_crypto[0].gmac[0] ^= 1; });\n+    BREAK_MHT(\"mht_4\", { mht_dec->data_nodes_crypto[ATTACHED_DATA_NODES_COUNT - 1].key[0] ^= 1; });\n+    BREAK_MHT(\"mht_5\", { mht_dec->data_nodes_crypto[ATTACHED_DATA_NODES_COUNT - 1].gmac[0] ^= 1; });\n+    BREAK_MHT(\"mht_6\", { mht_dec->mht_nodes_crypto[CHILD_MHT_NODES_COUNT - 1].key[0] ^= 1; });\n+    BREAK_MHT(\"mht_7\", { mht_dec->mht_nodes_crypto[CHILD_MHT_NODES_COUNT - 1].gmac[0] ^= 1; });\n+    BREAK_MHT(\"mht_8\", {\n+        gcm_crypto_data_t crypto;\n+        memcpy(&crypto, &mht_dec->data_nodes_crypto[0], sizeof(crypto));\n+        memcpy(&mht_dec->data_nodes_crypto[0], &mht_dec->data_nodes_crypto[1], sizeof(crypto));\n+        memcpy(&mht_dec->data_nodes_crypto[1], &crypto, sizeof(crypto));\n+    });\n+    BREAK_MHT(\"mht_9\", {\n+        gcm_crypto_data_t crypto;\n+        memcpy(&crypto, &mht_dec->mht_nodes_crypto[0], sizeof(crypto));\n+        memcpy(&mht_dec->mht_nodes_crypto[0], &mht_dec->mht_nodes_crypto[1], sizeof(crypto));\n+        memcpy(&mht_dec->mht_nodes_crypto[1], &crypto, sizeof(crypto));\n+    });\n+\n+    \/* data nodes start from node #2 *\/\n+    BREAK_PF(\"data_0\", \/*update=*\/false,\n+             { *(out + 2 * PF_NODE_SIZE) ^= 1; });\n+    BREAK_PF(\"data_1\", \/*update=*\/false,\n+             { *(out + 3 * PF_NODE_SIZE - 1) ^= 1; });\n+    BREAK_PF(\"data_2\", \/*update=*\/false, {\n+        \/* swap data nodes *\/\n+        memcpy(out + 2 * PF_NODE_SIZE, g_input_data + 3 * PF_NODE_SIZE, PF_NODE_SIZE);\n+        memcpy(out + 3 * PF_NODE_SIZE, g_input_data + 2 * PF_NODE_SIZE, PF_NODE_SIZE);\n+    });\n+\n+    free(mht_dec);\n+    free(meta_dec);\n+}\n+\n+int main(int argc, char* argv[]) {\n+    int ret = -1;\n+\n+    int option          = 0;\n+    char* input_path    = NULL;\n+    char* wrap_key_path = NULL;\n+    int input_fd        = -1;\n+\n+    while (true) {\n+        option = getopt_long(argc, argv, \"i:o:w:vh\", g_options, NULL);\n+        if (option == -1)\n+            break;\n+\n+        switch (option) {\n+            case 'i':\n+                input_path = optarg;\n+                break;\n+            case 'o':\n+                g_output_dir = optarg;\n+                break;\n+            case 'w':\n+                wrap_key_path = optarg;\n+                break;\n+            case 'v':\n+                set_verbose(true);\n+                break;\n+            case 'h':\n+                usage();\n+                return 0;\n+            default:\n+                ERROR(\"Unknown option: %c\\n\", option);\n+                usage();\n+        }\n+    }\n+\n+    if (!input_path) {\n+        ERROR(\"Input path not specified\\n\");\n+        usage();\n+        goto out;\n+    }\n+\n+    if (!g_output_dir) {\n+        ERROR(\"Output path not specified\\n\");\n+        usage();\n+        goto out;\n+    }\n+\n+    if (!wrap_key_path) {\n+        ERROR(\"Wrap key path not specified\\n\");\n+        usage();\n+        goto out;\n+    }\n+\n+    input_fd = open(input_path, O_RDONLY);\n+    if (input_fd < 0) {\n+        ERROR(\"Failed to open input file '%s': %s\\n\", input_path, strerror(errno));\n+        goto out;\n+    }\n+\n+    g_input_size = get_file_size(input_fd);\n+    if (g_input_size < 0) {\n+        ERROR(\"Failed to stat input file '%s': %s\\n\", input_path, strerror(errno));\n+        goto out;\n+    }\n+\n+    g_input_data = mmap(NULL, g_input_size, PROT_READ, MAP_PRIVATE, input_fd, 0);\n+    if (g_input_data == MAP_FAILED) {\n+        ERROR(\"Failed to mmap input file '%s': %s\\n\", input_path, strerror(errno));\n+        goto out;\n+    }\n+\n+    load_wrap_key(wrap_key_path, &g_wrap_key);\n+    derive_main_key(&g_wrap_key, &((metadata_plain_t*)g_input_data)->metadata_key_id,\n+                    &g_meta_key);\n+\n+    g_input_name = basename(input_path);\n+    g_output_path_size = strlen(g_input_name) + strlen(g_output_dir) + 256;\n+    g_output_path = malloc(g_output_path_size);\n+    if (!g_output_path) {\n+        ERROR(\"No memory\\n\");\n+        goto out;\n+    }\n+\n+    tamper_truncate();\n+    tamper_modify();\n+    ret = 0;\n+\n+out:\n+    \/* skip cleanup as we are in main() *\/\n+    return ret;\n+}\n","files":{"\/LibOS\/shim\/test\/fs\/test_pf.py":{"changes":[{"diff":"\n \n     def __corrupt_file(self, input_path, output_path):\n         cmd = [self.PF_TAMPER, '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path]\n-        return self.run_native_binary(cmd)\n+        return self.run_native_binary(cmd, libpath=os.path.join(os.getcwd(), 'lib'))\n \n     # invalid\/corrupted files\n-    @expectedFailureIf(HAS_SGX)\n-    # pylint: disable=fixme\n     def test_500_invalid(self):\n-        # TODO: port these to the new file format\n         invalid_dir = os.path.join(self.TEST_DIR, 'pf_invalid')\n-        # files below should work normally (benign modifications)\n-        should_pass = ['chunk_padding_1_fixed', 'chunk_padding_2_fixed', 'chunk_data_3',\n-                       'chunk_data_3_fixed', 'chunk_data_4', 'chunk_data_4_fixed']\n         if not os.path.exists(invalid_dir):\n             os.mkdir(invalid_dir)\n+\n         # prepare valid encrypted file (largest one for maximum possible corruptions)\n         original_input = self.OUTPUT_FILES[-1]\n         self.__encrypt_file(self.INPUT_FILES[-1], original_input)\n         # generate invalid files based on the above\n         self.__corrupt_file(original_input, invalid_dir)\n+\n         # try to decrypt invalid files\n         for name in os.listdir(invalid_dir):\n             invalid = os.path.join(invalid_dir, name)\n","add":3,"remove":7,"filename":"\/LibOS\/shim\/test\/fs\/test_pf.py","badparts":["        return self.run_native_binary(cmd)","    @expectedFailureIf(HAS_SGX)","        should_pass = ['chunk_padding_1_fixed', 'chunk_padding_2_fixed', 'chunk_data_3',","                       'chunk_data_3_fixed', 'chunk_data_4', 'chunk_data_4_fixed']"],"goodparts":["        return self.run_native_binary(cmd, libpath=os.path.join(os.getcwd(), 'lib'))"]},{"diff":"\n             input_path = os.path.join(invalid_dir, os.path.basename(original_input))\n             # copy the file so it has the original file name (for allowed path check)\n             shutil.copy(invalid, input_path)\n-            should_pass = any(s in name for s in should_pass)\n \n             try:\n                 args = ['decrypt', '-V', '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path]\n                 self.__pf_crypt(args)\n             except subprocess.CalledProcessError as exc:\n-                if should_pass:\n-                    self.assertEqual(exc.returncode, 0)\n-                else:\n-                    self.assertNotEqual(exc.returncode, 0)\n+                # decryption of invalid file must fail with -1 (wrapped to 255)\n+                self.assertEqual(exc.returncode, 255)\n             else:\n-                if not should_pass:\n-                    print('[!] Fail: successfully decrypted file: ' + name)\n-                    self.fail()\n+                print('[!] Fail: successfully decrypted file: ' + name)\n+                self.fail()","add":4,"remove":8,"filename":"\/LibOS\/shim\/test\/fs\/test_pf.py","badparts":["            should_pass = any(s in name for s in should_pass)","                if should_pass:","                    self.assertEqual(exc.returncode, 0)","                else:","                    self.assertNotEqual(exc.returncode, 0)","                if not should_pass:","                    print('[!] Fail: successfully decrypted file: ' + name)","                    self.fail()"],"goodparts":["                self.assertEqual(exc.returncode, 255)","                print('[!] Fail: successfully decrypted file: ' + name)","                self.fail()"]}],"source":"\n import filecmp import os import shutil import subprocess import unittest from test_fs import( TC_00_FileSystem, ) from regression import( HAS_SGX, expectedFailureIf, ) @unittest.skipUnless(HAS_SGX, 'Protected files require SGX support') class TC_50_ProtectedFiles(TC_00_FileSystem): @classmethod def setUpClass(cls): cls.PF_CRYPT='bin\/pf_crypt' cls.PF_TAMPER='bin\/pf_tamper' cls.WRAP_KEY=os.path.join(cls.TEST_DIR, 'wrap-key') cls.CONST_WRAP_KEY=[0xff, 0xee, 0xdd, 0xcc, 0xbb, 0xaa, 0x99, 0x88, 0x77, 0x66, 0x55, 0x44, 0x33, 0x22, 0x11, 0x00] cls.ENCRYPTED_DIR=os.path.join(cls.TEST_DIR, 'pf_input') cls.ENCRYPTED_FILES=[os.path.join(cls.ENCRYPTED_DIR, str(v)) for v in cls.FILE_SIZES] cls.LIB_PATH=os.path.join(os.getcwd(), 'lib') super().setUpClass() if not os.path.exists(cls.ENCRYPTED_DIR): os.mkdir(cls.ENCRYPTED_DIR) cls.OUTPUT_DIR=os.path.join(cls.TEST_DIR, 'pf_output') cls.OUTPUT_FILES=[os.path.join(cls.OUTPUT_DIR, str(x)) for x in cls.FILE_SIZES] cls.__set_default_key(cls) for i in cls.INDEXES: cmd=[cls.PF_CRYPT, 'encrypt', '-w', cls.WRAP_KEY, '-i', cls.INPUT_FILES[i], '-o', cls.ENCRYPTED_FILES[i]] cls.run_native_binary(cmd, libpath=os.path.join(os.getcwd(), 'lib')) def __pf_crypt(self, args): args.insert(0, self.PF_CRYPT) return self.run_native_binary(args, libpath=os.path.join(os.getcwd(), 'lib')) def __set_default_key(self): with open(self.WRAP_KEY, 'wb') as file: file.write(bytes(self.CONST_WRAP_KEY)) def copy_input(self, input_path, output_path): self.__encrypt_file(input_path, output_path) def __encrypt_file(self, input_path, output_path): args=['encrypt', '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path] stdout, stderr=self.__pf_crypt(args) return(stdout, stderr) def __decrypt_file(self, input_path, output_path): args=['decrypt', '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path] stdout, stderr=self.__pf_crypt(args) return(stdout, stderr) def test_000_gen_key(self): key_path=os.path.join(self.TEST_DIR, 'tmpkey') args=['gen-key', '-w', key_path] stdout, _=self.__pf_crypt(args) self.assertIn('Wrap key saved to: ' +key_path, stdout) self.assertEqual(os.path.getsize(key_path), 16) os.remove(key_path) def test_010_encrypt_decrypt(self): for i in self.INDEXES: self.__encrypt_file(self.INPUT_FILES[i], self.OUTPUT_FILES[i]) self.assertFalse(filecmp.cmp(self.INPUT_FILES[i], self.OUTPUT_FILES[i], shallow=False)) dec_path=os.path.join(self.OUTPUT_DIR, os.path.basename(self.OUTPUT_FILES[i]) +'.dec') self.__decrypt_file(self.OUTPUT_FILES[i], dec_path) self.assertTrue(filecmp.cmp(self.INPUT_FILES[i], dec_path, shallow=False)) def test_100_open_close(self): input_path=self.ENCRYPTED_FILES[-1] output_path=os.path.join(self.OUTPUT_DIR, 'test_100') stdout, stderr=self.run_binary(['open_close', 'R', input_path]) self.verify_open_close(stdout, stderr, input_path, 'input') try: stdout, stderr=self.run_binary(['open_close', 'W', output_path]) self.assertIn('ERROR: Failed to open output file', stderr) except subprocess.CalledProcessError as exc: self.assertNotEqual(exc.returncode, 0) self.assertTrue(os.path.isfile(output_path)) else: print('[!] Fail: open_close returned 0') self.fail() def test_101_open_flags(self): file_path=os.path.join(self.OUTPUT_DIR, 'test_101') stdout, stderr=self.run_binary(['open_flags', file_path]) self.verify_open_flags(stdout, stderr) def test_115_seek_tell(self): plaintext_path=self.INPUT_FILES[-1] input_path=self.ENCRYPTED_FILES[-1] output_path_1=os.path.join(self.OUTPUT_DIR, 'test_115a') output_path_2=os.path.join(self.OUTPUT_DIR, 'test_115b') self.copy_input(plaintext_path, output_path_1) self.copy_input(plaintext_path, output_path_2) stdout, stderr=self.run_binary(['seek_tell', input_path, output_path_1, output_path_2]) self.verify_seek_tell(stdout, stderr, input_path, output_path_1, output_path_2, self.FILE_SIZES[-1]) def test_130_file_stat(self): for i in self.INDEXES: input_path=self.ENCRYPTED_FILES[i] output_path=self.OUTPUT_FILES[i] size=str(self.FILE_SIZES[i]) self.copy_input(self.INPUT_FILES[i], output_path) stdout, stderr=self.run_binary(['stat', input_path, output_path]) self.verify_stat(stdout, stderr, input_path, output_path, size) def verify_size(self, file, size): dec_path=os.path.join(self.OUTPUT_DIR, os.path.basename(file) +'.dec') self.__decrypt_file(file, dec_path) self.assertEqual(os.stat(dec_path).st_size, size) @expectedFailureIf(HAS_SGX) def test_140_file_truncate(self): self.fail() def test_150_file_rename(self): path1=os.path.join(self.OUTPUT_DIR, 'test_150a') path2=os.path.join(self.OUTPUT_DIR, 'test_150b') self.copy_input(self.ENCRYPTED_FILES[-1], path1) shutil.copy(path1, path2) args=['decrypt', '-V', '-w', self.WRAP_KEY, '-i', path2, '-o', path1] try: self.__pf_crypt(args) except subprocess.CalledProcessError as exc: self.assertNotEqual(exc.returncode, 0) else: print('[!] Fail: successfully decrypted renamed file: ' +path2) self.fail() def verify_copy_content(self, input_path, output_path): dec_path=os.path.join(self.OUTPUT_DIR, os.path.basename(output_path) +'.dec') self.__decrypt_file(output_path, dec_path) self.assertTrue(filecmp.cmp(input_path, dec_path, shallow=False)) def do_copy_test(self, executable, timeout): stdout, stderr=self.run_binary([executable, self.ENCRYPTED_DIR, self.OUTPUT_DIR], timeout=timeout) self.verify_copy(stdout, stderr, self.ENCRYPTED_DIR, executable) def test_204_copy_dir_mmap_whole(self): self.do_copy_test('copy_mmap_whole', 30) def test_205_copy_dir_mmap_seq(self): self.do_copy_test('copy_mmap_seq', 60) def test_206_copy_dir_mmap_rev(self): self.do_copy_test('copy_mmap_rev', 60) def test_210_copy_dir_mounted(self): executable='copy_whole' stdout, stderr=self.run_binary([executable, '\/mounted\/pf_input', '\/mounted\/pf_output'], timeout=30) self.verify_copy(stdout, stderr, '\/mounted\/pf_input', executable) def __corrupt_file(self, input_path, output_path): cmd=[self.PF_TAMPER, '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path] return self.run_native_binary(cmd) @expectedFailureIf(HAS_SGX) def test_500_invalid(self): invalid_dir=os.path.join(self.TEST_DIR, 'pf_invalid') should_pass=['chunk_padding_1_fixed', 'chunk_padding_2_fixed', 'chunk_data_3', 'chunk_data_3_fixed', 'chunk_data_4', 'chunk_data_4_fixed'] if not os.path.exists(invalid_dir): os.mkdir(invalid_dir) original_input=self.OUTPUT_FILES[-1] self.__encrypt_file(self.INPUT_FILES[-1], original_input) self.__corrupt_file(original_input, invalid_dir) for name in os.listdir(invalid_dir): invalid=os.path.join(invalid_dir, name) output_path=os.path.join(self.OUTPUT_DIR, name) input_path=os.path.join(invalid_dir, os.path.basename(original_input)) shutil.copy(invalid, input_path) should_pass=any(s in name for s in should_pass) try: args=['decrypt', '-V', '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path] self.__pf_crypt(args) except subprocess.CalledProcessError as exc: if should_pass: self.assertEqual(exc.returncode, 0) else: self.assertNotEqual(exc.returncode, 0) else: if not should_pass: print('[!] Fail: successfully decrypted file: ' +name) self.fail() ","sourceWithComments":"#!\/usr\/bin\/env python3\n\nimport filecmp\nimport os\nimport shutil\nimport subprocess\nimport unittest\n\nfrom test_fs import (\n    TC_00_FileSystem,\n)\n\nfrom regression import (\n    HAS_SGX,\n    expectedFailureIf,\n)\n\n@unittest.skipUnless(HAS_SGX, 'Protected files require SGX support')\nclass TC_50_ProtectedFiles(TC_00_FileSystem):\n    @classmethod\n    def setUpClass(cls):\n        cls.PF_CRYPT = 'bin\/pf_crypt'\n        cls.PF_TAMPER = 'bin\/pf_tamper'\n        cls.WRAP_KEY = os.path.join(cls.TEST_DIR, 'wrap-key')\n        # CONST_WRAP_KEY must match the one in manifest\n        cls.CONST_WRAP_KEY = [0xff, 0xee, 0xdd, 0xcc, 0xbb, 0xaa, 0x99, 0x88,\n                              0x77, 0x66, 0x55, 0x44, 0x33, 0x22, 0x11, 0x00]\n        cls.ENCRYPTED_DIR = os.path.join(cls.TEST_DIR, 'pf_input')\n        cls.ENCRYPTED_FILES = [os.path.join(cls.ENCRYPTED_DIR, str(v)) for v in cls.FILE_SIZES]\n        cls.LIB_PATH = os.path.join(os.getcwd(), 'lib')\n\n        super().setUpClass()\n        if not os.path.exists(cls.ENCRYPTED_DIR):\n            os.mkdir(cls.ENCRYPTED_DIR)\n        cls.OUTPUT_DIR = os.path.join(cls.TEST_DIR, 'pf_output')\n        cls.OUTPUT_FILES = [os.path.join(cls.OUTPUT_DIR, str(x)) for x in cls.FILE_SIZES]\n        # create encrypted files\n        cls.__set_default_key(cls)\n        for i in cls.INDEXES:\n            cmd = [cls.PF_CRYPT, 'encrypt', '-w', cls.WRAP_KEY, '-i', cls.INPUT_FILES[i], '-o',\n                   cls.ENCRYPTED_FILES[i]]\n\n            cls.run_native_binary(cmd, libpath=os.path.join(os.getcwd(), 'lib'))\n\n    def __pf_crypt(self, args):\n        args.insert(0, self.PF_CRYPT)\n        return self.run_native_binary(args, libpath=os.path.join(os.getcwd(), 'lib'))\n\n    def __set_default_key(self):\n        with open(self.WRAP_KEY, 'wb') as file:\n            file.write(bytes(self.CONST_WRAP_KEY))\n\n    # overrides TC_00_FileSystem to encrypt the file instead of just copying\n    def copy_input(self, input_path, output_path):\n        self.__encrypt_file(input_path, output_path)\n\n    def __encrypt_file(self, input_path, output_path):\n        args = ['encrypt', '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path]\n        stdout, stderr = self.__pf_crypt(args)\n        return (stdout, stderr)\n\n    def __decrypt_file(self, input_path, output_path):\n        args = ['decrypt', '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path]\n        stdout, stderr = self.__pf_crypt(args)\n        return (stdout, stderr)\n\n    def test_000_gen_key(self):\n        # test random key generation\n        key_path = os.path.join(self.TEST_DIR, 'tmpkey')\n        args = ['gen-key', '-w', key_path]\n        stdout, _ = self.__pf_crypt(args)\n        self.assertIn('Wrap key saved to: ' + key_path, stdout)\n        self.assertEqual(os.path.getsize(key_path), 16)\n        os.remove(key_path)\n\n    def test_010_encrypt_decrypt(self):\n        for i in self.INDEXES:\n            self.__encrypt_file(self.INPUT_FILES[i], self.OUTPUT_FILES[i])\n            self.assertFalse(filecmp.cmp(self.INPUT_FILES[i], self.OUTPUT_FILES[i], shallow=False))\n            dec_path = os.path.join(self.OUTPUT_DIR,\n                                    os.path.basename(self.OUTPUT_FILES[i]) + '.dec')\n            self.__decrypt_file(self.OUTPUT_FILES[i], dec_path)\n            self.assertTrue(filecmp.cmp(self.INPUT_FILES[i], dec_path, shallow=False))\n\n    # overrides TC_00_FileSystem to change input dir (from plaintext to encrypted)\n    def test_100_open_close(self):\n        # the test binary expects a path to read-only (existing) file or a path to file that\n        # will get created\n        input_path = self.ENCRYPTED_FILES[-1] # existing file\n        output_path = os.path.join(self.OUTPUT_DIR, 'test_100') # new file\n        stdout, stderr = self.run_binary(['open_close', 'R', input_path])\n        self.verify_open_close(stdout, stderr, input_path, 'input')\n        # the following test tries to open multiple handles to a single writable PF, should fail\n        try:\n            stdout, stderr = self.run_binary(['open_close', 'W', output_path])\n            self.assertIn('ERROR: Failed to open output file', stderr)\n        except subprocess.CalledProcessError as exc:\n            self.assertNotEqual(exc.returncode, 0)\n            self.assertTrue(os.path.isfile(output_path))\n        else:\n            print('[!] Fail: open_close returned 0')\n            self.fail()\n\n    # overrides TC_00_FileSystem to change input dir (from plaintext to encrypted)\n    def test_101_open_flags(self):\n        # the test binary expects a path to file that will get created\n        file_path = os.path.join(self.OUTPUT_DIR, 'test_101') # new file\n        stdout, stderr = self.run_binary(['open_flags', file_path])\n        self.verify_open_flags(stdout, stderr)\n\n    # overrides TC_00_FileSystem to change input dir (from plaintext to encrypted)\n    def test_115_seek_tell(self):\n        # the test binary expects a path to read-only (existing) file and two paths to files that\n        # will get created\n        plaintext_path = self.INPUT_FILES[-1]\n        input_path = self.ENCRYPTED_FILES[-1] # existing file\n        output_path_1 = os.path.join(self.OUTPUT_DIR, 'test_115a') # writable files\n        output_path_2 = os.path.join(self.OUTPUT_DIR, 'test_115b')\n        self.copy_input(plaintext_path, output_path_1) # encrypt\n        self.copy_input(plaintext_path, output_path_2)\n        stdout, stderr = self.run_binary(['seek_tell', input_path, output_path_1, output_path_2])\n        self.verify_seek_tell(stdout, stderr, input_path, output_path_1, output_path_2,\n                              self.FILE_SIZES[-1])\n\n    # overrides TC_00_FileSystem to change input dir (from plaintext to encrypted)\n    def test_130_file_stat(self):\n        # the test binary expects a path to read-only (existing) file and a path to file that\n        # will get created\n        for i in self.INDEXES:\n            input_path = self.ENCRYPTED_FILES[i]\n            output_path = self.OUTPUT_FILES[i]\n            size = str(self.FILE_SIZES[i])\n            self.copy_input(self.INPUT_FILES[i], output_path)\n            stdout, stderr = self.run_binary(['stat', input_path, output_path])\n            self.verify_stat(stdout, stderr, input_path, output_path, size)\n\n    # overrides TC_00_FileSystem to decrypt output\n    def verify_size(self, file, size):\n        dec_path = os.path.join(self.OUTPUT_DIR, os.path.basename(file) + '.dec')\n        self.__decrypt_file(file, dec_path)\n        self.assertEqual(os.stat(dec_path).st_size, size)\n\n    @expectedFailureIf(HAS_SGX)\n    # pylint: disable=fixme\n    def test_140_file_truncate(self):\n        self.fail() # TODO: port these to the new file format\n\n    def test_150_file_rename(self):\n        path1 = os.path.join(self.OUTPUT_DIR, 'test_150a')\n        path2 = os.path.join(self.OUTPUT_DIR, 'test_150b')\n        self.copy_input(self.ENCRYPTED_FILES[-1], path1)\n        shutil.copy(path1, path2)\n        # accessing renamed file should fail\n        args = ['decrypt', '-V', '-w', self.WRAP_KEY, '-i', path2, '-o', path1]\n        try:\n            self.__pf_crypt(args)\n        except subprocess.CalledProcessError as exc:\n            self.assertNotEqual(exc.returncode, 0)\n        else:\n            print('[!] Fail: successfully decrypted renamed file: ' + path2)\n            self.fail()\n\n    # overrides TC_00_FileSystem to decrypt output\n    def verify_copy_content(self, input_path, output_path):\n        dec_path = os.path.join(self.OUTPUT_DIR, os.path.basename(output_path) + '.dec')\n        self.__decrypt_file(output_path, dec_path)\n        self.assertTrue(filecmp.cmp(input_path, dec_path, shallow=False))\n\n    # overrides TC_00_FileSystem to change input dir (from plaintext to encrypted)\n    def do_copy_test(self, executable, timeout):\n        stdout, stderr = self.run_binary([executable, self.ENCRYPTED_DIR, self.OUTPUT_DIR],\n                                         timeout=timeout)\n        self.verify_copy(stdout, stderr, self.ENCRYPTED_DIR, executable)\n\n    # overrides TC_00_FileSystem to not skip this on SGX\n    def test_204_copy_dir_mmap_whole(self):\n        self.do_copy_test('copy_mmap_whole', 30)\n\n    # overrides TC_00_FileSystem to not skip this on SGX\n    def test_205_copy_dir_mmap_seq(self):\n        self.do_copy_test('copy_mmap_seq', 60)\n\n    # overrides TC_00_FileSystem to not skip this on SGX\n    def test_206_copy_dir_mmap_rev(self):\n        self.do_copy_test('copy_mmap_rev', 60)\n\n    # overrides TC_00_FileSystem to change dirs (from plaintext to encrypted)\n    def test_210_copy_dir_mounted(self):\n        executable = 'copy_whole'\n        stdout, stderr = self.run_binary([executable, '\/mounted\/pf_input', '\/mounted\/pf_output'],\n                                         timeout=30)\n        self.verify_copy(stdout, stderr, '\/mounted\/pf_input', executable)\n\n    def __corrupt_file(self, input_path, output_path):\n        cmd = [self.PF_TAMPER, '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path]\n        return self.run_native_binary(cmd)\n\n    # invalid\/corrupted files\n    @expectedFailureIf(HAS_SGX)\n    # pylint: disable=fixme\n    def test_500_invalid(self):\n        # TODO: port these to the new file format\n        invalid_dir = os.path.join(self.TEST_DIR, 'pf_invalid')\n        # files below should work normally (benign modifications)\n        should_pass = ['chunk_padding_1_fixed', 'chunk_padding_2_fixed', 'chunk_data_3',\n                       'chunk_data_3_fixed', 'chunk_data_4', 'chunk_data_4_fixed']\n        if not os.path.exists(invalid_dir):\n            os.mkdir(invalid_dir)\n        # prepare valid encrypted file (largest one for maximum possible corruptions)\n        original_input = self.OUTPUT_FILES[-1]\n        self.__encrypt_file(self.INPUT_FILES[-1], original_input)\n        # generate invalid files based on the above\n        self.__corrupt_file(original_input, invalid_dir)\n        # try to decrypt invalid files\n        for name in os.listdir(invalid_dir):\n            invalid = os.path.join(invalid_dir, name)\n            output_path = os.path.join(self.OUTPUT_DIR, name)\n            input_path = os.path.join(invalid_dir, os.path.basename(original_input))\n            # copy the file so it has the original file name (for allowed path check)\n            shutil.copy(invalid, input_path)\n            should_pass = any(s in name for s in should_pass)\n\n            try:\n                args = ['decrypt', '-V', '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path]\n                self.__pf_crypt(args)\n            except subprocess.CalledProcessError as exc:\n                if should_pass:\n                    self.assertEqual(exc.returncode, 0)\n                else:\n                    self.assertNotEqual(exc.returncode, 0)\n            else:\n                if not should_pass:\n                    print('[!] Fail: successfully decrypted file: ' + name)\n                    self.fail()\n"}},"msg":"[Pal\/Linux-SGX] Test malicious modifications to protected files\n\nThis commit adds a new PF utility `pf_tamper` that tampers with\nvalid protected files and uses this utility to test that the PF\nlogic in Linux-SGX detects such malicious modifications.\n\nThis commit also moves out the PF-format macros and structs from\n`protected_files_internal.h` to `protected_files_format.h` for\nbetter readability.\n\nCo-authored-by: Dmitrii Kuvaiskii <dmitrii.kuvaiskii@intel.com>"}},"https:\/\/github.com\/gramineproject\/graphene":{"345d271e663725d58ce941fc49f08ebc02c32fa2":{"url":"https:\/\/api.github.com\/repos\/gramineproject\/graphene\/commits\/345d271e663725d58ce941fc49f08ebc02c32fa2","html_url":"https:\/\/github.com\/gramineproject\/graphene\/commit\/345d271e663725d58ce941fc49f08ebc02c32fa2","message":"[Pal\/Linux-SGX] Test malicious modifications to protected files\n\nThis commit adds a new PF utility `pf_tamper` that tampers with\nvalid protected files and uses this utility to test that the PF\nlogic in Linux-SGX detects such malicious modifications.\n\nThis commit also moves out the PF-format macros and structs from\n`protected_files_internal.h` to `protected_files_format.h` for\nbetter readability.\n\nCo-authored-by: Dmitrii Kuvaiskii <dmitrii.kuvaiskii@intel.com>","sha":"345d271e663725d58ce941fc49f08ebc02c32fa2","keyword":"tampering malicious","diff":"diff --git a\/LibOS\/shim\/test\/fs\/test_pf.py b\/LibOS\/shim\/test\/fs\/test_pf.py\nindex e430e02af8..7b1621d7ae 100644\n--- a\/LibOS\/shim\/test\/fs\/test_pf.py\n+++ b\/LibOS\/shim\/test\/fs\/test_pf.py\n@@ -193,24 +193,20 @@ def test_210_copy_dir_mounted(self):\n \n     def __corrupt_file(self, input_path, output_path):\n         cmd = [self.PF_TAMPER, '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path]\n-        return self.run_native_binary(cmd)\n+        return self.run_native_binary(cmd, libpath=os.path.join(os.getcwd(), 'lib'))\n \n     # invalid\/corrupted files\n-    @expectedFailureIf(HAS_SGX)\n-    # pylint: disable=fixme\n     def test_500_invalid(self):\n-        # TODO: port these to the new file format\n         invalid_dir = os.path.join(self.TEST_DIR, 'pf_invalid')\n-        # files below should work normally (benign modifications)\n-        should_pass = ['chunk_padding_1_fixed', 'chunk_padding_2_fixed', 'chunk_data_3',\n-                       'chunk_data_3_fixed', 'chunk_data_4', 'chunk_data_4_fixed']\n         if not os.path.exists(invalid_dir):\n             os.mkdir(invalid_dir)\n+\n         # prepare valid encrypted file (largest one for maximum possible corruptions)\n         original_input = self.OUTPUT_FILES[-1]\n         self.__encrypt_file(self.INPUT_FILES[-1], original_input)\n         # generate invalid files based on the above\n         self.__corrupt_file(original_input, invalid_dir)\n+\n         # try to decrypt invalid files\n         for name in os.listdir(invalid_dir):\n             invalid = os.path.join(invalid_dir, name)\n@@ -218,17 +214,13 @@ def test_500_invalid(self):\n             input_path = os.path.join(invalid_dir, os.path.basename(original_input))\n             # copy the file so it has the original file name (for allowed path check)\n             shutil.copy(invalid, input_path)\n-            should_pass = any(s in name for s in should_pass)\n \n             try:\n                 args = ['decrypt', '-V', '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path]\n                 self.__pf_crypt(args)\n             except subprocess.CalledProcessError as exc:\n-                if should_pass:\n-                    self.assertEqual(exc.returncode, 0)\n-                else:\n-                    self.assertNotEqual(exc.returncode, 0)\n+                # decryption of invalid file must fail with -1 (wrapped to 255)\n+                self.assertEqual(exc.returncode, 255)\n             else:\n-                if not should_pass:\n-                    print('[!] Fail: successfully decrypted file: ' + name)\n-                    self.fail()\n+                print('[!] Fail: successfully decrypted file: ' + name)\n+                self.fail()\ndiff --git a\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files.c b\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files.c\nindex 4f0c59ba23..d020a1e24a 100644\n--- a\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files.c\n+++ b\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files.c\n@@ -5,6 +5,8 @@\n  *\/\n \n #include \"api.h\"\n+#include \"protected_files.h\"\n+#include \"protected_files_format.h\"\n #include \"protected_files_internal.h\"\n \n #ifndef IN_PAL\n@@ -65,20 +67,6 @@ static pf_random_f          g_cb_random          = NULL;\n static pf_iv_t g_empty_iv = {0};\n static bool g_initialized = false;\n \n-#define METADATA_KEY_NAME \"SGX-PROTECTED-FS-METADATA-KEY\"\n-#define MAX_LABEL_SIZE    64\n-\n-static_assert(sizeof(METADATA_KEY_NAME) <= MAX_LABEL_SIZE, \"label too long\");\n-\n-#pragma pack(push, 1)\n-typedef struct {\n-    uint32_t index;\n-    char label[MAX_LABEL_SIZE]; \/\/ must be NULL terminated\n-    pf_keyid_t nonce;\n-    uint32_t output_len; \/\/ in bits\n-} kdf_input_t;\n-#pragma pack(pop)\n-\n \/\/ The key derivation function follow recommendations from NIST Special Publication 800-108:\n \/\/ Recommendation for Key Derivation Using Pseudorandom Functions\n \/\/ https:\/\/nvlpubs.nist.gov\/nistpubs\/Legacy\/SP\/nistspecialpublication800-108.pdf\ndiff --git a\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files_format.h b\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files_format.h\nnew file mode 100644\nindex 0000000000..2394e53ef7\n--- \/dev\/null\n+++ b\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files_format.h\n@@ -0,0 +1,139 @@\n+\/* SPDX-License-Identifier: LGPL-3.0-or-later *\/\n+\/* Copyright (C) 2019-2020 Invisible Things Lab\n+ *                         Rafal Wojdyla <omeg@invisiblethingslab.com>\n+ * Copyright (C) 2011-2020 Intel Corporation\n+ *\/\n+\n+#ifndef PROTECTED_FILES_FORMAT_H_\n+#define PROTECTED_FILES_FORMAT_H_\n+\n+#include <limits.h>\n+\n+#include \"assert.h\"\n+#include \"list.h\"\n+#include \"protected_files.h\"\n+\n+#define PF_FILE_ID       0x46505f4850415247 \/* GRAPH_PF *\/\n+#define PF_MAJOR_VERSION 0x01\n+#define PF_MINOR_VERSION 0x00\n+\n+#define METADATA_KEY_NAME \"SGX-PROTECTED-FS-METADATA-KEY\"\n+#define MAX_LABEL_SIZE    64\n+\n+static_assert(sizeof(METADATA_KEY_NAME) <= MAX_LABEL_SIZE, \"label too long\");\n+\n+#pragma pack(push, 1)\n+\n+typedef struct _metadata_plain {\n+    uint64_t   file_id;\n+    uint8_t    major_version;\n+    uint8_t    minor_version;\n+    pf_keyid_t metadata_key_id;\n+    pf_mac_t   metadata_gmac; \/* GCM mac *\/\n+} metadata_plain_t;\n+\n+#define PATH_MAX_SIZE (260 + 512)\n+\n+\/\/ these are all defined as relative to node size, so we can decrease node size in tests\n+\/\/ and have deeper tree\n+#define MD_USER_DATA_SIZE (PF_NODE_SIZE * 3 \/ 4) \/\/ 3072\n+static_assert(MD_USER_DATA_SIZE == 3072, \"bad struct size\");\n+\n+typedef struct _metadata_encrypted {\n+    char     path[PATH_MAX_SIZE];\n+    uint64_t size;\n+    pf_key_t mht_key;\n+    pf_mac_t mht_gmac;\n+    uint8_t  data[MD_USER_DATA_SIZE];\n+} metadata_encrypted_t;\n+\n+typedef uint8_t metadata_encrypted_blob_t[sizeof(metadata_encrypted_t)];\n+\n+#define METADATA_NODE_SIZE PF_NODE_SIZE\n+\n+typedef uint8_t metadata_padding_t[METADATA_NODE_SIZE -\n+                                   (sizeof(metadata_plain_t) + sizeof(metadata_encrypted_blob_t))];\n+\n+typedef struct _metadata_node {\n+    metadata_plain_t          plain_part;\n+    metadata_encrypted_blob_t encrypted_part;\n+    metadata_padding_t        padding;\n+} metadata_node_t;\n+\n+static_assert(sizeof(metadata_node_t) == PF_NODE_SIZE, \"sizeof(metadata_node_t)\");\n+\n+typedef struct _data_node_crypto {\n+    pf_key_t key;\n+    pf_mac_t gmac;\n+} gcm_crypto_data_t;\n+\n+\/\/ for PF_NODE_SIZE == 4096, we have 96 attached data nodes and 32 mht child nodes\n+\/\/ for PF_NODE_SIZE == 2048, we have 48 attached data nodes and 16 mht child nodes\n+\/\/ for PF_NODE_SIZE == 1024, we have 24 attached data nodes and 8 mht child nodes\n+\/\/ 3\/4 of the node size is dedicated to data nodes\n+#define ATTACHED_DATA_NODES_COUNT ((PF_NODE_SIZE \/ sizeof(gcm_crypto_data_t)) * 3 \/ 4)\n+static_assert(ATTACHED_DATA_NODES_COUNT == 96, \"ATTACHED_DATA_NODES_COUNT\");\n+\/\/ 1\/4 of the node size is dedicated to child mht nodes\n+#define CHILD_MHT_NODES_COUNT ((PF_NODE_SIZE \/ sizeof(gcm_crypto_data_t)) * 1 \/ 4)\n+static_assert(CHILD_MHT_NODES_COUNT == 32, \"CHILD_MHT_NODES_COUNT\");\n+\n+typedef struct _mht_node {\n+    gcm_crypto_data_t data_nodes_crypto[ATTACHED_DATA_NODES_COUNT];\n+    gcm_crypto_data_t mht_nodes_crypto[CHILD_MHT_NODES_COUNT];\n+} mht_node_t;\n+\n+static_assert(sizeof(mht_node_t) == PF_NODE_SIZE, \"sizeof(mht_node_t)\");\n+\n+typedef struct _data_node {\n+    uint8_t data[PF_NODE_SIZE];\n+} data_node_t;\n+\n+static_assert(sizeof(data_node_t) == PF_NODE_SIZE, \"sizeof(data_node_t)\");\n+\n+typedef struct _encrypted_node {\n+    uint8_t cipher[PF_NODE_SIZE];\n+} encrypted_node_t;\n+\n+static_assert(sizeof(encrypted_node_t) == PF_NODE_SIZE, \"sizeof(encrypted_node_t)\");\n+\n+#define MAX_PAGES_IN_CACHE 48\n+\n+typedef enum {\n+    FILE_MHT_NODE_TYPE  = 1,\n+    FILE_DATA_NODE_TYPE = 2,\n+} mht_node_type_e;\n+\n+\/\/ make sure these are the same size\n+static_assert(sizeof(mht_node_t) == sizeof(data_node_t),\n+              \"sizeof(mht_node_t) == sizeof(data_node_t)\");\n+\n+DEFINE_LIST(_file_node);\n+typedef struct _file_node {\n+    LIST_TYPE(_file_node) list;\n+    uint8_t type;\n+    uint64_t node_number;\n+    struct _file_node* parent;\n+    bool need_writing;\n+    bool new_node;\n+    struct {\n+        uint64_t physical_node_number;\n+        encrypted_node_t encrypted; \/\/ the actual data from the disk\n+    };\n+    union { \/\/ decrypted data\n+        mht_node_t mht;\n+        data_node_t data;\n+    } decrypted;\n+} file_node_t;\n+DEFINE_LISTP(_file_node);\n+\n+typedef struct {\n+    uint32_t index;\n+    char label[MAX_LABEL_SIZE]; \/\/ must be NULL terminated\n+    pf_keyid_t nonce;\n+    uint32_t output_len; \/\/ in bits\n+} kdf_input_t;\n+\n+#pragma pack(pop)\n+\n+#endif \/* PROTECTED_FILES_FORMAT_H_ *\/\n+\ndiff --git a\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files_internal.h b\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files_internal.h\nindex 4a31261d58..c06d1ecf69 100644\n--- a\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files_internal.h\n+++ b\/Pal\/src\/host\/Linux-SGX\/protected-files\/protected_files_internal.h\n@@ -13,116 +13,7 @@\n #include \"list.h\"\n #include \"lru_cache.h\"\n #include \"protected_files.h\"\n-\n-#define PF_FILE_ID       0x46505f4850415247 \/* GRAPH_PF *\/\n-#define PF_MAJOR_VERSION 0x01\n-#define PF_MINOR_VERSION 0x00\n-\n-#pragma pack(push, 1)\n-\n-typedef struct _metadata_plain {\n-    uint64_t   file_id;\n-    uint8_t    major_version;\n-    uint8_t    minor_version;\n-    pf_keyid_t metadata_key_id;\n-    pf_mac_t   metadata_gmac; \/* GCM mac *\/\n-} metadata_plain_t;\n-\n-#define PATH_MAX_SIZE (260 + 512)\n-\n-\/\/ these are all defined as relative to node size, so we can decrease node size in tests\n-\/\/ and have deeper tree\n-#define MD_USER_DATA_SIZE (PF_NODE_SIZE * 3 \/ 4) \/\/ 3072\n-static_assert(MD_USER_DATA_SIZE == 3072, \"bad struct size\");\n-\n-typedef struct _metadata_encrypted {\n-    char     path[PATH_MAX_SIZE];\n-    uint64_t size;\n-    pf_key_t mht_key;\n-    pf_mac_t mht_gmac;\n-    uint8_t  data[MD_USER_DATA_SIZE];\n-} metadata_encrypted_t;\n-\n-typedef uint8_t metadata_encrypted_blob_t[sizeof(metadata_encrypted_t)];\n-\n-#define METADATA_NODE_SIZE PF_NODE_SIZE\n-\n-typedef uint8_t metadata_padding_t[METADATA_NODE_SIZE -\n-                                   (sizeof(metadata_plain_t) + sizeof(metadata_encrypted_blob_t))];\n-\n-typedef struct _metadata_node {\n-    metadata_plain_t          plain_part;\n-    metadata_encrypted_blob_t encrypted_part;\n-    metadata_padding_t        padding;\n-} metadata_node_t;\n-\n-static_assert(sizeof(metadata_node_t) == PF_NODE_SIZE, \"sizeof(metadata_node_t)\");\n-\n-typedef struct _data_node_crypto {\n-    pf_key_t key;\n-    pf_mac_t gmac;\n-} gcm_crypto_data_t;\n-\n-\/\/ for PF_NODE_SIZE == 4096, we have 96 attached data nodes and 32 mht child nodes\n-\/\/ for PF_NODE_SIZE == 2048, we have 48 attached data nodes and 16 mht child nodes\n-\/\/ for PF_NODE_SIZE == 1024, we have 24 attached data nodes and 8 mht child nodes\n-\/\/ 3\/4 of the node size is dedicated to data nodes\n-#define ATTACHED_DATA_NODES_COUNT ((PF_NODE_SIZE \/ sizeof(gcm_crypto_data_t)) * 3 \/ 4)\n-static_assert(ATTACHED_DATA_NODES_COUNT == 96, \"ATTACHED_DATA_NODES_COUNT\");\n-\/\/ 1\/4 of the node size is dedicated to child mht nodes\n-#define CHILD_MHT_NODES_COUNT ((PF_NODE_SIZE \/ sizeof(gcm_crypto_data_t)) * 1 \/ 4)\n-static_assert(CHILD_MHT_NODES_COUNT == 32, \"CHILD_MHT_NODES_COUNT\");\n-\n-typedef struct _mht_node {\n-    gcm_crypto_data_t data_nodes_crypto[ATTACHED_DATA_NODES_COUNT];\n-    gcm_crypto_data_t mht_nodes_crypto[CHILD_MHT_NODES_COUNT];\n-} mht_node_t;\n-\n-static_assert(sizeof(mht_node_t) == PF_NODE_SIZE, \"sizeof(mht_node_t)\");\n-\n-typedef struct _data_node {\n-    uint8_t data[PF_NODE_SIZE];\n-} data_node_t;\n-\n-static_assert(sizeof(data_node_t) == PF_NODE_SIZE, \"sizeof(data_node_t)\");\n-\n-typedef struct _encrypted_node {\n-    uint8_t cipher[PF_NODE_SIZE];\n-} encrypted_node_t;\n-\n-static_assert(sizeof(encrypted_node_t) == PF_NODE_SIZE, \"sizeof(encrypted_node_t)\");\n-\n-#define MAX_PAGES_IN_CACHE 48\n-\n-typedef enum {\n-    FILE_MHT_NODE_TYPE  = 1,\n-    FILE_DATA_NODE_TYPE = 2,\n-} mht_node_type_e;\n-\n-\/\/ make sure these are the same size\n-static_assert(sizeof(mht_node_t) == sizeof(data_node_t),\n-              \"sizeof(mht_node_t) == sizeof(data_node_t)\");\n-\n-DEFINE_LIST(_file_node);\n-typedef struct _file_node {\n-    LIST_TYPE(_file_node) list;\n-    uint8_t type;\n-    uint64_t node_number;\n-    struct _file_node* parent;\n-    bool need_writing;\n-    bool new_node;\n-    struct {\n-        uint64_t physical_node_number;\n-        encrypted_node_t encrypted; \/\/ the actual data from the disk\n-    };\n-    union { \/\/ decrypted data\n-        mht_node_t mht;\n-        data_node_t data;\n-    } decrypted;\n-} file_node_t;\n-DEFINE_LISTP(_file_node);\n-\n-#pragma pack(pop)\n+#include \"protected_files_format.h\"\n \n struct pf_context {\n     metadata_node_t file_metadata; \/\/ actual data from disk's meta data node\ndiff --git a\/Pal\/src\/host\/Linux-SGX\/tools\/Makefile b\/Pal\/src\/host\/Linux-SGX\/tools\/Makefile\nindex a5903ad970..87bc78124b 100644\n--- a\/Pal\/src\/host\/Linux-SGX\/tools\/Makefile\n+++ b\/Pal\/src\/host\/Linux-SGX\/tools\/Makefile\n@@ -12,3 +12,4 @@ $(targets):\n \t$(MAKE) -C verify-ias-report $@\n \t$(MAKE) -C ra-tls $@\n \t$(MAKE) -C pf_crypt $@\n+\t$(MAKE) -C pf_tamper $@\ndiff --git a\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/.gitignore b\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/.gitignore\nnew file mode 100644\nindex 0000000000..a52536d82e\n--- \/dev\/null\n+++ b\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/.gitignore\n@@ -0,0 +1 @@\n+\/pf_tamper\ndiff --git a\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/Makefile b\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/Makefile\nnew file mode 100644\nindex 0000000000..94c826e4bd\n--- \/dev\/null\n+++ b\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/Makefile\n@@ -0,0 +1,31 @@\n+include ..\/..\/..\/..\/..\/..\/Scripts\/Makefile.configs\n+include ..\/..\/..\/..\/..\/..\/Scripts\/Makefile.rules\n+\n+CFLAGS += -I..\/.. \\\n+          -I..\/common \\\n+          -I..\/..\/protected-files \\\n+          -I..\/..\/..\/..\/..\/include\/lib \\\n+          -D_GNU_SOURCE\n+\n+LDLIBS += -L..\/common \\\n+          -L..\/..\/..\/..\/..\/lib\/crypto\/mbedtls\/install\/lib \\\n+          -lsgx_util -lmbedcrypto\n+\n+PREFIX ?= \/usr\/local\n+\n+pf_tamper: pf_tamper.o\n+\t$(call cmd,csingle)\n+\n+.PHONY: all\n+all: pf_tamper\n+\n+.PHONY: install\n+install:\n+\tinstall -D pf_tamper -t ${PREFIX}\/bin\n+\n+.PHONY: clean\n+clean:\n+\t$(RM) *.o pf_tamper\n+\n+.PHONY: distclean\n+distclean: clean\ndiff --git a\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/pf_tamper.c b\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/pf_tamper.c\nnew file mode 100644\nindex 0000000000..4d39a0d586\n--- \/dev\/null\n+++ b\/Pal\/src\/host\/Linux-SGX\/tools\/pf_tamper\/pf_tamper.c\n@@ -0,0 +1,476 @@\n+\/* SPDX-License-Identifier: LGPL-3.0-or-later *\/\n+\/* Copyright (C) 2019-2020 Invisible Things Lab\n+ *                         Rafal Wojdyla <omeg@invisiblethingslab.com>\n+ *\/\n+\n+#include <fcntl.h>\n+#include <getopt.h>\n+#include <stdlib.h>\n+#include <sys\/mman.h>\n+#include <unistd.h>\n+\n+#include \"pf_util.h\"\n+#include \"protected_files.h\"\n+#include \"protected_files_format.h\"\n+#include \"util.h\"\n+\n+\/* Tamper with a PF in various ways for testing purposes. The PF is assumed to be valid and have at\n+ * least enough data to contain two MHT nodes. *\/\n+\n+\/* Command line options *\/\n+struct option g_options[] = {\n+    { \"input\", required_argument, 0, 'i' },\n+    { \"output\", required_argument, 0, 'o' },\n+    { \"wrap-key\", required_argument, 0, 'w' },\n+    { \"verbose\", no_argument, 0, 'v' },\n+    { \"help\", no_argument, 0, 'h' },\n+    { 0, 0, 0, 0 }\n+};\n+\n+static void usage(void) {\n+    INFO(\"\\nUsage: pf_tamper [options]\\n\");\n+    INFO(\"\\nAvailable options:\\n\");\n+    INFO(\"  --help, -h           Display this help\\n\");\n+    INFO(\"  --verbose, -v        Enable verbose output\\n\");\n+    INFO(\"  --wrap-key, -w PATH  Path to wrap key file\\n\");\n+    INFO(\"  --input, -i PATH     Source file to be tampered with (must be a valid PF)\\n\");\n+    INFO(\"  --output, -o PATH    Directory where modified files will be written to\\n\");\n+}\n+\n+#define FATAL(fmt, ...) do { \\\n+    ERROR(fmt, ##__VA_ARGS__); \\\n+    exit(-1); \\\n+} while (0)\n+\n+ssize_t g_input_size = 0;\n+char* g_input_name = NULL;\n+void* g_input_data = MAP_FAILED;\n+char* g_output_dir = NULL;\n+char* g_output_path = NULL;\n+size_t g_output_path_size = 0;\n+pf_key_t g_wrap_key;\n+pf_key_t g_meta_key;\n+\n+static pf_iv_t g_empty_iv = {0};\n+\n+static void derive_main_key(const pf_key_t* kdk, const pf_keyid_t* key_id, pf_key_t* out_key) {\n+    kdf_input_t buf = {0};\n+    pf_status_t status;\n+\n+    buf.index = 1;\n+    strncpy(buf.label, METADATA_KEY_NAME, MAX_LABEL_SIZE);\n+    memcpy(&buf.nonce, key_id, sizeof(buf.nonce));\n+    buf.output_len = 0x80;\n+\n+    status = mbedtls_aes_gcm_encrypt(kdk, &g_empty_iv, &buf, sizeof(buf), NULL, 0, NULL, out_key);\n+    if (PF_FAILURE(status))\n+        FATAL(\"key derivation failed\\n\");\n+}\n+\n+static void make_output_path(const char* suffix) {\n+    snprintf(g_output_path, g_output_path_size, \"%s\/%s.%s\", g_output_dir, g_input_name, suffix);\n+    INFO(\"[*] %s\\n\", g_output_path);\n+}\n+\n+\/* PF layout (node size is PF_NODE_SIZE):\n+ * - Node 0: metadata (metadata_node_t)\n+ *   - metadata_plain_t\n+ *   - metadata_encrypted_t (may include MD_USER_DATA_SIZE bytes of data)\n+ *   - metadata_padding_t\n+ * - Node 1: MHT (mht_node_t)\n+ * - Node 2-97: data (ATTACHED_DATA_NODES_COUNT == 96)\n+ * - Node 98: MHT\n+ * - Node 99-195: data\n+ * - ...\n+ *\/\n+static void truncate_file(const char* suffix, size_t output_size) {\n+    int ret;\n+\n+    make_output_path(suffix);\n+\n+    if (output_size < g_input_size) {\n+        ret = write_file(g_output_path, output_size, g_input_data);\n+    } else {\n+        ret = write_file(g_output_path, g_input_size, g_input_data);\n+        if (ret < 0)\n+            goto out;\n+        ret = truncate(g_output_path, output_size);\n+    }\n+out:\n+    if (ret < 0)\n+        FATAL(\"truncate_file failed: %d\\n\", ret);\n+}\n+\n+#define FIELD_SIZEOF(t, f) (sizeof(((t*)0)->f))\n+#define FIELD_TRUNCATED(t, f) (offsetof(t, f) + (FIELD_SIZEOF(t, f) \/ 2))\n+#define DATA_CRYPTO_SIZE (FIELD_SIZEOF(mht_node_t, data_nodes_crypto))\n+\n+static void tamper_truncate(void) {\n+    size_t mdps = sizeof(metadata_plain_t);\n+    DBG(\"size(metadata_plain_t)             = 0x%04lx\\n\", sizeof(metadata_plain_t));\n+    DBG(\"metadata_plain_t.file_id           : 0x%04lx (0x%04lx)\\n\",\n+        offsetof(metadata_plain_t, file_id), FIELD_SIZEOF(metadata_plain_t, file_id));\n+    DBG(\"metadata_plain_t.major_version     : 0x%04lx (0x%04lx)\\n\",\n+        offsetof(metadata_plain_t, major_version), FIELD_SIZEOF(metadata_plain_t, major_version));\n+    DBG(\"metadata_plain_t.minor_version     : 0x%04lx (0x%04lx)\\n\",\n+        offsetof(metadata_plain_t, minor_version), FIELD_SIZEOF(metadata_plain_t, minor_version));\n+    DBG(\"metadata_plain_t.metadata_key_id   : 0x%04lx (0x%04lx)\\n\",\n+        offsetof(metadata_plain_t, metadata_key_id),\n+        FIELD_SIZEOF(metadata_plain_t, metadata_key_id));\n+    DBG(\"metadata_plain_t.metadata_gmac     : 0x%04lx (0x%04lx)\\n\",\n+        offsetof(metadata_plain_t, metadata_gmac),\n+        FIELD_SIZEOF(metadata_plain_t, metadata_gmac));\n+\n+    DBG(\"size(metadata_encrypted_t)         = 0x%04lx\\n\", sizeof(metadata_encrypted_t));\n+    DBG(\"metadata_encrypted_t.path          : 0x%04lx (0x%04lx)\\n\",\n+        mdps + offsetof(metadata_encrypted_t, path),\n+        FIELD_SIZEOF(metadata_encrypted_t, path));\n+    DBG(\"metadata_encrypted_t.size          : 0x%04lx (0x%04lx)\\n\",\n+        mdps + offsetof(metadata_encrypted_t, size), FIELD_SIZEOF(metadata_encrypted_t, size));\n+    DBG(\"metadata_encrypted_t.mht_key       : 0x%04lx (0x%04lx)\\n\",\n+        mdps + offsetof(metadata_encrypted_t, mht_key),\n+        FIELD_SIZEOF(metadata_encrypted_t, mht_key));\n+    DBG(\"metadata_encrypted_t.mht_gmac      : 0x%04lx (0x%04lx)\\n\",\n+        mdps + offsetof(metadata_encrypted_t, mht_gmac),\n+        FIELD_SIZEOF(metadata_encrypted_t, mht_gmac));\n+    DBG(\"metadata_encrypted_t.data          : 0x%04lx (0x%04lx)\\n\",\n+        mdps + offsetof(metadata_encrypted_t, data), FIELD_SIZEOF(metadata_encrypted_t, data));\n+\n+    DBG(\"size(metadata_padding_t)           = 0x%04lx\\n\", sizeof(metadata_padding_t));\n+    DBG(\"metadata_padding_t                 : 0x%04lx (0x%04lx)\\n\",\n+        mdps + sizeof(metadata_encrypted_t), sizeof(metadata_padding_t));\n+\n+    \/* node 0: metadata + 3k of user data *\/\n+    \/* plain metadata *\/\n+    truncate_file(\"trunc_meta_plain_0\", 0);\n+    truncate_file(\"trunc_meta_plain_1\", FIELD_TRUNCATED(metadata_plain_t, file_id));\n+    truncate_file(\"trunc_meta_plain_2\", offsetof(metadata_plain_t, major_version));\n+    truncate_file(\"trunc_meta_plain_3\", offsetof(metadata_plain_t, minor_version));\n+    truncate_file(\"trunc_meta_plain_4\", offsetof(metadata_plain_t, metadata_key_id));\n+    truncate_file(\"trunc_meta_plain_5\", FIELD_TRUNCATED(metadata_plain_t, metadata_key_id));\n+    truncate_file(\"trunc_meta_plain_6\", offsetof(metadata_plain_t, metadata_gmac));\n+    truncate_file(\"trunc_meta_plain_7\", FIELD_TRUNCATED(metadata_plain_t, metadata_gmac));\n+\n+    \/* encrypted metadata *\/\n+    truncate_file(\"trunc_meta_enc_0\", mdps + offsetof(metadata_encrypted_t, path));\n+    truncate_file(\"trunc_meta_enc_1\", mdps + FIELD_TRUNCATED(metadata_encrypted_t, path));\n+    truncate_file(\"trunc_meta_enc_2\", mdps + offsetof(metadata_encrypted_t, size));\n+    truncate_file(\"trunc_meta_enc_3\", mdps + FIELD_TRUNCATED(metadata_encrypted_t, size));\n+    truncate_file(\"trunc_meta_enc_4\", mdps + offsetof(metadata_encrypted_t, mht_key));\n+    truncate_file(\"trunc_meta_enc_5\", mdps + FIELD_TRUNCATED(metadata_encrypted_t, mht_key));\n+    truncate_file(\"trunc_meta_enc_6\", mdps + offsetof(metadata_encrypted_t, mht_gmac));\n+    truncate_file(\"trunc_meta_enc_7\", mdps + FIELD_TRUNCATED(metadata_encrypted_t, mht_gmac));\n+    truncate_file(\"trunc_meta_enc_8\", mdps + offsetof(metadata_encrypted_t, data));\n+    truncate_file(\"trunc_meta_enc_9\", mdps + FIELD_TRUNCATED(metadata_encrypted_t, data));\n+\n+    \/* padding *\/\n+    truncate_file(\"trunc_meta_pad_0\", mdps + sizeof(metadata_encrypted_t));\n+    truncate_file(\"trunc_meta_pad_1\", mdps + sizeof(metadata_encrypted_t)\n+                  + sizeof(metadata_padding_t) \/ 2);\n+\n+    \/* node 1: mht root *\/\n+    \/* after node 0 *\/\n+    truncate_file(\"trunc_mht_0\", PF_NODE_SIZE);\n+    \/* middle of data_nodes_crypto[0].key *\/\n+    truncate_file(\"trunc_mht_1\", PF_NODE_SIZE + PF_KEY_SIZE \/ 2);\n+    \/* after data_nodes_crypto[0].key *\/\n+    truncate_file(\"trunc_mht_2\", PF_NODE_SIZE + PF_KEY_SIZE);\n+    \/* middle of data_nodes_crypto[0].gmac *\/\n+    truncate_file(\"trunc_mht_3\", PF_NODE_SIZE + PF_KEY_SIZE + PF_MAC_SIZE \/ 2);\n+    \/* after data_nodes_crypto[0].gmac *\/\n+    truncate_file(\"trunc_mht_4\", PF_NODE_SIZE + PF_KEY_SIZE + PF_MAC_SIZE);\n+    \/* after data_nodes_crypto *\/\n+    truncate_file(\"trunc_mht_5\", PF_NODE_SIZE + DATA_CRYPTO_SIZE);\n+    \/* middle of mht_nodes_crypto[0].key *\/\n+    truncate_file(\"trunc_mht_6\", PF_NODE_SIZE + DATA_CRYPTO_SIZE + PF_KEY_SIZE \/ 2);\n+    \/* after mht_nodes_crypto[0].key *\/\n+    truncate_file(\"trunc_mht_7\", PF_NODE_SIZE + DATA_CRYPTO_SIZE + PF_KEY_SIZE);\n+    \/* middle of mht_nodes_crypto[0].gmac *\/\n+    truncate_file(\"trunc_mht_8\", PF_NODE_SIZE + DATA_CRYPTO_SIZE + PF_KEY_SIZE + PF_MAC_SIZE \/ 2);\n+    \/* after mht_nodes_crypto[0].gmac *\/\n+    truncate_file(\"trunc_mht_9\", PF_NODE_SIZE + DATA_CRYPTO_SIZE + PF_KEY_SIZE + PF_MAC_SIZE);\n+\n+    \/* node 2-3: data #0, #1 *\/\n+    \/* after mht root *\/\n+    truncate_file(\"trunc_data_0\", 2 * PF_NODE_SIZE);\n+    \/* middle of data #0 *\/\n+    truncate_file(\"trunc_data_1\", 2 * PF_NODE_SIZE + PF_NODE_SIZE \/ 2);\n+    \/* after data #0 *\/\n+    truncate_file(\"trunc_data_2\", 3 * PF_NODE_SIZE);\n+    \/* middle of data #1 *\/\n+    truncate_file(\"trunc_data_3\", 3 * PF_NODE_SIZE + PF_NODE_SIZE \/ 2);\n+\n+    \/* extend *\/\n+    truncate_file(\"extend_0\", g_input_size + 1);\n+    truncate_file(\"extend_1\", g_input_size + PF_NODE_SIZE \/ 2);\n+    truncate_file(\"extend_2\", g_input_size + PF_NODE_SIZE);\n+    truncate_file(\"extend_3\", g_input_size + PF_NODE_SIZE + PF_NODE_SIZE \/ 2);\n+}\n+\n+\/* returns mmap'd output contents *\/\n+static void* create_output(const char* path) {\n+    void* mem = MAP_FAILED;\n+    int fd = open(path, O_RDWR|O_CREAT, 0664);\n+    if (fd < 0)\n+        FATAL(\"Failed to open output file '%s': %s\\n\", path, strerror(errno));\n+\n+    if (ftruncate(fd, g_input_size) < 0)\n+        FATAL(\"Failed to ftruncate output file '%s': %s\\n\", path, strerror(errno));\n+\n+    mem = mmap(NULL, g_input_size, PROT_READ|PROT_WRITE, MAP_SHARED, fd, 0);\n+    if (mem == MAP_FAILED)\n+        FATAL(\"Failed to mmap output file '%s': %s\\n\", path, strerror(errno));\n+\n+    memcpy(mem, g_input_data, g_input_size);\n+\n+    close(fd);\n+    return mem;\n+}\n+\n+static void pf_decrypt(const void* encrypted, size_t size, const pf_key_t* key, const pf_mac_t* mac,\n+                       void* decrypted, const char* msg) {\n+    pf_status_t status = mbedtls_aes_gcm_decrypt(key, &g_empty_iv, NULL, 0,\n+                                                 encrypted, size,\n+                                                 decrypted, mac);\n+    if (PF_FAILURE(status))\n+        FATAL(\"decrypting %s failed\\n\", msg);\n+}\n+\n+static void pf_encrypt(const void* decrypted, size_t size, const pf_key_t* key, pf_mac_t* mac,\n+                       void* encrypted, const char* msg) {\n+    pf_status_t status = mbedtls_aes_gcm_encrypt(key, &g_empty_iv, NULL, 0,\n+                                                 decrypted, size,\n+                                                 encrypted, mac);\n+    if (PF_FAILURE(status))\n+        FATAL(\"encrypting %s failed\\n\", msg);\n+}\n+\n+\/* copy input PF and apply some modifications *\/\n+#define __BREAK_PF(suffix, ...) do { \\\n+    make_output_path(suffix); \\\n+    meta = create_output(g_output_path); \\\n+    out = (uint8_t*)meta; \\\n+    pf_decrypt(&meta->encrypted_part, sizeof(meta->encrypted_part), &g_meta_key, \\\n+               &meta->plain_part.metadata_gmac, meta_dec, \"metadata\"); \\\n+    mht_enc = (mht_node_t*)(out + PF_NODE_SIZE); \\\n+    pf_decrypt(mht_enc, sizeof(*mht_enc), &meta_dec->mht_key, &meta_dec->mht_gmac, mht_dec, \\\n+               \"mht\"); \\\n+    __VA_ARGS__ \\\n+    munmap(meta, g_input_size); \\\n+} while (0)\n+\n+\/* if update is true, also create a file with correct metadata MAC *\/\n+#define BREAK_PF(suffix, update, ...) do { \\\n+    __BREAK_PF(suffix, __VA_ARGS__); \\\n+    if (update) { \\\n+        __BREAK_PF(suffix \"_fixed\", __VA_ARGS__ { \\\n+                       pf_encrypt(meta_dec, sizeof(*meta_dec), &g_meta_key, \\\n+                                  &meta->plain_part.metadata_gmac, meta->encrypted_part, \\\n+                                  \"metadata\"); \\\n+                   } ); \\\n+    } \\\n+} while (0)\n+\n+#define BREAK_MHT(suffix, ...) do { \\\n+    __BREAK_PF(suffix, __VA_ARGS__ { \\\n+                   pf_encrypt(mht_dec, sizeof(*mht_dec), &meta_dec->mht_key, &meta_dec->mht_gmac, \\\n+                              mht_enc, \"mht\"); \\\n+               } ); \\\n+} while (0)\n+\n+#define LAST_BYTE(array) (((uint8_t*)&array)[sizeof(array) - 1])\n+\n+static void tamper_modify(void) {\n+    metadata_node_t* meta = NULL;\n+    uint8_t* out = NULL;\n+    metadata_encrypted_t* meta_dec = malloc(sizeof(*meta_dec));\n+    if (!meta_dec)\n+        FATAL(\"Out of memory\\n\");\n+    mht_node_t* mht_enc = NULL;\n+    mht_node_t* mht_dec = malloc(sizeof(*mht_dec));\n+    if (!mht_dec)\n+        FATAL(\"Out of memory\\n\");\n+\n+    \/* plain part of the metadata isn't covered by the MAC so no point updating it *\/\n+    BREAK_PF(\"meta_plain_id_0\", \/*update=*\/false,\n+             { meta->plain_part.file_id = 0; });\n+    BREAK_PF(\"meta_plain_id_1\", \/*update=*\/false,\n+             { meta->plain_part.file_id = UINT64_MAX; });\n+    BREAK_PF(\"meta_plain_version_0\", \/*update=*\/false,\n+             { meta->plain_part.major_version = 0; });\n+    BREAK_PF(\"meta_plain_version_1\", \/*update=*\/false,\n+             { meta->plain_part.major_version = 0xff; });\n+    BREAK_PF(\"meta_plain_version_2\", \/*update=*\/false,\n+             { meta->plain_part.minor_version = 0xff; });\n+\n+    \/* metadata_key_id is the keying material for encrypted metadata key derivation, so create also\n+     * PFs with updated MACs *\/\n+    BREAK_PF(\"meta_plain_keyid_0\", \/*update=*\/true,\n+             { meta->plain_part.metadata_key_id[0] ^= 1; });\n+    BREAK_PF(\"meta_plain_keyid_1\", \/*update=*\/true,\n+             { LAST_BYTE(meta->plain_part.metadata_key_id) ^= 0xfe; });\n+    BREAK_PF(\"meta_plain_mac_0\", \/*update=*\/true,\n+             { meta->plain_part.metadata_gmac[0] ^= 0xfe; });\n+    BREAK_PF(\"meta_plain_mac_1\", \/*update=*\/true,\n+             { LAST_BYTE(meta->plain_part.metadata_gmac) &= 1; });\n+\n+    BREAK_PF(\"meta_enc_filename_0\", \/*update=*\/true,\n+             { meta_dec->path[0] = 0; });\n+    BREAK_PF(\"meta_enc_filename_1\", \/*update=*\/true,\n+             { meta_dec->path[0] ^= 1; });\n+    BREAK_PF(\"meta_enc_filename_2\", \/*update=*\/true,\n+             { LAST_BYTE(meta_dec->path) ^= 0xfe; });\n+    BREAK_PF(\"meta_enc_size_0\", \/*update=*\/true,\n+             { meta_dec->size = 0; });\n+    BREAK_PF(\"meta_enc_size_1\", \/*update=*\/true,\n+             { meta_dec->size = g_input_size - 1; });\n+    BREAK_PF(\"meta_enc_size_2\", \/*update=*\/true,\n+             { meta_dec->size = g_input_size + 1; });\n+    BREAK_PF(\"meta_enc_size_3\", \/*update=*\/true,\n+             { meta_dec->size = UINT64_MAX; });\n+    BREAK_PF(\"meta_enc_mht_key_0\", \/*update=*\/true,\n+             { meta_dec->mht_key[0] ^= 1; });\n+    BREAK_PF(\"meta_enc_mht_key_1\", \/*update=*\/true,\n+             { LAST_BYTE(meta_dec->mht_key) ^= 0xfe; });\n+    BREAK_PF(\"meta_enc_mht_mac_0\", \/*update=*\/true,\n+             { meta_dec->mht_gmac[0] ^= 1; });\n+    BREAK_PF(\"meta_enc_mht_mac_1\", \/*update=*\/true,\n+             { LAST_BYTE(meta_dec->mht_gmac) ^= 0xfe; });\n+    BREAK_PF(\"meta_enc_data_0\", \/*update=*\/true,\n+             { meta_dec->data[0] ^= 0xfe; });\n+    BREAK_PF(\"meta_enc_data_1\", \/*update=*\/true,\n+             { LAST_BYTE(meta_dec->data) ^= 1; });\n+\n+    \/* padding is ignored *\/\n+    BREAK_PF(\"meta_padding_0\", \/*update=*\/false,\n+             { meta->padding[0] ^= 1; });\n+    BREAK_PF(\"meta_padding_1\", \/*update=*\/false,\n+             { LAST_BYTE(meta->padding) ^= 0xfe; });\n+\n+    BREAK_MHT(\"mht_0\", { mht_dec->data_nodes_crypto[0].key[0] ^= 1; });\n+    BREAK_MHT(\"mht_1\", { mht_dec->data_nodes_crypto[0].gmac[0] ^= 1; });\n+    BREAK_MHT(\"mht_2\", { mht_dec->mht_nodes_crypto[0].key[0] ^= 1; });\n+    BREAK_MHT(\"mht_3\", { mht_dec->mht_nodes_crypto[0].gmac[0] ^= 1; });\n+    BREAK_MHT(\"mht_4\", { mht_dec->data_nodes_crypto[ATTACHED_DATA_NODES_COUNT - 1].key[0] ^= 1; });\n+    BREAK_MHT(\"mht_5\", { mht_dec->data_nodes_crypto[ATTACHED_DATA_NODES_COUNT - 1].gmac[0] ^= 1; });\n+    BREAK_MHT(\"mht_6\", { mht_dec->mht_nodes_crypto[CHILD_MHT_NODES_COUNT - 1].key[0] ^= 1; });\n+    BREAK_MHT(\"mht_7\", { mht_dec->mht_nodes_crypto[CHILD_MHT_NODES_COUNT - 1].gmac[0] ^= 1; });\n+    BREAK_MHT(\"mht_8\", {\n+        gcm_crypto_data_t crypto;\n+        memcpy(&crypto, &mht_dec->data_nodes_crypto[0], sizeof(crypto));\n+        memcpy(&mht_dec->data_nodes_crypto[0], &mht_dec->data_nodes_crypto[1], sizeof(crypto));\n+        memcpy(&mht_dec->data_nodes_crypto[1], &crypto, sizeof(crypto));\n+    });\n+    BREAK_MHT(\"mht_9\", {\n+        gcm_crypto_data_t crypto;\n+        memcpy(&crypto, &mht_dec->mht_nodes_crypto[0], sizeof(crypto));\n+        memcpy(&mht_dec->mht_nodes_crypto[0], &mht_dec->mht_nodes_crypto[1], sizeof(crypto));\n+        memcpy(&mht_dec->mht_nodes_crypto[1], &crypto, sizeof(crypto));\n+    });\n+\n+    \/* data nodes start from node #2 *\/\n+    BREAK_PF(\"data_0\", \/*update=*\/false,\n+             { *(out + 2 * PF_NODE_SIZE) ^= 1; });\n+    BREAK_PF(\"data_1\", \/*update=*\/false,\n+             { *(out + 3 * PF_NODE_SIZE - 1) ^= 1; });\n+    BREAK_PF(\"data_2\", \/*update=*\/false, {\n+        \/* swap data nodes *\/\n+        memcpy(out + 2 * PF_NODE_SIZE, g_input_data + 3 * PF_NODE_SIZE, PF_NODE_SIZE);\n+        memcpy(out + 3 * PF_NODE_SIZE, g_input_data + 2 * PF_NODE_SIZE, PF_NODE_SIZE);\n+    });\n+\n+    free(mht_dec);\n+    free(meta_dec);\n+}\n+\n+int main(int argc, char* argv[]) {\n+    int ret = -1;\n+\n+    int option          = 0;\n+    char* input_path    = NULL;\n+    char* wrap_key_path = NULL;\n+    int input_fd        = -1;\n+\n+    while (true) {\n+        option = getopt_long(argc, argv, \"i:o:w:vh\", g_options, NULL);\n+        if (option == -1)\n+            break;\n+\n+        switch (option) {\n+            case 'i':\n+                input_path = optarg;\n+                break;\n+            case 'o':\n+                g_output_dir = optarg;\n+                break;\n+            case 'w':\n+                wrap_key_path = optarg;\n+                break;\n+            case 'v':\n+                set_verbose(true);\n+                break;\n+            case 'h':\n+                usage();\n+                return 0;\n+            default:\n+                ERROR(\"Unknown option: %c\\n\", option);\n+                usage();\n+        }\n+    }\n+\n+    if (!input_path) {\n+        ERROR(\"Input path not specified\\n\");\n+        usage();\n+        goto out;\n+    }\n+\n+    if (!g_output_dir) {\n+        ERROR(\"Output path not specified\\n\");\n+        usage();\n+        goto out;\n+    }\n+\n+    if (!wrap_key_path) {\n+        ERROR(\"Wrap key path not specified\\n\");\n+        usage();\n+        goto out;\n+    }\n+\n+    input_fd = open(input_path, O_RDONLY);\n+    if (input_fd < 0) {\n+        ERROR(\"Failed to open input file '%s': %s\\n\", input_path, strerror(errno));\n+        goto out;\n+    }\n+\n+    g_input_size = get_file_size(input_fd);\n+    if (g_input_size < 0) {\n+        ERROR(\"Failed to stat input file '%s': %s\\n\", input_path, strerror(errno));\n+        goto out;\n+    }\n+\n+    g_input_data = mmap(NULL, g_input_size, PROT_READ, MAP_PRIVATE, input_fd, 0);\n+    if (g_input_data == MAP_FAILED) {\n+        ERROR(\"Failed to mmap input file '%s': %s\\n\", input_path, strerror(errno));\n+        goto out;\n+    }\n+\n+    load_wrap_key(wrap_key_path, &g_wrap_key);\n+    derive_main_key(&g_wrap_key, &((metadata_plain_t*)g_input_data)->metadata_key_id,\n+                    &g_meta_key);\n+\n+    g_input_name = basename(input_path);\n+    g_output_path_size = strlen(g_input_name) + strlen(g_output_dir) + 256;\n+    g_output_path = malloc(g_output_path_size);\n+    if (!g_output_path) {\n+        ERROR(\"No memory\\n\");\n+        goto out;\n+    }\n+\n+    tamper_truncate();\n+    tamper_modify();\n+    ret = 0;\n+\n+out:\n+    \/* skip cleanup as we are in main() *\/\n+    return ret;\n+}\n","files":{"\/LibOS\/shim\/test\/fs\/test_pf.py":{"changes":[{"diff":"\n \n     def __corrupt_file(self, input_path, output_path):\n         cmd = [self.PF_TAMPER, '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path]\n-        return self.run_native_binary(cmd)\n+        return self.run_native_binary(cmd, libpath=os.path.join(os.getcwd(), 'lib'))\n \n     # invalid\/corrupted files\n-    @expectedFailureIf(HAS_SGX)\n-    # pylint: disable=fixme\n     def test_500_invalid(self):\n-        # TODO: port these to the new file format\n         invalid_dir = os.path.join(self.TEST_DIR, 'pf_invalid')\n-        # files below should work normally (benign modifications)\n-        should_pass = ['chunk_padding_1_fixed', 'chunk_padding_2_fixed', 'chunk_data_3',\n-                       'chunk_data_3_fixed', 'chunk_data_4', 'chunk_data_4_fixed']\n         if not os.path.exists(invalid_dir):\n             os.mkdir(invalid_dir)\n+\n         # prepare valid encrypted file (largest one for maximum possible corruptions)\n         original_input = self.OUTPUT_FILES[-1]\n         self.__encrypt_file(self.INPUT_FILES[-1], original_input)\n         # generate invalid files based on the above\n         self.__corrupt_file(original_input, invalid_dir)\n+\n         # try to decrypt invalid files\n         for name in os.listdir(invalid_dir):\n             invalid = os.path.join(invalid_dir, name)\n","add":3,"remove":7,"filename":"\/LibOS\/shim\/test\/fs\/test_pf.py","badparts":["        return self.run_native_binary(cmd)","    @expectedFailureIf(HAS_SGX)","        should_pass = ['chunk_padding_1_fixed', 'chunk_padding_2_fixed', 'chunk_data_3',","                       'chunk_data_3_fixed', 'chunk_data_4', 'chunk_data_4_fixed']"],"goodparts":["        return self.run_native_binary(cmd, libpath=os.path.join(os.getcwd(), 'lib'))"]},{"diff":"\n             input_path = os.path.join(invalid_dir, os.path.basename(original_input))\n             # copy the file so it has the original file name (for allowed path check)\n             shutil.copy(invalid, input_path)\n-            should_pass = any(s in name for s in should_pass)\n \n             try:\n                 args = ['decrypt', '-V', '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path]\n                 self.__pf_crypt(args)\n             except subprocess.CalledProcessError as exc:\n-                if should_pass:\n-                    self.assertEqual(exc.returncode, 0)\n-                else:\n-                    self.assertNotEqual(exc.returncode, 0)\n+                # decryption of invalid file must fail with -1 (wrapped to 255)\n+                self.assertEqual(exc.returncode, 255)\n             else:\n-                if not should_pass:\n-                    print('[!] Fail: successfully decrypted file: ' + name)\n-                    self.fail()\n+                print('[!] Fail: successfully decrypted file: ' + name)\n+                self.fail()","add":4,"remove":8,"filename":"\/LibOS\/shim\/test\/fs\/test_pf.py","badparts":["            should_pass = any(s in name for s in should_pass)","                if should_pass:","                    self.assertEqual(exc.returncode, 0)","                else:","                    self.assertNotEqual(exc.returncode, 0)","                if not should_pass:","                    print('[!] Fail: successfully decrypted file: ' + name)","                    self.fail()"],"goodparts":["                self.assertEqual(exc.returncode, 255)","                print('[!] Fail: successfully decrypted file: ' + name)","                self.fail()"]}],"source":"\n import filecmp import os import shutil import subprocess import unittest from test_fs import( TC_00_FileSystem, ) from regression import( HAS_SGX, expectedFailureIf, ) @unittest.skipUnless(HAS_SGX, 'Protected files require SGX support') class TC_50_ProtectedFiles(TC_00_FileSystem): @classmethod def setUpClass(cls): cls.PF_CRYPT='bin\/pf_crypt' cls.PF_TAMPER='bin\/pf_tamper' cls.WRAP_KEY=os.path.join(cls.TEST_DIR, 'wrap-key') cls.CONST_WRAP_KEY=[0xff, 0xee, 0xdd, 0xcc, 0xbb, 0xaa, 0x99, 0x88, 0x77, 0x66, 0x55, 0x44, 0x33, 0x22, 0x11, 0x00] cls.ENCRYPTED_DIR=os.path.join(cls.TEST_DIR, 'pf_input') cls.ENCRYPTED_FILES=[os.path.join(cls.ENCRYPTED_DIR, str(v)) for v in cls.FILE_SIZES] cls.LIB_PATH=os.path.join(os.getcwd(), 'lib') super().setUpClass() if not os.path.exists(cls.ENCRYPTED_DIR): os.mkdir(cls.ENCRYPTED_DIR) cls.OUTPUT_DIR=os.path.join(cls.TEST_DIR, 'pf_output') cls.OUTPUT_FILES=[os.path.join(cls.OUTPUT_DIR, str(x)) for x in cls.FILE_SIZES] cls.__set_default_key(cls) for i in cls.INDEXES: cmd=[cls.PF_CRYPT, 'encrypt', '-w', cls.WRAP_KEY, '-i', cls.INPUT_FILES[i], '-o', cls.ENCRYPTED_FILES[i]] cls.run_native_binary(cmd, libpath=os.path.join(os.getcwd(), 'lib')) def __pf_crypt(self, args): args.insert(0, self.PF_CRYPT) return self.run_native_binary(args, libpath=os.path.join(os.getcwd(), 'lib')) def __set_default_key(self): with open(self.WRAP_KEY, 'wb') as file: file.write(bytes(self.CONST_WRAP_KEY)) def copy_input(self, input_path, output_path): self.__encrypt_file(input_path, output_path) def __encrypt_file(self, input_path, output_path): args=['encrypt', '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path] stdout, stderr=self.__pf_crypt(args) return(stdout, stderr) def __decrypt_file(self, input_path, output_path): args=['decrypt', '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path] stdout, stderr=self.__pf_crypt(args) return(stdout, stderr) def test_000_gen_key(self): key_path=os.path.join(self.TEST_DIR, 'tmpkey') args=['gen-key', '-w', key_path] stdout, _=self.__pf_crypt(args) self.assertIn('Wrap key saved to: ' +key_path, stdout) self.assertEqual(os.path.getsize(key_path), 16) os.remove(key_path) def test_010_encrypt_decrypt(self): for i in self.INDEXES: self.__encrypt_file(self.INPUT_FILES[i], self.OUTPUT_FILES[i]) self.assertFalse(filecmp.cmp(self.INPUT_FILES[i], self.OUTPUT_FILES[i], shallow=False)) dec_path=os.path.join(self.OUTPUT_DIR, os.path.basename(self.OUTPUT_FILES[i]) +'.dec') self.__decrypt_file(self.OUTPUT_FILES[i], dec_path) self.assertTrue(filecmp.cmp(self.INPUT_FILES[i], dec_path, shallow=False)) def test_100_open_close(self): input_path=self.ENCRYPTED_FILES[-1] output_path=os.path.join(self.OUTPUT_DIR, 'test_100') stdout, stderr=self.run_binary(['open_close', 'R', input_path]) self.verify_open_close(stdout, stderr, input_path, 'input') try: stdout, stderr=self.run_binary(['open_close', 'W', output_path]) self.assertIn('ERROR: Failed to open output file', stderr) except subprocess.CalledProcessError as exc: self.assertNotEqual(exc.returncode, 0) self.assertTrue(os.path.isfile(output_path)) else: print('[!] Fail: open_close returned 0') self.fail() def test_101_open_flags(self): file_path=os.path.join(self.OUTPUT_DIR, 'test_101') stdout, stderr=self.run_binary(['open_flags', file_path]) self.verify_open_flags(stdout, stderr) def test_115_seek_tell(self): plaintext_path=self.INPUT_FILES[-1] input_path=self.ENCRYPTED_FILES[-1] output_path_1=os.path.join(self.OUTPUT_DIR, 'test_115a') output_path_2=os.path.join(self.OUTPUT_DIR, 'test_115b') self.copy_input(plaintext_path, output_path_1) self.copy_input(plaintext_path, output_path_2) stdout, stderr=self.run_binary(['seek_tell', input_path, output_path_1, output_path_2]) self.verify_seek_tell(stdout, stderr, input_path, output_path_1, output_path_2, self.FILE_SIZES[-1]) def test_130_file_stat(self): for i in self.INDEXES: input_path=self.ENCRYPTED_FILES[i] output_path=self.OUTPUT_FILES[i] size=str(self.FILE_SIZES[i]) self.copy_input(self.INPUT_FILES[i], output_path) stdout, stderr=self.run_binary(['stat', input_path, output_path]) self.verify_stat(stdout, stderr, input_path, output_path, size) def verify_size(self, file, size): dec_path=os.path.join(self.OUTPUT_DIR, os.path.basename(file) +'.dec') self.__decrypt_file(file, dec_path) self.assertEqual(os.stat(dec_path).st_size, size) @expectedFailureIf(HAS_SGX) def test_140_file_truncate(self): self.fail() def test_150_file_rename(self): path1=os.path.join(self.OUTPUT_DIR, 'test_150a') path2=os.path.join(self.OUTPUT_DIR, 'test_150b') self.copy_input(self.ENCRYPTED_FILES[-1], path1) shutil.copy(path1, path2) args=['decrypt', '-V', '-w', self.WRAP_KEY, '-i', path2, '-o', path1] try: self.__pf_crypt(args) except subprocess.CalledProcessError as exc: self.assertNotEqual(exc.returncode, 0) else: print('[!] Fail: successfully decrypted renamed file: ' +path2) self.fail() def verify_copy_content(self, input_path, output_path): dec_path=os.path.join(self.OUTPUT_DIR, os.path.basename(output_path) +'.dec') self.__decrypt_file(output_path, dec_path) self.assertTrue(filecmp.cmp(input_path, dec_path, shallow=False)) def do_copy_test(self, executable, timeout): stdout, stderr=self.run_binary([executable, self.ENCRYPTED_DIR, self.OUTPUT_DIR], timeout=timeout) self.verify_copy(stdout, stderr, self.ENCRYPTED_DIR, executable) def test_204_copy_dir_mmap_whole(self): self.do_copy_test('copy_mmap_whole', 30) def test_205_copy_dir_mmap_seq(self): self.do_copy_test('copy_mmap_seq', 60) def test_206_copy_dir_mmap_rev(self): self.do_copy_test('copy_mmap_rev', 60) def test_210_copy_dir_mounted(self): executable='copy_whole' stdout, stderr=self.run_binary([executable, '\/mounted\/pf_input', '\/mounted\/pf_output'], timeout=30) self.verify_copy(stdout, stderr, '\/mounted\/pf_input', executable) def __corrupt_file(self, input_path, output_path): cmd=[self.PF_TAMPER, '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path] return self.run_native_binary(cmd) @expectedFailureIf(HAS_SGX) def test_500_invalid(self): invalid_dir=os.path.join(self.TEST_DIR, 'pf_invalid') should_pass=['chunk_padding_1_fixed', 'chunk_padding_2_fixed', 'chunk_data_3', 'chunk_data_3_fixed', 'chunk_data_4', 'chunk_data_4_fixed'] if not os.path.exists(invalid_dir): os.mkdir(invalid_dir) original_input=self.OUTPUT_FILES[-1] self.__encrypt_file(self.INPUT_FILES[-1], original_input) self.__corrupt_file(original_input, invalid_dir) for name in os.listdir(invalid_dir): invalid=os.path.join(invalid_dir, name) output_path=os.path.join(self.OUTPUT_DIR, name) input_path=os.path.join(invalid_dir, os.path.basename(original_input)) shutil.copy(invalid, input_path) should_pass=any(s in name for s in should_pass) try: args=['decrypt', '-V', '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path] self.__pf_crypt(args) except subprocess.CalledProcessError as exc: if should_pass: self.assertEqual(exc.returncode, 0) else: self.assertNotEqual(exc.returncode, 0) else: if not should_pass: print('[!] Fail: successfully decrypted file: ' +name) self.fail() ","sourceWithComments":"#!\/usr\/bin\/env python3\n\nimport filecmp\nimport os\nimport shutil\nimport subprocess\nimport unittest\n\nfrom test_fs import (\n    TC_00_FileSystem,\n)\n\nfrom regression import (\n    HAS_SGX,\n    expectedFailureIf,\n)\n\n@unittest.skipUnless(HAS_SGX, 'Protected files require SGX support')\nclass TC_50_ProtectedFiles(TC_00_FileSystem):\n    @classmethod\n    def setUpClass(cls):\n        cls.PF_CRYPT = 'bin\/pf_crypt'\n        cls.PF_TAMPER = 'bin\/pf_tamper'\n        cls.WRAP_KEY = os.path.join(cls.TEST_DIR, 'wrap-key')\n        # CONST_WRAP_KEY must match the one in manifest\n        cls.CONST_WRAP_KEY = [0xff, 0xee, 0xdd, 0xcc, 0xbb, 0xaa, 0x99, 0x88,\n                              0x77, 0x66, 0x55, 0x44, 0x33, 0x22, 0x11, 0x00]\n        cls.ENCRYPTED_DIR = os.path.join(cls.TEST_DIR, 'pf_input')\n        cls.ENCRYPTED_FILES = [os.path.join(cls.ENCRYPTED_DIR, str(v)) for v in cls.FILE_SIZES]\n        cls.LIB_PATH = os.path.join(os.getcwd(), 'lib')\n\n        super().setUpClass()\n        if not os.path.exists(cls.ENCRYPTED_DIR):\n            os.mkdir(cls.ENCRYPTED_DIR)\n        cls.OUTPUT_DIR = os.path.join(cls.TEST_DIR, 'pf_output')\n        cls.OUTPUT_FILES = [os.path.join(cls.OUTPUT_DIR, str(x)) for x in cls.FILE_SIZES]\n        # create encrypted files\n        cls.__set_default_key(cls)\n        for i in cls.INDEXES:\n            cmd = [cls.PF_CRYPT, 'encrypt', '-w', cls.WRAP_KEY, '-i', cls.INPUT_FILES[i], '-o',\n                   cls.ENCRYPTED_FILES[i]]\n\n            cls.run_native_binary(cmd, libpath=os.path.join(os.getcwd(), 'lib'))\n\n    def __pf_crypt(self, args):\n        args.insert(0, self.PF_CRYPT)\n        return self.run_native_binary(args, libpath=os.path.join(os.getcwd(), 'lib'))\n\n    def __set_default_key(self):\n        with open(self.WRAP_KEY, 'wb') as file:\n            file.write(bytes(self.CONST_WRAP_KEY))\n\n    # overrides TC_00_FileSystem to encrypt the file instead of just copying\n    def copy_input(self, input_path, output_path):\n        self.__encrypt_file(input_path, output_path)\n\n    def __encrypt_file(self, input_path, output_path):\n        args = ['encrypt', '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path]\n        stdout, stderr = self.__pf_crypt(args)\n        return (stdout, stderr)\n\n    def __decrypt_file(self, input_path, output_path):\n        args = ['decrypt', '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path]\n        stdout, stderr = self.__pf_crypt(args)\n        return (stdout, stderr)\n\n    def test_000_gen_key(self):\n        # test random key generation\n        key_path = os.path.join(self.TEST_DIR, 'tmpkey')\n        args = ['gen-key', '-w', key_path]\n        stdout, _ = self.__pf_crypt(args)\n        self.assertIn('Wrap key saved to: ' + key_path, stdout)\n        self.assertEqual(os.path.getsize(key_path), 16)\n        os.remove(key_path)\n\n    def test_010_encrypt_decrypt(self):\n        for i in self.INDEXES:\n            self.__encrypt_file(self.INPUT_FILES[i], self.OUTPUT_FILES[i])\n            self.assertFalse(filecmp.cmp(self.INPUT_FILES[i], self.OUTPUT_FILES[i], shallow=False))\n            dec_path = os.path.join(self.OUTPUT_DIR,\n                                    os.path.basename(self.OUTPUT_FILES[i]) + '.dec')\n            self.__decrypt_file(self.OUTPUT_FILES[i], dec_path)\n            self.assertTrue(filecmp.cmp(self.INPUT_FILES[i], dec_path, shallow=False))\n\n    # overrides TC_00_FileSystem to change input dir (from plaintext to encrypted)\n    def test_100_open_close(self):\n        # the test binary expects a path to read-only (existing) file or a path to file that\n        # will get created\n        input_path = self.ENCRYPTED_FILES[-1] # existing file\n        output_path = os.path.join(self.OUTPUT_DIR, 'test_100') # new file\n        stdout, stderr = self.run_binary(['open_close', 'R', input_path])\n        self.verify_open_close(stdout, stderr, input_path, 'input')\n        # the following test tries to open multiple handles to a single writable PF, should fail\n        try:\n            stdout, stderr = self.run_binary(['open_close', 'W', output_path])\n            self.assertIn('ERROR: Failed to open output file', stderr)\n        except subprocess.CalledProcessError as exc:\n            self.assertNotEqual(exc.returncode, 0)\n            self.assertTrue(os.path.isfile(output_path))\n        else:\n            print('[!] Fail: open_close returned 0')\n            self.fail()\n\n    # overrides TC_00_FileSystem to change input dir (from plaintext to encrypted)\n    def test_101_open_flags(self):\n        # the test binary expects a path to file that will get created\n        file_path = os.path.join(self.OUTPUT_DIR, 'test_101') # new file\n        stdout, stderr = self.run_binary(['open_flags', file_path])\n        self.verify_open_flags(stdout, stderr)\n\n    # overrides TC_00_FileSystem to change input dir (from plaintext to encrypted)\n    def test_115_seek_tell(self):\n        # the test binary expects a path to read-only (existing) file and two paths to files that\n        # will get created\n        plaintext_path = self.INPUT_FILES[-1]\n        input_path = self.ENCRYPTED_FILES[-1] # existing file\n        output_path_1 = os.path.join(self.OUTPUT_DIR, 'test_115a') # writable files\n        output_path_2 = os.path.join(self.OUTPUT_DIR, 'test_115b')\n        self.copy_input(plaintext_path, output_path_1) # encrypt\n        self.copy_input(plaintext_path, output_path_2)\n        stdout, stderr = self.run_binary(['seek_tell', input_path, output_path_1, output_path_2])\n        self.verify_seek_tell(stdout, stderr, input_path, output_path_1, output_path_2,\n                              self.FILE_SIZES[-1])\n\n    # overrides TC_00_FileSystem to change input dir (from plaintext to encrypted)\n    def test_130_file_stat(self):\n        # the test binary expects a path to read-only (existing) file and a path to file that\n        # will get created\n        for i in self.INDEXES:\n            input_path = self.ENCRYPTED_FILES[i]\n            output_path = self.OUTPUT_FILES[i]\n            size = str(self.FILE_SIZES[i])\n            self.copy_input(self.INPUT_FILES[i], output_path)\n            stdout, stderr = self.run_binary(['stat', input_path, output_path])\n            self.verify_stat(stdout, stderr, input_path, output_path, size)\n\n    # overrides TC_00_FileSystem to decrypt output\n    def verify_size(self, file, size):\n        dec_path = os.path.join(self.OUTPUT_DIR, os.path.basename(file) + '.dec')\n        self.__decrypt_file(file, dec_path)\n        self.assertEqual(os.stat(dec_path).st_size, size)\n\n    @expectedFailureIf(HAS_SGX)\n    # pylint: disable=fixme\n    def test_140_file_truncate(self):\n        self.fail() # TODO: port these to the new file format\n\n    def test_150_file_rename(self):\n        path1 = os.path.join(self.OUTPUT_DIR, 'test_150a')\n        path2 = os.path.join(self.OUTPUT_DIR, 'test_150b')\n        self.copy_input(self.ENCRYPTED_FILES[-1], path1)\n        shutil.copy(path1, path2)\n        # accessing renamed file should fail\n        args = ['decrypt', '-V', '-w', self.WRAP_KEY, '-i', path2, '-o', path1]\n        try:\n            self.__pf_crypt(args)\n        except subprocess.CalledProcessError as exc:\n            self.assertNotEqual(exc.returncode, 0)\n        else:\n            print('[!] Fail: successfully decrypted renamed file: ' + path2)\n            self.fail()\n\n    # overrides TC_00_FileSystem to decrypt output\n    def verify_copy_content(self, input_path, output_path):\n        dec_path = os.path.join(self.OUTPUT_DIR, os.path.basename(output_path) + '.dec')\n        self.__decrypt_file(output_path, dec_path)\n        self.assertTrue(filecmp.cmp(input_path, dec_path, shallow=False))\n\n    # overrides TC_00_FileSystem to change input dir (from plaintext to encrypted)\n    def do_copy_test(self, executable, timeout):\n        stdout, stderr = self.run_binary([executable, self.ENCRYPTED_DIR, self.OUTPUT_DIR],\n                                         timeout=timeout)\n        self.verify_copy(stdout, stderr, self.ENCRYPTED_DIR, executable)\n\n    # overrides TC_00_FileSystem to not skip this on SGX\n    def test_204_copy_dir_mmap_whole(self):\n        self.do_copy_test('copy_mmap_whole', 30)\n\n    # overrides TC_00_FileSystem to not skip this on SGX\n    def test_205_copy_dir_mmap_seq(self):\n        self.do_copy_test('copy_mmap_seq', 60)\n\n    # overrides TC_00_FileSystem to not skip this on SGX\n    def test_206_copy_dir_mmap_rev(self):\n        self.do_copy_test('copy_mmap_rev', 60)\n\n    # overrides TC_00_FileSystem to change dirs (from plaintext to encrypted)\n    def test_210_copy_dir_mounted(self):\n        executable = 'copy_whole'\n        stdout, stderr = self.run_binary([executable, '\/mounted\/pf_input', '\/mounted\/pf_output'],\n                                         timeout=30)\n        self.verify_copy(stdout, stderr, '\/mounted\/pf_input', executable)\n\n    def __corrupt_file(self, input_path, output_path):\n        cmd = [self.PF_TAMPER, '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path]\n        return self.run_native_binary(cmd)\n\n    # invalid\/corrupted files\n    @expectedFailureIf(HAS_SGX)\n    # pylint: disable=fixme\n    def test_500_invalid(self):\n        # TODO: port these to the new file format\n        invalid_dir = os.path.join(self.TEST_DIR, 'pf_invalid')\n        # files below should work normally (benign modifications)\n        should_pass = ['chunk_padding_1_fixed', 'chunk_padding_2_fixed', 'chunk_data_3',\n                       'chunk_data_3_fixed', 'chunk_data_4', 'chunk_data_4_fixed']\n        if not os.path.exists(invalid_dir):\n            os.mkdir(invalid_dir)\n        # prepare valid encrypted file (largest one for maximum possible corruptions)\n        original_input = self.OUTPUT_FILES[-1]\n        self.__encrypt_file(self.INPUT_FILES[-1], original_input)\n        # generate invalid files based on the above\n        self.__corrupt_file(original_input, invalid_dir)\n        # try to decrypt invalid files\n        for name in os.listdir(invalid_dir):\n            invalid = os.path.join(invalid_dir, name)\n            output_path = os.path.join(self.OUTPUT_DIR, name)\n            input_path = os.path.join(invalid_dir, os.path.basename(original_input))\n            # copy the file so it has the original file name (for allowed path check)\n            shutil.copy(invalid, input_path)\n            should_pass = any(s in name for s in should_pass)\n\n            try:\n                args = ['decrypt', '-V', '-w', self.WRAP_KEY, '-i', input_path, '-o', output_path]\n                self.__pf_crypt(args)\n            except subprocess.CalledProcessError as exc:\n                if should_pass:\n                    self.assertEqual(exc.returncode, 0)\n                else:\n                    self.assertNotEqual(exc.returncode, 0)\n            else:\n                if not should_pass:\n                    print('[!] Fail: successfully decrypted file: ' + name)\n                    self.fail()\n"}},"msg":"[Pal\/Linux-SGX] Test malicious modifications to protected files\n\nThis commit adds a new PF utility `pf_tamper` that tampers with\nvalid protected files and uses this utility to test that the PF\nlogic in Linux-SGX detects such malicious modifications.\n\nThis commit also moves out the PF-format macros and structs from\n`protected_files_internal.h` to `protected_files_format.h` for\nbetter readability.\n\nCo-authored-by: Dmitrii Kuvaiskii <dmitrii.kuvaiskii@intel.com>"}},"https:\/\/github.com\/chrissorchard\/sidereal":{"d39a9fe31a96da745c3bec8e90c1f3e0a0f38dee":{"url":"https:\/\/api.github.com\/repos\/chrissorchard\/sidereal\/commits\/d39a9fe31a96da745c3bec8e90c1f3e0a0f38dee","html_url":"https:\/\/github.com\/chrissorchard\/sidereal\/commit\/d39a9fe31a96da745c3bec8e90c1f3e0a0f38dee","message":"Added DigestDict code, for verification of JSON\n\nWhen we're sending our JSON across the network, we can use this function\nto ensure that nothing's been changed. This isn't security against\nmalicious tampering, because frankly, md5 is completely broken.","sha":"d39a9fe31a96da745c3bec8e90c1f3e0a0f38dee","keyword":"tampering malicious","diff":"diff --git a\/sidereal\/server.py b\/sidereal\/server.py\nindex 278836d..271ceb6 100644\n--- a\/sidereal\/server.py\n+++ b\/sidereal\/server.py\n@@ -1,11 +1,47 @@\n+import json\n+import hashlib\n+\n from twisted.internet.protocol import DatagramProtocol\n from twisted.internet import reactor\n \n+class DigestDict(dict):\n+    def digest(self):\n+        # Given a dictionary, inserts a json digest, assuming that\n+        # the element 'digest' contains N zeros, and it's printed\n+        # with indent 4, sorted\n+        # where N is the hash length\n+        dcopy = self.copy()\n+        hasher = hashlib.new('md5')\n+        N = len(hasher.hexdigest())\n+        dcopy['digest'] = \"0\"*N\n+        \n+        representation = json.dumps(dcopy,sort_keys=True,indent=4)\n+        hasher.update(representation)\n+        digest = hasher.hexdigest()\n+\n+        self['digest'] = digest\n+\n+    def verify(self):\n+        our_digest = self['digest']\n+\n+        dcopy = self.copy()\n+\n+        hasher = hashlib.new('md5')\n+        N = len(hasher.hexdigest())\n+        dcopy['digest'] = \"0\"*N\n+        \n+        representation = json.dumps(dcopy,sort_keys=True,indent=4)\n+        hasher.update(representation)\n+        their_digest = hasher.hexdigest()\n+\n+        return our_digest == their_digest\n+\n+\n class Echo(DatagramProtocol):\n \n     def datagramReceived(self, data, (host, port)):\n         print \"received %r from %s:%d\" % (data, host, port)\n         self.transport.write(data, (host, port))\n \n-reactor.listenUDP(9999, Echo())\n-reactor.run()\n+#reactor.listenUDP(9999, Echo())\n+#reactor.run()\n","files":{"\/sidereal\/server.py":{"changes":[{"diff":"\n+import json\n+import hashlib\n+\n from twisted.internet.protocol import DatagramProtocol\n from twisted.internet import reactor\n \n+class DigestDict(dict):\n+    def digest(self):\n+        # Given a dictionary, inserts a json digest, assuming that\n+        # the element 'digest' contains N zeros, and it's printed\n+        # with indent 4, sorted\n+        # where N is the hash length\n+        dcopy = self.copy()\n+        hasher = hashlib.new('md5')\n+        N = len(hasher.hexdigest())\n+        dcopy['digest'] = \"0\"*N\n+        \n+        representation = json.dumps(dcopy,sort_keys=True,indent=4)\n+        hasher.update(representation)\n+        digest = hasher.hexdigest()\n+\n+        self['digest'] = digest\n+\n+    def verify(self):\n+        our_digest = self['digest']\n+\n+        dcopy = self.copy()\n+\n+        hasher = hashlib.new('md5')\n+        N = len(hasher.hexdigest())\n+        dcopy['digest'] = \"0\"*N\n+        \n+        representation = json.dumps(dcopy,sort_keys=True,indent=4)\n+        hasher.update(representation)\n+        their_digest = hasher.hexdigest()\n+\n+        return our_digest == their_digest\n+\n+\n class Echo(DatagramProtocol):\n \n     def datagramReceived(self, data, (host, port)):\n         print \"received %r from %s:%d\" % (data, host, port)\n         self.transport.write(data, (host, port))\n \n-reactor.listenUDP(9999, Echo())\n-reactor.run()\n+#reactor.listenUDP(9999, Echo())\n+#reactor.run()\n","add":38,"remove":2,"filename":"\/sidereal\/server.py","badparts":["reactor.listenUDP(9999, Echo())","reactor.run()"],"goodparts":["import json","import hashlib","class DigestDict(dict):","    def digest(self):","        dcopy = self.copy()","        hasher = hashlib.new('md5')","        N = len(hasher.hexdigest())","        dcopy['digest'] = \"0\"*N","        representation = json.dumps(dcopy,sort_keys=True,indent=4)","        hasher.update(representation)","        digest = hasher.hexdigest()","        self['digest'] = digest","    def verify(self):","        our_digest = self['digest']","        dcopy = self.copy()","        hasher = hashlib.new('md5')","        N = len(hasher.hexdigest())","        dcopy['digest'] = \"0\"*N","        representation = json.dumps(dcopy,sort_keys=True,indent=4)","        hasher.update(representation)","        their_digest = hasher.hexdigest()","        return our_digest == their_digest"]}],"source":"\nfrom twisted.internet.protocol import DatagramProtocol from twisted.internet import reactor class Echo(DatagramProtocol): def datagramReceived(self, data,(host, port)): print \"received %r from %s:%d\" %(data, host, port) self.transport.write(data,(host, port)) reactor.listenUDP(9999, Echo()) reactor.run() ","sourceWithComments":"from twisted.internet.protocol import DatagramProtocol\nfrom twisted.internet import reactor\n\nclass Echo(DatagramProtocol):\n\n    def datagramReceived(self, data, (host, port)):\n        print \"received %r from %s:%d\" % (data, host, port)\n        self.transport.write(data, (host, port))\n\nreactor.listenUDP(9999, Echo())\nreactor.run()\n"}},"msg":"Added DigestDict code, for verification of JSON\n\nWhen we're sending our JSON across the network, we can use this function\nto ensure that nothing's been changed. This isn't security against\nmalicious tampering, because frankly, md5 is completely broken."}},"https:\/\/github.com\/Osanyin-Taiwo-30\/Cartopy":{"e6684cfb73614acb2139605f6d627f469d41c6a3":{"url":"https:\/\/api.github.com\/repos\/Osanyin-Taiwo-30\/Cartopy\/commits\/e6684cfb73614acb2139605f6d627f469d41c6a3","html_url":"https:\/\/github.com\/Osanyin-Taiwo-30\/Cartopy\/commit\/e6684cfb73614acb2139605f6d627f469d41c6a3","message":"cartopy.io.shapereader: Download NaturalEarth Zip files via HTTPS\n\nAttempting to retrieve NaturalEarth files via HTTP could allow an\nadequately positioned attacker to supply maliciously modified resources\nto a user by tampering with the Natural Earth 301 redirect response\n(from http:\/\/naciscdn.org\/... to https:\/\/naciscdn.org\/...).\n\nHowever, this change is largely a matter of security best practice.\nPer the following, users are likely not faced with any significant\npractical risk:\n\n* The use of zip_file_contents() in shapereader.py enforces a whitelist\n  of files to extract from the zip file. This approach, as opposed to\n  extracting all zip file contents, mitigates potential risks\n  associated with unchecked path traversal.\n\n* Assuming that targeted users received a maliciously modified\n  NaturalEarth zip file, compromise would be predicated on the existence\n  of higher-impact vulnerabilities in the zipfile module and\/or ERSI\n  Shapefile files parsers.\n\n* Shapefiles saved and only downloaded as-needed. Assuming any\n  exploitable conditions were present, and a user was on a hostile network\n  (e.g. open Wi-Fi), the window of opportunity for an attacker would be\n  restricted to a situation in which a user is requesting a resource for\n  the first time.\n\nNonetheless, I'd encourage package maintainers and any concerned users to\nbackport this and related patches.","sha":"e6684cfb73614acb2139605f6d627f469d41c6a3","keyword":"tampering malicious","diff":"diff --git a\/lib\/cartopy\/io\/shapereader.py b\/lib\/cartopy\/io\/shapereader.py\nindex 5dbcab72..78875fd3 100644\n--- a\/lib\/cartopy\/io\/shapereader.py\n+++ b\/lib\/cartopy\/io\/shapereader.py\n@@ -307,7 +307,7 @@ class NEShpDownloader(Downloader):\n     # Define the NaturalEarth URL template. The natural earth website\n     # returns a 302 status if accessing directly, so we use the naciscdn\n     # URL directly.\n-    _NE_URL_TEMPLATE = ('http:\/\/naciscdn.org\/naturalearth\/{resolution}'\n+    _NE_URL_TEMPLATE = ('https:\/\/naciscdn.org\/naturalearth\/{resolution}'\n                         '\/{category}\/ne_{resolution}_{name}.zip')\n \n     def __init__(self,\n","files":{"\/lib\/cartopy\/io\/shapereader.py":{"changes":[{"diff":"\n     # Define the NaturalEarth URL template. The natural earth website\n     # returns a 302 status if accessing directly, so we use the naciscdn\n     # URL directly.\n-    _NE_URL_TEMPLATE = ('http:\/\/naciscdn.org\/naturalearth\/{resolution}'\n+    _NE_URL_TEMPLATE = ('https:\/\/naciscdn.org\/naturalearth\/{resolution}'\n                         '\/{category}\/ne_{resolution}_{name}.zip')\n \n     def __init__(self,\n","add":1,"remove":1,"filename":"\/lib\/cartopy\/io\/shapereader.py","badparts":["    _NE_URL_TEMPLATE = ('http:\/\/naciscdn.org\/naturalearth\/{resolution}'"],"goodparts":["    _NE_URL_TEMPLATE = ('https:\/\/naciscdn.org\/naturalearth\/{resolution}'"]}],"source":"\n \"\"\" Combine the shapefile access of pyshp with the geometry representation of shapely: >>> import cartopy.io.shapereader as shapereader >>> filename=shapereader.natural_earth(resolution='110m', ... category='physical', ... name='geography_regions_points') >>> reader=shapereader.Reader(filename) >>> len(reader) 3 >>> records=list(reader.records()) >>> print(', '.join(str(r) for r in sorted(records[0].attributes.keys()))) comment,... name, name_alt,... region,... >>> print(records[0].attributes['name']) Niagara Falls >>> geoms=list(reader.geometries()) >>> print(type(geoms[0])) <class 'shapely.geometry.point.Point'> >>> reader.close() \"\"\" from __future__ import(absolute_import, division, print_function) import glob import itertools import os import shapely.geometry as sgeom import shapefile import six from cartopy.io import Downloader from cartopy import config _HAS_FIONA=False try: import fiona _HAS_FIONA=True except ImportError: pass __all__=['Reader', 'Record'] class Record(object): \"\"\" A single logical entry from a shapefile, combining the attributes with their associated geometry. \"\"\" def __init__(self, shape, attributes, fields): self._shape=shape self._bounds=None if hasattr(shape, 'bbox'): self._bounds=tuple(shape.bbox) self._geometry=None \"\"\"The cached geometry instance for this Record.\"\"\" self.attributes=attributes \"\"\"A dictionary mapping attribute names to attribute values.\"\"\" self._fields=fields def __repr__(self): return '<Record: %r, %r, <fields>>' %(self.geometry, self.attributes) def __str__(self): return 'Record(%s, %s, <fields>)' %(self.geometry, self.attributes) @property def bounds(self): \"\"\" The bounds of this Record's:meth:`~Record.geometry`. \"\"\" if self._bounds is None: self._bounds=self.geometry.bounds return self._bounds @property def geometry(self): \"\"\" A shapely.geometry instance for this Record. The geometry may be ``None`` if a null shape is defined in the shapefile. \"\"\" if not self._geometry and self._shape.shapeType !=shapefile.NULL: self._geometry=sgeom.shape(self._shape) return self._geometry class FionaRecord(Record): \"\"\" A single logical entry from a shapefile, combining the attributes with their associated geometry. This extends the standard Record to work with the FionaReader. \"\"\" def __init__(self, geometry, attributes): self._geometry=geometry self.attributes=attributes self._bounds=geometry.bounds class BasicReader(object): \"\"\" Provide an interface for accessing the contents of a shapefile. The primary methods used on a Reader instance are :meth:`~Reader.records` and:meth:`~Reader.geometries`. \"\"\" def __init__(self, filename): self._reader=reader=shapefile.Reader(filename) if reader.shp is None or reader.shx is None or reader.dbf is None: raise ValueError(\"Incomplete shapefile definition \" \"in '%s'.\" % filename) self._fields=self._reader.fields def close(self): return self._reader.close() def __len__(self): return self._reader.numRecords def geometries(self): \"\"\" Return an iterator of shapely geometries from the shapefile. This interface is useful for accessing the geometries of the shapefile where knowledge of the associated metadata is not necessary. In the case where further metadata is needed use the :meth:`~Reader.records` interface instead, extracting the geometry from the record with the :meth:`~Record.geometry` method. \"\"\" for i in range(self._reader.numRecords): yield sgeom.shape(self._reader.shape(i)) def records(self): \"\"\" Return an iterator of:class:`~Record` instances. \"\"\" fields=self._reader.fields[1:] field_names=[field[0] for field in fields] for i in range(self._reader.numRecords): shape_record=self._reader.shapeRecord(i) attributes=dict(zip(field_names, shape_record.record)) yield Record(shape_record.shape, attributes, fields) class FionaReader(object): \"\"\" Provides an interface for accessing the contents of a shapefile with the fiona library, which has a much faster reader than pyshp. The primary methods used on a Reader instance are :meth:`~Reader.records` and:meth:`~Reader.geometries`. \"\"\" def __init__(self, filename, bbox=None): self._data=[] with fiona.open(filename) as f: if bbox is not None: assert len(bbox)==4 features=f.filter(bbox=bbox) else: features=f if hasattr(features, \"__geo_interface__\"): fs=features.__geo_interface__ else: fs=features if isinstance(fs, dict) and fs.get('type')=='FeatureCollection': features_lst=fs['features'] else: features_lst=features for feature in features_lst: if hasattr(f, \"__geo_interface__\"): feature=feature.__geo_interface__ else: feature=feature d={'geometry': sgeom.shape(feature['geometry']) if feature['geometry'] else None} d.update(feature['properties']) self._data.append(d) def close(self): pass def __len__(self): return len(self._data) def geometries(self): \"\"\" Returns an iterator of shapely geometries from the shapefile. This interface is useful for accessing the geometries of the shapefile where knowledge of the associated metadata is desired. In the case where further metadata is needed use the :meth:`~Reader.records` interface instead, extracting the geometry from the record with the :meth:`~Record.geometry` method. \"\"\" for item in self._data: yield item['geometry'] def records(self): \"\"\" Returns an iterator of:class:`~Record` instances. \"\"\" for item in self._data: yield FionaRecord(item['geometry'], {key: value for key, value in item.items() if key !='geometry'}) if _HAS_FIONA: Reader=FionaReader else: Reader=BasicReader def natural_earth(resolution='110m', category='physical', name='coastline'): \"\"\" Return the path to the requested natural earth shapefile, downloading and unziping if necessary. To identify valid components for this function, either browse NaturalEarthData.com, or if you know what you are looking for, go to https:\/\/github.com\/nvkelso\/natural-earth-vector\/tree\/master\/zips to see the actual files which will be downloaded. Note ---- Some of the Natural Earth shapefiles have special features which are described in the name. For example, the 110m resolution \"admin_0_countries\" data also has a sibling shapefile called \"admin_0_countries_lakes\" which excludes lakes in the country outlines. For details of what is available refer to the Natural Earth website, and look at the \"download\" link target to identify appropriate names. \"\"\" ne_downloader=Downloader.from_config(('shapefiles', 'natural_earth', resolution, category, name)) format_dict={'config': config, 'category': category, 'name': name, 'resolution': resolution} return ne_downloader.path(format_dict) class NEShpDownloader(Downloader): \"\"\" Specialise:class:`cartopy.io.Downloader` to download the zipped Natural Earth shapefiles and extract them to the defined location (typically user configurable). The keys which should be passed through when using the ``format_dict`` are typically ``category``, ``resolution`` and ``name``. \"\"\" FORMAT_KEYS=('config', 'resolution', 'category', 'name') _NE_URL_TEMPLATE=('http:\/\/naciscdn.org\/naturalearth\/{resolution}' '\/{category}\/ne_{resolution}_{name}.zip') def __init__(self, url_template=_NE_URL_TEMPLATE, target_path_template=None, pre_downloaded_path_template='', ): Downloader.__init__(self, url_template, target_path_template, pre_downloaded_path_template) def zip_file_contents(self, format_dict): \"\"\" Return a generator of the filenames to be found in the downloaded natural earth zip file. \"\"\" for ext in['.shp', '.dbf', '.shx']: yield('ne_{resolution}_{name}' '{extension}'.format(extension=ext, **format_dict)) def acquire_resource(self, target_path, format_dict): \"\"\" Download the zip file and extracts the files listed in :meth:`zip_file_contents` to the target path. \"\"\" from zipfile import ZipFile target_dir=os.path.dirname(target_path) if not os.path.isdir(target_dir): os.makedirs(target_dir) url=self.url(format_dict) shapefile_online=self._urlopen(url) zfh=ZipFile(six.BytesIO(shapefile_online.read()), 'r') for member_path in self.zip_file_contents(format_dict): ext=os.path.splitext(member_path)[1] target=os.path.splitext(target_path)[0] +ext member=zfh.getinfo(member_path.replace(os.sep, '\/')) with open(target, 'wb') as fh: fh.write(zfh.open(member).read()) shapefile_online.close() zfh.close() return target_path @staticmethod def default_downloader(): \"\"\" Return a generic, standard, NEShpDownloader instance. Typically, a user will not need to call this staticmethod. To find the path template of the NEShpDownloader: >>> ne_dnldr=NEShpDownloader.default_downloader() >>> print(ne_dnldr.target_path_template) {config[data_dir]}\/shapefiles\/natural_earth\/{category}\/\\ ne_{resolution}_{name}.shp \"\"\" default_spec=('shapefiles', 'natural_earth', '{category}', 'ne_{resolution}_{name}.shp') ne_path_template=os.path.join('{config[data_dir]}', *default_spec) pre_path_template=os.path.join('{config[pre_existing_data_dir]}', *default_spec) return NEShpDownloader(target_path_template=ne_path_template, pre_downloaded_path_template=pre_path_template) _ne_key=('shapefiles', 'natural_earth') config['downloaders'].setdefault(_ne_key, NEShpDownloader.default_downloader()) def gshhs(scale='c', level=1): \"\"\" Return the path to the requested GSHHS shapefile, downloading and unziping if necessary. \"\"\" gshhs_downloader=Downloader.from_config(('shapefiles', 'gshhs', scale, level)) format_dict={'config': config, 'scale': scale, 'level': level} return gshhs_downloader.path(format_dict) class GSHHSShpDownloader(Downloader): \"\"\" Specialise:class:`cartopy.io.Downloader` to download the zipped GSHHS shapefiles and extract them to the defined location. The keys which should be passed through when using the ``format_dict`` are ``scale``(a single character indicating the resolution) and ``level`` (a number indicating the type of feature). \"\"\" FORMAT_KEYS=('config', 'scale', 'level') _GSHHS_URL_TEMPLATE=('https:\/\/www.ngdc.noaa.gov\/mgg\/shorelines\/data\/' 'gshhs\/oldversions\/version2.2.0\/' 'GSHHS_shp_2.2.0.zip') def __init__(self, url_template=_GSHHS_URL_TEMPLATE, target_path_template=None, pre_downloaded_path_template=''): super(GSHHSShpDownloader, self).__init__(url_template, target_path_template, pre_downloaded_path_template) def zip_file_contents(self, format_dict): \"\"\" Return a generator of the filenames to be found in the downloaded GSHHS zip file for the specified resource. \"\"\" for ext in['.shp', '.dbf', '.shx']: yield(os.path.join('GSHHS_shp', '{scale}', 'GSHHS_{scale}_L{level}{extension}' ).format(extension=ext, **format_dict)) def acquire_all_resources(self, format_dict): from zipfile import ZipFile url=self.url(format_dict) shapefile_online=self._urlopen(url) zfh=ZipFile(six.BytesIO(shapefile_online.read()), 'r') shapefile_online.close() modified_format_dict=dict(format_dict) scales=('c', 'l', 'i', 'h', 'f') levels=(1, 2, 3, 4) for scale, level in itertools.product(scales, levels): modified_format_dict.update({'scale': scale, 'level': level}) target_path=self.target_path(modified_format_dict) target_dir=os.path.dirname(target_path) if not os.path.isdir(target_dir): os.makedirs(target_dir) for member_path in self.zip_file_contents(modified_format_dict): ext=os.path.splitext(member_path)[1] target=os.path.splitext(target_path)[0] +ext member=zfh.getinfo(member_path.replace(os.sep, '\/')) with open(target, 'wb') as fh: fh.write(zfh.open(member).read()) zfh.close() def acquire_resource(self, target_path, format_dict): \"\"\" Download the zip file and extracts the files listed in :meth:`zip_file_contents` to the target path. Note ---- Because some of the GSHSS data is available with the cartopy repository, scales of \"l\" or \"c\" will not be downloaded if they exist in the ``cartopy.config['repo_data_dir']`` directory. \"\"\" repo_fname_pattern=os.path.join(config['repo_data_dir'], 'shapefiles', 'gshhs', '{scale}', 'GSHHS_{scale}_L?.shp') repo_fname_pattern=repo_fname_pattern.format(**format_dict) repo_fnames=glob.glob(repo_fname_pattern) if repo_fnames: assert len(repo_fnames)==1, '>1 repo files found for GSHHS' return repo_fnames[0] self.acquire_all_resources(format_dict) if not os.path.exists(target_path): raise RuntimeError('Failed to download and extract GSHHS ' 'shapefile to{!r}.'.format(target_path)) return target_path @staticmethod def default_downloader(): \"\"\" Return a GSHHSShpDownloader instance that expects(and if necessary downloads and installs) shapefiles in the data directory of the cartopy installation. Typically, a user will not need to call this staticmethod. To find the path template of the GSHHSShpDownloader: >>> gshhs_dnldr=GSHHSShpDownloader.default_downloader() >>> print(gshhs_dnldr.target_path_template) {config[data_dir]}\/shapefiles\/gshhs\/{scale}\/\\ GSHHS_{scale}_L{level}.shp \"\"\" default_spec=('shapefiles', 'gshhs', '{scale}', 'GSHHS_{scale}_L{level}.shp') gshhs_path_template=os.path.join('{config[data_dir]}', *default_spec) pre_path_tmplt=os.path.join('{config[pre_existing_data_dir]}', *default_spec) return GSHHSShpDownloader(target_path_template=gshhs_path_template, pre_downloaded_path_template=pre_path_tmplt) _gshhs_key=('shapefiles', 'gshhs') config['downloaders'].setdefault(_gshhs_key, GSHHSShpDownloader.default_downloader()) ","sourceWithComments":"# (C) British Crown Copyright 2011 - 2018, Met Office\n#\n# This file is part of cartopy.\n#\n# cartopy is free software: you can redistribute it and\/or modify it under\n# the terms of the GNU Lesser General Public License as published by the\n# Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# cartopy is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Lesser General Public License for more details.\n#\n# You should have received a copy of the GNU Lesser General Public License\n# along with cartopy.  If not, see <https:\/\/www.gnu.org\/licenses\/>.\n\n\n\"\"\"\nCombine the shapefile access of pyshp with the\ngeometry representation of shapely:\n\n    >>> import cartopy.io.shapereader as shapereader\n    >>> filename = shapereader.natural_earth(resolution='110m',\n    ...                                      category='physical',\n    ...                                      name='geography_regions_points')\n    >>> reader = shapereader.Reader(filename)\n    >>> len(reader)\n    3\n    >>> records = list(reader.records())\n    >>> print(', '.join(str(r) for r in sorted(records[0].attributes.keys())))\n    comment, ... name, name_alt, ... region, ...\n    >>> print(records[0].attributes['name'])\n    Niagara Falls\n    >>> geoms = list(reader.geometries())\n    >>> print(type(geoms[0]))\n    <class 'shapely.geometry.point.Point'>\n    >>> reader.close()\n\n\"\"\"\n\nfrom __future__ import (absolute_import, division, print_function)\n\nimport glob\nimport itertools\nimport os\n\nimport shapely.geometry as sgeom\nimport shapefile\nimport six\n\nfrom cartopy.io import Downloader\nfrom cartopy import config\n_HAS_FIONA = False\ntry:\n    import fiona\n    _HAS_FIONA = True\nexcept ImportError:\n    pass\n\n__all__ = ['Reader', 'Record']\n\n\nclass Record(object):\n    \"\"\"\n    A single logical entry from a shapefile, combining the attributes with\n    their associated geometry.\n\n    \"\"\"\n    def __init__(self, shape, attributes, fields):\n        self._shape = shape\n\n        self._bounds = None\n        # if the record defines a bbox, then use that for the shape's bounds,\n        # rather than using the full geometry in the bounds property\n        if hasattr(shape, 'bbox'):\n            self._bounds = tuple(shape.bbox)\n\n        self._geometry = None\n        \"\"\"The cached geometry instance for this Record.\"\"\"\n\n        self.attributes = attributes\n        \"\"\"A dictionary mapping attribute names to attribute values.\"\"\"\n\n        self._fields = fields\n\n    def __repr__(self):\n        return '<Record: %r, %r, <fields>>' % (self.geometry, self.attributes)\n\n    def __str__(self):\n        return 'Record(%s, %s, <fields>)' % (self.geometry, self.attributes)\n\n    @property\n    def bounds(self):\n        \"\"\"\n        The bounds of this Record's :meth:`~Record.geometry`.\n\n        \"\"\"\n        if self._bounds is None:\n            self._bounds = self.geometry.bounds\n        return self._bounds\n\n    @property\n    def geometry(self):\n        \"\"\"\n        A shapely.geometry instance for this Record.\n\n        The geometry may be ``None`` if a null shape is defined in the\n        shapefile.\n\n        \"\"\"\n        if not self._geometry and self._shape.shapeType != shapefile.NULL:\n            self._geometry = sgeom.shape(self._shape)\n        return self._geometry\n\n\nclass FionaRecord(Record):\n    \"\"\"\n    A single logical entry from a shapefile, combining the attributes with\n    their associated geometry. This extends the standard Record to work\n    with the FionaReader.\n\n    \"\"\"\n    def __init__(self, geometry, attributes):\n        self._geometry = geometry\n        self.attributes = attributes\n        self._bounds = geometry.bounds\n\n\nclass BasicReader(object):\n    \"\"\"\n    Provide an interface for accessing the contents of a shapefile.\n\n    The primary methods used on a Reader instance are\n    :meth:`~Reader.records` and :meth:`~Reader.geometries`.\n\n    \"\"\"\n    def __init__(self, filename):\n        # Validate the filename\/shapefile\n        self._reader = reader = shapefile.Reader(filename)\n        if reader.shp is None or reader.shx is None or reader.dbf is None:\n            raise ValueError(\"Incomplete shapefile definition \"\n                             \"in '%s'.\" % filename)\n\n        self._fields = self._reader.fields\n\n    def close(self):\n        return self._reader.close()\n\n    def __len__(self):\n        return self._reader.numRecords\n\n    def geometries(self):\n        \"\"\"\n        Return an iterator of shapely geometries from the shapefile.\n\n        This interface is useful for accessing the geometries of the\n        shapefile where knowledge of the associated metadata is not necessary.\n        In the case where further metadata is needed use the\n        :meth:`~Reader.records`\n        interface instead, extracting the geometry from the record with the\n        :meth:`~Record.geometry` method.\n\n        \"\"\"\n        for i in range(self._reader.numRecords):\n            yield sgeom.shape(self._reader.shape(i))\n\n    def records(self):\n        \"\"\"\n        Return an iterator of :class:`~Record` instances.\n\n        \"\"\"\n        # Ignore the \"DeletionFlag\" field which always comes first\n        fields = self._reader.fields[1:]\n        field_names = [field[0] for field in fields]\n        for i in range(self._reader.numRecords):\n            shape_record = self._reader.shapeRecord(i)\n            attributes = dict(zip(field_names, shape_record.record))\n            yield Record(shape_record.shape, attributes, fields)\n\n\nclass FionaReader(object):\n    \"\"\"\n    Provides an interface for accessing the contents of a shapefile\n    with the fiona library, which has a much faster reader than pyshp.\n\n    The primary methods used on a Reader instance are\n    :meth:`~Reader.records` and :meth:`~Reader.geometries`.\n\n    \"\"\"\n    def __init__(self, filename, bbox=None):\n        self._data = []\n\n        with fiona.open(filename) as f:\n            if bbox is not None:\n                assert len(bbox) == 4\n                features = f.filter(bbox=bbox)\n            else:\n                features = f\n\n            # Handle feature collections\n            if hasattr(features, \"__geo_interface__\"):\n                fs = features.__geo_interface__\n            else:\n                fs = features\n\n            if isinstance(fs, dict) and fs.get('type') == 'FeatureCollection':\n                features_lst = fs['features']\n            else:\n                features_lst = features\n\n            for feature in features_lst:\n                if hasattr(f, \"__geo_interface__\"):\n                    feature = feature.__geo_interface__\n                else:\n                    feature = feature\n\n                d = {'geometry': sgeom.shape(feature['geometry'])\n                     if feature['geometry'] else None}\n                d.update(feature['properties'])\n                self._data.append(d)\n\n    def close(self):\n        # TODO: Keep the Fiona handle open until this is called.\n        # This will enable us to pass down calls for bounding box queries,\n        # rather than having to have it all in memory.\n        pass\n\n    def __len__(self):\n        return len(self._data)\n\n    def geometries(self):\n        \"\"\"\n        Returns an iterator of shapely geometries from the shapefile.\n\n        This interface is useful for accessing the geometries of the\n        shapefile where knowledge of the associated metadata is desired.\n        In the case where further metadata is needed use the\n        :meth:`~Reader.records`\n        interface instead, extracting the geometry from the record with the\n        :meth:`~Record.geometry` method.\n\n        \"\"\"\n        for item in self._data:\n            yield item['geometry']\n\n    def records(self):\n        \"\"\"\n        Returns an iterator of :class:`~Record` instances.\n\n        \"\"\"\n        for item in self._data:\n            yield FionaRecord(item['geometry'],\n                              {key: value for key, value in\n                               item.items() if key != 'geometry'})\n\n\nif _HAS_FIONA:\n    Reader = FionaReader\nelse:\n    Reader = BasicReader\n\n\ndef natural_earth(resolution='110m', category='physical', name='coastline'):\n    \"\"\"\n    Return the path to the requested natural earth shapefile,\n    downloading and unziping if necessary.\n\n    To identify valid components for this function, either browse\n    NaturalEarthData.com, or if you know what you are looking for, go to\n    https:\/\/github.com\/nvkelso\/natural-earth-vector\/tree\/master\/zips to\n    see the actual files which will be downloaded.\n\n    Note\n    ----\n        Some of the Natural Earth shapefiles have special features which are\n        described in the name. For example, the 110m resolution\n        \"admin_0_countries\" data also has a sibling shapefile called\n        \"admin_0_countries_lakes\" which excludes lakes in the country\n        outlines. For details of what is available refer to the Natural Earth\n        website, and look at the \"download\" link target to identify\n        appropriate names.\n\n    \"\"\"\n    # get hold of the Downloader (typically a NEShpDownloader instance)\n    # which we can then simply call its path method to get the appropriate\n    # shapefile (it will download if necessary)\n    ne_downloader = Downloader.from_config(('shapefiles', 'natural_earth',\n                                            resolution, category, name))\n    format_dict = {'config': config, 'category': category,\n                   'name': name, 'resolution': resolution}\n    return ne_downloader.path(format_dict)\n\n\nclass NEShpDownloader(Downloader):\n    \"\"\"\n    Specialise :class:`cartopy.io.Downloader` to download the zipped\n    Natural Earth shapefiles and extract them to the defined location\n    (typically user configurable).\n\n    The keys which should be passed through when using the ``format_dict``\n    are typically ``category``, ``resolution`` and ``name``.\n\n    \"\"\"\n    FORMAT_KEYS = ('config', 'resolution', 'category', 'name')\n\n    # Define the NaturalEarth URL template. The natural earth website\n    # returns a 302 status if accessing directly, so we use the naciscdn\n    # URL directly.\n    _NE_URL_TEMPLATE = ('http:\/\/naciscdn.org\/naturalearth\/{resolution}'\n                        '\/{category}\/ne_{resolution}_{name}.zip')\n\n    def __init__(self,\n                 url_template=_NE_URL_TEMPLATE,\n                 target_path_template=None,\n                 pre_downloaded_path_template='',\n                 ):\n        # adds some NE defaults to the __init__ of a Downloader\n        Downloader.__init__(self, url_template,\n                            target_path_template,\n                            pre_downloaded_path_template)\n\n    def zip_file_contents(self, format_dict):\n        \"\"\"\n        Return a generator of the filenames to be found in the downloaded\n        natural earth zip file.\n\n        \"\"\"\n        for ext in ['.shp', '.dbf', '.shx']:\n            yield ('ne_{resolution}_{name}'\n                   '{extension}'.format(extension=ext, **format_dict))\n\n    def acquire_resource(self, target_path, format_dict):\n        \"\"\"\n        Download the zip file and extracts the files listed in\n        :meth:`zip_file_contents` to the target path.\n\n        \"\"\"\n        from zipfile import ZipFile\n\n        target_dir = os.path.dirname(target_path)\n        if not os.path.isdir(target_dir):\n            os.makedirs(target_dir)\n\n        url = self.url(format_dict)\n\n        shapefile_online = self._urlopen(url)\n\n        zfh = ZipFile(six.BytesIO(shapefile_online.read()), 'r')\n\n        for member_path in self.zip_file_contents(format_dict):\n            ext = os.path.splitext(member_path)[1]\n            target = os.path.splitext(target_path)[0] + ext\n            member = zfh.getinfo(member_path.replace(os.sep, '\/'))\n            with open(target, 'wb') as fh:\n                fh.write(zfh.open(member).read())\n\n        shapefile_online.close()\n        zfh.close()\n\n        return target_path\n\n    @staticmethod\n    def default_downloader():\n        \"\"\"\n        Return a generic, standard, NEShpDownloader instance.\n\n        Typically, a user will not need to call this staticmethod.\n\n        To find the path template of the NEShpDownloader:\n\n            >>> ne_dnldr = NEShpDownloader.default_downloader()\n            >>> print(ne_dnldr.target_path_template)\n            {config[data_dir]}\/shapefiles\/natural_earth\/{category}\/\\\nne_{resolution}_{name}.shp\n\n        \"\"\"\n        default_spec = ('shapefiles', 'natural_earth', '{category}',\n                        'ne_{resolution}_{name}.shp')\n        ne_path_template = os.path.join('{config[data_dir]}', *default_spec)\n        pre_path_template = os.path.join('{config[pre_existing_data_dir]}',\n                                         *default_spec)\n        return NEShpDownloader(target_path_template=ne_path_template,\n                               pre_downloaded_path_template=pre_path_template)\n\n\n# add a generic Natural Earth shapefile downloader to the config dictionary's\n# 'downloaders' section.\n_ne_key = ('shapefiles', 'natural_earth')\nconfig['downloaders'].setdefault(_ne_key,\n                                 NEShpDownloader.default_downloader())\n\n\ndef gshhs(scale='c', level=1):\n    \"\"\"\n    Return the path to the requested GSHHS shapefile,\n    downloading and unziping if necessary.\n\n    \"\"\"\n    # Get hold of the Downloader (typically a GSHHSShpDownloader instance)\n    # and call its path method to get the appropriate shapefile (it will\n    # download it if necessary).\n    gshhs_downloader = Downloader.from_config(('shapefiles', 'gshhs',\n                                               scale, level))\n    format_dict = {'config': config, 'scale': scale, 'level': level}\n    return gshhs_downloader.path(format_dict)\n\n\nclass GSHHSShpDownloader(Downloader):\n    \"\"\"\n    Specialise :class:`cartopy.io.Downloader` to download the zipped\n    GSHHS shapefiles and extract them to the defined location.\n\n    The keys which should be passed through when using the ``format_dict``\n    are ``scale`` (a single character indicating the resolution) and ``level``\n    (a number indicating the type of feature).\n\n    \"\"\"\n    FORMAT_KEYS = ('config', 'scale', 'level')\n\n    _GSHHS_URL_TEMPLATE = ('https:\/\/www.ngdc.noaa.gov\/mgg\/shorelines\/data\/'\n                           'gshhs\/oldversions\/version2.2.0\/'\n                           'GSHHS_shp_2.2.0.zip')\n\n    def __init__(self,\n                 url_template=_GSHHS_URL_TEMPLATE,\n                 target_path_template=None,\n                 pre_downloaded_path_template=''):\n        super(GSHHSShpDownloader, self).__init__(url_template,\n                                                 target_path_template,\n                                                 pre_downloaded_path_template)\n\n    def zip_file_contents(self, format_dict):\n        \"\"\"\n        Return a generator of the filenames to be found in the downloaded\n        GSHHS zip file for the specified resource.\n\n        \"\"\"\n        for ext in ['.shp', '.dbf', '.shx']:\n            yield (os.path.join('GSHHS_shp', '{scale}',\n                                'GSHHS_{scale}_L{level}{extension}'\n                                ).format(extension=ext, **format_dict))\n\n    def acquire_all_resources(self, format_dict):\n        from zipfile import ZipFile\n\n        # Download archive.\n        url = self.url(format_dict)\n        shapefile_online = self._urlopen(url)\n        zfh = ZipFile(six.BytesIO(shapefile_online.read()), 'r')\n        shapefile_online.close()\n\n        # Iterate through all scales and levels and extract relevant files.\n        modified_format_dict = dict(format_dict)\n        scales = ('c', 'l', 'i', 'h', 'f')\n        levels = (1, 2, 3, 4)\n        for scale, level in itertools.product(scales, levels):\n            modified_format_dict.update({'scale': scale, 'level': level})\n            target_path = self.target_path(modified_format_dict)\n            target_dir = os.path.dirname(target_path)\n            if not os.path.isdir(target_dir):\n                os.makedirs(target_dir)\n\n            for member_path in self.zip_file_contents(modified_format_dict):\n                ext = os.path.splitext(member_path)[1]\n                target = os.path.splitext(target_path)[0] + ext\n                member = zfh.getinfo(member_path.replace(os.sep, '\/'))\n                with open(target, 'wb') as fh:\n                    fh.write(zfh.open(member).read())\n\n        zfh.close()\n\n    def acquire_resource(self, target_path, format_dict):\n        \"\"\"\n        Download the zip file and extracts the files listed in\n        :meth:`zip_file_contents` to the target path.\n\n        Note\n        ----\n            Because some of the GSHSS data is available with the cartopy\n            repository, scales of \"l\" or \"c\" will not be downloaded if they\n            exist in the ``cartopy.config['repo_data_dir']`` directory.\n\n        \"\"\"\n        repo_fname_pattern = os.path.join(config['repo_data_dir'],\n                                          'shapefiles', 'gshhs', '{scale}',\n                                          'GSHHS_{scale}_L?.shp')\n        repo_fname_pattern = repo_fname_pattern.format(**format_dict)\n        repo_fnames = glob.glob(repo_fname_pattern)\n        if repo_fnames:\n            assert len(repo_fnames) == 1, '>1 repo files found for GSHHS'\n            return repo_fnames[0]\n        self.acquire_all_resources(format_dict)\n        if not os.path.exists(target_path):\n            raise RuntimeError('Failed to download and extract GSHHS '\n                               'shapefile to {!r}.'.format(target_path))\n        return target_path\n\n    @staticmethod\n    def default_downloader():\n        \"\"\"\n        Return a GSHHSShpDownloader instance that expects (and if necessary\n        downloads and installs) shapefiles in the data directory of the\n        cartopy installation.\n\n        Typically, a user will not need to call this staticmethod.\n\n        To find the path template of the GSHHSShpDownloader:\n\n            >>> gshhs_dnldr = GSHHSShpDownloader.default_downloader()\n            >>> print(gshhs_dnldr.target_path_template)\n            {config[data_dir]}\/shapefiles\/gshhs\/{scale}\/\\\nGSHHS_{scale}_L{level}.shp\n\n        \"\"\"\n        default_spec = ('shapefiles', 'gshhs', '{scale}',\n                        'GSHHS_{scale}_L{level}.shp')\n        gshhs_path_template = os.path.join('{config[data_dir]}',\n                                           *default_spec)\n        pre_path_tmplt = os.path.join('{config[pre_existing_data_dir]}',\n                                      *default_spec)\n        return GSHHSShpDownloader(target_path_template=gshhs_path_template,\n                                  pre_downloaded_path_template=pre_path_tmplt)\n\n\n# Add a GSHHS shapefile downloader to the config dictionary's\n# 'downloaders' section.\n_gshhs_key = ('shapefiles', 'gshhs')\nconfig['downloaders'].setdefault(_gshhs_key,\n                                 GSHHSShpDownloader.default_downloader())\n"}},"msg":"cartopy.io.shapereader: Download NaturalEarth Zip files via HTTPS\n\nAttempting to retrieve NaturalEarth files via HTTP could allow an\nadequately positioned attacker to supply maliciously modified resources\nto a user by tampering with the Natural Earth 301 redirect response\n(from http:\/\/naciscdn.org\/... to https:\/\/naciscdn.org\/...).\n\nHowever, this change is largely a matter of security best practice.\nPer the following, users are likely not faced with any significant\npractical risk:\n\n* The use of zip_file_contents() in shapereader.py enforces a whitelist\n  of files to extract from the zip file. This approach, as opposed to\n  extracting all zip file contents, mitigates potential risks\n  associated with unchecked path traversal.\n\n* Assuming that targeted users received a maliciously modified\n  NaturalEarth zip file, compromise would be predicated on the existence\n  of higher-impact vulnerabilities in the zipfile module and\/or ERSI\n  Shapefile files parsers.\n\n* Shapefiles saved and only downloaded as-needed. Assuming any\n  exploitable conditions were present, and a user was on a hostile network\n  (e.g. open Wi-Fi), the window of opportunity for an attacker would be\n  restricted to a situation in which a user is requesting a resource for\n  the first time.\n\nNonetheless, I'd encourage package maintainers and any concerned users to\nbackport this and related patches."},"8fed5f9e6037aeb2101cb52b19205d2cc8609c1b":{"url":"https:\/\/api.github.com\/repos\/Osanyin-Taiwo-30\/Cartopy\/commits\/8fed5f9e6037aeb2101cb52b19205d2cc8609c1b","html_url":"https:\/\/github.com\/Osanyin-Taiwo-30\/Cartopy\/commit\/8fed5f9e6037aeb2101cb52b19205d2cc8609c1b","sha":"8fed5f9e6037aeb2101cb52b19205d2cc8609c1b","keyword":"tampering malicious","diff":"diff --git a\/lib\/cartopy\/io\/img_tiles.py b\/lib\/cartopy\/io\/img_tiles.py\nindex f44b0fef..704e850d 100644\n--- a\/lib\/cartopy\/io\/img_tiles.py\n+++ b\/lib\/cartopy\/io\/img_tiles.py\n@@ -243,15 +243,15 @@ def _image_url(self, tile):\n \n \n class MapQuestOSM(GoogleWTS):\n-    # http:\/\/developer.mapquest.com\/web\/products\/open\/map for terms of use\n-    # http:\/\/devblog.mapquest.com\/2016\/06\/15\/\n+    # https:\/\/developer.mapquest.com\/web\/products\/open\/map for terms of use\n+    # https:\/\/devblog.mapquest.com\/2016\/06\/15\/\n     # modernization-of-mapquest-results-in-changes-to-open-tile-access\/\n     # this now requires a sign up to a plan\n     def _image_url(self, tile):\n         x, y, z = tile\n-        url = 'http:\/\/otile1.mqcdn.com\/tiles\/1.0.0\/osm\/%s\/%s\/%s.jpg' % (\n+        url = 'https:\/\/otile1.mqcdn.com\/tiles\/1.0.0\/osm\/%s\/%s\/%s.jpg' % (\n             z, x, y)\n-        mqdevurl = ('http:\/\/devblog.mapquest.com\/2016\/06\/15\/'\n+        mqdevurl = ('https:\/\/devblog.mapquest.com\/2016\/06\/15\/'\n                     'modernization-of-mapquest-results-in-changes'\n                     '-to-open-tile-access\/')\n         warnings.warn('{} will require a log in and and will likely'\n@@ -260,19 +260,19 @@ def _image_url(self, tile):\n \n \n class MapQuestOpenAerial(GoogleWTS):\n-    # http:\/\/developer.mapquest.com\/web\/products\/open\/map for terms of use\n+    # https:\/\/developer.mapquest.com\/web\/products\/open\/map for terms of use\n     # The following attribution should be included in the resulting image:\n     # \"Portions Courtesy NASA\/JPL-Caltech and U.S. Depart. of Agriculture,\n     #  Farm Service Agency\"\n     def _image_url(self, tile):\n         x, y, z = tile\n-        url = 'http:\/\/oatile1.mqcdn.com\/tiles\/1.0.0\/sat\/%s\/%s\/%s.jpg' % (\n+        url = 'https:\/\/oatile1.mqcdn.com\/tiles\/1.0.0\/sat\/%s\/%s\/%s.jpg' % (\n             z, x, y)\n         return url\n \n \n class OSM(GoogleWTS):\n-    # http:\/\/developer.mapquest.com\/web\/products\/open\/map for terms of use\n+    # https:\/\/developer.mapquest.com\/web\/products\/open\/map for terms of use\n     def _image_url(self, tile):\n         x, y, z = tile\n         url = 'https:\/\/a.tile.openstreetmap.org\/%s\/%s\/%s.png' % (z, x, y)\n","message":"","files":{"\/lib\/cartopy\/io\/img_tiles.py":{"changes":[{"diff":"\n \n \n class MapQuestOSM(GoogleWTS):\n-    # http:\/\/developer.mapquest.com\/web\/products\/open\/map for terms of use\n-    # http:\/\/devblog.mapquest.com\/2016\/06\/15\/\n+    # https:\/\/developer.mapquest.com\/web\/products\/open\/map for terms of use\n+    # https:\/\/devblog.mapquest.com\/2016\/06\/15\/\n     # modernization-of-mapquest-results-in-changes-to-open-tile-access\/\n     # this now requires a sign up to a plan\n     def _image_url(self, tile):\n         x, y, z = tile\n-        url = 'http:\/\/otile1.mqcdn.com\/tiles\/1.0.0\/osm\/%s\/%s\/%s.jpg' % (\n+        url = 'https:\/\/otile1.mqcdn.com\/tiles\/1.0.0\/osm\/%s\/%s\/%s.jpg' % (\n             z, x, y)\n-        mqdevurl = ('http:\/\/devblog.mapquest.com\/2016\/06\/15\/'\n+        mqdevurl = ('https:\/\/devblog.mapquest.com\/2016\/06\/15\/'\n                     'modernization-of-mapquest-results-in-changes'\n                     '-to-open-tile-access\/')\n         warnings.warn('{} will require a log in and and will likely'\n","add":4,"remove":4,"filename":"\/lib\/cartopy\/io\/img_tiles.py","badparts":["        url = 'http:\/\/otile1.mqcdn.com\/tiles\/1.0.0\/osm\/%s\/%s\/%s.jpg' % (","        mqdevurl = ('http:\/\/devblog.mapquest.com\/2016\/06\/15\/'"],"goodparts":["        url = 'https:\/\/otile1.mqcdn.com\/tiles\/1.0.0\/osm\/%s\/%s\/%s.jpg' % (","        mqdevurl = ('https:\/\/devblog.mapquest.com\/2016\/06\/15\/'"]},{"diff":"\n \n \n class MapQuestOpenAerial(GoogleWTS):\n-    # http:\/\/developer.mapquest.com\/web\/products\/open\/map for terms of use\n+    # https:\/\/developer.mapquest.com\/web\/products\/open\/map for terms of use\n     # The following attribution should be included in the resulting image:\n     # \"Portions Courtesy NASA\/JPL-Caltech and U.S. Depart. of Agriculture,\n     #  Farm Service Agency\"\n     def _image_url(self, tile):\n         x, y, z = tile\n-        url = 'http:\/\/oatile1.mqcdn.com\/tiles\/1.0.0\/sat\/%s\/%s\/%s.jpg' % (\n+        url = 'https:\/\/oatile1.mqcdn.com\/tiles\/1.0.0\/sat\/%s\/%s\/%s.jpg' % (\n             z, x, y)\n         return url\n \n \n class OSM(GoogleWTS):\n-    # http:\/\/developer.mapquest.com\/web\/products\/open\/map for terms of use\n+    # https:\/\/developer.mapquest.com\/web\/products\/open\/map for terms of use\n     def _image_url(self, tile):\n         x, y, z = tile\n         url = 'https:\/\/a.tile.openstreetmap.org\/%s\/%s\/%s.png' % (z, x, y)\n","add":3,"remove":3,"filename":"\/lib\/cartopy\/io\/img_tiles.py","badparts":["        url = 'http:\/\/oatile1.mqcdn.com\/tiles\/1.0.0\/sat\/%s\/%s\/%s.jpg' % ("],"goodparts":["        url = 'https:\/\/oatile1.mqcdn.com\/tiles\/1.0.0\/sat\/%s\/%s\/%s.jpg' % ("]}],"source":"\n \"\"\" Implements image tile identification and fetching from various sources. The Matplotlib interface can make use of tile objects(defined below) via the :meth:`cartopy.mpl.geoaxes.GeoAxes.add_image` method. For example, to add a :class:`MapQuest Open Aerial tileset <MapQuestOpenAerial>` to an existing axes at zoom level 2, do ``ax.add_image(MapQuestOpenAerial(), 2)``. An example of using tiles in this way can be found at the :ref:`sphx_glr_gallery_eyja_volcano.py` example. \"\"\" from __future__ import(absolute_import, division, print_function) from abc import ABCMeta, abstractmethod import concurrent.futures import warnings from PIL import Image import shapely.geometry as sgeom import numpy as np import six import cartopy.crs as ccrs class GoogleWTS(six.with_metaclass(ABCMeta, object)): \"\"\" Implement web tile retrieval using the Google WTS coordinate system. A \"tile\" in this class refers to the coordinates(x, y, z). \"\"\" _MAX_THREADS=24 def __init__(self, desired_tile_form='RGB'): self.imgs=[] self.crs=ccrs.Mercator.GOOGLE self.desired_tile_form=desired_tile_form def image_for_domain(self, target_domain, target_z): tiles=[] def fetch_tile(tile): try: img, extent, origin=self.get_image(tile) except IOError: raise img=np.array(img) x=np.linspace(extent[0], extent[1], img.shape[1]) y=np.linspace(extent[2], extent[3], img.shape[0]) return img, x, y, origin with concurrent.futures.ThreadPoolExecutor( max_workers=self._MAX_THREADS) as executor: futures=[] for tile in self.find_images(target_domain, target_z): futures.append(executor.submit(fetch_tile, tile)) for future in concurrent.futures.as_completed(futures): try: img, x, y, origin=future.result() tiles.append([img, x, y, origin]) except IOError: pass img, extent, origin=_merge_tiles(tiles) return img, extent, origin def _find_images(self, target_domain, target_z, start_tile=(0, 0, 0)): \"\"\"Target domain is a shapely polygon in native coordinates.\"\"\" assert isinstance(target_z, int) and target_z >=0,('target_z must ' 'be an integer ' '>=0.') x0, x1, y0, y1=self._tileextent(start_tile) domain=sgeom.box(x0, y0, x1, y1) if domain.intersects(target_domain): if start_tile[2]==target_z: yield start_tile else: for tile in self._subtiles(start_tile): for result in self._find_images(target_domain, target_z, start_tile=tile): yield result find_images=_find_images def subtiles(self, x_y_z): x, y, z=x_y_z for xi in range(0, 2): for yi in range(0, 2): yield x * 2 +xi, y * 2 +yi, z +1 _subtiles=subtiles def tile_bbox(self, x, y, z, y0_at_north_pole=True): \"\"\" Return the ``(x0, x1),(y0, y1)`` bounding box for the given x, y, z tile position. Parameters ---------- x The x tile coordinate in the Google tile numbering system. y The y tile coordinate in the Google tile numbering system. z The z tile coordinate in the Google tile numbering system. y0_at_north_pole: optional Boolean representing whether the numbering of the y coordinate starts at the north pole(as is the convention for Google tiles) or not(in which case it will start at the south pole, as is the convention for TMS). Defaults to True. \"\"\" n=2 ** z assert 0 <=x <=(n -1),(\"Tile's x index is out of range. Upper \" \"limit %s. Got %s\" %(n, x)) assert 0 <=y <=(n -1),(\"Tile's y index is out of range. Upper \" \"limit %s. Got %s\" %(n, y)) x0, x1=self.crs.x_limits y0, y1=self.crs.y_limits box_h=(y1 -y0) \/ n box_w=(x1 -x0) \/ n n_xs=x0 +(x +np.arange(0, 2, dtype=np.float64)) * box_w n_ys=y0 +(y +np.arange(0, 2, dtype=np.float64)) * box_h if y0_at_north_pole: n_ys=-1 * n_ys[::-1] return n_xs, n_ys def tileextent(self, x_y_z): \"\"\"Return extent tuple ``(x0,x1,y0,y1)`` in Mercator coordinates.\"\"\" x, y, z=x_y_z x_lim, y_lim=self.tile_bbox(x, y, z, y0_at_north_pole=True) return tuple(x_lim) +tuple(y_lim) _tileextent=tileextent @abstractmethod def _image_url(self, tile): pass def get_image(self, tile): if six.PY3: from urllib.request import urlopen else: from urllib2 import urlopen url=self._image_url(tile) fh=urlopen(url) im_data=six.BytesIO(fh.read()) fh.close() img=Image.open(im_data) img=img.convert(self.desired_tile_form) return img, self.tileextent(tile), 'lower' class GoogleTiles(GoogleWTS): def __init__(self, desired_tile_form='RGB', style=\"street\", url=('https:\/\/mts0.google.com\/vt\/lyrs={style}' '@177000000&hl=en&src=api&x={x}&y={y}&z={z}&s=G')): \"\"\" Parameters ---------- desired_tile_form: optional Defaults to 'RGB'. style: optional The style for the Google Maps tiles. One of 'street', 'satellite', 'terrain', and 'only_streets'. Defaults to 'street'. url: optional URL pointing to a tile source and containing{x},{y}, and{z}. Such as: ``'https:\/\/server.arcgisonline.com\/ArcGIS\/rest\/services\/\\ World_Shaded_Relief\/MapServer\/tile\/{z}\/{y}\/{x}.jpg'`` \"\"\" styles=[\"street\", \"satellite\", \"terrain\", \"only_streets\"] style=style.lower() self.url=url if style not in styles: msg=\"Invalid style '%s'. Valid styles: %s\" % \\ (style, \", \".join(styles)) raise ValueError(msg) self.style=style if self.style in[\"satellite\", \"terrain\"] and \\ not hasattr(Image.core, \"jpeg_decoder\") or \\ not Image.core.jpeg_decoder: msg=\"The '%s' style requires pillow with jpeg decoding support.\" raise ValueError(msg % self.style) return super(GoogleTiles, self).__init__( desired_tile_form=desired_tile_form) def _image_url(self, tile): style_dict={ \"street\": \"m\", \"satellite\": \"s\", \"terrain\": \"t\", \"only_streets\": \"h\"} url=self.url.format( style=style_dict[self.style], x=tile[0], X=tile[0], y=tile[1], Y=tile[1], z=tile[2], Z=tile[2]) return url class MapQuestOSM(GoogleWTS): def _image_url(self, tile): x, y, z=tile url='http:\/\/otile1.mqcdn.com\/tiles\/1.0.0\/osm\/%s\/%s\/%s.jpg' %( z, x, y) mqdevurl=('http:\/\/devblog.mapquest.com\/2016\/06\/15\/' 'modernization-of-mapquest-results-in-changes' '-to-open-tile-access\/') warnings.warn('{} will require a log in and and will likely' ' fail. see{} for more details.'.format(url, mqdevurl)) return url class MapQuestOpenAerial(GoogleWTS): def _image_url(self, tile): x, y, z=tile url='http:\/\/oatile1.mqcdn.com\/tiles\/1.0.0\/sat\/%s\/%s\/%s.jpg' %( z, x, y) return url class OSM(GoogleWTS): def _image_url(self, tile): x, y, z=tile url='https:\/\/a.tile.openstreetmap.org\/%s\/%s\/%s.png' %(z, x, y) return url class Stamen(GoogleWTS): \"\"\" Retrieves tiles from maps.stamen.com. Styles include ``terrain-background``, ``terrain``, ``toner`` and ``watercolor``. For a full reference on the styles available please see http:\/\/maps.stamen.com. Of particular note are the sub-styles that are made available(e.g. ``terrain`` and ``terrain-background``). To determine the name of the particular[sub-]style you want, follow the link on http:\/\/maps.stamen.com to your desired style and observe the style name in the URL. Your style name will be in the form of: ``http:\/\/maps.stamen.com\/{STYLE_NAME}\/ Except otherwise noted, the Stamen map tile sets are copyright Stamen Design, under a Creative Commons Attribution(CC BY 3.0) license. Please see the attribution notice at http:\/\/maps.stamen.com on how to attribute this imagery. \"\"\" def __init__(self, style='toner', desired_tile_form='RGB'): super(Stamen, self).__init__(desired_tile_form=desired_tile_form) self.style=style def _image_url(self, tile): return('http:\/\/tile.stamen.com\/{self.style}\/{z}\/{x}\/{y}.png' .format(self=self, x=tile[0], y=tile[1], z=tile[2])) class StamenTerrain(Stamen): \"\"\" **DEPRECATED:** This class is deprecated. Please use ``Stamen('terrain-background')`` instead. Terrain tiles defined for the continental United States, and include land color and shaded hills. The land colors are a custom palette developed by Gem Spear for the National Atlas 1km land cover data set, which defines twenty-four land classifications including five kinds of forest, combinations of shrubs, grasses and crops, and a few tundras and wetlands. The colors are at their highest contrast when fully zoomed-out to the whole U.S., and they slowly fade out to pale off-white as you zoom in to leave room for foreground data and break up the weirdness of large areas of flat, dark green. References ---------- * http:\/\/mike.teczno.com\/notes\/osm-us-terrain-layer\/background.html * http:\/\/maps.stamen.com\/ * https:\/\/wiki.openstreetmap.org\/wiki\/List_of_OSM_based_Services * https:\/\/github.com\/migurski\/DEM-Tools \"\"\" def __init__(self): warnings.warn( \"The StamenTerrain class was deprecated in v0.17. \" \"Please use Stamen('terrain-background') instead.\") return super(StamenTerrain, self).__init__(style='terrain-background') class MapboxTiles(GoogleWTS): \"\"\" Implement web tile retrieval from Mapbox. For terms of service, see https:\/\/www.mapbox.com\/tos\/. \"\"\" def __init__(self, access_token, map_id): \"\"\" Set up a new Mapbox tiles instance. Access to Mapbox web services requires an access token and a map ID. See https:\/\/www.mapbox.com\/api-documentation\/ for details. Parameters ---------- access_token A valid Mapbox API access token. map_id An ID for a publicly accessible map(provided by Mapbox). This is the map whose tiles will be retrieved through this process. \"\"\" self.access_token=access_token self.map_id=map_id super(MapboxTiles, self).__init__() def _image_url(self, tile): x, y, z=tile url=('https:\/\/api.mapbox.com\/v4\/mapbox.{id}\/{z}\/{x}\/{y}.png' '?access_token={token}'.format(z=z, y=y, x=x, id=self.map_id, token=self.access_token)) return url class MapboxStyleTiles(GoogleWTS): \"\"\" Implement web tile retrieval from a user-defined Mapbox style. For more details on Mapbox styles, see https:\/\/www.mapbox.com\/studio-manual\/overview\/map-styling\/. For terms of service, see https:\/\/www.mapbox.com\/tos\/. \"\"\" def __init__(self, access_token, username, map_id): \"\"\" Set up a new instance to retrieve tiles from a Mapbox style. Access to Mapbox web services requires an access token and a map ID. See https:\/\/www.mapbox.com\/api-documentation\/ for details. Parameters ---------- access_token A valid Mapbox API access token. username The username for the Mapbox user who defined the Mapbox style. map_id A map ID for a map defined by a Mapbox style. This is the map whose tiles will be retrieved through this process. Note that this style may be private and if your access token does not have permissions to view this style, then map tile retrieval will fail. \"\"\" self.access_token=access_token self.username=username self.map_id=map_id super(MapboxStyleTiles, self).__init__() def _image_url(self, tile): x, y, z=tile url=('https:\/\/api.mapbox.com\/styles\/v1\/' '{user}\/{mapid}\/tiles\/256\/{z}\/{x}\/{y}' '?access_token={token}'.format(z=z, y=y, x=x, user=self.username, mapid=self.map_id, token=self.access_token)) return url class QuadtreeTiles(GoogleWTS): \"\"\" Implement web tile retrieval using the Microsoft WTS quadkey coordinate system. A \"tile\" in this class refers to a quadkey such as \"1\", \"14\" or \"141\" where the length of the quatree is the zoom level in Google Tile terms. \"\"\" def _image_url(self, tile): url=('http:\/\/ecn.dynamic.t1.tiles.virtualearth.net\/comp\/' 'CompositionHandler\/{tile}?mkt=en-' 'gb&it=A,G,L&shading=hill&n=z'.format(tile=tile)) return url def tms_to_quadkey(self, tms, google=False): quadKey=\"\" x, y, z=tms if not google: y=(2 ** z -1) -y for i in range(z, 0, -1): digit=0 mask=1 <<(i -1) if(x & mask) !=0: digit +=1 if(y & mask) !=0: digit +=2 quadKey +=str(digit) return quadKey def quadkey_to_tms(self, quadkey, google=False): assert isinstance(quadkey, six.string_types), \\ 'quadkey must be a string' x=y=0 z=len(quadkey) for i in range(z, 0, -1): mask=1 <<(i -1) if quadkey[z -i]=='0': pass elif quadkey[z -i]=='1': x |=mask elif quadkey[z -i]=='2': y |=mask elif quadkey[z -i]=='3': x |=mask y |=mask else: raise ValueError('Invalid QuadKey digit ' 'sequence.' +str(quadkey)) if not google: y=(2 ** z -1) -y return(x, y, z) def subtiles(self, quadkey): for i in range(4): yield quadkey +str(i) def tileextent(self, quadkey): x_y_z=self.quadkey_to_tms(quadkey, google=True) return GoogleWTS.tileextent(self, x_y_z) def find_images(self, target_domain, target_z, start_tile=None): \"\"\" Find all the quadtrees at the given target zoom, in the given target domain. target_z must be a value >=1. \"\"\" if target_z==0: raise ValueError('The empty quadtree cannot be returned.') if start_tile is None: start_tiles=['0', '1', '2', '3'] else: start_tiles=[start_tile] for start_tile in start_tiles: start_tile=self.quadkey_to_tms(start_tile, google=True) for tile in GoogleWTS.find_images(self, target_domain, target_z, start_tile=start_tile): yield self.tms_to_quadkey(tile, google=True) class OrdnanceSurvey(GoogleWTS): \"\"\" Implement web tile retrieval from Ordnance Survey map data. To use this tile image source you will need to obtain an API key from Ordnance Survey. For more details on Ordnance Survey layer styles, see https:\/\/apidocs.os.uk\/docs\/map-styles. For the API framework agreement, see https:\/\/developer.ordnancesurvey.co.uk\/os-api-framework-agreement. \"\"\" def __init__(self, apikey, layer='Road', desired_tile_form='RGB'): \"\"\" Parameters ---------- apikey: required The authentication key provided by OS to query the maps API layer: optional The style of the Ordnance Survey map tiles. One of 'Outdoor', 'Road', 'Light', 'Night', 'Leisure'. Defaults to 'Road'. Details about the style of layer can be found at: -https:\/\/apidocs.os.uk\/docs\/layer-information -https:\/\/apidocs.os.uk\/docs\/map-styles desired_tile_form: optional Defaults to 'RGB'. \"\"\" super(OrdnanceSurvey, self).__init__( desired_tile_form=desired_tile_form) self.apikey=apikey if layer not in['Outdoor', 'Road', 'Light', 'Night', 'Leisure']: raise ValueError('Invalid layer{}'.format(layer)) self.layer=layer def _image_url(self, tile): x, y, z=tile url=('https:\/\/api2.ordnancesurvey.co.uk\/' 'mapping_api\/v1\/service\/wmts?' 'key={apikey}&height=256&width=256&tilematrixSet=EPSG%3A3857&' 'version=1.0.0&style=true&layer={layer}%203857&' 'SERVICE=WMTS&REQUEST=GetTile&format=image%2Fpng&' 'TileMatrix=EPSG%3A3857%3A{z}&TileRow={y}&TileCol={x}') return url.format(z=z, y=y, x=x, apikey=self.apikey, layer=self.layer) def _merge_tiles(tiles): \"\"\"Return a single image, merging the given images.\"\"\" if not tiles: raise ValueError('A non-empty list of tiles should ' 'be provided to merge.') xset=[set(x) for i, x, y, _ in tiles] yset=[set(y) for i, x, y, _ in tiles] xs=xset[0] xs.update(*xset[1:]) ys=yset[0] ys.update(*yset[1:]) xs=sorted(xs) ys=sorted(ys) other_len=tiles[0][0].shape[2:] img=np.zeros((len(ys), len(xs)) +other_len, dtype=np.uint8) -1 for tile_img, x, y, origin in tiles: y_first, y_last=y[0], y[-1] yi0, yi1=np.where((y_first==ys) |(y_last==ys))[0] if origin=='upper': yi0=tile_img.shape[0] -yi0 -1 yi1=tile_img.shape[0] -yi1 -1 start, stop, step=yi0, yi1, 1 if yi0 < yi1 else -1 if step==1 and stop==img.shape[0] -1: stop=None elif step==-1 and stop==0: stop=None else: stop +=step y_slice=slice(start, stop, step) xi0, xi1=np.where((x[0]==xs) |(x[-1]==xs))[0] start, stop, step=xi0, xi1, 1 if xi0 < xi1 else -1 if step==1 and stop==img.shape[1] -1: stop=None elif step==-1 and stop==0: stop=None else: stop +=step x_slice=slice(start, stop, step) img_slice=(y_slice, x_slice, Ellipsis) if origin=='lower': tile_img=tile_img[::-1,::] img[img_slice]=tile_img return img,[min(xs), max(xs), min(ys), max(ys)], 'lower' ","sourceWithComments":"# (C) British Crown Copyright 2011 - 2018, Met Office\n#\n# This file is part of cartopy.\n#\n# cartopy is free software: you can redistribute it and\/or modify it under\n# the terms of the GNU Lesser General Public License as published by the\n# Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# cartopy is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Lesser General Public License for more details.\n#\n# You should have received a copy of the GNU Lesser General Public License\n# along with cartopy.  If not, see <https:\/\/www.gnu.org\/licenses\/>.\n\n\"\"\"\nImplements image tile identification and fetching from various sources.\n\n\nThe Matplotlib interface can make use of tile objects (defined below) via the\n:meth:`cartopy.mpl.geoaxes.GeoAxes.add_image` method. For example, to add a\n:class:`MapQuest Open Aerial tileset <MapQuestOpenAerial>` to an existing axes\nat zoom level 2, do ``ax.add_image(MapQuestOpenAerial(), 2)``. An example of\nusing tiles in this way can be found at the\n:ref:`sphx_glr_gallery_eyja_volcano.py` example.\n\n\"\"\"\n\nfrom __future__ import (absolute_import, division, print_function)\n\nfrom abc import ABCMeta, abstractmethod\nimport concurrent.futures\nimport warnings\n\nfrom PIL import Image\nimport shapely.geometry as sgeom\nimport numpy as np\nimport six\n\nimport cartopy.crs as ccrs\n\n\nclass GoogleWTS(six.with_metaclass(ABCMeta, object)):\n    \"\"\"\n    Implement web tile retrieval using the Google WTS coordinate system.\n\n    A \"tile\" in this class refers to the coordinates (x, y, z).\n\n    \"\"\"\n    _MAX_THREADS = 24\n\n    def __init__(self, desired_tile_form='RGB'):\n        self.imgs = []\n        self.crs = ccrs.Mercator.GOOGLE\n        self.desired_tile_form = desired_tile_form\n\n    def image_for_domain(self, target_domain, target_z):\n        tiles = []\n\n        def fetch_tile(tile):\n            try:\n                img, extent, origin = self.get_image(tile)\n            except IOError:\n                # Some services 404 for tiles that aren't supposed to be\n                # there (e.g. out of range).\n                raise\n            img = np.array(img)\n            x = np.linspace(extent[0], extent[1], img.shape[1])\n            y = np.linspace(extent[2], extent[3], img.shape[0])\n            return img, x, y, origin\n\n        with concurrent.futures.ThreadPoolExecutor(\n                max_workers=self._MAX_THREADS) as executor:\n            futures = []\n            for tile in self.find_images(target_domain, target_z):\n                futures.append(executor.submit(fetch_tile, tile))\n            for future in concurrent.futures.as_completed(futures):\n                try:\n                    img, x, y, origin = future.result()\n                    tiles.append([img, x, y, origin])\n                except IOError:\n                    pass\n\n        img, extent, origin = _merge_tiles(tiles)\n        return img, extent, origin\n\n    def _find_images(self, target_domain, target_z, start_tile=(0, 0, 0)):\n        \"\"\"Target domain is a shapely polygon in native coordinates.\"\"\"\n\n        assert isinstance(target_z, int) and target_z >= 0, ('target_z must '\n                                                             'be an integer '\n                                                             '>=0.')\n\n        # Recursively drill down to the images at the target zoom.\n        x0, x1, y0, y1 = self._tileextent(start_tile)\n        domain = sgeom.box(x0, y0, x1, y1)\n        if domain.intersects(target_domain):\n            if start_tile[2] == target_z:\n                    yield start_tile\n            else:\n                for tile in self._subtiles(start_tile):\n                    for result in self._find_images(target_domain, target_z,\n                                                    start_tile=tile):\n                        yield result\n\n    find_images = _find_images\n\n    def subtiles(self, x_y_z):\n        x, y, z = x_y_z\n        # Google tile specific (i.e. up->down).\n        for xi in range(0, 2):\n            for yi in range(0, 2):\n                yield x * 2 + xi, y * 2 + yi, z + 1\n\n    _subtiles = subtiles\n\n    def tile_bbox(self, x, y, z, y0_at_north_pole=True):\n        \"\"\"\n        Return the ``(x0, x1), (y0, y1)`` bounding box for the given x, y, z\n        tile position.\n\n        Parameters\n        ----------\n        x\n            The x tile coordinate in the Google tile numbering system.\n        y\n            The y tile coordinate in the Google tile numbering system.\n        z\n            The z tile coordinate in the Google tile numbering system.\n\n        y0_at_north_pole: optional\n            Boolean representing whether the numbering of the y coordinate\n            starts at the north pole (as is the convention for Google tiles)\n            or not (in which case it will start at the south pole, as is the\n            convention for TMS). Defaults to True.\n\n\n        \"\"\"\n        n = 2 ** z\n        assert 0 <= x <= (n - 1), (\"Tile's x index is out of range. Upper \"\n                                   \"limit %s. Got %s\" % (n, x))\n        assert 0 <= y <= (n - 1), (\"Tile's y index is out of range. Upper \"\n                                   \"limit %s. Got %s\" % (n, y))\n\n        x0, x1 = self.crs.x_limits\n        y0, y1 = self.crs.y_limits\n\n        # Compute the box height and width in native coordinates\n        # for this zoom level.\n        box_h = (y1 - y0) \/ n\n        box_w = (x1 - x0) \/ n\n\n        # Compute the native x & y extents of the tile.\n        n_xs = x0 + (x + np.arange(0, 2, dtype=np.float64)) * box_w\n        n_ys = y0 + (y + np.arange(0, 2, dtype=np.float64)) * box_h\n\n        if y0_at_north_pole:\n            n_ys = -1 * n_ys[::-1]\n\n        return n_xs, n_ys\n\n    def tileextent(self, x_y_z):\n        \"\"\"Return extent tuple ``(x0,x1,y0,y1)`` in Mercator coordinates.\"\"\"\n        x, y, z = x_y_z\n        x_lim, y_lim = self.tile_bbox(x, y, z, y0_at_north_pole=True)\n        return tuple(x_lim) + tuple(y_lim)\n\n    _tileextent = tileextent\n\n    @abstractmethod\n    def _image_url(self, tile):\n        pass\n\n    def get_image(self, tile):\n        if six.PY3:\n            from urllib.request import urlopen\n        else:\n            from urllib2 import urlopen\n\n        url = self._image_url(tile)\n\n        fh = urlopen(url)\n        im_data = six.BytesIO(fh.read())\n        fh.close()\n        img = Image.open(im_data)\n\n        img = img.convert(self.desired_tile_form)\n\n        return img, self.tileextent(tile), 'lower'\n\n\nclass GoogleTiles(GoogleWTS):\n    def __init__(self, desired_tile_form='RGB', style=\"street\",\n                 url=('https:\/\/mts0.google.com\/vt\/lyrs={style}'\n                      '@177000000&hl=en&src=api&x={x}&y={y}&z={z}&s=G')):\n        \"\"\"\n        Parameters\n        ----------\n        desired_tile_form: optional\n            Defaults to 'RGB'.\n        style: optional\n            The style for the Google Maps tiles.  One of 'street',\n            'satellite', 'terrain', and 'only_streets'.  Defaults to 'street'.\n        url: optional\n            URL pointing to a tile source and containing {x}, {y}, and {z}.\n            Such as: ``'https:\/\/server.arcgisonline.com\/ArcGIS\/rest\/services\/\\\nWorld_Shaded_Relief\/MapServer\/tile\/{z}\/{y}\/{x}.jpg'``\n\n        \"\"\"\n        styles = [\"street\", \"satellite\", \"terrain\", \"only_streets\"]\n        style = style.lower()\n        self.url = url\n        if style not in styles:\n            msg = \"Invalid style '%s'. Valid styles: %s\" % \\\n                (style, \", \".join(styles))\n            raise ValueError(msg)\n        self.style = style\n\n        # The 'satellite' and 'terrain' styles require pillow with a jpeg\n        # decoder.\n        if self.style in [\"satellite\", \"terrain\"] and \\\n                not hasattr(Image.core, \"jpeg_decoder\") or \\\n                not Image.core.jpeg_decoder:\n            msg = \"The '%s' style requires pillow with jpeg decoding support.\"\n            raise ValueError(msg % self.style)\n        return super(GoogleTiles, self).__init__(\n            desired_tile_form=desired_tile_form)\n\n    def _image_url(self, tile):\n        style_dict = {\n            \"street\": \"m\",\n            \"satellite\": \"s\",\n            \"terrain\": \"t\",\n            \"only_streets\": \"h\"}\n        url = self.url.format(\n            style=style_dict[self.style],\n            x=tile[0], X=tile[0],\n            y=tile[1], Y=tile[1],\n            z=tile[2], Z=tile[2])\n        return url\n\n\nclass MapQuestOSM(GoogleWTS):\n    # http:\/\/developer.mapquest.com\/web\/products\/open\/map for terms of use\n    # http:\/\/devblog.mapquest.com\/2016\/06\/15\/\n    # modernization-of-mapquest-results-in-changes-to-open-tile-access\/\n    # this now requires a sign up to a plan\n    def _image_url(self, tile):\n        x, y, z = tile\n        url = 'http:\/\/otile1.mqcdn.com\/tiles\/1.0.0\/osm\/%s\/%s\/%s.jpg' % (\n            z, x, y)\n        mqdevurl = ('http:\/\/devblog.mapquest.com\/2016\/06\/15\/'\n                    'modernization-of-mapquest-results-in-changes'\n                    '-to-open-tile-access\/')\n        warnings.warn('{} will require a log in and and will likely'\n                      ' fail. see {} for more details.'.format(url, mqdevurl))\n        return url\n\n\nclass MapQuestOpenAerial(GoogleWTS):\n    # http:\/\/developer.mapquest.com\/web\/products\/open\/map for terms of use\n    # The following attribution should be included in the resulting image:\n    # \"Portions Courtesy NASA\/JPL-Caltech and U.S. Depart. of Agriculture,\n    #  Farm Service Agency\"\n    def _image_url(self, tile):\n        x, y, z = tile\n        url = 'http:\/\/oatile1.mqcdn.com\/tiles\/1.0.0\/sat\/%s\/%s\/%s.jpg' % (\n            z, x, y)\n        return url\n\n\nclass OSM(GoogleWTS):\n    # http:\/\/developer.mapquest.com\/web\/products\/open\/map for terms of use\n    def _image_url(self, tile):\n        x, y, z = tile\n        url = 'https:\/\/a.tile.openstreetmap.org\/%s\/%s\/%s.png' % (z, x, y)\n        return url\n\n\nclass Stamen(GoogleWTS):\n    \"\"\"\n    Retrieves tiles from maps.stamen.com. Styles include\n    ``terrain-background``, ``terrain``, ``toner`` and ``watercolor``.\n\n    For a full reference on the styles available please see\n    http:\/\/maps.stamen.com. Of particular note are the sub-styles\n    that are made available (e.g. ``terrain`` and ``terrain-background``).\n    To determine the name of the particular [sub-]style you want,\n    follow the link on http:\/\/maps.stamen.com to your desired style and\n    observe the style name in the URL. Your style name will be in the\n    form of: ``http:\/\/maps.stamen.com\/{STYLE_NAME}\/#9\/37\/-122``.\n\n    Except otherwise noted, the Stamen map tile sets are copyright Stamen\n    Design, under a Creative Commons Attribution (CC BY 3.0) license.\n\n    Please see the attribution notice at http:\/\/maps.stamen.com on how to\n    attribute this imagery.\n\n    \"\"\"\n    def __init__(self, style='toner', desired_tile_form='RGB'):\n        super(Stamen, self).__init__(desired_tile_form=desired_tile_form)\n        self.style = style\n\n    def _image_url(self, tile):\n        return ('http:\/\/tile.stamen.com\/{self.style}\/{z}\/{x}\/{y}.png'\n                .format(self=self, x=tile[0], y=tile[1], z=tile[2]))\n\n\nclass StamenTerrain(Stamen):\n    \"\"\"\n    **DEPRECATED:** This class is deprecated. Please use\n    ``Stamen('terrain-background')`` instead.\n\n    Terrain tiles defined for the continental United States, and include land\n    color and shaded hills. The land colors are a custom palette developed by\n    Gem Spear for the National Atlas 1km land cover data set, which defines\n    twenty-four land classifications including five kinds of forest,\n    combinations of shrubs, grasses and crops, and a few tundras and wetlands.\n    The colors are at their highest contrast when fully zoomed-out to the\n    whole U.S., and they slowly fade out to pale off-white as you zoom in to\n    leave room for foreground data and break up the weirdness of large areas\n    of flat, dark green.\n\n    References\n    ----------\n\n     * http:\/\/mike.teczno.com\/notes\/osm-us-terrain-layer\/background.html\n     * http:\/\/maps.stamen.com\/\n     * https:\/\/wiki.openstreetmap.org\/wiki\/List_of_OSM_based_Services\n     * https:\/\/github.com\/migurski\/DEM-Tools\n\n\n    \"\"\"\n    def __init__(self):\n        warnings.warn(\n            \"The StamenTerrain class was deprecated in v0.17. \"\n            \"Please use Stamen('terrain-background') instead.\")\n\n        # NOTE: This subclass of Stamen exists for legacy reasons.\n        # No further Stamen subclasses will be accepted as\n        # they can easily be created in user code with Stamen(style_name).\n        return super(StamenTerrain, self).__init__(style='terrain-background')\n\n\nclass MapboxTiles(GoogleWTS):\n    \"\"\"\n    Implement web tile retrieval from Mapbox.\n\n    For terms of service, see https:\/\/www.mapbox.com\/tos\/.\n\n    \"\"\"\n    def __init__(self, access_token, map_id):\n        \"\"\"\n        Set up a new Mapbox tiles instance.\n\n        Access to Mapbox web services requires an access token and a map ID.\n        See https:\/\/www.mapbox.com\/api-documentation\/ for details.\n\n        Parameters\n        ----------\n        access_token\n            A valid Mapbox API access token.\n        map_id\n            An ID for a publicly accessible map (provided by Mapbox).\n            This is the map whose tiles will be retrieved through this process.\n\n        \"\"\"\n        self.access_token = access_token\n        self.map_id = map_id\n        super(MapboxTiles, self).__init__()\n\n    def _image_url(self, tile):\n        x, y, z = tile\n        url = ('https:\/\/api.mapbox.com\/v4\/mapbox.{id}\/{z}\/{x}\/{y}.png'\n               '?access_token={token}'.format(z=z, y=y, x=x,\n                                              id=self.map_id,\n                                              token=self.access_token))\n        return url\n\n\nclass MapboxStyleTiles(GoogleWTS):\n    \"\"\"\n    Implement web tile retrieval from a user-defined Mapbox style. For more\n    details on Mapbox styles, see\n    https:\/\/www.mapbox.com\/studio-manual\/overview\/map-styling\/.\n\n    For terms of service, see https:\/\/www.mapbox.com\/tos\/.\n\n    \"\"\"\n    def __init__(self, access_token, username, map_id):\n        \"\"\"\n        Set up a new instance to retrieve tiles from a Mapbox style.\n\n        Access to Mapbox web services requires an access token and a map ID.\n        See https:\/\/www.mapbox.com\/api-documentation\/ for details.\n\n        Parameters\n        ----------\n        access_token\n            A valid Mapbox API access token.\n        username\n            The username for the Mapbox user who defined the Mapbox style.\n        map_id\n            A map ID for a map defined by a Mapbox style. This is the map whose\n            tiles will be retrieved through this process. Note that this style\n            may be private and if your access token does not have permissions\n            to view this style, then map tile retrieval will fail.\n\n        \"\"\"\n        self.access_token = access_token\n        self.username = username\n        self.map_id = map_id\n        super(MapboxStyleTiles, self).__init__()\n\n    def _image_url(self, tile):\n        x, y, z = tile\n        url = ('https:\/\/api.mapbox.com\/styles\/v1\/'\n               '{user}\/{mapid}\/tiles\/256\/{z}\/{x}\/{y}'\n               '?access_token={token}'.format(z=z, y=y, x=x,\n                                              user=self.username,\n                                              mapid=self.map_id,\n                                              token=self.access_token))\n        return url\n\n\nclass QuadtreeTiles(GoogleWTS):\n    \"\"\"\n    Implement web tile retrieval using the Microsoft WTS quadkey coordinate\n    system.\n\n    A \"tile\" in this class refers to a quadkey such as \"1\", \"14\" or \"141\"\n    where the length of the quatree is the zoom level in Google Tile terms.\n\n    \"\"\"\n    def _image_url(self, tile):\n        url = ('http:\/\/ecn.dynamic.t1.tiles.virtualearth.net\/comp\/'\n               'CompositionHandler\/{tile}?mkt=en-'\n               'gb&it=A,G,L&shading=hill&n=z'.format(tile=tile))\n        return url\n\n    def tms_to_quadkey(self, tms, google=False):\n        quadKey = \"\"\n        x, y, z = tms\n        # this algorithm works with google tiles, rather than tms, so convert\n        # to those first.\n        if not google:\n            y = (2 ** z - 1) - y\n        for i in range(z, 0, -1):\n            digit = 0\n            mask = 1 << (i - 1)\n            if (x & mask) != 0:\n                digit += 1\n            if (y & mask) != 0:\n                digit += 2\n            quadKey += str(digit)\n        return quadKey\n\n    def quadkey_to_tms(self, quadkey, google=False):\n        # algorithm ported from\n        # https:\/\/msdn.microsoft.com\/en-us\/library\/bb259689.aspx\n        assert isinstance(quadkey, six.string_types), \\\n            'quadkey must be a string'\n\n        x = y = 0\n        z = len(quadkey)\n        for i in range(z, 0, -1):\n            mask = 1 << (i - 1)\n            if quadkey[z - i] == '0':\n                pass\n            elif quadkey[z - i] == '1':\n                x |= mask\n            elif quadkey[z - i] == '2':\n                y |= mask\n            elif quadkey[z - i] == '3':\n                x |= mask\n                y |= mask\n            else:\n                raise ValueError('Invalid QuadKey digit '\n                                 'sequence.' + str(quadkey))\n        # the algorithm works to google tiles, so convert to tms\n        if not google:\n            y = (2 ** z - 1) - y\n        return (x, y, z)\n\n    def subtiles(self, quadkey):\n        for i in range(4):\n            yield quadkey + str(i)\n\n    def tileextent(self, quadkey):\n        x_y_z = self.quadkey_to_tms(quadkey, google=True)\n        return GoogleWTS.tileextent(self, x_y_z)\n\n    def find_images(self, target_domain, target_z, start_tile=None):\n        \"\"\"\n        Find all the quadtrees at the given target zoom, in the given\n        target domain.\n\n        target_z must be a value >= 1.\n\n        \"\"\"\n        if target_z == 0:\n            raise ValueError('The empty quadtree cannot be returned.')\n\n        if start_tile is None:\n            start_tiles = ['0', '1', '2', '3']\n        else:\n            start_tiles = [start_tile]\n\n        for start_tile in start_tiles:\n            start_tile = self.quadkey_to_tms(start_tile, google=True)\n            for tile in GoogleWTS.find_images(self, target_domain, target_z,\n                                              start_tile=start_tile):\n                yield self.tms_to_quadkey(tile, google=True)\n\n\nclass OrdnanceSurvey(GoogleWTS):\n    \"\"\"\n    Implement web tile retrieval from Ordnance Survey map data.\n    To use this tile image source you will need to obtain an\n    API key from Ordnance Survey.\n\n    For more details on Ordnance Survey layer styles, see\n    https:\/\/apidocs.os.uk\/docs\/map-styles.\n\n    For the API framework agreement, see\n    https:\/\/developer.ordnancesurvey.co.uk\/os-api-framework-agreement.\n    \"\"\"\n    # API Documentation: https:\/\/apidocs.os.uk\/docs\/os-maps-wmts\n    def __init__(self, apikey, layer='Road', desired_tile_form='RGB'):\n        \"\"\"\n        Parameters\n        ----------\n        apikey: required\n            The authentication key provided by OS to query the maps API\n        layer: optional\n            The style of the Ordnance Survey map tiles. One of 'Outdoor',\n            'Road', 'Light', 'Night', 'Leisure'. Defaults to 'Road'.\n            Details about the style of layer can be found at:\n             - https:\/\/apidocs.os.uk\/docs\/layer-information\n             - https:\/\/apidocs.os.uk\/docs\/map-styles\n        desired_tile_form: optional\n            Defaults to 'RGB'.\n        \"\"\"\n        super(OrdnanceSurvey, self).__init__(\n            desired_tile_form=desired_tile_form)\n        self.apikey = apikey\n\n        if layer not in ['Outdoor', 'Road', 'Light', 'Night', 'Leisure']:\n            raise ValueError('Invalid layer {}'.format(layer))\n\n        self.layer = layer\n\n    def _image_url(self, tile):\n        x, y, z = tile\n        url = ('https:\/\/api2.ordnancesurvey.co.uk\/'\n               'mapping_api\/v1\/service\/wmts?'\n               'key={apikey}&height=256&width=256&tilematrixSet=EPSG%3A3857&'\n               'version=1.0.0&style=true&layer={layer}%203857&'\n               'SERVICE=WMTS&REQUEST=GetTile&format=image%2Fpng&'\n               'TileMatrix=EPSG%3A3857%3A{z}&TileRow={y}&TileCol={x}')\n        return url.format(z=z, y=y, x=x,\n                          apikey=self.apikey,\n                          layer=self.layer)\n\n\ndef _merge_tiles(tiles):\n    \"\"\"Return a single image, merging the given images.\"\"\"\n    if not tiles:\n        raise ValueError('A non-empty list of tiles should '\n                         'be provided to merge.')\n    xset = [set(x) for i, x, y, _ in tiles]\n    yset = [set(y) for i, x, y, _ in tiles]\n\n    xs = xset[0]\n    xs.update(*xset[1:])\n    ys = yset[0]\n    ys.update(*yset[1:])\n    xs = sorted(xs)\n    ys = sorted(ys)\n\n    other_len = tiles[0][0].shape[2:]\n    img = np.zeros((len(ys), len(xs)) + other_len, dtype=np.uint8) - 1\n\n    for tile_img, x, y, origin in tiles:\n        y_first, y_last = y[0], y[-1]\n        yi0, yi1 = np.where((y_first == ys) | (y_last == ys))[0]\n        if origin == 'upper':\n            yi0 = tile_img.shape[0] - yi0 - 1\n            yi1 = tile_img.shape[0] - yi1 - 1\n        start, stop, step = yi0, yi1, 1 if yi0 < yi1 else -1\n        if step == 1 and stop == img.shape[0] - 1:\n            stop = None\n        elif step == -1 and stop == 0:\n            stop = None\n        else:\n            stop += step\n        y_slice = slice(start, stop, step)\n\n        xi0, xi1 = np.where((x[0] == xs) | (x[-1] == xs))[0]\n\n        start, stop, step = xi0, xi1, 1 if xi0 < xi1 else -1\n\n        if step == 1 and stop == img.shape[1] - 1:\n            stop = None\n        elif step == -1 and stop == 0:\n            stop = None\n        else:\n            stop += step\n\n        x_slice = slice(start, stop, step)\n\n        img_slice = (y_slice, x_slice, Ellipsis)\n\n        if origin == 'lower':\n            tile_img = tile_img[::-1, ::]\n\n        img[img_slice] = tile_img\n\n    return img, [min(xs), max(xs), min(ys), max(ys)], 'lower'\n"}},"msg":"io\/img_tiles: Update doc links and image retrieval to HTTPS, where possible\n\nThese change are largely just a matter of security best practice.\n\nFor any practical risk to occur, a user would need to...\n\n1. be operating on a hostile network with an attacker actively\n   attempting to tamper with requests made to hosts via HTTP,\n\n2. either...\n    a. visit http:\/\/ links in the source\/documentation and then\n       either perform a dangerous action on an attacker-controlled\n       page, or be running a browser version with an exploitable\n       vulnerability.\n\n    b. download maliciously modified tile resources and render images\n       with code having an exploitable security vulnerability.\n\nArguably, the preconditions for 2a and 2b are already significantly\nhigher-impact issues that would presumably be affecting significantly\nmore people than Cartopy community alone.\n\nImportant Notes:\n\n* Neither otile1.mqcdn.com nor devblog.mapquest.com appear to\n  exist anymore, so these changes are not particularly meaningful.\n\n* Links to ecn.dynamic.t1.tilesvirtualearth.net and tile.stamen.com have\n  been left as-is. Both of these hosts appear to be using TLS\n  certificates whose CN does not match their host names, resulting\n  in failures to access them via HTTPS."}}}