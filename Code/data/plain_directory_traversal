{"https:\/\/github.com\/RealJammy\/The-Jambot":{"76460cb37bc857eea9ed493879ebb7a5de6098fc":{"url":"https:\/\/api.github.com\/repos\/RealJammy\/The-Jambot\/commits\/76460cb37bc857eea9ed493879ebb7a5de6098fc","html_url":"https:\/\/github.com\/RealJammy\/The-Jambot\/commit\/76460cb37bc857eea9ed493879ebb7a5de6098fc","sha":"76460cb37bc857eea9ed493879ebb7a5de6098fc","keyword":"directory traversal vulnerable","diff":"diff --git a\/cogs\/maths.py b\/cogs\/maths.py\nindex d3df4d0..2a5a3bd 100644\n--- a\/cogs\/maths.py\n+++ b\/cogs\/maths.py\n@@ -16,7 +16,8 @@ def __init__(self,client):\n     self.client = client\n \n   @commands.command(brief = \"Generate Junior Mathematical Competition problem, usage: .jmc <year>\")\n-  async def jmc(self, ctx, year): # from years 2004 - 2018\n+  async def jmc(self, ctx, year: int): # from years 2004 - 2018\n+    year = str(year)\n     try:\n       allProblems = os.listdir(\"MATHS-IMAGES\/junior\/\" + year)\n       problem = random.choice(allProblems)\n@@ -25,7 +26,8 @@ async def jmc(self, ctx, year): # from years 2004 - 2018\n       await ctx.send(\"Junior Mathematical Competition problems only available from 2004 to 2018\")\n \n   @commands.command(brief = \"Generate Intermediate Mathematical Competition problem, usage: .imc <year>\")\n-  async def imc(self, ctx, year): # from years 2004 - 2018\n+  async def imc(self, ctx, year: int): # from years 2004 - 2018\n+    year = str(year)\n     try:\n       allProblems = os.listdir(\"MATHS-IMAGES\/intermediate\/\" + year)\n       problem = random.choice(allProblems)\n@@ -34,7 +36,8 @@ async def imc(self, ctx, year): # from years 2004 - 2018\n       await ctx.send(\"Intermediate Mathematical Competition problems only available from 2004 to 2018\")\n   \n   @commands.command(brief = \"Generate Senior Mathematical Competition problem, usage: .smc <year>\")\n-  async def smc(self, ctx, year): # from years 2005 - 2018\n+  async def smc(self, ctx, year: int): # from years 2005 - 2018\n+    year = str(year)\n     try:\n       allProblems = os.listdir(\"MATHS-IMAGES\/senior\/\" + year)\n       problem = random.choice(allProblems)\n@@ -60,4 +63,4 @@ async def latexify(self, ctx, expr: str):\n           await ctx.send(file=file)\n \n def setup(client):\n-  client.add_cog(Maths(client))\n\\ No newline at end of file\n+  client.add_cog(Maths(client))\n","message":"","files":{"\/cogs\/maths.py":{"changes":[{"diff":"\n           await ctx.send(file=file)\n \n def setup(client):\n-  client.add_cog(Maths(client))\n\\ No newline at end of file\n+  client.add_cog(Maths(client))\n","add":1,"remove":1,"filename":"\/cogs\/maths.py","badparts":["  client.add_cog(Maths(client))"],"goodparts":["  client.add_cog(Maths(client))"]}],"source":"\nimport discord from discord.ext import commands import os import random from io import BytesIO from urllib.parse import quote from aiohttp import ClientSession from sympy import latex from sympy.parsing.sympy_parser import parse_expr LATEX_URL=\"https:\/\/latex.codecogs.com\/png.download?%5Cdpi%7B150%7D%20%5Cbg_white%20%5Chuge%20\" class Maths(commands.Cog): def __init__(self,client): self.client=client @commands.command(brief=\"Generate Junior Mathematical Competition problem, usage:.jmc <year>\") async def jmc(self, ctx, year): try: allProblems=os.listdir(\"MATHS-IMAGES\/junior\/\" +year) problem=random.choice(allProblems) await ctx.send(file=discord.File(\"MATHS-IMAGES\/junior\/\" +year +\"\/\" +problem)) except: await ctx.send(\"Junior Mathematical Competition problems only available from 2004 to 2018\") @commands.command(brief=\"Generate Intermediate Mathematical Competition problem, usage:.imc <year>\") async def imc(self, ctx, year): try: allProblems=os.listdir(\"MATHS-IMAGES\/intermediate\/\" +year) problem=random.choice(allProblems) await ctx.send(file=discord.File(\"MATHS-IMAGES\/junior\/\" +year +\"\/\" +problem)) except: await ctx.send(\"Intermediate Mathematical Competition problems only available from 2004 to 2018\") @commands.command(brief=\"Generate Senior Mathematical Competition problem, usage:.smc <year>\") async def smc(self, ctx, year): try: allProblems=os.listdir(\"MATHS-IMAGES\/senior\/\" +year) problem=random.choice(allProblems) await ctx.send(file=discord.File(\"MATHS-IMAGES\/senior\/\" +year +\"\/\" +problem)) except: await ctx.send(\"Senior Mathematical Competition problems only available from 2005 to 2018\") @commands.command(brief=\"Latex\") async def latexify(self, ctx, expr: str): fixed_expr=expr.replace('^', '**') try: parsed=parse_expr(fixed_expr, evaluate=False) except SyntaxError: await ctx.send(\"Invalid expression!\") else: ltx=latex(parsed) urlsafe=quote(ltx) async with ClientSession() as session: async with session.get(LATEX_URL +urlsafe) as resp: bytes_img=await resp.read() file=discord.File(fp=BytesIO(bytes_img), filename=\"latex.png\") await ctx.send(file=file) def setup(client): client.add_cog(Maths(client)) ","sourceWithComments":"import discord\nfrom discord.ext import commands\nimport os\nimport random\nfrom io import BytesIO\nfrom urllib.parse import quote\nfrom aiohttp import ClientSession\nfrom sympy import latex\nfrom sympy.parsing.sympy_parser import parse_expr\n\nLATEX_URL = \"https:\/\/latex.codecogs.com\/png.download?%5Cdpi%7B150%7D%20%5Cbg_white%20%5Chuge%20\"\n\nclass Maths(commands.Cog):\n\n  def __init__(self,client):\n    self.client = client\n\n  @commands.command(brief = \"Generate Junior Mathematical Competition problem, usage: .jmc <year>\")\n  async def jmc(self, ctx, year): # from years 2004 - 2018\n    try:\n      allProblems = os.listdir(\"MATHS-IMAGES\/junior\/\" + year)\n      problem = random.choice(allProblems)\n      await ctx.send(file=discord.File(\"MATHS-IMAGES\/junior\/\" + year + \"\/\" + problem))\n    except:\n      await ctx.send(\"Junior Mathematical Competition problems only available from 2004 to 2018\")\n\n  @commands.command(brief = \"Generate Intermediate Mathematical Competition problem, usage: .imc <year>\")\n  async def imc(self, ctx, year): # from years 2004 - 2018\n    try:\n      allProblems = os.listdir(\"MATHS-IMAGES\/intermediate\/\" + year)\n      problem = random.choice(allProblems)\n      await ctx.send(file=discord.File(\"MATHS-IMAGES\/junior\/\" + year + \"\/\" + problem))\n    except:\n      await ctx.send(\"Intermediate Mathematical Competition problems only available from 2004 to 2018\")\n  \n  @commands.command(brief = \"Generate Senior Mathematical Competition problem, usage: .smc <year>\")\n  async def smc(self, ctx, year): # from years 2005 - 2018\n    try:\n      allProblems = os.listdir(\"MATHS-IMAGES\/senior\/\" + year)\n      problem = random.choice(allProblems)\n      await ctx.send(file=discord.File(\"MATHS-IMAGES\/senior\/\" + year + \"\/\" + problem))\n    except:\n      await ctx.send(\"Senior Mathematical Competition problems only available from 2005 to 2018\")\n\n  @commands.command(brief = \"Latex\")\n  async def latexify(self, ctx, expr: str):\n      fixed_expr = expr.replace('^', '**')\n      try:\n          parsed = parse_expr(fixed_expr, evaluate=False)\n      except SyntaxError:\n          await ctx.send(\"Invalid expression!\")\n      else:\n          ltx = latex(parsed)\n          urlsafe = quote(ltx)\n          async with ClientSession() as session:\n              async with session.get(LATEX_URL + urlsafe) as resp:\n                  bytes_img = await resp.read()\n\n          file = discord.File(fp=BytesIO(bytes_img), filename=\"latex.png\")\n          await ctx.send(file=file)\n\ndef setup(client):\n  client.add_cog(Maths(client))"}},"msg":"Fixed directory traversal vulnerability\n\nIn jmc, smc and imc, sending a year name of \"..\/..\" for example allowed traversal. Fixed by requiring the year to be an integer."}},"https:\/\/github.com\/Linbreux\/wikmd":{"8c3f1fd38c06304c914b1fcb0b8d49c5a3e01566":{"url":"https:\/\/api.github.com\/repos\/Linbreux\/wikmd\/commits\/8c3f1fd38c06304c914b1fcb0b8d49c5a3e01566","html_url":"https:\/\/github.com\/Linbreux\/wikmd\/commit\/8c3f1fd38c06304c914b1fcb0b8d49c5a3e01566","sha":"8c3f1fd38c06304c914b1fcb0b8d49c5a3e01566","keyword":"directory traversal vulnerable","diff":"diff --git a\/image_manager.py b\/image_manager.py\nindex 57cbbd2..80196a0 100644\n--- a\/image_manager.py\n+++ b\/image_manager.py\n@@ -5,6 +5,7 @@\n from base64 import b32encode\n from hashlib import sha1\n \n+from flask import safe_join\n from werkzeug.utils import secure_filename\n \n \n@@ -91,7 +92,7 @@ def cleanup_images(self):\n             self.delete_image(not_used_image)\n \n     def delete_image(self, image_name):\n-        image_path = os.path.join(self.images_path, image_name)\n+        image_path = safe_join(self.images_path, image_name)\n         self.logger.info(f\"Deleting file >>> {image_path}\")\n         try:\n             os.remove(image_path)\ndiff --git a\/wiki.py b\/wiki.py\nindex e3e1f83..f18b960 100644\n--- a\/wiki.py\n+++ b\/wiki.py\n@@ -57,7 +57,7 @@ def save(page_name):\n     app.logger.info(f\"Saving >>> '{page_name}' ...\")\n \n     try:\n-        filename = os.path.join(cfg.wiki_directory, page_name + '.md')\n+        filename = safe_join(cfg.wiki_directory, f\"{page_name}.md\")\n         dirname = os.path.dirname(filename)\n         if not os.path.exists(dirname):\n             os.makedirs(dirname)\n@@ -102,9 +102,8 @@ def list_full_wiki():\n @app.route('\/list\/<path:folderpath>\/', methods=['GET'])\n def list_wiki(folderpath):\n     folder_list = []\n-    safe_folder = os.path.realpath(cfg.wiki_directory)\n-    requested_path = os.path.join(cfg.wiki_directory,folderpath) \n-    if os.path.commonprefix((os.path.realpath(requested_path),safe_folder)) != safe_folder: \n+    requested_path = safe_join(cfg.wiki_directory, folderpath)\n+    if requested_path is None:\n         app.logger.info(\"Requesting unsafe path >> showing homepage\")\n         return index()\n     app.logger.info(\"Showing >>> 'all files'\")\n@@ -159,7 +158,7 @@ def file_page(file_page):\n         if \"favicon\" in file_page:  # if the GET request is not for the favicon\n             return\n \n-        md_file_path = os.path.join(cfg.wiki_directory, file_page + \".md\")\n+        md_file_path = safe_join(cfg.wiki_directory, f\"{file_page}.md\")\n         mod = \"Last modified: %s\" % time.ctime(os.path.getmtime(md_file_path))\n         folder = file_page.split(\"\/\")\n         file_page = folder[-1:][0]\n@@ -261,7 +260,7 @@ def remove(page):\n     if bool(cfg.protect_edit_by_password) and (request.cookies.get('session_wikmd') not in SESSIONS):\n         return redirect(url_for(\"file_page\", file_page=page))\n \n-    filename = os.path.join(cfg.wiki_directory, page + '.md')\n+    filename = safe_join(cfg.wiki_directory, f\"{page}.md\")\n     os.remove(filename)\n     git_sync_thread = Thread(target=wrm.git_sync, args=(page, \"Remove\"))\n     git_sync_thread.start()\n@@ -273,7 +272,7 @@ def edit(page):\n     if bool(cfg.protect_edit_by_password) and (request.cookies.get('session_wikmd') not in SESSIONS):\n         return login(page)\n \n-    filename = os.path.join(cfg.wiki_directory, page + '.md')\n+    filename = safe_join(cfg.wiki_directory, f\"{page}.md\")\n     if request.method == 'POST':\n         page_name = fetch_page_name()\n         if page_name != page:\n","message":"","files":{"\/image_manager.py":{"changes":[{"diff":"\n             self.delete_image(not_used_image)\n \n     def delete_image(self, image_name):\n-        image_path = os.path.join(self.images_path, image_name)\n+        image_path = safe_join(self.images_path, image_name)\n         self.logger.info(f\"Deleting file >>> {image_path}\")\n         try:\n             os.remove(image_path)","add":1,"remove":1,"filename":"\/image_manager.py","badparts":["        image_path = os.path.join(self.images_path, image_name)"],"goodparts":["        image_path = safe_join(self.images_path, image_name)"]}],"source":"\nimport os import re import shutil import tempfile from base64 import b32encode from hashlib import sha1 from werkzeug.utils import secure_filename class ImageManager: \"\"\" Class that manages the images of the wiki. It can save, optimize and delete images. \"\"\" def __init__(self, app, cfg): self.logger=app.logger self.cfg=cfg self.images_path=os.path.join(self.cfg.wiki_directory, self.cfg.images_route) self.temp_dir=\"\/tmp\/wikmd\/images\" self.logger.info(\"Checking if webp is available for image optimization...\") self.can_optimize=os.system(\"cwebp -version\")==0 and os.system(\"gif2webp -version\")==0 if not self.can_optimize and self.cfg.optimize_images in[\"lossless\", \"lossy\"]: self.logger.error(\"To use image optimization webp and gif2webp need to be installed and in the $PATH. They could not be found.\") def save_images(self, file): \"\"\" Saves the image from the filepond upload. The image is renamed to the hash of the content, so the image is immutable. This makes it possible to cache it indefinitely on the client side. \"\"\" img_file=file[\"filepond\"] original_file_name, img_extension=os.path.splitext(img_file.filename) temp_file_handle, temp_file_path=tempfile.mkstemp() img_file.save(temp_file_path) if self.cfg.optimize_images in[\"lossless\", \"lossy\"] and self.can_optimize: temp_file_handle, temp_file_path, img_extension=self.__optimize_image(temp_file_path, img_file.content_type) hasher=sha1() with open(temp_file_handle, \"rb\") as f: data=f.read() hasher.update(data) img_digest=b32encode(hasher.digest()).decode(\"utf-8\").lower()[:-4] hash_file_name=secure_filename(f\"{original_file_name}-{img_digest}{img_extension}\") hash_file_path=os.path.join(self.images_path, hash_file_name) if os.path.exists(hash_file_path): self.logger.info(f\"Image already exists '{img_file.filename}' as '{hash_file_name}'\") else: self.logger.info(f\"Saving image >>> '{img_file.filename}' as '{hash_file_name}'...\") shutil.move(temp_file_path, hash_file_path) return hash_file_name def cleanup_images(self): \"\"\"Deletes images not used by any page\"\"\" saved_images=set(os.listdir(self.images_path)) saved_images.discard(\".gitignore\") image_link_pattern=fr\"\\[(.*?)\\]\\(({os.path.join('\/', self.cfg.images_route)}.+?)\\)\" image_link_regex=re.compile(image_link_pattern) used_images=set() for root, sub_dir, files in os.walk(self.cfg.wiki_directory): if os.path.join(self.cfg.wiki_directory, '.git') in root: continue if self.images_path in root: continue for filename in files: path=os.path.join(root, filename) with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f: content=f.read() matches=image_link_regex.findall(content) for _caption, image_path in matches: used_images.add(os.path.basename(image_path)) not_used_images=saved_images.difference(used_images) for not_used_image in not_used_images: self.delete_image(not_used_image) def delete_image(self, image_name): image_path=os.path.join(self.images_path, image_name) self.logger.info(f\"Deleting file >>>{image_path}\") try: os.remove(image_path) except IsADirectoryError | FileNotFoundError: self.logger.error(f\"Could not delete '{image_path}'\") def __optimize_image(self, temp_file_path_original, content_type): \"\"\" Optimizes gif, jpg and png by converting them to webp. gif and png files are always converted lossless. jpg files are either converted lossy or near lossless depending on cfg.optimize_images. Uses the external binaries cwebp and gif2webp. \"\"\" temp_file_handle, temp_file_path=tempfile.mkstemp() if content_type in[\"image\/gif\", \"image\/png\"]: self.logger.info(f\"Compressing image lossless...\") if content_type==\"image\/gif\": os.system(f\"gif2webp -quiet -m 6{temp_file_path_original} -o{temp_file_path}\") else: os.system(f\"cwebp -quiet -lossless -z 9{temp_file_path_original} -o{temp_file_path}\") os.remove(temp_file_path_original) elif content_type in[\"image\/jpeg\"]: if self.cfg.optimize_images==\"lossless\": self.logger.info(f\"Compressing image near lossless...\") os.system(f\"cwebp -quiet -near_lossless -m 6{temp_file_path_original} -o{temp_file_path}\") elif self.cfg.optimize_images==\"lossy\": self.logger.info(f\"Compressing image lossy...\") os.system(f\"cwebp -quiet -m 6{temp_file_path_original} -o{temp_file_path}\") os.remove(temp_file_path_original) return temp_file_handle, temp_file_path, \".webp\" ","sourceWithComments":"import os\nimport re\nimport shutil\nimport tempfile\nfrom base64 import b32encode\nfrom hashlib import sha1\n\nfrom werkzeug.utils import secure_filename\n\n\nclass ImageManager:\n    \"\"\"\n    Class that manages the images of the wiki.\n    It can save, optimize and delete images.\n    \"\"\"\n\n    def __init__(self, app, cfg):\n        self.logger = app.logger\n        self.cfg = cfg\n        self.images_path = os.path.join(self.cfg.wiki_directory, self.cfg.images_route)\n        self.temp_dir = \"\/tmp\/wikmd\/images\"\n        # Execute the needed programs to check if they are available. Exit code 0 means the programs were executed successfully\n        self.logger.info(\"Checking if webp is available for image optimization ...\")\n        self.can_optimize = os.system(\"cwebp -version\") == 0 and os.system(\"gif2webp -version\") == 0\n        if not self.can_optimize and self.cfg.optimize_images in [\"lossless\", \"lossy\"]:\n            self.logger.error(\"To use image optimization webp and gif2webp need to be installed and in the $PATH. They could not be found.\")\n\n    def save_images(self, file):\n        \"\"\"\n        Saves the image from the filepond upload.\n        The image is renamed to the hash of the content, so the image is immutable.\n        This makes it possible to cache it indefinitely on the client side.\n        \"\"\"\n        img_file = file[\"filepond\"]\n        original_file_name, img_extension = os.path.splitext(img_file.filename)\n\n        temp_file_handle, temp_file_path = tempfile.mkstemp()\n        img_file.save(temp_file_path)\n\n        if self.cfg.optimize_images in [\"lossless\", \"lossy\"] and self.can_optimize:\n            temp_file_handle, temp_file_path, img_extension = self.__optimize_image(temp_file_path, img_file.content_type)\n\n        # Does not matter if sha1 is secure or not. If someone has the right to edit they can already delete all pages.\n        hasher = sha1()\n        with open(temp_file_handle, \"rb\") as f:\n            data = f.read()\n            hasher.update(data)\n\n        # Using base32 instead of urlsafe base64, because the Windows file system is case-insensitive\n        img_digest = b32encode(hasher.digest()).decode(\"utf-8\").lower()[:-4]\n        hash_file_name = secure_filename(f\"{original_file_name}-{img_digest}{img_extension}\")\n        hash_file_path = os.path.join(self.images_path, hash_file_name)\n\n        # We can skip writing the file if it already exists. It is the same file, because it has the same hash\n        if os.path.exists(hash_file_path):\n            self.logger.info(f\"Image already exists '{img_file.filename}' as '{hash_file_name}'\")\n        else:\n            self.logger.info(f\"Saving image >>> '{img_file.filename}' as '{hash_file_name}' ...\")\n            shutil.move(temp_file_path, hash_file_path)\n\n        return hash_file_name\n\n    def cleanup_images(self):\n        \"\"\"Deletes images not used by any page\"\"\"\n        saved_images = set(os.listdir(self.images_path))\n        # Don't delete .gitignore\n        saved_images.discard(\".gitignore\")\n\n        # Matches [*](\/img\/*) it does not matter if images_route is \"\/img\" or \"img\"\n        image_link_pattern = fr\"\\[(.*?)\\]\\(({os.path.join('\/', self.cfg.images_route)}.+?)\\)\"\n        image_link_regex = re.compile(image_link_pattern)\n        used_images = set()\n        # Searching for Markdown files\n        for root, sub_dir, files in os.walk(self.cfg.wiki_directory):\n            if os.path.join(self.cfg.wiki_directory, '.git') in root:\n                # We don't want to search there\n                continue\n            if self.images_path in root:\n                # Nothing interesting there too\n                continue\n            for filename in files:\n                path = os.path.join(root, filename)\n                with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n                    content = f.read()\n                    matches = image_link_regex.findall(content)\n                    for _caption, image_path in matches:\n                        used_images.add(os.path.basename(image_path))\n\n        not_used_images = saved_images.difference(used_images)\n        for not_used_image in not_used_images:\n            self.delete_image(not_used_image)\n\n    def delete_image(self, image_name):\n        image_path = os.path.join(self.images_path, image_name)\n        self.logger.info(f\"Deleting file >>> {image_path}\")\n        try:\n            os.remove(image_path)\n        except IsADirectoryError | FileNotFoundError:\n            self.logger.error(f\"Could not delete '{image_path}'\")\n\n    def __optimize_image(self, temp_file_path_original, content_type):\n        \"\"\"\n        Optimizes gif, jpg and png by converting them to webp.\n        gif and png files are always converted lossless.\n        jpg files are either converted lossy or near lossless depending on cfg.optimize_images.\n\n        Uses the external binaries cwebp and gif2webp.\n        \"\"\"\n\n        temp_file_handle, temp_file_path = tempfile.mkstemp()\n        if content_type in [\"image\/gif\", \"image\/png\"]:\n            self.logger.info(f\"Compressing image lossless ...\")\n            if content_type == \"image\/gif\":\n                os.system(f\"gif2webp -quiet -m 6 {temp_file_path_original} -o {temp_file_path}\")\n            else:\n                os.system(f\"cwebp -quiet -lossless -z 9 {temp_file_path_original} -o {temp_file_path}\")\n            os.remove(temp_file_path_original)\n\n        elif content_type in [\"image\/jpeg\"]:\n            if self.cfg.optimize_images == \"lossless\":\n                self.logger.info(f\"Compressing image near lossless ...\")\n                os.system(f\"cwebp -quiet -near_lossless -m 6 {temp_file_path_original} -o {temp_file_path}\")\n            elif self.cfg.optimize_images == \"lossy\":\n                self.logger.info(f\"Compressing image lossy ...\")\n                os.system(f\"cwebp -quiet -m 6 {temp_file_path_original} -o {temp_file_path}\")\n            os.remove(temp_file_path_original)\n\n        return temp_file_handle, temp_file_path, \".webp\"\n"},"\/wiki.py":{"changes":[{"diff":"\n     app.logger.info(f\"Saving >>> '{page_name}' ...\")\n \n     try:\n-        filename = os.path.join(cfg.wiki_directory, page_name + '.md')\n+        filename = safe_join(cfg.wiki_directory, f\"{page_name}.md\")\n         dirname = os.path.dirname(filename)\n         if not os.path.exists(dirname):\n             os.makedirs(dirname)\n","add":1,"remove":1,"filename":"\/wiki.py","badparts":["        filename = os.path.join(cfg.wiki_directory, page_name + '.md')"],"goodparts":["        filename = safe_join(cfg.wiki_directory, f\"{page_name}.md\")"]},{"diff":"\n @app.route('\/list\/<path:folderpath>\/', methods=['GET'])\n def list_wiki(folderpath):\n     folder_list = []\n-    safe_folder = os.path.realpath(cfg.wiki_directory)\n-    requested_path = os.path.join(cfg.wiki_directory,folderpath) \n-    if os.path.commonprefix((os.path.realpath(requested_path),safe_folder)) != safe_folder: \n+    requested_path = safe_join(cfg.wiki_directory, folderpath)\n+    if requested_path is None:\n         app.logger.info(\"Requesting unsafe path >> showing homepage\")\n         return index()\n     app.logger.info(\"Showing >>> 'all files'\")\n","add":2,"remove":3,"filename":"\/wiki.py","badparts":["    safe_folder = os.path.realpath(cfg.wiki_directory)","    requested_path = os.path.join(cfg.wiki_directory,folderpath) ","    if os.path.commonprefix((os.path.realpath(requested_path),safe_folder)) != safe_folder: "],"goodparts":["    requested_path = safe_join(cfg.wiki_directory, folderpath)","    if requested_path is None:"]},{"diff":"\n         if \"favicon\" in file_page:  # if the GET request is not for the favicon\n             return\n \n-        md_file_path = os.path.join(cfg.wiki_directory, file_page + \".md\")\n+        md_file_path = safe_join(cfg.wiki_directory, f\"{file_page}.md\")\n         mod = \"Last modified: %s\" % time.ctime(os.path.getmtime(md_file_path))\n         folder = file_page.split(\"\/\")\n         file_page = folder[-1:][0]\n","add":1,"remove":1,"filename":"\/wiki.py","badparts":["        md_file_path = os.path.join(cfg.wiki_directory, file_page + \".md\")"],"goodparts":["        md_file_path = safe_join(cfg.wiki_directory, f\"{file_page}.md\")"]},{"diff":"\n     if bool(cfg.protect_edit_by_password) and (request.cookies.get('session_wikmd') not in SESSIONS):\n         return redirect(url_for(\"file_page\", file_page=page))\n \n-    filename = os.path.join(cfg.wiki_directory, page + '.md')\n+    filename = safe_join(cfg.wiki_directory, f\"{page}.md\")\n     os.remove(filename)\n     git_sync_thread = Thread(target=wrm.git_sync, args=(page, \"Remove\"))\n     git_sync_thread.start()\n","add":1,"remove":1,"filename":"\/wiki.py","badparts":["    filename = os.path.join(cfg.wiki_directory, page + '.md')"],"goodparts":["    filename = safe_join(cfg.wiki_directory, f\"{page}.md\")"]},{"diff":"\n     if bool(cfg.protect_edit_by_password) and (request.cookies.get('session_wikmd') not in SESSIONS):\n         return login(page)\n \n-    filename = os.path.join(cfg.wiki_directory, page + '.md')\n+    filename = safe_join(cfg.wiki_directory, f\"{page}.md\")\n     if request.method == 'POST':\n         page_name = fetch_page_name()\n         if page_name != page:\n","add":1,"remove":1,"filename":"\/wiki.py","badparts":["    filename = os.path.join(cfg.wiki_directory, page + '.md')"],"goodparts":["    filename = safe_join(cfg.wiki_directory, f\"{page}.md\")"]}],"source":"\nimport os from shutil import ExecError import shutil import platform import time import logging import uuid from lxml.html.clean import clean_html import pypandoc import knowledge_graph import secrets from flask import Flask, render_template, request, redirect, url_for, make_response, safe_join, send_file from threading import Thread from hashlib import sha256 from cache import Cache from image_manager import ImageManager from config import WikmdConfig from git_manager import WikiRepoManager from search import Search, Watchdog from web_dependencies import get_web_deps SESSIONS=[] cfg=WikmdConfig() UPLOAD_FOLDER=os.path.join(cfg.wiki_directory, cfg.images_route) app=Flask(__name__) app.config['UPLOAD_FOLDER']=UPLOAD_FOLDER app.logger.setLevel(logging.INFO) logger=logging.getLogger('werkzeug') logger.setLevel(logging.ERROR) wrm=WikiRepoManager(flask_app=app) SYSTEM_SETTINGS={ \"darktheme\": False, \"listsortMTime\": False, \"web_deps\": get_web_deps(cfg.local_mode, app.logger) } cache=Cache(cfg.cache_dir) im=ImageManager(app, cfg) def save(page_name): \"\"\" Function that saves a *.md page. :param page_name: name of the page \"\"\" content=request.form['CT'] app.logger.info(f\"Saving >>> '{page_name}'...\") try: filename=os.path.join(cfg.wiki_directory, page_name +'.md') dirname=os.path.dirname(filename) if not os.path.exists(dirname): os.makedirs(dirname) with open(filename, 'w') as f: f.write(content) except Exception as e: app.logger.error(f\"Error while saving '{page_name}' >>>{str(e)}\") def search(search_term: str, page: int): \"\"\" Function that searches for a term and shows the results. \"\"\" app.logger.info(f\"Searching >>> '{search_term}'...\") search=Search(cfg.search_dir) page=int(page) results, num_results, num_pages, suggestions=search.search(search_term, page) return render_template( 'search.html', search_term=search_term, num_results=num_results, num_pages=num_pages, current_page=page, suggestions=suggestions, results=results, system=SYSTEM_SETTINGS, ) def fetch_page_name() -> str: page_name=request.form['PN'] if page_name[-4:]==\"{id}\": page_name=f\"{page_name[:-4]}{uuid.uuid4().hex}\" return page_name @app.route('\/list\/', methods=['GET']) def list_full_wiki(): return list_wiki(\"\") @app.route('\/list\/<path:folderpath>\/', methods=['GET']) def list_wiki(folderpath): folder_list=[] safe_folder=os.path.realpath(cfg.wiki_directory) requested_path=os.path.join(cfg.wiki_directory,folderpath) if os.path.commonprefix((os.path.realpath(requested_path),safe_folder)) !=safe_folder: app.logger.info(\"Requesting unsafe path >> showing homepage\") return index() app.logger.info(\"Showing >>> 'all files'\") for root, subfolder, files in os.walk(requested_path): if root[-1]=='\/': root=root[:-1] for item in files: path=os.path.join(root, item) mtime=os.path.getmtime(os.path.join(root, item)) if( root.startswith(os.path.join(cfg.wiki_directory, '.git')) or root.startswith(os.path.join(cfg.wiki_directory, cfg.images_route)) ): continue folder=root[len(cfg.wiki_directory +\"\/\"):] if folder==\"\": if item==cfg.homepage: continue url=os.path.splitext( root[len(cfg.wiki_directory +\"\/\"):] +\"\/\" +item)[0] else: url=\"\/\" +\\ os.path.splitext( root[len(cfg.wiki_directory +\"\/\"):] +\"\/\" +item)[0] info={'doc': item, 'url': url, 'folder': folder, 'folder_url': folder, 'mtime': mtime, } folder_list.append(info) if SYSTEM_SETTINGS['listsortMTime']: folder_list.sort(key=lambda x: x[\"mtime\"], reverse=True) else: folder_list.sort(key=lambda x:(str(x[\"url\"]).casefold())) return render_template('list_files.html', list=folder_list, folder=folderpath, system=SYSTEM_SETTINGS) @app.route('\/<path:file_page>', methods=['GET']) def file_page(file_page): if request.args.get(\"q\"): return search(request.args.get(\"q\"), request.args.get(\"page\", 1)) else: html=\"\" mod=\"\" folder=\"\" if \"favicon\" in file_page: return md_file_path=os.path.join(cfg.wiki_directory, file_page +\".md\") mod=\"Last modified: %s\" % time.ctime(os.path.getmtime(md_file_path)) folder=file_page.split(\"\/\") file_page=folder[-1:][0] folder=folder[:-1] folder=\"\/\".join(folder) cached_entry=cache.get(md_file_path) if cached_entry: app.logger.info(f\"Showing HTML page from cache >>> '{file_page}'\") return render_template( 'content.html', title=file_page, folder=folder, info=cached_entry, modif=mod, system=SYSTEM_SETTINGS ) try: app.logger.info(f\"Converting to HTML with pandoc >>> '{md_file_path}'...\") html=pypandoc.convert_file(md_file_path, \"html5\", format='md', extra_args=[\"--mathjax\"], filters=['pandoc-xnos']) html=clean_html(html) cache.set(md_file_path, html) app.logger.info(f\"Showing HTML page >>> '{file_page}'\") except Exception as a: app.logger.info(a) return render_template('content.html', title=file_page, folder=folder, info=html, modif=mod, system=SYSTEM_SETTINGS) @app.route('\/', methods=['GET']) def index(): if request.args.get(\"q\"): return search(request.args.get(\"q\"), request.args.get(\"page\", 1)) else: html=\"\" app.logger.info(\"Showing HTML page >>> 'homepage'\") md_file_path=os.path.join(cfg.wiki_directory, cfg.homepage) cached_entry=cache.get(md_file_path) if cached_entry: app.logger.info(\"Showing HTML page from cache >>> 'homepage'\") return render_template( 'index.html', homepage=cached_entry, system=SYSTEM_SETTINGS ) try: app.logger.info(\"Converting to HTML with pandoc >>> 'homepage'...\") html=pypandoc.convert_file( md_file_path, \"html5\", format='md', extra_args=[\"--mathjax\"], filters=['pandoc-xnos']) html=clean_html(html) cache.set(md_file_path, html) except Exception as e: app.logger.error(f\"Conversion to HTML failed >>>{str(e)}\") return render_template('index.html', homepage=html, system=SYSTEM_SETTINGS) @app.route('\/add_new', methods=['POST', 'GET']) def add_new(): if bool(cfg.protect_edit_by_password) and(request.cookies.get('session_wikmd') not in SESSIONS): return login(\"\/add_new\") if request.method=='POST': page_name=fetch_page_name() save(page_name) git_sync_thread=Thread(target=wrm.git_sync, args=(page_name, \"Add\")) git_sync_thread.start() return redirect(url_for(\"file_page\", file_page=page_name)) else: return render_template('new.html', upload_path=cfg.images_route, image_allowed_mime=cfg.image_allowed_mime, system=SYSTEM_SETTINGS) @app.route('\/edit\/homepage', methods=['POST', 'GET']) def edit_homepage(): if bool(cfg.protect_edit_by_password) and(request.cookies.get('session_wikmd') not in SESSIONS): return login(\"\/edit\/homepage\") if request.method=='POST': page_name=fetch_page_name() save(page_name) git_sync_thread=Thread(target=wrm.git_sync, args=(page_name, \"Edit\")) git_sync_thread.start() return redirect(url_for(\"file_page\", file_page=page_name)) else: with open(os.path.join(cfg.wiki_directory, cfg.homepage), 'r', encoding=\"utf-8\", errors='ignore') as f: content=f.read() return render_template(\"new.html\", content=content, title=cfg.homepage_title, upload_path=cfg.images_route, image_allowed_mime=cfg.image_allowed_mime, system=SYSTEM_SETTINGS) @app.route('\/remove\/<path:page>', methods=['GET']) def remove(page): if bool(cfg.protect_edit_by_password) and(request.cookies.get('session_wikmd') not in SESSIONS): return redirect(url_for(\"file_page\", file_page=page)) filename=os.path.join(cfg.wiki_directory, page +'.md') os.remove(filename) git_sync_thread=Thread(target=wrm.git_sync, args=(page, \"Remove\")) git_sync_thread.start() return redirect(\"\/\") @app.route('\/edit\/<path:page>', methods=['POST', 'GET']) def edit(page): if bool(cfg.protect_edit_by_password) and(request.cookies.get('session_wikmd') not in SESSIONS): return login(page) filename=os.path.join(cfg.wiki_directory, page +'.md') if request.method=='POST': page_name=fetch_page_name() if page_name !=page: os.remove(filename) save(page_name) git_sync_thread=Thread(target=wrm.git_sync, args=(page_name, \"Edit\")) git_sync_thread.start() return redirect(url_for(\"file_page\", file_page=page_name)) else: with open(filename, 'r', encoding=\"utf-8\", errors='ignore') as f: content=f.read() return render_template(\"new.html\", content=content, title=page, upload_path=cfg.images_route, image_allowed_mime=cfg.image_allowed_mime, system=SYSTEM_SETTINGS) @app.route(os.path.join(\"\/\", cfg.images_route), methods=['POST', 'DELETE']) def upload_file(): if bool(cfg.protect_edit_by_password) and(request.cookies.get('session_wikmd') not in SESSIONS): return login() app.logger.info(f\"Uploading new image...\") if request.method==\"POST\": return im.save_images(request.files) if request.method==\"DELETE\": file_name=request.data.decode(\"utf-8\") im.delete_image(file_name) return 'OK' @app.route('\/knowledge-graph', methods=['GET']) def graph(): global links links=knowledge_graph.find_links() return render_template(\"knowledge-graph.html\", links=links, system=SYSTEM_SETTINGS) @app.route('\/login', methods=['GET','POST']) def login(page): if request.method==\"POST\": password=request.form[\"password\"] sha_string=sha256(password.encode('utf-8')).hexdigest() if sha_string==cfg.password_in_sha_256.lower(): app.logger.info(\"User successfully logged in\") resp=make_response(redirect(page)) session=secrets.token_urlsafe(1024 \/\/ 8) resp.set_cookie(\"session_wikmd\",session) SESSIONS.append(session) return resp else: app.logger.info(\"Login failed!\") else: app.logger.info(\"Display login page\") return render_template(\"login.html\", system=SYSTEM_SETTINGS) @app.route('\/nav\/<path:id>\/', methods=['GET']) def nav_id_to_page(id): for i in links: if i[\"id\"]==int(id): return redirect(\"\/\"+i[\"path\"]) return redirect(\"\/\") @app.route(os.path.join(\"\/\", cfg.images_route, \"<path:image_name>\")) def display_image(image_name): image_path=safe_join(UPLOAD_FOLDER, image_name) app.logger.info(f\"Showing image >>> '{image_path}'\") response=send_file(image_path) response.headers[\"Cache-Control\"]=\"max-age=31536000, immutable\" return response @app.route('\/toggle-darktheme\/', methods=['GET']) def toggle_darktheme(): SYSTEM_SETTINGS['darktheme']=not SYSTEM_SETTINGS['darktheme'] return redirect(request.args.get(\"return\", \"\/\")) @app.route('\/toggle-sorting\/', methods=['GET']) def toggle_sort(): SYSTEM_SETTINGS['listsortMTime']=not SYSTEM_SETTINGS['listsortMTime'] return redirect(\"\/list\") def setup_search(): search=Search(cfg.search_dir, create=True) app.logger.info(\"Search index creation...\") items=[] for root, subfolder, files in os.walk(cfg.wiki_directory): for item in files: if( root.startswith(os.path.join(cfg.wiki_directory, '.git')) or root.startswith(os.path.join(cfg.wiki_directory, cfg.images_route)) ): continue page_name, ext=os.path.splitext(item) if ext.lower() !=\".md\": continue path=os.path.relpath(root,cfg.wiki_directory) items.append((item, page_name, path)) search.index_all(cfg.wiki_directory, items) def run_wiki(): \"\"\" Function that runs the wiki as a Flask app. \"\"\" if int(cfg.wikmd_logging)==1: logging.basicConfig(filename=cfg.wikmd_logging_file, level=logging.INFO) if not os.path.exists(UPLOAD_FOLDER): app.logger.info(f\"Creating upload folder >>>{UPLOAD_FOLDER}\") os.mkdir(UPLOAD_FOLDER) im.cleanup_images() setup_search() app.logger.info(\"Spawning search indexer watchdog\") watchdog=Watchdog(cfg.wiki_directory, cfg.search_dir) watchdog.start() app.run(host=cfg.wikmd_host, port=cfg.wikmd_port, debug=True, use_reloader=False) if __name__=='__main__': run_wiki() ","sourceWithComments":"import os\nfrom shutil import ExecError\nimport shutil\nimport platform\nimport time\nimport logging\nimport uuid\nfrom lxml.html.clean import clean_html\nimport pypandoc\nimport knowledge_graph\nimport secrets\n\nfrom flask import Flask, render_template, request, redirect, url_for, make_response, safe_join, send_file\nfrom threading import Thread\nfrom hashlib import sha256\nfrom cache import Cache\nfrom image_manager import ImageManager\nfrom config import WikmdConfig\nfrom git_manager import WikiRepoManager\nfrom search import Search, Watchdog\nfrom web_dependencies import get_web_deps\n\n\nSESSIONS = []\n\ncfg = WikmdConfig()\nUPLOAD_FOLDER = os.path.join(cfg.wiki_directory, cfg.images_route)\n\napp = Flask(__name__)\napp.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER\n\n# console logger\napp.logger.setLevel(logging.INFO)\n\n# file logger\nlogger = logging.getLogger('werkzeug')\nlogger.setLevel(logging.ERROR)\n\nwrm = WikiRepoManager(flask_app=app)\n\nSYSTEM_SETTINGS = {\n    \"darktheme\": False,\n    \"listsortMTime\": False,\n    \"web_deps\": get_web_deps(cfg.local_mode, app.logger)\n}\n\ncache = Cache(cfg.cache_dir)\n\nim = ImageManager(app, cfg)\n\ndef save(page_name):\n    \"\"\"\n    Function that saves a *.md page.\n    :param page_name: name of the page\n    \"\"\"\n    content = request.form['CT']\n    app.logger.info(f\"Saving >>> '{page_name}' ...\")\n\n    try:\n        filename = os.path.join(cfg.wiki_directory, page_name + '.md')\n        dirname = os.path.dirname(filename)\n        if not os.path.exists(dirname):\n            os.makedirs(dirname)\n        with open(filename, 'w') as f:\n            f.write(content)\n    except Exception as e:\n        app.logger.error(f\"Error while saving '{page_name}' >>> {str(e)}\")\n\n\ndef search(search_term: str, page: int):\n    \"\"\"\n    Function that searches for a term and shows the results.\n    \"\"\"\n    app.logger.info(f\"Searching >>> '{search_term}' ...\")\n    search = Search(cfg.search_dir)\n    page = int(page)\n    results, num_results, num_pages, suggestions = search.search(search_term, page)\n    return render_template(\n        'search.html',\n        search_term=search_term,\n        num_results=num_results,\n        num_pages=num_pages,\n        current_page=page,\n        suggestions=suggestions,\n        results=results,\n        system=SYSTEM_SETTINGS,\n    )\n\n\ndef fetch_page_name() -> str:\n    page_name = request.form['PN']\n    if page_name[-4:] == \"{id}\":\n        page_name = f\"{page_name[:-4]}{uuid.uuid4().hex}\"\n    return page_name\n\n\n@app.route('\/list\/', methods=['GET'])\ndef list_full_wiki():\n    return list_wiki(\"\")\n\n\n@app.route('\/list\/<path:folderpath>\/', methods=['GET'])\ndef list_wiki(folderpath):\n    folder_list = []\n    safe_folder = os.path.realpath(cfg.wiki_directory)\n    requested_path = os.path.join(cfg.wiki_directory,folderpath) \n    if os.path.commonprefix((os.path.realpath(requested_path),safe_folder)) != safe_folder: \n        app.logger.info(\"Requesting unsafe path >> showing homepage\")\n        return index()\n    app.logger.info(\"Showing >>> 'all files'\")\n    for root, subfolder, files in os.walk(requested_path):\n        if root[-1] == '\/':\n            root = root[:-1]\n        for item in files:\n            path = os.path.join(root, item)\n            mtime = os.path.getmtime(os.path.join(root, item))\n            if (\n                root.startswith(os.path.join(cfg.wiki_directory, '.git')) or\n                root.startswith(os.path.join(cfg.wiki_directory, cfg.images_route))\n            ):\n                continue\n\n            folder = root[len(cfg.wiki_directory + \"\/\"):]\n            if folder == \"\":\n                if item == cfg.homepage:\n                    continue\n                url = os.path.splitext(\n                    root[len(cfg.wiki_directory + \"\/\"):] + \"\/\" + item)[0]\n            else:\n                url = \"\/\" + \\\n                    os.path.splitext(\n                        root[len(cfg.wiki_directory + \"\/\"):] + \"\/\" + item)[0]\n\n            info = {'doc': item,\n                    'url': url,\n                    'folder': folder,\n                    'folder_url': folder,\n                    'mtime': mtime,\n                    }\n            folder_list.append(info)\n\n    if SYSTEM_SETTINGS['listsortMTime']:\n        folder_list.sort(key=lambda x: x[\"mtime\"], reverse=True)\n    else:\n        folder_list.sort(key=lambda x: (str(x[\"url\"]).casefold()))\n\n    return render_template('list_files.html', list=folder_list, folder=folderpath, system=SYSTEM_SETTINGS)\n\n\n@app.route('\/<path:file_page>', methods=['GET'])\ndef file_page(file_page):\n    if request.args.get(\"q\"):\n        return search(request.args.get(\"q\"), request.args.get(\"page\", 1))\n    else:\n        html = \"\"\n        mod = \"\"\n        folder = \"\"\n\n        if \"favicon\" in file_page:  # if the GET request is not for the favicon\n            return\n\n        md_file_path = os.path.join(cfg.wiki_directory, file_page + \".md\")\n        mod = \"Last modified: %s\" % time.ctime(os.path.getmtime(md_file_path))\n        folder = file_page.split(\"\/\")\n        file_page = folder[-1:][0]\n        folder = folder[:-1]\n        folder = \"\/\".join(folder)\n\n        cached_entry = cache.get(md_file_path)\n        if cached_entry:\n            app.logger.info(f\"Showing HTML page from cache >>> '{file_page}'\")\n            return render_template(\n                'content.html', title=file_page, folder=folder, info=cached_entry, modif=mod,\n                system=SYSTEM_SETTINGS\n            )\n\n        try:\n            app.logger.info(f\"Converting to HTML with pandoc >>> '{md_file_path}' ...\")\n            html = pypandoc.convert_file(md_file_path, \"html5\",\n                                         format='md', extra_args=[\"--mathjax\"], filters=['pandoc-xnos'])\n            html = clean_html(html)\n            cache.set(md_file_path, html)\n\n            app.logger.info(f\"Showing HTML page >>> '{file_page}'\")\n        except Exception as a:\n            app.logger.info(a)\n\n        return render_template('content.html', title=file_page, folder=folder, info=html, modif=mod,\n                               system=SYSTEM_SETTINGS)\n\n\n@app.route('\/', methods=['GET'])\ndef index():\n    if request.args.get(\"q\"):\n        return search(request.args.get(\"q\"), request.args.get(\"page\", 1))\n    else:\n        html = \"\"\n        app.logger.info(\"Showing HTML page >>> 'homepage'\")\n\n        md_file_path = os.path.join(cfg.wiki_directory, cfg.homepage)\n        cached_entry = cache.get(md_file_path)\n        if cached_entry:\n            app.logger.info(\"Showing HTML page from cache >>> 'homepage'\")\n            return render_template(\n                'index.html', homepage=cached_entry, system=SYSTEM_SETTINGS\n            )\n\n        try:\n            app.logger.info(\"Converting to HTML with pandoc >>> 'homepage' ...\")\n            html = pypandoc.convert_file(\n                md_file_path, \"html5\", format='md', extra_args=[\"--mathjax\"],\n                filters=['pandoc-xnos'])\n            html = clean_html(html)\n            cache.set(md_file_path, html)\n\n        except Exception as e:\n            app.logger.error(f\"Conversion to HTML failed >>> {str(e)}\")\n\n        return render_template('index.html', homepage=html, system=SYSTEM_SETTINGS)\n\n\n@app.route('\/add_new', methods=['POST', 'GET'])\ndef add_new():\n    if bool(cfg.protect_edit_by_password) and (request.cookies.get('session_wikmd') not in SESSIONS):\n        return login(\"\/add_new\")\n    if request.method == 'POST':\n        page_name = fetch_page_name()\n        save(page_name)\n        git_sync_thread = Thread(target=wrm.git_sync, args=(page_name, \"Add\"))\n        git_sync_thread.start()\n\n        return redirect(url_for(\"file_page\", file_page=page_name))\n    else:\n        return render_template('new.html', upload_path=cfg.images_route,\n                               image_allowed_mime=cfg.image_allowed_mime, system=SYSTEM_SETTINGS)\n\n\n@app.route('\/edit\/homepage', methods=['POST', 'GET'])\ndef edit_homepage():\n    if bool(cfg.protect_edit_by_password) and (request.cookies.get('session_wikmd') not in SESSIONS):\n        return login(\"\/edit\/homepage\")\n\n    if request.method == 'POST':\n        page_name = fetch_page_name()\n        save(page_name)\n        git_sync_thread = Thread(target=wrm.git_sync, args=(page_name, \"Edit\"))\n        git_sync_thread.start()\n\n        return redirect(url_for(\"file_page\", file_page=page_name))\n    else:\n\n        with open(os.path.join(cfg.wiki_directory, cfg.homepage), 'r', encoding=\"utf-8\", errors='ignore') as f:\n\n            content = f.read()\n        return render_template(\"new.html\", content=content, title=cfg.homepage_title, upload_path=cfg.images_route,\n                               image_allowed_mime=cfg.image_allowed_mime, system=SYSTEM_SETTINGS)\n\n\n@app.route('\/remove\/<path:page>', methods=['GET'])\ndef remove(page):\n    if bool(cfg.protect_edit_by_password) and (request.cookies.get('session_wikmd') not in SESSIONS):\n        return redirect(url_for(\"file_page\", file_page=page))\n\n    filename = os.path.join(cfg.wiki_directory, page + '.md')\n    os.remove(filename)\n    git_sync_thread = Thread(target=wrm.git_sync, args=(page, \"Remove\"))\n    git_sync_thread.start()\n    return redirect(\"\/\")\n\n\n@app.route('\/edit\/<path:page>', methods=['POST', 'GET'])\ndef edit(page):\n    if bool(cfg.protect_edit_by_password) and (request.cookies.get('session_wikmd') not in SESSIONS):\n        return login(page)\n\n    filename = os.path.join(cfg.wiki_directory, page + '.md')\n    if request.method == 'POST':\n        page_name = fetch_page_name()\n        if page_name != page:\n            os.remove(filename)\n\n        save(page_name)\n        git_sync_thread = Thread(target=wrm.git_sync, args=(page_name, \"Edit\"))\n        git_sync_thread.start()\n\n        return redirect(url_for(\"file_page\", file_page=page_name))\n    else:\n        with open(filename, 'r', encoding=\"utf-8\", errors='ignore') as f:\n            content = f.read()\n        return render_template(\"new.html\", content=content, title=page, upload_path=cfg.images_route,\n                               image_allowed_mime=cfg.image_allowed_mime, system=SYSTEM_SETTINGS)\n\n\n@app.route(os.path.join(\"\/\", cfg.images_route), methods=['POST', 'DELETE'])\ndef upload_file():\n    if bool(cfg.protect_edit_by_password) and (request.cookies.get('session_wikmd') not in SESSIONS):\n        return login()\n    app.logger.info(f\"Uploading new image ...\")\n    # Upload image when POST\n    if request.method == \"POST\":\n        return im.save_images(request.files)\n\n    # DELETE when DELETE\n    if request.method == \"DELETE\":\n        # request data is in format \"b'nameoffile.png\" decode to utf-8\n        file_name = request.data.decode(\"utf-8\")\n        im.delete_image(file_name)\n        return 'OK'\n\n\n@app.route('\/knowledge-graph', methods=['GET'])\ndef graph():\n    global links\n    links = knowledge_graph.find_links()\n    return render_template(\"knowledge-graph.html\", links=links, system=SYSTEM_SETTINGS)\n\n\n@app.route('\/login', methods=['GET','POST'])\ndef login(page):\n    if request.method == \"POST\":\n        password = request.form[\"password\"]\n        sha_string = sha256(password.encode('utf-8')).hexdigest()\n        if sha_string == cfg.password_in_sha_256.lower():\n            app.logger.info(\"User successfully logged in\")\n            resp = make_response(redirect(page))\n            session = secrets.token_urlsafe(1024 \/\/ 8)\n            resp.set_cookie(\"session_wikmd\",session)\n            SESSIONS.append(session)\n            return resp\n        else:\n            app.logger.info(\"Login failed!\")\n    else:\n        app.logger.info(\"Display login page\")\n    return render_template(\"login.html\", system=SYSTEM_SETTINGS)\n\n# Translate id to page path\n\n\n@app.route('\/nav\/<path:id>\/', methods=['GET'])\ndef nav_id_to_page(id):\n    for i in links:\n        if i[\"id\"] == int(id):\n            return redirect(\"\/\"+i[\"path\"])\n    return redirect(\"\/\")\n\n\n@app.route(os.path.join(\"\/\", cfg.images_route, \"<path:image_name>\"))\ndef display_image(image_name):\n    image_path = safe_join(UPLOAD_FOLDER, image_name)\n    app.logger.info(f\"Showing image >>> '{image_path}'\")\n    response = send_file(image_path)\n    # cache indefinitely\n    response.headers[\"Cache-Control\"] = \"max-age=31536000, immutable\"\n    return response\n\n\n@app.route('\/toggle-darktheme\/', methods=['GET'])\ndef toggle_darktheme():\n    SYSTEM_SETTINGS['darktheme'] = not SYSTEM_SETTINGS['darktheme']\n    return redirect(request.args.get(\"return\", \"\/\"))  # redirect to the same page URL\n\n\n@app.route('\/toggle-sorting\/', methods=['GET'])\ndef toggle_sort():\n    SYSTEM_SETTINGS['listsortMTime'] = not SYSTEM_SETTINGS['listsortMTime']\n    return redirect(\"\/list\")\n\n\ndef setup_search():\n    search = Search(cfg.search_dir, create=True)\n\n    app.logger.info(\"Search index creation...\")\n    items = []\n    for root, subfolder, files in os.walk(cfg.wiki_directory):\n        for item in files:\n            if (\n                root.startswith(os.path.join(cfg.wiki_directory, '.git')) or\n                root.startswith(os.path.join(cfg.wiki_directory, cfg.images_route))\n            ):\n                continue\n            page_name, ext = os.path.splitext(item)\n            if ext.lower() != \".md\":\n                continue\n            path = os.path.relpath(root,cfg.wiki_directory)\n            items.append((item, page_name, path))\n\n    search.index_all(cfg.wiki_directory, items)\n\n\ndef run_wiki():\n    \"\"\"\n    Function that runs the wiki as a Flask app.\n    \"\"\"\n    if int(cfg.wikmd_logging) == 1:\n        logging.basicConfig(filename=cfg.wikmd_logging_file, level=logging.INFO)\n\n    if not os.path.exists(UPLOAD_FOLDER):\n        app.logger.info(f\"Creating upload folder >>> {UPLOAD_FOLDER}\")\n        os.mkdir(UPLOAD_FOLDER)\n\n    im.cleanup_images()\n    setup_search()\n    app.logger.info(\"Spawning search indexer watchdog\")\n    watchdog = Watchdog(cfg.wiki_directory, cfg.search_dir)\n    watchdog.start()\n    app.run(host=cfg.wikmd_host, port=cfg.wikmd_port, debug=True, use_reloader=False)\n\n\nif __name__ == '__main__':\n    run_wiki()\n"}},"msg":"fix the directory traversal vulnerabilities as described in #80"}},"https:\/\/github.com\/rhertzog\/distro-tracker":{"674d89ff943ec8d831ebdc1b0aee4724d1c7ca69":{"url":"https:\/\/api.github.com\/repos\/rhertzog\/distro-tracker\/commits\/674d89ff943ec8d831ebdc1b0aee4724d1c7ca69","html_url":"https:\/\/github.com\/rhertzog\/distro-tracker\/commit\/674d89ff943ec8d831ebdc1b0aee4724d1c7ca69","message":"Adding tarfile member sanitization to extractall()\n\nThis is known as CVE-2007-4559. In distro-tracker, we only extract\ntarfile from Debian source packages but still it's better to avoid\nthat directory traversal vulnerability.\n\n[hertzog@debian.org: simplified the original pull request received\nat https:\/\/github.com\/rhertzog\/distro-tracker\/pull\/1]","sha":"674d89ff943ec8d831ebdc1b0aee4724d1c7ca69","keyword":"directory traversal vulnerable","diff":"diff --git a\/distro_tracker\/core\/utils\/packages.py b\/distro_tracker\/core\/utils\/packages.py\nindex 45234349..90656aee 100644\n--- a\/distro_tracker\/core\/utils\/packages.py\n+++ b\/distro_tracker\/core\/utils\/packages.py\n@@ -495,7 +495,24 @@ def _extract_quilt_package_debian_tar(self, debian_tar_path, outdir):\n         Extracts the given tarball to the given output directory.\n         \"\"\"\n         with tarfile.open(debian_tar_path) as archive_file:\n-            archive_file.extractall(outdir)\n+\n+            def is_within_directory(directory, target):\n+                abs_directory = os.path.abspath(directory)\n+                abs_target = os.path.abspath(target)\n+\n+                prefix = os.path.commonprefix([abs_directory, abs_target])\n+\n+                return prefix == abs_directory\n+\n+            def safe_extract(tar, path=\".\"):\n+                for member in tar.getmembers():\n+                    member_path = os.path.join(path, member.name)\n+                    if not is_within_directory(path, member_path):\n+                        raise Exception(\"Attempted Path Traversal in Tar File\")\n+\n+                tar.extractall(path)\n+\n+            safe_extract(archive_file, outdir)\n \n     def get_package_source_cache_directory(self, package_name):\n         \"\"\"\n","files":{"\/distro_tracker\/core\/utils\/packages.py":{"changes":[{"diff":"\n         Extracts the given tarball to the given output directory.\n         \"\"\"\n         with tarfile.open(debian_tar_path) as archive_file:\n-            archive_file.extractall(outdir)\n+\n+            def is_within_directory(directory, target):\n+                abs_directory = os.path.abspath(directory)\n+                abs_target = os.path.abspath(target)\n+\n+                prefix = os.path.commonprefix([abs_directory, abs_target])\n+\n+                return prefix == abs_directory\n+\n+            def safe_extract(tar, path=\".\"):\n+                for member in tar.getmembers():\n+                    member_path = os.path.join(path, member.name)\n+                    if not is_within_directory(path, member_path):\n+                        raise Exception(\"Attempted Path Traversal in Tar File\")\n+\n+                tar.extractall(path)\n+\n+            safe_extract(archive_file, outdir)\n \n     def get_package_source_cache_directory(self, package_name):\n         \"\"\"\n","add":18,"remove":1,"filename":"\/distro_tracker\/core\/utils\/packages.py","badparts":["            archive_file.extractall(outdir)"],"goodparts":["            def is_within_directory(directory, target):","                abs_directory = os.path.abspath(directory)","                abs_target = os.path.abspath(target)","                prefix = os.path.commonprefix([abs_directory, abs_target])","                return prefix == abs_directory","            def safe_extract(tar, path=\".\"):","                for member in tar.getmembers():","                    member_path = os.path.join(path, member.name)","                    if not is_within_directory(path, member_path):","                        raise Exception(\"Attempted Path Traversal in Tar File\")","                tar.extractall(path)","            safe_extract(archive_file, outdir)"]}],"source":"\n \"\"\"Utilities for processing Debian package information.\"\"\" import os import re import shutil import subprocess import tarfile import apt import apt_pkg from debian import deb822 from django.conf import settings from django.urls import reverse from django.utils.encoding import force_bytes from distro_tracker.core.utils.email_messages import \\ name_and_address_from_string as parse_address from distro_tracker.core.utils.email_messages import \\ names_and_addresses_from_string as parse_addresses def package_hashdir(package_name): \"\"\" Returns the name of the hash directory used to avoid having too many entries in a single directory. It's usually the first letter of the package except for lib* packages where it's the first 4 letters. :param package_name: The package name. :type package_name: str :returns: Name of the hash directory. :rtype: str \"\"\" if package_name is None: return None if package_name.startswith('lib'): return package_name[0:4] else: return package_name[0:1] def package_url(package_name): \"\"\" Returns the URL of the page dedicated to this package name. :param package_name: The package name. :type package_name: str or PackageName model :returns: Name of the hash directory. :rtype: str \"\"\" if package_name is None: return None return reverse('dtracker-package-page', kwargs={'package_name': str(package_name)}) def extract_vcs_information(stanza): \"\"\" Extracts the VCS information from a package's Sources entry. :param stanza: The ``Sources`` entry from which to extract the VCS info. Maps ``Sources`` key names to values. :type stanza: dict :returns: VCS information regarding the package. Contains the following keys: type[, browser, url, branch] :rtype: dict \"\"\" vcs={} for key, value in stanza.items(): key=key.lower() if key=='vcs-browser': vcs['browser']=value elif key.startswith('vcs-'): vcs['type']=key[4:] vcs['url']=value if vcs['type']=='git': match=re.match(r'(?P<url>.*?)\\s+-b\\s*(?P<branch>\\S+)', value) if match: vcs['url']=match.group('url') vcs['branch']=match.group('branch') return vcs def extract_dsc_file_name(stanza): \"\"\" Extracts the name of the.dsc file from a package's Sources entry. :param stanza: The ``Sources`` entry from which to extract the VCS info. Maps ``Sources`` key names to values. :type stanza: dict \"\"\" for field in('checksums-sha256', 'checksums-sha1', 'files'): for entry in stanza.get(field,[]): if entry.get('name', '').endswith('.dsc'): return entry['name'] return None def extract_information_from_sources_entry(stanza): \"\"\" Extracts information from a ``Sources`` file entry and returns it in the form of a dictionary. :param stanza: The raw entry's key-value pairs. :type stanza: Case-insensitive dict \"\"\" binaries=[ binary.strip() for binary in stanza['binary'].split(',') ] entry={ 'version': stanza['version'], 'homepage': stanza.get('homepage', ''), 'priority': stanza.get('priority', ''), 'section': stanza.get('section', ''), 'architectures': stanza['architecture'].split(), 'binary_packages': binaries, 'maintainer': parse_address(stanza['maintainer']), 'uploaders': parse_addresses(stanza.get('uploaders', '')), 'standards_version': stanza.get('standards-version', ''), 'vcs': extract_vcs_information(stanza), 'dsc_file_name': extract_dsc_file_name(stanza), 'directory': stanza.get('directory', ''), } return entry def extract_information_from_packages_entry(stanza): \"\"\" Extracts information from a ``Packages`` file entry and returns it in the form of a dictionary. :param stanza: The raw entry's key-value pairs. :type stanza: Case-insensitive dict \"\"\" entry={ 'version': stanza['version'], 'short_description': stanza.get('description', '')[:300], } return entry class SourcePackageRetrieveError(Exception): pass class AptCache(object): \"\"\" A class for handling cached package information. \"\"\" DEFAULT_MAX_SIZE=1 * 1024 ** 3 QUILT_FORMAT='3.0(quilt)' class AcquireProgress(apt.progress.base.AcquireProgress): \"\"\" Instances of this class can be passed to:meth:`apt.cache.Cache.update` calls. It provides a way to track which files were changed and which were not by an update operation. \"\"\" def __init__(self, *args, **kwargs): super(AptCache.AcquireProgress, self).__init__(*args, **kwargs) self.fetched=[] self.hit=[] def done(self, item): self.fetched.append(os.path.split(item.owner.destfile)[1]) def ims_hit(self, item): self.hit.append(os.path.split(item.owner.destfile)[1]) def pulse(self, owner): return True def __init__(self): self.cache_root_dir=os.path.join( settings.DISTRO_TRACKER_CACHE_DIRECTORY, 'apt-cache' ) self.sources_list_path=os.path.join( self.cache_root_dir, 'etc', 'sources.list') self.conf_file_path=os.path.join(self.cache_root_dir, 'etc', 'apt.conf') os.environ['APT_CONFIG']=self.conf_file_path self.sources=[] self.packages=[] self.cache_max_size=getattr( settings, 'DISTRO_TRACKER_APT_CACHE_MAX_SIZE', self.DEFAULT_MAX_SIZE) self.source_cache_directory=os.path.join(self.cache_root_dir, 'packages') self._cache_size=None self.configure_cache() @property def cache_size(self): if self._cache_size is None: self._cache_size=\\ self.get_directory_size(self.source_cache_directory) return self._cache_size def get_directory_size(self, directory_path): \"\"\" Returns the total space taken by the given directory in bytes. :param directory_path: The path to the directory :type directory_path: string :rtype: int \"\"\" directory_path=force_bytes(directory_path) total_size=0 for dirpath, dirnames, filenames in os.walk(directory_path): for file_name in filenames: file_path=os.path.join(dirpath, file_name) stat=os.lstat(file_path) total_size +=stat.st_size return total_size def clear_cache(self): \"\"\" Removes all cache information. This causes the next update to retrieve fresh repository files. \"\"\" self._remove_dir(self.cache_root_dir) self.configure_cache() def update_sources_list(self): \"\"\" Updates the ``sources.list`` file used to list repositories for which package information should be cached. \"\"\" from distro_tracker.core.models import Repository directory=os.path.dirname(self.sources_list_path) if not os.path.exists(directory): os.makedirs(directory) with open(self.sources_list_path, 'w') as sources_list: for repository in Repository.objects.all(): sources_list.write(repository.sources_list_entry +'\\n') def update_apt_conf(self): \"\"\" Updates the ``apt.conf`` file which gives general settings for the :class:`apt.cache.Cache`. In particular, this updates the list of all architectures which should be considered in package updates based on architectures that the repositories support. \"\"\" from distro_tracker.core.models import Architecture with open(self.conf_file_path, 'w') as conf_file: conf_file.write('APT::Architectures{ ') for architecture in Architecture.objects.all(): conf_file.write('\"{arch}\"; '.format(arch=architecture)) conf_file.write('};\\n') conf_file.write('Acquire::CompressionTypes::Order:: \"xz\";\\n') conf_file.write('Dir \"{}\/\";\\n'.format(self.cache_root_dir)) conf_file.write('Dir::State \"state\/\";\\n') conf_file.write('Dir::State::status \"dpkg-status\";\\n') conf_file.write('Dir::Etc \"etc\/\";\\n') conf_file.write('Dir::Etc::sourcelist \"{src}\";\\n'.format( src=self.sources_list_path)) conf_file.write('Dir::Etc::Trusted \"{src}\";\\n'.format( src=settings.DISTRO_TRACKER_TRUSTED_GPG_MAIN_FILE)) conf_file.write('Dir::Etc::TrustedParts \"{src}\";\\n'.format( src=settings.DISTRO_TRACKER_TRUSTED_GPG_PARTS_DIR)) def configure_cache(self): \"\"\" Configures the cache based on the most current repository information. \"\"\" self.update_sources_list() self.update_apt_conf() for root_key in apt_pkg.config.list(): apt_pkg.config.clear(root_key) apt_pkg.init() for apt_dir in[apt_pkg.config.find_dir('Dir::State::lists'), apt_pkg.config.find_dir('Dir::Etc::sourceparts'), apt_pkg.config.find_dir('Dir::Cache::archives')]: if not os.path.exists(apt_dir): os.makedirs(apt_dir) def _index_file_full_path(self, file_name): \"\"\" Returns the absolute path for the given cached index file. :param file_name: The name of the cached index file. :type file_name: string :rtype: string \"\"\" return os.path.join( apt_pkg.config.find_dir('Dir::State::lists'), file_name ) def _match_index_file_to_repository(self, sources_file): \"\"\" Returns a two-tuple ``(class:`Repository <distro_tracker.core. models.Repository>`, component)``. The class:`Repository <distro_tracker.core.models.Repository>` instance which matches the given cached ``Sources`` file and the ``component`` of the ``Source``. :rtype:(:class:`Repository <distro_tracker.core.models.Repository>`, string) \"\"\" from distro_tracker.core.models import Repository sources_list=apt_pkg.SourceList() sources_list.read_main_list() component_url=None component=None for entry in sources_list.list: for index_file in entry.index_files: if os.path.basename(sources_file) in index_file.describe: base_url, component, _=index_file.describe.split(None, 2) base_url=base_url.rstrip('\/') component_url=base_url +'\/' +component break components=component.split('\/') if len(components) >=2: component=components[1].strip() for repository in Repository.objects.all(): if component_url in repository.component_urls: return repository, component def _get_all_cached_files(self): \"\"\" Returns a list of all cached files. \"\"\" lists_directory=apt_pkg.config.find_dir('Dir::State::lists') try: return[ os.path.join(lists_directory, file_name) for file_name in os.listdir(lists_directory) if os.path.isfile(os.path.join(lists_directory, file_name)) ] except OSError: return[] def get_cached_files(self, filter_function=None): \"\"\" Returns cached files, optionally filtered by the given ``filter_function`` :param filter_function: Takes a file name as the only parameter and returns a:class:`bool` indicating whether it should be included in the result. :type filter_function: callable :returns: A list of cached file names :rtype: list \"\"\" if filter_function is None: def filter_function(x): return True return[ file_name for file_name in self._get_all_cached_files() if filter_function(file_name) ] def get_sources_files_for_repository(self, repository): \"\"\" Returns all ``Sources`` files which are cached for the given repository. For instance, ``Sources`` files for different suites are cached separately. :param repository: The repository for which to return all cached ``Sources`` files :type repository::class:`Repository <distro_tracker.core.models.Repository>` :rtype: ``iterable`` of strings \"\"\" return self.get_cached_files( lambda file_name:( file_name.endswith('Sources') and self._match_index_file_to_repository( file_name)[0]==repository)) def get_packages_files_for_repository(self, repository): \"\"\" Returns all ``Packages`` files which are cached for the given repository. For instance, ``Packages`` files for different suites are cached separately. :param repository: The repository for which to return all cached ``Packages`` files :type repository::class:`Repository <distro_tracker.core.models.Repository>` :rtype: ``iterable`` of strings \"\"\" return self.get_cached_files( lambda file_name:( file_name.endswith('Packages') and self._match_index_file_to_repository( file_name)[0]==repository)) def update_repositories(self, force_download=False): \"\"\" Initiates a cache update. :param force_download: If set to ``True`` causes the cache to be cleared before starting the update, thus making sure all index files are downloaded again. :returns: A two-tuple ``(updated_sources, updated_packages)``. Each of the tuple's members is a list of (:class:`Repository <distro_tracker.core.models.Repository>`, ``component``, ``file_name``) tuple representing the repository which was updated, component, and the file which contains the fresh information. The file is either a ``Sources`` or a ``Packages`` file respectively. \"\"\" if force_download: self.clear_cache() self.configure_cache() cache=apt.Cache(rootdir=self.cache_root_dir) progress=AptCache.AcquireProgress() cache.update(progress) updated_sources=[] updated_packages=[] for fetched_file in progress.fetched: if fetched_file.endswith('Sources'): dest=updated_sources elif fetched_file.endswith('Packages'): dest=updated_packages else: continue repository, component=self._match_index_file_to_repository( fetched_file) dest.append(( repository, component, self._index_file_full_path(fetched_file) )) return updated_sources, updated_packages def _get_format(self, record): \"\"\" Returns the Format field value of the given source package record. \"\"\" record=deb822.Deb822(record) return record['format'] def _extract_quilt_package_debian_tar(self, debian_tar_path, outdir): \"\"\" Extracts the given tarball to the given output directory. \"\"\" with tarfile.open(debian_tar_path) as archive_file: archive_file.extractall(outdir) def get_package_source_cache_directory(self, package_name): \"\"\" Returns the path to the directory where a particular source package is cached. :param package_name: The name of the source package :type package_name: string :rtype: string \"\"\" package_hash=( package_name[0] if not package_name.startswith('lib') else package_name[:4] ) return os.path.join( self.source_cache_directory, package_hash, package_name) def get_source_version_cache_directory(self, package_name, version): \"\"\" Returns the path to the directory where a particular source package version files are extracted. :param package_name: The name of the source package :type package_name: string :param version: The version of the source package :type version: string :rtype: string \"\"\" package_dir=self.get_package_source_cache_directory(package_name) return os.path.join(package_dir, package_name +'-' +version) def _remove_dir(self, directory_path): \"\"\" Removes the given directory, including any subdirectories and files. The method makes sure to correctly handle the situation where the directory contains files with names which are invalid utf-8. \"\"\" directory_path=force_bytes(directory_path) if os.path.exists(directory_path): shutil.rmtree(directory_path) def clear_cached_sources(self): \"\"\" Clears all cached package source files. \"\"\" self._remove_dir(self.source_cache_directory) self._cache_size=self.get_directory_size(self.source_cache_directory) def _get_apt_source_records(self, source_name, version): \"\"\" Returns a:class:`apt_pkg.SourceRecords` instance where the given source package is the current working record. \"\"\" apt.Cache(rootdir=self.cache_root_dir) source_records=apt_pkg.SourceRecords() source_records.restart() found=False while source_records.lookup(source_name): if source_records.version==version: found=True break if not found: raise SourcePackageRetrieveError( \"Could not retrieve package{pkg} version{ver}:\" \" No such version found in the cache\".format( pkg=source_name, ver=version)) return source_records def _extract_dpkg_source(self, retrieved_files, outdir): \"\"\" Uses dpkg-source to extract the source package. \"\"\" dsc_file_path=next( file_path for file_path in retrieved_files if file_path.endswith('.dsc')) dsc_file_path=os.path.abspath(dsc_file_path) outdir=os.path.abspath(outdir) subprocess.check_output([\"dpkg-source\", \"-x\", dsc_file_path, outdir], stderr=subprocess.STDOUT) def _apt_acquire_package(self, source_records, dest_dir_path, debian_directory_only): \"\"\" Using:class:`apt_pkg.Acquire`, retrieves the source files for the source package described by the current source_records record. :param source_records: The record describing the source package whose files should be retrieved. :type source_records::class:`apt_pkg.Acquire` :param dest_dir_path: The path to the directory where the downloaded files should be saved. :type dest_dir_path: string :param debian_directory_only: A flag indicating whether only the debian directory should be downloaded. :returns: A list of absolute paths of all retrieved source files. :rtype: list of strings \"\"\" package_format=self._get_format(source_records.record) files=[] acquire=apt_pkg.Acquire(apt.progress.base.AcquireProgress()) for srcfile in source_records.files: base=os.path.basename(srcfile.path) dest_file_path=os.path.join(dest_dir_path, base) if debian_directory_only and package_format==self.QUILT_FORMAT: if srcfile.type !='diff': continue files.append(apt_pkg.AcquireFile( acquire, source_records.index.archive_uri(srcfile.path), srcfile.hashes, srcfile.size, base, destfile=dest_file_path )) acquire.run() retrieved_paths=[] for item in acquire.items: if item.status !=item.STAT_DONE: raise SourcePackageRetrieveError( 'Could not retrieve file{file}:{error}'.format( file=item.destfile, error=item.error_text.decode('utf-8'))) retrieved_paths.append(item.destfile) return retrieved_paths def retrieve_source(self, source_name, version, debian_directory_only=False): \"\"\" Retrieve the source package files for the given source package version. :param source_name: The name of the source package :type source_name: string :param version: The version of the source package :type version: string :param debian_directory_only: Flag indicating if the method should try to retrieve only the debian directory of the source package. This is usually only possible when the package format is 3.0(quilt). :type debian_directory_only: Boolean :returns: The path to the directory containing the extracted source package files. :rtype: string \"\"\" if self.cache_size > self.cache_max_size: self.clear_cached_sources() source_records=self._get_apt_source_records(source_name, version) dest_dir_path=self.get_package_source_cache_directory(source_name) if not os.path.exists(dest_dir_path): os.makedirs(dest_dir_path) old_size=self.get_directory_size(dest_dir_path) retrieved_files=self._apt_acquire_package( source_records, dest_dir_path, debian_directory_only) outdir=self.get_source_version_cache_directory(source_name, version) self._remove_dir(outdir) package_format=self._get_format(source_records.record) if debian_directory_only and package_format==self.QUILT_FORMAT: self._extract_quilt_package_debian_tar(retrieved_files[0], outdir) else: self._extract_dpkg_source(retrieved_files, outdir) new_size=self.get_directory_size(dest_dir_path) size_delta=new_size -old_size self._cache_size +=size_delta return outdir def html_package_list(packages): \"\"\"Return a HTML-formatted list of packages.\"\"\" packages_html=[] for package in packages: if \"\/\" in package: (source_package_name, remain)=package.split(\"\/\", 1) remain=\"\/%s\" %(remain,) else: (source_package_name, remain)=(package, \"\") html='<a href=\"{}\">{}<\/a>{}'.format( package_url(source_package_name), source_package_name, remain) packages_html.append(html) return ', '.join(packages_html) ","sourceWithComments":"# Copyright 2013-2018 The Distro Tracker Developers\n# See the COPYRIGHT file at the top-level directory of this distribution and\n# at https:\/\/deb.li\/DTAuthors\n#\n# This file is part of Distro Tracker. It is subject to the license terms\n# in the LICENSE file found in the top-level directory of this\n# distribution and at https:\/\/deb.li\/DTLicense. No part of Distro Tracker,\n# including this file, may be copied, modified, propagated, or distributed\n# except according to the terms contained in the LICENSE file.\n\"\"\"Utilities for processing Debian package information.\"\"\"\nimport os\nimport re\nimport shutil\nimport subprocess\nimport tarfile\n\nimport apt\n\nimport apt_pkg\n\nfrom debian import deb822\n\nfrom django.conf import settings\nfrom django.urls import reverse\nfrom django.utils.encoding import force_bytes\n\nfrom distro_tracker.core.utils.email_messages import \\\n    name_and_address_from_string as parse_address\nfrom distro_tracker.core.utils.email_messages import \\\n    names_and_addresses_from_string as parse_addresses\n\n\ndef package_hashdir(package_name):\n    \"\"\"\n    Returns the name of the hash directory used to avoid having too\n    many entries in a single directory. It's usually the first letter\n    of the package except for lib* packages where it's the first 4\n    letters.\n\n    :param package_name: The package name.\n    :type package_name: str\n\n    :returns: Name of the hash directory.\n    :rtype: str\n    \"\"\"\n    if package_name is None:\n        return None\n    if package_name.startswith('lib'):\n        return package_name[0:4]\n    else:\n        return package_name[0:1]\n\n\ndef package_url(package_name):\n    \"\"\"\n    Returns the URL of the page dedicated to this package name.\n\n    :param package_name: The package name.\n    :type package_name: str or PackageName model\n\n    :returns: Name of the hash directory.\n    :rtype: str\n    \"\"\"\n    if package_name is None:\n        return None\n    return reverse('dtracker-package-page',\n                   kwargs={'package_name': str(package_name)})\n\n\ndef extract_vcs_information(stanza):\n    \"\"\"\n    Extracts the VCS information from a package's Sources entry.\n\n    :param stanza: The ``Sources`` entry from which to extract the VCS info.\n        Maps ``Sources`` key names to values.\n    :type stanza: dict\n\n    :returns: VCS information regarding the package. Contains the following\n        keys: type[, browser, url, branch]\n    :rtype: dict\n    \"\"\"\n    vcs = {}\n    for key, value in stanza.items():\n        key = key.lower()\n        if key == 'vcs-browser':\n            vcs['browser'] = value\n        elif key.startswith('vcs-'):\n            vcs['type'] = key[4:]\n            vcs['url'] = value\n            if vcs['type'] == 'git':\n                match = re.match(r'(?P<url>.*?)\\s+-b\\s*(?P<branch>\\S+)', value)\n                if match:\n                    vcs['url'] = match.group('url')\n                    vcs['branch'] = match.group('branch')\n    return vcs\n\n\ndef extract_dsc_file_name(stanza):\n    \"\"\"\n    Extracts the name of the .dsc file from a package's Sources entry.\n\n    :param stanza: The ``Sources`` entry from which to extract the VCS info.\n        Maps ``Sources`` key names to values.\n    :type stanza: dict\n\n    \"\"\"\n    for field in ('checksums-sha256', 'checksums-sha1', 'files'):\n        for entry in stanza.get(field, []):\n            if entry.get('name', '').endswith('.dsc'):\n                return entry['name']\n\n    return None\n\n\ndef extract_information_from_sources_entry(stanza):\n    \"\"\"\n    Extracts information from a ``Sources`` file entry and returns it in the\n    form of a dictionary.\n\n    :param stanza: The raw entry's key-value pairs.\n    :type stanza: Case-insensitive dict\n    \"\"\"\n    binaries = [\n        binary.strip()\n        for binary in stanza['binary'].split(',')\n    ]\n    entry = {\n        'version': stanza['version'],\n        'homepage': stanza.get('homepage', ''),\n        'priority': stanza.get('priority', ''),\n        'section': stanza.get('section', ''),\n        'architectures': stanza['architecture'].split(),\n        'binary_packages': binaries,\n        'maintainer': parse_address(stanza['maintainer']),\n        'uploaders': parse_addresses(stanza.get('uploaders', '')),\n        'standards_version': stanza.get('standards-version', ''),\n        'vcs': extract_vcs_information(stanza),\n        'dsc_file_name': extract_dsc_file_name(stanza),\n        'directory': stanza.get('directory', ''),\n    }\n\n    return entry\n\n\ndef extract_information_from_packages_entry(stanza):\n    \"\"\"\n    Extracts information from a ``Packages`` file entry and returns it in the\n    form of a dictionary.\n\n    :param stanza: The raw entry's key-value pairs.\n    :type stanza: Case-insensitive dict\n    \"\"\"\n    entry = {\n        'version': stanza['version'],\n        'short_description': stanza.get('description', '')[:300],\n    }\n\n    return entry\n\n\nclass SourcePackageRetrieveError(Exception):\n    pass\n\n\nclass AptCache(object):\n    \"\"\"\n    A class for handling cached package information.\n    \"\"\"\n    DEFAULT_MAX_SIZE = 1 * 1024 ** 3  # 1 GiB\n    QUILT_FORMAT = '3.0 (quilt)'\n\n    class AcquireProgress(apt.progress.base.AcquireProgress):\n        \"\"\"\n        Instances of this class can be passed to :meth:`apt.cache.Cache.update`\n        calls.\n        It provides a way to track which files were changed and which were not\n        by an update operation.\n        \"\"\"\n        def __init__(self, *args, **kwargs):\n            super(AptCache.AcquireProgress, self).__init__(*args, **kwargs)\n            self.fetched = []\n            self.hit = []\n\n        def done(self, item):\n            self.fetched.append(os.path.split(item.owner.destfile)[1])\n\n        def ims_hit(self, item):\n            self.hit.append(os.path.split(item.owner.destfile)[1])\n\n        def pulse(self, owner):\n            return True\n\n    def __init__(self):\n        # The root cache directory is a subdirectory in the\n        # DISTRO_TRACKER_CACHE_DIRECTORY\n        self.cache_root_dir = os.path.join(\n            settings.DISTRO_TRACKER_CACHE_DIRECTORY,\n            'apt-cache'\n        )\n        self.sources_list_path = os.path.join(\n            self.cache_root_dir, 'etc', 'sources.list')\n        self.conf_file_path = os.path.join(self.cache_root_dir,\n                                           'etc', 'apt.conf')\n        os.environ['APT_CONFIG'] = self.conf_file_path\n\n        self.sources = []\n        self.packages = []\n        self.cache_max_size = getattr(\n            settings, 'DISTRO_TRACKER_APT_CACHE_MAX_SIZE',\n            self.DEFAULT_MAX_SIZE)\n        #: The directory where source package files are cached\n        self.source_cache_directory = os.path.join(self.cache_root_dir,\n                                                   'packages')\n        self._cache_size = None  # Evaluate the cache size lazily\n\n        self.configure_cache()\n\n    @property\n    def cache_size(self):\n        if self._cache_size is None:\n            self._cache_size = \\\n                self.get_directory_size(self.source_cache_directory)\n        return self._cache_size\n\n    def get_directory_size(self, directory_path):\n        \"\"\"\n        Returns the total space taken by the given directory in bytes.\n\n        :param directory_path: The path to the directory\n        :type directory_path: string\n\n        :rtype: int\n        \"\"\"\n        # Convert the directory path to bytes to make sure all os calls deal\n        # with bytes, not unicode objects.\n        # This way any file names with invalid utf-8 names, are correctly\n        # handled, without causing an error.\n        directory_path = force_bytes(directory_path)\n        total_size = 0\n        for dirpath, dirnames, filenames in os.walk(directory_path):\n            for file_name in filenames:\n                file_path = os.path.join(dirpath, file_name)\n                stat = os.lstat(file_path)\n                total_size += stat.st_size\n\n        return total_size\n\n    def clear_cache(self):\n        \"\"\"\n        Removes all cache information. This causes the next update to retrieve\n        fresh repository files.\n        \"\"\"\n        self._remove_dir(self.cache_root_dir)\n        self.configure_cache()\n\n    def update_sources_list(self):\n        \"\"\"\n        Updates the ``sources.list`` file used to list repositories for which\n        package information should be cached.\n        \"\"\"\n        from distro_tracker.core.models import Repository\n\n        directory = os.path.dirname(self.sources_list_path)\n        if not os.path.exists(directory):\n            os.makedirs(directory)\n\n        with open(self.sources_list_path, 'w') as sources_list:\n            for repository in Repository.objects.all():\n                sources_list.write(repository.sources_list_entry + '\\n')\n\n    def update_apt_conf(self):\n        \"\"\"\n        Updates the ``apt.conf`` file which gives general settings for the\n        :class:`apt.cache.Cache`.\n\n        In particular, this updates the list of all architectures which should\n        be considered in package updates based on architectures that the\n        repositories support.\n        \"\"\"\n        from distro_tracker.core.models import Architecture\n\n        with open(self.conf_file_path, 'w') as conf_file:\n            conf_file.write('APT::Architectures { ')\n            for architecture in Architecture.objects.all():\n                conf_file.write('\"{arch}\"; '.format(arch=architecture))\n            conf_file.write('};\\n')\n            conf_file.write('Acquire::CompressionTypes::Order:: \"xz\";\\n')\n            conf_file.write('Dir \"{}\/\";\\n'.format(self.cache_root_dir))\n            conf_file.write('Dir::State \"state\/\";\\n')\n            conf_file.write('Dir::State::status \"dpkg-status\";\\n')\n            conf_file.write('Dir::Etc \"etc\/\";\\n')\n            conf_file.write('Dir::Etc::sourcelist \"{src}\";\\n'.format(\n                src=self.sources_list_path))\n            conf_file.write('Dir::Etc::Trusted \"{src}\";\\n'.format(\n                src=settings.DISTRO_TRACKER_TRUSTED_GPG_MAIN_FILE))\n            conf_file.write('Dir::Etc::TrustedParts \"{src}\";\\n'.format(\n                src=settings.DISTRO_TRACKER_TRUSTED_GPG_PARTS_DIR))\n\n    def configure_cache(self):\n        \"\"\"\n        Configures the cache based on the most current repository information.\n        \"\"\"\n        self.update_sources_list()\n        self.update_apt_conf()\n        # Clean up the configuration we might have read during \"import apt\"\n        for root_key in apt_pkg.config.list():\n            apt_pkg.config.clear(root_key)\n        # Load the proper configuration\n        apt_pkg.init()\n        # Ensure we have the required directories\n        for apt_dir in [apt_pkg.config.find_dir('Dir::State::lists'),\n                        apt_pkg.config.find_dir('Dir::Etc::sourceparts'),\n                        apt_pkg.config.find_dir('Dir::Cache::archives')]:\n            if not os.path.exists(apt_dir):\n                os.makedirs(apt_dir)\n\n    def _index_file_full_path(self, file_name):\n        \"\"\"\n        Returns the absolute path for the given cached index file.\n\n        :param file_name: The name of the cached index file.\n        :type file_name: string\n\n        :rtype: string\n        \"\"\"\n        return os.path.join(\n            apt_pkg.config.find_dir('Dir::State::lists'),\n            file_name\n        )\n\n    def _match_index_file_to_repository(self, sources_file):\n        \"\"\"\n        Returns a two-tuple ``(class:`Repository <distro_tracker.core.\n        models.Repository>`, component)``. The class:`Repository\n        <distro_tracker.core.models.Repository>` instance which matches the\n        given cached ``Sources`` file and the ``component`` of the ``Source``.\n\n        :rtype: (:class:`Repository <distro_tracker.core.models.Repository>`,\n            string)\n        \"\"\"\n        from distro_tracker.core.models import Repository\n\n        sources_list = apt_pkg.SourceList()\n        sources_list.read_main_list()\n        component_url = None\n        component = None\n        for entry in sources_list.list:\n            for index_file in entry.index_files:\n                if os.path.basename(sources_file) in index_file.describe:\n                    base_url, component, _ = index_file.describe.split(None, 2)\n                    base_url = base_url.rstrip('\/')\n                    component_url = base_url + '\/' + component\n                    break\n\n        components = component.split('\/')\n        if len(components) >= 2:\n            component = components[1].strip()\n\n        for repository in Repository.objects.all():\n            if component_url in repository.component_urls:\n                return repository, component\n\n    def _get_all_cached_files(self):\n        \"\"\"\n        Returns a list of all cached files.\n        \"\"\"\n        lists_directory = apt_pkg.config.find_dir('Dir::State::lists')\n        try:\n            return [\n                os.path.join(lists_directory, file_name)\n                for file_name in os.listdir(lists_directory)\n                if os.path.isfile(os.path.join(lists_directory, file_name))\n            ]\n        except OSError:\n            # The directory structure does not exist => nothing is cached\n            return []\n\n    def get_cached_files(self, filter_function=None):\n        \"\"\"\n        Returns cached files, optionally filtered by the given\n        ``filter_function``\n\n        :param filter_function: Takes a file name as the only parameter and\n            returns a :class:`bool` indicating whether it should be included\n            in the result.\n        :type filter_function: callable\n\n        :returns: A list of cached file names\n        :rtype: list\n        \"\"\"\n        if filter_function is None:\n            # Include all files if the filter function is not provided\n            def filter_function(x):\n                return True\n\n        return [\n            file_name\n            for file_name in self._get_all_cached_files()\n            if filter_function(file_name)\n        ]\n\n    def get_sources_files_for_repository(self, repository):\n        \"\"\"\n        Returns all ``Sources`` files which are cached for the given\n        repository.\n\n        For instance, ``Sources`` files for different suites are cached\n        separately.\n\n        :param repository: The repository for which to return all cached\n            ``Sources`` files\n        :type repository: :class:`Repository\n            <distro_tracker.core.models.Repository>`\n\n        :rtype: ``iterable`` of strings\n        \"\"\"\n        return self.get_cached_files(\n            lambda file_name: (\n                file_name.endswith('Sources') and\n                self._match_index_file_to_repository(\n                    file_name)[0] == repository))\n\n    def get_packages_files_for_repository(self, repository):\n        \"\"\"\n        Returns all ``Packages`` files which are cached for the given\n        repository.\n\n        For instance, ``Packages`` files for different suites are cached\n        separately.\n\n        :param repository: The repository for which to return all cached\n            ``Packages`` files\n        :type repository: :class:`Repository\n            <distro_tracker.core.models.Repository>`\n\n        :rtype: ``iterable`` of strings\n        \"\"\"\n        return self.get_cached_files(\n            lambda file_name: (\n                file_name.endswith('Packages') and\n                self._match_index_file_to_repository(\n                    file_name)[0] == repository))\n\n    def update_repositories(self, force_download=False):\n        \"\"\"\n        Initiates a cache update.\n\n        :param force_download: If set to ``True`` causes the cache to be\n            cleared before starting the update, thus making sure all index\n            files are downloaded again.\n\n        :returns: A two-tuple ``(updated_sources, updated_packages)``. Each of\n            the tuple's members is a list of\n            (:class:`Repository <distro_tracker.core.models.Repository>`,\n            ``component``, ``file_name``) tuple representing the repository\n            which was updated, component, and the file which contains the fresh\n            information. The file is either a ``Sources`` or a ``Packages``\n            file respectively.\n        \"\"\"\n        if force_download:\n            self.clear_cache()\n\n        self.configure_cache()\n\n        cache = apt.Cache(rootdir=self.cache_root_dir)\n        progress = AptCache.AcquireProgress()\n        cache.update(progress)\n\n        updated_sources = []\n        updated_packages = []\n        for fetched_file in progress.fetched:\n            if fetched_file.endswith('Sources'):\n                dest = updated_sources\n            elif fetched_file.endswith('Packages'):\n                dest = updated_packages\n            else:\n                continue\n            repository, component = self._match_index_file_to_repository(\n                fetched_file)\n            dest.append((\n                repository, component, self._index_file_full_path(fetched_file)\n            ))\n\n        return updated_sources, updated_packages\n\n    def _get_format(self, record):\n        \"\"\"\n        Returns the Format field value of the given source package record.\n        \"\"\"\n        record = deb822.Deb822(record)\n        return record['format']\n\n    def _extract_quilt_package_debian_tar(self, debian_tar_path, outdir):\n        \"\"\"\n        Extracts the given tarball to the given output directory.\n        \"\"\"\n        with tarfile.open(debian_tar_path) as archive_file:\n            archive_file.extractall(outdir)\n\n    def get_package_source_cache_directory(self, package_name):\n        \"\"\"\n        Returns the path to the directory where a particular source package is\n        cached.\n\n        :param package_name: The name of the source package\n        :type package_name: string\n\n        :rtype: string\n        \"\"\"\n        package_hash = (\n            package_name[0]\n            if not package_name.startswith('lib') else\n            package_name[:4]\n        )\n        return os.path.join(\n            self.source_cache_directory,\n            package_hash,\n            package_name)\n\n    def get_source_version_cache_directory(self, package_name, version):\n        \"\"\"\n        Returns the path to the directory where a particular source package\n        version files are extracted.\n\n        :param package_name: The name of the source package\n        :type package_name: string\n\n        :param version: The version of the source package\n        :type version: string\n\n        :rtype: string\n        \"\"\"\n        package_dir = self.get_package_source_cache_directory(package_name)\n        return os.path.join(package_dir, package_name + '-' + version)\n\n    def _remove_dir(self, directory_path):\n        \"\"\"\n        Removes the given directory, including any subdirectories and files.\n        The method makes sure to correctly handle the situation where the\n        directory contains files with names which are invalid utf-8.\n        \"\"\"\n        # Convert the directory path to bytes to make sure all os calls deal\n        # with bytes, not unicode objects.\n        # This way any file names with invalid utf-8 names, are correctly\n        # handled, without causing an error.\n        directory_path = force_bytes(directory_path)\n        if os.path.exists(directory_path):\n            shutil.rmtree(directory_path)\n\n    def clear_cached_sources(self):\n        \"\"\"\n        Clears all cached package source files.\n        \"\"\"\n        self._remove_dir(self.source_cache_directory)\n        self._cache_size = self.get_directory_size(self.source_cache_directory)\n\n    def _get_apt_source_records(self, source_name, version):\n        \"\"\"\n        Returns a :class:`apt_pkg.SourceRecords` instance where the given\n        source package is the current working record.\n        \"\"\"\n        apt.Cache(rootdir=self.cache_root_dir)  # must be pre-created\n        source_records = apt_pkg.SourceRecords()\n        source_records.restart()\n        # Find the cached record matching this source package and version\n        found = False\n        while source_records.lookup(source_name):\n            if source_records.version == version:\n                found = True\n                break\n\n        if not found:\n            # Package version does not exist in the cache\n            raise SourcePackageRetrieveError(\n                \"Could not retrieve package {pkg} version {ver}:\"\n                \" No such version found in the cache\".format(\n                    pkg=source_name, ver=version))\n\n        return source_records\n\n    def _extract_dpkg_source(self, retrieved_files, outdir):\n        \"\"\"\n        Uses dpkg-source to extract the source package.\n        \"\"\"\n        dsc_file_path = next(\n            file_path\n            for file_path in retrieved_files\n            if file_path.endswith('.dsc'))\n        dsc_file_path = os.path.abspath(dsc_file_path)\n        outdir = os.path.abspath(outdir)\n        subprocess.check_output([\"dpkg-source\", \"-x\", dsc_file_path, outdir],\n                                stderr=subprocess.STDOUT)\n\n    def _apt_acquire_package(self,\n                             source_records,\n                             dest_dir_path,\n                             debian_directory_only):\n        \"\"\"\n        Using :class:`apt_pkg.Acquire`, retrieves the source files for the\n        source package described by the current source_records record.\n\n        :param source_records: The record describing the source package whose\n            files should be retrieved.\n        :type source_records: :class:`apt_pkg.Acquire`\n\n        :param dest_dir_path: The path to the directory where the downloaded\n            files should be saved.\n        :type dest_dir_path: string\n\n        :param debian_directory_only: A flag indicating whether only the debian\n            directory should be downloaded.\n\n        :returns: A list of absolute paths of all retrieved source files.\n        :rtype: list of strings\n        \"\"\"\n        package_format = self._get_format(source_records.record)\n        # A reference to each AcquireFile instance must be kept\n        files = []\n        acquire = apt_pkg.Acquire(apt.progress.base.AcquireProgress())\n        for srcfile in source_records.files:\n            base = os.path.basename(srcfile.path)\n            dest_file_path = os.path.join(dest_dir_path, base)\n            if debian_directory_only and package_format == self.QUILT_FORMAT:\n                if srcfile.type != 'diff':\n                    # Only retrieve the .debian.tar.* file for quilt packages\n                    # when only the debian directory is wanted\n                    continue\n            files.append(apt_pkg.AcquireFile(\n                acquire,\n                source_records.index.archive_uri(srcfile.path),\n                srcfile.hashes,\n                srcfile.size,\n                base,\n                destfile=dest_file_path\n            ))\n\n        acquire.run()\n\n        # Check if all items are correctly retrieved and build the list of file\n        # paths.\n        retrieved_paths = []\n        for item in acquire.items:\n            if item.status != item.STAT_DONE:\n                raise SourcePackageRetrieveError(\n                    'Could not retrieve file {file}: {error}'.format(\n                        file=item.destfile,\n                        error=item.error_text.decode('utf-8')))\n            retrieved_paths.append(item.destfile)\n\n        return retrieved_paths\n\n    def retrieve_source(self, source_name, version,\n                        debian_directory_only=False):\n        \"\"\"\n        Retrieve the source package files for the given source package version.\n\n        :param source_name: The name of the source package\n        :type source_name: string\n        :param version: The version of the source package\n        :type version: string\n        :param debian_directory_only: Flag indicating if the method should try\n            to retrieve only the debian directory of the source package. This\n            is usually only possible when the package format is 3.0 (quilt).\n        :type debian_directory_only: Boolean\n\n        :returns: The path to the directory containing the extracted source\n            package files.\n        :rtype: string\n        \"\"\"\n        if self.cache_size > self.cache_max_size:\n            # If the maximum allowed cache size has been exceeded,\n            # clear the cache\n            self.clear_cached_sources()\n\n        source_records = self._get_apt_source_records(source_name, version)\n\n        dest_dir_path = self.get_package_source_cache_directory(source_name)\n        if not os.path.exists(dest_dir_path):\n            os.makedirs(dest_dir_path)\n        # Remember the size of the directory in the beginning\n        old_size = self.get_directory_size(dest_dir_path)\n\n        # Download the source files\n        retrieved_files = self._apt_acquire_package(\n            source_records, dest_dir_path, debian_directory_only)\n\n        # Extract the retrieved source files\n        outdir = self.get_source_version_cache_directory(source_name, version)\n        # dpkg-source expects this directory not to exist\n        self._remove_dir(outdir)\n\n        package_format = self._get_format(source_records.record)\n        if debian_directory_only and package_format == self.QUILT_FORMAT:\n            # dpkg-source cannot extract an incomplete package\n            self._extract_quilt_package_debian_tar(retrieved_files[0], outdir)\n        else:\n            # Let dpkg-source handle the extraction in all other cases\n            self._extract_dpkg_source(retrieved_files, outdir)\n\n        # Update the current cache size based on the changes made by getting\n        # this source package.\n        new_size = self.get_directory_size(dest_dir_path)\n        size_delta = new_size - old_size\n        self._cache_size += size_delta\n\n        return outdir\n\n\ndef html_package_list(packages):\n    \"\"\"Return a HTML-formatted list of packages.\"\"\"\n    packages_html = []\n    for package in packages:\n        if \"\/\" in package:\n            (source_package_name, remain) = package.split(\"\/\", 1)\n            remain = \"\/%s\" % (remain,)\n        else:\n            (source_package_name, remain) = (package, \"\")\n        html = '<a href=\"{}\">{}<\/a>{}'.format(\n            package_url(source_package_name), source_package_name, remain)\n        packages_html.append(html)\n\n    return ', '.join(packages_html)\n"}},"msg":"Adding tarfile member sanitization to extractall()\n\nThis is known as CVE-2007-4559. In distro-tracker, we only extract\ntarfile from Debian source packages but still it's better to avoid\nthat directory traversal vulnerability.\n\n[hertzog@debian.org: simplified the original pull request received\nat https:\/\/github.com\/rhertzog\/distro-tracker\/pull\/1]"}},"https:\/\/github.com\/jrspruitt\/ubi_reader":{"c6a1272b178a4a2a04cfc88c87f6e195b16eddb5":{"url":"https:\/\/api.github.com\/repos\/jrspruitt\/ubi_reader\/commits\/c6a1272b178a4a2a04cfc88c87f6e195b16eddb5","html_url":"https:\/\/github.com\/jrspruitt\/ubi_reader\/commit\/c6a1272b178a4a2a04cfc88c87f6e195b16eddb5","message":"Fix path traversal vulnerability.\n\nubireader_extract_files could lead to path traversal when run against\nspecifically crafted UBIFS files, allowing the attacker to overwrite\nfiles outside of the extraction directory (provided the process has\nwrite access to that file or directory).","sha":"c6a1272b178a4a2a04cfc88c87f6e195b16eddb5","keyword":"directory traversal vulnerable","diff":"diff --git a\/ubireader\/ubifs\/output.py b\/ubireader\/ubifs\/output.py\nindex e74f1f7..88cae3e 100755\n--- a\/ubireader\/ubifs\/output.py\n+++ b\/ubireader\/ubifs\/output.py\n@@ -26,6 +26,10 @@\n from ubireader.ubifs.misc import decompress\n from ubireader.debug import error, log, verbose_log\n \n+def is_safe_path(basedir, path):\n+    basedir = os.path.realpath(basedir)\n+    path = os.path.realpath(os.path.join(basedir, path))\n+    return basedir == os.path.commonpath((basedir, path))\n \n def extract_files(ubifs, out_path, perms=False):\n     \"\"\"Extract UBIFS contents to_path\/\n@@ -59,8 +63,12 @@ def extract_dents(ubifs, inodes, dent_node, path='', perms=False):\n         return\n \n     inode = inodes[dent_node.inum]\n-    dent_path = os.path.join(path, dent_node.name)\n-        \n+\n+    if not is_safe_path(path, dent_node.name):\n+        error(extract_dents, 'Warning', 'Path traversal attempt: %s, discarding' % (dent_node.name))\n+        return\n+    dent_path = os.path.realpath(os.path.join(path, dent_node.name))\n+\n     if dent_node.type == UBIFS_ITYPE_DIR:\n         try:\n             if not os.path.exists(dent_path):\n","files":{"\/ubireader\/ubifs\/output.py":{"changes":[{"diff":"\n         return\n \n     inode = inodes[dent_node.inum]\n-    dent_path = os.path.join(path, dent_node.name)\n-        \n+\n+    if not is_safe_path(path, dent_node.name):\n+        error(extract_dents, 'Warning', 'Path traversal attempt: %s, discarding' % (dent_node.name))\n+        return\n+    dent_path = os.path.realpath(os.path.join(path, dent_node.name))\n+\n     if dent_node.type == UBIFS_ITYPE_DIR:\n         try:\n             if not os.path.exists(dent_path):\n","add":6,"remove":2,"filename":"\/ubireader\/ubifs\/output.py","badparts":["    dent_path = os.path.join(path, dent_node.name)"],"goodparts":["    if not is_safe_path(path, dent_node.name):","        error(extract_dents, 'Warning', 'Path traversal attempt: %s, discarding' % (dent_node.name))","        return","    dent_path = os.path.realpath(os.path.join(path, dent_node.name))"]}],"source":"\n import os import struct from ubireader import settings from ubireader.ubifs.defines import * from ubireader.ubifs import walk from ubireader.ubifs.misc import decompress from ubireader.debug import error, log, verbose_log def extract_files(ubifs, out_path, perms=False): \"\"\"Extract UBIFS contents to_path\/ Arguments: Obj:ubifs --UBIFS object. Str:out_path --Path to extract contents to. \"\"\" try: inodes={} bad_blocks=[] walk.index(ubifs, ubifs.master_node.root_lnum, ubifs.master_node.root_offs, inodes, bad_blocks) if len(inodes) < 2: raise Exception('No inodes found') for dent in inodes[1]['dent']: extract_dents(ubifs, inodes, dent, out_path, perms) if len(bad_blocks): error(extract_files, 'Warning', 'Data may be missing or corrupted, bad blocks, LEB[%s]' % ','.join(map(str, bad_blocks))) except Exception as e: error(extract_files, 'Error', '%s' % e) def extract_dents(ubifs, inodes, dent_node, path='', perms=False): if dent_node.inum not in inodes: error(extract_dents, 'Error', 'inum: %s not found in inodes' %(dent_node.inum)) return inode=inodes[dent_node.inum] dent_path=os.path.join(path, dent_node.name) if dent_node.type==UBIFS_ITYPE_DIR: try: if not os.path.exists(dent_path): os.mkdir(dent_path) log(extract_dents, 'Make Dir: %s' %(dent_path)) if perms: _set_file_perms(dent_path, inode) except Exception as e: error(extract_dents, 'Warn', 'DIR Fail: %s' % e) if 'dent' in inode: for dnode in inode['dent']: extract_dents(ubifs, inodes, dnode, dent_path, perms) _set_file_timestamps(dent_path, inode) elif dent_node.type==UBIFS_ITYPE_REG: try: if inode['ino'].nlink > 1: if 'hlink' not in inode: inode['hlink']=dent_path buf=_process_reg_file(ubifs, inode, dent_path) _write_reg_file(dent_path, buf) else: os.link(inode['hlink'], dent_path) log(extract_dents, 'Make Link: %s > %s' %(dent_path, inode['hlink'])) else: buf=_process_reg_file(ubifs, inode, dent_path) _write_reg_file(dent_path, buf) _set_file_timestamps(dent_path, inode) if perms: _set_file_perms(dent_path, inode) except Exception as e: error(extract_dents, 'Warn', 'FILE Fail: %s' % e) elif dent_node.type==UBIFS_ITYPE_LNK: try: os.symlink('%s' % inode['ino'].data.decode('utf-8'), dent_path) log(extract_dents, 'Make Symlink: %s > %s' %(dent_path, inode['ino'].data)) except Exception as e: error(extract_dents, 'Warn', 'SYMLINK Fail: %s' % e) elif dent_node.type in[UBIFS_ITYPE_BLK, UBIFS_ITYPE_CHR]: try: dev=struct.unpack('<II', inode['ino'].data)[0] if not settings.use_dummy_devices: os.mknod(dent_path, inode['ino'].mode, dev) log(extract_dents, 'Make Device Node: %s' %(dent_path)) if perms: _set_file_perms(dent_path, inode) else: log(extract_dents, 'Create dummy device.') _write_reg_file(dent_path, str(dev)) if perms: _set_file_perms(dent_path, inode) except Exception as e: error(extract_dents, 'Warn', 'DEV Fail: %s' % e) elif dent_node.type==UBIFS_ITYPE_FIFO: try: os.mkfifo(dent_path, inode['ino'].mode) log(extract_dents, 'Make FIFO: %s' %(path)) if perms: _set_file_perms(dent_path, inode) except Exception as e: error(extract_dents, 'Warn', 'FIFO Fail: %s: %s' %(dent_path, e)) elif dent_node.type==UBIFS_ITYPE_SOCK: try: if settings.use_dummy_socket_file: _write_reg_file(dent_path, '') if perms: _set_file_perms(dent_path, inode) except Exception as e: error(extract_dents, 'Warn', 'SOCK Fail: %s: %s' %(dent_path, e)) def _set_file_perms(path, inode): os.chown(path, inode['ino'].uid, inode['ino'].gid) os.chmod(path, inode['ino'].mode) verbose_log(_set_file_perms, 'perms:%s, owner: %s.%s, path: %s' %(inode['ino'].mode, inode['ino'].uid, inode['ino'].gid, path)) def _set_file_timestamps(path, inode): os.utime(path,(inode['ino'].atime_sec, inode['ino'].mtime_sec)) verbose_log(_set_file_timestamps, 'timestamps: access: %s, modify: %s, path: %s' %(inode['ino'].atime_sec, inode['ino'].mtime_sec, path)) def _write_reg_file(path, data): with open(path, 'wb') as f: f.write(data) log(_write_reg_file, 'Make File: %s' %(path)) def _process_reg_file(ubifs, inode, path): try: buf=bytearray() if 'data' in inode: compr_type=0 sorted_data=sorted(inode['data'], key=lambda x: x.key['khash']) last_khash=sorted_data[0].key['khash']-1 for data in sorted_data: if data.key['khash'] -last_khash !=1: while 1 !=(data.key['khash'] -last_khash): buf +=b'\\x00'*UBIFS_BLOCK_SIZE last_khash +=1 compr_type=data.compr_type ubifs.file.seek(data.offset) d=ubifs.file.read(data.compr_len) buf +=decompress(compr_type, data.size, d) last_khash=data.key['khash'] verbose_log(_process_reg_file, 'ino num: %s, compression: %s, path: %s' %(inode['ino'].key['ino_num'], compr_type, path)) except Exception as e: error(_process_reg_file, 'Warn', 'inode num:%s:%s' %(inode['ino'].key['ino_num'], e)) if inode['ino'].size > len(buf): buf +=b'\\x00' *(inode['ino'].size -len(buf)) return bytes(buf) ","sourceWithComments":"#!\/usr\/bin\/env python\n#############################################################\n# ubi_reader\/ubifs\n# (c) 2013 Jason Pruitt (jrspruitt@gmail.com)\n#\n# This program is free software: you can redistribute it and\/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http:\/\/www.gnu.org\/licenses\/>.\n#############################################################\n\nimport os\nimport struct\n\nfrom ubireader import settings\nfrom ubireader.ubifs.defines import *\nfrom ubireader.ubifs import walk\nfrom ubireader.ubifs.misc import decompress\nfrom ubireader.debug import error, log, verbose_log\n\n\ndef extract_files(ubifs, out_path, perms=False):\n    \"\"\"Extract UBIFS contents to_path\/\n\n    Arguments:\n    Obj:ubifs    -- UBIFS object.\n    Str:out_path  -- Path to extract contents to.\n    \"\"\"\n    try:\n        inodes = {}\n        bad_blocks = []\n\n        walk.index(ubifs, ubifs.master_node.root_lnum, ubifs.master_node.root_offs, inodes, bad_blocks)\n\n        if len(inodes) < 2:\n            raise Exception('No inodes found')\n\n        for dent in inodes[1]['dent']:\n            extract_dents(ubifs, inodes, dent, out_path, perms)\n\n        if len(bad_blocks):\n            error(extract_files, 'Warning', 'Data may be missing or corrupted, bad blocks, LEB [%s]' % ','.join(map(str, bad_blocks)))\n\n    except Exception as e:\n        error(extract_files, 'Error', '%s' % e)\n\n\ndef extract_dents(ubifs, inodes, dent_node, path='', perms=False):\n    if dent_node.inum not in inodes:\n        error(extract_dents, 'Error', 'inum: %s not found in inodes' % (dent_node.inum))\n        return\n\n    inode = inodes[dent_node.inum]\n    dent_path = os.path.join(path, dent_node.name)\n        \n    if dent_node.type == UBIFS_ITYPE_DIR:\n        try:\n            if not os.path.exists(dent_path):\n                os.mkdir(dent_path)\n                log(extract_dents, 'Make Dir: %s' % (dent_path))\n\n                if perms:\n                    _set_file_perms(dent_path, inode)\n        except Exception as e:\n            error(extract_dents, 'Warn', 'DIR Fail: %s' % e)\n\n        if 'dent' in inode:\n            for dnode in inode['dent']:\n                extract_dents(ubifs, inodes, dnode, dent_path, perms)\n\n        _set_file_timestamps(dent_path, inode)\n\n    elif dent_node.type == UBIFS_ITYPE_REG:\n        try:\n            if inode['ino'].nlink > 1:\n                if 'hlink' not in inode:\n                    inode['hlink'] = dent_path\n                    buf = _process_reg_file(ubifs, inode, dent_path)\n                    _write_reg_file(dent_path, buf)\n                else:\n                    os.link(inode['hlink'], dent_path)\n                    log(extract_dents, 'Make Link: %s > %s' % (dent_path, inode['hlink']))\n            else:\n                buf = _process_reg_file(ubifs, inode, dent_path)\n                _write_reg_file(dent_path, buf)\n\n            _set_file_timestamps(dent_path, inode)\n\n            if perms:\n                _set_file_perms(dent_path, inode)\n\n        except Exception as e:\n            error(extract_dents, 'Warn', 'FILE Fail: %s' % e)\n\n    elif dent_node.type == UBIFS_ITYPE_LNK:\n        try:\n            # probably will need to decompress ino data if > UBIFS_MIN_COMPR_LEN\n            os.symlink('%s' % inode['ino'].data.decode('utf-8'), dent_path)\n            log(extract_dents, 'Make Symlink: %s > %s' % (dent_path, inode['ino'].data))\n\n        except Exception as e:\n            error(extract_dents, 'Warn', 'SYMLINK Fail: %s' % e) \n\n    elif dent_node.type in [UBIFS_ITYPE_BLK, UBIFS_ITYPE_CHR]:\n        try:\n            dev = struct.unpack('<II', inode['ino'].data)[0]\n            if not settings.use_dummy_devices:\n                os.mknod(dent_path, inode['ino'].mode, dev)\n                log(extract_dents, 'Make Device Node: %s' % (dent_path))\n\n                if perms:\n                    _set_file_perms(dent_path, inode)\n            else:\n                log(extract_dents, 'Create dummy device.')\n                _write_reg_file(dent_path, str(dev))\n\n                if perms:\n                    _set_file_perms(dent_path, inode)\n                \n        except Exception as e:\n            error(extract_dents, 'Warn', 'DEV Fail: %s' % e)\n\n    elif dent_node.type == UBIFS_ITYPE_FIFO:\n        try:\n            os.mkfifo(dent_path, inode['ino'].mode)\n            log(extract_dents, 'Make FIFO: %s' % (path))\n\n            if perms:\n                _set_file_perms(dent_path, inode)\n        except Exception as e:\n            error(extract_dents, 'Warn', 'FIFO Fail: %s : %s' % (dent_path, e))\n\n    elif dent_node.type == UBIFS_ITYPE_SOCK:\n        try:\n            if settings.use_dummy_socket_file:\n                _write_reg_file(dent_path, '')\n                if perms:\n                    _set_file_perms(dent_path, inode)\n        except Exception as e:\n            error(extract_dents, 'Warn', 'SOCK Fail: %s : %s' % (dent_path, e))\n\n\ndef _set_file_perms(path, inode):\n    os.chown(path, inode['ino'].uid, inode['ino'].gid)\n    os.chmod(path, inode['ino'].mode)\n    verbose_log(_set_file_perms, 'perms:%s, owner: %s.%s, path: %s' % (inode['ino'].mode, inode['ino'].uid, inode['ino'].gid, path))\n\ndef _set_file_timestamps(path, inode):\n    os.utime(path, (inode['ino'].atime_sec, inode['ino'].mtime_sec))\n    verbose_log(_set_file_timestamps, 'timestamps: access: %s, modify: %s, path: %s' % (inode['ino'].atime_sec, inode['ino'].mtime_sec, path))\n\ndef _write_reg_file(path, data):\n    with open(path, 'wb') as f:\n        f.write(data)\n    log(_write_reg_file, 'Make File: %s' % (path))\n\n\ndef _process_reg_file(ubifs, inode, path):\n    try:\n        buf = bytearray()\n        if 'data' in inode:\n            compr_type = 0\n            sorted_data = sorted(inode['data'], key=lambda x: x.key['khash'])\n            last_khash = sorted_data[0].key['khash']-1\n\n            for data in sorted_data:\n                \n                # If data nodes are missing in sequence, fill in blanks\n                # with \\x00 * UBIFS_BLOCK_SIZE\n                if data.key['khash'] - last_khash != 1:\n                    while 1 != (data.key['khash'] - last_khash):\n                        buf += b'\\x00'*UBIFS_BLOCK_SIZE\n                        last_khash += 1\n\n                compr_type = data.compr_type\n                ubifs.file.seek(data.offset)\n                d = ubifs.file.read(data.compr_len)\n                buf += decompress(compr_type, data.size, d)\n                last_khash = data.key['khash']\n                verbose_log(_process_reg_file, 'ino num: %s, compression: %s, path: %s' % (inode['ino'].key['ino_num'], compr_type, path))\n\n    except Exception as e:\n        error(_process_reg_file, 'Warn', 'inode num:%s :%s' % (inode['ino'].key['ino_num'], e))\n    \n    # Pad end of file with \\x00 if needed.\n    if inode['ino'].size > len(buf):\n        buf += b'\\x00' * (inode['ino'].size - len(buf))\n        \n    return bytes(buf)\n"}},"msg":"Fix path traversal vulnerability.\n\nubireader_extract_files could lead to path traversal when run against\nspecifically crafted UBIFS files, allowing the attacker to overwrite\nfiles outside of the extraction directory (provided the process has\nwrite access to that file or directory)."}},"https:\/\/github.com\/misamoylov\/otus-python":{"7cf5635080163735e5c925b9640bd654f9322453":{"url":"https:\/\/api.github.com\/repos\/misamoylov\/otus-python\/commits\/7cf5635080163735e5c925b9640bd654f9322453","html_url":"https:\/\/github.com\/misamoylov\/otus-python\/commit\/7cf5635080163735e5c925b9640bd654f9322453","message":"Fix vulnerability directory traversal","sha":"7cf5635080163735e5c925b9640bd654f9322453","keyword":"directory traversal vulnerable","diff":"diff --git a\/web-server\/httpd.py b\/web-server\/httpd.py\nindex d785b19..70fddfc 100644\n--- a\/web-server\/httpd.py\n+++ b\/web-server\/httpd.py\n@@ -2,7 +2,7 @@\n import logging\n import mimetypes\n import os\n-import re\n+import posixpath\n import socket\n import sys\n import time\n@@ -15,28 +15,6 @@\n \n                     datefmt='%a %b %d %H:%M:%S %Y')\n \n-PCT_ENCODED = '%[A-Fa-f0-9]{2}'\n-SUB_DELIMITERS_RE = \"!$&'()\\*+,;=\"\n-UNRESERVED_RE = 'A-Za-z0-9._~\\-'\n-PCHAR = '([' + UNRESERVED_RE + SUB_DELIMITERS_RE + ':@]|%s)' % PCT_ENCODED\n-segments = {\n-    'segment': PCHAR + '*',\n-    # Non-zero length segment\n-    'segment-nz': PCHAR + '+',\n-    # Non-zero length segment without \":\"\n-    'segment-nz-nc': PCHAR.replace(':', '') + '+'\n-}\n-PATH_EMPTY = '^$'\n-PATH_ROOTLESS = '%(segment-nz)s(\/%(segment)s)*' % segments\n-PATH_NOSCHEME = '%(segment-nz-nc)s(\/%(segment)s)*' % segments\n-PATH_ABSOLUTE = '\/(%s)?' % PATH_ROOTLESS\n-PATH_ABEMPTY = '(\/%(segment)s)*' % segments\n-PATH_RE = '^(%s|%s|%s|%s|%s)$' % (\n-    PATH_ABEMPTY, PATH_ABSOLUTE, PATH_NOSCHEME, PATH_ROOTLESS, PATH_EMPTY\n-)\n-\n-PATH_MATCHER = re.compile(PATH_RE)\n-\n \n class Worker(Thread):\n     \"\"\"Thread executing tasks from a given tasks queue\"\"\"\n@@ -175,34 +153,29 @@ def _handle_client(self, client, address):\n \n                 # If get has parameters ('?'), ignore them\n                 file_requested = file_requested.split('?')[0]\n-\n-                if not self.path_is_valid(file_requested):\n-                    response_data = '<html><body><center><h3>Error 400: Bad request<\/h3><p>' \\\n-                                    'Python HTTP Server<\/p><\/center><\/body><\/html>'.encode()\n-                    response_header = self._generate_headers(400)\n-                else:\n-                    if file_requested == \"\/\":\n-                        file_requested = \"\/index.html\"\n-                    elif file_requested == \"\/directory\/\":\n-                        file_requested = \"\/directory\/index.html\"\n-\n-                    filepath_to_serve = self.doc_root + file_requested\n-                    logging.info(\"Serving web page [{fp}]\".format(fp=filepath_to_serve))\n-\n-                # Load and Serve files content\n-                    try:\n-                        with open(filepath_to_serve, 'rb') as f:\n-                            if request_method == \"GET\":  # Read only for GET\n-                                response_data = f.read()\n-                        response_header = self._generate_headers(200, filepath_to_serve)\n-\n-                    except Exception as e:\n-                        logging.info(\"File not found. Serving 404 page.\")\n-                        response_header = self._generate_headers(404)\n-\n-                        if request_method == \"GET\":  # Temporary 404 Response Page\n-                            response_data = '<html><body><center><h3>Error 404: File not found<\/h3><p>' \\\n-                                            'Python HTTP Server<\/p><\/center><\/body><\/html>'.encode()\n+                file_requested = posixpath.normpath(file_requested)\n+                if file_requested == \"\/\":\n+                    file_requested = \"\/index.html\"\n+                elif file_requested == \"\/directory\/\":\n+                    file_requested = \"\/directory\/index.html\"\n+\n+                filepath_to_serve = self.doc_root + file_requested\n+                logging.info(\"Serving web page [{fp}]\".format(fp=filepath_to_serve))\n+\n+            # Load and Serve files content\n+                try:\n+                    with open(filepath_to_serve, 'rb') as f:\n+                        if request_method == \"GET\":  # Read only for GET\n+                            response_data = f.read()\n+                    response_header = self._generate_headers(200, filepath_to_serve)\n+\n+                except Exception as e:\n+                    logging.info(\"File not found. Serving 404 page.\")\n+                    response_header = self._generate_headers(404)\n+\n+                    if request_method == \"GET\":  # Temporary 404 Response Page\n+                        response_data = '<html><body><center><h3>Error 404: File not found<\/h3><p>' \\\n+                                        'Python HTTP Server<\/p><\/center><\/body><\/html>'.encode()\n                 response = response_header.encode()\n                 if request_method == \"GET\":\n                     response += response_data\n@@ -221,18 +194,6 @@ def _handle_client(self, client, address):\n                 logging.error(\"Unknown HTTP request method: {method}\".format(method=request_method))\n                 break\n \n-    def path_is_valid(self, path):\n-        \"\"\"Determine if a value is valid based on the provided matcher.\n-        :param str path:\n-            The path string to validate.\n-        :param matcher:\n-            Compiled regular expression to use to validate the value.\n-        :param require:\n-            Whether or not the value is required.\n-        \"\"\"\n-        return (path is not None\n-                and PATH_MATCHER.match(path))\n-\n \n def parse_args():\n     parser = argparse.ArgumentParser(\n","files":{"\/web-server\/httpd.py":{"changes":[{"diff":"\n import logging\n import mimetypes\n import os\n-import re\n+import posixpath\n import socket\n import sys\n import time\n","add":1,"remove":1,"filename":"\/web-server\/httpd.py","badparts":["import re"],"goodparts":["import posixpath"]},{"diff":"\n                     datefmt='%a %b %d %H:%M:%S %Y')\n \n-PCT_ENCODED = '%[A-Fa-f0-9]{2}'\n-SUB_DELIMITERS_RE = \"!$&'()\\*+,;=\"\n-UNRESERVED_RE = 'A-Za-z0-9._~\\-'\n-PCHAR = '([' + UNRESERVED_RE + SUB_DELIMITERS_RE + ':@]|%s)' % PCT_ENCODED\n-segments = {\n-    'segment': PCHAR + '*',\n-    # Non-zero length segment\n-    'segment-nz': PCHAR + '+',\n-    # Non-zero length segment without \":\"\n-    'segment-nz-nc': PCHAR.replace(':', '') + '+'\n-}\n-PATH_EMPTY = '^$'\n-PATH_ROOTLESS = '%(segment-nz)s(\/%(segment)s)*' % segments\n-PATH_NOSCHEME = '%(segment-nz-nc)s(\/%(segment)s)*' % segments\n-PATH_ABSOLUTE = '\/(%s)?' % PATH_ROOTLESS\n-PATH_ABEMPTY = '(\/%(segment)s)*' % segments\n-PATH_RE = '^(%s|%s|%s|%s|%s)$' % (\n-    PATH_ABEMPTY, PATH_ABSOLUTE, PATH_NOSCHEME, PATH_ROOTLESS, PATH_EMPTY\n-)\n-\n-PATH_MATCHER = re.compile(PATH_RE)\n-\n \n class Worker(Thread):\n     \"\"\"Thread executing tasks from a given tasks queue\"\"\"\n","add":0,"remove":22,"filename":"\/web-server\/httpd.py","badparts":["PCT_ENCODED = '%[A-Fa-f0-9]{2}'","SUB_DELIMITERS_RE = \"!$&'()\\*+,;=\"","UNRESERVED_RE = 'A-Za-z0-9._~\\-'","PCHAR = '([' + UNRESERVED_RE + SUB_DELIMITERS_RE + ':@]|%s)' % PCT_ENCODED","segments = {","    'segment': PCHAR + '*',","    'segment-nz': PCHAR + '+',","    'segment-nz-nc': PCHAR.replace(':', '') + '+'","}","PATH_EMPTY = '^$'","PATH_ROOTLESS = '%(segment-nz)s(\/%(segment)s)*' % segments","PATH_NOSCHEME = '%(segment-nz-nc)s(\/%(segment)s)*' % segments","PATH_ABSOLUTE = '\/(%s)?' % PATH_ROOTLESS","PATH_ABEMPTY = '(\/%(segment)s)*' % segments","PATH_RE = '^(%s|%s|%s|%s|%s)$' % (","    PATH_ABEMPTY, PATH_ABSOLUTE, PATH_NOSCHEME, PATH_ROOTLESS, PATH_EMPTY",")","PATH_MATCHER = re.compile(PATH_RE)"],"goodparts":[]},{"diff":"\n                 logging.error(\"Unknown HTTP request method: {method}\".format(method=request_method))\n                 break\n \n-    def path_is_valid(self, path):\n-        \"\"\"Determine if a value is valid based on the provided matcher.\n-        :param str path:\n-            The path string to validate.\n-        :param matcher:\n-            Compiled regular expression to use to validate the value.\n-        :param require:\n-            Whether or not the value is required.\n-        \"\"\"\n-        return (path is not None\n-                and PATH_MATCHER.match(path))\n-\n \n def parse_args():\n     parser = argparse.ArgumentParser(\n","add":0,"remove":12,"filename":"\/web-server\/httpd.py","badparts":["    def path_is_valid(self, path):","        \"\"\"Determine if a value is valid based on the provided matcher.","        :param str path:","            The path string to validate.","        :param matcher:","            Compiled regular expression to use to validate the value.","        :param require:","            Whether or not the value is required.","        \"\"\"","        return (path is not None","                and PATH_MATCHER.match(path))"],"goodparts":[]}],"source":"\nimport argparse import logging import mimetypes import os import re import socket import sys import time from queue import Queue from threading import Thread logging.basicConfig(format='[%(asctime)s] %(levelname)s %(message)s', level=logging.INFO, datefmt='%a %b %d %H:%M:%S %Y') PCT_ENCODED='%[A-Fa-f0-9]{2}' SUB_DELIMITERS_RE=\"!$&'()\\*+,;=\" UNRESERVED_RE='A-Za-z0-9._~\\-' PCHAR='([' +UNRESERVED_RE +SUB_DELIMITERS_RE +':@]|%s)' % PCT_ENCODED segments={ 'segment': PCHAR +'*', 'segment-nz': PCHAR +'+', 'segment-nz-nc': PCHAR.replace(':', '') +'+' } PATH_EMPTY='^$' PATH_ROOTLESS='%(segment-nz)s(\/%(segment)s)*' % segments PATH_NOSCHEME='%(segment-nz-nc)s(\/%(segment)s)*' % segments PATH_ABSOLUTE='\/(%s)?' % PATH_ROOTLESS PATH_ABEMPTY='(\/%(segment)s)*' % segments PATH_RE='^(%s|%s|%s|%s|%s)$' %( PATH_ABEMPTY, PATH_ABSOLUTE, PATH_NOSCHEME, PATH_ROOTLESS, PATH_EMPTY ) PATH_MATCHER=re.compile(PATH_RE) class Worker(Thread): \"\"\"Thread executing tasks from a given tasks queue\"\"\" def __init__(self, tasks): Thread.__init__(self) self.tasks=tasks self.daemon=True self.start() def run(self): while True: func, args, kargs=self.tasks.get() try: func(*args, **kargs) except Exception as e: logging.error(e) self.tasks.task_done() class ThreadPool: \"\"\"Pool of threads consuming tasks from a queue\"\"\" def __init__(self, num_threads): self.tasks=Queue(num_threads) for _ in range(num_threads): Worker(self.tasks) def add_task(self, func, *args, **kargs): \"\"\"Add a task to the queue\"\"\" self.tasks.put((func, args, kargs)) def wait_completion(self): \"\"\"Wait for completion of all the tasks in the queue\"\"\" self.tasks.join() class WebServer(ThreadPool): def __init__(self, port=8080, doc_root=\"DOCUMENT_ROOT\", workers=1): super().__init__(workers) self.host=socket.gethostname().split('.')[0] self.port=port self.doc_root=doc_root self.socket=socket.socket(socket.AF_INET, socket.SOCK_STREAM) self.workers=workers def start(self): \"\"\" Attempts to create and bind a socket to launch the server \"\"\" try: self.socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, True) logging.info(\"Starting server on{host}:{port}\".format(host=self.host, port=self.port)) self.socket.bind((self.host, int(self.port))) logging.info(\"Server started on port{port}.\".format(port=self.port)) except Exception as e: logging.info(\"Error: Could not bind to port{port}\".format(port=self.port)) logging.info(e) self.shutdown() sys.exit(1) else: self._listen() def shutdown(self): \"\"\" Shutdown server \"\"\" try: logging.info(\"Shutting down server\") self.socket.shutdown(socket.SHUT_RDWR) except Exception as e: pass def _generate_headers(self, response_code, request_file=\"\"): \"\"\" Generate HTTP response headers. Parameters: -response_code: HTTP response code to add to the header. 200 and 404 supported -request_file: Path to requested file Returns: A formatted HTTP header for the given response_code \"\"\" header='' if response_code==200: header +='HTTP\/1.1 200 OK\\n' mimetype=mimetypes.types_map[os.path.splitext(request_file)[1]] header +='Content-length:{}\\n'.format(str(os.path.getsize(request_file))) header +='Content-type:{}\\n'.format(mimetype) elif response_code==404: header +='HTTP\/1.1 404 Not Found\\n' elif response_code==400: header +='HTTP\/1.1 400 Bad request\\n' time_now=time.strftime(\"%a, %d %b %Y %H:%M:%S\", time.localtime()) header +='Date:{now}\\n'.format(now=time_now) header +='Server: Mikhail Samoylov for Otus Python Server\\n' header +='Connection: close\\n\\n' return header def _listen(self): \"\"\" Listens on self.port for any incoming connections \"\"\" self.socket.listen(5) while True: (client, address)=self.socket.accept() client.settimeout(60) logging.info(\"Recieved connection from{addr}\".format(addr=address)) self.add_task(self._handle_client, client, address) self.wait_completion() def _handle_client(self, client, address): \"\"\" Main loop for handling connecting clients and serving files from DOCUMENT_ROOT Parameters: -client: socket client from accept() -address: socket address from accept() \"\"\" PACKET_SIZE=1024 while True: logging.info(\"CLIENT:{c}\".format(c=client)) data=client.recv(PACKET_SIZE).decode() if not data: break request_method=data.split(' ')[0] logging.info(\"Method:{m}\".format(m=request_method)) logging.info(\"Request Body:{b}\".format(b=data)) if request_method==\"GET\" or request_method==\"HEAD\": file_requested=data.split(' ')[1] file_requested=file_requested.split('?')[0] if not self.path_is_valid(file_requested): response_data='<html><body><center><h3>Error 400: Bad request<\/h3><p>' \\ 'Python HTTP Server<\/p><\/center><\/body><\/html>'.encode() response_header=self._generate_headers(400) else: if file_requested==\"\/\": file_requested=\"\/index.html\" elif file_requested==\"\/directory\/\": file_requested=\"\/directory\/index.html\" filepath_to_serve=self.doc_root +file_requested logging.info(\"Serving web page[{fp}]\".format(fp=filepath_to_serve)) try: with open(filepath_to_serve, 'rb') as f: if request_method==\"GET\": response_data=f.read() response_header=self._generate_headers(200, filepath_to_serve) except Exception as e: logging.info(\"File not found. Serving 404 page.\") response_header=self._generate_headers(404) if request_method==\"GET\": response_data='<html><body><center><h3>Error 404: File not found<\/h3><p>' \\ 'Python HTTP Server<\/p><\/center><\/body><\/html>'.encode() response=response_header.encode() if request_method==\"GET\": response +=response_data client.send(response) client.close() break else: response_header=self._generate_headers(405) response_data='<html><body><center><h3>Error 405: Method not allowed<\/h3><p>' \\ 'Python HTTP Server<\/p><\/center><\/body><\/html>'.encode() response=response_header.encode() response +=response_data client.send(response) client.close() logging.error(\"Unknown HTTP request method:{method}\".format(method=request_method)) break def path_is_valid(self, path): \"\"\"Determine if a value is valid based on the provided matcher. :param str path: The path string to validate. :param matcher: Compiled regular expression to use to validate the value. :param require: Whether or not the value is required. \"\"\" return(path is not None and PATH_MATCHER.match(path)) def parse_args(): parser=argparse.ArgumentParser( description='Web Server For Education.') parser.add_argument(\"--p\", dest=\"port\", help=\"Server port.\", default=8080) parser.add_argument(\"--w\", dest=\"workers\", help=\"Workers count\", default=1) parser.add_argument(\"--r\", dest=\"doc_root\", help=\"Document Root directory.\", default=\"DOCUMENT_ROOT\") return parser.parse_args() def main(): options=parse_args() server=WebServer(options.port, options.doc_root, int(options.workers)) try: server.start() logging.info(\"Press Ctrl+C to shut down server.\") except KeyboardInterrupt: server.shutdown() logging.info(\"User stopped server process.\") if __name__==\"__main__\": main() ","sourceWithComments":"import argparse\nimport logging\nimport mimetypes\nimport os\nimport re\nimport socket\nimport sys\nimport time\n\nfrom queue import Queue\nfrom threading import Thread\n\n\nlogging.basicConfig(format='[%(asctime)s] %(levelname)s %(message)s', level=logging.INFO,\n\n                    datefmt='%a %b %d %H:%M:%S %Y')\n\nPCT_ENCODED = '%[A-Fa-f0-9]{2}'\nSUB_DELIMITERS_RE = \"!$&'()\\*+,;=\"\nUNRESERVED_RE = 'A-Za-z0-9._~\\-'\nPCHAR = '([' + UNRESERVED_RE + SUB_DELIMITERS_RE + ':@]|%s)' % PCT_ENCODED\nsegments = {\n    'segment': PCHAR + '*',\n    # Non-zero length segment\n    'segment-nz': PCHAR + '+',\n    # Non-zero length segment without \":\"\n    'segment-nz-nc': PCHAR.replace(':', '') + '+'\n}\nPATH_EMPTY = '^$'\nPATH_ROOTLESS = '%(segment-nz)s(\/%(segment)s)*' % segments\nPATH_NOSCHEME = '%(segment-nz-nc)s(\/%(segment)s)*' % segments\nPATH_ABSOLUTE = '\/(%s)?' % PATH_ROOTLESS\nPATH_ABEMPTY = '(\/%(segment)s)*' % segments\nPATH_RE = '^(%s|%s|%s|%s|%s)$' % (\n    PATH_ABEMPTY, PATH_ABSOLUTE, PATH_NOSCHEME, PATH_ROOTLESS, PATH_EMPTY\n)\n\nPATH_MATCHER = re.compile(PATH_RE)\n\n\nclass Worker(Thread):\n    \"\"\"Thread executing tasks from a given tasks queue\"\"\"\n\n    def __init__(self, tasks):\n        Thread.__init__(self)\n        self.tasks = tasks\n        self.daemon = True\n        self.start()\n\n    def run(self):\n        while True:\n            func, args, kargs = self.tasks.get()\n            try:\n                func(*args, **kargs)\n            except Exception as e:\n                logging.error(e)\n            self.tasks.task_done()\n\n\nclass ThreadPool:\n    \"\"\"Pool of threads consuming tasks from a queue\"\"\"\n\n    def __init__(self, num_threads):\n        self.tasks = Queue(num_threads)\n        for _ in range(num_threads):\n            Worker(self.tasks)\n\n    def add_task(self, func, *args, **kargs):\n        \"\"\"Add a task to the queue\"\"\"\n        self.tasks.put((func, args, kargs))\n\n    def wait_completion(self):\n        \"\"\"Wait for completion of all the tasks in the queue\"\"\"\n        self.tasks.join()\n\n\nclass WebServer(ThreadPool):\n\n    def __init__(self, port=8080, doc_root=\"DOCUMENT_ROOT\", workers=1):\n        super().__init__(workers)\n        self.host = socket.gethostname().split('.')[0]\n        self.port = port\n        self.doc_root = doc_root\n        self.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.workers = workers\n\n    def start(self):\n        \"\"\"\n        Attempts to create and bind a socket to launch the server\n        \"\"\"\n        try:\n            self.socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, True)\n            logging.info(\"Starting server on {host}:{port}\".format(host=self.host, port=self.port))\n            self.socket.bind((self.host, int(self.port)))\n            logging.info(\"Server started on port {port}.\".format(port=self.port))\n        except Exception as e:\n            logging.info(\"Error: Could not bind to port {port}\".format(port=self.port))\n            logging.info(e)\n            self.shutdown()\n            sys.exit(1)\n        else:\n            self._listen()  # Start listening for connections\n\n    def shutdown(self):\n        \"\"\"\n        Shutdown server\n        \"\"\"\n        try:\n            logging.info(\"Shutting down server\")\n            self.socket.shutdown(socket.SHUT_RDWR)\n        except Exception as e:\n            pass  # Pass if socket is already closed\n\n    def _generate_headers(self, response_code, request_file=\"\"):\n        \"\"\"\n        Generate HTTP response headers.\n        Parameters:\n            - response_code: HTTP response code to add to the header. 200 and 404 supported\n            - request_file: Path to requested file\n        Returns:\n            A formatted HTTP header for the given response_code\n        \"\"\"\n        header = ''\n        if response_code == 200:\n            header += 'HTTP\/1.1 200 OK\\n'\n            mimetype = mimetypes.types_map[os.path.splitext(request_file)[1]]\n            header += 'Content-length: {}\\n'.format(str(os.path.getsize(request_file)))\n            header += 'Content-type: {}\\n'.format(mimetype)\n\n        elif response_code == 404:\n            header += 'HTTP\/1.1 404 Not Found\\n'\n        elif response_code == 400:\n            header += 'HTTP\/1.1 400 Bad request\\n'\n        # How to count content length\n        time_now = time.strftime(\"%a, %d %b %Y %H:%M:%S\", time.localtime())\n        header += 'Date: {now}\\n'.format(now=time_now)\n        header += 'Server: Mikhail Samoylov for Otus Python Server\\n'\n        header += 'Connection: close\\n\\n'  # Signal that connection will be closed after completing the request\n        return header\n\n    def _listen(self):\n        \"\"\"\n        Listens on self.port for any incoming connections\n        \"\"\"\n        self.socket.listen(5)\n        while True:\n            (client, address) = self.socket.accept()\n            client.settimeout(60)\n            logging.info(\"Recieved connection from {addr}\".format(addr=address))\n            self.add_task(self._handle_client, client, address)\n            self.wait_completion()\n\n    def _handle_client(self, client, address):\n        \"\"\"\n        Main loop for handling connecting clients and serving files from DOCUMENT_ROOT\n        Parameters:\n            - client: socket client from accept()\n            - address: socket address from accept()\n        \"\"\"\n        PACKET_SIZE = 1024\n        while True:\n            logging.info(\"CLIENT: {c}\".format(c=client))\n            data = client.recv(PACKET_SIZE).decode()  # Recieve data packet from client and decode\n\n            if not data:\n                break\n\n            request_method = data.split(' ')[0]\n            logging.info(\"Method: {m}\".format(m=request_method))\n            logging.info(\"Request Body: {b}\".format(b=data))\n\n            if request_method == \"GET\" or request_method == \"HEAD\":\n                # Ex) \"GET \/index.html\" split on space\n                file_requested = data.split(' ')[1]\n\n                # If get has parameters ('?'), ignore them\n                file_requested = file_requested.split('?')[0]\n\n                if not self.path_is_valid(file_requested):\n                    response_data = '<html><body><center><h3>Error 400: Bad request<\/h3><p>' \\\n                                    'Python HTTP Server<\/p><\/center><\/body><\/html>'.encode()\n                    response_header = self._generate_headers(400)\n                else:\n                    if file_requested == \"\/\":\n                        file_requested = \"\/index.html\"\n                    elif file_requested == \"\/directory\/\":\n                        file_requested = \"\/directory\/index.html\"\n\n                    filepath_to_serve = self.doc_root + file_requested\n                    logging.info(\"Serving web page [{fp}]\".format(fp=filepath_to_serve))\n\n                # Load and Serve files content\n                    try:\n                        with open(filepath_to_serve, 'rb') as f:\n                            if request_method == \"GET\":  # Read only for GET\n                                response_data = f.read()\n                        response_header = self._generate_headers(200, filepath_to_serve)\n\n                    except Exception as e:\n                        logging.info(\"File not found. Serving 404 page.\")\n                        response_header = self._generate_headers(404)\n\n                        if request_method == \"GET\":  # Temporary 404 Response Page\n                            response_data = '<html><body><center><h3>Error 404: File not found<\/h3><p>' \\\n                                            'Python HTTP Server<\/p><\/center><\/body><\/html>'.encode()\n                response = response_header.encode()\n                if request_method == \"GET\":\n                    response += response_data\n\n                client.send(response)\n                client.close()\n                break\n            else:\n                response_header = self._generate_headers(405)\n                response_data = '<html><body><center><h3>Error 405: Method not allowed<\/h3><p>' \\\n                                    'Python HTTP Server<\/p><\/center><\/body><\/html>'.encode()\n                response = response_header.encode()\n                response += response_data\n                client.send(response)\n                client.close()\n                logging.error(\"Unknown HTTP request method: {method}\".format(method=request_method))\n                break\n\n    def path_is_valid(self, path):\n        \"\"\"Determine if a value is valid based on the provided matcher.\n        :param str path:\n            The path string to validate.\n        :param matcher:\n            Compiled regular expression to use to validate the value.\n        :param require:\n            Whether or not the value is required.\n        \"\"\"\n        return (path is not None\n                and PATH_MATCHER.match(path))\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        description='Web Server For Education.')\n    parser.add_argument(\"--p\", dest=\"port\", help=\"Server port.\",\n                        default=8080)\n    parser.add_argument(\"--w\", dest=\"workers\", help=\"Workers count\",\n                        default=1)\n    parser.add_argument(\"--r\", dest=\"doc_root\", help=\"Document Root directory.\",\n                        default=\"DOCUMENT_ROOT\")\n    return parser.parse_args()\n\n\ndef main():\n    options = parse_args()\n    server = WebServer(options.port, options.doc_root, int(options.workers))\n    try:\n        server.start()\n        logging.info(\"Press Ctrl+C to shut down server.\")\n    except KeyboardInterrupt:\n        server.shutdown()\n        logging.info(\"User stopped server process.\")\n\n\nif __name__ == \"__main__\":\n    main()\n"}},"msg":"Fix vulnerability directory traversal"}},"https:\/\/github.com\/aladdinwang\/django-filebrowser-no-grappelli":{"681868ea093d7c54ffb98865576497b1ba3912a1":{"url":"https:\/\/api.github.com\/repos\/aladdinwang\/django-filebrowser-no-grappelli\/commits\/681868ea093d7c54ffb98865576497b1ba3912a1","html_url":"https:\/\/github.com\/aladdinwang\/django-filebrowser-no-grappelli\/commit\/681868ea093d7c54ffb98865576497b1ba3912a1","message":"fix a browse directory traversal vulnerability","sha":"681868ea093d7c54ffb98865576497b1ba3912a1","keyword":"directory traversal vulnerable","diff":"diff --git a\/filebrowser\/views.py b\/filebrowser\/views.py\nindex ec0d713..e0d1d2e 100644\n--- a\/filebrowser\/views.py\n+++ b\/filebrowser\/views.py\n@@ -7,7 +7,7 @@\n # django imports\n from django.shortcuts import render_to_response, HttpResponse\n from django.template import RequestContext as Context\n-from django.http import HttpResponseRedirect\n+from django.http import HttpResponseRedirect, Http404\n from django.contrib.admin.views.decorators import staff_member_required\n from django.views.decorators.cache import never_cache\n from django.utils.translation import ugettext as _\n@@ -53,7 +53,15 @@ def browse(request):\n     query = request.GET.copy()\n     path = get_path(query.get('dir', ''))\n     directory = get_path('')\n-    \n+\n+    if path is not None:\n+        abs_path = os.path.abspath(os.path.join(\n+            fb_settings.MEDIA_ROOT, fb_settings.DIRECTORY, path))\n+        if not abs_path.startswith(os.path.abspath(os.path.join(\n+                fb_settings.MEDIA_ROOT, fb_settings.DIRECTORY))):\n+            # cause any attempt to leave media root directory to fail\n+            raise Http404\n+\n     if path is None:\n         msg = _('The requested Folder does not exist.')\n         messages.warning(request,message=msg)\n@@ -62,7 +70,6 @@ def browse(request):\n             raise ImproperlyConfigured, _(\"Error finding Upload-Folder. Maybe it does not exist?\")\n         redirect_url = reverse(\"fb_browse\") + query_helper(query, \"\", \"dir\")\n         return HttpResponseRedirect(redirect_url)\n-    abs_path = os.path.join(fb_settings.MEDIA_ROOT, fb_settings.DIRECTORY, path)\n     \n     # INITIAL VARIABLES\n     results_var = {'results_total': 0, 'results_current': 0, 'delete_total': 0, 'images_total': 0, 'select_total': 0 }\n","files":{"\/filebrowser\/views.py":{"changes":[{"diff":"\n # django imports\n from django.shortcuts import render_to_response, HttpResponse\n from django.template import RequestContext as Context\n-from django.http import HttpResponseRedirect\n+from django.http import HttpResponseRedirect, Http404\n from django.contrib.admin.views.decorators import staff_member_required\n from django.views.decorators.cache import never_cache\n from django.utils.translation import ugettext as _\n","add":1,"remove":1,"filename":"\/filebrowser\/views.py","badparts":["from django.http import HttpResponseRedirect"],"goodparts":["from django.http import HttpResponseRedirect, Http404"]},{"diff":"\n             raise ImproperlyConfigured, _(\"Error finding Upload-Folder. Maybe it does not exist?\")\n         redirect_url = reverse(\"fb_browse\") + query_helper(query, \"\", \"dir\")\n         return HttpResponseRedirect(redirect_url)\n-    abs_path = os.path.join(fb_settings.MEDIA_ROOT, fb_settings.DIRECTORY, path)\n     \n     # INITIAL VARIABLES\n     results_var = {'results_total': 0, 'results_current': 0, 'delete_total': 0, 'images_total': 0, 'select_total': 0 }\n","add":0,"remove":1,"filename":"\/filebrowser\/views.py","badparts":["    abs_path = os.path.join(fb_settings.MEDIA_ROOT, fb_settings.DIRECTORY, path)"],"goodparts":[]}],"source":"\n import os, re from time import gmtime, strftime from django.shortcuts import render_to_response, HttpResponse from django.template import RequestContext as Context from django.http import HttpResponseRedirect from django.contrib.admin.views.decorators import staff_member_required from django.views.decorators.cache import never_cache from django.utils.translation import ugettext as _ from django.conf import settings from django import forms from django.core.urlresolvers import reverse from django.core.exceptions import ImproperlyConfigured from django.dispatch import Signal from django.core.paginator import Paginator, InvalidPage, EmptyPage from django.utils.encoding import smart_str try: from django.views.decorators.csrf import csrf_exempt except: from django.contrib.csrf.middleware import csrf_exempt from django.contrib import messages from filebrowser.settings import * from filebrowser.conf import fb_settings from filebrowser.functions import path_to_url, sort_by_attr, get_path, get_file, get_version_path, get_breadcrumbs, get_filterdate, get_settings_var, handle_file_upload, convert_filename from filebrowser.templatetags.fb_tags import query_helper from filebrowser.base import FileObject from filebrowser.decorators import flash_login_required filter_re=[] for exp in EXCLUDE: filter_re.append(re.compile(exp)) for k,v in VERSIONS.iteritems(): exp=(r'_%s.(%s)') %(k, '|'.join(EXTENSION_LIST)) filter_re.append(re.compile(exp)) def browse(request): \"\"\" Browse Files\/Directories. \"\"\" query=request.GET.copy() path=get_path(query.get('dir', '')) directory=get_path('') if path is None: msg=_('The requested Folder does not exist.') messages.warning(request,message=msg) if directory is None: raise ImproperlyConfigured, _(\"Error finding Upload-Folder. Maybe it does not exist?\") redirect_url=reverse(\"fb_browse\") +query_helper(query, \"\", \"dir\") return HttpResponseRedirect(redirect_url) abs_path=os.path.join(fb_settings.MEDIA_ROOT, fb_settings.DIRECTORY, path) results_var={'results_total': 0, 'results_current': 0, 'delete_total': 0, 'images_total': 0, 'select_total': 0} counter={} for k,v in EXTENSIONS.iteritems(): counter[k]=0 dir_list=os.listdir(abs_path) files=[] for file in dir_list: filtered=file.startswith('.') for re_prefix in filter_re: if re_prefix.search(file): filtered=True if filtered: continue results_var['results_total'] +=1 fileobject=FileObject(os.path.join(fb_settings.DIRECTORY, path, file)) append=False if fileobject.filetype==request.GET.get('filter_type', fileobject.filetype) and get_filterdate(request.GET.get('filter_date', ''), fileobject.date): append=True if request.GET.get('q') and not re.compile(request.GET.get('q').lower(), re.M).search(file.lower()): append=False if append: try: if fileobject.filetype=='Image': results_var['images_total'] +=1 if fileobject.filetype !='Folder': results_var['delete_total'] +=1 elif fileobject.filetype=='Folder' and fileobject.is_empty: results_var['delete_total'] +=1 if query.get('type') and query.get('type') in SELECT_FORMATS and fileobject.filetype in SELECT_FORMATS[query.get('type')]: results_var['select_total'] +=1 elif not query.get('type'): results_var['select_total'] +=1 except OSError: continue else: files.append(fileobject) results_var['results_current'] +=1 if fileobject.filetype: counter[fileobject.filetype] +=1 query['o']=request.GET.get('o', DEFAULT_SORTING_BY) query['ot']=request.GET.get('ot', DEFAULT_SORTING_ORDER) files=sort_by_attr(files, request.GET.get('o', DEFAULT_SORTING_BY)) if not request.GET.get('ot') and DEFAULT_SORTING_ORDER==\"desc\" or request.GET.get('ot')==\"desc\": files.reverse() p=Paginator(files, LIST_PER_PAGE) try: page_nr=request.GET.get('p', '1') except: page_nr=1 try: page=p.page(page_nr) except(EmptyPage, InvalidPage): page=p.page(p.num_pages) return render_to_response('filebrowser\/index.html',{ 'dir': path, 'p': p, 'page': page, 'results_var': results_var, 'counter': counter, 'query': query, 'title': _(u'FileBrowser'), 'settings_var': get_settings_var(), 'breadcrumbs': get_breadcrumbs(query, path), 'breadcrumbs_title': \"\" }, context_instance=Context(request)) browse=staff_member_required(never_cache(browse)) filebrowser_pre_createdir=Signal(providing_args=[\"path\", \"dirname\"]) filebrowser_post_createdir=Signal(providing_args=[\"path\", \"dirname\"]) def mkdir(request): \"\"\" Make Directory. \"\"\" from filebrowser.forms import MakeDirForm query=request.GET path=get_path(query.get('dir', '')) if path is None: msg=_('The requested Folder does not exist.') messages.warning(request,message=msg) return HttpResponseRedirect(reverse(\"fb_browse\")) abs_path=os.path.join(fb_settings.MEDIA_ROOT, fb_settings.DIRECTORY, path) if request.method=='POST': form=MakeDirForm(abs_path, request.POST) if form.is_valid(): server_path=os.path.join(abs_path, form.cleaned_data['dir_name']) try: filebrowser_pre_createdir.send(sender=request, path=path, dirname=form.cleaned_data['dir_name']) os.mkdir(server_path) os.chmod(server_path, 0775) filebrowser_post_createdir.send(sender=request, path=path, dirname=form.cleaned_data['dir_name']) msg=_('The Folder %s was successfully created.') %(form.cleaned_data['dir_name']) messages.success(request,message=msg) redirect_url=reverse(\"fb_browse\") +query_helper(query, \"ot=desc,o=date\", \"ot,o,filter_type,filter_date,q,p\") return HttpResponseRedirect(redirect_url) except OSError,(errno, strerror): if errno==13: form.errors['dir_name']=forms.util.ErrorList([_('Permission denied.')]) else: form.errors['dir_name']=forms.util.ErrorList([_('Error creating folder.')]) else: form=MakeDirForm(abs_path) return render_to_response('filebrowser\/makedir.html',{ 'form': form, 'query': query, 'title': _(u'New Folder'), 'settings_var': get_settings_var(), 'breadcrumbs': get_breadcrumbs(query, path), 'breadcrumbs_title': _(u'New Folder') }, context_instance=Context(request)) mkdir=staff_member_required(never_cache(mkdir)) def upload(request): \"\"\" Multipe File Upload. \"\"\" from django.http import parse_cookie query=request.GET path=get_path(query.get('dir', '')) if path is None: msg=_('The requested Folder does not exist.') messages.warning(request,message=msg) return HttpResponseRedirect(reverse(\"fb_browse\")) abs_path=os.path.join(fb_settings.MEDIA_ROOT, fb_settings.DIRECTORY, path) cookie_dict=parse_cookie(request.META.get('HTTP_COOKIE', '')) engine=__import__(settings.SESSION_ENGINE,{},{},['']) session_key=cookie_dict.get(settings.SESSION_COOKIE_NAME, None) return render_to_response('filebrowser\/upload.html',{ 'query': query, 'title': _(u'Select files to upload'), 'settings_var': get_settings_var(), 'session_key': session_key, 'breadcrumbs': get_breadcrumbs(query, path), 'breadcrumbs_title': _(u'Upload') }, context_instance=Context(request)) upload=staff_member_required(never_cache(upload)) @csrf_exempt def _check_file(request): \"\"\" Check if file already exists on the server. \"\"\" from django.utils import simplejson folder=request.POST.get('folder') fb_uploadurl_re=re.compile(r'^.*(%s)' % reverse(\"fb_upload\")) folder=fb_uploadurl_re.sub('', folder) fileArray={} if request.method=='POST': for k,v in request.POST.items(): if k !=\"folder\": v=convert_filename(v) if os.path.isfile(smart_str(os.path.join(fb_settings.MEDIA_ROOT, fb_settings.DIRECTORY, folder, v))): fileArray[k]=v return HttpResponse(simplejson.dumps(fileArray)) filebrowser_pre_upload=Signal(providing_args=[\"path\", \"file\"]) filebrowser_post_upload=Signal(providing_args=[\"path\", \"file\"]) @csrf_exempt @flash_login_required def _upload_file(request): \"\"\" Upload file to the server. \"\"\" from django.core.files.move import file_move_safe if request.method=='POST': folder=request.POST.get('folder') fb_uploadurl_re=re.compile(r'^.*(%s)' % reverse(\"fb_upload\")) folder=fb_uploadurl_re.sub('', folder) abs_path=os.path.join(fb_settings.MEDIA_ROOT, fb_settings.DIRECTORY, folder) if request.FILES: filedata=request.FILES['Filedata'] filedata.name=convert_filename(filedata.name) filebrowser_pre_upload.send(sender=request, path=request.POST.get('folder'), file=filedata) uploadedfile=handle_file_upload(abs_path, filedata) if os.path.isfile(smart_str(os.path.join(fb_settings.MEDIA_ROOT, fb_settings.DIRECTORY, folder, filedata.name))): old_file=smart_str(os.path.join(abs_path, filedata.name)) new_file=smart_str(os.path.join(abs_path, uploadedfile)) file_move_safe(new_file, old_file) filebrowser_post_upload.send(sender=request, path=request.POST.get('folder'), file=FileObject(smart_str(os.path.join(fb_settings.DIRECTORY, folder, filedata.name)))) return HttpResponse('True') filebrowser_pre_delete=Signal(providing_args=[\"path\", \"filename\"]) filebrowser_post_delete=Signal(providing_args=[\"path\", \"filename\"]) def delete(request): \"\"\" Delete existing File\/Directory. When trying to delete a Directory, the Directory has to be empty. \"\"\" query=request.GET path=get_path(query.get('dir', '')) filename=get_file(query.get('dir', ''), query.get('filename', '')) if path is None or filename is None: if path is None: msg=_('The requested Folder does not exist.') else: msg=_('The requested File does not exist.') messages.warning(request,message=msg) return HttpResponseRedirect(reverse(\"fb_browse\")) abs_path=os.path.join(fb_settings.MEDIA_ROOT, fb_settings.DIRECTORY, path) msg=\"\" if request.GET: if request.GET.get('filetype') !=\"Folder\": relative_server_path=os.path.join(fb_settings.DIRECTORY, path, filename) try: filebrowser_pre_delete.send(sender=request, path=path, filename=filename) for version in VERSIONS: try: os.unlink(os.path.join(fb_settings.MEDIA_ROOT, get_version_path(relative_server_path, version))) except: pass os.unlink(smart_str(os.path.join(abs_path, filename))) filebrowser_post_delete.send(sender=request, path=path, filename=filename) msg=_('The file %s was successfully deleted.') %(filename.lower()) messages.success(request,message=msg) redirect_url=reverse(\"fb_browse\") +query_helper(query, \"\", \"filename,filetype\") return HttpResponseRedirect(redirect_url) except OSError: msg=OSError else: try: filebrowser_pre_delete.send(sender=request, path=path, filename=filename) os.rmdir(os.path.join(abs_path, filename)) filebrowser_post_delete.send(sender=request, path=path, filename=filename) msg=_('The folder %s was successfully deleted.') %(filename.lower()) messages.success(request,message=msg) redirect_url=reverse(\"fb_browse\") +query_helper(query, \"\", \"filename,filetype\") return HttpResponseRedirect(redirect_url) except OSError: msg=OSError if msg: request.user.message_set.create(message=msg) return render_to_response('filebrowser\/index.html',{ 'dir': dir_name, 'file': request.GET.get('filename', ''), 'query': query, 'settings_var': get_settings_var(), 'breadcrumbs': get_breadcrumbs(query, dir_name), 'breadcrumbs_title': \"\" }, context_instance=Context(request)) delete=staff_member_required(never_cache(delete)) filebrowser_pre_rename=Signal(providing_args=[\"path\", \"filename\", \"new_filename\"]) filebrowser_post_rename=Signal(providing_args=[\"path\", \"filename\", \"new_filename\"]) def rename(request): \"\"\" Rename existing File\/Directory. Includes renaming existing Image Versions\/Thumbnails. \"\"\" from filebrowser.forms import RenameForm query=request.GET path=get_path(query.get('dir', '')) filename=get_file(query.get('dir', ''), query.get('filename', '')) if path is None or filename is None: if path is None: msg=_('The requested Folder does not exist.') else: msg=_('The requested File does not exist.') messages.warning(request,message=msg) return HttpResponseRedirect(reverse(\"fb_browse\")) abs_path=os.path.join(fb_settings.MEDIA_ROOT, fb_settings.DIRECTORY, path) file_extension=os.path.splitext(filename)[1].lower() if request.method=='POST': form=RenameForm(abs_path, file_extension, request.POST) if form.is_valid(): relative_server_path=os.path.join(fb_settings.DIRECTORY, path, filename) new_filename=form.cleaned_data['name'] +file_extension new_relative_server_path=os.path.join(fb_settings.DIRECTORY, path, new_filename) try: filebrowser_pre_rename.send(sender=request, path=path, filename=filename, new_filename=new_filename) for version in VERSIONS: try: os.unlink(os.path.join(fb_settings.MEDIA_ROOT, get_version_path(relative_server_path, version))) except: pass os.rename(os.path.join(fb_settings.MEDIA_ROOT, relative_server_path), os.path.join(fb_settings.MEDIA_ROOT, new_relative_server_path)) filebrowser_post_rename.send(sender=request, path=path, filename=filename, new_filename=new_filename) msg=_('Renaming was successful.') messages.success(request,message=msg) redirect_url=reverse(\"fb_browse\") +query_helper(query, \"\", \"filename\") return HttpResponseRedirect(redirect_url) except OSError,(errno, strerror): form.errors['name']=forms.util.ErrorList([_('Error.')]) else: form=RenameForm(abs_path, file_extension) return render_to_response('filebrowser\/rename.html',{ 'form': form, 'query': query, 'file_extension': file_extension, 'title': _(u'Rename \"%s\"') % filename, 'settings_var': get_settings_var(), 'breadcrumbs': get_breadcrumbs(query, path), 'breadcrumbs_title': _(u'Rename') }, context_instance=Context(request)) rename=staff_member_required(never_cache(rename)) def versions(request): \"\"\" Show all Versions for an Image according to ADMIN_VERSIONS. \"\"\" query=request.GET path=get_path(query.get('dir', '')) filename=get_file(query.get('dir', ''), query.get('filename', '')) if path is None or filename is None: if path is None: msg=_('The requested Folder does not exist.') else: msg=_('The requested File does not exist.') messages.warning(request,message=msg) return HttpResponseRedirect(reverse(\"fb_browse\")) abs_path=os.path.join(fb_settings.MEDIA_ROOT, fb_settings.DIRECTORY, path) return render_to_response('filebrowser\/versions.html',{ 'original': path_to_url(os.path.join(fb_settings.DIRECTORY, path, filename)), 'query': query, 'title': _(u'Versions for \"%s\"') % filename, 'settings_var': get_settings_var(), 'breadcrumbs': get_breadcrumbs(query, path), 'breadcrumbs_title': _(u'Versions for \"%s\"') % filename }, context_instance=Context(request)) versions=staff_member_required(never_cache(versions)) ","sourceWithComments":"# coding: utf-8\n\n# general imports\nimport os, re\nfrom time import gmtime, strftime\n\n# django imports\nfrom django.shortcuts import render_to_response, HttpResponse\nfrom django.template import RequestContext as Context\nfrom django.http import HttpResponseRedirect\nfrom django.contrib.admin.views.decorators import staff_member_required\nfrom django.views.decorators.cache import never_cache\nfrom django.utils.translation import ugettext as _\nfrom django.conf import settings\nfrom django import forms\nfrom django.core.urlresolvers import reverse\nfrom django.core.exceptions import ImproperlyConfigured\nfrom django.dispatch import Signal\nfrom django.core.paginator import Paginator, InvalidPage, EmptyPage\nfrom django.utils.encoding import smart_str\n\ntry:\n    # django SVN\n    from django.views.decorators.csrf import csrf_exempt\nexcept:\n    # django 1.1\n    from django.contrib.csrf.middleware import csrf_exempt\n\nfrom django.contrib import messages\n\n# filebrowser imports\nfrom filebrowser.settings import *\nfrom filebrowser.conf import fb_settings\nfrom filebrowser.functions import path_to_url, sort_by_attr, get_path, get_file, get_version_path, get_breadcrumbs, get_filterdate, get_settings_var, handle_file_upload, convert_filename\nfrom filebrowser.templatetags.fb_tags import query_helper\nfrom filebrowser.base import FileObject\nfrom filebrowser.decorators import flash_login_required\n\n# Precompile regular expressions\nfilter_re = []\nfor exp in EXCLUDE:\n   filter_re.append(re.compile(exp))\nfor k,v in VERSIONS.iteritems():\n    exp = (r'_%s.(%s)') % (k, '|'.join(EXTENSION_LIST))\n    filter_re.append(re.compile(exp))\n\n\ndef browse(request):\n    \"\"\"\n    Browse Files\/Directories.\n    \"\"\"\n    # QUERY \/ PATH CHECK\n    query = request.GET.copy()\n    path = get_path(query.get('dir', ''))\n    directory = get_path('')\n    \n    if path is None:\n        msg = _('The requested Folder does not exist.')\n        messages.warning(request,message=msg)\n        if directory is None:\n            # The DIRECTORY does not exist, raise an error to prevent eternal redirecting.\n            raise ImproperlyConfigured, _(\"Error finding Upload-Folder. Maybe it does not exist?\")\n        redirect_url = reverse(\"fb_browse\") + query_helper(query, \"\", \"dir\")\n        return HttpResponseRedirect(redirect_url)\n    abs_path = os.path.join(fb_settings.MEDIA_ROOT, fb_settings.DIRECTORY, path)\n    \n    # INITIAL VARIABLES\n    results_var = {'results_total': 0, 'results_current': 0, 'delete_total': 0, 'images_total': 0, 'select_total': 0 }\n    counter = {}\n    for k,v in EXTENSIONS.iteritems():\n        counter[k] = 0\n    \n    dir_list = os.listdir(abs_path)\n    files = []\n    for file in dir_list:\n        \n        # EXCLUDE FILES MATCHING VERSIONS_PREFIX OR ANY OF THE EXCLUDE PATTERNS\n        filtered = file.startswith('.')\n        for re_prefix in filter_re:\n            if re_prefix.search(file):\n                filtered = True\n        if filtered:\n            continue\n        results_var['results_total'] += 1\n        \n        # CREATE FILEOBJECT\n        fileobject = FileObject(os.path.join(fb_settings.DIRECTORY, path, file))\n        \n        # FILTER \/ SEARCH\n        append = False\n        if fileobject.filetype == request.GET.get('filter_type', fileobject.filetype) and get_filterdate(request.GET.get('filter_date', ''), fileobject.date):\n            append = True\n        if request.GET.get('q') and not re.compile(request.GET.get('q').lower(), re.M).search(file.lower()):\n            append = False\n        \n        # APPEND FILE_LIST\n        if append:\n            try:\n                # COUNTER\/RESULTS\n                if fileobject.filetype == 'Image':\n                    results_var['images_total'] += 1\n                if fileobject.filetype != 'Folder':\n                    results_var['delete_total'] += 1\n                elif fileobject.filetype == 'Folder' and fileobject.is_empty:\n                    results_var['delete_total'] += 1\n                if query.get('type') and query.get('type') in SELECT_FORMATS and fileobject.filetype in SELECT_FORMATS[query.get('type')]:\n                    results_var['select_total'] += 1\n                elif not query.get('type'):\n                    results_var['select_total'] += 1\n            except OSError:\n                # Ignore items that have problems\n                continue\n            else:\n                files.append(fileobject)\n                results_var['results_current'] += 1\n        \n        # COUNTER\/RESULTS\n        if fileobject.filetype:\n            counter[fileobject.filetype] += 1\n    \n    # SORTING\n    query['o'] = request.GET.get('o', DEFAULT_SORTING_BY)\n    query['ot'] = request.GET.get('ot', DEFAULT_SORTING_ORDER)\n    files = sort_by_attr(files, request.GET.get('o', DEFAULT_SORTING_BY))\n    if not request.GET.get('ot') and DEFAULT_SORTING_ORDER == \"desc\" or request.GET.get('ot') == \"desc\":\n        files.reverse()\n    \n    p = Paginator(files, LIST_PER_PAGE)\n    try:\n        page_nr = request.GET.get('p', '1')\n    except:\n        page_nr = 1\n    try:\n        page = p.page(page_nr)\n    except (EmptyPage, InvalidPage):\n        page = p.page(p.num_pages)\n    \n    return render_to_response('filebrowser\/index.html', {\n        'dir': path,\n        'p': p,\n        'page': page,\n        'results_var': results_var,\n        'counter': counter,\n        'query': query,\n        'title': _(u'FileBrowser'),\n        'settings_var': get_settings_var(),\n        'breadcrumbs': get_breadcrumbs(query, path),\n        'breadcrumbs_title': \"\"\n    }, context_instance=Context(request))\nbrowse = staff_member_required(never_cache(browse))\n\n\n# mkdir signals\nfilebrowser_pre_createdir = Signal(providing_args=[\"path\", \"dirname\"])\nfilebrowser_post_createdir = Signal(providing_args=[\"path\", \"dirname\"])\n\ndef mkdir(request):\n    \"\"\"\n    Make Directory.\n    \"\"\"\n    \n    from filebrowser.forms import MakeDirForm\n    \n    # QUERY \/ PATH CHECK\n    query = request.GET\n    path = get_path(query.get('dir', ''))\n    if path is None:\n        msg = _('The requested Folder does not exist.')\n        messages.warning(request,message=msg)\n        return HttpResponseRedirect(reverse(\"fb_browse\"))\n    abs_path = os.path.join(fb_settings.MEDIA_ROOT, fb_settings.DIRECTORY, path)\n    \n    if request.method == 'POST':\n        form = MakeDirForm(abs_path, request.POST)\n        if form.is_valid():\n            server_path = os.path.join(abs_path, form.cleaned_data['dir_name'])\n            try:\n                # PRE CREATE SIGNAL\n                filebrowser_pre_createdir.send(sender=request, path=path, dirname=form.cleaned_data['dir_name'])\n                # CREATE FOLDER\n                os.mkdir(server_path)\n                os.chmod(server_path, 0775)\n                # POST CREATE SIGNAL\n                filebrowser_post_createdir.send(sender=request, path=path, dirname=form.cleaned_data['dir_name'])\n                # MESSAGE & REDIRECT\n                msg = _('The Folder %s was successfully created.') % (form.cleaned_data['dir_name'])\n                messages.success(request,message=msg)\n                # on redirect, sort by date desc to see the new directory on top of the list\n                # remove filter in order to actually _see_ the new folder\n                # remove pagination\n                redirect_url = reverse(\"fb_browse\") + query_helper(query, \"ot=desc,o=date\", \"ot,o,filter_type,filter_date,q,p\")\n                return HttpResponseRedirect(redirect_url)\n            except OSError, (errno, strerror):\n                if errno == 13:\n                    form.errors['dir_name'] = forms.util.ErrorList([_('Permission denied.')])\n                else:\n                    form.errors['dir_name'] = forms.util.ErrorList([_('Error creating folder.')])\n    else:\n        form = MakeDirForm(abs_path)\n    \n    return render_to_response('filebrowser\/makedir.html', {\n        'form': form,\n        'query': query,\n        'title': _(u'New Folder'),\n        'settings_var': get_settings_var(),\n        'breadcrumbs': get_breadcrumbs(query, path),\n        'breadcrumbs_title': _(u'New Folder')\n    }, context_instance=Context(request))\nmkdir = staff_member_required(never_cache(mkdir))\n\n\ndef upload(request):\n    \"\"\"\n    Multipe File Upload.\n    \"\"\"\n    \n    from django.http import parse_cookie\n    \n    # QUERY \/ PATH CHECK\n    query = request.GET\n    path = get_path(query.get('dir', ''))\n    if path is None:\n        msg = _('The requested Folder does not exist.')\n        messages.warning(request,message=msg)\n        return HttpResponseRedirect(reverse(\"fb_browse\"))\n    abs_path = os.path.join(fb_settings.MEDIA_ROOT, fb_settings.DIRECTORY, path)\n    \n    # SESSION (used for flash-uploading)\n    cookie_dict = parse_cookie(request.META.get('HTTP_COOKIE', ''))\n    engine = __import__(settings.SESSION_ENGINE, {}, {}, [''])\n    session_key = cookie_dict.get(settings.SESSION_COOKIE_NAME, None)\n    \n    return render_to_response('filebrowser\/upload.html', {\n        'query': query,\n        'title': _(u'Select files to upload'),\n        'settings_var': get_settings_var(),\n        'session_key': session_key,\n        'breadcrumbs': get_breadcrumbs(query, path),\n        'breadcrumbs_title': _(u'Upload')\n    }, context_instance=Context(request))\nupload = staff_member_required(never_cache(upload))\n\n\n@csrf_exempt\ndef _check_file(request):\n    \"\"\"\n    Check if file already exists on the server.\n    \"\"\"\n    \n    from django.utils import simplejson\n    \n    folder = request.POST.get('folder')\n    fb_uploadurl_re = re.compile(r'^.*(%s)' % reverse(\"fb_upload\"))\n    folder = fb_uploadurl_re.sub('', folder)\n    \n    fileArray = {}\n    if request.method == 'POST':\n        for k,v in request.POST.items():\n            if k != \"folder\":\n                v = convert_filename(v)\n                if os.path.isfile(smart_str(os.path.join(fb_settings.MEDIA_ROOT, fb_settings.DIRECTORY, folder, v))):\n                    fileArray[k] = v\n    \n    return HttpResponse(simplejson.dumps(fileArray))\n\n\n# upload signals\nfilebrowser_pre_upload = Signal(providing_args=[\"path\", \"file\"])\nfilebrowser_post_upload = Signal(providing_args=[\"path\", \"file\"])\n\n@csrf_exempt\n@flash_login_required\ndef _upload_file(request):\n    \"\"\"\n    Upload file to the server.\n    \"\"\"\n    \n    from django.core.files.move import file_move_safe\n    \n    if request.method == 'POST':\n        folder = request.POST.get('folder')\n        fb_uploadurl_re = re.compile(r'^.*(%s)' % reverse(\"fb_upload\"))\n        folder = fb_uploadurl_re.sub('', folder)\n        abs_path = os.path.join(fb_settings.MEDIA_ROOT, fb_settings.DIRECTORY, folder)\n        if request.FILES:\n            filedata = request.FILES['Filedata']\n            filedata.name = convert_filename(filedata.name)\n            # PRE UPLOAD SIGNAL\n            filebrowser_pre_upload.send(sender=request, path=request.POST.get('folder'), file=filedata)\n            # HANDLE UPLOAD\n            uploadedfile = handle_file_upload(abs_path, filedata)\n            # MOVE UPLOADED FILE\n            # if file already exists\n            if os.path.isfile(smart_str(os.path.join(fb_settings.MEDIA_ROOT, fb_settings.DIRECTORY, folder, filedata.name))):\n                old_file = smart_str(os.path.join(abs_path, filedata.name))\n                new_file = smart_str(os.path.join(abs_path, uploadedfile))\n                file_move_safe(new_file, old_file)\n            # POST UPLOAD SIGNAL\n            filebrowser_post_upload.send(sender=request, path=request.POST.get('folder'), file=FileObject(smart_str(os.path.join(fb_settings.DIRECTORY, folder, filedata.name))))\n    return HttpResponse('True')\n#_upload_file = flash_login_required(_upload_file)\n\n\n# delete signals\nfilebrowser_pre_delete = Signal(providing_args=[\"path\", \"filename\"])\nfilebrowser_post_delete = Signal(providing_args=[\"path\", \"filename\"])\n\ndef delete(request):\n    \"\"\"\n    Delete existing File\/Directory.\n    \n    When trying to delete a Directory, the Directory has to be empty.\n    \"\"\"\n    \n    # QUERY \/ PATH CHECK\n    query = request.GET\n    path = get_path(query.get('dir', ''))\n    filename = get_file(query.get('dir', ''), query.get('filename', ''))\n    if path is None or filename is None:\n        if path is None:\n            msg = _('The requested Folder does not exist.')\n        else:\n            msg = _('The requested File does not exist.')\n        messages.warning(request,message=msg)\n        return HttpResponseRedirect(reverse(\"fb_browse\"))\n    abs_path = os.path.join(fb_settings.MEDIA_ROOT, fb_settings.DIRECTORY, path)\n    \n    msg = \"\"\n    if request.GET:\n        if request.GET.get('filetype') != \"Folder\":\n            relative_server_path = os.path.join(fb_settings.DIRECTORY, path, filename)\n            try:\n                # PRE DELETE SIGNAL\n                filebrowser_pre_delete.send(sender=request, path=path, filename=filename)\n                # DELETE IMAGE VERSIONS\/THUMBNAILS\n                for version in VERSIONS:\n                    try:\n                        os.unlink(os.path.join(fb_settings.MEDIA_ROOT, get_version_path(relative_server_path, version)))\n                    except:\n                        pass\n                # DELETE FILE\n                os.unlink(smart_str(os.path.join(abs_path, filename)))\n                # POST DELETE SIGNAL\n                filebrowser_post_delete.send(sender=request, path=path, filename=filename)\n                # MESSAGE & REDIRECT\n                msg = _('The file %s was successfully deleted.') % (filename.lower())\n                messages.success(request,message=msg)\n                redirect_url = reverse(\"fb_browse\") + query_helper(query, \"\", \"filename,filetype\")\n                return HttpResponseRedirect(redirect_url)\n            except OSError:\n                # todo: define error message\n                msg = OSError\n        else:\n            try:\n                # PRE DELETE SIGNAL\n                filebrowser_pre_delete.send(sender=request, path=path, filename=filename)\n                # DELETE FOLDER\n                os.rmdir(os.path.join(abs_path, filename))\n                # POST DELETE SIGNAL\n                filebrowser_post_delete.send(sender=request, path=path, filename=filename)\n                # MESSAGE & REDIRECT\n                msg = _('The folder %s was successfully deleted.') % (filename.lower())\n                messages.success(request,message=msg)\n                redirect_url = reverse(\"fb_browse\") + query_helper(query, \"\", \"filename,filetype\")\n                return HttpResponseRedirect(redirect_url)\n            except OSError:\n                # todo: define error message\n                msg = OSError\n    \n    if msg:\n        request.user.message_set.create(message=msg)\n    \n    return render_to_response('filebrowser\/index.html', {\n        'dir': dir_name,\n        'file': request.GET.get('filename', ''),\n        'query': query,\n        'settings_var': get_settings_var(),\n        'breadcrumbs': get_breadcrumbs(query, dir_name),\n        'breadcrumbs_title': \"\"\n    }, context_instance=Context(request))\ndelete = staff_member_required(never_cache(delete))\n\n\n# rename signals\nfilebrowser_pre_rename = Signal(providing_args=[\"path\", \"filename\", \"new_filename\"])\nfilebrowser_post_rename = Signal(providing_args=[\"path\", \"filename\", \"new_filename\"])\n\ndef rename(request):\n    \"\"\"\n    Rename existing File\/Directory.\n    \n    Includes renaming existing Image Versions\/Thumbnails.\n    \"\"\"\n    \n    from filebrowser.forms import RenameForm\n    \n    # QUERY \/ PATH CHECK\n    query = request.GET\n    path = get_path(query.get('dir', ''))\n    filename = get_file(query.get('dir', ''), query.get('filename', ''))\n    if path is None or filename is None:\n        if path is None:\n            msg = _('The requested Folder does not exist.')\n        else:\n            msg = _('The requested File does not exist.')\n        messages.warning(request,message=msg)\n        return HttpResponseRedirect(reverse(\"fb_browse\"))\n    abs_path = os.path.join(fb_settings.MEDIA_ROOT, fb_settings.DIRECTORY, path)\n    file_extension = os.path.splitext(filename)[1].lower()\n    \n    if request.method == 'POST':\n        form = RenameForm(abs_path, file_extension, request.POST)\n        if form.is_valid():\n            relative_server_path = os.path.join(fb_settings.DIRECTORY, path, filename)\n            new_filename = form.cleaned_data['name'] + file_extension\n            new_relative_server_path = os.path.join(fb_settings.DIRECTORY, path, new_filename)\n            try:\n                # PRE RENAME SIGNAL\n                filebrowser_pre_rename.send(sender=request, path=path, filename=filename, new_filename=new_filename)\n                # DELETE IMAGE VERSIONS\/THUMBNAILS\n                # regenerating versions\/thumbs will be done automatically\n                for version in VERSIONS:\n                    try:\n                        os.unlink(os.path.join(fb_settings.MEDIA_ROOT, get_version_path(relative_server_path, version)))\n                    except:\n                        pass\n                # RENAME ORIGINAL\n                os.rename(os.path.join(fb_settings.MEDIA_ROOT, relative_server_path), os.path.join(fb_settings.MEDIA_ROOT, new_relative_server_path))\n                # POST RENAME SIGNAL\n                filebrowser_post_rename.send(sender=request, path=path, filename=filename, new_filename=new_filename)\n                # MESSAGE & REDIRECT\n                msg = _('Renaming was successful.')\n                messages.success(request,message=msg)\n                redirect_url = reverse(\"fb_browse\") + query_helper(query, \"\", \"filename\")\n                return HttpResponseRedirect(redirect_url)\n            except OSError, (errno, strerror):\n                form.errors['name'] = forms.util.ErrorList([_('Error.')])\n    else:\n        form = RenameForm(abs_path, file_extension)\n    \n    return render_to_response('filebrowser\/rename.html', {\n        'form': form,\n        'query': query,\n        'file_extension': file_extension,\n        'title': _(u'Rename \"%s\"') % filename,\n        'settings_var': get_settings_var(),\n        'breadcrumbs': get_breadcrumbs(query, path),\n        'breadcrumbs_title': _(u'Rename')\n    }, context_instance=Context(request))\nrename = staff_member_required(never_cache(rename))\n\n\ndef versions(request):\n    \"\"\"\n    Show all Versions for an Image according to ADMIN_VERSIONS.\n    \"\"\"\n    \n    # QUERY \/ PATH CHECK\n    query = request.GET\n    path = get_path(query.get('dir', ''))\n    filename = get_file(query.get('dir', ''), query.get('filename', ''))\n    if path is None or filename is None:\n        if path is None:\n            msg = _('The requested Folder does not exist.')\n        else:\n            msg = _('The requested File does not exist.')\n        messages.warning(request,message=msg)\n        return HttpResponseRedirect(reverse(\"fb_browse\"))\n    abs_path = os.path.join(fb_settings.MEDIA_ROOT, fb_settings.DIRECTORY, path)\n    \n    return render_to_response('filebrowser\/versions.html', {\n        'original': path_to_url(os.path.join(fb_settings.DIRECTORY, path, filename)),\n        'query': query,\n        'title': _(u'Versions for \"%s\"') % filename,\n        'settings_var': get_settings_var(),\n        'breadcrumbs': get_breadcrumbs(query, path),\n        'breadcrumbs_title': _(u'Versions for \"%s\"') % filename\n    }, context_instance=Context(request))\nversions = staff_member_required(never_cache(versions))\n\n\n"}},"msg":"fix a browse directory traversal vulnerability"}},"https:\/\/github.com\/paolobenve\/myphotoshare":{"31fa421468de812c92bc4795046d894013a8dfd2":{"url":"https:\/\/api.github.com\/repos\/paolobenve\/myphotoshare\/commits\/31fa421468de812c92bc4795046d894013a8dfd2","html_url":"https:\/\/github.com\/paolobenve\/myphotoshare\/commit\/31fa421468de812c92bc4795046d894013a8dfd2","message":"fixed directory traversal security vulnerability; file name can now contain +","sha":"31fa421468de812c92bc4795046d894013a8dfd2","keyword":"directory traversal vulnerable","diff":"diff --git a\/scanner\/CachePath.py b\/scanner\/CachePath.py\nindex 2ebff9b..341ffd8 100644\n--- a\/scanner\/CachePath.py\n+++ b\/scanner\/CachePath.py\n@@ -44,7 +44,7 @@ def cache_base(path, filepath=False):\n \tif not filepath:\n \t\tpath = remove_album_path(path)\n \tif path:\n-\t\tpath = path.replace('\/', Options.config['cache_folder_separator']).replace(' ', '_').replace('(', '').replace('&', '').replace(',', '').replace(')', '').replace('#', '').replace('[', '').replace(']', '').replace('\"', '').replace(\"'\", '').replace('_-_', '-').lower()\n+\t\tpath = path.replace('\/', Options.config['cache_folder_separator']).replace(' ', '_').replace('+', '_').replace('(', '').replace('&', '').replace(',', '').replace(')', '').replace('#', '').replace('[', '').replace(']', '').replace('\"', '').replace(\"'\", '').replace('_-_', '-').lower()\n \t\twhile path.find(\"--\") != -1:\n \t\t\tpath = path.replace(\"--\", \"-\")\n \t\twhile path.find(\"__\") != -1:\ndiff --git a\/web\/index.php b\/web\/index.php\nindex 9f10a02..2e042e7 100644\n--- a\/web\/index.php\n+++ b\/web\/index.php\n@@ -34,20 +34,53 @@\n \t<script type=\"text\/javascript\" src=\"js\/012-display.js\"><\/script>\n \t\n \t<?php\n+\t\t\/\/~ ini_set('display_errors', 1);\n+\t\t\/\/~ error_reporting(E_ALL);\n \t\tfunction join_paths() {\n \t\t\treturn preg_replace('~[\/\\\\\\]+~', DIRECTORY_SEPARATOR, implode(DIRECTORY_SEPARATOR, func_get_args()));\n \t\t}\n \t\t\n+\t\t\/\/ from http:\/\/skills2earn.blogspot.it\/2012\/01\/how-to-check-if-file-exists-on.html , solution # 3\n+\t\tfunction url_exist($url) {\n+\t\t\tif (@fopen($url,\"r\"))\n+\t\t\t\treturn true;\n+\t\t\telse \n+\t\t\t\treturn false;\n+\t\t}\n+\n+\t\t\n \t\t\/\/ put the <link rel=\"..\"> tag in <head> for getting the image thumbnail when sharing\n-\t\tif ($_GET['t']) {\n-\t\t\tif ($_GET['t'] == 'a') {\n-\t\t\t\t$i = 0;\n-\t\t\t\t$srcImagePaths = array();\n-\t\t\t\t$url = $_SERVER['REQUEST_SCHEME'] . ':\/\/' . $_SERVER['HTTP_HOST'] . $_SERVER['CONTEXT_PREFIX'] . '\/';\n-\t\t\t\twhile (array_key_exists('s' . $i, $_GET)) {\n-\t\t\t\t\t$srcImagePaths[] = $url . $options['server_cache_path'] . '\/' . $_GET['s' . $i];\n-\t\t\t\t\t$i ++;\n+\t\tif (isset($_GET['t']) && $_GET['t']) {\n+\t\t\t$i = 0;\n+\t\t\t$srcImagePaths = array();\n+\t\t\t$url = $_SERVER['REQUEST_SCHEME'] . ':\/\/' . $_SERVER['HTTP_HOST'] . $_SERVER['CONTEXT_PREFIX'] . '\/';\n+\t\t\twhile (array_key_exists('s' . $i, $_GET) && $_GET['s' . $i]) {\n+\t\t\t\t\/\/ Prevent directory traversal security vulnerability\n+\t\t\t\t$realPath = realpath($_GET['s' . $i]);\n+\t\t\t\techo \"\\n\\n$realPath\";\n+\t\t\t\t$addCachePath = false;\n+\t\t\t\tif ($realPath == \"\") {\n+\t\t\t\t\t\/\/ it's a thumbnail add the cache prefix\n+\t\t\t\t\t$realPath = realpath(join_paths($options['server_cache_path'], $_GET['s' . $i]));\n+\t\t\t\t\t$addCachePath = true;\n \t\t\t\t}\n+\t\t\t\techo \"\\n$realPath\";\n+\t\t\t\t\n+\t\t\t\tif (strpos($realPath, realpath($options['server_cache_path'])) === 0 || strpos($realPath, realpath($options['server_album_path'])) === 0) {\n+\t\t\t\t\t\/\/~ $path = $url . join_paths($options['server_cache_path'], $_GET['s' . $i]);\n+\t\t\t\t\t$path = $url . $_GET['s' . $i];\n+\t\t\t\t\tif ($addCachePath)\n+\t\t\t\t\t\t$path = $path = $url . join_paths($options['server_cache_path'], $_GET['s' . $i]);\n+\t\t\t\t\tif (url_exist($realPath)) {\n+\t\t\t\t\t\t$srcImagePaths[] = $path;\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t\t$i ++;\n+\t\t\t}\n+\t\t\tvar_dump($srcImagePaths);\n+\t\t\tif ($_GET['t'] == 'a') {\n+\t\t\t\t\/\/ album: generate the composite image\n+\t\t\t\t\n \t\t\t\t$maxThumbnailNumber = count($srcImagePaths);\n \t\t\t\t\/\/ following code got from\n \t\t\t\t\/\/ https:\/\/stackoverflow.com\/questions\/30429383\/combine-16-images-into-1-big-image-with-php#30429557\n@@ -99,18 +132,22 @@ function indexToCoords($index)\n \t\t\t\t\/\/ save the image\n \t\t\t\t$result = imagejpeg($mapImage, $absoluteImagePath);\n \t\t\t\t$media = $serverImagePath;\n+\t\t\t\t$pathInfo = pathinfo($_SERVER['PHP_SELF'])['dirname'];\n+\t\t\t\t$mediaWithPath = '\/' .$media;\n+\t\t\t\tif ($pathInfo != '\/')\n+\t\t\t\t\t$mediaWithPath = $pathInfo .$mediaWithPath;\n \t\t\t\t\n \t\t\t} else {\n-\t\t\t\t$media = $_GET['s0'];\n+\t\t\t\t\/\/ photo or video\n+\t\t\t\t$mediaWithPath = $srcImagePaths[0];\n \t\t\t}\n-\t\t\t$pathInfo = pathinfo($_SERVER['PHP_SELF'])['dirname'];\n-\t\t\t$mediaWithPath = '\/' .$media;\n-\t\t\tif ($pathInfo != '\/')\n-\t\t\t\t$mediaWithPath = $pathInfo .$mediaWithPath;\n+\t\t\t\n \t\t\t$linkTag = '<link ';\n \t\t\tif ($_GET['t'] == 'p' || $_GET['t'] == 'a')\n+\t\t\t\t\/\/ 'p': photo, 'a': album\n \t\t\t\t$linkTag .= 'rel=\"image_src\" ';\n \t\t\telse if ($_GET['t'] == 'v')\n+\t\t\t\t\/\/ 'v': video\n \t\t\t\t$linkTag .= 'rel=\"video_src\" ';\n \t\t\t$linkTag .= 'href=\"' . $mediaWithPath . '\"';\n \t\t\t$linkTag .= '>';\n@@ -155,7 +192,7 @@ function indexToCoords($index)\n \t\t<noscript><p><img src=\"\/\/cathopedia.org:8080\/piwik\/piwik.php?idsite=15\" style=\"border:0;\" alt=\"\" \/><\/p><\/noscript>\n \t\t<!-- End Piwik Code -->\n \t<?php } ?>  \n-\t<?php if ($options['google_analitics_id']) { ?>\n+\t<?php if (isset($options['google_analitics_id'])) { ?>\n \t\t<!-- google analytics -->\n \t\t<script type=\"text\/javascript\">\n \t\t\t\/\/ from https:\/\/git.zx2c4.com\/PhotoFloat\/tree\/web\/js\/999-googletracker.js\ndiff --git a\/web\/js\/010-libphotofloat.js b\/web\/js\/010-libphotofloat.js\nindex cc8d439..7835b4f 100644\n--- a\/web\/js\/010-libphotofloat.js\n+++ b\/web\/js\/010-libphotofloat.js\n@@ -142,6 +142,7 @@\n \t\t\tpath = path.substring(1);\n \t\tpath = path\n \t\t\t.replace(\/ \/g, \"_\")\n+\t\t\t.replace(\/\\+\/g, \"_\")\n \t\t\t.replace(\/\\\/\/g, Options.cache_folder_separator)\n \t\t\t.replace(\/\\(\/g, \"\")\n \t\t\t.replace(\/\\)\/g, \"\")\ndiff --git a\/web\/js\/012-display.js b\/web\/js\/012-display.js\nindex d3ae845..44afa3b 100644\n--- a\/web\/js\/012-display.js\n+++ b\/web\/js\/012-display.js\n@@ -237,8 +237,12 @@ $(document).ready(function() {\n \t\t\t$(\".thumbnail\").each(function() {\n \t\t\t\tsrc = $(this).attr(\"src\");\n \t\t\t\tposition = src.search(re);\n-\t\t\t\tsrc = src.substring(0, position + 1) + Options.album_thumb_size + \"as.jpg\";\n-\t\t\t\tsrc = src.substring(Options.server_cache_path.length);\n+\t\t\t\tif (position == -1) {\n+\t\t\t\t\t\/\/ it's not a thumbnail, it's a small image\n+\t\t\t\t} else {\n+\t\t\t\t\tsrc = src.substring(0, position + 1) + Options.album_thumb_size + \"as.jpg\";\n+\t\t\t\t\tsrc = src.substring(Options.server_cache_path.length + 1);\n+\t\t\t\t}\n \t\t\t\tif (allThumbnails.indexOf(src) == -1)\n \t\t\t\t\tallThumbnails.push(src);\n \t\t\t});\n@@ -274,7 +278,8 @@ $(document).ready(function() {\n \t\t\t\t}\n \t\t\t}\n \t\t} else {\n-\t\t\tmediaArray.push($(\"#media\").attr(\"src\"));\n+\t\t\tmediaArray[0] = $(\"#media\").attr(\"src\");\n+\t\t\t\/\/~ mediaArray[0] = mediaArray[0].substring(Options.server_cache_path.length + 1);\n \t\t\tif (currentMedia.mediaType == \"video\") {\n \t\t\t\ttype = \"v\";\n \t\t\t} else if (currentMedia.mediaType == \"photo\") {\n","files":{"\/scanner\/CachePath.py":{"changes":[{"diff":"\n \tif not filepath:\n \t\tpath = remove_album_path(path)\n \tif path:\n-\t\tpath = path.replace('\/', Options.config['cache_folder_separator']).replace(' ', '_').replace('(', '').replace('&', '').replace(',', '').replace(')', '').replace('#', '').replace('[', '').replace(']', '').replace('\"', '').replace(\"'\", '').replace('_-_', '-').lower()\n+\t\tpath = path.replace('\/', Options.config['cache_folder_separator']).replace(' ', '_').replace('+', '_').replace('(', '').replace('&', '').replace(',', '').replace(')', '').replace('#', '').replace('[', '').replace(']', '').replace('\"', '').replace(\"'\", '').replace('_-_', '-').lower()\n \t\twhile path.find(\"--\") != -1:\n \t\t\tpath = path.replace(\"--\", \"-\")\n \t\twhile path.find(\"__\") != -1:","add":1,"remove":1,"filename":"\/scanner\/CachePath.py","badparts":["\t\tpath = path.replace('\/', Options.config['cache_folder_separator']).replace(' ', '_').replace('(', '').replace('&', '').replace(',', '').replace(')', '').replace('#', '').replace('[', '').replace(']', '').replace('\"', '').replace(\"'\", '').replace('_-_', '-').lower()"],"goodparts":["\t\tpath = path.replace('\/', Options.config['cache_folder_separator']).replace(' ', '_').replace('+', '_').replace('(', '').replace('&', '').replace(',', '').replace(')', '').replace('#', '').replace('[', '').replace(']', '').replace('\"', '').replace(\"'\", '').replace('_-_', '-').lower()"]}],"source":"\nimport os.path from datetime import datetime import hashlib import Options max_verbose=0 def message(category, text, verbose=0): \t \t \t \t \t \t \tglobal usrOptions \ttry: \t\tmax_verbose=Options.config['max_verbose'] \texcept KeyError: \t\tmax_verbose=0 \texcept AttributeError: \t\tmax_verbose=0 \tif(verbose <=max_verbose): \t\tif message.level <=0: \t\t\tsep=\" \" \t\telse: \t\t\tsep=\"--\" \t\tprint \"%s %s%s[%s]%s%s\" %(datetime.now().isoformat(' '), max(0, message.level) * \" |\", sep, str(category), max(1,(45 -len(str(category)))) * \" \", str(text)) message.level=0 def next_level(verbose=0): \tif(verbose <=max_verbose): \t\tmessage.level +=1 def back_level(verbose=0): \tif(verbose <=max_verbose): \t\tmessage.level -=1 def trim_base_custom(path, base): \tif path.startswith(base): \t\tpath=path[len(base):] \tif path.startswith('\/'): \t\tpath=path[1:] \treturn path def remove_album_path(path): \treturn trim_base_custom(path, Options.config['album_path']) def cache_base(path, filepath=False): \tif not filepath: \t\tpath=remove_album_path(path) \tif path: \t\tpath=path.replace('\/', Options.config['cache_folder_separator']).replace(' ', '_').replace('(', '').replace('&', '').replace(',', '').replace(')', '').replace(' \t\twhile path.find(\"--\") !=-1: \t\t\tpath=path.replace(\"--\", \"-\") \t\twhile path.find(\"__\") !=-1: \t\t\tpath=path.replace(\"__\", \"_\") \telse: \t\tpath=\"root\" \treturn path def json_name(path): \treturn cache_base(path) +\".json\" def photo_cache_name(path, size, thumb_type=\"\"): \t \tsuffix=\"_\" +str(size) \tif size==Options.config['album_thumb_size']: \t\tsuffix +=\"a\" \t\tif thumb_type==\"square\": \t\t\tsuffix +=\"s\" \t\telif thumb_type==\"fit\": \t\t\tsuffix +=\"f\" \telif size==Options.config['media_thumb_size']: \t\tsuffix +=\"t\" \t\tif thumb_type==\"square\": \t\t\tsuffix +=\"s\" \t\telif thumb_type==\"fixed_height\": \t\t\tsuffix +=\"f\" \tsuffix +=\".jpg\" \tresult=cache_base(path, True) +suffix \treturn result def video_cache_name(path): \treturn cache_base(path, True) +\"_transcoded_\" +Options.config['video_transcode_bitrate'] +\".mp4\" def file_mtime(path): \treturn datetime.fromtimestamp(int(os.path.getmtime(path))) ","sourceWithComments":"import os.path\nfrom datetime import datetime\nimport hashlib\nimport Options\n\nmax_verbose = 0\ndef message(category, text, verbose = 0):\n\t# verbosity levels:\n\t# 0 = fatal errors only\n\t# 1 = add non-fatal errors\n\t# 2 = add warnings\n\t# 3 = add info\n\t# 4 = add more info\n\tglobal usrOptions\n\ttry:\n\t\tmax_verbose = Options.config['max_verbose']\n\texcept KeyError:\n\t\tmax_verbose = 0\n\texcept AttributeError:\n\t\tmax_verbose = 0\n\tif (verbose <= max_verbose):\n\t\tif message.level <= 0:\n\t\t\tsep = \"  \"\n\t\telse:\n\t\t\tsep = \"--\"\n\t\tprint \"%s %s%s[%s]%s%s\" % (datetime.now().isoformat(' '), max(0, message.level) * \"  |\", sep, str(category), max(1, (45 - len(str(category)))) * \" \", str(text))\n\nmessage.level = 0\ndef next_level(verbose = 0):\n\tif (verbose <= max_verbose):\n\t\tmessage.level += 1\ndef back_level(verbose = 0):\n\tif (verbose <= max_verbose):\n\t\tmessage.level -= 1\ndef trim_base_custom(path, base):\n\tif path.startswith(base):\n\t\tpath = path[len(base):]\n\tif path.startswith('\/'):\n\t\tpath = path[1:]\n\treturn path\ndef remove_album_path(path):\n\treturn trim_base_custom(path, Options.config['album_path'])\ndef cache_base(path, filepath=False):\n\tif not filepath:\n\t\tpath = remove_album_path(path)\n\tif path:\n\t\tpath = path.replace('\/', Options.config['cache_folder_separator']).replace(' ', '_').replace('(', '').replace('&', '').replace(',', '').replace(')', '').replace('#', '').replace('[', '').replace(']', '').replace('\"', '').replace(\"'\", '').replace('_-_', '-').lower()\n\t\twhile path.find(\"--\") != -1:\n\t\t\tpath = path.replace(\"--\", \"-\")\n\t\twhile path.find(\"__\") != -1:\n\t\t\tpath = path.replace(\"__\", \"_\")\n\telse:\n\t\tpath = \"root\"\n\treturn path\ndef json_name(path):\n\treturn cache_base(path) + \".json\"\ndef photo_cache_name(path, size, thumb_type = \"\"):\n\t# this function is used for video thumbnails too\n\tsuffix = \"_\" + str(size)\n\tif size == Options.config['album_thumb_size']:\n\t\tsuffix += \"a\"\n\t\tif thumb_type == \"square\":\n\t\t\tsuffix += \"s\"\n\t\telif thumb_type == \"fit\":\n\t\t\tsuffix += \"f\"\n\telif size == Options.config['media_thumb_size']:\n\t\tsuffix += \"t\"\n\t\tif thumb_type == \"square\":\n\t\t\tsuffix += \"s\"\n\t\telif thumb_type == \"fixed_height\":\n\t\t\tsuffix += \"f\"\n\tsuffix += \".jpg\"\n\tresult = cache_base(path, True) + suffix\n\treturn result\ndef video_cache_name(path):\n\treturn cache_base(path, True) + \"_transcoded_\" + Options.config['video_transcode_bitrate'] + \".mp4\"\ndef file_mtime(path):\n\treturn datetime.fromtimestamp(int(os.path.getmtime(path)))\n"}},"msg":"fixed directory traversal security vulnerability; file name can now contain +"}},"https:\/\/github.com\/rastabda22\/rastabda22":{"31fa421468de812c92bc4795046d894013a8dfd2":{"url":"https:\/\/api.github.com\/repos\/rastabda22\/rastabda22\/commits\/31fa421468de812c92bc4795046d894013a8dfd2","html_url":"https:\/\/github.com\/rastabda22\/rastabda22\/commit\/31fa421468de812c92bc4795046d894013a8dfd2","message":"fixed directory traversal security vulnerability; file name can now contain +","sha":"31fa421468de812c92bc4795046d894013a8dfd2","keyword":"directory traversal vulnerable","diff":"diff --git a\/scanner\/CachePath.py b\/scanner\/CachePath.py\nindex 2ebff9bc..341ffd8c 100644\n--- a\/scanner\/CachePath.py\n+++ b\/scanner\/CachePath.py\n@@ -44,7 +44,7 @@ def cache_base(path, filepath=False):\n \tif not filepath:\n \t\tpath = remove_album_path(path)\n \tif path:\n-\t\tpath = path.replace('\/', Options.config['cache_folder_separator']).replace(' ', '_').replace('(', '').replace('&', '').replace(',', '').replace(')', '').replace('#', '').replace('[', '').replace(']', '').replace('\"', '').replace(\"'\", '').replace('_-_', '-').lower()\n+\t\tpath = path.replace('\/', Options.config['cache_folder_separator']).replace(' ', '_').replace('+', '_').replace('(', '').replace('&', '').replace(',', '').replace(')', '').replace('#', '').replace('[', '').replace(']', '').replace('\"', '').replace(\"'\", '').replace('_-_', '-').lower()\n \t\twhile path.find(\"--\") != -1:\n \t\t\tpath = path.replace(\"--\", \"-\")\n \t\twhile path.find(\"__\") != -1:\ndiff --git a\/web\/index.php b\/web\/index.php\nindex 9f10a025..2e042e71 100644\n--- a\/web\/index.php\n+++ b\/web\/index.php\n@@ -34,20 +34,53 @@\n \t<script type=\"text\/javascript\" src=\"js\/012-display.js\"><\/script>\n \t\n \t<?php\n+\t\t\/\/~ ini_set('display_errors', 1);\n+\t\t\/\/~ error_reporting(E_ALL);\n \t\tfunction join_paths() {\n \t\t\treturn preg_replace('~[\/\\\\\\]+~', DIRECTORY_SEPARATOR, implode(DIRECTORY_SEPARATOR, func_get_args()));\n \t\t}\n \t\t\n+\t\t\/\/ from http:\/\/skills2earn.blogspot.it\/2012\/01\/how-to-check-if-file-exists-on.html , solution # 3\n+\t\tfunction url_exist($url) {\n+\t\t\tif (@fopen($url,\"r\"))\n+\t\t\t\treturn true;\n+\t\t\telse \n+\t\t\t\treturn false;\n+\t\t}\n+\n+\t\t\n \t\t\/\/ put the <link rel=\"..\"> tag in <head> for getting the image thumbnail when sharing\n-\t\tif ($_GET['t']) {\n-\t\t\tif ($_GET['t'] == 'a') {\n-\t\t\t\t$i = 0;\n-\t\t\t\t$srcImagePaths = array();\n-\t\t\t\t$url = $_SERVER['REQUEST_SCHEME'] . ':\/\/' . $_SERVER['HTTP_HOST'] . $_SERVER['CONTEXT_PREFIX'] . '\/';\n-\t\t\t\twhile (array_key_exists('s' . $i, $_GET)) {\n-\t\t\t\t\t$srcImagePaths[] = $url . $options['server_cache_path'] . '\/' . $_GET['s' . $i];\n-\t\t\t\t\t$i ++;\n+\t\tif (isset($_GET['t']) && $_GET['t']) {\n+\t\t\t$i = 0;\n+\t\t\t$srcImagePaths = array();\n+\t\t\t$url = $_SERVER['REQUEST_SCHEME'] . ':\/\/' . $_SERVER['HTTP_HOST'] . $_SERVER['CONTEXT_PREFIX'] . '\/';\n+\t\t\twhile (array_key_exists('s' . $i, $_GET) && $_GET['s' . $i]) {\n+\t\t\t\t\/\/ Prevent directory traversal security vulnerability\n+\t\t\t\t$realPath = realpath($_GET['s' . $i]);\n+\t\t\t\techo \"\\n\\n$realPath\";\n+\t\t\t\t$addCachePath = false;\n+\t\t\t\tif ($realPath == \"\") {\n+\t\t\t\t\t\/\/ it's a thumbnail add the cache prefix\n+\t\t\t\t\t$realPath = realpath(join_paths($options['server_cache_path'], $_GET['s' . $i]));\n+\t\t\t\t\t$addCachePath = true;\n \t\t\t\t}\n+\t\t\t\techo \"\\n$realPath\";\n+\t\t\t\t\n+\t\t\t\tif (strpos($realPath, realpath($options['server_cache_path'])) === 0 || strpos($realPath, realpath($options['server_album_path'])) === 0) {\n+\t\t\t\t\t\/\/~ $path = $url . join_paths($options['server_cache_path'], $_GET['s' . $i]);\n+\t\t\t\t\t$path = $url . $_GET['s' . $i];\n+\t\t\t\t\tif ($addCachePath)\n+\t\t\t\t\t\t$path = $path = $url . join_paths($options['server_cache_path'], $_GET['s' . $i]);\n+\t\t\t\t\tif (url_exist($realPath)) {\n+\t\t\t\t\t\t$srcImagePaths[] = $path;\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t\t$i ++;\n+\t\t\t}\n+\t\t\tvar_dump($srcImagePaths);\n+\t\t\tif ($_GET['t'] == 'a') {\n+\t\t\t\t\/\/ album: generate the composite image\n+\t\t\t\t\n \t\t\t\t$maxThumbnailNumber = count($srcImagePaths);\n \t\t\t\t\/\/ following code got from\n \t\t\t\t\/\/ https:\/\/stackoverflow.com\/questions\/30429383\/combine-16-images-into-1-big-image-with-php#30429557\n@@ -99,18 +132,22 @@ function indexToCoords($index)\n \t\t\t\t\/\/ save the image\n \t\t\t\t$result = imagejpeg($mapImage, $absoluteImagePath);\n \t\t\t\t$media = $serverImagePath;\n+\t\t\t\t$pathInfo = pathinfo($_SERVER['PHP_SELF'])['dirname'];\n+\t\t\t\t$mediaWithPath = '\/' .$media;\n+\t\t\t\tif ($pathInfo != '\/')\n+\t\t\t\t\t$mediaWithPath = $pathInfo .$mediaWithPath;\n \t\t\t\t\n \t\t\t} else {\n-\t\t\t\t$media = $_GET['s0'];\n+\t\t\t\t\/\/ photo or video\n+\t\t\t\t$mediaWithPath = $srcImagePaths[0];\n \t\t\t}\n-\t\t\t$pathInfo = pathinfo($_SERVER['PHP_SELF'])['dirname'];\n-\t\t\t$mediaWithPath = '\/' .$media;\n-\t\t\tif ($pathInfo != '\/')\n-\t\t\t\t$mediaWithPath = $pathInfo .$mediaWithPath;\n+\t\t\t\n \t\t\t$linkTag = '<link ';\n \t\t\tif ($_GET['t'] == 'p' || $_GET['t'] == 'a')\n+\t\t\t\t\/\/ 'p': photo, 'a': album\n \t\t\t\t$linkTag .= 'rel=\"image_src\" ';\n \t\t\telse if ($_GET['t'] == 'v')\n+\t\t\t\t\/\/ 'v': video\n \t\t\t\t$linkTag .= 'rel=\"video_src\" ';\n \t\t\t$linkTag .= 'href=\"' . $mediaWithPath . '\"';\n \t\t\t$linkTag .= '>';\n@@ -155,7 +192,7 @@ function indexToCoords($index)\n \t\t<noscript><p><img src=\"\/\/cathopedia.org:8080\/piwik\/piwik.php?idsite=15\" style=\"border:0;\" alt=\"\" \/><\/p><\/noscript>\n \t\t<!-- End Piwik Code -->\n \t<?php } ?>  \n-\t<?php if ($options['google_analitics_id']) { ?>\n+\t<?php if (isset($options['google_analitics_id'])) { ?>\n \t\t<!-- google analytics -->\n \t\t<script type=\"text\/javascript\">\n \t\t\t\/\/ from https:\/\/git.zx2c4.com\/PhotoFloat\/tree\/web\/js\/999-googletracker.js\ndiff --git a\/web\/js\/010-libphotofloat.js b\/web\/js\/010-libphotofloat.js\nindex cc8d4397..7835b4f2 100644\n--- a\/web\/js\/010-libphotofloat.js\n+++ b\/web\/js\/010-libphotofloat.js\n@@ -142,6 +142,7 @@\n \t\t\tpath = path.substring(1);\n \t\tpath = path\n \t\t\t.replace(\/ \/g, \"_\")\n+\t\t\t.replace(\/\\+\/g, \"_\")\n \t\t\t.replace(\/\\\/\/g, Options.cache_folder_separator)\n \t\t\t.replace(\/\\(\/g, \"\")\n \t\t\t.replace(\/\\)\/g, \"\")\ndiff --git a\/web\/js\/012-display.js b\/web\/js\/012-display.js\nindex d3ae8459..44afa3bb 100644\n--- a\/web\/js\/012-display.js\n+++ b\/web\/js\/012-display.js\n@@ -237,8 +237,12 @@ $(document).ready(function() {\n \t\t\t$(\".thumbnail\").each(function() {\n \t\t\t\tsrc = $(this).attr(\"src\");\n \t\t\t\tposition = src.search(re);\n-\t\t\t\tsrc = src.substring(0, position + 1) + Options.album_thumb_size + \"as.jpg\";\n-\t\t\t\tsrc = src.substring(Options.server_cache_path.length);\n+\t\t\t\tif (position == -1) {\n+\t\t\t\t\t\/\/ it's not a thumbnail, it's a small image\n+\t\t\t\t} else {\n+\t\t\t\t\tsrc = src.substring(0, position + 1) + Options.album_thumb_size + \"as.jpg\";\n+\t\t\t\t\tsrc = src.substring(Options.server_cache_path.length + 1);\n+\t\t\t\t}\n \t\t\t\tif (allThumbnails.indexOf(src) == -1)\n \t\t\t\t\tallThumbnails.push(src);\n \t\t\t});\n@@ -274,7 +278,8 @@ $(document).ready(function() {\n \t\t\t\t}\n \t\t\t}\n \t\t} else {\n-\t\t\tmediaArray.push($(\"#media\").attr(\"src\"));\n+\t\t\tmediaArray[0] = $(\"#media\").attr(\"src\");\n+\t\t\t\/\/~ mediaArray[0] = mediaArray[0].substring(Options.server_cache_path.length + 1);\n \t\t\tif (currentMedia.mediaType == \"video\") {\n \t\t\t\ttype = \"v\";\n \t\t\t} else if (currentMedia.mediaType == \"photo\") {\n","files":{"\/scanner\/CachePath.py":{"changes":[{"diff":"\n \tif not filepath:\n \t\tpath = remove_album_path(path)\n \tif path:\n-\t\tpath = path.replace('\/', Options.config['cache_folder_separator']).replace(' ', '_').replace('(', '').replace('&', '').replace(',', '').replace(')', '').replace('#', '').replace('[', '').replace(']', '').replace('\"', '').replace(\"'\", '').replace('_-_', '-').lower()\n+\t\tpath = path.replace('\/', Options.config['cache_folder_separator']).replace(' ', '_').replace('+', '_').replace('(', '').replace('&', '').replace(',', '').replace(')', '').replace('#', '').replace('[', '').replace(']', '').replace('\"', '').replace(\"'\", '').replace('_-_', '-').lower()\n \t\twhile path.find(\"--\") != -1:\n \t\t\tpath = path.replace(\"--\", \"-\")\n \t\twhile path.find(\"__\") != -1:","add":1,"remove":1,"filename":"\/scanner\/CachePath.py","badparts":["\t\tpath = path.replace('\/', Options.config['cache_folder_separator']).replace(' ', '_').replace('(', '').replace('&', '').replace(',', '').replace(')', '').replace('#', '').replace('[', '').replace(']', '').replace('\"', '').replace(\"'\", '').replace('_-_', '-').lower()"],"goodparts":["\t\tpath = path.replace('\/', Options.config['cache_folder_separator']).replace(' ', '_').replace('+', '_').replace('(', '').replace('&', '').replace(',', '').replace(')', '').replace('#', '').replace('[', '').replace(']', '').replace('\"', '').replace(\"'\", '').replace('_-_', '-').lower()"]}],"source":"\nimport os.path from datetime import datetime import hashlib import Options max_verbose=0 def message(category, text, verbose=0): \t \t \t \t \t \t \tglobal usrOptions \ttry: \t\tmax_verbose=Options.config['max_verbose'] \texcept KeyError: \t\tmax_verbose=0 \texcept AttributeError: \t\tmax_verbose=0 \tif(verbose <=max_verbose): \t\tif message.level <=0: \t\t\tsep=\" \" \t\telse: \t\t\tsep=\"--\" \t\tprint \"%s %s%s[%s]%s%s\" %(datetime.now().isoformat(' '), max(0, message.level) * \" |\", sep, str(category), max(1,(45 -len(str(category)))) * \" \", str(text)) message.level=0 def next_level(verbose=0): \tif(verbose <=max_verbose): \t\tmessage.level +=1 def back_level(verbose=0): \tif(verbose <=max_verbose): \t\tmessage.level -=1 def trim_base_custom(path, base): \tif path.startswith(base): \t\tpath=path[len(base):] \tif path.startswith('\/'): \t\tpath=path[1:] \treturn path def remove_album_path(path): \treturn trim_base_custom(path, Options.config['album_path']) def cache_base(path, filepath=False): \tif not filepath: \t\tpath=remove_album_path(path) \tif path: \t\tpath=path.replace('\/', Options.config['cache_folder_separator']).replace(' ', '_').replace('(', '').replace('&', '').replace(',', '').replace(')', '').replace(' \t\twhile path.find(\"--\") !=-1: \t\t\tpath=path.replace(\"--\", \"-\") \t\twhile path.find(\"__\") !=-1: \t\t\tpath=path.replace(\"__\", \"_\") \telse: \t\tpath=\"root\" \treturn path def json_name(path): \treturn cache_base(path) +\".json\" def photo_cache_name(path, size, thumb_type=\"\"): \t \tsuffix=\"_\" +str(size) \tif size==Options.config['album_thumb_size']: \t\tsuffix +=\"a\" \t\tif thumb_type==\"square\": \t\t\tsuffix +=\"s\" \t\telif thumb_type==\"fit\": \t\t\tsuffix +=\"f\" \telif size==Options.config['media_thumb_size']: \t\tsuffix +=\"t\" \t\tif thumb_type==\"square\": \t\t\tsuffix +=\"s\" \t\telif thumb_type==\"fixed_height\": \t\t\tsuffix +=\"f\" \tsuffix +=\".jpg\" \tresult=cache_base(path, True) +suffix \treturn result def video_cache_name(path): \treturn cache_base(path, True) +\"_transcoded_\" +Options.config['video_transcode_bitrate'] +\".mp4\" def file_mtime(path): \treturn datetime.fromtimestamp(int(os.path.getmtime(path))) ","sourceWithComments":"import os.path\nfrom datetime import datetime\nimport hashlib\nimport Options\n\nmax_verbose = 0\ndef message(category, text, verbose = 0):\n\t# verbosity levels:\n\t# 0 = fatal errors only\n\t# 1 = add non-fatal errors\n\t# 2 = add warnings\n\t# 3 = add info\n\t# 4 = add more info\n\tglobal usrOptions\n\ttry:\n\t\tmax_verbose = Options.config['max_verbose']\n\texcept KeyError:\n\t\tmax_verbose = 0\n\texcept AttributeError:\n\t\tmax_verbose = 0\n\tif (verbose <= max_verbose):\n\t\tif message.level <= 0:\n\t\t\tsep = \"  \"\n\t\telse:\n\t\t\tsep = \"--\"\n\t\tprint \"%s %s%s[%s]%s%s\" % (datetime.now().isoformat(' '), max(0, message.level) * \"  |\", sep, str(category), max(1, (45 - len(str(category)))) * \" \", str(text))\n\nmessage.level = 0\ndef next_level(verbose = 0):\n\tif (verbose <= max_verbose):\n\t\tmessage.level += 1\ndef back_level(verbose = 0):\n\tif (verbose <= max_verbose):\n\t\tmessage.level -= 1\ndef trim_base_custom(path, base):\n\tif path.startswith(base):\n\t\tpath = path[len(base):]\n\tif path.startswith('\/'):\n\t\tpath = path[1:]\n\treturn path\ndef remove_album_path(path):\n\treturn trim_base_custom(path, Options.config['album_path'])\ndef cache_base(path, filepath=False):\n\tif not filepath:\n\t\tpath = remove_album_path(path)\n\tif path:\n\t\tpath = path.replace('\/', Options.config['cache_folder_separator']).replace(' ', '_').replace('(', '').replace('&', '').replace(',', '').replace(')', '').replace('#', '').replace('[', '').replace(']', '').replace('\"', '').replace(\"'\", '').replace('_-_', '-').lower()\n\t\twhile path.find(\"--\") != -1:\n\t\t\tpath = path.replace(\"--\", \"-\")\n\t\twhile path.find(\"__\") != -1:\n\t\t\tpath = path.replace(\"__\", \"_\")\n\telse:\n\t\tpath = \"root\"\n\treturn path\ndef json_name(path):\n\treturn cache_base(path) + \".json\"\ndef photo_cache_name(path, size, thumb_type = \"\"):\n\t# this function is used for video thumbnails too\n\tsuffix = \"_\" + str(size)\n\tif size == Options.config['album_thumb_size']:\n\t\tsuffix += \"a\"\n\t\tif thumb_type == \"square\":\n\t\t\tsuffix += \"s\"\n\t\telif thumb_type == \"fit\":\n\t\t\tsuffix += \"f\"\n\telif size == Options.config['media_thumb_size']:\n\t\tsuffix += \"t\"\n\t\tif thumb_type == \"square\":\n\t\t\tsuffix += \"s\"\n\t\telif thumb_type == \"fixed_height\":\n\t\t\tsuffix += \"f\"\n\tsuffix += \".jpg\"\n\tresult = cache_base(path, True) + suffix\n\treturn result\ndef video_cache_name(path):\n\treturn cache_base(path, True) + \"_transcoded_\" + Options.config['video_transcode_bitrate'] + \".mp4\"\ndef file_mtime(path):\n\treturn datetime.fromtimestamp(int(os.path.getmtime(path)))\n"}},"msg":"fixed directory traversal security vulnerability; file name can now contain +"}},"https:\/\/github.com\/rastabda22\/rastabda23":{"31fa421468de812c92bc4795046d894013a8dfd2":{"url":"https:\/\/api.github.com\/repos\/rastabda22\/rastabda23\/commits\/31fa421468de812c92bc4795046d894013a8dfd2","html_url":"https:\/\/github.com\/rastabda22\/rastabda23\/commit\/31fa421468de812c92bc4795046d894013a8dfd2","message":"fixed directory traversal security vulnerability; file name can now contain +","sha":"31fa421468de812c92bc4795046d894013a8dfd2","keyword":"directory traversal vulnerable","diff":"diff --git a\/scanner\/CachePath.py b\/scanner\/CachePath.py\nindex 2ebff9bc..341ffd8c 100644\n--- a\/scanner\/CachePath.py\n+++ b\/scanner\/CachePath.py\n@@ -44,7 +44,7 @@ def cache_base(path, filepath=False):\n \tif not filepath:\n \t\tpath = remove_album_path(path)\n \tif path:\n-\t\tpath = path.replace('\/', Options.config['cache_folder_separator']).replace(' ', '_').replace('(', '').replace('&', '').replace(',', '').replace(')', '').replace('#', '').replace('[', '').replace(']', '').replace('\"', '').replace(\"'\", '').replace('_-_', '-').lower()\n+\t\tpath = path.replace('\/', Options.config['cache_folder_separator']).replace(' ', '_').replace('+', '_').replace('(', '').replace('&', '').replace(',', '').replace(')', '').replace('#', '').replace('[', '').replace(']', '').replace('\"', '').replace(\"'\", '').replace('_-_', '-').lower()\n \t\twhile path.find(\"--\") != -1:\n \t\t\tpath = path.replace(\"--\", \"-\")\n \t\twhile path.find(\"__\") != -1:\ndiff --git a\/web\/index.php b\/web\/index.php\nindex 9f10a025..2e042e71 100644\n--- a\/web\/index.php\n+++ b\/web\/index.php\n@@ -34,20 +34,53 @@\n \t<script type=\"text\/javascript\" src=\"js\/012-display.js\"><\/script>\n \t\n \t<?php\n+\t\t\/\/~ ini_set('display_errors', 1);\n+\t\t\/\/~ error_reporting(E_ALL);\n \t\tfunction join_paths() {\n \t\t\treturn preg_replace('~[\/\\\\\\]+~', DIRECTORY_SEPARATOR, implode(DIRECTORY_SEPARATOR, func_get_args()));\n \t\t}\n \t\t\n+\t\t\/\/ from http:\/\/skills2earn.blogspot.it\/2012\/01\/how-to-check-if-file-exists-on.html , solution # 3\n+\t\tfunction url_exist($url) {\n+\t\t\tif (@fopen($url,\"r\"))\n+\t\t\t\treturn true;\n+\t\t\telse \n+\t\t\t\treturn false;\n+\t\t}\n+\n+\t\t\n \t\t\/\/ put the <link rel=\"..\"> tag in <head> for getting the image thumbnail when sharing\n-\t\tif ($_GET['t']) {\n-\t\t\tif ($_GET['t'] == 'a') {\n-\t\t\t\t$i = 0;\n-\t\t\t\t$srcImagePaths = array();\n-\t\t\t\t$url = $_SERVER['REQUEST_SCHEME'] . ':\/\/' . $_SERVER['HTTP_HOST'] . $_SERVER['CONTEXT_PREFIX'] . '\/';\n-\t\t\t\twhile (array_key_exists('s' . $i, $_GET)) {\n-\t\t\t\t\t$srcImagePaths[] = $url . $options['server_cache_path'] . '\/' . $_GET['s' . $i];\n-\t\t\t\t\t$i ++;\n+\t\tif (isset($_GET['t']) && $_GET['t']) {\n+\t\t\t$i = 0;\n+\t\t\t$srcImagePaths = array();\n+\t\t\t$url = $_SERVER['REQUEST_SCHEME'] . ':\/\/' . $_SERVER['HTTP_HOST'] . $_SERVER['CONTEXT_PREFIX'] . '\/';\n+\t\t\twhile (array_key_exists('s' . $i, $_GET) && $_GET['s' . $i]) {\n+\t\t\t\t\/\/ Prevent directory traversal security vulnerability\n+\t\t\t\t$realPath = realpath($_GET['s' . $i]);\n+\t\t\t\techo \"\\n\\n$realPath\";\n+\t\t\t\t$addCachePath = false;\n+\t\t\t\tif ($realPath == \"\") {\n+\t\t\t\t\t\/\/ it's a thumbnail add the cache prefix\n+\t\t\t\t\t$realPath = realpath(join_paths($options['server_cache_path'], $_GET['s' . $i]));\n+\t\t\t\t\t$addCachePath = true;\n \t\t\t\t}\n+\t\t\t\techo \"\\n$realPath\";\n+\t\t\t\t\n+\t\t\t\tif (strpos($realPath, realpath($options['server_cache_path'])) === 0 || strpos($realPath, realpath($options['server_album_path'])) === 0) {\n+\t\t\t\t\t\/\/~ $path = $url . join_paths($options['server_cache_path'], $_GET['s' . $i]);\n+\t\t\t\t\t$path = $url . $_GET['s' . $i];\n+\t\t\t\t\tif ($addCachePath)\n+\t\t\t\t\t\t$path = $path = $url . join_paths($options['server_cache_path'], $_GET['s' . $i]);\n+\t\t\t\t\tif (url_exist($realPath)) {\n+\t\t\t\t\t\t$srcImagePaths[] = $path;\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t\t$i ++;\n+\t\t\t}\n+\t\t\tvar_dump($srcImagePaths);\n+\t\t\tif ($_GET['t'] == 'a') {\n+\t\t\t\t\/\/ album: generate the composite image\n+\t\t\t\t\n \t\t\t\t$maxThumbnailNumber = count($srcImagePaths);\n \t\t\t\t\/\/ following code got from\n \t\t\t\t\/\/ https:\/\/stackoverflow.com\/questions\/30429383\/combine-16-images-into-1-big-image-with-php#30429557\n@@ -99,18 +132,22 @@ function indexToCoords($index)\n \t\t\t\t\/\/ save the image\n \t\t\t\t$result = imagejpeg($mapImage, $absoluteImagePath);\n \t\t\t\t$media = $serverImagePath;\n+\t\t\t\t$pathInfo = pathinfo($_SERVER['PHP_SELF'])['dirname'];\n+\t\t\t\t$mediaWithPath = '\/' .$media;\n+\t\t\t\tif ($pathInfo != '\/')\n+\t\t\t\t\t$mediaWithPath = $pathInfo .$mediaWithPath;\n \t\t\t\t\n \t\t\t} else {\n-\t\t\t\t$media = $_GET['s0'];\n+\t\t\t\t\/\/ photo or video\n+\t\t\t\t$mediaWithPath = $srcImagePaths[0];\n \t\t\t}\n-\t\t\t$pathInfo = pathinfo($_SERVER['PHP_SELF'])['dirname'];\n-\t\t\t$mediaWithPath = '\/' .$media;\n-\t\t\tif ($pathInfo != '\/')\n-\t\t\t\t$mediaWithPath = $pathInfo .$mediaWithPath;\n+\t\t\t\n \t\t\t$linkTag = '<link ';\n \t\t\tif ($_GET['t'] == 'p' || $_GET['t'] == 'a')\n+\t\t\t\t\/\/ 'p': photo, 'a': album\n \t\t\t\t$linkTag .= 'rel=\"image_src\" ';\n \t\t\telse if ($_GET['t'] == 'v')\n+\t\t\t\t\/\/ 'v': video\n \t\t\t\t$linkTag .= 'rel=\"video_src\" ';\n \t\t\t$linkTag .= 'href=\"' . $mediaWithPath . '\"';\n \t\t\t$linkTag .= '>';\n@@ -155,7 +192,7 @@ function indexToCoords($index)\n \t\t<noscript><p><img src=\"\/\/cathopedia.org:8080\/piwik\/piwik.php?idsite=15\" style=\"border:0;\" alt=\"\" \/><\/p><\/noscript>\n \t\t<!-- End Piwik Code -->\n \t<?php } ?>  \n-\t<?php if ($options['google_analitics_id']) { ?>\n+\t<?php if (isset($options['google_analitics_id'])) { ?>\n \t\t<!-- google analytics -->\n \t\t<script type=\"text\/javascript\">\n \t\t\t\/\/ from https:\/\/git.zx2c4.com\/PhotoFloat\/tree\/web\/js\/999-googletracker.js\ndiff --git a\/web\/js\/010-libphotofloat.js b\/web\/js\/010-libphotofloat.js\nindex cc8d4397..7835b4f2 100644\n--- a\/web\/js\/010-libphotofloat.js\n+++ b\/web\/js\/010-libphotofloat.js\n@@ -142,6 +142,7 @@\n \t\t\tpath = path.substring(1);\n \t\tpath = path\n \t\t\t.replace(\/ \/g, \"_\")\n+\t\t\t.replace(\/\\+\/g, \"_\")\n \t\t\t.replace(\/\\\/\/g, Options.cache_folder_separator)\n \t\t\t.replace(\/\\(\/g, \"\")\n \t\t\t.replace(\/\\)\/g, \"\")\ndiff --git a\/web\/js\/012-display.js b\/web\/js\/012-display.js\nindex d3ae8459..44afa3bb 100644\n--- a\/web\/js\/012-display.js\n+++ b\/web\/js\/012-display.js\n@@ -237,8 +237,12 @@ $(document).ready(function() {\n \t\t\t$(\".thumbnail\").each(function() {\n \t\t\t\tsrc = $(this).attr(\"src\");\n \t\t\t\tposition = src.search(re);\n-\t\t\t\tsrc = src.substring(0, position + 1) + Options.album_thumb_size + \"as.jpg\";\n-\t\t\t\tsrc = src.substring(Options.server_cache_path.length);\n+\t\t\t\tif (position == -1) {\n+\t\t\t\t\t\/\/ it's not a thumbnail, it's a small image\n+\t\t\t\t} else {\n+\t\t\t\t\tsrc = src.substring(0, position + 1) + Options.album_thumb_size + \"as.jpg\";\n+\t\t\t\t\tsrc = src.substring(Options.server_cache_path.length + 1);\n+\t\t\t\t}\n \t\t\t\tif (allThumbnails.indexOf(src) == -1)\n \t\t\t\t\tallThumbnails.push(src);\n \t\t\t});\n@@ -274,7 +278,8 @@ $(document).ready(function() {\n \t\t\t\t}\n \t\t\t}\n \t\t} else {\n-\t\t\tmediaArray.push($(\"#media\").attr(\"src\"));\n+\t\t\tmediaArray[0] = $(\"#media\").attr(\"src\");\n+\t\t\t\/\/~ mediaArray[0] = mediaArray[0].substring(Options.server_cache_path.length + 1);\n \t\t\tif (currentMedia.mediaType == \"video\") {\n \t\t\t\ttype = \"v\";\n \t\t\t} else if (currentMedia.mediaType == \"photo\") {\n","files":{"\/scanner\/CachePath.py":{"changes":[{"diff":"\n \tif not filepath:\n \t\tpath = remove_album_path(path)\n \tif path:\n-\t\tpath = path.replace('\/', Options.config['cache_folder_separator']).replace(' ', '_').replace('(', '').replace('&', '').replace(',', '').replace(')', '').replace('#', '').replace('[', '').replace(']', '').replace('\"', '').replace(\"'\", '').replace('_-_', '-').lower()\n+\t\tpath = path.replace('\/', Options.config['cache_folder_separator']).replace(' ', '_').replace('+', '_').replace('(', '').replace('&', '').replace(',', '').replace(')', '').replace('#', '').replace('[', '').replace(']', '').replace('\"', '').replace(\"'\", '').replace('_-_', '-').lower()\n \t\twhile path.find(\"--\") != -1:\n \t\t\tpath = path.replace(\"--\", \"-\")\n \t\twhile path.find(\"__\") != -1:","add":1,"remove":1,"filename":"\/scanner\/CachePath.py","badparts":["\t\tpath = path.replace('\/', Options.config['cache_folder_separator']).replace(' ', '_').replace('(', '').replace('&', '').replace(',', '').replace(')', '').replace('#', '').replace('[', '').replace(']', '').replace('\"', '').replace(\"'\", '').replace('_-_', '-').lower()"],"goodparts":["\t\tpath = path.replace('\/', Options.config['cache_folder_separator']).replace(' ', '_').replace('+', '_').replace('(', '').replace('&', '').replace(',', '').replace(')', '').replace('#', '').replace('[', '').replace(']', '').replace('\"', '').replace(\"'\", '').replace('_-_', '-').lower()"]}],"source":"\nimport os.path from datetime import datetime import hashlib import Options max_verbose=0 def message(category, text, verbose=0): \t \t \t \t \t \t \tglobal usrOptions \ttry: \t\tmax_verbose=Options.config['max_verbose'] \texcept KeyError: \t\tmax_verbose=0 \texcept AttributeError: \t\tmax_verbose=0 \tif(verbose <=max_verbose): \t\tif message.level <=0: \t\t\tsep=\" \" \t\telse: \t\t\tsep=\"--\" \t\tprint \"%s %s%s[%s]%s%s\" %(datetime.now().isoformat(' '), max(0, message.level) * \" |\", sep, str(category), max(1,(45 -len(str(category)))) * \" \", str(text)) message.level=0 def next_level(verbose=0): \tif(verbose <=max_verbose): \t\tmessage.level +=1 def back_level(verbose=0): \tif(verbose <=max_verbose): \t\tmessage.level -=1 def trim_base_custom(path, base): \tif path.startswith(base): \t\tpath=path[len(base):] \tif path.startswith('\/'): \t\tpath=path[1:] \treturn path def remove_album_path(path): \treturn trim_base_custom(path, Options.config['album_path']) def cache_base(path, filepath=False): \tif not filepath: \t\tpath=remove_album_path(path) \tif path: \t\tpath=path.replace('\/', Options.config['cache_folder_separator']).replace(' ', '_').replace('(', '').replace('&', '').replace(',', '').replace(')', '').replace(' \t\twhile path.find(\"--\") !=-1: \t\t\tpath=path.replace(\"--\", \"-\") \t\twhile path.find(\"__\") !=-1: \t\t\tpath=path.replace(\"__\", \"_\") \telse: \t\tpath=\"root\" \treturn path def json_name(path): \treturn cache_base(path) +\".json\" def photo_cache_name(path, size, thumb_type=\"\"): \t \tsuffix=\"_\" +str(size) \tif size==Options.config['album_thumb_size']: \t\tsuffix +=\"a\" \t\tif thumb_type==\"square\": \t\t\tsuffix +=\"s\" \t\telif thumb_type==\"fit\": \t\t\tsuffix +=\"f\" \telif size==Options.config['media_thumb_size']: \t\tsuffix +=\"t\" \t\tif thumb_type==\"square\": \t\t\tsuffix +=\"s\" \t\telif thumb_type==\"fixed_height\": \t\t\tsuffix +=\"f\" \tsuffix +=\".jpg\" \tresult=cache_base(path, True) +suffix \treturn result def video_cache_name(path): \treturn cache_base(path, True) +\"_transcoded_\" +Options.config['video_transcode_bitrate'] +\".mp4\" def file_mtime(path): \treturn datetime.fromtimestamp(int(os.path.getmtime(path))) ","sourceWithComments":"import os.path\nfrom datetime import datetime\nimport hashlib\nimport Options\n\nmax_verbose = 0\ndef message(category, text, verbose = 0):\n\t# verbosity levels:\n\t# 0 = fatal errors only\n\t# 1 = add non-fatal errors\n\t# 2 = add warnings\n\t# 3 = add info\n\t# 4 = add more info\n\tglobal usrOptions\n\ttry:\n\t\tmax_verbose = Options.config['max_verbose']\n\texcept KeyError:\n\t\tmax_verbose = 0\n\texcept AttributeError:\n\t\tmax_verbose = 0\n\tif (verbose <= max_verbose):\n\t\tif message.level <= 0:\n\t\t\tsep = \"  \"\n\t\telse:\n\t\t\tsep = \"--\"\n\t\tprint \"%s %s%s[%s]%s%s\" % (datetime.now().isoformat(' '), max(0, message.level) * \"  |\", sep, str(category), max(1, (45 - len(str(category)))) * \" \", str(text))\n\nmessage.level = 0\ndef next_level(verbose = 0):\n\tif (verbose <= max_verbose):\n\t\tmessage.level += 1\ndef back_level(verbose = 0):\n\tif (verbose <= max_verbose):\n\t\tmessage.level -= 1\ndef trim_base_custom(path, base):\n\tif path.startswith(base):\n\t\tpath = path[len(base):]\n\tif path.startswith('\/'):\n\t\tpath = path[1:]\n\treturn path\ndef remove_album_path(path):\n\treturn trim_base_custom(path, Options.config['album_path'])\ndef cache_base(path, filepath=False):\n\tif not filepath:\n\t\tpath = remove_album_path(path)\n\tif path:\n\t\tpath = path.replace('\/', Options.config['cache_folder_separator']).replace(' ', '_').replace('(', '').replace('&', '').replace(',', '').replace(')', '').replace('#', '').replace('[', '').replace(']', '').replace('\"', '').replace(\"'\", '').replace('_-_', '-').lower()\n\t\twhile path.find(\"--\") != -1:\n\t\t\tpath = path.replace(\"--\", \"-\")\n\t\twhile path.find(\"__\") != -1:\n\t\t\tpath = path.replace(\"__\", \"_\")\n\telse:\n\t\tpath = \"root\"\n\treturn path\ndef json_name(path):\n\treturn cache_base(path) + \".json\"\ndef photo_cache_name(path, size, thumb_type = \"\"):\n\t# this function is used for video thumbnails too\n\tsuffix = \"_\" + str(size)\n\tif size == Options.config['album_thumb_size']:\n\t\tsuffix += \"a\"\n\t\tif thumb_type == \"square\":\n\t\t\tsuffix += \"s\"\n\t\telif thumb_type == \"fit\":\n\t\t\tsuffix += \"f\"\n\telif size == Options.config['media_thumb_size']:\n\t\tsuffix += \"t\"\n\t\tif thumb_type == \"square\":\n\t\t\tsuffix += \"s\"\n\t\telif thumb_type == \"fixed_height\":\n\t\t\tsuffix += \"f\"\n\tsuffix += \".jpg\"\n\tresult = cache_base(path, True) + suffix\n\treturn result\ndef video_cache_name(path):\n\treturn cache_base(path, True) + \"_transcoded_\" + Options.config['video_transcode_bitrate'] + \".mp4\"\ndef file_mtime(path):\n\treturn datetime.fromtimestamp(int(os.path.getmtime(path)))\n"}},"msg":"fixed directory traversal security vulnerability; file name can now contain +"}},"https:\/\/github.com\/wardi\/django-filebrowser-no-grappelli":{"681868ea093d7c54ffb98865576497b1ba3912a1":{"url":"https:\/\/api.github.com\/repos\/wardi\/django-filebrowser-no-grappelli\/commits\/681868ea093d7c54ffb98865576497b1ba3912a1","html_url":"https:\/\/github.com\/wardi\/django-filebrowser-no-grappelli\/commit\/681868ea093d7c54ffb98865576497b1ba3912a1","message":"fix a browse directory traversal vulnerability","sha":"681868ea093d7c54ffb98865576497b1ba3912a1","keyword":"directory traversal vulnerability","diff":"diff --git a\/filebrowser\/views.py b\/filebrowser\/views.py\nindex ec0d713..e0d1d2e 100644\n--- a\/filebrowser\/views.py\n+++ b\/filebrowser\/views.py\n@@ -7,7 +7,7 @@\n # django imports\n from django.shortcuts import render_to_response, HttpResponse\n from django.template import RequestContext as Context\n-from django.http import HttpResponseRedirect\n+from django.http import HttpResponseRedirect, Http404\n from django.contrib.admin.views.decorators import staff_member_required\n from django.views.decorators.cache import never_cache\n from django.utils.translation import ugettext as _\n@@ -53,7 +53,15 @@ def browse(request):\n     query = request.GET.copy()\n     path = get_path(query.get('dir', ''))\n     directory = get_path('')\n-    \n+\n+    if path is not None:\n+        abs_path = os.path.abspath(os.path.join(\n+            fb_settings.MEDIA_ROOT, fb_settings.DIRECTORY, path))\n+        if not abs_path.startswith(os.path.abspath(os.path.join(\n+                fb_settings.MEDIA_ROOT, fb_settings.DIRECTORY))):\n+            # cause any attempt to leave media root directory to fail\n+            raise Http404\n+\n     if path is None:\n         msg = _('The requested Folder does not exist.')\n         messages.warning(request,message=msg)\n@@ -62,7 +70,6 @@ def browse(request):\n             raise ImproperlyConfigured, _(\"Error finding Upload-Folder. Maybe it does not exist?\")\n         redirect_url = reverse(\"fb_browse\") + query_helper(query, \"\", \"dir\")\n         return HttpResponseRedirect(redirect_url)\n-    abs_path = os.path.join(fb_settings.MEDIA_ROOT, fb_settings.DIRECTORY, path)\n     \n     # INITIAL VARIABLES\n     results_var = {'results_total': 0, 'results_current': 0, 'delete_total': 0, 'images_total': 0, 'select_total': 0 }\n","files":{"\/filebrowser\/views.py":{"changes":[{"diff":"\n # django imports\n from django.shortcuts import render_to_response, HttpResponse\n from django.template import RequestContext as Context\n-from django.http import HttpResponseRedirect\n+from django.http import HttpResponseRedirect, Http404\n from django.contrib.admin.views.decorators import staff_member_required\n from django.views.decorators.cache import never_cache\n from django.utils.translation import ugettext as _\n","add":1,"remove":1,"filename":"\/filebrowser\/views.py","badparts":["from django.http import HttpResponseRedirect"],"goodparts":["from django.http import HttpResponseRedirect, Http404"]},{"diff":"\n             raise ImproperlyConfigured, _(\"Error finding Upload-Folder. Maybe it does not exist?\")\n         redirect_url = reverse(\"fb_browse\") + query_helper(query, \"\", \"dir\")\n         return HttpResponseRedirect(redirect_url)\n-    abs_path = os.path.join(fb_settings.MEDIA_ROOT, fb_settings.DIRECTORY, path)\n     \n     # INITIAL VARIABLES\n     results_var = {'results_total': 0, 'results_current': 0, 'delete_total': 0, 'images_total': 0, 'select_total': 0 }\n","add":0,"remove":1,"filename":"\/filebrowser\/views.py","badparts":["    abs_path = os.path.join(fb_settings.MEDIA_ROOT, fb_settings.DIRECTORY, path)"],"goodparts":[]}],"source":"\n import os, re from time import gmtime, strftime from django.shortcuts import render_to_response, HttpResponse from django.template import RequestContext as Context from django.http import HttpResponseRedirect from django.contrib.admin.views.decorators import staff_member_required from django.views.decorators.cache import never_cache from django.utils.translation import ugettext as _ from django.conf import settings from django import forms from django.core.urlresolvers import reverse from django.core.exceptions import ImproperlyConfigured from django.dispatch import Signal from django.core.paginator import Paginator, InvalidPage, EmptyPage from django.utils.encoding import smart_str try: from django.views.decorators.csrf import csrf_exempt except: from django.contrib.csrf.middleware import csrf_exempt from django.contrib import messages from filebrowser.settings import * from filebrowser.conf import fb_settings from filebrowser.functions import path_to_url, sort_by_attr, get_path, get_file, get_version_path, get_breadcrumbs, get_filterdate, get_settings_var, handle_file_upload, convert_filename from filebrowser.templatetags.fb_tags import query_helper from filebrowser.base import FileObject from filebrowser.decorators import flash_login_required filter_re=[] for exp in EXCLUDE: filter_re.append(re.compile(exp)) for k,v in VERSIONS.iteritems(): exp=(r'_%s.(%s)') %(k, '|'.join(EXTENSION_LIST)) filter_re.append(re.compile(exp)) def browse(request): \"\"\" Browse Files\/Directories. \"\"\" query=request.GET.copy() path=get_path(query.get('dir', '')) directory=get_path('') if path is None: msg=_('The requested Folder does not exist.') messages.warning(request,message=msg) if directory is None: raise ImproperlyConfigured, _(\"Error finding Upload-Folder. Maybe it does not exist?\") redirect_url=reverse(\"fb_browse\") +query_helper(query, \"\", \"dir\") return HttpResponseRedirect(redirect_url) abs_path=os.path.join(fb_settings.MEDIA_ROOT, fb_settings.DIRECTORY, path) results_var={'results_total': 0, 'results_current': 0, 'delete_total': 0, 'images_total': 0, 'select_total': 0} counter={} for k,v in EXTENSIONS.iteritems(): counter[k]=0 dir_list=os.listdir(abs_path) files=[] for file in dir_list: filtered=file.startswith('.') for re_prefix in filter_re: if re_prefix.search(file): filtered=True if filtered: continue results_var['results_total'] +=1 fileobject=FileObject(os.path.join(fb_settings.DIRECTORY, path, file)) append=False if fileobject.filetype==request.GET.get('filter_type', fileobject.filetype) and get_filterdate(request.GET.get('filter_date', ''), fileobject.date): append=True if request.GET.get('q') and not re.compile(request.GET.get('q').lower(), re.M).search(file.lower()): append=False if append: try: if fileobject.filetype=='Image': results_var['images_total'] +=1 if fileobject.filetype !='Folder': results_var['delete_total'] +=1 elif fileobject.filetype=='Folder' and fileobject.is_empty: results_var['delete_total'] +=1 if query.get('type') and query.get('type') in SELECT_FORMATS and fileobject.filetype in SELECT_FORMATS[query.get('type')]: results_var['select_total'] +=1 elif not query.get('type'): results_var['select_total'] +=1 except OSError: continue else: files.append(fileobject) results_var['results_current'] +=1 if fileobject.filetype: counter[fileobject.filetype] +=1 query['o']=request.GET.get('o', DEFAULT_SORTING_BY) query['ot']=request.GET.get('ot', DEFAULT_SORTING_ORDER) files=sort_by_attr(files, request.GET.get('o', DEFAULT_SORTING_BY)) if not request.GET.get('ot') and DEFAULT_SORTING_ORDER==\"desc\" or request.GET.get('ot')==\"desc\": files.reverse() p=Paginator(files, LIST_PER_PAGE) try: page_nr=request.GET.get('p', '1') except: page_nr=1 try: page=p.page(page_nr) except(EmptyPage, InvalidPage): page=p.page(p.num_pages) return render_to_response('filebrowser\/index.html',{ 'dir': path, 'p': p, 'page': page, 'results_var': results_var, 'counter': counter, 'query': query, 'title': _(u'FileBrowser'), 'settings_var': get_settings_var(), 'breadcrumbs': get_breadcrumbs(query, path), 'breadcrumbs_title': \"\" }, context_instance=Context(request)) browse=staff_member_required(never_cache(browse)) filebrowser_pre_createdir=Signal(providing_args=[\"path\", \"dirname\"]) filebrowser_post_createdir=Signal(providing_args=[\"path\", \"dirname\"]) def mkdir(request): \"\"\" Make Directory. \"\"\" from filebrowser.forms import MakeDirForm query=request.GET path=get_path(query.get('dir', '')) if path is None: msg=_('The requested Folder does not exist.') messages.warning(request,message=msg) return HttpResponseRedirect(reverse(\"fb_browse\")) abs_path=os.path.join(fb_settings.MEDIA_ROOT, fb_settings.DIRECTORY, path) if request.method=='POST': form=MakeDirForm(abs_path, request.POST) if form.is_valid(): server_path=os.path.join(abs_path, form.cleaned_data['dir_name']) try: filebrowser_pre_createdir.send(sender=request, path=path, dirname=form.cleaned_data['dir_name']) os.mkdir(server_path) os.chmod(server_path, 0775) filebrowser_post_createdir.send(sender=request, path=path, dirname=form.cleaned_data['dir_name']) msg=_('The Folder %s was successfully created.') %(form.cleaned_data['dir_name']) messages.success(request,message=msg) redirect_url=reverse(\"fb_browse\") +query_helper(query, \"ot=desc,o=date\", \"ot,o,filter_type,filter_date,q,p\") return HttpResponseRedirect(redirect_url) except OSError,(errno, strerror): if errno==13: form.errors['dir_name']=forms.util.ErrorList([_('Permission denied.')]) else: form.errors['dir_name']=forms.util.ErrorList([_('Error creating folder.')]) else: form=MakeDirForm(abs_path) return render_to_response('filebrowser\/makedir.html',{ 'form': form, 'query': query, 'title': _(u'New Folder'), 'settings_var': get_settings_var(), 'breadcrumbs': get_breadcrumbs(query, path), 'breadcrumbs_title': _(u'New Folder') }, context_instance=Context(request)) mkdir=staff_member_required(never_cache(mkdir)) def upload(request): \"\"\" Multipe File Upload. \"\"\" from django.http import parse_cookie query=request.GET path=get_path(query.get('dir', '')) if path is None: msg=_('The requested Folder does not exist.') messages.warning(request,message=msg) return HttpResponseRedirect(reverse(\"fb_browse\")) abs_path=os.path.join(fb_settings.MEDIA_ROOT, fb_settings.DIRECTORY, path) cookie_dict=parse_cookie(request.META.get('HTTP_COOKIE', '')) engine=__import__(settings.SESSION_ENGINE,{},{},['']) session_key=cookie_dict.get(settings.SESSION_COOKIE_NAME, None) return render_to_response('filebrowser\/upload.html',{ 'query': query, 'title': _(u'Select files to upload'), 'settings_var': get_settings_var(), 'session_key': session_key, 'breadcrumbs': get_breadcrumbs(query, path), 'breadcrumbs_title': _(u'Upload') }, context_instance=Context(request)) upload=staff_member_required(never_cache(upload)) @csrf_exempt def _check_file(request): \"\"\" Check if file already exists on the server. \"\"\" from django.utils import simplejson folder=request.POST.get('folder') fb_uploadurl_re=re.compile(r'^.*(%s)' % reverse(\"fb_upload\")) folder=fb_uploadurl_re.sub('', folder) fileArray={} if request.method=='POST': for k,v in request.POST.items(): if k !=\"folder\": v=convert_filename(v) if os.path.isfile(smart_str(os.path.join(fb_settings.MEDIA_ROOT, fb_settings.DIRECTORY, folder, v))): fileArray[k]=v return HttpResponse(simplejson.dumps(fileArray)) filebrowser_pre_upload=Signal(providing_args=[\"path\", \"file\"]) filebrowser_post_upload=Signal(providing_args=[\"path\", \"file\"]) @csrf_exempt @flash_login_required def _upload_file(request): \"\"\" Upload file to the server. \"\"\" from django.core.files.move import file_move_safe if request.method=='POST': folder=request.POST.get('folder') fb_uploadurl_re=re.compile(r'^.*(%s)' % reverse(\"fb_upload\")) folder=fb_uploadurl_re.sub('', folder) abs_path=os.path.join(fb_settings.MEDIA_ROOT, fb_settings.DIRECTORY, folder) if request.FILES: filedata=request.FILES['Filedata'] filedata.name=convert_filename(filedata.name) filebrowser_pre_upload.send(sender=request, path=request.POST.get('folder'), file=filedata) uploadedfile=handle_file_upload(abs_path, filedata) if os.path.isfile(smart_str(os.path.join(fb_settings.MEDIA_ROOT, fb_settings.DIRECTORY, folder, filedata.name))): old_file=smart_str(os.path.join(abs_path, filedata.name)) new_file=smart_str(os.path.join(abs_path, uploadedfile)) file_move_safe(new_file, old_file) filebrowser_post_upload.send(sender=request, path=request.POST.get('folder'), file=FileObject(smart_str(os.path.join(fb_settings.DIRECTORY, folder, filedata.name)))) return HttpResponse('True') filebrowser_pre_delete=Signal(providing_args=[\"path\", \"filename\"]) filebrowser_post_delete=Signal(providing_args=[\"path\", \"filename\"]) def delete(request): \"\"\" Delete existing File\/Directory. When trying to delete a Directory, the Directory has to be empty. \"\"\" query=request.GET path=get_path(query.get('dir', '')) filename=get_file(query.get('dir', ''), query.get('filename', '')) if path is None or filename is None: if path is None: msg=_('The requested Folder does not exist.') else: msg=_('The requested File does not exist.') messages.warning(request,message=msg) return HttpResponseRedirect(reverse(\"fb_browse\")) abs_path=os.path.join(fb_settings.MEDIA_ROOT, fb_settings.DIRECTORY, path) msg=\"\" if request.GET: if request.GET.get('filetype') !=\"Folder\": relative_server_path=os.path.join(fb_settings.DIRECTORY, path, filename) try: filebrowser_pre_delete.send(sender=request, path=path, filename=filename) for version in VERSIONS: try: os.unlink(os.path.join(fb_settings.MEDIA_ROOT, get_version_path(relative_server_path, version))) except: pass os.unlink(smart_str(os.path.join(abs_path, filename))) filebrowser_post_delete.send(sender=request, path=path, filename=filename) msg=_('The file %s was successfully deleted.') %(filename.lower()) messages.success(request,message=msg) redirect_url=reverse(\"fb_browse\") +query_helper(query, \"\", \"filename,filetype\") return HttpResponseRedirect(redirect_url) except OSError: msg=OSError else: try: filebrowser_pre_delete.send(sender=request, path=path, filename=filename) os.rmdir(os.path.join(abs_path, filename)) filebrowser_post_delete.send(sender=request, path=path, filename=filename) msg=_('The folder %s was successfully deleted.') %(filename.lower()) messages.success(request,message=msg) redirect_url=reverse(\"fb_browse\") +query_helper(query, \"\", \"filename,filetype\") return HttpResponseRedirect(redirect_url) except OSError: msg=OSError if msg: request.user.message_set.create(message=msg) return render_to_response('filebrowser\/index.html',{ 'dir': dir_name, 'file': request.GET.get('filename', ''), 'query': query, 'settings_var': get_settings_var(), 'breadcrumbs': get_breadcrumbs(query, dir_name), 'breadcrumbs_title': \"\" }, context_instance=Context(request)) delete=staff_member_required(never_cache(delete)) filebrowser_pre_rename=Signal(providing_args=[\"path\", \"filename\", \"new_filename\"]) filebrowser_post_rename=Signal(providing_args=[\"path\", \"filename\", \"new_filename\"]) def rename(request): \"\"\" Rename existing File\/Directory. Includes renaming existing Image Versions\/Thumbnails. \"\"\" from filebrowser.forms import RenameForm query=request.GET path=get_path(query.get('dir', '')) filename=get_file(query.get('dir', ''), query.get('filename', '')) if path is None or filename is None: if path is None: msg=_('The requested Folder does not exist.') else: msg=_('The requested File does not exist.') messages.warning(request,message=msg) return HttpResponseRedirect(reverse(\"fb_browse\")) abs_path=os.path.join(fb_settings.MEDIA_ROOT, fb_settings.DIRECTORY, path) file_extension=os.path.splitext(filename)[1].lower() if request.method=='POST': form=RenameForm(abs_path, file_extension, request.POST) if form.is_valid(): relative_server_path=os.path.join(fb_settings.DIRECTORY, path, filename) new_filename=form.cleaned_data['name'] +file_extension new_relative_server_path=os.path.join(fb_settings.DIRECTORY, path, new_filename) try: filebrowser_pre_rename.send(sender=request, path=path, filename=filename, new_filename=new_filename) for version in VERSIONS: try: os.unlink(os.path.join(fb_settings.MEDIA_ROOT, get_version_path(relative_server_path, version))) except: pass os.rename(os.path.join(fb_settings.MEDIA_ROOT, relative_server_path), os.path.join(fb_settings.MEDIA_ROOT, new_relative_server_path)) filebrowser_post_rename.send(sender=request, path=path, filename=filename, new_filename=new_filename) msg=_('Renaming was successful.') messages.success(request,message=msg) redirect_url=reverse(\"fb_browse\") +query_helper(query, \"\", \"filename\") return HttpResponseRedirect(redirect_url) except OSError,(errno, strerror): form.errors['name']=forms.util.ErrorList([_('Error.')]) else: form=RenameForm(abs_path, file_extension) return render_to_response('filebrowser\/rename.html',{ 'form': form, 'query': query, 'file_extension': file_extension, 'title': _(u'Rename \"%s\"') % filename, 'settings_var': get_settings_var(), 'breadcrumbs': get_breadcrumbs(query, path), 'breadcrumbs_title': _(u'Rename') }, context_instance=Context(request)) rename=staff_member_required(never_cache(rename)) def versions(request): \"\"\" Show all Versions for an Image according to ADMIN_VERSIONS. \"\"\" query=request.GET path=get_path(query.get('dir', '')) filename=get_file(query.get('dir', ''), query.get('filename', '')) if path is None or filename is None: if path is None: msg=_('The requested Folder does not exist.') else: msg=_('The requested File does not exist.') messages.warning(request,message=msg) return HttpResponseRedirect(reverse(\"fb_browse\")) abs_path=os.path.join(fb_settings.MEDIA_ROOT, fb_settings.DIRECTORY, path) return render_to_response('filebrowser\/versions.html',{ 'original': path_to_url(os.path.join(fb_settings.DIRECTORY, path, filename)), 'query': query, 'title': _(u'Versions for \"%s\"') % filename, 'settings_var': get_settings_var(), 'breadcrumbs': get_breadcrumbs(query, path), 'breadcrumbs_title': _(u'Versions for \"%s\"') % filename }, context_instance=Context(request)) versions=staff_member_required(never_cache(versions)) ","sourceWithComments":"# coding: utf-8\n\n# general imports\nimport os, re\nfrom time import gmtime, strftime\n\n# django imports\nfrom django.shortcuts import render_to_response, HttpResponse\nfrom django.template import RequestContext as Context\nfrom django.http import HttpResponseRedirect\nfrom django.contrib.admin.views.decorators import staff_member_required\nfrom django.views.decorators.cache import never_cache\nfrom django.utils.translation import ugettext as _\nfrom django.conf import settings\nfrom django import forms\nfrom django.core.urlresolvers import reverse\nfrom django.core.exceptions import ImproperlyConfigured\nfrom django.dispatch import Signal\nfrom django.core.paginator import Paginator, InvalidPage, EmptyPage\nfrom django.utils.encoding import smart_str\n\ntry:\n    # django SVN\n    from django.views.decorators.csrf import csrf_exempt\nexcept:\n    # django 1.1\n    from django.contrib.csrf.middleware import csrf_exempt\n\nfrom django.contrib import messages\n\n# filebrowser imports\nfrom filebrowser.settings import *\nfrom filebrowser.conf import fb_settings\nfrom filebrowser.functions import path_to_url, sort_by_attr, get_path, get_file, get_version_path, get_breadcrumbs, get_filterdate, get_settings_var, handle_file_upload, convert_filename\nfrom filebrowser.templatetags.fb_tags import query_helper\nfrom filebrowser.base import FileObject\nfrom filebrowser.decorators import flash_login_required\n\n# Precompile regular expressions\nfilter_re = []\nfor exp in EXCLUDE:\n   filter_re.append(re.compile(exp))\nfor k,v in VERSIONS.iteritems():\n    exp = (r'_%s.(%s)') % (k, '|'.join(EXTENSION_LIST))\n    filter_re.append(re.compile(exp))\n\n\ndef browse(request):\n    \"\"\"\n    Browse Files\/Directories.\n    \"\"\"\n    # QUERY \/ PATH CHECK\n    query = request.GET.copy()\n    path = get_path(query.get('dir', ''))\n    directory = get_path('')\n    \n    if path is None:\n        msg = _('The requested Folder does not exist.')\n        messages.warning(request,message=msg)\n        if directory is None:\n            # The DIRECTORY does not exist, raise an error to prevent eternal redirecting.\n            raise ImproperlyConfigured, _(\"Error finding Upload-Folder. Maybe it does not exist?\")\n        redirect_url = reverse(\"fb_browse\") + query_helper(query, \"\", \"dir\")\n        return HttpResponseRedirect(redirect_url)\n    abs_path = os.path.join(fb_settings.MEDIA_ROOT, fb_settings.DIRECTORY, path)\n    \n    # INITIAL VARIABLES\n    results_var = {'results_total': 0, 'results_current': 0, 'delete_total': 0, 'images_total': 0, 'select_total': 0 }\n    counter = {}\n    for k,v in EXTENSIONS.iteritems():\n        counter[k] = 0\n    \n    dir_list = os.listdir(abs_path)\n    files = []\n    for file in dir_list:\n        \n        # EXCLUDE FILES MATCHING VERSIONS_PREFIX OR ANY OF THE EXCLUDE PATTERNS\n        filtered = file.startswith('.')\n        for re_prefix in filter_re:\n            if re_prefix.search(file):\n                filtered = True\n        if filtered:\n            continue\n        results_var['results_total'] += 1\n        \n        # CREATE FILEOBJECT\n        fileobject = FileObject(os.path.join(fb_settings.DIRECTORY, path, file))\n        \n        # FILTER \/ SEARCH\n        append = False\n        if fileobject.filetype == request.GET.get('filter_type', fileobject.filetype) and get_filterdate(request.GET.get('filter_date', ''), fileobject.date):\n            append = True\n        if request.GET.get('q') and not re.compile(request.GET.get('q').lower(), re.M).search(file.lower()):\n            append = False\n        \n        # APPEND FILE_LIST\n        if append:\n            try:\n                # COUNTER\/RESULTS\n                if fileobject.filetype == 'Image':\n                    results_var['images_total'] += 1\n                if fileobject.filetype != 'Folder':\n                    results_var['delete_total'] += 1\n                elif fileobject.filetype == 'Folder' and fileobject.is_empty:\n                    results_var['delete_total'] += 1\n                if query.get('type') and query.get('type') in SELECT_FORMATS and fileobject.filetype in SELECT_FORMATS[query.get('type')]:\n                    results_var['select_total'] += 1\n                elif not query.get('type'):\n                    results_var['select_total'] += 1\n            except OSError:\n                # Ignore items that have problems\n                continue\n            else:\n                files.append(fileobject)\n                results_var['results_current'] += 1\n        \n        # COUNTER\/RESULTS\n        if fileobject.filetype:\n            counter[fileobject.filetype] += 1\n    \n    # SORTING\n    query['o'] = request.GET.get('o', DEFAULT_SORTING_BY)\n    query['ot'] = request.GET.get('ot', DEFAULT_SORTING_ORDER)\n    files = sort_by_attr(files, request.GET.get('o', DEFAULT_SORTING_BY))\n    if not request.GET.get('ot') and DEFAULT_SORTING_ORDER == \"desc\" or request.GET.get('ot') == \"desc\":\n        files.reverse()\n    \n    p = Paginator(files, LIST_PER_PAGE)\n    try:\n        page_nr = request.GET.get('p', '1')\n    except:\n        page_nr = 1\n    try:\n        page = p.page(page_nr)\n    except (EmptyPage, InvalidPage):\n        page = p.page(p.num_pages)\n    \n    return render_to_response('filebrowser\/index.html', {\n        'dir': path,\n        'p': p,\n        'page': page,\n        'results_var': results_var,\n        'counter': counter,\n        'query': query,\n        'title': _(u'FileBrowser'),\n        'settings_var': get_settings_var(),\n        'breadcrumbs': get_breadcrumbs(query, path),\n        'breadcrumbs_title': \"\"\n    }, context_instance=Context(request))\nbrowse = staff_member_required(never_cache(browse))\n\n\n# mkdir signals\nfilebrowser_pre_createdir = Signal(providing_args=[\"path\", \"dirname\"])\nfilebrowser_post_createdir = Signal(providing_args=[\"path\", \"dirname\"])\n\ndef mkdir(request):\n    \"\"\"\n    Make Directory.\n    \"\"\"\n    \n    from filebrowser.forms import MakeDirForm\n    \n    # QUERY \/ PATH CHECK\n    query = request.GET\n    path = get_path(query.get('dir', ''))\n    if path is None:\n        msg = _('The requested Folder does not exist.')\n        messages.warning(request,message=msg)\n        return HttpResponseRedirect(reverse(\"fb_browse\"))\n    abs_path = os.path.join(fb_settings.MEDIA_ROOT, fb_settings.DIRECTORY, path)\n    \n    if request.method == 'POST':\n        form = MakeDirForm(abs_path, request.POST)\n        if form.is_valid():\n            server_path = os.path.join(abs_path, form.cleaned_data['dir_name'])\n            try:\n                # PRE CREATE SIGNAL\n                filebrowser_pre_createdir.send(sender=request, path=path, dirname=form.cleaned_data['dir_name'])\n                # CREATE FOLDER\n                os.mkdir(server_path)\n                os.chmod(server_path, 0775)\n                # POST CREATE SIGNAL\n                filebrowser_post_createdir.send(sender=request, path=path, dirname=form.cleaned_data['dir_name'])\n                # MESSAGE & REDIRECT\n                msg = _('The Folder %s was successfully created.') % (form.cleaned_data['dir_name'])\n                messages.success(request,message=msg)\n                # on redirect, sort by date desc to see the new directory on top of the list\n                # remove filter in order to actually _see_ the new folder\n                # remove pagination\n                redirect_url = reverse(\"fb_browse\") + query_helper(query, \"ot=desc,o=date\", \"ot,o,filter_type,filter_date,q,p\")\n                return HttpResponseRedirect(redirect_url)\n            except OSError, (errno, strerror):\n                if errno == 13:\n                    form.errors['dir_name'] = forms.util.ErrorList([_('Permission denied.')])\n                else:\n                    form.errors['dir_name'] = forms.util.ErrorList([_('Error creating folder.')])\n    else:\n        form = MakeDirForm(abs_path)\n    \n    return render_to_response('filebrowser\/makedir.html', {\n        'form': form,\n        'query': query,\n        'title': _(u'New Folder'),\n        'settings_var': get_settings_var(),\n        'breadcrumbs': get_breadcrumbs(query, path),\n        'breadcrumbs_title': _(u'New Folder')\n    }, context_instance=Context(request))\nmkdir = staff_member_required(never_cache(mkdir))\n\n\ndef upload(request):\n    \"\"\"\n    Multipe File Upload.\n    \"\"\"\n    \n    from django.http import parse_cookie\n    \n    # QUERY \/ PATH CHECK\n    query = request.GET\n    path = get_path(query.get('dir', ''))\n    if path is None:\n        msg = _('The requested Folder does not exist.')\n        messages.warning(request,message=msg)\n        return HttpResponseRedirect(reverse(\"fb_browse\"))\n    abs_path = os.path.join(fb_settings.MEDIA_ROOT, fb_settings.DIRECTORY, path)\n    \n    # SESSION (used for flash-uploading)\n    cookie_dict = parse_cookie(request.META.get('HTTP_COOKIE', ''))\n    engine = __import__(settings.SESSION_ENGINE, {}, {}, [''])\n    session_key = cookie_dict.get(settings.SESSION_COOKIE_NAME, None)\n    \n    return render_to_response('filebrowser\/upload.html', {\n        'query': query,\n        'title': _(u'Select files to upload'),\n        'settings_var': get_settings_var(),\n        'session_key': session_key,\n        'breadcrumbs': get_breadcrumbs(query, path),\n        'breadcrumbs_title': _(u'Upload')\n    }, context_instance=Context(request))\nupload = staff_member_required(never_cache(upload))\n\n\n@csrf_exempt\ndef _check_file(request):\n    \"\"\"\n    Check if file already exists on the server.\n    \"\"\"\n    \n    from django.utils import simplejson\n    \n    folder = request.POST.get('folder')\n    fb_uploadurl_re = re.compile(r'^.*(%s)' % reverse(\"fb_upload\"))\n    folder = fb_uploadurl_re.sub('', folder)\n    \n    fileArray = {}\n    if request.method == 'POST':\n        for k,v in request.POST.items():\n            if k != \"folder\":\n                v = convert_filename(v)\n                if os.path.isfile(smart_str(os.path.join(fb_settings.MEDIA_ROOT, fb_settings.DIRECTORY, folder, v))):\n                    fileArray[k] = v\n    \n    return HttpResponse(simplejson.dumps(fileArray))\n\n\n# upload signals\nfilebrowser_pre_upload = Signal(providing_args=[\"path\", \"file\"])\nfilebrowser_post_upload = Signal(providing_args=[\"path\", \"file\"])\n\n@csrf_exempt\n@flash_login_required\ndef _upload_file(request):\n    \"\"\"\n    Upload file to the server.\n    \"\"\"\n    \n    from django.core.files.move import file_move_safe\n    \n    if request.method == 'POST':\n        folder = request.POST.get('folder')\n        fb_uploadurl_re = re.compile(r'^.*(%s)' % reverse(\"fb_upload\"))\n        folder = fb_uploadurl_re.sub('', folder)\n        abs_path = os.path.join(fb_settings.MEDIA_ROOT, fb_settings.DIRECTORY, folder)\n        if request.FILES:\n            filedata = request.FILES['Filedata']\n            filedata.name = convert_filename(filedata.name)\n            # PRE UPLOAD SIGNAL\n            filebrowser_pre_upload.send(sender=request, path=request.POST.get('folder'), file=filedata)\n            # HANDLE UPLOAD\n            uploadedfile = handle_file_upload(abs_path, filedata)\n            # MOVE UPLOADED FILE\n            # if file already exists\n            if os.path.isfile(smart_str(os.path.join(fb_settings.MEDIA_ROOT, fb_settings.DIRECTORY, folder, filedata.name))):\n                old_file = smart_str(os.path.join(abs_path, filedata.name))\n                new_file = smart_str(os.path.join(abs_path, uploadedfile))\n                file_move_safe(new_file, old_file)\n            # POST UPLOAD SIGNAL\n            filebrowser_post_upload.send(sender=request, path=request.POST.get('folder'), file=FileObject(smart_str(os.path.join(fb_settings.DIRECTORY, folder, filedata.name))))\n    return HttpResponse('True')\n#_upload_file = flash_login_required(_upload_file)\n\n\n# delete signals\nfilebrowser_pre_delete = Signal(providing_args=[\"path\", \"filename\"])\nfilebrowser_post_delete = Signal(providing_args=[\"path\", \"filename\"])\n\ndef delete(request):\n    \"\"\"\n    Delete existing File\/Directory.\n    \n    When trying to delete a Directory, the Directory has to be empty.\n    \"\"\"\n    \n    # QUERY \/ PATH CHECK\n    query = request.GET\n    path = get_path(query.get('dir', ''))\n    filename = get_file(query.get('dir', ''), query.get('filename', ''))\n    if path is None or filename is None:\n        if path is None:\n            msg = _('The requested Folder does not exist.')\n        else:\n            msg = _('The requested File does not exist.')\n        messages.warning(request,message=msg)\n        return HttpResponseRedirect(reverse(\"fb_browse\"))\n    abs_path = os.path.join(fb_settings.MEDIA_ROOT, fb_settings.DIRECTORY, path)\n    \n    msg = \"\"\n    if request.GET:\n        if request.GET.get('filetype') != \"Folder\":\n            relative_server_path = os.path.join(fb_settings.DIRECTORY, path, filename)\n            try:\n                # PRE DELETE SIGNAL\n                filebrowser_pre_delete.send(sender=request, path=path, filename=filename)\n                # DELETE IMAGE VERSIONS\/THUMBNAILS\n                for version in VERSIONS:\n                    try:\n                        os.unlink(os.path.join(fb_settings.MEDIA_ROOT, get_version_path(relative_server_path, version)))\n                    except:\n                        pass\n                # DELETE FILE\n                os.unlink(smart_str(os.path.join(abs_path, filename)))\n                # POST DELETE SIGNAL\n                filebrowser_post_delete.send(sender=request, path=path, filename=filename)\n                # MESSAGE & REDIRECT\n                msg = _('The file %s was successfully deleted.') % (filename.lower())\n                messages.success(request,message=msg)\n                redirect_url = reverse(\"fb_browse\") + query_helper(query, \"\", \"filename,filetype\")\n                return HttpResponseRedirect(redirect_url)\n            except OSError:\n                # todo: define error message\n                msg = OSError\n        else:\n            try:\n                # PRE DELETE SIGNAL\n                filebrowser_pre_delete.send(sender=request, path=path, filename=filename)\n                # DELETE FOLDER\n                os.rmdir(os.path.join(abs_path, filename))\n                # POST DELETE SIGNAL\n                filebrowser_post_delete.send(sender=request, path=path, filename=filename)\n                # MESSAGE & REDIRECT\n                msg = _('The folder %s was successfully deleted.') % (filename.lower())\n                messages.success(request,message=msg)\n                redirect_url = reverse(\"fb_browse\") + query_helper(query, \"\", \"filename,filetype\")\n                return HttpResponseRedirect(redirect_url)\n            except OSError:\n                # todo: define error message\n                msg = OSError\n    \n    if msg:\n        request.user.message_set.create(message=msg)\n    \n    return render_to_response('filebrowser\/index.html', {\n        'dir': dir_name,\n        'file': request.GET.get('filename', ''),\n        'query': query,\n        'settings_var': get_settings_var(),\n        'breadcrumbs': get_breadcrumbs(query, dir_name),\n        'breadcrumbs_title': \"\"\n    }, context_instance=Context(request))\ndelete = staff_member_required(never_cache(delete))\n\n\n# rename signals\nfilebrowser_pre_rename = Signal(providing_args=[\"path\", \"filename\", \"new_filename\"])\nfilebrowser_post_rename = Signal(providing_args=[\"path\", \"filename\", \"new_filename\"])\n\ndef rename(request):\n    \"\"\"\n    Rename existing File\/Directory.\n    \n    Includes renaming existing Image Versions\/Thumbnails.\n    \"\"\"\n    \n    from filebrowser.forms import RenameForm\n    \n    # QUERY \/ PATH CHECK\n    query = request.GET\n    path = get_path(query.get('dir', ''))\n    filename = get_file(query.get('dir', ''), query.get('filename', ''))\n    if path is None or filename is None:\n        if path is None:\n            msg = _('The requested Folder does not exist.')\n        else:\n            msg = _('The requested File does not exist.')\n        messages.warning(request,message=msg)\n        return HttpResponseRedirect(reverse(\"fb_browse\"))\n    abs_path = os.path.join(fb_settings.MEDIA_ROOT, fb_settings.DIRECTORY, path)\n    file_extension = os.path.splitext(filename)[1].lower()\n    \n    if request.method == 'POST':\n        form = RenameForm(abs_path, file_extension, request.POST)\n        if form.is_valid():\n            relative_server_path = os.path.join(fb_settings.DIRECTORY, path, filename)\n            new_filename = form.cleaned_data['name'] + file_extension\n            new_relative_server_path = os.path.join(fb_settings.DIRECTORY, path, new_filename)\n            try:\n                # PRE RENAME SIGNAL\n                filebrowser_pre_rename.send(sender=request, path=path, filename=filename, new_filename=new_filename)\n                # DELETE IMAGE VERSIONS\/THUMBNAILS\n                # regenerating versions\/thumbs will be done automatically\n                for version in VERSIONS:\n                    try:\n                        os.unlink(os.path.join(fb_settings.MEDIA_ROOT, get_version_path(relative_server_path, version)))\n                    except:\n                        pass\n                # RENAME ORIGINAL\n                os.rename(os.path.join(fb_settings.MEDIA_ROOT, relative_server_path), os.path.join(fb_settings.MEDIA_ROOT, new_relative_server_path))\n                # POST RENAME SIGNAL\n                filebrowser_post_rename.send(sender=request, path=path, filename=filename, new_filename=new_filename)\n                # MESSAGE & REDIRECT\n                msg = _('Renaming was successful.')\n                messages.success(request,message=msg)\n                redirect_url = reverse(\"fb_browse\") + query_helper(query, \"\", \"filename\")\n                return HttpResponseRedirect(redirect_url)\n            except OSError, (errno, strerror):\n                form.errors['name'] = forms.util.ErrorList([_('Error.')])\n    else:\n        form = RenameForm(abs_path, file_extension)\n    \n    return render_to_response('filebrowser\/rename.html', {\n        'form': form,\n        'query': query,\n        'file_extension': file_extension,\n        'title': _(u'Rename \"%s\"') % filename,\n        'settings_var': get_settings_var(),\n        'breadcrumbs': get_breadcrumbs(query, path),\n        'breadcrumbs_title': _(u'Rename')\n    }, context_instance=Context(request))\nrename = staff_member_required(never_cache(rename))\n\n\ndef versions(request):\n    \"\"\"\n    Show all Versions for an Image according to ADMIN_VERSIONS.\n    \"\"\"\n    \n    # QUERY \/ PATH CHECK\n    query = request.GET\n    path = get_path(query.get('dir', ''))\n    filename = get_file(query.get('dir', ''), query.get('filename', ''))\n    if path is None or filename is None:\n        if path is None:\n            msg = _('The requested Folder does not exist.')\n        else:\n            msg = _('The requested File does not exist.')\n        messages.warning(request,message=msg)\n        return HttpResponseRedirect(reverse(\"fb_browse\"))\n    abs_path = os.path.join(fb_settings.MEDIA_ROOT, fb_settings.DIRECTORY, path)\n    \n    return render_to_response('filebrowser\/versions.html', {\n        'original': path_to_url(os.path.join(fb_settings.DIRECTORY, path, filename)),\n        'query': query,\n        'title': _(u'Versions for \"%s\"') % filename,\n        'settings_var': get_settings_var(),\n        'breadcrumbs': get_breadcrumbs(query, path),\n        'breadcrumbs_title': _(u'Versions for \"%s\"') % filename\n    }, context_instance=Context(request))\nversions = staff_member_required(never_cache(versions))\n\n\n"}},"msg":"fix a browse directory traversal vulnerability"}}}